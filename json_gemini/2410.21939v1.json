{"title": "BENCHMARKING OPENAI 01 IN CYBER SECURITY", "authors": ["Dan Ristea", "Vasilios Mavroudis", "Chris Hicks"], "abstract": "We evaluate OpenAI's o1-preview and o1-mini models, benchmarking their performance against the earlier GPT-4o model. Our evaluation focuses on their ability to detect vulnerabilities in real-world software by generating structured inputs that trigger known sanitizers. Using DARPA's AI Cyber Challenge (AIxCC) framework and the Nginx challenge project- a deliberately modified version of the widely-used Nginx web server we create a well-defined yet complex environment for testing LLMs on automated vulnerability detection (AVD) tasks. Our results show that the o1-preview model significantly outperforms GPT-4o in both success rate and efficiency, especially in more complex scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have recently transformed various domains, offering new possibilities for automating complex tasks. In the realm of cybersecurity, these models present exciting opportunities for vulnerability detection and program repair-two critical areas that traditionally require significant manual effort. However, evaluating LLMs in such specialized tasks demands carefully designed benchmarks that reflect the complexity and unpredictability of real-world systems.\nIn this paper, we explore the capabilities of OpenAI's newly released o1 model [25] and its smaller, cost-effective counterpart, o1-mini [26], by benchmarking their performance against the earlier GPT-4o model [24]. The goal is to assess how well these models can detect and repair vulnerabilities in real software environments, specifically by generating inputs that successfully trigger known sanitizers.\nWe conducted this evaluation within the framework of DARPA's AI Cyber Challenge (AIxCC) [5], which builds upon the 2016 Cyber Grand Challenge (CGC) [4]. AIxCC introduces a series of challenge projects that task LLMs with automated vulnerability detection (AVD) and automatic program repair (APR), providing an ideal testbed to assess the practical effectiveness of these models in real-world cybersecurity scenarios."}, {"title": "2 VULNERABILITY DISCOVERY AS A TASK", "content": "The AI Cyber Challenge (AIxCC) [5] is an ongoing competition organized by DARPA that leverages LLMs for automated vulnerability detection (AVD) and automatic program re-pair (APR) tasks. Building on DARPA's 2016 Cyber Grand Challenge (CGC) [4], AIxCC introduces new Challenge Projects, where vulnerabilities are deliberately introduced into codebases. These projects are split between a project repository, which contains configu-ration files and details about vulnerabilities detected by sanitizers (e.g., AddressSanitizer (ASan) [10]), and one or more source repositories containing the vulnerable code itself.\nFor the AVD task, competitors' cyber-reasoning systems (CRS) must submit a proof of vulnerability (PoV), which includes a test harness, the input that triggers the vulnerability, and the corresponding sanitizer output. For the APR task, the CRS must provide a patch that fixes the vulnerability without altering the program's functionality, verified by a suite of tests."}, {"title": "2.1 VULNERABILITY DATASET", "content": "One of the projects used in the competition semifinals was a fork of the Nginx web server [28], which has had its git commit history re-written to reduce the scope of the search for vul-nerabilities and remove any information the commit messages present. After the semifinals, the Nginx challenge project was open-sourced [6; 7]. The release also included solutions to the AVD and APR components of the challenge, listing the details of 14 distinct Challenge Project Vulnerabilities (CPV). This includes human-readable explanations \u2013 including a Common Weakness Enumeration (CWE) [16] number -, the sanitizer that detects the vul-nerability, input that triggers it, and which test harness to use, as well as git patches that fix"}, {"title": "3 BENCHMARKING METHODOLOGY", "content": "We benchmarked each model inside a reflexion loop [27]: the LLM is prompted to give feedback on its failed attempts and then use its own feedback on subsequent generations. Figure 1 shows at a high level the evaluation process as a directed graph. The loop was implemented using the LangGraph library [3]. Therefore, we could directly mirror the logic displayed in the graph in the code."}, {"title": "4 QUANTITATIVE RESULTS", "content": "Table 2 provides a detailed breakdown of the performance of the three models tested: GPT-4o-2024-05-13, o1-mini-2024-09-12, and o1-preview-2024-09-12. The table also includes the total evaluation costs for each model, expressed in US dollars. As shown, the o1-preview model significantly outperforms GPT-4o. While the reduced capabilities of o1-mini prevent it from reaching the same performance level as o1-preview, it achieves results comparable to GPT-4o at just one-fifth of the cost. For a clearer comparison, Figure 5 presents the success rate of each model as a percentage of the challenges attempted.\nAccording to OpenAI, the defining characteristic of the o1 models is their ability to \"spend more time thinking before they respond\" [25]. Our benchmark results align with this, as both o1-preview and o1-mini exhibited longer response times compared to GPT-4o. On average, GPT-4o generated a test input in 18 seconds, while o1-mini took 42 seconds, and o1-preview required 1 minute 29 seconds. A similar pattern was observed when generating reflective responses, with GPT-4o taking 13 seconds, o1-mini taking 35 seconds, and o1-preview taking 1 minute 33 seconds.\nAlthough cost reduction was not a primary goal of our evaluation since we submitted the complete content of the harness and associated files to each model-we did observe a significant cost advantage for o1-preview compared to GPT-4o. Despite its higher per-token cost, o1-preview proved more cost-effective overall due to its success rate, which minimized the number of reflexion loops and, consequently, reduced the total token consumption. Techniques such as code slicing, which reduce the size of input files before submission to the LLM, could further lower costs in future evaluations."}, {"title": "5 QUALITATIVE OBSERVATIONS", "content": "We carefully selected the wording of the prompts shown in Figures 2 and 4 to minimize the likelihood of the prompts being flagged as malicious. Despite this, the o1-preview model still returned responses such as \"I'm sorry, but I can't assist with that request\" during the evaluation of cpv2. These refusals may have been triggered by the model's own reflexion output, as they occurred in subsequent attempts. This uncooperative behavior likely contributed to the model's failure on this task, particularly since the refusal did not impact the number of retries allowed. To further understand the capabilities of the new o1-preview model, we now discuss the output of the LLM in specific tasks.\nFigure 6 compares the final output produced by the o1-preview in a failed task against a sample input that successfully triggers the vulnerability. o1-preview correctly identified that the Range header was vulnerable, whereas GPT-4o did not, but was unable to trigger the vulnerability as it required the bytes to be read in reverse order using the non-standard reverse flag -r [8]."}, {"title": "6 RELATED WORK", "content": "This work joins a number of projects that aim to evaluate LLMs on various tasks.\nThe OpenAI o1 preview report [15] evaluates models on a range of autonomy tasks, includ-ing safety-critical and cybersecurity scenarios. For cybersecurity, the models are assessed on tasks such as exploiting web vulnerabilities, demonstrating their ability to navigate real-world threats. The evaluation procedure focuses on measuring models' decision-making under uncertain and dynamic conditions, balancing operational utility and safety, with at-tention to ethical concerns and robustness in adversarial settings. This is the only benchmark that evaluates OpenAI's o1-preview on select cybersecurity tasks, although these tasks differ in nature from ours.\nMeta CyberSecEval2 [2] is a comprehensive benchmark suite designed to evaluate the se-curity risks and capabilities of large language models in offensive cybersecurity tasks. It introduces new tests for prompt injection and interpreter abuse, expanding on previous work to assess models like GPT-4, Mistral, Meta LLAMA 3, and Code LLaMA. The bench-mark also measures the trade-off between safety and utility, quantifying how models respond to borderline benign prompts while rejecting unsafe ones.\nProject Naptime [11] evaluates the offensive security capabilities of large language models (LLMs) by automating vulnerability research. This framework leverages specialized tools such as debuggers, code browsers, and Python interpreters to mimic human researchers' workflows, improving LLM performance in tasks like vulnerability discovery and variant analysis. Naptime showed significant performance improvements over existing benchmarks like CyberSecEval2, although challenges remain in applying LLMs effectively to real-world security research tasks. Naptime was released on June 20, 2024 and evaluated GPT 3.5 Turbo, GPT 4 Turbo, Gemini 1.5 Flash, and Gemini 1.5 Pro."}, {"title": "7 FUTURE WORK", "content": "The next immediate step will be to open-source the benchmark code, allowing for broader collaboration and extension. The benchmark framework is designed to be easily adaptable for testing other language models from various providers. Regarding benchmark tasks, the AIxCC competition's challenge projects offer additional exercises for vulnerability discov-ery. Moreover, the competition's Automated Program Repair (APR) component presents an opportunity to assess a related but distinct capability. In the long term, we envision expanding the benchmark with tasks beyond those provided by AIxCC to further evaluate and push the boundaries of LLM performance in this domain. This will also address the potential risk of these challenges becoming part of the training set of newer language models."}, {"title": "8 CONCLUSION", "content": "We present an early demonstration of the code parsing and vulnerability discovery capabil-ities of OpenAI's newly released o1 models, which significantly outperforms its predecessor, GPT-4o. The o1 model successfully solved 11 tasks, whereas GPT-4o managed only 3, all of which were a subset of those solved by o1. Moreover, o1 required fewer reflexion cycles and achieved a higher task completion rate (11 out of 14), resulting in costs being halved compared to GPT-4o, despite its higher per-token pricing. This benchmark frame-work establishes a foundation for further evaluation of LLM capabilities in the Automated Vulnerability Discovery (AVD) domain."}]}