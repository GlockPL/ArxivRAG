{"title": "Learning Dynamic Cognitive Map with Autonomous Navigation", "authors": ["Daria de Tinguy", "Tim Verbelen", "Bart Dhoedt"], "abstract": "Inspired by animal navigation strategies, we introduce a novel computational model to navigate and map a space rooted in biologically inspired principles. Animals exhibit extraordinary navigation prowess, harnessing memory, imagination, and strategic decision-making to traverse complex and aliased environments adeptly. Our model aims to replicate these capabilities by incorporating a dynamically expanding cognitive map over predicted poses within an Active Inference framework, enhancing our agent's generative model plasticity to novelty and environmental changes. Through structure learning and active inference navigation, our model demonstrates efficient exploration and exploitation, dynamically expanding its model capacity in response to anticipated novel un-visited locations and updating the map given new evidence contradicting previous beliefs. Comparative analyses in mini-grid environments with the Clone-Structured Cognitive Graph model (CSCG), which shares similar objectives, highlight our model's ability to rapidly learn environmental structures within a single episode, with minimal navigation overlap. Our model achieves this without prior knowledge of observation and world dimensions, underscoring its robustness and efficacy in navigating intricate environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Humans effortlessly discern their position in space, plan their next move, and rapidly grasp the layout of their surroundings (1, 2) when faced with ambiguous sensory input (3). Replicating these abilities in autonomous artificial agents is a significant challenge, requiring robust sensory systems, efficient memory management, and sophisticated decision-making algorithms. Unlike humans, artificial agents lack inherent cognitive abilities and adaptive learning mechanisms, particularly when confronted with aliased observations, where sensory inputs are ambiguous or misleading (4).\nTo replicate human navigational abilities, an agent must capture the dynamic spatial layout of the environment, localise itself and predict the consequences of its actions. Most attempts to achieve this combine those fundamental elements in SLAM algorithms (Simultaneous Localisation and Mapping), often based on Euclidian maps (5, 6). However, these methods require substantial memory as the world expands. Other strategies involve deep learning models, which depend on large datasets and struggle to adapt to unexpected events not encountered during training (7). A more efficient alternative lies in cognitive graphs or maps and learning a mental representation of the world from partial observations (8, 9), creating a symbolic structure of the environment (10, 11). Cognitive graphs, by definition, represent a \"mental understanding of an environment\u201d derived from contextual cues like spatial relationships (12). Alongside"}, {"title": "2 RELATED WORK", "content": "As agents navigate their surroundings, they connect observations to construct an internal map or graph of the environment. This intuitive decision-making process is propelled by incentives such as food, safety, or exploration, rapidly guiding agents toward their objective (2).\nMotion planning\nNavigation tasks are often categorised into two primary scenarios based on the agent's familiarity with the environment and its objectives. In scenarios where the environment is partially known and the agent is already aware of its destination, the primary focus is achieving efficient retrieval and executing actions reliably. This entails leveraging existing knowledge about the environment's structure and landmarks to navigate swiftly and accurately towards the predetermined goal (7). Typically, this task centers around solving the motion planning problem: \"How to move from point A to B?\u201d Methods such as those in (22, 23) propose enhanced versions of Model Predictive Path Integral (MPPI) navigation, which calculates the optimal sequence of control actions by simulating multiple future trajectories and selecting the one minimising a cost function. They allow real-time adaptation to obstacles in dynamic environments by continuously updating their navigation based on the most recent state and cost estimates of the control trajectory. However, these methods often rely on precise localisation within a local map, require substantial computational capacity, and may be vulnerable to sensor failures.\nGoal identification\nOur current model is not focusing specifically on solving motion planning, but on determining where the agent is and where it should go based on available information. Unlike traditional models, it does not require precise or absolute knowledge of its position; instead, it relies on its internally inferred belief about its location and considers obstacles relative to this position rather than to a global map. As a result, sensor failures are less critical, provided the agent can adapt to situations with either no sensory input or multiple inputs. In this work, the agent is expected to receive visual observations and detect obstacles through a system analogous to LiDAR. Failure of these sensors, however, would halt navigation.\nWhen the agent must determine both its location and intended direction in addition to planning its movement, the task becomes significantly more complex, especially in unknown environments. The agent must engage in map building and employ a form of reasoning to effectively operate and navigate"}, {"title": "Spatial representation", "content": "Spatial representation plays a significant role in robot navigation and boasts a rich history, offering diverse approaches with their own set of advantages and challenges (6). While many SLAM systems rely on metric maps to navigate, which provide precise spatial information (4, 5), there is a growing interest in topological mapping (29, 28) due to its biological plausibility and lower computational memory requirements. Cognitive maps are mental representations of spatial knowledge that explain how agents navigate and apprehend their environment (21). A cognitive map encompasses the layout of physical spaces (9), landmarks, distances between locations, and the relationships among different elements within the environment (30, 10). Models like the Clone-Structured Cognitive Graph (CSCG) (31) and Transformer representations (32) create cognitive maps (usually represented as topological graphs) using partial observations, offering reusable, and flexible representations of the environment. However, these models often entail significant training time, typically involving fixed policies or random motions and require a statically defined cognitive map dimension.\nIn biological systems, such as animals, the hippocampus plays a crucial role in managing episodic memory, spatial reasoning, and rapid learning (33) while structured knowledge about the environment is gradually acquired by the neocortex (3, 34). This enables remarkable adaptability and efficiency in navigation, with animals often requiring minimal instances to learn and navigate complex environments (1). They leverage cognitive mapping strategies to adapt to changes, swiftly grasp the layout of their surroundings, and efficiently return to previously visited places.\nDrawing inspiration from these natural mechanisms, the compact cognitive map (35) proposes an extendable internal map based on movement information (how far the agent is from any past location) and current visual recognition. Our system shares this goal of remembering significant observations, forming a spatial representation, and resolving ambiguity through contextual cues (36). Furthermore, we extend its adaptability by proactively forecasting the extension of the internal cognitive map before the experience of new spatial information. This anticipatory capability empowers the system to predict potential action outcomes in unobserved areas, enhancing its ability to navigate in unknown territory. This proactive approach aligns more closely with the efficient decision-making processes observed in biological agents, allowing for rapid adaptation and effective navigation in diverse environments."}, {"title": "Active Inference", "content": "Animals navigate their environments adeptly by combining sensory inputs with proprioception (3). This enables them to avoid being misled by repeated evidence, a phenomenon known as aliasing. Such navigation"}, {"title": "3 METHOD", "content": "The method section describes our approach to dynamic cognitive mapping and navigation in unfamiliar environments. Figure 2 provides a step-by-step overview of our model's process, visualised as a flowchart. This includes observing the environment, updating internal matrices with new information, inferring the current state, predicting possible motions, and selecting the next action based on updated beliefs.\nTo demonstrate the agent's navigation abilities, we tested it in a series of mini-grid environments, as illustrated in figure 1, where each map comprises rooms with a consistent floor colour connected by randomly positioned corridors separated by closed doors, all the map layouts used in this work are presented in Appendix 1 and will be further detailed in section 4.\nOur agent starts exploring without prior knowledge of the environment's dimensions or potential observations. It starts by inferring the initial state and pose from the initial observation and expands its model based on anticipating the outcomes of moving in the four cardinal directions. It accounts for potential unexplored adjacent areas before formulating decision-making policies, based on the model's"}, {"title": "3.1 State Inference", "content": "Before each step, the agent engages in state inference by integrating the latest observation and internal positioning, following the POMDP graph depicted in Figure 3, where the current state st and position pt are inferred based on the previous state st\u22121, position pt\u22121 and action at\u22121 leading to the current observation"}, {"title": "3.2 Free Energy", "content": "Typically, agents are assumed to minimise their variational free energy denoted F, which can serve as a metric to quantify the discrepancy between the joint distribution P and the approximate posterior Q as presented in Equation (3) (38). This equation shows how adequate is the model to explain the past and current observations."}, {"title": "3.3 Active Inference and Cognitive Map Navigation", "content": "When navigating within an environment, the agent uses AIF to continuously refine its knowledge of the world by updating its model parameters, more specifically, transition probabilities (how likely it is to move from one state to another) and likelihoods (how likely it is to observe certain features given a state). These quantities are updated, considering the actions taken by the agent and the observations gathered resulting from them. These updates help the agent to reduce the gap between its predicted and actual observations, effectively fine-tuning its internal model to match the real environment better. This approach allows the agent to anticipate future states better and select actions that will minimise future EFE, thus optimising its navigation strategy. Typically, this process assumes that the agent has some prior knowledge about the environment, such as its dimensions or the types of observations it might encounter (44, 13). This prior knowledge allows the agent to form expectations over observations or states, even though it might not know which observations correspond to which locations. For instance, the agent might expect to encounter walls, doors, or specific floor colours, but it does not know where exactly these will be.\nIdentifying a model architecture with the right level of complexity to capture accurately the environment is problematic. Therefore, we take the approach of letting the model expand dynamically and explore as needed. Model expansion, however, requires a trigger indicating the need for an increase in model complexity.\nSome have expanded their model upon receiving new patterns of observations (17, 42). When considering the case of exploring room structured mazes it implies the creation of new states only when the agent physically transitions to a new location/room."}, {"title": "3.4 Cognitive Map Extension", "content": "In this section, we present how the model learns the structure of the environment by expanding the cognitive map based on predictions, before obtaining actual observations, and updating beliefs upon given new evidence.\nEach time the agent moves, the model infers its current state and pose by integrating its motion and observations, subsequently updating the model parameters. In line with the Bayesian reduction model (41), if the observation likelihood Ao fails to encompass the current observation, this indicates the novelty of the observation. As a result, the dimensionality of the observation likelihood is expanded to incorporate the new observation, and the model parameters @ are updated accordingly to reflect this extended observation likelihood.\nThe optimisation of beliefs regarding the generative model parameters @ involves minimising the Free Energy Fe, while accounting for prior beliefs and uncertainties related to both parameters and policies, as outlined in (38):"}, {"title": "4 RESULTS", "content": "Our experiments are designed to assess the efficacy of our model in constructing cognitive graphs by linking visited locations with anticipated ones, thereby enabling efficient exploration and goal achievement.\nWe conducted comparative analyses with CSCG to assess several functionalities. These include learning spatial maps under aliased observations and decision-making strategies aimed at internal objectives, such as exploration or reaching specific observations. Those tests were done with and without prior environmental information (i.e. is the model familiar with the environment layout). Finally, we validated our agent's self-localisation capability following a kidnapping and re-localisation at a random place and how it re-plans after observing obstacles on its path."}, {"title": "4.1 Dynamic Extension", "content": "By moving through space, gathering observations, and updating their internal beliefs, both our model and CSCG exhibit the ability to learn the underlying spatial map of the environment, akin to the navigational capabilities observed in animals (9). However, while a CSCG has a static cognitive map and learns through random exploration, our model starts with a smaller state dimension and makes informed decisions at each step based on its internal beliefs and preferences. This mechanism enhances the relevance of each movement and accelerates the learning process, mirroring the efficient navigation strategies seen in animals (14)."}, {"title": "4.2 Autonomous Localisation", "content": "While exploring, our agent prioritised information gain, in the following experiments we give a preferred observation (a floor colour) to stimulate exploitative behaviour instead. The agent can navigate guided by a preference regardless of whether the environment is familiar or not with the desire to reach that objective. A goal is considered reached if the agent decides to stay at the desired location, thus discarding random stumbling upon it as a successful attempt. Figure 7 shows the average number of steps each model requires to reach the goal, considering both aliased and non-aliased environments. The results are averaged over environment type (aliased maze or not), each model had 20 runs per goal distance and environment. Without prior knowledge about the environment, our model systematically explores the map until it encounters the goal, resulting in a higher number of steps than the oracle, which has precise knowledge of the goal location. However, when we relocate our agent after exploration, keeping the map in memory (i.e. ours wt prior), the results align closely with the oracle in non-aliased environments (Figure 7b) and approach oracle performance in aliased environments (Figure 7a), where the agent may need a few steps to localise itself while searching for the goal. A demonstration of our localisation process is presented in Figure 8. CSCG, on the other hand, exclusively operates with prior knowledge of the environment and employs a Viterbi algorithm for navigation (46). Despite having prior information similar to our agent, CSCG's performance is not consistently superior when provided only with colour observations. As our agent, CSCG also requires self-localisation based on multiple observations in such scenarios. Overall our agent demonstrates excellent efficiency in reaching and recognising the goal. Even without prior over the map, the agent does not return to already explored areas unless necessary, demonstrating a biologically plausible path planning.\nThe impact of aliased observations over our model is demonstrated in Figures 8. The left panel of each figure presents the agent (represented by an X) provided with a prior distribution over the map (agent's state value reported at the upper left of each tile) and the policies' expected free energy with a darker grey colour signifying higher preference to proceed toward that path. The right panel depicts the agent's confidence in its location (i.e. state) given collected observations. The agent starts at the bottom of the T-maze (Figure 8a), observing only the present colour, which is observable in three different locations (here in states 0,1 and 7). The agent can't tell from just this single observation where it is yet. This results in divided confidence between those three states when inferring its potential localisation. In the right panel, we can see that the agent considers going forward as the best option. Notably, the imagined T turns are incorrect due to the agent's confidence in its localisation being mainly split between states 0 and 1, while the agent is, in reality, at state 7. Moving to Figure 8b, representing step 1, the agent adjusts its internal beliefs regarding localisation based on the colour of the previous room and the current observation. This correction in beliefs leads to a refinement in the agent's perceived localisation, as can be seen in the associated bar plot. In Figure 8c, corresponding to step 2, the agent exhibits a significantly higher level of certainty regarding its whereabouts in location 1. It confidently determines that the goal is located either to the left or right but rules out the possibility of it being behind. Finally, Figure 8d portrays the fourth step, where the agent demonstrates full confidence in its localisation and successfully identifies the objective with dark grey shading on the goals. This high level of confidence indicates the agent's strong belief in its internal representations. Therefore, the next steps will correctly lead the agent either right or left toward the preferred observation."}, {"title": "4.3 Dynamic Remapping", "content": "Can the model correctly update its internal cognitive map given new evidence contradicting its prior? To verify that we conduct two experiments. One in the Donut environment, where there are only two paths to reach a goal (a long and a shorter path), by going left or right and a second one reproducing Tolman's second maze experiment with obstacles located at several positions.\nIn our Donut experiment, the agent has previously learned the environment from a randomly selected starting point without any obstacle. Then, the agent was kidnapped to a corner of the map with the newly implemented objective of observing a lightly pink colour (highlighted by a red square in Figure 9). Our model is given a floor colour as objective and exploitation and exploration have equivalent weights in the navigation. Path planning steps should be followed following the sequence from 1) to 10) depicted in Figure 9. At first, the agent successfully plans its path to accommodate the new objective by moving through the upper path, as depicted in the first frame 1) of Figure 9. The dark grey shading indicates the high level of confidence in this path, the darker the shade, the higher the certitude this imagined policy leads to a desired outcome. We disrupted the shortest path by adding an obstacle, as highlighted by a white square in Figure 9. This obstruction invalidated the agent's original route, leading to a notable decrease in the probability of reaching the goal along the intended path (as evident in frame 2) of Figure 9). Consequently, the agent initiates a remapping process promptly, updating its beliefs regarding graph connectivity. The subsequent frames, from frames 3) to 10) in Figure 9, depict the agent's increasing confidence as it navigates closer to the objective using the longest path.\nIn this scenario where a room along the imagined path was unexpectedly closed off, the agent demonstrates its capability to dynamically adapt its navigation strategy in response to changes in the environment, effectively leveraging new observations to revise its path and achieve its objective. Failure to imagine an alternative path would have resulted in the agent stubbornly trying to pass a closed door. This flexible connectivity exists despite using a generative model, which is known to have a hard time revising strong beliefs. This is due to two things, firstly, when experimenting with a blockage, our agent updates its internal model with negative parameter learning (see appendix 2.1). Secondly, the inherent growth of our agent, as it grows in its state or observation dimension, the probabilities get more distributed, so transitions to past states that have not been experimented for a long time naturally weaken over time. Reproducing the mechanism of animal-like synaptic plasticity (47).\nOur second test to assert the robustness of our model to dynamic environments entails replicating the Tolman maze experiment outlined in (48). This maze configuration, illustrated in Figure 10a, involves placing starving rats at a starting point (marked by a red triangle) with food positioned at the opposite end of the map. Three routes are available to reach the food, with Route 1 being the shortest and Route 3 the longest. However, these routes can be blocked at points A and B, where the blockages affect the accessibility of Routes 1 and 2. In our case, the agent can't observe obstacles from a distance, it has to be in contact with them from an adjacent room to learn about it. Each coloured grid in Figure 10b represents an independent closed room and the white box represents an obstacle disposed at positions A and B. Thus, we rely on the \"insight\u201d (49) of our agent and its ability to update its beliefs rather than its perceptual capabilities to reach the goal.\nThis \"insight\" of our agent is based on two key parameters: its planning ability, determining how far ahead it can imagine, and cognitive map plasticity, dictating how well it can adjust past beliefs to new evidence. We allowed the agent to predict action consequences up to 14 steps ahead, thus from any obstacle, the agent could imagine reaching the goal anyway.\nThe model is provided with a red visual observation as its preferred prior throughout the entire experiment with a weight on EFE utility term of 2 (which results in the agent favouring reaching the objective over exploring). The 10 agents consistently start at the bottom of the maze as depicted in figure 10a red triangle. The agents followed three series of 12 runs in the Tolman maze with no obstacle (figure 10b.o), then with an obstacle at position A (figure 10b.A) and finally at position B (figure 10b.B)."}, {"title": "5 DISCUSSION", "content": "This study proposes a novel high-level abstraction model grounded in biologically inspired principles, aiming to replicate key aspects of animal navigation strategies (3, 52). Integrating a dynamic cognitive"}, {"title": "1 ENVIRONMENTS", "content": "The agent's perceptual capabilities are limited to perceiving the current room's colour and obstacle position.\nThe colour observations received by the model are detailed in Figure 12, they are associated with numerical values to aid visualisation. Negative values correspond to un-observable positions, indicating that moving"}, {"title": "2 MODELS COMPLEMENTARY INFORMATION", "content": "Both models know in advance the actions they can take, defined in our mini-grid environments as being up, down, left, right and stay -do not move-. To ease learning we inform both agents that staying means not changing state."}, {"title": "2.1 Our Model Parameters", "content": "Our model leverages the pymdp framework (60), a Python library designed for active inference in discrete state spaces. Initially, the model starts with a 2x2 dimension for both state and observation matrices due to Python's matrix requirements. However, only the first dimension contains meaningful information at the start (indicating the initial position and current observation). As the agent explores, the model dynamically expands its state and observation dimensions, incorporating new information from both its predictions and actual observations.\nThe agent contains as sub-models:\n\u2022 the Markov matrices A\uff61 (observation likelihood) P(ot|st), Bs (state transition) P(st|st\u22121, at\u22121) and Ap (position likelihood) P(pt|st).\n\u2022 the list of successive predicted poses as tuples Bp (position transition) P(pt|Pt\u22121, at-1)\nThe transition probability update, as depicted in Equation 10, incorporates various learning rates based on different scenarios. Specifically, an experimented transition employs a learning rate of 10, while the reverse transition is assigned a rate of 7. An experimented impossible transition (state can't be reached because of an obstacle) is given the same values but a negative learning rate. In contrast, a predicted transition adopts a learning rate of 5 in the forward direction and 3 in the reverse direction. This difference in learning rate is used to adjust the certainty we have about the chosen policy \u03c0."}, {"title": "2.2 Our Model Policies Generation", "content": "We have tried diverse policy creation to reduce the computational load on imagining all possible transitions. Policy algorithms are linked to our models' versioning as well. We have 3 different ways to create policies, from the most computationally expensive to the lightest.\nConsidering the look-ahead as being the number of steps the agent can project itself over. The first policy creation is simply implied to consider all possible combinations for our 5 actions (Up/Down/Left/Right/Stay). This results in an exponential number of combinations:\nWhile training its internal beliefs given the sequence of actions and observations we set the pseudocount"}, {"title": "2.3 CSCG Model Parameters", "content": "The CSCG model is imagined as a static matrix, it is therefore given the information of how many observations it is going to encounter at initialisation. The number of clones has been set up to 10 so it could work in all of our environments without issue. This means that the CSCG model can't cope with environments larger than what its model dimension can incorporate."}, {"title": "2.4 Computational Costs", "content": "We averaged the computation time over 5 runs for both our model and the CSCG model to complete 100 exploration steps. The results indicate that our model is significantly faster, underscoring its relevance for real-world applications. The slower performance of the CSCG model can be attributed to its need to fully retrain every 5 steps, whereas our model continuously updates its internal map with predicted and observed changes at each step. In these tests, our model operated with a look-ahead prediction range of 6 steps."}]}