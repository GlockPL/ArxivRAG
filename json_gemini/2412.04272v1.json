{"title": "POTABLE: PROGRAMMING STANDARDLY ON\nTABLE-BASED REASONING LIKE A HUMAN ANALYST", "authors": ["Qingyang Mao", "Qi Liu", "Zhi Li", "Mingyue Cheng", "Zheng Zhang", "Rui Li"], "abstract": "Table-based reasoning has garnered substantial research interest, particularly in\nits integration with Large Language Model (LLM) which has revolutionized the\ngeneral reasoning paradigm. Numerous LLM-based studies introduce symbolic\ntools (e.g., databases, Python) as assistants to extend human-like abilities in struc-\ntured table understanding and complex arithmetic computations. However, these\nstudies can be improved better in simulating human cognitive behavior when us-\ning symbolic tools, as they still suffer from limitations of non-standard logical\nsplits and constrained operation pools. In this study, we propose POTABLE as a\nnovel table-based reasoning method that simulates a human tabular analyst, which\nintegrates a Python interpreter as the real-time executor accompanied by an LLM-\nbased operation planner and code generator. Specifically, POTABLE follows a\nhuman-like logical stage split and extends the operation pool into an open-world\nspace without any constraints. Through planning and executing in each distinct\nstage, POTABLE standardly completes the entire reasoning process and produces\nsuperior reasoning results along with highly accurate, steply commented and com-\npletely executable programs. Accordingly, the effectiveness and explainability of\nPOTABLE are fully demonstrated. Extensive experiments over three evaluation\ndatasets from two public benchmarks on two backbones show the outstanding\nperformance of our approach. In particular, GPT-based POTABLE achieves over\n4% higher absolute accuracy than runner-ups on all evaluation datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Tables are widely applied in various scenarios (e.g., healthcare (Ghasemi & Amyot, 2016), finance\n(Li et al., 2021)), since they can visually present the core information in various types of scientific\ndocuments (e.g., articles, reports, websites) (Embley et al., 2006) through a structured format. With\nthe growing development of AI techniques, there has been an increasing demand for automated table\nprocessing, attracting significant attention from both academia and industry (Borisov et al., 2022).\nRecently, the evolution of Large Language Model (LLM) (Zhao et al., 2023) has raised a brand\nnew prompting paradigm for table processing (Lu et al., 2024). This training-free method facilitates\ncomplex understanding and reasoning procedures in table question answering (Pasupat & Liang,\n2015), table fact verification (Chen et al., 2020) and other downstream tasks (shown in Figure 1(a)).\nThroughout the history of humankind, tools have been regarded as the crystallization of human\nwisdom and a core factor in social productivity development (Washburn, 1960). This consensus has\ninspired LLM-based techniques to go a step further in simulating more extensive human behavior,\ni.e., collaborating with symbolic tools to overcome LLMs' inherent limitations (Qu et al., 2024). In\ntable processing, two unique challenges have been issued in earlier studies (Lu et al., 2024; Dong\n& Wang, 2024): (1) Tables are structured in two-dimension, leading to unstable memorization of\nLLMs trained in next-token prediction mode (Sui et al., 2024). (2) Table-based reasoning inevitably\ninvolves logical and arithmetic operations, and LLMs may produce misleading results due to their\nlimited calculation abilities. Nevertheless, with a rising trend to utilize databases (Li et al., 2023b),\nPython (Chen et al., 2022; Gao et al., 2023) and other symbolic tools as assistants, recent approaches\neffectively reduce table processing errors and misleading computational results by storing the tabular"}, {"title": "2 POTABLE", "content": ""}, {"title": "2.1 TASK FORMULATION", "content": "Our study focuses on two table-based reasoning tasks, i.e., table question answering and table fact\nverification. Each sample can be represented as (T, Q, A), where T denotes the structured table, Q\ndenotes a question to be answered or a statement to be verified. Given T and Q, our goal is to find\nthe answer A in the table question answering task, while in the table fact verification task, we have\nto decide A = 1 or A = 0 indicating whether the statement is true or false, respectively."}, {"title": "2.2 OVERVIEW", "content": "We propose POTABLE (Programming on Tables Standardly), a novel table-based reasoning method\nshown in Figure 2, where we integrate a Python interpreter as the real-time executor along with\nan LLM-based operation planner and code generator. In POTABLE, the entire analysis process is\nsplit into five logical stages: initialization, row selection, data type cleaning, reasoning and final\nanswering. In each distinct stage, the planning module is given a macroscopic stage goal to generate\noperation chains, while the executing module sequentially generates code for the current operation\nwith the existing code base. In most prior studies, symbolic tools are utilized to obtain intermediate\nresults for certain specified operations of decomposition or computation, as LLMs in reality play a\ndominant role as the global planner and the main processor. In contrast, POTABLE offloads the duty\nof all operation execution to the Python interpreter installing pandas package, while the LLM is\nresponsible for planning reasoning operations and generating code. Consequently, POTABLE better\nleverages LLMs' advantages in thinking decomposition and code generation, reducing errors from\ntheir disadvantages in structured table processing and arithmetic computations."}, {"title": "2.3 HUMAN-LIKE ANALYSIS STAGE SPLIT", "content": "In tabular analysis, humans follow a standard processing pipeline with several stages, such as ini-\ntialization, pre-processing, comprehensive analysis and post-processing. Standard and clear logical\nstage splits enhance reasoning effects and enable humans to draw highly accurate and well-supported\nconclusions. Inspired by human cognitive behavior in tabular analysis, we split the overall analysis\nprocedure into several standard logical stages. These stages will be implemented through Python\ncode with pandas methods as a common choice of human tabular analysts. Specifically, the overall\nanalysis procedure is split into five stages with their own macroscopic goal:\n\u2022 Initialization: Store the table data into pandas.DataFrame object.\n\u2022 Row Selection: Remove redundant rows that do not represent distinct records.\n\u2022 Data Type Cleaning: Transform the data type of table columns into a suitable form.\n\u2022 Reasoning: Conduct flexible reasoning operations that are useful to find the final answer.\n\u2022 Final Answering: Print out the final answer as the output of the evaluated sample.\nIn the above stages, the initialization stage is implemented by executing the pre-defined Python code\nas import pandas as pd and df = pd.DataFrame(data=..., columns=...),"}, {"title": "2.4 PLANNING AND EXECUTING", "content": "To implement the whole table analysis procedure, we adopt a planning module and an executing\nmodule to complete the macroscopic goal in each stage. Such deployments leverage the LLM's\nadvantage in thinking decomposition and code generation, and enjoy the benefits of robust memo-\nrization of structured tables and precise computational results of the symbolic tools simultaneously.\nPlanning. Inspired by Chain-of-Thought (CoT) (Wei et al., 2022) prompting, the LLM decomposes\nthe stage target into operation chains based on the current status of table df, while the output is\nalways formatted as <START>->[OP.]->[OP.]->-><END> for easy operation extraction.\nWe do not restrict the scope of planned operations but only require the operations to be useful in\nachieving the stage target even the overall tabular task goal. To prompt the LLM, we adopt a few-\nshot learning strategy (Brown et al., 2020) with three self-made examples for the planning module.\nExecuting. Given an operation, the LLM generates code based on the current table status df and\nthe existing code base. For the final answering stage, we adopt few-shot prompting with three self-\nmade examples to obtain the code to print out the answer, while in other stages we adopt zero-shot\nprompting to generate the code. Next, the generated code is sent to the Python interpreter for real-\ntime execution. Most of the time, the execution is successful and then the table status is updated\nas the next input from df stored in the Python interpreter. Occasionally, the execution fails as the\ninterpreter raises grammar error information or returns illegal output in the final answering stage. In\nthis case, POTABLE will roll back the interpreter to the status before the current execution, and urge\nthe LLM to regenerate suitable code based on the abnormal information.\nConsequently, the final answer is obtained from the output of the executed code, instead of the direct\nLLM response of an LLM query. The overall algorithmic procedure is shown in Algorithm 1."}, {"title": "2.5 SUMMARIZATION", "content": "According to the detailed procedure, we can conclude that POTABLE overcomes two explicit lim-\nitations in the previous symbolic tools utilization, which enables our work to make great progress\nin simulating human cognitive behavior in tabular data analysis: (1) Human-like analysis stage\nsplit: POTABLE programs on tabular data following a standard logical stage split as a human ana-"}, {"title": "3 EXPERIMENTS", "content": ""}, {"title": "3.1 EXPERIMENTAL SETUP", "content": "Datasets. We conduct experiments on three evaluation sets of two public benchmarks: WikiTQ\n(Pasupat & Liang, 2015) and TabFact (Chen et al., 2020). WikiTQ is a benchmark for table question\nanswering, which requires answering the question with a short corpus based on the given table. We\nconduct experiments over the validation (dev.) set with 2,831 questions and the test set with 4,344\nquestions as previous studies do, and use the official denotation accuracy for evaluation. TabFact is\na benchmark for table fact verification, which requires judging whether the given statement is true\nor false based on the given table. We conduct experiments over the released small test set with 2,024\nstatements as previous studies do, and use the binary classification accuracy for evaluation.\nBackbones. We select two representative language models as the backbones of POTABLE and other\nbaseline approaches in our experiments. Specifically, we choose GPT-4o-mini (2024-07-18) (GPT)\nas the closed-source small language model, which is competent and cost-efficient to cover a wide\nrange of downstream tasks. In addition, we choose Llama-3.1-70B-Instruct (LLAMA) as the open-\nsource LLM for evaluation, which shows strong reasoning capabilities among released foundation\nmodels. Please refer to Appendix for the detailed parameter settings of the backbone models.\nBaselines. We select four competitive LLM-based approaches as baselines for comparison. Binder\n(Cheng et al., 2023) is a neural-symbolic framework that maps the reasoning task into a specific pro-"}, {"title": "3.2 MAIN RESULTS", "content": "Comparison Results. We conduct experiments to compare POTABLE with other baselines over\nthree evaluation datasets of WikiTQ and TabFact on GPT and LLAMA backbones. The result table\nis presented in Table 1. From the main results, it is clear that our POTABLE significantly outperforms\nall other baselines over all evaluation datasets from WikiTQ and TabFact on GPT and LLAMA,\nrespectively. In particular, GPT-based POTABLE achieves over 4% higher absolute accuracy than\nrunner-ups on all evaluation datasets, which demonstrates the superior effectiveness of our method.\nTo be more specific, we make a more comprehensive analysis of results from all approaches and\nbase models. Firstly, Binder is always the runner-up in GPT-based approaches, while its accuracy\ndrop based on LLAMA is 6%-9%. As Binder contains an important step to generate the whole\nprogram for the question, it seems that GPT-4o-mini enjoys a higher ability of code generation in\none go. In comparison, POTABLE has a standard logical split of the whole tabular analysis process\nand generates code once for a single operation with error checking, reducing possible blurred and\nuncleared code in the overall programs. Consequently, this may be one reason that our method\nachieves significant improvement over Binder and others in accuracy. Secondly, in LLAMA-based\napproaches, Chain-of-Table is the second-best approach although it has a constrained operation pool\nfor dynamic selection, while its accuracy drop based on GPT is around 6% in WikiTQ and 1.44% in\nTabFact. Its reasoning performance mainly depends on the LLM's ability to plan and decompose the\noperations rather than code generating since the codes for all operations are pre-defined, which may\nindicate that Llama-3.1-70B-Instruct enjoys a stronger ability to plan and reason. In our POTABLE,\nthe two LLM abilities are fully stimulated, integrating the advantages of the open-world operation\nspace of symbolic tools for more flexible planning and executing simultaneously. This may be\nanother reason that our method outperforms Chain-of-Table and others in accuracy.\nAs a result, POTABLE overcomes the two limitations in the utilization of symbolic tools by following\na human-like logical stage split and executing with an open-world operation space. Compared with"}, {"title": "3.3 PERFORMANCE ANALYSIS GROUPED BY TASK DIFFICULTY AND TABLE SIZE", "content": "To make further performance analysis of POTABLE, we recompute the main performance at different\nlevels of task difficulty and table size. Specifically, we label the difficulty of the evaluated questions\nor statements as \"simple\" or \"complex\u201d. In WikiTQ, a question with a length less than 50 is labeled\nas \"simple\", while a \u201ccomplex\u201d question is longer. As for TabFact, we use the official difficulty\nlabel for all statements. In addition, we group the table content size as \u201csmall\u201d (S), \u201cmedium\" (M)\nand \"large\" (L), in situations when the table has 1-49 cells, 50-99 cells and no less than 100 table\ncontent cells respectively. The detailed grouped results are reported in Table 2.\nThe results illustrate that more complex tasks always lead to performance drop as expected, yet the\nnegative correlation between table size and performance is not always obvious. We can draw two\npreliminary inferences: (1) POTABLE may ignore task decomposition as a potential improvement,\nalthough it is trivial to see performance drop on difficult tasks. (2) POTABLE seems somewhat robust\non the table size, yet a deeper study grouped by table tokens may be more persuasive."}, {"title": "3.4 ABLATION STUDY ON LOGICAL STAGES", "content": "In POTABLE, the overall tabular analysis procedure is split into five stages inspired by standard\nhuman cognitive behavior. To validate the effect of the logical stage split, we present an ablation\nstudy by adopting different stage splits in the compared settings. Specifically, we compare the\noriginal GPT-based POTABLE with the following four settings: (1) Only Reasoning: we discard all\nother unnecessary stages except for initialization, reasoning and final answering. In fact, this setting\nshows no explicit stage split. (2) Removing Row Selection: we give up checking redundant rows\nbefore further processing and reasoning stages, which is commonly regarded as an operation of sub-\ntable data extraction. (3) Removing Data Type Cleaning: we give up checking whether the table\ncolumn data needs type transformation. As all table columns are stored with an initial type of string,\ndiscarding this operation may cause more error execution. (4) Adding Column Selection: we add\na new column selection stage to select relative columns before further processing and reasoning\nstages. This stage has been included in most studies as an operation of sub-table data extraction.\nThe overall results of the ablation study on logical stages are shown in Figure 3.\nWe can see that the original GPT-based POTABLE outperforms all ablated settings in the three eval-\nuation datasets. To be more specific, we make a more comprehensive analysis of results from all\nsettings. Firstly, we focus on the \u201conly reasoning\" setting (abbreviated as only-rea. below). Com-\npared with the original setting, only-rea. scores nearly 0.6%-1% less in WikiTQ and around 3% less\nin TabFact. These results indicate that a standard logical split benefits the tabular analysis process.\nIn addition, only-rea. shows few weaknesses among all other settings in WikiTQ but has more accu-\nracy drop in TabFact. From the task perspective, WikiTQ is about a clear task to answer the question\ndirectly, while TabFact asks to judge the statement, containing intermediate reasoning processes to\njudge the potential sub-facts. Therefore, the logical split for TabFact may be more necessary than\nWikiTQ, and it is also crucial to make the logical stage split as reasonable as possible."}, {"title": "3.5 CASE STUDY", "content": "We conduct a case study of POTABLE in Figure 4 by presenting an evaluated sample from WikiTQ\n(T) with its generated Python program and output answer. The tabular task sample is fed into\nPOTABLE, experiencing a standard analysis process including five logical stages. From the complete\nprogram present, we notice the planned operations (shown as split comments in the stage block)\nand high-quality generated code of each operation (matching the former comment) for real-time\nexecution, which allows us to review the whole process precisely and discover the true reason why it\nleads to right or wrong answers. In addition, the generated operations for execution are open-world\nas long as they are helpful to solve the stage goal, including some helpful commands like inter-\nmediate variable definition and finding the correct variable as the correct answer to output. Along\nwith the answer, POTABLE produces highly accurate, steply commented and completely executable\ncode. These produced outputs indicate the strong effectiveness and explainability of our pipeline."}, {"title": "4 RELATED WORK", "content": "Table Processing with Language Models. Table processing has been a popular research domain\nover the past decade. Before the era of LLMs, numerous efforts were made to process tables with\npre-trained language models. TaPas (Herzig et al., 2020) extends BERT (Devlin et al., 2019) by\nconducting masked pre-training with joint encoding of questions and flattening tables. TaBERT\n(Yin et al., 2020) combines content snapshot and vertical attention based on BERT to obtain joint\ntextual and tabular representations for further understanding. TUTA (Wang et al., 2021) enhances\ntransformers (Vaswani et al., 2017) with structure-aware mechanisms to effectively capture spatial,\nhierarchical and semantic information. TAPEX (Liu et al., 2022) pre-trains BART (Lewis et al.,\n2020) on a large synthetic SQL dataset to imitate the SQL executor that better understands tabular\nstructure information. With the development of LLMs, the paradigm of table processing has been\ndeeply revolutionized, especially in tabular data encoding and reasoning. In prompting methods,\nSui et al. (2024) designs a benchmark to evaluate the structural understanding capabilities of LLMs,\nfollowed by a novel self-augmentation for effective structural prompting. Dater (Ye et al., 2023)"}, {"title": "5 CONCLUSION", "content": "In this paper, we focused on effective utilization of symbolic tools in LLM-based tabular reasoning.\nWe proposed POTABLE, a novel method integrating a Python interpreter as the real-time executor\nalong with an LLM-based operation planner and code generator. POTABLE followed a human-like\nanalysis stage split with an open-world operation space for execution. Through standard program-\nming style, POTABLE returned answers with highly accurate, steply commented and completely\nexecutable code, demonstrating its strong effectiveness and explainability. Extensive experiments\nunder three evaluation datasets of two benchmarks on different backbones presented a dominating\nperformance of POTABLE on table-based reasoning. This study targeted standardized tables in the"}]}