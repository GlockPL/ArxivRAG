{"title": "Improving the Diffusability of Autoencoders", "authors": ["Ivan Skorokhodov", "Sharath Girish", "Benran Hu", "Willi Menapace", "Yanyu Li", "Rameen Abdal", "Sergey Tulyakov", "Aliaksandr Siarohin"], "abstract": "Latent diffusion models have emerged as the leading approach for generating high-quality images and videos, utilizing compressed latent representations to reduce the computational burden of the diffusion process. While recent advancements have primarily focused on scaling diffusion backbones and improving autoencoder reconstruction quality, the interaction between these components has received comparatively less attention. In this work, we perform a spectral analysis of modern autoencoders and identify inordinate high-frequency components in their latent spaces, which are especially pronounced in the autoencoders with a large bottleneck channel size. We hypothesize that this high-frequency component interferes with the coarse-to-fine nature of the diffusion synthesis process and hinders the generation quality. To mitigate the issue, we propose scale equivariance: a simple regularization strategy that aligns latent and RGB spaces across frequencies by enforcing scale equivariance in the decoder. It requires minimal code changes and only up to 20K autoencoder fine-tuning steps, yet significantly improves generation quality, reducing FID by 19% for image generation on ImageNet-1K 2562 and FVD by at least 44% for video generation on Kinetics-700 17 \u00d7 2562.", "sections": [{"title": "1. Introduction", "content": "In recent years, diffusion models (DMs) have emerged as the dominant generative modeling paradigm in computer vision. However, the high dimensionality of visual data poses a significant challenge, making the direct application of diffusion models impractical. Latent diffusion models (LDMs) (Vahdat et al., 2021; Rombach et al., 2022) have become the main approach in mitigating this issue, demonstrating remarkable success in generating high-resolution images (Black Forest Labs, 2023; Betker et al., 2023; Esser et al., 2024) and videos (Brooks et al., 2024; Yang et al., 2024; Kong et al., 2024). A typical LDM consists of two main components: an autoencoder and a diffusion backbone. Most recent breakthroughs have been driven by scaling up diffusion backbones (Peebles & Xie, 2022), while autoencoders (AEs) have received comparatively less attention.\nRecently, the research community has begun focusing more on improving the autoencoders, recognizing their crucial impact on overall performance, but most effort has been concentrated on enhancing reconstruction quality (Black Forest Labs, 2023; Hong et al., 2022; Agarwal et al., 2025; HaCohen et al., 2024; Chen et al., 2025) and achieving higher compression ratios (Agarwal et al., 2025; HaCohen et al., 2024; Chen et al., 2025) to accelerate the diffusion process. However, we argue that a critical yet under-explored aspect, which we refer to as diffusability\u00b9, also plays a key role in determining the utility of autoencoders. Indeed, all three factors-reconstruction quality, compression efficiency, and diffusability are essential for the practical effectiveness of LDMs. Specifically, inaccurate reconstruction sets an upper"}, {"title": "2. Related Work", "content": "Diffusion models. Diffusion models (Vahdat et al., 2021; Rombach et al., 2022; Song et al., 2021; Ho et al., 2020; Nichol & Dhariwal, 2021; Karras et al., 2022) have emerged as the dominant framework for generative modeling, surpassing traditional approaches like GANs (Goodfellow et al., 2014; Karras et al., 2019) and VAEs (Kingma & Welling, 2013). Diffusion models express generation as a denoising process producing the generated content by progressively denoising an initial noise sample. Owing to their efficiency and scalability, foundational generative models (Saharia et al., 2022; Ho et al., 2022; Yang et al., 2024; Podell et al., 2023; Blattmann et al., 2023; Polyak et al., 2024) have made significant strides in synthesizing visually stunning and semantically aligned images and videos.\nInitially applied to low-resolution visual content in the pixel space (Vahdat et al., 2021; Ho et al., 2020; Nichol & Dhariwal, 2021; Karras et al., 2022), they have soon been extended to higher resolutions. In Latent Diffusion Models (LDMs) (Vahdat et al., 2021; Rombach et al., 2022) high-resolution visual content is modeled in the compact latent space produced by a variational autoencoder (VAE) (Kingma & Welling, 2013) within a two-stage framework. Latent Flow Models (LFMs) (Dao et al., 2023; Liu et al., 2024), follow the same approach but leverage Rectified Flows (RFs) to enable faster and more stable sampling.\nRecent work attributes the success of diffusion models to a form of implicit spectral autoregression (Rissanen et al., 2023; Ning et al., 2024) implied by the progressive removal of noise during sampling, resulting in the generation of visual content in a coarse-to-fine manner. Such result holds in the pixel-space of natural images, based on its pattern of decreasing spectral power (Ruderman, 1997). We show that popular autoencoders (Black Forest Labs, 2023; Agarwal et al., 2025; HaCohen et al., 2024; Yang et al., 2024) have a less pronounced pattern of decreasing spectral power, inhibiting implicit spectral autoregressive generation. Building on this observation, this work proposes a regularization scheme that re-establishes this property, consistently showing improved LDM performance and avoiding the need for explicit coarse-to-fine generation.\nImage and video autoencoders. Due to the success of LDMs, a lot of effort has been devoted to the development of better AEs. Image LDMs (Rombach et al., 2022) and early video diffusion models (Blattmann et al., 2023; Guo et al., 2023) employ a spatial AE with a compression ratio of 1\u00d78\u00d78. The rapid advancement of video diffusion models poses the demand for 3D AEs that jointly compress spatial and temporal dimensions to further improve efficiency (Hong et al., 2022; Zhou et al., 2024; Kong et al., 2024). Among them, Open-Sora (Zheng et al., 2024) inherits the 1\u00d78\u00d78 spatial AE and trains a decoupled 4\u00d7"}, {"title": "3. Improving Diffusability", "content": "We begin this section by discussing the spectral decomposition of 2D signals and providing some background on discrete cosine transform in Section 3.1. In Section 3.2, we analyze the spectral properties of latent spaces across different autoencoders and compare them to those of the RGB space. Our main insight is that the frequency profile of the latent space includes large-magnitude high-frequency components. We also show that as the channel size increases, the high-frequency components become more pronounced. Additionally, we demonstrate that the widely adopted KL regularization only increases the strength of these components. Finally, Section 3.3 presents a straightforward method to improve the diffusability of a latent space of an autoencoder by enhancing its spectral properties."}, {"title": "3.1. Background: Blockwise 2D DCT", "content": "The discrete cosine transform (DCT) (Ahmed et al., 2006) over a 2D signal is a transformation converting the signal's representation between the spatial and frequency domains. DCT, in particular, represents the original input signal as coefficients for a set of horizontal and vertical cosine basis oscillating with different frequencies. More formally, given a 2D signal block $A \\in \\mathbb{R}^{B\\times B}$ whose values $A_{xy}$ denote the pixel intensity at position $(x, y)$, the two-dimensional type-II DCT yields a frequency-domain block $D \\in \\mathbb{R}^{B\\times B}$ where $D_{uv}$ captures the coefficient for the corresponding horizontal and vertical cosine bases:\n$D_{uv} = \\alpha(u)\\alpha(v) \\sum_{x=0}^{B-1} \\sum_{y=0}^{B-1} A_{xy} f(x, u) f (y, v)$,\nwhere $\\alpha(u) =\\begin{cases}\\sqrt{1/B}, & u = 0,\\\\sqrt{2/B}, & u\\neq 0,\\end{cases}$\n$f(x, u) = cos(\\frac{(2x+1)u\\pi}{2B})$.\nIn practice, we split the input 2D signal into non-overlapping blocks of size B \u00d7 B and treat each channel independently.\nBy analyzing RGB images and latents in the DCT frequency domain, we produce a frequency profile that relates to the energy of the signal at every frequency. A zigzag frequency index is used to map each DCT block D \u2208 RB\u00d7B into a one-dimensional sequence following the standard zigzag ordering as in JPEG (Wallace, 1991), which indexes the DCT co-efficients from lowest frequency D0,0 to highest frequency DB-1,B\u22121. Formally, let zigzag(u, v) \u2208 {0, . . ., B2 \u2013 1} denote the ranks of the coefficient Duv in ascending frequency order. Given a block, we compute its DCT and"}, {"title": "3.2. Spectral Analysis of the Latent Space", "content": "We begin our analysis by observing the frequency profile of the latent space in the Flux (Black Forest Labs, 2023) family of autoencoders to establish a relationship with diffusability. For the purpose of this study, we train a family of FluxAE models with various channel sizes for 100k steps"}, {"title": "3.3. Scale Equivariance Regularization", "content": "Effective regularization should achieve two key objectives: (i) to suppress high-frequency components in the latent"}, {"title": "4. Experiments", "content": "Data. We trained all the autoencoders on in-the-wild data which do not overlap with ImageNet-1K (Deng et al., 2009) or Kinetics-700 (Carreira et al., 2019) to make sure that there is no data leak in the autoencoders, and that they remain general-purpose. For this, we used our internal image and video datasets of the 2562 resolution, which are similar in distribution of concepts and aesthetics to the publicly available in-the-wild datasets like COYO (Byeon et al., 2022) and Panda-70M (Chen et al., 2024b). To control for the impact of the data (and also the training recipe), we trained a separate autoencoder baseline for each setup"}, {"title": "4.1. Improving Existing Autoencoders", "content": "We apply our training pipeline on top of 3 different autoencoders. For each autoencoder, we trained it while freezing the last output layers to avoid breaking their adversarial fine-tuning, which should have no impact on the latent space (Chen et al., 2025). We emphasize that none of the explored modern autoencoders publicly released their training pipelines. For the pretrained snapshots of autoencoders, we used the original snapshots available in the diffusers library (von Platen et al., 2022).\nImproving image autoencoders. For the image autoencoder, we used FluxAE (Black Forest Labs, 2023) with 8 \u00d7 8 compression ratio and 16 latent channels (since it is the most popular modern autoencoder in the commu-"}, {"title": "4.2. Ablations", "content": "Does scale equivariance hurt reconstruction? scale equivariance in VAEs improves downstream generation quality in terms of FID (Tables 1 and 2). We now examine its impact on AE reconstruction quality. Table 4 presents results across four reconstruction metrics PSNR, SSIM, LPIPS (Zhang et al., 2018), and FID, on 50,000 samples from ImageNet"}, {"title": "5. Conclusion", "content": "We have shown that modern Latent Diffusion Models (LDMs) rely just as critically on their autoencoders as on the more frequently investigated diffusion architectures. While prior work has largely focused on improving reconstruction quality and compression rates for the autoencoders, our study focuses on diffusability, revealing how latent spectra with excessive high-frequency components can hamper the downstream diffusion process. Through a systematic analysis of several autoencoders, we uncovered stark discrepancies between latent and RGB spectral properties and demonstrated that they lead to worse LDM synthesis quality. Building on this insight, we developed a regularization strategy that aligns the latent and RGB spaces across different frequencies. Our approach maintains reconstruction fidelity and improves diffusion training by suppressing spurious high-frequency details in the latent code. Potential future directions include exploring more advanced frequency-based regularizations, adaptive compression methods and scale equivariance regularization in the temporal axis for video autoencoders to further optimize the trade-off between reconstruction quality, compression rate, and diffusability."}, {"title": "Impact Statement", "content": "This work focuses on improving the representations in autoencoders that serve as a backbone for latent diffusion training, ultimately enhancing generative performance. Our improvements can facilitate beneficial applications such as boosting creativity, supporting educational content creation, and reducing computational overhead in generative workflows. Beyond these considerations, we do not identify additional ethical or societal implications beyond those already known to accompany large-scale generative modeling."}, {"title": "A. Implementation Details", "content": "DiT model details. To strengthen the baseline DiT performance, we integrated into it the latest advancements from the diffusion model literature. Namely, we used self conditioning (Jabri et al., 2023) and RoPE (Su et al., 2024) positional embeddings. Besides, we switched to the rectified flow diffusion parametrization (Albergo & Vanden-Eijnden, 2022; Liu et al., 2022; Vahdat et al., 2021), which was recently shown to have better scalability with a fewer amount of inference steps (Esser et al., 2024).\nDiT training details. All the DiT models are trained for 400,000 steps with 10,000 warmup steps of the learning rate from 0 to 0.0003 and then its gradual decay towards 0.00001. We used weight decay of 0.01 and AdamW (Loshchilov, 2017) optimizer with beta coefficients of 0.9 and 0.99. We used posterior sampling from the encoder distribution for VAE-based autoencoders. In contrast to the original work, we found it helpful to do learning rate decay to 0.00001 using the cosine learning rate schedule. We used the same model sizes for DiT-S (small), DiT-B (base), DiT-L (large) and DiT-XL (extra large), as the original work (Peebles & Xie, 2022):\n\u2022 DiT-S: hidden dimensionality of 384, 12 transformer blocks, and 6 attention heads in the multi-head attention.\n\u2022 DiT-B: hidden dimensionality of 768, 12 transformer blocks, and 12 attention heads in the multi-head attention.\n\u2022 DiT-L: hidden dimensionality of 1024, 24 transformer blocks, and 16 attention heads in the multi-head attention.\n\u2022 DiT-XL: hidden dimensionality of 1152, 28 transformer blocks, and 16 attention heads in the multi-head attention.\nWe used gradient clipping with the norm of 16 for all the DiT models. Our models were trained in the FSDP (Zhao et al., 2023) framework with the full sharding strategy on a single node of 8\u00d7 NVidia A100 80GB GPUs or 8\u00d7 NVidia H100 80GB GPUs (depending on their availability in our computational cluster).\nFor CV-AE, since it is considerably slower than other autoencoders, we trained LDMs on pre-extracted latents. For this, we pre-extracted them on random 17-frames clips. In essence, this reduces\nAutoencoders training details. Since none of the autoencoders had their training pipelines released, we had to develop the training recipes for each of the autoencoder baselines individually which would not be detrimental to neither their reconstruction capability nor downstream diffusion performance. To do this, we ablated multiple hyperparameters (the most important ones being learning rate and KL regularization strength) to arrive to a proper setup. We chose the KL weight in such a way that the KL penalty maintains approximately the same magnitude as the pre-trained checkpoint.\nEach autoencoder is trained with AdamW (Loshchilov, 2017) optimizer, with betas of 0.9 and 0.99, and weight decay of 0.01. The learning rate was grid-searched individually for each autoencoder and is provided in Table 5. In all the cases, we used mixed precision training with BFloat16.\nDuring training, we maintained an exponential moving average of the weights (Karras et al., 2024), initialized from the same parameters as the starting model, and having a half life of 5,000 steps.\nWe emphasize that, when applying our regularization strategy on top of an autoencoder baseilne, we do not alter other hyperparameters (like learning rate), except for KL regularization which we disable for SE-regularized models (even though we found it helpful in some of our explorations).\nFor each autoencoder, we freeze the last output layers of the decoder. The motivation is the following: they were fine-tuned with the adversarial loss, which we want to exclude from the equation without hurting the ability of an autoencoder to model textural details which FID would be sensitive to (Rombach et al., 2022) and which do not influence the latent space properties. Namely, we freeze the last normalization and output convolution layers. In each case, the amount of frozen parameters constitute a negligible amount of total parameters.\nOther hyperparameters for autoencoders training are provided in Table 5."}, {"title": "B. Additional Exploration", "content": "In Section 3, we outlined the base scale equivariance strategy to regularize the spectrum of an autoencoder which has a strong advantage of being very easy to implement by a practitioner. However, it could be beneficial to possess more advanced tools for a finer-grained control over the latent space spectral properties. This section outlines them and provides the corresponding ablation."}, {"title": "B.1. Explicitly Chopping off High-Frequency Components", "content": "Rather than applying downsampling to produce latents and RGB targets for regualrization, it is possible to replace some ratio of high-frequency components with zeros. To do so, DCT is applied to the latents and RGB targets where a chosen set of frequency components are masked out. The modified components are then translated back to the spatial domain by inverse DCT to form the training latents and reconstruction targets.\n$\\mathcal{L}_{CHF}(x) = d(x, Dec(z)) + \\alpha d(\\mathcal{D}^{-1}(\\mathcal{D}(x) * M), Dec(\\mathcal{D}^{-1}(\\mathcal{D}(z) * M)) + L_{reg}$,\nwhere $\\mathcal{D}$ and $\\mathcal{D}^{-1}$ represent DCT and its inverse, respectively. M is a B \u00d7 B binary mask indicating which frequencies to zero out defined as follows:\n$\\mathcal{M}(u, v) =\\begin{cases}1 & \\text{if } zigzag(u, v) < B^2 - N,\\\\0 & \\text{otherwise}.\\end{cases}$ \nN controls the frequency cutoff. We provide the ablation for this strategy in Table 6."}, {"title": "B.2. Soft Penalty for High-Frequency Components", "content": "Instead of directly removing some of the components, which might become a too strict regularization signal, one can consider penalizing the amplitudes of high-frequency components in a soft manner. Concretely, given a B \u00d7 B block, we construct the following weight penalty matrix:\n$W_{uv} = (u + v)^{P/B^{P}} .$\nNext, the soft regularization loss itself is computed as:\n$\\mathcal{L}_{softreg} = \\sum_{u,v} |D_{uv}(z)|.W_{uv} .$"}, {"title": "B.3. ImageNet 5122 experiments", "content": "We trained our DiT-L/2 for class-conditional 5122 ImageNet-1K generation for 400K steps for FluxAE (Black Forest Labs, 2023), the results are presented in Table 8."}, {"title": "B.4. Ablating regularization strength a", "content": "To ablate the importance of the regularization strength a, we train FluxAE for 10,000 steps with a varying strength. The results are presented in Table 9."}, {"title": "C. Additional visualizations", "content": "This section provides additional visualizations for the LDM experiments."}, {"title": "D. Limitations.", "content": "We identify the following limitations of our work and the proposed regularization:\n1. While we did our best to verify that our framework works in the most general setup possible, testing 4 different autoencoders across 2 different domains (image and videos), our study would be more complete when verified across other diffusion parametrizations (Karras et al., 2022; Ho et al., 2020; Kingma & Gao, 2023) or architectures (Karras et al., 2024).\n2. We observed that our regularization still affects the reconstruction slightly: for example, Table 4 shows that FluxAE FID increased from 0.183 to 0.55 (though for some AEs, like CogVideoX-AE, it improves). We are convinced that this FID increase could be mitigated by training with adversarial losses, which we omitted in this work for simplicity.\n3. There is a mild sensitivity to hyperparameters: for example, we found that varying the SHF regularization weight might improve the results (see Table 9), or adding a small KL regularization (which we disabled in the end for our regularization for simplicity).\n4. None of the explored autoencoders released their training pipelines, and it is non-trivial to fine-tune them even without any extra regularization. For example, we observed that any fine-tuning of DC-AE (Chen et al., 2025) was leading to divergent reconstructions in our training pipeline (we explored dozens of different hyperparameter setups).\nWe leave the exploration of these limitations for future work."}]}