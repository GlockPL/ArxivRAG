{"title": "MoE-CT: A Novel Approach For Large Language Models Training With Resistance To Catastrophic Forgetting", "authors": ["Tianhao Li", "Shangjie Li", "Binbin Xie", "Deyi Xiong", "Baosong Yang"], "abstract": "The advent of large language models (LLMs) has predominantly catered to high-resource languages, leaving a disparity in performance for low-resource languages. Conventional Continual Training (CT) approaches to bridge this gap often undermine a model's original linguistic proficiency when expanding to multilingual contexts. Addressing this issue, we introduce a novel MoE-CT architecture, a paradigm that innovatively separates the base model's learning from the multilingual expansion process. Our design freezes the original LLM parameters, thus safeguarding its performance in high-resource languages, while an appended MoE module, trained on diverse language datasets, augments low-resource language proficiency. Our approach significantly outperforms conventional CT methods, as evidenced by our experiments, which show marked improvements in multilingual benchmarks without sacrificing the model's original language performance. Moreover, our MoE-CT framework demonstrates enhanced resistance to forgetting and superior transfer learning capabilities. By preserving the base model's integrity and focusing on strategic parameter expansion, our methodology advances multilingual language modeling and represents a significant step forward for low-resource language inclusion in LLMs, indicating a fruitful direction for future research in language technologies.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Ouyang et al., 2022; Brown et al., 2020b) have achieved remarkable progress in recent years, particularly in areas such as language generation (Radford et al., 2019; Brown et al., 2020a), machine translation (Vaswani et al., 2017; Wu et al., 2019), text summarization (See et al., 2017; Rush et al., 2015), and language understanding (Devlin et al., 2019a; Peters et al., 2018). However, the majority of these models have focused on resource-rich languages such as English, leaving substantial potential for performance improvements in low-resource languages. To alleviate above mentioned issues, researchers have aimed to Parameter-Efficient Fine-Tuning (PEFT) methods. Specifically, continual training (CT) has been proposed, which have proven to be effective in enhancing the performance of low-resource languages by training on specific language data. Besides, other researchers have resorted to method of Low-Rank Adaptation (LoRA) (Hu et al., 2021). By introducing a low-rank structure to reduce the model parameters for efficient updating. Thereby, it achieves a successful success between maintaining model performance and saving computation abd storage. Despite these developments, research on how to extend the multilingual capabilities of large models are still limited. We conducted an in-depth analysis of existing capability extension technologies.\n1.  Firstly, it is difficult to obtain original training data. In conventional training methods, the original training data accounts for a significant proportion, so that the lack of data\nposes a significant challenge for the training in use. In addition, due to the different distributions between the training data of different stages, the absence of original training data will significantly exacerbate the catastrophic forgetting problems.\n2.  Secondly, adding large amounts of raw data will limit the improvement of multilingual capabilities. As illustrated in Figure 1, with the volume of original language data greatly alleviated the issue of catastrophic forgetting, significantly improving the generation capability in original data distribution. When the volume of original language data is five times greater than oth new one, the catastrophic forgetting issue is almost disappeared. However, it limits the model's final performance on multilingual tasks. Furthermore, a large amount of original data significantly increases the training costs of the model.\nIn response to the aforementioned issues, we propose a method that utilizes a Mixture of Experts (MoE) (Fedus et al., 2021; Lepikhin et al., 2020)approach. The Mixture of Experts model integrates multiple experts into the model architecture, where each expert is tasked with learning a specific task or feature subspace, thereby enhancing the model's learning capacity and generalization performance. Our specific approach involves extending additional expert networks on top of the pre-trained model to more efficiently learn new multilingual capabilities. To effectively prevent catastrophic forgetting, we froze most of the parameters of the original LLM. In addition, we employed a frozen shared feed-forward network (shared-ffn) to preserve the original knowledge, and implemented a gating mechanism to dynamically merge the original knowledge with the newly acquired knowledge from the expert networks. The crux of this method is that it enables us to effectively retain the capabilities of the base model, even with only a limited amount of original data. Our approach loosens the restrictions on the proportion of multilingual data, allowing it to play a more significant role during training. This not only raises the ceiling for the model's performance on multilingual tasks but also reduces training costs due to a substantial reduction in the overall volume of data.\nIn our proposed method, we selected Qwen as our foundational model and, based on this, expanded the MoE network with 2-8 experts per layer."}, {"title": "2 Related Work", "content": "Multilingual Pretraining and Fine-tuning in Language Models.Developments in deep learning have significantly improved performance across a broad spectrum of natural language processing (NLP) tasks. Early efforts utilizing neural architectures such as recurrent neural networks (RNNs) (Zaremba et al., 2014)and long short-term memory (LSTM) (Sutskever et al., 2011) networks have laid the foundation for understanding and generating"}, {"title": "3 Method", "content": "As discussed above, it is evident that the multilingual capabilities of large-scale models remain constrained. Our goal is to maintain a robust performance in the generation of high-resource languages, effectively mitigating the risk of catastrophic forgetting, while simultaneously enhancing the multilingual capabilities of the model.\nTo achieve this, we introduce a novel architectural paradigm named MoE-CT, which substantially integrates diverse knowledge domains into our model, by employing an MoE (Mixture of Experts) approach tailored specifically for the multilingual context."}, {"title": "3.1 Mixture of Experts", "content": "In the described model, we have a collection of N feed-forward neural networks, which are all structurally equivalent and operate independently, denoted as a set of experts $E_{i=1}^N$. These experts are integrated with a dedicated gating mechanism, denoted as $G(\\cdot)$, which functions as a decision-maker. This gating mechanism is responsible for determining the contribution of each expert's output in the final response of the system. Specifically, if we consider h to be the resultant vector from an attention mechanism in any block of the model, then the final output y from the mixture of experts (MoE) layer is derived by taking a weighted sum of each expert's output. Mathematically, this can be described by the equation:\n$y = \\sum_{i=1}^N G(h)_i E_i(h)$\nIn this expression, $E_i(h)$ represents the output of the i-th expert network, while $G(h)_i$ signifies the weight allocated to that output by the gating function. The gating function itself is defined using a softmax operation applied to the dot product of the input h and a set of learned parameters encapsulated in the matrix $W_g$, which is expressed as:\n$G(\\cdot) = Softmax(h \\cdot W_g)$\nIn essence, the matrix $W_g$ holds the parameters that the gating function adapts during training to optimize the routing of information through the various experts in the MoE architecture."}, {"title": "3.2 Routing mechanism design", "content": "Our goal is to store multilingual knowledge in the MoE layer and store English/Chinese knowledge in the old FFN layer of pretrained LLM. We hope that the sparse extended multilingual LLM can combine the outputs of the two layers to achieve the best world of multilingual and English/Chinese abilities. Therefore, we have added a fusion module with different structures to explore the best combination of MoE and FFN layers. The output of the fusion module can be described as follows:\n$Fusion(x, y) = w \\cdot x + (1 - w) \\cdot y$ (1)\nIn the equation, w represents the weight of the MoE layer, which, when combined with the weight of the shared-ffn, sums to 1."}, {"title": "3.3 An innovative model training methodology", "content": "As shown in Figure 2, our method extends large language models from dense structures to sparse structures by adding MoE layer parallel to the Feed Forward Layer. The MoE layer includes multiple FFN layers, but only a small portion will be activated, and the activation strategy is determined by the router module. We use the parameters of FFN layer from pretrained LLM to initialize multiple FFN layers in the MoE layer, which can significantly accelerate the training process and provide valuable knowledge transfer from old one to new one..\nThroughout the continuation training process, we exclusively train the expert networks and the embedding layer within the expanded MoE model architecture, while all other structural parameters and shared-FFN parameters are kept fixed. Through experimental validation, we have ascertained that such a training strategy optimally retains the model's original capabilities while effectively expanding its multilingual capacity."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Training Datasets", "content": "The composition of our data and the proportion of each language variety are reflected in Table 1. Our continued training data set comprises a total of 24 billion tokens, of which 20 billion are multilingual data encompassing ten languages (Arabic, Indonesian, Vietnamese, German, French, Japanese, Thai, Portuguese, Spanish, and Italian), with each language accounting for 2 billion tokens. Additionally, there are 2 billion tokens of Chinese data and 2 billion tokens of English data. All of the data were extracted from mC4 (Xue et al., 2020) and Wikipedia.\nIn addition, we prepared an extra 50 billion tokens for Chinese and 50 billion tokens for English to ensure that the proficiency in these languages does not diminish during the standard continual training (CT) process. Empirical evidence suggests that for the expanded multilingual dataset of 20 billion tokens, we need to incorporate at least five times more Chinese and English data to maintain their resistance to forgetting. However, within our Mixture of Experts (MoE) expanded architecture, we found that only 4 billion tokens of Chinese and English data are required to achieve resistance to forgetting in these languages, which substantially reduces the cost of training data."}, {"title": "4.2 Architecture Setting", "content": "We leverage Qwen (Bai et al., 2023), a large language model pre-trained using a large amount of Chinese and English data as our base model, exhibits industry-leading performance in both Chinese and English. Due to limitations in training"}, {"title": "4.3 Multilingual Benchmark", "content": "We used six test datasets to evaluate the multilingual and Chinese/English capabilities of our models, and we divided them into three task categories: NLU, Knowledge task and Machine Translation.\nPlease refer to Table 3 for detailed information.\nNLU For Natural Language Understanding (NLU) task category, we choose three multilingual test task: XNLI, XCOPA and PAWS-X.\nKnowledge The Knowledge task category includes two test dataset, C-Eval and MMLU, which are used to evaluate the Chinese and English capabilities of our models respectively.\nMT For Machine Translation tasks, we selected 9 language test datasets from WMT and IWSLT to evaluate the translation ability from these languages to English and English to these languages."}, {"title": "4.4 Main Results", "content": "The main results are shown in Table 4 and Table 5, we conducted experiments on Qwen-1b8 and Qwen-7b models to verify the general effectiveness of our method. As shown in Table 4, Qwen-1b8-CT is a multilingual continue-training version of Qwen-1b8, which has a significant improvements of multilingual abilities over Qwen-1b8 model, improves the accuracy of XCOPA from 54.7% to 62.3%, XNLI from 38.0% to 44.7%. For machine translation task, multilingual continue-training brings in an average of 4.0 BLEU on En-XX directions, but no improvement is found on XX-EN directions.\nAlthough continue-training strategy can improve multilingual ability of LLM, it always causes problem of catastrophic forgetting, which damages the original abilities of the model. We can find that Qwen-1b8-CT model has a significant drop on English-to-Chinese and Chinese-to-English translation performance, from 35.6 to 27.2 and 22.1 to 15.2, respectively. For Chinese and English ability evaluation, Qwen-1b8-CT achieves a performance decline from 48.5 to 29.7 and 37.0 to 29.5 on C-Eval and MMLU test set, respectively. What's more, simply continue train the LLM on multilingual corpus may not improve multilingual abilities, as we find a performance decline on PAWS-X test dataset (53.0 vs 51.0). Catastrophic forgetting problem can also be found on Qwen-7b model in Table 5. We have also experimented with the use of LORA-CT, setting the LoRA dimension to 8, as shown in Table 5, although LoRA-CT can effectively alleviate the issue of catastrophic forgetting, its limited number of changed parameters results in a significant performance gap in extended multilingual capabilities when compared to conventional CT.\nBy using the sparse MoE architecture for continual training of LLM, the best results can be achieved in both multilingual and Chinese and English ability. In Table 5, we can find that Qwen-7b-MoE-CT performs equivalently to Qwen-7b-CT in multilingual tasks, and significantly prevents the performance degradation of Chinese and English capabilities, where performance on C-eval only decreases from 57.4 to 55.5 and MMLU decreases from 42.7 to 42.3. Our method also alleviates the decline in the performance of English-to-Chinese and Chinese-to-English translation, and has a significant improvement in the translation ability on English-XX directions from 24.0 to 28.4. The details of translation results on Qwen-1b8 and Qwen-7b can be found in Table 9 and Table 10. In our experiments, we have identified a significant data ratio problem when employing conventional continual training (CT) methods to combat catastrophic forgetting. Specifically, to preserve the proficiency in Chinese and English, the volume of data for these languages must be at least five times greater than that of the multilingual dataset. However, as indicated in Table 6, such an excessive reliance on Chinese and English data significantly hampers the enhancement of multilingual capabilities. Moreover, it substantially increases the training cost, posing a major obstacle to the expansion of large language models.\nIn contrast to the aforesaid conventional CT, our proposed MoE expansion technique significantly reduces the dependence on Chinese and English data, with these languages' data requiring only a one-fifth proportion of the multilingual dataset. As demonstrated in Table 6, due to the reduced incorporation of Chinese and English data in our MoE framework, the model can more effectively assimilate multilingual knowledge. Consequently, the amplification of multilingual abilities is more pronounced compared to the conventional CT methods. This approach highlights the efficiency of our MoE expansion method in achieving a better and more efficient balance between preserving original language capabilities and enhancing expanded multilingual proficiencies."}, {"title": "4.5 Ablation Study", "content": "In this study, we conducted ablation experiments on the freezing strategy, routing mechanism, and the number of experts used in the MoE expanded architecture to verify their effects on multilingual improvement and resistance to forgetting of Chinese-English bilingual capabilities.\nTraining strategy. In our experiments on the Qwen-1.8B scale model, we attempted to freeze different parts of the MoE model to determine which components could retain the model's original capabilities. As indicated in Table 7, continual training all parameters or attention layers will result in a significant decrease in the model's original ability, while training expert layers and embedding layers can balance the original ability and expanded ability. Therefore, in the final model, we chose to train the parameters of the experts and the embedding layer.\nRouting mechanism. In our experiments, we attempted various combination methods for the outputs of the shared feed-forward network and the MoE layer. Initially, we utilized a weighted sum approach for integrating the two, assigning MoE output weights from 0.1 to 0.9. Our experiments revealed that a lower MoE weight corresponded to a more pronounced resistance to catastrophic forgetting, yet resulted in a smaller improvement in multilingual capabilities. Conversely, a higher MoE weight weakened the resistance to catastrophic forgetting while yielding a greater enhancement in multilingual capabilities. As shown in table 8, when the weights for both the MoE and the shared-ffn are set to 0.5, an optimal balance is achieved between the enhancement of multilingual capabilities and the resistance to forgetting."}, {"title": "5 Conclusion", "content": "In conclusion, our research presents a significant advancement in the field of multilingual language modeling, particularly addressing the challenges posed by catastrophic forgetting in large language models (LLMs). Through the introduction of the MoE-CT structure, we have demonstrated a novel approach that not only enhances the extension of LLMs to low-resource languages but also preserves the original linguistic competencies in high-resource languages. Our experiments on models such as Qwen-1b8 and Qwen-7b have validated the effectiveness of MoE-CT, marking clear improvements in multilingual benchmarks while maintaining or even improving performance in the original languages.\nThe MoE-CT structure showcases a delicate balance between stability and plasticity, ensuring that the base model's parameters remain undisturbed and that the newly introduced MoE layers absorb the additional linguistic knowledge. This balance is crucial for achieving a harmonious integration of multilingual capabilities without the detriment of pre-existing language proficiencies. Our findings indicate that MoE-CT can achieve substantial resistance to forgetting with a minimal amount of pre-training data, which is a considerable stride towards reducing training costs and resources.\nThe implications of our work are manifold. Primarily, it facilitates the creation of more inclusive language technologies that do not favor solely high-resource languages. Furthermore, it paves the way for future research into continual learning for LLMs, emphasizing the importance of models that can continually evolve and adapt to new language without losing previously established knowledge."}, {"title": "Limitations", "content": "In summary, we have proposed the MoE-CT architecture to address the issue of catastrophic forgetting encountered by LLMs during the expansion of multilingual capabilities. Due to the limitations of computational resources, we have not attempted to extend the MoE-CT architecture to other open-source models beyond Qwen, which may not fully demonstrate the catastrophic forgetting challenges faced by all LLMs. Therefore, our future work will explore whether this structure can be adapted to a wider range of open-source models."}, {"title": "Ethics Statement", "content": "Our work on the MoE-CT for LLMs considers several ethical concerns. Primarily, we aim to address linguistic biases by enhancing LLMs performance in low-resource languages, promoting inclusivity and cultural diversity. We recognize the risk of potential biases in model training and commit to their mitigation. We also acknowledge the responsibility to prevent the misuse of our model for deceptive purposes and advocate for its transparent and responsible use. Environmental impacts due to the high computational requirements of LLMs are also considered. Our model aims to reduce training resources, and we encourage sustainable practices in Al research."}]}