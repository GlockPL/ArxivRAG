{"title": "Improving ICD coding using Chapter based Named Entities and Attentional Models", "authors": ["Abhijith R. Beeravolu", "Mirjam Jonkman", "Sami Azam", "Friso De Boer"], "abstract": "Given considerable breakthroughs in natural language processing (NLP), many state-of-the-art methods were created to automate various processes. However, NLP in the clinical domain is mainly limited to curated and benchmark datasets, which may not reflect how well they can operate in real-world situations. Automatic ICD coding is one such NLP process that heavily relies on public datasets (MIMIC-III), which are imbalanced and old. Most of them have used the attention mechanism to assign weights to the features in the dataset that belong to each ICD code. The F1 scores from the existing methods show that the micro-averages are between 0.4 - 0.7, suggesting that there are still a lot of false positives. Our research allowed us to propose an approach that can improve the ICD coding and achieve better F1-scores. This approach uses chapter-based named entities and attentional models to perform two tasks: categorize discharge summaries in ICD-9 Chapters and create an attentional model using only the data that belongs to a chapter because the models don't need to see or know the outside data to identify codes within a chapter. For categorizations, we have proposed an approach using Chapter-IV for de-biasing and influencing the important entities and weights of the Chapter without relying on neural networks. This allowed us to create categorization thresholds that can fit the data perfectly and provide faulty threshold values and interpretability for the human coders to correct and validate the categorizations. Once the categorizations are validated, we have selected 3 frequent and 3 non-frequent codes that belong to Chapter-IV to create the attentional models using two architectures: Bidirectional-Gated Recurrent Units (GRUs) + Attention and Transformer with Multi-head Attention. The selected codes reflect most of the codes in Chapter-IV. Averaging the Micro-F1 scores from all six models for the two architectures gives 0.79 and 0.81, showcasing significant improvements in the performance of ICD coding.", "sections": [{"title": "I. INTRODUCTION", "content": "Clinical narratives serve as the primary mode of communication in the healthcare industry, offering a unique account of a patient's history and assessments [1] that can be used in both the clinical context and research. However, the majority of valuable information is in unstructured/semi-structured text [1][2] that is large, unrestrictive, and ambiguous [2]. Sometimes using human language is not precise enough to communicate information, especially in healthcare; it is sometimes difficult to understand or interpret another clinician's diagnosis summary if the language consists of ambiguities, metaphors, and colloquial expressions [2]. To tackle this problem, several standard computer-readable vocabularies (ICD, RxNorm, SNOWMED, etc.,) have been created for healthcare systems so that clinical and administrative information can be communicated effectively. For decades, healthcare providers have relied on manual 'medical coders' for coding diagnosis or treatment information in a clinical text based on well-maintained vocabularies. With significant advances in natural language processing (NLP), researchers have proposed methods to automate/semi-automate the processes, showcasing state-of-the-art performances. However, performance evaluation of such approaches has been limited to curated and clean benchmark datasets that may not properly reflect how robustly these systems can operate in real-world situations [3][32]. And one key obstacle in the clinical domain is data accessibility, mainly due to ethical constraints [32] on sharing documents with personal information and strict governance regulations to de-identify the electronic health records (EHR) data [4].\nInternational Classification of Diseases (ICD) is one such standardized vocabulary that relied on manual code assigners for a long time. And existing methods for automating the ICD code assignment using the clinical text have heavily relied on the benchmark (MIMIC-III) [5] and curated datasets. With MIMIC-III, there are only ~9000 ICD-9 codes in total, covering 58,976 unique admissions with a mean of 11 ICD codes per admission (min: 1, 25%: 6, 50%: 9, 75%: 15, max: 39). Various state-of-the-art architectures and methods [6] \u2013 [15] for ICD coding were proposed using MIMIC-III. Most of them have used the attention mechanism to assign weights to words/text snippets in the dataset associated with each ICD code. Along with the semantic features from the clinical notes, researchers have also integrated knowledge bases that are created using original ICD code titles and associated"}, {"title": "II. METHODS", "content": "A. Data Creation\nThe dataset used for this study is MIMIC-III, which is a freely available database that consists of health-related information associated with distinct hospital admissions. Most of the existing methods for ICD coding have used discharge summaries for performing their experiments. To create the datasets for this research, two major tables from MIMIC-III were selected: Diagnosis_ICD & Noteevents.\n1) Dataset\nThe Diagnosis_ICD dataset consists of 58,976 unique HADM_IDs with a mean of 11 ICD codes per admission. In MIMIC-III, each ICD code of a patient is separated by a 'row'. Therefore, the ICD codes are grouped together based on their HADM IDs.\nAnd the Noteevents dataset consists of medical texts that belong to 15 categories (Discharge Summaries: 59,652). The datasets Diagnosis_ICD and Noteevents are grouped and merged based on their HADM_IDs, creating 52,726 discharge summaries.\n2) Labelling\nTo label the data according to the ICD codes (ex: 285.9 vs Rest in Chapter - IV), we have created an algorithm (see Fig. 5) that can create the required datasets automatically, that is, labelling 285.9 as normal (1) and other ICD codes between 280-289 as anomalous (0). Before applying the labelling function (Fig. 4), we used the values 2, 3, 4, 5 to extract digits from the ICD codes based on their index (see Fig. 3). This allows us to easily divide the original data according to its chapters and sub-chapters. For example, extracting the first two digits (28) of the ICD codes (281.1, 285.1) by using the value '2' allows us to easily divide the data according to its chapters (Chapter-IV (280-289)). Similarly, value '3' is good for the sub-chapters.\n3) Pre-processing of Discharge Summaries\nUsing simple regular expression patterns (see Table. 2), we have searched (Search()) and extracted the required sections present in the discharge summaries: history of present illness, pertinent results, brief hospital course, and discharge diagnosis. Not all the discharge summaries (52,726) in MIMIC-III follow a similar structure or have similar section headings, but most of them do."}, {"title": "III. IMPLEMENTATION AND RESULTS", "content": "A. Named Entity Categorization (NEC) Model\nIn this research, we are not building NEC models for all the chapters (19), rather, we are building 1 NEC model for Chapter IV (diseases of the blood and blood-forming organs). This model is used for performing the experiments on the de-biasing and influencing criteria used in this research. Fig. 9 showcases how the final weights and entities (146) are used for categorizing the short-summaries into Chapter-IV.\nWhen compared with the results in Table. 3, the categorization thresholds created using our approach (entities and weights) can outperform the Bi-GRUs + Attention Model by a significant margin in categorizing the short-summaries into Chapter-IV.\nB. Attentional Models for Codes between 280-289\nFrom the list of Chapter-IV ICD codes available in the MIMIC-III dataset, we have selected 3 frequent and 3 non-frequent codes (see Table. 5). Datasets are created for these codes using only the short-summaries that belong to Chapter-IV (15,293). They are labelled based on one-vs.-rest approach (see Fig. 4).\nWe have selected the codes that can reflect all the codes that belong to Chapter-IV. There are ICD codes that are very rare (< 30 summaries) in MIMIC-III. To create the attentional models for these codes we should use enough data/summaries to collect the code features, which is not possible with MIMIC-III. It is better not to consider them at all. Researchers have used knowledge bases created using ICD code names and associated synonyms as a workaround for rare labels; however, if we cannot extract a good number of textual features from the summaries, even the KBs won't be enough.\nTo create the attentional models, we have used two attention mechanisms: dot-product Attention and multi-head Attention. Tabel. 6 showcases the configuration we used for training the models. We have mostly used similar configurations for the codes in the Table. 5. Only the gated recurrent unit dimensions (G) in the BiGRUs model and feed-forward upward projection (Df) in the Transformer model are selected based in the best performance."}, {"title": "C. Attention Models for Codes between 280-289 (IV)", "content": "In this research, we have implemented two simple network architectures that are based on the attention mechanism [28]: Bidirectional Gated Recurrent Units (Bi-GRUs) with Attention and a Transformer with Multi-Head Attention. For the past few years, the attention mechanism has garnered a significant amount of interest, particularly in sequence tasks [29]. The term \u201cattention\u201d can have quite a few different meanings, depending on who you ask or where you look, but the one that applies best to this research is as follows: The attention mechanism is used to describe a weighted average of (sequence) elements, with the weights being dynamically computed based on an input query and the keys of the elements, as shown in the equation below.\nAttention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V\nHere, Q is a set of queries Q \u2208 R^{T \u00d7 dk} for each token in an input sentence, which are used to match against a series of keys K\u2208 R^{T x dk} that describe values V\u2208 R^{Txdy}, where T is the sequence length, and dk and dy are the hidden dimensionality for queries/keys and values respectively.\nMost attention mechanisms differ in terms of what Queries they use, how the Key and Value vectors are defined, and what Score function is used. Sometimes using a single weighted average is not enough to attend to multiple different aspects present in a sequence, thus, allowing the extension of the attention mechanism to multiple heads, which runs through the attention mechanism several times in parallel (multi-head Attention). Using Multi-attention heads allows us to attend to the parts of an input sequence differently [28]. Specifically, given a Q, K, and V matrix, we transform them into h sub-queries, sub-keys, and sub-values passed through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix. Mathematically, we can express this operation as:\nMultihead(Q, K, V) = Concat(head\u2081, ...., head)Wo\nWhere head\u2081 = Attention(QW\u00a1\u00ba, KWK, VW\u2081)\nUsing these architectures, we have created the attentional models for six codes between 280-289 {3: frequent, 3: non-frequent}. To create training and testing datasets for the architectures, we have used the short-summaries (15,293) that belong to Chapter-IV. These summaries are labelled based on one-vs-rest approach, that is, labelling the summaries for the code 285.9 as '1' and others in 280-289 as '0'. This is how we labelled the datasets for all the codes we used in this research. Creating separate attentional models for codes in Chapter-IV using only the data within Chapter-IV showcased better performance than creating the attentional models directly for all the codes."}, {"title": "IV. EXPERIMENTS", "content": "A. Interpretability for NEC Model\nThe approach we proposed for the NEC model does not require much computation or training. It is a simple approach that uses only 146 entities and their weights to create the categorization thresholds (see Table. 4) that can categorize the short summaries into Chapter-IV or REST. One of the primary and initial steps for the NEC model is using a pre-trained model to extract DISEASE entities [] from the summaries. Using simple regular expression patterns and matching functions, we are able to create weights and find entity matches in the summaries to sum up scores that can fit into the thresholds. This simple approach is as effective as neural networks. However, due to the imbalanced and unstructured nature of the data in MIMIC-III, both approaches can perform incorrect categorizations. One way to tackle this problem is to provide interpretability, which allows us to show the 'human coders' what entities (out of 146) are found in the summaries. These entities can be used to provide information such as anatomical regions, diseases, signs, and symptoms, which can be used for associating with the ICD code names and also provide a way for correcting and validating the categorizations.\nWe conduct our experiments to identify the thresholds with many incorrect categorizations. Whenever a summary falls into these \"faulty\" thresholds, a human coder can verify them using the matched entities"}, {"title": "V. CONCLUSION", "content": "This study proposes a chapter-based method for improving ICD coding from discharge summaries. Our approach first categorizes the discharge summaries into a chapter, and only then the discharge summaries that belong to that Chapter are used to create data based on the one-vs-rest approach. Initially, we processed the full-length discharge summaries by removing unnecessary information/headings that can add noise, thus, allowing us to create short summaries with less vocab size. These short summaries are used in our research to perform chapter-based categorization and create attentional models. We have proposed a simple approach using regular expression patterns and matching functions for the chapter-based categorization to identify/match the important entities from the short summaries. This important entity list is created with the help of a pre-trained NER model that can detect diseases, conditions, signs, and symptoms from clinical texts. We initially extracted the entities from all the summaries using the pre-trained model. We have extracted 375 most common entities from these entities based on document frequency. After applying the de-biasing and influencing criteria, we have decreased the entities to 146 with modified or improved weights. These 146 entities are used to find matches in the short summaries, and the weights are used to sum up, the identified entities. This allows us to create categorization thresholds that fit the data perfectly and provide faulty thresholds and interpretability for the human coders to correct and validate the categorizations.\nSeveral researchers have proposed various methods for automating the ICD coding, but so far, the performance evaluation of these approaches has been limited to carefully curated and clean benchmark datasets (MIMIC-III), which may not be reflective of how well these systems perform in the real world. As a whole, MIMIC-III only includes around 9,000 ICD-9 codes, most of which are unbalanced. The MIMIC-III results from existing methods show that Micro-average F1-scores are between 0.4 and 0.7, indicating that there are still many false positives. Our approach achieved average Micro-F1 Scores (6 codes in Table. 7) of 0.79 and 0.81 for our models (BiGRU, Transformer), showcasing significant improvements in the performance of ICD coding."}]}