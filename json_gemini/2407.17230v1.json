{"title": "Improving ICD coding using Chapter based Named Entities and Attentional Models", "authors": ["Abhijith R. Beeravolu", "Mirjam Jonkman", "Sami Azam", "Friso De Boer"], "abstract": "Given considerable breakthroughs in natural language processing (NLP), many state-of-the-art methods were created to automate various processes. However, NLP in the clinical domain is mainly limited to curated and benchmark datasets, which may not reflect how well they can operate in real-world situations. Automatic ICD coding is one such NLP process that heavily relies on public datasets (MIMIC-III), which are imbalanced and old. Most of them have used the attention mechanism to assign weights to the features in the dataset that belong to each ICD code. The F1 scores from the existing methods show that the micro-averages are between 0.4 - 0.7, suggesting that there are still a lot of false positives. Our research allowed us to propose an approach that can improve the ICD coding and achieve better F1-scores. This approach uses chapter-based named entities and attentional models to perform two tasks: categorize discharge summaries in ICD-9 Chapters and create an attentional model using only the data that belongs to a chapter because the models don't need to see or know the outside data to identify codes within a chapter. For categorizations, we have proposed an approach using Chapter-IV for de-biasing and influencing the important entities and weights of the Chapter without relying on neural networks. This allowed us to create categorization thresholds that can fit the data perfectly and provide faulty threshold values and interpretability for the human coders to correct and validate the categorizations. Once the categorizations are validated, we have selected 3 frequent and 3 non-frequent codes that belong to Chapter-IV to create the attentional models using two architectures: Bidirectional-Gated Recurrent Units (GRUs) + Attention and Transformer with Multi-head Attention. The selected codes reflect most of the codes in Chapter-IV. Averaging the Micro-F1 scores from all six models for the two architectures gives 0.79 and 0.81, showcasing significant improvements in the performance of ICD coding.", "sections": [{"title": "I. INTRODUCTION", "content": "Clinical narratives serve as the primary mode of communication in the healthcare industry, offering a unique account of a patient's history and assessments [1] that can be used in both the clinical context and research. However, the majority of valuable information is in unstructured/semi-structured text [1][2] that is large, unrestrictive, and ambiguous [2]. Sometimes using human language is not precise enough to communicate information, especially in healthcare; it is sometimes difficult to understand or interpret another clinician's diagnosis summary if the language consists of ambiguities, metaphors, and colloquial expressions [2]. To tackle this problem, several standard computer-readable vocabularies (ICD, RxNorm, SNOWMED, etc.,) have been created for healthcare systems so that clinical and administrative information can be communicated effectively. For decades, healthcare providers have relied on manual 'medical coders' for coding diagnosis or treatment information in a clinical text based on well-maintained vocabularies. With significant advances in natural language processing (NLP), researchers have proposed methods to automate/semi-automate the processes, showcasing state-of-the-art performances. However, performance evaluation of such approaches has been limited to curated and clean benchmark datasets that may not properly reflect how robustly these systems can operate in real-world situations [3][32]. And one key obstacle in the clinical domain is data accessibility, mainly due to ethical constraints [32] on sharing documents with personal information and strict governance regulations to de-identify the electronic health records (EHR) data [4].\nInternational Classification of Diseases (ICD) is one such standardized vocabulary that relied on manual code assigners for a long time. And existing methods for automating the ICD code assignment using the clinical text have heavily relied on the benchmark (MIMIC-III) [5] and curated datasets. With MIMIC-III, there are only ~9000 ICD-9 codes in total, covering 58,976 unique admissions with a mean of 11 ICD codes per admission (min: 1, 25%: 6, 50%: 9, 75%: 15, max: 39). Various state-of-the-art architectures and methods [6] \u2013\n[15] for ICD coding were proposed using MIMIC-III. Most of them have used the attention mechanism to assign weights to words/text snippets in the dataset associated with each ICD code. Along with the semantic features from the clinical notes, researchers have also integrated knowledge bases that are created using original ICD code titles and associated synonymous terminology to improve code classification [16] \u2013\n[19]. The results available from these methods (on MIMIC-III) showcase that micro-average F-scores are in the range of 0.4 to 0.7, with the area under the curve (AUC) close to 1, suggesting that there are still a lot of incorrect predictions (see Table. 1). This could be due to various factors such as:"}, {"title": "1) Class imbalance", "content": "Most of the ICD codes in MIMIC-III consist of imbalanced data with large deviations between normal and anomalous classes, and some ICD codes only have a few samples or even no samples for training the model, which results in poor prediction accuracy in rare labels and even providing unreliable results for frequent codes due to class imbalance."}, {"title": "2) High false-positives from the models (or feature weights)", "content": "To demonstrate this point, we have selected the ICD codes (280 289) from Chapter IV (Diseases of the Blood and Blood-Forming Organs (ICD-9)). Most of the diseases and conditions (ICD-9 codes) that belong to Chapter-IV in MIMIC-III are not part of the primary diagnosis (i.e., code is not present at index 0 or 1 (mostly are primary) in the ICD code list that belongs to a patient/discharge summary). In MIMIC-III, there are 19,006 (total = 52,726) unique discharge summaries that belong to Chapter-IV. Reducing the original ICD code list of a patient to 'list of 3' and 'list of 5' (i.e., from the head) allowed us to identify whether Chapter-IV codes are present in them or not. Out of 19,006 discharge summaries, there are only 3,328 and 6,632 summaries that belong to the reduced lists 3 and 5, which suggests that codes from Chapter- IV mainly arise due to adverse reactions from prescribed drugs or complications in various anatomical regions. Counting the occurrence of each ICD code at index 0 and 1 showcases that Chapter-IV codes are mainly associated with the chapters: diseases of the circulatory system, infectious and parasitic diseases, and diseases of the digestive system (at Index 0); diseases of the genitourinary system, diseases of the circulatory system, and diseases of the respiratory system (at Index 1). These associations can cause the models (or feature weights) to provide wrong codes for many discharge summaries.\nWe have implemented two architectures: Bidirectional- Gated Recurrent Units (GRUs) with Attention and a Transformer with Multi-head Attention, to fit the normal class against the anomalous class, that is, to extract feature weights about Chapter-IV, all the discharge summaries in MIMIC-III that belong to that Chapter (19,006) are labelled as 'normal' and others in the dataset as 'anomalous' (33,720). This allows us to see how the models (or feature weights) distinguish between the ICD codes of Chapter IV (280-289) and the Rest of the codes. All the summaries are processed to remove unnecessary information (symbols, numbers, whitespaces, etc.) that might degrade the performance; and create a lower-case string (summary) with a continuous sequence of words from head to tail. To train the models, we used a maximum sequence length (M) of 3000 as the input length because it covers ~95% of the data and is two standard deviations from the average (~1500). And all the other parameters are selected based on the trial and error method. When tested using a test- set {1: 2,773, 0: 4,993}, the models gave TPR of 75% (1) and TNR of 68% (0) for the transformer model in Table. 1. Even with bias, the transformer model at epoch 20 is classifying the anomalous class as normal, producing many false positives."}, {"title": "3) ICD codes with no associated words (synonyms) in some discharge summaries", "content": "In MIMIC-III, there are discharge summaries that describe similar conditions differently. Some of these summaries don't have medical words (or features) that can be directly linked with an ICD code name or associated synonyms, as others do. This suggests that some of the codes for discharge summaries in MIMIC-III are based on various disease conditions and their associated symptoms. This can cause the models to not provide some codes for the discharge summaries during prediction because, for some discharge summaries, there are fewer 'attentional features' in the text than others."}, {"title": "4) Unnecessary information in the discharge summaries", "content": "Most of the discharge summaries in the MIMIC-III dataset are organized into different sections, such as history of present illness, past medical history, various examination reports, pertinent results, brief hospital course, medications on admission and discharge, discharge diagnosis, and others. Some of these sections are not useful during auto-ICD coding and could add noise to the models during training. For example, in the section 'past medical history,' there is past diagnosis information that has nothing to do with current admission or diagnosis. When we use past medical information with the present information to train a model, the features obtained by the model from the past section can provide codes that are not related to the present admission to discharge information. This can result in a lot of incorrect or prediction of codes for full-length discharge summaries in MIMIC-III. To tackle this problem, we have applied various regular expression patterns (see Section- II) to remove unnecessary sections and create a \u2018short-summary' that only has information related to present admission till discharge. For the Rest of the research, we have used the short summaries for training the algorithms and extracting feature weights associated with different labels (or codes).\nVarious researchers have proposed methods for auto-ICD coding by implementing approaches such as: using ~9,000 ICD codes (all) or ~7000 ICD codes in MIMIC-III and using the Top 100 or 50 codes in MIMIC-III [6][8][16]. Most proposed methods are evaluated using AUC scores (along with others) with specious values greater than 90%. However, the fl-scores (0.4-0.7) for these methods don't have specious values and tell a different story about the overall performance. If the extracted features cannot differentiate Chapter IV Codes vs Rest accurately, applying the attention mechanism could not produce good attentional models/weights with fewer incorrect predictions.\nThe current version of ICD (ICD-11) has over 55,000 diagnostic codes, up from around 14,000 with ICD-10, making it difficult for researchers to propose methods (using MIMIC- III or others) that won't result in high redundancies when capturing the features for all the ICD codes separately. And the existing methods that are proposed using ~9000 codes have assigned attentional weights for all the codes together from the dataset (52,726). This can cause the codes to have similar features (with different weights) in their attentional models, resulting in many incorrect predictions. Therefore, rather than creating the attentional models directly from the entire dataset, this research would like to create and separate the attentional models according to ICD chapters, as shown in Fig. 2. To create the attentional weights for codes in a chapter, only data (in MIMIC-III) that belongs to that Chapter will be used, because the algorithm during training does not need to see the data from chapters that are outside, which is not the case in most of the existing methods.\nIn recent years, many deep neural network models have achieved state-of-the-art performance gains in various natural language processing (NLP) tasks [20]. However, these gains mostly relied on the availability of large amounts of labelled examples, without which state-of-the-art performance is rarely achievable [21]. This is especially inconvenient for many NLP fields where labelled examples are scarce, such as medical texts. Our research believes combining named entity recognition (NER) with deep learning (or attention mechanism), as shown in Fig. 2, can improve the performance of auto-ICD coding.\nInitially, the NER models are used to categorize the short-summaries according to their chapters, and then the attentional models that belong to the chapters are used to provide codes for the categorized summaries. The NER models can provide interpretability by listing the medical entities (diseases, symptoms, etc.,) that are present in a discharge summary. These entities can be used to validate or correct the categorization. And the attentional models for the respective chapters are created using only the data that belongs to those chapters; that is, to create an attentional model (or feature weights) for code 285.9, we should create a dataset based on one-vs-rest approach (285.9 vs. Rest in Chapter \u2013 IV). This allows us to extract better feature weights for the ICD codes than before and improve the overall performance. Section - II describes in detail the methods used in this research, from processing the discharge summaries to the prediction of the codes."}, {"title": "II. METHODS", "content": "The dataset used for this study is MIMIC-III, which is a freely available database that consists of health-related information associated with distinct hospital admissions. Most of the existing methods for ICD coding have used discharge summaries for performing their experiments. To create the datasets for this research, two major tables from MIMIC-III were selected: Diagnosis_ICD & Noteevents."}, {"title": "1) Dataset", "content": "The Diagnosis_ICD dataset consists of 58,976 unique HADM_IDs with a mean of 11 ICD codes per admission. In MIMIC-III, each ICD code of a patient is separated by a 'row'. Therefore, the ICD codes are grouped together based on their HADM_IDs.\nAnd the Noteevents dataset consists of medical texts that belong to 15 categories (Discharge Summaries: 59,652). The datasets Diagnosis_ICD and Noteevents are grouped and merged based on their HADM_IDs, creating 52,726 discharge summaries."}, {"title": "2) Labelling", "content": "To label the data according to the ICD codes (ex: 285.9 vs Rest in Chapter - IV), we have created an algorithm (see Fig. 5) that can create the required datasets automatically, that is, labelling 285.9 as normal (1) and other ICD codes between 280-289 as anomalous (0). Before applying the labelling function (Fig. 4), we used the values 2, 3, 4, 5 to extract digits from the ICD codes based on their index (see Fig. 3). This allows us to easily divide the original data according to its chapters and sub-chapters. For example, extracting the first two digits (28) of the ICD codes (281.1, 285.1) by using the value '2' allows us to easily divide the data according to its chapters (Chapter-IV (280-289)). Similarly, value '3' is good for the sub-chapters."}, {"title": "3) Pre-processing of Discharge Summaries", "content": "Using simple regular expression patterns (see Table. 2), we have searched (Search()) and extracted the required sections present in the discharge summaries: history of present illness, pertinent results, brief hospital course, and discharge diagnosis. Not all the discharge summaries (52,726) in MIMIC-III follow a similar structure or have similar section headings, but most of them do. Table. 2 showcases the patterns that are used and the number of matches that are found in MIMIC-III.\nWe want to create data that covers all the four sections. To do so, we have merged all the matches found for the sections, creating a dataset of 35,352 discharge summaries (or short- summaries). Finally, these short-summaries are processed (cleaned) using various regular expression patterns to remove symbols, numbers, whitespaces, newline-spaces, etc. And all the words in the summary are converted to lower-case letters so that the model won't assign separate weights for Anemia and anemia. This allowed us to create short-summaries with continuous sequence of words from beginning to end, with no breaks or numbers. These short-summaries are used for creating NER models for chapter-level prediction and attentional models for ICD code predictions. Using \u201cshort- summary/version\" of the discharge summaries showcased similar or better performance than full-length summaries. Comparing the results in Table. 1 and Table. 3 suggests that, short-summaries can classify (on a test-set) the normal class with 75% true-positive rate when trained till 3 epochs, which is better than the full-length results at epoch 3 (67%) (see Table 1). Here, we have used a maximum sequence length (M) of 1000 and GRU dimension (LSTM units) of 1024 for training the models with short-summaries, compared to 3000 and 1024 for full-length. The M values are assigned by calculating the mean length of sequences in the dataset. And the GRU dimension is decided based on the trail and error method. For example, training the models with full-length summaries using the parameters of short-summary models in Table. 3, the true-positive and -negative rates at epoch 3 are 59% (1) and 90% (0), which is evident that the parameters should be adjusted based on the datasets."}, {"title": "B. Named Entity Categorization (NEC) Models", "content": "Machine reading (MR) is essential for unlocking valuable knowledge present in millions of medical documents of an EHR [22]. Recognizing entities such as body parts, diseases, disease conditions, and symptoms from discharge summaries can be helpful in categorizing the data according to its main chapters. Catling, F. et al. [23] in their research used text representations from gated recurrent units (GRUs) to classify the discharge summaries (MIMIC-III) into 19 chapter-level (Level-1) labels with fl-scores of 0.68 0.7. GRUs are particularly attractive, as they produce a similar performance to long short-term memory (LSTMs) while using a simpler design with fewer trainable parameters [24]. LSTMs and GRUs have been used to represent sequential healthcare data, such as multivariate time series, text documents, and others [25]. Various researchers have utilized them to detect the word and character-level features automatically in texts using NER and POS-tagging [26], eliminating the need for most feature engineering. However, in this research, rather than using CNN and LSTMs directly for classifying the discharge summaries into 19 Chapters, we propose to create simple and separate named entity categorization models for each Chapter (i.e, 19 Models). The NEC models created in this research do not have many underlying machine-learning components. The only component we used is a pre-trained model (en_ner_bc5dr_md), which is used for extracting \u201cDISEASE\" entities present in all the short summaries (35,352) (see Fig. 5). It is crucial to notice that the pre-trained model we used has F1-Score of 84.53% [27], which means, not all the medical entities present in the summaries can be identified by the pre-trained model. There will be some unnecessary entities (Ex: 'of,' 'the,' 'and,' \u2018pain,' 'htn,' etc.). From the entities of short-summaries in Chapter-IV (15,293), we have selected 375 of the most prevalent entities, which are then reduced to 300 after cleaning (Fig. 5)."}, {"title": "1) Criteria for De-biasing", "content": "After generating two sets (set1 and set2) of frequency scores/weights for the 300 entities, we have subtracted them (set1 set2) so that the weights in set2 that are greater than or equal to set1 can be removed (see Fig. 6). For example, the entities 'stenosis' and 'chronic' has weights of {0.35, 0.35} and {0.29, 0.35} in the two sets (Fig. 6). Subtracting the set2 weight's from set1 produces values {0} and {-0.06}, which means, if the entities 'stenosis' and 'chronic' are used for categorizing the short summaries of Chapter-IV, they could be responsible for the wrong classification of summaries in the REST dataset. Therefore, our criteria for de-biasing is to remove the entities that have negative values or 0 after subtracting both sets (see Fig. 6). This reduced the total entities from 300 to 141. And the categorization results (for Chapter-IV) obtained using the de-biased weights of the 141 entities showcased better or similar performance to that of the original entities and weights (300) from Chapter-IV (set1) and the attention model in Table. 3."}, {"title": "2) Criteria for Influencing", "content": "After creating the de-biased weights, we have influenced them using two rules/criteria:\n\u2022 Create a list of dictionary items closely related to Chapter-IV according to literature and some official websites. And add the new items to the de-biased weights list by assigning a weight of 0.5 for all of them (see Fig. 7).\n\u2022 If the difference between the weights of entities in set1 and set2 is greater than 0.1, then the weights for those entities are doubled (see Fig. 7)."}, {"title": "C. Attention Models for Codes between 280-289 (IV)", "content": "In this research, we have implemented two simple network architectures that are based on the attention mechanism [28]: Bidirectional Gated Recurrent Units (Bi-GRUs) with Attention and a Transformer with Multi-Head Attention. For the past few years, the attention mechanism has garnered a significant amount of interest, particularly in sequence tasks [29]. The term \u201cattention\u201d can have quite a few different meanings, depending on who you ask or where you look, but the one that applies best to this research is as follows: The attention mechanism is used to describe a weighted average of (sequence) elements, with the weights being dynamically computed based on an input query and the keys of the elements, as shown in the equation below.\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\nHere, Q is a set of queries $Q \\in R^{T \\times d_k}$ for each token in an input sentence, which are used to match against a series of keys $K \\in R^{T \\times d_k}$ that describe values $V \\in R^{T \\times d_v}$, where T is the sequence length, and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively.\nMost attention mechanisms differ in terms of what Queries they use, how the Key and Value vectors are defined, and what Score function is used. Sometimes using a single weighted average is not enough to attend to multiple different aspects present in a sequence, thus, allowing the extension of the attention mechanism to multiple heads, which runs through the attention mechanism several times in parallel (multi-head Attention). Using Multi-attention heads allows us to attend to the parts of an input sequence differently [28]. Specifically, given a Q, K, and V matrix, we transform them into h sub- queries, sub-keys, and sub-values passed through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix. Mathematically, we can express this operation as:\nMultihead(Q, K, V) = Concat(head\u2081, ...., head)Wo\nWhere head\u2081 = Attention(QW\u00a1\u00ba, KWK, VW\u2081)\nUsing these architectures, we have created the attentional models for six codes between 280-289 {3: frequent, 3: non- frequent}. To create training and testing datasets for the architectures, we have used the short-summaries (15,293) that belong to Chapter-IV. These summaries are labelled based on one-vs-rest approach, that is, labelling the summaries for the code 285.9 as '1' and others in 280-289 as '0'. This is how we labelled the datasets for all the codes we used in this research. Creating separate attentional models for codes in Chapter-IV using only the data within Chapter-IV showcased better performance than creating the attentional models directly for all the codes."}, {"title": "III. IMPLEMENTATION AND RESULTS", "content": ""}, {"title": "A. Named Entity Categorization (NEC) Model", "content": "In this research, we are not building NEC models for all the chapters (19), rather, we are building 1 NEC model for Chapter IV (diseases of the blood and blood-forming organs). This model is used for performing the experiments on the de-biasing and influencing criteria used in this research. Fig. 9 showcases how the final weights and entities (146) are used for categorizing the short-summaries into Chapter-IV.\nWhen compared with the results in Table. 3, the categorization thresholds created using our approach (entities and weights) can outperform the Bi-GRUs + Attention Model by a significant margin in categorizing the short-summaries into Chapter-IV."}, {"title": "B. Attentional Models for Codes between 280-289", "content": "From the list of Chapter-IV ICD codes available in the MIMIC-III dataset, we have selected 3 frequent and 3 non- frequent codes (see Table. 5). Datasets are created for these codes using only the short-summaries that belong to Chapter- IV (15,293). They are labelled based on one-vs.-rest approach (see Fig. 4).\nWe have selected the codes that can reflect all the codes that belong to Chapter-IV. There are ICD codes that are very rare (< 30 summaries) in MIMIC-III. To create the attentional models for these codes we should use enough data/summaries to collect the code features, which is not possible with MIMIC-III. It is better not to consider them at all. Researchers have used knowledge bases created using ICD code names and associated synonyms as a workaround for rare labels; however, if we cannot extract a good number of textual features from the summaries, even the KBs won't be enough.\nTo create the attentional models, we have used two attention mechanisms: dot-product Attention and multi-head Attention. Tabel. 6 showcases the configuration we used for training the models. We have mostly used similar configurations for the codes in the Table. 5. Only the gated recurrent unit dimensions (G) in the BiGRUs model and feed- forward upward projection (Df) in the Transformer model are selected based in the best performance."}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Interpretability for NEC Model", "content": "The approach we proposed for the NEC model does not require much computation or training. It is a simple approach that uses only 146 entities and their weights to create the categorization thresholds (see Table. 4) that can categorize the short summaries into Chapter-IV or REST. One of the primary and initial steps for the NEC model is using a pre-trained model to extract DISEASE entities [] from the summaries. Using simple regular expression patterns and matching functions, we are able to create weights and find entity matches in the summaries to sum up scores that can fit into the thresholds. This simple approach is as effective as neural networks. However, due to the imbalanced and unstructured nature of the data in MIMIC-III, both approaches can perform incorrect categorizations. One way to tackle this problem is to provide interpretability, which allows us to show the 'human coders' what entities (out of 146) are found in the summaries. These entities can be used to provide information such as anatomical regions, diseases, signs, and symptoms, which can be used for associating with the ICD code names and also provide a way for correcting and validating the categorizations.\nWe conduct our experiments to identify the thresholds with many incorrect categorizations. Whenever a summary falls into these \"faulty\" thresholds, a human coder can verify them using the matched entities, as illustrated in Fig. 10. For example, let's consider the thresholds SUM > 0.6 (for Chapter-IV) and SUM < 0.6 (for REST). A total of 25,344 {Chapter-IV: 72%, REST: 71%} summaries form 35,352 short summaries fit those thresholds perfectly (correct categorizations). Table. 8 below lists the thresholds near 0.6 (> and <) that human coders should worry and not worry about. For the thresholds greater than 0.6, 0 indicates incorrect categorizations from the REST summaries that fall into the thresholds in Table. 8. And for thresholds that are less than 0.6, 1 indicates the incorrect categorizations from Chapter-IV summaries."}, {"title": "V. CONCLUSION", "content": "This study proposes a chapter-based method for improving ICD coding from discharge summaries. Our approach first categorizes the discharge summaries into a chapter, and only then the discharge summaries that belong to that Chapter are used to create data based on the one-vs-rest approach. Initially, we processed the full-length discharge summaries by removing unnecessary information/headings that can add noise, thus, allowing us to create short summaries with less vocab size. These short summaries are used in our research to perform chapter-based categorization and create attentional models. We have proposed a simple approach using regular expression patterns and matching functions for the chapter- based categorization to identify/match the important entities from the short summaries. This important entity list is created with the help of a pre-trained NER model that can detect diseases, conditions, signs, and symptoms from clinical texts. We initially extracted the entities from all the summaries using the pre-trained model. We have extracted 375 most common entities from these entities based on document frequency. After applying the de-biasing and influencing criteria, we have decreased the entities to 146 with modified or improved weights. These 146 entities are used to find matches in the short summaries, and the weights are used to sum up, the identified entities. This allows us to create categorization thresholds that fit the data perfectly and provide faulty thresholds and interpretability for the human coders to correct and validate the categorizations.\nSeveral researchers have proposed various methods for automating the ICD coding, but so far, the performance evaluation of these approaches has been limited to carefully curated and clean benchmark datasets (MIMIC-III), which may not be reflective of how well these systems perform in the real world. As a whole, MIMIC-III only includes around 9,000 ICD-9 codes, most of which are unbalanced. The MIMIC-III results from existing methods show that Micro-average F1- scores are between 0.4 and 0.7, indicating that there are still many false positives. Our approach achieved average Micro- F1 Scores (6 codes in Table. 7) of 0.79 and 0.81 for our models (BiGRU, Transformer), showcasing significant improvements in the performance of ICD coding."}]}