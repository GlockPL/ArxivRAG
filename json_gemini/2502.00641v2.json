{"title": "Evaluating Small Language Models for News Summarization: Implications and Factors Influencing Performance", "authors": ["Borui Xu", "Yao Chen", "Zeyi Wen", "Bingsheng He"], "abstract": "The increasing demand for efficient summarization tools in resource-constrained environments highlights the need for effective solutions. While large language models (LLMs) deliver superior summarization quality, their high computational resource requirements limit practical use applications. In contrast, small language models (SLMs) present a more accessible alternative, capable of real-time summarization on edge devices. However, their summarization capabilities and comparative performance against LLMs remain underexplored. This paper addresses this gap by presenting a comprehensive evaluation of 19 SLMs for news summarization across 2,000 news samples, focusing on relevance, coherence, factual consistency, and summary length. Our findings reveal significant variations in SLM performance, with top-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results comparable to those of 70B LLMs while generating more concise summaries. Notably, SLMs are better suited for simple prompts, as overly complex prompts may lead to a decline in summary quality. Additionally, our analysis indicates that instruction tuning does not consistently enhance the news summarization capabilities of SLMs. This research not only contributes to the understanding of SLMs but also provides practical insights for researchers seeking efficient summarization solutions that balance performance and resource use.", "sections": [{"title": "1 Introduction", "content": "Automatic text summarization is a fundamental challenge in Natural Language Processing (NLP) that plays a crucial role in various applications, including search engine optimization (Mato\u0161evi\u0107, 2019), financial forecasting (Li et al., 2023), and public sentiment analysis (Chakraborty et al., 2019). The ability to distill large volumes of information into concise summaries is particularly important in today's fast-paced digital environment, where users often seek quick insights from news articles. Recent advancements in large language models (LLMs) such as GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022), and Llama2 (Touvron et al., 2023) have significantly enhanced the quality and coherence of generated summaries compared to traditional models (Luhn, 1958; Dong et al., 2018; Zhang et al., 2018; Raffel et al., 2020; Genest and Lapalme, 2012). However, the hundreds of billions of parameters in these LLMs require substantial computational and storage resources. For example, deploying the Llama2-70B model at FP16 precision typically necessitates two Nvidia A100 GPUs with 80GB of memory each. This means that only organizations with significant computing power can offer LLM services, raising concerns about service stability and data privacy.\nAs a result, general-purpose small language models (SLMs) have emerged as a potential alternative, as shown in Figure 1. They share the same decoder-only architecture as LLMs but have fewer than 4B parameters \u00b9 (Microsoft Azure, 2024; Yang et al., 2024; Abdin et al., 2024; Meta AI, 2024). Same as"}, {"title": "2 Background and Related Work", "content": "Small language models (SLMs) are characterized by their reduced parameter count and computational requirements compared to larger LLMs. These models can be general-purpose or specialized, and this paper focuses on the evaluation of pre-trained general-purpose SLMs, which will be referred to simply as SLMs throughout the remainder of the paper. Structurally, SLMs and LLMs share a common architecture, consisting of stacked decoder-only layers from the transformer framework (Vaswani et al., 2017), generating outputs in an autoregressive manner. SLMs typically have fewer decoder layers, attention heads, and smaller hidden dimensions, and they are trained on smaller, high-quality datasets (Lu et al., 2024). While there is no clear definition for the parameter count that qualifies a model as an SLM, models that can operate on consumer-grade devices are generally considered to fall within this category. This paper focuses on models smaller than 8GB in FP16 precision, corresponding to fewer than 4B parameters."}, {"title": "2.2 Text Summarization and Its Evaluation", "content": "Text summarization is the process of compressing a large volume of text into a shorter version while retaining its main ideas and essential information. Traditional text summarization models usually take a single news article as input and output a summary. However, SLMs require both a prompt and an article to generate a summary.\nThe evaluation methods of text summarization can be categorized into three categories, as shown in Figure 2: human evaluation, reference-based evaluation, and LLM evaluation.\nHuman evaluation is an intuitive and effective method, involving the hiring of annotators to score the generated text. For instance, Huang et al. (2020) employed human evaluation to evaluate 10 representative summarization models, revealing that extractive summary methods often performed better in human evaluation. The Summeval benchmark (Fabbri et al., 2021) utilized human evaluation methods to evaluate 23 traditional models and found Pegasus (Zhang et al., 2020a) performed best. Pu et al. (2023a) used human evaluation to assess LLMs and found that they were more favored by evaluators. Zhang et al. (2024b) also used human evaluation to evaluate the LLMs, discovering that instruction tuning had a greater impact on performance than model size. INSTRUSUM (Liu et al., 2024) evaluated instruction controllable summarization of LLMs by annotators and found them still unsatisfactory in summarizing text based on complex instructions.\nReference-based evaluation is a commonly used method. It scores generated text by calculating its similarity to reference texts using various metrics, and many such metrics have been proposed (Lin, 2004; Papineni et al., 2002; Sellam et al., 2020; Zhang et al., 2020b; Yuan et al., 2021). In addition to human evaluation, Huang et al. (2020) and Fabbri et al. (2021) also used many reference-based metrics to evaluated models and found metrics like Rouge and BertScore are very close to human evaluations. Maynez et al. (2023) used the Rouge metric to measure text similarity. Tiny Titans (Fu et al., 2024) assessed the performance of small models in meeting summarization and found FLAN-T5 performed best.\nLLM evaluation is explored by recent studies (Goyal et al., 2022; Chiang and Lee, 2023; Gao et al., 2023b; Liu et al., 2024; Wang et al., 2023). This approach involves guiding LLMs with"}, {"title": "3 LLM-Augmented Reference-Based Evaluation", "content": "As the reference-based method offers higher efficiency, better reproducibility, and lower cost compared to human and LLM evaluation (Fabbri et al., 2021; Zhang et al., 2024b), we use the reference-based method for large-scale SLM evaluation. However, some existing research (Tejaswin et al., 2021; Fabbri et al., 2021) indicates that the poor quality of heuristic reference summaries in current datasets weakens their correlation with human preferences. Moreover, Zhang et al. (2024b) find that using high-quality summaries as references can significantly improve alignment with human preferences. Thus, we use LLM-generated summaries as references in our SLM evaluation instead of relying on original dataset references.\nLLM-generated reference summaries offer two key advantages. First, they have higher quality than original heuristic summaries; Zhang et al. (2024b) find LLMs averaged 4.5 on a five-point scale, while original references scored only 3.6. Additionally, Pu et al. (2023a) find that LLM-generated summaries are preferred to those written by freelance authors by up to 84% of the time. Second, LLMs produce high-quality summaries quickly, generating hundreds in a few hours, which benefits large-scale news evaluations. In contrast, a freelance writer typically takes about 15 minutes for a single summary (Zhang et al., 2024b).\nWe also verify whether the LLM-generated reference summaries can improve the effectiveness of reference-based evaluation on three human evaluation datasets (Zhang et al., 2024b; Fabbri et al., 2021) in Table 1. These datasets include summaries generated by various models as well as human ratings. We compare the correlation of three popular similarity metrics with human scoring using different references. BertScore and BLEURT calculate similarity based on fine-tuned language models, while RougeL measures similarity through textual"}, {"title": "4 Benchmark Design", "content": "In this section, we first introduce the dataset for news summarization evaluation, followed by the SLM details, evaluation metrics, and reference summary generation."}, {"title": "4.1 News Article Selection", "content": "All experiments are conducted on the following four datasets: CNN/DM (Hermann et al., 2015), XSum (Narayan et al., 2018), Newsroom (Grusky et al., 2018), and BBC2024. The first three datasets have been widely used for verifying model text summarization (Goyal et al., 2022; Maynez et al., 2023; Zhang et al., 2024b). However, their older data may overlap with the training datasets of some models. To address this, we create BBC2024, featuring news articles from January to March 2024 on the BBC website 3. Based on prior research, 500 samples per dataset are sufficient to differentiate the models (Maynez et al., 2023). We randomly select 500 samples from each of the four test sets and construct 2000 samples in total. Each sample contains 500 to 1500 tokens according to the Qwen1.5-72B-Chat tokenizer."}, {"title": "4.2 Model Selection", "content": "We select 21 popular models with parameter sizes not exceeding 4 billion for benchmarking, including 19 SLMs and 2 models specifically designed for text summarization, Pegasus-Large and Brio, which have been fine-tuned on text summarization datasets, detailed in Table 2.\nAll models are sourced from the Hugging Face and tested based on the lm-evaluation-harness tool (Gao et al., 2023a). For general-purpose SLMs, we use the prompt from the huggingface hallucination leaderboard (Prompt 1 in Figure 3). To ensure reproducibility, we employ the greedy decoding strategy, generating until a newline character or <EOS> token is reached. Outputs are post-processed to remove prompt words and incomplete sentences. Given the 2048-token limit of some models, we only evaluate the zero-shot approach."}, {"title": "4.3 Evaluation Metric", "content": "We evaluate the summary quality in relevance, coherence, factual consistency, and text compression using BertScore (Zhang et al., 2020b), HHEM-2.1-Open (Vectara, 2024), and summary length.\nBertScore is a robust semantic similarity metric designed to assess the quality of the generated text and achieves the best correlation in Table 1. It compares the embeddings of candidate and reference sentences through contextualized word representations from BERT (Devlin et al., 2019). It can measure how well the summary captures key information (relevance) and maintains logical flow (coherence). We use the F1 score of BertScore, ranging from 0 to 100, where higher values indicate better quality.\nHHEM-2.1-Open is a hallucination detection model which outperforms GPT-3.5-Turbo and even GPT-4. It can evaluate whether news summaries are factually consistent with the original article. It is based on a fine-tuned T5 model to flag hallucinations with a score. When the score falls below 0.5, it indicates a summary is inconsistent with the source. We report the percentage of summaries deemed factually consistent; higher values indicate better performance.\nSince BertScore is insensitive to summary length, we also track the average summary length"}, {"title": "4.4 Reference Summary Generation", "content": "To mitigate the impact of occasional low-quality reference summaries on evaluation results, as well as the tendency for models within the same series to score higher due to possible similar output content, we utilize two LLMs, Qwen1.5-72B-Chat and Llama2-70B-Chat, to generate two sets of reference summaries. We then average the scores during the SLM evaluation. We use Prompt 2 from Figure 3 as the prompt template and apply a greedy strategy to generate summaries based on the vLLM inference framework (Kwon et al., 2023)."}, {"title": "5 Evaluation Results", "content": ""}, {"title": "5.1 Relevance and Coherence Evaluation", "content": "Table 3 shows the average BertScore results of SLMs. All models demonstrate consistent performance across datasets. Due to the significant score differences, we categorize the models into three approximate ranges, using two specialized summarization models, Pegasus-Large and Brio, as reference points.\nThe first range includes scores below 60. Models scoring below 60 struggle to effectively summarize articles. LiteLlama, Bloom, and GPT-Neo series fall into this range.. In terms of relevance, these models often miss key points; in terms of coherence, these models sometimes produce repetitive outputs, leading to overly long summaries. Figure 4 shows an example from LiteLlama, it scores only 40.81, deviating significantly from the news.\nThe second range covers scores between 60 and 70. Models in this range produce useful summaries but occasionally lack key points. Models like TinyLlama and Qwen-0.5B series fall into this range. In relevance, these models generally relate to the original content but may omit crucial details. In coherence, the sentences can be well-organized. Fig 4 shows examples from Qwen2-0.5B. Although its summary is relevant and coherent to the news article, it omits the final score.\nThe third range includes scores above 70. Models in this range produce summaries comparable to LLMs, with occasional inconsistencies. They effectively capture key points and maintain coherent structure. Phi3-Mini and Llama3.2-3B-Ins stand out in this group. As shown in Figure 4, Llama3.2-3B-Ins generates a concise, accurate summary, correctly capturing the match outcome even though the source news lacks a direct score description."}, {"title": "5.2 Factual Consistency Evaluation", "content": "During summary generation, SLMs may produce false information due to hallucinations. Therefore,"}, {"title": "5.3 Summary Length Evaluation", "content": "Figure 5 shows the average summary lengths on 4 datasets produced by SLMs, which vary significantly across models. For scores below 60, summaries typically exceed 100 words due to redundant content and poor summarization. In the 60-70 range, length varies widely; e.g., the"}, {"title": "5.4 Comparison with LLMS", "content": "To further evaluate the SLM performance in news summarization, we compare the well-performing SLMs with LLMs, including ChatGLM3 (GLM et al., 2024), Mistral-7B-Ins (Jiang et al., 2023), Llam3-70B-Ins (AI@Meta, 2024), and Qwen2 series, all in instruction-tuned versions. As the models show similar performance across various reference summaries and datasets, we use Llama2-70B-Chat to generate references and present the average results on BBC2024 in Table 5.\nWe highlight the top two metrics within each category. As can be seen, the summarization quality produced by the SLMs is on par with that of the LLMs. The difference in BertScore and factual consistency is minimal, with SLMs occasionally performing better. Notably, when BertScore scores are similar, smaller models tend to generate shorter summaries that facilitate quicker reading and comprehension of news outlines."}, {"title": "5.5 Human Evaluation", "content": "To further validate the effectiveness of our metrics, we performed a small-scale human evaluation, referencing existing studies (Fabbri et al., 2021). Specifically, six highly educated annotators evaluated the relevance of news summaries generated"}, {"title": "5.6 Overall Evaluation and Model Selection", "content": "SLMs exhibit considerable variability in text summarization performance, with larger and newer models generally showing stronger capabilities. Among them, Phi3-Mini and Llama3.2-3B-Ins perform the best, matching the 70B LLM in relevance, coherence, and factual consistency, while producing shorter summaries. All of these comparisons suggest that SLMs can effectively replace LLMs on edge devices for news summarization.\nAlthough we evaluate 19 SLMs, there is considerable variation in their parameter sizes. To optimize deployment on edge devices, we provide model selection recommendations based on size. For models under 1B parameters, Brio and Qwen2-0.5B are the top choices overall. General-purpose language models in this range offer no clear advantage over those specialized in text summarization. For models between 1B and 2B parameters, Llama3.2-1B-Ins demonstrates a clear edge. For models above 2B parameters, Llama3.2-3B-Ins and Phi3-Mini outperform others across all criteria."}, {"title": "6 Influencing Factor Analysis", "content": ""}, {"title": "6.1 Prompt Design", "content": "Prompt engineering has demonstrated significant power in many tasks, helping to improve the output quality of LLMs (Zhao et al., 2021; Sahoo et al., 2024). Therefore, we use different prompt templates shown in Figure 3 to analyze the impact of prompt engineering on BertScore scores, factual consistency, and summary length. The instructions become more detailed from Prompt 1 to Prompt 3. Table 6 compares the BertScore. The small score differences among prompts suggest that detailed descriptions do not significantly improve summary relevance and coherence and may even negatively affect some models. This suggests that simple instructions are more suitable for SLMs, and overly complex prompts may degrade the performance. For instance, the BertScore of Qwen2-0.5B-Ins drops significantly. The decline is likely due to prompt noise, where the model struggles to balance multiple instructions, resulting in less"}, {"title": "6.2 Instruction Tuning", "content": "Instruction tuning plays a crucial role in training LLMs by enhancing their ability to follow specific instructions. And some existing studies claim that instruction-tuned language models have stronger summarization capabilities (Goyal et al., 2022; Zhang et al., 2024b). However, in our extensive evaluations, with the exception of the Llama3.2 series, models perform very similarly before and after instruction tuning. In fact, models without instruction tuning often exhibit higher factual consistency. In the sample analysis, we find that the Llama3.2 models without instruction tuning include too many details, while instruction tuning helps it produce more general summaries. However, this trend is not observed in other models like Qwen2 and InternLM2 series, which is different from the conclusions of previous work (Zhang et al., 2024b). We leave the deep study of instruction tuning on summarization ability as a future research direction."}, {"title": "7 Conclusion", "content": "This paper presents the comprehensive evaluation of SLMs for news summarization, comparing 19 models across diverse datasets. Our results demonstrate that top-performing models like Phi3-Mini and Llama3.2-3B-Ins can match the performance of larger 70B LLMs while producing shorter, more concise summaries. These findings highlight the potential of SLMs for real-world applications, particularly in resource-constrained environments. Further exploration shows that simple prompts are more effective for SLM summarization, while instruction tuning provides inconsistent benefits, necessitating further research."}, {"title": "Limitations", "content": "Although using LLM-generated summaries improved the reliability of reference-based summarization evaluation methods, it may introduce bias. For instance, if the LLM fails to accurately summarize the news, an SLM that produces similar content might receive an undeservedly high score. To mitigate this potential bias, we use summaries generated by multiple LLMs as references and calculate the average scores. Furthermore, although BertScore demonstrates a high level of consistency with human evaluations in coherence, it is not specifically designed to evaluate coherence. In"}]}