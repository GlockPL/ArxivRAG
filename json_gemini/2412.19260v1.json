{"title": "MEDEC: A BENCHMARK FOR MEDICAL ERROR DETECTION\nAND CORRECTION IN CLINICAL NOTES", "authors": ["Asma Ben Abacha", "Wen-wai Yim", "Yujuan Fu", "Zhaoyi Sun", "Meliha Yetisgen", "Fei Xia", "Thomas Lin"], "abstract": "Several studies showed that Large Language Models (LLMs) can answer medical questions correctly,\neven outperforming the average human score in some medical exams. However, to our knowledge,\nno study has been conducted to assess the ability of language models to validate existing or generated\nmedical text for correctness and consistency. In this paper, we introduce MEDEC\u00b9, the first publicly\navailable benchmark for medical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal Organism). MEDEC\nconsists of 3,848 clinical texts, including 488 clinical notes from three US hospital systems that were\nnot previously seen by any LLM. The dataset has been used for the MEDIQA-CORR shared task\nto evaluate seventeen participating systems [Ben Abacha et al., 2024]. In this paper, we describe\nthe data creation methods and we evaluate recent LLMs (e.g., ol-preview, GPT-4, Claude 3.5\nSonnet, and Gemini 2.0 Flash) for the tasks of detecting and correcting medical errors requiring\nboth medical knowledge and reasoning capabilities. We also conducted a comparative study where\ntwo medical doctors performed the same task on the MEDEC test set. The results showed that\nMEDEC is a sufficiently challenging benchmark to assess the ability of models to validate existing\nor generated notes and to correct medical errors. We also found that although recent LLMs have a\ngood performance in error detection and correction, they are still outperformed by medical doctors in\nthese tasks. We discuss the potential factors behind this gap, the insights from our experiments, the\nlimitations of current evaluation metrics, and share potential pointers for future research.", "sections": [{"title": "Introduction", "content": "A survey study from US health care organizations showed that 1 in 5 patients who read a clinical note reported finding a\nmistake and 40% perceived the mistake as serious, with the most common category of mistakes being related to current\nor past diagnoses [Bell et al., 2020].\nOn the other hand, more and more medical documentation tasks (e.g., clinical note generation) are being supported by\nLLMs. In multiple studies, LLMs have shown the ability to answer accurately questions from medical exams [Gilson\net al., 2023, Johnson et al., 2023, Schubert et al., 2023] and to imitate clinical reasoning in providing diagnoses [Savage\net al., 2024]. However, one of the main obstacles in adopting LLMs in medical documentation tasks is their potential\nto generate hallucinations or incorrect information [Tang et al., 2023] and harmful content that might alter clinical"}, {"title": "Related Work", "content": "Jang et al. [2022] introduced a benchmark for consistency evaluation and evaluated pretrained language models (e.g,\nBERT, T5, and GPT-2) on three main categories: semantic, logical, and factual consistency. They found that those\nlanguage models do not perform well in every test case and have a high level of inconsistency in many cases. Jang and\nLukasiewicz [2023] investigated the trustworthiness of more recent language models, ChatGPT and GPT-4, regarding\nsemantic consistency and found that while both models appear to show an enhanced language understanding and\nreasoning ability, they often fail at generating logically consistent predictions."}, {"title": "MEDEC Dataset", "content": "We created a new dataset of 3,848 clinical texts from different specialties. Eight medical annotators participated in the\nannotation task. The dataset covers five types of errors, selected after analyzing the most frequent types of questions\nasked in medical board exams. The error types are:\n\u2022 Diagnosis - The provided diagnosis is inaccurate.\n\u2022 Management - The next step provided in management is inaccurate.\n\u2022 Pharmacotherapy - The recommended pharmacotherapy is inaccurate.\n\u2022 Treatment - The recommended treatment is inaccurate.\n\u2022 CausalOrganism - The indicated causal organism or causal pathogen is inaccurate."}, {"title": "Data Creation Method #1 (MS)", "content": "In this method, we leverage medical board exams from the MedQA collection [Jin et al., 2020]. Four annotators with\nmedical backgrounds used the medical narratives and multiple choice questions in these exams to inject a wrong answer\ninto the scenario text, after checking the original questions and answers and excluding QA pairs containing errors or\nambiguous information.\nThe medical annotators followed these guidelines:\n\u2022 Using medical narrative multiple choice questions, introduce a wrong answer into the scenario text and create\ntwo versions with the error injected either in the middle of the text or at the end.\n\u2022 Using medical narrative multiple choice questions, introduce the right answer into the scenario text to create a\ncorrect version, as described in Figure 2 (Generated Text with Correct Answer).\n\u2022 Check manually if the automatically rewritten text is faithful to the original scenario and the included answer.\nWe randomly selected one correct and one incorrect version for each note from the two different scenarios (error injected\nin the middle of the text or at the end) in the final dataset."}, {"title": "Data Creation Method #2 (UW)", "content": "We used a database of real clinical notes between 2009 and 2021 from three University of Washington (UW) hospital\nsystems\u00b2: Harborview Medical Center, UW Medical Center, and Seattle Cancer Care Alliance.\nFrom this database, we randomly selected 488 out of 17,453 diagnosis supports, which summarize patients' medical\nconditions and provide rationales for treatments.\nA team of four medical students manually introduced errors into 244 of these notes. Initially, each note was marked\nwith several candidate entities identified as Unified Medical Language System (UMLS)\u00b3 concepts by QuickUMLS\u2074.\nAn annotator either selected a concise medical entity from these candidates or created a new span. This span was then\nlabeled with one of the five error types. The annotator then replaced this span with an erroneous version using similar\nbut distinct concepts, crafted by the annotators themselves or provided by a SNOMED- and LLM-based method. This\nmethod was used to suggest alternative concepts to the annotators without using the input text. Medical annotators\ndecided on the final concepts/errors to inject manually in the text.\nDuring this process, each error span was required to contradict at least two other parts of the clinical notes (and\nannotators provided a justification for each error introduced). We de-identified the clinical notes (post error injection)\nwith Philters\u2075 for automatic de-identification. Each note was then independently reviewed by two annotators to ensure\nproper de-identification. A third annotator adjudicated any remaining discrepancies."}, {"title": "Medical Error Detection & Correction Approaches", "content": "In order to evaluate models on medical error detection and correction, we divide the process into 3 subtasks:\n\u2022 Subtask A: Predicting the error flag (0: if the text has no error; 1: if the text contains an error).\n\u2022 Subtask B: Extracting the sentence that contains the error for flagged texts (-1: if the text has no error; Sentence\nID: if the text contains an error).\n\u2022 Subtask C: Generating a corrected sentence for flagged texts with errors (NA: if the text has no error; Generated\nsentence/correction: if the text has an error).\nFor comparison, we build LLM-based solutions using two different prompts to generate the outputs required to assess\nthe models on the three subtasks:\n\u2022 P#1: The following is a medical narrative about a patient. You are a skilled medical doctor reviewing the clinical text.\nThe text is either correct or contains one error. The text has one sentence per line. Each line starts with the sentence ID,\nfollowed by a pipe character then the sentence to check. Check every sentence of the text. If the text is correct return the\nfollowing output: CORRECT. If the text has a medical error related to treatment, management, cause, or diagnosis, return\nthe sentence id of the sentence containing the error, followed by a space, and then a corrected version of the sentence.\nFinding and correcting the error requires medical knowledge and reasoning.\n\u2022 P#2 Similar to the first prompt, but includes an example of input and output, randomly selected from the\ntraining set: Here is an example. 0 A 35-year-old woman presents to her physician with a complaint of pain and\nstiffness in her hands. 1 She says that the pain began 6 weeks ago a few days after she had gotten over a minor upper\nrespiratory infection. (...) 9 Bilateral radiographs of the hands demonstrate mild periarticular osteopenia around the\nleft fifth metacarpophalangeal joint. 10 Methotrexate is given. In this example, the error is in the sentence number 10:\nMethotrexate is given. The correction is: Prednisone is given. The output is: 101 Prednisone is given. End of Example."}, {"title": "Experiments & Results", "content": "We experiment with several recent small and large language models:\n1. Phi-3-7B, a Small Language Model (SLM) with 7 billion parameters [Abdin et al., 2024]\n2. Claude 3.5 Sonnet (2024-10-22), the latest model (~175B parameters) from the Claude 3.5 family offering\nstate-of-the-art performance across several coding, vision, and reasoning tasks [Anthropic, 2024].\n3. Gemini 2.0 Flash: the latest/most advanced Gemini model [Google, 2024]. Other Google models such as\nMed-PaLM models (540B) [Singhal et al., 2023], designed for medical purposes, were not publicly available.\n4. ChatGPT (~175B) [OpenAI, 2023a] and GPT-4 (~1.76T), a \"high-intelligence\" model [OpenAI, 2023b].\n5. GPT-4o (~200B) providing \"GPT-4-level intelligence but faster\" [OpenAI, 2024a] and the GPT-4o-mini\n(gpt-4o-2024-05-13) small model (~8B parameters) for focused tasks [OpenAI, 2024b].\n6. The latest o1-mini (01-mini-2024-09-12) model (~100B) [OpenAI, 2024c], and o1-preview (o1-preview-2024-\n09-12) model (~300B) with \"new AI capabilities\" for complex reasoning tasks [OpenAI, 2024d].\nThe exact numbers of parameters of several LLMs (e.g., GPT, Gemini 2.0 Flash) have not been publicly disclosed yet.\nMost numbers of parameters are estimate reported to provide more context for understanding the models' performance.\nPlease refer to the original/future documentation for more precise information about these models.\nFew models (e.g., Phi-3 and Claude) required minimal automatic post-processing to correct some formatting issues."}, {"title": "Evaluation Metrics", "content": "To evaluate the models' performance in recognizing medical errors in texts, we relied on Accuracy for Error Flag\nPrediction (subtask A) and Error Sentence Detection (subtask B).\nTo further analyze the results for each error type, we also computed the Recall using the subset of test examples with\nerrors (error flag = 1) for each type.\nTo evaluate the generated corrections (subtask C), we selected lexical and contextual embedding-based metrics that\nhighly correlate with human judgments on clinical texts [Ben Abacha et al., 2023]. These metrics are: ROUGE \u2013 1\n[Lin, 2004], BLEURT [Sellam et al., 2020], and BERTScore (microsoft/deberta-xlarge-mnli) [Zhang et al., 2020].\nWe also used an additional score for error correction, Aggregate Score (AggregateScore), which is the average of the\nfollowing evaluation metrics: ROUGE-1, BLEURT-20, and BERTScore.\nWe computed these four error correction scores when both the reference and system corrections are provided (other than\nNA). Our evaluation scripts are available online: https://github.com/abachaa/MEDIQA-CORR-2024/tree/main/\nevaluation."}, {"title": "Comparison with Expert Labeling", "content": "Two medical doctors performed the same subtasks on the MEDEC dataset to assess the difficulty of detecting and\ncorrecting the errors. The doctors annotated 569 clinical notes from the full test set of 925 texts.\nGiven a clinical text from the test set without the ground truth (without the error flag, error sentence, and reference\ncorrection), the medical doctors were tasked to:\n\u2022 Judge whether a medical error exists in the text.\n\u2022 If an error exists, write the sentence ID of the sentence where the error occurred.\n\u2022 Provide the most likely error correction and its type (e.g., diagnosis, management, treatment)."}, {"title": "Results", "content": "Table 2 presents the results of the manual annotation performed by the medical doctors and the results of several recent\nLLMs using the two prompts described above. Claude 3.5 Sonnet outperformed the other LLM-based methods in error\nflag detection with 70.16% Accuracy and in error sentence detection with 65.62% Accuracy."}, {"title": "Conclusion", "content": "This paper presented the MEDEC benchmark for medical error detection and correction in clinical notes. An empirical\nevaluation of LLM-based methods showed that, while recent LLMs have a good performance, they are still outperformed\nby medical doctors. The results of the doctors' annotation showed that the MEDEC dataset is a sufficiently challenging\nbenchmark to assess the ability of models to validate existing or generated notes and to correct medical errors. We hope\nthat this dataset will enable further studies on medical error detection and correction in clinical notes, enhancing clinical\nreasoning capabilities of LLMs, and facilitate additional efforts on evaluation metrics for clinical texts and applications."}, {"title": "Limitations", "content": "The paper does not cover all types of possible methods and models for the detection and correction of medical errors.\nThe dataset is also limited in terms of size and types of medical errors."}]}