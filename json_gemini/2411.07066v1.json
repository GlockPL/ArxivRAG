{"title": "ZEROTH-ORDER ADAPTIVE NEURON ALIGNMENT BASED PRUNING WITHOUT RE-TRAINING", "authors": ["Elia Cunegatti", "Leonardo Lucio Custode", "Giovanni Iacca"], "abstract": "Network pruning is a set of computational techniques that aim to reduce a given model's computational cost by removing a subset of its parameters while having minimal impact on performance. Throughout the last decade, the most widely used pruning paradigm has focused on pruning and re-training, which nowadays is inconvenient due to the vast amount of pre-trained models, which are in any case too expensive to re-train. In this paper, we exploit functional information from dense pre-trained models, i.e., their activations, to obtain sparse models that maximize the activations' alignment w.r.t. their corresponding dense models. Hence, we propose NEURONAL, a top-up algorithm that can be used on top of any given pruning algorithm for LLMs, that modifies the block-wise and row-wise sparsity ratios to maximize the neuron alignment among activations. Moreover, differently from existing methods, our approach adaptively selects the best parameters for the block-wise and row-wise sparsity ratios w.r.t. to the model and the desired sparsity (given as input), and requires no re-training. We test our method on 4 different LLM families and 3 different sparsity ratios, showing how it consistently outperforms the latest state-of-the-art techniques. The code is available at https://github.com/eliacunegatti/NeuroAL.", "sections": [{"title": "1 Introduction", "content": "In recent times, Large Language Models (LLMs) have shown incredible performance over almost any language task [44, 33, 4]. However, their performance tends to grow with their sizes (i.e., the number of trainable parameters), which in turn is proportional to the computational burden required to train and then use such models. One way to reduce the computational cost of LLMs is through network pruning, i.e., algorithms that remove parameters while minimizing performance degradation. This approach has been extensively studied on Convolutional Neural Networks (CNNs) [13, 25, 43, 11], but nowadays the focus has shifted towards pre-trained models [41, 42, 22]. This shift has required a change of paradigm in pruning techniques: in fact, while in CNNs the main paradigm is iterative pruning (with re-training) [13], with pre-trained models (such as LLMs) in most cases it is not possible to fully re-train such models, because (1) training data are often not accessible, and (2) full re-training would be anyway too expensive. This calls for \u201cexploiting\u201d as much as possible the information contained in a pre-trained model to obtain a performant sparse version of it, using weight's information [21], activations [40, 39], or reconstruction error [15], without the need for re-training. More recently, a new category of pruning algorithms, which we may call top-up algorithms (i.e., methods that can be applied on top of a given pruning algorithm for LLMs), has emerged, aiming at further improving pruning performance. Such approaches can be divided into two categories: those that minimize the reconstruction error [18, 45, 49], and those that impose non-uniform sparsity distribution modifying the block-wise sparsity [46]. The latter category is extremely effective for improving performance in CNNs [14, 38], while its application to LLMs is still limited [46].\nContributions In this paper, we first analyze the reasons behind the effectiveness of non-uniform sparsity distribution in sparse LLM. To do so, we carefully analyze the state-of-the-art top-up method OWL [46], investigating the reason underlying its better performance (w.r.t. other methods from the state of the art) as well as its limitations in terms of sensitiveness to its hyperparameters. Leveraging this knowledge, we introduce a new top-up method, that we call NEURONAL. The algorithm consists of a two-step approach that re-distributes the block-wise sparsity, i.e., the sparsity among Transformer blocks, and the row-wise sparsity, i.e., the sparsity for each row of a given layer's matrix,"}, {"title": "2 Related Work", "content": "In this section, we provide a comprehensive discussion about network pruning applied to LLMs. We first introduce structured and unstructured network pruning; then, we focus on the latter, introducing the latest approaches proposed for improving sparse model performance.\nStructured Network Pruning. Given a layer's weight matrix $W \\in \\mathbb{R}^{n\\times m}$ to sparsify, structured pruning removes either entire rows (n) or columns (m) (see the next section) aiming at speeding up both training and inference time. The first approach that applies structured pruning to LLMs has been proposed in [28], and focuses on the dependency of Transformers, i.e., it removes components of the networks while maximizing their original functionality. In [24], a pruning mechanism has been devised to remove components with the worst balance between loss and runtime. Other structured pruning approaches have been proposed based on combinatorial optimization [30], perturbative forward-pass only [9], and reduction of the embedding dimension through PCA [1]. Finally, in [17] it has been found that the last Transformer blocks are redundant, hence they can be completely removed with minor performance drops. The reason behind this phenomenon lies in the similarity between the learnable representation of consecutive blocks, which turns out to increase when the block depth increases. While all these approaches can achieve valuable inference speed-ups, the performance of the resulting sparse models w.r.t. their dense counterparts can be matched only at low sparsity values, such as 20% in [28] or 30% in [1]. This somehow limits the applicability of these methods, since in the case of models with billions of parameters one may need more aggressive pruning strategies to meet stringent hardware requirements.\nUnstructured Network Pruning. Differently from structure pruning, unstructured pruning works by removing weights in a scattered (i.e., non-structured) way. While in this scenario the inference speed-up is limited (although techniques for reordering weights are available [26, 34, 50]), the performance w.r.t. the dense model can be preserved also at high sparsity ratios (i.e., above 50%), with the performance at lower sparsity being almost always completely preserved. The first approach of this kind has been proposed in [15], where weight pruning and reconstruction are combined based on the Hessian matrix. Even a simple magnitude-based approach turned out to perform well [21], as well as when integrated with information on the neuron activations [40, 12]. All these approaches work by computing a score for each weight and then removing, uniformly for each layer, the worst-performing ones for a given sparsity ratio.\nTop-Up Algorithms To improve the performance of unstructured pruning algorithms, several top-up algorithms have been devised. These approaches can be categorized into two distinct groups: methods that minimize the reconstruction error keeping the sparsity uniform for each block, and methods that modify the block-wise sparsity of the model resulting in non-uniform sparsity distribution across blocks.\nThe first group firstly sparsifies the model using a pruning algorithm and then, either dynamically [49] or by backprop-agation [18], updates the pruning mask. The second group (to which our method belongs) modifies the block-wise sparsity (obtained by a given pruning algorithm) based either on activations' outliers [46] or on layer-wise sparsity using block-wise reconstruction error with gradient information [45].\nThe idea of simply redistributing the layer-wise sparsity is known to be extremely well-performing on Multi-Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). The first approach of this kind, based on the Erd\u0151s-R\u00e9nyi (ER) model, has been proposed in [35] for MLPs and then adjusted for CNNs in [11], while an empirical study about the effect of layer-wise pruning using different sparsity ratios has been done in [27]. Regarding Transformers (both for vision and text), the state-of-the-art algorithms [15, 40] have been devised to set the block-wise sparsity across the Transformer blocks in a uniform way. Later on, OWL has been proposed to build upon scoring-based pruning algorithms, adjusting the block-wise sparsity in a non-uniform way w.r.t. the number of outliers computed for each block. This approach improves the performance of several pruning algorithms, e.g. [15, 40], especially at sparsity above 60%. On the same line, BESA [45] allocates layer-wise sparsity across each block's layer using gradient information. Recently, modality-wise sparsity distribution has been investigated in the case of multimodal tasks in [12, 19]."}, {"title": "3 Observational Study", "content": "As discussed earlier, OWL [46] uses a non-uniform sparsity distribution across blocks, which provides an advantage in terms of performance w.r.t. uniform distribution. However, the reason why this occurs remains unclear. In the first part of this section, we focus on uncovering the reason behind this phenomenon. Furthermore, OWL suffers from some disadvantages: it requires the setting of two hyperparameters, one for the sparsity difference between consecutive blocks to utilize in the non-uniform distribution (A), and one for the outlier definition (M), In the second part of this section, we investigate how the choice of the best hyperparameters is tied to the model and sparsity selected. This limitation leads to either selecting non-optimal hyperparameters or requiring a grid search to find the optimal values.\nThe observational study has been performed on two models (Phi-2 and LLama-1 7B) and three different sparsity ratios (0.6, 0.7, 0.8) using as pruning algorithm both Wanda [40] and MULTIFLOW [12]\u00b9.\n3.11 Non-Uniform Block-Wise Sparsity Distribution\nWe start by analyzing if the OWL's performance improvement, compared to uniform sparsity distribution, could be unequivocally associated with its outlier-based score. Since the number of outliers for each block turns out to decrease with the layer depth, we test three straightforward non-uniform sparsity schedules (namely linear, exponential, and logarithmic), which do not require any forward step and do not depend on the outliers. Given a fixed parameter A, these schedules work by redistributing the sparsity across layers in a monotonically increasing way (i.e., the sparsity of layer i is always larger than the sparsity of layer i - 1, Vi > 1). Fig. 1 displays the improvement, w.r.t. uniform distribution, achieved by the three sparsity schedules on two pruning algorithms (Wanda and MULTIFLOW) with X = 0.08 (as in [46]). The results highlight how non-uniform sparsity schedules, with no outlier information, can match, and in some cases even improve, the performance of OWL. Overall, the linear schedule turns out to be the most reliable one since it does not show oscillations in performance across the different sparsity ratios (while this happens for the logarithmic and exponential schedule).\n3.22 Effect of X and M\nWe then analyze the effect of the two hyperparameters set by OWL, namely A and M. The first hyperparameter is used to set how much the sparsity can vary across blocks (i.e., [s \u2013 X, s + \u5165]) while keeping the overall sparsity fixed as s. In the experimentation reported in [46], A is set differently for each model, with 0.08 being the most used value. As"}, {"title": "4 Methodology", "content": "In this section, we propose our method, NEURONAL (Neuron Alignment). Given a pruning algorithm for a pre-trained LLM, our method re-computes the block-wise sparsity ratios for each Transformer block and the row-wise sparsity ratios based on the alignment between the dense activations and the sparse activations. The main strength of this method lies in its ability to be fully adaptive to the model and desired sparsity (given as input), requiring no single hyperparameter to be chosen a priori, but rather a set of suitable values from which NEURONAL can pick the most performing one.\nPreliminaries Given a dense model D, a pruning algorithm P, and a desired sparsity s, unstructured network pruning generally computes a saliency score \u03a8 for each weight w \u2208 D and then binarizes these scores w.r.t. the topk elements, where $k = 1 - |D| \\times s$. This allows to obtain a binary mask M to apply over D, from which the final sparse model can be computed as $S = D \\odot M$. Since LLMs are composed of stacked Transformer blocks (each one denoted as $B_l$), i.e., sets of linear layers (each one denoted as $l$) that implement the self-attention mechanism followed by an MLP, the binarization step is usually done uniformly per each layer l [15, 40] as:\n$M_\\ell = \\text{top}_{k} \\left(\\Psi(D_\\ell)\\right)$.\n(1)\nNeuron Alignment Our proposed method is based on the idea of combining the concept of neuron alignment, which requires no a priori definition of outliers (hence no M parameter, as in OWL), with that of adaptivity, to remove the dependence from \u5165. Differently from the well-established reconstruction error [15], computed as arg min me, we ||WeXe - (Me We)Xe||2 for each layer or block (hence measuring the difference between the sparse and dense representation of the output prior the non-linearity functions), neuron alignment provides a single scalar value that evaluates the model in its completeness focusing on the difference between sparse and dense activations, hence after the non-linearity. The method takes as input both D and its sparse version S generated by P with sparsity ratio s, and uses a small calibration data C\u2081 to make a forward pass on both models, to retrieve the dense and sparse activations, respectively AD and As. The main idea behind NEURONAL is to maximize the neuron alignment by firstly modifying the vector of sparsity ratios for all blocks (sb) and then for all rows (s\u201d), where each row corresponds to the layer's weight matrix W (for each layer l in B\u2081), where We \u2208 Rr\u00d7m. The main strength of this approach is that it does not require any weight update nor gradient information, but just a block- and row-wise recalibration and mask update via Eq. (1), using the same scoring criteria of P.\nHowever, as tested in the previous observational study, finding the best block/row-wise sparsity requires defining a factor A to control the block/row-wise sparsity difference between consecutive blocks/rows while ensuring the desired global sparsity. As seen earlier, while OWL requires A to be set a priori, we design NEURONAL to automatically select, from a suitable set of values, the best A for each combination of D, P and s, yielding an adaptive top-up method. The only constraint we set is that we use a linear sparsity schedule over A for the block-wise step, demonstrated to be effective in our observational study \u2460. This choice has been made (1) because we found that the performance improvement obtained with the linear sparsity schedule is more stable, see Fig. 1, and (2) to align our method to the latest research that shows how the last layers of an LLM have a small influence on the final performance [17, 29].\n4.1 Block-Wise Sparsity Ratio\nThe first step concerns the block-wise redistribution over the whole model. Our method takes as input the dense and sparse models (D and S), the desired sparsity ratio (s), the calibration data Cx, and a set of different A parameters (set). Then, it computes a set of |\\set| different vectors of block-wise sparsity values for the whole model set B = {sB1, sB2,..., sB|set|}, where each element sBi indicates a vector of block-wise sparsity values obtained with a linear schedule in [s \u2013 \u03bb\u03ba, \u03c2 + \u03bb\u03ba]. For each sB\u03ba \u2208 set, we then forward the calibration data C\u2081 through the model, and calculate the corresponding neuron alignment:\n$\\text{neur} _{\\text{align}} = \\sum_\\ell \\sum_i \\frac{\\left|A^{\\mathcal{D}}\\_{\\ell i} - A^{\\mathcal{S}} _{\\ell i}\\right|}{\\sum_i A^{\\mathcal{D}}\\_{\\ell i}}$\n(2)\nwhere A means that the activations are normalized to sum up to one. Then, we select (set)*, i.e., the \u00c0 parameters per block that minimize Eq. (2). Finally, we update the block-wise sparsity with the selected (set)* via Eq. (1), thus obtaining a sparsified model SB."}, {"title": "4.2 Row-Wise Sparsity Ratio", "content": "The second step is complementary to the previous one, but in this case, the sparsity is modified w.r.t. the rows of each layer. It is established [40] that pruning using the row as a comparison group\u00b2 achieves better performance w.r.t. using the whole layer since it inherently maximizes the network connectivity [20, 7]. Here, we rely on such discovery to strengthen our approach and change the row-wise sparsity based on the neuron alignment of each layer. In this case, for each layer l (i.e., for each W\u2208 Rr\u00d7m) we redistribute the sparsity across the r rows. Also in this case the A parameters are critical for deciding how to control the sparsity difference between consecutive rows. We take our sparse model obtained with the block-wise redistribution (SB) and, for each layer l, we compute different row-wise sparsity values obtaining set = {S1, S2,..., S|set|}, where each s indicates a vector of row-wise sparsity ranging in [s \u2013 Ak, s + \u5165k], where each element is inversely proportional to the alignment of the corresponding row. In this case, we select in set the row-wise vector (set)* that minimizes Eq. (2), and then apply Eq. (1) to S\u00df, using each row as comparison group.\nThe full procedure composed of both the block-wise and row-wise recalibration is summarized in Algorithm 1.\nAlgorithm 1 Proposed top-up pruning procedure\nRequire: D, P, s, Cx, Aset\nM\u2190 P(D, s)\n\u25b7 Prune D uniformly per layer\nS-DOM\nADD(CA)\n\u25b7 Dense activations\n)set)* \u2190 NEURONAL(D, S, Aset, C'\u05d2, AD(\n\u25b7 Block-wise step\nSBD top(s)* (\u03a8, D)\n(Set)* \u2190 NEURONAL(D, S\u00df, Aset, CA, AD)\n\u25b7 Row-wise step\nSfinal D top (set)* (\u03a8, D)\nfunction NEURONAL(D, S, s, Aset, CA, AD)\ns* \u00d8, neur\u2190\u221e\nfor \u2208 Aset do\nsx = GETDIST(s,A)\n> Compute s or s\nAs\u2190 (D tops, (\u03a8, D))(Cx)\n\u25b7 Sparse activations\nneural \u2190 GETALIGN(AD, As, SA)\n\u25b7 Via Eq. (2)\nif neural < neural then\nS* Sx\nreturn s*"}, {"title": "5 Experiments", "content": "We apply our proposed NEURONAL to different pruning algorithms tailored for LLMs. Specifically, we test how it compares in terms of performance over Language Modeling datasets and Zero-Shot tasks w.r.t. the most recent top-up algorithms for pruning. We also perform scalability and ablation studies to show the effectiveness of our NEURONAL.\n5.1 Experimental Setup\nLanguage Modeling Datasets To measure the models' perplexity on Language Modeling datasets, we use the following three datasets: (1) WikiText2 [31], a collection of 2.6M tokens from Wikipedia; (2) Colossal Clean Common Crawl (C4) [37], which is a clean version of the Common Crawl dataset produced in April 2019, containing 154G tokens [10]; and (3) Penn Treebank (PTB), which is a dataset extracted from the Wall Street Journal containing 2.4M tokens.\nZero-Shot Tasks To assess more thoroughly how the different pruning algorithms affect the models' capabilities, we employ the following 7 datasets. (1) Recognizing Textual Entailment (RTE) [8, 2, 16, 3], a dataset composed of 3000 test samples in which a model has to detect the entailment between two sentences. (2) WinoGrande [23], a dataset consisting of 1767 test cases in which the model has to fill a blank using two choices. (3) BoolQ [5], a question-answering dataset containing 3270 test samples, where each question requires a yes/no answer. (4) HellaSwag [47], a dataset composed of 10k test cases that requires a model to select the appropriate completion for a given sentence"}, {"title": "6 Conclusion and Limitations", "content": "In this paper, we proposed NEURONAL, a new approach to prune LLMs based on the alignment between sparse and dense activations. Our approach has the main strength of requiring no outliers or gradient information. On top of that, our method is also adaptive, since it is designed to automatically select the best hyperparameters for the model, pruning algorithm, and desired sparsity. Throughout extensive experiments, we showed how our approach outperforms all the latest state-of-the-art methods both on Language Modeling datasets and Zero-Shot tasks, on 4 different LLM families and 3 sparsity ratios. We also included an extensive ablation study to show the robustness of our approach to the calibration data as well as the effect of its algorithmic steps.\nIn our view, the present version of NEURONAL has three main limitations. (1) With NEURONAL, it is not possible to make use of optimized structured-sparsity inference implementations (e.g, the NVIDIA N:M sparsity [36]): for a given sparsity, NEURONAL produces customized sparsity constraints for each layer in a block, and then for each row of each layer. Therefore, these implementations cannot be employed as they often require continuity in the sparsity of matrices. (2) NEURONAL requires two forward steps more than base pruning algorithms, and one more than the closest top-up competitor, OWL. (3) Finally, it requires the definition of Aset. However, it is worth noticing that |\\set| as well as |Cx| affect the computational cost of the forward step."}]}