{"title": "Reflective Human-Machine Co-adaptation for\nEnhanced Text-to-Image Generation Dialogue System", "authors": ["Yuheng Feng", "Yangfan He", "Yinghui Xia", "Tianyu Shi", "Jun Wang", "Jinsong Yang"], "abstract": "Today's image generation systems are capable of producing\nrealistic and high-quality images. However, user prompts often\ncontain ambiguities, making it difficult for these systems to\ninterpret users' potential intentions. Consequently, machines\nneed to interact with users multiple rounds to better understand\nusers' intents. The unpredictable costs of using or learning im-\nage generation models through multiple feedback interactions\nhinder their widespread adoption and full performance poten-\ntial, especially for non-expert users. In this research, we aim to\nenhance the user-friendliness of our image generation system.\nTo achieve this, we propose a reflective human-machine co-\nadaptation strategy, named RHM-CAS. Externally, the Agent\nengages in meaningful language interactions with users to re-\nflect on and refine the generated images. Internally, the Agent\ntries to optimize the policy based on user preferences, ensur-\ning that the final outcomes closely align with user preferences.\nVarious experiments on different tasks demonstrate the effec-\ntiveness of the proposed method.", "sections": [{"title": "Introduction", "content": "CGenerative artificial intelligence has demonstrated immense\npotential in facilitating economic development by helping\noptimize creative and non-creative tasks. Models such as\nDALLE 2, IMAGEN, Stable Diffusion, and Muse have\nachieved this through their capability to produce unique,\nconvincing, and lifelike images and artwork from textual\ndescriptions(Gozalo-Brizuela and Garrido-Merchan 2023).\nDespite the considerable progress achieved, there remains\nsubstantial potential for improvement, particularly in gener-\nating higher-resolution images that more accurately reflect\nthe semantics of the input text and in designing more user-\nfriendly interfaces(Frolov et al. 2021). Many models find it\nhard to accurately comprehend the nuanced intentions behind\nhuman instructions, often leading to a mismatch between\nuser expectations and model outputs.\nMoreover, the impact of certain adjustments to variables\non the final image output is not always straightforward, pos-\ning a significant challenge for non-expert users who haven't\nsystematically learned prompt engineering courses. The in-\ntricacy involved in comprehending and manipulating these\nvariables presents a substantial obstacle for individuals with-\nout a technical background. Furthermore, given the same\ninput text, the model may still generate images with sub-\nstantially different content or layouts, where aspects such\nas background, color, and perspective can vary. In such in-\nstances, the user must engage in multiple trials, and acquiring\nan image that meets their specific requirements can depend\nsignificantly on chance.\nTo address these challenges, we introduce an innovative\ndialogic approach designed to enhance the user experience\nfor non-professional users. Within this dialogic interaction\nprocess, we posit the existence of a latent generative objec-\ntive in the user's mind. A single image may represent the\nuser's latent and unconscious generative goal. By iteratively\nquerying the user, we can progressively elicit more detailed\ndescriptions, with the ultimate aim of producing an image\nthat closely aligns with the user's underlying intent. Figure\n1 illustrates the operational flow of this project as interacted\nby the users. This approach is inspired by the concept of\nhuman-in-loop co-adaptation (Reddy, Levine, and Dragan\n2022), where the model evolves alongside user feedback to\nbetter align with user expectations. Our main contributions\nare:\n\u2022 We delve into human-machine interaction methods within\nimage generation tasks, guiding users through the process\nto effectively create images that reflect their intentions\nand preferences.\n\u2022 We introduce an enhanced Text-to-Image dialogue based\nAgent, which leverages both external interactions with\nusers and internal reflections to enhance its performance.\n\u2022 Application across general image and fashion image gen-\neration demonstrates the versatility and potential value of\nour approach."}, {"title": "Related work", "content": ""}, {"title": "Text-Driven Image Editing Framework", "content": "Recent advancements in text-to-image generation have fo-\ncused on aligning models with human preferences, using\nfeedback to refine image generation. Studies range from\nHertz et al. (Hertz et al. 2022)'s framework, which lever-\nages diffusion models' cross-attention layers for high-quality,\nprompt-driven image modifications, to innovative methods\nlike ImageReward (Xu et al. 2024), which develops a re-\nward model based on human preferences. These approaches\ncollect rich human feedback (Wu et al. 2023; Liang et al."}, {"title": "Ambiguity Resolution in Text-to-Image Generation", "content": "From visual annotations (Endo 2023) and model evaluation\nbenchmarks (Lee et al. 2024) to auto-regressive models (Yu\net al. 2022) for rich visuals, along with frameworks for ab-\nstract (Liao et al. 2023) and inclusive imagery (Zhang et al.\n2023), the text-to-image field is advancing through strategies\nlike masked transformers (Chang et al. 2023), layout guid-\nance (Qu et al. 2023) without human input, and feedback\nmechanisms (Liang et al. 2023) for quality. The TIED frame-\nwork and TAB dataset (Mehrabi et al. 2023) notably enhance\nprompt clarity through user interaction, improving image\nalignment with user intentions, thereby boosting precision\nand creativity."}, {"title": "Human Preference-Driven Optimization for\nText-to-Image Generation Models", "content": "Zhong et al. (Zhong et al. 2024) significantly advance\nthe adaptability of LLMs to human preferences with their\ninnovative contributions. Zhong et al.'s method stands out\nby leveraging advanced mathematical techniques for a\nnuanced, preference-sensitive model adjustment, eliminating\nthe exhaustive need for model retraining. Xu et al. (Xu et al.\n2024) take a unique approach by harnessing vast amounts\nof expert insights to sculpt their ImageReward system,\nsetting a new benchmark in the creation of images that\nresonate more deeply with human desires. Together, these\nadvancements mark a pivotal shift towards more intuitive,\nuser-centric LLMs technologies, heralding a future where\nAI seamlessly aligns with the complex mosaic of individual\nhuman expectations."}, {"title": "Proposed method", "content": "We developed a modular architecture tailored for image gen-\neration tasks within multi-turn dialogues. This architecture\nis designed to facilitate deep introspection of the generation\nsystem and effectively guide user interactions. The system\ncomprises several key components: The Memory stores the\ndialogue, denoted as h. The Summarizer, denoted as Ms,\nintegrates users' historical dialogue content, and generates a\nPrompt, denoted as P, for image generation. The Generation\nModel, denoted as MG, is responsible for transforming P\ninto specific images. The Reflection Block, denoted as BR,\nplays a crucial role. It not only handles the reasoning process\n(completing tasks in collaboration with the user) but also en-\ngages in internal reflection on the model. Within this module,\nthe Evaluator, marked as ME, is tasked with providing a\ncomprehensive description of the generated images. The Am-\nbiguity Inference Minf analyses the potential ambiguity and\noutputs an internal label r. Finally, the Action, designated as\nMA, displays the image and poses questions to the user. We\nprovide a detailed exposition of this interactive framework,\ndistinguishing between its internal and external workflows."}, {"title": "External Reflection via Verbal Reflection", "content": "The external reflection is contingent on user interactions.\nWhen the user presents a new prompt, the agent generates a\ncorresponding image and subsequently reflects on which\nintents to inquire about based on that image. This inter-\nactive process is termed Human-Machine Reflection (HM-\nReflection)."}, {"title": "Memory and Summarizer", "content": "The historical dialogues be-\ntween the user and the agent are stored in the Memory, while\nthe Summarizer Ms generates the prompt for controlling\nimage generation based on these historical dialogues. Let h\nrepresent the historical dialogues, t represent the current time,\nwt represent the current user's response, and Pt represent\nthe internal prompt used for image generation. The entire\nprocess can be expressed with the following formula:\nPt = Ms(wt, h)."}, {"title": "Generation Model", "content": "The Generation Model MG is central\nto the image generation, creating images based on provided\nprompts. Besides generating images that align with user in-\ntentions, it also incorporates additional details not explicitly\nmentioned by the user. For the general image generation\ntask, we use the Stable Diffusion model v1.4 (Rombach et al.\n2022). Specifically, for the fashion image generation task, we\nemploy a Stable Diffusion XL v1.0 (Podell et al. 2023), fine-\ntuned on fashion-related datasets. This is because fashion\nimages are generally uniform in layout and demand a richer\nrepresentation of fine-grained features. Let It represent the\ncurrently generated image. This process can be expressed as:\nIt = MG(Pt)."}, {"title": "Evaluator", "content": "In this interactive reflection framework, the\nEvaluator ME plays a critical role in assessing the quality of\nthe generated images. The Evaluator uses a visual language\nmodel (VLM) to describe the image content and generates\ncaptions that include aspects such as content, style, and back-\nground. We utilize Qwen-VL (7B) (Bai et al. 2023) in the\ngeneral image generation task and ChatGPT 4.0 (OpenAI\n2023) in the fashion image creation task, as the VLM evalua-\ntor. The generated captions are represented as Ct, where Ct\nencompasses N aspects of the description.\nCt = ME(It), Ct = {C1,C2,...,CN}."}, {"title": "Inference and Action", "content": "By comparing the similarity be-\ntween multiple captions Ct and the prompt Pt, the Ambiguity\nInference Model Minf identifies which contents are expected\nby the user and which are randomly generated, and output\nan Ambiguitiy label rt. Based on the detected ambiguities rt,\nthe Action MA asks the user for more detailed information.\nQuestion qt+1 can be selected from a predefined list of ques-\ntions or generated by a large language model (LLM) based\non the captions and prompts.\nrt = Minf (Ct, Pt),\nqt+1 = MA(Ct, rt)."}, {"title": "Internal Reflection via Direct Preference\nOptimization", "content": "An efficient intelligent interaction system not only provides\neffective feedback and guidance to users but also has the\nability to self-reflect. As illustrated in Figure 1, the Agent\nfeatures a 'Refine Image' step that optimizes the model or\noutput results. After generating multiple images, users can\nmark the ones they prefer. The Agent then learns user prefer-\nences from this feedback to produce images that better align\nwith user preferences. We employ a reinforcement learning\nmethod D3PO (Yang et al. 2023) for preference learning,\nwhich directly learns from user feedback without the need\nfor training a reward model. This functionality is designated\nas Tool 1. Additionally, we offer Tool 2, which checks the\nquality of generated images and regenerates those that do not\nalign with the corresponding prompt."}, {"title": "Tool 1: Direct Preference Optimization (DPO)", "content": "Figure 1\nillustrates the method of internal reflection via DPO. In Stage\n1, the generation model undergoes supervised fine-tuning\nto adapt to a specific generation task. In Stage 2, a certain\namount of preference feedback is accumulated through mul-\ntiple interactions with the user. This feedback is then used to\noptimize the model, resulting in more personalized outputs.\nThe optimization method employed is D3PO (Yang et al.\n2023), which expands the theoretical DPO into a multi-step\nMDP (Markov Decision Process) and applies it to diffusion\nmodels.\nGiven two image samples, the user selects the image they\nprefer, denoted as xw, while the other sample can be repre-\nsented as x1. Using the same weight, initialize a reference\nmodel tref, and a target model \u03c0\u03c1. During the denoising pro-\ncess, the diffusion model takes a latent s as input and outputs\na latent a. Based on the probability of tref, the overall loss\nof the D3PO algorithm gives:\nL(0) = - E log p\u1e9elog(\u03c0\u03bf(\u03b1 Tea | s\u00b9) / Tref(al | sl))."}, {"title": "Tool 2: Attend-and-Excite", "content": "Publicly available Stable Dif-\nfusion model The publicly available Stable Diffusion model\nexhibits issues with catastrophic neglect, where the model\nfails to generate the subjects or attributes from the input\nprompt. To address this issue in diffusion models and im-\nprove text-image alignment, we utilize the A&E algorithm\n(Chefer et al. 2023).\nFirst, we calculate the CLIP similarity score Sim between\nthe image and prompt. Then, we identify the neglected words\nby backpropagating the loss function l = 1-Sim. During the\nprocess of regenerating the image, we use the A&E method\nto activate these neglected words. Repeat the above process a\ncertain number of times. This Tool is detailed in Algorithm\n3."}, {"title": "Experiment", "content": "We explore the application of our proposed Enhanced Text-to-Image Reflexion Agent in two distinct scenarios: general\nimage generation and specific fashion product creation. Due\nto the different requirements of these applications, adjust-\nments have been made to our approach accordingly. In the\nexperiments, the focus varies between the two tasks. For\nthe general image generation task, we emphasize the effec-\ntiveness of our external reflection via verbal reflection. The\nemphasis of the fashion product creation task is placed on cap-\nturing fine-grained features within the images and addressing\nuser preferences."}, {"title": "Task 1 General Image Generation", "content": "The General Image Generation Task, powered by the En-\nhanced Text-to-Image Reflexion Agent, is designed to en-\nhance the user experience in image creation. Our agent not\nonly generates images based on textual instructions but also\nengages in dynamic dialogues with users, ensuring the im-\nages align more closely with their underlying intentions. This\ninteractivity ensures that the images are not only visually\nappealing but also meet the content expectations and needs\nof the users. Moreover, through real-time feedback loops and\ncontinuous interaction, the agent guides users and enhances\ntheir creative expression, allowing even those with minimal\nexperience to easily produce professional-level images."}, {"title": "Setting", "content": "In this task, the process begins with the Summarizer\ngenerating prompts by aggregating the user's input words.\nThese prompts are then used to generate images. The gener-\nated images are subsequently captioned by Qwen-VL (Bai\net al. 2023), a Vision-Language Model, covering seven as-\npects: 'Content', 'Style', 'Background', 'Size', 'Color', 'Per-\nspective', and 'Other'. By comparing the CLIP text similarity\nscores between the user's historical inputs and each caption,\nwe identify which aspects of the image contain ambiguity.\nFrom the three aspects with the lowest scores, one is ran-\ndomly selected for questioning. The question is displayed,\nand the user can choose whether to respond.\nTo quantify the effectiveness of human-in-the-loop image\ngeneration, we assumed a reference image as the user's gener-\nation target in the experiments. After each image generation,\nthe user responds based on the content of the target image\nuntil a certain number of iterations are completed. The simi-\nlarity between each generated image and the target image is\nthen evaluated to assess the effectiveness of our approach."}, {"title": "Data Collection", "content": "We collected those high-scoring image-text pairs from the ImageReward (Xu et al. 2024) dataset,\nwhich were gathered from real users. These high-scoring\nimages exhibit excellent visual quality and a high degree of\nconsistency with the original prompts. We excluded samples\nthat were abstract or difficult to understand, as well as those\nwith excessively long input prompts. Ultimately, we obtained\n496 samples covering a variety of subjects, including peo-\nple, animals, scenes, and artworks. And obtained over 2000\nprompts from users for image generation. Some of these im-\nages also contained content not explicitly mentioned in the\noriginal prompts. These reference images served as potential\ntargets for multi-turn dialogue generation, with each sample\nundergoing at least four rounds of dialogue."}, {"title": "Baseline setup", "content": "To demonstrate the effectiveness of our\nReflective Human-Machine Co-adaptation Strategy in un-\ncovering users' underlying intentions, we established sev-\neral baselines. One approach to resolving ambiguity in user\nprompts is to use Large Language Models (LLMs) to rewrite\nthe prompts. We employed several LLMs to augment the\ninitial prompts, allowing these models to infer the users' in-\ntentions. These LLMs include: ChatGPT-3.5, ChatGPT-4\n(Achiam et al. 2023), LLaMA-2 (Touvron et al. 2023), and\nYi-34B (AI et al. 2024). The relevant experiments are shown\nin Table 1. Table 1 presents the alignment between the gen-\nerated prompt and target image, as well as the alignment\nbetween the output image and target image. A subjective\nvisual evaluation (Human Voting) was used to select the im-\nage result that most closely resembles the target image. All\nexperiments were conducted on four Nvidia A6000 GPUs.\nThe diffusion model SD-1.4 employed the DDIM sampler.\nAdditionally, we validated the effectiveness of our Multi-dialog (HM-Reflection) approach in uncovering users' under-\nlying intentions by using different generative models. The\nrelevant experiments are shown in Table 2, including Stable\nDiffusion (v1.4), Stable Diffusion (v1.5) (Rombach et al.\n2022), and DALL-E (Ramesh et al. 2021)."}, {"title": "Task 2 Fashion Product Creation", "content": "Our second task is fashion product creation, a key applica-\ntion of image generation technology. In the future, generat-\ning fashion products like dresses and jackets that users can\npurchase or customize holds great potential. This approach\ncombines personalization and automation, offering highly\ncustomized shopping experiences. Users can generate ideal\ndesigns through simple text descriptions, reducing trial and\nerror costs. Brands and designers can quickly test market\nreactions, lower inventory risks. Overall, image generation\ntechnology in fashion has a promising future."}, {"title": "Setting", "content": "Fashion product creation is more challenging than\ngeneral image generation due to higher demands on image\nquality and diversity. Our Agent system also requires en-\nhanced reasoning and multimodal understanding capabilities.\nDuring the experiments, we used ChatGPT 4.0 for reason-\ning tasks beyond image generation, facilitating multimodal\ndialogues.\nFor image generation, we used the SD-XL 1.0 model for its\nsuperior capabilities. We referred to the DeepFashion dataset\n(Liu et al. 2016) for clothing types and attributes, creating\nlabels for collecting SD-XL 1.0 image samples. These images"}, {"title": "Conclusion", "content": "In this study, we explored the application of advanced im-\nage generation techniques integrated with human-machine\ninteraction frameworks to enhance personalization and visual\nappeal in both general image generation and fashion product\ncreation. Our Enhanced Text-to-Image Reflection System\ndemonstrated significant capabilities in guiding users to ar-\nticulate their generative intentions effectively. By leveraging\nboth external interactions and internal reflections, our agent\nwas able to learn from human feedback and align its outputs\nmore closely with user preferences. Future work will focus\non integrating finer user feedback mechanisms and leverag-\ning advancements in AI to further enhance the generative\nprocess, aiming to broaden the applicability and effectiveness\nof these technologies in various domains."}, {"title": "Limitations", "content": "This study, although advanced with the RHM-CAS, has cer-\ntain limitations. In the interaction process, due to prompts\ncontaining multiple high-level descriptions, the image gener-\nation model might not fully transform all of them into images.\nMoreover, the VL model's ability to capture fine-grained\ndetails is limited, which may result in inaccurate captions.\nThese cross-modal information transfer processes also lead\nto errors in information propagation, obstructing the expres-\nsion of user intent, and thereby affecting communication"}, {"title": "", "content": "efficiency. Apart from this, the method is computationally\nintensive, requiring substantial resources, which may limit\nits accessibility for users with less powerful hardware. Fur-\nthermore, the iterative refinement process, while effective,\ncan be time-consuming. This could potentially lead to user\nfrustration in time-sensitive situations.\nFuture efforts should aim to enhance computational effi-\nciency and broaden the system's ability to generalize across\nmore diverse inputs, improving usability in real-world appli-\ncations."}]}