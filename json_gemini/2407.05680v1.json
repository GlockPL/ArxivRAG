{"title": "Fine-Grained Multi-View Hand Reconstruction Using Inverse Rendering", "authors": ["Qijun Gan", "Wentong Li", "Jinwei Ren", "Jianke Zhu"], "abstract": "Reconstructing high-fidelity hand models with intricate textures plays a crucial role in enhancing human-object interaction and advancing real-world applications. Despite the state-of-the-art methods excelling in texture generation and image rendering, they often face challenges in accurately capturing geometric details. Learning-based approaches usually offer better robustness and faster inference, which tend to produce smoother results and require substantial amounts of training data. To address these issues, we present a novel fine-grained multi-view hand mesh reconstruction method that leverages inverse rendering to restore hand poses and intricate details. Firstly, our approach predicts a parametric hand mesh model through Graph Convolutional Networks (GCN) based method from multi-view images. We further introduce a novel Hand Albedo and Mesh (HAM) optimization module to refine both the hand mesh and textures, which is capable of preserving the mesh topology. In addition, we suggest an effective mesh-based neural rendering scheme to simultaneously generate photo-realistic image and optimize mesh geometry by fusing the pre-trained rendering network with vertex features. We conduct the comprehensive experiments on InterHand2.6M, DeepHandMesh and dataset collected by ourself, whose promising results show that our proposed approach outperforms the state-of-the-art methods on both reconstruction accuracy and rendering quality. Code and dataset are publicly available at https://github.com/agnJason/FMHR.", "sections": [{"title": "Introduction", "content": "3D human reconstruction has attracted considerable research attentions (Saito et al. 2019; Peng et al. 2021a; Weng et al. 2022; Chen et al. 2021; Noguchi et al. 2021; Bhatnagar et al. 2020) in recent years. While there have been promising advancements in reconstructing the human body and face (Lei et al. 2023; Grassal et al. 2022; Peng et al. 2021b; Xiu et al. 2022), it still remains a formidable challenge to achieve highly accurate hand reconstruction due to the inherent complexity of joint variations. By taking consideration of the distinctive nature for hands, it is essential to investigate the hand geometry and rendering for obtaining the realistic and fine-grained representations.\nConventional model-based methods, such as MANO (Romero, Tzionas, and Black 2017) and Nimble (Li et al. 2022b), often rely on smoothing meshes and texture maps for hand representation. Nevertheless, it typically requires costly scanning data and artistic expertise in order to achieve intricate and personalized hand meshes with texture maps. Moreover, the complex nature of hand movements and challenges posed by occlusions hinder the faithful restoration of hand mesh. Model-free approaches like LISA (Corona et al. 2022) aim to address these challenges by reconstructing coherent hands from image sequences, while HandAvatar (Chen, Wang, and Shum 2023) focuses on reconstructing and rendering hands in arbitrary poses by disentangling reflectance and lighting. Nonetheless, the resulting mesh often exhibits smoothness due to large pose variations across different frames.\nNeural rendering-based methods, such as NeRF (Mildenhall et al. 2021) and NeuS (Wang et al. 2021; Fu et al. 2022), have been widely used in synthesizing static objects from multi-view images. As for the dynamic objects like hands, these methods have the difficulties in obtaining a fixed topological structure through implicit surface representation. Despite their impressive rendering results, NeuS (Wang et al. 2021) requires a large amount of training time due to sampling along the ray. Meanwhile, NeuralBody (Peng et al. 2021b) attaches latent codes to SMPL (Loper et al. 2015) vertices, which enables to diffuse into space through sparse convolution. However, this may lead to some artifacts on the predicted mesh. To address these limitations, Hand-NeRF (Guo et al. 2023) proposes a pose-driven deformation field to render photo-realistic hands from various views and poses, while it does not explicitly provide a hand mesh for animation and real-time rendering.\nTo overcome the above challenges, we propose an effective coarse-to-fine approach to hand mesh reconstruction from multi-view images. By synergizing the benefits of parametric models and mesh-based rendering, our method achieves high-fidelity reconstruction results while maintaining fast training time with only 1.5 minutes. By incorporating multi-layer perceptrons into Graph Convolutional Networks (GCN) (Kipf and Welling 2017), we can simultaneously recover hand mesh and MANO parameters. Unlike learning-based methods that demand extensive training data, we introduce a Hand Albedo and Mesh (HAM) optimization module by leveraging inverse rendering to enhance level of details. The resulting fine-grained mesh preserves the flexibility provided by the parametric model MANO (Romero, Tzionas, and Black 2017), which enables to repose it into novel configurations. To further enhance rendering quality and refine the mesh, we suggest a mesh-based neural rendering scheme by fusing a pre-trained rendering network with vertex features. Our proposed method is evaluated on the InterHand2.6M, DeepHandMesh and dataset collected by ourself. The promising experimental results demonstrate its efficacy in reconstructing high-fidelity hands from multi-view images, as illustrated in Fig. 1.\nOur main contributions are summarized as below.\n\u2022 We propose a coarse-to-fine approach to accurately recover the fine-grained hand mesh model from multi-view images by taking advantage of inverse rendering.\n\u2022 A novel HAM optimization module is presented to refine the over-smoothing results of parametric hand models.\n\u2022 We devise an effective mesh-based neural rendering scheme to simultaneously generate photo-realistic image and optimize mesh geometry by fusing the pre-trained rendering network with vertex features."}, {"title": "Related Work", "content": "Model-Based Hand Reconstruction\nParametric models have been widely used in representing objects with the fixed typologies, such as the human body (Loper et al. 2015; Pavlakos et al. 2019; Osman, Bolkart, and Black 2020; Chen et al. 2022), face (Li et al. 2017; Hong et al. 2022), hands (Romero, Tzionas, and Black 2017; Li et al. 2022b), and animals (Zuffi et al. 2017). These models enable the transformation of mesh geometry by adjusting model parameters corresponding to pose and shape variations. Moreover, hand pose can be effectively estimated by images (Li, Gao, and Sang 2021) or point clouds (Cheng et al. 2022; Ren et al. 2023). In hand reconstruction, the parametric models like MANO (Romero, Tzionas, and Black 2017), NIMBLE (Li et al. 2022b), have been used to recover the hand in the input image (Boukhayma, de Bem, and Torr 2019; Hasson et al. 2019; Kong et al. 2022; Cao et al. 2021; Doosti et al. 2020; Hasson et al. 2020). In (Fan et al. 2021; Ren, Zhu, and Zhang 2023; Kim, Kim, and Baek 2021; Zhang et al. 2021; Li et al. 2022a), the parametric models are employed to reconstruct two hands, where hand interactions and gestures could be simulated. Recently, (Chen et al. 2023) employ the HandTrackNet to track the variations of MANO parameters, which is utilized for hand-object interactions. While parametric methods are able to recover hand poses and shapes, the resulting meshes lack the capability to represent geometric textures.\nModel-Free Hand Reconstruction\nParametric models (Romero, Tzionas, and Black 2017) are valuable for incorporating the prior knowledge of pose and shape, while their representation power is constrained by imposing the template shape and details. To overcome this limitation, various approaches have been explored. Instead of directly regressing the MANO parameters, I2L-MeshNet (Moon and Lee 2020) predicts a 1D heatmap for each vertex coordinate, and (Ge et al. 2019; Kulon et al. 2020) employ GCN-based methods to recover hand meshes. On the other hand, DeepHandMesh (Moon, Shiratori, and Lee 2020) make use of an encoder-decoder framework to generate highly detailed hand meshes. To achieve photo-realistic hand rendering, HARP (Karunratanakul et al. 2023) suggest an optimization-based approach to recover both normal and albedo maps. Recently, (Luan et al. 2023) introduce a frequency decomposition loss to capture personalized hand from a single image, which address the problem of data scarcity through multi-view reshaping. HandAvatar (Chen, Wang, and Shum 2023) yields occupancy and illumination field to generate free-pose photo-realistic hand avatar.\nNeural Rendering-Based Reconstruction\nIn past few years, rapid progress in 3D modeling and image synthesis has been obtained through neural implicit representations (Mescheder et al. 2019; Mildenhall et al. 2021). In contrast to the classical discrete representations like meshes, point clouds, and voxels, neural implicit representations leverage neural networks to model scenes, which offer continuous results with higher fidelity and flexibility. Among the various methods, Neural Radiance Fields (NeRF) (Mildenhall et al. 2021) gains great popularity and demonstrates impressive performance across various tasks. Nevertheless, the traditional NeRF-based methods face challenges in dealing with objects having temporal changes (Fu et al. 2022; Wang et al. 2021).\nNeural Body (Peng et al. 2021b) combines NeRF with the parametric SMPL body model (Loper et al. 2015), which is able to recover dynamic objects. Moreover, (Liu et al. 2021) leverage NeRF to learn pose-related geometric deformations and textures in canonical space from multi-view videos. Recently, LISA (Corona et al. 2022) fuses volumetric rendering with hand geometric priors to capture animatable hand appearances. HandNeRF (Guo et al. 2023) represents the interactive hands by deformable neural radiance fields to generate photo-realistic images."}, {"title": "Method", "content": "Our objective is to achieve fine-grained 3D hand mesh reconstruction and synthesize photo-realistic novel views. To tackle this challenge, our method consists of three key steps. Firstly, we estimate an initial coarse hand mesh and the parameters of MANO model (Romero, Tzionas, and Black 2017), which are recovered from multi-view images through incorporating multi-layer perceptrons (MLPs) into GCN. Secondly, an HAM optimization module is introduced to restore a fine-grained mesh with the albedo map and surface details by leveraging the power of inverse rendering. Finally, we suggest a mesh-based neural rendering scheme to efficiently generate photo-realistic images and refine the mesh through joint optimization. Fig. 2 illustrates the entire pipeline of our presented method."}, {"title": "Coarse Initialization", "content": "In hand reconstruction, it is crucial to accurately predict the 3D pose and shape due to the high flexibility of the hand. Since generating 3D poses is typically challenging in the wild cases, some approaches like HandAvatar (Chen, Wang, and Shum 2023) employ the annotated hand poses and shapes as initialization. Instead, we aim to estimate the consistent 3D hand poses from multi-view images.\nGiven a set of images $I = \\{I^1,\\dots,I^n\\}$ captured by the calibrated cameras and their corresponding 2D_joints $J^{2D} = \\{J^{2D}_p,\\dots, J^{2D}_p\\}$, 3D hand joints $J^{3D} \\in R^{B \\times 3}$ in world space with B per-bone parts can be estimated. To address the limitations of representing joint Euler angles directly with 3D joints information, we introduce a GCN-based network G to recover the MANO model as in (Choi, Moon, and Lee 2020).\n$\\mathcal{M}(\\theta, \\beta) = G(J^{3D}),$ (1)\nwhere $\\theta \\in R^{3 \\times 3}$ and $\\beta \\in R^{10}$ represent pose and shape parameters of MANO model, respectively. The network G is designed as a four-layer GCN with a MANO head. The MANO head consists of MLPs to obtain the corresponding MANO parameters via the features of the first three GCN layers, as shown in Fig. 3.\nGiven n-view images $I^n$ along with their corresponding camera parameters $\\pi^n$ and the positions of the hand joints $J^{2D}$ in the images, $J^{3D}$ can be obtained by minimizing the reprojection error across different views. The camera parameters include the intrinsic matrix K and extrinsic matrix T. In the i-th view, $J^{2D}_p$ can be calculated by $J^{2D}_p = \\pi^i(J^{3D})$. Since it is difficult to circumvent the issue of imperfect estimation of $J^{2D}$, we recover $J^{3D}$ by minimizing the following objective\n$L_{joints} = \\frac{1}{n} \\sum_{i=1}^{n} ||J^{2D} - \\pi^i(J^{3D})||^2.$ (2)\nOnce the 3D joints $\\hat{J}^{3D}$ with multi-view consistency is obtained, the GCN-based network G can be used to recover the MANO parameters. G is trained using the annotated datasets by minimizing the following energy function\n$L_G = L_v + L_n + L_j + L_{MANO}.$ (3)\nSpecifically, $L_v$ measures the $L_1$ distance between the predicted vertices V and annotated MANO vertices V as below\n$L_v = \\sum |V - V|.$ (4)\n$L_n$ ensures the alignment of mesh normals n with MANO, which is defined as\n$L_n = \\sum |n - n|.$ (5)\n$L_j$ constrains the discrepancy between the generated MANO joints $J^{3D}$ and the input joints $\\hat{J}^{3D}$ as follows\n$L_j = \\sum |J^{3D} - \\hat{J}^{3D}|.$ (6)"}, {"title": "Mesh Refinement", "content": "Additionally, $L_{MANO}$ is used to supervise the generation of MANO parameters, which is formulated as below\n$L_{MANO} = \\sum [(\\theta, \\beta) - (\\hat{\\theta}, \\hat{\\beta})].$ (7)\nWhile the GCN-based network G excels at accurately recovering the pose by utilizing 3D joints as input, it lacks information about hand fatness or leanness. To overcome this limitation, we incorporate segmentation map obtained by promptable SAM (Kirillov et al. 2023) in order to optimize the shape parameters \u03b2. This extra optimization step further enhances the accuracy of the MANO parameters.\nThe MANO model generated by G represents a smoothing mesh without geometric textures. Inspired by the Shape from Shading (SFS) algorithm (Horn 1970) and PatchShading (Lin et al. 2024; Lin, Zhu, and Zhang 2022), we accurately capture the folds and textures on the coarse hand mesh. Assuming that the diffuse reflection of human skin follows Lambertian reflectance, we propose a Hand Albedo and Mesh (HAM) optimization module to refine the coarse mesh, which takes advantage of both SFS and inverse rendering. To ensure the coherence in the mesh optimization process, we incorporate four effective regularization terms described in the following.\nWe utilize $M_r$ as the initial model that is obtained from the MANO subdivision with 49,281 vertices and 98,432 faces. The albedo values $\\rho$ are assigned to each vertex and an inverse renderer $\\zeta$ is employed to generate albedo map $C_{\\rho}$ and normal map $C_N$. The inverse renderer $\\zeta$ is represented as follows\n$C = \\zeta(c, V, F, \\pi),$ (8)\nwhere V and F denote the vertices and faces of mesh $M_r$, respectively. The feature at each vertex, such as normal and albedo, is represented by c. The rendering map C is obtained by the inverse renderer \u03b6. With the calibrated images $I^i$, the HAM optimizes the vertex positions V and vertex albedo \u03c1 by minimizing the texture loss $L_t$ in the following equation\n$L_t = \\sum [\\mathcal{B}(\\pi^i) - I^i],$ (9)\nwhere $\\mathcal{B}(\\pi^i)$ represents the rendered image under camera parameters $\\pi^i$. It is obtained by computing the illumination matrix G, the normal map $C_N$ and the albedo map $C_{\\rho}$ as below\n$\\mathcal{B}(\\pi^i) = C_{\\rho} \\cdot SH(G, C_N),$ (10)\nwhere $SH(\\cdot)$ represents sphere harmonic (SH) function with the third order. Considering the variations in lighting across different views, we optimize the lighting matrix G during the process. We compute the texture loss between $\\mathcal{B}(\\pi^i)$ and the original image $I^i$ as the loss $L_t$.\nTo enhance the efficacy of extracting geometric information from shadows, the regularization term $L_p$ is introduced. This term is especially designed to align with the observation that human skin tends to exhibit the consistent color. $L_p$ is defined as follows\n$L_p = \\lambda_1 L * \\rho,$ (11)\nwhere L denotes the Laplacian matrix. $\u03bb_1$ is the balanced weight. To ensure the optimized vertices to be smoothing, we introduce the following term for conforming to the geometric characteristics of the hand\n$L_r = \\lambda_2 L * V + \\lambda_3 L_{mask} + \\lambda_4 L_e + \\lambda_5 L_d$\n$= \\lambda_2 L * V + \\lambda_3 \\sum |M - M_{MANO}|$\n$+ \\lambda_4 \\sum||E_{i,j}||^2 + \\lambda_5 \\sum||\\triangle V_i||^2.$ (12)\nSpecifically, $L_{mask}$ represents the $L_1$ loss between the rendered mask and the original MANO mask. $E_{ij}$ is obtained by calculating the Euclidean distance $||\\cdot||^2$ between adjacent vertices $V_i$ and $V_j$ on the mesh edges. $L_e$ is employed to restrict the length of $E_{ij}$. Let $\\triangle V_i$ represent the displacement distance of vertices $V_i$. $L_d$ is utilized to ensure that the optimized hand remains close to the MANO model. Each term is assigned with a constant coefficient denoted by \u03bb. The overall loss function $L_{total}$ is defined as below\n$L_{total} = L_t + L_p + L_r.$ (13)\nThe HAM module facilitates the refinement of the hand model by jointly optimizing both the mesh vertices V and albedo $\\hat{\\rho}$. This results in a high-quality output $M_f$ that retains the consistent topology."}, {"title": "Texture Optimization and Joint Refinement", "content": "While having achieved geometric alignment of the fine mesh, there are still some limitations in image rendering and mesh refinement. To alleviate this issue, we adopt a mesh-based neural rendering method. Leveraging the preserved topology of our refined hands, the neural rendering model can be pre-trained with diverse hand data, thereby reducing the training time for each individual hand. We propose an efficient strategy that involves the pre-training a neural rendering network using a large amount of hand data, followed by fine-tuning on individual data, and ultimately conducting joint optimization with the mesh to achieve hyper-realistic rendering and accurate geometry.\nHaving acquired a refined mesh denoted as $M_f(V, F)$ along with its corresponding vertex albedo $\\hat{\\rho}$, we aim to enhance the accuracy of the rendered texture. To this end, we design a neural renderer T defined as below\n$t(r) = T(x, f, \\rho, n),$ (14)\nwhere the output pixel value t is determined by the direction of the light ray r. At a given pixel position, the texture field t is adjusted with respect to the position x, feature vector f, albedo \u03c1, and normal n, which are obtained by inverse rendering . For the neural renderer, we employ the MLPs with four layers, each of which consists of 256 dimensions. The length of vertex features f is set to 20. The training loss is defined as follows\n$L_{tex} = \\sum |t_i - I_i|.$ (15)\nPre-training. During the pre-training, we conduct sampling from the subdivided MANO mesh $M_r$ instead of utilizing $M_f$. Through the pre-training, we obtain a well-trained neural renderer that unifies the vertices features f independent from position x, albedo \u03c1 and normal n.\nFine-tuning. Given the diversity of data and potential overfitting during training, the pre-trained neural rendering model often yields average results, which is unable to capture intricate texture details. To synthesize high-fidelity hand images and achieve promising rendering quality, it is necessary to fine-tune the model for each specific dataset. The advantage of the pre-trained model lies in its capacity to accelerate neural rendering, allowing for efficient completion of fine-tuning within minutes. During the fine-tuning process, we employ the vertices of mesh $M_f$ obtained from Mesh Refinement along with its corresponding albedo for training. Both the vertex features f and the neural renderer T are set to be learnable.\nJoint Optimization. After achieving fine geometric structures and realistic image rendering, it becomes necessary to perform joint optimization on both mesh and texture to further enhance the overall quality. Drawing inspiration from (Walker et al. 2023), we adopt a geometry-based shader t with the detached output t of t(r), which is illustrated as follows\n$t(r) = \\gamma(t, f, \\rho, n).$ (16)\nTo fine-tune the geometry and train the geometry-based shader, the loss functions $L_e$ and $L_d$ depicted in Eq. 12 are employed to ensure surface smoothness. Moreover, the vertices are designated to be learnable. The shader loss is formulated as follows\n$L_{geo} = \\sum |t_i - I_i| + \\gamma_1 L_e + \\gamma_2 L_d.$ (17)\nwhere $\u03b3_1$ and $\u03b3_2$ are weights for $L_e$ and $L_d$, respectively."}, {"title": "Experiments", "content": "Datasets\nInterHand2.6M. InterHand2.6M (Moon et al. 2020) is a large-scale dataset, comprising images of size 512 \u00d7 334 pixels and associated MANO annotations. The dataset contains multi-view temporal sequences of single hand as well as interacting hands. Our experiments primarily focus on the 5 FPS version of the InterHand2.6M dataset. In all experiments, both the GCN-based network and pre-trained neural rendering network are trained using the data from the training set.\nDeepHandMesh. The DeepHandMesh dataset (Moon, Shiratori, and Lee 2020) consists of images captured from five different views with the same size as the images in Inter-Hand2.6M dataset. Additionally, this dataset provides the corresponding 3D hand scans, which enables the validation of the mesh reconstruction quality against 3D ground truth.\nOur Dataset. Due to the restricted resolution of the above datasets, the attainment of higher geometric precision and color fidelity requires high-resolution hand images. To address this issue, we collect a dataset using 16 calibrated cameras, which captures the synchronized images at a resolution of 1280 x 1024 pixels at 15 FPS. The cameras are distributed mainly in a semi-circle and placed at various heights to ensure a comprehensive visual coverage.\nImplementation Details and Metrics\nOptimization. To achieve the fine-grained mesh, we adopt a subdivision technique inspired by (Chen, Wang, and Shum 2023), which expands the original 778 vertices in the MANO model to a total of 49,281 vertices. During the optimization process, we utilize the Adam optimizer (Kingma and Ba 2014) with the balanced weights of $\\lambda_1 = 20$, $\u03bb_2 = 40, \u03bb_3 = 20, \u03bb_4 = 100$, and $\u03bb_5 = 2$ to jointly optimize the vertices, vertex albedo, and lighting coefficients over 100 iterations. This optimization process takes approximately 20 seconds. Additionally, the neural renderer is pre-trained on InterHand2.6M dataset for 20 epochs. Subsequently, for fine-tuning and joint optimization, each process requires 100 epochs of training with $\u03b3_1 = 100$ and $\u03b3_2 = 2$, respectively. Notably, the entire optimization pipeline is computationally efficient, which takes approximately 90 seconds on a single NVIDIA 3090Ti GPU.\nEvaluation Metrics. We evaluate the accuracy of the reconstructed 3D surface by computing the average point-to-surface Euclidean distance (P2S) between the vertices of the recovered surface and their corresponding ground truth, which are measured in millimetre. Due to the disparate size ranging between the generated hand mesh and the 3D scans in the DeepHandMesh data, the Chamfer distance metric is considered unsuitable. In line with prior research in neural rendering, peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and learned perceptual image patch similarity (LPIPS) are adopted as evaluation metrics to gauge the fidelity of the synthesized image results.\nResults on Hand Reconstruction\nResults of InterHand2.6M. To evaluate the quality of novel view synthesis, we conduct experiments on the InterHand2.6M dataset using 10 views and evaluate on the rest views. Table 1 summarizes the evaluation results on rendering quality. Fig. 4 illustrates the visual comparisons against HandNeRF (Guo et al. 2023), HandAvatar (Chen, Wang, and Shum 2023) and NeuS (Wang et al. 2021). The metrics of Ani-NeRF are extracted from the data presented in (Guo et al. 2023). It is important to note that HandAvatar lacks support for interactive hands, while HandNeRF is not able to directly predict geometry. Both HandNeRF and HandAvatar rely on learning from video sequence for voxel rendering with large pose variations, which may result in smoothing texture. By taking advantage of the design of our topology-consistent hand mesh and the mesh-based neural"}, {"title": "Conclusion", "content": "In this paper, we introduced a novel fine-grained multi-view hand mesh reconstruction method by leveraging effective inverse rendering to restore hand poses and intricate details. Our approach predicted a parametric hand mesh model by a GCN-based network while refining both the hand mesh and textures through the Hand Albedo and Mesh (HAM) optimization module. To generate photo-realistic image, we suggested an effective mesh-based neural rendering scheme by fusing the pre-trained rendering network with vertex features. Through extensive experiments on diverse datasets, the promising results demonstrated the efficacy of our proposed approach."}, {"title": "More Details of Our Method", "content": "Coarse Initialization\nDetails of GCN-based Network. As described in (?), Graph Convolutional Networks (GCN) can be employed to extend joints to the vertices of a parameterized model in cases that the corresponding parameters are missing. To this end, we introduce an MANO head that acquires the MANO parameters based on the vertex features obtained from the GCN layers. The MANO head is composed of a 4-layer Multi-Layer Perceptron (MLP) with dimensionality reduction of half at each layer, starting from an initial dimension of 256. Calculating loss for both MANO parameters and vertices involves the use of corresponding ground truth. This ensures that the head effectively acquires the parameters corresponding to the generated vertices.\nIn order to improve the robustness of our GCN-based network and account for the varying scales present in different datasets, we adopt a strategy that involves initially adjusting the joints to a standardized space during the training process. Subsequently, the adjusted joints undergo processing through the GCN-based network, which are further restored back to their original coordinate system based on the previous adjustment. This guarantees that the network operates on the standardized inputs, thereby enabling its applicability across diverse datasets.\nTo further refine the MANO parameters, segmentation maps $M$ from SAM (?) are used to optimize $\\beta$ by following energy function\n$\\mathcal{L}_M = \\sum|M - \\hat{M}|,$\nwhere $\\hat{M}$ is the mask rendered from MANO mesh.\nMesh Refinement\nMANO Subdivision. The original MANO mesh has limited capacity to represent fine-grained details (?), as it consists of only 778 vertices and 1538 faces. To address this limitation and enhance the mesh resolution to capture intricate features, we adopt an uniform subdivision strategy to the MANO template mesh. This process involves adding new vertices at edge midpoints, resulting in a refined mesh denoted as $M_r$, with 49,281 vertices and 98,432 faces. To assign the skinning weights to these additional vertices, we compute the average of weights assigned to each endpoint of the corresponding edges."}, {"title": "Additional Results", "content": "Novel Pose Synthesis. After obtaining a detailed mesh and rendering network for a specific pose, our method makes use of MANO parameters and linear blending skinning (LBS) algorithms to transform the mesh to novel poses. The quantitative results are summarized into Table 1. Also, Fig. 1 shows some visual examples. These experimental results demonstrate that our approach is able to accurately render hands in novel poses by deforming the refined mesh. In comparison, HandAvatar (?) has realistic hand lighting across poses due to training with video sequences, while its texture tends to be more smoother. Note that PSNR reflects the degree of image distortion, and LPIPS is considered to be more meaningful (?).\nTraining Time. To compare the efficiency of generating personalized hands, we conduct experiments and compare our method with LISA (?), HandAvatar (?), and NeuS (?) in Table 2. The experiments are conducted by taking consideration of the time required to reconstruct hands for the same individual.\nNovel View Synthesis. In addition to the visual results provided in the main paper, we report more experimental results"}, {"title": "Limitations and Future Works", "content": "In this paper, we leveraged multi-view 2D keypoints to recover the MANO parameters. The accuracy of 2D keypoints estimation plays a crucial role in determining the quality of our results. In future research, it would be beneficial to explore the integration of image information with the refinement network to fine-tune the MANO parameters.\nIt is important to note that our method operates under the assumption of diffuse reflection. However, the presence of specular reflections may lead to less desirable results. It is worthy of exploring hand mesh reconstruction based on the Bidirectional Reflectance Distribution Function (BRDF), which takes into account the presence of oily skin and other surface properties."}]}