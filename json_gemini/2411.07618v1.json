{"title": "DIRECT PREFERENCE Optimization Using Sparse FEATURE-LEVEL CONSTRAINTS", "authors": ["Qingyu Yin", "Chak Tou Leong", "Minju Zhu", "Hongbo Zhang", "Hanqi Yan", "Qiang Zhang", "Yulan He", "Wenjie Li", "Jun Wang", "Yue Zhang", "Linyi Yang"], "abstract": "The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often experience computational inefficiencies and training instability. In this paper, we propose Feature-level constrained Preference Optimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves an above 5% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.", "sections": [{"title": "1 INTRODUCTION", "content": "Aligning large language models (LLMs) with human values and practical objectives is a critical challenge in AI development (Wang et al., 2023). Post-training methods, including fine-tuning (Wei et al., 2022; Chung et al., 2024) and alignment strategies (Tunstall et al., 2023), have played a significant role in refining LLM behavior. Among these, Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022) has emerged as a leading technique, integrating human feedback to guide models towards producing valuable and useful outputs. Despite its success, RLHF involves complex mechanisms such as reward modeling and policy gradients, which introduce significant training complexity and computational cost (Zheng et al., 2023b; Rafailov et al., 2024). To address these limitations, Direct Preference Optimization (DPO) (Rafailov et al., 2024) has been proposed as a more efficient alternative. Unlike reward-based methods such as Proximal Policy Optimization (PPO) (Schulman et al., 2017), DPO directly adjusts the model's output probabilities based on human preferences, reducing training complexity and computational cost. DPO-like approaches can offer a more stable and faster alignment process by bypassing the challenges associated with reward models and policy updates, making it a compelling solution for efficient LLM alignment since DPO uses a reference model to stabilize post-training.\nRecent advancements in DPO focus on mainly two directions: efficiency i.e., further simplifying the constraints of DPO, and controllability i.e., keeping the balance between alignment and generation diversity. In terms of simplicity, methods like SimPO (Meng et al., 2024) and Odds Ratio Preference Optimization (ORPO) (Hong et al., 2024) eliminate the need for a reference model by using the average log probability of sequences as an implicit normalizer, thereby reducing memory usage and computational demands. However, DPO's performance is sensitive to the strength of constraints from the reference policy (Liu et al., 2024), and these reference-free alignment approaches (Hong et al., 2024; Meng et al., 2024) can compromise control, resulting in unstable training. In terms of controllability, Token-level Direct Preference Optimization (TDPO) (Zeng et al., 2024) introduces"}, {"title": "2 PRELIMINARY", "content": "Direct Preference Optimization (DPO). DPO, derived from Reinforcement Learning from Human Feedback (RLHF), provides a direct way to align Language Models (LLMs) with human preferences without explicitly using a reward model. In practice, an LLM is prompted with a sequence x (e.g., a question) to generate a corresponding sequence y (e.g., an answer), where both x and y consist of tokens. DPO maps the reward function r(x, y) to the optimal policy by minimizing the reverse KL divergence from a reference model. This results in the following equation for the reward:\n r(x,y) = Blog \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)} + Blog Z(x),\nwhere $\\pi_{\\theta}(\\cdot|x)$ and $\\pi_{ref}(\\cdot|x)$ are policy (i.e, the LLM for post-training) and reference (i.e., the base LLM) models, respectively. $\\beta$ is the coefficient that governs the strength of the KL divergence penalty, Z(x) is the partition function. To align with human preferences, DPO uses the Bradley-Terry (BT) model for pairwise comparisons. By incorporating the reward function into the BT model and using the negative log-likelihood, DPO computes the loss:\nL_{DPO} (\\pi_{\\theta}; \\pi_{ref}) = -E_{(x,y_w,y_l)\\sim D} \\left[log \\sigma \\left(\\beta \\left(Blog \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - Blog \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)}\\right)\\right)\\right] \nHere, D represents the dataset with human preference pairs. yw and yl are the preferred and less preferred completions, respectively. DPO provides a direct way to align LLMs with human preferences without the explicit use of a reward model, leveraging preference comparisons.\nSimple Preference Optimization (SimPO). SimPO simplifies preference optimization by removing the need for a reference model and aligning rewards directly with the length-normalized log-likelihood of the policy model's output. The SimPO objective can be formulated as:\nL_{SimPO}(\\pi_{\\theta}) = -E_{(x,y_w,y_l)\\sim D} \\left[log \\sigma \\left(\\frac{Blog \\pi_{\\theta} (y_w|x)}{\\vert y_w \\vert} - \\frac{Blog \\pi_{\\theta} (y_l|x)}{\\vert y_l \\vert} - \\gamma \\right)\\right],\nwhere $\\gamma$ is a positive margin ensuring the reward for the preferred response exceeds that of the less preferred one by at least $\\gamma$. SimPO's key innovations are (1) eliminating the reference model and (2) incorporating a target reward margin $\\gamma$. However, while SimPO is computationally efficient, the lack of reference control (Roy et al., 2021) results in instability. As shown by Liu et al. (2024), the reference model plays a crucial role in stabilizing training and improving performance.\nToken-Level Direct Preference Optimization (TDPO). Token-Level Direct Preference Optimization (TDPO) refines the DPO framework by operating at the token level, accounting for the sequential nature of text generation. The TDPO objective function is defined as:\n$\\max E_{x,y<t\\sim D,z\\sim\\pi_{\\theta}(\\cdot[[x,y<t])} [A_{ref} ([x, y<t], z) \u2013 \u03b2D_{KL}(\\pi_{\\theta}(\\cdot|[x, y<t])||\\pi_{ref}(\\cdot|[x, y<t]))]$,\nwhere $A_{ref} ([x, y<t], z)$ is the token-level advantage function, and $D_{KL}(\\pi_1||\\pi_2)$ denotes the KL divergence between $\\pi_1$ and $\\pi_2$. The first version of the loss function is given by:\nL_{TDPO1}(\\pi_{\\theta}; \\pi_{ref}) = -E_{(x,y_w,y_l)\\sim D} \\left[log \\sigma \\left(\\beta \\left(Blog \\frac{\\pi_{\\theta} (y_w|x)}{\\pi_{ref}(y_w|x)} - Blog \\frac{\\pi_{\\theta} (y_l|x)}{\\pi_{ref}(y_l|x)}\\right) - \\zeta_{TDPO_1}(X, y_w, y_l)\\right)\\right],\nwhere $\\zeta_{TDPO_1} (x, y_w, y_l)$ is the difference in forward KL divergence between the preferred and less preferred completions:\n$\\zeta_{TDPO_1} (x, y_w, y_l) = \u03b2D_{TDPO_1} (x, y_l; \\pi_{ref}||\\pi_{\\theta}) \u2013 \u03b2D_{TDPO_1} (x, y_w; \\pi_{ref}||\\pi_{\\theta})$,\nand the sequential KL divergence between policy and reference output with sequence length T is defined as $D_{TDPO}(x, y; \\pi_{ref} ||\\pi_{\\theta}) = \\sum_{t=1}^{T}D_{KL} (\\pi_{ref} (\\cdot|[x, y<t])||\\pi_{\\theta}(\\cdot|[x, y<t]))$. To further stabilize the gradient within the optimization, an improved loss function $L_{TDPO2}$ is given by replacing the regularization $\\zeta_{TDPO1}$ with:\n$\\zeta_{TDPO2} (x, y_w, y_l) = \\alpha (\u03b2D_{TDPO} (x, y_l; \\pi_{ref}||\\pi_{\\theta}) \u2013 sg (\u03b2D_{TDPO} (x, y_w; \\pi_{ref}||\\pi_{\\theta})))$,\nwhere a is an additional hyperparameter to balance between alignment and regularization, $\\beta$ is the coefficient that governs the strength of the KL divergence, and sg denotes the stop-gradient operator. Unlike DPO, TDPO introduces token-level forward KL divergence, allowing for finer control over model alignment and diversity in generation, also introducing additional computational overhead."}, {"title": "3 FEATURE-LEVEL DIRECT PREFERENCE OPTIMIZATION", "content": "In the right table of Figure 2, we present a comparison of FPO with other methods from three perspectives: reference model usage, efficiency, and constraint control, which is distinguished from existing methods in the following aspects:\nReference-free methods such as SimPO and ORPO are memory and computation efficient. However, they struggle with instability brought by the lack of reference constraints.\nAlignment methods with KL control on output logits, like TDPO and KTO (Ethayarajh et al., 2024), are powerful yet controllable, but their sequential KL based on output probabilities makes them costly.\nInterpretability methods such as SAE are widely used for interpreting the inner representations of LLMs due to their sparse and monosemantic activations Chen et al. (2017); Huben et al. (2024). However, this feature has not yet been applied in areas outside of interpretability.\nDPO with Reference-base Target Margin. To begin, we examine the loss functions of DPO and its enhanced variants, specifically SimPO and TDPO. By comparing Equation (2) and Equation (4), we notice that TDPO and DPO share an identical implicit reward difference term: $Blog \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - Blog \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)}$. Essentially, TDPO can be viewed as an extension of DPO, where a KL constraint d(x, yw, yl) is incorporated into the sigmoid function $\\sigma(\\cdot)$ in addition to the implicit reward difference. Taking a step further, we can isolate $\\pi_{ref}$ from each implicit reward term:\nBlog \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - Blog \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)} = \u03b2 log \\pi_{\\theta} (y_w|x) \u2013 Blog \\pi_{\\theta}(y_l|x) - \u03b2 (log \\pi_{ref}(y_w|x) \u2013 log \\pi_{ref}(y_l|x)). \n:=\u03b3ref\nThe loss function of KTO is similar to that of TDPO in terms of its use of KL divergence."}, {"title": "4 EXPERIMENTAL SETUP", "content": "Model and Training Settings. Our model selection is guided by two key principles: scalability and transparency. For scalability, we first select a series of models spanning different parameter sizes, including Gemma-2-2B and Gemma-2-9B (Team et al., 2024). This ensures that we can evaluate our approach's performance as the model parameters scale and assess its robustness across diverse model architectures. For transparency, we exclusively select foundational models, which have not undergone supervised fine-tuning (SFT) or alignment processes. We begin by fine-tuning these models using a unified conversational format provided by the Halos dataset, applying it to the Ultrachat-200K (Ding et al., 2023). Dataset for initial instruction tuning. This establishes a baseline conversational capability and ensures that all our methods are compared on a consistent SFT model. Subsequently, we employ the UltraFeedback (Cui et al., 2024). Dataset to align the SFT models using various methods. This approach maintains transparency and control throughout the process, as all data and methods are open-sourced across the experimental setup.\nFor the hyperparameters related to alignment methods, such as a and \u03b2, we initially refer to the hyperparameter settings from the corresponding papers. If these settings are explicitly provided, we"}, {"title": "5 RESULTS AND DISCUSSIONS", "content": "FPO Consistently Outperforms Strong Baselines on Three Benchmarks. We evaluate the performance differences between FPO and other methods across three key aspects: training accuracy, generation diversity, and performance on downstream tasks. In terms of downstream tasks, we assess the model's performance including the winning rate or score on the AlpacaEval2 Benchmark, Arena Hard, and MT Bench. As shown in Table 2, FPO achieves highly competitive results, with up to a 5.08% improvement in winning rate compared to other methods when testing on Gemma-2-2B. Additionally, based on Gemma-2-9B, we observe a consistent improvement in our method compared to baselines. However, the performance improvements on the 9B model introduced by FPO are limited compared to the 2B model. We argue that this is because, with the same width of the SAE, smaller models, due to their lower complexity, achieve a more thorough decomposition of features, filtering more noisy features, and leading to more accurate constraints."}, {"title": "5.1 THE TRADE-OFF BETWEEN CONTROLLABILITY AND EFFICIENCY", "content": "Accuracy vs. Diversity. We measure the training accuracy on the UltraFeedback dataset, which is defined as the probability that the chosen answer's token-wise probabilities exceed those of the rejected answer. Table 3 shows the model's generation diversity by measuring the entropy of the top 100 results on AlpacaEval2, where the \u2191 indicates higher values are preferable. We use bold to show the best-performing result across all metrics, and underline to denote the second-best result. The results indicate that FPO achieved the second-highest training accuracy, only behind TDPO2, outperforms other baselines, and has the highest diversity. We also demonstrate that FPO exhibits entropy levels comparable to methods like TDPO-2, which excel in controlling output diversity, indicating the effectiveness of FPO.\nFPO Yields Better Controllability and Efficiency Trade-off. Using Gemma-2-2B as the base model, we first conduct dialogue fine-tuning and proceed with the testing phase. For the calculation of KL divergence, we consistently apply TDPO's sequential KL divergence method. Specifically, we compute the KL divergence of the policy model relative to the reference model for both the preferred response (i.e., chosen) and the dispreferred response (i.e., rejected). The results (See Table 3) indicate that, due to FPO's excellent KL control and well-designed reward structure, it achieves performance comparable to other methods while maintaining lower computational costs.\nHardware Efficiency of FPO. Given the efficiency of FPO compared to TDPO2, as shown in the left one in Figure 4, we consider this result to be highly competitive. The efficiency of FPOis reflected primarily in two aspects: (1) Offline Processing. FPO does not require an additional reference model to be loaded during training, but only incurs minimal I/O overhead to read pre-stored information at each step, specifically the one-dimensional tensors needed for training. This process can be efficiently handled by the dataloader. (2) Sparsity. Due to the sparse activation values in the SAE encoder, we only need to process the activated values, reducing computational overhead. To validate its efficiency, we tested the memory consumption of different methods during training. In terms of memory usage, FPO maintains nearly the same level of memory consumption as reference-free methods like SimPO. Compared to methods that introduce more computation, such as TDPO, FPO achieves approximately a 17% memory optimization.\nIt is important to note that, compared to reference-free methods like SimPO, FPO still requires pre-computation of the reference model's log probabilities and SAE feature activations. However, this reduces the peak computational and memory demands, making the model easier to run on smaller devices with lower costs. Considering that scaling up computational resources is generally more challenging than extending runtime, we believe this represents a reasonable trade-off between perfor-"}, {"title": "5.2 ABLATION STUDY", "content": "To validate the insertion position of the SAE encoder and the settings of other hyperparameters, we conduct an ablation study as shown in Table 4. We train Gemma-2-2B on UltraFeedback for one epoch to evaluate the performance of different configurations. In terms of metrics, we focus on accuracy and diversity (measured by entropy) to balance alignment and diversity. Regarding the insertion position of the SAE encoder, we test the following: (1) Inserting at different layers, including shallow, middle, and deep layers. (2) Inserting the encoder after the residual stream, i.e., immediately after the residual connection to extract features, versus inserting it after the output of the MLP layer. We did not test the insertion after the attention output, as SAE is designed to capture more polysemous features in the MLP layer and the final residual output. Prior work supports this design. (3) Varying the value of a, which affects the strength of the constraint. (4) The use of the stop-gradient operator. From Table 4, we show that inserting the encoder closer to the final output leads to better performance. We hypothesize that this is because the layers near the final output have"}, {"title": "6 RELATED WORK", "content": "Preference Optimization Methods in LLMs. Reinforcement learning (RL) has become a popular post-training technique, enabling models to learn implicit rewards from human feedback (Ouyang et al., 2022; Dubey et al., 2024; Yang et al., 2024a). The traditional RL pipeline involves training a reward model and updating the policy model via Proximal Policy Optimization (PPO) (Schulman et al., 2017). Recent work, such as DPO (Rafailov et al., 2024), leverages the log ratio between policy and target models to directly update policies based on the reward model's objective. Extensions of DPO have introduced further refinements. KTO (Ethayarajh et al., 2024) eliminates pairwise data by modifying the value function using prospect theory, allowing training on individual sequences. Token-level DPO (Zeng et al., 2024) enforces constraints at the token level to improve generative diversity and also extends to the selection of specific tokens in pre-training (Lin et al., 2024) and post-training (Yang et al., 2024b). To reduce computational costs, ORPO (Hong et al., 2024) and SimPO (Meng et al., 2024) remove the reference model, streamlining training. Our approach similarly omits the reference model for computational efficiency but uniquely integrates feature-level constraints to achieve both high efficiency and quality in preference learning.\nInterpretating LLMs in Feature Space. One approach to LLM alignment focuses on enhancing transparency via mechanism interpretability (Shen et al., 2023; Wu et al., 2024). A central research goal in this area is understanding how LLMs internally extract, represent, and compute human-understandable features (Rai et al., 2024; Ferrando et al., 2024). Contrary to earlier assumptions, most neurons in LLMs do not activate exclusively for specific features but form polysemantic neurons (Mu & Andreas, 2020; Gurnee et al., 2023), a phenomenon termed \u2018superposition' (Elhage et al., 2022), which arises from compressing numerous learnable features into a limited number of dimensions (H\u00e4nni et al., 2024). Recent work shows that sparse autoencoders (SAE) can address this by decomposing internal representations into sparse, monosemantic features, improving interpretability (Huben et al., 2024; Templeton et al., 2024; Gao et al., 2024). Due to its scalability, SAE has been used to analyze LLM monosemanticity. Yan et al. (2024) found that alignment increases monosemanticity, while Marks et al. (2023) revealed that aligned LLMs learned feedback features related to human preferences, enhancing their output alignment. However, SAE has not yet been applied to construct feature-level constraints for improving LLM alignment."}, {"title": "7 CONCLUSION", "content": "In conclusion, we proposed FPO, a novel method for efficient and stable alignment of large language models using feature-level constraints. By leveraging sparse autoencoders and pre-computed offline references, FPO reduced the computational overhead traditionally associated with alignment methods like DPO and TDPO. Our experimental results demonstrate that FPO achieved significant improvements in alignment accuracy and diversity while maintaining low resource consumption. We prove that FPO achieved improvements over current state-of-the-art methods along all three dimensions: simplicity of implementation, efficiency, and generation quality."}, {"title": "A TRAINING SETTINGS", "content": ""}, {"title": "B BOUNDING KL DIVERGENCE WITH MSE OF SPARSE ACTIVATION", "content": "Theorem 1. Let $\\pi_{\\theta}$ and $\\pi_{ref}$ be two models with final layer outputs $h_{\\theta}^{t,L}, h_{ref}^{t,L} \\in R^d$ at position $t$. Let $c_{\\theta}^{t,L}, c_{ref}^{t,L} \\in R^m$ be their respective sparse activation generated by a SAE. Under certain conditions, minimizing the MSE between these sparse activation values leads to a reduction in the upper bound of the KL divergence between their token probability distributions.\nProof. We begin by establishing key definitions and conditions:\nDefinition 1 (Sparse Activations).\n$c^{t,L} = ReLU(W_{ench}^{t,L} + b)$\nDefinition 2 (Token Logits and Probabilities).\nz = W_{out}h^{t,L}, p = softmax(z)\nDefinition 3 (KL Divergence).\n$D_{KL} (p_{ref} ||p_{\\theta}) = \\sum_{i=1}^{V} p_{ref} (i) log \\frac{p_{ref} (i)}{p_{\\theta}(i)}$\nCondition 1 (Accurate Reconstruction). The SAE reconstructs hidden representations accurately, i.e., for some small \u20ac > 0:\n||W_{dec}c^{t,L} - h^{t,L} ||_2 < \u20ac\nCondition 2 (Bounded Operator Norm).\n||K||_2 < M for K = W_{out}^T W_{dec} and some M > 0\nCondition 3 (Small Logit Differences). The difference in logits $\\Delta z^{t} = z_{\\theta}^{t} - z_{ref}^{t}$ is small enough for the quadratic approximation of the KL divergence to hold."}]}