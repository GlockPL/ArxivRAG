{"title": "UrFound: Towards Universal Retinal Foundation Models via Knowledge-Guided Masked Modeling", "authors": ["Kai Yu", "Yang Zhou", "Yang Bai", "Zhi Da Soh", "Xinxing Xu", "Rick Siow Mong Goh", "Ching-Yu Cheng", "Yong Liu"], "abstract": "Retinal foundation models aim to learn generalizable representations from diverse retinal images, facilitating label-efficient model adaptation across various ophthalmic tasks. Despite their success, current retinal foundation models are generally restricted to a single imaging modality, such as Color Fundus Photography (CFP) or Optical Coherence Tomography (OCT), limiting their versatility. Moreover, these models may struggle to fully leverage expert annotations and overlook the valuable domain knowledge essential for domain-specific representation learning. To overcome these limitations, we introduce UrFound, a retinal foundation model designed to learn universal representations from both multimodal retinal images and domain knowledge. UrFound is equipped with a modality-agnostic image encoder and accepts either CFP or OCT images as inputs. To integrate domain knowledge into representation learning, we encode expert annotation in text supervision and propose a knowledge-guided masked modeling strategy for model pre-training. It involves reconstructing randomly masked patches of retinal images while predicting masked text tokens conditioned on the corresponding retinal image. This approach aligns multimodal images and textual expert annotations within a unified latent space, facilitating generalizable and domain-specific representation learning. Experimental results demonstrate that UrFound exhibits strong generalization ability and data efficiency when adapting to various tasks in retinal image analysis. By training on ~180k retinal images, UrFound significantly outperforms the state-of-the-art retinal foundation model trained on up to 1.6 million unlabelled images across 8 public retinal datasets. Our code and data are available at https://github.com/yukkai/UrFound.", "sections": [{"title": "1 Introduction", "content": "Foundation models (FMs) are large, powerful artificial intelligence (AI) models pre-trained on vast amounts of unlabeled data. By learning fundamental pat-terns and relationships within diverse data, FMs gain the ability to adapt to diverse downstream tasks with minimal additional training [52]. Notable exam-ples of FMs, such as CLIP [40], SAM [24], and GPT4 [36], have demonstrated impressive generalization capabilities in various real-world scenarios, including image classification, image segmentation, and natural language processing. Medical FMs are a specialized type of FM designed for the medical do-main [33,34,43,45,51], representing one of the most notable advancements in medical AI. Among these, Medical Vision-Language pre-training stands out as a specific solution that improves medical image analysis by learning domain-specific features from medical images paired with corresponding clinical descrip-tions or reports [49,27,9,4]. Recent medical FMs have focused heavily on radi-ology, particularly chest X-rays [47,48,31]. For retinal FMS, RETFound [52] has been proposed, which is pre-trained on 1.6 million retinal images using Masked Autoencoders (MAE). Another notable example is FLAIR [41], a vision-language model that leverages the CLIP architecture to enhance performance in retinal imaging analysis, supporting zero-shot and few-shot inference through text su-pervision. Unlike task-specific models that may yield suboptimal results in the presence of domain shifts, retinal FMs demonstrate robust generalization ca-pabilities across different retinal datasets and tasks. This presents an attractive solution to enhance model efficacy and reduce the annotation burden on experts, thereby enabling widespread clinical AI applications in retinal imaging. Albeit impressive, existing retinal FMs are restricted to processing a single imaging modality, such as Colour Fundus Photography (CFP) and Optical Co-herence Tomography (OCT). In clinical ophthalmology, diagnosis often involves multiple modalities, including CFP, OCT, and Fundus Fluorescence Angiog-raphy (FFA) images. This requires training separate FMs for each modality, resulting in higher maintenance costs and hindering the acquisition of comple-mentary information across modalities. The question arises: Can a retinal FM be developed to process multiple modalities? Moreover, expert domain knowledge, often in the form of labels or medical reports, is crucial for effective retinal image analysis. It guides models in capturing clinically relevant information, ensuring clinical significance in real-world healthcare scenarios. However, current retinal FMs struggle to fully leverage expert annotations, potentially hindering special-ized representation learning. Another question arises: Can domain knowledge be incorporated into a retinal FM for better generalization ability? To address the research problems mentioned above, we introduce UrFound, a universal retinal FM designed to learn versatile representations from both mul-timodal retinal images and domain knowledge. UrFound employs a modality-agnostic image encoder for processing CFP or OCT images and integrates do-main knowledge from categorical labels and clinical descriptions through text supervision. To achieve this, we convert expert annotations into detailed clini-cal descriptions and propose a knowledge-guided masked modeling strategy for"}, {"title": "2 The UrFound Model", "content": "In this section, we propose UrFound, a retinal FM designed for CFP and OCT images, as the initial step toward developing universal retinal FMs. UrFound is trained with guidance from expert annotations, which can take the form of categorical labels, clinical descriptions, or any other formats that can be encoded in text supervision. UrFound aims to learn domain-specific representations by reconstructing masked patches of a retinal image while predicting masked word tokens of textual domain knowledge conditioned on the unmasked image patches."}, {"title": "2.1 Knowledge-guided Masked Modeling", "content": "Formally, given a retinal image X, it is first reshaped into n patches with the patch size s (e.g., 16 \u00d7 16 in ViT [15]). A random mask \\(M \\in \\{0,1\\}^n\\) is generated with the mask ratio p where 1 indicates a masked patch and 0 indicates an unmasked patch. The masked image \\(X_M\\) is obtained as: \\(X_{M_i} = X_i \\cdot (1 - M_i) + X_0 \\cdot M_i\\), \\(\\forall i \\in \\{1,..., n\\}\\), where \\(X_0\\) represents the image [MASK] token. Let \\(f(\\cdot)\\) be the image encoder that maps each image patch to a latent representation \\(z_i = f(X_i)\\), and \\(g^o(\\cdot)\\) be the image decoder that reconstructs the original image patch \\(X_i\\) from the latent representation. Then the mask imaging modeling (MIM) can be achieved by minimizing the following mean square error (MSE) loss:\n\\[L_{MIM} = \\sum_{i=1}^{n} M_i\\cdot ||X_i - g^o(z_i)||_2, \\tag{1}\\]\nwhich measures the differences between the reconstructed and original image patches. We adopt the high-resolution trick in [51] to let the model reconstruct high-resolution patches at 2\u00d7 the input resolution, which allows the model to learn more detailed local features.\nFor conditional masked language modeling (MLM), the input text is transformed into a sequence of tokens \\(W = [w_1,......,w_L]\\), where L is the sequence length. Then, a certain percentage of tokens in the sequence are randomly replaced with a special [MASK] token, leading to a masked set \\(W_M\\) and an un-masked set \\(W_w\\). Let z be the average pooling of the unmasked image patch representations, and h(.) be the text decoder to restore the masked text tokens."}, {"title": "2.2 Text Preparation", "content": "For retinal images, the majority of publicly available expert annotations come in the form of categorical labels rather than text. To maximize the utilization of domain knowledge for pre-training, we follow FLAIR [41] to enhance categorical image labels by augmenting relevant medical findings sourced from established knowledge bases and clinical literature. For instance, the label \"drusens\" might be described as \"yellow deposits under the retina\" or \"numerous uniform round yellow-white lesions\". Each label may have a varying number of descriptions. During pre-training, we randomly select one of these descriptions for samples in each batch, enhancing the diversity and robustness of the text supervision."}, {"title": "2.3 Multimodal Image Processing", "content": "UrFound directly learns representations for CFP and OCT images using a modality-agnostic encoder. We have observed that this straightforward implementation performs well and achieves superior generalization, particularly when training is guided by domain knowledge through masked modeling. We also explore vari-ants that use separate patch embedding layers, encoders, and decoders for CFP and OCT imaging, respectively, while such modifications do not lead to better results in our experiments."}, {"title": "3 Experiments", "content": "In this section, we assess the performance of UrFound compared to the state-of-the-art retinal FMs, and conduct comprehensive experiments to address the following key questions: Q1. Can the imaging modalities of CFP and OCT be encoded in a universal FM? Q2. Does domain knowledge improve the general-ization ability of FMs? Q3. Do CFP and OCT images contain supplementary information that helps representation learning? Q4. How well do retinal FMs perform in terms of data efficiency? Q5. How effective are retinal FMs in adapt-ing to downstream tasks compared with task-specific models?"}, {"title": "3.1 Experimental Setup", "content": "We assess the capabilities of UrFound in adapting to diagnostic classification tasks with minimal additional training. In line with common practice, we add a linear classifier head on top of the learned image encoder and then fine-tune both the encoder and classifier with task-specific labels. We compare the pro-posed UrFound against the MAE model pre-trained on natural images as well as state-of-the-art retinal FMs including RETFound [52] and FLAIR [41]. For these compared models, we use official checkpoints for fine-tuning. We report the area under the receiver operating curve (ROC) and the area under the precision-recall curve (PRC) as evaluation metrics. And we choose the best checkpoints with the highest ROC scores on the validation set for final evaluation.\nDatasets. For pre-training, we construct a training set by collecting 25 CFP datasets and one large OCT dataset, which include 103,786 CFP images and 83,484 OCT images with expert annotations, covering a wide range of ophthalmic diseases. We follow [41] to augment domain knowledge and transform categorical labels into textual inputs. For the evaluation of fine-tuning performance, we test 8 publicly available datasets across three diagnostic classification tasks including diabetic retinopathy grading (IDRID [39], MESSIDOR [11], APTOS [22]), glau-coma detection (PAPILA [25], GF [2]), and multi-disease diagnosis (JSIEC [8], Retina, OCTID [18]).\nImplementation details. We implement UrFound by using PyTorch on a single NVIDIA A100 GPU. We employ a Vision Transformer (ViT-base) with 12 Transformer blocks and a patch embedding layer as the retinal image encoder. We utilize 8 and 6 Transformer blocks as the image and text decoders, respec-tively. In the pre-training stage, we initialize UrFound with the MAE model and use the tokenizer of BERT-Base [13] to convert clinical descriptions into word tokens. We use a mask ratio of 0.75 for image modeling and 0.5 for language modeling. We resize the input image to 224\u00d7224 both in the pre-training stage and fine-tuning stage. Random horizontal flip and random crop are implemented in the pre-training stage. And random horizontal flip, and color jitter for data augmentation in the fine-tuning stage, each with a probability of 0.5. The total training epoch is set to 200 with a warm-up period of 40 epochs. The learning rate is set to 1.5e-4, and the batch size is set to 128. In the fine-tuning stage,"}, {"title": "3.2 Main Results", "content": "Table 1 shows the classification results of the compared retinal FMs fine-tuned for various retinal disease diagnosis tasks. And our results show that UrFound performs similarly to the second-best method on IDRID and JSIEC, and sig-nificantly better on the other six datasets. It can be observed that retinal FMs such as RETFound-CFP and UrFound achieve significantly better results than MAE in all the cases, which demonstrates the effectiveness of retinal FMs in learning generalizable representations for retinal imaging analysis. UrFound con-sistently outperforms the second best method, RETFound. This superiority can be attributed to the integrated domain knowledge in UrFound through text supervision. In contrast, although FLAIR also leverages domain knowledge, it does not perform well and lags behind MAE in some cases. This is possibly be-cause FLAIR focuses on image-text alignment rather than capturing the visual features of retinal images. It results in a sub-optimal image encoder for image understanding in the pretrain-finetune setting.\nRETFound-CFP and FLAIR are designed specifically for CFP images, ex-hibiting subpar performance when applied to OCT images. Similarly, RETFound-OCT yields the poorest results on CFP datasets. In contrast, UrFound showcases its superiority in processing both CFP and OCT modalities. It achieves this by learning universal and comprehensive representations that span across modali-ties, demonstrating its capability to effectively handle diverse imaging types.\nImpact of multimodal imaging and domain knowledge. To investi-gate how multimodal data and domain knowledge affect the performance of Ur-Found, we compared UrFound against its single-modality variants, either with or without domain knowledge. As shown in Table 2, without text supervision, UrFound trained from CFP+OCT images achieves reasonably good results on both CFP and OCT datasets. This indicates that it is promising to learn uni-versal FMs for multiple retinal imaging modalities (Q1). Furthermore, the in-clusion of text supervision significantly enhances the performance of UrFound,"}, {"title": "4 Conclusion", "content": "We proposed UrFound, a Universal retinal Foundation model, which features a modality-agnostic image encoder and utilizes knowledge-guided mask modeling as a pre-training objective, allowing it to learn generalizable representations from both multimodal images and expert annotations. Through comprehensive exper-iments on 8 public retinal datasets, we demonstrated its strong generalization ability and data efficiency in adapting to various downstream tasks. Neverthe-less, UrFound has two limitations: 1. UrFound is designed to process CFP and OCT images while there exist other retinal imaging modalities such as FFA. 2. UrFound is pre-trained on a relatively small dataset with disease labels as expert annotations. In practice, many unlabeled data are available for pre-training."}, {"title": "1 Supplementary Material", "content": ""}, {"title": "1.1 Dataset Preparation", "content": "Pre-training Dataset. Based on FLAIR [41], we collected a large dataset (Tabel. 1) comprising 187,270 publicly accessible CFP and OCT images for the pre-training of our foundation model and the experiments conducted. Specif-ically, the OCT subset, sourced from the OCTCELL database [23], includes 83,484 images, categorized under four distinct pathological labels: CNV (Choroidal Neovascularization), DME (Diabetic Macular Edema), DRUSEN, and NOR-MAL. Meanwhile, the CFP subset, compiled from 25 public datasets, consists of 103,786 images, extensively covering a wide array of retinal diseases.\nCategories. Here, based on FLAIR [41], we provide the abbreviations for retinal diseases from our pre-training dataset along with their corresponding full names. Asteroid hyalosis (AH), Anterior ischemic optic neuropathy (AION), Age-related macular degeneration (ARMD), Branch retinal artery occlusion (BRAO), Branch retinal vein occlusion (BRVO), Cataract (CAT), Colobomas (CB), Choroidal"}, {"title": "1.2 Expert Knowledge Descriptions.", "content": "For the domain knowledge descriptors related to retinal diseases based on CFP, we referred to FLAIR [41] for guidance. Meanwhile, for the domain knowledge descriptors concerning retinal diseases based on OCT, we utilized ChatGPT-4 to summarize four distinct descriptions for the corresponding disease label names, which were then employed as the domain knowledge descriptors (Tabel. 3)."}]}