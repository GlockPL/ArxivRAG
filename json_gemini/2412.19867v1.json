{"title": "Data-Free Group-Wise Fully Quantized Winograd Convolution via Learnable Scales", "authors": ["Shuokai Pan", "Gerti Tuzi", "Sudarshan Sreeram", "Dibakar Gope"], "abstract": "Despite the revolutionary breakthroughs of large-scale text-to-image diffusion models for complex vision and down-stream tasks, their extremely high computational and stor-age costs limit their usability. Quantization of diffusion models has been explored in recent works to reduce com-pute costs and memory bandwidth usage. To further im-prove inference time, fast convolution algorithms such as Winograd can be used for convolution layers, which ac-count for a significant portion of computations in diffusion models. However, the significant quality loss of fully quan-tized Winograd using existing coarser-grained post-training quantization methods, combined with the complexity and cost of finetuning the Winograd transformation matrices for such large models to recover quality, makes them unsuitable for large-scale foundation models. Motivated by the pres-ence of a large range of values in them, we investigate the impact of finer-grained group-wise quantization in quantiz-ing diffusion models. While group-wise quantization can largely handle the fully quantized Winograd convolution, it struggles to deal with the large distribution imbalance in a sizable portion of the Winograd domain computation. To re-duce range differences in the Winograd domain, we propose finetuning only the scale parameters of the Winograd trans-form matrices without using any domain-specific training data. Because our method does not depend on any training data, the generalization performance of quantized diffusion models is safely guaranteed. For text-to-image generation task, the 8-bit fully-quantized diffusion model with Wino-grad provides near-lossless quality (FID and CLIP scores) in comparison to the full-precision model. This, coupled with the development of highly optimized kernels for group-wise fully quantized Winograd, improves CPU wall-clock time by 31.3% when compared to the convolution layers of a diffusion model. For image classification, our method outperforms the state-of-the-art Winograd PTQ method by 1.62% and 2.56% in top-1 ImageNet accuracy on ResNet-18 and ResNet-34, respectively, with Winograd F(6, 3).", "sections": [{"title": "1. Introduction", "content": "In recent years, foundational pre-trained diffusion models have gained a rapid rise in popularity in the field of im-age generation due to their ability to produce complex and incredibly detailed high-quality photorealistic images from natural language prompts [28, 30, 31, 35]. Furthermore, foundation DMs have successfully demonstrated their flexi-bility in supporting and achieving high-quality performance on a wide range of downstream computer vision tasks, such as image edition, style transformation, image super-resolution, image-to-image translation, and many others, in contrast to previous Generative Adversarial Networks-based image generation models that suffered from unstable training [9]. However, diffusion models typically require many denoising steps and forward passes to convert Gaus-sian noises into real images using neural network layers with over 1 billion parameters. Therefore, deploying these large-scale diffusion models to on-device for inference has been a significant challenge due to their unprecedented size, memory bandwidth, and compute cost requirements [38].\nQuantization has proven to be an effective method for converting high-precision (16 or 32-bit) model weights and activations to lower-precision values, such as 8-bit inte-gers, reducing the model's memory and computational re-quirements while maintaining accuracy, making it more suitable for deployment on devices with limited resources. Among different quantization methods, data-free post-training quantization (PTQ) compresses model parameters after training. While previous studies attempted to quantize weights and activations in diffusion models using coarse-grained PTQ techniques [4, 12, 17, 33, 39, 42, 43, 46], such as tensor-wise or channel-wise quantization, they often re-sulted in tangible loss of quality, especially under low-bit settings. One issue with these coarser-grained quantization approaches is that outlier values, i.e., outliers, can have a disproportionate impact on scaling: the full range of the lower-precision data type is not used effectively, which low-ers the quantized model's accuracy. Finer-grained group-wise quantization instead has shown great promise in suc-"}, {"title": "2. Related work", "content": "Diffusion models. Diffusion models [31] can produce high-quality images using an iterative denoising process. They primarily consist of a text encoder, a denoising neural network, such as UNet, in which the convolution layers can take up a significant portion of the time, and an image de-coder. Although denoising diffusion models have demon-strated phenomenal capabilities in a range of generative tasks, their slow generation speed prevents their widespread deployment. This can be attributed to their lengthy itera-tive denoising process through the UNet and high compu-tational demand of the UNet at each step. While numerous studies have been conducted to accelerate this sampling pro-cess [21, 24, 32, 45] and design fast samplers [22, 35, 36], in this work we investigate model quantization on diffusion models, which is an orthogonal direction to the above meth-ods and can significantly reduce the computational com-plexity of the denoising network at each sampling step, thereby further accelerating the sampling process.\nDiffusion model quantization. Model quantization can be categorized into two types: quantization-aware train-ing (QAT) and post-training quantization (PTQ). QAT of-ten has negligible impacts on model quality after quantiza-tion. However, this requires the original training pipeline and datasets, which can be very challenging to set up, es-pecially for large-scale generative AI foundation models. PTQ, on the other hand, applies model quantization after training and usually only requires a small number of sam-ples for calibration of the quantization parameters. Hence, it is much less time-consuming and computationally inten-sive, generally more favored in network deployment, and is the focus of this work. However, PTQ can lead to sig-nificant model quality degradation if not applied carefully. Many PTQ methods have thus been proposed to tackle this issue, for instance, by minimizing the reconstruction errors of tensors before and after quantization [18, 27]. The uni-form quantization of a floating-point tensor x into b-bit in-teger can be written as,\n$x_{int} = clamp(\\frac{x}{s}, C_{min}, C_{max})$\n$x = s * x_{int}$\n(1)\nPTQ of diffusion models has been studied in a number of previous works [4, 12, 17, 33, 34, 39, 40, 42, 43, 46]. One of the key observations is that the activation distributions of the noise estimation network change greatly over the sam-pling time steps [17]. [43] emphasizes the importance of ac-tivation outliers. Thus, advanced calibration processes have been proposed in these works, but they can be quite com-plicated and are not easily transferable from one architec-ture to another [4]. In this work, we proposed to tackle this problem with the more flexible group-wise quantization, which is inherently more robust to distribution changes be-cause the activation quantization parameters are computed for each group of values and dynamically.\nQuantized Winograd convolution. Winograd algo-rithms [15] are fast convolution algorithms based on mini-mal filtering theory [44], and they are well-known for be-ing the fastest implementation of small convolution ker-nels found in modern neural networks. The Winograd al-gorithm, similar to the FFT, converts tiles of input activa-tions and weight elements into the Winograd domain be-fore performing element-wise multiplications or Hadamard product, which reduces theoretical computation complex-ity, as shown in Figure 2. In general, the larger the tile size, the greater the reduction in computational complex-ity. However, this is not always preferred because larger tile sizes result in greater numerical errors due to the expo-nentially increasing values of the Winograd transformation matrices as tile size increases. This is also the main reason why Winograd algorithms are typically implemented using 32-bit floating-point arithmetic and with relatively small tile sizes, such as 4 x 4.\nPrevious studies [5, 6, 10, 16, 25] have investigated combining Winograd convolution with quantization; how-ever, some [16] only quantized the Hadamard multiplica-tion while doing the input and output transformations in floating-point arithmetic. [10] proposes a flexible Winograd convolution scheme by treating the transformation matrices as learnable parameters. However, this requires a full train-ing pipeline setup, similar to the complexity of QAT. Instead of expensive QAT, [5] performs a two-stage finetuning pro-cess to finetune the Winograd transformation matrices with a small set of training samples. However, it cannot fully restore full-precision model accuracy for larger Winograd F(6,3). Our work nearly bridges the accuracy gap with the full-precision model while fully quantizing Winograd con-volution without requiring any training samples or domain-specific data."}, {"title": "3. Group-wise fully quantized Winograd convolution", "content": "3.1. Winograd convolution\nUsing Winograd, the computation of a 2D convolution out-put tile, y, of size m \u00d7 m with a kernel filter of size r \u00d7 r, as presented in [15] as F(m \u00d7 m, r \u00d7 r), is often given by,\n$y = A^T [[GwG^T] \\odot [B^T x B]] A$\n(2)\nor in more details,\n$W = GwG^T$\n(3)\n$X = B^TxB$\n(4)\n$Y = W \\odot X$\n(5)\n$y = A^T Y A$\n(6)\nwhere w is a r \u00d7 r filter, and x is an (m + r - 1) \u00d7(m + r\n1) input tile. B, G, and A are called Wino-grad transformation matrices, where B and G transform the weights and input feature maps, respectively, from the spa-tial domain to the Winograd domain, and A transforms the output feature maps (Y) back to the spatial domain, after the element-wise multiplication denoted by $\\odot$. This is also often short-handed to F(m,r) when the tile and filter are square matrices, which is used in this work.\nThe Winograd transformation matrices can be con-structed from the Chinese remainder theorem by choosing n=m+r-1 pairs of so-called polynomial points or La-grange interpolation points (fi, gi). The matrices can then be derived from their Vandermonde matrix V, as shown be-low.\n$A^T=V_{nxn}^{-T}S_A V_{nxm}^T$\n(7)\n$B^T = S_BV_{nxn}^{-T}$\n(8)\n$G = S_GV_{nxr}$\n(9)\nwhere SA, SB, and SG are diagonal square matrices sat-isfying the following condition. For more details of deriva-tion, please refer to [41].\n$S_AS_BS_G = I$\n(10)\n3.2. Group-wise quantization and optimized kernels\nNaive application of model quantization to Winograd con-volutions is often a challenging task, as shown in other stud-ies [5, 10]. This is mainly because the quantization errors would exacerbate the numerical errors of Winograd convo-lution, especially for large tile sizes, thus usually leading to significant model accuracy degradations.\nInspired by the recent advances in LLM quantization, we adopt a fine-grained group-wise quantization scheme for both the weights and activations because of its re-duced quantization errors and minimal dependence on the calibration data. Tensors are first divided into hardware vectorization-friendly groups, as illustrated in Figure 1, and each group is then quantized individually to reduce the im-pact of outliers and improve precision. We also apply this for the Winograd input transform (Eq. 4) and output trans-form (Eq. 6), resulting in a fully quantized Winograd con-volution pipeline.\nIn addition, we develop a set of highly optimized matrix multiply kernels for group-wise quantized 8-bit diffusion models to unleash the full potential of modern CPUs. These kernels can fully take advantage of available vector and ma-trix multiply instructions to maximize MAC unit utilization, amortize the cost of loading the operands, minimize over-head and memory accesses, and achieve the best possible"}, {"title": "4. Learnable scales for group-wise qantized Winograd convolution", "content": "4.1. Learnable Winograd transform scales\nBecause of the fine granularity of group-wise quantization, we observe that direct application of it to the Winograd in-put transform and Hadamard product computation gener-ally would not have a significant impact on model quality. However, this is not the case for the output transform. This is mainly because of the huge dynamic range differences across different taps or pixels of the Winograd domain out-put, Y (Eq. 5), as shown in Figure 3(a). In order to utilize efficient integer arithmetic operations, there should be ei-ther a single scale factor for the entire output tile or one for each row or column. However, both would lead to sig-nificant quantization errors due to the \u2018cross'-like dynamic range differences at the 8\u00d78 locations. Although pixel-wise quantization can effectively reduce this quantization error, it precludes the efficient use of integer computation kernels.\nGiven the fact that the output feature map Y in the Winograd domain depends on inputs, original pre-trained weights, and the input transformation (B) and weight trans-formation (G) matrices, the large dynamic range of Y across pixels is primarily attributed to the values of G and W, as well as the variances of values in weights and inputs. In the absence of finetuning original weights in the PTQ setup, the large range differences may be effectively reduced by ma-nipulating the two transformation matrices, B and G, and in turn their norm of rows. From Eq. 8 and Eq. 9, each row of the Vandermonde matrices of BT and G are scaled by the diagonal scaling matrices SB and SG, respectively, which are directly controlling the norms of the row vectors in $B^T$ and G.\nFollowing this intuition, we propose to reduce the quan-tization noise of Winograd output transform by learning the diagonal scaling matrices SB and SG, while $S_A= (S_BS_G)^{-1}$, as given by Eq. 10, and can be easily computed. More formally, if we define the Vandermonde matrices as VB = V, VG = Vnxr and VA = $V_{nxn}^{-T}$, to simplify no-tation, then the Winograd transformations can be rewritten as:\n$W = S_GV_GW V_{n x r}^TS_G$\n(11)\n$X = S_BV_B x V_{nxn} B S_B$\n(12)\n$Y = V_A S_A Y S_A V_A^T$\n(13)\nThen we apply group-wise quantization and integer ma-trix multiplication to all stages of Winograd convolution, specifically:\n$X\\approx S_{qB}\\odot S_{qB}\\odot Q(\\frac{S_B V_B}{S_{qB}})Q(\\frac{V_{n x n}^T}{S_{qB}})Q(X_{in}) = \\hat{X}$\n(14)\n$Y \\approx S_{qW}\\odot S_{qX}\\odot Q(\\frac{W}{S_{qW}})Q(\\frac{\\hat{X}}{S_{qX}}) = \\hat{Y}$\n(15)\n$y \\approx S_{qA}\\odot S_{qA}\\odot S_{qY}\\odot Q(\\frac{V_{n x n}^T}{S_{qA}})Q(\\frac{S_A\\hat{Y}}{S_{qY}})Q(\\frac{S_A}{S_{qA}}V_A^T) = \\hat{y}$\n(16)\nwhere sq* are the group-wise quantization scaling factors for the weights, activations, intermediate results, and Wino-grad transformation matrices and Q is quantization func-tion. We use simple min-max to dynamically quantize all activations during the forward pass."}, {"title": "4.2. Random Gaussian noise for fine-tuning scales", "content": "We then use gradient descent (SGD) to optimize the follow-ing objective and use the learned Winograd scale matrices SG and SB to determine the group-wise quantization scale factors, sq* in Eq. 14-16. For ease of setup, we treat each convolution layers independently. Unlike previous studies that use domain-specific data for QAT or finetuning, we in-stead only use random Gaussian or random uniform noise to learn the Winograd scaling matrices. In previous stud-ies, the quantized model was calibrated and finetuned using a few samples from the training data, which might affect the quantized foundation diffusion models' generalization to unknown cases and downstream tasks. In addition, rather than finetuning scale factors separately for each convolution layer of a diffusion model, we learn a single set of finetuned scale factors for all layers. This further enhances the gener-alizability of our method.\n$S_{G}, S_{B}= arg \\underset{S_{G}, S_{B}}{min} \\sum_{i \\in D} ||y_{i} - \\hat{y}_{i}||$\n(17)"}, {"title": "5. Experiments", "content": "5.1. Experimental settings\nIn this section, we evaluate the proposed learnable Wino-grad scales method on two latent diffusion models, InstaFlow-0.9B [21] and Stable Diffusion [30], for text-to-image generation using the MS-COCO 2017 dataset [19], which contains 5000 images. We use pre-trained check-points with a image resolution of 512 \u00d7 512, and for stable diffusion, we follow the setup in [21] and choose the DPM-Solver++ [23] sampler with 25 time steps and the classifier-free guidance scale of 5.0. To further demonstrate the ap-plicability of the proposed method, we also experiment with ResNets [11] on image classification task with the Imagenet [8] dataset.\nWe apply group-wise quantization to all linear and con-volution layers of all components of the diffusion model pipeline, including the text encoder, UNet, and decoder. We also quantize the attention weights, attention query and key multiplication, and value multiplication, which are not of-ten performed in previous studies [17]. Furthermore, we compare the effects of two autoencoder models, the origi-nal autoencoder in [30], denoted as AKL, and the tiny au-toencoder [3], denoted as TAESD. To evaluate the quality of images generated, we followed the practices of previ-ous works and computed FID [13] and CLIP [29] (ViT-g-14 model) scores using the torchmetrics package.\nIn the following, we first show results of applying only group-wise quantization and then the results of combining it with Winograd convolution using our proposed method."}, {"title": "5.2. Text-to-image generation", "content": "Table 1 and table 2 show the results for the InstaFlow-0.9B model with AKL and TAESD, respectively. It can be seen that with a bit width of W8A8, group-wise quantiza-tion maintains the same level of image generation quality and text-image alignment as the original FP16 model. For TAESD, it even improved upon the baseline FP16 model in both metrics. Quantizing the weights to 4-bit leads to more distortion, but there is no significant degradation of image quality. When applying group-wise quantization to the stan-dard Winograd convolution, FID and CLIP scores degrade drastically. Our method can restore most of the image gen-eration quality for both F(4,3) and F(6, 3) configurations.\nFigure 4 and Figure 5 show some qualitative examples of the challenges of direct application of group-wise quantiza-tion to the Winograd pipeline, especially for TAESD where the images are completely destroyed. Because of this, we do not compute FID and CLIP scores for TAESD under this condition. As shown in Figure 3, this is mainly due to the huge dynamic range differences in the Winograd out-put Y. After applying the learned Winograd scales obtained from Algorithm 1, both images are mostly recovered. Fig-ure 3(c) also shows the effectiveness of our method in dy-namic range equalization in the Winograd output Y."}, {"title": "5.3. Image classification", "content": "The evaluation results from ResNets on the ImageNet are shown in Table 5. We compare it to two methods: BQW [6] and PAW+FSQ [5]. Similar to findings from the diffu-sion models, group-wise quantization of the standard Wino-grad convolution to W8A8 significantly degraded ResNets classification accuracy in both F(4,3) and F(6,3) config-urations. After utilizing the learned Winograd transform scales, we recovered most of the accuracies of the orig-inal full-precision model. Furthermore, our method out-performed PAW+FSQ by 1.62% and 2.56% in top-1 Ima-geNet accuracy on ResNet-18 and ResNet-34, respectively, for Winograd F(6, 3)."}, {"title": "5.4. Runtime improvements", "content": "Finally, we measure the runtime improvement of convo-lution layers as well as the end-to-end runtime of the InstaFlow-0.9B diffusion model on Arm Graviton3 CPUs with varying thread counts (cores). We use the stable-diffusion.cpp [1] framework to collect the inference run-time. As evident from Figure 6(a), the highly optimized ker-nels offer a significant improvement in runtime for group-wise quantized convolution layers. The fully-quantized Winograd convolution provides an additional 31.3% rela-tive improvement over the optimized standard convolution"}, {"title": "6. Conclusion", "content": "The exorbitant computational and storage overhead of large-scale diffusion models limit their practicality for on-device inference. We present a novel, lightweight method that (1) employs group-wise PTQ on both weights and ac-tivations, and (2) fully quantizes fast Winograd convolution by fine-tuning only the scale parameters of the transfor-mation matrices, all without requiring any calibration data. Our method, implemented using highly optimized CPU ker-nels, maintains image generation quality and offers nearly"}, {"title": "7. Winograd transformations for convolution", "content": "7.1. Standard Winograd transforms\nFollowing [41], given a set of polynomial points (fi, gi), the Vandermonde matrix Vaxb is constructed as below,\n$\\begin{pmatrix}\nf_0^{q-1} & g_0^0 & f_0^{b-1} g_0^0\\\\\nf_1^{q-1} & g_1^0 & f_1^{b-1} g_1^0\\\\\n\\vdots & \\vdots & \\vdots\\\\\nf_{a-1}^{q-1} & g_{a-1}^0 & f_{a-1}^{b-1} g_{a-1}^0\n\\end{pmatrix}$\n(18)\nFor standard Winograd convolution, we adopt the widely used polynomial points and scaling factors, as mentioned in [2, 6, 15, 41]. Specifically, for F(4,3) the polynomial points, scaling factors, Vandermonde and transformation matrices are the following,\n(fi, gi) =[(0, 1), (1, 1), (\u22121, 1),\n(2, 1), (-2, 1), (1, 0)]\nSA =[1, 1, 1, 1, 1, 1]\n(19)\n\u0405\u0432 =[4, -6, -6, 24, 24, 1]\nSG =[1/4, 1/6, -1/6, 1/24, 1/24, 1]\n$\\begin{pmatrix}\n1 & 1 & 1 & 1\\\\\\\n1 & -1 & 1 & -1\\\\\\\n1 & 2 & 4 & 8\\\\\\\n1 & -2 & 4 & -8\\\\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}$\n(20)\n$\\begin{pmatrix}\n0 & -5/4 & 0 & 1/4 & 0\\\\\\\n2/3 & 2/3 & -1/6 & -1/6 & 0\\\\\\\n-2/3 & 2/3 & 1/6 & -1/6 & 0\\\\\\\n0 & -1/12 & -1/24 & 1/12 & 1/24 & 0\\\\\\\n0 & 1/12 & -1/24 & -1/12 & 1/24 & 0\\\\\\\n0 & 4 & 0 & -5 & 0 & 1\n\\end{pmatrix}$\n(21)\n$\\begin{pmatrix}\n1 & 1 & 1\\\\\\\n1 & 1 & 1\\\\\\\n1 & -1 & 1\\\\\\\n1 & -1 & 1\\\\\\\n1 & 2 & 4\\\\\\\n1 & -2 & 4\\\\\\\n0 & 0 & 1\n\\end{pmatrix}$\n(22)\n$\\begin{pmatrix}\n1 & 1 & 1 & 1\\\\\\\n0 & 1 & -1 & 2\\\\\\\n0 & 1 & -1 & -2\\\\\\\n0 & 1 & 1 & 8\n\\end{pmatrix}$\n(23)\n$\\begin{pmatrix}\n4 & 0 & -5 & 0 & 1 & 0\\\\\\\n0 & -4 & -4 & 1 & 1 & 0\\\\\\\n0 & 4 & -4 & -1 & 1 & 0\\\\\\\n0 & -2 & -1 & 2 & 1 & 0\\\\\\\n0 & 2 & -1 & -2 & 1 & 0\\\\\\\n0 & 4 & 0 & -5 & 0 & 1\n\\end{pmatrix}$\n(24)\n$\\begin{pmatrix}\n1/4 & 0 & 7\\\\\\\n-1/6 & -1/6 & -1/6\\\\\\\n1/6 & -1/6 & -1/6\\\\\\\n1/24 & 1/12 & 1/6\\\\\\\n1/24 & -1/12 & 1/6\\\\\\\n0 & 0 & 1\n\\end{pmatrix}$\n(25)\nFor F(6,3), the polynomial points, scaling factors, Van-dermonde and transformation matrices are the following,\n(fi, gi) =[(0, 1), (1, 1), (\u22121, 1), (2, 1),\n(-2, 1), (1/2, 1), (-1/2, 1), (1, 0)]\nSA =[1, 1, 1, 1, 1, 1, 1, 1]\n(26)\n\u0405\u0432 =[1, -9/2, -9/2, 90, 90, 45/32, 45/32, 1]\nSG =[1, -2/9, -2/9, 1/90, 1/90, 32/45, 32/45, 1]\n$\\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1\\\\\\\n1 & -1 & 1 & -1 & 1 & -1\\\\\\\n1 & 2 & 4 & 8 & 16 & 32\\\\\\\n1 & -2 & 4 & -8 & 16 & -32\\\\\\\n1 & 1/2 & 1/4 & 1/8 & 1/16 & 1/32\\\\\\\n1 & -1/2 & 1/4 & -1/8 & 1/16 & -1/32\\\\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{pmatrix}$\n(27)\n$\\begin{pmatrix}\n-21/4 & 0 & 21/4 & 0 & -1 & 0\\\\\\\n0 & -2/9 & -2/9 & 17/18 & 17/18 & -2/9 & -2/9 & 0\\\\\\\n0 & 2/9 & -2/9 & -17/18 & -17/18 & 2/9 & -2/9 & 0\\\\\\\n0 & 1/180 & 1/360 & -1/36 & -1/72 & 1/45 & 1/90 & 0\\\\\\\n0 & -1/180 & 1/360 & 1/36 & -1/72 & -1/45 & 1/90 & 0\\\\\\\n0 & 64/45 & 128/45 & -16/9 & -32/9 & 16/45 & 32/45 & 0\\\\\\\n0 & -64/45 & 128/45 & 16/9 & -32/9 & -16/45 & 32/45 & 0\\\\\\\n0 & -1/4 & 0 & 21/4 & 0 & -21/4 & 0 & 1\n\\end{pmatrix}$\n(28)"}, {"title": "7.2. Learned Winograd scales", "content": "Table 6 shows the difference in values between the stan-dard Winograd scales and our learned Winograd scales for F(6,3). It is worth noting that the magnitudes of SA have become smaller while those of SG are bigger, and SB stays relatively unchanged."}, {"title": "8. Comparison with learning transformation matrices instead of Winograd scales", "content": "[10] proposed to treat the Winograd transformation matrices A, B, and G as learnable parameters and jointly optimize them with other model weights and biases in a QAT setup. Although this is much less practical in the domain of Gener-ative AI, as mentioned above, we still adopt this paradigm to compare with our method. All training setups are the same except that transformation matrices A, B, and G are learned directly using random noise inputs instead of learning only the scaling factors SA, SB, and SG. The results are shown in Table 7 using the InstaFlow-0.9B model with AKL au-toencoder. It can be seen that learning the transformation matrices directly offers almost no improvement compared to the standard Winograd transforms. This could be due to the use of random noise as layer inputs and treating each layer independently."}, {"title": "9. Comparison of group-wise quantization against other quantization methods", "content": "Table 8 and Table 9 show the comparison between our group-wise quantization method against a popular, recently proposed quantization scheme, called Q-Diffusion[17], for Stable Diffusion V1.4[30] with DPMSolver++[23] and PLMS[20] sampler, respectively. All models were sampled for 25 steps, and MSCOCO 2017 was used to generate FID and CLIP scores. We conducted the experiments with Q-Diffusion using the official codebase and pre-calibrated quantized checkpoints released by the authors."}, {"title": "11. Comparison to prior QAT works for fully quantizing Winograd", "content": "In comparison to previous QAT studies [10] that achieve full quantization by learning Winograd transformation matrices, we can achieve comparable results by fine-tuning only the Winograd scales. As shown in Table 10, 8-bit ResNet18 with Winograd achieves an accuracy of 93% for F(4,3) (an accuracy drop of 0.07% in comparison to the full-precision model) using our learned scales, whereas Winograd-aware QAT [10] observes an accuracy drop of 0.7% for the similar setting."}]}