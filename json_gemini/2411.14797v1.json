{"title": "Continual SFT Matches Multimodal RLHF with Negative Supervision", "authors": ["Ke Zhu", "Yu Wang", "Yanpeng Sun", "Qiang Chen", "Jiangjiang Liu", "Gang Zhang", "Jingdong Wang"], "abstract": "Multimodal RLHF usually happens after supervised fine-tuning (SFT) stage to continually improve vision-language models' (VLMs) comprehension. Conventional wisdom holds its superiority over continual SFT during this preference alignment stage. In this paper, we observe that the inherent value of multimodal RLHF lies in its negative supervision, the logit of the rejected responses. We thus propose a novel negative supervised finetuning (nSFT) approach that fully excavates these information resided. Our nSFT disentangles this negative supervision in RLHF paradigm, and continually aligns VLMs with a simple SFT loss. This is more memory efficient than multimodal RLHF where 2 (e.g., DPO) or 4 (e.g., PPO) large VLMs are strictly required. The effectiveness of nSFT is rigorously proved by comparing it with various multimodal RLHF approaches, across different dataset sources, base VLMs and evaluation metrics. Besides, fruitful of ablations are provided to support our hypothesis. We hope this paper will stimulate further research to properly align large vision language models.", "sections": [{"title": "1. Introduction", "content": "Large vision-language models (VLMs) emerged [16, 43] thanks to the intelligence of large language models (LLMs). Typically, such models [4, 9, 14] usually experience a pre-traning stage with diverse image-text pairs before supervised finetuned (SFT) in a multitask fashion [14, 25, 41].\nThe successful application of reinforcement learning from human feedback (RLHF, also called preference alignment) in LLM [23, 26] has shed light on vision-language areas [10, 22, 29, 46], which aims to further leverage VLMs' multimodal comprehension ability after the standard SFT stage and to reduce potential image hallucinations.\nA widespread belief in multimodal RLHF is they all assume the inferiority of SFT during this preference alignment process [3, 22, 46]. They evidently shows that continual SFT (e.g., applying SFT in preference alignment) falls short in addressing image-level hallucination [46] and are becoming less effective in data utilization [3, 22]. These opinions holds true, as also demonstrated in our quantitative results in Fig. 2c (cf. 'Cont. SFT' vs 'DPO'). However, we found practically that multimodal RLHF is not as perfect as it does, since it will face GPU memory shortage [46] and usually confronts unstable training issue [29].\nIn this paper, we aim to address the following question: is multimodal RLHF indeed superior than continual SFT during preference alignment? Through theoretical analysis in gradient and optimization aspects, we find key success of multimodal RLHF (e.g., DPO [23]) mostly attributes to the negative supervision in the rejected responses, whereas naive continual SFT fails to capture.\nBased on this discovery, this paper proposes a novel negative supervied finetuning (nSFT) pipeline that continually aligns VLMs with a simple SFT loss. Specifically, an LLM (e.g., GPT-4) is adopted to identify the misaligned details in the negative supervision, by referring to a large vision error codebook that contains both instance- and image-level error. The LLM then constructs a new conversation talking about the image, helping the model to realize these self mistakes from these rejected responses (cf. Fig. 3).\nOur proposed approach disentangles the negative supervision from RLHF optimization. This makes our training more efficient in both memory and training time, as multimodal RLHF usually requires 2 or 4 models during alignment stage, as shown in Fig. 2b.\nTo verify whether nSFT matches multimodal RLHF with this constructed negative supervision, we carefully select different preference alignment data, covering different image types and responses length. Then, a controlled experiment is done with nSFT, SFT and 3 multimodal RLHF (GT-DPO, SeVa [46], SIMA [31]) methods. Experiment in Table 1 shows that nSFT not only remedy pure SFT, but also achieves the overall best result across evaluation metrics. Our hypothesis also holds for more powerful VLMs like LLaVA-NeXT [15], and variants of multimodal RLHF approaches (PPO [26] and iterative DPO [42]), too.\nFinally, our ablations shows that nSFT can be further improved with a similar KL constraint adopted in RLHF [35], and captures more fine-grained detail in comparison. Our study reveals that multimodal RLHF is still more effective in addressing worst cases in comparison (e.g., reduce the frequency of the worst response sentence or tokens).\nOverall, our contributions are as follows:\n\u2022 We analyze key factor (negative supervision) that makes multimodal RLHF successful, and propose a novel nSFT method that fully excavate this negative supervision.\n\u2022 Our nSFT strictly matches multimodal preference alignment methods (both DPO and PPO) under different training database, scale and numerous evaluation metrics.\n\u2022 We provide fruitful ablations to verify the generalization ability of nSFT as well as common RLHF approaches, stimulating future research to properly align large VLMs."}, {"title": "2. Related Work", "content": "Supervised finetuning (SFT). The notion of instruction tuning (also called supervised finetuning) initially derives from natural language processing (NLP) domain [5, 25], with the aim to fully unlock the models response intelligence [41] with diverse formatted prompts [33]. In vision-language area, the SFT paradigm is similar, given that the VLMs are sufficiently pretrained with abundant image-text pairs [2, 6, 39]. Such technique basically aims to conduct image-based multi-task training [32] that these models can behave well in diverse zero-shot benchmarks [28, 43]. For examples, VLMs will demonstrate strong multimodal comprehension skills after this stage, such as multi-turn conversation, math and image reasoning [14, 30]. A distinct feature of SFT is that only positive targeted are settled, without letting the model know its wrong answers. In this paper, we found that this property of SFT might hinder model from further improvement.\nMultimodal preference alignment. Multimodal preference alignment [11, 36, 40] (also called RLHF) happens after SFT stage to continual align VLMs with user intentions and to reduce hallucinations. Direct preference optimization [18, 19, 23] (DPO), one representative in this family, becomes the most popular due to its flexibility and efficiency. [3, 22] adopt groudtruth (GT) as chosen response and self-response as negative, which iteratively improve the model in a GT-DPO style. More recent work, such as SeVa [46], adopts noised model output as rejected samples to conduct preference optimization. Effective but, we have observed a shared point in these works [3, 22, 46]: they all assume the inferiority of continual SFT during preference alignment stage. In this paper, we discover the root cause of such phenomenon: negative supervision. We advocate that continual SFT could match the performance of multimodal RLHF with this supervision signal fully integrated."}, {"title": "3. Method", "content": "We will first introduce background of multimodal SFT and RLHF, then discuss their loss function relations. Based on this, we propose a negative supervised finetuning (nSFT) approach that continually improve VLMs' capability."}, {"title": "3.1. Preliminaries", "content": "SFT. Most VLMs are trained with a next token prediction loss (e.g., SFT loss). Specifically, for a given image input I, it went through an H (combination of vision encoder and connector [44]) to get the latent embeddings v: v = \u0397(\u0399), which are concatenated with the question embeddings q: x = (v, q), and are fed into a large language model \\(\\pi_{\\theta}(\\cdot)\\) that sequentially produces the next token:\n\\(\\pi_{\\theta}(Y|x) = \\prod_{i=1}^{L} \\pi_{\\theta}(Y_i|Y_{<i}, x) \\).\nHere L denotes the response token length. During SFT, a cross entropy loss is applied to Eq. 1, as follows:\n\\(L_{sft}(y) = -\\sum_{i=1}^{L}\\log \\pi_{\\theta}(Y_i|Y_{<i}, X)\\).\nRLHF and DPO. RLHF aims to further align LLMs or VLMs with user specific intentions. In RLHF, a reward model \\(r_{\\xi}(\\cdot)\\) is parameterized by \\(\\xi\\) and usually obtained by optimizing a Bradley-Terry (BT) model [23]:\n\\(p^*(y_c y_r x) = \\frac{\\exp \\left(r^* (x, y_c)\\right)}{\\exp \\left(r^*(x, y_c)\\right) + \\exp \\left(r^*(x, y_r)\\right)}\\).\nIn Eq. 3, \\(r^*(\\cdot)\\) is the optimal reward model, and \\(y_c y_r\\) represent the chosen and rejected response, respectively. \\(r_{\\xi}(\\cdot)\\) are usually obtained through MLE [23] using preference dataset D containing set of (x, y, yr). After that, we would obtain the policy model \\(\\pi_{\\theta'}\\) by maximizing:\n\\(\\max_{\\theta'} E_{x,y} \\{r_{\\xi}(x, y) \u2013 \\beta D_{KL}[\\pi_{\\theta'}(Y|X)|\\pi_{ref}(y|x)]\\}\\),\nwhere \\(\\pi_{\\theta'}\\), \\(\\pi_{ref}\\) represents the policy model and reference model, respectively. \\(D_{KL}\\) and \\(\\beta\\) means the KL divergence and the hyper-parameter. DPO simplifies the optimization process of RLHF by resorting to the closed form of Eq. 4:\n\\(r^*(x, y) = \\beta \\log \\frac{\\pi^*(y x)}{\\pi_{ref}(y x)} +C\\),\nin which the optimal reward model r* is a function of the optimal policy model \\(\\pi^*\\) (C is a constant). Thus, we can directly optimize the BT model in Eq. 3 by substituting Eq. 5 into it, and get the final DPO loss function:\n\\(L_d = -E_{D} \\left[\\log\\sigma\\left(\\beta \\log\\frac{\\pi_{\\theta'}(Y_c x)}{\\pi_{ref}(Y_c x)} - \\beta \\log\\frac{\\pi_{\\theta'}(Y_r x)}{\\pi_{ref}(Y_r x)}\\right)\\right]\\).\nIn Eq. 6, \\(\\sigma(\\cdot)\\) denotes the sigmoid function that normalize the DPO logtis, and the whole optimization process is conducted by sampling preference tuple (x, yc, yr) from D."}, {"title": "3.2. Negative supervision matters", "content": "We will take DPO for illustrations, given its wide popularity in VLMs [22, 31, 46]. Note we also quantitatively verify PPO in Table 5 to make our hypothesis more general.\nRelations between DPO and SFT. Following many previous multimodal RLHF methods [3, 22], we take GT-DPO to illustrate their relations (e.g., yc, yr are GT annotations and model self response, respectively). From Eq. 6, we can define the 'logit' in DPO loss function as:\n\\(\\rho_{dpo} = \\log\\frac{\\pi_{\\theta'}(Y_c x)}{\\pi_{ref}(Y_c x)} - \\log\\frac{\\pi_{\\theta'}(Y_r x)}{\\pi_{ref}(Y_r x)}\\)\nIf we omit the reference constraint in Eq. 7 and sequentially stretch the productive function of \\(\\pi\\), we will obtain:\n\\(\\rho_{dpo} = \\sum \\left[\\log \\pi_{\\theta'}(Y_{c,i}|Y_{c,<i}, x) \u2013 \\log \\pi_{\\theta'}(Y_{r,i}|Y_{r,<i}, X)\\right] = -(L_{sft}(y_c) \u2013 L_{sft}(y_r))\\)\nThat is, the core component of DPO is actually the subtraction of two SFT loss: the chosen and reject sequences. With more strict derivation (cf. appendix), we show DPO's gradient is a linear combination of two SFT gradient:\n\\(\\frac{\\partial L_d}{\\partial \\theta'} = \\frac{1}{\\partial \\rho_{dpo}} \\left[ \\frac{\\partial L_{sft} (c)}{\\partial \\theta'} - \\frac{\\partial L_{sft}(yr)}{\\partial \\theta'} \\right]\\)\nand the only missing information comes from the absence of rejected SFT loss \\(L_{sft}(y_r)\\). We thus conclude: the inferior performance mainly derive from the lack of negative supervision resided in the rejected responses!\nThe role of negative supervision. We further investigate the importance of negative supervision. Note that a con-current work in NLP discover a similar trend [7], but not aiming at relating to SFT. Here we first simplify Eq. 6:\n\\(t_1 = \\frac{\\pi_{\\theta'}(Y_c x)}{\\pi_{ref}(Y_c x)}, t_2 = \\frac{\\pi_{\\theta'}(Y_r x)}{\\pi_{ref}(y_rx)}\\),\nand reformulate the DPO loss function as follows:\n\\(L_d = -\\log(\\frac{t_1}{t_1 + t_2})\\)\nwhere we safely ignore the expectation term \\(E_D\\) for better clarification. Then the gradient with regard to t1, t2 is:\n\\(\\frac{d \\mathcal{L}}{dt_1} = \\frac{-\\beta t_1}{t_1(t_1+t_2)}, \\frac{d \\mathcal{L}}{dt_2} = \\frac{\\beta t_2}{t_1(t_1+t_2)}\\)\nAnd we can obtain the update rate ratio as:\n\\(\\frac{Its}{Itz} = t_2/t_1\\)."}, {"title": "3.3. Our nSFT approach", "content": "Disentangle negative supervision. Since the negative supervision is deeply entangled in DPO \u2018logit' and pairwise relations exists, it is hard to obtain those negative supervision by directly optimizing in an SFT style as Eq. 2. As a result, we involve a construction function \\(G(\\cdot)\\) (an LLM like GPT-4), and define our loss as (note that y equals the GT captions, as we are discussing in GT-DPO phase):\n\\(L_{nSFT} = L_{sft}(Y_c) + L_{sft}(G(y_r))\\).\nThe aim of G() is identify and re-organize the false information embedded in the rejected responses yr, where the model can learn from in an SFT manner.\nVision error codebook. As the rejected responses contain abundant (possibly hallucinatory) information of the image, the construction function G(\u00b7) should cover as much image related error as possible. Thus, we introduce a large vision error codebook Q, containing all possible recognition error types, during identification.\nSpecifically, an LLM is first obliged to identify both image-level and instance-level error in the rejected sentences, by referring to GT information y and the table Q. The LLM is then asked to formulate a new conversation talking about this image, to reinforce the mistakes that the model already made. That is, yc, yr and Q are all prompts that are fed into the LLM G(\u00b7) (cf. appendix). Putting it all together, we formulate our final loss of nSFT:\n\\(L_{nSFT} = L_{sft}(Y_c) + L_{sft}(G(y_r; y_c, Q))\\).\nThe merit of our nSFT is in two fold. On one hand, we break the pairwise relation of chosen and response Yc, Yr, as strictly required during RLHF [31, 46] optimization. On the other hand, the whole preference alignment is optimized with an SFT loss, much more memory efficient than multimodal RLHF where at least 2 models are indispensible.\nNote there is no KL constraint in Eq. 15 as adopted in RLHF. In ablations, we show that adding a per-token constraint to nSFT will further improve the results."}, {"title": "4. Experiments", "content": "We will first introduce the specific negative construction pipeline before showing the general comparisons. Finally,"}, {"title": "4.1. Constructing nSFT", "content": "Due to a huge discrepancy of mutlimodal RLHF literature [22, 46] in dataset choices, we select three alignment dataset from various source: OCRVQA [21], TextCaps [27] and LLaVA-150k [14], covering object centric and scene images, short, medium and long responses.\nOCRVQA. This dataset are set of object centric images containing books' title pages. During experiment, we choose 10k such data and generate model's response to construct the rejected answers, which are then directly compared (w/o an LLM) with the GT output to find out mistakes. For each mistake, we construct doubled question-answer pairs. Below shows a constructed sample where the models misclassify a travel book as a recipe book:\nQ1: Is this a travel book? A1: Yes\nQ2: Is this a recipe book? A2: No\nThese newly constructed question-answer pair are appended to the end of original GT conversations.\nTextCaps & LLaVA-150k These databases contain image captions of medium token length (around 10 tokens per caption in TextCaps) and long captions (around 100 token in LLaVA-150k). In implementation, we choose 10k data each to generate response and compare it with groundtruth using LLM (e.g., GPT-4). The LLM will identify any hallucination in the model original output by referring to the vision error codebook and GT captions, and correspondingly construct rectified conversation. If no error exists, the LLM simply construct a conversation based on GT information. The constructed data are concatenated with original GT to form the total question-answer pairs for each image."}, {"title": "4.2. Experimental settings", "content": "Training. In our main Table 1, we choose LLaVA-1.5-7B as baseline and reproduce 3 different multimodal RLHF methods: GT-DPO [31], SeVa [46], SIMA [31], 2 SFT methods: pure continual SFT and our nSFT. We strictly reproduce SIMA and SeVa following its official implementation, and conduct GT-DPO with the same experiment setting as SeVa [46]. The architecture of continual SFT and nSFT all follow LLaVA-1.5 [14]. Specifically, during nSFT, we adopt deepspeed framework and ZeRO-3 [14] optimization. The batch size, learning rate and weight decay are set as 128, 2e-6 and 0, respectively, following a cosine scheduler. When conducting state-of-the-art comparison Table 2, we use a mixture of 5k OCRVQA, 5k TextCaps and 5k LLaVA-150k, and reproduce other multimodal RLHF approaches with their original data (e.g., SIMA uses a 17k training data). We also evaluate nSFT on larger VLMs like LLaVA-1.5-13B and LLaVA-NeXT-13B [15] in Table 4.\nEvaluation. We mainly choose 9 benchmarks for evaluation, and categorize them into 3 main clusters, containing traditional VQA: SQA [20], GQA [8], TextVQA [27] (e.g. VQAT), multimodal comprehension: MMVet [37], MME [46], MMB [17], and multimodal hallucination: POPE [12], CHAIR [31], MMHal [31]. Please refer to appendix for more discussion abount MMHal."}, {"title": "4.3. General comparisons", "content": "We first compare nSFT with other RLHF methods. As shown in Table 1, all preference alignment methods lead to a general positive effect. However, pure continual SFT is shown to hurt original model's capability, or make the improvement trivial. When the negative supervision is integrated (our nSFT), the performance have seen a great boost, surpassing all preference alignment methods across benchmarks. Interestingly, a much more significant increase is observed in hallucination benchmarks in nSFT (e.g., a 15.1 increase using LLaVA-150k data). Since we involve the vision error codebook (cf. Table 6), the model would be more sensitive to image-related details afterward (cf. Fig. 5). DPO paradigm is also effective, but relatively coarse grained, as shown in later visualizations in Fig. 5.\nWe also visualize these results in Fig. 4. In Fig. 4a-4c, the alignment data is a 10k subset of LLaVA-150k, and 'DPO' is the average score of SeVa, SIMA and GT-DPO. In Fig. 4d, we list all continual learning method, and average their 9 benchmark results (cf. Table 1). As shown in the fig-"}, {"title": "4.4. Comparing with state-of-the-art", "content": "We move onto compare our nSFT with current VLMs. In Table 2, we use a mixture of 15k dataset for nSFT, comprised of 5k data for each specific data source (OCRVQA, TextCaps and LLaVA-150k). To improve the stability for nSFT, we merge this 15k mixed data into LLaVA 665k, which are jointly trained during LLaVA-1.5 SFT stage. This could be viewed as a special case of continual learning to avoid model distribution shift [1]. As shown in the table, all continual alignment/learning methods leverage the baseline LLaVA-1.5-7B results, showing that there is still room for improvement even after the standard SFT stage. Our nSFT consistently surpasses other alignment methods for 9 out of the 10 benchmarks, especially on MMB. SeVa obtains the best results in MM-Vet, possibly due to its enlarged response length [46] that is favored by GPT-4 evaluation [37]."}, {"title": "4.5. Ablations", "content": "Last, we conduct a series of ablations with step-by-step controllable analysis, to reveal the relationships between SFT & RLHF and verify the generalization ability of our nSFT.\nDifference between SFT and RLHF. We take DPO for illustration since all recent publicity [31, 46] adopt it for its efficacy. First, an in-domain evaluation was conducted, where the LLaVA-1.5 are trained using a 10k subset of OCRVQA, TextCaps and LLaVA-150k, which are then evaluated on its own held-out data source. Not surprising in Table 3, all continual learning methods improve the 'IF' and 'Accuracy' metrics. Besides, the best cases of our nSFT is generally higher than DPO methods, while ACC1 leans towards DPO paradigm. Together with the previous observation in Sec. 3, we deduce that DPO mostly try to reject those worst cases, and that SFT hinges on leverage the model's best results. Our conjecture was further verified with visualizations in Fig. 6, where a same phenomenon can be observed in the whole score distribution.\nWe also visualize the logit in Fig. 5 (in the lower right). Results show that DPO helps correct the errors by decreasing the logit of bad predictions (e.g., the logit of color 'White'), while nSFT enhance the model by lifting those 'good' logits. By referring to left of Fig. 5, we found that our nSFT focus more on fine-grained details (such as counting, object, etc.), thanks to a large vision error codebook during the negative data construction. In contrast, DPO are optimized in a accumulated logit manner, and can fail to recognize such level correctness (also pointed out in [7])."}, {"title": "5. Conclusion", "content": "In this paper, we advocate the key success of multimodal RLHF lies in its negative supervision, and consequently propose a novel nSFT approach to fully matches RLHF."}, {"title": "A. Theoretical derivation", "content": "Our hypothesis is strictly verified through theoretical observation and quantitative experiments, as well as fruitful of ablation studies. The nSFT continually aligns VLMs with only one model, making alignment more efficient, too.\nAs for the limitations, it remains unclear how the proposed nSFT approach suits other LLM area. For example, in natural language processing (NLP) area, the goal of RLHF is to reduce toxic, offensive token output, or to transfer the output generated sentence style, while the goal of multimodal RLHF usually hinges on eliminating the instance- or image-level hallucination. In the future, we might explore whether a similar negative construction style could adapt to RLHF methods in NLP domains."}, {"title": "A.1. Relations between DPO and SFT.", "content": "In this section, we want to analyze the relations between DPO and SFT, from the gradient perspective. We first define the logit of DPO loss function with and without the reference model as \\(p_{dpo}\\) and \\(p'_{dpo}\\), respectively:\n\\(p_{dpo} = \\log \\frac{\\pi_{\\theta'}(Y_c x)}{\\pi_{ref}(Y_c x)} - \\log \\frac{\\pi_{\\theta'}(Y_r x)}{\\pi_{ref}(Y_r x)}\\)\n\\(p'_{apo} = \\log \\pi_{\\theta'}(Y_c x) \u2013 \\log \\pi_{\\theta'}(Y_r x) = -(L_{sft}(y_c) \u2013 L_{sft}(y_r))\\)\nThen the standard DPO loss function changes to:\n\\(L_d = - \\log \\sigma(\\beta p_{dpo})\\).\nIf we take the partial derivation of the DPO loss function to the LLM parameter \u03b8', we will obtain DPO gradient as:\n\\(\\frac{\\partial L_d}{\\partial \\theta'} = \\frac{1}{\\beta p_{dpo}} \\frac{\\partial (\\beta p_{dpo})}{\\partial \\theta'}\\)\n\\(\\frac{\\partial (\\beta p_{dpo})}{\\partial \\theta'} = \\frac{1}{p_{dpo}} \\frac{\\partial p_{dpo}}{\\partial \\theta'}\\)\n\\(\\frac{\\partial p_{dpo}}{\\partial \\theta'} = \\frac{L_{sft}(Y_c)}{\\partial \\theta'} -  \\frac{L_{sft}(y_r)}{\\partial \\theta'}\\)\nNote that during derivation, \\(\\frac{\\partial p'_{apo}}{\\partial \\theta'}=\\frac{\\partial p_{apo}}{\\partial \\theta'}\\) since the reference model do not receive gradient. In the samewhile, the gradient of common SFT loss to the LLM parameter @ is represented as (we denote the parameter of SFT during continual learning as 0'):\n\\(\\frac{\\partial L_{sft} (y)}{\\partial \\theta'}\\)"}, {"title": "A.2. Gradient analysis", "content": "That is, the DPO gradient is just a linear combination of two SFT gradient (positive response y and negative response y), respectively, with just a dynamic scaling factor. This makes their optimization process similar, and can explain the inferior performance brought by the lack of negative supervision in SFT loss.\nIn this subsection, we analyze the gradient direction of DPO loss function towards the chosen response and reject responses, respectively. From Sec. 3, we know that:\n\\(t_1 = \\frac{\\pi_{\\theta'}(Y_c x)}{\\pi_{ref}(Y_c x)}, t_2 = \\frac{\\pi_{\\theta'}(Y_r x)}{\\pi_{ref}(y_rx)}\\)\n\\(\\frac{It_1}{dt_1} = t_2/t_1\\)\nDuring the optimization process, t\u2081 tends to increase and t2 tends to decrease (in order to optimize the final DPO loss), which make the division factor t2/t1 less than 1 [7]. As a result, the gradient will be biased towards t2, the negative supervision in DPO loss function.\nHere we want to clarify that this conclusion also trivially holds in our derivation where the reference term is omitted (e.g., the DPO loss function changes to L\u2081). In such cases, we can simply let:\n\\(t'_1 = \\pi_{\\theta'}(Y_c x), t'_2 = \\pi_{\\theta'}(Y_r x),\\)\nand derive the same conclusion where the loss function L'a will be biased towards optimizing t'2. This will exactly match the basic form of our derivation in Sec. A.1 (how DPO loss is related to SFT loss without a reference term)."}, {"title": "B. Experiment details", "content": "B.1. How to construct nSFT?\nWe now describe the constructed nSFT data in detail. Note that in OCRVQA, the newly constructed conversation length is 2 (manually constructed). In TextCaps and LLaVA-150k, the new constructed conversation length is 5 (GPT-4 is adopted).\nOCRVQA. These dataset are set of book title pages (usually be viewed as object-centric images). As described in our experiment sections. We construct doubled Q-A (question-answer) pairs for each mistake that the model made, and appended these constructed pairs into the tail of the original GT conversation. During our implementation, we found that without the original GT conversation (only the negative constructed pairs are used for training), the training process can be unstable and the performance is unsatisfactory. However, this phenomenon does not apply to TextCaps and LLaVA-150k dataset, where the role of GT"}]}