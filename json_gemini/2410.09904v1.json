{"title": "Equitable Access to Justice: Logical LLMs Show Promise", "authors": ["Manuj Kant", "Marzieh Nabi", "Manav Kant", "Preston Carlson", "Megan Ma"], "abstract": "The costs and complexity of the American judicial system limit access to legal solutions for many Americans. Large language models (LLMs) hold great potential to improve access to justice. However, a major challenge in applying AI and LLMs in legal contexts, where consistency and reliability are crucial, is the need for System 2 reasoning. In this paper, we explore the integration of LLMs with logic programming to enhance their ability to reason, bringing their strategic capabilities closer to that of a skilled lawyer. Our objective is to translate laws and contracts into logic programs that can be applied to specific legal cases, with a focus on insurance contracts. We demonstrate that while GPT-40 fails to encode a simple health insurance contract into logical code, the recently released OpenAI o1-preview model succeeds, exemplifying how LLMs with advanced System 2 reasoning capabilities can expand access to justice.", "sections": [{"title": "1 Introduction", "content": "Access to legal solutions has become increasingly limited across the low, middle, and upper-middle classes, with all facing significant barriers. More than 75% of litigants represent themselves [1], with California alone reporting over 4.3 million self-represented litigants [2]. This trend is largely attributed to the high cost of legal services and widespread distrust of attorneys, as indicated by surveys conducted by the California judicial system [3]. Addressing these challenges requires the development of reliable and transparent technological solutions to bridge the considerable gaps in the legal system for consumers.\nRecently, legal applications have garnered significant attention as a promising use case for LLMs. Several scientific studies and business initiatives have highlighted both the potential and limitations of LLMs in the legal domain [4]. Considerable progress is still required before these technologies can deliver consistent and transparent solutions. While human lawyers can articulate the reasoning behind their decisions and strategies, LLMs currently lack this capability to a sufficient degree. [5, 6, 7, 8, 9, 10].\nBefore discussing which AI solutions might be appropriate for legal applications, it is crucial to first consider whether law is inherently deterministic. While laws provide a structured framework that can seem deterministic, the human element, interpretation, judgment, and discretion introduce a degree of uncertainty, making the law not entirely deterministic in practice. In summary, the deterministic aspects of law includes a) legal rules and statutes; and b) case precedents. The non-deterministic aspects are a) judicial interpretation; b) human judgment; and c) equity and fairness.\nGiven the multifaceted nature of legal practices, we propose that a combination of probabilistic and deterministic AI solutions is required to effectively address legal planning and reasoning. This raises the next logical question: which AI algorithms and relational frameworks are most suitable for developing reliable legal assistance? In the following sections of this article, we outline our current"}, {"title": "2 Our Current Approach", "content": "LLMs are great probabilistic solutions with rapid improvements in their capabilities. However, given their inherent probabilistic nature, there is always a chance of hallucination and inconsistent answers. On the other hand, we have logic programs with highly consistent responses and explainable answers. But their main limitation lies in their inherent lack of flexibility and scalability for handling certain complex tasks, especially in real-world applications as they struggle to model uncertainty, probabilistic reasoning, or temporal dynamics.\nTo leverage the strengths of both LLMs and logic programming, we explore various hybrid approaches that combine these two methodologies. In one such approach, LLMs are employed to automatically generate logical representations of legal statutes or rules. Once these representations are constructed, the specific details of a given case can be applied to this logic-based framework. This allows for a structured reasoning process, where the law's application to individual cases is derived through formal logic, thereby enhancing the interpretability and precision of legal decision-making.\nThe integration of LLMs with logic programming (neuro-symbolic AI) is becoming increasingly popular. AlphaGeometry [11] is a great example of the new horizons achievable by leveraging the strengths of each method. In [12], the authors proposed a neuro-symbolic approach, leveraging LLMs to generate logical representations of problems, with Prolog handling the deductive reasoning."}, {"title": "2.1 Limitations of Our Current Approach", "content": "In our current approach, we leverage LLMs to generate logical representations from legal texts. LLMs offer a significant advantage in developing these representations at scale, enabling the efficient processing of vast and complex legal corpora. However, the accuracy and quality of the logic produced by LLMs remain a critical concern, as these models can misinterpret legal terms, omit critical details, generate logical inconsistencies, or overgeneralize legal principles. Additionally, LLMs may struggle with nuances, ambiguities, and the conditional or temporal relationships inherent in legal texts, leading to potential errors. Moreover, potential biases in their original training data can further compromise the validity of the generated logic. Therefore, it is essential to implement robust mechanisms to prevent these types of errors and mitigate the potential negative impact of LLMs. Ensuring the integrity of the generated logic is crucial for maintaining the reliability and trustworthiness of our proposed approach.\nEncouragingly, however, we have found that the quality of the logical representations generated by LLMs is significantly improving as these models become more powerful and sophisticated. In the Experiment section, we demonstrate the quality differences between two of the most recent OpenAI models, GPT-40 and OpenAI 01-preview, specifically in generating Prolog representations of certain legal contracts. Another important mechanism to ensure the accuracy of these logical representations is incorporating human feedback. To achieve this, we propose having expert attorneys, familiar with the specific legal domains, review the generated logic to validate and further enhance its quality."}, {"title": "3 Experiment - Hospital Cash Benefit Policy", "content": "In our experiments, we focus on legal contracts, particularly the challenges consumers face in under- standing health insurance coverage. A June 2024 Stanford survey revealed that 83% of participants used traditional methods to check their insurance policy, with 82% finding the process frustrat- ing. Computational law experts highlight the importance of \"computable contracts\" [14] to reduce confusion and help identify coverage gaps.\nComputable Contracts: Ideally, insurance contracts would be represented as interpretable computer programs (computable contracts), which could be easily audited by regulatory bodies and legal/domain experts. These programs would allow consumers to check their coverage through a simple command rather than poring over complex documents. Logic programming languages like Prolog, which exhibit logical reasoning, offer both interpretability and automation.\nHowever, manually encoding contracts into logic programs is time-consuming and not scalable. We demonstrate how LLMs can assist in scaling this encoding process. Specifically, the recent OpenAI"}, {"title": "4 Our Future Approaches", "content": "We are on the cusp of an exciting era where AI can make legal solutions more accessible by applying human-like thinking, including planning and reasoning. In addition to our approach in this paper- using LLMs to generate logical representations-we explore several other potential approaches.\nOne approach within the realm of LLMs and logic programming is to fine-tune language models using logic-based explanations. In [9], the authors demonstrated how the \"Self-Taught Reasoner (STaR)\" method enhances language model reasoning through rationale generation, which provides step-by-step explanations, and rationalization, which corrects incorrect answers. This iterative process improves reasoning capabilities without the need for large annotated datasets. Applying a similar approach with logic generation could enhance the legal reasoning capabilities of language models\nOur second proposal focuses on generating knowledge graph representations of legal systems, similar to the mental models experienced attorneys develop. These models integrate local and federal laws, case precedents, and relevant facts, as well as more nuanced factors. We propose using LLMs to create local knowledge graphs that mirror these mental models. Each jurisdiction would have its own tailored graph, which experts would review and refine, enabling more accurate legal decision-making."}]}