{"title": "RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large Language Models on Dietary Supplements", "authors": ["Zaifu Zhan", "Shuang Zhou", "Mingchen Li", "Rui Zhang"], "abstract": "Objective: We aimed to develop an advanced multi-task large language model (LLM) framework to extract multiple types of information about dietary supplements (DS) from clinical records.\nMethods: We used four core DS information extraction tasks-namely, named entity recognition (NER: 2,949 clinical sentences), relation extraction (RE: 4,892 sentences), triple extraction (TE: 2,949 sentences), and usage classification (UC: 2,460 sentences) as our multitasks. We introduced a novel Retrieval-Augmented Multi-task Information Extraction (RAMIE) Framework, including: 1) employed instruction fine-tuning techniques with task-specific prompts, 2) trained LLMs for multiple tasks with improved storage efficiency and lower training costs, and 3) incorporated retrieval augmentation generation (RAG) techniques by retrieving similar examples from the training set. We compared RAMIE's performance to LLMs with instruction fine-tuning alone and conducted an ablation study to assess the contributions of multi-task learning and RAG to improved multitasking performance.\nResults: With the aid of the RAMIE framework, Llama2-13B achieved an F1 score of 87.39 (3.51% improvement) on the NER task and demonstrated outstanding performance on the RE task with an F1 score of 93.74 (1.15% improvement). For the TE task, Llama2-7B scored 79.45 (14.26% improvement), and MedAlpaca-7B achieved the highest F1 score of 93.45 (0.94% improvement) on the UC task. The ablation study revealed that while MTL increased efficiency with a slight trade-off in performance, RAG significantly boosted overall accuracy.\nConclusion: This study presents a novel RAMIE framework that demonstrates substantial improvements in multi-task information extraction for DS-related data from clinical records. Our framework can potentially be applied to other domains.", "sections": [{"title": "1 Introduction", "content": "Dietary supplements (DSs) play a pivotal role in promoting health and wellness by providing essential nutrients that may be lacking in regular diets. According to the 2023 CRN consumer survey\u00b9, 74% of U.S. adults use supplements, with 55% being regular users, and 92% of these users agree that DSs are essential for health. Despite their widespread acceptance, concerns persist regarding the quality\u00b2, effectiveness\u00b3, and safety3,4 of DSs due to their classification as food rather than medicine, exempting them from FDA approval. This lack of regulation leads to several issues, including insufficient transparency regarding ingredient identities, a shortage of rigorously designed clinical trials7, and limited laboratory studies clarifying their mechanisms, which could result in adverse events (AEs), some of which may be severe or even fatal10.\nClinical records contain extensive DS information and their AEs11\u201313, offering valuable insights for public health, medical research, and regulation. However, this information is often embedded in unstructured text within electronic health records14\u201316, requiring advanced information extraction methods to comprehensively and accurately identify DS entities, related events, and their interrelations\u00b97. Several tasks in extracting DS information include named entity recognition (NER) for identifying specific DS names18, relations extraction (RE) for discerning associations between DSs and AEs versus indications or no relations 19,20, triple extraction (TE) or end-to-end NER with RE, and usage classification (UC) for capturing DS usage status (such as start, continuation, or discontinuation)13. Moreover, the variability in clinical language21,22, including misspellings23, abbreviations24, and ambiguous terms, adds to the complexity of automated information extraction. Given these challenges, there is a pressing need for advanced methods that can handle the complexity and diversity of DS-related information embedded in clinical records."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Overview of methods", "content": "In this work, we propose a comprehensive framework for performing multi-task information extraction on clinical narratives related to DSs. Our framework, RAMIE (Retrieval-Augmented Multi-task Information Extraction), is designed to conduct four tasks including NER, RE, TE, and UC within a single model. RAMIE combines MTL, RAG, and instruction fine-tuning to optimize extraction accuracy, model efficiency, and scalability across tasks. The framework has the potential to be applied to other domains. By using instruction-tuning and integrating retrievers to enhance context relevancy, RAMIE demonstrates adaptability to the complexities of clinical text analysis. In subsequent sections, we describe the tasks, datasets, architectural innovations that underpin RAMIE's capabilities, and experimental setup."}, {"title": "2.2 Tasks and Datasets", "content": "In this work, we address four core information extraction tasks: NER18, RE19, TE20, and UC13. These tasks are foundational to NLP for extracting structured data from unstructured clinical text. Each task serves a specific role in transforming clinical text into a format that can support downstream applications, such as clinical decision-making or research on DSs.\n1. NER: This task involves identifying and categorizing DS entities and AEs in the text. The model detects mentions of supplements and associated events in clinical notes, marking the entities with predefined categories. For instance, in a sentence like \"The patient reported taking cranberry juice for a urinary tract infection\", the model would tag \"cranberry juice\" as a dietary supplement and \"urinary tract infection\" as an adverse event.\n2. RE: Once entities are identified, this task determines the relationships between them. In the context of DS-related data, RE identifies whether a supplement is positively or negatively associated with an event, or if there is no direct relationship. This task enables the identification of cause-effect relationships or therapeutic uses of DSs. For example, in \"The patient experienced nausea after taking ginseng\", RE would identify a negative relationship between \"ginseng\" and \"nausea.\""}, {"title": "2.3 RAMIE framework", "content": "The motivation for to proposed RAMIE framework is to maintain the high performance of LLMs while achieving efficiency. As depicted in Fig. 1, the RAMIE framework integrates MTL, RAG, and instruction fine-tuning, leveraging their complementary strengths to enhance task performance. MTL enables LLMs to handle multiple tasks within a single LLM. MTL brings in not only efficiency but also complexity, thus mitigating performance. The challenge is LLMs should learn to distinguish task types, memorize the response format, and also learn the corresponding task-solving abilities. Instruction fine-tuning and RAG enhance the performance by providing various instructions for each task and providing task-relevant examples dynamically, respectively. This combination offers high-level guidance, similar examples, and response templates to enhance understanding and reduce ambiguity in predictions.\nTo lay a solid groundwork, we evaluate 8 state-of-the-art LLMs in our framework. LLMs include Mistral-7B52, Llama-2-7B32, Llama-2-13B32, Llama-3-8B53 which were pre-trained for general downstream tasks and BioMistral-7B54, PMC-Llama-13B55, MedAlpaca-7B56 and MedAlpaca-13B56 which were pre-trained for biomedical domain."}, {"title": "A. Multi-task Learning", "content": "The MTL in the RAMIE framework offers a practical and scalable solution for deploying LLMs to address diverse tasks, especially in resource-constrained environments. As shown in Fig. 1, the RAMIE framework consolidates task-specific datasets for NER, RE, TE, and UC, each containing thousands of labeled sentences, into a unified training set. The task-specific labels serve dual purposes: they not only guide the training process but also play a crucial role in the retrieval and prompt construction phases, where retrieved examples are augmented with task-specific instructions.\nThe unified dataset is used in a multi-task instruction fine-tuning process, enabling the LLM to be trained simultaneously on sentences from all tasks. During training, the model learns to differentiate between distinct task instructions encoded in the dataset and generates accurate task-specific outputs. By employing MTL approach, RAMIE allows a single LLM to acquire the capability to solve multiple tasks with just one fine-tuning operation. This significantly reduces storage redundancy, as only one model needs to be maintained, and minimizes computational resources required for training and deployment. Furthermore, the unified structure and shared learning strategy enhance the adaptability and efficiency of the model across a broad spectrum of information extraction tasks."}, {"title": "B. Retrieval-Augmented Generation", "content": "In the RAMIE framework, as illustrated in Fig. 1, we incorporate RAG20,49,50 to further enhance the model's performance across tasks. During both the training and testing phases, the framework employs retrievers to identify the most relevant sentence-response pair from the corresponding training set based on the cosine similarity of sentence embeddings, determined by the input sentence and its associated task label. To ensure alignment between the retrieved examples and the input format, the retrievers are constrained to search within the training set specific to the input's labeled task.\nThe retrieval process involves concatenating the input sentence and its corresponding response to form a single representation, which is then compared to the input sentence embedding using cosine similarity. During training, the retrievers often identify pairs where the sentence is identical to the input sentence due to the high embedding similarity. However, such instances may cause the model to rely on copying the response directly from the example, rather than developing analytical reasoning. To mitigate this issue, the retrievers are restricted from selecting the input sentence itself during training and are instead configured to retrieve the most semantically similar example from the remaining dataset. Conversely, during testing, the retrievers are permitted to select the most relevant example from the entire training set, as there is no overlap between the input and retrieved examples.\nThe retrieved examples are incorporated into a dynamically constructed prompt, which is subsequently used for instruction fine-tuning within the LLM. To optimize retrieval quality, we utilize three state-of-the-art retrievers\u2014MedCPT57, Contriever58, and BMRetriever59. These retrievers are specifically tailored for domain-specific tasks and have demonstrated superior performance in biomedical information extraction, making them well-suited for the diverse and complex tasks addressed in RAMIE."}, {"title": "C. Instruction Fine-tuning", "content": "To optimize the performance of LLMs across multiple tasks, we employed instruction fine-tuning60,61, a technique that helps models generalize by providing explicit task instructions within the input prompt. Following the LEAP framework19, we designed the template of prompts for each task, as illustrated in Figure 1. The prompt consists of an instruction part, an example part, and the input sentence with its response. The input sentences are from the combined training set, and the examples are retrieved from the retrievers. The instructions are designed to provide instruction for each task as follows,\n\u2022 For the NER task, the model is instructed to extract dietary supplements and adverse events from a given sentence and recognize their entity types. The predefined entity types include 'event', 'folic acid', \u2018milk thistle', 'ginger', 'chamomile', 'garlic', 'black cohosh', 'ginkgo', 'lavender', 'melatonin', 'cranberry', 'ginseng', 'glucosamine', 'dandelion', 'saw palmetto', and 'green tea'. The output is expected in the format: a list of entities with their corresponding types, for example, [entity_name1: entity_type1, entity_name2: entity_type2, ...].\n\u2022 In the RE task, the model is prompted to predict the relationship between a given head entity and tail entity within the provided sentence. The relation must be selected from the predefined set: 'negative', 'not_related', or 'positive'. The response is expected as a single-item list, such as ['negative'].\n\u2022 For the TE task, the model is asked to extract triples consisting of a head entity, relation, and tail entity from the sentence. The relation types are the same as in RE, and the entity types are the same as in NER. The output format is a list of dictionaries, where each dictionary represents a triple, e.g., {'head entity': 'entity_name', 'relation': 'relation_type', 'tail entity': 'entity_name'}.\n\u2022 In the UC task, the model is required to predict the dietary supplement usage within the sentence, selecting from the predefined usage types: 'continue', 'discontinue', 'uncertain', or 'start'. The expected output is a list containing the predicted usage type, such as ['continue']."}, {"title": "2.4 Experiments", "content": "We designed three groups of experiments to comprehensively evaluate the proposed RAMIE framework: (1) single-task instruction fine-tuning as a benchmark, (2) the RAMIE framework, and (3) multi-task instruction fine-tuning as an ablation study. The ablation study was designed to provide insights into the individual contributions of MTL and RAG within the RAMIE framework. All experiments were conducted on NVIDIA A100 GPUs with 80GB memory. The training and evaluation batch sizes per device were set to 4, and inference was performed on a sentence-by-sentence basis for efficiency. To fine-tune the model effectively, we employed the LoRA approach62, with the rank set to 64, alpha to 32, and a dropout rate of 0.1. The AdamW optimizer was used with a learning rate of 1e-5, and the model was trained for 5000 steps. Evaluations were performed every 1000 steps, and the best-performing model was selected for inference.\nFor the baseline, we implemented a BERT-based model with separate task-specific heads to handle the four tasks, following the methodology in prior work48. To evaluate model performance, we adopted Micro Precision, Recall, and F1-score metrics, in line with established studies17,20. A prediction was considered correct only if the entire output exactly matched the ground truth 17,28. For single-task instruction fine-tuning, models were trained on a single dataset and evaluated on its corresponding test set. In contrast, for multi-task instruction fine-tuning, the models were trained on a blended dataset combining all tasks and evaluated on the test sets of each task individually, both with and without RAG integration. To further evaluate the effectiveness of the retrievers, experiments were conducted using a random retriever as a baseline for comparison."}, {"title": "3 Results", "content": ""}, {"title": "3.1 Instruction Fine-tuning Performance", "content": "presents a comprehensive comparison of various models evaluated under single-task instruction-tuning setting across four tasks. Overall, compared to BERT, all the LLMs achieved F1 scores above 80, significantly surpassing BERT's average score of 69.55. This highlights the superior performance of LLMs on DS-related tasks. The key difference lies in the NER task: while BERT classifies each token individually, LLMs extract information directly from entire sentences, leading to a substantial performance gap. For LLMs, shows that the MedAlpaca-7B model achieves the highest average F1-score of 86.38, outperforming other models such as BioMistral-7B (85.36) and Mistral-7B (85.06). This indicates that MedAlpaca-7B is particularly effective when fine-tuned for individual tasks.\nAs for the individual task, for example, MedAlpaca-7B achieved an F1 score of 88.57 in NER task. Llama3-8B demonstrated exceptional performance in the RE task, with an F1 score of 93.61. In the TE task, MedAlpaca-7B scored 75.36, while BERT achieved the highest F1 score of 92.61 in the UC task, with LLM models performing closely behind. Notably, traditional models like BERT lag significantly behind, highlighting the advancements brought by more recent architectures."}, {"title": "3.2 RAMIE Performance", "content": "presents the performance of 8 LLMs across NER, RE, TE, and UC. Each row reflects the performance of a single model for each dataset. Across all models, the BMRetriever configuration consistently yields the highest average F1 scores. For example, BioMistral-7B achieves its best average F1 score of 85.29 when using BMRetriever. Similarly, Llama2-7B, MedAlpaca-7B, and Mistral-7B also show their best performance under the BMRetriever setting. Notably, Llama2-13B achieves its highest score of 85.96 using Contriever, indicating a strong performance in this configuration.\nIn terms of performance of individual tasks, RE generally demonstrates higher scores across the models compared to the other tasks, with several models achieving F1 scores above 90. For instance, Llama2-13B under BMRetriever achieves an F1 score of 93.74 in RE. TE however tends to have more variable results, with F1 scores ranging from around 60 to 73 across most models, suggesting the relative difficulty of this task."}, {"title": "3.3 Ablation Study", "content": "summarizes the evaluation results under the multi-task instruction fine-tuning setting without RAG. Among the models, Mistral-7B stands out with the highest average F1 score of 84.06, surpassing BioMistral-7B (82.10) and Llama3-8B (83.18), indicating its strong and consistent performance across tasks. Most models perform reliably in the RE task, with several exceeding an F1 score of 90. Notably, Llama3-8B, BioMistral-7B, and MedAlpaca-7B all achieved F1 scores of 92.22 or higher in this task. However, the TE task exhibits greater variability in F1 scores, with Mistral-7B achieving the highest at 70.74, highlighting the relative difficulty of this task.\nAdditionally, we computed the relative F1 score drop for each task and model, comparing to single-task instruction fine-tuning. Under the multi-task setting, the MedAlpaca-13B model experienced the largest drop of 5.66%, while the average"}, {"title": "4 Discussion", "content": "Extracting DS information in clinical notes is crucial due to the widespread and growing use of DSs and the need for accurate information on their usage, efficacy, and safety. Given limited regulatory oversight, extracting reliable DS-related information from clinical records is essential for patient safety and informed decision-making. LLMs, with their advanced NLP capabilities, are uniquely suited to this task. Their ability to manage complex, unstructured data allows them to efficiently handle multiple DS-related tasks. By leveraging techniques like instruction fine-tuning, MTL, and RAG, LLMs enhance the precision and relevance of extracted DS data, making them a transformative tool for advancing DS safety and effectiveness in healthcare. In this section, we reflect on the results in the context of single-task instruction fine-tuning, the RAMIE framework, and the effects of removing RAG from the framework. In the end, we analyzed errors and discussed the limitations and future directions.\nThe first key finding from this study is that LLMs demonstrated exceptional performance on DS-related tasks, significantly outperforming BERT-based models. For example, MedAlpaca-7B achieved the highest F1 score (86.38) under the single-task instruction fine-tuning setting, which shows the superiority of LLMs in handling complex, multi-faceted tasks. Compared to BERT, LLMs have a more advanced architecture and larger model size, allowing them to capture deeper semantic patterns and make use of richer contextual information. Moreover, LLMs' generative nature enables better adaptability across diverse tasks without requiring specific architecture changes, which is a limitation in smaller models like BERT. The ability of LLMs to generalize across tasks while maintaining high performance makes them particularly well-suited for complex domains such as biomedical information extraction.\nSecond, our RAMIE provided a framework for MTL, achieving comparable performances of single-task instruction fine-tuning. For instance, Llama2-13B, when combined with the Contriever retriever, reached an F1 score of 85.96, outperforming the single-task fine-tuning counterpart (F1 of 84.99). We conducted a comprehensive analysis of all experiments, comparing the performance of models in single-task settings and models with RAG across four tasks. This included combinations of each model with three different retrievers, totaling 96 experimental setups. Among these, 46 setups achieved better results using our framework, demonstrating that the outcomes from our framework are comparable to those from single-task instruction fine-tuned models. The key advantage of the RAMIE framework lies in its use of RAG, which enhances LLMs by providing additional examples during training and testing. RAG helps the model retrieve relevant examples that augment the training data, thus enabling more accurate output generation. As shown in Fig. 2, we compared the performance of each LLM across five settings: zero-shot (labeled as 'w/o RAG' in table), random example, and three retrievers (MedCPT, Contriever, and BMRetriever). Zero-shot serves as the baseline without RAG and the random example setting serves as the baseline\nfor using examples. Interestingly, using a random example in the prompt actually decreases model performance, while all retrievers-MedCPT, Contriever, and BMRetriever-consistently improved the performance of LLMs. Contriever shows the best overall performance, with six out of eight models achieving their highest scores when combined with Contriever. BMRetriever follows closely, yielding the best results in two models. By dynamically fetching task-specific examples, RAG alleviates the burden on the LLM to store all knowledge internally, allowing the model to focus on generating more contextually accurate predictions in DS-related information extraction.\nIn addition, except that the performances of single-task instruction fine-tuning and our RAMIE framework are comparable, the model trained in our framework demonstrates excellent storage efficiency and reduction on training cost, since a single model is trained once and then can handle all four tasks simultaneously, which verifies the effectiveness of the proposed RAMIE framework. By reducing the storage requirements and training cost while maintaining or even improving performance for some models, this approach is particularly suited for resource-constrained environments where running LLMs on multiple devices is not feasible.\nLastly, we conducted an ablation study where we ran MTL experiments (without RAG). When comparing the results to single-task instruction fine-tuning, we observed that the overall average F1 scores decreased, as shown in Fig. 3. The reason behind this decline is that MTL requires the model to manage multiple tasks simultaneously, which leads to increased complexity in distinguishing between tasks and adjusting to varying output formats. This cognitive load on the model can cause a negative influence, where learning multiple tasks hinders the model's performance on individual tasks. The need to allocate capacity to learn the differences between tasks, as well as the varying output formats for each task, leads to a slight drop in performance. This is consistent with findings from previous research48.\nDiving into these promising results, through observing the generation, we found LLMs are suffering from the following errors:\n\u2022 Redundant Information: In tasks like NER and TE, the language model occasionally extracts unnecessary information. For instance, while expert annotations label \"motion sickness\" as an adverse event, the model often extracts \"mild motion sickness\" as the result. Here, \"mild\" is an adjective describing the adverse event rather than a core part of it.\n\u2022 Information omission: The model occasionally failed to capture all relevant information. In sentences with multiple entities, it might extract only a subset of them-capturing three out of four entities, for instance. Similarly, when there are multiple triples in a sentence, the model sometimes retrieves only one, overlooking additional relevant relationships.\n\u2022 Incorrect generation: It includes misclassifying entities, incorrectly assigning relationships between head and tail entities, and extracting non-entity words as entities."}, {"title": "Limitations and future Directions", "content": "This work has a few limitations. First, we did not include particularly large models (e.g., Llama-2-70B) in our experiments. Larger models often have greater capacity, which may enable them to better manage the complexities of learning multiple tasks concurrently, potentially enhancing MTL performance. Second, to our knowledge, no retriever exists that is specifically optimized for DS tasks, which might further improve model accuracy and relevance in this domain. Lastly, we did not investigate few-shot learning scenarios. Examining the model's performance under few-shot conditions could offer valuable insights into its adaptability and ability to generalize in contexts with limited labeled data. For future research, we recommend further exploration of retrieval-augmented multi-task frameworks using various LLM architectures and datasets. This could provide a deeper understanding of the framework's scalability across different domains. Additionally, experimenting with a broader range of retrievers beyond MedCPT, Contriever, and BMRetriever may uncover more effective methods to enhance model performance. Finally, investigating transfer learning between related tasks or integrating more advanced model architectures could help address the observed performance limitations in MTL, leading to improved efficiency and accuracy across tasks."}, {"title": "5 Conclusion", "content": "This paper presented the RAMIE framework, a retrieval-augmented, multi-task LLM solution for extracting dietary supplement information from clinical records. RAMIE effectively handled NER, RE, TE, and UC tasks, achieving high accuracy and efficiency through instruction fine-tuning, MTL, and RAG. Experimental results demonstrated that RAMIE demonstrates its excellent abilities in DS information extraction while achieving efficiency, confirming its potential for scalable, efficient information extraction in healthcare applications."}, {"title": "6 Contributorship Statement", "content": "Zaifu Zhan and Rui Zhang conceptualized and designed the study. Zaifu Zhan and MingChen Li curated the data. Zaifu Zhan executed the experiments. Zaifu Zhan and Shuang Zhou drafted the initial manuscript, and Rui Zhang reviewed and finalized the manuscript. Rui Zhang supervised the whole project."}, {"title": "7 Funding Statement", "content": "This work was supported by the National Institutes of Health's National Center for Complementary and Integrative Health under grant numbers R01AT009457 and U01AT012871, the National Institute on Aging under grant number R01AG078154, the National Cancer Institute under grant number R01CA287413, and the National Institute of Diabetes and Digestive and Kidney Diseases under grant number R01DK115629. The content is solely the responsibility of the authors and does not represent the official views of the National Institutes of Health."}, {"title": "8 Acknowledgements", "content": "We would like to express our sincere gratitude to the reviewers for any suggestions."}, {"title": "9 Competing Interests Statement", "content": "The authors state that they have no competing interests to declare."}]}