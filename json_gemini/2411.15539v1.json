{"title": "Large Language Model with Region-guided Referring and Grounding for CT Report Generation", "authors": ["Zhixuan Chen", "Yequan Bie", "Haibo Jin", "Hao Chen"], "abstract": "Computed tomography (CT) report generation is crucial to assist radiologists in interpreting CT volumes, which can be time-consuming and labor-intensive. Existing methods primarily only consider the global features of the entire volume, making it struggle to focus on specific regions and potentially missing abnormalities. To address this issue, we propose Reg2RG, the first region-guided referring and grounding framework for CT report generation, which enhances diagnostic performance by focusing on anatomical regions within the volume. Specifically, we utilize masks from a universal segmentation module to capture local features for each referring region. A local feature decoupling (LFD) strategy is proposed to preserve the local high-resolution details with little computational overhead. Then the local features are integrated with global features to capture inter-regional relationships within a cohesive context. Moreover, we propose a novel region-report alignment (RRA) training strategy. It leverages the recognition of referring regions to guide the generation of region-specific reports, enhancing the model's referring and grounding capabilities while also improving the report's interpretability. A large language model (LLM) is further employed as the language decoder to generate reports from integrated visual features, facilitating region-level comprehension. Extensive experiments on two large-scale chest CT-report datasets demonstrate the superiority of our method, which outperforms several state-of-the-art methods in terms of both natural language generation and clinical efficacy metrics while preserving promising interpretability. The code will be made publicly available.", "sections": [{"title": "I. INTRODUCTION", "content": "Computed tomography (CT) is widely used in clinical practice and crucial for diagnoses [1]. However, this process is labor-intensive [2] as radiologists need to analyze entire CT volumes and produce detailed diagnostic reports. To alleviate the workload, automatic CT report generation has attracted increasing attention [3]\u2013[5].\nCT report generation involves creating detailed and accurate diagnostic reports from CT volumes, where each anatomical region must be thoroughly analyzed and described to ensure reliable clinical insights. Existing methods [3]-[5] primarily rely on global features extracted from the entire volume to generate reports. While effective to some extent, this approach overlooks the inherent complexity of CT data as a three-dimensional (3D) imaging modality. CT volumes encode rich anatomical details across various regions, presenting significant challenges for decoders to accurately capture region-specific abnormalities using only volume-level embeddings. As depicted in Fig. 1 (a), the vanilla method often fails to identify certain abnormalities when limited to global features alone. Therefore, enhancing the model's ability to process and integrate region-specific information is crucial for generating comprehensive and clinically relevant CT reports.\nRecently, universal segmentation models [6]\u2013[8] have achieved remarkable progress and possess potent zero-shot capabilities. This inspires us to leverage these models as off-the-shelf tools to extract anatomical masks from CT volumes, providing key regional information for report generation. In this work, we propose Reg2RG, a Region-guided Referring and Grounding framework for Report Generation, leveraging the in-context learning and long-term referencing capabilities of LLMs. Referring and grounding are closely related but distinct concepts in visual understanding tasks. Referring focuses on understanding the semantics of designated regions within an image and providing their descriptions. In contrast, grounding involves locating specific regions based on textual information, effectively linking language to visual elements. With these abilities, the model can accurately refer to specific regions for detailed diagnoses and ground reports to the correct regions for better interpretability. As shown in Fig. 1 (b), our method can generate more accurate reports, which are well-grounded in each referring region of the volume.\nTo reduce computational costs, previous works [4], [5] often downsample volumes or pool features. However, these methods may lose crucial high-resolution texture details [9]. To address this issue, we propose a local feature decoupling (LFD) strategy to preserve local high-resolution texture details and essential geometry with low computational cost. We further integrate global and local features to jointly capture significant inter-regional relationships within a cohesive context.\nIn medicine, interpretability is vital for radiologists to comprehend the visual basis of the generated reports. Previous works [10], [11] mainly use attention maps to link report keywords with image regions. However, these attention maps are often coarse and ambiguous, lacking precision and reliability. To enhance interpretability, we propose a novel region-report alignment (RRA) training strategy that enforces an explicit association between the referring region and the report. Specifically, the referring region order is shuffled for the model to identify anatomical areas before generating the report. By doing so, the reports are certainly grounded in the identified regions, enhancing interpretability and reliability. In summary, our contributions are as follows:\n\u2022 We propose Reg2RG, a novel Region-guided Referring and Grounding framework for Report Generation. It generates accurate reports by focusing on target regions from multiple candidates and aligning them with the correct regions for better interpretability. To our knowledge, this is the first work introducing referring and grounding for CT report generation.\n\u2022 We design a local feature decoupling (LFD) strategy to decouple texture and geometry, preserving high-resolution texture details and essential geometry with minimal overhead. The global features are integrated with local features to collaboratively capture inter-regional relationships within a cohesive context.\n\u2022 A region-report alignment (RRA) training strategy is utilized to boost the model's referring and grounding abilities, making it more interpretable and reliable.\n\u2022 We highlight the critical role of large-scale LLMs in report generation lies in their exceptional region-level referencing and grounding capabilities. These abilities allow for precise focus on region-specific features and inter-regional relationships, enabling the creation of more accurate and clinically relevant reports.\n\u2022 Experiments on two large-scale 3D chest CT datasets demonstrate the effectiveness of our method, achieving both superior performance and interpretability."}, {"title": "II. RELATED WORKS", "content": "To alleviate the heavy workload of pathologists, medical report generation has emerged as an effective solution for the automatic interpretation of medical images. The previous works [10]\u2013[13] mostly focus on the 2D chest X-ray (CXR) report generation. To produce higher-quality reports, recent efforts [10], [13], [14] have focused on incorporating additional information to enhance the accuracy of key abnormality details in the generated reports. For instance, PromptMRG [13] utilizes abnormality classification results as diagnostic prompts for the decoder, enhancing both the clinical relevance and effectiveness of the generated reports. Similarly, ORGAN [10] constructs an observation graph to better aggregate clinically significant information, further improving the quality and coherence of the reports. Building on this trend, RGRG [14] utilizes a detector to introduce regional information, supporting fine-grained report generation. However, this approach has not been thoroughly explored in the more expansive and information-rich 3D space of CT volumes, where capturing regional details is particularly essential.\nLeveraging large-scale CT-report datasets [15]\u2013[17], automatic CT report generation has also garnered increasing attention recently. CT2Rep [3] represents the first exploration into 3D CT report generation, leveraging a memory-driven decoder to generate detailed reports directly from global volume features. Inspired by the advancements in multi-modal LLMs [18], [19], recent approaches have attempted to bootstrap LLMs for CT report generation. Dia-LLaMA [20] integrates critical diagnostic prompts as prior medical knowledge, while HILT [9] emphasizes efficient encoding strategies for high-resolution volume features to enhance performance. Additionally, several studies [4], [5], [21], [22] have explored the development of medical generalist models based on LLMs, demonstrating their capacity to perform CT report generation as part of broader diagnostic tasks.\nHowever, these approaches generate reports solely based on global features, overlooking the critical role of local anatomical information. Furthermore, LLMs' referencing and context-learning capabilities are underutilized. To address these issues, we propose a region-guided referring and grounding framework that leverages local features to enhance regional comprehension, fully harnessing the capabilities of LLM to optimize CT report generation."}, {"title": "B. Region-level Referring and Grounding", "content": "To promote region-level understanding, several works in the general domain [23]\u2013[27] have explored integrating LLMS with region-level features, showcasing their potential to enhance fine-grained reasoning and context-aware interpretations. However, the regional interpretation of medical images remains relatively underexplored. RGRG [14] extracts region features for CXR report generation but generates reports independently for each region, neglecting inter-regional relationships and becoming inefficient with more regions. Some methods [28]\u2013[30] attempt to ground diagnostic text to targeted regions in medical images, yet they fail to utilize region-level"}, {"title": "III. METHODS", "content": "In this section, we first overview our framework in Sec. III-A. Next, we detail the local feature decoupling (LFD) strategy in Sec. III-B, and explain their integration with global features in Sec. III-C. Finally, we describe the region-report alignment (RRA) training strategy in Sec. III-D."}, {"title": "A. Overview", "content": "The overview of our method Reg2RG is shown in Fig. 2. CT report generation involves creating report R based on CT volumes $V \\in \\mathbb{R}^{H\\times W\\times D}$. Unlike the previous works [3], [5] that use only global features G of V, we additionally extract a set of local features $L = \\{L_1,\\ldots, L_n\\}$ with the universal segmentation module. To preserve the local high-resolution texture details and significant geometry information with minimal computational overhead, local features are decoupled into texture and geometry. These local features work alongside global features to generate the report R using the LLM. Furthermore, we propose a training strategy that strengthens the correspondence between the local feature $L_i$ and the region-specific report $R_i$, improving clarity and trustworthiness. The overall generating process can be described as follows:\n$R = LLM(G, L) = LLM(G, L_1,..., L_n)$, (1)\nwhere n is the number of referring regions in V."}, {"title": "B. Local Feature Decoupling Strategy", "content": "As shown in Fig. 2, we first utilize the existing universal segmentation module $f_s$ to extract mask $M_{A_i}$ for the anatomical area $A_i$ given the CT volumes V. The segmentation process can be formulated as follows:\n$\\{M_{A_1},\\ldots, M_{A_n}\\} = f_s(V)$. (2)\nThe region mask $M_{A_i}$ is then utilized to construct the corresponding local features $L_{A_i}$. To preserve higher-resolution details without increasing much computational burden, we design a local feature decoupling (LFD) strategy that separates texture and geometry information.\nFor the texture information, we first use region mask $M_{A_i}$ to extract the region volume $V_{A_i}$ from V by element-wise multiplication. This results in a large redundant area outside the target region, which is not informative. Therefore, we crop the region volume $V_{A_i}$ to exclude these irrelevant parts. Since $V_{A_i}$ is much smaller than the entire volume V, we can retain higher-resolution details without increasing the input size. Next, we utilize a 3D volume encoder $f_v$ to extract local texture features $L_{A_i}^t$. The following adapter module $f_a$ is used to compress and align these local texture features with the embedding space of the LLM. This process can be formulated as follows:\n$L_{A_i}^t = f_a(f_v(Crop(V_{A_i}))) = f_a(f_v(Crop(M_{A_i} \\odot V)))$, (3)\nwhere $\\odot$ indicates element-wise multiplication.\nThe previous works [23]-[27] solely consider texture features, ignoring the essential geometry features for assessing lesion size and location in medical contexts. In contrast, our method includes it as a supplementary feature. Specifically, we introduce the geometry information by encoding the region mask $M_{A_i}$, which is uncropped to preserve the original size and position. A lightweight volume encoder $f_M$ is used to extract geometry features $L_{A_i}^g$, followed by a fully connected layer $f_p$ to project these features for LLM input. This process is formulated as follows:\n$L_{A_i}^g = f_p(f_M(M_{A_i}))$. (4)\nThe local features $L_{A_i}$ are obtained by concatenating the texture features $L_{A_i}^t$ and geometry features $L_{A_i}^g$:\n$L_{A_i} = Concat(L_{A_i}^t, L_{A_i}^g)$. (5)\nTypically, the local features LA comprise multiple regions $L_{A_i}$, each providing specific information for a distinct anatomical area within the CT volumes:\n$L_A = \\{L_{A_1},\\cdots, L_{A_n}\\}$. (6)\nNote that LA is not identical to L in Eq. (1); further details will be provided in Sec. III-D."}, {"title": "C. Global-Local Features Collaboration", "content": "In medicine, different regions are interactive rather than isolated. Therefore, we include global features for context. Specifically, the same volume encoder fv and adapter fA (as in Sec. III-B) are used to extract global features G:\n$G = f_a(f_v(V))$. (7)\nThe collaboration of global and local features is achieved by embedding them into the prompt. Our designed prompt P consists of two parts: P = {1,T}, where I denotes the special tokens for the visual embedding and T represents the text tokens of the instruction. As depicted in Fig. 2, we utilize <image> and <region i> as the special tokens for the global and local features, respectively. These special tokens are replaced by the corresponding features G and $L_i$, which then interact within the LLM to generate R."}, {"title": "D. Region-Report Alignment Training Strategy", "content": "To enhance the explicit link between the referring region and the report, we propose a training strategy that first recognizes the anatomical area of the referring region and then generates the corresponding region report Ri.\nWhile the segmentation module can provide the anatomical area name, this information may bias the model during report generation since it can lead the model to rely on the name rather than the region features. Therefore, we train the model to recognize this information independently from the region features. This strategy helps the model better understand the referred region, making the generated report more reliably grounded. Furthermore, we shuffle the order of the local features at each step to prevent the model from identifying anatomical areas by a fixed sequence. Thus, although the content of L and LA is unchanged, the correspondence of each region feature varies. The generation process is defined as follows:\n$R = \\{(P_1, R_1),\\ldots, (P_n, R_n)\\}$\n$LLM(G, L_1,\\cdots, L_n)$\n$= LLM(G, Shuffle (L_{A_1},\\cdots, L_{A_n}))$. (8)\nDuring training, region-level reports $R_i$ of each referring region are used as ground truth. In addition, we add a prefix $P_i=\\text{\"The region [i] is [area name]\"}$ to indicate the anatomical area name. The restructured report remains a sequence of text tokens: $R = \\{(P_1, R_1),\\ldots, (P_n, R_n)\\} = \\{r_1,r_2,\\ldots,r_T\\}$ (T is the report length). This format integrates seamlessly with the native auto-regressive training process of the LLM. The training process is optimized by minimizing the language-modeling loss as shown below:\n$L_{LLM} = -\\sum_{t=1}^T log p(r_t|P,r_1,\\cdots, r_{t-1})$, (9)\nwhere P indicates the prompt, as mentioned in Sec. III-C. During inference, the prefix $P_i$ is removed from the generated report R before evaluation."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "Datasets. We train and evaluate our model alongside comparative methods using two large-scale chest CT datasets, ensuring a more comprehensive assessment.\nThe RadGenome-ChestCT dataset [16], designed for region-guided 3D chest CT interpretation, is based on the CT-RATE [15] dataset. It consists of 25,692 region-guided CT-report pairs sourced from 21,304 patients. The CT volumes possess a consistent voxel spacing of 1 mm \u00d7 1 mm \u00d7 3 mm, with anatomical masks generated using the SAT [6] segmentation module. Region-grounded reports are generated with GPT-4 [32] and a named entity recognition model, covering 10 chest anatomical regions: abdomen, bones, breasts, esophagus, heart, lungs, trachea and bronchi, mediastinum, pleura, and thyroid. Following the official data split, 24,128 pairs are allocated for training, while 1,564 pairs are reserved for evaluation.\nThe CTRG-Chest-548K dataset [17] includes 1,804 CT volume-report pairs. To extract anatomical masks, we employ the segmentation module SAT [6]. Since region-level grounded reports are not provided within this dataset, we utilize Qwen2.5-14B [33] to segment the reports into region-level sections. Regarding the data split, the dataset is randomly partitioned into training and testing sets in an 8:2 proportion."}, {"title": "C. Quantitative Results", "content": "Table I presents the NLG results of our model alongside comparisons to other methods. On the RadGenome-ChestCT dataset, our model outperforms all others across all NLG metrics, underscoring its capability to generate high-quality reports. Specifically, we take the MedVInT [22] with second-best results as an example. Our model achieves a relative improvement of 1.1% to 6.7% on BLEU metrics, highlighting enhanced expression similarity to the reference reports. For the METEOR metric considering inflectional variations and synonym matching, our model shows a notable 9.1% improvement, indicating better lexical flexibility and semantic alignment. Our model also surpasses the second-best by 12.4% in ROUGE-L, highlighting its consistent performance across metrics. A similar pattern is observed on the CTRG-Chest-584K dataset, where our method outperforms the second-best model with a 2.0% to 3.7% improvement in BLEU metrics and a 0.8% gain in METEOR. The lower ROUGE-L score on this dataset may be attributed to the fragmented nature of region-level report generation. This fragmentation affects the coherence between reports for different regions and consequently impacts the evaluation of the longest common sequence. However, this inconsistency does not affect the quality of individual region reports, as evidenced by higher performance on other metrics. These results highlight the effectiveness of our model in generating high-quality reports."}, {"title": "2) Clinical Efficacy Metrics:", "content": "Table II showcases the CE performance of our model compared to other methods on the RadGenome-ChestCT dataset. Our model surpasses the second-best approach by 3.9%, 22.3%, and 19.3% in precision, recall, and F1 score, respectively. This demonstrates the superiority of our model in generating reports with higher diagnostic accuracy. It is worth noting that there is an inherent trade-off between precision and recall. Achieving higher precision requires minimizing false positives, which often leads the model to adopt a more conservative approach to predicting abnormalities. On the other hand, higher recall necessitates reducing false negatives, encouraging a more aggressive stance in abnormality detection. Therefore, balancing these competing priorities is particularly challenging, as demonstrated by the performance of other models. For instance, MedVInT [22] achieves the second-highest recall but struggles with relatively low precision, while M3D [5] exhibits the reversed trend, favoring precision at the expense of recall. In contrast, our model effectively balances this trade-off, maintaining high performance in both metrics and achieving a significantly improved F1 score. These results underscore our model's ability to maintain clinical relevance and diagnostic reliability while delivering high linguistic quality in the generated reports."}, {"title": "3) Region Recognition Results:", "content": "Table III presents the region recognition performance of our model on the RadGenome-ChestCT dataset [16]. To enhance the model's interpretability and reliability of generated reports, our approach explicitly requires the model to first identify the anatomical area corresponding to the referring region before generating the associated report. This intermediate recognition step simplifies evaluation and interpretation compared to directly analyzing the generated reports, providing an early-stage validation of both their alignment with the target regions and the reliability of the reports. The results show that our model accurately identifies most anatomical regions except for the lung and pleura, suggesting the generated reports are reliably aligned with the target regions, thereby enhancing interpretability. The subpar results for the lung and pleura are attributed to the performance of the SAT [6] segmentation module, which struggles to produce masks that adequately differentiate between these closely related regions. Once the segmentation model provides correct region masks, our model can effectively identify referring regions, leading to more trustworthy and clinically reliable reports."}, {"title": "D. Ablation Study", "content": "To demonstrate the effectiveness of each component in our model, we conduct comprehensive ablation studies on the RadGenome-ChestCT dataset [16]. Given that our volume encoder fy and adapter fa are initialized using the pre-trained checkpoint from RadFM [4], we take it as the baseline for comparison. First, we examine the contribution of the local feature decoupling (LFD) strategy in Sec. IV-D.1. Next, we investigate the effectiveness of global-local features collaboration by incrementally adding different visual features in Sec. IV-D.2. Additionally, we analyze the performance of our region-report alignment (RRA) training strategy in Sec. IV-D.3. Finally, we validate the necessity of employing large-scale LLMs for region-level referring and grounding in Sec. IV-D.4."}, {"title": "1) Local Feature Decoupling Strategy:", "content": "First, we validate the efficacy of the local feature decoupling (LFD) strategy in improving model performance. In the baseline setup where decoupling is not applied, the model uses masked volumes without cropping as local features, combining texture and geometry information without separation. As Table IV shows, the model with decoupled features demonstrates superior performance across most metrics. The significant improvement in CE metrics highlights the value of local high-resolution details from decoupled texture information in enhancing diagnostic accuracy. The slight decreases in BLEU-4 and ROUGE-L scores may be due to these metrics relying only on word overlap, which often fails to capture nuanced semantic similarities. By contrast, the higher METEOR score supports this assumption because it takes synonym matching and semantic relationships into consideration."}, {"title": "2) Global-Local Features Collaboration:", "content": "As shown in Table V, we assess the effectiveness of global-local features collaboration by incrementally adding different visual features across settings (a) to (c). Without position and size information, the performance of setting (a) falls significantly below the baseline, underscoring the critical role of geometric information in medical imaging for accurately representing local features. Including geometric features in setting (b) leads to notable improvement, surpassing the baseline in most metrics and demonstrating the value of spatial and structural details. Further incorporating global features in setting (c) boosts performance across most CE and NLG metrics, highlighting the efficacy of global context in capturing inter-regional relationships and improving report coherence. However, we observe that the improvement in recall comes at the expense of precision. This trade-off may result from the influence of abnormality information within global features, which may misrepresent the diagnosis of individual regions and lead to an increase in false positives across regions."}, {"title": "3) Region-Report Alignment Training Strategy:", "content": "In setting (d) of Table V, we validate the efficacy of our region-report alignment (RRA) training strategy, which guides the LLM to generate reports based on referring region information for reliable grounding. The results indicate that our complete model outperforms the setting (c) model across all CE and NLG metrics except ROUGE-L. This improvement demonstrates the effectiveness of the RRA strategy in aligning region-specific features with report generation, ensuring more accurate and clinically relevant outputs. The slightly lower ROUGE-L score can be attributed to its reliance on exact sequence matching, which may overlook the use of synonyms or paraphrased expressions. As a result, ROUGE-L may not fully reflect report quality in cases where the phrasing differs but the semantic content remains consistent. The top METEOR performance supports this, as it better captures semantic similarity by considering synonyms and paraphrasing.\nAdditionally, the proposed RRA strategy mitigates the precision-recall trade-off in (c). By directing the LLM to reference explicit region information, our model not only ensures that each report is grounded in the correct region but also improves its diagnostic and linguistic quality."}, {"title": "4) LLM as the Language Decoder:", "content": "To validate the necessity of employing a large-scale LLM as the language decoder for region-level referring and grounding, we conduct a comparative study using GPT-2 [44] as the decoder. As demonstrated in Table IV, the LLaMA2-7B model consistently outperforms the GPT-2 model across all metrics, highlighting the advantages of large-scale LLMs in this context. While the GPT-2 decoder performs competently with only global features in report generation [45], [46], it struggles to effectively refer to the specific regional features and capture the inter-regional relationships required for region-specific reports. Complex region-level referring and grounding need the powerful in-context learning and long-term referencing abilities of large-scale LLMs, underscoring their importance in generating accurate and contextually rich medical reports tailored to specific anatomical regions."}, {"title": "E. Qualitative Results", "content": "Following [11], we analyze the report length distributions of both generated and ground-truth reports. Fig. 3 presents the distributions of report lengths for the ground-truth reports alongside those generated by our method and the SOTA MedVInT [22]. We leverage Kernel Density Estimation (KDE) to visualize the probability distributions and compute KL divergence to quantify the differences between the distributions of our method and MedVInT relative to the ground-truth reports. The results indicate that our model generates reports with lengths more closely aligned with the ground-truth reports than those generated by MedVInT, as reflected in the lower KL divergence. This suggests that the reports generated by our model are more complete and accurate, whereas MedVInT tends to produce shorter reports, potentially leading to information loss."}, {"title": "2) Analysis of Generated Reports:", "content": "We present two cases from the RadGenome-ChestCT dataset [16] in Fig. 4. Different colors represent the various correctly diagnosed regions described in the reports, while the gray background highlights incorrect diagnoses. Due to the full report length, we focus on the most relevant sections concerning abnormalities in the ground-truth reports.\nIn the first case, the MedVInT model misdiagnoses multiple regions as normal, missing critical abnormalities. In contrast, our model identifies most abnormalities except the liver lesion. This demonstrates our model's ability to provide more comprehensive and precise diagnostic information. Notably, it also accurately identifies the location and severity of emphysematous changes in the lungs.\nIn the second case, our model accurately identifies all abnormalities except for bone degenerative change, whereas MedVInT fails to detect any of them mentioned in the ground-truth report. Although MedVInT points out an abnormality of linear atelectatic changes in the lungs, the diagnosis is likely incorrect as it is not referenced in the ground-truth report. These results demonstrate that our model achieves higher diagnostic accuracy."}, {"title": "3) Report with Referring and Grounding:", "content": "A key advantage of our model is its ability to generate reports explicitly grounded to specific regions. As illustrated in Fig. 5, our generated reports are segmented into distinct sections, each corresponding to a particular anatomical region. We use the same color scheme as in Fig. 4 to represent the region masks and reports associated with different regions.\nEach regional report begins with the referring region, providing an initial hint about the focus of the LLM. The area recognition result is presented before the report, helping verify the reliability of the generated report. Successfully identifying the referring region indicates that the LLM refers to the correct regional information, enabling the report to be properly grounded in the corresponding region. This enhances the model's interpretability and reliability of the reports, which is valuable for clinical practice. Conversely, if the referenced region is misidentified, the report becomes unreliable regardless of its content and cannot be definitely linked to any region. This strategy provides a straightforward yet effective mechanism for validating reports and offers reliable reference information to assist radiologists in their interpretation."}, {"title": "V. CONCLUSION", "content": "In this study, we propose the Reg2RG framework for CT report generation. Unlike existing methods relying only on global features, our approach integrates local features with global features, improving the model's ability to identify detailed lesions in sub-regions, while also enhancing the model's interpretability and the reliability of the reports. Specifically, we use anatomical masks from a universal segmentation model to capture local features of referring regions. To retain high-resolution local details with low computational cost, a local feature decouple strategy (LFD) is introduced to decouple local features into two parts. Texture features capture fine-grained details of cropped region volumes, while geometric"}]}