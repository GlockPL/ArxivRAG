{"title": "QWEN2 TECHNICAL REPORT", "authors": ["An Yang", "Baosong Yang", "Binyuan Hui", "Bo Zheng", "Bowen Yu", "Chang Zhou", "Chengpeng Li", "Chengyuan Li", "Dayiheng Liu", "Fei Huang", "Guanting Dong", "Haoran Wei", "Huan Lin", "Jialong Tang", "Jialin Wang", "Jian Yang", "Jianhong Tu", "Jianwei Zhang", "Jianxin Ma", "Jin Xu", "Jingren Zhou", "Jinze Bai", "Jinzheng He", "Junyang Lin", "Kai Dang", "Keming Lu", "Keqin Chen", "Kexin Yang", "Mei Li", "Mingfeng Xue", "Na Ni", "Pei Zhang", "Peng Wang", "Ru Peng", "Rui Men", "Ruize Gao", "Runji Lin", "Shijie Wang", "Shuai Bai", "Sinan Tan", "Tianhang Zhu", "Tianhao Li", "Tianyu Liu", "Wenbin Ge", "Xiaodong Deng", "Xiaohuan Zhou", "Xingzhang Ren", "Xinyu Zhang", "Xipin Wei", "Xuancheng Ren", "Yang Fan", "Yang Yao", "Yichang Zhang", "Yu Wan", "Yunfei Chu", "Zeyu Cui", "Zhenru Zhang", "Zhihao Fan"], "abstract": "This report introduces the Qwen2 series, the latest addition to our large lan-\nguage models and large multimodal models. We release a comprehensive suite of\nfoundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n\nThe flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as\na base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains\n9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,\nQwen2 demonstrates robust multilingual capabilities, proficient in approximately\n30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Rus-\nsian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility\nand global reach.\n\nTo foster community innovation and accessibility, we have made the Qwen2 model\nweights openly available on Hugging Face\u00b9 and ModelScope\u00b2, and the supplemen-\ntary materials including example code on GitHub\u00b3. These platforms also include\nresources for quantization, fine-tuning, and deployment, facilitating a wide range\nof applications and research endeavors.", "sections": [{"title": "1 INTRODUCTION", "content": "Following the emergence of ChatGPT (OpenAI, 2022), enthusiasm for large language models\n(LLMs) has escalated globally. The release of the Llama series (Touvron et al., 2023) has further\nignited interests within the open-source community, particularly regarding GPT-level local LLMs.\nRecently, Claude-3 Opus (Anthropic, 2024) and GPT-40 (omni) (OpenAI, 2024), the updated model\nfor ChatGPT, have ascended to the pinnacle of the Chatbot Arena (Chiang et al., 2024) in quick\nsuccession. This platform is well-regarded for its human evaluations of LLMs. Moreover, Llama-\n3 (AI@Meta, 2024) has emerged as the state-of-the-art open-weight model series, narrowing the\nperformance gap with leading proprietary models and widely acknowledged as GPT-4-level. An\nincreasing number of competitive LLMs are now pursuing advancements similar to those made by the\nGPT series from OpenAI. Many of these models, including Qwen (Bai et al., 2023a), Mistral (Jiang\net al., 2023a), Gemma (Mesnard et al., 2024), etc., have been released in an open-weight manner.\n\nOver recent months, we have successively introduced the Qwen series (Bai et al., 2023a) and\nprogressed to Qwen1.5 (Qwen Team, 2024a). In the meantime, we have unveiled the vision-language\nmodel Qwen-VL (Bai et al., 2023b), and launched the audio-language model Qwen-Audio (Chu\net al., 2023). In this work, we introduce the newest addition to the Qwen family of large language\nmodels and large multimodal modles: Qwen2. Qwen2 is a series of LLMs, grounded in the\nTransformer architecture (Vaswani et al., 2017), trained using next-token prediction. The model\nseries encompasses foundational, i.e., base language models, pre-trained but unaligned to human\npreferences, and instruction-tuned models, fine-tuned with single-turn and multi-turn instruction-\nfollowing datasets suitable for chat and agent purposes. Our release comprises four dense models\nwith parameter counts of 0.5 billion, 1.5 billion, 7 billion, and 72 billion, plus a Mixture-of-Experts\n(MoE) model with 57 billion parameters, of which 14 billion are activated for each token. The smaller\nmodels, specifically Qwen2-0.5B and Qwen2-1.5B, are designed for easy deployment on portable\ndevices such as smartphones, earphones, and smart glasses. Conversely, the larger models cater to\ndeployment across GPUs of varying scales.\n\nAll models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens,\ncovering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2\nincludes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathe-\nmatics content. This enrichment is hypothesized to improve reasoning abilities of LLMs. Regarding\npost-training, all models underwent supervised fine-tuning and direct preference optimization (DPO,\nRafailov et al., 2023), aligning them with human preferences through learning from human feedback.\nThis process endows the models with the capability to follow instructions effectively.\n\nWe have conducted a thorough evaluation of Qwen2, alongside a selection of baseline models includ-\ning both open-weight and proprietary models accessible via API. Qwen2 outperforms competing\nmodels in evaluations of both fundamental language capabilities and instruction-tuned functionalities\nSpecifically, Qwen2-72B-Instruct, our instruction-tuned variant, scores 9.1 on MT-Bench (Zheng\net al., 2023), 48.1 on Arena-Hard (Chiang et al., 2024), and 35.7 on LiveCodeBench (Jain et al.,\n2024). Meanwhile, Qwen2-72B, the base language model, achieves 84.2 on MMLU (Hendrycks\net al., 2021a), 37.9 on GPQA (Rein et al., 2023), 64.6 on HumanEval (Chen et al., 2021), 89.5 on\nGSM8K (Cobbe et al., 2021), and 82.4 on BBH (Suzgun et al., 2023)."}, {"title": "2 TOKENIZER & MODEL", "content": "This section introduces the tokenizer and model design of Qwen2. We detail the model architecture\nand configurations for different model sizes."}, {"title": "2.1 TOKENIZER", "content": "Following Qwen (Bai et al., 2023a), we employ the identical tokenizer based on byte-level byte-\npair encoding. Notably, this tokenizer exhibits high encoding efficiency, as evidenced by its better\ncompression rate relative to alternatives, facilitating the multilingual capabilities of Qwen2.\n\nModels of all sizes employ a common vocabulary consisting of 151,643 regular tokens and 3 control\ntokens. For more information, please refer to Bai et al. (2023a). It should be noted that, owing to\nconsiderations in distributed training, the effective size for the embeddings is larger."}, {"title": "2.2 MODEL ARCHITECTURE", "content": "The Qwen2 series fundamentally constitute large language models based on the Transformer ar-\nchitecture, featuring self-attention with causal masks (Vaswani et al., 2017). Specifically, this\nseries encompasses dense language models of 4 scales and a Mixture-of-Experts (MoE) model. We\nintroduce the specifics of the dense models before delving into the MoE model's distinctive attributes."}, {"title": "2.2.1 QWEN2 DENSE MODEL", "content": "The architecture of the Qwen2 dense models comprises multiple Transformer layers, each equipped\nwith causal attention mechanisms and feed-forward neural networks (FFNs). Key differences from\nQwen are described below:\n\nGrouped Query Attention We adopt Grouped Query Attention (GQA, Ainslie et al., 2023) instead\nof conventional multi-head attention (MHA). GQA optimizes KV cache usage during inference,\nsignificantly enhancing throughput. Detailed KV head configurations for various model sizes are\nreported in Section 2.2.3.\n\nDual Chunk Attention with YARN To expand the context window of Qwen2, we implement Dual\nChunk Attention (DCA, An et al., 2024), which segments long sequences into chunks of manageable\nlengths. If the input can be handled in a chunk, DCA produces the same result as the original\nattention. Otherwise, DCA facilitates effective capture of relative positional information between\ntokens within and across chunks, thereby improving long context performance. Moreover, we also\nemploy YARN (Peng et al., 2023) to rescale the attention weights for better length extrapolation.\n\nMoreover, we follow Qwen with the usage of SwiGLU (Dauphin et al., 2017) for activation, Rotary\nPositional Embeddings (RoPE, Su et al., 2024) for positional embedding, QKV bias (Su, 2023) for\nattention, RMSNorm (Jiang et al., 2023b) and pre-normalization for training stability."}, {"title": "2.2.2 QWEN2 MIXTURE-OF-EXPERTS MODEL", "content": "The architecture of Qwen2 MoE models closely mirrors that of Qwen1.5-MoE-A2.7B (Qwen Team,\n2024c). As a substitute for the original FFN, the MoE FFN consists of n individual FFNs, each serving\nas an expert. Each token is directed to a specific expert $E_i$ for computation based on probabilities\nassigned by a gated network G:\n\n$p = \\text{softmax} (G(x))$,\n(1)\n\n$y = \\sum_{i \\in \\text{top}(p)} E_i(x)$.\n(2)\n\nIn the following, we present critical design considerations of Qwen2 MoE.\n\nExpert Granularity The key structural difference between MoE models and dense models is\nthat MoE layers incorporate multiple FFNs, each serving as an individual expert. Consequently,\none straightforward strategy to transition from a dense architecture to an MoE architecture is to set\nthe parameters of each expert equal to those of a single FFN from the original dense model. For\nexample, transitioning from Mistral-7B (Jiang et al., 2023a) to Mixtral 8x7B (Jiang et al., 2024),\ninvolves activating one of the eight experts at a time. Differently, our model employs fine-grained\nexperts (Dai et al., 2024), creating smaller-scale experts while activating a greater number of experts\nsimultaneously. Given an equal total number of expert parameters and activated parameters, fine-\ngrained experts offer a richer set of expert combinations. By leveraging these fine-grained experts,\nQwen2 MoE facilitates more diverse and dynamic expert utilization, thereby enhancing overall\nperformance and adaptability.\n\nExpert Routing The design of expert routing mechanisms is crucial for enhancing the performance\nof MoE models. Recently, there has been a notable trend towards integrating both shared and\nrouting-specific experts within MoE layers (Rajbhandari et al., 2022; Dai et al., 2024). We adopt this\napproach, as it facilitates the application of shared experts across various tasks while reserving others\nfor selective use in specific routing scenarios. The introduction of shared and specialized experts\noffers a more adaptable and efficient method for developing MoE routing mechanisms."}, {"title": "2.2.3 MODEL CONFIGURATION", "content": "In the following, we provide the key configuration and information for the Qwen2 series.\n\nThe Qwen2 series consists of models of 5 sizes, which are Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B,\nQwen2-57B-A14B, and Qwen2-72B. Table 1 lists the hyper-parameters and important information,\ne.g., the number of pre-trained tokens. Particularly, Qwen2-57B-A14B is upscaled from Qwen2-7B.\nNotably, Qwen2 models demonstrate a substantially lower Key-Value (KV) size per token relative\nto Qwen1.5 models. This characteristic translates into a reduced memory footprint, particularly\nadvantageous in long-context inference tasks."}, {"title": "3 PRE-TRAINING", "content": "In the pre-training of Qwen2, our efforts were focused on refining the dataset and investigating\nmethods to handle extended context lengths effectively."}, {"title": "3.1 PRE-TRAINING DATA", "content": "The pre-training of the Qwen2 models involves the development of a new, large-scale, high-quality\nmultilingual dataset. This dataset represents an improvement over the corpora used in previous\nQwen and Qwen1.5 models (Bai et al., 2023a; Qwen Team, 2024a), enhancing the scale, quality, and\ndiversity of the pre-training data in several key areas:\n\nQuality Enhancement The filtering algorithm has been refined with additional heuristic and model-\nbased methods, including the use of the Qwen models to filter out low-quality data. Moreover, these\nmodels are utilized to synthesize high-quality pre-training data."}, {"title": "3.2 LONG-CONTEXT TRAINING", "content": "To enhance the long-context capability of Qwen2, we augmented the context length from 4,096 tokens\nto 32,768 tokens during the concluding phase of pre-training. This expansion was complemented by\nthe introduction of a significantly increased volume of high-quality, lengthy data. In conjunction with\nthese enhancements, we modified the base frequency of ROPE from 10,000 to 1,000,000 to optimize\nperformance in long-context scenarios (Xiong et al., 2023).\n\nTo fully leverage the model's length extrapolation potential, we adopted the YARN mechanism (Peng\net al., 2023) and the Dual Chunk Attention mechanism (An et al., 2024). These strategies enable\nthe model to process sequences of up to 131,072 tokens while maintaining high performance, as\nevidenced by minimal perplexity degradation in preliminary experiments."}, {"title": "4 POST-TRAINING", "content": "Following extensive large-scale pre-training, we engage in a post-training phase for Qwen2. This\nprocess is pivotal in enhancing its proficiency across a broad spectrum of domains, including coding,\nmathematics, logical reasoning, instruction following, and multilingual comprehension. Moreover,\nit ensures that the generation from the models is in harmony with human values, making it helpful,\nhonest, and harmless. Unlike traditional methods that heavily rely on extensive human supervision,\nour approach focuses on scalable alignment with minimal human annotation (Cao et al., 2024).\nSpecifically, we investigate methods to acquire high-quality demonstration and preference data for\nSupervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), aiming\nto minimize the need for human labeling while maximizing the quality and reliability of the data."}, {"title": "4.1 POST-TRAINING DATA", "content": "The post-training data primarily consists of two components: demonstration data $D = \\{(x_i, Y_i)\\}$ and\npreference data $P = \\{(x_i, y_i^+, y_i^-)\\}$, where $x_i$ represents the instruction, $Y_i$ represents a satisfactory\nresponse, and $y_i^+$ and $y_i^-$ are two responses to $x_i$, with $y_i^+$ being the preferred choice over $y_i^-$. The\nset D is utilized in SFT, whereas P is employed in RLHF.\n\nThe construction of training data entails a two-step process: collaborative data annotation and\nautomated data synthesis. First, we extract the data ontology from large-scale instruction corpora,\nleading to a broad and diverse set of high-quality instructions. These instructions are systematically\nenhanced to incorporate greater complexity. Through human annotation, we obtain the target response\n$Y_i$ and their positive and negative counterparts ($y_i^+, y_i^-$). Subsequently, a variety of automated"}, {"title": "4.1.1 COLLABORATIVE DATA ANNOTATION", "content": "Automatic Ontology Extraction The process initiates with the application of InsTag (Lu et al.,\n2024c), an open-set fine-grained tagger, to extract the underlying ontology from a large-scale\ninstruction dataset. Subsequent manual refinement ensures the accuracy of the extracted ontology.\n\nInstruction Selection Each instruction, with tags annotated, is evaluated for tag diversity, semantic\nrichness, complexity, and intent completeness. Based on these criteria, we select a set of representative\ninstructions (Dong et al., 2023).\n\nInstruction Evolution To enrich the instruction dataset, a self-evolution strategy (Zhao et al., 2024)\nis employed, prompting the Qwen models to add constraints or requirements to existing instructions,\nthereby increasing their complexity and ensuring a diverse range of difficulty levels within the dataset.\n\nHuman Annotation Multiple responses to an instruction are obtained using diverse generation\nstrategies and Qwen models of different scales. Annotators rank these responses based on their\npreferences, ensuring the best response meets established criteria, yielding both demonstration and\npreference data."}, {"title": "4.1.2 AUTOMATED DATA SYNTHESIS", "content": "Maintaining the quality of annotations for responses to instructions presents significant challenges on\na large scale, particularly those that require expertise, experience, carefulness, or patience. To address\nthese challenges, we devised various automated alignment strategies to synthesize data at scale.\n\nRejection Sampling For mathematical or similar tasks with definitive final answers, rejection\nsampling (Yuan et al., 2023) is applied to improve the quality of solutions. Large language models\n(LLMs) are tasked to generate multiple responses, namely the reasoning paths, for each instruction.\nPaths that result in accurate conclusions and are considered reasonable by the model are preserved,\nserving as demonstration data. Preference data is generated by contrasting correct and incorrect paths.\n\nExecution Feedback For coding tasks, LLMs are employed to generate solutions and associated\ntest cases. The efficacy of these solutions is evaluated by compiling and executing them against the\ntest cases, thereby creating demonstration and preference data. This methodology is also applicable\nto assessing instruction following (Dong et al., 2024). For each instruction with constraints, e.g.,\nlength limit, the LLM is tasked to generate a Python verification function to ensure the response\naligns with the instruction requirements.\n\nData Repurposing Creating skilled responses in literary writing tasks is challenging for annotators\nwithout specialized training. To tackle this problem, we aggregate high-quality literary works\nfrom the public domain and employ LLMs to develop instructions with varying levels of detail.\nThese instructions, paired with the original works, serve as demonstration data. For example, to\ncompile roleplay data with vivid and engaging responses, we source detailed character profiles from\nknowledge repositories such as Wikipedia and instruct LLMs to generate corresponding instructions\nand responses (Lu et al., 2024b). This process, similar to a reading comprehension task, ensures that\nthe integrity of the character's profile is maintained.\n\nConstitutional Feedback Constitutional AI refers to the process of guiding LLMs to generate\nresponses based on predefined sets of principles (Bai et al., 2022). To ensure adherence to guidelines\nsuch as safety and values, a constitution dataset was compiled. This dataset delineates principles to\nbe followed and those to be avoided. It was used to instruct LLMs to produce responses that either\nare aligned with or deviated from these guidelines, serving as a reference for demonstration and\npreference data."}, {"title": "4.2 SUPERVISED FINE-TUNING", "content": "We have assembled an extensive instruction dataset featuring more than 500,000 examples that\ncover skills such as instruction following, coding, mathematics, logical reasoning, role-playing,\nmultilingualism, and safety. Our model was fine-tuned for two epochs with a sequence length of\n32,768 tokens. To optimize learning, the learning rate was gradually decreased from 7 \u00d7 10-6 to\n7 \u00d7 10-7. To address overfitting, we applied a weight decay of 0.1 and gradients were clipped at a\nmaximum value of 1.0."}, {"title": "4.3 REINFORCEMENT LEARNING FROM HUMAN FEEDBACK", "content": "Our training regime for RLHF comprises two sequential stages: offline and online training. In the\noffline training stage, we use a pre-compiled preference dataset P to maximize the difference in\nlikelihood between $y_i^+$ and $y_i^-$ with Direct Preference Optimization (DPO, Rafailov et al., 2023). In\nthe online training stage, the model iteratively refines its performance in real-time, leveraging reward\nmodels for immediate feedback. Specifically, we sample multiple responses from the current policy\nmodel, and the reward model selects the most and the least preferred responses, forming preference\npairs that are used for DPO in each episode. Moreover, we employ Online Merging Optimizer (Lu\net al., 2024a) to mitigate the alignment tax, i.e., the performance degradation associated with aligning\nmodel generation with human preferences."}, {"title": "5 EVALUATION", "content": "To thoroughly assess the Qwen2 models, consisting of both base and instruction-tuned models,\nwe implement a comprehensive evaluation protocol. This protocol examines a range of compe-\ntencies, including general knowledge understanding, language comprehension, generation, coding,\nmathematics, reasoning, and additional areas of expertise. Specifically, base models are assessed\nusing established benchmark datasets for large language models (LLMs), with responses elicited\nthrough few-shot prompting, unless specified otherwise. For instruction-tuned models, in addition to\nbenchmark evaluations, we prioritize human preference assessments."}, {"title": "5.1 BASE LANGUAGE MODELS", "content": "In this section, we illustrate the evaluation of the base language models of the Qwen2 series. Specifi-\ncally, we evaluate the models on benchmark datasets for knowledge and basic capabilities and apply\nmultilingual benchmark datasets to evaluate their support of languages. As there are multiple model\nsizes, we compare them with the state-of-the-art (SOTA) models of similar or larger sizes."}, {"title": "5.1.1 CORE CAPABILITIES", "content": "Benchmarks and Evaluation Protocol The common practice of evaluating the core capabilities\nof base language models is the implementation of benchmark dataset evaluation with few-shot or\nzero-shot prompting. The evaluation mainly focuses on the model performance of natural language\nunderstanding, general question answering, coding, mathematics, scientific knowledge, reasoning, etc.\nThe datasets for evaluation include MMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang\net al., 2024) (5-shot), GPQA (Rein et al., 2023) (5shot), Theorem QA (Chen et al., 2023a) (5-shot),\nBBH (Suzgun et al., 2023) (3-shot), HellaSwag (Zellers et al., 2019) (10-shot), Winogrande (Sak-\naguchi et al., 2021) (5-shot), TruthfulQA (Lin et al., 2022a) (0-shot), ARC-C (Clark et al., 2018)\n(25-shot), HumanEval (Chen et al., 2021) (0-shot), MBPP (Austin et al., 2021) (0-shot), EvalPlus(Liu\net al., 2023a) (0-shot), MultiPL-E (Cassano et al., 2023) (0-shot on Python, C++, Java, PHP, Type-\nScript, C#, Bash, and JavaScript), GSM8K (Cobbe et al., 2021) (5-shot), MATH (Hendrycks et al.,\n2021b) (4-shot), C-Eval (Huang et al., 2023) (5-shot), and CMMLU (Li et al., 2023) (5-shot). Multi-\nlingual datasets can be grouped into four categories: (a) Exam: M3Exam (5-shot, we only choose\nexamples that require no image), IndoMMLU (Koto et al., 2023) (3-shot), ruMMLU (Fenogenova\net al., 2024) (5-shot), and translated MMLU (Chen et al., 2023b) (5-shot on Arabic, Spanish, French,\nPortuguese, German, Italian, Japanese, and Korean); (b) Understanding: BELEBELE (Bandarkar\net al., 2023) (5-shot), XCOPA (Ponti et al., 2020) (5-shot), XWinograd (Muennighoff et al., 2023)\n(5-shot), XStoryCloze (Lin et al., 2022b) (0-shot) and PAWS-X (Yang et al., 2019) (5-shot); (c)"}, {"title": "5.2 INSTRUCTION-TUNED MODEL", "content": "To critically evaluate instruction-tuned models, we implement a multifaceted approach. Assessments\nof foundational skills and human preferences are conducted using open datasets and benchmarks. Our\ndetailed in-house examinations further probe model competencies in key areas. A particular focus is\nplaced on assessing long context capability. Safety measures include multilingual safety assessments\nand red teaming exercises. The following sections detail the evaluation methods and their outcomes."}, {"title": "5.2.1 OPEN BENCHMARK EVALUATION", "content": "To comprehensively evaluate the quality of instruction-tuned models, we compile automatic and\nhuman evaluation to assess the capabilities and human preference. For the evaluation of basic\ncapabilities, we apply similar datasets in the pre-trained model evaluation, which target on natural\nlanguage understanding, coding, mathematics, and reasoning. Specifically, we evaluate on MMLU,\nMMLU-Pro, GPQA, and Theorem QA for language understanding and knowledge, HumanEval,\nMBPP, MultiPL-E, and LiveCodeBench v1 (Jain et al., 2024) for coding, GSM8K and MATH for\nmathematics. Additionally, we assess the performance of human preference alignment and instruction\nfollowing by evaluating on benchmarks including MT-Bench (Zheng et al., 2023), Arena-Hard (Li\net al., 2024), AlignBench (Liu et al., 2023b), MixEval (Ni et al., 2024) whose results approximate\nthose of Chatbot Arena, and IFEval (Zhou et al., 2023) for instruction following.\n\nQwen2-72B-Instruct We compare Qwen2-72B-Instruct against the instruction-tuned models in-\ncluding Mixtral-8x22B-Instruct, Llama-3-70B-Instruct, as well as Qwen1.5-72B-Chat. The results are\npresented in Table 6. It can be found that a strong base language model can help boost the downstream\nperformance of the instruction-tuned model. Specifically, Qwen2-72B-Instruct outshines its peers in\nareas such as language understanding, coding, and mathematics, with the exception of GPQA and\nMBPP. Regarding human preference alignment and instruction following, Qwen2-72B has significant\nadvantages over the baselines. We assume this achievement is attributed to both the high-quality\npre-trained model and improvements in both data and training techniques for post-training."}, {"title": "5.2.2 IN-HOUSE AUTOMATIC EVALUATION", "content": "Despite a number of open benchmark datasets for the evaluation, we believe that it is far from\nsufficient to fully comprehend the capabilities of LLMs. Specifically, we have made a series of\nin-house datasets that assess different capabilities of the models, e.g., knowledge understanding,\ntext generation, coding, etc. The evaluation is in Chinese and English. The results are gathered in\nTable 10 and Table 11, respectively.\n\nChinese Evaluation For the evaluations in Chinese, we focus on comparing the performance of\nQwen2 models with the Qwen1.5 counterparts. For the small models, Qwen2-1.5B-Instruct generally\noutperforms Qwen1.5-1.8B-Chat in almost all the evaluations even with fewer parameters. In terms of\nthe comparison of 7B models, the advantages of Qwen2 are more significant. Noteworthy is Qwen2-\n72B's superior performance to Qwen1.5-110B-Chat, despite the latter's greatly more parameters.\nThe MoE model displays superior performance across most domains relative to Qwen1.5-32B-Chat,\nexcluding knowledge understanding. This discrepancy may be attributed to a short of pre-training\ntokens. In the near future, we are about to continue the pre-training of the MoE model to discover its\nscaling behaviors.\n\nEnglish Evaluation For English, we compare Qwen2 with both Qwen1.5 and Llama-3. Similarly,\nthe small models of Qwen2 significantly outcompete the Qwen1.5 counterparts. However, in com-\nparison with Llama-3-70B, Qwen2-72B-Instruct is falling behind by small margins especially in\ncomprehension and coding. We assume both the amount of English tokens for pre-training and the\nquantity and diversity of data for post-training lead to the performance gap in English."}, {"title": "5.2.3 LONG CONTEXT CAPABILITIES", "content": "Three methods to evaluate long context capabilities are employed: the Needle in a Haystack (NIAH,\nKamradt, 2023), NeedleBench (OpenCompass Contributors, 2023), and LV-Eval (Yuan et al., 2024).\n\nNeedle in a Haystack This experiment assesses a model's proficiency in pinpointing facts within\nvoluminous texts. Texts with 8K, 16K, 128K tokens in length were crafted, with facts strategically\npositioned at varying depths. Each depth interval, e.g., from 0% to 10%, encompassed two instances.\nFor contexts over 32K, YARN (Peng et al., 2023) was applied in this evaluation. As illustrated in\nFigure 1, Qwen2-72B-Instruct exhibits exceptional accuracy in retrieving information from the entire\n128K context. Coupled with its inherent strength, this model emerges as the optimal choice for\nprocessing extensive texts, assuming sufficient resources are accessible. Additionally, models within\nthe same series showcases remarkable performance across different context lengths. Precisely, Qwen2-\n7B-Instruct achieves a high level of accuracy in handling contexts up to 128K tokens. Meanwhile,\nQwen2-57B-A14B-Instruct manages contexts up to 64K tokens proficiently, and the two smaller\nmodels in the Qwen2 series could support contexts of 32K tokens."}, {"title": "5.2.4 MULTILINGUAL EVALUATION", "content": "For the multilingual evaluation, we implement a comprehensive human evaluation for the assessment\nof multilingual capabilities. Specifically, we design diverse test cases assessing different capabilities\nof large language models, and we have test cases that are in a number of languages. For the annotators,\nwe invite one professional annotator for each language who majors in the language for the evaluation.\nFor each test case, the annotator grades the response from model with a score from 1 to 5.\n\nWe report the results of our model and the baselines in the evaluation of different languages. From\nTable 13, it can be found that on average Qwen2-72B-Instruct significantly outperforms GPT-3.5-\nTurbo and it is competitive with GPT-4-Turbo and slightly falls behind Claude-3-Opus. This shows\nthat our multilingual pre-training and instruction tuning data contribute to the multilingual capabilities\nof Qwen2-72B-Instruct and it is competitive with most state-of-the-art proprietary LLMs."}, {"title": "5.2.5 SAFETY & RESPONSIBILITY", "content": "LLMs with openly accessible weights effectively accelerate the development of the research as well\nas their applications. Moreover, we believe that it is crucial to build safe and responsible LLMs so\nthat the effect of the misuse of AI technologies could be significantly alleviated.\n\nWe implement a multilingual"}]}