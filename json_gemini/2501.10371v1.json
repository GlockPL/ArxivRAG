{"title": "What we learned while automating bias detection in AI hiring systems for compliance with NYC Local Law 144", "authors": ["Gemma Galdon Clavell", "Rub\u00e9n Gonz\u00e1lez Sendino"], "abstract": "Since July 5, 2023, New York City's Local Law 144 requires employers to conduct independent bias audits for any automated employment decision tools (AEDTS) used in hiring processes. The law outlines a minimum set of bias tests that AI developers and implementers must perform to ensure compliance. Over the past few months, we have collected and analyzed audits conducted under this law, identified best practices, and developed a software tool to streamline employer compliance. Our tool, ITACA_144, tailors our broader bias auditing framework to meet the specific requirements of Local Law 144. While automating these legal mandates, we identified several critical challenges that merit attention to ensure AI bias regulations and audit methodologies are both effective and practical. This document presents the insights gained from automating compliance with NYC Local Law 144. It aims to support other cities and states in crafting similar legislation while addressing the limitations of the NYC framework. The discussion focuses on key areas including data requirements, demographic inclusiveness, impact ratios, effective bias, metrics, and data reliability.", "sections": [{"title": "1 Introduction", "content": "At Eticas.ai we have been conducting bias audits and other assessments of AI systems for years. Recently, we have begun to automate our solutions around a software platform, ITACA_OS, which offers bias measurement, benchmarking and analytics for Al systems deployed in any field.\nWith the passing of NYC Local Law 144 in 2021, which prohibits \"employers and employment agencies from using an automated employment decision tool unless the tool has been subject to a bias audit within one year of the use of the tool's, we developed a software platform to facilitate bias measurement following the specifications of the law. Our Local Law 144 compliance solution is a \"lite\" version of our main bias auditing SaaS, ITACA_OS, called ITACA_1441.\nOur use of software and AI to automate compliance has allowed us to achieve two major break-throughs:\n\u2022 To facilitate and encourage bias auditing by offering a compliance solution at a fraction of the cost of manual auditing services.\n\u2022 To turn a compliance effort and cost into a product optimization solution that can identify and minimize error rates and boost system accuracy.\nWe designed our ITACA_OS solution as a comprehensive, sector-agnostic bias measurement and auditing platform that allows our clients to identify bias in direct and indirect identifiers, measure"}, {"title": "2 Learnings & Discussion", "content": "drift and error rates for several established attributes throughout the AI system, and monitor outlier performance both in training, production and impact data. ITACA_OS provides a robust bias mea-surement and identification tool that allows technical and compliance teams to access insights into model performance and build bias prevention in model production."}, {"title": "2.1 Data requirements", "content": "The current text for Local Law 144 does not specify data requirements. Companies auditing their systems could use any dataset, from anywhere, and for any time range. We have observed some population breakdowns in published audits that lead us to believe that the data being used may not be from NYC, for instance.\nWe recommend that future iterations of the law include specific data requirements that specify, at least, that:\n\u2022 The dataset used for auditing should only include historical data from the last 12 months alone.\n\u2022 The dataset used for auditing should include data from NYC-relevant hiring processes\nThese specifications would reduce deployment and temporal bias and ensure that all organizations use comparable data that can be measured against relevant benchmarks and metrics."}, {"title": "2.2 Demographic inclusiveness", "content": "In its current formulation, the Law systematically excludes American Indian, Alaska Native, Native Hawaiian, Pacific Islander, Two or more race, and Some Other Races communities, and intersec-tional groups.\nThe law states that: \"An independent auditor may exclude a category that represents less than 2% of the data being used for the bias audit from the required calculations for impact ratio.\" Based on this provision, auditors are excluding categories with a representation below 2% by justifying that such representation is insufficient for meaningful statistical analysis.\nWe recommend the removal of the 2% rule, and to add further clarity to the definition of \"Some Other Race\"."}, {"title": "2.3 Impact ratio vs. Fairness", "content": "In the last few years, many developers and researchers have suggested different model fairness metrics to identify and measure bias in AI models. Law 144 requires AI hiring system developers and implementors to calculate only one: impact ratio, which focuses on measuring differences in selection rates between protected categories. We understand this choice, because IR is a ratio that can be applied in production environments without requiring true labels. However, this metric alone is both limited and misleading if the overall goal is to assess whether a system is fair or unfair.\nEqual Impact Ratios between groups do not necessarily guarantee fairness. Proportional outcomes indicate alignment in selection rates, but they do not confirm unbiased treatment of individuals or that the model accurately evaluates qualifications or characteristics.\nTo thoroughly evaluate a model's behavior, a deeper analysis is essential. This includes examining whether proxy features or differential behavior based on inherent group characteristics exist. For instance, even if race is not explicitly used in a model, attributes correlated with race, such as spoken"}, {"title": "2.4 Effective bias", "content": "We'd also like to highlight how limited model metrics are to assess whether an AI system is fair or not. Collecting only model metrics and not impact outcomes is like analyzing one ingredient to assess whether a final dish is safe and tasty. It can't be done. In hiring, the AI model and its metrics are only one piece in a long and complex process of decision-making, which typically includes a screening process, assessments or tests, technical interviews, behavioral interviews, and manager interviews. In our work as auditors, we have seen how technical decisions made before the AI model intervenes, and human decisions made after the AI model produces an output, can have significant impacts on the final outcome of an algorithmic process. Model metrics are always blind to the structural discrimination and bias they inherit in the training and input data, and any data curation taking place. They are also blind to whatever comes after: a model may tell us who received a high ranking, but not who was offered a job, or who accepted it. This limited vision means that any assessment of bias and fairness at the model level will be partial and never able to capture effective bias, nor point developers to what best practices to incorporate to improve outcomes.\nIn our audits using our full suite, ITACA_OS, we capture data on outlier performance for each at-tribute in pre-processing, in-processing and post-processing, measuring the impact of both technical and human interventions, but also how societal dynamics are inherited by AI models. This allows us to measure bias from different angles and in different moments and scenarios, ensuring compliance but also that developers have insight on where corrections and mitigation efforts should focus.\nDocumentation and transparency requirements for AI hiring developers and implementors should span throughout the life-cycle of the AI system. Without addressing potential biases at every stage, the overall fairness of the hiring system may be compromised, regardless of the quality of the AEDT system."}, {"title": "2.5 Metrics", "content": "We'd finally like to point to the need for Al policies to work on and enforce metrics. Law 144 relies on an established metric, the 80/20 rule, to assess whether a model is performing within an"}, {"title": "2.6 Data Reliability", "content": "Finally, we want to highlight a common challenge in AI auditing, inspection, and assessment: the reliance on data provided by auditees. In our experience, organizations seeking AI audits today demonstrate a strong commitment to compliance and accountability. However, to prevent the emer-gence of bad practices, mechanisms must be in place to make dishonest reporting difficult. One effective approach would be for regulators requiring independent AI audits to commit to conducting full, in-depth evaluations of a small percentage of audited systems. These evaluations would involve executing the system alongside developers and auditors to verify the provided data and assess coun-terfactual fairness. Such random sampling and oversight, a practice widely used in other sectors, would help ensure that the data submitted for auditing reflects the actual system in operation. This added layer of scrutiny would serve as a deterrent to bad practices and strengthen the credibility of the audit results."}, {"title": "3 Conclussions", "content": "As an organization committed to furthering of the AI auditing profession and its methods, we wel-come Local Law 144 and any efforts made by public and private actors to ensure that AI systems are safe and fair. Local Law 144 is an important step toward the standardization of independent AI audits as a crucial inspection and accountability mechanism for the AI systems that shape the life chances of millions of people in NYC, the US, and globally. It is also a key precedent that will in-spire similar regulatory developments in other US states and elsewhere. As independent AI auditors, at Eticas.ai we are encouraged by these developments and a growing recognition of the need for AI auditing solutions and methods.\nHowever, as we have oftentimes seen the best intentions of regulators turn into inadequate require-ments for compliance, we hope that sharing our insights can help develop better bias measurement standards. Measuring the wrong things helps no one: it forces AI developers to invest in compliance exercises that do not build more robust systems, it makes policy-makers focus on accountability exercises that don't really protect users and leaves citizens and society as a whole vulnerable to the negative impacts of AI innovation.\nAs we continue to develop software solutions, benchmarks and metrics to audit AI systems, we aim to shape a field that is robust, useful and focused on achieving meaningful results in terms of AI safety, fairness and accountability."}]}