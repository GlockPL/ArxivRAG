{"title": "Vision-Language Navigation with Continual Learning", "authors": ["Zhiyuan Li", "Yanfeng Lv", "Ziqin Tu", "Di Shang", "Hong Qiao"], "abstract": "Vision-language navigation (VLN) is a critical domain within embedded intelligence, requiring agents to navigate 3D environments based on natural language instructions. Traditional VLN research has focused on improving environmental understanding and decision accuracy. However, these approaches often exhibit a significant performance gap when agents are deployed in novel environments, mainly due to the limited diversity of training data. Expanding datasets to cover a broader range of environments is impractical and costly. We propose the Vision-Language Navigation with Continual Learning (VLNCL) paradigm to address this challenge. In this paradigm, agents incrementally learn new environments while retaining previously acquired knowledge. VL-NCL enables agents to maintain an environmental memory and extract relevant knowledge, allowing rapid adaptation to new environments while preserving existing information. We introduce a novel dual-loop scenario replay method (Dual-SR) inspired by brain memory replay mechanisms integrated with VLN agents. This method facilitates consolidating past experiences and enhances generalization across new tasks. By utilizing a multi-scenario memory buffer, the agent efficiently organizes and replays task memories, thereby bolstering its ability to adapt quickly to new environments and mitigating catastrophic forgetting. Our work pioneers continual learning in VLN agents, introducing a novel experimental setup and evaluation metrics. We demonstrate the effectiveness of our approach through extensive evaluations and establish a benchmark for the VLNCL paradigm. Comparative experiments with existing continual learning and VLN methods show significant improvements, achieving state-of-the-art performance in continual learning ability and highlighting the potential of our approach in enabling rapid adaptation while preserving prior knowledge.", "sections": [{"title": "Introduction", "content": "Vision-Language Navigation (VLN) (Anderson et al. 2018) is crucial for the embedding intelligence field. The agent follows natural language instructions and moves around in the 3D environment. By integrating natural language processing, visual perception, and decision-making, the agent could navigate to the destination. Most research in VLN focuses on environment understanding ability improvement (Hong et al. 2020) and accuracy of target decision policy (Hao et al. 2020). While these advancements have significantly improved VLN performance, there is still a critical issue: the generalization of agents to diverse unseen scenes, which is essential for real-world applications. In practical scenarios, agents must continually adapt to new environments while retaining the knowledge acquired from previous tasks. The significant performance gap between seen and unseen (Anderson et al. 2018) environments underscores this challenge. The primary cause of this issue is the limited availability of diverse environmental data, which constrains the agents' ability to generalize effectively (Zhang, Tan, and Bansal 2020). Yet, massively expanding the dataset with various environments is unrealistic and expensive (Shah et al. 2023). Therefore, we consider an alternative approach by introducing the continual learning (CL) framework. This framework enables the agents to incrementally learn and adapt to new environments while retaining the knowledge acquired from previous tasks (Srinivasan et al. 2022). By using this strategy, we aim to enhance the generalization capabilities of VLN agents, making them more robust and effective in real-world applications where they must navigate an ever-changing array of environments.\nTo enable the VLN agent to accumulate knowledge from tasks, handling the challenge known as catastrophic forgetting (French 1999) is vital. We combine it with vision-language navigation tasks to introduce the Vision-Language Navigation with Continual Learning (VLNCL) paradigm. The agent must continuously accumulate information and maintain former knowledge by motivating the agent with new tasks. That means a balance between stability and plasticity (Kim et al. 2023). Furthermore, considering that real-world tasks often occur within the same environment simultaneously, we split the tasks by scene to raise them to the agent. This way, tasks are divided into different domains.\nBuilding on this foundation, we propose the dual-loop scenario replay vision-language navigation agent (Dual-SR) as a novel method for VLNCL. Inspired by the mechanism of memory replay in the resting brain (Zhong, Yan, and Xie 2024), we designed a dual-loop memory replay framework to enable the model to consolidate earlier scenario memories while balancing new task learning. Randomly replaying scenario memory from the memory buffer of the agent brings a former task memory bias while the inner loop weight up-"}, {"title": "Related Work", "content": "Vision-and-Language Navigation (VLN) aims to develop in-telligent agents capable of interacting with humans using natural language, perceiving the environment, and executing real-world tasks. This field has attracted significant attention across natural language processing, computer vision, robotics, and machine learning. Anderson et al. (Anderson et al. 2018) laid the foundation by introducing the Room-to-Room (R2R) dataset, where agents navigate virtual indoor environments using a simulator. Building on R2R, researchers quickly established other VLN benchmarks like R4R (Jain et al. 2019) and RxR (Ku et al. 2020).\nFor outdoor navigation, Touchdown (Chen et al. 2019) is a crucial benchmark where agents navigate a simulated New York City street view. Conversational navigation tasks, such as CVDN (Thomason et al. 2020) and HANNA (Nguyen and Daum\u00e9 III 2019), enable agents to interact with humans to aid navigation. Remote object navigation tasks, including REVERIE (Qi et al. 2020) and SOON (Zhu et al. 2021), require agents to infer object locations and identify them based on language instructions.\nTo address cross-modal alignment, PTA (Landi et al. 2021) leverages CNN-based visual features, while HAMT (Chen et al. 2021) incorporates long-term history into multimodal decision-making using a hierarchical vision trans-former. Kerm (Li et al. 2023) models relationships between scenes, objects, and directions using graphs. For navigation decisions, Dreamwalker (Wang et al. 2023) employs reinforcement learning with an intrinsic reward for cross-modal matching. Transformer-based models (Zhao et al. 2022) has also gained popularity, integrating visual and linguistic information to enhance decision-making.\nDespite these advances, introducing new environments requires retraining on seen environments to prevent catastrophic forgetting, leading to high costs and limited adaptability. Thus, enabling VLN agents with continual learning capabilities is crucial."}, {"title": "Continual Learning", "content": "Continual learning is vital for agents to acquire new tasks without forgetting prior ones. Compared with traditional machine learning, which relies on static datasets, continual learning processes sequential data streams, making earlier data inaccessible. First introduced by Thrun (Thrun 1998), recent research has focused on mitigating catastrophic forgetting (McCloskey and Cohen 1989). Approaches in this field generally fall into three categories: rehearsal-based, regularization-based, and parameter isolation methods.\nRehearsal-based methods, like experience replay, retain data from previous tasks to update the model during new task training (Chaudhry et al. 2019). Techniques by Lopez-Paz (Lopez-Paz and Ranzato 2017) and Chaudhry (Chaudhry et al. 2018) use data replay to prevent gradient conflicts. While combined with meta-learning and experience replay, these methods help optimize network features and reduce conflicts between tasks (Le et al. 2019; Ho et al. 2023). Capacity expansion approaches (Liu et al. 2022; Ramesh and Chaudhari 2021) and online meta-learning (Gupta, Yadav, and Paull 2020) offer additional strategies to mitigate forgetting.\nIn the VLN domain, continual learning research is still developing. Jeong et al. (Jeong et al. 2024) recently proposed rehearsal-based methods like \"PerpR\" and \"ESR\" to enhance robots' adaptability in new environments, showing promise on specific datasets. However, these methods face memory and computational efficiency challenges, especially with complex, long-sequence tasks. Further refinement of the continual learning framework and evaluation metrics is needed."}, {"title": "Method", "content": "Vision-language navigation (VLN) involves an agent navigating a visual environment following natural language instructions. This task requires the agent to comprehend and integrate visual and linguistic inputs to achieve the goal. Let V = {V1, V2, ..., vn } represent the sequence of visual observations from the environment and I represent the natural language instruction. The objective is to learn a policy \u03c0(\u03b1t | V,I), where at is the action taken at time t, which maximizes the cumulative reward R = \u2211t=1 rt and mini-\nmize the loss\n$L = E_{(I,A^*)\\in D}[L(\\pi_{\\rho}(a_t | V, I), A^*)]$\t(1)\nwhere rt being the reward at time t, A* is the ground truth trajectory, @ is parameters, and T the total number of time steps. The integration of vision and language occurs by projecting I and V into a common feature space, creating a joint embedding space. This embedding allows the agent to align visual cues with linguistic references, enabling accurate navigation."}, {"title": "Formulation of Vision-Language Navigation with Continual Learning", "content": "Considering the reality application scenario, we take the validation dataset as a continual learning dataset and split it into several data streams. In real-world applications, the agent might navigate a scene for several tasks. Thus, we split the dataset by scene and divided the scene with the same ID into one data stream as one task domain, which agents learn sequentially. During this process, the agent needs to learn current tasks without forgetting knowledge of former tasks. This approach is more comparable to the realistic situations of agents in the real world.\nThe dataset is separated into d task domains S =\n{S1, S2, ..., Sd}, each containing tasks in the same scene. Task domains are independent of each other. Therefore, we can identify each task domain s as one distribution Ds. The definition of the loss function of VLNCL agents is\narg $min_{\\theta}$ $\\sum_{i=1}^d E_{(I,A^*)\\in D_{s_i}} [L(\\pi_{\\theta} (a_t | V, I), A^*)]$(2)\nwhere parameters @ are iterated in each task domain. The agent can learn multiple task domains by optimizing the parameters in sequence. The main challenge of it is how to balance each task domain and promote generalization."}, {"title": "Dual-loop Scenario Replay", "content": "In the VLNCL setup, agents must minimize forgetting and improve transfer learning by leveraging prior knowledge to improve performance on current and former tasks. However, most existing VLN agents struggle with continual learning, unlike humans, who efficiently learn from a few examples by integrating sensory input with long-term memory (Goelet et al. 1986). The human brain continuously extracts and stores knowledge, reinforcing long-term memory through replay during rest (Dewar et al. 2012). Inspired by this, we propose the Dual-loop Scenario Replay Continual Learning (Dual-SR) algorithm for VLN agents. This algorithm simulates working memory as an inner loop and long-term memory as an outer loop, creating two weight update loops to balance prior and current task information while enhancing generalization.\nIn the VLNCL setup, agents might encounter an overfitting problem due to the limited samples. Hence, we leverage the meta-update mechanism in the Reptile algorithm (Nichol, Achiam, and Schulman 2018) to mimic long-term memory formation. The reptile algorithm is equivalent in effectiveness to MAML (Genzel et al. 2015), which provides a means for models to acquire a standard structure from the current task domain, enabling them to quickly adapt to other similar new tasks. Thus, we can maximize generalization ability rather than data fitting. By that, the outer loop can improve the generalization capacity of agents. In the outer loop, the update of weight can be defined as:\n$\\theta = \\theta + \\beta * (\\theta' - \\theta)$       (3)\nwhere \u03b8, \u03b8', and \u1e9e separately denote weights of the model before the inner cycle, weights after it, and meta-learning rate.\nTo mimic the brain's abstraction and consolidation of long-term memories (Goelet et al. 1986), we designed the inner loop by simulating the memory retrieval in working memory. The agent can train with stable data selected randomly from previous task domains by maintaining the buffer and replaying old samples in the inner loop. Applying the memory buffer can ensure each former task is equally likely to be selected in the buffer. The newly received samples are combined with randomly selected old samples from the buffer to form a mini-batch, which is then used for meta-learning. In the inner loop, the update of the model can be defined as:\n$U_k(\\theta) = \\theta - \\alpha \\circ \\nabla L_{(a_t|V,I)}(\\theta)$        (4)\nwhere Uk (0) is a update by learning (at | V, I) and a are parameters of the meta-learner to be learned, and denotes element-wise product. Specifically, a is a vector of the same size as @ that decides both the update direction and learning rates.\nMeta-updates can extract common structures learned across tasks, thereby enhancing the knowledge transfer capabilities of agents. The VLNCL setup exposes agents to a dynamic and unpredictable data stream. This procedure necessitates that agents adapt and perform effectively across an evolving array of tasks without following a predefined sequence. This approach differs significantly from traditional methods that require partitioning a fixed dataset into multiple batches for a set number of tasks. To address this, we implement experience replay within the inner loop. By storing task indices, the agent can revisit and leverage previously learned tasks when faced with new ones.\nThis approach contrasts with traditional replay-based methods, which indiscriminately use memory across all tasks. Our method randomly replays scenarios within each task domain to ensure balance. Additionally, we introduce the memory buffer size Z. When a task belongs to a previous task domain and the task ID t is a multiple of Z, the agent updates the memory buffer M by replacing one of the tasks in the corresponding domain with the current task. The agent efficiently manages memory size by updating scenario memory between task domains, even when handling many tasks. This strategy also encourages the model to prioritize tasks inspired by working memory principles."}, {"title": "Structured Transformer VLN Agents with Continual Learning", "content": "Building upon the Dual-SR algorithm, we employ a cross-modal structured transformer as the planner to enhance the performance of the VLN agent in continual learning settings. The Dual-SR algorithm provides the foundation for this approach by balancing integrating new information and retaining prior knowledge. At each navigation step t, the model processes five forms of tokens, global token gt-1, candidate target tokens C = {c0, c1,..., cq}, history tokens H = {h\u22121, h\u22121, ..., h=1}, encoded instruction tokens I = {i0, i1, ..., im }, and encoded vision tokens V = {v1, v2, ..., vr}. The instruction tokens remain constant through time to reduce computation, and other tokens are updated based on previous time steps. The system initializes the global token as the sentence embedding g0 = i0.\nTo encode candidate target tokens, we apply the grid coding form to address the modeling of possible long-term target challenges in unseen scenes. Each cell center can represent a potential navigation target token by discretizing the environment into a d x d grid to cover the navigation area. Initially, candidate target tokens c0, c1, ..., cq are created using the positional embedding of the targets, formulated as:\nc0 = fop (sj).io, j\u2208 {1, 2, . . ., q} (5)\nwhere fp is the positional encoder, si is the spatial location expressed by position coordinates, op is the parameter of encoder, and x0 is the sentence embedding.\nDuring navigation, these candidate target tokens are refined with new visual clues and instruction tokens to predict a more precise long-term target. The probability of each target being the navigation destination is calculated using a multi-layer perceptron (MLP) based target predictor:\nP(c|0) = softmax{MLP(cgt)},i\u2208 {1,2,...,q} (6)\nwhere gt is the global token.\nThe agent constructs and maintains a structured representation of the explored area with the transformer architecture to capture the structured environment layouts. At time step t, the model constructs a graph St, where the nodes represent previously visited locations and the edges represent the navigability of those locations. We construct the history token h using panoramic view embedding, action embedding, temporal embedding, and positional embedding as follows:\nh = fv(v, ..., v\u00ef) + fa(rt) + fr(t) + fp(st) (7)\nwhere fv is a panoramic visual feature extractor, rt = (sin 0, cos 0, sin 4, cos) is the moving direction, fa is the action encoder, fr is the temporal encoder, and fp is the positional encoder.\nThe adjacency matrix E of history tokens at time step t is defined such that if a navigation viewpoint n; is navigable from ni, then Eij = 1; otherwise, Eij = 0. The attention mask matrix M controls the information flow among tokens, with a sub-matrix MH for history tokens:\nMH = MH * E (8)\nwhere * denotes element-wise multiplication.\nThe structured transformer enables the agent to access structured information of the past, allowing decisions from adjacent and previously visited locations. The local action space at time step t is:\nAt = {T(v0), T(v1),...,T(vt)} (9)\nand the global action space is:\nAF = {T(v1),...,T(vt),T(h1),...,T(h\u00b9)} (10)\nwhere T maps the token to its corresponding location. The probability of each possible action is:\n\u03c0(\u03b1t | \u03b8) = softmax{MLP(r\u00af\u00b9(at) \u00b7 gt)}, at \u2208 AF (11)\nThe optimization of the model involves both imitation learning (IL) loss LIL and reinforcement learning (RL) loss LRL, alternating between teacher forcing (using ground truth actions) and student forcing (using actions sampled from the policy). To further consider the action chosen and target chosen, the history teacher loss LHT and target prediction loss Ly are incorporated. The history teacher loss is defined as:\n$L_{HT} =  \\sum_{t=1}^{T}log \\pi(a_t | \\theta)$  (12)\nand the target prediction loss is:\n$L_{T} =  \\sum_{t=1}^{T}log P(c | \\theta)$    (13)\nwhere the i-th target token is closest to the navigation destination. The total loss function is given by:\nL = \u03b1\u2081LIL + A2LRL + A3LHT + Q4LT (14)\nwhere ai are the loss coefficients.\nAfter training the foundational model, the agent prompts continual learning inference within validation environments. The agent processes the task-domain-based data stream sequentially for the Val-Seen and Val-Unseen splits. The agent executes the inner loop based on the loss function specified in Equation 4 to iteratively update parameters 0, achieved through continuous memory buffer updates and scenario replays. Upon completing the learning for the current task domain, the agent proceeds to perform the outer loop as described in Equation 3.\nThe continual learning methodology equips the VLN agent to learn and adapt within complex environments, maintaining and enhancing knowledge across multiple tasks. The Dual-SR algorithm in structured transformers allows for effective navigation and adaptation capabilities in continually changing scenarios."}, {"title": "Experiments", "content": "The experiment adopts the VLNCL framework and divides the R2R dataset (Anderson et al. 2018) into distinct task domains to evaluate resistance to forgetting and knowledge transfer capabilities. By sequentially inputting each task domain into the agent, we separately assess the average Seen Transfer (ST) and Unseen Transfer (UT) across each dataset split.\nTo assess the forgetting resistance and knowledge transfer abilities of the VLNCL agent, we introduce two metrics: Seen Transfer (ST) and Unseen Transfer (UT), analogous to Backward and Forward Transfer in continual learning. ST measures the average performance difference on the i-th task domain after training on T domains compared to training on"}, {"title": "Implementation Details", "content": "The agent's hyperparameters and the structured transformer architecture align with those utilized in previous studies. The candidate target grid size, d, is set to 5, resulting in a 5 \u00d7 5 grid with a spacing of 6 meters between adjacent positions. The agent follows the ground-truth path when applying teacher force under specific loss configurations. Conversely, actions are sampled from the predicted probability distribution during student forcing, leveraging distinct loss configurations. We train the base model using the Adam optimizer on two NVIDIA V100 GPUs for 100,000 iterations, employing a batch size 20 and a learning rate of 1 \u00d7 10\u22125 over 72 GPU hours. Continual learning is subsequently conducted on two NVIDIA V100 GPUs for 1k iterations per task domain with learnable learning rates."}, {"title": "Comparative Experiment Results", "content": "To evaluate the performance of our continual learning method in vision-language navigation tasks, we conducted a comparative experiment against previous works using the R2R dataset. We adopted the latest VLN agents with continual learning capabilities, adhering to the experimental settings and metrics outlined in ESR (Jeong et al. 2024), which evaluate the average performance across all seen tasks throughout the continual learning process. Additionally, we compared our framework with the ASA continual learning strategy (Chi et al. 2020), which fine-tunes the agent using new data. However, distinguished from the original ASA, our experiment employed augmented data rather than interaction data collected from human-agent interactions. Agents used a randomly initialized HAMT (Chen et al. 2021) as the backbone model and set the agent's buffer size to 2000 tasks.\nDue to GPU memory constraints, the agent stored only each task's index and environment ID. The baseline for the experiment was the agent that did not employ any continual learning algorithm.\nThe comparative results demonstrate that our method significantly enhances the task performance of the agent. The success rate increased by 16%, and the oracle success rate increased by 8% compared to the base agent, highlighting"}, {"title": "Resisting Forgetting and Transferring Evaluation", "content": "With continual learning, we introduce the concepts of Seen Transfer and Unseen Transfer for the VLN agent to evaluate its resistance to forgetting and ability to transfer knowledge. To assess forgetting resistance, we apply the average Seen Transfer to the validation train split and the validation seen split. We utilize the average Unseen Transfer on the validation unseen split to evaluate the agent's capability to transfer knowledge. As a baseline, we use the results of fine-tuning each task domain.\nThe Seen Transfer measures the agent's performance on the seen task domain set, Steen, within the current split, determining whether the agent retains its prior knowledge. In contrast, Unseen Transfer evaluates performance on the unseen task domain set, Sunseen, within the current split, assessing knowledge transfer from previous tasks to current ones.\n$ST = \\frac{1}{T-1} \\sum_{i=1}^{T-1}SR^T(s_i) - SR_i(s_i)      s_i \\in S^{seen}_{Val}$ (15)\nThe Unseen Transfer is defined as\n$UT = \\frac{1}{T-1} \\sum_{i=2}^{T}SR^{T-1}(s_i) - SR_0(s_i)      s_i \\in S^{unseen}_{Val}$ (16)\nwhere the SRj(si) is the average success rate in the task domain si for agent after trained on the j task domain. The SRo(si) represents the success rate of the base agent.\nThe experimental results demonstrated that our method has advantages in forgetting resistance and knowledge transfer. To further elucidate these capabilities, we present the changes in success rates across different splits in Figure 3. In the Val Unseen split, we evaluate the success rate in the unseen portion to assess knowledge transfer ability. Conversely, in the Val Seen and Train Seen splits, we consider the success rate in the seen portion to evaluate the agent's forgetting resistance.\nAdditionally, we can track the highest and lowest Success Rates (SR) and Oracle Success Rates (OSR) across all dataset splits in Table 5. The evaluation of performance also follows the VLNCL setup. This result allows us to observe the changes in performance more clearly.\nAnalyzing performance changes shows that our method exhibits strong resistance to forgetting and robust knowledge transfer capabilities. Our method consistently improves performance on unseen tasks in the Val Unseen split, highlighting the significant generalization capability introduced by continual learning in VLN agents. In the Val Seen and"}, {"title": "Conclusion", "content": "This paper presents the Vision-Language Navigation with Continual Learning (VLNCL) paradigm, where the agent learns from unseen tasks while retaining knowledge from prior scenarios, closely reflecting real-world application demands. To achieve this, we introduce the Dual-loop Scenario Replay (Dual-SR) algorithm, which improves the agent's generalization and task performance. We also establish a benchmark for VLNCL using the R2R dataset. Experiments demonstrate that our method surpasses existing continual learning approaches under comparable conditions, advancing VLN agent performance and setting the stage for further research into real-world application-ready agents. Our future work will concentrate on developing more sophisticated inference mechanisms and advancing continual learning strategies to improve generalization."}]}