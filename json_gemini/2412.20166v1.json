{"title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System", "authors": ["Hyucksung Kwon", "Kyungmo Koo", "Janghyeon Kim", "Woongkyu Lee", "Minjae Lee", "Hyungdeok Lee", "Yousub Jung", "Jaehan Park", "Yosub Song", "Byeongsu Yang", "Haerang Choi", "Guhyun Kim", "Jongsoon Won", "Woojae Shin", "Changhyun Kim", "Gyeongcheol Shin", "Yongkee Kwon", "Ilkon Kim", "Euicheol Lim", "John Kim", "Jungwook Choi"], "abstract": "The expansion of large language models (LLMs) with hundreds of billions of parameters presents significant challenges to computational resources, particularly data movement and memory bandwidth. Long-context LLMs, which process sequences of tens of thousands of tokens, further increase the demand on the memory system as the complexity in attention layers and key-value cache sizes is proportional to the context length. Processing-in-Memory (PIM) maximizes memory bandwidth by moving compute to the data and can address the memory bandwidth challenges; however, PIM is not necessarily scalable to accelerate long-context LLM because of limited per-module memory capacity and the inflexibility of fixed-functional unit PIM architecture and static memory management. In this work, we propose LoL-PIM-a multi-node PIM architecture that accelerates long context LLM through hardware-software co-design. In particular, we propose how pipeline parallelism can be exploited across a multi-PIM module while a direct PIM access (DPA) controller (or DMA for PIM) is proposed that enables dynamic PIM memory management and results in efficient PIM utilization across a diverse range of context length. We developed an MLIR-based compiler for LOL-PIM- a commercial PIM-based compiler where the software modifications were implemented and evaluated, while the hardware changes were modeled in the simulator. Our evaluations demonstrate that LoL-PIM significantly improves throughput and reduces latency for long-context LLM inference, outperforming both multi-GPU and GPU-PIM systems (up to 8.54\u00d7 and 16.0\u00d7 speedup, respectively), thereby enabling more efficient deployment of LLMs in real-world applications.", "sections": [{"title": "INTRODUCTION", "content": "The rapid advancement of large language models (LLMs) has transformed fields ranging from natural language processing to intelligent agents by enabling the generation of contextually relevant responses across diverse applications [11, 25, 53, 61, 63]. In particular, long-context LLMs, capable of maintaining coherence across tens of thousands of tokens, have significantly enhanced contextual relevance in various tasks. For instance, long document summarization [73] generates cohesive summaries from dispersed information across different sections of extensive text, while repository-level code analysis [45] extends programming assistants' capabilities to analyze entire codebases comprising thousands of lines. Furthermore, chain-of-thought (COT) reasoning has recently improved answer quality by leveraging multi-step contextual reasoning [52, 67]. These advancements rely on inference-time processing of extended contexts, with recent LLMs supporting long-context handling capabilities ranging from 16K to 200K tokens [7, 11, 48, 52].\nThere have been many recent works on accelerating LLM inference, but most have focused on LLM inferences with shorter context (i.e., up to 4 or 8k) [5, 21, 69, 72]. As the context length increases, both the memory capacity and the memory bandwidth become a greater bottleneck during LLM inference. In particular, the memory capacity for the key-value cache(KV-cache), which is needed during Attention computation, increases linearly with the input context length. Since Attention relies on general matrix-vector multiplication (GEMV), the compute density (operations per byte) of LLM decoding is reduced and decoding becomes memory bandwidth bound. To provide sufficient memory capacity and bandwidth, multi-GPUs are commonly used but results in underutilization of GPU compute.\nTo address the memory bandwidth challenges, DRAM-based Processing-in-Memory (PIM) technology offers a promising solution by embedding computational capabilities directly within the memory [6, 10, 19, 32, 38, 40, 49, 54]. Fixed-functional unit PIM systems, such as AiMX [18, 32, 34, 35, 41], are particularly well-suited for accelerating GEMV in LLM inference by exploiting the high internal memory bandwidth within DRAM modules. While PIM can accelerate GEMV, leveraging PIM for long-context LLM decoding presents new challenges, including the following.\n(1) Memory capacity: Individual PIM chip memory capacity is limited to a few GBs and is problematic for large KV-caches that require up to hundreds of GBs for long context.\n(2) Static PIM memory management: Long-context LLM have a diverse set of input context lengths and a fixed (or static) memory management results in inefficient usage of PIM memory and reduces overall throughput.\n(3) I/O buffer bottleneck: Non-square, aspect ratio of KV-cache, caused by long context inputs, creates a new PIM bottleneck in the I/O buffers.\nRecent work [19, 54, 57] exploit PIM and heterogeneous architectures (e.g., GPU-PIM or NPU-PIM) to accelerate LLM inference; however, they focus on short-context LLM (under 2k tokens) and do not address the challenges of long-context LLM. In particular, prior work often have assumed fixed context length when serving multiple requests; however, making the same assumption for long-context"}, {"title": "BACKGROUND", "content": "Long-context LLM Decoding\nLong-context LLM Inference. Long-context LLMs excel at maintaining coherence over tens of thousands of tokens, significantly enhancing contextual relevance across various tasks e.g., long document summarization [73] generates cohesive summaries from information spread across different sections of extensive text, and repository-level code analysis [45] extends programming assistants' capabilities to analyze entire codebases comprising thousands of lines. Chain-of-thought (CoT) reasoning also improves answer quality by leveraging multi-step contextual reasoning [52, 67]. These advancements rely on inference-time processing of extended contexts, with modern LLMs supporting token lengths ranging from 16K to 200K [7, 11, 48, 52].\nDecoding Computation. Long-context large language models (LLMs) are constructed based on the Transformer decoder [64]. As illustrated in Fig. 1, each decoder of $n_l$ layers comprises Multi-Head Attention (MHA) and Feed-Forward Network (FFN). Each head h of MHA starts with the Query/Key/Value generation (QKV-Gen) that generates vectors $\\{Q, K, V\\} \\in R^{d_h}$, where K and V are stacked to KV-cache $\\in R^{t\\times d_h}$ for t tokens. It is followed by the score generation (QKT), which is then scaled and softmaxed ($S = softmax(\\frac{QKT}{\\sqrt{d_h}})$) to generate the scaled-dot-product attention (SV). Then, SV concatenated across $n_h$ heads is projected (Proj) to become the MHA output. FFN consists of two consecutive fully-connected (FC) layers, FFN1 and FFN2, with an activation function in between for non-linear transformation. In sum, the Transformer decoder computation can be categorized into two types: FC-layers (QKV-Gen, Proj, FFN1, FFN2) that involve weight matrix, and attention-layers (QKT and SV) that involve KV-cache."}, {"title": "Multi-node Domain-specific PIM System", "content": "DRAM-based Domain-specific PIM. DRAM-based Processing-in-Memory (PIM) technology addresses memory-bound challenges by integrating computational capabilities directly within the memory subsystem [6, 10, 19, 32, 38, 40, 49, 54]. Fixed functional unit PIM systems are optimized for accelerating LLM inference through GEMV operations. These PIM feature processing units adjacent to DRAM banks for parallel dot-product computations, pre-generated PIM commands for simplified control, and limited conditional logic to minimize area overhead [18, 32, 34, 35, 41]. For example, AiMX [18, 32, 34, 35, 41] includes a vector multiply-accumulate unit in each DRAM bank for 16-element dot-product computations, a shared 2KB global buffer for input data, and a minimal pair of 2-byte output registers per processing unit to transfer results off-chip for module-level processing (e.g., Softmax) using Extra Processing Unit (Fig. 2(a)). Additionally, all channels share a broadcasting input data path to minimize resource usage.\nMulti-node PIM System. Fig. 2 (a) illustrates the LLM inference on a multi-node system. The system consists of a host CPU connected with multiple nodes, each consisting of a compute-intensive xPU (i.e., GPU or NPU) connected with multiple PIM modules that provide extensive internal memory bandwidth. As motivated by prior works [19, 54, 57], xPU takes responsibility for the compute-intensive context summarization phase (prefill), while PIM modules handle the memory-bandwidth/capacity-demanding token generation phase (decoding). PIM Controller supports program execution for LLM inference compiled for model-parallel serving. Tensor-parallel (TP) processing [59] performs head-wise partitionining of MHA and FFN computations to minimize synchronization, while pipelined parallelism (PP) [22, 46] groups the layers and assigns a node to each layer-group for batch-wise parallel processing. As an example, Fig. 2(b) illustrates a parallel execution of four inference requests in a batch are partitioned over four PIM modules (i.e., (TP, PP) = (2, 2)), being dynamically managed as the request (= Request1) with the end-of-sequence (EOS) token is replaced with a new request (= Request5).\nExecution Flow. Executing LLM inference on PIM hardware involves generating PIM commands (Table 3) to manage tasks such as matrix multiplications and attention mechanisms. Fig. 2(c) illustrates the execution flow, where the PIM module interacts with DRAM to store KV caches and directly compute dot-products. These PIM commands incorporate key configuration details such as layers, attention heads, and batch size. However, since the number of tokens during inference is unknown at compile time, PIM commands must be pre-generated for all potential token ranges up to the maximum size, and memory must be pre-allocated accordingly."}, {"title": "MOTIVATION", "content": "Characteristics of Long-context LLM\nLong-context LLM inference presents significant challenges due to the memory-intensive demands of Attention layers, particularly during token generation for batches of tens to hundreds of requests in the decoding phase. To analyze workload characteristics, we evaluate a long-context LLM (Qwen1.5-7B) on an A100-80GB GPU with input context lengths ($L_{in}$) ranging from 4K to 32K tokens. Weak scaling is used \u2013 thus, as the number of GPUs is scaled to ensure sufficient memory capacity, the batch size is maximized based on total GPU memory. The key observations are as follows:\n\u2022 KV-cache Memory Dominance: The KV-cache consumes a far greater portion of memory compared to weight parameters, as its size is proportional to the context length. Each token generation requires a prefilled KV-cache for Attention computations, leading to significant memory usage as the context length increases.\n\u2022 Attention Bottleneck: The growing computational demands of Attention become the major bottleneck at longer context lengths. Attention computations rely on general matrix-vector multiplication (GEMV), which reduces compute density (operations per byte) and makes decoding heavily reliant on memory bandwidth. While batch processing enhances throughput for fully connected (FC) layers, which use general matrix-matrix multiplication (GEMM), it exacerbates bandwidth limitations for Attention layers."}, {"title": "Challenges of PIM Scaling", "content": "In this subsection, we highlight the challenges of scaling fixed-functional unit PIM architecture for long-context LLM decoding. First, bank-parallel dot-product efficiency of PIM relies on matrix partitioning, but KV-caches may exceed the capacity of a single PIM module. Increasing the number of PIM modules to accommodate longer contexts raises TP, distorting the partitioned subsection's aspect ratio and incurring frequent input/output transfers that lead to I/O transfer overhead. Fig. 4(a) illustrates the scaling of context length and node count. In GPU systems, throughput suffers from low compute utilization caused by memory bottlenecks. While PIM improves throughput in LLM decoding, its effectiveness diminishes as TP-only parallelization limits scaling. This work identifies inefficiencies in PP from existing partitioning approaches and introduces a new strategy that enhances bank-parallel computation by enabling PP-friendly partitioning (Sec. 4).\nSecond, varying context lengths in decoding requests require dynamic PIM memory management, but fixed-functional PIM units have limited conditional control to maximize compute density. Commercial PIM with fixed-functional units are managed by pre-generated commands with embedded operand addresses, forcing memory allocation based on the maximum context length regardless of actual context length, leading to underutilized memory and constrained batch sizes. Fig. 4(b) demonstrates slow batch size growth in PIM systems. To address this, we propose a dynamic memory management method using lightweight dynamic control commands for lazy memory allocation, significantly increasing average batch size. As shown in Fig. 4(b), the proposed method achieves near-ideal batch sizes (Sec. 5).\nThird, the KV-cache's structure suffers from distortion due to low $d_h$, while the aspect ratio of weight parameters becomes increasingly"}, {"title": "SCALABLE PIM ARCHITECTURE", "content": "PIM-aware Partitioning\nLLM decoding workloads rely on weight parameter matrices and the KV-cache. Workload partitioning ensures that the dimensions of the matrix are distributed across PIM modules, with each subsection fitting within the memory capacity of the module: $N_{ch} \\times N_{bank} \\times D_{row} \\times N_{row}$. When partitioning LLM workloads across multiple PIM modules, two primary parallelization methods are used: Pipeline Parallelization (PP) [22, 46], which groups LLM layers into PP pipeline stages, and Tensor Parallelization (TP) [59], which divides matrix dimensions within each group by TP. While both approaches are widely applied in LLM inference systems, TP is more commonly preferred in multi-node PIM systems. A widely used strategy in prior works, called head-first allocation (HFA) [19, 54, 57], is summarized below:\n\u2022 Partitioning Weight Parameters. In FC layers, weight parameters are typically partitioned by TP: $d_{out}$ for QKV and FFN1, and $d_{in}$ for Proj and FFN2, as per Tensor Parallelization [59].\n\u2022 Partitioning KV-cache. In Attention computation, the dimensions $n_h$ and B are independent and parallelized by TP, ensuring each request's per-head KV-cache resides within a single channel [19, 54].\nIncreasing the number of PIM modules to support longer contexts raises TP, but this approach introduces inefficiencies: 1) In FC layers, excessive TP distorts the partitioned subsection's aspect ratio, causing frequent input/output transfers and resulting in I/O overhead. 2) In Attention layers, increasing TP reduces channel occupancy, leading to bank underutilization."}, {"title": "Challenges for PIM-based Pipeline Parallelization", "content": "As the number of PIM modules increases, PP becomes essential to address inefficiencies in bandwidth utilization caused by overly aggressive TP. However, using PP with the HFA partitioning method presents a challenging performance trade-off. As shown in Fig. 5(b), each PIM module processes a micro-batch ($B_{\\mu}$) of the KV-cache during Attention computation. For a given batch size B, determined by the memory capacity for static max-context-length allocation, the number of batch processing steps is $B/B_{\\mu}$.\nPipeline bubbles, caused by host-PIM synchronization, grow as $B_{\\mu}$ increases, leading to significant overhead. Conversely, smaller $B_{\\mu}$ values result in fewer banks being used for parallel processing, causing underutilization. This trade-off has made PP less favorable in multi-PIM parallelization, as noted in [19]."}, {"title": "Intra-module Token-parallel Partitioning", "content": "Achieving balanced channel utilization requires minimizing batch dimension partitioning during KV-cache allocation. To address this, we propose a new strategy called intra-module token-parallel partitioning (ITPP). As shown in Fig. 5(c), this approach prioritizes the partitioning of output dimensions over the batch dimension, allocating the KV-cache based on their respective output dimensions: the token dimension for the Key cache and the head dimension for the Value cache.\nITPP differs from prior strategies in two key ways. First, it allocates the token dimension for bank-level parallelization, leveraging the abundant token dimension in long-context LLM workloads to avoid bank underutilization. Unlike HFA, which over-partitions the batch dimension and suffers from PP trade-offs, ITPP ensures more efficient utilization. Second, ITPP aggregates token-parallel outputs of QKT within a module, enabling the Extra Processing Unit (EPU) in the PIM Controller Hub to compute the Softmax aggregation."}, {"title": "DYNAMIC PIM MEMORY MANAGEMENT", "content": "Managing memory in long-context LLM decoding is challenging due to variability in context lengths. Although GPU-based approaches [13] have implemented dynamic memory allocation for such variability, PIM's static control capabilities limit its adaptability. To overcome this, we introduce the Direct PIM Access (DPA)"}, {"title": "Managing KV-cache in PIM", "content": "PIM accelerates Attention computations by managing the KV-cache within its memory modules. Recent studies [19, 54, 57] have used static memory allocation for the KV-cache, based on the maximum token length, and pre-generated PIM commands with fixed physical addresses (e.g., DOT-PROD(row,col) in Table 3). However, PIM lacks flow control instructions, requiring the compiler to pre-generate commands for all possible token lengths and select the appropriate one at runtime. This approach results in inefficient static memory allocation and token-length-agnostic command generation. Pre-generated commands with fixed operand indices cannot adapt to varying KV-cache sizes encountered during LLM decoding. To address these issues, we make two key observations:\n\u2022 Repetitive Compute Patterns: The computation sequence for Attention operations (e.g., QKT, SV) follows a repetitive pattern across layers and tokens. As shown in Fig. 6(a), the sequence consists of identical PIM commands, differing only in the number of repetitions (e.g., repeated DOT-PROD and RD-OUT commands as token length increases). This suggests the possibility of"}, {"title": "Direct PIM Access (DPA) Commands", "content": "Table 4 outlines the DPA commands designed to enhance the flexibility of PIM commands. These commands encapsulate repetitive token operations, enabling runtime execution without requiring re-compilation. The Dyn-Loop command encodes a loop structure, with Loop-Bound (LB) specifying the number of repetitions and Loop-Entry (LE) defining the number of commands to repeat. These parameters are determined dynamically based on the token length"}, {"title": "On-module PIM Command Dispatcher", "content": "To support DPA, we enhance the Instruction Sequencer within the PIM Controller (Fig. 2) to enable runtime dispatching of PIM commands. The on-module dispatcher manages dynamic command generation and memory allocation locally within the PIM module. Fig. 6(c) illustrates its components and operation, which include a Configuration Buffer, a Virtual-to-Physical Address (Va2Pa) Table, a Command Buffer, and a Decoding Unit. The total buffer size required for the dispatcher is minimal, less than 200KB(Table 5, which is significantly smaller than the 512KB GPR capacity in typical PIM Control Hubs [70], ensuring efficient integration. The Configuration Buffer stores decoding information such as the total number of layers, the current layer ID, request ID, and the token index (Tcur). The Va2Pa table and the Command Buffer are initialized with the virtual-to-physical address mapping of the KV-cache and the DPA-encoded PIM command stack, respectively. The host updates the Configuration Buffer each iteration with current decoding information, such as Tcur. Upon completing a request, the allocated memory chunk is released by updating the Va2Pa table, while the new request ID and associated Tcur are received. The Decoding Unit uses the Configuration Buffer, Command Buffer, and Va2Pa table to dispatch the corresponding PIM command sequence."}, {"title": "Use Case: Lazy Memory Allocation", "content": "Integrating the proposed DPA and on-module dispatcher enables lazy memory allocation, which dynamically adjusts PIM memory capacity based on requests' context lengths. The KV-cache increases with each iteration and eventually exceeds the pre-allocated chunk capacity. At this point, the Va2Pa table is updated to allocate a new chunk to accommodate the additional KV-cache. As shown in Fig. 6(d), when Request-1 requires more capacity, the host allocates a new chunk and updates the Va2Pa table. Notably, the new chunk does not need to be adjacent to the previous one, allowing on-demand allocation in non-contiguous memory regions and improving memory utilization. Lazy memory allocation also increases the batch size for pipeline parallelization. The host determines the batch size for each iteration of pipeline processing. Under static memory allocation, requests must be assigned memory sufficient for the maximum context length, which limits the number of requests that can be processed simultaneously, even when current requests have smaller context lengths. In contrast, lazy memory allocation uses a smaller number of non-contiguous chunks to handle requests with small KV-cache sizes, significantly"}, {"title": "PIM I/O BOTTLENECK", "content": "I/O Bottleneck Analysis. While our mapping strategy improves LoL-PIM's efficiency for LLM inference, it doesn't address the I/O data transfer overhead inherent to it. LoL-PIM's buffers are significantly smaller than those in recent DRAM-PIM architectures [19, 54], limiting input and output data reuse. Fig. 7(a) reveals that data transfer latency for Out-Reg (DT-Out) and GB (DT-GB) accounts for over 50% of total latency in QKT and SV operations. This highlights the need for architectural improvements in LoL-PIM to reduce input and output data transfer overhead.\nI/O-aware buffering for Overcoming I/O Bottleneck. We propose I/O-aware buffering, strategy for LoL-PIM to address data transfer bottlenecks. It uses two buffer sets: one for current dot-product operations and another for preparing for the next, hiding transfer cycles within the computation. Fig. 7(b) illustrates I/O-aware buffering applied to input and output transfers via Global Buffer (GB) and Out-Reg. This approach maintains LoL-PIM's main computation datapath with minimal hardware changes: extended control logic (a pingpong-bit for alternating buffers), additional muxes for buffer selection, and an extra GB shared across banks. I/O-aware buffering leverages existing Out-Reg sets, suiting PIM's constrained environment.\nEvaluation. To evaluate the proposed method, we modified LoL-PIM's control path in Ramulator [30], implementing a dual command queue and pingpong-bit mechanism. This allows intermittent execution blocking and bit reversal for subsequent commands. The MAC units' datapath remains unchanged, preserving critical path elements. The main overhead comes from the controller's pingpong-bit-based command execution and an extra GB. Area estimation using CACTI and Synopsys Design Compiler with SAED 32nm technology shows only a 1.7% increase for a 256-MAC unit. Fig. 7(a) demonstrates I/O-aware buffering's significant latency reductions: 40% in QKT (overlapping DT-Out with DOT-PROD(MAC) latency), 44% in SV"}, {"title": "LOL-PIM SYSTEM ARCHITECTURE", "content": "We implement LoL-PIM as a software-hardware codesign to enable the proposed ideas of PIM-aware partitioning, dynamic memory management, and I/O aware buffering to enhance PIM's capability for long-context LLM decoding. We extend the existing machine learning compiler framework to integrate PIM-specific compilation flow and code generation for diverse variants of Transformer decoding workloads. Also, we augment AiMX's microarchitecture to enable on-module command dispatching and I/O aware buffering."}, {"title": "Multi-PIM Compiler and Runtime", "content": "We extend a popular compiler framework, MLIR [37], to enable seamless mapping of various LLM operations to the PIM command stack. MLIR's multi-level abstractions, ranging from high-level tensor computations (e.g., linalg.batch_matmul) to hardware-centric operations (e.g., pim.wr_inp), facilitate customized optimizations and transformations for multi-PIM architectures. A runtime launches the compiled PIM command stacks on the multi-PIM environment. We extend IREE [62] and its hardware abstraction layer (HAL) to integrate PIM's software development kit (SDK) for memory allocation and command dispatching in end-to-end LLM inference. Compiling Transformer Variants. Fig. 8 illustrates LoL-PIM's compilation workflow. The Lowering Pass starts with the processing of Transformer decoder operations represented in the StableHLO dialect [60] and translating them into lower-level MLIR dialects like linalg and arith. Mapping Transformer operations of varying sizes and structures is a key challenge. To address this, we implement a Pattern-Matching Pass that extends existing IR traversal to identify sequences of operation types matching predefined decoder patterns like GQA and SwiGLU. In the case of GQA, for example, we detect mismatched vector dimensions between Q and K, V. Upon pattern matching, the compiler generates Execution Table specifying the Transformer operation type, partitioning direction, and communication strategy for each operation."}, {"title": "Hardware Overhead.", "content": "To construct the LoL-PIM hardware, a on-module dispatcher is introduced to the HUB to support dynamic memory management, and a I/O-aware buffer is applied to AiM to overcome the I/O bottleneck. To evaluate the overhead of these two hardware components, we use the PIM architecture from [70] as the baseline, with the AiM architecture based on information from [31]. The baseline PIM architecture consists of 2 HUBs and a total of 16 AiMs [70]. To model our LoL-PIM, the on-module dispatcher and I/O-aware buffer are applied to the HUB and AiM, respectively, in the baseline PIM. Since the most dominant hardware component in this PIM architecture is the AiM, the overhead of the I/O-aware buffer is modeled as a percentage of the hardware area of a single AiM chip, while the overhead of the on-module dispatcher is modeled as a percentage of the area corresponding to eight AiMs.\nThe area of a single AiM, which consists of 16 PUs and 16 bank cells, is approximately 12.16 mm\u00b2 [31]. Firstly, for the overhead of the I/O-aware buffer, the 2KB area required for additional Global Buffer is modeled using CACTI, resulting in an area of 0.053 mm\u00b2, which corresponds to an overhead of approximately 0.4%. Secondly, the on-module dispatcher requires a Va2Pa Table, CMD Buffer, configuration buffer, and a decoding unit. The area for the decoding unit is modeled using Synopsys Design Compiler under SAED 32nm technology, while the buffers are modeled using CACTI, resulting in a total area of 0.48 mm\u00b2. Since one on-module dispatcher is required per eight AiMs, the total overhead for the entire PIM system is approximately 0.5%."}, {"title": "EVALUATION", "content": "Evaluation Settings\nEvaluation LLMs. We evaluate LoL-PIM's compiler capability using diverse LLMs specified in Table 1, with 1.8B to 72B parameters, considering context lengths up to 32K tokens, and popular activation types (RELU, SwiGLU) implemented in AiM's LUT. We also consider three tasks with distinct context length characteristics from a representative long-context LLM benchmarks, LongBench [8]; QMSum (Summarization), HotpotQA and Musique (Multi-Document QA) (Table 2).\nBaselines. We evaluate our system against three baselines: (1) a 16-GPU DGX-A100 system, each GPU equipped with 80GB HBM3 and simulated using a DGX simulator (GPU-HBM), (2) 16 AiMX cards, each comprising 8 devices with one Xilinx Virtex UltraScale+ VU9P FPGA and eight 1GB AiM chips utilizing GDDR6 and QSFP links for inter-card communication (PIM), and (3) a GPU-only system using GDDR6 for GPU evaluation with the matched external bandwidth of the PIM system (GPU-GDDR). These baselines were simulated using validated tools, including the DGX simulator [24] and our extended version of Ramulator [30], which we validated to"}, {"title": "Performance Comparison", "content": "Scalability. We evaluate LoL-PIM's scalability across three long-context LLM benchmarks(Table 2), focusing on the Qwen1.5-7B model (Fig. 9(a)). The baseline PIM-only system showed the poorest scalability due to inefficient PIM utilization. While it outperformed the GPU-only system at a smaller capacity of 128GB, its scalability limitations led to the lowest performance at the largest capacity of 1024GB, highlighting the drawbacks of conventional PIM mapping approaches at larger scales. In contrast, LoL-PIM demonstrated significantly better scalability. At 1024GB, it outperformed the baseline GPU-GDDR and PIM-only systems by 3.53x and 4.74x, respectively, showcasing its superior efficiency. This improvement stems from two key factors: (1) pipeline parallelization, which sustains tensor parallelization and prevents PIM channel utilization drops at larger scales, and (2) larger batch sizes enabled by scaling, which reduce pipeline bubble overhead and enhance throughput.\nA similar trend was observed with the Qwen1.5-72B model (Fig. 9(b)). The baseline PIM-only system again exhibited the lowest scalability"}, {"title": "Standalone vs. Heterogeneous Systems", "content": "We further compare the scalability of standalone and the heterogeneous systems. Prior works [19, 54, 57] have suggested promising performance of the heterogeneous xPU+PIM architectures, yet they have not been carefully evaluated in the long-context LLM workloads. In Fig. 9 and Fig. 10, we compare the throughput of the standalone and heterogeneous systems for 7B and 72B models, respectively. Each system was evaluated with capacities ranging from 128GB to 1024GB, showcasing the average tokens per second (tok/sec) achieved while processing the evaluation dataset. Although showing scalability in general, GPU+LoL-PIM achieves inferior throughputs compared to the standalone systems (LoL-PIM). This disatifactory performance is due to the long-context LLM's memory-bounded characteristics; the higher throughput can be achieved with the more internal memory bandwidth. Yet, the speedup of PIM-only system over GPU+PIM is less than the internal memory bandwidth gap (2\u00d7) thanks to"}, {"title": "Ablation Study", "content": "Tensor vs. Pipeline Parallelization. LoL-PIM (\u2460) employs a token-parallel approach to maximize pipeline parallelism. However, increasing the number of pipeline stages introduces pipeline bubbles, which reduce utilization. These overheads diminish with larger batch sizes, necessitating different pipeline parallelism strategies depending on the presence of dynamic PIM memory management. Fig. 11 shows how the optimal balance of tensor and pipeline parallelism varies with and without DPA across system scales. With fixed parallelism strategies, DPA improves performance by up to 1.3x. Additionally, under the same DPA condition, performance differences between parallelism combinations can reach 1.73\u00d7. The performance gap between optimal parallelism points with and without dynamic memory management is up to 7%, underscoring the sensitivity of LOL-PIM (\u2460) to dynamic adjustments in batch size and parallelism strategies.\nLatency Breakdown. Fig. 12 highlights LoL-PIM's effectiveness compared to baseline GPU and PIM systems. For the 72B model, the large weight parameters and KV cache sizes lead to smaller"}, {"title": "RELATED WORK", "content": "Multi-Node LLM Serving. As LLMs scale, they require more memory and bandwidth, making parallelization techniques like tensor [59] and pipeline parallelization [50] essential for LLM serving. Advanced serving systems [1, 2, 9] now incorporate continuous batching [71] and efficient KV cache management [33] to improve GPU utilization. However, recent studies reveal GPU underutilization in multi-GPU LLM serving, especially during token generation, leading to approaches leveraging hardware heterogeneity [17, 55]. This paper proposes a multi-node, PIM-only LLM serving system to address GPU underutilization and presents an efficient parallelization strategy for multi-node environments.\nDRAM-Based PIM. DRAM-based Processing-in-Memory (PIM) has become a key commercial solution for accelerating computational tasks, especially for Large Language Models (LLMs). This includes general-purpose solutions like UPMEM-PIM [16, 23, 47] and domain-specific accelerators such as HBM-PIM [27-29, 36, 39] and AiM [18, 32, 41], optimized for matrix multiplication. Recent research addresses bottlenecks in homogeneous PIM environments [26, 74] and heterogeneous xPU-PIM configurations [19, 42, 54, 57, 65]. The introduction of AiMX [34, 35], combining FPGA with AiM chips, marks a significant advancement in computational support and parallel processing for LLMs, reflecting the industry's move towards more efficient, specialized hardware."}, {"title": "CONCLUSIONS", "content": "We introduce LoL-PIM, a multi-node PIM acceleration system addressing these issues. It features advanced parallelism techniques, direct PIM access commands and on-module dispatcher, and I/O-aware buffering for reducing I/O transfer overhead. Evaluations show LoL-PIM significantly enhances throughput and reduces latency for long-context LLM inference, outperforming multi-GPU and GPU-PIM systems (up to 8.54\u00d7 and 16.0x speedup, respectively), enabling more efficient LLM deployment."}]}