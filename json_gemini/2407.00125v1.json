{"title": "A Survey on Failure Analysis and Fault Injection in AI Systems", "authors": ["Guangba Yu", "Gou Tan", "Haojia Huang", "Zhenyu Zhang", "Pengfei Chen", "Roberto Natella", "Zibin Zheng"], "abstract": "The rapid advancement of Artificial Intelligence (AI) has led to its integration into various areas, especially with Large Language Models (LLMs) significantly enhancing capabilities in Artificial Intelligence Generated Content (AIGC). However, the complexity of AI systems has also exposed their vulnerabilities, necessitating robust methods for failure analysis (FA) and fault injection (FI) to ensure resilience and reliability. Despite the importance of these techniques, there lacks a comprehensive review of FA and FI methodologies in AI systems. This study fills this gap by presenting a detailed survey of existing FA and FI approaches across six layers of AI systems. We systematically analyze 160 papers and repositories to answer three research questions including (1) what are the prevalent failures in AI systems, (2) what types of faults can current FI tools simulate, (3) what gaps exist between the simulated faults and real-world failures. Our findings reveal a taxonomy of AI system failures, assess the capabilities of existing FI tools, and highlight discrepancies between real-world and simulated failures. Moreover, this survey contributes to the field by providing a framework for fault diagnosis, evaluating the state-of-the-art in FI, and identifying areas for improvement in FI techniques to enhance the resilience of AI systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial Intelligence (AI) has made significant strides over the past decade, permeating both academic and industrial areas. Large Language Models (LLMs), in particular, have proven to be a game changer, propelling AI to unprecedented heights and facilitating a myriad of applications in fields such as software engineering [141, 192, 196] and human language translation [8, 59, 85]. This evolution has led to the integration of AI models into a growing array of products, transforming them into sophisticated AI systems. Notable examples of such integration include Gemini [51], Bing [117], and ChatGPT [137], which underscore the pivotal role of AI in enhancing and expanding the capabilities of modern technology solutions.\nThe escalating complexity and ubiquity of AI systems necessitate addressing their inherent vulnerabilities and failure-related challenges. A Meta AI report [208] points out over 100 failures during the training of OPT-175B. Similarly, ChatGPT encountered 173 outages in 2023, causing a maximum user impact over 427 minutes [138]. Such failures can degrade user experience, and even incur financial losses. Hence, mitigating AI system failures is of paramount importance.\nFailure analysis (FA) and fault injection (FI) techniques are instrumental in identifying limitations and bolstering the reliability of AI systems. Researchers and practitioners alike have embarked on extensive investigations into Al system failures. Studies [22, 67, 69, 94, 100, 111, 178, 209] have analyzed AI system failures from platforms like Stack Overflow or Github, while others [45, 53, 73, 139, 176, 182, 207] have focused on failures in large-scale production AI systems. Such failure analyses enable the identification of patterns, root causes, and locations, thereby informing FI techniques. FI, a proactive approach, uncovers system weaknesses on resiliency before they become catastrophic failures. By deliberately injecting faults or abnormal conditions into systems, teams can evaluate and enhance their resilience to unexpected disruptions. Some existing FI approaches [16, 17, 58, 68, 74, 106, 164, 170] mimic faults in Al systems engineered by humans, while others [34, 91, 136, 162, 206, 210] simulate hardware errors.\nDespite the progress, a comprehensive survey on FA and FI in AI systems is conspicuously absent. Furthermore, a gap exists between FI and FA, resulting in insufficient consideration of FA outcomes when crafting FI tools. Therefore, this study presents a comprehensive survey aimed at exploring and evaluating existing research for FA and FI in AI systems. We have meticulously reviewed and analyzed 160 corresponding papers and code repositories. As shown in Fig. 1, an AI system typically comprises six layers - AI Service, AI Model, AI Framework, AI Toolkit, AI Platform, and AI Infrastructure [187]. We attempt to address three research questions at each layer as follows.\n\u2022 RQ1: What are the prevalent failures in current AI systems?\n\u2022 RQ2: What types of faults can current FI tools simulate?\n\u2022 RQ3: What gaps do exist between the simulated faults and the real-world failures?\nRQ1 aims to catalog and analyze the failures that have occurred in current AI systems. Understanding these failures is crucial for several reasons. Since it helps in identifying common vulnerabilities within AI systems, informs developers about potential areas of improvement, and contributes to the development of more reliable AI applications. RQ2 explores the capabilities of existing FI tools designed for AI systems. The ability to simulate a wide range of faults is essential for evaluating and enhancing the robustness and fault tolerance of AI systems. RQ3 investigates the gap between simulated faults and real-world AI system failures, aiming to understand the limitations of current FI tools in producing the full spectrum of potential failures. Moreover, understanding these gaps helps in improving FI tools, and ultimately contributes to develop more resilient AI systems.\nBy examining the current landscape and identifying critical research gaps, this survey provides valuable insights for researchers and practitioners working towards building reliable and resilient AI systems. This study makes the following contributions:\n\u2022 We present a comprehensive analysis and taxonomy of failures occurring at different layers of Al systems. By systematically characterizing these failures, we provide a valuable framework that can serve as a reference for failure diagnosis in AI systems.\n\u2022 We conduct an in-depth examination of the capabilities of existing FI tools across various layers of AI systems. We offer insights into the state-of-the-art in simulating and reproducing potential failures. This work provides a foundation for assessing the reliability of AI systems.\n\u2022 We explore the discrepancies between FI tools and real-world AI system failures. We identify the limitations of current FI approaches in simulating potential failure scenarios. By shedding light on these gaps, we emphasize the need for more comprehensive FI techniques in AI systems.\nThe rest of this paper is organized as follows. Section 2 provides background information on FA and FI in AI systems, followed by Section 3, which outlines our systematic literature review methodology. The subsequent sections analyze FA and FI in different layers of AI systems, including the AI service layer (Section 4), AI model layer (Section 5), AI framework layer (Section 6), AI toolkit layer (Section 7), AI platform layer (Section 8), and AI infrastructure layer (Section 9). Section 10 highlights research opportunities about FI in AI systems. The article concludes in Section 11."}, {"title": "2 BACKGROUND AND DEFINITIONS", "content": "We adopt the definitions of failures and faults proposed by previous work [7, 163]. Furthermore, we provide additional extensions and interpretations specific to AI systems.\n\u2022 Failure is defined as \"an incident that occurs when the delivered service deviates from the correct service\" [7]. In the context of AI systems, failures can manifest in various ways. For example, a failure can occur when Al services become unreachable, and when the behavior of Al services does not meet the expected outcome (e.g., generating semantically incorrect text). These failures indicate a deviation from the desired or expected behavior of the AI system.\n\u2022 Fault is the root cause of a failure. In AI systems, faults can be attributed to various sources, including algorithmic flaws, model design issues, or problems with the quality of the data used for training or inference. It is important to note that faults in AI systems may remain uncovered for some time, due to fault-tolerant approaches implemented in the system."}, {"title": "2.2 Failure Comparisons between Al and Cloud System", "content": "Failure analysis (FA) and fault injection (FI) are longstanding topics within the field of computer science, traditionally focusing on the robustness and reliability of systems. Historically, much of the literature has focused on cloud systems, reflecting their critical role in modern computing infrastructure [47, 52, 79, 93, 98, 172]. These systems adhere to a logic-based programming paradigm, where developers encode decision logic directly into the source code, facilitating a structured approach to FA and FI. In contrast, AI systems represent a paradigm shift towards a data-driven programming model. Here, developers design neural networks that derive decision-making logic from extensive datasets [22, 67, 69, 94, 100, 111, 178, 209]. This shift introduces both similarities and differences in the approach to FA between AI systems and traditional cloud systems.\nAs illustrated in Figure 2, while both AI and cloud systems are susceptible to common failures such as power disruptions and network outages, certain faults are unique or more critically impactful to Al systems. For instance, GPU failures, which might be relatively inconsequential in traditional cloud environments, can severely affect the performance and availability of AI systems. This distinction underscores the importance of conducting a comprehensive survey on FA and FI specifically for AI systems, especially in the current era dominated by \"Large Models\". This need forms one of the primary motivations behind the research presented in this article."}, {"title": "2.3 Failure Analysis and Fault Injection", "content": "Figure 3 demonstrates the relationship between faults, FA, and FI. In large-scale AI systems, faults commonly occur in Al systems due to their inherent complexity and the numerous interconnected components involved, leading to fault origination at various stages of system operation [22, 67, 69, 100, 111, 178, 209]. Once a fault is activated and a failure is detected (stage 1), engineers responsible for AI system maintenance engage in mitigating the failure based on observed behaviors (stage \u2461). During the failure mitigation process, engineers generate comprehensive incident reports encompassing failure details such as occurrence time, impact, failure manifestation, and mitigation strategies. The objective of FA is to utilize these incident reports as inputs to summarize the fault pattern (e.g., recurring type and location) (stage \u2462).\nFI is a widely adopted technique for assessing and improving the reliability and security of systems, including AI systems. It involves the deliberate introduction of faults into a system to observe its behavior and validate its fault tolerance mechanisms. FI can be applied in various forms, such as software fault injection (e.g., mutation test [76] and API interception [189, 202]) or hardware fault injection (e.g., simulating hardware failures [130] and environmental disturbances [83]).\nLeveraging the knowledge acquired from historical FA, engineers employ FI techniques to validate the reliability of AI systems. Following the injection of faults, engineers closely monitor AI system performance and behavior, ensuring the accurate identification and appropriate handling of the injected faults (stage \u2463). Based on the analysis of FI experiments, engineers can identify system vulnerabilities, weaknesses, and areas for improvement. By iteratively conducting FI experiments and refining the system based on the obtained results, the AI system can be continually improved, enhancing its reliability and effectiveness in real-world scenarios.\nConsequently, FA and FI form closely intertwined processes that significantly contribute to the assessment and enhancement of AI system reliability. The insights derived from FA guide the selection and design of FI scenarios. The iterative feedback loop established by fault analysis and FI facilitates the continuous improvement of AI systems, thereby serving as the other motivation driving the research presented in this article."}, {"title": "3 SURVEY METHODOLOGY", "content": "To systematically collect the publications for conducting this survey, we specially constructed and maintained a repository \u00b9 about FA and FI in AI systems. We first searched relevant papers in online digital libraries and extended the repository by manually inspecting all references of these papers.\nTo begin with, we searched several popular online digital libraries (e.g., IEEE Xplore, ACM Digital Library, Springer, Elsevier, and Wiley) with the following keywords: \"failure and machine learning\", \"failure and deep learning\", \"failure and AI\", \"fault injection and machine learning\", \"fault injection and deep learning\", \"fault injection and AI\", etc. We mainly focused on regular papers published relevant conferences (e.g., ICSE, FSE, ASE, SOSP, OSDI, ATC, NSDI, SC, DSN, ISSRE, AAAI, ICML, NeurIPS, etc) and journals (e.g., TSE, TOSEM, EMSE, TDSC, TPDS, TSC, etc). Upon identifying relevant papers, we conducted a recursive examination of the references, which allowed us to manually inspect each reference and code repository of these papers. This process enabled us to collect additional publications and repositories related to our survey topics.\nIn total, our recursive search methodology enabled us to collect 160 publications and code repositories about FA and FI in AI systems, spanning from the year 2001 to 2024. These publications and repositories are classified into six layers by research topics including AI service, AI model, AI framework, AI toolkit, AI platform, and AI infrastructure. Moreover, the distribution of different classes is presented in Fig.4. It is important to note that some papers may address more than one research topic. Consequently, the total number of papers and repositories in Fig.4 is larger than 160. Next, we will show the details on FA and FI in different layers of AI systems."}, {"title": "4 FAILURE ANALYSIS AND FAULT INJECTION IN AI SERVICE", "content": "In the context of AI systems, the AI service layer can be considered as the topmost layer that directly interacts with users or other systems. This layer acts as an interface or entry point for users to access and utilize the AI capabilities provided by the underlying layers of the AI system. Potential failures at this layer can include unavailability of the service or incorrect outputs, such as incorrect inference from a classifier or hallucinations from an LLM-based service. This section delves into the FA and FI in AI services"}, {"title": "4.1 Failure Analysis in Al Service", "content": "In our detailed exploration of potential failures in AI services, we have identified a broad spectrum of faults. These can be classified into several major categories including data fault, code and software fault, network transmission fault, and external attack. We summarize the types of failures in Table 1 that can occur in AI services. The paper column in Table 1 shows some representative papers in this study, as do the following tables below.\nData Fault. These faults related to the format, type, and noise of data can lead to the failure of Al services [165, 179, 211]. For example, incorrect data encoding (e.g., requesting UTF-8 but receiving ASCII) or inappropriate data types (e.g., expecting a string but receiving an integer) can prevent AI services from working properly. The JENGA framework explores the impact of data faults on predictions of machine learning models [165]. Furthermore, data drift and concept drift are common problems [179]. Data drift refers to a model trained on a specific distribution of data but then encountering a different distribution in practice. While concept drift occurs as the relationship between features and labels becomes invalid over time. Zhao et al. [211] investigate the impact of concept drift on model performance.\nDevelopment Fault. The primary reason for service failures is code quality such as bugs, logical faults in code, and poor software design [46, 63, 90]. Code faults typically originate from coding mistakes. Logical faults in code often involve incorrect algorithm implementation, impacting the accuracy and efficiency of inference. Thus, poor software design affects system performance. Additionally, failures in AI service API calls [189], originating from API incompatibility, API change, and API misuse, can lead to AI service failures.\nDeployment Fault. During the deployment of AI services, various faults can arise, including outdated models [122], path configuration errors [60], and inappropriate resource allocation [112]. These faults can impact system performance and stability. As data changes over time, model perfor- mance may degrade, necessitating regular updates and retraining to maintain their effectiveness. Path configuration faults can prevent the proper loading of models and data. Inadequate resource allocation [112], especially inefficient use of CPU and GPU resources, can lead to decreased system performance and unnecessary waste.\nNetwork Transmission Fault. It may arise from network congestion, bandwidth limitations, or failures in network hardware, leading to packet delays or losses [87]. Network congestion occurs when data traffic exceeds the network's bandwidth capacity, preventing data transmission on time. Bandwidth limitations are imposed by the maximum transmission rate of a network connection, often determined by service provider or the capabilities of network hardware. Furthermore, physical damage or configuration faults in network devices can also lead to packet delays or losses. failures in network hardware, leading to packet delays or losses [87]. Network congestion occurs when data traffic exceeds the network's bandwidth capacity, preventing data transmission on time. Bandwidth limitations are imposed by the maximum transmission rate of a network connection, often determined by service provider or the capabilities of network hardware. Furthermore, physical damage or configuration faults in network devices can also lead to packet delays or losses.\nExternal Attack. In addition to internal faults within Al services, external attacks can also lead to service failures. These include network attacks such as Distributed Denial of Service (DDoS) attacks [113] and Man-in-the-Middle (MITM) attacks [149], which can not only cause temporary service interruptions but also lead to data leakage or corruption. Moreover, adversarial attacks target the AI model by designing malicious inputs (e.g., meticulously modified images or texts) to deceive the AI into making incorrect actions [2, 4, 145]. Adversarial attacks further divide into white-box attacks and black-box attacks. In white-box attacks, the deployed model is fully understood, including inputs and architecture, allowing for targeted attacks. In black-box attacks, only the model's inputs and output labels/confidences are accessible."}, {"title": "4.2 Fault Injection in Al Service", "content": "Fault injection in AI service layer encompasses four dimensions including data, service API, network transmission, and external attack. By simulating diverse fault scenarios in these dimensions, it is possible to assess the system's robustness and reliability, thereby preventing inaccurate predictions or even system failures. Table 2 illustrates the current FI tools in the AI service layer, followed by a description of each of them.\nData Fault. Data perturbation is the most intuitive method of FI at the data dimension. Intro- ducing noise or modifying data manually can simulate uncertainties in real-world data. It can be achieved using tools such as NumPy [132], Scikit-learn [166], and so on. JENGA [165] is a frame- work that studies the impact of data faults (e.g., missing values, outliers, typing errors, and noisy) on the predictions of machine learning models. Additionally, data and concept drift can be simulated through carefully designed data disturbances. Moreover, tools such as scikit-multiflow [120] and MOA [12] enable the simulation of sudden, gradual, and incremental drifts in data streams.\nService API Fault. Istio, an open-source service mesh, addresses provides robust traffic man- agement features. Istio's fault injection capabilities are primarily exposed through its Service API, which allows users to define fault injection rules declaratively [19, 71]. These rules are specified within Istio's VirtualService resources, which are then propagated to the Envoy proxies deployed as sidecars alongside each service instance. However, deploying and managing Istio can add com- plexity to the AI infrastructure. The learning curve for Istio is steep, and managing its components alongside Al services can be challenging.\nNetwork Fault. Common network fault injections include network delays, network jitter, packet loss, and reordering. The injection of these network faults may result in the delayed or non-response of user requests for AI services. Toxiproxy [184] is a TCP proxy used to simulate network and system conditions for chaos and resilience testing. ChaosBlade [18] is an open source experimental injection tool that adheres to the principles of chaos engineering and chaos experimental models, supporting a rich array of experimental scenarios. Istio can also leverage the Envoy's advanced traffic management capabilities to simulate network conditions such as delay, packet loss, and service unavailability [72].\nExternal Attack. Adversarial attacks are deliberate attacks on the AI service, including both white-box and black-box attacks. White-box attacks leverage model structure and parameter information to strategically generate adversarial samples, such as FGSM [50], PGD [108], and DI-AA [191]. Black-box attacks lack insight into the model and rely on observing model inputs"}, {"title": "4.3 Gap between Failure Analysis and Fault Injection in Al Service", "content": "Based on the comparative analysis presented in Table 3, which outlines the capabilities of various FI tools in addressing diverse failure modalities within AI services, several critical insights regarding the discrepancies between FA and FI can be elucidated.\nIncomplete Coverage. The scrutinized FI tools, in aggregate, encompass a substantial proportion of the delineated fault types. Nonetheless, certain fault types (e.g., \"Defective Code\") remain unaddressed by any of the existing tools. This revelation underscores the exigency for subsequent research to ameliorate these deficiencies in fault simulation.\nDiversity of Tools. Each FI tool exhibits a predilection for addressing specific fault types. For instance, JENGA is attuned to data quality faults, whereas MOA is adept at handling data and concept drift faults. This diversity implies that practitioners may be compelled to deploy a suite of tools to achieve a comprehensive simulation and analysis of failures within Al services, contingent upon the specific fault types of interest.\nNew Emerging Fault Types. The incorporation of fault types such as \"Prompt Injection Attacks\" accentuates the burgeoning significance of accounting for external factors and security facets in the FA of AI services. Nevertheless, traditional FI tools (e.g., ChaosBlade) are designed for cloud computing services and do not consider the specific scenarios of AI systems. As AI services become increasingly interconnected and susceptible to a myriad of threats, it is imperative to cultivate FI tools capable of efficaciously simulating failures emanating from these nascent domains."}, {"title": "5 FAILURE ANALYSIS AND FAULT INJECTION IN AI MODEL", "content": "Al model layer is a crucial component in AI systems, residing beneath the AI service layer. This layer is responsible for managing the various AI models and algorithms that power the AI capabilities of the system. Similar to the AI service layer, potential failures at this layer can include unavailability of the model or incorrect outputs. In this section, we delve into the FA and FI in AI models."}, {"title": "5.1 Failure Analysis in Al Model", "content": "Recent research has explored multiple reasons for AI model failures. Researchers analyze various sources, including GitHub commits, Stack Overflow posts, and expert interviews. These analyses have provided crucial insights into enhancing the reliability and robustness of AI systems. For example, Humbatova et al. [67] categorize faults within deep learning systems by examining 1,059 GitHub commits and issues of AI systems, while Islam et al. [69] analyze error-prone stages, bug types, and root causes in deep learning pipelines from 2,716 Stack Overflow posts and 500 GitHub bug fix commits. Additionally, Nikanjam et al. [131] classify faults in deep reinforcement learning programs from 761 documents. We focus on the training and testing phases of AI models, where faults are categorized as data faults, model hyperparameter faults, and model structure and algorithm faults, as shown in Table 4.\nData Fault. The quality of training data is pivotal for successful model training. Alice et al. [32] conduct a systematic analysis of data quality attributes (accuracy, uniqueness, consistency, com- pleteness, and timeliness) across five software bug datasets and find that 20-71% of data labels are inaccurate, which could severely hinder model training in extreme cases. Some studies [95, 193] have examined challenges faced by data quality. Furthermore, data preprocessing and augmentation are crucial. Raw data are susceptible to noise, damage, loss, and inconsistency, thus necessitating preprocessing steps (e.g., data cleaning, integration, transformation, and reduction) to facilitate easier knowledge extraction from datasets. Data augmentation aims to expand the training dataset through specific transformations to enhance the model's generalizability. Das [33] lists ten common faults in data preprocessing, while Maharana et al. [109] discuss various data preprocessing and data augmentation techniques to enhance model performance.\nModel Hyperparameter Fault. The parameters of an Al model include both pre-training hyperparameters (e.g., the number of hidden layers, batch size, and learning rate) and post-training model parameters (e.g., network weights and biases). Basha et al. [9] examine the effects of different numbers of fully connected layers on convolutional neural networks (CNNs). Uzair et al. [188] investigate how the number of hidden layers affects the efficiency of neural networks. Short- GPT [116] points out that many layers in LLMs exhibit high redundancy, with some layers playing minimal roles. Gurnee et al. [54] utilize seven different models (ranging from 70 million to 6.9 billion parameters) to study the sparsity of activations in LLM neurons. Additionally, the learning rate (LR), epochs, and batch size (BS) influence the training speed and the performance of the trained model. He et al. [57] indicate that the ratio of batch size to learning rate should not be too large to ensure good generalization. Shafi et al. [167] explore the optimization of hyperparameters, including the learning rate, batch size, and epochs, as well as their interrelationships.\nModel Structure and Algorithm Fault. Shiri et al. [171] investigate various aspects of different models and evaluate their performance on three public datasets. Activation functions are crucial for introducing non-linearity. Dubey et al. [40] compare the performance of 18 distinct activation functions (e.g., Sigmoid, Tanh, and ReLU) on various datasets. Model training requires regularization to avoid overfitting. Tian et al. [181] compare different regularization techniques, including sparse regularization, low-rank regularization, dropout, batch normalization, and others. They discusse the selection of regularization techniques for specific tasks. The selection of optimizers significantly affects model performance and training speed. Haji et al. [55] compare various optimizers like SGD, Adam, AdaGrad, etc. They highlight the advantages and disadvantages of these optimizers in terms of training speed, convergence rate, and performance. The loss function is also essential for minimizing the discrepancy between predicted results and target values. Wang et al. [190] introduce 31 loss functions from five aspects: classification, regression, unsupervised learning of traditional machine learning, object detection, and face recognition of deep learning. Additionally, the ratio of training to validation data must be considered to ensure the model has enough data for learning while the validation data is adequate for model adjustments [124]."}, {"title": "5.2 Fault Injection in Al Model", "content": "Fault injection in AI models typically involves interfering with the training process to create models with inferior performance. This technique was called mutation testing. The concept of mutation testing that we will discuss next is analogous to fault injection. A common method for conducting mutation testing on AI models involves designing mutation operators that introduce faults into the training data or the model training program, and then analyzing the behavioral differences between the original and mutated models.\nRecent studies in mutation testing have made significant contributions. We have summarized these works as shown in Table 5. DeepMutation [106] designs 13 mutation operators that inject faults into both the training data and code of deep learning models. DeepMutation++ [62] combines DeepMutation (eight model-level operators for FNN models) and proposes nine new operators specifically for RNN models, enabling both static mutations in FNNs and RNNs and dynamic mutations in RNNs. MuNN [170] develops five mutation operators, focusing on model-level fault injection. DeepCrime [68] implements 24 deep learning mutation operators to test the robustness of deep learning systems, including training data operators, hyperparameters operators, activation function operators, regularization operators, weights operators, loss function operators, optimiza- tion operators, and validation operators. Additionally, some studies have focused on mutation testing in reinforcement learning [104, 177] and unsupervised learning [103]."}, {"title": "5.3 Gap between Failure Analysis and Fault Injection in Al Model", "content": "Based on the comparative analysis presented in Table 6, which delineates the capabilities of various FI tools in addressing diverse failure modalities within AI models, two critical insights regarding the discrepancies between FA and FI can be elucidated:\nDifferentiated Focus. The distinct FI tools appear to concentrate on disparate facets of AI model failures. For instance, DeepMutation and DeepCrime are adept at handling data quality and preprocessing faults, whereas MuNN is tailored towards layer and neuron quantity faults. This specialization implies that the selection of an FI tool should be contingent upon the specific fault types targeted for analysis.\nCoverage Inconsistency. Table 6 reveals a disparity in coverage, with certain fault types, such as \"Layer and Neuron Quantity Fault\" and \"Misuse Activation Function\", being addressed by multiple tools, while others, like \"Inappropriate LR, Epochs, and BS\", are solely within the purview of DeepCrime. This inconsistency may reflect the inherent challenges associated with simulating specific fault types or indicate a relative lack of focus within the FI."}, {"title": "6 FAILURE ANALYSIS AND FAULT INJECTION FOR AI FRAMEWORK", "content": "Al framework layer, including TensorFlow [1], PyTorch [140], and Keras [82], acts as a bridge between the AI model layer and the underlying hardware and system infrastructure. Akin to other software systems, these AI frameworks are susceptible to a variety of faults [94, 178]. Failures at the fault in AI frameworks is the problem in cross-programming-language communication. This kind of fault is particularly common in AI frameworks that utilize multi-programming-language [94]. Algorithm Fault is related to the defects in algorithm design [22, 178, 198]. This algorithm fault can be primarily categorized into two aspects including the incorrect implementation logic of an algorithm and the inefficient algorithm implementation. The former aspect mainly pertains to bugs present in the algorithm implementation within the AI framework [22, 146, 178]. The latter aspect arises from the challenge faced by AI framework developers in keeping up with the latest research and incorporating the most efficient methods for algorithm implementation [75, 111]."}, {"title": "6.1 Failure Analysis in Al Framework", "content": "Over recent years, a significant volume of research [1, 22, 67, 69, 94, 178, 198, 209] has been dedicated to the analysis of failures in AI frameworks. Studies on faults in AI failure can be primarily bifurcated into two categories including failures arising from the usage of AI frameworks, and failures stemming from the frameworks' implementation. We summarize the types of failures in Table 7 that can occur both in the utilization and the implementation of AI frameworks.\nData Fault may occur during the data input stage of an AI model. This type of fault is typically caused by unaligned tensors or incorrect formatting of input data. For example, it could occur when inputting a tensor array instead of an individual element from the array, or when mistakenly using a transposed tensor instead of the original tensor [67, 209]. Even more critically, the shape or type of the data inputted to the model may completely mismatch the expected input by the model which leads to the model unable to run correctly [67, 69]. One additional fault that occurs during the data input stage is the dataloader crash fault [64]. This fault primarily occurs in training tasks of LLM in multiple workers. This issue arises from a gradual memory leak due to PyTorch's implementation of dataloader, which is caused by the copy-on-write mechanism used in the multiprocessing fork start method, combined with a suboptimal design of the Python list.\nAPI Fault typically occurs during the call to APIs provided by AI frameworks. Such faults may due to the using of an API in a way that does not conform to the logic set out by developers of the framework [67]. Indeed, lack of inter-API compatibility and versioning issues could be one of the main culprits [67, 69, 209]. When different APIs are not compatible with each other or when the version of the API being used is not compatible with the requirements of the code or dependencies, it can result in API faults.\nConfiguration Fault typically occurs due to incorrect configuration of the framework. One example of this type of fault in TensorFlow is the confusion with computation model. Users may incorrectly constructed TensorFlow computation graphs using control-flow instead of data-flow semantics [198, 209]. Quan et al. [146] also analyse the failures in building and initialing JavaScript- based DL systems, such as npm package installation and multi-backend initialization. Another situation of this fault is the misconfiguration of the computing device (e.g., GPU). This type of misconfiguration can include selecting the wrong GPU device, mistakenly using CPU tensors instead of GPU tensors, or improper allocation of resources between CPU and GPU [67, 198].\nEnvironment configuration faults mainly encompass the problems during the development and deployment processes of Al framework. Given that Al frameworks typically function in heterogeneous environments, ensuring compatibility with various devices and systems becomes crucial during the development process [22, 198]. This can result in failures during the build and compilation process, which hinders the development of AI frameworks. Apart from encountering environment configuration faults during the development process, deploying an AI framework also entails addressing environment faults, such as \"path not found\", \"library not found\" and \"permission denied\" [207]. Moreover, deploying the AI framework on various operating systems (e.g., Linux, Windows, Mac, and Docker environments) or utilizing different types of acceleration devices within the framework can also give rise to environment-related faults [94].\nPerformance Fault typically does not result in system downtime but can significantly impact the runtime of the system. In the aspect of AI framework, there is a wide range of causes for performance faults which can be quite diverse. One of the causes is memory inefficiencies. Existing Al frameworks such as PyTorch [140] and TensorFlow [1] are typically implemented using C/C++ and CUDA, and their memory management is often done manually [111, 198]. These frameworks need to handle memory exchanges between heterogeneous devices, which can potentially introduce memory inefficiency faults [146]. Apart from memory management faults, another cause for performance faults is threading inefficiency [111]. Such fault is commonly found in GPU related code. Insufficient parallelism can result in underutilization of device resources, while excessive parallelism can introduce additional overhead(e.g., context switches). Another cause is the trade-off of using different linear algebra libraries/operators. For example, when performing a small matrix operation on a GPU, the computation time may be longer compared to performing the same operation on a CPU [143].\nCode Fault primarily refers to logic faults that occur during the implementation of AI framework. One example of code fault in AI framework is a syntax fault, which may occur both in the utilization and the implementation of AI framework. Expect traditional syntax fault occurring in command software system, AI frameworks also face faults related to tensor syntax faults during the imple- mentation [22, 178]. Such faults may occur on account of tensor shape misalignment and operation on tensors across different devices. Apart from common syntax faults, another noteworthy code"}, {"title": "6.2 Fault Injection in Al Framework", "content": "In recent years, there has been a significant increase in research focusing on FI techniques specif- ically targeted at Al frameworks. As shown in Table 8, we elaborate on these works that are categorized according to different AI frameworks. These techniques often rely on a process known as \"instrumentation\". This is a method used in fault injection where the system, such as source code or logic gates, is modified to inject faults more accurately or efficiently.\nTensorflow. There are a series of works focus on designing an FI system for TensorFlow as it is one of the most popular frameworks in AI application. TensorFI [23", "125": "utilizes the Keras API to intercept the state of tensors and injects fault to TensorFlow 2. TensorFI2 employs the Keras Model API to modify the layer state or weight matrices that holds the learned model parameters, and utilizes the Keras backend API to intercept the layer computation or activation matrices that holds the output states of the layers. These make TensorFI2 avoid the overhead of graph duplication and inject faults into the model parameters"}]}