{"title": "A Survey on Failure Analysis and Fault Injection in AI Systems", "authors": ["Guangba Yu", "Gou Tan", "Haojia Huang", "Zhenyu Zhang", "Pengfei Chen", "Roberto Natella", "Zibin Zheng"], "abstract": "The rapid advancement of Artificial Intelligence (AI) has led to its integration into various areas, especially\nwith Large Language Models (LLMs) significantly enhancing capabilities in Artificial Intelligence Generated\nContent (AIGC). However, the complexity of AI systems has also exposed their vulnerabilities, necessitating\nrobust methods for failure analysis (FA) and fault injection (FI) to ensure resilience and reliability. Despite the\nimportance of these techniques, there lacks a comprehensive review of FA and FI methodologies in AI systems.\nThis study fills this gap by presenting a detailed survey of existing FA and FI approaches across six layers of AI\nsystems. We systematically analyze 160 papers and repositories to answer three research questions including\n(1) what are the prevalent failures in AI systems, (2) what types of faults can current FI tools simulate, (3)\nwhat gaps exist between the simulated faults and real-world failures. Our findings reveal a taxonomy of AI\nsystem failures, assess the capabilities of existing FI tools, and highlight discrepancies between real-world and\nsimulated failures. Moreover, this survey contributes to the field by providing a framework for fault diagnosis,\nevaluating the state-of-the-art in FI, and identifying areas for improvement in FI techniques to enhance the\nresilience of Al systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial Intelligence (AI) has made significant strides over the past decade, permeating both\nacademic and industrial areas. Large Language Models (LLMs), in particular, have proven to be a\ngame changer, propelling AI to unprecedented heights and facilitating a myriad of applications in\nfields such as software engineering [141, 192, 196] and human language translation [8, 59, 85]. This\nevolution has led to the integration of AI models into a growing array of products, transforming\nthem into sophisticated AI systems. Notable examples of such integration include Gemini [51],\nBing [117], and ChatGPT [137], which underscore the pivotal role of AI in enhancing and expanding\nthe capabilities of modern technology solutions.\nThe escalating complexity and ubiquity of AI systems necessitate addressing their inherent\nvulnerabilities and failure-related challenges. A Meta AI report [208] points out over 100 failures\nduring the training of OPT-175B. Similarly, ChatGPT encountered 173 outages in 2023, causing a\nmaximum user impact over 427 minutes [138]. Such failures can degrade user experience, and even\nincur financial losses. Hence, mitigating AI system failures is of paramount importance.\nFailure analysis (FA) and fault injection (FI) techniques are instrumental in identifying limitations\nand bolstering the reliability of AI systems. Researchers and practitioners alike have embarked\non extensive investigations into Al system failures. Studies [22, 67, 69, 94, 100, 111, 178, 209]\nhave analyzed AI system failures from platforms like Stack Overflow or Github, while others [45,\n53, 73, 139, 176, 182, 207] have focused on failures in large-scale production AI systems. Such\nfailure analyses enable the identification of patterns, root causes, and locations, thereby informing\nFI techniques. FI, a proactive approach, uncovers system weaknesses on resiliency before they\nbecome catastrophic failures. By deliberately injecting faults or abnormal conditions into systems,\nteams can evaluate and enhance their resilience to unexpected disruptions. Some existing FI\napproaches [16, 17, 58, 68, 74, 106, 164, 170] mimic faults in Al systems engineered by humans,\nwhile others [34, 91, 136, 162, 206, 210] simulate hardware errors.\nDespite the progress, a comprehensive survey on FA and FI in AI systems is conspicuously absent.\nFurthermore, a gap exists between FI and FA, resulting in insufficient consideration of FA outcomes\nwhen crafting FI tools. Therefore, this study presents a comprehensive survey aimed at exploring\nand evaluating existing research for FA and FI in AI systems. We have meticulously reviewed\nand analyzed 160 corresponding papers and code repositories. As shown in Fig. 1, an AI system\ntypically comprises six layers - AI Service, AI Model, AI Framework, AI Toolkit, AI Platform, and\nAI Infrastructure [187]. We attempt to address three research questions at each layer as follows.\n\u2022 RQ1: What are the prevalent failures in current AI systems?\n\u2022 RQ2: What types of faults can current FI tools simulate?\n\u2022 RQ3: What gaps do exist between the simulated faults and the real-world failures?\nRQ1 aims to catalog and analyze the failures that have occurred in current AI systems. Un-\nderstanding these failures is crucial for several reasons. Since it helps in identifying common\nvulnerabilities within AI systems, informs developers about potential areas of improvement, and\ncontributes to the development of more reliable AI applications. RQ2 explores the capabilities of\nexisting FI tools designed for AI systems. The ability to simulate a wide range of faults is essential\nfor evaluating and enhancing the robustness and fault tolerance of AI systems. RQ3 investigates\nthe gap between simulated faults and real-world AI system failures, aiming to understand the\nlimitations of current FI tools in producing the full spectrum of potential failures. Moreover, un-\nderstanding these gaps helps in improving FI tools, and ultimately contributes to develop more\nresilient AI systems.\nBy examining the current landscape and identifying critical research gaps, this survey provides\nvaluable insights for researchers and practitioners working towards building reliable and resilient\nAI systems. This study makes the following contributions:\n\u2022 We present a comprehensive analysis and taxonomy of failures occurring at different layers of\nAl systems. By systematically characterizing these failures, we provide a valuable framework\nthat can serve as a reference for failure diagnosis in AI systems.\n\u2022 We conduct an in-depth examination of the capabilities of existing FI tools across various layers\nof AI systems. We offer insights into the state-of-the-art in simulating and reproducing potential\nfailures. This work provides a foundation for assessing the reliability of AI systems.\n\u2022 We explore the discrepancies between FI tools and real-world AI system failures. We identify the\nlimitations of current FI approaches in simulating potential failure scenarios. By shedding light\non these gaps, we emphasize the need for more comprehensive FI techniques in AI systems.\nThe rest of this paper is organized as follows. Section 2 provides background information on\nFA and FI in AI systems, followed by Section 3, which outlines our systematic literature review\nmethodology. The subsequent sections analyze FA and FI in different layers of AI systems, including\nthe AI service layer (Section 4), AI model layer (Section 5), AI framework layer (Section 6), AI toolkit\nlayer (Section 7), AI platform layer (Section 8), and AI infrastructure layer (Section 9). Section 10\nhighlights research opportunities about FI in AI systems. The article concludes in Section 11."}, {"title": "2 BACKGROUND AND DEFINITIONS", "content": "We adopt the definitions of failures and faults proposed by previous work [7, 163]. Furthermore,\nwe provide additional extensions and interpretations specific to AI systems.\n\u2022 Failure is defined as \"an incident that occurs when the delivered service deviates from the correct\nservice\" [7]. In the context of AI systems, failures can manifest in various ways. For example, a\nfailure can occur when Al services become unreachable, and when the behavior of Al services\ndoes not meet the expected outcome (e.g., generating semantically incorrect text). These failures\nindicate a deviation from the desired or expected behavior of the AI system.\n\u2022 Fault is the root cause of a failure. In AI systems, faults can be attributed to various sources,\nincluding algorithmic flaws, model design issues, or problems with the quality of the data used\nfor training or inference. It is important to note that faults in AI systems may remain uncovered\nfor some time, due to fault-tolerant approaches implemented in the system."}, {"title": "2.1 Failures and Faults", "content": "We adopt the definitions of failures and faults proposed by previous work [7, 163]. Furthermore,\nwe provide additional extensions and interpretations specific to AI systems.\n\u2022 Failure is defined as \"an incident that occurs when the delivered service deviates from the correct\nservice\" [7]. In the context of AI systems, failures can manifest in various ways. For example, a\nfailure can occur when Al services become unreachable, and when the behavior of Al services\ndoes not meet the expected outcome (e.g., generating semantically incorrect text). These failures\nindicate a deviation from the desired or expected behavior of the AI system.\n\u2022 Fault is the root cause of a failure. In AI systems, faults can be attributed to various sources,\nincluding algorithmic flaws, model design issues, or problems with the quality of the data used\nfor training or inference. It is important to note that faults in AI systems may remain uncovered\nfor some time, due to fault-tolerant approaches implemented in the system."}, {"title": "2.2 Failure Comparisons between Al and Cloud System", "content": "Failure analysis (FA) and fault injection (FI) are longstanding topics within the field of computer\nscience, traditionally focusing on the robustness and reliability of systems. Historically, much of\nthe literature has focused on cloud systems, reflecting their critical role in modern computing\ninfrastructure [47, 52, 79, 93, 98, 172]. These systems adhere to a logic-based programming paradigm,\nwhere developers encode decision logic directly into the source code, facilitating a structured\napproach to FA and FI. In contrast, AI systems represent a paradigm shift towards a data-driven\nprogramming model. Here, developers design neural networks that derive decision-making logic\nfrom extensive datasets [22, 67, 69, 94, 100, 111, 178, 209]. This shift introduces both similarities\nand differences in the approach to FA between AI systems and traditional cloud systems.\nAs illustrated in Figure 2, while both AI and cloud systems are susceptible to common failures such\nas power disruptions and network outages, certain faults are unique or more critically impactful to\nAl systems. For instance, GPU failures, which might be relatively inconsequential in traditional cloud\nenvironments, can severely affect the performance and availability of AI systems. This distinction\nunderscores the importance of conducting a comprehensive survey on FA and FI specifically for AI\nsystems, especially in the current era dominated by \"Large Models\". This need forms one of the\nprimary motivations behind the research presented in this article."}, {"title": "2.3 Failure Analysis and Fault Injection", "content": "Figure 3 demonstrates the relationship between faults, FA, and FI. In large-scale AI systems, faults\ncommonly occur in Al systems due to their inherent complexity and the numerous interconnected\ncomponents involved, leading to fault origination at various stages of system operation [22, 67, 69,\n100, 111, 178, 209]. Once a fault is activated and a failure is detected (stage 1), engineers responsible\nfor AI system maintenance engage in mitigating the failure based on observed behaviors (stage\n\u2461). During the failure mitigation process, engineers generate comprehensive incident reports\nencompassing failure details such as occurrence time, impact, failure manifestation, and mitigation\nstrategies. The objective of FA is to utilize these incident reports as inputs to summarize the fault\npattern (e.g., recurring type and location) (stage \u2462).\nFI is a widely adopted technique for assessing and improving the reliability and security of\nsystems, including AI systems. It involves the deliberate introduction of faults into a system to\nobserve its behavior and validate its fault tolerance mechanisms. FI can be applied in various forms,\nsuch as software fault injection (e.g., mutation test [76] and API interception [189, 202]) or hardware\nfault injection (e.g., simulating hardware failures [130] and environmental disturbances [83]).\nLeveraging the knowledge acquired from historical FA, engineers employ FI techniques to validate\nthe reliability of AI systems. Following the injection of faults, engineers closely monitor AI system\nperformance and behavior, ensuring the accurate identification and appropriate handling of the\ninjected faults (stage \u2463). Based on the analysis of FI experiments, engineers can identify system\nvulnerabilities, weaknesses, and areas for improvement. By iteratively conducting FI experiments\nand refining the system based on the obtained results, the AI system can be continually improved,\nenhancing its reliability and effectiveness in real-world scenarios.\nConsequently, FA and FI form closely intertwined processes that significantly contribute to the\nassessment and enhancement of AI system reliability. The insights derived from FA guide the\nselection and design of FI scenarios. The iterative feedback loop established by fault analysis and\nFI facilitates the continuous improvement of AI systems, thereby serving as the other motivation\ndriving the research presented in this article."}, {"title": "3 SURVEY METHODOLOGY", "content": "To systematically collect the publications for conducting this survey, we specially constructed and\nmaintained a repository \u00b9 about FA and FI in AI systems. We first searched relevant papers in online\ndigital libraries and extended the repository by manually inspecting all references of these papers.\nTo begin with, we searched several popular online digital libraries (e.g., IEEE Xplore, ACM Digital\nLibrary, Springer, Elsevier, and Wiley) with the following keywords: \"failure and machine learning\",\n\"failure and deep learning\", \"failure and AI\", \"fault injection and machine learning\", \"fault injection\nand deep learning\", \"fault injection and AI\", etc. We mainly focused on regular papers published\nrelevant conferences (e.g., ICSE, FSE, ASE, SOSP, OSDI, ATC, NSDI, SC, DSN, ISSRE, AAAI, ICML,\nNeurIPS, etc) and journals (e.g., TSE, TOSEM, EMSE, TDSC, TPDS, TSC, etc). Upon identifying\nrelevant papers, we conducted a recursive examination of the references, which allowed us to\n1https://github.com/IntelligentDDS/awesome-papers/tree/main/Fault_tolerance#ai-system"}, {"title": "4 FAILURE ANALYSIS AND FAULT INJECTION IN AI SERVICE", "content": "In the context of AI systems, the AI service layer can be considered as the topmost layer that\ndirectly interacts with users or other systems. This layer acts as an interface or entry point for\nusers to access and utilize the AI capabilities provided by the underlying layers of the AI system.\nPotential failures at this layer can include unavailability of the service or incorrect outputs, such\nas incorrect inference from a classifier or hallucinations from an LLM-based service. This section\ndelves into the FA and FI in AI services"}, {"title": "4.1 Failure Analysis in Al Service", "content": "In our detailed exploration of potential failures in AI services, we have identified a broad spectrum\nof faults. These can be classified into several major categories including data fault, code and software\nfault, network transmission fault, and external attack. We summarize the types of failures in Table 1\nthat can occur in AI services. The paper column in Table 1 shows some representative papers in\nthis study, as do the following tables below.\nData Fault. These faults related to the format, type, and noise of data can lead to the failure\nof Al services [165, 179, 211]. For example, incorrect data encoding (e.g., requesting UTF-8 but\nreceiving ASCII) or inappropriate data types (e.g., expecting a string but receiving an integer)\ncan prevent AI services from working properly. The JENGA framework explores the impact of\ndata faults on predictions of machine learning models [165]. Furthermore, data drift and concept\ndrift are common problems [179]. Data drift refers to a model trained on a specific distribution of\ndata but then encountering a different distribution in practice. While concept drift occurs as the\nrelationship between features and labels becomes invalid over time. Zhao et al. [211] investigate\nthe impact of concept drift on model performance.\nDevelopment Fault. The primary reason for service failures is code quality such as bugs,\nlogical faults in code, and poor software design [46, 63, 90]. Code faults typically originate from\ncoding mistakes. Logical faults in code often involve incorrect algorithm implementation, impacting\nthe accuracy and efficiency of inference. Thus, poor software design affects system performance.\nAdditionally, failures in AI service API calls [189], originating from API incompatibility, API change,\nand API misuse, can lead to AI service failures.\nDeployment Fault. During the deployment of AI services, various faults can arise, including\noutdated models [122], path configuration errors [60], and inappropriate resource allocation [112].\nThese faults can impact system performance and stability. As data changes over time, model perfor-\nmance may degrade, necessitating regular updates and retraining to maintain their effectiveness.\nPath configuration faults can prevent the proper loading of models and data. Inadequate resource\nallocation [112], especially inefficient use of CPU and GPU resources, can lead to decreased system\nperformance and unnecessary waste.\nNetwork Transmission Fault. It may arise from network congestion, bandwidth limitations,\nor failures in network hardware, leading to packet delays or losses [87]. Network congestion occurs\nwhen data traffic exceeds the network's bandwidth capacity, preventing data transmission on time.\nBandwidth limitations are imposed by the maximum transmission rate of a network connection,\noften determined by service provider or the capabilities of network hardware. Furthermore, physical\ndamage or configuration faults in network devices can also lead to packet delays or losses. failures\nin network hardware, leading to packet delays or losses [87]. Network congestion occurs when\ndata traffic exceeds the network's bandwidth capacity, preventing data transmission on time.\nBandwidth limitations are imposed by the maximum transmission rate of a network connection,\noften determined by service provider or the capabilities of network hardware. Furthermore, physical\ndamage or configuration faults in network devices can also lead to packet delays or losses.\nExternal Attack. In addition to internal faults within Al services, external attacks can also lead\nto service failures. These include network attacks such as Distributed Denial of Service (DDoS)\nattacks [113] and Man-in-the-Middle (MITM) attacks [149], which can not only cause temporary\nservice interruptions but also lead to data leakage or corruption. Moreover, adversarial attacks\ntarget the AI model by designing malicious inputs (e.g., meticulously modified images or texts)\nto deceive the AI into making incorrect actions [2, 4, 145]. Adversarial attacks further divide\ninto white-box attacks and black-box attacks. In white-box attacks, the deployed model is fully\nunderstood, including inputs and architecture, allowing for targeted attacks. In black-box attacks,\nonly the model's inputs and output labels/confidences are accessible."}, {"title": "4.2 Fault Injection in Al Service", "content": "Fault injection in AI service layer encompasses four dimensions including data, service API, network\ntransmission, and external attack. By simulating diverse fault scenarios in these dimensions, it is\npossible to assess the system's robustness and reliability, thereby preventing inaccurate predictions\nor even system failures. Table 2 illustrates the current FI tools in the AI service layer, followed by a\ndescription of each of them.\nData Fault. Data perturbation is the most intuitive method of FI at the data dimension. Intro-\nducing noise or modifying data manually can simulate uncertainties in real-world data. It can be\nachieved using tools such as NumPy [132], Scikit-learn [166], and so on. JENGA [165] is a frame-\nwork that studies the impact of data faults (e.g., missing values, outliers, typing errors, and noisy) on\nthe predictions of machine learning models. Additionally, data and concept drift can be simulated\nthrough carefully designed data disturbances. Moreover, tools such as scikit-multiflow [120] and\nMOA [12] enable the simulation of sudden, gradual, and incremental drifts in data streams.\nService API Fault. Istio, an open-source service mesh, addresses provides robust traffic man-\nagement features. Istio's fault injection capabilities are primarily exposed through its Service API,\nwhich allows users to define fault injection rules declaratively [19, 71]. These rules are specified\nwithin Istio's VirtualService resources, which are then propagated to the Envoy proxies deployed\nas sidecars alongside each service instance. However, deploying and managing Istio can add com-\nplexity to the AI infrastructure. The learning curve for Istio is steep, and managing its components\nalongside Al services can be challenging.\nNetwork Fault. Common network fault injections include network delays, network jitter, packet\nloss, and reordering. The injection of these network faults may result in the delayed or non-response\nof user requests for AI services. Toxiproxy [184] is a TCP proxy used to simulate network and\nsystem conditions for chaos and resilience testing. ChaosBlade [18] is an open source experimental\ninjection tool that adheres to the principles of chaos engineering and chaos experimental models,\nsupporting a rich array of experimental scenarios. Istio can also leverage the Envoy's advanced\ntraffic management capabilities to simulate network conditions such as delay, packet loss, and\nservice unavailability [72].\nExternal Attack. Adversarial attacks are deliberate attacks on the AI service, including both\nwhite-box and black-box attacks. White-box attacks leverage model structure and parameter\ninformation to strategically generate adversarial samples, such as FGSM [50], PGD [108], and\nDI-AA [191]. Black-box attacks lack insight into the model and rely on observing model inputs"}, {"title": "4.3 Gap between Failure Analysis and Fault Injection in Al Service", "content": "Based on the comparative analysis presented in Table 3, which outlines the capabilities of various FI\ntools in addressing diverse failure modalities within AI services, several critical insights regarding\nthe discrepancies between FA and FI can be elucidated.\nIncomplete Coverage. The scrutinized FI tools, in aggregate, encompass a substantial proportion\nof the delineated fault types. Nonetheless, certain fault types (e.g., \"Defective Code\") remain\nunaddressed by any of the existing tools. This revelation underscores the exigency for subsequent\nresearch to ameliorate these deficiencies in fault simulation.\nDiversity of Tools. Each FI tool exhibits a predilection for addressing specific fault types. For\ninstance, JENGA is attuned to data quality faults, whereas MOA is adept at handling data and\nconcept drift faults. This diversity implies that practitioners may be compelled to deploy a suite of\ntools to achieve a comprehensive simulation and analysis of failures within Al services, contingent\nupon the specific fault types of interest.\nNew Emerging Fault Types. The incorporation of fault types such as \"Prompt Injection Attacks\"\naccentuates the burgeoning significance of accounting for external factors and security facets in\nthe FA of AI services. Nevertheless, traditional FI tools (e.g., ChaosBlade) are designed for cloud\ncomputing services and do not consider the specific scenarios of AI systems. As AI services become\nincreasingly interconnected and susceptible to a myriad of threats, it is imperative to cultivate FI\ntools capable of efficaciously simulating failures emanating from these nascent domains."}, {"title": "5 FAILURE ANALYSIS AND FAULT INJECTION IN AI MODEL", "content": "Al model layer is a crucial component in AI systems, residing beneath the AI service layer. This layer\nis responsible for managing the various AI models and algorithms that power the AI capabilities of\nthe system. Similar to the AI service layer, potential failures at this layer can include unavailability\nof the model or incorrect outputs. In this section, we delve into the FA and FI in AI models."}, {"title": "5.1 Failure Analysis in Al Model", "content": "Recent research has explored multiple reasons for AI model failures. Researchers analyze various\nsources, including GitHub commits, Stack Overflow posts, and expert interviews. These analyses\nhave provided crucial insights into enhancing the reliability and robustness of AI systems. For\nexample, Humbatova et al. [67] categorize faults within deep learning systems by examining\n1,059 GitHub commits and issues of AI systems, while Islam et al. [69] analyze error-prone stages,\nbug types, and root causes in deep learning pipelines from 2,716 Stack Overflow posts and 500\nGitHub bug fix commits. Additionally, Nikanjam et al. [131] classify faults in deep reinforcement\nlearning programs from 761 documents. We focus on the training and testing phases of AI models,\nwhere faults are categorized as data faults, model hyperparameter faults, and model structure and\nalgorithm faults, as shown in Table 4.\nData Fault. The quality of training data is pivotal for successful model training. Alice et al. [32]\nconduct a systematic analysis of data quality attributes (accuracy, uniqueness, consistency, com-\npleteness, and timeliness) across five software bug datasets and find that 20-71% of data labels are\ninaccurate, which could severely hinder model training in extreme cases. Some studies [95, 193]\nhave examined challenges faced by data quality. Furthermore, data preprocessing and augmentation\nare crucial. Raw data are susceptible to noise, damage, loss, and inconsistency, thus necessitating\npreprocessing steps (e.g., data cleaning, integration, transformation, and reduction) to facilitate\neasier knowledge extraction from datasets. Data augmentation aims to expand the training dataset\nthrough specific transformations to enhance the model's generalizability. Das [33] lists ten common\nfaults in data preprocessing, while Maharana et al. [109] discuss various data preprocessing and\ndata augmentation techniques to enhance model performance.\nModel Hyperparameter Fault. The parameters of an Al model include both pre-training\nhyperparameters (e.g., the number of hidden layers, batch size, and learning rate) and post-training\nmodel parameters (e.g., network weights and biases). Basha et al. [9] examine the effects of different\nnumbers of fully connected layers on convolutional neural networks (CNNs). Uzair et al. [188]\ninvestigate how the number of hidden layers affects the efficiency of neural networks. Short-\nGPT [116] points out that many layers in LLMs exhibit high redundancy, with some layers playing\nminimal roles. Gurnee et al. [54] utilize seven different models (ranging from 70 million to 6.9\nbillion parameters) to study the sparsity of activations in LLM neurons. Additionally, the learning\nrate (LR), epochs, and batch size (BS) influence the training speed and the performance of the\ntrained model. He et al. [57] indicate that the ratio of batch size to learning rate should not be too\nlarge to ensure good generalization. Shafi et al. [167] explore the optimization of hyperparameters,\nincluding the learning rate, batch size, and epochs, as well as their interrelationships."}, {"title": "5.2 Fault Injection in Al Model", "content": "Fault injection in AI models typically involves interfering with the training process to create models\nwith inferior performance. This technique was called mutation testing. The concept of mutation\ntesting that we will discuss next is analogous to fault injection. A common method for conducting\nmutation testing on AI models involves designing mutation operators that introduce faults into\nthe training data or the model training program, and then analyzing the behavioral differences\nbetween the original and mutated models.\nRecent studies in mutation testing have made significant contributions. We have summarized\nthese works as shown in Table 5. DeepMutation [106] designs 13 mutation operators that inject\nfaults into both the training data and code of deep learning models. DeepMutation++ [62] combines\nDeepMutation (eight model-level operators for FNN models) and proposes nine new operators\nspecifically for RNN models, enabling both static mutations in FNNs and RNNs and dynamic\nmutations in RNNs. MuNN [170] develops five mutation operators, focusing on model-level fault\ninjection. DeepCrime [68] implements 24 deep learning mutation operators to test the robustness\nof deep learning systems, including training data operators, hyperparameters operators, activation\nfunction operators, regularization operators, weights operators, loss function operators, optimiza-\ntion operators, and validation operators. Additionally, some studies have focused on mutation\ntesting in reinforcement learning [104, 177] and unsupervised learning [103]."}, {"title": "5.3 Gap between Failure Analysis and Fault Injection in Al Model", "content": "Based on the comparative analysis presented in Table 6, which delineates the capabilities of various\nFI tools in addressing diverse failure modalities within AI models, two critical insights regarding\nthe discrepancies between FA and FI can be elucidated:\nDifferentiated Focus. The distinct FI tools appear to concentrate on disparate facets of AI\nmodel failures. For instance, DeepMutation and DeepCrime are adept at handling data quality and\npreprocessing faults, whereas MuNN is tailored towards layer and neuron quantity faults. This\nspecialization implies that the selection of an FI tool should be contingent upon the specific fault\ntypes targeted for analysis.\nCoverage Inconsistency. Table 6 reveals a disparity in coverage, with certain fault types,\nsuch as \"Layer and Neuron Quantity Fault\" and \"Misuse Activation Function\", being addressed by\nmultiple tools, while others, like \"Inappropriate LR, Epochs, and BS\", are solely within the purview"}, {"title": "6 FAILURE ANALYSIS AND FAULT INJECTION FOR AI FRAMEWORK", "content": "Al framework layer, including TensorFlow [1], PyTorch [140], and Keras [82], acts as a bridge\nbetween the AI model layer and the underlying hardware and system infrastructure. Akin to other\nsoftware systems, these AI frameworks are susceptible to a variety of faults [94, 178]. Failures at the\nfault in AI frameworks is the problem in cross-programming-language communication. This kind\nof fault is particularly common in AI frameworks that utilize multi-programming-language [94].\nAlgorithm Fault is related to the defects in algorithm design [22, 178, 198]. This algorithm fault\ncan be primarily categorized into two aspects including the incorrect implementation logic of an\nalgorithm and the inefficient algorithm implementation. The former aspect mainly pertains to bugs\npresent in the algorithm implementation within the AI framework [22, 146, 178]. The latter aspect\narises from the challenge faced by AI framework developers in keeping up with the latest research\nand incorporating the most efficient methods for algorithm implementation [75, 111]."}, {"title": "6.1 Failure Analysis in Al Framework", "content": "Over recent years, a significant volume of research [1, 22, 67, 69, 94, 178, 198, 209] has been\ndedicated to the analysis of failures in AI frameworks. Studies on faults in AI failure can be\nprimarily bifurcated into two categories including failures arising from the usage of AI frameworks,\nand failures stemming from the frameworks' implementation. We summarize the types of failures\nin Table 7 that can occur both in the utilization and the implementation of AI frameworks.\nData Fault may occur during the data input stage of an AI model. This type of fault is typically\ncaused by unaligned tensors or incorrect formatting of input data. For example, it could occur when\ninputting a tensor array instead of an individual element from the array, or when mistakenly using\na transposed tensor instead of the original tensor [67, 209]. Even more critically, the shape or type\nof the data inputted to the model may completely mismatch the expected input by the model which\nleads to the model unable to run correctly [67, 69]. One additional fault that occurs during the data\ninput stage is the dataloader crash fault [64]. This fault primarily occurs in training tasks of LLM in\nmultiple workers. This issue arises from a gradual memory leak due to PyTorch's implementation\nof dataloader, which is caused by the copy-on-write mechanism used in the multiprocessing fork\nstart method, combined with a suboptimal design of the Python list.\nAPI Fault typically occurs during the call to APIs provided by AI frameworks. Such faults may\ndue to the using of an API in a way that does not conform to the logic set out by developers of the\nframework [67]. Indeed, lack of inter-API compatibility and versioning issues could be one of the\nmain culprits [67, 69, 209]. When different APIs are not compatible with each other or when the\nversion of the API being used is not compatible with the requirements of the code or dependencies,\nit can result in API faults.\nConfiguration Fault typically occurs due to incorrect configuration of the framework. One\nexample of this type of fault in TensorFlow is the confusion with computation model. Users may\nincorrectly constructed TensorFlow computation graphs using control-flow instead of data-flow\nsemantics [198, 209]. Quan et al. [146] also analyse the failures in building and initialing JavaScript-\nbased DL systems, such as npm package installation and multi-backend initialization. Another\nsituation of this fault is the misconfiguration of the computing device (e.g., GPU). This type of\nmisconfiguration can include selecting the wrong GPU device, mistakenly using CPU tensors\ninstead of GPU tensors, or improper allocation of resources between CPU and GPU [67, 198].\nEnvironment configuration faults mainly encompass the problems during the development\nand deployment processes of Al framework. Given that Al frameworks typically function in\nheterogeneous environments, ensuring compatibility with various devices and systems becomes\ncrucial during the development process [22, 198]. This can result in failures during the build and\ncompilation process, which hinders the development of AI frameworks. Apart from encountering\nenvironment configuration faults during the development process, deploying an AI framework also\nentails addressing environment faults, such as \"path not found\", \"library not found\" and \"permission\ndenied\" [207]. Moreover, deploying the AI framework on various operating systems (e.g., Linux,\nWindows, Mac, and Docker environments) or utilizing different types of acceleration devices within\nthe framework can also give rise to environment-related faults [94].\nPerformance Fault typically does not result in system downtime but can significantly impact\nthe runtime of the system. In the aspect of AI framework, there is a wide range of causes for\nperformance faults which can be quite diverse. One of the causes is memory inefficiencies. Existing\nAl frameworks such as PyTorch [140] and TensorFlow [1] are typically implemented using C/C++\nand CUDA, and their memory management is often done manually [111, 198]. These frameworks\nneed to handle memory exchanges between heterogeneous devices, which can potentially introduce\nmemory inefficiency faults [146]. Apart from memory management faults, another cause for\nperformance faults is threading inefficiency [111]. Such fault is commonly found in GPU related\ncode. Insufficient parallelism can result in underutilization of device resources, while excessive\nparallelism can introduce additional overhead(e.g., context switches). Another cause is the trade-\noff of using different linear algebra libraries/operators. For example, when performing a small\nmatrix operation on a GPU, the computation time may be longer compared to performing the same\noperation on a CPU [143].\nCode Fault primarily refers to logic faults that occur during the implementation of AI framework.\nOne example of code fault in AI framework is a syntax fault, which may occur both in the utilization\nand the implementation of AIframework. Expect traditional syntax fault occurring in command\nsoftware system, AI frameworks also face faults related to tensor syntax faults during the imple-\nmentation [22, 178]. Such faults may occur on account of tensor shape misalignment and operation\non tensors across different devices. Apart from common syntax faults, another noteworthy code"}, {"title": "6.2 Fault Injection in Al Framework", "content": "In recent years, there has been a significant increase in research focusing on FI techniques specif-\nically targeted at Al frameworks. As shown in Table 8, we elaborate on these works that are\ncategorized according to different AI frameworks. These techniques often rely on a process known\nas \"instrumentation\". This is a method used in fault injection where the system, such as source\ncode or logic gates, is modified to inject faults more accurately or efficiently.\nTensorflow. There are a series of works focus on designing an FI system for TensorFlow as it is\none of the most popular frameworks in AI application. TensorFI [23] introduces an interface-level FI\napproach that focuses on the data-flow graph of TensorFlow. During the inference phase, TensorFI\ninjects both hardware or software faults into TensorFlow operators, corrupting the output of the\naffected operators. As AI applications developed using TensorFlow 2 do not necessarily depend\non data flow graphs, TensorFI2 [125] utilizes the Keras API to intercept the state of tensors and\ninjects fault to TensorFlow 2. TensorFI2 employs the Keras Model API to modify the layer state or\nweight matrices that holds the learned model parameters, and utilizes the Keras backend API to\nintercept the layer computation or activation matrices that holds the output states of the layers.\nThese make TensorFI2 avoid the overhead of graph duplication and inject faults into the model\nparameters. InjectTF [10] is another FI framework designed for TensorFlow. InjectTF implements"}, {"title": "6.3 Gap between Failure Analysis and Fault Injection in Al Framework", "content": "Upon comparison of the fault types derived from FA, it becomes evident that there is room for\nenhancement in existing FI tools for AI frameworks. By identifying the gaps between FI tools and\nthe outcomes of FA, engineers can gain a deeper understanding of the limitations of FI tools.\nFault Type Accommodation. The first gap is related to the fault types that the FI tool needs to\naccommodate and implement. The fault types identified in FA may not always have corresponding\ninjection implementations. Table 9 presents the fault types supported by existing FI tools in\nAl framework, along with the fault types revealed during FA. Existing FI tools primarily inject\nfaults by modifying specific tensors and constants or by using bit flipping techniques. Therefore,\nthere are relatively fewer implementations of FI specifically targeting faults that are unrelated to"}, {"title": "7 FAILURE ANALYSIS AND FAULT INJECTION FOR AI TOOLKIT", "content": "AI toolkit layer acts as an intermediate interface between the AI framework and underlying\ndevices (e.g., GPU and NIC), facilitating the AI framework to utilize external functionalities in its\nimplementation. The most commonly used AI toolkit is CUDA [133], a parallel computing runtime\nand API developed by NVIDIA. Besides CUDA, there are other GPU-specific toolkits available for\nAI and high-performance computing (HPC) solution development. Potential failures at this layer\ncan include unavailability or incorrect outputs. This section delves into FA and FI in AI toolkits"}, {"title": "7.1 Failure Analysis in Al Toolkit", "content": "In the following section, we will delve into and analyze the failures that typically occur within the\nAI toolkit. As shown in Table 10, these failures can be categorized into several types, including\nSynchronization Fault, Memory Safety Fault, Dependency Fault etc.\nSynchronization Fault is frequently encountered due to the concurrent nature as GPU pro-\ngrams commonly operate using multiple threads. In contrast to CPUs that commonly utilize lock\nmechanisms for data synchronization, GPUs predominantly rely on barriers as their synchroniza-\ntion mechanism. In particular, a barrier is represented as a barrier function __syncthreads() in CUDA\nkernel functions [133]. There are primarily three main causes for synchronization faults: data race,"}, {"title": "7.2 Fault Injection in Al Toolkits", "content": "In recent years, several FI techniques specifically designed for AI toolkits have emerged. In this\nsection, we will elaborate on these works, categorizing them according to the different AI toolkits\nthey target (details shown in Table 11).\nCUDA. As a parallel computing platform and application programming interface (API) developed\nby NVIDIA, CUDA offers AI systems high-performance and highly parallel capabilities. Recently,\nthere are two main areas of work in fault injection at the CUDA level.\n\u2022 FI in CUDA Programs. Simulee [194] utilizes LLVM bytecode to trace the execution of CUDA\nprograms, enabling the detection of synchronization faults in CUDA. During the test input\ngeneration phase, Simulee incorporates the principles of fuzzing and introduces Evolutionary\nProgramming [199] as a method to generate CUDA programs with built-in synchronization faults.\nSimulee introduces synchronization faults into CUDA programs by altering the dimensions and\narguments that control the organization of threads within CUDA kernel functions.\n\u2022 FI in CUDA Compilers. Faults in the CUDA compiler can either lead to compile-time faults or\nemit executable codes that may lead to runtime faults. CUDAsmith [77] proposes a FI framework\nfor CUDA compilers, which can be used to test several versions of NVCC and Clang compilers for\nCUDA with different optimization levels. CUDAsmith continuously generates CUDA programs\nto detect potential bugs in the CUDA compiler and utilizes equivalence modulo inputs (EMI)\ntesting techniques to solve the test oracle problem.\nOpenCL. Open Computing Language (OpenCL) is a framework that provides programming\ncapabilities on heterogeneous devices(e.g., GPUs, CPUs, and FPGAs) [86, 105]. Christopher et\nal. [96] investigate many-core compiler fuzzing in the context of OpenCL and introduce a tool,\nCLsmith. They utilize many-core random differential testing and many-core EMI testing to detect\nbugs in OpenCL compilers by injecting EMI blocks into existing OpenCL kernels.\nCollective Communication. Collective communication is defined as communication that\ninvolves a group of processes, which plays a significant role in distributed AI scenarios. The\nintricate communication among different nodes poses significant challenges to the reliability of\ncollective communication. FastFIT [43] is a fault injection tool specifically designed for MPI. It injects\nfaults randomly into the input parameters of collective interface. In particular, FastFIT manifests\nthe fault by one bit flip in one of the input parameters, which typically include the send/receive\nbuffer address, data elements, data type, communication destination, and communicator."}, {"title": "7.3 Gap between Failure Analysis and Fault Injection in Al Tookit", "content": "Injecting faults into an AI toolkit is a complex process that comes with various challenges. As\nshown in Table 12, it is evident that there exists a significant gap between the capabilities of various\nfault injection tools in simulating specific types of faults within AI tookits.\nIncomplete coverage of fault types. Table 12 highlights that certain types of faults, such\nas \"Temporal Safety Fault\", \"Failed Free Operation\", \"Intra-dependency Fault\", \"NCCL Fault\", and"}, {"title": "8 FAILURE ANALYSIS AND FAULT INJECTION FOR AI PLATFORM", "content": "AI platform layer plays a crucial role in the overall architecture of an AI system. This layer serves\nas the foundation of the above layers. It abstracts the complexities of underlying hardware, offering\na unified interface for functionalities like data management and sharing, workflow scheduling,\nand resource allocation. Failures in the AI platform layer can hinder data collaboration between\ndifferent AI applications, cause scheduling failures for AI training or inference tasks, and so on.\nThis section delves into the FA and FI in AI platforms."}, {"title": "8.1 Failure Analysis in Al Platform", "content": "In this section, we primarily introduce FA about Spark [203], Ray [121] and Platform-X in Mi-\ncrosoft [45], which are three representative AI platforms. Due to limited FA work in platform layer,\nwe supplement several fault types based on the merged pull requests (PRs) that are responsible\nto fix bugs on GitHub [160]. Notably, not all pull requests in this category are exclusively for bug\nfixes. Some may focus on introducing new features or updating documentation. To specifically\nidentify bug-fixing pull requests, we employ keyword searches in the tags and titles, leveraging\nestablished bug-related terms such as fix, defect, fault, bug, issue, mistake, correct, fault, and flaw,\naligning with prior research [69, 169]. Table 13 shows the detailed failures of AI platforms.\nCode Faults are prevalent in AI platforms due to their inherent complexity (e.g., intricate\nsoftware stacks and distributed environment). Concurrency faults, often caused by race conditions\nor deadlocks, have been reported in several issues [150, 157-159]. API incompatibility issues, where\nthe platform encounters compatibility problems with external APIs, have also been observed [155,\n156, 207]. Misconfigurations, where incorrect system configurations lead to malfunctions, and\ninadequate access control mechanisms, allowing unauthorized access, have also been documented"}, {"title": "8.2 Fault Injection in Al Platform", "content": "In recent years, several FI techniques specifically designed for AI Platform have emerged. We have\nsummarized these works as shown in Table 14.\nAs a distributed computing architecture, the communication between nodes and the influence\nbetween node states are commom faults in AI platforms, such as node partition caused by net-\nwork fault and node recovery bugs caused by node crashes. Therefore, there are currently some\nrelated works that inject faults into these issues to discover corresponding bugs. ChaosBlade [18]\ncan introduce resource hog faults into target system to test its resilience. Chen et al. propose a\nconsistency-guided fault injection technique called CoFI to systematically injects network partitions\nto effectively expose partition bugs in distributed systems [20]. Gao et al. propose CrashFuzz, a\nfault injection testing approach that can effectively test crash recovery behaviors and reveal crash\nrecovery bugs in distributed systems [44].\nData-intensive scalable computing (DISC) has become popular due to the increasing demands of\nanalyzing big data. For example, Apache Spark and Hadoop allow developers to write dataflow-\nbased applications with user-defined functions to process data with custom logic. Testing such\napplications is difficult. Many programming details in data processing code within Spark programs\nare prone to false statements that need to be correctly and automatically tested. Hence, Jo\u00e3o et\nal. propose TRANSMUT-SPARK, a tool that automates the mutation testing process of Big Data\nprocessing code within Spark programs [127]. Ahmad et al. propose DepFuzz [66] to increase the\neffectiveness and efficiency of fuzz testing dataflow-based big data applications such as Apache\nSpark-based DISC applications written in Scala."}, {"title": "8.3 Gap between Failure Analysis and Fault Injection in Al Platform", "content": "From Table 15, two insights regarding the gap between FA and FI in AI platforms can be gleaned.\nLimited Coverage. Not all types of failures are covered by the listed FI tools. For instance,\nfailures due to misconfiguration, wrong access control, memory leaks, tool/library faults, and GPU\nresource contention are not being simulated by any of the tools. This indicates a significant gap in\nthe current FI capabilities and highlights areas where further research are needed.\nLack of Specific Design. The current fault injection tools appear to be designed primarily\nfor traditional cloud platforms, such as Kubernetes, with less consideration given to the unique\ncharacteristics and requirements of AI platforms. For instance, none of the listed tools can simulate\nfailures related to GPU resource contention, which is a critical aspect of AI platforms due to their\nheavy reliance on GPU resources for computation-intensive tasks. This lack of specific design for\nAI platforms introduces a substantial gap in the ability to accurately simulate and study the full\nrange of potential failures in these systems."}, {"title": "9 FAILURE ANALYSIS AND FAULT INJECTION FOR AI INFRASTRUCTURE", "content": "Al infrastructure layer serves as the foundational layer in an AI system architecture, providing the\nunderlying physical and virtualized resources necessary for the deployment and operation of AI\napplications and services. This layer is responsible for managing and orchestrating the computing,\nstorage, and networking resources required by the AI platform layer and other higher-level layers.\nPotential failures in this layer can lead to service unavailability, system deployment failures, model\ntraining failures, etc. This section delves into the FA and FI in AI infrastructure layer."}, {"title": "9.1 Failure Analysis in Al Infrastructure", "content": "In this section, we primarily introduce FA in the AI Infrastructure layer. Table 16 shows the detailed\nfailures of AI infrastructure.\nWe analyze the failure of GPU, FPGA and TPU, which are three\nrepresentative hardware accelerators."}, {"title": "9.1.1 Hardware Accelerators.", "content": "GPU. GPU has become the most commonly used underlying hardware in AI and HPC systems.\nHowever, according to existing research, the frequency of failures caused by GPU is still high [176].\nResearch conducted on the Titan supercomputer explores different aspects of GPU failures. This\nincludes the examination of GPU faults in a broader context [182], the analysis of specific GPU\nsoftware faults [128], the characterization of GPU failures concerning temperature and power [129],\nand the investigation of spatial characteristics associated with failures [183]. Ostrouchov et al. [139]\nfind that GPU reliability is dependent on heat dissipation to an extent that strongly correlates\nwith detailed nuances of the cooling architecture and job scheduling. Nie et al. [129] analyze the\nrelationship between single bit faults occurrence and temperature on the Titan supercomputer, and\npropose a machine learning based technique for GPU soft-fault prediction. A study about another\nsupercomputer, Blue Water, analyzes GPU failures among other hardware failures [38]. The study\nreveals that GPUs rank among the top three most prone to failures, and notably, GPU memory\nexhibits greater sensitivity to uncorrectable faults compared to main memory.\nGiven the distinctions in workload between HPC and AI systems, the following discussion\ndelves into GPU failure analysis specifically tailored to AI systems. Zhang et al. [207] present the\nfirst comprehensive empirical study on program failures of deep learning jobs and found that\nthe GPU \"Out of Memory\" fault accounts for 65.0% of the failures in the deep learning specific\ndimension. Since in a large-scale deep learning cluster, GPU failures are inevitable and they cause\nsevere consequences, Liu et al. [97] propose prediction models of GPU failures under large-scale\nproduction deep learning workloads. The prediction model takes into account static parameters\nsuch as GPU type, as well as dynamic parameters such as GPU temperature and power consumption,\nand integrates parallel and cascading architectures to make good predictions of GPU failures.\nFPGA. FPGA is a digital technology designed to be configured by a customer or a designer after\nmanufacturing, hence the term \"field-programmable\". As a neural network accelerator, FPGA is\nthe subject of various studies related to reliability. Radu et al. [147] propose a new probabilistic\nmethod, the Component Failure Analysis (CFA), that uses FPGA specific techniques and algorithms\nfor analyzing SEUs in implemented FPGA designs. McNelles et al. [115] use Dynamic Flowgraph\nMethodology (DFM) to model FPGA, showing the potential advantage of DFM for modeling FPGA-\nbased systems compared with static methods and simulation.\nExamining an FPGA from diverse perspectives leads to varied insights and advantages. Conmy\net al. [29] employ a semi-automated FPTC analysis technique, customized for specific fault types\nidentified on an FPGA, to thoroughly examine individual faults within electronic components.\nThese components support a modularized design embedded on the FPGA. The study demonstrates\nhow the analysis of these individual faults can be seamlessly integrated with crosscutting safety\nanalysis, thereby reinforcing and validating the necessary safety properties. Xu et al. [197] take the\nentire FPGA-based neural network accelerator, including the control unit and DMA modules into\nconsideration. The experiments on four typical neural networks showed that hardware faults can\nincur both system exceptions, such as system stall and prediction accuracy loss.\nTPU. TPU is initially developed by Google to accelerate machine learning workloads, specifically\ntargeting the training and inference of deep neural networks within the TensorFlow framework.\nPablo et al. [13] measure TPU's atmospheric neutrons reliability at different temperatures, that goes\nfrom -40\u00b0C to +90\u00b0C. They show a decrease in the FIT rate of almost 4\u00d7 as temperature increases.\nRubens et al. [81] investigat the reliability of TPU executing 2D-3D convolutions and eight CNNs\nto high-energy, mono-energetic, and thermal neutron. They find that despite the high fault rate,\nmost neutron-induced faults do not change the CNNs detection/classification. Rubens et al. [80]\ninvestigate the reliability of TPUs to atmospheric neutrons, reporting experimental data equivalent\nto more than 30 million years of natural irradiation."}, {"title": "9.1.2 Network.", "content": "Network failures have long been a significant area of research, particularly con-\ncerning traditional faults such as congestion, packet loss, and latency, which have been extensively\ndiscussed and studied. Hassan et al. [83] study how network faults occurring in the links between\nthe nodes of the cloud management platforms can propagate and affect the applications that are\nhosted on the virtual machines.\nHowever, with the advancement of AI systems, the networking infrastructure of AI systems has\nbecome increasingly complex, leading to the emergence of unique fault types. Distributed deep\nlearning training across multiple compute nodes is pretty common and these nodes are internally\ninterconnected with a high-speed network (e.g., via InfiniBand). Gao et al. [45] and C4 [39] classify\nnetwork faults on AI platform into InfiniBand-related and Ethernet-related."}, {"title": "9.1.3 Node.", "content": "A node in AI platform is a distinct schedulable unit for computation with GPUs, CPUs,\nmain memory, disks, and network. Gao et al. [45] classify node faults on AI platform into node\noutage, node damage and node preemption. These faults summarize the impact caused by faults\noccurring inside the node. In addition to these faults, communication faults between nodes are also\nof concern [20]. Thus, we classify node faults into two types, namely node crash, node partition."}, {"title": "9.2 Fault Injection in Al Infrastructure", "content": "Research on FI in GPUs is rich and can be broadly categorized\ninto three types including Software, Hardware/Simulation and Hybrid.\n\u2022 Software. At present, various FI techniques exist at different levels of programming languages.\nCommonly, faults are injected at the GPU assembly code (SASS) level, which is the instruction-\nlevel code running directly on the GPU. For instance, SASSIFI [56] employs the SASSI (SASS\nInstrumentation) framework for compile-time instrumentation of SASS code to insert fault\ninjection code. GPU-Qin [42] utilizes CUDA-GDB to control faults during runtime without"}, {"title": "9.2.1 Hardware Accelerator.", "content": "modifying the code. NVBitFI [186] dynamically loads relevant code as a library during runtime\nfor fault injection. Besides SASS level, there are also works at the PTX and LLVM IR levels. For\ninstance, LLFI-GPU [92] improves FI in LLVM IR, the intermediate representation language.\n\u2022 Hardware/Simulation. Hardware or simulation-based approaches provide a more realistic\nreflection of fault impacts. Direct methods include radiation experiments to evaluate hardware\nreliability. For example, Oliveira et al. [136] use beam tests to study the radiation effects on\nNVIDIA and Intel accelerators, quantifying and limiting radiation effects by observing amplitude\nand fault propagation in final outputs. Simulation approaches replace actual hardware faults\n(e.g., electromagnetic interferences at the physical level) by injecting their expected effects\non memory and registers (e.g., flipped and stuck bits), thus approximating the hardware fault\nprocess [35, 180, 206]. They simulate faults at different levels, such as modifying simulator\nvariables, introducing faults at the RTL level, and injecting faults at the gate level. The NVIDIA\nData Center GPU Manager (DCGM) includes an fault injection framework allows users to simulate\nthe fault handling behavior of the DCGM APIs when GPU faults are encountered [134].\n\u2022 Hybrid. Some research endeavors have sought to combine software and hardware-level ap-\nproaches. Josie et al. [28] combine the accuracy of microarchitecture simulation with the speed\nof software-level FI. It performs detailed microarchitecture FI on a GPU model (FlexGripPlus),\ndescribing the impact of faults on convolutional calculations.\nFPGA. Compared to GPUs, the programmability of FPGAs makes it easier to implement hardware-\nlevel fault injection. There are two major classes for FPGA-based fault injection methods.\n\u2022 Reconfiguration-based techniques. In reconfiguration-based techniques, faults are injected by\nchanging the bit stream needed for configuring FPGA. Antoni et al. introduce a novel methodology\nfor injecting single event upsets (SEUs) in memory elements. This approach involves conducting\nthe injection directly within the reconfigurable FPGA, leveraging the runtime reconfiguration\ncapabilities of the device [6]. Gabriel et al. propose a fault injection tool to evaluate the impact of\nfaults in an FPGA's configuration memory [126].\n\u2022 Instrumentation-based techniques. In instrumentation-based techniques, supplementary\ncircuits are incorporated into the original circuits, and both are integrated within the FPGA\nafter synthesis. Mojtaba et al. propose an FPGA-based fault injection technique [41], which\nutilizes debugging facilities of Altera FPGAs in order to inject single event upset (SEU) and\nmultiple bit upset (MBU) fault models in both flip-flops and memory units. Pierluigi et al. propose\na method that utilizes FPGA devices to emulate systems and employs an innovative system\ninstrumentation approach for fault injection. This approach significantly reduces experimental\ntime without requiring FPGA reconfiguration, achieving notable performance improvements in\nboth compute-intensive and input/output-intensive applications [25].\nTPU. TPU, as a variant of systolic arrays, represents a parallel computing architecture. The\nintrinsic parallelism and matrix multiplication efficiency inherent in systolic arrays empower them\nto achieve superior performance in both the training and inference phases of deep neural networks.\nNumerous studies have been conducted to investigate faults associated with systolic arrays. Udit et\nal. [3] propose an RTL-level fault injection framework for systolic arrays. Using this framework,\nthey characterized the software effect of faults induced by stuck-at faults within the multiply and\naccumulation units of the systolic array. Zhang et al. [205] and Holst et al. [61] study the effects of\ntiming faults in systolic arrays, thus, degrading DNN's accuracy."}, {"title": "9.2.2 Network.", "content": "In addition to general purpose fault injection tools such as ChaosBlade [18] that can\nintroduce common network faults such as network delay and network packet loss, there are now\nsome network fault injection tools for large infrastructure. Domenico et al. propose ThorFI [31], a"}, {"title": "9.2.3 Node.", "content": "Currently, the work of fault injection for nodes mainly focuses on two aspects. On\nthe one hand, it is aimed at the failure of the node itself, which means injecting OS-level faults\ninto the virtual machine or host machine to simulate faults such as node outage. This kind of fault\ncan be implemented by general fault injection tools such as ChaosBlade [18]. On the other hand,\nnode crash is simulated in the process of communication between nodes to test the reliability of\nthe whole system [20, 44, 99]."}, {"title": "9.3 Gap between Failure Analysis and Fault Injection in Al Infrastructure", "content": "From the Table 18, several key observations regarding the gap between FA and FI at the infrastructure\nlevel in AI systems can be made:\nLimited coverage of fault types. The table shows that several types of faults, such as \"Out of\nMemory\", \"Off the Bus\", \"InfiniBand Fault\", and \"Ethernet Fault\", are not currently being simulated\nby any of the listed FI tools. This suggests a significant gap in the ability of current tools to simulate\na comprehensive set of failure scenarios at the infrastructure level in AI systems.\nHardware and network-specific FI The existing tools seem to focus on specific areas of the\ninfrastructure. For example, \"Bit-flip Fault\" and \"Stuck-at Fault\" are well-covered by tools designed\nfor hardware faults like SASSIFI, LLFI-GPU, and ThunderVolt. On the other hand, network-related\nfaults such as \"Network Jam\", \"Network Loss\" are covered by NetLoiter, FCatch. This suggests that\nthe current fault injection tools are specialized for either hardware or network faults but not both.\nLack of realism in hardware accelerator faults. Most of the existing fault injection tools for\nhardware accelerators, such as GPUs, operate at the software level or are based on simulations. The\nfaults generated by these methods can be classified as emulated faults. While emulation provides\nhigh efficiency, a significant drawback is that the faults may lack realism. This is because emulated\nfaults may not accurately represent the complex physical processes that cause real hardware faults.\nAs a result, the conclusions drawn from studies using these tools may not fully apply to real-world\nscenarios where hardware faults occur. This lack of realism in emulated faults represents another\nsignificant gap in the current state of fault injection for AI systems."}, {"title": "10 FUTURE OPPORTUNITIES OF FAULT INJECTION IN AI SYSTEMS", "content": "FI is a widely used technique for evaluating the reliability of AI systems. However, as discussed\nabove, there is a huge gap between FA and FI. Bridging this gap is a research opportunity in future.\n10.1 More Comprehensive Fault Injection\nSupport for more faults types. As shown in previous sections, there are a number of real faults\nthat have occurred that cannot yet be simulated by existing FI tools. For example, no FI tool in\nTable 14 can simulate GPU contention faults in Table 13. Ignoring these faults leads to incomplete FI\ntesting and may hide risks in AI systems. Therefore, based on the FA results in this paper, designing\nFI tools that cover as much as possible all the faults that have occurred historically can provide a\nmore comprehensive assessment of the reliability and fault tolerance of AI systems.\nCross-layer multiple fault injection. Current FI tools typically only inject a fault at a single\nlayer. However, in distributed systems, there are situations where multiple faults occur simultane-\nously [200]. Designing tools that support simultaneous injection of multiple faults can facilitate the\ncreation of more complex fault scenarios. At the same time, considering the dependencies between\nAl system layers can also enable the simulation of richer fault scenarios through cross-layer fault\nlinkage injection. Consequently, the development of tools that facilitate cross-layer multiple fault\ninjection will prove advantageous in evaluating the reliability and fault-tolerance of AI systems\nwhen multiple faults occur at different layers simultaneously.\nLLM-specific FI tool. As LLMs continue to gain prominence in both academic and industrial\nsectors, the importance of assessing their reliability cannot be overstated. Given the unique char-\nacteristics and potential failure of LLMs, there is a pressing need for the development of FI tools\nspecifically designed for these systems. Such tools should be capable of simulating LLM-specific\nfailures, including those related to language understanding, reasoning, and generation.\n10.2 More Generalizable Fault Injection\nCompatible with more layers and frameworks. From the perspective of FI generality, current\nFI tools are in a state of fragmentation. For example, PytorchFI [144] can only inject faults related to\nPyTorch, while TensorFI [36] can only inject faults to TensorFlow. Even for the same AI framework,\nthere may be conceptual differences between versions. For example, TensorFlow 1 and TensorFlow\n2 exhibit significant differences in API usage and runtime logic, requiring separate fault injection\ntools (e.g., TensorFI [36] and InjectTF [114]) to be designed for them. This results in a considerable\nnumber of FI tools that engineers must maintain, as well as a significant amount of time required\nto learn to use them. Consequently, the design of a more unified tool that can inject faults across\ndifferent layers and across different frameworks is of great importance in the future.\nNon-instrumented injection. Numerous contemporary FI tools in Table 8 and Table 11 neces-\nsitate the instrumentation of the target for FI (e.g., modifying the framework source code). This\nincreases the difficulty of utilising FI tools. Given that the majority of frameworks and algorithms\nassociated with AI systems are implemented in Python, it is possible to implement python bytecode\nmodifications that do not necessitate code instrumentation, as was previously the cases [84, 202].\nEven for non-Python implementations, it is possible to achieve non-instrumented FI through\neBPF [185, 201]. Future research into work injection without code instrumentation could facilitate\nthe development of more user-friendly FI tools.\n10.3 More Intelligent Fault Injection\nA FI policy must specify the location of the FI, the type and intensity of the fault, and so forth. This\ncombination of several attributes forms a vast search space for FI policies. The objective is to identify\nvaluable FI policies from this vast search space and to discover as many faults as possible with as\nfew fault injections as possible. Currently, this process relies mainly on expert experience, resulting\nin high and inefficient labour costs. In the future, it is anticipated that intelligent algorithms will be\nintroduced to facilitate the selection of fault injection strategies in an intelligent manner.\nLLM-based Fault Injection. LLM has already demonstrated its capabilities in several software\nengineering tasks [141, 192, 196]. In the future, the integration of LLM and Reinforcement Learning\nfrom Human Feedback (RLHF) will enable the translation of natural language descriptions of\nfault scenarios directly into FI policies, thereby reducing the manual effort required to design and\nimplement fault scenarios in AI systems [30]."}, {"title": "11 CONCLUSION", "content": "In this study, we have examined the current state of FA and FI in AI systems, providing a critical\noverview of prevalent failures, the capabilities of existing FI tools, and the gaps between simulated\nand actual failures. Our analysis, based on a thorough review of relevant paper and code repositories,\nhas revealed significant gaps in the ability of current FI tools to simulate the wide range of failures\nthat occur in real-world AI systems. Moreover, this survey contributes by discussing technical\nchallenges of FI in AI systems and outlining future research avenues. The findings of this study\nserve as a foundation for further advancements in the field of FA and FI for AI systems."}]}