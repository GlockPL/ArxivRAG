{"title": "Improved GUI Grounding via Iterative Narrowing", "authors": ["Anthony Nguyen"], "abstract": "GUI grounding, the task of identifying a precise location on an interface image from a natural language query, plays a crucial role in enhancing the capabilities of Vision-Language Model (VLM) agents. While general VLMs, such as GPT-4V, demonstrate strong performance across various tasks, their proficiency in GUI grounding remains suboptimal. Recent studies have focused on fine-tuning these models specifically for one-shot GUI grounding, yielding significant improvements over baseline performance. We introduce a visual prompting framework called Iterative Narrowing (IN) to further enhance the performance of both general and fine-tuned models in GUI grounding. For evaluation, we tested our method on a comprehensive benchmark comprising different UI platforms.", "sections": [{"title": "Introduction", "content": "GUI Grounding is the task of identifying the visual location on an interface image given a natural language query [4, 8]. This task is essential for improving the capabilities of Vision Language Model (VLM) agents, enabling them to better understand and interact with graphical user interfaces [9]. While general VLMs, such as GPT-4V, have demonstrated high performance across a variety of visual-linguistic tasks, their accuracy in GUI grounding tasks remains suboptimal [4, 9].\nExisting solutions have attempted to bridge this performance gap by fine-tuning VLMs for GUI grounding, resulting in improvements over baseline models [5, 4, 8, 6]. However, these approaches often come with the need for extensive data and re-training.\nInstead of training a model, we propose a visual prompting framework called Iterative Narrowing (IN) to improve the performance of current ones. Our approach treats the model's initial position predictions as an approximation. The framework iteratively refines the prediction by focusing on progressively smaller, cropped regions that center around the previous prediction. This refinement process can be repeated for multiple iterations."}, {"title": "Related Work", "content": "Cheng et al. [4] trained SeeClick, a GUI-specific grounding model and introduced the ScreenSpot benchmark, showing that grounding pre-training enhances task performance. Gou et al. [5] expanded on this by training a pixel-level model on a dataset of 10 million GUI elements, achieving up to 20% accuracy improvements and demonstrating effective human-like visual grounding. Wu et al. [8] introduced a cross-platform dataset of 13 million GUI elements and trained two models: OS-Atlas-Base-4B and OS-Atlas-Base-7B, improving performance across diverse benchmarks."}, {"title": "Iterative Visual Reasoning Beyond Convolutions", "content": "Chen et al. [1] proposed an iterative visual reasoning framework that extends beyond traditional CNNs by incorporating a local module with spatial memory and a global graph-reasoning module for spatial and semantic interactions. The global module builds a graph connecting knowledge, image regions, and class assignments, enabling iterative updates through cross-feeding between the modules to refine predictions."}, {"title": "GUI Agents", "content": "Xie et al. [9] introduced the OSWorld benchmark, designed to evaluate an agent's ability to complete tasks within GUI environments. One of the key categories tested is the screenshot-only scenario, where the model is provided solely with an image of the screen, without any auxiliary information. This scenario emphasizes the importance of strong grounding capabilities, as a common failure case observed by the authors stems from inadequate grounding performance.\nWu et al. [8] expanded on this challenge by integrating their pre-trained grounding model, OS-Atlas-Base-7B, with GPT-4o, demonstrating significant improvements over GPT-4o alone on the OSWorld benchmark. Their results further validate the crucial role of effective grounding in improving task completion in GUI environments.\nHong et al. [6] also addressed GUI grounding by training and evaluating CogAgent on the screenshot-only category of OSWorld. CogAgent was developed to function both as a grounding model as well as a standalone visual agent."}, {"title": "Methodology", "content": "Given an input image and a corresponding text query, our method begins by preprocessing the image to standardize its size to 999 \u00d7 999 pixels. The model is then prompted to predict normalized coordinates (x, y) that identify a relevant location on the image. Using these predicted coordinates, we create an image crop centered around the prediction for the next iteration (see Figure 1). If the original image has a landscape orientation, the new crop has its width halved and its height halved upon each iteration. For portrait-oriented images, the width is reduced to 80% of its original size and the height is reduced to one-third of the original size.\nThis iterative process of generating new coordinates and refining the image crop continues for n iterations. On the final iteration, we take the coordinates generated as the model's prediction of the target location after converting them to be relative to the original image."}, {"title": "Experiments", "content": "We evaluated our method on the ScreenSpot [4] benchmark, a comprehensive assessment tool for single-step GUI grounding across multiple platforms. The ScreenSpot benchmark is structured into three primary categories: mobile, web, and desktop, each designed to reflect common user interface environments. Furthermore, each category is divided into two subcategories based on the type of target element: text or icon.\nFor the entire evaluation, we used n = 3 iterations. While this choice provided promising results, we did not conduct in-depth investigations to determine an optimal value for n or how aggressive each crop should be shrunken. Future work may explore the impact of varying n and the extent of each cropping step on performance and inference time."}, {"title": "Weaknesses & Observations", "content": "A key weakness of our method is the loss of contextual information as iterations progress, posing challenges for identifying target elements that rely on spatially distant cues. GUI-specific models like OS-Atlas-Base-7B [8] can accurately identify such targets in the baseline, while our method often fails due to limited context.\nOur method improves precision but at the cost of underperforming in context-dependent scenarios. This limitation likely explains the greater gains observed in generalist VLMs (e.g., InternVL-2-4B [3, 2], Qwen2-VL-7B [7]) versus OS-Atlas-Base-7B [8], which already has strong precision abilities from training. Addressing this limitation by maintaining context throughout the iterations could further enhance performance for all models."}, {"title": "Preliminary Solutions and Future Work", "content": "To address the loss of contextual information, we conducted preliminary experiments where the model was provided with both the entire screenshot and the current crop during each iteration. This approach was inspired by the work of Chen et al. [1], which introduced a mechanism to leverage both local and global modules that iteratively exchange information for enhanced reasoning. Our goal was to enable the model to retain a global understanding while refining its focus through local crops.\nHowever, during these initial experiments, we observed that the Visual Language Model (VLM) frequently confused the local crop with the global context image. This confusion led to incorrect coordinate predictions, as the model sometimes generated coordinates referencing the entire global image rather than accurately focusing on the local context intended for refinement.\nWe believe that further fine-tuning could help the model more effectively distinguish between global and local contexts, ultimately improving its performance in context-dependent scenarios."}, {"title": "Conclusion", "content": "In this work, we introduced Iterative Narrowing (IN), a visual prompting framework designed to enhance the GUI grounding capabilities of Vision-Language Models (VLMs). Through iterative refinement of predictions, IN improves accuracy by narrowing the focus on progressively smaller regions, allowing models to more precisely identify visual elements on a graphical user interface. Evaluation on the ScreenSpot [4] benchmark demonstrated that IN improves performance, especially for generalist VLMs like InternVL-2-4B [2, 3] and Qwen2-VL-7B [7], where substantial improvements were observed compared to baselines. However, the method shows limitations when handling spatially distant contextual cues, which affects performance in context-dependent scenarios.\nFuture work may focus on addressing these contextual limitations by incorporating both global and local context information more effectively. Promising directions may include refining the model's ability to differentiate between local crops and the entire image. There is potential to further push the boundaries of VLM precision in GUI grounding tasks, contributing to the development of more effective visual agents."}]}