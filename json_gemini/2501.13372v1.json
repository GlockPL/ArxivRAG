{"title": "Generative Data Augmentation Challenge: Zero-Shot Speech Synthesis for Personalized Speech Enhancement", "authors": ["Jae-Sung Bae", "Anastasia Kuznetsova", "Dinesh Manocha", "John Hershey", "Trausti Kristjansson", "Minje Kim"], "abstract": "This paper presents a new challenge that calls for zero-shot text-to-speech (TTS) systems to augment speech data for the downstream task, personalized speech enhancement (PSE), as part of the Generative Data Augmentation workshop at ICASSP 2025. Collecting high-quality personalized data is challenging due to privacy concerns and technical difficulties in recording audio from the test scene. To address these issues, synthetic data generation using generative models has gained significant attention. In this challenge, participants are tasked first with building zero-shot TTS systems to augment personalized data. Subsequently, PSE systems are asked to be trained with this augmented personalized dataset. Through this challenge, we aim to investigate how the quality of augmented data generated by zero-shot TTS models affects PSE model performance. We also provide baseline experiments using open-source zero-shot TTS models to encourage participation and benchmark advancements. Our baseline code implementation and checkpoints are available online\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "In this paper, we introduce a new challenge that accompanies the Generative Data Augmentation workshop at ICASSP 2025. The main research problem the challenge addresses is the typical data shortage issues when a machine-learning model is developed for a particular target user so that the model respects and exploits individuals' specificity. This type of problem contrasts the remarkable advancements driven by deep neural networks (DNNs) in speech technology, including automatic speech recognition (ASR) [1], text-to-speech (TTS) [2]\u2013[5], and speech enhancement (SE) [6], [7]. These models are typically large in size and trained on extensive datasets, designed to perform robustly across diverse inputs; we refer to such systems as generalist systems. However, due to their large size, these systems are challenging to run directly on user devices, necessitating the transfer of user data to servers, which can raise privacy concerns. To address this, smaller personalized models have recently been proposed [8], [9]. These models, being compact, can operate on user devices and achieve high performance by focusing on individual users. In theory, personalized models can be created by fine-tuning pre-trained models using individual-specific data. However, gathering enough personalized data for each individual remains a significant challenge due to privacy concerns and technical difficulties in recording clean voices at test time.\nData augmentation is an essential technique to improve the perfor-mance of many DNN models. In image processing, a variety of data augmentation techniques are employed, ranging from simple methods like rotating or flipping images [10] to generating new images using other state-of-the-art image generation models [11]. Similarly, numerous data augmentation techniques are also being explored in the field of speech [8], [9], [12], [13]. SpecAugment [12] is one of the most commonly used data augmentation methods in speech self-supervised learning and automatic speech recognition models. Recently, with advancements in TTS models enabling near-human-level speech generation, approaches using TTS for data augmentation have been actively attempted [8], [9], [14], [15].\nFurthermore, the performance of zero-shot TTS systems [3]\u2013[5], which can generate speech that mimics a target speaker's characteris-tics from just a short speech signal, has significantly improved. These systems, with the ability to generate an unlimited number of speech samples that reflect a target speaker's characteristics from a single speech signal, are gaining attention as a data augmentation approach to address the data shortage problem of personalized models [9], [16]. Although prior research has demonstrated this potential, studies exploring the relationship between various zero-shot TTS models' performance and downstream tasks remain limited.\nTo conduct a more comprehensive investigation into how zero-shot TTS systems can benefit downstream personalized systems, we pro-pose the Zero-shot Speech Synthesis Challenge for Personalized Speech Enhancement (PSE). We hypothesize that the higher the quality of augmented speech samples generated by zero-shot TTS systems, the better the performance of downstream tasks fine-tuned"}, {"title": "II. CHALLENGE DESCRIPTION", "content": "There are two main technical components in this challenge: the generative models that the participants develop to synthesize per-sonalized speech utterances (i.e., the zero-shot TTS systems) and the downstream PSE task where the synthesized personal speech signals are used as the training target of the PSE systems. The challenge organizers will evaluate the submissions mainly based on their usefulness on the PSE tasks, while some basic speech quality estimation will also be conducted for a comprehensive evaluation of the TTS systems."}, {"title": "A. Zero-Shot TTS Models", "content": "Participants are required to develop a zero-shot TTS system, which is supposed to synthesize new utterances from a short enrollment signal of the target speaker. The organizers will provide from 3 to 14 second-long enrollment signal per target speaker, who is one of ten randomly chosen speakers from the LibriTTS test-clean dataset [17] (Sec. III-A) or another ten virtual speakers we create for the challenge (Sec. III-B). Additionally, the organizers provide 50 text sentences per target speaker for the participating TTS systems to synthesize corresponding speech signals. This will be used to evaluate the overall quality of generated utterances from zero-shot TTS systems. For the evaluation details of the zero-shot TTS system, please refer to Section IV-A.\nSynthesized signals from zero-shot TTS systems are supposed to be used to train the PSE systems, whose SE performance is assumed to be associated with the TTS systems' performance, i.e., the better the synthesized speech preserves the target speaker identity and speech quality, the more useful it is to personalize a speech enhancement system. The organizers impose no restrictions on the number of utterances the zero-shot TTS system generates for training the PSE systems. While there are no other specific restrictions on the zero-shot TTS model in terms of its architecture or how it is trained, the LibriTTS test-clean dataset must be excluded in the TTS model's training as it is used for testing."}, {"title": "B. The Downstream Personalized Speech Enhancement Task", "content": "PSE is a category of SE methods designed to train specialized models for individual users at test time. Here, we assume that PSE systems are personalized not only to specific speakers but also to the noise environments that particular users frequently encounter. Unlike the speaker and noise-agnostic SE models, which are trained to generalize to any arbitrary test speakers and noises, PSE models focus on the specific speaker and noise types of interest. Ideally, personalization could be achieved by training an SE model by setting up the clean speech of the target speaker as the denoising target and noisy speech with specific noise types as the noisy input. However, in practice, such personal data are difficult to acquire due to privacy concerns and the technical difficulties of recording the clean voice samples from the test-time user.\nSuccessful TTS systems can resolve the data shortage issue. They can synthesize virtually as many clean speech signals as a PSE model needs for its personalized training. Since the main goal of personalization is to narrow down its usage to the target user, it is also crucial for the downstream task to be able to use personality-preserving, high-quality speech signals. Likewise, PSE performance is sensitive to the quality of participants' TTS systems, making it a suitable task for evaluating the generative data augmentation systems.\nParticipants are asked to use their synthesized speech to develop 20 PSE models for 20 target speakers, respectively. In addition, the organizers provide a test set consisting of 45 noisy utterances for each speaker, which are mixtures of nine clean utterances and five speaker-specific types of noise sources (Sec. III-C). The input signal-to-noise ratio (SNR) is randomly selected from {-2.5, 0, 2.5} dB. Participants are required to submit enhanced speech for these samples. For a fair comparison, participants are first requested to enhance the noisy signals with the baseline PSE model architecture that organizers provide. Then, participants can optionally provide results from their own model architectures. We strongly encourage participants to build lightweight PSE models as one of the main benefits of personalization is about being able to reduce model size. To this end, the complexity of the model must be reported as well."}, {"title": "III. CHALLENGE DATASETS", "content": ""}, {"title": "A. Real-World Speakers", "content": "From the LibriTTS [17] dataset's test-clean fold, five male and five female speakers are randomly selected as the personalization target. After excluding utterances that are either shorter than 3 seconds or longer than 16 seconds, we collect 60 utterances from each speaker that are then divided into three subsets: one utterance for enrollment, 50 for evaluating the generated speech directly, and nine for testing the PSE systems."}, {"title": "B. Virtual Speakers", "content": "In order to propose a solution and draw attention to the potential ethics and privacy issues that revolve around using the speech corpus as a seed to synthesize personalized speech, we additionally adopted ten virtual speakers as target speakers. Similar to the real-world speakers, these virtual speakers include five male and five female speakers. However, unlike real-world speakers, who primarily exhibit reading-style speech, we aimed to leverage the diversity of TTS models to generate virtual speakers with various accents and emotional expressions. Note that all the challenge configurations remain the same for these ten speakers as well. The significant difference is that these virtual speakers were generated using a state-of-the-art TTS system provided by Meta."}, {"title": "C. Noise Sources", "content": "Not only to personalize the PSE systems for speakers but also to adapt to noisy environments, we designate five specific noise types per target speaker, which are randomly chosen from the sound-bible subset of the MUSAN [18] dataset."}, {"title": "IV. EVALUATION METRICS", "content": ""}, {"title": "A. Basic TTS Performance Evaluation", "content": "Although the challenge's main objective is to prove the generative models' capabilities in the downstream task, we can conduct some basic performance evaluation of the synthesized data themselves. To this end, the zero-shot TTS system's performance is evaluated across three aspects: speaker similarity, intelligibility, and perceptual quality. To measure speaker similarity between the generated speech and the reference speech, we calculate the cosine distance of speaker embeddings (SECS). X-vectors are extracted using a pre-trained speaker verification model from SpeechBrain [19] and utilized to calculate SECS. To assess the intelligibility of the generated speech, we employ the open-source Whisper model [1] for speech recognition and calculate the word error rate (WER).\nSubjective evaluations are commonly conducted to assess the perceptual quality of TTS models. However, conducting these evalu-ations across numerous models poses practical challenges. Recently, neural network-based perceptual quality metrics have been widely studied [20], [21] and increasingly adopted [9], [22], [23]. In this work, we chose to use the UTMOS [24] metric\u2014one of the best-performed MOS prediction networks in VoiceMOS Challenge 2024 [25]\u2014to evaluate the perceptual quality and naturalness of the gener-ated speech."}, {"title": "B. Evaluation metrics for PSE models", "content": "To evaluate the performance of the PSE models and examine the effectiveness of incorporating augmented speech from zero-shot TTS systems, we employ four metrics commonly used in SE: signal-to-distortion ratio improvement (SDRI), signal-to-distortion ratio (SDR) [26], extended short-time objective intelligibility (eSTOI) [27], and perceptual evaluation of speech quality (PESQ) [28]. All the metrics are based on the direct comparison between the participants' submissions, i.e., the enhanced versions of the noisy test signals and the held-out ground-truth clean speech signals. SDRI measures the improvement in SDR, indicating how effectively the model reduces distortion relative to the input. SDR evaluates the absolute quality of the enhanced signal by comparing the energy ratio between the target and distortion signals, reflecting the fidelity preservation of the model's output. eSTOI, a measure of intelligibility, assesses how well the model retains short-time temporal patterns. Lastly, PESQ evaluates the perceptual quality of generated speech by simulating human auditory perception."}, {"title": "V. BASELINE MODELS", "content": "In this section, we introduce the baseline zero-shot TTS and PSE models and report their performances as a reference for the participants."}, {"title": "A. Baseline zero-shot TTS models", "content": "We use three open-source zero-shot TTS models as the baseline models: YourTTS [3], SpeechT5-based zero-shot TTS model [29], and XTTS [4]. YourTTS is built on VITS [2] and conditions it via a speaker embedding extracted from an external pre-trained speaker verification model. SpeechT5 is a pre-trained encoder-decoder model for various spoken language processing tasks, including its TTS application, which we use as a baseline. Finally, XTTS is a zero-shot TTS model built on Tortoise [30], which incorporates a decoder-only Transformer with some modifications to improve voice cloning and enable faster training and inference."}, {"title": "B. Baseline PSE models", "content": "We adopt the ConvTasNet [31]-based architecture for our PSE models based on the original PSE model architecture proposed in [9], [32]. Following their recipe, the generalist models are first trained on LibriSpeech [33] and FSD50K [34], which have clean and noise-mixed speech samples, respectively. To introduce artificial noise addition, we utilize MUSAN [18] datasets. Given that reduced size is one of the main advantages of the personalized system, we focus on the medium, small, and tiny models from [9], [32], containing 437K, 224K, and 138.8K parameters, respectively.\nThen, we fine-tune the generalist model into a test speaker-specific version for each test speaker, using the personalized speech datasets synthesized by the TTS models. To this end, we run the zero-shot TTS models to generate 40 new clean utterances per speaker. These synthesized signals work as the training target and are mixed with speaker-specific noise sources to create corresponding noisy input mixtures. In addition, we also provide an oracle performance by fine-tuning the generalist model with the ground-truth clean speech, i.e., 40 actual utterances from the same test speaker. Since the duration of these 40 utterances is about six minutes, we refer to these PSE models as 6min models. To investigate the performance gains from using additional synthesized speech, we generate 180 additional utterances for fine-tuning, resulting in 220 training utterances (about 30 minutes). We refer to the PSE models with this extended dataset as 30min models. Note that fine-tuning uses 10 and 30 validation utterances per speaker for the 6min and 30min models, respectively.\nWe use the negative SDR loss function as in [9]. For the optimiza-tion, Adam [35] is used with a low learning rate of $10^{-6}$. The batch size is 8. We stop fine-tuning if the validation loss does not improve after 20 epochs. The input mixtures are with a randomly selected SNR value from the range of [-5, 5]."}, {"title": "C. Results", "content": "Basic TTS performance: The performance of zero-shot TTS models for real-world and virtual speakers is detailed in Table II and Table III, respectively. SpeechT5 achieves the best speaker similarity (SECS) and intelligibility (WER) scores, despite having the lowest perceptual quality (UTMOS) scores in both cases. For real-world speakers, XTTS achieves the highest UTMOS score but the worst WER score, while for virtual speakers, it ranks second in both the UTMOS score and WER score. We assume that the diversity of accents and emotions among the virtual speakers, along with some artificial noise already present in the reference speech, influenced these performance outcomes. Overall, each TTS system has its own unique property, which can be measured in different ways. Next, we examine how these properties affect the downstream task performance when used as a data augmentation method.\nPSE performance: The PSE results for real-world and virtual speakers are detailed in Table IV and Table V, respectively. Across all model sizes, the generalist models performed the worst on every evaluation metric. This implies that even PSE models built using the lowest-performing zero-shot TTS systems achieved significantly better performance than the generalist SE model. This demonstrates the effectiveness of personalized data augmentation using external generative models. For all sizes, the GT-6min model achieved the highest scores across all metrics, outperforming all 30min models. This suggests that the data quality is crucial for PSE performance; even with a larger dataset of lower quality, performances were inferior to those achieved with a smaller amount of high-quality data. Compared to [9], where some TTS models did not introduce PSE improvement, this time, the adaptation to the noise sources could have contributed to better PSE performance.\nWhen comparing the 6min and 30min models fine-tuned with augmented data from various zero-shot TTS models, we observed that in most cases, PSE performance improved as the amount of the augmented data increased, although the improvement is marginal. For both real-world and virtual speakers, the SpeechT5-30min model achieved the best performance in SDRI, SDR, and eSTOI. Given the high speaker similarity of the SpeechT5 model, we believe that speaker similarity is an important factor in building an effective PSE model. The XTTS-30min model achieved the highest PESQ scores for real-world speakers and for medium-sized PSE model for virtual speakers, while the SpeechT5-30min model performed best for small and tiny-sized models for virtual speakers. Since PESQ focuses on perpetual quality, the XTTS model's high perceptual quality likely contributed to its strong PESQ performance."}, {"title": "VI. DISCUSSION AND FUTURE WORK", "content": "In our baseline experiments, we demonstrated the potential of zero-shot TTS models for data augmentation in PSE applications. We also highlighted the importance of adaptation data quality for PSE model performance. Speaker similarity and intelligibility emerged as the most relevant factors, with perceptual quality also influencing PSE outcomes. However, as the number of zero-shot TTS models in our baseline experiments was limited, we anticipate that this challenge will enable a more in-depth exploration of the relationship between TTS model performance and PSE outcomes through a broader variety of zero-shot TTS systems. Data augmentation stands to benefit significantly from advances in generative AI, though its application requires careful consideration due to the complex nature of synthetic data usability. We also explored virtual speakers as a privacy-preserving alternative. A possible application is to build a PSE model that reflects the target speaker's characteristics using virtual speakers, thereby addressing privacy concerns associated with collecting target speaker data. In the future, the organizers plan to expand this challenge to additional downstream tasks."}]}