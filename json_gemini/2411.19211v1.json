{"title": "On the Ethical Considerations of Generative Agents", "authors": ["N'yoma Diamond", "Soumya Banerjee"], "abstract": "The Generative Agents framework recently developed by Park et al. has enabled numerous new technical solutions and problem-solving approaches. Academic and industrial interest in generative agents has been explosive as a result of the effectiveness of generative agents toward emulating human behaviour. However, it is necessary to consider the ethical challenges and concerns posed by this technique and its usage. In this position paper, we discuss the extant literature that evaluate the ethical considerations regarding generative agents and similar generative tools, and identify additional concerns of significant importance. We also suggest guidelines and necessary future research on how to mitigate some of the ethical issues and systemic risks associated with generative agents.", "sections": [{"title": "Introduction", "content": "The seminal work by Park et al. [1] introduced the Generative Agents framework for simulating human behaviour using generative language models. Generative agents have the ability to operate independently and creatively, making decisions to reach a goal with minimal user input. To do this, each agent maintains a distinct and dedicated set of memories and behavioural parameters that are provided to a generative language model as needed. By recording memories, prompting reflection on them, planning future actions, and reacting to short-term observations, generative agents can produce realistic emulations of human behaviour and decision-making [1]. As a result, generative agents have massive potential for widespread adoption and may revolutionise many modern problems. However, this introduces a number of critical ethical challenges regarding their research, development, and application. In this position paper, we examine the existing literature on the ethical aspects of generative agents, highlight a handful of critical ethical concerns, and discuss recommendations for their mitigation. We note that the concerns posed in this work are not exhaustive or entirely exclusive to generative agents, but are believed to be especially important to discuss in this specific context."}, {"title": "Existing Literature", "content": "Bail [2] provides a detailed qualitative analysis and discussion of the potential benefits of generative artificial intelligence (GAI) and generative agents towards social science. With respect to"}, {"title": "Ethical Concerns of Generative Agents", "content": ""}, {"title": "Anthropomorphisation and Misunderstanding of Experimental Results", "content": "Anthropomorphisation can serve as an effective means to discuss and rationalize the behaviour of AI tools, such as generative agents. However, generative agents and other GAI tools as they exist today do not actually possess any level of consciousness and are incapable of producing true emo-"}, {"title": "Creation of Parasocial Relationships", "content": "Anthropomorphisation also risks the creation of parasocial relationships between AI and its users [7, 10]. This is particularly true for generative agents, which may be individualised and/or physically embodied in future applications, such as explored by Gabriel et al. [10]. The structure of generative agents lend themselves towards the creation of individualised agents which non-technical users may develop relationships. Individualised or embodied generative agents may be an effective tool toward solving day-to-day problems\u2014much like existing virtual assistants such as Siri and Google Assistant. However, the persona, memory, and reflection characteristics of generative agents allow them to be perceived as more similar to humans than existing virtual assistants despite not being fundamentally any more \u201cintelligent\". As a result, the users of such agents risk harmfully anthropomorphising GAI and developing harmful relationships or attachments to these tools.\nWhile Gabriel et al. [10] discuss ways to mitigate risks induced by parasocial relationships, they mainly focus on post hoc utilitarian and material approaches to optimise reliable agent performance in cases of human-AI relationships, rather than avoiding them altogether. Park et al. [1] and Aber-crombie et al. [11] suggest that generative agents and other GAI tools should be designed to ex-plicitly state their nature as generative models and directly avoid anthropomorphic language andbehaviour. However, little consideration has yet to be given to how this may be reliably imple-mented and the effectiveness of such approaches. In particular, many of these methods burden theuser with identifying and mitigating parasocial relationships. Such approaches may prove unhelp-ful or even counterproductive if users ignore, misinterpret, or intentionally circumvent the providedwarnings or guardrails [9]. Further, anthropomorphic characteristics are not objectively identifiableand the boundaries distinguishing anthropomorphic behaviour are becoming exceedingly unclear.As a result, approaches to reliably avoid anthropomorphic behaviours will be extremely difficult todevelop.\nAs a simple example, the implementation provided by Park et al. [1] for their generative agent sim-ulation framework defers to the underlying generative model to identify unsafe behaviour. Uponquery, the generative model is prompted to score how much a user's prompt anthropomorphizes theagent. If the score is greater than a critical threshold, then the user is told that attributing humanagency to generative agents is inappropriate and their query is rejected. This aims to ensure that theframework is being used safely. However, this method is very simplistic and relies on the untestedassumption that the underlying model is capable of accurately identifying harmful anthropomorphi-sation. In reality, the ability to identify such characteristics will vary greatly between models andis not provably accurate. In addition, this system provides no guardrails to prevent the user fromignoring or intentionally circumventing these checks."}, {"title": "Excessive Trust and Insufficient Scepticism", "content": "Similar to concerns regarding the misattribution of human characteristics, over-reliance and overcon-fidence in generative agents may result in the unintentional spread of misinformation. Specifically,users may prescribe undue trustworthiness to generative agents [9, 10, 12, 13]. Intuitively, gen-erative agents should be more capable than existing technologies (such as AI chatbots or virtualassistants) due to their memory and reflection modules, as these modules allow generative agents tobetter recall information and develop critical insights [1]. However, generative agents are still justan extension of existing language models and thus are prone to the same mistakes and errors. Sucherrors include hallucinations, poisoning, failure to recall available information, or recapitulating andreinforcing endemic biases. Further, the stochasticity of generative language models can induceunreliable and/or inaccurate behaviour from generative agents [2, 14].\nA lack of critical analysis and scepticism of responses produced by generative agents runs the riskof unintentionally spreading misinformation and reinforcing harmful biases [9, 10, 12, 13]. Gabrielet al. [10] specifically discuss the angle of (misplaced) trust and misinformation associated with AIassistants. In particular, they identify varying types of trust that a user may have in GAI-based sys-tems like generative agents, such as overestimating the competence of an agent or the quality of itsalignment. They also point out that undue trust makes people highly vulnerable to misinformation,manipulation, and ideological entrenchment-dynamics highly similar to those observed in humans,and potentially even more concerning if generative agents face widespread adoption. To addressthese risks, Gabriel et al. [10] collate a handful of technical design and policy-based proposals, suchas identifying and indicating uncertainty, minimising (un)necessary complexity, and improving tech-nical transparency and public understanding of GAI systems. However, many of these approachesrequire severely limiting desirable agent functionality or leveraging external response analysis meth-ods which are vulnerable to circumvention. Thus, they may only serve as stop-gap solutions. Asa result, there is still a significant need for further research and development of techniques that caninherently mitigate these concerns.\nIdeally, internal technical mechanisms should be developed to provably ensure that provided infor-mation is true and impartial, or otherwise marked as uncertain or potentially biased. To this end,some researchers have suggested that retrieval-augmented generation (RAG) techniques may pro-vide a technical solution to mitigate generative agents spreading or unintentionally creating misin-formation by leveraging access to external knowledge resources during inference [15\u201317]. However,we were unable to identify any literature applying these techniques to memory-/reflection-enabledgenerative agents as proposed by Park et al. [1]. Further, RAG simply provides more reliable accessto information and does not inherently prevent them from producing misinformation. It is possiblefor RAG-based agents to produce erroneous inferences that are only partially informed by retrievedinformation. That is, RAG-enabled generative agents may draw incorrect conclusions when pro-vided with incomplete or highly complex knowledge. In addition, information retrieval can itselfunintentionally provide the model with misinformation under certain circumstances, such as via de-sign error or scope misalignment. This can cause even greater harm due to the elevated credibilitythat would likely be attributed to RAG-enabled generative agents [15, 17]."}, {"title": "Usage by Malicious Actors", "content": "The prevalence of automated bots has become a key driver in the spread of misinformation online,significantly harming access to trustworthy and accurate information [9, 10, 18\u201322]. Simultaneously,GAI tools such as ChatGPT are beginning to be applied toward the creation of automated scams andphishing attacks [9, 10, 23, 24]. As automated means for conducting malicious acts become moreprevalent, generative agents may be particularly susceptible to misuse. Malicious actors can leveragegenerative agents as automated tools to spread disinformation, execute scams, or conduct cyberat-tacks. The distinct memory and reflection characteristics of generative agents make them capable ofperforming malicious actions (such as spreading misinformation or conducting scams) with greaterrealism and effectiveness than existing technologies. Thus, malicious generative agents may be bet-ter at deceiving humans and avoiding automated detection than existing approaches. Gabriel et al.[10] briefly discuss the types of security vulnerabilities GAI systems introduce to users, and howmalefactors' abilities may be enhanced by GAI systems leveraging memory, planning, and reflec-tions capabilities like those proposed by Park et al. [1]. However, they do not consider specificconcerns posed by automated generative agents."}, {"title": "Vulnerability to Hijacking", "content": "In addition to direct usage by malicious actors, developers of generative agents must be wary of theirvulnerability to hijacking or jailbreaking. GAI tools are prime targets for attacks that aim to derailsystem behaviour. This is due to their usage in a wide variety of applications, the transferability of at-tacks between implementations, and the difficulty of developing effective behavioural guardrails [27,28]. As with previously discussed concerns, generative agents' memory and reflection capabilitiesmake them highly desirable and effective for an increasing range of tasks, raising direct concernsabout hijacking as they see greater adoption. Hijacked generative agents may be difficult to recover,have access to more sensitive information and actions than other GAI tools, and potentially automat-ically hijack other agents or systems like a worm virus. This is especially concerning as substantialresearch has already been conducted towards developing techniques to compromise generative lan-guage models [27-31]. Notably, the desire to hijack GAI systems is not exclusive to maliciousactors, as normal users may also directly benefit from hijacking (or \u201cjailbreaking\u201d) automated AI-based tools [32\u201334]. Thus, hijacking and jailbreaking serve both as a threat vector for external actorsto harm users of generative agents and for users to circumvent their safety features [9, 10].\nIn response to these concerns, many authors suggest a need to improve approaches to detect, under-stand, and mitigate agent hijacking [9, 10]. However, we believe a simpler approach should be con-sidered with greater interest; critical assessment of the usage of generative agents altogether. Giventhe difficulty of detecting and preventing model hijacking and our as-yet lacking understanding ofthese problems [9], developers must be wary of where, when, how, and if they should implementgenerative agents at all. In particular, generative agents should not be utilised in contextswhere their hijacking may enable significant threat vectors, such as those suggested by Anwar et al. [9],Gabriel et al. [10], and Greshake et al. [29]. In cases where it may not be possible to eliminate theusage of vulnerable generative agents, safeguards must be developed to prevent agents from beingactionable when hijacked. For example, sensitive information or services may require secondaryhuman authentication to be accessed, or generative agents may be sandboxed to prevent a hijackedagent from spreading undesirable behaviour to other agents or systems."}, {"title": "Displacement of Human Labour", "content": "Discussion of the ethical concerns of any technology would be incomplete without analysis of itseffect on human labour. This is especially true for generative agents, as automated generative agentsmay be highly attractive to organisations that wish to automate tasks that are generally performedby humans. Generative language models have already begun to replace humans in both low- andhigh-skill occupations, such as customer service agents [35], translators [36], and varying forms ofknowledge experts [36, 37]. As a result, generative agents are expected to have significant impacton the quantity and quality of human labour, among other socioeconomic factors [9, 10]\nCurrently, AI tools are still not sophisticated enough to fully replace humans in many positions [38].However, the design benefits of generative agents potentially introduce the necessary capabilities tobe effectively leveraged in many of these applications. As a result, overzealous usage of generativeagents may cause the rapid displacement of human workers across many occupations in a particularlydisruptive and unprecedented manner [9]. As such, organisations and researchers should prioritizetechniques that use generative agents as collaborative tools to supplement human workers instead ofattempting to replace human labour. This approach is validated by the extant literature, which asserts that human-AI collaboration is highly effective at improving productivity while simultaneously not harming the current human workforce [39\u201343]."}, {"title": "Exploitation of Developing Nations and Modern Slavery", "content": "As demand grows for generative agents and other GAI-based tools, so will the necessity to man-ufacture physical hardware and electronic devices capable of leveraging them (such as GPUs andsmartphones). Much of the natural and human resources used in the manufacture of these devicescomes from developing nations, where there are significant risks of exploitation and contributingto modern slavery [44-47]. Notably, we were unable to find any literature providing meaningfuldiscussion of these concerns as they relate to GAI, despite substantial documentation of the impactsof the physical resources they require. In particular, the mining of silica-a primary componentof computer chips-and lithium-a primary component of batteries and other materials used tomanufacture these electronic devices can pose significant environmental and personal health risksfor miners and their communities [44, 45, 48-50].\nIndividuals and organisations wishing to develop and use generative agents or similar tools mustensure that they are not contributing to these risks. This can be done by evaluating the hardware re-quirements associated with using generative agents and minimising them wherever possible. Futurework should aim to improve the efficiency of generative agents and the required hardware to reducethe need for materials and components whose manufacture and usage may contribute to exploitationand modern slavery. Further, independent audits should be conducted on manufacturers and othersections of the supply-chain to ensure adherence to these principles [47]. Finally, requirements forsuch hardware should be avoided wherever possible, such as through the using simpler and moresustainable techniques or eliminating superfluous usage of generative agents."}, {"title": "Environmental Impact", "content": "Alongside increased demand for AI-based tools, the environmental impact and effective carbon foot-print of GAI systems have also increased substantially [10, 51\u201354]. Gabriel et al. [10] identify arange of ways in which the creation and usage of AI assistants, a notable potential application ofgenerative agents, can impact the environment. Specifically, high energy usage during training andinference, emissions embodied by the production of required hardware, and supporting environmen-tally irresponsible industries and applications all pose substantial environmental risk. Further, thenature and magnitude of embodied emissions associated with model training and the manufactureof required hardware are only just beginning to be investigated and understood [10, 54\u201357].\nTo mitigate these concerns, Gabriel et al. [10] suggest a number of emissions-reduction approaches,such as minimising model size, improving hardware efficiency, sourcing carbon-free energy, andimplementing public policy to encourage environmentally sustainable development and deployment.However, these approaches heavily rely on the accessibility and feasibility of low-emissions re-sources and techniques. As such, similar to the discussed concerns about hijacking, we believe asimpler and more feasible approach would be to minimise or eliminate the usage of generative agentswherever possible. Specifically, the implementation of generative agents must be critically analysedwith respect to their necessity and weighed against their environmental impact to avoid superfluousor redundant usage [51, 54, 56, 57]."}, {"title": "Conclusion", "content": "In summary, the development and application of generative agents present many ethical challengesand concerns. Ethical problems arise from all sides of GAI usage, including developers, maliciousactors, and normal users. Frivolous implementation, unsafe or inefficient design, unreliable anduntrustworthy behaviour, and user error all pose significant threats to the ethical usage of generativeagents. Given these challenges, continued overzealous acceleration of generative agent developmentneeds to be considered critically and addressed. Currently, the ethical evaluations and impacts ofnewly developed GAI and generative agent techniques are often left as an afterthought, if they arediscussed at all. Thus, future research and applications of generative agents should directly accountfor the ethical concerns posed in this work and those identified in other works, such as Bail [2], Lazar[7], Chan et al. [8], Anwar et al. [9], and Gabriel et al. [10], among others. To this end, researchers,developers, and legislators should apply the proposed mitigation approaches wherever possible, andcreate new mitigation techniques where solutions have yet to be developed."}]}