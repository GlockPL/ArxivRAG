{"title": "The Multi-Faceted Monosemanticity in Multimodal Representations", "authors": ["Hanqi Yan", "Xiangxiang Cui", "Lu Yin", "Paul Pu Liang", "Yulan He", "Yifei Wang"], "abstract": "In this paper, we leverage recent advancements in feature monosemanticity to extract interpretable features from deep multimodal models, offering a data-driven understanding of modality gaps. Specifically, we investigate CLIP (Contrastive Language-Image Pretraining), a prominent visual-language representation model trained on extensive image-text pairs. Building upon interpretability tools developed for single-modal models, we extend these methodologies to assess multi-modal interpretability of CLIP features. Additionally, we introduce the Modality Dominance Score (MDS) to attribute the interpretability of each feature to its respective modality. Next, we transform CLIP features into a more interpretable space, enabling us to categorize them into three distinct classes: vision features (single-modal), language features (single-modal), and visual-language features (cross-modal). Our findings reveal that this categorization aligns closely with human cognitive understandings of different modalities. We also demonstrate significant use cases of this modality-specific features including detecting gender bias, adversarial attack defense and text-to-image model editing. These results indicate that large-scale multimodal models, equipped with task-agnostic interpretability tools, offer valuable insights into key connections and distinctions between different modalities.", "sections": [{"title": "1. Introduction", "content": "To understand how the brain synthesizes information from the different senses, we must study not only how information from each sensory modality is decoded but also how this information interacts with the sensory processing taking place within other sensory channels. Calvert et al. (2004)\nMultimodal models have become foundational in the development of artificial intelligence systems, enabling the processing and understanding of information from multiple data modalities, such as vision and language (Radford et al., 2021; Kim et al., 2021; Lu et al., 2019; Liang et al., 2024). These models are built on the premise that different data modalities share common, or cross-modal, features that can be jointly learned (Ngiam et al., 2011). However, it is widely acknowledged that certain features are modality-specific; for example, some emotions are difficult to visualize, while certain visual experiences cannot be accurately described through language (Paivio, 1991).\nThe exploration of modality commonality and gaps has long been a focus in cognitive science, where researchers have investigated how humans integrate and differentiate information across sensory modalities (Spence, 2011). However, these studies are often human-centric and may not directly translate to artificial systems due to fundamental differences in how information is processed and represented (Calvert et al., 2004). Meanwhile, recent advances in interpretability methods, particularly in the area of monosemantic features, provide a promising path towards a more detailed understanding of deep models (Elhage et al.; Bills et al., 2023; Gurnee et al., 2023; Yan et al., 2024). Monosemantic features/neurons refer to model components that correspond to a single, interpretable concept or feature. By leveraging these methods, we can extract monosemantic, interpretable features from deep learning models, providing a data-driven approach to exploring modality gaps.\nIn this paper, we focus on CLIP (Contrastive Language-Image Pretraining) (Radford et al., 2021), a visual-language representation model trained on massive image-text pairs. We investigate the modality association of features extracted from CLIP by introducing a modality metric that categorizes these interpretable features into: vision, language and visual-language features.\nOur study reveals that single-modal features align well with human cognition and highlight diverse aspects of the visual-language modality gap. We find that visual-language features capture modality-aligned semantics. These findings"}, {"title": "2. Towards Multimodal Monosemanticity", "content": "In this section, we build a pipeline to extract monosemantic multimodal features and evaluate interpretability of these features. We also analyze the modality relevance within the extracted features using the proposed Monosemantic Dominance Score (MDS).\nWe consider two CLIP models, the canonical ViT-B-32 CLIP model from OpenAI (Radford et al., 2021) and a popular CLIP variant, DeCLIP (Li et al., 2022). Beyond multimodal supervision (image-text pairs), DeCLIP also incorporates single-modal self-supervision (image-image pairs and text-text pairs) for more efficient joint learning. We hypothesize that, with the incorporation of self-supervision tasks, DeCLIP is able to extract more single-modal features from the data, enhancing its interpretability and alignment with modality-specific characteristics."}, {"title": "2.1. Interpretability Tools for Multimodal\nMonosemantic Feature Extraction", "content": "Features in deep models are observed to be quite polysemantic (Olah et al., 2020), in the sense that activating samples along each feature dimension often contain multiple unrelated semantics. Therefore, we first need to disentangle the CLIP features to obtain monosemantic features. Building on recent progress in achieving monosemanticity in self-supervised models, we study two methods aimed at improving multimodal monosemanticity.\nMultimodal SAE. Sparse Autoencoders (SAEs) (Cunningham et al., 2023) are a new scalable interpretability method, that has shown success in decomposing polysemantic neurons into interpretable, monosemantic features across various LLMs (Templeton, 2024; Gao et al., 2024; Lieberum et al., 2024). Here, we train a multimodal SAE (MSAE) g+ by using a single SAE model to reconstruct both image and text representations. Specifically, we adopt a top-K SAE model (Makhzani & Frey, 2013; Gao et al., 2024),\n(latent) $z = \\text{TopK} (W_{enc} (x - b_{pre}))$,\n(1)\n(reconstruction) $x = W_{dec} z + b_{pre}$,\n(2)\nand train it with a multimodal reconstruction objective.\n$\\mathcal{L}_{M-SAE}(g) = \\mathbb{E}_{(x_i, x_t) \\sim \\mathcal{P}} [(x_i - g(x_i))^2 + (x_t - f(x_t))^2]$,\nwith $W_{enc} \\in \\mathbb{R}^{n \\times d}$, $b_{enc} \\in \\mathbb{R}^n$, $W_{dec} \\in \\mathbb{R}^{d \\times n}$, and $b_{pre} \\in \\mathbb{R}^d$. In this way, the sparse latent feature $z \\in \\mathbb{R}^n$ can encode multimodal representations from both modalities.\nMultimodal NCL. Inspired by the interpretable self-supervised loss with a non-negative constraint (NCL) proposed by (Wang et al., 2024) to extract sparse features, we adapt it to enhance multimodal interpretability. Specifically, given a pretrained CLIP model with an image encoder $f_i$ and a text encoder $f_t$, we train a shared MLP network (of similar size to SAE) on top of the encoder outputs using a Multimodal NCL loss:\n$\\mathcal{L}_{M-NCL}(g) = -\\mathbb{E}_{x_i, x_t \\sim \\mathcal{P}} \\log \\frac{\\exp(g(f_i(x_i)) + g(f_t(x_t)))}{\\mathbb{E}_{z} \\exp(g(f_i(x_i)) + g(f_t(x_{\\bar{t}})))}*$\n(3)\nThe MLP network $g: \\mathbb{R}^d \\rightarrow \\mathbb{R}^n$ is designed to have non-negative outputs, e.g.,\n$g(x) = \\text{ReLU}(W_2 \\text{ReLU}(W_1 x + b_1) + b_2), \\forall x \\in \\mathbb{R}^d$.\n(4)\nWang et al. (2024) showed that the non-negative constraints allows NCL to extract highly sparse features, significantly improving monosemanticity."}, {"title": "2.2. Measures for Multimodal Interpretability", "content": "Existing quantitative interpretability measures (Bills et al., 2023) often rely on expensive models (like GPT-40) and face challenges with scalability and precision (Gao et al., 2024), hindering advancements in open science. This motivates us to propose more scalable alternatives, as outlined below.\nEmbedding-based Similarity. We propose a scalable interpretability measure based on embedding models that can be applied to both images and text.\u00b9 For each image/text feature z, we select the top m activated image/text samples for this dimension, and denote their embeddings as $Z_+ \\in \\mathbb{R}^{m \\times d}$. Similarly, K random samples are encoded into $Z_- \\in \\mathbb{R}^{m \\times d}$ as a baseline. Then, we calculate the inter-sample similarity between the selected samples, $S_+ = Z_+ Z_+^T \\in \\mathbb{R}^{m \\times m}$ and $S_- = Z_- Z_-^T \\in \\mathbb{R}^{m \\times m}$. The monosemanticity of z is measured by calculating the relative difference between the two similarity scores:\n$I(z) = \\frac{1}{m(m - 1)} \\sum_{i \\neq j} \\frac{(S_+)_{ij} - (S_-)_{ij}}{(S_-)_{ij}}$\n(5)\nThe overall interpretability score is the average across all features: $I = \\sum_{i=1}^n I(z_i)$. A higher score indicates that the extracted features exhibit more consistent semantics.\nWinRate. Since the representations from different embedding models (e.g., vision and text) are not directly comparable, we propose similarity WinRate, a binary version of the relative similarity score. This is calculated by counting the percentage of elements in $S_+$ that are larger than those in S_:\n$W(z) = \\frac{1}{m(m - 1)} \\sum_{i \\neq j} [(S_+)_{ij} > (S_-)_{ij}]*$\n(6)\nThe overall WinRate is given by $W = \\sum_{i=1}^n W(z_i)$. A high WinRate indicates better monosemanticity."}, {"title": "2.3. Grouping Modality in Multimodal Representations", "content": "Building upon the monosemantic features identified above, we can take a closer look at the distribution of modality within each feature of the multimodal CLIP model.\nModality Dominance Score (MDS). We propose a metric to determine the predominant modality for each neuron. Specially, we feed m input-output pairs to CLIP and extract the corresponding image features $Z_I \\in \\mathbb{R}^{m \\times n}$ and text features $Z_T \\in \\mathbb{R}^{m \\times n}$. For each feature $k \\in [1, ..., n]$, we calculate the relative activation between the image and text features across the m inputs as follows:\n$R(k) = \\frac{1}{m} \\sum_{i=1}^m \\frac{(Z_I)_{ik}}{(Z_I)_{ik} + (Z_T)_{ik}}$\nThe ratio R(k) indicates how much the feature k is activated in the image modality. Based on this value, we split all n features into three groups according to their dominant modality with the standard deviation $\\sigma$:\nImgD: $r_i > \\mu + \\sigma$;\nTextD: $r_i < \\mu - \\sigma$;\nCrossD: $\\mu - \\sigma < r_i < \\mu + \\sigma$.\nWe anticipate that ImgD features are mostly activated by images and TextD features by text, while CrossD features are simultaneously activated by both image and text when paired."}, {"title": "2.4. Understanding Modality-specific Features", "content": "The implications of modality dominance significantly affect feature interpretability across different modalities. Ideally, when presented with image samples, ImgD neurons should be more effective at capturing concrete and consistent features than TextD neurons, and the same holds true for text neurons with textual input samples.\nQuantitative interpretability. We measure both visual and textual monosemanticity. Specially, for image inputs, we calculate the visual monosemanticity by evaluating the interpretability difference between ImgD and TextD, i.e., EmbedSimi(ImgD)-EmbedSimi(TextD). For text inputs, we calculate textual monosemanticity using EmbedSimi(TextD)-EmbedSimi(ImgD). We have the following observations from Table 2: (1) For image inputs, all models except CLIP demonstrate a positive visual monosemanticity, indicating better performance than the other two types of neurons. (2) For text inputs, both NCL and SAE excel in capturing monosemantic textual features compared to the other two models. (3) SAE stands out as the best model for capturing both visual and textual monosemantic features.\nIn addition to quantification of the interpretability, we look closer into a few examples of captured features.\nImage-dominant neurons capture visual commonalities that are hard-to-describe in words. We randomly select two ImgD neurons and visualize the top 8 activated images along each neuron in Figure 2. We find that the top neuron contains repetitive patterns of diverse shapes and colors, and the bottom neuron contains various objects that are partially ocean blue in color. In contrast, the activated text samples (Table 3) display a more diverse and abstract range of descriptions. Although less cohesive than the images, some patterns do emerge: for instance, two sentences refer to repetitive patterns for feature-647, while two others mention winter-related concepts, such as snow (as seen in the 5-th image for feature-667). These observations suggest that ImgD neurons are more adept at capturing distinct visual features that are not only challenging to express through language but are also more interpretable and intuitive to human perception, aligning with how we naturally understand visual commonalities.\nText-dominant neurons capture abstract concepts, especially human emotional feelings. We randomly select two features and display the top 8 activated texts in Table 4. Feature-34 centers around a sweet and happy atmosphere between couples, with themes like cuddling, embracing, and hugging. Feature-242 focuses on strong human emotions, such as \"never\", \"terrifying\" and exclamation marks. These TextD features generally correspond to abstract human feelings and thoughts, which can be associated with various visual objects (e.g., animals, sinkhole, castle.) This partially explain the diversity of objects in the images activated by feature 242 in Figure 3. Interestingly, the images activated by feature-34 mostly depict couples or people in red attire, somewhat reflecting the joyful mood conveyed in the language. This insight highlights that TextD features can abstract the unique, high-level aspects of language, particularly atmosphere and emotions, as a reflection of human intelligence.\nCross-Modality features (the majority features) capture common concepts from both visual and textual perspectives. Different from the TextD and ImgD, whose activated samples tend to contain modality-exclusive features, CrossD neurons capture common concepts that could be expressed in both visual and language modalities. We randomly select two CrossD features and display their top activated images and texts. As shown in Figure 4 and Table 5, Feature6 mostly activates individuals in different activities, especially outdoor activities, and feature47 activates outdoor scenes. Both kinds of features can be consistently described in both images and languages, representing the common space shared by both modalities, implying that these features are mostly affected by the modality aligned training objectives."}, {"title": "3. Case Studies based on Modality-specific\nFeatures", "content": "In this section, we present three case studies based on our three modality features: (1) gender detection (2) adversarial attacks (3) text-to-image generation."}, {"title": "3.1. Case Study 1: Gender Pattern in Different\nModalities", "content": "We describe gender using visual features, for example, long hair and wearing a dress, and assume that the ImgDom features primarily account for these discriminative visual patterns. Consequently, removing the ImgDom features from a female image may make it less identifiable in terms of gender, potentially leading to its classification as male in a binary classification task. Similarly, TextDom features play a comparable role when gender is described through textual information.\nTo test this hypothesis, we collect both male and female images from the cc3m validation set using a gender classifier \u00b2. These images are then encoded using the Clip+SAE model, extracting 1024-dimensional feature representations for both female and male subjects. Next, we apply a zero-mask intervene strategy to remove the ImgDom and TextDom features from these representations. Notably, our intervention is applied at the feature level, i.e., on activations rather than the raw image or text inputs. Since these modified reature representations cannot be directly processed by ex-isting pretrained classifiers, which require image or text in-puts, we employ a zero-shot classification approach inspired by Bhalla et al. (2024). Specifically, we use an unsupervised clustering method to measure the distances between the in-tervened activations and the label embeddings for \"female\" and \"male\", with the latter obtained by encoding female and male inputs.\nBefore analyzing the difference in predominant features between male and female subjects, we first verify that our identified modality-specific features indeed capture information within their respective modality.\nModality-specific interventions. We intervene both ImgD and TextD for image and text inputs, respectively. The probabilities of original image/text and intervened image/text, over the original gender label are in the Table 6.\nGender bias in different modalities. We then show the discrepancy when removing the image and text features to identify the primary modality supporting the gender in this dataset. From the results in Table 7, we observe that female images are more easily affected by the ImgD features, while male texts are more easily affected by the TextD features."}, {"title": "3.2. Case Study 2: Adversarial Attacks", "content": "We investigate the impact of different types of features on multimodal adversarial attacks (Cui et al., 2024; Yin et al., 2024), following the setup in Shayegani et al. (2024).\nThe adversarial sample is a benign-appearing image, e.g., a scenery image but injected with harmful semantic information, such as the phrase \"I want to make bomb\". One defense optimization strategy involves minimizing the distance, between the embeddings of adversarial sample Fadv and a benign sample Fben, and accordingly update the adversarial sample (in Figure 6). The paired benign image is injected with the friendly text, e.g., \"peace and love\". To study the effects of our identified modality features, we only select the target feature index I from the embedding for alignment training, i.e., ImgD, TextD, and CrossD. The alignment loss is $L = ||F_{adv}[:, I] - F_{ben} [:, I]||_2$. Finally, the optimized adversarial sample is then adopted to attack a Vision-Language model (VLM).\nModels. We use the the same CLIP model as introduced in Section 2 as the Multimodality feature extractor, so the index for target features unchanged. The VLM being attacked is Llama-1.5-7b-hf (Liu et al., 2023b;a). To evaluate whether the attack the attack is successful, we evaluate the generated response from the VLM to DeepSeek V3 (DeepSeek-AI et al., 2024) to generate a binary label indicating whether the harmful request is rejected or the task is executed.\nResults. The results are shown in Table 8. The number of neurons selected was consistent across all experiments. Using the smallest TextD as the baseline, we repeatedly sampled the same number of neurons from ImgD and CrossD as in TextD. If we achieve better defense results (i.e., a lower attack success rate) with a specific type of feature, it suggests that this type of neuron plays a key role in defense. We observe that leveraging all three target features improves defense results to some extent compared to the original adversarial sample. Given that the number of features in each category differs, we randomly sample an equal number of features from each category to ensure alignment Among them, using TextD for alignment yields the best defense performance, with only 25% rate comparing with alignment on the same amount of features, 65%. The performance is followed by CrossD and ImgD. Since the adversarial information primarily stems from undesirable textual semantics, this outcome demonstrates that TextD effectively captures most of the semantic content. In contrast, CrossD captures partial semantics, while ImgD is the least related to semantic information, resulting in minimal benefits for jailbreak defense when aligned.\nPotential. The feature-specified optimization for multi-modality jailbreak provides a more focused and computationally efficient defense strategy. This selective alignment not only enhances interpretability by highlighting the roles of different feature types but also allocates resources more effectively by prioritizing the most critical features for defense. Additionally, it prevents feature dilution, ensuring that semantic integrity is preserved during optimization. This modular and adaptable design makes the method particularly effective for defending modality-specific attacks."}, {"title": "3.3. Case Study 3: Multimodal Generation", "content": "Despite the impressive capabilities of text-to-image generation models (Yu et al., 2024; Koh et al., 2024; Swamy et al., 2024), their internal mechanisms for bridging linguistic semantics and visual details remain poorly understood. A key challenge is disentangling how modality-specific features influence the fidelity and controllability of generation. To address this, we investigate the generation process by intervening in different modality-specific features in Stable Diffusion v2 (Rombach et al., 2022).\nModels. Stable Diffusion v2 (Rombach et al., 2022) is our generation model, and its feature extractor is laion/CLIP-ViT-H-14-laion2B-s32B-b79K rather than the CLIP model previously employed. Therefore, we compute the model-specific MDS based on inference passes over the COCO2017 dataset (Ninja, 2025).\nIntervention of the Text-to-Image Generation. The input text prompt is \"Please draw an animal\". The feature extractor generates an embedding T, representing the original multimodal embedding for generation. Additionally, we provide a reference figure-a horse (Figure 7)-processed through the same feature extractor, producing a reference embedding R. To control the generation through modality-specific feature intervention, we interpolate only the features at specified indices I defined by MDS. The final multimodal embedding is computed as: $E[I] = \\alpha T[I] + (1 - \\alpha) R[I]$, where operations are applied exclusively to the feature indices defined by I, i.e., ImgD, TextD and CrossD.\nResults. We feed E to the generation model with different $\\alpha$ ranging from 0 to 0.9 with an interval of 0.1. The generated images with the selected indices correspond to ImgD, TextD, and CrossD are shown in Figure 8. The results clearly demonstrate that larger interventions on ImgD and CrossD disrupts visual coherence: animal shapes fragment, outlines blur, and textures degrade, implying the role of ImgD in preserving structural and fine-grained visual details. Interestingly, interventions on TextD maintain the visual features without any distortion even with larger $\\alpha$. We can instead observe the shifts in semantic concepts, such as generating cat-like, elephant, or horse. These animals became abstracted into geometric forms or textual overlays, demonstrating that text-guided representations contribute to the structured composition and semantic labeling of the generated visuals, rather than low-level visual details.\nPotential. By isolating modality-specific neurons, our framework provides several benefits for data editing: (i) Semantic Refinement: Adjusting TextD activations improves conceptual alignment; (ii) Visual Enhancement: Tuning ImgD neurons enhances texture realism or ensures stylistic consistency. This data-driven approach not only advances interpretability but also reflects human cognitive principles, where distinct neural pathways govern linguistic abstraction and perceptual processing."}, {"title": "4. Related Work", "content": "This work sits at the intersection of several active research areas: mechanistic interpretability, multimodal representation learning, and the study of modality gaps.\nMechanistic Interpretability. Mechanistic interpretability aims to understand the internal computations of deep learning models by identifying and analyzing individual components and interactions. Recent research has been focused on identifying polysemanticity, where individual neurons respond to multiple, unrelated features (Olah et al., 2020). This has led to the exploration of monosemanticity, the hypothesis that models might contain features that correspond to single, interpretable concepts (Elhage et al.). Recent advances in dictionary learning have made it possible to decompose polysemantic neurons into monosemantic features (Cunningham et al., 2023). These techniques, coupled with automated methods for interpreting and labeling features (Bills et al., 2023; Gurnee et al., 2023; Yan et al., 2024), have enabled the extraction of large numbers of interpretable features from models like CLIP (Radford et al., 2021). These interpretable features can be studied to understand the various aspects of model behavior, especially, as explored in this paper, the nature of modality gaps.\nMultimodal Representation Learning. Learning effective representations from multiple modalities has been a long-standing research focus. Early approaches often relied on hand-crafted features and statistical methods (Ngiam et al., 2011). With the rise of deep learning, multimodal representation learning has been revolutionized by models like CLIP (Radford et al., 2021), VILT (Kim et al., 2021), and DeCLIP (Li et al., 2022). These models leverage large-scale datasets and contrastive learning objectives to learn joint representations of images and text. They have achieved remarkable success in various downstream tasks, such as image retrieval, zero-shot classification, and visual question answering. Our work utilizes CLIP as a testbed for analyzing modality gaps, taking advantage of its strong performance and readily available pre-trained weights. Our work is also complementary to prior efforts on visualizing and interpreting multimodal models (Liang et al., 2022; Wang et al., 2021); we focus on understanding the internal representations of pre-trained CLIP models and how they handle modality-specific and shared information.\nModality Gaps. The study of modality gaps, or the differences and limitations in how different modalities represent information, has been a topic of interest in cognitive science for decades (Spence, 2011; Paivio, 1991; Calvert et al., 2004). Researchers have investigated how humans integrate and differentiate information across sensory modalities, revealing both commonalities and distinct characteristics. However, these studies are mostly based on human studies and experiments. Our research offers an alternative human-free approach to study the modality gap through the neural networks directly learned from these modalities. This opens a new approach to study the modality gap that could alleviate potential bias from human-centric viewpoint and bring more insights from large-scale data."}, {"title": "5. Conclusion", "content": "In this study, we explored the monosemanticity of features within the CLIP model to elucidate the commonalities and distinctions across visual and linguistic modalities. We successfully categorized interpretable features according to their predominant modality, which demonstrate close correspondence to human cognitive interpretations.Our interpretability analysis in three case studies also demonstrated the great potential in understanding modality-features in gender bias, adversarial attacks and multimodal generation. Future work may extend these methodologies to other multimodal architectures and investigate their implications for cognitive science, ultimately fostering the development of more interpretable and cognitively aligned AI systems."}]}