{"title": "MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning", "authors": ["Lulu Zhao", "Weihao Zeng", "Xiaofeng Shi", "Hua Zhou"], "abstract": "Recently, LoRA has emerged as a crucial technique for fine-tuning large pre-trained models, yet its performance in multi-task learning scenarios often falls short. In contrast, the MoE architecture presents a natural solution to this issue. However, it introduces challenges such as mutual interference of data across multiple domains and knowledge forgetting of various tasks. Additionally, MoE significantly increases the number of parameters, posing a computational cost challenge. Therefore, in this paper, we propose MoSLD, a mixture-of-shared-LoRAs model with a dropout strategy. MoSLD addresses these challenges by sharing the upper projection matrix in LoRA among different experts, encouraging the model to learn general knowledge across tasks, while still allowing the lower projection matrix to focus on the unique features of each task. The application of dropout alleviates the imbalanced update of parameter matrix and mitigates parameter overfitting in LoRA. Extensive experiments demonstrate that our model exhibits excellent performance in both single-task and multi-task scenarios, with robust out-of-domain generalization capabilities.", "sections": [{"title": "1 Introduction", "content": "The emergence of Large Language Models (LLMs) has significantly advanced Natural Language Processing (NLP) technology, serving as a robust foundation with broad applicability. However, as the parameter scale increases, the process of full parameter fine-tuning (FP-tuning) demands substantial computational and memory resources. To strike a balance between resource requirements and effectiveness, the research community is increasingly turning to parameter-efficient fine-tuning (PEFT) methods"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Mixture-of-Expert", "content": "The Mixture of Experts (MoE) functions as an ensemble method, conceptualized as a collection of sub-modules or experts, each tailored to process distinct types of input data. Guided by a router, each expert is selectively activated based on the input data type. This technique has garnered increasing attention and demonstrated remarkable performance across various domains, including computer vision, speech recognition, and multimodal applications. Evolution of MoE techniques spans from early sample-level approaches to contemporary token-level implementations, which have now become mainstream. Concurrently, some researchers are delving into the router selection problem within MoE. Notably, the majority of these endeavors aim to scale up model parameters while mitigating computational costs."}, {"title": "2.2 Mixture-of-LoRA", "content": "As LoRA gradually becomes the most common parameter-efficient fine-tuning method, researchers pay more attention to combining MoE and LoRA for more efficient and effective model tuning. Huang et al. and Feng et al. pioneer the approach of training several LoRA weights on upstream tasks and then integrating the LoRA modules into a shared LLM using a routing mechanism. However, these methods necessitate the training of numerous pre-defined LoRA modules. Chen et al. initially engage in instruction fine-tuning through sparse mixing of LoRA experts in the multi-modal domain, while Dou et al. split the LoRA experts into two groups to explicitly"}, {"title": "3 Methodology", "content": "In this section, we describe our MoSLD from the sharing mechanism, dropout strategy and optimization details, as shown in Figure 2."}, {"title": "3.1 Sharing Mechanism of LoRAs", "content": "In the area of parameter-efficient fine-tuning, LoRA introduces the concept of training only two low-rank matrices as an alternative to dense layer updates. In other words, it reformulates the parameter fine-tuning process in LLMs as a low-rank decomposition. Specifically, the equation $W_o + \\Delta W = W_o + BA$ captures this decomposition. Here, $W_o \\in R^{d_{in} \\times d_{out}}$ represents the parameter matrix of the pre-trained LLM, while $\\Delta W \\in R^{d_{in} \\times d_{out}}$ denotes the matrix updated during fine-tuning. The matrices $B \\in R^{d_{in} \\times r}$ and $A \\in R^{r \\times d_{out}}$ are low-rank and trainable.\nIn order to achieve the transfer of general features between different tasks and capture the shared general knowledge, we design a novel sharing mechanism. Specifically, given a Transformer model with L layers, we allocate $N_l$ experts for layer l and create $N_l$ pairs of low-rank matrices ${A_{i,l}, B_{i,l}}_{i=1}^{N_l}$, where $A_{i,l}$ is initialized from a random Gaussian distribution and each $B_{i,l}$ is set to zero. It is worth noting that the matrix $A_{i,l}$ is shared among all experts in each layer, i.e., $A_{1,l} = A_{2,l}... = A_{N_l,l}$ ($l \\in L$). In other words, the core idea is to share the matrix A as the general-feature matrix and keep matrix B as specific-feature matrix. In this way, we can only keep L central general-feature matrices for a L-layer MoE architecture, which significantly reduces the parameters of the MoE architecture. A router with a trainable weight matrix $W_l \\in R^{d_{in} \\times N_l}$ is used to specify different experts for the input x. As in the original MoE, MOSLD selects the top K experts for computation, and the gate score $S_f$ is calculated as follows:"}, {"title": "3.2 Dropout Strategy", "content": "In order to alleviate the imbalance and over-fitting problems caused by frequent general-feature matrix updates, we propose to apply the dropout strategy on the general-feature parameter matrix $A_l$. Dropout involves randomly ignoring a proportion of updates to the parameter matrix during each it-"}, {"title": "4 Experimental Setup", "content": ""}, {"title": "4.1 Datasets", "content": "To evaluate the effectiveness of MoSLD, we conduct experiments on six commonsense reasoning datasets, including commonsense QA task (OBQA, CSQA), reading comprehension task (Race, MCTest), and subject knowledge QA task (Arc-e and Arc-c ). We denote the six datasets as {$D_1, D_2, ..., D_6$}, and we also create a mixed dataset $D_{mix}$, corresponding to the single setting and the mixture setting respectively. The dataset sizes are as follows for training and testing: 5,457/500, 10,962/1140, 10,083/4934, 1,330/147, 2,821/2,376, and 1,418/1,172. We allocate 10% of the training set for validation. For all datasets, we use answer accuracy as the evaluation metric."}, {"title": "4.2 Baselines", "content": "We compare MoSLD with five parameter-efficient fine-tuning methods: Prefix-tuning , LoRA, MOLORA, SiRA, MOLA , MixLoRA. Additionally, we evaluate full-parameter fine-tuning. The details can be seen in Appendix A."}, {"title": "4.3 Training Details", "content": "We take LLaMA2-7B which contains 32 layers as our base model. For plain LoRA and its variants, the r is set to 8 and a is 16. Beside, the LoRA modules are used in matrix Q and matrix V in attention layers. Our MoSLD also follows the same settings. We allocate 8 experts to each layer for 1-8 layers, 6 experts to each layer for 9-16 layers, 4 experts to each layer for 17-24 layers, and 2 experts to each layer for the last 8 layers. The K of the selected experts is 2. For training details, we finetune models with 10 epochs"}, {"title": "5 Qualitative Analysis", "content": ""}, {"title": "5.1 Out-of-domain Test", "content": "To assess the generalization capability of our proposed model, we conducted out-of-domain experiments using the test set of MMLU. Figure 4 presents a boxplot, where the top and bottom horizontal lines represent the mixture and single settings, respectively. Our models, MoSL and MOSLD, consistently outperform others in both settings, exhibiting significant improvements, particularly on Race, Arc-e, and Arc-c datasets. This highlights the effectiveness of our models in disentangling domain knowledge and transferring general features across diverse datasets. OBQA and CSQA exhibit similar trends in the boxplot, indicating similar data distributions between the two datasets. Conversely, for MCTest, while improvements are observed in the mixture settings, the single settings remain relatively unchanged. This divergence may stem from the substantial differences between the MCTest and MMLU test sets, suggesting that introducing data from other domains or tasks could inspire general domain knowledge. In summary, our model demonstrates strong generalization capabilities, particularly in multi-task scenarios."}, {"title": "5.2 Effect of Model Parameters", "content": "In this section, we conduct parameter search experiments.\nDropout Location As shown in Table 2, we show the results of applying our methods on matrix A and matrix B. We found that in the single setting, MoSLD (matrix B) does not achieve much improvement, 0.94 points lower than the ordinary LORA and 1.04 points higher than MoLA. The mixture setting still achieves good results. However, the results of applying our method on matrix B are lower than those of applying it on matrix A in both the single and mixture settings. This also shows that matrix A is more used to extract general features.\nDropout Ratio In Figure 5, we depict the per-"}, {"title": "5.3 Mix with Other Data", "content": "Mathematical Reasoning Data We construct a new multi-task setting, including commonsense QA task (OBQA), reading comprehension task"}, {"title": "5.4 Scaling of Model Size", "content": "Table 4 shows the results of our model for the six datasets both in single and mixture settings as the model size scalings. We find that the performance of our model increases with the size of the model, whether in single or mixture settings, which is in line with our expectations. In addition, it is observed that the results improve by 1.36%, 1.61%, and 1.91% from single to mixture for LLaMA2-7B, LLaMA2-13B, and LLaMA-33B, respectively. The experimental results show that our method has achieved good performance on models of different sizes, and has a certain scaling ability. We also give the model size scaling results of other LoRA-based baselines, which can be seen in the Appendix C."}, {"title": "5.5 Analysis of Computation Efficiency", "content": "In Table 5, we further show the computational efficiency of our model. We first analyze the number of new LoRA modules inserted in ordinary LoRA, MOLA, and MoSLD. Since MoLA introduces the MoE framework, the trainable parameters become 5 times that of ordinary LoRA, and its results are improved by 0.43 points from 69.57 to 70.00. We believe that despite the introduction of a large number of trainable parameters, the change in results is not very large, which is a method of sacrificing efficiency for effect. In addition, we also found that"}, {"title": "6 Conclusion", "content": "In this paper, we propose MoSLD, which is a mixture-of-shared-LoRAs model with dropout strategy. Unlike traditional LoRA-MoE approaches, we design a sharing mechanism for matrix A, which aims to capture the general-feature among various tasks. A dropout strategy is also applied to the matrix A, solving the overfitting caused by parameter redundancy to a certain extent. Evaluations show that MoSLD outperforms the baseline in both single-task and multi-task scenarios. Especially in multi-task scenarios, where it can effectively alleviate knowledge conflict and forgetting problems. In general, our model is extremely parameter-efficient for fine-tuning."}, {"title": "Limitations", "content": "Although MoSLD achieves significant improvements over existing baselines, there are still avenues worth exploring in future research. (1) This paper focuses on applying MoSLD on the matrix Q and V of the attention layer. We hope to extend this method to the FFN layer. (2) This paper explores the multi-task setting of directly mixing multiple datasets and compares with the performance of a single task. We plan to study the impact of multi-task data ratio on MoSLD. (3) This paper emphasizes the extraction of general and unique features by the upper and lower projection matrices in LoRA, and intends to visualize this phenomenon in the future."}, {"title": "Ethics Statement", "content": "LORA has emerged as a pivotal technique for refining extensive pre-trained models. Nevertheless, its efficacy tends to fail in multi-task learning. Conversely, the MoE architecture offers a promising remedy to this setback. However, it introduces hurdles such as the interference of data across diverse domains and the risk of forgetting knowledge from various tasks. Furthermore, MoE substantially inflates parameter counts, presenting computational challenges. In light of these considerations, we present MoSLD in this paper, a model that integrates the strengths of both approaches. MoSLD, a mixture-of-shared-LoRAs model with a dropout strategy, addresses these obstacles ingeniously. By sharing the upper projection matrix in LoRA among different experts, MoSLD fosters the acquisition of broad knowledge across tasks while allowing the lower projection matrix to concentrate on task-specific features. Additionally, the application of dropout mitigates parameter overfitting in LoRA. The experimental results prove the effectiveness of our model andevaluation framework. Besides, there is no hugebiased content in the datasets and the models. Ifthe knowledge base is further used, the biased con-tent will be brought into the generated responses,just like biased content posted by content creatorson the Web which is promoted by a search engine.To prevent the technology from being abused fordisinformation, we look forward to more research effort being paid to fake/biased/offensive contentdetection and encourage developers to carefullychoose the proper dataset and content to build theknowledge base."}, {"title": "A Baselines", "content": "In this section, we introduce the baselines in detail.\nPrefix-tuning : This method involves incorporating soft prompts into each attention layer of the Large Language Model (LLM). These soft prompts are a series of virtual tokens pre-appended to the text. During fine-tuning, the LLM remains frozen, and only the virtual tokens are optimized.\nLORA : A popular parameter-efficient tuning approach widely used in LLM fine-tuning, LoRA leverages low-rank matrix decomposition of pre-trained weight matrices to significantly reduce the number of training parameters."}, {"title": "B Effect on Rank", "content": "In this section, we add experiments on the effect of rank for our MoSLD, with r ranging from 2 to 32. Overall, the results of the six datasets did not fluctuate much, and the best value was obtained at 8 or 16. From the perspective of efficiency, 8 is indeed a suitable hyperparameter, which is also in line with the change law of LoRA's rank. The results are as shown in Table 6:"}, {"title": "C Scaling of Model Size", "content": "In this section, We add model scaling experiments on LoRA-based baselines, such as LoRA, MOLORA, SIRA, and MoLA. We find that for each baseline, the results improve as the model size increases, among which our model MoSLD scales even better. The results are shown in Table 7 :"}]}