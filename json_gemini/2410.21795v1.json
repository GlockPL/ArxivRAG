{"title": "Robot Policy Learning with Temporal Optimal Transport Reward", "authors": ["Yuwei Fu", "Haichao Zhang", "Di Wu", "Wei Xu", "Benoit Boulet"], "abstract": "Reward specification is one of the most tricky problems in Reinforcement Learn-\ning, which usually requires tedious hand engineering in practice. One promising\napproach to tackle this challenge is to adopt existing expert video demonstrations\nfor policy learning. Some recent work investigates how to learn robot policies from\nonly a single/few expert video demonstrations. For example, reward labeling via\nOptimal Transport (OT) has been shown to be an effective strategy to generate a\nproxy reward by measuring the alignment between the robot trajectory and the\nexpert demonstrations. However, previous work mostly overlooks that the OT\nreward is invariant to temporal order information, which could bring extra noise to\nthe reward signal. To address this issue, in this paper, we introduce the Temporal\nOptimal Transport (TemporalOT) reward to incorporate temporal order information\nfor learning a more accurate OT-based proxy reward. Extensive experiments on the\nMeta-world benchmark tasks validate the efficacy of the proposed method. Code is\navailable at: https://github.com/fuyw/TemporalOT.", "sections": [{"title": "Introduction", "content": "Reinforcement Learning (RL) [51] has achieved great success across a wide array of applications [40].\nHowever, it typically requires a large number of interactions with the environment [26, 35], which\nlimits its practical application in the robotic control [9, 49]. A large body of work has been developed\nto address this issue from different aspects [52], i.e., using curiosity-based intrinsic reward to\nencourage exploration [2, 47], leveraging better representation pretrained on large scale robotics\ndatasets [33, 38], incorporating external knowledge from the Vision-Language Models (VLMs) [13,\n32], and imitating the behaviors from pre-collected expert demonstrations [50, 62].\nReward specification plays a central role in RL [6]. Since the goal of the RL agent is to maximize the\nexpected cumulative rewards, the reward signal directly influences the learned behaviors [8]. One\nmajor challenge in applying RL to real-world problems is how to design the reward functions [10].\nA well-designed reward function can guide the agent towards desirable behaviors more efficiently,\nwhile a poorly designed one could lead to sub-optimal behaviors [25]. However, designing a good\nreward function is a nontrivial task [11], which requires related expert domain knowledge and (or)\ntime-consuming hand reward engineering [48]. The lack of a good reward function is one of the main\nbottlenecks for the low sample-efficiency issue in RL [58].\nImitation Learning (IL) has been proven to be an effective technique to learn control policies without\nthe oracle task reward [24]. Given an expert demonstration dataset, IL formulates the policy learning\nas a supervised learning paradigm [27]. The IL objective aims to learn a policy that mimics the\nexpert behaviors via minimizing a distance measure of the learned policy and an approximated expert\npolicy [23, 28]. Depending on if the RL agent can learn from further online interactions, IL can be\ncrudely classified as offline IL and online IL [29]. In offline IL, the RL agent purely learns from a\nfixed dataset of collected expert experiences. In online IL, the RL agent usually learns a proxy reward\nfunction to relabel the collected online trajectories with respect to the expert demonstrations [53].\nOne notable weakness of IL is that it generally requires a diverse and high-quality demonstration\ndataset to achieve desired performances [5]. Recently, some researchers found that Optimal Transport\n(OT) [18, 44] based proxy reward function enables us to learn effective robot policies with only\na few expert demonstrations [30, 31]. In this paper, we follow this line of research in applying\nOT-based proxy rewards to online IL without using any task reward information. In particular, we first\nrevisit the efficacy of OT-based proxy reward in RL and then discuss some challenges of the existing\nmethods due to the overlook of temporal order information. To mitigate this issue, we introduced the\nTemporal Optimal Transport (TemporalOT) reward, which incorporates temporal order information\nto the OT-based proxy reward via using context embeddings and a mask mechanism.\nThe primary contributions of this work can be summarized as follows:\n\u2022 we pointed out a weakness of existing OT-based proxy reward methods for imitation learning\ndue to the overlook of temporal order information;\n\u2022 we designed a simple yet effective algorithm to incorporate temporal order information into\nOT-based proxy reward via using context embeddings and a mask mechanism;\n\u2022 experiments show that the proposed method outperforms other SOTA algorithms."}, {"title": "Background", "content": "In this work, we consider the standard Markov Decision Process (MDP) [46] setting $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, R, P, \\rho_0, \\gamma)$, where $\\mathcal{S}$ and $\\mathcal{A}$ are state and action spaces, $R : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is a reward\nfunction, $P : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ is the state-transition probability function, $\\rho_0 : \\mathcal{S} \\rightarrow \\mathbb{R_+}$ is the initial\nstate distribution and $\\gamma \\in [0, 1)$ is a discount factor. Our goal is to learn a policy $\\pi(a|s) : \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})$\nthat maximizes the expected cumulative discounted rewards $\\mathbb{E}_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$ where $s_0 \\sim \\rho_0$,\n$s_{t+1} \\sim P(.|s_t, a_t)$ and $a_t \\sim \\pi(\\cdot|s_t)$. In partially observable MDP (POMDP) [19], we can only\nreceive an observation $o_t \\in \\mathcal{O}$, i.e., image observation, of the current state $s_t$.\nTo solve this optimization problem, value-based RL methods typically learn a state-action value\nfunction $Q^{\\pi}(s, a) := \\mathbb{E}_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t r_t|s_0 = s, a_0 = a]$, which is defined as the expected return under\npolicy $\\pi$. For convenience, we adopt the vector notation $Q \\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$, and define the one-step Bellman\noperator $T^{\\pi} : \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\rightarrow \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ such that $T^{\\pi}Q(s, a) := r(s, a) + \\gamma \\mathbb{E}_{s'\\sim P, a'\\sim \\pi}[Q(s', a')]$. The\n$Q$-function $Q^{\\pi}$ is the fixed point of $T^{\\pi}$ such that $Q^{\\pi} = T^{\\pi}Q^{\\pi}$ [51]. Similarly, we define the\noptimality Bellman operator as follows $TQ(s, a) := r(s, a) + \\gamma \\mathbb{E}_{s'\\sim P}[\\max_{a'} Q(s', a')]$ and the\noptimal $Q$-value function $Q^*$ is the fixed point of $TQ^* = Q^*$. In deep RL, we use neural networks\n$Q_\\theta(s, a)$ to approximate the $Q$-functions by minimizing the empirical Bellman error:\n$\\mathbb{E}_{(s,a,r,s')}[(r + \\gamma \\max_{a'} Q_\\theta(s', a') - Q_\\theta(s, a))^2]$,\nwhere we sample transitions $(s, a, r, s')$ from a replay buffer and $Q_{\\theta'}(s, a)$ is the target network."}, {"title": "Inverse Reinforcement Learning", "content": "Inverse Reinforcement Learning (IRL) aims to infer the underlying reward function from expert\ndemonstrations [39], which further facilitates an RL agent to learn the policy. One key assumption\nof IRL is that the observed behaviors are optimal such that the observed trajectories maximize the\ncumulative rewards [17]. Due to the ability to avoid the manual reward specification, IRL holds the\npromise for practical real-world RL applications. Denote $\\mathcal{M}$ as an MDP and $\\pi_\\mathcal{E}$ as an expert policy,\nthe IRL problem is to find an optimal reward function $R^*$ such that:\n$\\mathbb{E} \\bigg[\\sum_{t=0}^{\\infty} \\gamma^t R^*(s_t)|\\pi_\\mathcal{E} \\bigg] \\ge \\mathbb{E} \\bigg[\\sum_{t=0}^{\\infty} \\gamma^t R^*(s_t)|\\pi \\bigg], \\forall \\pi \\in \\Pi,$\nwhere $\\Pi$ is the feasible policy set. That is, the expert policy $\\pi_\\mathcal{E}$ will achieve the maximum expected\ncumulative discounted reward than any other policy."}, {"title": "Optimal Transport", "content": "Optimal Transport is an optimization problem which aims to find an optimal mapping that transforms\none probability distribution into another with the least cost. OT has a wide application in various\ndomains such as economics [14], physics [15], and machine learning [1]. Consider two probability\ndistributions $p \\in \\mathbb{R}^n$, $q \\in \\mathbb{R}^m$ and a joint distribution $\\mu(p, q)$ on product space $\\mathcal{X} \\times \\mathcal{Y}$, the Wasserstein\ndistance [54] between $p$ and $q$ is defined as:\n$W(p, q) = \\inf_{\\mu} \\int_{\\mathcal{X}\\times \\mathcal{Y}} c(x, y) d\\mu,$\nwhere $c(x, y)$ is the cost function for moving mass form $x$ to $y$. In the RL scenario, $p$ and $q$ are\nusually in the state space $\\mathcal{S}$ or the observation space $\\mathcal{O}$ [21]. For example, given an expert trajectory\n$\\tau_\\mathcal{E} = {o_0^\\mathcal{E}, \\dots, o_T^\\mathcal{E}}$ and an agent trajectory $\\tau = {o_1, \\dots, o_T}$ where $o_i$ is the image observation at\nstep $i$, the Wasserstein distance between $\\tau_\\mathcal{E}$ and $\\tau$ is defined in the following discrete form:\n$W(\\tau, \\tau_\\mathcal{E}) = \\min_{\\mu \\in \\mathbb{R}^{T\\times T_\\mathcal{E}}} \\sum_{i=1}^T \\sum_{j=1}^{T_\\mathcal{E}} c(o_i, o_j^\\mathcal{E}) \\mu(i, j),$\ns.t. $\\sum_{i=1}^T \\mu(i, j) = \\frac{1}{T}, \\sum_{j=1}^{T_\\mathcal{E}} \\mu(i, j) = \\frac{1}{T}$\nwhere $\\mu \\in \\mathbb{R}^{T \\times T_\\mathcal{E}}$ is called the transport plan, and we denote the optimal transport plan as $\\mu^*$."}, {"title": "Method", "content": "In this section, we first revisit the application of OT-based proxy reward in RL. In particular, we point\nout the influence of temporal order information, which has been overlooked in most prior work. Next,\nwe introduce the main idea and formulation of the proposed method based on these observations."}, {"title": "A Recap of OT Reward in RL", "content": ""}, {"title": "OT reward helps to rank states and actions", "content": "In RL, we usually adopt the Wasserstein distance to measure the similarity of two trajectories [31],\nas illustrated in Figure 1. Given an agent trajectory $\\tau = {o_1, \\dots, o_T}$ and an expert trajectory\n$\\tau_\\mathcal{E} = {o_0^\\mathcal{E}, \\dots, o_T^\\mathcal{E}}$, we first compute the optimal transport plan $\\mu^*$ in Eqn.(4) using some iterative\noptimization algorithms, i.e., Sinkhorn algorithm [4]. Then, the OT-based proxy reward at the $i$-th\nstep is defined as follows:\n$r_i^{\\text{OT}} = \\sum_{j=1}^{T_\\mathcal{E}} c(o_i, o_j^\\mathcal{E}) \\mu^*(i, j),$\nwhere $c(o_i, o_j^\\mathcal{E})$ is a cost function that measures the similarity between $o_i$ and $o_j^\\mathcal{E}$. One popular choice\nin prior work is the cosine similarity based cost function $c(o_i, o_j^\\mathcal{E}) = 1 - \\frac{\\langle f(o_i), f(o_j^\\mathcal{E}) \\rangle}{||f(o_i)|| \\cdot ||f(o_j^\\mathcal{E})||}$, where $f(o_i)$\nis the latent representation of observation $o_i$ extracted by a visual encoder [3].\nSimilar to the curiosity-based exploration bonus [2], OT reward is used to distinguish the goodness of\ndifferent states. We consider the toy example in Figure 1, two agents start from the same state with\nobservation $o_2$ and take different actions $a_2$ and $a'_2$, respectively. Then the goodness of $a_2$ and $a'_2$ at\n$o_2$ is measured by the OT reward computed w.r.t. the observation $o'_3$ and $o''_3$ at the next step. As long\nas the OT reward can rank $r^{\\text{OT}}(o'_3, \\tau_\\mathcal{E})$ and $r^{\\text{OT}}(o''_3, \\tau_\\mathcal{E})$ correctly, then the policy will be able to\nlearn a better action at $o_2$. From Figure 2 (right), we can observe that the OT reward for the better\ntrajectory b is generally larger, which validates the previous explanations."}, {"title": "Two Key Observations", "content": "Notably, there are two key observations of OT reward that have been less discussed in prior work:\n1. The OT reward is order invariant.\n2. The OT reward at step $i$ is influenced by the later steps so that two transitions with the same\nstate-action pair could have different OT rewards.\nOur first observation is that the standard OT-reward is order invariant. As shown in Eqn.(5), the order\ninformation is discarded and the frames from the demo trajectory are treated as bag-of-temporally-\ncollapsed frames. In our view, collapsing the temporal axis drops arguably one of the most important\ncharacteristic features of temporal order information. More concretely, consider a demo trajectory\nof $\\tau_1 = (o_1, o_1, o_2)$, meaning the agent first stays in the first state and then moves to the second\nstate. Our goal is to imitate this behavior. However, if we discard the order information as in Eqn.(5),\nfrom the perspective of OT reward, there is no ability to differentiate between $\\tau_1$ and some other\nundesired trajectories, i.e., $\\tau_2 = (o_1, o_2, o_1)$ which first moves to the second state and then moves\nback to the first state. Therefore, discarding the temporal order information in reward calculation\nmakes the reward on top of it under-constrained, thereby increasing the likelihood of convergence\ntoward undesired solutions."}, {"title": "Temporal Optimal Transport Reward (TemporalOT)", "content": "In this subsection, we present the Temporal Optimal Transport (TemporalOT) reward. We first\nexplain our motivations for the model design, and then introduce the details of the proposed method."}, {"title": "Motivation for the Model Design", "content": "The pipeline of a standard OT-based reward calculation usually consists of two stages:\n(Stage-1) first define a transport cost function $c(\\cdot, \\cdot)$ between two states;\n(Stage-2) then solve an OT optimization problem in Eqn.(4) to approximate the optimal transport\nplan $\\mu^*$ and compute the OT reward $r^{\\text{OT}}$ in Eqn.(5) for each state in a trajectory $\\tau$.\nAfter the OT-reward calculation step, the transition will be relabeled with the OT-reward for training\nan RL agent as in Eqn.(1). Our method aims to improve both stages of OT-reward calculation. Firstly,\nprevious methods usually use a pair-wise cosine similarity based cost function in Stage-1, which\nsometimes could be inaccurate and noisy. Secondly, previous methods ignore the temporal order\ninformation in Stage-2 as discussed in Section 3.1.2. We will introduce two simple solutions to\naddress these two points, respectively."}, {"title": "Context Embedding-based Cost Matrix for Improving Stage-1", "content": "To learn a more accurate transport cost function, we introduce a context embedding based cost matrix.\nUnlike previous methods that use a pair-wise cosine similarity as the transport cost, we adopt a\ngroup-wise cosine similarity that we define the transport cost between agent observation $o_i$ and expert\nobservation $o_j^\\mathcal{E}$ as following:\n$\\hat{c}(o_i, o_j^\\mathcal{E}) = \\frac{1}{k_c} \\sum_{h=0}^{k_c-1} \\bigg[ 1 - \\frac{\\langle f(o_{i+h}), f(o_{j+h}^\\mathcal{E}) \\rangle}{||f(o_{i+h})|| \\cdot ||f(o_{j+h}^\\mathcal{E})||} \\bigg],$\nwhere $k_c$ is the parameter of the context length and $f(\\cdot)$ is a fixed visual encoder. The goal of the\ncontext cost matrix $\\hat{C}$ is to facilitate expert progress estimation by taking nearby information into\nconsideration. For example, we use $k_c = 3$ in Figure 3 and the transport cost between $o_1$ and $o_0^\\mathcal{E}$ is\n$\\hat{c}(o_1, o_0^\\mathcal{E}) = 1 - [\\cos(f(o_1), f(o_0^\\mathcal{E})) + \\cos(f(o_2), f(o_1^\\mathcal{E})) + \\cos(f(o_3), f(o_2^\\mathcal{E}))]/3$."}, {"title": "Temporal-masked Optimal Transport Objective for Improving Stage-2", "content": "The prevalent OT reward ignores the temporal order information and takes the information of every\nstep in the trajectory into consideration, as shown in the Eqn.(5). As pointed out by some previous\nwork, the OT reward is not always correct where noisy OT rewards could distract the agent from\nlearning some key early behaviors [30]. To mitigate this issue, we introduce a concise solution by\nadding a temporal mask to the cost matrix.\nFor an agent trajectory $\\tau = {o_1, \\dots, o_T}$ and an expert trajectory $\\tau_\\mathcal{E} = {o_0^\\mathcal{E}, \\dots, o_T^\\mathcal{E}}$, we denote\nthe context cost matrix as $\\hat{C} \\in \\mathbb{R}^{T\\times T_\\mathcal{E}}$ and the transport plan as $\\mu \\in \\mathbb{R}^{T\\times T_\\mathcal{E}}$. The row sum and column\nsum of $\\mu$ equal to the constraint $s = [\\frac{1}{T}, \\dots, \\frac{1}{T}] \\in \\mathbb{R}^T$. We proposed to introduce a temporal mask\n$M \\in \\mathbb{R}^{T\\times T_\\mathcal{E}}$ to the transport plan, where $M(i, j) \\in [0, 1]$. We can express the masked optimal\ntransport objective in the following vector form [16]:\n$\\mu^* = \\underset{\\mu}{\\text{arg}}\\min (M \\odot \\mu, \\hat{C})_F - \\epsilon H(M \\odot \\mu), \\text{ s.t. } \\mu \\mathbb{1} = \\mu^T \\mathbb{1} = s,$\nwhere $(\\cdot, \\cdot)_F$ is the Frobenius norm and we add an entropy regularizer $H(\\cdot)$ of the masked transport\nplan $M \\odot \\mu$. We can solve Eqn.(7) by the Lagrangian:\n$\\mathcal{L}(\\mu, \\alpha, \\beta) = (M \\odot \\mu, \\hat{C})_F + \\epsilon (\\langle M \\odot \\mu, \\log(M \\odot \\mu) \\rangle_F - \\mathbb{1}^T(M \\odot \\mu)\\mathbb{1}) - (\\alpha, (M \\mu)\\mathbb{1} - S)_F - (\\beta, (M \\odot \\mu)^T \\mathbb{1} - s)_F,$\nwhere $\\alpha$ and $\\beta$ are two Lagrangian multipliers. By using different temporal mask, we can control\nwhat kind of temporal order information we use in the OT reward. For example, $M = \\mathbb{1}$ degrades to\nthe original OT reward without temporal order information, and a lower triangle matrix corresponds\nto the causal mask in the Transformer decoder [55], which indicates that we only concern the past\nsteps observations. In our method, we use a variant of the diagonal matrix:\n$M(i, j) = \\begin{cases}\n1, & \\text{if } j \\in [i - k_m, i + k_m],\\\\\n0, & \\text{otherwise},\n\\end{cases}$\nwhere $k_m$ is a window size parameter that controls the scope we use in the masked OT rewards. A\nsmaller mask window size $k_m$ refers to a closer match w.r.t. to the expert demonstration. We select a\ndiagonal-like matrix because we follow previous learning from demonstration literature to assume\nthat the agent has a similar movement speed as the expert [30]. Under this assumption, we adopt the\ndistance between the time step indexes to represent temporal affinity information. Figure 3 illustrates\nthe main ideas of the proposed TemporalOT method."}, {"title": "Experiments", "content": "In this section, we aim to answer the following questions: (1) How does the proposed TemporalOT\nmethod perform compared with other baselines? (2) Are the proposed context-embedding based cost\nfunction and temporal mask useful? (3) How do the key parameters influence the performances? (4)\nIs TemporalOT effective with both state-based and pixel-based observations?"}, {"title": "Experimental Setup", "content": "We implement TemporalOT-RL in PyTorch [42] based on the official ADS implementation\u00b9. We use a\npretrained ResNet50 [20] network as the fixed visual encoder to extract the image embedding for each\npixel observation. Unlike the original ADS experiments which use a fixed goal in each task, we adopt\na more challenging setting where the goal position changes for each episode. Moreover, we only\nprovide two expert video demonstrations to the RL agent. For the experiment results, we evaluate the\nRL agent for 100 trajectories every 20000 steps. We report the mean and standard deviation of the\nevaluation success rate across 5 random seeds. We define one trajectory to be successful if the RL\nagent solves the task at the last step. More detailed information is available in the Appendix B."}, {"title": "Baselines", "content": "We compare the following baseline methods. (1) TaskReward: training a backbone RL agent from\nDrQ-v2 [56] with the oracle task reward $r_{\\text{task}} = \\text{success}$. The reward is 1 when the task is solved and\notherwise the reward is 0. (2) BC: a naive behavior cloning agent which has the access of the expert\nactions. (3) GAIfO: another IL baseline which learns a discriminator to provide proxy reward [53]. (4)\nOT: we use an online version of OTR [31] agent where we first rollout the RL agent to collect online\ntrajectories and then relabel the reward with OTR for RL training. (5) ADS: a variant of OT baseline\nwhich adaptively adjusts the discount factor w.r.t. a progress tracker [30]."}, {"title": "Results on the Meta-world Benchmark Tasks", "content": "We first validate the effectiveness of TemporalOT on nine Meta-world [57] tasks. Experiment results\nare shown in Table 1. We can observe that TemporalOT generally outperforms the other baselines\nwithout using the task rewards. Moreover, the TaskReward baseline only shows good performance on\nthe Door-lock and Window-open tasks. The main reason is that the RL agent fails to collect the first\nsuccessful trajectory. For example, the goal of the Basketball task as shown in Figure 1 is to pick\nup the basketball and move to a target position above the rim. With the oracle sparse task reward,\nthe RL agent only receives a nonzero reward until it first successfully solves the task. Under such\ncircumstances, it is particularly challenging to collect the first successful trajectory with only zero\ntask rewards and random action explorations. On the other hand, we can observe that the two IL\nbaselines, BC and GAIfO [53], perform much worse than other OT-reward based baselines. This\nis because we only provide two expert video demonstrations with a few hundred samples, where\nthe IL-based methods suffer from an over-fitting issue. We further compare two OT agent baselines,\nwhere OT0.99 uses $\\gamma = 0.99$, and OT0.9 uses $\\gamma = 0.9$. We have a similar conclusion as in ADS that\nusing a smaller discount factor is helpful to learn early behaviors in some tasks that strongly rely on\nthe progress dependency, i.e., Basketball. TemporalOT outperforms the recent SOTA baseline ADS\nin 8 out of 9 tasks, which proves the effectiveness of the proposed method."}, {"title": "Ablation Studies on Different Model Components", "content": "We further conduct ablation studies to validate the effectiveness of the proposed context cost matrix\nand temporal mask in TemporalOT. In Figure 4, no-mask refers to a variant of TemporalOT without\nmask and no-context refers to a variant of TemporalOT without the context cost matrix. We can"}, {"title": "Ablation Studies on Different Key Parameters", "content": "We then validate the efficacy of different key parameters, i.e., the context length $k_c$ for the context\nembedding, window size $k_m$ for the temporal mask, and the demonstration number $N_E$. From\nFigure 5, we can observe that a medium number of $k_c$ and $k_m$ performs the best and a larger $N_E\nimproves the performances. A large context length $k_c$ does not perform well because it will distract the\nOT reward from the current step and introduce extra reward noise. A smaller mask window size $k_m\nmakes the learning more difficult because it only receives information from nearby observations, and\na larger $k_E$ will gradually degrade to the naive OT reward. Further, having more expert demonstrations\nis helpful in mitigating the potential over-fitting issue and improving the final performance."}, {"title": "Results with Pixel-based Observations", "content": "We also evaluate the proposed method with pixel-based observations, where we follow the same DrQ-\nv2 model setting as the ADS baseline. Figure 6 shows the results of the comparison of TemporalOT\nwith ADS. We can observe similar conclusions as in Table 1 that our proposed TemporalOT method\nalso outperforms the ADS baseline with pixel-based inputs, where TemporalOT usually converges\nfaster than ADS and (or) achieves a higher final success rate. Moreover, we can observe that\nsometimes the pixel-based agent learns faster than its dense state-based counterpart, which indicates\nthat the agent can extract more effective representations from the pixel inputs."}, {"title": "Visualization for Bad Cases", "content": "In this subsection, we visualize some bad cases of OT-based RL agents to provide readers more\ninsights about when OT-based RL agents are less useful. Figure 7 plots a typical bad case for the\nOT/ADS/TemporalOT agents in the Hand-insert task. The top row is the expert trajectory, and the\nsecond row is the agent trajectory. The goal of the Hand-insert task is to move the brown block to a\ntarget position in the hole. We can observe that the RL agent mainly focuses on imitating the arm\nbehaviors which ends in the target position, but it ignores the brown block. The main reason for the\nthis bad case is that the color of brown box is very close to the table background which sometimes\nmake it difficult for the pretrained visual encoder to capture the subtle information. More bad case\nanalyses are available in the Appendix A.2."}, {"title": "Related Work", "content": "Learning with a Few Demonstrations. There is a large body of work on leveraging demonstrations\nfor policy learning, ranging from the basic behavior cloning [12, 45] to demonstration-aided RL [43].\nThere is also work on leveraging demonstration data for offline pre-training [60, 36, 61], to either\nwarm-start the policy [36] or help with exploration [61, 37, 22]. However, the amount of demonstra-\ntions required for a high-quality pre-training is typically large. In this work, we focus on the setting\nwhere only a small number of demonstrations are provided [5], thus greatly relieving the burden of\ngenerating demonstrations. Optimal Transport based imitation is a recently emerged approach in this\ndirection, which will be reviewed in the subsequent section.\nOptimal Transport-based Reward for Imitation and RL. Optimal Transport (OT) has been shown\nto be effective for imitation learning [18, 31]. Optimal Transport Reward Labeling (OTR) [31] uses\nSinkhorn distance [4] to compute a similarity metric for a trajectory w.r.t. an expert demonstration\nand uses this metric as rewards for offline RL datasets without rewards [31]. Automatic Discount\nScheduling (ADS) [30] uses a similar OT-based approach for reward calculation. The core idea of\nADS is to incorporate a scheduling of the discount factor for online RL to mitigate the potentially\ndistracting OT reward from temporally distant states. Our work aligns with previous work in this\ncategory, and addresses some common issues that are shared by previous methods."}, {"title": "Limitations", "content": "Since our work is closely related to IL, our method shares some common limitations of IL. For\nexample, the success of our method heavily depends on high-quality expert video demonstrations.\nIf we are facing a new task without any available expert demonstrations, our method will be less\nuseful. Moreover, if the given demonstrations are sub-optimal or biased, the learned policies will\ninherit these flaws as well. Moreover, the performance of the proposed method relies on the quality of\nthe pretrained visual encoder. If the pretrained visual encoder fails to capture some key information\nin the pixel observation, then our method will also fail to take such key details into consideration.\nAnother limitation of our work is that the computation cost of the proposed method is related to the\nnumber of the given expert video demonstrations. A larger number of expert demonstrations will\nincrease the computation cost when we compute the optimal transport plan."}, {"title": "Conclusion", "content": "This paper studies the problem of learning effective robot policies with expert video demonstrations.\nWe focus on a challenging setting where there are only two demonstrations available and the environ-\nment does not provide any task reward. Following the line of research of OT-based proxy reward, we\nfirst discuss some challenges of the existing methods due to the overlook of temporal information.\nFurther, we introduced a new method named TemporalOT, which incorporates temporal information\nto existing baseline by using a context-embedding based cost matrix and a mask mechanism. Experi-\nments on nine Meta-world benchmark tasks showcase the effectiveness of the proposed method. One\ninteresting future direction is to extend the current method to a camera-view invariant agent, where\nwe can learn policies w.r.t. expert video demonstrations from different camera views."}, {"title": "Additional Experiment Results", "content": ""}, {"title": "Evaluation Curves", "content": "Figure 8 shows the evaluation curves corresponding to the results in Table 1. We can observe that\nTemporalOT generally outperforms the other baselines which do no use the task reward."}, {"title": "Bad Case Analysis", "content": "In this subsection, we analyze more bad cases in different tasks. Similar to Figure 7, we visualize\nthe expert trajectory (top) and an agent trajectory (bottom) of some typical bad cases in different\ntasks. We can observe that the robot arm generally displays a similar behavior as demonstrated in the\nexpert trajectory. The agent did not solve these tasks because it \u2013 (A) failed to grab the handle in the\nDoor-open task; (B) failed to touch the knob in the Door-unlock task; (C) failed to pick the red block\nin the Push task; (D) failed to pick up the blue stick in the Stick-push task."}, {"title": "Pretraining with Expert Data", "content": "In this subsection, we validate the effectiveness of using imitation learning, i.e., behavior cloning\n(BC), to first initialize the robot policy and then fine-tune it with TemporalOT. In this experiment, we\nuse action-inclusive expert demonstrations to pretrain the robot policy with behavior cloning loss.\nTable 2 shows the results on the Door-open task. The BC baseline is a pure offline method where the\nparameters are fixed after pretraining. TemporalOT-P is the variant that fine-tunes a pretrained BC\npolicy using TemporalOT. We can observe that incorporating pretraining helps to improve the sample\nefficiency. Moreover, we can notice a small success rate drop (which recovers later) at the initial\nphase when we transit from offline to online training from step 0 to step 4e4. This is an initial-dipping"}, {"title": "Ablation Studies"}]}