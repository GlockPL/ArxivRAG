{"title": "Boosting Healthcare LLMs Through Retrieved Context", "authors": ["Jordi Bayarri-Planas", "Ashwin Kumar Gururajan", "Dario Garcia-Gasulla"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, and yet, their factual inaccuracies and hallucinations limits their application, particularly in critical domains like healthcare. Context retrieval methods, by introducing relevant information as input, have emerged as a crucial approach for enhancing LLM factuality and reliability. This study explores the boundaries of context retrieval methods within the healthcare domain, optimizing their components and benchmarking their performance against open and closed alternatives. Our findings reveal how open LLMs, when augmented with an optimized retrieval system, can achieve performance comparable to the biggest private solutions on established healthcare benchmarks (multiple-choice question answering). Recognizing the lack of realism of including the possible answers within the question (a setup only found in medical exams), and after assessing a strong LLM performance degradation in the absence of those options, we extend the context retrieval system in that direction. In particular, we propose OpenMedPrompt a pipeline that improves the generation of more reliable open-ended answers, moving this technology closer to practical application.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are the default solution for most text-related tasks. However, a critical concern remains: their factual accuracy\u00b9, a limitation inherent to their generative nature. LLMs are not designed to retrieve precise information, but rather to generate plausible text based on learned patterns. A popular approach to enhance the factuality of LLMs is to contextualize them by biasing their output through relevant input tokens. This ranges from simple prompting techniques, such as \"Let's think step by step,\" to more sophisticated Retrieval Augmented Generation (RAG) systems. Indeed, integrating context retrieval systems can significantly boost the performance and reliability of LLMs.\nIn the domain of medical multi-choice question-answering (MCQA), the current state-of-the-art is dominated by private models like GPT-4 and MedPalm-2. According to popular evaluation methods, open models significantly lag behind, limiting their practical use. However, a comprehensive assessment must include context retrieval systems and consider more realistic evaluation methods. This study addresses two key research questions: First, how competitive can open LLMs be in healthcare when enhanced with optimized context retrieval systems? Second, how can these systems be extended beyond the limited domain of multi-choice QA?\n\u00b9We consider reasoning to be beyond a text task"}, {"title": "2 Related Work", "content": "Addressing the challenge of LLM factuality has spurred extensive research. Initial attempts to improve it centered on harnessing the inherent In-Context Learning (ICL) abilities of LLMs [5], allowing them to adapt to new tasks with minimal examples and without specific training. This paved the way for the development of sophisticated prompting techniques designed to elicit more accurate and reasoned responses. Chain of Thought (CoT) prompting [32] guides LLMs through intermediate reasoning steps, enhancing their performance on complex tasks. In contrast, Self-Consistency (SC) [31] exploits the stochastic nature of LLMs, producing and comparing several outcomes for the same input, before producing a unified answer. Both are combined in Self-Consistency Chain of Thought (SC-CoT). In a similar fashion, tree of Thought (ToT) [33] builds a search space across generated steps towards the answer. At each step several follow-up options are generated, evaluated, and selected, building a pruned tree in the process, from which the final answer is extracted. Reflection methods [23, 25] employ the same generation model as a critique model to iteratively critique the intermediate output and then generate an improved output based on this feedback.\nRecognizing the limitations of relying solely on internal knowledge, researchers turned to external knowledge integration through prompting techniques, ultimately leading to the emergence of Retrieval Augmented Generation (RAG) [13]. RAG systems retrieve and integrate relevant information from external knowledge bases, significantly enhancing LLM performance by biasing responses with factual data. In the healthcare domain, few-shot, CoT and SC are recurrently used to improve factuality [15, 21, 24, 29], and a combination of those was proposed in Medprompt [19], a context retrieval system designed for medical MCQA that achieves state-of-the-art results with GPT-4. While Medprompt has been adapted for open-source models like [16], a thorough investigation into the optimal configuration of its components (e.g., DBs, embeddings) remains an open area of research."}, {"title": "3 Methods", "content": "This section details the methodology employed to optimize a context retrieval system and evaluate its performance. We first outline the core components of the context retrieval system under investigation (\u00a73.1), followed by a description of the benchmark datasets used for evaluation (\u00a73.2), and conclude with the specific models and computational resources employed in the study (\u00a73.3).\nLet us start with the retrieval system architecture, which is based on the Medprompt design. Figure 1 illustrates this question-answering system, highlighting the key components that will be explored and optimized in this work."}, {"title": "3.1 Retrieval Components", "content": "In this work, we consider the role and impact of the following components:\n\u2022 Choice shuffling. This technique involves randomly shuffling the order of the answer choices presented in multiple-choice questions to mitigate position bias [15, 19]).\n\u2022 Number of ensembles. Refers to the number of independently generated responses produced by the LLM for a given question. These responses are then aggregated, to arrive at the final answer (through techniques such as majority voting). The effect of this component is experimented with the SC-CoT technique.\n\u2022 Database. This component represents the external knowledge source used to contextualize and bias the model. We experiment with two distinct database approaches: utilizing the validation set of the datasets to generate examples in execution time as seen in the original Medprompt methodology, and constructing custom databases derived from the training sets.\n\u2022 Embedding model. The embedding model is crucial for retrieving relevant information from the database. It transforms both the input question and the database entries into numerical vector representations, allowing for similarity comparisons. We evaluate various embedding models, considering factors like dimensionality (e.g., 768 vs. 4096) and domain specificity (e.g., general purpose vs. healthcare-specific).\n\u2022 Reranking model. This optional component aims to refine the initial retrieval results by re-ranking the candidate QA's retrieved from the database based on their relevance to the input question. We employ the MedCPT-Cross-Encoder [12], a specialized medical reranking model trained on a large corpus of biomedical literature.\nBeyond these core components, we also explore the impact of hyperparameters such as temperature and the number of few-shot examples, though their impact is less pronounced."}, {"title": "3.2 Datasets", "content": "To evaluate the performance of our optimized system, we employ four widely recognized medical Multiple-Choice Question Answering (MCQA) datasets:\n\u2022 MedQA [11]: Consists of 1,273 multiple-choice questions in the format of the US Medical License Exam (USMLE).\n\u2022 MedMCQA [22]: Large-scale multiple-choice question answering dataset with questions from Indian medical school entrance exams. We use the validation set which is composed of 4,183 questions, as the answers of the test set are private.\n\u2022 CareQA [6]: Multiple-choice question answering dataset based on the access exam for Spanish Specialised Healthcare Training. It consists of 5,621 questions.\n\u2022 MMLU [9]: MMLU is a multitask benchmark suite of 57 different datasets spanning domains across STEM. From all these tasks we use the medical related: anatomy, clinical knowledge, college biology, college medicine, medical genetics and professional medicine, which account for a total of 1,089 questions.\nThese datasets collectively provide a comprehensive and diverse evaluation platform for assessing the performance of our system across different multi-choice medical question styles and sources."}, {"title": "3.3 Models and Compute", "content": "The main model used in our experiments is Llama3-Aloe-8B-Alpha [7], a state-of-the-art open-source LLM specifically fine-tuned for the healthcare domain. With 8 billion parameters, this model builds upon the Meta Llama-3 architecture, leveraging a curated combination of high-quality medical data, synthetic data, and targeted alignment via DPO and Red-teaming datasets. It offers a compelling balance between performance and computational cost.\nAll experiments are conducted on a single compute node equipped with 4x NVIDIA H100 (64GB) GPUs. We utilize a single GPU for smaller models (<10B parameters) and employ tensor parallelism"}, {"title": "4 Retrieval Experiments", "content": "This section details the experiments conducted to evaluate the impact of different Context Retrieval (CR) components on the performance of Large Language Models (LLMs) in answering healthcare questions. We begin by establishing a baseline performance without any context retrieval (\u00a74.1), followed by a systematic investigation of individual components within the Self-Consistency with Chain-of-Thought (SC-CoT) framework. We then study the impact of adding external knowledge sources and the various components in the Medprompt architecture (\u00a74.2). Based on these findings, we propose an optimized CR configuration (\u00a74.3) and compare its performance against state-of-the-art models (\u00a74.4)."}, {"title": "4.1 SC-CoT Experiments", "content": "Table 1 presents the baseline performance of Llama3-Aloe-8B-Alpha, our primary evaluation model, on four benchmark datasets using zero-shot next token prediction, CoT, and SC-CoT. This serves as a reference point for evaluating the subsequent improvements achieved through the integration of various components."}, {"title": "4.2 Medprompt Experiments", "content": "At this point, we extend the SC-CoT scheme with retrieval components, exploiting external data sources. This will include the main elements included in Medprompt: An embedding model, a database, and reranker model. The embedding model encodes both input and database items before computing their similarity scores. For this component, we consider four different models, ranging in embedding size and in number of parameters. We also consider whether they have been specialized in the healthcare domain or not. These properties are listed in Table 4, and their performance in Table 5. Results show all models achieve comparable performance in most datasets, with no embedding model clearly outperforming the rest. As a result, we select the healthcare-specific, cheaper model PubMedBERT to be the embedding model for our future experiments.\nThe second retrieval component is the database. That is the documental source from which pieces of text are extracted and introduced into the model prompt for contextualization. Both the quality and the diversity of those databases have a large impact on performance. To test this hypothesis we test two setups. First, a smaller database, composed by the validation set of each dataset. For CareQA (which lacks a validation split) we use the MedMCQA validation split as database. The second setup includes a larger and augmented database. Instead of the validation split, we use the training splits of MedMCQA (180K QA pairs, also used for CareQA and MMLU) and MedQA (10K QA pairs).\nIn Medprompt, these samples are enhanced using CoT. In addition, we consider ToT as well. In both cases, samples are enhanced by prompting an LLM chosen for its instruction-following capability. For CoT, we prompted the Mixtral-8x7B [2] model with the question, the possible answers, and the correct choice, and then asked to analyse each option individually, to explain the answer through detailed reasoning, and to end with a re-identification of the selected option (which is tested for validity). For ToT, we followed the same approach but used Llama-3.1-70B-Instruct to generate the answers. We adapted the original ToT prompt to simulate three logical experts collaborating to answer the question. The size of both databases is exactly the same.\nWhile the MedMCQA and MedQA experiments study the impact of size and quality in databases, the experiments on CareQA and MMLU add a factor of generalization (by using a DB from a different source). Results are shown in Table 6. In three out of four cases, the synthetically enhanced data improves the performance of the retrieval system. Even when the database comes from a different source, the extended database contributes to increase accuracy. Between CoT and ToT, the first outperforms the second in three out of four datasets. All further experiments will make use of the CoT extended databases.\nThe final component we study is the use of a reranker model. The role of the reranker is to sort the most similar items retrieved from the database, which yields performance boosts in certain domains [30]. To that end, we use the MedCPT-Cross-Encoder model. That is a contrastively pre-trained transformer, tuned on PubMed information retrieval. Table 7 shows the inconsistent performance gains achieved with the reranker, leading us to exclude it from further experimentation due to its added computational cost."}, {"title": "4.3 Proposed Scheme", "content": "Based on our empirical findings, we propose an optimized context retrieval configuration that follows the Medprompt (Figure 1) scheme. The main component removed is the reranker model. The proposed setup utilizes the following:\n\u2022 Choice Shuffling: Enabled to mitigate position bias.\n\u2022 Number of Ensembles: 5 as the default, as this provides the best trade-off between performance gains and computational cost associated. 20 for benchmarking.\n\u2022 Database: CoT-augmented training sets, providing the most comprehensive and effective knowledge source.\n\u2022 Embedding Model: PubMedBERT, a small and efficient healthcare-specific embedding model, is chosen.\n\u2022 Reranking Model: Excluded due to its inconsistent performance gains and added computational overhead."}, {"title": "4.4 State-of-the-art Comparison", "content": "In this section, we benchmark the performance of our optimized CR system against a diverse set of open-source state-of-the-art general-purpose and healthcare-specific LLMs, including models of varying sizes and architectures.\n\u2022 Aloe-8B: A fine-tuned version of Llama3 8B for the healthcare domain, tuned with multi-choice QA data. The retrieval system was optimized for this model, its results may be biased.\n\u2022 Llama-3.1-8B/70B [3]: Latest models released by Meta in 2024. Instruct versions are tuned using supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF).\n\u2022 Qwen-2-7B/72B [4]: Instructed versions of the new series of Qwen large language models.\n\u2022 Mistral-7B-Instruct-v0.3 [10]: Third version of the 7B model developed by Mistral AI.\n\u2022 Gemma-2-27B-it [27]: Instruct tuned version of the larger model from the Gemma family, which is a family of state-of-the-art open models from Google.\n\u2022 Yi-1.5B-34B-Chat-16K [1]: The Yi series of models are a family of LLMs trained from scratch by 01.AI. Yi-1.5 is an upgraded version of Yi, continuously pre-trained on top of it. The chat version is the instruct-tuned version.\nFor each model in this list, we conduct the same evaluation, with and without the context retrieval setup. Results, shown in Table 8, indicate a unanimous boost in performance in all datasets and models. The gains are generalized but non-homogeneous. These are higher on less-performing models, clearly influenced by both the smaller model size and the larger room for improvement.\nPerformance improvement seems to be highly dependent on the model family. Llama-based models (Llama3-Aloe 8B, Llama 3.1 8B/70B) seem to benefit particularly from context retrieval systems, while other models enjoy lower gains (e.g., Qwen2 72B). LLM performance on a retrieval system is in fact being consistently affected by training policies. There are also differences in gains across benchmarks, with some obtaining consistently higher benefits from context retrieval than others (e.g., MedMCQA). That is a remarkable difference taking into account the huge task similarities shared by all four datasets (i.e., multiple-choice medical question answering). This points towards the importance of data sources, and the challenges of generalization.\nOverall, the performance gain provided by the context retrieval scheme is highly valuable, as it reduces the costs of having highly reliable healthcare systems. It shows a well-tuned system based on small LLMs reaching the accuracy levels of much bigger models."}, {"title": "4.5 Private Comparison", "content": "To contextualize our results further, we include performance data reported for prominent private models, not been reproduced by this work. The open models under study include:\n\u2022 GPT-4 [19, 20]: Developed by OpenAI, with an undisclosed number of parameters but estimated to be at least in the"}, {"title": "5 Open-Ended Answer Generation", "content": "While multiple-choice question answering (MCQA) benchmarks have been valuable in evaluating Large Language Models (LLMs) for medical applications, they fail to fully capture the complexities of real-world clinical scenarios. In practice, healthcare professionals often need to formulate comprehensive answers without pre-defined options. This necessitates a shift towards open-ended question-answering capabilities in medical AI systems.\nOur preliminary analysis revealed a significant performance gap when transitioning from multiple-choice to open-ended formats. Table 10 illustrates this disparity, showing a substantial decrease in accuracy of 10% for the Llama-3.1-8B-Instruct model on the MedQA dataset when transitioning from multiple choice questions (MCQs) to open-ended (OE) questions. Surprisingly, the incorporation of"}, {"title": "5.1 OpenMedprompt", "content": "OpenMedprompt adapts the Medprompt architecture for open-ended question answering. We replace the MCQA database with an OE-QA database and remove components specific to multiple-choice formats. We propose two strategies for consensus-building and answer refinement:\nOpenMedprompt with Ensemble Refining (OM-ER): This strategy leverages the diversity of multiple generated answers to produce a refined and more accurate final response. It involves generating N initial answers with randomized temperature and top_p parameters, incorporating K relevant examples from the database into the prompt. Then, the LLM synthesizes these N answers into a single, refined response.\nOpenMedprompt with Self-reflection(OM-SR) [23, 25]: This strategy employs a feedback loop to improve the generated answer. It begins by generating an initial answer using the K most similar examples from the database. Then, it performs N iterations of self-reflection, where the model generates feedback on its previous response and produces an improved answer based on this feedback. We integrate attribute scores from ArmoRM-Llama3-8B [28], a reward model along with the critique model's reflection as an external feedback to guide answer generation.\nThe ArmoRM-Llama3-8B reward model provides reward scores across 19 attributes. Out of the nineteen total, we give the generation model only the following scores ultrafeedback-truthfulness, ultrafeedback-honesty,ultrafeedback-helpfullness and prometheus-score which have a good correlation with correct responses."}, {"title": "5.2 Dataset and Database Construction", "content": "We utilized the MedQA dataset, which is particularly suitable for this task as it contains medical questions that can be answered without providing multiple-choice options, with minimal or no rephrasing of the original question required, making it ideal for testing our proposed framework.\nTo create a robust context retrieval system, we construct two distinct databases using the MedQA training set. We use LLaMA-3.1-70B-Instruct to generate CoT and ToT answers for each question in the training set. Both databases are carefully curated to ensure the accuracy of the generated responses and their alignment with the ground truth answers."}, {"title": "5.3 Experiments", "content": "We evaluate the performance of OM-ER and OM-SR using an automated LLM-based judge (LLaMA-3.1-70B-Instruct). This judge assesses the correctness of the generated open-ended answers by comparing them to the ground truth answers.\nTable 11 presents the results of our experiments of OM-ER using the CoT and ToT databases. We see that both CoT and ToT databases show improvement over the baseline open-ended performance. ToT"}, {"title": "5.4 Results and Discussion", "content": "Our results demonstrate the effectiveness of OpenMedprompt in improving open-ended answer generation accuracy in the medical domain. Both OM-ER and OM-SR contribute to performance gains compared to the baseline. The choice of database and the number of retrieved examples also play a significant role in performance.\nSpecifically, we observe that OM-SR generally outperforms OM-ER across most configurations. This suggests that the iterative feedback loop and incorporation of reward model scores provide a more effective mechanism for refining the generated answers. The choice of database (CoT vs. ToT) also has an impact on the performance differently for each approach. OM-ER benefits more from the ToT database, while OM-SR shows stronger results with the CoT database at higher iteration counts. OM-SR often achieves good performance with fewer iterations compared to the number of ensembles required for OM-ER to reach similar accuracy levels. However, OM-ER might be preferred when speed and simplicity are prioritized, or when exploring diverse perspectives is beneficial.\nChoosing the Right Configuration:\n\u2022 Accuracy Priority: Choose OM-SR, particularly with higher iteration counts (N \u2265 10).\n\u2022 Complex Reasoning: OM-SR's self-reflection mechanism may be more effective for questions requiring intricate logical steps.\n\u2022 Speed and Simplicity: OM-ER with moderate ensemble sizes (N = 5-10) offers a good balance of improved accuracy and computational efficiency."}, {"title": "6 Conclusions", "content": "This work underscores the significant potential of augmenting Large Language Models (LLMs) with context retrieval systems to enhance their accuracy and reliability in the healthcare domain. Our exploration of Self-Consistency with Chain-of-Thought (SC-CoT) components revealed substantial gains through choice shuffling and an optimal number of ensembles, striking a balance between performance and computational cost. Further investigation into the Medprompt architecture highlighted the effectiveness of small, healthcare-specific embedding models and the value of enriching databases with Chain-of-Thought augmented examples. Conversely, the inclusion of a reranking model was found to be computationally expensive with inconsistent benefits, leading us to recommend against its use.\nOur optimized context retrieval configuration, when applied to a diverse set of open-source LLMs, consistently boosted performance across multiple medical question-answering benchmarks. Notably, smaller models experienced the most significant improvements, showcasing the ability of well-tuned retrieval systems to bridge the performance gap between smaller open models and larger private alternatives. This finding has profound implications for democratizing access to high-performing healthcare AI systems, reducing reliance on resource-intensive large models. Moreover, our results demonstrate that open LLMs augmented with our optimized CR system can achieve accuracy comparable to, and in some cases surpassing, state-of-the-art private solutions like MedPalm-2 and GPT-4.\nRecognizing the limitations of multiple-choice question answering (MCQA) in mirroring real-world clinical scenarios, we extended our approach to develop OpenMedprompt, a novel framework for open-ended medical question answering. Two distinct strategies, OpenMedprompt with Ensemble Refining (OM-ER) and OpenMedprompt with Self-Reflection (OM-SR), were introduced and evaluated, revealing their effectiveness in improving open-ended answer generation accuracy. OM-SR, with its iterative feedback loop and integration of reward model scores, generally outperformed OM-ER, suggesting the potential of self-reflection mechanisms for complex medical reasoning.\nBy releasing our custom software repository as an open-source resource, we aim to empower the research community to further explore and refine these techniques, contributing to the development of more accurate, accessible, and impactful LLM applications in healthcare."}, {"title": "6.1 Future Work", "content": "Several avenues for future research remain, encompassing both the optimization of retrieval-augmented generation for multiple-choice questions and the further development of OpenMedprompt for open-ended answer generation.\nExpanding Retrieval System Capabilities: Dynamic Retrieval: Exploring dynamic retrieval techniques that adapt the number of retrieved examples based on the complexity of the question, potentially improving efficiency and accuracy. Multi-Database Integration: Investigating the integration of multiple knowledge sources, such as medical ontologies or clinical guidelines, to enrich the retrieval database and enhance the LLM's understanding of complex medical concepts. Cross-Lingual Retrieval: Adapting the retrieval system to support multiple languages, facilitating broader access to medical information and enabling cross-lingual medical question answering. Enhancing OpenMedprompt: Hybrid Approaches: Exploring combinations of OM-ER and OM-SR to leverage the strengths of both methods, potentially leading to a more robust and adaptable system for open-ended medical question answering. Advanced Reward Models: Investigating more sophisticated reward models tailored specifically to medical knowledge evaluation, capturing nuanced aspects of medical reasoning, factual accuracy, and clinical relevance. Prompt Engineering: Fine-tuning prompt structures for both initial answer generation and refinement stages, incorporating specific instructions, constraints, or contextual information to guide the LLM towards more accurate and comprehensive responses. Larger Models: Evaluating the performance of OpenMedprompt with more powerful language models to assess scalability and explore the potential for further accuracy gains. Domain Adaptation: Extending OpenMedprompt to other specialized domains beyond medicine, such as law or engineering, focusing on tailoring the retrieval database and reward models to the specific knowledge requirements of each domain."}, {"title": "6.2 Carbon Footprint", "content": ""}, {"title": "6.3 Ethical Considerations", "content": "This work seeks to increase the factuality and reliability of LLM-based systems. However, any work that deals with generative models like LLMs should be done in awareness of the several ethical limitations that the technology entails. This includes the computational footprint (which is considered in this work), as well as the factuality (which is the main target of this work). Limitations related to the potential for impersonation, self-diagnose and other non-approved uses, are further discussed in publications related with healthcare model releases (e.g., [7])."}]}