{"title": "Q-LIME \u03c0: \u0391 Quantum-Inspired Extension to LIME", "authors": ["Nelson Col\u00f3n Vargas"], "abstract": "Machine learning models offer powerful predictive capabilities but often lack transparency. Local Interpretable Model-agnostic Explanations (LIME) addresses this by perturbing features and measuring their impact on a model's output. In text-based tasks, LIME typically removes present words (bits set to 1) to identify high-impact tokens. We propose Q-LIME \u03c0 (Quantum LIME \u03c0), a quantum- inspired extension of LIME that encodes a binary feature vector in a quantum state, leveraging superposition and interference to explore local neighborhoods more efficiently. Our method focuses on flipping bits from 1 \u2192 0 to emulate LIME's \"removal\" strategy, and can be extended to 0\u21921 where adding fea- tures is relevant. Experiments on subsets of the IMDb dataset demonstrate that Q-LIME \u03c0 often achieves near-identical top-feature rankings compared to clas- sical LIME while exhibiting lower runtime in small- to moderate-dimensional feature spaces. This quantum-classical hybrid approach thus provides a new path- way for interpretable AI, suggesting that, with further improvements in quantum hardware and methods, quantum parallelism may facilitate more efficient local explanations for high-dimensional data.", "sections": [{"title": "1 Introduction", "content": "Machine learning models, particularly deep neural networks, have become ubiquitous in domains such as natural language processing, computer vision, and recommenda- tion systems. Despite their success, these models often lack transparency, prompting a growing need for methods that provide insight into how features contribute to their predictions. Local Interpretable Model-agnostic Explanations (LIME) [8] is one such framework that approximates the decision boundary of any black-box model locally around a single instance. By perturbing features and measuring the resulting change in prediction, LIME assigns importance scores to individual features. However, classical LIME faces limitations when working with high-dimensional feature spaces. Generat- ing, evaluating, and fitting a surrogate model to large numbers of perturbations can be computationally expensive, and the assumption of feature independence often fails to capture interaction effects.\nIn parallel, the field of quantum computing has seen significant advances, particu- larly in the realm of quantum machine learning [3]. Although fully scalable quantum hardware remains an open challenge, quantum-inspired methods have shown promise for potentially reducing complexity in certain data-driven tasks. Properties such as superposition and interference may offer new ways to sample local neighborhoods or capture interactions that are difficult to handle classically.\nBuilding on these developments, this paper proposes Q-LIME \u03c0 (Quantum LIME \u03c0), a quantum-inspired extension of LIME. By selectively encoding only the active features of a binary feature vector x into a quantum-like state, Q-LIME \u03c0 employs partial quantum-inspired superposition to represent and perturb local neighborhoods efficiently. This selective encoding focuses computational effort on flipping only the bits corresponding to features that are \"on\" (1), mirroring LIME's feature-removal strategy in a more efficient manner. Flipping bits (e.g., from 1 \u2192 0) corresponds to simple quantum operations such as the Pauli-X gate, which can be extended to investigate additional feature toggles if needed. In principle, this approach leverages superposition to provide a compact representation of multiple perturbation states, allowing us to capture feature contributions and potential interactions without linearly scaling the number of measurements.\nOur key contributions include:\n1. Quantum Encoding and Perturbation: We formalize how binary feature vec- tors can be embedded into a quantum state, and illustrate how flipping a feature maps to a Pauli-X operation in the quantum-inspired circuit.\n2. Surrogate Model Construction: We show how Q-LIME \u03c0 uses quantum-derived feature perturbations to fit an interpretable surrogate model akin to classical LIME.\n3. Complexity and Practical Considerations: We discuss the potential for effi- ciency gains, as well as limitations, especially regarding state preparation and measurement collapse on near-term quantum devices.\n4. Experimental Sketch and Use Cases: We provide a proof-of-concept exper- iment using a sentiment classification task, and explore how the approach might generalize to other domains."}, {"title": "2 Related Work", "content": "Local Interpretable Model-agnostic Explanations (LIME), proposed by Ribeiro et al. [8], has become a widely-used framework for explaining the predictions of black-box models. By locally perturbing an instance's features and training a simple surro- gate model around that instance, LIME provides estimated feature importances in a model-agnostic manner. Subsequent works have addressed various extensions and refinements, including measuring the robustness of explanations [1] and deterministic local expansions [10]. These studies underscore LIME's flexibility and the breadth of its adoption. Our proposed Q-LIME \u03c0 maintains LIME's core idea of local approx- imation but adopts a quantum-inspired perturbation strategy to potentially reduce computational overhead.\nQuantum computing has gained traction in machine learning research. Biamonte et al. [3] discuss quantum algorithms that could potentially enhance or accelerate classical ML tasks, including clustering and dimensionality reduction. Advances in quantum hardware have enabled the optimization of parametrized quantum circuits, with techniques like analytic gradient evaluation on quantum devices [9] streamlining the training process. Although current quantum devices face limitations in scale and noise, quantum-inspired methods those that emulate quantum properties such as superposition on classical hardware-are increasingly investigated for tasks involving high-dimensional feature spaces.\nThe emerging hybrid paradigm, which combines quantum and classical approaches, has garnered attention for its potential to unify the strengths of both computing paradigms. Du et al. [5] demonstrate that parametrized quantum circuits can achieve expressive power comparable to certain classical neural networks, while Cerezo et al. [4] survey variational quantum algorithms capable of optimizing quantum-classical models. Such work hints at promising avenues for interpretability: employing quantum or quantum-inspired techniques to shed light on the inner workings of machine learning models. Tools like PennyLane [2] further facilitate the integration of quantum hardware with classical frameworks, enabling automatic differentiation of hybrid computations.\nWhile Pira and Ferrie's Q-LIME [7] primarily addresses the interpretability of quantum neural networks by identifying and characterizing such \"indecisive\" regions, our Q-LIME \u03c0 extends LIME-inspired perturbation logic to a broader set of quantum classifiers particularly those that act on binary feature vectors, flipping bits 1 \u2192 0 to see how predictions shift. This perspective emphasizes a local surrogate model that can be either quantum or classical but remains simpler than the underlying quantum classifier. As such, both approaches underscore the necessity of local interpretability in"}, {"title": "3 Preliminaries", "content": "Definition 1 (Binary Feature Vector). Let $x = [x_1,x_2,...,x_n] \\in B^n$, where $x_i \\in {0,1}$, denote the presence (1) or absence (0) of the i-th feature.\nDefinition 2 (Classifier). A classifier is a function $f : B^n \\rightarrow [0,1]$ that maps a feature vector \u00d7 to the predicted probability of a specific class (e.g., positive sentiment).\nDefinition 3 (Feature Contribution). The contribution of a feature $x_k$ to the classifier's prediction is defined as:\n$Af_k = f(x) - f(x_{\\text{perturbed},k}),$\nwhere $x_{\\text{perturbed},k} = [x_1,...,1 - x_k,...,x_n]$ is the feature vector obtained by flipping $x_k$.\nDefinition 4 (Quantum State Encoding). A binary feature vector x is encoded as a quantum state:\n$|\\psi\\rangle = \\bigotimes_{i=1}^{n} R_y(\\theta_i)|0\\rangle,$\nwhere $R_y(\\theta_i) = e^{-i\\theta_iY/2}$ is a rotation about the y-axis, and:\n$\\theta_i = \\begin{cases} 2, & \\text{if } x_i = 1, \\\\ 0, & \\text{if } x_i = 0. \\end{cases}$\nThis ensures that a feature set to 1 places its corresponding qubit in state |+), while a feature set to 0 keeps its qubit in |0)."}, {"title": "4 Methodology", "content": "4.1 Quantum Perturbation\nThe central idea behind Q-LIME \u03c0 is to mirror the classical \"flip\" operation in LIME with a simple quantum gate. To analyze the contribution of $x_k$, the k-th qubit is flipped using the Pauli-X gate:\n$|\\text{perturbed},k\\rangle = X_k|\\psi\\rangle.$"}, {"title": "4.2 Algorithmic Sketch", "content": "Algorithm 1 summarizes Q-LIME \u03c0. We encode x (Definition 4), compute f(x), and for each bit set to 1, flip it to 0, measure, and compute $Afk$.\nImplementation Details. We developed Q-LIME \u03c0using:\n\u2022 Pennylane [2] for quantum simulation, specifying either shots=None for analytic mode or shots = 100 for partial sampling.\n\u2022 scikit-learn for training a logistic regression or other classifiers on binary bag-of-words feature vectors.\n\u2022 lime for generating classical LIME explanations with a typical setting of ~300 random perturbations per instance.\nWe remove HTML tags, lowercase the text, and optionally remove stopwords before vectorizing each review as a binary presence/absence vector. During quantum flips, if $x_i = 1$, we set the corresponding RY angle to 0, measure probabilities over all basis states, and sample one outcome. This new bitstring is then fed into our logistic classical_classifier to compute $Af_i$. Finally, we compare Q-LIME \u03c0'\u03c2 top features to LIME's. Since classical simulation scales exponentially with the number of qubits, we limit max features to around 15 in our experiments. Beyond that, runtime grows quickly on a standard CPU."}, {"title": "5 Experiments and Results", "content": "We run a proof-of-concept on the IMDb dataset [6], processing 500 reviews with HTML tags removed, text lowercased, and optionally removing stopwords. A logistic regres- sion classifier is trained on 80% of this data. For testing, we pick 5 random instances (per parameter configuration) to compare:\n1. Accuracy: on the 20% test split,\n2. Runtime: classical LIME vs. Q-LIM\u0395 \u03c0,\n3. Overlap: average number of top-5 features shared.\nTable 1 presents a subset of our measurements. We vary max_features \u2208 {5, 10, 15}, stopwords_option \u2208 {True, False}, and shots \u2208 {None, 100}. We observe that Q- LIME \u03c0'\u03c2 overlap with classical LIME can exceed 4 out of 5 features in some runs and that Q-LIME \u03c0 is often substantially faster than LIME when the number of features is small.\nTable 2 highlights three test instances (truncated for brevity) from a random sam- ple restricted to at most 15 features. In each row, we compare the top five words identified by classical LIME and Q-LIME \u03c0. In example (1), both methods select the exact same five tokens, indicating a complete overlap. Meanwhile, examples (2) and (3) each share three tokens, revealing minor differences in how each method ranks or includes certain words. Overall, the strong alignment underscores how Q-LIME \u03c0'\u03c2 flipping of bits from 1 \u2192 0 closely mirrors classical LIME's \"removal\" of present fea- tures. Small discrepancies typically stem from LIME's random sampling process or sample size, yet the overlap in all three examples remains substantial."}, {"title": "6 Conclusion and Future Directions", "content": "In this work, we introduced Q-LIME \u03c0, a quantum-inspired enhancement of the clas- sical LIME framework, aimed at improving the efficiency and scalability of local interpretability methods in machine learning. By leveraging quantum-inspired state encoding and bit-flipping perturbations, Q-LIME \u03c0 preserves the core mechanisms of LIME while offering computational advantages, particularly in low-to-moderate dimensional feature spaces. Experimental results on the IMDb sentiment analysis dataset revealed that Q-LIME \u03c0 achieves a high degree of alignment with classical presence or absence of transaction attributes can provide insights into risk models.\nLooking forward, this study opens avenues for further exploration of quantum-inspired approaches in AI explainability. Potential directions include investigating multi-feature interactions, optimizing encoding and perturbation strategies, and implementing these methods on emerging quantum hardware. With continued advancements in quantum computing, such hybrid frameworks may become increasingly valuable for addressing the challenges of high-dimensional data and complex feature interactions. By bridg- ing the domains of quantum computing and interpretable AI, Q-LIME \u03c0 represents an innovative step toward more transparent and computationally efficient machine learning models."}]}