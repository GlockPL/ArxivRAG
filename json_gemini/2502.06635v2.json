{"title": "Steel-LLM: FROM SCRATCH TO OPEN SOURCE \u2013 A PERSONAL JOURNEY IN BUILDING A CHINESE-CENTRIC LLM", "authors": ["Qingshui Gu", "Tianyu Zheng", "Shu Li", "Zhaoxiang Zhang"], "abstract": "Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and the training script are available at https://github.com/zhanshijinwat/Steel-LLM.", "sections": [{"title": "INTRODUCTION", "content": "The rapid advancements in open-source large language models (LLMs) have led to significant achievements in natural language processing (NLP), enabling applications ranging from conversational agents to code generation. However, despite these advances, challenges remain in the transparency, accessibility, and resource efficiency of LLM development. Many prominent LLMs, such as the Qwen series (Bai et al., 2023a; Yang et al., 2024), Llama (Touvron et al., 2023b), and Deepseek (DeepSeek-AI et al., 2024a), provide only the final model weights while withholding essential details such as training data, code, and intermediate checkpoints. This limited openness creates obstacles to reproducibility and prevents the broader research community from building upon these models effectively.\nSeveral initiatives, such as LLM360's Amber (Liu et al., 2023; Tan et al., 2024), M-A-P's Neo (Zhang et al., 2024b), and AI2's OLMO series (Groeneveld et al., 2024a; OLMo et al., 2024; Muennighoff et al., 2024), have addressed these limitations by releasing complete training pipelines, datasets, and intermediate checkpoints. While these contributions are invaluable, their development typically requires extensive computational resources, making them inaccessible to smaller research teams or individual practitioners. This creates a significant gap for fully transparent and resource-efficient LLMs tailored for smaller-scale research efforts, particularly in non-English languages such as Chinese."}, {"title": "RELATED WORKS", "content": "Recent developments in open source LLM have varied widely in terms of transparency and accessibility. Many models, such as Qwen (Bai et al., 2023a; Yang et al., 2024), Llama (Touvron et al., 2023b), Deepseek (DeepSeek-AI et al., 2024a), Gemma (Team et al., 2024), InternLM (Cai et al., 2024), Mixtral (Jiang et al., 2023), Yi (AI et al., 2024), GLM (GLM et al., 2024), have been released with only the final model checkpoints and weights, while withholding crucial details such as training data, codes, and intermediate checkpoints. This limited transparency hinders reproducibility and makes it difficult for the broader research community to fully understand or build upon these models.\nIn response to these challenges, several initiatives have adopted a more open approach by releasing complete training pipelines, datasets, and intermediate checkpoints. Notable examples include LLM360's Amber (Liu et al., 2023; Tan et al., 2024), M-A-P's MAP-Neo (Zhang et al., 2024b), and AI2's OLMO series (Groeneveld et al., 2024a; OLMo et al., 2024; Muennighoff et al., 2024),"}, {"title": "ARCHITECTURE", "content": "The model structure of Steel-LLM is adapted from Qwen(Bai et al. (2023a)).A Transformer block can be roughly divided into self-attention and Feed-Forward Network (FFN). An efficient implementation of self-attention is Flash Attention(Dao et al. (2022)), which is widely utilized. Flash Attention not only improves the efficiency of model training and inference but also saves GPU memory. Steel-LLM reuses Flash Attention and only makes improvements to the FFN layer. In the FFN layer, we adopts Soft Mixture of Experts (Soft MoE, Puigcerver et al. (2024)) and enhances the second layer of the MLP. The architecture of Steel-LLM's Transformer block is illustrated in Figure 1."}, {"title": "3.1 SOFT MOE", "content": "Mixture of Experts (MoE) (Jacobs et al., 1991) was first proposed in 1991 and is widely used in the field of recommendation systems (Ma et al., 2018). In the architecture of large language models, sparse MoE is commonly employed, such as in deepSeekMoE(Dai et al. (2024)), Qwen-MoE(Team (2024c)), etc. A typical practice to construct an MoE language model usually replaces FFNs in a Transformer with MoE layers(Lepikhin et al. (2021);Zoph (2022);Fedus et al. (2022)). An MoE layer consists of multiple experts, and each expert is a FFN. A gating network calculates scores to assign each token to a small subset of experts. Under the condition of the same parameter scale, the sparse MoE model activates fewer parameters than a dense model, thus reducing the number of FLOPS.\nSteel-LLM is trained using only 8 GPUs with limited GPU memory. For a model featuring sparse MoE structure, all parameters must be loaded into the GPU memory. Owing to the sparse nature of expert selection, in contrast to a dense model, the FFN parameters of the sparse MoE model cannot be fully trained. Steel-LLM is a small-scale language model with 1 billion parameters, leading to a relatively low computational load. Meanwhile, our objective is to fully train each parameter and leverage the advantages of the MoE structure to enhance model performance. Consequently, we ultimately opt for the soft MoE(Puigcerver et al. (2024)) structure. Soft MoE is fully differentiable. Therefore, there is no need to consider problems such as expert imbalance that exist in sparse MoE.\nWe denote the input tokens for a single sequence as $X \\in \\mathbb{R}^{m \\times d}$, where m represents the number of tokens and d denotes their dimensions. In the soft MoE layer, each expert processes p slots, and each slot has a corresponding d-dimensional learnable parameter matrix $\\Phi \\in \\mathbb{R}^{d \\times (n \\cdot p)}$. The number of slots is a crucial hyperparameter for adjusting the time complexity of the soft MoE layer. The input slots $\\mathcal{X} \\in \\mathbb{R}^{(n \\cdot p) \\times d}$ are obtained through convex combinations of all the m input tokens, which can be computed as follows:\n$D_{ij} = \\frac{\\exp((\\mathcal{X}\\Phi)_{ij})}{\\sum_{i'=1}^{m}\\exp((\\mathcal{X}\\Phi)_{i'j})}  \\mathcal{X} = D^T \\mathcal{X}$\nWe denote D as the dispatch weight matrix. It is obtained by applying the softmax function column-wise to the matrix $\\mathcal{X}\\Phi$. Then, the corresponding expert function is applied to each slot (i.e., on rows of $\\mathcal{X}$) to obtain the output slots $Y_i = f_{[i/p]}(\\mathcal{X}_i)$. The combination process is then carried out as follows:\n$C_{ij} = \\frac{\\exp((\\mathcal{X}\\Phi)_{ij})}{\\sum_{j'=1}^{i}\\exp((\\mathcal{X}\\Phi)_{ij'})} Y = CY$\nWe refer to C as the combine weights, which is the result of applying the softmax function row-wise to the matrix $\\mathcal{X}\\Phi$. The output tokens Y are computed as a convex combination of all $(n \\cdot p)$ output slots."}, {"title": "3.2 ENHANCED FEED-FORWARD NETWORK", "content": "Since Steel-LLM employs the soft MoE approach, within this framework, the Feed-Forward Network (FFN) effectively represents an expert. In a vanilla Transformer, The FFN comprises two layers of Multi-Layer Perceptrons (MLPs). In the architecture of large language models, a prevalent strategy is to enhance the first layer of the MLP using the SwiGLU(Shazeer (2020)) activation function(Bai et al. (2023a);Touvron et al. (2023a);DeepSeek-AI et al. (2024b)). The SwiGLU activation function enhances the model's non-linear representational capabilities, thereby improving its performance. Additionally, we extended the application of the SwiGLU activation function to the second layer of the MLP within the FFN.\nRegarding other architectural elements, Steel-LLM adopts the modifications made by Qwen(Bai et al. (2023a)) to the transformer block. These modifications are widely used in open-source models such as LLama, Mixtral, and Deepseek.\n\u2022 Positional embedding.We intend to use the Rotary Position Embedding (RoPE)(Su et al. (2023)) for Steel-LLM. RoPE is a relative position encoding technique. Its core idea is to encode absolute position information into a rotation matrix, thereby representing the relative position relationships among tokens. During the training process, we adopt a global training precision of BF16, while for RoPE, we employ local FP32 precision.\n\u2022 Bias. Most layers of Steel-LLM have no bias, except for the QKV layer(Chowdhery et al. (2022)).\n\u2022 Pre-Norm & RMSNorm. Pre-normalization improves training stability compared to post-normalization. We normalize the inputs of self-attention layers and FFN layers with RMSNorm(Zhang & Sennrich (2019))."}, {"title": "4 TRAINING FRAMEWORK", "content": "Steel-LLM is trained using only 8 NVIDIA A100/H800 GPUs, so it is essential to maximize the training speed. Moreover, considering the potential reuse of our training code by others, we have made some usability optimizations.\nOur pretraining framework, which is modified from TinyLlama(Zhang et al. (2024c)), has undergone the following optimizations.\n\u2022 Model loading. In the open-source community, the file format of LLM usually follows the standards of the Transformers(Wolf et al. (2020)) library and it can be easily loaded through the AutoModelForCausalLM.from_pretrained function. To enhance usability and facilitate model loading for different architectures, our framework supports the Transformer library's model loading method. The original framework, by contrast, was designed for training standard Llama architecture.\n\u2022 Training Progress Restoration. In the training process, we are required to save not only the checkpoints of both the model and the optimizer but also the training progress of the data. The original framework's method of saving the training step is the simplest for recording data progress, provided that there is no data change (no addition or deletion). This limitation restricts the flexibility of data organization during the training process. We choose to serialize the entire data management class PackedDatasetIterator using the pickle library and then save it. This includes the file names of all training data, the index of each data piece, etc.\n\u2022 Appending Data During the Training Process. Training LLMs typically spans dozens of days. During this training, appending new data is a common practice. The simplest implementation approach is to train the newly added data at the end. Nevertheless, to safeguard against the significant distributional disparities between new and old data that could impact the model's training performance, we have devised a method for re-shuffling the indices of both the newly appended data and the hitherto untrained data from the old dataset. Additionally, to avert the inadvertent repeated addition of data files to the training process, Steel-LLM has incorporated a function that utilizes a hash value, specifically the MD5(Rivest (1992)) hash algorithm, to detect data content duplication.\nTo improve training speed and conserve GPU memory, during the training process, we utilized techniques such as bfloat16 mixed-precision(Kalamkar et al. (2019)), Fully Sharded Data Parallel (FSDP, Zhao et al. (2023)), and FlashAttention(Dao et al. (2022)). Operator fusion represents an additional training optimization approach. By integrating multiple computational steps, it reduces intermediate activations and memory access, and can be implemented via CUDA or Triton. Specifically, during training, we adopted the CUDA-based version of Rotary Position Embedding (ROPE,Su et al. (2023)) and the Triton-based(Tillet et al. (2019)) cross entropy loss function. We then conducted an ablation study using a micro-batch size of 8 and a 1.8-billion-parameter model on NVIDIA A100 GPU to analyze the impact of these techniques on training efficiency and GPU memory usage, as presented in Table 2. Employing all training acceleration techniques, the training speed can be enhanced by approximately 50%."}, {"title": "5 PRETRAINING", "content": "The pretraining corpus employed in our study predominantly consists of Chinese texts and is entirely derived from open-source datasets. It includes prominent datasets such as SkyPile-150B (Wei et al., 2023), Wanjuan1.0 (He et al., 2023), Wikipedia-cn, as well as diverse chat data from multiple sources and Starcode. Further details of these datasets are provided in Appendix A.\nTo ensure consistency across the data, we initially standardized all datasets into a uniform format. Subsequently, we utilized Alibaba's open-source tool, Data-Juicer, to meticulously filter and transcribe both the text and code data. We utilized a total of 21 text processing operators, as described in Appendix C, and 13 code processing operators, as listed in Appendix D. The final step involved employing the tokenizer from Qwen1.5. This tokenizer was instrumental in converting the entire corpus into token-ids, which were then merged and segmented into manageable chunks. The distribution of the pretraining data across different domains is illustrated in Figure 2. Despite these efforts, our data preprocessing workflow still exhibits certain limitations, particularly the lack of balance in the proportion of texts from different domains within the pre-training corpus.\nOver a span of 30 days, Steel-LLM was trained through 1.07 million steps. The initial 200,000 steps utilized NVIDIA 8 A100-80G GPUs for training, while the remaining steps employed 8 H800-80G GPUs. The loss curve is shown in Appendix E. We set the maximum sequence length to 2048, the batch size to 8, and the number of gradient accumulation steps to 8. Consequently, Steel-LLM was trained on approximately one trillion tokens. We employ the AdamW optimizer (Kingma & Ba (2017)) with hyper-parameters set to \u03b2\u2081= 0.9, \u03b22= 0.95, and weight_decay = 0.05. The maximum learning rate is set to 3 \u00d7 10-4, and the gradient clipping norm is set to 1.0. We employed a cosine-annealing learning rate schedule with 2,000 warmup steps, such that the final learning rate is 0."}, {"title": "6 POST TRAINING", "content": ""}, {"title": "6.1 SUPERVISED FINETUNING", "content": "There has been substantial discussion and some debate among researchers regarding the volume of datasets required for supervised fine-tuning. For instance, Zhou et al. (2023) advocates for selecting a few hundred to a few thousand high-quality data points for fine-tuning, while Ji et al. (2023) suggests that using a significantly larger amount of data can yield better results. Given the capabilities of our base model, we determined that augmenting the fine-tuning phase with additional data was necessary. This strategy not only aids the model in mastering conversational techniques but also enriches it with supplementary knowledge. Consequently, We curated the following fine-tuning datasets: Infinity-Instruct Chinese data (BAAI, 2024) (approximately 700,000 entries, with all English data removed), multiple-choice questions from the Wanjuan-cn dataset (He et al., 2023) which tested in both CoT and non-CoT formats, Ruozhiba data (Ruozhiba, 2024), self-awareness data with template modified from (Team, 2024a), and three English datasets: Code-Feedback (Zheng et al., 2025), WebInstructSub (Yue et al., 2024) and OpenHermes-2.5 (Teknium, 2023). Further details are listed in Appendix B.\nWe fine-tuned the pre-trained Steel-LLM for approximately 4 epochs using the SFT dataset. The global batch size was set to 256, the maximum learning rate was set to 2 \u00d7 10-5, and a cosine-annealing learning rate schedule was employed. The loss curve is shown in Appendix F. Below, we present a series of ablation studies and evaluations to analyze the impact of different fine-tuning strategies on model performance."}, {"title": "6.2 ABLATION STUDIES AND EVALUATION", "content": "To systematically evaluate the effectiveness of our fine-tuning approach, we conducted several experiments with varying data compositions and formats. The results are summarized in Table 3, and the key findings are discussed below."}, {"title": "6.3 LEARNING FROM HUMAN PREFERENCES", "content": "To align Steel-LLM with human preferences, we employ the Direct Preference Optimization (DPO)Rafailov et al. (2024) algorithm and sorted response pairs for model optimization.\nIn the preference dataset, the ratio of Chinese to English stands at 4:1, consistent with that in the pre-training phase. The Chinese dataset is derived from ultrafeedback-chinese\u00b9,"}, {"title": "6.4 DISCUSSION", "content": "Our ablation studies reveal several key insights:\n\u2022 Data Distribution Matters: Fine-tuning with a data distribution that closely matches the pretraining phase leads to better performance on both Chinese and English benchmarks.\n\u2022 Small Models Benefit from Additional Data: Even for small models, fine-tuning with a larger dataset can improve performance, particularly when the pretraining data is limited.\n\u2022 Balanced Multilingual Fine-Tuning: Incorporating a small proportion of English data during fine-tuning can enhance the model's multilingual capabilities without degrading its performance on Chinese tasks.\n\u2022 Exam-Style Data Enhances Performance: Including domain-specific question-answering data, such as multiple-choice questions, can improve the model's reasoning abilities and overall benchmark performance.\nTo contextualize the performance of Steel-LLM, we compare it with several state-of-the-art models on the CEVAL and CMMLU benchmarks, as shown in Table 4. Our best-performing model, Steel-LLM-Chat, achieves competitive results, outperforming models of similar scale such as Tiny-Llama-1.1B and Gemma-2b-it, and approaching the performance of larger models like CT-LLM-SFT-2B. While Steel-LLM-Chat does not yet match the performance of significantly larger models like Qwen1.5-1.8B-Chat or Qwen-7B, its results demonstrate the effectiveness of our resource-efficient approach, particularly given the limited computational resources used for training."}, {"title": "7 CONCLUSION", "content": "This paper introduces Steel-LLM, a fully open-source Chinese-centric language model developed with limited computational resources, achieving competitive performance on benchmarks such as CEVAL (41.90%) and CMMLU (36.08%). By leveraging innovative techniques like Soft Mixture of Experts and enhanced feed-forward networks, along with systematic training optimizations, we demonstrate that high-quality LLMs can be built efficiently. Our work provides complete transparency, releasing the training pipeline, datasets, and intermediate checkpoints, offering practical guidance for small-scale LLM development. All resources are made publicly available to foster collaboration and advance accessible language technologies."}]}