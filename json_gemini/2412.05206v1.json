{"title": "ConQRet: Benchmarking Fine-Grained Evaluation of Retrieval Augmented Argumentation with LLM Judges", "authors": ["Kaustubh D. Dhole", "Kai Shu", "Eugene Agichtein"], "abstract": "Computational argumentation, which involves generating answers or summaries for controversial topics like abortion bans and vaccination, has become increasingly important in today's polarized environment. Sophisticated LLM capabilities offer the potential to provide nuanced, evidence-based answers to such questions through Retrieval-Augmented Argumentation (RAArg), leveraging real-world evidence for high-quality, grounded arguments. However, evaluating RAArg remains challenging, as human evaluation is costly and difficult for complex, lengthy answers on complicated topics. At the same time, re-using existing argumentation datasets is no longer sufficient, as they lack long, complex arguments and realistic evidence from potentially misleading sources, limiting holistic evaluation of retrieval effectiveness and argument quality. To address these gaps, we investigate automated evaluation methods using multiple fine-grained LLM judges, providing better and more interpretable assessments than traditional single-score metrics and even previously reported human crowdsourcing. To validate the proposed techniques, we introduce ConQRet, a new benchmark featuring long and complex human-authored arguments on debated topics, grounded in real-world websites, allowing an exhaustive evaluation across retrieval effectiveness, argument quality, and groundedness. We validate our LLM Judges on a prior dataset and the new ConQRet benchmark. Our proposed LLM Judges and the ConQRet benchmark can enable rapid progress in computational argumentation and can be naturally extended to other complex retrieval-augmented generation tasks.", "sections": [{"title": "1 Introduction", "content": "Computational argumentation (Wachsmuth et al., 2017b; El Baff et al., 2019), or generating arguments for controversial topics\u2014such as debates over banning abortion or the legalization of marijuana\u2014inherently involve sensitive discussions and diverse opinions on the appropriate course of action. Well-informed argumentation requires retrieving publicly available sources of knowledge, such as blogs and news articles, which are often lengthy and noisy, and using them to create well-grounded and persuasive arguments. Large Language Models (LLMs) employed in a Retrieval Augmented Generation approach are a natural choice for this task due to their ability to handle large context lengths and strong performance across various tasks. To emphasize the importance of retrieved evidence for effective argumentation we call this task Retrieval Augmented Argumentation, or RAArg, which we investigate in this paper.\nWhile computational argumentation is a challenging task, even evaluating RAArg systems is a daunting challenge in itself. The generated arguments are often long and complex, making human evaluation costly and time-consuming. Thus, there is an increasing need for automated evaluation methods that can rigorously assess both the influence of retrieval involved in argument generation and the overall quality of the generated arguments, especially when dealing with a diverse set of controversial topics.\nWhile previous research on automated RAG evaluation has largely focused on metrics like context relevance, answer relevance, and answer groundedness, often using LLM-based evaluation methods, also known as LLM-as-a-Judge (TruEra, 2024; Saad-Falcon et al., 2024). In contrast, work in argumentation has concentrated on assessing argument quality without considering the impact of retrieval (El Baff et al., 2019; Wachsmuth et al., 2018a; Alshomary et al., 2020). Besides, these evaluations have notable limitations, particularly their reliance on single-score outputs that lack interpretability and provide minimal insights into specific performance dimensions like argument cogency, reasonableness, and performance at the granularity of individual documents. Additionally, many studies focus on short contexts (Salemi and Zamani, 2024), often chunking documents into passage-level contexts, which can omit essential contextual information. This lack of generalizability is problematic, especially for tasks like argumentation, which require evaluation of long, documents from the web, often unfamiliar to LLMs (Jeremy Howard, 2024).\nSpecifically, a successful automated evaluation of an argumentation system should test the ability to retrieve relevant context and to generate grounded, and high-quality arguments for a diverse set of controversial topics. Additionally, the evaluation of the argument must agree with human preferences and provide cues of irrelevant retrieval and hallucination, ensuring that the system's performance is judged on, both the correctness of its answers and the influence of retrieved evidence.\nTo address these challenges, we propose a systematic evaluation of LLM-based argumentation in a retrieval-augmented setting by validating multiple fine-grained metrics, which we call LLM Judges, as different variants of LLM-based evaluation are needed for each of the metrics. The high level overviews of the task and our LLM Judges are depicted in Figure 1. Figure 1a shows the processing of our RAArg pipeline for a sample controversial topic and Figure 1b shows our proposed fine-grained LLM Judges being employed to evaluate the retrieved evidence documents and the generated argument.\nSpecifically, our contributions are four-fold: (1) first, we demonstrate the feasibility of using model-based evaluation for the task of argumentation; (2) we extend our analysis to the retrieval-augmented setting (RAArg), with evidence retrieved from Web documents; (3) we introduce a novel benchmark\u2014ConQRet\u00b9\u2014Controversial Questions for Argumentation and Retrieval, consisting of human-authored arguments based on a popular debate portal; and (4) we investigate the"}, {"title": "2 Related Work", "content": "We now describe some of the related work in argumentation and LLM evaluation."}, {"title": "2.1 Computational Argumentation", "content": "The field of computational argumentation has focused on multiple problems in modeling the processes of debate and improving argumentation. Most approaches have attempted to model individual sub-tasks typically seen in debates like argument retrieval, argument construction, argument summarization, and counterargument generation. For instance, Hua et al. (2019) employed a BIL-STM encoder-decoder neural network by retrieving evidence from Wikipedia, and showed improved topic relevance. Hua and Wang (2018) combined sophisticated retrieval mechanisms with a two-step generation model, enhancing the quality and appropriateness of generating counter-arguments. They evaluate their approaches over a counter-argument generation dataset based on the ChangeMyView website (Tan et al., 2016b). Building on the next utterance retrieval and generative dialogue systems, Le et al. (2018) employed LSTMs to debate on controversial topics, showing promise on a corpus from the Convinceme website, an informal debating portal. Further, El Baff et al. (2019) select, arrange, and phrase Argument Discourse Units to synthesize arguments through pathos (emotional) and logos (logical) strategies for a dataset of 260 arguments and 10 debate topics (Wachsmuth et al., 2018a). Wachsmuth et al. (2018b) focus on retrieving potent counterarguments without prior topic knowledge, achieving significant accuracy through a model that evaluates argument similarities and differences. Lastly, Alshomary et al. (2020) performed extracted snippets that more accurately represent the core reasoning of arguments collecting 73 arguments over 10 topics."}, {"title": "2.2 LLMs for Argumentation", "content": "In recent years, large language models (LLMs) have made significant progress across a variety of language tasks (Srivastava et al., 2023), demonstrating particular promise in generating discourse-level text, which had previously been more challenging to model. Consequently, LLMs have also been employed to improve debate and argumentation (Khan et al., 2024; Alshomary and Wachsmuth, 2023). Li et al. (2023) create a manually annotated corpus of  triplets and then instruct tune a Llama-7B (Touvron et al., 2023) model to generate counterargument sentences. Verma et al. (2024) evaluate LLMs' argument generation ability to incorporate style and external evidence from an automatically generated retrieval corpus. Mirzakhmedova et al. (2024) perform a fine-grained evaluation of arguments from Wachsmuth et al. (2017a) in a pointwise fashion, by probing 15 argumentation metrics.\nIn a nutshell, most of the approaches that focused on both argument retrieval and argument generation were generally evaluated on informal debate corpora and generally lacked grounding over human-annotated evidence."}, {"title": "2.3 LLM-based Evaluation of RAG Systems", "content": "Prior work on automated evaluations of RAG systems, particularly in multi-hop QA, has primarily focused on metrics like context relevance, answer relevance, answer groundedness (TruEra, 2024). However, these metrics fall short when applied to tasks like argumentation, which require longer contexts and more detailed answers. In argumentation over controversial topics, the effects of irrelevant context and hallucinations are more pronounced, making evaluation crucial. Moreover, it is unclear if current evaluators can handle partial amounts of irrelevant context or provide actionable feedback. Moreover, the traditional reliance on single-score outputs (Es et al., 2024; Saad-Falcon et al., 2024) designed for short answers limits interpretability and usefulness in longer, more complex contexts such as arguments. This highlights the need for specialized evaluation methods that can better address the unique challenges of argumentation."}, {"title": "3 Retrieval Augmented Argumentation", "content": "In this section, we formally describe the task of retrieval augmented argumentation, or RAArg, and describe our reference implementation using state-of-the-art models and best practices."}, {"title": "3.1 Task", "content": "Given a controversial topic q and a stance s, computational argumentation involves generating a stance-conditioned argument As with or without the retrieval of relevant evidence documents Ds."}, {"title": "3.1.1 Evidence Retrieval", "content": "The first subtask, evidence retrieval, involves identifying relevant evidence documents Ds to substantiate or refute the stance s on a given topic q."}, {"title": "3.1.2 Argument Generation", "content": "The second subtask, argument generation, focuses on using the retrieved documents to construct coherent and persuasive arguments, As. This step requires the synthesis of the extracted evidence and ensuring that the arguments maintain logical consistency and are effectively communicated to address the controversial topic q."}, {"title": "3.2 Reference RAArg Implementation", "content": "Our reference implementation follows the retrieve-and-read paradigm (Lewis et al., 2020; Izacard and Grave, 2021), where a retrieval system's top-ranked documents are input to a subsequent generator. Our RAArg system consists of: i) a 2-stage stance-conditioned evidence retriever and ii) a structured argument generator. Both components are evaluated with multiple strategies. Note that RAArg is solely a reference implementation of a RAG system, designed to follow current best practices for retrieval and generation. Our aim is not to present a novel RAG system; rather, to implement a reasonable reference system following current best practices. Future work could explore more sophisticated retrieval and generation approaches.\nFor obtaining evidence relevant to the topic, we first retrieve the top-k relevant evidence documents Ds for a given controversial topic q and stance s \u2208 {pro,con} using the following ranking methodology."}, {"title": "3.2.1 BM25 + LLM Reranking", "content": "Specifically, we first employ a two-stage BM25+Listwise Reranking paradigm popularised in studies like RankGPT (Sun et al., 2023) and RankVicuna (Pradeep et al., 2023). This paradigm has shown SoTA results on a range of ranking tasks (Thakur et al., 2021). We use the name of the controversial topic as the query and index all documents with PyTerrier (Macdonald et al., 2021). For LLM reranking, we use the Pyterrier_GenRank (Dhole, 2024) plugin with GPT-40-mini (OpenAI, 2024) as our reranker. We use two types of instructions-generic (Sun et al., 2023) and stance-specific-along with a query and a list of documents, to generate a ranked list of document IDs for reranking. Other evidence retrieval and reranking methods could be investigated in the future, but as we will show this version performs adequately to enable generation of human-quality arguments."}, {"title": "3.2.2 Few-Shot Argument Generation", "content": "We employ GPT-40-mini in a few-shot fashion, using the prompt reported in Appendix Table 10. For in-context examples, we select three controversial topics from the training set, along with their documents and corresponding expert-written arguments. Our choice of GPT-40-mini is based on its SoTA performance across various tasks, its ability to handle contexts up to 128K tokens, and its affordable pricing. We prompt the model to generate a structured argument that consists of a set of conclusions, each followed by premises that justify the conclusion.\nDue to limitations of the (large) context size of popular LLMs, it is advantageous to prioritize documents that support the expected stance, as these are likely to produce a favorable argument. While RankGPT's instructions are useful, they remain generic and do not differentiate between documents supporting or opposing the stance. In contrast, we introduce stance-specific instructions during reranking, as illustrated in Appendix Table 9, enabling the generation of arguments tailored to each specific stance.\nFurthermore, web documents could range in length from a few hundred tokens to as many as 100K tokens. Therefore, a context of 10 documents concatenated along with 3-few shot examples could exceed the (current) 128K token limit. In such cases, instead of trimming the documents only from the end, which could potentially exclude entire documents, we proportionally trim an equal portion from each document to minimize the possibility of removing a document in its entirety."}, {"title": "4 The ConQRet Benchmark", "content": "In order to enable research on complex, evidence-based argumentation, we develop and present a novel benchmark, ConQRet. The benchmark is constructed from a popular website of expert-generated arguments, which we further augment by retrieving the actual sources selected by the experts to be used as the ground truth evidence for retrieval augmented argument argumentation. The details of the dataset construction follow."}, {"title": "4.1 Web Scraping of ProCon.Org Pages", "content": "We use ProCon.org, a debate portal featuring controversial questions with human-written pro and con arguments grounded in sources like journals and news outlets. Each argument includes paragraphs linked to external websites. We scrape these to gather grounded arguments on various topics, with details provided in appendix A."}, {"title": "4.2 Retrieving Evidence Text from the Web", "content": "Of the 7,177 sources cited on ProCon webpages, 6,514 (92%) were successfully identified and extracted using Google or Bing. The remaining sources were either behind firewalls or required specialized parsing techniques, detailed in appendix B. Most source documents fall between a few hundred and 10,000 tokens, while the remainder are in the long tail, as illustrated by the document length distribution in Figure 2."}, {"title": "4.3 Construction of Query-Relevance Pairs", "content": "To enable evidence retrieval evaluation, we construct query-document relevance pairs as follows:\n1. Relevant Documents: We use the scraped texts of each webpage mentioned in the sources section of each topic to create pairs of queries and relevant documents.\n2. Irrelevant Documents: To create irrelevant (query, document) pairs, we randomly pick an equal number of documents from other queries.\nThe pairs are split topicwise into two\u201470% for train and 30% for evaluation. Both sets do not have any topic or document overlaps."}, {"title": "5 Experiments and Analysis", "content": "In this section, we describe the various LLM Judges used in our analysis. We first evaluate our LLM Judges in the non-RAG setting and then extend the evaluation to address the RAG counterpart."}, {"title": "5.1 Fine-Grained Argument Quality Evaluation", "content": "We now describe how we validate our LLM Judges for argument quality against fine-grained human annotations.\nWe assess argument quality through the 15 fine-grained dimensions proposed by Wachsmuth et al. (2017b). Specifically, we introduce a Listwise evaluation prompt (Appendix Table 11), which presents the detailed definitions of these dimensions within a single inference call. These definitions, originally provided to argument expert annotators by Wachsmuth et al. (2017b), are included along with the argument to be evaluated.\nThe Listwise approach requires a single inference call and a reduced number of tokens compared to a pointwise approach, which would require 15 separate prompts-one for each dimension-resulting in a substantially higher token count. Due to the known lack of robustness of LLMs (Liu et al., 2024; Dhole et al., 2023), we also explore the effect of varying the temperature and altering the order of the 15 dimensions in the prompt. Additionally, we employ a self-consistency (Wang et al., 2023) style listwise prompt (Listwise+SC) and compute the mean across three different orders. Our evaluations are compared against pointwise methods outlined in Mirzakhmedova et al. (2024) and human crowd ratings from Wachsmuth et al. (2017b).\nFor both datasets, expert annotator ratings are used, and the agreement between the model's predictions and the expert ratings is quantified using Krippendorff's \u03b1 (Krippendorff, 2011)."}, {"title": "5.2 RAG Evaluation", "content": "To comprehensively evaluate an argumentation system, it is essential to assess the quality of the final argument as well as the effect of retrieval. Particularly, we evaluate the following two metrics:\n1. Context Relevance\u2014How relevant are the evidence documents to the controversial topic?\n2. Argument Groundedness\u2014How grounded are the generated arguments in the retrieved evidence?"}, {"title": "5.2.1 Context Relevance: Traditional Eval", "content": "To measure context relevance, we first use the human-annotated evidence in the form of qrels or (query, document) pairs to compute traditional IR metrics, viz., nDCG@k (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) and P@k for ConQRet. The results for the different retrievers are shown in Table 4.\nIn order to validate our LLM Judges, we resort to using these QRELS, similar to many automatic relevance labeling studies (Rahmani et al., 2024b; MacAvaney and Soldaini, 2023; Rahmani et al., 2024a).\nWe now describe our LLM Judges in the following subsection."}, {"title": "5.2.2 Context Relevance: LLM Judges", "content": "We introduce multiple LLM Judges for evaluating context relevance to simulate the more realistic setting when relevance annotations are unavailable. We particularly introduce different prompt variations, which take the topic q, documents D, and argument A as inputs and generate context relevance scores along with argument groundedness and argument quality.\nThe different prompts are described below:\n\u2022 Listwise+RAG: Here, we instruct the model to generate other retrieval-based metrics, viz., argument groundedness and context relevance along with the argument quality. The context relevance is computed for a concatenation of all documents.\n\u2022 Listwise+RAG Fine-Grained: This is the fine-grained counterpart of the Listwise+RAG metric where the context relevance is computed at the granularity of each document. The corresponding prompt is shown in Figure 11.\n\u2022 Others: In addition to the above, we also explore other formats of prompts to compute context relevance. They are described in Table 3 and appendix D.\nWe use GPT-40 (OpenAI, 2024) for performing the evaluations due to its SOTA performance for other tasks for longer contexts. We also analyze other models viz., phi-3-small/medium-128k-instruct-4 (7B/14B params) (Abdin et al., 2024), which can ingest large contexts and find that they are unable to understand the intent of evaluation often generating related but undesirable content missing values or unformatted JSON. We find that GPT-40-mini (OpenAI, 2024), Llama-3.1-70B (Dubey et al., 2024) and Gemini-1.5-Flash (Reid et al., 2024) are among the ones which show the necessary sensitivity to irrelevant context. Additional details on the varying levels of irrelevant context across different LLM Judges and models are provided in the Appendix Table 17."}, {"title": "6 Results and Analysis", "content": "We now present the results of each of the above evaluations on both the datasets."}, {"title": "6.1 Fine-Grained Argument Quality Evaluation", "content": "We present the results of the agreement between human experts and various LLM judges in Table 5. We find that when we employ a listwise approach with GPT3.5, we obtain better overall quality than a previous pointwise implementation (Mirzakhmedova et al., 2024), and employing a listwise approach with self-consistency further improves overall quality as well as most of the individual dimensions. We obtain further improvements when we employ GPT-40 in the listwise setting.\nSpecifically, our listwise LLM-judge exhibits the highest agreement with expert annotators for both overall argument quality and across various argumentation dimensions, even compared to human crowd workers. Furthermore, the overall agreement is aligned with the average across the different dimensions, indicating consistency and alignment of the overall quality annotation with the intrinsic dimensions of argument quality. Our listwise LLM Judge is able to achieve overall quality agreement on par with humans and better than the pointwise setting employed with Palm2 (Anil et al., 2023) and GPT3.5 (Mirzakhmedova et al., 2024). Self-consistency (SC) further improves the overall quality as well as the average quality. Our Listwise + SC is also better than Mirzakhmedova et al. (2024)'s \"novice\" prompt variation for 3 out of 4 dimensions.\nFurther, we investigate if there is variation in argument quality prediction when we change the order of the 15 dimensions. We find that there is substantial variability when the order of the dimensions is changed. We show in Appendix Table 13 (a)-(c) how the results of three different runs change. We hence use a combined mean rating for each dimension, and see that there is high agreement in both the overall quality and the average quality, only at the expense of 2 additional inference calls. We also find prompting in a zero-shot fashion is more effective as compared to few-shot (Detailed results are shown in Appendix Table 13 and Table 14)."}, {"title": "6.2 How close are context relevance predictions to Human Annotated Qrels?", "content": "To validate how well our best LLM-Judge viz. Listwise + RAG Fine-Grained predict context relevance, we compare its precision at different levels of irrelevant content with the true precision, obtained through human annotated qrels. We evaluate how sensitive are context relevance predictions across different levels of irrelevant context. We specifically evaluate the context relevance by replacing some percentage of the retrieved documents with random irrelevant documents. We measure for no (0%), low (10%, 20%), and high (50%, 70%) amounts of irrelevant content.\nFirst, we compare it with the most popular approach of computing context relevance directly i.e. the direct metric (TruEra, 2024). As we see in Table 7, the Direct metric does not strictly decrease with increasing levels of irrelevant context lacking the necessary context sensitivity. However, our Listwise+RAG Fine-grained approach decreases gradually with increasing irrelevant context. Besides, our fine-grained approach provides precision estimates close to the True Precision. Also note that the Direct metric does not provide precision estimates at the granularity of individual documents, unlike the Listwise+RAG Fine-grained approach.\nWe also find that most fine-grained metrics, as shown in Appendix Tables 16 and 17 show better degradation with increasing irrelevant context than single-score metrics. Appendix Figure 13 and Table 16 show the results when evaluated with GPT-40. We find that while most metrics decrease with increasing irrelevant context, the decrease is not fully monotonic. The RAG-Direct and RAG-Rubric metrics display a near-perfect monotonic decrease (Pearson \u03c1 \u2248 -1), while the direct metric assigns the same context relevance score to 10% and 50% of irrelevant content. This trend is consistent across arguments of both stances. This demonstrates that single score metrics are less useful in practice vis-\u00e0-vis fine-grained metrics, like Listwise+RAG Fine-grained which are consistently sensitive to irrelevant context as well as interpretable."}, {"title": "6.3 Do argument groundedness predictions reflect the number of hallucinations appearing in the argument?", "content": "In the context of controversial topics, hallucination becomes increasingly significant. When assessing argumentation, it is essential to evaluate to what extent LLM judges are reliable for identifying hallucinated content in their argument.\nTo simulate varying levels of hallucinations, we start with expert-written arguments paired with their corresponding documents from ConQRet, then select a random subset of sentences from the original argument for modification. These selected sentences are replaced with hallucinated sentences, resulting in a modified argument as shown in Figure 3. We anticipate that an effective metric will show a gradual reduction in its score as the proportion of modified sentences increases. To generate the hallucinations, we prompt GPT-40 with the original argument and the original documents, and instruct the model to replace already grounded sentences by generating sentences which are contradictory to the original sentence as well as the content appearing in the document. We generate such modified arguments by modifying 5, and 20 sentences. The complete prompt is shown in Appendix Figure 12. We find that all metrics are effective in displaying explainable differences across varying levels of groundedness. The results of the same are shown in Appendix Table 18."}, {"title": "6.4 Metric Consistency Analysis", "content": "As our Listwise+RAG Fine-Grained LLM Judge generates multiple metrics, it is also imperative to understand how they affect each other. For instance, does irrelevant context affect the evaluation of groundedness and argument quality?\nTo measure the same, we increase the levels of both\u2014irrelevant context (0%, 10%, 50%, 75%) and hallucinated sentences (0, 5, 20)\u00b2\u2013systematically and evaluate the effects of our metrics using gemini-1.5-flash and GPT-40 as our evaluators. We present the results in Figure 4.\nWe find that across these coarse-grained levels of hallucinated infusions, context relevance scores and their monotonic degradations remain unaffected (Figure 4 (a) and (b)). Moreover, at varying coarse-grained levels of irrelevant context, the groundedness scores and argument quality decrease monotonically with increasing hallucinated content. When we further add more fine-grained levels of hallucinations namely (2, 10), we find that context relevance scores are not strongly informative. The scores decrease with increased levels of injected irrelevant context. However, they are unable to distinguish fine-grained levels of irrelevant context. GPT-40 seems more reliable than Gemini for groundedness estimation.\nArgument quality estimations follow a decreasing trend at some levels of irrelevant context. GPT-40 argument evaluations are more resistant to context than Gemini. For a low number of hallucinations, argument quality evaluations are more reliable.\nIn summary, through our metrics, we observe that LLMs can be reliably used for estimating context relevance. However, prompt-based LLM Judges are not able to reliably evaluate argument groundedness and overall quality. For all metrics, LLM judges are not able to distinguish reliably between finer levels of non-relevant evidence or hallucinated arguments.\nThese results have important implications for argumentation as well as language modeling methods relying on LLMs for eliciting pseudo-human preferences, or employing them as teachers for subsequent distillation, as our methods reveal several shortcomings."}, {"title": "7 Conclusions and Future Work", "content": "Human annotation of complex arguments and corresponding evidence requires extensive knowledge of the topics, and copious amounts of reading of evidence for each argument, making fine-grained annotations infeasible at scale. This paper introduces an automatic evaluation procedure and provides its validation without the need for large-scale scale human-annotation. We propose the first study on using model-based evaluation for retrieval-augmented argumentation systems. This problem is more important as long documents and complex arguments are involved over controversial topics. We demonstrate how argumentation can be effectively evaluated over two datasets, showing that our proposed LLM judges are more efficient, interpretable, and align strongly with human evaluations. Additionally, we introduce a novel benchmark ConQRet designed to evaluate both retrievers as well as RAG systems in scenarios that closely resemble real-world contexts, characterized by long documents. Our contribution also includes the proposal of new and modified metrics through LLM judges for measuring context relevance and answer groundedness, and we describe methods to validate these metrics without requiring large-scale annotation. We find that larger models exhibit necessary degradation on fine-grained metrics, in contrast to popularly employed metrics that provide single, often uninterpretable, scores.\nOur work has several crucial applications. Our LLM judges, in addition to demonstrating high agreement with human evaluations, also showed the capability to compute multiple metrics in a single inference call. This encourages research on a holistic evaluation of retrieval-augmented systems, allowing metrics to complement and inform each other during the evaluation process. Furthermore, these fine-grained metrics are promising, as they offer multiple cues to improve the training and alignment of LLMs, ultimately leading to more accurate and reliable systems."}, {"title": "8 Limitations", "content": "Our study does have some limitations, primarily related to the choice of models used for evaluation, namely GPT-40, GPT-40-mini, LLaMA-3.2-70B, and Gemini-1.5-Flash. While these models are SoTA in most aspects, additional models might provide more comprehensive performance insights. However, the selected models are highly representative of current capabilities in language models, and the findings hence offer valuable perspectives. Challenges such as the quadratic growth in memory requirements with increasing context lengths and occasional difficulties in handling specific instructions with large contexts in LLaMA-3.1 did arise, but these issues are inherent to many models and do not detract from the overall results."}, {"title": "9 Ethical Considerations", "content": "Controversial topics often involve complex disagreements on political, financial, or life-threatening issues and hence any computational argumentation system, built with LLMs should be treated as a part of a broader socio-technical context, where LLMs act as subsystems (Dhole, 2023). Moreover, these systems, functioning as black boxes, camouflage the relationship between their outputs and the extensive pre-training data they are based on, making it hard to deduce the rationale for an argument or an evaluation score with high certainty.\nOn the other hand, LLMs have demonstrated biases across political spectrums and exhibit disparities in fairness across different languages and cultures, rendering them unsuitable for making impartial decisions on sensitive matters. Furthermore, when deployed in real-world applications, these systems are vulnerable to adversarial exploitation. For instance, adversaries may inject irrelevant content into retrieval processes or prompt the models to generate biased or misleading arguments that unfairly favor specific positions, often without proper attribution.\nOur work highlights the susceptibility of LLM-based evaluations, emphasizing both their robustness and sensitivity. This underscores the critical need for robust evaluation frameworks that can assess argument quality while detecting shifts in context to ensure fairness. Additionally, LLM judges may themselves exhibit biases towards particular arguments. For this reason, we advocate for fine-grained, interpretative evaluations, which we believe will prove relatively more effective in practice than traditional single-score metrics. Our research is intended to encourage effort in that direction."}]}