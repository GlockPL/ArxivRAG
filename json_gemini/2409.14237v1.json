{"title": "An Instance-based Plus Ensemble Learning Method for\nClassification of Scientific Papers", "authors": ["Fang Zhang", "Shengli Wu"], "abstract": "The exponential growth of scientific publications in recent\nyears has posed a significant challenge in effective and efficient catego-\nrization. This paper introduces a novel approach that combines instance-\nbased learning and ensemble learning techniques for classifying scien-\ntific papers into relevant research fields. Working with a classification\nsystem with a group of research fields, first a number of typical seed pa-\npers are allocated to each of the fields manually. Then for each paper that\nneeds to be classified, we compare it with all the seed papers in every\nfield. Contents and citations are considered separately. An ensemble-\nbased method is then employed to make the final decision. Experiment-\ning with the datasets from DBLP, our experimental results demonstrate\nthat the proposed classification method is effective and efficient in cate-\ngorizing papers into various research areas. We also find that both con-\ntent and citation features are useful for the classification of scientific pa-\npers.", "sections": [{"title": "Introduction", "content": "Classification of scientific papers is an important task that can be useful in various\nways. It can be used for effective and efficient search of scientific papers. It can also be\nused for other tasks, for example to analyze the tendency of a research topic or a re-\nsearch field. It can be beneficial for researchers to identify relevant or high-impact pa-\npers and research topics. It can be used for research institutions and government agen-\ncies to understand the trends in each research field to help set research and funding\npolicies, among other applications.\nUp to now, there are two types of approaches for this research problem. One is the\nmanual plus machine approach (Kandimalla et al., 2020; Mendoza et al., 2022; Zhang\net al., 2022), and the other is a completely automatic machine-based approach (Shen et\nal., 2018; Toney-Wails & Dunham, 2022; Zhang et al., 2022, Liu et al., 2022). For the\nmanual plus machine approach, we first define a classification system, which is a hier-\narchical structure including all the categories and sub-categories into which all scien-\ntific papers should fit. A classification system also provides specifications of those"}, {"title": "Related Work", "content": "In this section we review some recent work on the classification of scientific papers.\nWe also briefly review some major methods in ensemble learning."}, {"title": "Classification of scientific papers", "content": "The classification of scientific papers has become an important issue when organ-\nizing and managing an increasing number of publications through computerized solu-\ntions. In previous research, typically, meta-data such as title, abstract, keywords, and\ncitations of papers were used for this task, while full text was not considered due to its\nunavailability in most situations.\nVarious machine learning methods, such as K-Nearest Neighbors (Waltman and\nVan Eck, 2012; Lukasik et al., 2013), K-means (Kim and Gil, 2019), and Na\u00efve Bayes\n(Eykens et al., 2021), have been applied. Recently, deep neural network models, such\nas Convolutional Neural Networks (Rivest et al., 2021; Daradkeh et al., 2022), Recur-\nrent Neural Networks (Semberecki and Maciejewski, 2017; Hoppe et al., 2021), and\npre-trained language models (Kandimalla et al., 2020; Hande et al., 2021), have also\nbeen utilized.\nOne key issue is the classification system to be used. There are many different clas-\nsification systems. Both Thomson Reuters' Web of Science database (WoS) and Else-\nvier's Scopus database have their own general classification systems, covering many\nsubjects/research areas. Some systems focus on one particular subject, such as the Med-\nical Subject Headings (MeSH), the Physics and Astronomy Classification Scheme\n(PACS), the Chemical Abstracts Sections, the Journal of Economic Literature (JEL),\nand the ACM Computing Classification System.\nBased on the WoS classification system, Kandimalla et al. (2020) applied a Deep\nAttentive Neural Network (DANN) to a collection of papers from the WoS database"}, {"title": "Ensemble learning", "content": "Ensemble learning has been investigated by many researchers over the last thirty\nyears for classification tasks (Zhou, 2012). Mainly ensemble learning approaches can\nbe categorized into three typical types: stacking (Wolpert, 1992), bagging (Breiman,\n1996), and boosting (Opelt & Pinz, 2006). Staking is focused on combining different\ntypes of base learners to achieve better results. A few different types of combination\nschemes have been proposed: Stacking (Ting & Witten, 1999), StackingC (Seewald,\n2002), and Euclidean Distance-based method (Wu et al., 2023). Bagging is focused on\ngenerating a group of diversified base learners through instance or feature-level manip-\nulation of a dataset (Breiman, 2001; Xu &Yu, 2023). Boosting is also focused on gen-\nerating a group of useful base learners, however, in a different way from Bagging. It\ngenerates base classifiers one after another and more and more focused on those in-\nstances that have been classified incorrectly. AdaBoost (Freund & Schapire, 1995) and\nXGBoost (Chen & Guestrin, 2016) are two representatives. In this piece of work, we\nused the Euclidean Distance-based method. It fits our purpose well.\nSimilarly, ensemble learning is referred to as data fusion in information retrieval\n[Huang & Xu, 2022]. The primary principle remains the same for both of them. Alt-\nhough it seems that classification and regression are the main problems in machine\nlearning, while ranking is the main problem in information retrieval, they can be trans-\nformed from one type to another."}, {"title": "Methodology", "content": ""}, {"title": "Computing classification system", "content": "To carry out the classification task of academic papers, a suitable classification sys-\ntem is required. There are many classification systems available for natural science,\nsocial science, humanities, or specific branches of science or technology. Since one of\nthe datasets used in this study is DBLP, which includes over four million papers on\ncomputer science so far, we will focus our discussion on classification systems and\nmethods for computer science.\nIn computer science, there are quite a few classification systems available. For ex-\nample, both the Association for Computing Machinery (ACM) and the China Computer\nFederation (CCF) define their own classification systems. However, neither are suitable"}, {"title": "Paper classification", "content": "For this research, we need a classification algorithm that can perform the classifi-\ncation task for all the papers in the DBLP dataset effectively and efficiently.\nAlthough many classification methods have been proposed, we could not find a\nmethod that suits our case well. Therefore, we developed our own approach. Using the\nclassification system of CSRankings, we assume that all the papers published in those\nidentified venues belong to that given research area, referred to as seed papers. For all\nthe non-seed papers, we need to decide the areas to which they belong. This is done by\nconsidering three aspects together: content, references, and citations. Let us look at the\nfirst aspect first.\nThe collection of all the seed papers, denoted as C, was indexed using the Search\nengine Lucene\u00b9 with the BM25 model. Both titles and abstracts were used in the index-\ning process. Each research area ak is presented by all its seed papers C(ak). For a given\nnon-seed paper p, we use its title and abstract as a query to search for similar papers in\nC. Then each seed paper s will obtain a score (similarity between s and p)\n$$sim(p, s) = \\sum idf (qj) \\times \\frac{f(t_{j}, s) \\times (k_{1} + 1)}{f(t,s) + c_{1} x (1-b + b\\frac{|S|}{avg\\_d\\_l})}$$"}, {"content": "In which b and k\u2081 are two parameters (set to 0.75 and 1.2, respectively, as default set-\nting values of Lucene in the experiments), S is the set of all the terms in s, P is the set\nof all the terms in p, avg_d_l is the average length of all the documents in C, f(tj, s)\nis the term frequency of tj in s, idf (t;) is the inverse document frequency of t; in"}, {"title": null, "content": "collection C with all the seed papers. idf (t;) is defined as\n$$idf (t_{j}) = log(1 + \\frac{|C| \u2212 f(t_{j}) + 0.5}{f(t_{j}) + 0.5})$$"}, {"content": "in which |C| is the number of papers in C, and f(tj) is the number of papers in C sat-\nisfying the condition that t; appears in them. For a paper p and a research area ak, we\ncan calculate the average similarity score between p and all the seed papers in C(ak) as\n$$sim(p, a_{k}) = \\frac{1}{|C(a_{k})|} \\sum_{s\\in C(a_{k})} sim(p, s)$$"}, {"title": null, "content": "where C(ak) is the collection of seed papers in ak.\nWe also consider citations between p and any of the papers in C. Cita-\ntions in two different directions are considered separately: citingNum(p, ak)\ndenotes the number of papers in C(ak) that p cites, and citedNum(p, ak) de-\nnotes the number of papers in C(ak) that cites p. Now we want to combine the\nthree features. Normalization is required. For example, sim(p, ak) can be nor-\nmalized by\n$$sim' (p, a_{k}) = \\frac{sim(p, a_{k})}{\\sum_{a\\in area}sim(p, a_{i})}$$"}, {"title": null, "content": "in which RArea is the set of 26 research areas. citingNum' (p,ak) and\ncitedNum' (p, ak) can be normalized similarly. Then we let\n$$score(p, a_{k}) = \\beta_{1} \\times sim'(p, a_{k}) + \\beta_{2} \\times citingNum' (p, a_{k})\n+ \\beta_{3} \\times citedNum' (p, a_{k})$$"}, {"title": null, "content": "where C(ak) is the collection of seed papers in ak.\nfor any ak \u2208 RArea, in which B1, B2, and \u1e9e3 are three parameters. When applying\nEquation 5 to p and all 26 research areas, we may obtain corresponding scores for each\narea. p can be put to research area ak if score(p, ak) is the biggest among all 26 scores\nfor all research areas. The values of B1, B2, and \u1e9e3are decided by Euclidean Distance\nwith multiple linear regression with a training data set (Wu et al., 2023). Compared\nwith other similar methods such as Stacking with MLS and StackingC, this method can\nachieve comparable performance but much more efficient than the others. It should be\nvery suitable for large-scale datasets.\nIn this study, we assume that each paper just belongs to one of the research areas.\nIf required, this method can be modified to support multi-label classification, then a\npaper may belong to more than one research area at the same time. We may set a rea-\nsonable threshold t, and for any testing paper p and research area a, if score (p, \u0430\u043a) >\n\u03c4, then paper p belongs to research area a. However, this is beyond the scope of this\nresearch, and we leave it for further study.\nIn summary, the proposed classification algorithm IBL (Instance-Based Learning) is\nsketched as follows:"}, {"title": null, "content": "ALGORITHM 1: IBL\nInput: a set of research areas ak (1 \u2264 k \u2264 n), each includes a group of papers that\nbelong to that area, a paper p to be classified, citation data of p, and\nparameter \u03b2\u2081, B2, B3, for the regression model\nOutput: the research area that p belongs to\n1 For every research areas ak (1 \u2264 k \u2264 n)\n2 calculate the average similarity sim' (p, ak) between p and every paper in ak\nby Equation 4\n3 calculate normalized citation count that p cites citingNum' (p, \u0430\u043a)\n4 calculate normalized citation count that p is been cited citedNum' (p, ak)\n5 calculate score (p, ak) by Equation 5\n6 End For\n7 compare all the scores that p obtains, return the research area a\u2081 that p obtains\nthe maximal score"}, {"title": "Experimental Settings and Results", "content": ""}, {"title": "Dataset and measures", "content": "We downloaded a DBLP dataset (Tang et al., 2008)2. It contains 4,107,340 papers\nin computer science and 36,624,464 citations from 1961 to 2019. For every paper, the\ndataset provides its metadata such as title, abstract, references, authors and their affili-\nations, publication year, the venue in which the paper was published, and citations since\npublication. Some subsets of it were used in this study.\nWe used two subsets of the dataset. The first one (S1) is all the papers published in\nthose 72 recommended venues in CSRankings between 1965 and 2019. There are\n191,727 papers. S\u2081 is used as seed papers for all 26 research areas. The second subset\n(S2) includes 1300 papers, 50 for each research area. Those papers were randomly se-\nlected from a group of 54 conferences and journals and judged manually. C2 is used for\nthe testing of the proposed classification method.\nBoth accuracy and F-measure are used for the evaluation of the classification re-\nsults."}, {"title": "Classification results", "content": "In the CSRankings classification system, there are a total of 26 special research\nareas. A few top venues are recommended for each of them. We assume that all the\npapers published in those recommended conferences belong to the corresponding re-\nsearch area solely. For example, three conferences CVPR, ECCV, and ICCV are rec-\nommended for Computer Vision. We assume that all the papers published in these three\nconferences belong to the Computer Vision research area but no others.\nTo evaluate the proposed method, we used a set of 1300 non-seed papers (S2). It"}, {"title": null, "content": "included 50 papers for each research area. All of them were labelled manually. In Equa-\ntion 5, three parameters need to be trained. Therefore, we divided those 1300 papers\ninto two equal partitions of 650, and each included the same number of papers in every\nresearch area. then a two-fold cross-validation approach was performed. Table 1 shows\nthe average performance.\nWe can see that the proposed method with all three features, content similarity\n(sim'), citation to other papers (citingNum'), and citation by others (citedNum'), are\nuseful for the classification task. Roughly citation in both directions (citingNum' +\ncitedNum') and content similarity (sim') have the same ability. Considering three fea-\ntures together, we can obtain an accuracy, or an F-measure, of approaching 0.8. We are\nsatisfied with this solution. On the one hand, its classification performance is good\ncompared with other methods in the same category, e.g., (Kandimalla et al., 2020, Am-\nbalavanan & Devarakonda, 2020). In Kandimalla et al. (2020), F-scores across 81 sub-\nject categories are between 0.5-0.8 (See Figure 5 in that paper). In Ambalavanan &\nDevarakonda (2020), the four models ITL, Cascade Learner, Ensemble-Boolean, and\nEnsemble-FFN obtain an F-score of 0.553, 0.753, 0.628, and 0.477, respectively, on the\nMarshall dataset they experimented with (see Table 4 in their paper). Although those\nresults may not be comparable since the datasets used are different, it is an indicator\nthat our method is very good. Besides, our method can be implemented very efficiently.\nWhen the seed papers are indexed, we can deal with a large collection of papers very\nquickly with very little resource. The method is very scalable."}, {"title": "Conclusions", "content": "In this paper, we have presented a novel instance-based and ensemble learning\nmethod for the classification of scientific papers. Both content and citations in both\ndirections are considered in the classification model at the same time. Experimented\nwith the datasets from DBLP, we find that the proposed method is effective and effi-\ncient. All three factors impact classification performance significantly.\nThis piece of work can be further investigated in a few different directions. Firstly,\nsome papers may be of the nature of cross-disciplinary. It would be better to allocate\nsuch papers to more than one category. Secondly, some other information such as au-\nthors and venues are also useful information, which can be employed to improve clas-\nsification performance. Some graph-based machine learning methods can be good op-\ntions for dealing with such situations."}]}