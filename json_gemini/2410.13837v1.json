{"title": "ORSO: ACCELERATING REWARD DESIGN VIA ONLINE REWARD SELECTION AND POLICY OPTIMIZATION", "authors": ["Chen Bo Calvin Zhang", "Zhang-Wei Hong", "Aldo Pacchiano", "Pulkit Agrawal"], "abstract": "Reward shaping is a critical component in reinforcement learning (RL), particularly for complex tasks where sparse rewards can hinder learning. While shaping rewards have been introduced to provide additional guidance, selecting effective shaping functions remains challenging and computationally expensive. This paper introduces Online Reward Selection and Policy Optimization (ORSO), a novel approach that frames shaping reward selection as an online model selection problem. ORSO employs principled exploration strategies to automatically identify promising shaping reward functions without human intervention, balancing exploration and exploitation with provable regret guarantees. We demonstrate ORSO's effectiveness across various continuous control tasks using the Isaac Gym simulator. Compared to traditional methods that fully evaluate each shaping reward function, ORSO significantly improves sample efficiency, reduces computational time, and consistently identifies high-quality reward functions that produce policies comparable to those generated by domain experts through hand-engineered rewards.", "sections": [{"title": "INTRODUCTION", "content": "Reward functions are crucial in reinforcement learning (RL; Sutton & Barto (2018)) as they guide the learning of successful policies. In many real-world scenarios, the ultimate objective involves maximizing long-term rewards that are not immediately available, making optimization challenging. To address this, practitioners often introduce shaping rewards (Margolis & Agrawal, 2022; Liu et al., 2024; Mahmood et al., 2018; Ng et al., 1999) to provide additional guidance during training. Instead of directly maximizing the task rewards (R), it is therefore common for the RL algorithm to maximize an easier-to-optimize shaped reward function F in the hope of obtaining high performance as measured by task rewards, R. While shaping rewards contain helpful hints, maximizing them does not necessarily solve the task. For instance, an agent tasked with finding an exit (i.e.,, longer-term reward in the future) may be provided with shaping rewards to avoid obstacles. However, the task success ultimately depends on reaching the exit, not just avoiding obstacles. If poorly designed, the shaped rewards F can mislead the RL algorithm, causing the agent to focus on maximizing F while neglecting R (Chen et al., 2022; Agrawal, 2021), leading to training failure or suboptimal performance.\nDesigning effective shaping reward functions F that improve RL algorithm performance is challenging and time-consuming. It requires multiple iterations of training agents with different shaping rewards, evaluating their performance on the task reward R, and refining F accordingly. This process is inefficient due to the lengthy training runs and because the performance measured early in training may be misleading, making it challenging to quickly iterate over different shaping rewards.\nTo address this challenge, we propose treating the design of the shaping reward function as an exploration-exploitation problem and to solve it using provably efficient online decision-making algorithms similar to those in multi-armed bandits (Auer et al., 2002; Auer, 2002) and model selection"}, {"title": "PRELIMINARIES", "content": "Reinforcement Learning (RL) In RL, the objective is to learn a policy for an agent (e.g., a robot) that maximizes the expected cumulative reward during the interaction with the environment. The interaction between the agent and the environment is formulated as a Markov decision process (MDP) (Puterman, 2014), M = (S, A, P, r, \u03b3, \u03c1\u03bf), where the S and A denote state and action spaces, respectively, P : S \u00d7 A \u2192 \u2206s' is the state transition dynamics, r : S \u00d7 A \u2192 \u2206\u20a8 denotes the reward function, \u03b3\u2208 [0,1) is the discount factor, and po \u2208 As is the initial state distribution. At each timestep t \u2208 N of interaction, the agent selects an action at ~ \u03c0(\u00b7 | st) based on its policy \u03c0, receives a (possibly) stochastic reward rt ~ r(st,at), and transitions to the next state St+1 ~ P(\u00b7 | St, at) according to the transition dynamics. Here, r is the task reward, also referred to as extrinsic reward (Chen et al., 2022). RL algorithms aim to find a policy \u03c0* that maximizes the discounted cumulative reward, i.e.,"}, {"title": "METHOD: REWARD DESIGN AS SEQUENTIAL DECISION MAKING", "content": "As previously stated, the reward function r encodes the task objective but can be sparse, making it difficult to directly optimize using RL methods. We formalize the reward design problem as follows.\nDefinition 3.1 (Reward Design). Let A be a reinforcement learning algorithm that takes an MDP M = (S, A, P, r, \u03b3, po), a reward function f, and a number of iterations N as input and returns a policy \u03c0f = Af(M, N) that approximately maximizes reward f in M after N iterations.\nGiven M and A, the reward design problem aims to find a reward function f : S \u00d7 A \u2192 AR, with f \u2208 R, the space of reward functions, such that the policy \u03c0f = Af(M, N) achieves an expected return under the task reward f, such that I (\u03c0f) \u2248 maxr'\u2208R I(\u03c0\"') = I (\u03c0*).\nWhile this could be achieved by running the algorithm A on every possible reward function r' \u2208 R, this is computationally prohibitive. The reward space R can be extremely large, and attempting to optimize over all possible rewards is impractical, especially when the available interaction budget is constrained.\nTo make the problem tractable, we assume access to a finite set of candidate shaping reward functions RK = {f1,...,fK} ~ G(R), where G is a distribution over the set of reward functions,\""}, {"title": "ORSO: ONLINE REWARD SELECTION AND POLICY OPTIMIZATION", "content": "In this section, we introduce ORSO (Online Reward Selection and Optimization), a novel approach to efficiently and effectively design reward functions for reinforcement learning. Our method operates in two phases: (1) reward generation and (2) online reward selection and policy optimization.\nReward Generation In the first phase of ORSO, we generate a set of candidate reward functions RK for the online selection phase. Given an MDP M = (S, A, P, r, po) and a stochastic generator G, we sample a set of K reward function candidates, RK = {f1,...,fK | Vi \u2208 [K], fi : S\u00d7A\u2192 AR, f\u00b2 ~ G}, from G during the reward design phase. The generator G can be any distribution over the reward function space R. For instance, if the set of possible reward functions is given by a linear combination of two reward components C1, C2, which are functions of the current state and action, such that r(s, a) = w1c1(s, a) + w2c2(s, a), then the generator G can be represented by the means and variances of two normal distributions, one for each weight w\u2081, W2.\nOnline Reward Selection and Policy Optimization Our algorithm for online reward selection and policy optimization is described in Algorithm 1. On a high level, the algorithm proceeds as follows. Given an MDP M = (S, A, P, r, po), an RL algorithm A and a reward generator G, we sample set of K reward functions RK ~ G and initialize K distinct policies \u03c0\u00b9, ..., \u03c0\u039a. At step t of the reward selection process, the algorithm selects a learner it \u2208 [K] according to a selection strategy. We then perform N iterations of training with algorithm A, updating the policy corresponding to reward function it to obtain \u03c0\u00b2\u207a. Policy \u03c0\u00b2\u207a is simultaneously evaluated under the task reward function r and the necessary variables for the model selection algorithm are then updated (e.g., reward estimates, reward function visitation counts, and confidence intervals). The algorithm returns the reward function f+ and the corresponding policy \u3160 that performs the best under the task reward function r.\nChoice of Selection Algorithm While ORSO is a general algorithm that can employ any selection method to pick the reward function to train on, the performance depends on the choice of algorithm. For instance, using a simple selection method like \u025b-greedy introduces an element of exploration by occasionally selecting a random reward function (with probability \u025b), but it risks overcommitting to a seemingly promising reward function early on. This can lead to suboptimal performance if the chosen reward function causes the task performance to plateau in the long run. However, greedier methods, such as \u025b-greedy, can achieve lower regret if they commit to the actual optimal reward function early in the process. These methods are particularly effective when early performance signals are strong indicators of long-term success.\nHowever, if initial performance is not a reliable predictor of future outcomes, these greedy ap- proaches may struggle, as they risk prematurely locking onto suboptimal rewards. In contrast, more exploratory algorithms like the exponential-weight algorithm for exploration and exploita- tion (Exp3) (Auer et al., 2002) maintain a broader search, potentially discovering better rewards in the long run, especially in environments where early signals are less informative. We empirically validate different choices of selection algorithms in Section 5."}, {"title": "THEORETICAL GUARANTEES", "content": "In this section, we provide regret guarantees for ORSO with the Doubling Data-Driven Regret Bal- ancing (D\u00b3RB) algorithm by Dann et al. (2024). A discussion of the intuition behind the D\u00b3RB algorithm and the full pseudo-code for ORSO with D\u00b3RB is provided in Appendix C. We note that the regret definition used in the online model selection literature is an upper bound for the practi- cal cumulative regret defined in Section 3. We provide a further discussion of this relationship in Appendix B.\nWe first introduce some useful definitions for our analysis.\nDefinition 4.1 (Definition 2.1 from Dann et al. (2024)). The regret scale of learner i after being played t times is \u2211e=1 reg(\u03c0\u00b2(e)) where reg(\u03c0\u00b2(e)) = I (\u03c0*) \u2013 I (\u03c0\u00b2(e)) in the reward design problem.\nFor a positive constant dmin > 0, the regret coefficient of learner i after being played for t rounds is\nthat is, d(t) \u2265 dmin is the smallest number such that the\nDann et al. (2024) use \u221at as this is the most commonly targeted regret rate in stochastic settings.\nThe main idea underlying our regret guarantees is that the internal state of all suboptimal reward functions is only updated up to a point where the regret equals that of the best policy so far.\nWe assume there exists a learner that monotonically dominates every other learner.\nAssumption 4.2. There is a learner i such that at all time steps, its expected sum of rewards dominates any other learner, i.e.,\nVt \u2208 N. This is equivalent to saying that\nAssumption 4.2 guarantees that the cumulative expected reward of the optimal learner it is always at least as large as the cumulative expected reward of any other learner and that its average performance increases monotonically.\nFollowing the notation of Dann et al. (2024), we refer to the event that the confidence intervals for the reward estimator are valid as E.\nDefinition 4.3 (Definition 8.1 from Dann et al. (2024)). We define the event E as the event in which\nThen we can refine Lemma 9.3 from Dann et al. (2024) in the case where Assumption 4.2 holds.\nLemma 4.4. Under event E and Assumption 4.2, with probability 1 \u2013 8, the regret of all learners i is bounded in all rounds T as\nWe provide the proof for Lemma 4.4 in Appendix D. Lemma 4.4 implies that when Assumption 4.2 holds, the regrets are perfectly balanced. This is in stark contrast with the regret guarantees of Dann et al. (2024) that prove the D3RB algorithm's overall regret to scale as (d) \u221aT where d = maxe<td. Instead, our results above depend not on the monotonic regret coefficients d but on the true regret coefficients d* . Even if learner i, has a slow start (and therefore a large d), as long as monotonicity holds and the i-th learner recovers in the later stages of learning, our results show that D\u00b3RB will achieve a regret guarantee comparable with running learner i in isolation."}, {"title": "PRACTICAL IMPLEMENTATION AND EXPERIMENTAL RESULTS", "content": "In this section, we present a practical implementation\u00b2 of ORSO and its experimental results on several continuous control tasks. We study the ability of ORSO to design effective reward functions with varying budget constraints. We also study how different sample sizes, K, of the set of reward functions RK influence the performance of ORSO and compare different selection algorithms.\nThis section is structured as follows. First, we present the experimental setup, including the environ- ments and baselines, and the practical consideration of the reward generator G and the algorithms used in the online reward selection phase. Then, we present the main results and ablate our design choices. Further experimental results can be found in Appendix H\nEXPERIMENTAL SETUP\nEnvironments and RL Algorithm We evaluate ORSO on a set of continuous control tasks us- ing the Isaac Gym simulator (Makoviychuk et al., 2021). Specifically, we consider the following tasks: CARTPOLE and BALLBALANCE, which are relatively simple; two locomotion tasks, ANT and HUMNAOID, which have dense but unshaped task rewards \u2013 for instance, the agent is rewarded for running fast, but the reward function lacks terms to encourage upright posture or smooth move- ment; and two complex manipulation tasks, ALLEGROHAND and SHADOWHAND, which feature sparse task reward functions.\nOur policies are trained using the proximal policy optimization (PPO) algorithm (Schulman et al., 2017), with our implementation built on CleanRL (Huang et al., 2022). We chose PPO because the Makoviychuk et al. (2021) provide hyperparameters, which we use, that enable it to perform well on these tasks when using the human-engineered reward functions.\nBASELINES\nIn our experiments, we consider three baselines. We analyze the performance of policies trained using each reward function detailed below. We evaluate the reward function selection efficiency of ORSO compared to more naive selection strategies.\nNo Design (Task Reward with No Shaping) We train the agent with the task reward function r for each MDP. These reward functions can be sparse (for manipulation) or unshaped (for locomotion). We use the same reward definitions as prior work (Ma et al., 2024), which we report in Appendix E.\nHuman We consider the human-engineered reward functions for each task provided by (Makoviy- chuk et al., 2021). We note that these are constructed such that training PPO with the given hyperpa- rameters yields a performant policy with respect to the task reward function. The function definitions are reported in Appendix E.\nNaive Selection We employ EUREKA (Ma et al., 2024) as a baseline for the naive selection ap- proach. EUREKA uses a large language model to generate Python code for the reward functions of several continuous control tasks. EUREKA uses an evolutionary scheme to evaluate and improve its reward functions. During each iteration, EUREKA samples a set of reward functions from an LLM, trains a policy on each reward function, and uses the best-performing reward function as a context for the LLM to perform the evolutionary step. However, this selection strategy can be seen as naive, as it uniformly explores each reward function for a fixed number of iterations, regardless of its actual performance on the task.\nIMPLEMENTATION\nReward Generation Similarly to recent works on reward design, which demonstrate that LLMs can generate effective reward functions for training agents (Park et al., 2024; Ma et al., 2024; Xie et al., 2024), we follow this paradigm by using GPT-4 (Achiam et al., 2023) to avoid manually de- signing reward function components. The language model is prompted to generate reward function"}, {"title": "RESULTS", "content": "In this section, we present the experimental results of ORSO. We evaluate ORSO's ability to effi- ciently select reward functions with varying budget constraints and reward function set size K. We consider budgets B \u2208 {5, 10, 15} \u00d7 n_iters and sample sizes K \u2208 {4,8,16}.\nORSO is Twice as Fast as the Naive Selection Strategy In Figure 2 (left), we plot the number of iterations required to reach different percentages of the performance achieved by policies trained with human-engineered reward functions. The y-axis represents the percentage of human perfor- mance, while the x-axis shows progress in the selection algorithm, normalized so that a value of 1.0 corresponds to B \u00d7 n_iters for each task. Results are aggregated across 6 tasks, 3 different budgets, and 3 reward function sets, with 3 seeds per configuration, totaling 162 runs.\nWe observe that ORSO with D\u00b3RB achieves human-level performance more than twice as fast as the naive selection strategy. The naive selection strategy on average does not manage to select an effective reward function within the limited budget. Detailed per-task and per-budget results are reported in Appendix H.\nORSO Surpasses Human-Designed Reward Functions Not only does ORSO reach human-level performance quickly, but it also has the potential to surpass it. Figure 2 (middle) illustrates the aver- age performance of ORSO compared to human-designed reward functions, the task reward function, and the naive selection strategy across different tasks. We observe that ORSO consistently matches or exceeds human-designed rewards, particularly in more complex environments. Again, the results are averaged over multiple seeds and configurations. The full breakdown is reported in Appendix H.\nORSO Scales with Budget Figure 2 (right) demonstrates how ORSO's performance scales with increasing budgets. While both ORSO and naive selection benefit from larger budgets, ORSO is consistently superior and surpasses human-designed rewards when B > 10.\nABLATION STUDY\nChoice of Selection Algorithm In Figure 3, we compare different selection algorithms for ORSO. We find that D3RB performs best on average, consistently outperforming other algorithms, followed closely by Exp3. These algorithms allow ORSO to balance exploration and exploitation effectively, leading to superior performance compared to more greedy approaches like UCB, ETC, and EG. Interestingly, even simpler strategies like EG and ETC substantially outperform the naive strategy, which highlights the importance of properly balancing exploration and exploitation for efficient reward selection. By framing reward design as an exploration-exploitation problem, we demonstrate that even basic strategies offer considerable gains over static, inefficient methods."}, {"title": "LIMITATIONS AND FUTURE WORK", "content": "A key limitation of ORSO is its reliance on a predefined task reward, which is typically straightfor- ward to construct for simpler tasks but can be challenging for more complex ones or for tasks that include a qualitative element to them, e.g., making a quadruped walk with a \"nice\" gait. Future work could explore eliminating the need for such hand-crafted task rewards by leveraging techniques that translate natural language instructions directly into evaluators, potentially using vision-language models, similarly to Wang et al. (2024); Rocamonde et al. (2024). Another alternative is to use preference data to learn a task reward model (Christiano et al., 2017; Zhang & Ramponi, 2023) and use the latter as a signal for the model selection algorithm."}, {"title": "RELATED WORK", "content": "Reward Design for RL Designing effective reward functions for reinforcement learning has been a long-standing challenge. Several approaches have been proposed to tackle it.\nTraditionally, researchers manually specify reward components and tune their coefficients (Ng et al., 1999; Margolis & Agrawal, 2022; Liu et al., 2024). This method often demands significant domain expertise and can be highly resource-intensive, involving numerous iterations of trial and error in designing reward functions, training policies, and adjusting reward parameters.\nAnother approach is to learn reward functions from expert demonstrations via methods like appren- ticeship learning (Abbeel & Ng, 2004) and maximum entropy inverse RL (Ziebart et al., 2008). While these methods can capture complex behaviors, they often rely on high-quality demonstrations and may struggle in environments where such data is scarce or noisy.\nPreferences can also be used to learn reward functions (Zhang & Ramponi, 2023; Christiano et al., 2017). This approach involves collecting feedback in the form of preferences between different trajectories, which are then used to infer a reward function that aligns with the desired behavior. This method is particularly useful in scenarios where it is difficult to explicitly define a reward function or obtain expert demonstrations, as it allows for more intuitive and accessible feedback from users.\nFoundation Models and Reward Functions Recent work has explored the use of large lan- guage/vision models (LL/VMs) to aid in the reward design process. L2R (Yu et al., 2023) leverages large language models to generate reward functions for RL tasks by first creating a natural language \"motion description\" and then converting it into code using predefined reward API primitives. While innovative, L2R has notable limitations: it requires significant manual effort in designing templates and primitives and is constrained by the latter. EUREKA (Ma et al., 2024) and Text2Reward (Xie et al., 2024) use LLMs to generate dense reward functions for RL given the task description in natural language and the environment code.\nFoundation models have also been directly used as reward models. Rocamonde et al. (2024) uses the cosine similarity of CLIP embeddings of language instructions and renderings of the state as a state- only reward model. Similarly, Wang et al. (2024) automatically generates reward functions for RL using a vision language model to label pairs of trajectories with preference, given a task description. Motif (Klissarov et al., 2024) first constructs a pair-wise preferences dataset using a large language model (LLM), learns a preference-based intrinsic reward model with the Bradley-Terry (Bradley & Terry, 1952) model, and then uses this reward model to train a reinforcement learning agent. Kwon et al. (2023) uses a similar approach, where an LLM is used during training to evaluate an RL policy, given a few examples of successful behavior or a description of the desired behavior.\nOnline Model Selection The problem of model selection in sequential decision-making environ- ments has gained significant attention in recent years (Agarwal et al., 2017; Foster et al., 2019; \u0420\u0430\u0441- chiano et al., 2020; Lee et al., 2021). This area of research addresses the challenge of dynamically choosing the most suitable model or algorithm from a set of candidates while learning.\nAgarwal et al. (2017) introduced CORRAL, a method to combine multiple bandit algorithms in a master algorithm. Foster et al. (2019) proposed model selection guarantees for linear contextual bandits. Pacchiano et al. (2020) extend the CORRAL algorithm and propose Stochastic CORRAL. Lastly, Lee et al. (2021) propose Explore-Commit-Eliminate (ECE), an algorithm for model selec- tion in RL with function approximation. A common requirement across all these approaches is the need to know the regret guarantees of the base algorithms.\nOur work is closely related to Dann et al. (2024), which removes the need for known regret guaran- tees and instead uses realized regret bounds for the base learners. In our setting, the set of models comprises the reward functions set and their corresponding policies."}, {"title": "ONLINE MODEL SELECTION", "content": "In this section, we introduce the model selection problem and some necessary notation modified from Dann et al. (2024) for our analysis.\nWe consider a general sequential decision-making process consisting of a meta learner interacting with an environment over T\u2208 N rounds via a set of base learners. At each round of interaction t = 1,2,..., T, the meta learner selects a base learner bt and after executing bt, the environment returns a model selection reward Rt \u2208 R. The objective of the meta learner is to sequentially choose base learners b1, ..., by to maximize the expected cumulative sum of model selection rewards, i.e., max E\nWe denote by vb = E[R | b] the expected model selection reward, given that the learner chooses base learner b, i.e., the value of base learner b. The total model selection reward accumulated by the algorithm over T rounds is denoted by ur = \u2211t=1 vbt. The objective is to minimize the cumulative regret after T rounds of interaction,\nIn our setting, each base learner corresponds to a reward function r and its associated policy \u03c0, i.e., b = (f, \u03c0). In this case, choosing to execute base learner b means training with algorithm A starting from checkpoint \u03c0 and using RL reward function f. The model selection reward R is then the evaluation of the trained policy under the task reward r, i.e., \u0399(\u03c0). The regret of base learner b can therefore be written as reg(b) = v* \u2212 v' = I(\u03c0*) \u2013 I(\u03c0), where \u03c0* is the optimal policy. Therefore the objective becomes minimizing\nWe note an important difference between the online model selection problem and the multi-armed bandit (MAB) problem. In model selection, the meta learner interacts with an environment over T rounds, selecting from K base learners. In each round t, the meta learner picks a base learner it \u2208 [K] (index of base learner chosen at step t) and follows its policy, updating the base learner's state with new data. Unlike MAB problems, where mean rewards are stationary, the mean rewards here are non-stationary due to the stateful nature of base learners (the base learners are learning as they see more data), making the design of effective model selection algorithms challenging.\nNotation The policy associated with base learner i at round t is denoted by \u03c0\u03ad, so that \u03c0\u03b9 = \u03c0\u03ad\u03b5. We denote the number base learner i has been played up to round t as n = \u2211e=1 1 {ie = i} and the total cumulative reward for learner i as u = \u2211=1 1 {ie = i} v\", where we use v = v to highlight that the policy associated with base learner i changes over time, but the reward function used for RL does not. We denote the internal clock for each base learner with a subscript (k) such that (4) is the policy of learner i when chosen for the k-th time, i.e., \u03c0\u00b2 = \u03c0\u00b2(k)\nRemark 1. The cumulative regret in Equation (6) is an upper bound for the practical cumulative regret.\nProof. This is straightforward to see. Let us first note that, by definition, for all t \u2208 [T], we have\""}, {"title": "ORSO WITH DOUBLING DATA-DRIVEN REGRET BALANCING", "content": "Here, we present the complete ORSO algorithm with Doubling Data-Driven Regret Balancing (D\u00b3RB) as the model selection algorithm.\nD\u00b3RB is built upon the idea of regret balancing, which aims to optimize the performance of multiple models by balancing their respective regrets. Imagine weighing two models on a balance scale where the \"weight\" corresponds to their regret; the goal is to keep the regret of both models balanced. This approach ensures that models with higher regret rates are selected less frequently, while those with lower regret rates are favored.\nConcretely, regret balancing involves associating each learner with a candidate regret bound. The model selection algorithm then competes against the regret bound of the best-performing learner among those that are well-specified - meaning their realized regret stays within their candidate bounds. Traditional approaches often rely on known expected regret bounds. In contrast, D\u00b3RB focuses on realized regret, allowing the model selection algorithm to compete based on the actual regret outcomes of each base learner. The algorithm dynamically adjusts the regret bounds in a data-driven manner, adapting to the realized regret of the best-performing learner over time. This approach overcomes the limitation of needing known regret bounds, which are often unavailable for complex problems.\nD\u00b3RB maintains three estimators for each base learner: regret coefficients d, average rewards u/n and balancing potentials \u03c6. At each step t, D\u00b3RB selects the base learner with the lower balancing potential and executes it. Then it performs the misspecification test in Equation (10) to check if the estimated regret coefficient for base learner it is consistent with the observed data. If the test triggers, i.e., the ai* is too small, then the algorithm doubles it. Lastly, D\u00b3RB sets the balancing potential \u03c6\u03b9 to dit\u221ant."}, {"title": "IMPLEMENTATION DETAILS", "content": "Rejection Sampling While the LLM produces seemingly good code, this does not guarantee that the sampled code is bug-free and runnable. In ORSO, we employ a simple rejection sampling tech- nique to construct sets of only valid reward functions with high probability, such that reward func- tions that cannot be compiled or produce \u00b1\u221e or NaN values are discarded.\nGiven criteria & to be satisfied, our rejection sampling scheme repeats the steps in Algorithm 3 until we have sampled the desired number, K, of valid reward functions.\nIn our practical implementation, checking if criteria & are satisfied consists of instantiating an envi- ronment with the generated reward function, running a random policy on it, and checking the values produced by the reward function. If the environment cannot be instantiated or if the values returned by the reward function are \u00b1\u221e or NaN, the reward function is rejected. It is worth making two im- portant observations. First, this is much computationally cheaper than instantiating the environment for training because one does not need to initialize large neural networks and can use fewer parallel environments than the number necessary for training. Moreover, we note that the rejection sampling mechanism only guarantees a higher probability of a valid reward function code as the policy used to evaluate the function is random and the optimization process used during the training of an RL algorithm could still induce undesirable values.\nIterative Improvement of the Reward Function Set In the initial phase of ORSO, the algorithm generates a set of candidate reward functions RK for the online reward selection and policy opti- mization step. While this approach is effective if RK contains an effective reward function, any selection process will fail to achieve a high task reward if the set does not contain a good reward function. To address this limitation, we introduce a mechanism for improving the reward function set through iterative resampling and in-context evolution. This is similar to Ma et al. (2024), how- ever, we introduce some important changes to prevent the in-context evolution from overfitting to initially suboptimal reward functions.\nResampling is triggered when at least one reward function has been used to train a policy for the number of iterations specified in the environment configuration or if all the reward functions in the set incurred too large a regret compared to the previous best policy if the algorithm has undergone at least one resampling step.\nThere are several strategies for resampling reward functions, each with its trade-offs. The simplest approach is to sample new reward functions from scratch, using the same generator G that was used in the initial phase. However, this method may not provide significant improvement, as it essentially restarts the search process without leveraging the information gained from the previous iterations of training.\nA more sophisticated approach is to greedily in-context evolve the reward function from the best- performing candidate so far as is done in Ma et al. (2024). This involves making incremental ad- justments to the reward function that has shown the most promise, potentially moving it closer to an optimal reward function. However, while this greedy strategy can lead to improvements, it also has the risk of overfitting to an initially suboptimal reward function if, for example, the initial set does not contain effective reward functions.\nTo mitigate the risk of overfitting, we introduce a simple strategy that allows the algorithm to be more exploratory. Specifically, we combine greedy evolution with random sampling: half of the reward functions are evolved in context from the best-performing candidate, while the other half is sampled from scratch. This approach allows the algorithm to explore new regions of the reward"}, {"title": "SELECTION ALGORITHMS AND HYPERPARAMETERS", "content": "In this section, we present the pseudocode for all reward selection algorithms used in our experi- ments with their associated hyperparameters in Table 2."}]}