{"title": "Joint Optimization of Prompt Security and System Performance in Edge-Cloud LLM Systems", "authors": ["Haiyang Huang", "Tianhui Meng", "Weijia Jia"], "abstract": "Large language models (LLMs) have significantly facilitated human life, and prompt engineering has improved the efficiency of these models. However, recent years have witnessed a rise in prompt engineering-empowered attacks, leading to issues such as privacy leaks, increased latency, and system resource wastage. Though safety fine-tuning based methods with Reinforcement Learning from Human Feedback (RLHF) are proposed to align the LLMs, existing security mechanisms fail to cope with fickle prompt attacks, highlighting the necessity of performing security detection on prompts. In this paper, we jointly consider prompt security, service latency, and system resource optimization in Edge-Cloud LLM (EC-LLM) systems under various prompt attacks. To enhance prompt security, a vector-database-enabled lightweight attack detector is proposed. We formalize the problem of joint prompt detection, latency, and resource optimization into a multi-stage dynamic Bayesian game model. The equilibrium strategy is determined by predicting the number of malicious tasks and updating beliefs at each stage through Bayesian updates. The proposed scheme is evaluated on a real implemented EC-LLM system, and the results demonstrate that our approach offers enhanced security, reduces the service latency for benign users, and decreases system resource consumption compared to state-of-the-art algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) are widely used in fields such as instruction understanding [1], information retrieval [2], software engineering [3], translation [4], education [5] and summarization [6]. Companies are competing to launch their LLMs such as ChatGPT, Claude, and Qwen, which have significantly enhanced the efficiency of human activities. Prompt engineering (PE) involves carefully crafting input prompts to ensure that LLMs generate outputs that meet user requirements, thus improving response quality [7]. However, PE can also be exploited by attackers to design malicious prompts, resulting in prompt attacks that manipulate LLMs into producing unsafe content, including the \"grandma exploit\" [8] and jailbreak [9]. These prompt attacks can have significant societal impacts, leading to the spread of misinformation [10], discrimination [11] and violations of social norms and morals [12]. Additionally, they can enable unauthorized access to private data. For example, Microsoft's LLM-integrated Bing Chat and GPT4 can be compromised by prompt injection attacks that expose sensitive information [13]. Furthermore, LLMs require substantial computational resources, and processing malicious prompts increases system resource consumption and average latency for benign users [14].\nThe safety of LLM outputs has attracted significant attention. To mitigate the risks associated with prompt attacks, model developers have implemented safety mechanisms through safety fine-tuning, utilizing the advanced technique of Reinforcement Learning from Human Feedback (RLHF). Particularly, Dai et al. proposed a Safe RLHF method to improve the security of LLM outputs [15]. This method involves training a reward model (helpfulness) using preference data and a cost model (harmlessness) with safety data, followed by fine-tuning through the proximal policy optimization reinforcement learning method. However, Wei et al. point out that safety training will fail because the two objectives of helpfulness and harmlessness are sometimes in conflict [9].\nCompared to merely employing RLHF for safety tuning, proactive detection before processing can significantly enhance LLM system safety and improve Quality of Service (QoS) by reducing malicious resource consumption. While LLMs can be directly utilized to detect harmful prompts [16], the conflict between helpfulness and harmlessness in safety fine-tuning methods often leads to performance degradation. In addition, models with large numbers of parameters can be resource-intensive, negatively affecting system throughput. However, detecting all prompts before inputting them into the LLM will increase resource consumption and service latency. To solve this problem, we constructed an Edge-Cloud LLM (EC-LLM) architecture that deploys the detection model at the edge, leveraging edge computing to reduce network throughput and latency [17].\nThe EC-LLM system faces many challenges. The first is how to improve the accuracy of detection objects. Existing anomaly detection models often face the problem of false positives [18], where benign prompts are mistakenly classified as malicious, negatively impacting QoS. Thus, we innovatively deployed a vector database (VDB) at each edge server (ES)"}, {"title": "II. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "In our EC-LLM scenario, the task set is denoted by K = {1, ..., K}, generated by a user set U = {1,..., U}. Tasks include benign tasks $K_b$ from benign users $U_b$ and malicious users $U_m$, and malicious tasks $K_m$ from malicious users $U_m$. The sets satisfy $K_b \\cup K_M = K$ and $U_b \\cup U_m = U$. Each task $k \\in K$ is characterized by four attributes: the input prompt $X_k$, the generating user $u \\in U$, the required floating-point of operations per second (FLOPS) $f_k$, and the number of prompt tokens $o_k$, which is obtained by using a tokenizer to convert the prompt into tokens within the LLM. The LLM is tasked with generating an appropriate response for each prompt.\nIn our system, there is a set of M ESs, denoted as $M_e = {1,..., M}$. The GPU FLOPS capability available on these ESs is defined as $G_e = {G_1, . . .,G_M}$. Additionally, the GPU bandwidth for each server is specified as $B_e = {B_1,..., B_M}$. Each ES has an owner, denoted as $L = {L_1,..., L_M}$, and each is considered a defender. Each ES hosts a VDB, denoted as $V = {V_1,...,V_M}$, which stores data in the format {$U_q$, $S_q$}$_{q=1}^Q$. Here, $u_q$ is a vectorized prompt derived from security datasets. The safety label $s_q \\in {0, 1}$ indicates whether $u_q$ is benign. Specifically, $s_q = 1$ denotes that $v_q$ is malicious, while $s_q = 0$ indicates a benign prompt.\nMalicious prompts pose a risk of generating unsafe outputs, leading to resource wastage when processed by LLMs. To differentiate between benign and potentially malicious prompts, a Bert-based detector is deployed on each ES to ascertain the safety of prompt $x_k$. If a malicious prompt is detected by the detector, it will not be sent to the cloud. Instead, the user will receive a direct response stating: \"This prompt is unsafe.\""}, {"title": "C. Cost Model", "content": "The detector's response is categorized as either benign or malicious. Consequently, the detection time and resource usage for the detector can be estimated. The number of parameters in the detector is denoted by A. For the transformer-based model, according to OpenAI [21], the floating-point of operations (FLOPs) per token is:\n$C_f = 2A + 2n_{layer}N_{ctx}d_{attn}$,\nwhere $n_{layer}$ is the number of layers, $n_{ctx}$ is the maximum number of input tokens and $d_{attn}$ is the dimension of the attention output. In the inference process, for the detector, the parameters need to be loaded into GPU memory. The GPU bandwidth is denoted as $B_g$ (TB/s). For a batch size, the memory latency $t_{mem}$ (ms) is calculated as:\n$t_{mem} = 2A/B_g$,\nwhere 2A indicates each parameter is 2 bytes. The FLOPS of the deployed ES detector are denoted as $C_g$. The computational latency for the detector with one batch size is calculated as follows:\n$t_{com} = C_f /C_g$.\nWhen the batch size is small, the memory latency exceeds the computational latency, creating a bottleneck in detector inference. Hence, the detection time for prompt $x_k$ is\n$d_k^d = o_k \\cdot max{t_{mem}, t_{com}}$.\nThe detection process may increase the latency for benign users. A binary variable $z_k \\in {0,1}$ is used to indicate whether prompt $x_k$ is detected. If $z_k = 1$, this prompt is detected; Otherwise $z_k = 0$. The $d_k$ represents the latency for prompt $x_k$ after the detection step, and it can be calculated as follows:\n$d_k = z_k \\cdot d_k^d + d_k^e, \\forall k \\in K$,\nwhere $d_k^e$ is the latency of LLM for processing task k. For LLM, the number of output tokens is unknown, so the processing time cannot be predicted accurately. We define a binary variable $\\eta_k$ to express the detection result of the detector. If $\\eta_k = 0$, the detection result is benign. $\\eta_k = 1$ means that prompt $x_k$ is malicious. If the prompt $x_k$ is not selected for detection, $\\eta_k = 0$. The resources consumed after detection for task k can be calculated as:\n$C_p^k = z_k(o_kC_f - \\eta_k \\cdot f_k) + f_k$."}, {"title": "D. Formulation", "content": "We divide time into T time slots. For time slot t \u2208 T, the task set $K_t$ is defined as $K_t = {k_{t,1},\u2026, k_{t,K_t}}$, where $K_t$ is the number of prompts at time slot t and $K = \\bigcup_{t=1}^T K_t$. Each task $k_{t,i}$ has four attributes: the input prompt $x_{t,i}$, the source user $u' \\in U_t$, the required FLOPS $f_{t,i}$, and the number of tokens $o_{t,i}$. The objective of our PDLRO problem is to reduce the total latency for benign users and reduce the resources consumed by malicious users and detection, which can be formulated as follows:\n$min \\sum_{t \\in T}(\\alpha_1 \\sum_{k_{t,i} \\in K_b}d_{k_{t,i}} + \\alpha_2 \\sum_{k_{t,i} \\in K}C_p^{k_{t,i}})$\ns.t.\n$z_{k_{t,i}} \\in {0,1}, \\forall k_{t,i} \\in K_t$.\n$\\eta_{k_{t,i}} \\in {0,1}, \\forall k_{t,i} \\in K_t$.\n$\\alpha_1, \\alpha_2 > 0$,\nwhere $\u03b1_1$ and $\u03b1_2$ are two weights used to balance latency and resource consumption. The problem is inherently challenging as it is an online problem where the detected result $\u03b7_{k_{t,i}}$ is unknown before making the detection decision $z_{k_{t,i}}$.\nTheorem 1. This problem is NP-hard.\nProof. The Knapsack Problem involves selecting items with specific weights and values to maximize the total value within a weight limit, and it is NP-hard. We can map it to our problem.\nIn the Knapsack Problem, each item has a weight and a value. In our problem, each task prompt $x_k$ has required FLOPS (analogous to weight) and $\\alpha d_k^e + \\beta C_k$ (analogous to value). The Knapsack Problem has a maximum weight capacity. In our problem, the ESs have a limited amount of GPU FLOPS and bandwidth. The decision variable of the Knapsack Problem is whether the item is included in the knapsack. In our problem, the decision variable is whether prompt $x_k$ is detected by the ES. The objective of the Knapsack Problem is to maximize the total value of selected items. In our problem, the objective is to minimize the total latency for benign users and the resources consumed by malicious users and detection. By constructing an instance of our problem that mimics the structure of the Knapsack Problem, we can show that solving our problem requires solving the Knapsack Problem as a subproblem, thus establishing NP-hardness."}, {"title": "III. BAYESIAN GAME MODEL", "content": "The PDLRO problem involves three parties: benign users, malicious users, and defenders. The interactions among these parties are complex, and the defenders' decisions regarding whether to conduct detection must be based on information. These characteristics make it highly appropriate to model the situation using a T-stage Bayesian game with incomplete information, corresponding to T time slots.\nOur game model is based on the following assumptions:\n1) All players are rational.\n2) Each player is unaware of the strategies, actions, or utility functions of other players.\n3) The total number of players remains constant over time.\n4) Malicious players randomly send either benign or malicious prompts. If a malicious player sends a benign prompt, they are considered a benign user at this stage.\nThe basic T-stage game model is denoted as $G_t = (N, \u03a6, A_t, P_t)$. The set of players is defined as $N = {U_b, U_m, L}$. The set of types, \u03a6, includes all types for all players, specifically \u03a6 = {b,m, d}, corresponding to benign users, malicious users, and defenders, respectively. The set of actions at time slot t, $A_t$, is defined as $A_t = A_b \\times A_m \\times A_d$. The action set for benign users $A_b = M_e$, involves selecting an ES to minimize latency. The action for malicious users, $A_m = M_e$, involves selecting an ES to target. The defender's action set, $A_d = {2^{K_t}}$, is defined as the power set of $K_t$, representing various combinations of prompts used for detection. The function $p_t(\u03a6) \u2208 P_t$ represents the prior probability that a player belongs to type \u03a6 \u2208 \u03a6.\nThe information structure at time slot t is denoted by $\u0398_t = {(h_{t,n})_{n\u2208N}, \u03c4(\u00b7,\u00b7)}$. It includes $h_{t,n}$, which represents the private information of each player at time slot t, inaccessible to other players. $\u03c4(\u00b7, \u00b7)$ denotes the common prior belief shared among all participants. The information structure is designed to meet the following criteria:\n$\\sum_{n \\in N} \u03c4(h_{t,n}, \u03c6) = p_t(\u03c6), \\forall \u03c6 \u2208 \u03a6$.\nThe basic T-stage game and the information structure form an incomplete information game. In this game model, a pure strategy is a definite action choice, while mixed strategies involve randomness to maximize the total reward. The strategy of player n is defined as $\u03c4_{t,n}: h_{t,n} \u2192 A_t$, mapping the information set to the action set. $\u03bc(\u03c6 | h_{t,n})$ is the belief of player n, which represents the estimate of type \u03c6 after receiving information $h_{t,n}$.\nFor benign player n \u2208 N, the strategy is denoted as $\u03c0_{t,n,m}^b$, which represents the probability that player n selects edge m at time slot t. The strategies of other players are defined as $\u03c0_{t,-n,m}$. The utility function is denoted as follows:\n$U_n(\u03c0_{t,n}^b, \u03c0_{t,-n,m}) = \\sum_{m \\in M} \u03c0_{t,n,m}^b d_{t,n,m}, t \u2208 T$,\nwhere $d_{t,n,m}$ denotes the latency experienced by player n when a task is processed on ES m. Additional latency arises if benign prompts are incorrectly identified during detection."}, {"title": "B. Game Model Analysis", "content": "The PDLRO problem is a dynamic game with incomplete information. Harsanyi transformation is a standard method to convert an incomplete information game to an imperfect information game [22]. Initially, a prior move by Nature is introduced to determine the type of each player. Each player knows their type but unknown other players' type. Subsequently, benign and malicious players select an ES, after which defenders receive tasks and decide whether to conduct detection. Since we use Bayes' rule to update beliefs, the game model is a multi-stage Bayesian Dynamic Game.\nFor a benign or malicious player n (\u03c6 \u2208 {b, m} and m \u2208 M\u00b2), the vector $\u03c0_{t,n}^\u03a6 = {\u03c0_{t,n,1}^\u03a6, \u00b7\u00b7\u00b7, \u03c0_{t,n,M}^\u03a6}$, has the same dimension as the number of ESs M. For the defender, the dimension of $\u03c0_{t,n}^d$ corresponds to the number of tasks $K_t$.\nA strategy combination $(\u03c0_b^*, \u03c0_m^*, \u03c0_d^*)$ is a Bayesian Nash equilibrium (BNE) of $G_t$ if, at each time slot t, for each player \u03b7 \u2208 N and \u03c6 \u2208 \u03a6, the following conditions hold:\n$U_n(\u03c0_{t,n}, \u03c0_{t,-n}) > U_n(\u03c0_{t,n}', \u03c0_{t,-n}), \\forall t \u2208 T$.\nwhere $\u03c0_{t,n}^*$ is the optimal strategy of opponents and $\u03c0_{t,n}$ is the optimal strategy for player n."}, {"title": "IV. GAME MODEL-BASED DETECTION RESOURCE ALLOCATION", "content": "In this section, we propose our Game Model-based Detection Resource Allocation (GMDRA) method."}, {"title": "A. VDB", "content": "VDB aids decision-making through belief updates. Defender n reviews the most recent detection results and vector similarity metrics. The vector similarity between prompt kt and benign data is denoted as $p_{in}^n$, whereas the vector similarity with malicious data is represented as $p_{un}^n$. The defender's belief indicates the safety probability for each prompt."}, {"title": "B. Belief Updating", "content": "Benign users, malicious users, and defenders all influence one another. Malicious users can mislead detection, increasing benign users' latency. At each time slot t, all parties act on updated beliefs from prior information. For a benign player n, minimizing latency is crucial, relying on the previous response $Y_{t-1,n}$, and the response latency $d_{t-1,n}$. Malicious users, with predicted number of tokens $O_{t\u22121,n}$, seek to consume more resources and increase latency. Each user's belief reflects their estimated detection probability. The belief update algorithm is shown in Algorithm 1.\nAlgorithm 1 updates beliefs based on the different types of players. Players assess their prompt's detection status and calculate the base likelihood values $p_{ld}$ and $p_{nld}$ based on the responses received (lines 3-6, 20-24 and 31-35). For benign user n, $d_e$ represents the ideal latency per token. The update to the likelihood function considers the difference between actual and ideal response latency (lines 10-11 and 14-15), capping values at 1 using the min function to ensure logical consistency. For malicious player n, the likelihood function is adjusted based on the actual versus expected number of response tokens (lines 25-26). Defender n updates the likelihood using vector similarity measures (lines 35-36). Finally, beliefs are updated using the Bayesian formula in line 38. For the three different user types, the time complexity of conditional judgments, probability calculations, and belief updates are all O(1), resulting in an overall time complexity of O(N)."}, {"title": "C. Defender Strategy", "content": "After determining their beliefs, players act accordingly by choosing an ES to send their prompts. Both benign and malicious players contribute to a prompt queue, denoted as $K_{t,m}$, from which the defender must develop a strategy to for detecting specific prompts.\nHowever, defender n lacks knowledge of the exact count of malicious prompts $K_{t,n}^m$. To address this, we proposed a prediction algorithm based on sequential marginal analysis, which is an economic method that examines the marginal costs and benefits during continuous decision-making [24]."}, {"title": "V. EVALUATION", "content": "In this section, comprehensive experiments are conducted to evaluate our methods."}, {"title": "A. Experimental Setup", "content": "Our system comprises four edge cloud clusters. Three devices serve as ESs, each equipped with an Intel(R) Core(TM) i9-10900K CPU featuring 10 physical cores, operating at a base frequency of 3.70 GHz, with a maximum turbo frequency of 5.3 GHz. These ESs run Rocky Linux 9.3 (Blue Onyx) with kernel version 5.14.0 362.8.1.el9_3.x86_64, and are equipped with GeForce RTX 2070 SUPER GPUs. The fourth device functions as the cloud server, featuring an Intel Core i9-14900KF CPU with 24 cores (32 threads) and a base frequency of 3.20 GHz. It operates on Debian with kernel version 6.1.0-16-amd64, also featuring a GeForce RTX 4090 GPU.\nDeployed in the cloud is the Qwen1.5-7B-Chat\u00b9 LLM, accessible via an API developed from the api-for-open-llm project\u00b2, encapsulated in a container image. Each ES hosts a Milvus VDB\u00b3 using Docker Compose, at version v2.3.3, with public datasets for prompt injection and jailbreak uploaded for vector similarity matching. We employ the deberta-v3-base-prompt-injection-v2 detector\u2074, a specialized model fine-tuned from Microsoft's deberta-v3-base [25], specifically designed to detect and classify prompt injection attacks."}, {"title": "Datasets", "content": "We use three datasets for testing and these data are not stored in the VDB.\n1) Classification: This dataset includes 139 jailbreaks and 123 benign prompts.\n2) Jailbreak-Benign-Malicious: This dataset aims to help detect prompt injections and jailbreak intent.\n3) Malicious-Prompts7: This is a prompt injection and jailbreak dataset sampled from various sources."}, {"title": "B. The Effectiveness of the Architectures", "content": "To evaluate the effectiveness of our architecture, we conducted an experiment comparing three setups: cloud-only, EC-LLM without detector, and EC-LLM with detector. In the cloud-only setup, users send prompts directly to the cloud. In the EC-LLM without detector architecture, prompts are sent to the edge and forwarded to the cloud without any filtering. In the EC-LLM with detector, a detector identifies and discards malicious prompts. Due to the large size of the Benign-Malicious and Malicious-Prompts datasets, we randomly selected 100 entries from each for testing, resulting in a total of 262 malicious and 200 benign prompts.\nThroughput is measured as the number of output tokens per second from the LLM. Malicious prompts are regarded as positive, and the F1 score is used to measure the accuracy of the detector."}, {"title": "C. The Effectiveness of detectors", "content": "Given the dataset's imbalance, we utilize precision, recall, and F1 score to evaluate the performance of the models. For detector, true positive (TP) is defined as instances where both the detection result and prompt label indicate malicious. For the LLM, a malicious prompt that goes unanswered is considered TP, while receiving a normal response is classified as a false negative. A benign prompt receiving a normal response is a true negative, and if it is rejected, it is considered a false positive. We use both GPT-4 and manual evaluations to assess the LLM's responses.\nFrom the table, it is evident that this LLM has some ability to resist prompt attacks, it is generally less effective than detectors. Deploying detectors at the edge effectively enhances the filtering of malicious prompts."}, {"title": "D. The Influence of Different Detection Strategies", "content": "To evaluate the effectiveness of our GMDRA with VDB strategy, we compare it against four strategies: None-detection, full detection, evolutionary genetic detection, and GMDRA without VDB. The evaluation involves a game configured with 20 players, divided into five stages, each lasting 75 seconds to account for task prompt processing times. Average latency per token is defined as the average time taken for each token (input + output) within a time slot. A fixed number of four malicious players randomly send benign or malicious prompts during each interval. The ES assignment for users is determined by the game. All tasks are completed within these time slots, and the results are illustrated in Fig. 3.\nFig. 3a presents the cumulative average latency per token for benign users across different strategies. The none-detection strategy incurs the highest latency, while the other strategies reduce latency by filtering out malicious prompts, which consume significant resources. The evolutionary genetic strategy further decreases latency compared to full detection. For the GMDRA without VDB approach, the absence of an integrated VDB forces the system to rely solely on historical data for evaluating current prompts, leading to an increase in latency as benign prompts from malicious players are incorrectly detected. In contrast, the GMDRA with VDB achieves the lowest latency widening over time. This approach reduces latency by approximately 16.24% compared to the none-detection strategy.\nIn Fig. 3b and Fig. 3c, the trends for cumulative average GPU memory and GPU utilization show a consistent pattern. The full detection strategy identifies malicious prompts, resulting in lower average GPU memory consumption and reduced average GPU utilization compared to the none-detection approach. Although the evolutionary genetic strategy and the GMDRA without VDB method do lessen resource consumption compared to the none-detection strategy, they are not as effective as the full detection strategy. The GMDRA with VDB not only filters out malicious prompts but also reduces detection time. This results in a decrease in both cumulative average GPU memory and GPU utilization by shortening the task completion time within each time slot. Compared to the none-detection strategy, GMDRA with VDB reduces average GPU memory consumption by nearly 17.7% and average GPU utilization by about 17.87%."}, {"title": "E. The Influence of the Number of Players", "content": "To explore the impact of the player count on various strategies, we design a comparative scheme with the number of malicious prompts fixed at 2 and the total number of players set to 10, 20, and 30.\nFig. 4a displays the average latency per token for benign users across different player counts. As the number of players increases, latency for all strategies increases. The average latency of none-detection is the largest, followed by the evolutionary genetic detection strategy. Among different numbers of players, our GMDRA with VDB strategy performs best. Figs. 4b-4c show GPU memory usage and utilization for benign users at different numbers of players. As the number of players increases, the volume of prompts received by LLM increases, leading to a nearly linear increase in average GPU memory consumption and utilization. The non-detection strategy results in high GPU memory and usage, while other methods are more efficient in reducing resource consumption. Our GMDRA with VDB detection strategy demonstrates superior performance and stability across various player counts."}, {"title": "F. The Influence of the Number of Malicious Prompts", "content": "To assess the impact of different numbers of malicious prompts on our identification strategy, we conduct a comparative experiment with 20 players. Here, 10 players are malicious, and the total number of malicious prompts sent is varied at 2, 4, 6, 8, and 10.\nIn Fig. 5a, the average latency per token remains relatively low despite an increase in malicious prompts. This is attributed to the decrease in benign prompts and the LLM's partial rejection of malicious submissions. When malicious players submit benign prompts, the GMDRA without VDB often misclassifies them, causing increased latency. In contrast, our GMDRA with VDB strategy consistently achieves the lowest latency for benign users.\nIn Figs. 5b-5c, average GPU memory usage and utilization decrease as the number of benign users declines. The none-detection strategy consumes significantly more GPU resources, while our GMDRA with VDB strategy shows the lowest GPU memory usage and utilization, proving effective across different scenarios with varying malicious prompts."}, {"title": "VI. RELATED WORK", "content": "Prompt attacks have emerged as a significant threat to LLMs. Although security mechanisms have been developed for LLMs, they remain vulnerable to attacks such as jailbreak and prompt injection. Wei et al. highlighted the vulnerability of LLMs to prompt jailbreak attacks due to conflicting and competing training objectives [9]. They successfully executed jailbreak attacks on prominent models like GPT-4 and Claude v1.3, arguing that increasing model size and generalization capabilities without adequate defenses can intensify these vulnerabilities.\nXue et al. [26] proposed TrojLLM, a black-box framework that generates toxic triggers to manipulate LLM outputs with specific prompts. Shi et al. [27] contend that adversarial and backdoor attacks are feasible on prompt-based models, developing a malicious prompt template construction method through gradient search that diminishes LLM accuracy. Du et al. demonstrated a poisoned prompt tuning method achieving a 99% success rate by creating a direct shortcut between a trigger word and a target label, enabling attackers to manipulate model predictions with minimal input [28]. Greshake et al. revealed that LLM-integrated applications blur the distinction between data and instructions, uncovering several new attack vectors [13]. They described how adversaries can exploit these applications remotely by injecting prompts into data likely to be accessed during inference."}, {"title": "B. Defenses", "content": "To address prompt attacks, various defense methods have been proposed, categorized into prevention-based and detection-based defenses [29]. Paraphrasing alters the text's appearance while retaining its meaning, while retokenization splits tokens in a prompt into smaller components. Both methods aim to neutralize malicious inputs from attackers but may unintentionally alter prompt content and affect response quality. Hines e al. employed delimiting, datamarking, and encoding to improve LLMs' ability to differentiate between data and instructions, reducing the attack success rate from 50% to 2% with minimal impact on task effectiveness [30]. However, this approach could increase the prompt complexity and require additional computing resources.\nFor detection-based methods, adversarial suffixes are a feature of jailbreak attacks, which can be detected by learning adversarial suffixes [31]. Caselli et al. introduced HateBERT, an improved model for detecting abusive language that outperforms the general BERT model [32]. While HateBERT effectively identifies objectionable content, it may fail to detect carefully crafted prompt attacks. However, these detection methods do not address issues related to resource consumption and latency optimization."}, {"title": "C. Bayesian Game Model", "content": "Harsanyi introduced Bayesian methods into game theory, formalizing the analysis of games with incomplete information [22]. In this framework, the concept of \u201ctype\u201d represents the private information of each participant, allowing for the application of Bayes' rule to update beliefs about other's types. Bayesian game models are widely used in network security.\nYan et al. proposed a Bayesian network game framework to analyze the dynamics of distributed denial-of-service attacks and defenses, elucidating the strategic interactions between attackers and defenders. Mabrouk et al. designed an intrusion detection game based on a signaling game theory to secure data exchanges between ambulances and hospitals, using BNE to predict node types and enhance security [33]. Ge et al. [34] constructed the GAZETA framework, employing a dynamic Bayesian game model for interdependent trust evaluation and authentication strategies, demonstrating significant improvements in network security.\nThese studies overlook the impact on benign users and do not simultaneously address system security and resource optimization, highlighting a gap in integrating comprehensive security measures with efficient resource management."}, {"title": "VII. CONCLUSION", "content": "In this paper, we address the joint optimization of prompt security and system resource allocation in EC-LLM systems. We design an EC-LLM structure that deploys a detector and a VDB at each ES to enhance the security and efficiency of prompt processing. Subsequently, we formulate the PDLRO problem as a multi-stage Bayesian dynamic game model, developing solutions for predicting malicious prompts and updating beliefs through various stages, we also propose the GMDRA method to optimize detection resource allocation. Finally, tests on a real-world EC-LLM system demonstrate that our approach significantly enhances system security while reducing user latency and resource consumption."}]}