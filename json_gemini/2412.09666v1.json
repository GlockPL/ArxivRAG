{"title": "SYSTEMATIC ANALYSIS OF LLM CONTRIBUTIONS TO PLANNING: SOLVER, VERIFIER, HEURISTIC", "authors": ["Haoming Li", "Zhaoliang Chen", "Songyuan Liu", "Yiming Lu", "Fei Liu"], "abstract": "In this work, we provide a systematic analysis of how large language models (LLMs) contribute to solving planning problems. In particular, we examine how LLMs perform when they are used as problem solver, solution verifier, and heuristic guidance to improve intermediate solutions. Our analysis reveals that although it is difficult for LLMs to generate correct plans out-of-the-box, LLMs are much better at providing feedback signals to intermediate/incomplete solutions in the form of comparative heuristic functions. This evaluation framework provides insights into how future work may design better LLM-based tree-search algorithms to solve diverse planning and reasoning problems. We also propose a novel benchmark to evaluate LLM's ability to learn user preferences on the fly, which has wide applications in practical settings.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have revolutionized nat-ural language processing, showcasing extraordinary ca-pabilities in a wide range of tasks, from text generationto reasoning and knowledge retrieval. However, when itcomes to solving problems that require structured planningor sequential decision-making, LLMs face inherent lim-itations. Their ability to generate coherent outputs oftendoes not translate into systematic exploration or deliberate reasoning, which are essential for tasks involving complexproblem-solving and multi-step planning.\nTo address these limitations, researchers have exploredintegrating LLMs with structured reasoning frameworksthat provide the necessary scaffolding for planning. Onepromising avenue is the use of test-time scaling techniques,where additional computational resources are applied dur-ing inference to enable dynamic exploration and reasoning(Snell et al. [2024]). These techniques often leverage tree-based search methods, such as Monte Carlo Tree Search(MCTS) or hierarchical frameworks, allowing LLMs to de-liberate, evaluate, and backtrack over potential solutions in a structured manner (Hao et al. [2023], Yao et al. [2023a],Long [2023]).\nThis integration has demonstrated remarkable success inenhancing LLM capabilities. By embedding LLMs withinframeworks that emulate human-like trial-and-error reason-ing, they can serve as heuristic functions or world models,guiding the search process and improving decision-making.Such approaches have been employed in various applica-tions, ranging from mathematical problem-solving to cre-ative tasks, highlighting their potential to expand the scopeof LLMs in domains where raw generative abilities areinsufficient.\nIn this work, we present the following key contributions:\n\u2022 We establish a systematic framework to evaluate thecontributions of LLMs when integrated with structuredalgorithms\n\u2022 We empirically validate the widely held assumption thatLLMs can serve effectively as heuristic functions withintree-based algorithms. While this assumption underpinsmany existing methods, it is not a natural consequenceof LLM design and requires rigorous testing to confirm.\n\u2022 We propose a novel benchmark for evaluating LLMs'ability to learn user preferences. This benchmark hassignificant practical implications, as preference model-ing is crucial in applications ranging from personalizedrecommendations to decision-support systems.\nBy addressing these dimensions, our work advances the un-derstanding of how LLMs can be systematically integratedinto planning and decision-making frameworks, paving theway for more robust and adaptable AI systems."}, {"title": "Related Works", "content": ""}, {"title": "LLM in Planning", "content": "There has been a lot of work focusing on the planningabilities of LLMs and designing systems that use LLMsas a part of core mechanism. A key advantage of LLMs istheir capacity to interpret and generate natural language,enabling them to craft plans without extensive domain-specific training. However, their inherent non-determinismintroduces challenges in predictability, unlike symbolicplanners. Techniques like Reflexion Shinn et al. [2023]and ReAct Yao et al. [2023b] leverage iterative feedbackloops and interleaved reasoning-action frameworks, re-spectively, to refine plans dynamically, reducing errors likehallucinations. Similarly, methods like Translated (LM)Huang et al. [2022] and LLM-Planner Song et al. [2023]enhance planning by generating free-form action plans andadjusting based on real-world observations or semantictranslations.\nFor structured task decomposition, approaches like SELF-GOAL Yang et al. [2024] decompose complex goals intoactionable subgoals using modules powered by LLMs,balancing decomposition with post-hoc summarization toguide agent behavior in multi-agent scenarios. Tree ofThoughts Yao et al. [2023a] and RAP Hao et al. [2023] ex-tend LLM reasoning through tree-based search strategies,with the latter incorporating Monte Carlo Tree Search forstrategic exploration. Heuristic approaches, such as Say-CanPay Hazra et al. [2024], further blend LLM-generatedactions with feasibility assessments to achieve groundedand cost-effective plans.\nAdditionally, code-based planning systems such as PROG-PROMPT Singh et al. [2022] and AdaPlanner Sun et al.[2023] employ programming-inspired prompts and adap-tive feedback loops to generate executable plans that re-spond dynamically to environmental feedback. Separatelytrained components like those in LID Li et al. [2022] andDEPS Wang et al. [2023] combine pre-trained languagemodels with multimodal systems or selector modules, en-abling iterative plan refinement through active data gath-ering or error explanations. Collectively, these diversestrategies illustrate the transformative potential of LLMsin adaptive, interactive planning while addressing chal-lenges in determinism and execution fidelity."}, {"title": "Test-time Scaling", "content": "The key concept behind test-time scaling is intuitive \u2013 solv-ing harder problems should not require the same amountof compute as solving easier ones. Most of the approachesdescribed in the previous section already employs test-timescaling. Indeed, methods like ReAct Yao et al. [2023b],Reflexion Shinn et al. [2023], ToT Yao et al. [2023a] usesmultiple interactions with LLM to get a final solution. Ingeneral, any algorithms that involve tree search, reflectionmechanisms, or iterative feedback inherently embracesthe concept of test-time scaling. More recently, OpenAI'snew model ol has demonstrated remarkable capabilities,OpenAI [2024]. While the exact technical details are un-published, it is evident that ol leverages more test-timecompute to generate bette solutions.\nThere has been an increasing interest in systematically un-derstanding this technique. A recent paper from DeepMindSnell et al. [2024] emphasizes that allocating additionalcomputational resources during inference allows modelsto refine their outputs iteratively, leading to substantial per-formance gains in tasks requiring complex reasoning. This\"compute-optimal\" strategy highlights a cost-effective ap-proach that prioritizes flexible deployment over expensivepre-training cycles. By focusing on how models processand reprocess input during inference, this line of researchshifts attention from scaling the number of parameters toscaling computational intensity during test-time, demon-strating that efficient problem-solving is achievable evenwith existing architectures. Similarly, Large LanguageMonkeysBrown et al. [2024] underscores the impact ofgenerating and sampling multiple outputs, showing a log-linear improvement in problem coverage with increasedinference sampling. Together, these studies establish test-time computation as a powerful lever for enhancing LLMaccuracy and adaptability. They also underline the chal-lenges of balancing computational overhead with resultquality, especially in tasks without automated solution ver-ification. This burgeoning field paves the way for moreefficient and versatile LLM deployments across diverseapplications."}, {"title": "Evaluation Method", "content": "We now introduce the proposed evaluation framework. Wespecifically focus on 3 different roles of LLM that maycontribute to solving a planning task \u2013 solver, verifier, andcomparative heuristic function. We emphasize that theseare independent components that can be composed togetherin very interesting ways. It is also possible to introduceexternal tools, such as symbolic planners, into the planningalgorithm to further enhance performance and efficiency.\nWe test those independent components on 3 tasks: TravelPlanning, Course Planning, and Fitness Planning. Weuse TravelPlanner Xie et al. [2024] for the first task, andconstruct our own datasets for the other two. Notably,Fitness Planning benchmark is an interactive environmentthat tests LLM's ability to learn user preferences, insteadof a fixed set of problems.\nOn these 3 datasets, we have natural ways to define how\"close\" a proposed solution is to the actual solution for anygiven problem instance. This means that we can define anoracle heuristic function for each dataset, which is used toevaluate how well LLM-parameterized heuristic performs."}, {"title": "LLM as Solvers", "content": "The most natural way to use LLMs in a planning problemis to simply ask the LLM to solve it. Therefore, as the"}, {"title": "LLM as Verifiers", "content": "We also explore whether LLMs can reliably verify the so-lution of a problem. Earlier work such as LLM MonkeysBrown et al. [2024] have shown that given sufficientlymany attempts, LLMs are generally able to get the correctanswer. Identifying the correct solution out of all gen-erated responses without an automatic evaluation engineis essential, since in many real world applications, suchoracle verifier is usually not available.\nA natural option to choose a solution out of N is throughmajority voting, but as Brown et al. [2024] demonstrates,this technique is not powerful enough and does not scalevery well with N. An intuitive explanation is that thismethod only works if the correct solution has high like-lihood to begin with, which is not necessarily the case,especially for hard problems. In this paper, we evaluatewhether LLMs can serve as verifiers out of the box."}, {"title": "LLM as Heuristic Functions", "content": "Real-valued Formulation. For task T, we define an oracleheuristic function $f_T(\\hat{y}, y, x) \\rightarrow \\mathbb{R}$, where y is a groundtruth solution, $\\hat{y}$ is LLM-generated solution, and x is prob-lem description. We want to evaluate LLM's performanceas an approximate of $f_T$, denoted $\\hat{f}_T(\\hat{y}, x) \\rightarrow \\mathbb{R}$, whoseoutput does not rely on ground truth solution y. We canuse $| f_T(\\hat{y}, y, x) - \\hat{f}_T(\\hat{y}, x)|$ as a metric to evaluate LLM'sperformance.\nIssues with real-valued formulation. Estimating the ex-act value produced by $f_T$ may prove difficult. The rangeof such value is also highly dependent on the probleminstance. For example, if we use editing distance as $f_T$for course planning, then the output range is strictly in-tegers within (0, total number of classes). This examplealso sheds light on the difficulty of estimating $f_T$. Indeed,without ground truth solution y, solving for editing dis-tance could be as complex as solving the original problemdirectly, making this estimated heuristic unuseful. In short,in a real-valued formulation, we require $f_T$ to be 1) power-ful enough to provide useful guidance for planning and 2)sufficiently independent of solution y such that LLM canreasonably estimate $f_T$ without y. Finding such $f_T$ couldbe very difficult.\nComparative Formulation. One of the main purposesof having a heuristic function is to guide LLM's plan-ning/reasoning process through tree search, where thesolver LLM proposes N candidate next steps and heuris-tic LLM $\\hat{f}_T(\\hat{y}, x)$ evaluates each candidate. However, ob-serve that the exact values are not strictly necessary hereto provide guidance (although having it could be benefi-cial). Indeed, to effectively search the tree, providing an"}, {"title": "Datasets", "content": ""}, {"title": "Fitness Planning", "content": "Previous studies focusing on personal fitness planning suchas PlanFittingShin et al. [2023] provides a comprehensiveframework for implementing planning on user's end. How-ever, evaluation of LLM performance on such a systemdepends on continuous human feedback, which is bothcostly and time-inefficient. In this paper, we propose a newframework for evaluating LLM performance on personalfitness planing.\nThe goal of this problem is to optimize a workout plan for auser by adjusting assigned exercises and corresponding rep-etitions (reps) iteratively, based on the feedback providedby the user after each workout session. The agent is taskedwith finding the optimal plan by exploring various exercisecombinations under a set of constraints, while continuouslyrefining the plan based on user satisfaction. Specifically,within the task, there will be two entities, a User who pro-vides basic information and feedback (satisfaction scores,complaints, etc.) after completing workout plans, and anAgent who generates initial and updated workout plans,aiming to maximize user satisfaction through iterative feed-back.\nTo modularize the problem for flexible adjustments, wedivide the problem into four major components: the user"}, {"title": "Problem Formulation", "content": "Problem Formulation Our objective is to maximize Usersatisfaction, denoted as $f(P_t, \\hat{P}_t) \\rightarrow F_t$, subject to con-straints $C(B, Z)$, where f is hidden from both the agentand the user. Specifically:\n\u2022 $P_t$: The fitness plan at timestep t, represented as $P_t =(p_{1,t}, p_{2,t},..., p_{k,t})$, where k is the number of exercises,and p refers to the reps assigned to each exercise.\n\u2022 $U$: The user preference, represented as $U =(u_1, u_2,..., u_k), \\forall u \\in [0,10]$ where k is the numberof exercises, and u refers to the user's preference toeach exercise.\n\u2022 $\\hat{P}_t$: The desired fitness plan at time step t given the gen-erated plan $P_t$, represented as $\\hat{P}_t =(\\hat{p}_{1,t}, \\hat{p}_{2,t},..., \\hat{p}_{k,t})$.It can also be considered as the target the agent is tryingto uncover. The specific values of $\\hat{p}_{i,t}$ is calculated byoptimizing the below expression:\nMaximize $\\sum_{i} u_i \\cdot p_{i,t}$\nSubject to $\\sum_{i} t_i p_i \\leq T$,\n$0 \\leq p_{i,t} \\leq max\\_reps$\nwhere $t_i$ is the time required to complete exercise $p_i$, Tthe total available time of the user.\n\u2022 $F_t$: The user feedback at timestep t. This can either be areal number in the range [0, 10], representing the overallsatisfaction with $P_t$, or a set of real numbers in the samerange, reflecting feedback on specific attributes of $P_t$(e.g., satisfaction with individual exercises).\n\u2022 B: A set of boolean constraints on $P_t$, such as the avail-ability of gym equipment.\n\u2022 Z: A set of integer constraints on $P_t$, such as the user'savailable time or stamina.\n\u2022 f: The utility function that maps $P_t, \\hat{P}_t$ to a real number,or the satisfaction score. It is consisted of three com-ponents, evaluation on selected exercise, assigned reps,and diversity. We argue that such composition suits withreal-world workout planning.\n$Plan(P_t, U) = \\frac{\\sum_{i=1}^{n}I(U_i(P_{t,i} \\neq 0))}{\\sum_{i=1}^{n}I(\\hat{P}_{t,i} \\neq 0)} \\cdot min (1, \\frac{\\sum_{i=1}^{n}\\hat{P}_{t,i}}{\\sum_{i=1}^{n}||\\hat{P}_{t,i}||})$\n$Rep(\\hat{P}_t, P_t) = \\frac{\\hat{P}_t P_t}{|| \\hat{P}_t |||| P_t ||}$\n$OverLap(\\hat{P}_{t-k}, ..., \\hat{P}_t) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\sum_{j=0}^{k} I(\\hat{P}_{t-j, i} \\neq 0)}{||P_t|| ||\\hat{P}_t||}$\nSince the output of all three functions are with the range[0, 1], the final score will be the weighted sum of the"}, {"title": "Evaluating Solver", "content": "Evaluating Solver The optimization process will be mea-sured by the following metrics: 1) Feasibility: Percentageof time the Agent generates a feasible plan; 2) Optimality:How closely the plan matches with the user's preferencesin the last three days; 3) Cost Utility: How many iterationsdoes the Agent take to reach an optimal solution; and 4)Diversity: The diversity of exercises explored by the Agentduring the iterations."}, {"title": "Evaluating Verifier", "content": "Evaluating Verifier We define verifier for fitness planningunder two settings, zero-shot and few-shot verification. Inzero-shot setting, the agent will be provided with the initialplan $P_0$, the user preference U, the constraints B and Z. Itwill then be asked to verify if $P_0$ is admissible. At few-shotsetting, the agent will be additionally provided with the$P_1,..., P_t$ and $F_0, ..., F_{t-1}$. Alternatively, the agent willbe asked to verify the admissibility of the plan $P_t$. Weevaluate the performance of LLMs using passrate, whichmeasures whether a plan satisfies all given constraints."}, {"title": "Evaluating LLM Heuristic", "content": "Evaluating LLM Heuristic Similar to the verifier, wedefine the heuristic function for fitness planning undertwo settings, zero-shot and few-shot heuristic ranking. Inzero-shot setting, the agent will be provided with n initialplans $P_0,..., P_n$, the user preference U, the constraints Band Z. It will then be asked to rank the plans under our"}, {"title": "Course Planning", "content": "Although previous paper Kasneci et al. [2023] has empha-sized the opportunities and challenges of applying LLMsfor lesson planning, which is to generate a syllabus for aparticular course, few words mentioned the application ofcombining LLMs with course schedule planning. In thispaper, we propose a new benchmark to address this blank.\nThe goal of this problem is to test LLMs' ability to gen-erate and evaluate a plan that arranges different sectionsof courses into a calendar. Specifically, there are severalavailable classrooms ($C_1, C_2, ..., C_n$), each with a spe-cific seating capacity ($C_1, C_2, ..., C_n$), and multiple coursesections ($S_1, S_2, ..., S_m$), each assigned a specific time slotand set of days for their scheduled classes, with each sec-tion also having an estimated enrollment ($s_1, s_2, \u2026, s_m$),waiting to be assigned to appropriate classrooms. Theagent is going to assign classrooms to each section andto get the overall classroom occupancy above a certainthreshold."}, {"title": "Problem Formulation", "content": "Problem Formulation The objective of this task is togenerate a plan P = ($P_1, P_2, ..., P_n$) where $p_i = (S_{i,j}, C_{i,k})$is a section-classroom pair chosen from every action i andtry to get as high a classroom occupancy as possible so thatthe overall occupancy exceeds a certain threshold value.\n$J(P) = \\sum_{i=0}^{n} \\frac{C_{i,k}}{s_{i,j}} > \\delta$"}, {"title": "Evaluating Solver", "content": "Evaluating Solver As the problem is NP-hard, it's hard tofind all possible answers in polynomial time, therefore, wemeasure the generated plan to be an answer by three met-rics: 1) Completeness: if the generated plan contains allsections.(fulfill constraint 3) 2) Feasibility: for those com-pleted plans, whether there is no time conflict and whetherthe classroom capacity is no smaller than estimated enroll-ment.(fulfill constraint 1 and 2) 3)Optimality: for thosefeasible plans, whether the overall classroom occupancyexceeds the threshold $\\delta = 1.3$."}, {"title": "Evaluating Verifier", "content": "Evaluating Verifier To evaluate the ability of LLMs toverify whether a given plan is a valid solution, the LLMsare tasked with assessing candidate plans, which are eithergenerated by other LLMs or derived from the ground truthpool through exhaustive search to obtain a valid solution.We evaluate LLM as verifier through two metrics: feasibility:whether the assessment of the feasibility of the givenplan is accurate., Optimality: for feasible plans, whetherthe assessment of their optimality is accurate."}, {"title": "Evaluating LLM using Heuristic", "content": "Evaluating LLM using Heuristic To evaluate the abilityof LLMs using heuristic, we define the task under twosettings, zero-shot and one-shot. In either setting, LLMsare tasked to give a rank over two or four candidate plans.These candidate plans ($P_1, P_2, ..., P_i$) are derived from acorrect plan $P_{gold}$, each representing an intermediate statein the process of arriving at the correct solution. Addition-ally, the content of these plans is randomly altered at acertain rate to produce $P_i$. The goal of this task is to testwhether LLMs can detect the incompleteness and incorrect-ness of a plan and estimate a distance dist($P_i$, $P_{gold}$) fromcurrent plan to the correct plan. In the one-shot setting, anexample containing two candidate plans and a comparativeheuristic function is given. LLMs can follow the idea andthought of the heuristic to determine the rank of plans.We measure the performance of LLM through hit@k met-rics, which evaluates whether at least one correct answeris included in the top k answers given by LLMs."}, {"title": "Travel Planning", "content": "The goal of TravelPlanner dataset is to test LLM's ability togenerate comprehensive travel itineraries while satisfyinguser-specified needs and commonsense constraints. Eachtask is initiated by a user query that provides details suchas departure and destination cities, travel dates, budget,"}, {"title": "Experimental Results", "content": "In this section, we present main experimental findings fromour proposed evaluation framework. We use claude-3-5-sonnet-20240620, GPT-40-2024-08-06, and DeepSeek-V2.5 to show that the same pattern manifests across differ-ent models. The detailed report for LLMs work as solvercan be find in 6."}, {"title": "Fitness Planning", "content": "In Table 6, GPT-40 demonstrates superior performancecompared to DeepSeek across all conditions. Withoutdynamic constraints, GPT-4o achieves high feasibility, op-timality, and diversity scores. However, an interestingobservation arises at 10 iterations, where the feasibilityscore (0.6670) is slightly lower than at 5 iterations (0.7370).This anomaly suggests that while additional iterations al-low for more exploration and refinement of plans, theymay occasionally lead to a trade-off in feasibility due tooverfitting or over-exploration of the solution space. By20 iterations, GPT-40 recovers, achieving its highest fea-sibility (0.7500) and optimality (0.8520) in this condition,showing that longer iterations generally benefit the model'sperformance."}, {"title": "Course Planning", "content": "In Table 6, LLMs perform well in generating completeplans with all sections included when the problem sizeis small. However, as the problem description becomeslonger, they struggle to identify the total number of sec-tions, often resulting in incomplete plans. Across all taskdifficulties, LLMs fail to produce feasible plans, let aloneoptimal ones, even if the plans are complete. Among thesemodels, Claude-3.5-Sonnet demonstrates the best overallperformance in understanding natural language instruc-tions and creating complete plans.\nTable 3 summarizes the performance of LLMs as verifiersin course planning tasks across varying difficulty levels.Overall, the results show that all models struggle signif-icantly to evaluate both the feasibility and optimality ofplans, with performance dropping as task difficulty in-creases.\nGPT-40 outperforms other models in detecting feasibleplans, achieving the highest rate of 61.25% in easy level.However, the ability to identify optimal solution is not asexpected. Same situation happens to claude and DeepSeekwhere the ability to detect feasible plans is much betterthan to detect optimal plans while DeepSeek performs theworst among them. For all models, when problems becomeharder, the ability to verify plans is still limited, remainingchallenge for further exploration.\nFrom Table 5, it is evident that Claude consistently out-performs other models across all evaluation settings. Astask difficulty increases, the performance of all modelsdeteriorates. However, while Claude experiences a rela-tively mild decline, GPT-4o and DeepSeek-Chat exhibitsteeper drops in performance, highlighting their greatersensitivity to task complexity. In contrast, the fine-tunedreward model LoRA underperforms across all configura-tions, further emphasizing the challenges faced by simpleror more narrowly tuned models in handling heuristic-basedevaluations.\nThe comparison between zero-shot and one-shot evalu-ations underscores the benefits of applying comparativeheuristic functions. When provided with a heuristic, lan-guage models are better guided in their reasoning, reducingthe likelihood of errors by narrowing their focus in align-ment with the heuristic's direction. Furthermore, the com-parison between the one-shot and reward model configura-tions demonstrates the effectiveness of heuristic guidance,"}, {"title": "Travel Planning", "content": "In Table ??, GPT-40 demonstrates a significant perfor-mance advantage over DeepSeek across both evaluationsettings. However, despite this relative superiority, neitherGPT-40 nor DeepSeek achieves results that can be deemedsatisfactory or robust for practical applications. Whenthe Chain-of-Thought (CoT) strategy is applied, GPT-40exhibits a modest improvement in performance, leverag-ing its step-by-step reasoning capability to better addressthe problem space. In contrast, DeepSeek's performancedeteriorates further, failing to produce any correct plansunder this strategy. Both models struggle with detectingimplicit constraints that are intuitive and readily under-stood by humans, and they consistently fail to adhere to allspecified constraints, underscoring the limitations of cur-rent methodologies in handling complex, constraint-richplanning tasks.\nIn Table 3, the performance of LLMs as verifiers on theTravel Planning dataset highlights significant limitations.GPT-40 achieves a pass rate of 0.4550, slightly underper-forming compared to DeepSeek, which records a pass rateof 0.4890. Despite these marginal differences, neithermodel demonstrates acceptable verification capabilities.The observed results fall short of expectations for practicalapplications, as the dataset is balanced, and the models'performance is barely above a random baseline.\nThe suboptimal outcomes suggest fundamental challengesin the models' ability to recognize and validate the complexconstraints inherent in travel planning tasks. This underscores the limitations of current LLM-based approaches in"}, {"title": "Discussion", "content": "This study investigated the performance of LLMs on plan-ning tasks across three distinct domains: travel planning,course planning, and fitness planning. A common threadacross these domains is the crucial role of constraint fol-lowing in generating successful solutions. This discussionwill delve into the observed limitations of LLMs in adher-ing to constraints and explore the surprising challengesencountered in verifying the generated plans."}, {"title": "Limitations on Constraint Following", "content": "Constraint satisfaction is paramount for effective problem-solving in the chosen benchmarks. Travel planning often"}, {"title": "The Unexpected Difficulty of Verification", "content": "Algorithmically, many problems, including those investi-gated in this study, are easier to verify than to solve. How-ever, our findings revealed a surprising difficulty for LLMs in verifying solutions. This was particularly evident in thecourse planning task, where LLMs struggled to determinewhether a generated plan adhered to time and capacityconstraints. The challenge was even more pronounced inthe travel planner dataset, where models failed to reliablyevaluate solution correctness and performed poorly evenwhen tasked with ranking two solutions.\nThe ability to automatically verify solutions is essential forreal-world applications. While tasks like course planningmay allow for straightforward implementation of verifiers,many real-life scenarios, such as travel planning, do not.When evaluation criteria are subjective and verification re-lies on conceptual understanding rather than purely mathe-matical checks (e.g., assessing adherence to commonsenseversus verifying time conflict absence), verification itselfbecomes a formidable challenge. This highlights the needfor research into robust and adaptable verification methods that can handle the nuances of real-world planningproblems."}, {"title": "Conclusion", "content": "This study offers a comprehensive evaluation of the roleof large language models (LLMs) in addressing planningproblems. We specifically investigate the effectiveness ofLLMs as planners, solution verifiers, and sources of heuris-tic feedback for refining intermediate solutions. Our find-ings indicate that while LLMs struggle to directly produceaccurate plans, they excel at generating comparative feed-back that can act as a heuristic function. This evaluationframework offers valuable guidance for the development offuture LLM-driven tree-search algorithms for a variety ofplanning and reasoning tasks. Furthermore, we introducea new benchmark for assessing LLMs' capacity to adaptto user preferences in real-time, a crucial capability withbroad practical implications."}]}