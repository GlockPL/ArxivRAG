{"title": "ABCI 3.0: EVOLUTION OF THE LEADING AI INFRASTRUCTURE\nIN JAPAN", "authors": ["Ryousei Takano", "Shinichiro Takizawa", "Yusuke Tanimura", "Hidemoto Nakada", "Hirotaka Ogawa"], "abstract": "ABCI 3.0 is the latest version of the ABCI, a large-scale open AI infrastructure that AIST has been\noperating since August 2018 and will be fully operational in January 2025. ABCI 3.0 consists of\ncomputing servers equipped with 6128 of the NVIDIA H200 GPUs and an all-flash storage system.\nIts peak performance is 6.22 exaflops in half precision and 3.0 exaflops in single precision, which\nis 7 to 13 times faster than the previous system, ABCI 2.0. It also more than doubles both stor-\nage capacity and theoretical read/write performance. ABCI 3.0 is expected to accelerate research\nand development, evaluation, and workforce development of cutting-edge AI technologies, with a\nparticular focus on generative AI.", "sections": [{"title": "Introduction", "content": "AI Bridging Cloud Infrastructure (ABCI) is the world's first large-scale open AI infrastructure, designed and developed\nby AIST with the aim of accelerating the development of AI technologies in Japan [1, 2, 3]. It has been installed in the\nAI Data Center located at AIST's Kashiwa Center, and began operating in August 2018. To date, many organizations\nhave achieved remarkable results by using ABCI, including the successful construction of Japanese large language\nmodels (LLMs) such as PLaMo [4], Swallow LLM [5], and so on. On the other hand, as the demand for generative A\u0399\nin industry, academia, and government in Japan is growing rapidly, ABCI cannot provide enough resources to meet\nsuch user demands in a timely manner. In addition, the development of generative AI is still at an early stage and\ncurrently focuses mainly on natural languages. In the near future, it is important to develop real world multimodal\nfoundation models that are built using large amounts of image, audio and sensor data obtained from the real world,\nsuch as manufacturing, transportation, and so on. In order to demonstrate such cutting-edge AI technologies, it is\nimperative to improve the computational capability of ABCI. Since November 2024, AIST has been gradually starting\nto operate the latest system in the ABCI series, ABCI 3.0, while leveraging the existing technical assets of ABCI.\nABCI 3.0 will be fully operational in January 2025.\nThis paper is organized as follows. Section 2 introduces the hardware configuration of ABCI 3.0. In Section 3, we\ndescribe the initial software installed. The data center facility where we have upgraded for ABCI 3.0 is presented in\nSection 4. Finally, Section 5 briefly mentions the future perspective."}, {"title": "ABCI 3.0 Hardware", "content": "Figure 1 shows the overview of ABCI 3.0 hardware configuration. The compute node consists of HPE Cray XD670, that is equipped with two Intel Xeon Platinum 8558 Processors\n(2.1GHz, 48core) and eight NVIDIA H200 SXM GPUs, 2048 GiB DDR5-5600 memory, two 7.68TB U.3 NVMe\nSSDs, eight InfiniBand NDR200 adapters, and an InfiniBand HDR adapter.\nThe interconnection network adopts a three-tier fat-tree topology, and all compute nodes are connected with full\nbisection bandwidth. Each compute node has eight InfiniBand NDR200 HBAs, and the total injection bandwidth is\n12 times larger than that of ABCI 2.0. Compute nodes and the storage system are connected via InfiniBand HDR\nnetwork.\nABCI 3.0 achieves a highly cost-effective all-flash storage system by adopting QLC (Quad-Level Cell, 4-bit MLC)\nSSDs. The storage system provides Lustre parallel file system and AWS S3 compatible object storage.\nABCI 3.0 also has Interactive Nodes and Gateway Nodes. Interactive nodes are servers to login and used to compile\nprograms and submit jobs to compute nodes. Gateway nodes are NAT servers that are located between compute nodes\nand the Internet. Compute nodes are not directly connected to the Internet, their access to the Internet is relayed\nthrough gateway nodes."}, {"title": "ABCI 3.0 Software", "content": "ABCI 3.0 provides a set of system software that makes maximum use of hardware resources and realizes advanced\ndevelopment of foundation models, and cloud operation. In order to facilitate easy migration from the\nprevious system, the usage model is basically the same, including operating system, job scheduler, a temporal shared\nfile system using local SSDs (BeeOND), and container support. A notable additional feasure to the previous system\nis Open OnDemand [6, 7] support, which provides users with a way to access computing resources via a web browser\ninstead of SSH.\nIn order to achieve high resource utilization, ABCI 3.0 uses Altair PBS Professional job scheduler for resource man-\nagement. Each compute node has eight GPUs, but some workloads may require fewer GPUs and/or CPU cores. ABCI\n3.0 allocates the same compute node to multiple jobs without resource oversubscription, with the job scheduler dy-\nnamically allocating a partial resources using cgroups and disk quota. Table 3 summarises ABCI 3.0 resource types.\nIt intends a single compute node (rt_HF) can be divided into eight rt_HG instances and two rt_HC instances.\nUnlike HPC, AI related software updates frequently, making it difficult for system operators to prepare all the neces-\nsary software in advance. Therefore, it is necessary for users to be able to freely customize their software environment.\nOne such mechanism is environment modules, and the other is containers. We provide several environment modules\nto enable users easily use various kinds and versions of software libraries, including CUDA, cuDNN, NCCL, MPI,\nand Python. Users can build their development environments by loading environment modules and then installing nec-\nessary Python libraries such as PyTorch, DeepSpeed, Hugging Face Hub by running pip, Anaconda, or similar tools.\nThere are several container runtime systems that integrates well with HPC environments. For example, Singularity,"}, {"title": "AI Data Center Facility", "content": "Figure 2 shows the overview of our AI data center facility that was built at the same time as ABCI 1.0. We have\nreported the detailed design of the original AI data center in [8]. For the installation of ABCI 3.0, we have upgraded\nthe electrical and cooling capacities. Currently, we have 6MW of electrical capacity, 5.2MW of cooling capacity, and\n144 racks of space."}, {"title": "Future Perspective", "content": "ABCI 1.0 and 2.0 are very successful systems for advancing AI R&D in Japan. However, in the era of generative\nAI, AI industry is changing rapidly, and the situation surrounding ABCI is completely different from when we started\nthe ABCI project. With government and other investment, the presence of domestic commercial cloud providers is\ngrowing. As a result of this situation, the role of ABCI must be shifted. ABCI 3.0 should focus on challenging research\nand development based on national demands, such as sovereign AI, and encouraging AI startups should also be an\nimportant mission. We provide large-scale computing resources needed for more challenging R&D by relaxing the\ncurrent usage limits, including the number of nodes per job, node-time product per job, maximum reserved duration,\nand etc. We also plan to conduct development acceleration programs and user events. The latest information is\navailable on the ABCI homepage [1]."}]}