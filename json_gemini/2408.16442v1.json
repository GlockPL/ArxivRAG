{"title": "Integrating Features for Recognizing Human Activities through Optimized Parameters in Graph Convolutional Networks and Transformer Architectures", "authors": ["Mohammad Belal", "Nael Alsheikh", "Taimur Hassan", "Noureldin Elhendawi", "Abdelfatah Hassan", "Irfan Hussain"], "abstract": "Abstract-Human activity recognition is a major field of study that employs computer vision, machine vision, and deep learning techniques to categorize human actions. The field of deep learning has made significant progress, with architectures that are extremely effective at capturing human dynamics. This study emphasizes the influence of feature fusion on the accuracy of activity recognition. This technique addresses the limitation of conventional models, which face difficulties in identifying activities because of their limited capacity to understand spatial and temporal features. The technique employs sensory data obtained from four publicly available datasets: HuGaDB, PKU-MMD, LARa, and TUG. The accuracy and F1-score of two deep learning models, specifically a Transformer model and a Parameter-Optimized Graph Convolutional Network (PO-GCN), were evaluated using these datasets. The feature fusion technique integrated the final layer features from both models and inputted them into a classifier. Empirical evidence demonstrates that PO-GCN outperforms standard models in activity recognition. HuGaDB demonstrated a 2.3% improvement in accuracy and a 2.2% increase in F1-score. TUG showed a 5% increase in accuracy and a 0.5% rise in F1-score. On the other hand, LARa and PKU-MMD achieved lower accuracies of 64% and 69% respectively. This indicates that the integration of features enhanced the performance of both the Transformer model and PO-GCN.", "sections": [{"title": "I. INTRODUCTION", "content": "Human activity recognition is an important component in the science of computer vision, as it is responsible for identifying and categorizing human actions collected on video. The goal of this field is to decode and learn human interactions in video sequences for a variety of applications ranging from surveillance systems to proficiency evaluations [1]. Particularly in the context of assistive robotic exoskeletons, this topic is useful in a variety of applications including video mate- rial categorization and enabling instantaneous and accurate changes in reaction to human motions [8] [2]. Another study focused on several strategies for human activity recognition, with deep learning-based approaches significantly improving the precision and efficacy of these recognition systems [3] [22]. Deep learning methods combined with sensor-derived data have produced complex models able to evaluate video-based human activities. These models have shown promise in en- hancing the interface between human movement and tech- nologies like exoskeletons, enabling exact and timely changes in reaction to human motions [29]. Minimizing latency in the connection between human motions and the associated adjustments in mechanical aids while maintaining accuracy and integrity, is an important area of research. Deep learning approaches have proved essential in resolving the challenges inherent in this element of human-robot interaction [7], [23], [28]."}, {"title": "II. RELATED WORK", "content": "A lot of study has lately been done on how to identify indi- viduals based on their movements. Using computer algorithms, several researchers\u2014including Mohsen\u2014have developed fresh approaches for estimating these activities, and the results show great promise [24]. Others used ready-made systems including ResNet50 and ViT, and they were practically always able to accurately spot activity [26]. Huang et al. [6] introduced a new method offering better knowledge of activities by concentrating on the movement sequence over time. Research on using several approaches in deep learning to learn from less data while nonetheless precisely identifying actions has also been done. Learning the spatial and temporal elements of the human body's movement. In [4] introduced a technique for separating activities into smaller pieces. This lets computers decide what individuals are doing more precisely. Moreover, in [5] a new approach to identify actions via feature diffusion is suggested, which uses a different approach to predict activities depending on video data. This approach works well for addressing problems of recognizing activities over time, especially in terms of knowledge of where the activity starts and finishes as well as the relationships between multiple acts. Deep learning models still have difficulties even if they have pushed limits in identifying human activity, they cannot completely grasp the space and timing of movements. This work makes important strides in overcoming these issues:\n\u2022\tDiverse data sources: We train and test two types of models: the PO-GCN and a Transformer using informa- tion from four distinct datasets.\n\u2022\tComparing models:Examining how the Transformer and PO-GCN perform on every dataset helps us to identify areas of strength and areas where each model may be improved.\n\u2022\tMerging features: Combining several types of data can help identify what activity someone is engaged in, there- fore facilitating more accurate and dependable systems, according to our researcher.\n\u2022\tTaking the advantages of two deep learning mod- els: We take advantage of the Transformer's ability to understand temporal patterns and the PO-GCN's ability in obtaining complex spatial and temporal features. This shows that the use of multiple models combined together produces better results."}, {"title": "III. PROPOSED METHOD", "content": "This section describes our proposed approach. We employed four different datasets-HuGaDB, PKU-MMD, LARa, and TUG-to develop two different models: the PO-GCN and a Transformer, with the purpose of detecting human activity. We retrieved features from the last layers of both models and combined them using concatenation. These merged features were then fed into a Fully Connected Network classifier for the final classification."}, {"title": "A. Parameter-Optimized Graph Convolutional Network", "content": "This part describes how the PO-GCN model, meant to find activities based on human skeletal structure, has shown good performance in several studies. Because the PO-GCN has been refined through parameter changes, its architecture is outstanding in capturing the temporal and spatial correla- tions present in human movement. Layers in every numerous stage of the model process and transmit data across the spatiotemporal graph of human skeletal movements using graph convolutional networks (GCN). It generates spatial and temporal convolutions at every stage. After that, the revised feature mappings move to the next stage; thereafter, a graph pooling phase reduces the size of the graph.\nCross-entropy loss (CE) and mean squared error (MSE) loss were the two loss functions we used in the training phase. Defined as the CE loss\u2014summed over all phases\u2014this is:\n$CE = \\sum_{stage=1}^{stage} CEstages,class$ \n$CEclass = - \\frac{1}{N} \\sum_n Yn,c log (\u0177n,c),$ \nwhere CE shows the total loss over all stages where $CEclass$ reflects the loss between the actual label ync and the predicted likelihood \u0177n,c for class (c) at sample (n). Usually applied for regression problems, the MSE loss comes from:\n$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (Yi-Yi)^2$\nwhere yi is the true value; N is the sample count; and \u0177\u2081 is the expected value for sample (i). These two losses taken together produce the loss function:\n$LTotal = \\sum_{stage=1}^{stage} CE + \u03c3MSE,$\nwhere LTotal is the overall combined loss; sigma (\u03c3) is the weight given to the MSE loss inside this combined function. Combining loss functions aims mostly to produce predictions with less over-recognition mistakes. The model is flexible enough for many applications of activity recognition since its last output assigns action labels to every frame in the sequence."}, {"title": "B. Transformer", "content": "We reported in this work a Transformer-based model for human activity recognition. The identical set of hyperparame- ters applied for the PO-GCN was applied to train this model. Making use of the Transformer design has several advantages. It is mostly better in extracting features since it can handle data inputs in their whole instead of in segments. The model is able to find complex patterns that it might miss if it only looked at individual parts because it looks at the whole picture. Moreover, transformers are quite famous for their capacity to precisely depict long-range dependencies in data. This shows how well the model understands the link between activities occurring at different times, which is crucial for correctly analyzing movement sequences concerning human activity detection. By considering these temporal connections, the Transformer model may more precisely project future actions or the continuation of an activity [27]."}, {"title": "C. Last Layer Feature Fusion", "content": "Feature fusion, sometimes referred to as late fusion, is a fun- damental method in deep learning whereby features from sev- eral models are combined to improve the general performance of the model [15]. Using two separate models-the Trans- former and the Parameter-Optimized GCN-activity recogni- tion helped to extract features from their last layers [16] [17]. The next method applied to mix the obtained features was concatenation. This approach can perhaps increase the general representational and predictive strength by combining the sev- eral insights acquired by every particular model. Fundamental ideas in deep learning and machine learning, feature fusion has been extensively investigated in academic publications, where two separate models were used in the framework of this work for feature extraction from their last layers: Trans- former and Parameter-Optimized GCN. Concatenation then helped to combine the retrieved features. This method helps to combine different and complementary insights gained by every particular model, hence perhaps increasing the general representational and predictive power. With much research in academic publications, feature fusion is a pillar in both the machine and deep learning fields. Researchers have proposed several fusion strategies including attentional feature fusion and guided training in an attempt to enhance classification task performance [18]."}, {"title": "D. Classifier", "content": "This research uses the fully connected network meant to analyze the fused feature set obtained from the combined outputs of the Transformer and Parameter-Optimized GCN models as a classifier [19] [20]. The architecture of this network consists of several layers cooperating to maximize classification performance. Its batch normalizing layer helps every neuron to standardize the inputs, so reducing internal covariate shift and improving neural network stability. Two dense layers follow from this: a final output layer producing the classification results and a flattening layer transforming the multi-dimensional input data features into a one-dimensional array so enabling the dense layers to handle it.\nDeep learning algorithm Adaptive Moment Estimation (Adam) is the highly esteemed optimizer used in the network [21]. Adam is well-known in real-time parameter changes that significantly raise the model's learning rate and accuracy. It comprises an adjustable learning rate that changes with learning progress and a momentum component pushing the optimizer in the proper direction. Consequently, the conver- gence toward the optimal parameter selection that reduces the loss function can happen faster.\nMost often used loss function in classification systems is cross-entropy (CE) loss. By means of a comparison between the predicted and actual probability distribution, this loss function measures the performance of the model, therefore rendering it suitable for classification problems. The degree to which the expected likelihood differs from the actual label determines how much the CE loss rises. By reducing the CE loss during training, the model teaches it to generate predictions that closely match the true labels and so improves classification accuracy."}, {"title": "IV. NETWORK SETUP", "content": ""}, {"title": "A. Public Datasets", "content": "1) Human Gait Database (HuGaDB): Human gait data is gathered in the HuGaDB collection and applied for activity recognition [9]. Among the several activities included in the collection are continuous recordings of sitting, running, walking, climbing and descending stairs. Apart from two elec- tromyography sensors tracking muscle activation, wearable inertial sensors on the thighs, shins, and feet were employed to gather data. This dataset simultaneously serves two purposes: it provides information on the relative movements of various leg components and aids in the identification of various activities by means of which they are executed.\n2) PKU-MMD: a vast action recognition resource with 1,076 lengthy video sequences in 51 action categories. 66 dif- ferent subjects participated in three separate camera positions to document these actions [11]. Comprising close to 20,000 action instances and 5.4 million frames, the PKU-MMD dataset Captured using the Kinect v2 sensor it offers multi- modal data streams including RGB, depth, infrared radiation, and skeletal data [11].\n3) Logistic Activity Recognition Challenge (LARa): First openly available dataset targeted on human activity recognition in logistics environments: LARa, logistic activity recognition challenge [10]. It developed under the Innovation Lab Hybrid Services in Logistics at TU Dortmund University. Recorded using motion capture, inertial measurement units, and RGB cameras, the dataset comprises recordings of 14 individuals engaged in selecting and packing jobs. The dataset consists in 758 minutes of well labeled records spanning 474 person-hours by 12 annotators. The data consists of eight activity classifications and nineteen binary semantic features [10].\n4) Timed Up and Go: A well-known assessment tool for measuring the mobility, balance, walking capacity, and fall risk of an older adult [12]. The patient gets out of a chair, moves three meters at a comfortable and safe pace, turns around, moves back to the chair, and then settles in to finish the test. Time it takes a patient to complete this activity will help doctors ascertain their functional mobility and fall risk. In the realm of geriatric care, the TUG test is a priceless tool since it provides vital information about the physical abilities and fall risk of an elderly person [12]."}, {"title": "B. Metrics", "content": "Using a range of conventional metrics, including F1-score and general accuracy, the suggested model's performance was evaluated. Particular application of the F1-score for segment- wise assessment using [13] where every anticipated action segment is categorized as either a true positive (TP) or a false positive (FP) [14].The F1 score computed follows:\n$F1 score = \\frac{TP}{TP +0.5(FN + FP)}$\nOne classification evaluation measure is accuracy [13]. Since accuracy is measured by the number of properly classified cases, it can be used to test classification tasks when the classes are balanced, which means that each has almost the same number of samples. It offers a direct evaluation of the efficacy of a model and is easily comprehensible. Accuracy is defined as:\n$Arr = \\frac{TP+TN}{TP+TN+FP+ FN}$\nThese metrics are for every action category, these measures were computed segmentally."}, {"title": "V. RESULTS & DISCUSSION", "content": "This work aims to evaluate and compare the performance of Transformer model and other top activity identification models with Parameter-Optimized GCN. Evaluating the robustness and efficacy of our proposed method was the aim. This study aims to improve the awareness of the advantages and drawbacks inherent in every model by way of a comparison examination."}, {"title": "VI. CONCLUSION", "content": "This work presents a novel approach for recognizing human actions by means of data from four separate sources. We thus applied two models: PO-GCN and Transformer model. Following considerable training and evaluation on the datasets, each model's efficacy was assessed using accuracy and F1- score measures. We used the unique features of transformers and graph convolutional networks by means of a deep learning method termed feature fusion. This approach aggregates last layer characteristics from both models before feeding them into a classifier, hence enhancing performance. Regarding ac- tivity recognition, the PO-GCN beats other models according to the data. Furthermore, in three of the datasets the feature fusion method exceeded the stand-alone PO-GCN based on recognition rates. Together in this approach, the Transformer's capacity to process long-term patterns and the sensitivity of the PO-GCN to temporal and spatial details complement each other."}]}