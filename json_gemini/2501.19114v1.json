{"title": "Principal Components for Neural Network Initialization", "authors": ["Nhan Phan", "Thu Nguyen", "P\u00e5l Halvorsen", "Michael A. Riegler"], "abstract": "Principal Component Analysis (PCA) is a commonly used tool for dimension reduction and denoising. Therefore, it is also widely used on the data prior to training a neural network. However, this approach can complicate the explanation of explainable AI (XAI) methods for the decision of the model. In this work, we analyze the potential issues with this approach and propose Principal Components-based Initialization (PCsInit), a strategy to incorporate PCA into the first layer of a neural network via initialization of the first layer in the network with the principal components, and its two variants PCsInit-Act and PCsInit-Sub. Explanations using these strategies are as direct and straightforward as for neural networks and are simpler than using PCA prior to training a neural network on the principal components. Moreover, as will be illustrated in the experiments, such training strategies can also allow further improvement of training via backpropagation.", "sections": [{"title": "I. INTRODUCTION", "content": "Principal Component Analysis (PCA) is a widely used dimensionality reduction technique that transforms high- dimensional data into a lower-dimensional space while pre- serving as much variance as possible. By identifying the principal components, which are the orthogonal directions that capture the most variance in the data, PCA helps to eliminate redundancy, improve computational efficiency, and mitigate the effects of noise. It achieves this through eigenvalue decom- position of the covariance matrix or singular value decompo- sition (SVD) of the data matrix. PCA is extensively applied in fields such as machine learning, image processing, and bioinformatics to uncover patterns, visualize high-dimensional data, and enhance model performance by reducing overfitting.\nIn the context of neural network training, PCA offers sig- nificant advantages by decorrelating the input features, which in turn leads to a better-conditioned Hessian matrix. This is crucial because a poorly conditioned Hessian, often caused by correlated features, can result in slow convergence during gradient-based optimization, leading to inefficiencies in train- ing [1]. By transforming the input data into a set of orthogonal principal components, PCA ensures that gradient updates during backpropagation are more stable and effective, reducing the likelihood of oscillations or excessive adjustments along certain directions [2]. As a result, the optimization process becomes more efficient, with faster convergence toward the optimal solution. This makes PCA a valuable preprocessing technique for reducing the computational cost and enhancing the performance of neural networks, particularly in high- dimensional settings where the curse of dimensionality and correlated features are common [3]. Consequently, incorpo- rating PCA in the training pipeline can lead to significant improvements in both speed and accuracy for models trained on complex, high-dimensional datasets [4]. Therefore, it is common to train neural networks on the principal compo- nents (PCA-NN).\nHowever, this approach also has some limitations. First, the dimensionality reduction performed by PCA leads to a loss of information, meaning the back-projected gradients in the original input space may not fully capture the network's behavior. Second, the principal components are linear combi- nations of original features, which may make the resulting features harder to interpret compared to the original data. So, while one can use the PCA loading matrix to transform back to the original features requires using the PCA loading matrix, which defines the contribution of each original feature to each principal component, applying PCA prior to training a neural network as in PCA-NN complicates the explanation process due to the further transformation step in PCA. There- fore, we introduce Principal Components based Initialization (PCsInit), a novel initialization technique for the first layer of a neural network based on the principal components. In addition, we also introduce PCsInit-Act, which applies an activation layer after the principal components to increase the neural network ability of nonlinear patterns. Moreover, we also introduce PCsInit-Sub, another variant of PCsInit that compute the principal components based on the subset of the input to increase the computational efficiency for large datasets. Note that these approaches are different from Neural PCA (Principal Component Analysis), which refers to using neural networks to approximate Principal Component Analysis (PCA), a technique used for dimensionality reduction and feature extraction in machine learning.\nIn summary, our main contributions are as follows:\n\u2022 We introduce PCsInit, a novel method for incorporating PCA into neural networks through weight initialization"}, {"title": "II. MOTIVATION: EXPLAINABILITY WHEN PCA IS APPLIED BEFORE A NEURAL NETWORK", "content": "When PCA is applied before a neural network (PCA-NN), perturbation-based XAI methods, such as LIME [5], Occlusion [6], and Feature Permutation [7], are heavily impacted because they rely on modifying input features and observing their effect on predictions. This is because perturbing the input features instead of the principal components directly affects the SVD of the input or the covariance matrix of the input and principal components are linear combinations of multiple features. On the other hand, if perturbations are applied to principal components, the results are meaningful only in the transformed PCA space. Mapping the effects back to the original features requires using the PCA loading matrix, which defines the contribution of each original feature to each principal component. By redistributing the changes in principal components proportionally to their feature weights in the loading matrix, approximate feature-level insights can be obtained. However, this process introduces inaccuracies because the dimensionality reduction step in PCA discards some variance, and the transformation is not fully invertible. Overall, perturbation-based XAI methods are inherently less interpretable in PCA-NN pipelines because the transformations obscure the direct relationship between input features and predictions.\nNext, let us consider gradient-based XAI methods. In a PCA-NN pipeline, the original input data X is transformed into a lower-dimensional representation $Z = W^T(X \u2013 \\mu)$, where W is the PCA projection matrix and u is the mean of the input data, before being passed to the neural network. Gradient-based visualization methods such as saliency maps [8] and Grad-CAM [9], which rely on backpropagating gra- dients from the network's output to its input, can still be applied in this setup by mapping the gradients from the PCA- transformed space Z back to the original input space X.\nThis mapping can be achieved using the chain rule, where $\\frac{\\partial \\text{output}}{\\partial X} = \\frac{\\partial \\text{output}}{\\partial Z} \\frac{\\partial Z}{\\partial X}$, and for PCA, the Jacobian $\\frac{\\partial Z}{\\partial X} = W^T$.\nConsequently, gradients computed in the PCA space can be back-projected into the original input space to enable visual- ization. However, PCA may remove or distort fine-grained input features that are crucial for effective gradient-based visualization, potentially reducing the interpretability and quality of methods like saliency maps or SmoothGrad.\nNext, while feature attribution methods like SHAP [10] and LIME [5] can still explain predictions in the PCA-transformed space, mapping these explanations back to the original feature space for explanation of the input requires leveraging the PCA loading matrix and comes with the limitation of ap- proximation. To be more detailed, SHAP explains predictions by calculating feature contributions based on Shapley values.\nIn a PCA-NN setup, SHAP can be applied directly to the PCA-transformed features Z. To attribute importance to the original input features X, the contributions to Z must be back-projected to X using the PCA matrix W. However, W may not be an invertible matrix. Therefore, the explanation is not complete, but rather an approximation. A similar thing happens to LIME. In summary, these methods can attribute importance in PCA-NN systems, but additional steps are required to map contributions from the reduced-dimensional PCA space back to the original input space, and the quality of these attributions depends on how much information is preserved by PCA.\nOn the other hand, if we can put PCA inside the neural network via initialization based on the similarity between the multiplication in PCA and the multiplication between neural network input and weight matrix, the explanation using XAI methods will be the same as for a regular neural network without applying PCA."}, {"title": "III. METHODOLOGIES", "content": "In this section, we will introduce motivation and the formal training process of Principal Components Analysis Initializa- tion (PCsInit), its two variations (PCsInit-Act, PCsInit-Sub), and relevant theoretical properties."}, {"title": "A. Principal Components-based Initialization (PCsInit) - Basic Ideas.", "content": "This section details our proposed PCsInit framework. While PCA preprocessing requires maintaining and applying the transformation matrix during inference, PCsInit incorporates this information directly into the network weights, reducing operational complexity. Additionally, PCsInit allows the net- work to adaptively refine these initialized weights during train- ing, potentially capturing more nuanced feature relationships that static PCA transformation might miss. Most importantly, PCsInit simplifies the explainability pipeline by eliminating the need to back-project through a separate PCA transforma- tion step.\nThe basic idea of our proposed PCsInit approach using the PCA and the neural network can be described as follows. First, let $X = [x_{ij}]$, where $i = 1, ..., n; j = 1, ..., p$, be an input data matrix of n observations and p features. Assume that the features are centered and scaled. Then, recall that"}, {"title": "B. Principal Components Analysis Initialization (PCsInit)", "content": "Formally, the training process of PCsInit is described in Algorithm 1. In the first step, PCA is applied to the entire input dataset X to extract the top r principal components, which are stored in the projection matrix $W_r$. These principal components represent the directions of maximum variance in the data, allowing for an efficient lower-dimensional represen- tation while preserving critical information.\nIn the second step, the matrix $W_r$ is used as the weight matrix for the first layer of a neural network f. This initial- ization ensures that the first layer performs a transformation aligned with the most informative features of the input data, providing a strong inductive bias that can facilitate learning.\nAt the third step, to stabilize training and allow the deeper layers to adapt to the PCA-based initialization, the first layer is frozen, meaning its weights remain unchanged, while the remaining layers of the network are trained for $n_{frozen}$ epochs. This prevents drastic weight updates in the first layer, ensuring that the extracted principal components provide a meaningful starting point for training.\nAt the fourth step, after the deeper layers have sufficiently adapted, the first layer is unfrozen, allowing the entire neural network to be trained jointly for an additional n epochs. This fine-tuning step enables all layers, including the first one, to update their parameters in a coordinated manner, optimizing the overall network performance and leading to better feature extraction and representation learning.\nPCsInit versus PCA prior to neural network. The goal of Xw is to find the principal directions (eigenvectors of the covariance matrix) that capture the most variance in the data, while for neural networks, the weight matrix W is learned through backpropagation to minimize a task-specific loss func- tion. In addition, $w_r$ (the principal component directions) are computed directly from the data (using eigendecomposition or SVD) and are fixed once determined. Meanwhile, PCsInit for neural networks allows W it to be adjusted iteratively during training for possible room for improvement.\nDespite the differences, note that for the PCsInit approach, if the weights of the first layer are frozen throughout the training process then it is equivalent to applying PCA to the input data and then training a model on the principal components. Even in this case, using PCA in the PCsInit manner with the first layer frozen during training makes it easier to explain the model. Specifically, a neural network built upon principal components makes decisions based on a transformed feature space where the most significant variations in the data are captured. Since PCA reduces dimensionality by keeping only the most informative features, the network learns patterns in terms of these principal components rather than the original raw features. The decision can be explained by analyzing which principal components contributed most to the output, mapping them back to the original features, and using techniques like SHAP or sensitivity analysis. Meanwhile, PCsInit allows using SHAP and other XAI techniques directly on the input. Therefore, PCsInit offers a more straightforward explanation than explaining the decision of a neural network trained on principal components.\nHowever, sometimes computing the principal components based on the whole input matrix can induce computational cost. Meanwhile, if the first layer is not completely frozen throughout training, it will be fine-tuned later. Therefore, it may be more computationally efficient to compute the princi- pal components based on a subset of the input matrix instead. This is the motivation for Algorithm 2, which is a slight"}, {"title": "C. Theoretical analysis", "content": "PCA transforms the data into uncorrelated features. There- fore, the Hessian matrix (second derivative of the loss) be- comes more well-conditioned after PCA, making optimization more stable.\nTheorem 1: Consider training a single-layer neural network for a linear regression problem, where we have the input matrix X in $R^{n\u00d7d}$, the label \u0176 in $R^{n\u00d71}$, and we are using the weight matrix W in $R^{d\u00d71}$. Then, PCA removes small eigenvalues of the Hessian matrix of the MSE loss function, leading to faster convergence.\nProof 1: It is known that the steepest descent method for nonlinear optimization is highly sensitive to scaling and the problem's condition number and that convergence can be significantly slow when the condition number of the Hessian matrix is large [11].\nFor a linear regression problem with single-layer neural network, input matrix X, the label \u0176, and the weight matrix W, the Mean Squared Error (MSE) loss function is\n$\\frac{1}{2}||XW - Y||^2$.\nTherefore, the gradient of the loss function with respect to W is:\n$\\triangledown L(W) = X^T(XW \u2013 Y)$,\nand the second derivative (Hessian) is\n$H = \\triangledown^2 L(W) = X^T X$\nThus, the Hessian depends on the structure of $X^T X$. Its eigenvalues determine how efficiently gradient descent con- verges. The condition number of the Hessian matrix is:\n$\\kappa(H) = \\frac{\\lambda_{max}}{\\lambda_{min}}$\nwhere $\\lambda_{max}$ and $\\lambda_{min}$ are the largest and smallest eigenval- ues of $X^T X$.\nWhen input features are highly correlated, some eigenvalues of $X^T X$ become very small, making \u03ba(H) large and slowing optimization.\nPCA transforms the input X into uncorrelated principal components Z using an orthonormal transformation matrix V:\n$Z = XV$\nSince V is an orthonormal matrix (i.e., $V^T V = I$), the covariance of Z is:\n$\\text{Cov}(Z) = V^T X^T X V$\nBy construction, PCA diagonalizes $X^T X$, meaning the Hessian in the transformed space becomes:\n$H' = Z^T Z = A$\nwhere A is a diagonal matrix containing the eigenvalues of $X^T X$.\nSince PCA removes the smallest eigenvalues (low-variance directions), the condition number of H' is lower, leading to faster convergence."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we will illustrate the performance of PCsInit and its variations compared to training neural network based on principal components (PCA-NN), and the performance of neural network without PCA (NN).\nTo illustrate the effectiveness of the proposed initialization strategies, we conduct various experiments on various datasets. The descriptions of the datasets used are as in Table I.\nFor each experiment, we choose the number of principal components so that a minimum of 95% of variance is retained, as commonly used in various research articles [12]\u2013[15]. The width of the network used for each dataset has a width equal to the number of principal components retained. All models are trained with a total of 200 epochs using Cross Entropy Loss and Adam optimizer with a learning rate of 0.01. The ReLU activation function is applied to all layers except the last layer. The number of layers used for all datasets is 5.\nIn PCsInit and its variations, the first layer is initialized with the principal components, and then it is frozen for the first 30 epochs. After that, we unfreeze this first layer and train the whole model for 170 more epochs. To facilitate a fair comparison, for all strategies, all layers are initialized with the same weights, except for the first layer, since PCsInit and its variants are initialized based on principal components, while PC-NN and NN are not. We compared each dataset with three different initialized techniques: He initialization, Xavier initialization, and Orthogonal initialization, and report the performance.\nThe experiments run on a CPU of AMD Ryzen 3 3100 4- Core Processor, installed RAM 16.0 GB. The codes for the experiments is available at github.com/pthnhan/pcsinit.\nThe five datasets we evaluated are all of which involve binary classification and are categorized into two primary groups based on the number of features. The first group includes datasets with a smaller number of features such as Heart and Ionosphere. The second group consists of datasets with a larger number of features: MicroMass and Parkinson.\nOur experiments cover a diverse set of datasets, ranging from datasets with few features to those where the number of features exceeds the number of samples (Micromass dataset). The experiments on these datasets illustrate the outperfor- mance results of PCsInit, PCsInit-Act, PCsInit-sub, and PCA- NN compared to NN. Moreover, PCsInit and its variations also show excellent results in various data sets under three initialization techniques with better results compared to PCA- NN in both testing accuracy and convergence speed.\nThe experimental results demonstrate that PCsInit variations and PCA-NN outperform the standard NN initialization across most datasets and initialization techniques. In many cases, such as the Heart dataset, PCsInit consistently outperforms PCA-NN across all initialization techniques (figures 1, 2, 3), highlighting its adaptability and efficiency in optimizing the network's performance.\nIn some cases, PCsInit shows performance comparable to PCA-NN, particularly evident in the ionosphere dataset, where both methods achieve similar levels of accuracy and training loss stability. On the other hand, PCA-NN shows its strength on the micromass dataset, achieving higher accuracy compared to PCsInit. Notably, PCsInit-Sub on this dataset exhibits competitive performance with PCA-NN, demonstrating that it can achieve similar results while reducing computational costs. These findings underscore that while both PCsInit and PCA-NN provide substantial improvements over NN, the variations of PCsInit, such as PCsInit-Sub and PCsInit-Act, offer additional flexibility and adaptability, enabling robust performance across a broader range of scenarios.\nThe results demonstrate that PCsInit-Sub achieves com- petitive performance compared to the full PCsInit method, while significantly reducing computational overhead. Across all datasets, PCsInit-Sub consistently delivers similar levels of accuracy and training loss stability, closely mirroring the trends observed in PCsInit. For example, on the Heart dataset, PCsInit-Sub performs better under He initialization (figure 1), slightly worse under Orthogonal initialization (figure 3), and achieves the same performance under Xavier initialization (2). On the Ionosphere dataset, PCsInit-Sub demonstrates mostly the same performance across all initialization techniques (figures 4, 5, and 6), maintaining its robustness. Additionally, on the Micromass dataset, PCsInit-Sub exhibits more stable and better performance under He initialization (figure 7), while on the Parkinson dataset, it outperforms PCsInit under Xavier initialization (figure 11). This ability to maintain high performance, stability, and reduced computational speed as indicated in figures 13 and 14 highlights the efficiency of PCsInit-Sub, making it an appealing alternative for scenarios with large datasets or limited resources.\nNext, the results also show that PCsInit-Act many times can be a more stable and competitive variation compared to the standard PCsInit, with slight improvements observed on the Ionosphere and Micromass datasets across all ini- tialization techniques (figures 4, 5, 6, 7, 8, and 9). This highlights its ability to adapt effectively to varying data structures while maintaining consistent performance. Notably, PCsInit-Act demonstrates exceptional results under the Xavier initialization technique for other layers, emerging as the best-performing approach on three out of the four datasets: Heart, Ionosphere, and Micromass (figures 2, 5, and 8). This consistent superiority under Xavier initialization underscores the robustness of PCsInit-Act in leveraging the initialization"}, {"title": "V. CONCLUSION", "content": "In conclusion, in this work, we propose PCsInit, which initializes the first layer of a neural network with the principal components obtained from the input data and illustrates its advantages compared to training a neural network on principal components. In general, PCsInit and its variations (PCsInit- Sub and PCsInit-Act) not only provide a more direct and easy way to explain the decision of a model but also outperform PCA-NN and NN across various datasets and initialization techniques. We also show theoretically and experimentally that PCsInit and its variants converge faster than NN.\nIn the future, we will examine the performance of the pro- posed approaches under various neural network architectures and various types of data (noisy, imbalanced). In addition, we will explore the potential usage of Kernel PCA instead of PCA for initializing neural networks."}]}