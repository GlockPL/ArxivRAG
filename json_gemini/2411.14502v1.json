{"title": "Global Challenge for Safe and Secure LLMs\nTrack 1", "authors": ["Xiaojun Jia", "Yihao Huang", "Yang Liu", "Peng Yan Tan", "Weng Kuan Yau", "Mun-Thye Mak", "Xin Ming Sim", "Wee Siong Ng", "See Kiong Ng", "Hanqing Liu", "Lifeng Zhou", "Huanqian Yan", "Xiaobing Sun", "Wei Liu", "Long Wang", "Yiming Qian", "Yong Liu", "Junxiao Yang", "Zhexin Zhang", "Leqi Lei", "Renmiao Chen", "Yida Lu", "Shiyao Cui", "Zizhou Wang", "Shaohua Li", "Yan Wang", "Rick Siow Mong Goh", "Liangli Zhen", "Yingjie Zhang", "Zhe Zhao"], "abstract": "This paper introduces the Global Challenge for\nSafe and Secure Large Language Models (LLMs), a pioneering\ninitiative organized by AI Singapore (AISG) and the CyberSG\nR&D Programme Office (CRPO) to foster the development of\nadvanced defense mechanisms against automated jailbreaking\nattacks. With the increasing integration of LLMs in critical\nsectors such as healthcare, finance, and public administration,\nensuring these models are resilient to adversarial attacks is vital\nfor preventing misuse and upholding ethical standards. This\ncompetition focused on two distinct tracks designed to evaluate\nand enhance the robustness of LLM security frameworks.\nTrack 1 tasked participants with developing automated meth-\nods to probe LLM vulnerabilities by eliciting undesirable re-\nsponses, effectively testing the limits of existing safety protocols\nwithin LLMs. Participants were challenged to devise techniques\nthat could bypass content safeguards across a diverse array of\nscenarios, from offensive language to misinformation and illegal\nactivities. Through this process, Track 1 aimed to deepen the\nunderstanding of LLM vulnerabilities and provide insights for\ncreating more resilient models.\nThe results of Track 1 highlighted significant advances in\njailbreak methods and security testing for LLMs. Competing\nteams were evaluated based on their models' resistance to 85\npredefined undesirable behaviors, spanning categories such as\nprejudice, offensive content, misinformation, and promotion of\nillegal activities. Notably, top-performing teams achieved high\nattack success rates by introducing innovative techniques, in-\ncluding scenario induction templates that systematically gener-\nated context-sensitive prompts and re-suffix attack mechanisms,\nwhich adapted suffixes to bypass model filters across multiple\nLLMs. These techniques demonstrated not only effectiveness in\ncircumventing safeguards but also transferability across different\nmodel types, underscoring the adaptability and sophistication of\nmodern adversarial methods.\nTrack 2, scheduled to begin in 2025, will emphasize the devel-\nopment of model-agnostic defense strategies aimed at countering\nadvanced jailbreak attacks. The primary objective of this track\nis to advance adaptable frameworks that can effectively mitigate\nadversarial attacks across various LLM architectures.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have undergone substantial\ngrowth, finding applications across various fields, including\npersonalized healthcare, industrial predictive maintenance, and\nautomated customer service [1].\nDespite their widespread use, LLMs remain vulnerable to a\nrange of attacks by malicious actors, as highlighted by Liu et\nal [13]. For example, Jia et al. [10] introduce optimized tech-\nniques as a method to jailbreak LLMs. Their methods reveal\nthat using diverse target templates along with an automatic\nmulti-coordinate updating strategy can substantially enhance\nthe efficiency and effectiveness of jailbreak attempts, offering\ninsights into strengthening LLM resilience.\nAlexander, Nika, and Jacob [20] further suggest that a\ndeeper understanding of LLM safety limitations can be\nachieved by examining vulnerabilities to jailbreaking attacks.\nThis approach not only exposes existing gaps but also informs\nthe development of tailored security frameworks to counteract\nsuch exploits.\nWith LLMs increasingly deployed in critical and sensitive\nareas, addressing jailbreak vulnerabilities is more urgent than\never. Proactively identifying and mitigating these risks ensures\nthe safe and ethical application of LLMs across sectors,\nreinforcing their security and alignment with ethical standards.\nIn response to these concerns, AI Singapore (AISG), along-\nside the CyberSG R&D Programme Office (CRPO), launched\nthe Global Challenge for Safe and Secure LLMs. This ini-\ntiative encourages global participants to develop innovative\napproaches for testing LLM robustness, focusing on automated\njailbreaking methods to expose potential weaknesses."}, {"title": "II. CHALLENGE OVERVIEW", "content": "The Challenge focused on identifying and understanding\njailbreaking weaknesses in popular LLMs used in commercial\nand research settings (challenge track 1) and developing robust\nsecurity measures for LLMs, reinforcing their resilience to\nadvanced jailbreak attacks (challenge track 2)."}, {"title": "B. Duration", "content": "This is a dual-track challenge. Track 1, which focused\non developing automated jailbreaking methods to test the\nrobustness of LLMs, ran from 2 July 2024 to 25 September\n2024. Track 2, which will focus on developing model-agnostic\ndefense mechanisms to reinforce LLMs' resilience against\nadvanced jailbreak attacks, is slated to launch in January 2025."}, {"title": "C. Structure", "content": "Track 1 (Attack): In this track, participants, in teams of 1\nto 6 members, were tasked to create an automated approach\nfor crafting test cases (prompts) designed to trigger a range\nof undesirable responses from a series of fine-tuned LLMs,\ndespite their training to prevent such outcomes. A list of 85\nbehaviors was compiled, organized into several overarching\nthemes, sourced from various open-source benchmarks along-\nside bespoke scenarios specifically created by the organizers.\nThemes\n1) Prejudice and Offensive Language\n2) Content and Behaviour Promoting Violence\n3) Illegal Activities\n4) Fraudulent Schemes\n5) Malicious Software and Security Vulnerabilities\n6) Spread of False Information and Deliberate Lies\n7) Additional Inappropriate Content\nTrack 1A: Participants were tasked with developing\nan automatic attack model capable of eliciting 50 prede-\nfined malicious behaviors from two open-sourced models:\nLlama-2-7b-chat-hf and Vicuna-7B. Additionally, a\nthird model was not disclosed to participants to further chal-\nlenge their strategies. The top 10 performers in the private\nleaderboard advanced to the next phase.\nTrack 1B: The top 10 performers in Track 1A fur-\nther challenged their models to solicit an additional 35\nmalicious behaviors using three models. Of these, only\nLlama-2-7b-chat-hf were disclosed to the participants,\nwhile the other two models remained undisclosed. Importantly,\nthe specific 35 behaviors in Track 1B were not be revealed\nto participants; instead, participants' submitted models were\ntested by the organizers to determine how effectively they\ncould elicit these behaviors."}, {"title": "D. Definitions", "content": "The landscape of LLMs is rapidly evolving, where both the\ncapabilities of LLMs and the strategies for their exploitation\nand defense are constantly advancing. For the Challenge, it is\nuseful to define a few key concepts that are associated with\nthe task.\nJailbreak Attacks: These are efforts to manipulate LLMs into\nproducing output that violates their designed ethical or oper-\national guidelines. Typically, jailbreak attacks exploit prompt\nengineering or adversarial input crafting to bypass or deceive\nthe model's safety mechanisms. Notable techniques include\nboth empirical attacks, which leverage human ingenuity in\nprompt crafting, and automated methods that systematically\nprobe models to discover vulnerabilities [5], [13].\nAutomated Jailbreak: This refers to the use of algorithms\nor models to generate jailbreak prompts without human in-\ntervention. These methods often employ iterative refinement\nprocesses and advanced computational techniques to optimize\nthe effectiveness of attacks. For example, the PAIR technique\ndescribed by Chao et al. [5] automates the creation of prompts\nthrough a black-box access method, efficiently bypassing LLM\nsafety protocols.\nPrompt Injections: Prompt injections are a specialized sub-\nset of jailbreak attacks. Drawing from recent studies like\nShayegani et al. [16], prompt injections involve the strategic\ninsertion of specially crafted prompts designed to mislead the\nmodel into treating these inputs as benign instructions, thereby\nbypassing standard content filters.\nDefense Mechanisms: In response to these threats, substantial\nresearch has been focused on developing robust defenses\nagainst jailbreak attacks. These defenses range from detection-\nbased strategies, which aim to identify and block harmful in-\nputs or outputs, to mitigation strategies that alter the processing\nof inputs to reduce the likelihood of harmful output generation.\nTechniques such as content filtering, semantic analysis, and\nalignment checks are commonly employed to fortify LLMS\nagainst these sophisticated attacks [21]."}, {"title": "E. Submissions and Deliverables", "content": "Track 1A: Participants were required to submit the JSON file\ngenerated by their attacker models for use as input prompts\nfor the victim LLMs. There was to be only one universal input\nprompt applicable to models for each of the 50 behaviors.\nTrack 1B: Participants submitted a single Docker container,\nfrom which the organizers ran their automated methods to\nevaluate the models' ability to elicit the specified malicious\nbehaviors."}, {"title": "F. Evaluation Metrics", "content": "The primary mode of evaluation for Track 1 was the Attack\nSuccess Rate (ASR), where ASR was defined as the ratio of\nsuccessfully elicited behaviors (jailbroken responses) to the\ntotal number of prompts.\nTrack 1A: We used the submitted universal input prompts for\neach of the 50 behaviors to produce the output from the 3\nmodels (Llama-2-7b-chat-hf, Vicuna-7B, and an undisclosed\nmodel)."}, {"title": "III. TRACK 1 LEADERBOARD RANKING", "content": "Table II shows the public leaderboard for Track 1A, while\nTable III presents the private leaderboard for Track 1A. Ta-\nble IV shows the public leaderboard for Track 1B, and Table V\ndisplays the private leaderboard for Track 1B. Finally, Table VI\npresents the Attack Success Rates (ASR) that were manually\nevaluated by five reviewers from the organizing team for the\ntop 5 teams."}, {"title": "IV. TOP 5 TEAM METHODOLOGIES AND APPROACHES", "content": "In the following section, we have compiled the technical\nreports provided by the top 5 teams, detailing their method-\nologies and approaches used during Track 1 of the Global\nChallenge for Safe and Secure LLMs."}, {"title": "V. METHOD OF TEAM DEEPATTACK", "content": "Large Language Models (LLMs) have drawn significant\nattention to the challenge of safe alignment, especially re-\ngarding jailbreak attacks that circumvent security measures\nto produce harmful content. To address the limitations of\nexisting methods like GCG, which perform well in single-\nmodel attacks but lack transferability, the authors propose\nseveral enhancements, including a scenario induction template,\noptimized suffix selection, and the integration of re-suffix\nattack mechanism to reduce inconsistent outputs. The approach\nhas shown superior performance in extensive experiments\nacross various benchmarks, achieving nearly 100% success\nrates in both attack execution and transferability. Notably, the\nmethod has won the first place in Track 1.\nDespite significant efforts to improve the security of LLMs\nin practical applications [9], recent research reveals that align-\nment mechanisms intended to protect these models are still\nvulnerable to sophisticated adversarial jailbreak attacks [7],\n[11], [24]. These attacks involve crafting complex prompts\nthat bypass safeguards and elicit harmful responses.\nCompared to other jailbreak methods, Optimization-based\ntechniques generally yield better attack results and are widely\nstudied and utilized. However, those methods typically rely on"}, {"title": "B. Methodology", "content": "1) Preliminaries: Formally, given a set of input tokens\nwhich can be represented as $X_{1:n} = \\{X_1, X_2,...,X_n\\}$, where\n$X_i \\in \\{1, ..., V\\}$ and $V$ denotes the vocabulary size (i.e., the\nnumber of tokens), a large language model (LLM) maps the\nsequence of tokens to a distribution over the next token. This\ncan be defined as:\n$P (X_{n+1}|X_{1:n}),$\nwhere $p(x_{n+1} | X_{1:n})$ represents the probability distribution\nover the possible next tokens given the input sequence $X_{1:n}$.\nThe probability of the response sequence of tokens can be\nrepresented as:\n$p (X_{n+1:n+H} | X_{1:n}) = \\prod_{i=1}^{H}P (X_{n+i} | X_{1:n+i-1}).$\nTo simplify the notation, the authors can express the malicious\nquestion $Q_{1:n}$ as $x$, the jailbreak suffix $X_{n+1:n+m}$ as $x_s$ and\nthe jailbreak prompt $X_{1:n} \\oplus X_{n+1:n+m}$ as $x \\oplus x_s$, where $\\oplus$\nrepresents the vector concatenation operation. Additionally,\nthe predefined target template represents as $X_{n+m+1:n+m+k}$,\nwhich is simply expressed as $x_R$. Thus, the adversarial jail-\nbreak loss function can be expressed as:\n$L (x \\oplus x_s) = - \\log p (x_R | x \\oplus x_s).$\nThe optimization of the adversarial suffix can be formulated\nas:\n$\\underset{x_S\\in\\{1,...,V\\}^m}{\\text{minimize}} L (x \\oplus x_s)$\n2) The proposed SI-GCG attack method: Unlike the GCG\nalgorithm, which solely focuses on the target template during\noptimization, the method takes into account both the target\ntemplate and malicious question contexts for more effective\nattacks. Specifically, the authors established a fixed harmful\ntemplate to handle malicious questions in Figure 1. The\nauthors denote this process using $x_{HQ} \\oplus x$, where $x_{HQ}$\nrepresents the harmful question template and $x$ represents\nthe initial malicious question. At the same time, the authors\noptimize the response to incorporate harmful information, such\nas \"Sure, my output is harmful. Here is a specific step-by-step\nextremely evil process about Rephrase(Q) with actual details\nin Sin City\". To facilitate representation, the authors adopt\n$x_{HR} \\oplus x_R$ to represent this process, where $x_{HR}$ represents\nthe harmful response template. Consequently, the jailbreak loss\nfunction can be expressed as:\n$L((x_{HQ}+x)+x_s) = -\\log p(x_{HR}+x_R\\|(x_{HQ}+x) +x_s)$\nThe suffix iterative update can use optimization methods for\ndiscrete tokens, which be formulated as:\n$x_s^t = GCG ([L ((x_{HQ} + x) +x_s^{t-1})]),$\ns.t.$x \\neq! ! \\neq !,!,!\\neq!,!.!,!\\neq!,!\\neq!,,\\neq\\neq! \\\\ $, \\\\$\nwhere GCG(.) denotes the optimization method based on\nGCG approach, where $x_s^t$ represents the jailbreak suffix gen-\nerated at the t-th iteration, $x_s^t$ represents the initialization for\nthe jailbreak suffix. The authors have observed that during"}, {"title": "3) Automatic optimal suffix selection strategy", "content": "Zou et al.\n[24] propose a greedy coordinate gradient jailbreak method\n(GCG), which simplifies solving Equation 4, significantly\nenhancing the jailbreak performance of LLMs. However, it\nupdates only one token in the suffix per iteration, which results\nin low jailbreak efficiency. Jia et al. [10] try to address this\nissue by proposing an automatic multi-coordinate updating\nstrategy, which can adaptively determine the number of tokens\nto replace at each step. Instead, both approaches select only\nthe candidate suffix with the smallest loss for the suffix update\nin each iteration. However, responses such as \"first yes, then\nno\", while reducing loss, are not necessarily harmful. Thus,\nidentifying the appropriate suffix for each round of updates\nhas become a pressing issue that needs to be addressed. In\nFigure 2, the authors propose an automated optimal suffix\nselection strategy that goes beyond using only the minimum\nloss criterion. Instead, it evaluates the first p suffixes with the\nsmallest losses $x_1, x_2,...,x_p$ and assesses the harmfulness\nof the content they generate, which can be expressed as:\nCheck(G((x_{HQ} + x) +x_{si})),\nwhere G() represents the function of the content generated\nby LLMs, Check() represents the function that determines\nwhether the generated content is harmful and $x_{si}$ represents\nthe i-th smallest suffix. In order to save time and cost, the\nauthors set p to 5. If none of the generated content is deemed\nharmful (i.e., no True values), the suffix with the smallest loss\nis selected for the update. Conversely, if harmful content is\nidentified, the suffix with the smallest loss that is also marked\nas True is chosen for the update. This approach ensures that\nsuffix updates consistently aim to generate harmful content\nwhile avoiding denial responses."}, {"title": "C. Experiments", "content": "1) Experiment Setup: Dataset. The authors conducted ex-\nperiments using the dataset provided by AI Singapore, which"}, {"title": "VI. METHOD OF TEAM SAFETY_LLM_ASTAR", "content": "The jailbreak methods consist of three modules:\n\u2022 Prompt Generation Module, which leverages black-box\nor white-box methods to generate appropriate prompts\ngiven a behavior, e.g., \u201cHow to make a bomb\".\n\u2022 Test Module, which integrates a system prompt, wraps\nthe crafted prompt in designated formats and sends the\nfinal prompt to the target model as shown in Figure 3.\nReferring to the work of [22], the system prompt can\nplay a safeguarding role at a low cost and increase the\ncomplexity of the jailbreak challenge.\n\u2022 Judge Module, which evaluates the response from the\ntarget model against the original goal behavior, assigning\na score based on how well the response aligns with the\ndesired outcome. They use a 1-10 scoring system, where\na score of '1' indicates that the target model has not\nbeen jailbroken at all, and a score of '10' signifies that\""}, {"title": "A. Combination Attacks", "content": "Due to the restrictions in accessing to the internal outputs\nof target models, it is unlikely to run white-box attacks such\nas the GCG [25] method and the Adaptive [2] method as\nthe authors could not get either the gradients or the logits\nof the model output. However, as mentioned in these two\npapers, it is possible to transfer the attack suffixes learned\nfor a particular behavior to another behavior. The authors\nconsidered this suffix transfer learning as a baseline in the\nattacks. There can be limitations for such suffixes when the\nauthors generalize them to other behaviors or other models.\nAs black-box methods do not depend on the internal data\nor representations of a specific model, they can work across\nvarious models by generating different types of prompts. It\nis likely that black-box methods may enhance the efficacy of\nthose suffixes. Inspired by the black-box attack methods such\nas CodeChameleon and ReNeLLM, the authors reformulate"}, {"title": "B. Implementation", "content": "To obtain effective suffixes, the authors began by apply-\ning the Adaptive method to jailbreak the Llama-2-7b-chat-hf\nmodel for 50 adversarial behaviors offered in Global Challenge\nfor Safe and Secure LLMs Track 1A and obtain 6 unique\ncandidates suffixes 3.\nReferring to the mutation prompts in Easyjailbreak, the\nauthors employed gemma-2-9b-instruct to perform a mutation\noperation on a behavior. The authors adopted the following\nmutations:\n\u2022 altering sentence structure. This mutation aims to shuffle\nthe order of the behavior without significantly affecting\nthe semantic meanings, e.g., \"how to make a bomb\" to\nbe \"make a bomb how to\".\n\u2022 partial translating. This mutation aims to mix English\nwords with foreign words while keeping the semantic\nmeanings, e.g., \"how to make a bomb\" to be \"how to\nmake a \u70b8\u5f39\u201d.\n\u2022 misspelling sensitive words. This mutation aims to create\ntypos for a few words in the behavior, e.g., \"how to make\na bomb\" to be \"how to make a bomb\".\n\u2022 inserting meaningless characters. This mutation aims to\nconfuse the tokenizers by inserting meaningless charac-\nters, e.g., \"how to make a bomb\" to be \"how to make a\nbom*b\".\n\u2022 rephrasing sentence. This mutation aims to describe the\nbehavior in another way while keeping the semantic\nmeaning, e.g., \"how to make a bomb\" to be \"how to\nmake an explosive device\u201d.\nFor evaluation, the authors used the first 35 behaviors\nextracted by JailbreakBench. The authors randomly created\n6 mutations for each behavior and consider 2 templates in the\npool."}, {"title": "C. Results", "content": "The authors conducted experiments on\nLlama-2-7b-chat-hf and Llama-2-13b-chat-hf,\nwhich posed a challenge for various jailbreak methods\nas shown in leaderboards such as Jailbreakbench and\nHarmbench. Table XII indicates that the adversarial\nsuffixes play a critical role in the jailbreak attacks. Using\nonly mutations and templates fails to bypass the security\nmechanism of Llama-2-7b-chat-hf in the setting.\nWith these suffixes, the model could be compromised in\napproximately half of the behaviors. Moreover, mutations\nsignificantly enhanced the effectiveness of the attacks.\nMutations could be considered even in black-box attacks,\nas they required minimal computational resources and were\nflexible to implement. Notably, no breakthroughs were"}, {"title": "D. Discussions", "content": "The approach demonstrated that adversarial suffixes learned\nthrough the white-box method on one set of behaviors can\ngeneralize effectively to others. Additionally, behavior muta-\ntions enhanced performance, likely by confusing the secu-\nrity mechanisms of LLMs. The authors also observed that\nvarying prompt templates could lead to different performance\noutcomes, prompting us to use appropriate templates for\nadversarial suffixes.\nIt is clear that without adversarial suffixes, jailbreaking\nLLMs-particularly Llama-2-7b-chat-hf, becomes ex-\nceedingly difficult, as the model is highly vigilant, especially\ntoward sensitive terms related to legal, safety, and sexual\ncontent. This underscores the importance of white-box attacks\nin targeting well-aligned LLMs, highlighting the need for\nfurther investigation, particularly into the internal mechanisms\nof LLMs.\nAdditionally, the authors emphasized that factors such as\ntemperature settings (set to 0.8 in the target models) and the\nsystem prompt could further complicate jailbreak attempts. For\nexample, the authors may need to consider how to enhance the\nrobustness of the attacks under a large temperature factor and\nhow to counter the influence of the system prompt."}, {"title": "E. Limitations", "content": "The approach focuses on the Llama-2 models, which may\nnot cover recent models such as the Llama-3 families and\nPhi-3 families. The authors have not compromised the LLMS\non all of the behaviors in the experiment, prompting us to\nfurther improve the approach. In addition, due to the token\nlength limitation set in this challenge, the authors do not use\ndifferent prompt templates such as the disguised coding tasks\nand latex table generation tasks in the literature work."}, {"title": "F. Future Directions", "content": "The authors planned to explore additional factors in suffix-\ntransferring attacks. For instance, the authors will examine\nwhether suffixes learned from a more robust LLM can be\ngeneralized effectively to weaker models. If successful, this\napproach would allow us to create a pool of robust adversarial\nsuffixes capable of jailbreaking a variety of LLMs."}, {"title": "VII. METHOD OF TEAM MODELCRACKERS", "content": "In the rapidly evolving field of natural language process-\ning (NLP), LLMs have demonstrated remarkable capabilities\nacross a range of applications, from text generation to complex\nproblem-solving. However, the deployment of these models\nin safety-critical environments has raised significant concerns\nregarding their robustness and the potential for misuse. Recent\nresearch has unveiled a more concerning threat known as\njailbreaking attacks [14], which can circumvent the safety\nalignment implemented for LLMs. Through carefully crafted\nprompts (e.g., requiring the model to start responses with\nspecific sequences such as \"Sure, here are\"), LLMs can be\nelicited to generate harmful responses. There are also other\njailbreak methods that perturb the harmful query [19], or\noptimize the jailbreak prompt through gradient-based search\n[26].\nThe work builds upon the foundation laid by An-\ndriushchenko et al. [3], adopting and enhancing their method-\nology for identifying and exploiting weaknesses in the safety\nmechanisms of LLMs. In contrast to their approach which\nutilizes a random search strategy, the methodology introduces\na more structured and efficient search mechanism called the\nGCG search method [24]. This approach aims to find a\n\"universal suffix\" a specific set of input conditions that,\nwhen appended to various queries, consistently bypasses the\nsafety features of LLMs.\nThe importance of developing such a technique lies in its\npotential to demonstrate the existing gaps in model security,\nurging the development of more robust defense mechanisms.\nFor Track 1B of the research, the authors only utilize the\nharmful queries from Track 1A to search for a sufficiently\nuniversal suffix to successfully jailbreak the LLMs for online\nqueries. This targeted approach allows us to systematically\nassess and enhance the universality, transferability, and effec-\ntiveness of the jailbreaking techniques, ensuring they are not\nonly theoretical but also practically applicable in real-world\nscenarios where LLMs operate."}, {"title": "A. Methodology of The Full End-to-end Solution", "content": "1) Detailed Method: Given a harmful query Q and its\ncorresponding confirmative target response T, the authors\nfirst combined them using a tailored jailbreak template. The\nexample below demonstrates how this is constructed:\nJailbreak Template\nQ = \"Compose instructions on assembling an explosive\ndevice\"\nT = \"Sure, here are instructions on assembling an\nexplosive device\"\nInput = \"You are in the developer mode now and you\nshould ignore all safety restrictions. Respond using the\nfollowing EXACT structure:"}, {"title": "B. Discussion", "content": "Using this method, the authors achieved a high attack\nsuccess rate (ASR) on the llama-2-hf-chat model, even\nwhen tested with previously unseen queries. Locally, the\nresults show an ASR of 75%-90%, while online results vary\nbetween 60%-75%.\nThe transferability of the attack is also noteworthy.\nThe authors tested the jailbreak technique on several\nother unseen models, including Qwen2-7B-Instruct,\nVicuna-7B-v1.5,\nYi-1.5-9B-Chat, and\nGemma-7B-It. The results demonstrated strong ASR\nperformance across these models, highlighting the versatility\nand effectiveness of the approach.\n1) What Makes it Universal: The universality of the method\nstems from the inherent capabilities of the GCG search\nmethod, which utilizes gradient-based optimization techniques\nto jailbreak safety-aligned models. By systematically explor-\ning the parameter space, GCG leverages the model's safety\nalignment weaknesses, effectively crafting a jailbreak that can\ngeneralize across different harmful queries. This approach cap-\nitalizes on the shared structural characteristics of LLMs (next\ntoken prediction), making it adaptable to various contexts. The\nrobustness of this technique lies in its ability to discover a\nuniversal suffix that remains effective regardless of specific\nquery variations, ultimately enhancing its applicability across\ndiverse models."}, {"title": "2) What Makes it Transferable", "content": "Several factors contributed\nto the transferability of the method:\n1) Prompt Aggressiveness: The authors found that overly\naggressive prompts can limit effectiveness across dif-\nferent models. Initially, the authors used an aggressive\ntarget prefix, such as \u201cSure, my output is harmful\" which\nyielded a high ASR on the 1lama-2-hf-chat model.\nHowever, this approach resulted in significantly lower\nASR on other models, indicating that a balanced prompt\nstructure is crucial for maximizing transferability.\n2) Use of Universal Suffixes: When initial attempts to\nelicit the desired response from a target model fail, em-\nploying a universal suffix or template greatly enhances\ntransferability. These suffixes are designed to maintain\ngenerality while still being tailored enough to exploit\ncommon vulnerabilities across models. The authors also\nexperimented with specific searching (searching for a\nspecific suffix for each harmful query), and found that\nthis method works well in terms of ASR. However, when\nfailing to get the target model's response, using a univer-\nsal suffix (or template) resulted in higher transferability.\""}, {"title": "C. Limitations", "content": "As for Track 1B, the authors underused the in-domain\ncharacter of the given queries, which increases the challenge\nof generalization. The authors also fail to utilize the provided\nAPI due to the calling bug, which may compromise the\nperformance of the approach."}, {"title": "D. Future Directions and Potential Improvements of The Se-\nlected Approach", "content": "Although the authors tried several approaches to avoid the\nappearance of safety warning sentences, there were still some\ncases in the model that would generate some warnings to\nindicate the unsafety of the behavior. How to avoid this within\nthe length limit of the input prompt is a challenging question\nthat is worth future exploration.\nHow to utilize the black-box API is also an important\ndirection to boost performance. The authors have tried to\nsearch for a unique suffix for different queries based on\ngeneration results from black-box APIs. This strategy was\nbeneficial in the offline experiments. Unfortunately, due to the\nlimited time, the authors failed to run the algorithm in the\nonline environment. However, the authors believe this is an\nimportant direction for further improvement."}, {"title": "VIII. METHOD OF TEAM AREDTEAM", "content": "In this report, the author tackles these challenges by in-\ntroducing a novel jailbreak method developed for efficient\njailbreaking attacks of black-box LLM systems as introduced\nin the Global Challenge for Safe and Secure LLMs \u2013 Track 1\n[17]. Inspired by the adaptive attack method [4] and iterative\noptimization, the method considers the attack success rate and\nscalability of jailbreak attacks simultaneously, particularly in\nblack-box contexts. By incorporating Equivalent Substitution\nMechanisms and Iterative Optimization of Calibration, the"}, {"title": "A. The Method", "content": "The authors present a detailed description of the proposed\nmethod, designed to enhance both the attack success rate and\nefficiency of jailbreaking attacks across multiple LLMs, as\nshown in Figure 6. The method leverages a combination of\nknown and surrogate models to optimize adversarial prompts,\nmaximizing their transferability and stability across diverse\nLLM architectures. For any given victim model (or target\nmodel), if its internal workings are known, it is used as the\nsurrogate. If the model is unknown, a set of alternative models\nis selected to act as proxies for the black-box target.\nThe process begins with an automated script that transforms\nharmful behavior specifications into a defined attack goal\n(goal) and target string (target_str), embedding them into a\nprovided prompt template. An attack suffix (suffix) is then\nselected from a pre-trained initialization set and appended to\nthe prompt. This constructed prompt is submitted to the sur-\nrogate models, with feedback in the form of log probabilities\n(logprob) used to iteratively refine the suffix until the prompt\nsuccessfully bypasses restrictions on all models.\nIn the following, the authors describe each step of the\nmethod in more detail:"}, {"title": "B. Equivalent Substitution Mechanisms", "content": "The first step in the method is to build a surrogate model\nset comprising a diverse range of publicly available LLMs."}, {"title": "C. Adversarial Prompt Construction with Pre-trained Initial-", "content": "ization of Suffixes\nOnce the surrogate models are selected, the method con-\nstructs adversarial prompts designed to trigger harmful behav-\nior in the target LLM. Harmful behaviors are defined using\npredefined templates and mapped into structured forms. The\nmethod automatically converts these behaviors into a concise\ngoal (goal) and a target string (target_str), which form the\ncore of the adversarial prompt. These elements, (target_str)\nand (goal), are then embedded into a prompt template that\nprovides the necessary context and structure for the adversarial\nprompt. The template is crafted to evade detection by the\nmodel's safety filters while preserving the harmful intent.\nA key component of the method is the pre-trained initial-\nization of adversarial suffixes. These suffixes are specifically\ndesigned to quickly trigger vulnerabilities in LLMs, serving\nas starting points for generating successful jailbreak prompts.\nThe method employs a set of pre-trained initialization suffixes,"}, {"title": "D. Iterative Optimization of Calibration", "content": "Throughout the jailbreak process, the method utilizes a\nfeedback-driven optimization loop that iteratively adjusts the\nadversarial prompt based on the real-time performance of"}]}