{"title": "Bridging Information Gaps with Comprehensive Answers:\nImproving the Diversity and Informativeness of Follow-Up Questions", "authors": ["Zhe Liu", "Taekyu Kang", "Haoyu Wang", "Seyed Hossein Alavi", "Vered Shwartz"], "abstract": "Effective conversational systems are expected\nto dynamically generate contextual follow-up\nquestions to elicit new information while main-\ntaining the conversation flow. While humans\nexcel at asking diverse and informative ques-\ntions by intuitively assessing both obtained and\nmissing information, existing models often fall\nshort of human performance on this task. To\nmitigate this, we propose a method that gener-\nates diverse and informative questions based\non targeting unanswered information using a\nhypothetical LLM-generated \u201ccomprehensive\nanswer\". Our method is applied to augment\nan existing follow-up questions dataset. The\nexperimental results demonstrate that language\nmodels fine-tuned on the augmented datasets\nproduce follow-up questions of significantly\nhigher quality and diversity. This promising\napproach could be effectively adopted to fu-\nture work to augment information-seeking dia-\nlogues for reducing ambiguities and improving\nthe accuracy of LLM answers\u00b9.", "sections": [{"title": "1 Introduction", "content": "Asking questions is a fundamental mechanism for\nhumans to acquire new information, particularly\nwhen existing information is incomplete. While\nlarge language models (LLMs) excel at passively\nanswering user queries, their ability to proactively\nguide conversations, by identifying and address-\ning gaps in information, remains underdeveloped\n(Liu et al., 2025). Question generation (QG) has\ngained significant attention in NLP, particularly\nfor its role in improving information-seeking di-\nalogue systems (Chen et al., 2024). By generat-\ning effective questions, these dialogue systems can\nmake information-seeking more accurate and effi-\ncient (Qi et al., 2020), resolve ambiguities (Li et al.,\n2017), and ultimately better understand the user's\nneeds, providing suitable assistance across various\ndomains, including education (Laban et al., 2022)\nand healthcare (Arslan et al., 2024; Li et al., 2024).\nWhile most existing QG tasks focus on generat-\ning questions directly answerable by a given con-\ntext (Zhao et al., 2018; Pan et al., 2020; Ghanem\net al., 2022), which diverges from the human cog-\nnitive process of inferring and pursuing missing\ninformation, Meng et al. (2023) proposes that mod-.\nels must generate follow-up questions that build\non-but are not answerable by\u2014an initial question-\nanswer pair. FOLLOWUPQG is more reasonable,\nbut Meng et al. (2023) found that existing models\nstruggle to replicate this human behavior, often pro-\nducing repetitive or context-bound questions that\nfail to target unexplored information.\nThis core challenge can be formulated into two\ndimensions: (1) identifying information gaps, the\nunanswered aspects of the initial question, and\n(2) generating diverse questions that address these\ngaps. Traditional QG methods (Zhao et al., 2018;"}, {"title": "2 Related Work", "content": "Question generation (QG) focuses on automati-\ncally generating semantically meaningful and well-\nstructured questions based on a given text (Ali\net al., 2010). While traditional QG techniques have\nmade significant strides in domains such as ma-\nchine comprehension (Du et al., 2017; Uto et al.,\n2023), e-commerce (Wang et al., 2021), and edu-\ncation (Luo et al., 2024), they primarily generate\nquestions based on known answers. This approach\ncontrasts sharply with human questioning behavior,\nwhich actively seeks new information from various\nperspectives. This limitation has led to the emer-\ngence of follow-up QG, a task whose goal is to\ngenerate questions that explore previously unan-\nswered or underexplored aspects of a given text.\nFollow-up QG has evolved from simpler meth-\nods, such as template-based and retrieval-driven\napproaches (Kumar and Joshi, 2017; Soni and\nRoberts, 2019; B et al., 2020), to more advanced\ntechniques that prioritize informativeness (Ma-\njumder et al., 2021; Mazzaccara et al., 2024).\nKnowledge-enhanced approaches, like those in Ge\net al. (2023) and Gupta et al. (2022), leverage entity-\nrelation pairs and knowledge graphs to improve the\ndepth of the generated questions. Further advanc-\ning this, Liu et al. (2025) combined knowledge\ngraphs with LLMs to increase question informa-\ntiveness. Efforts to model human-like questioning\nbehavior, such as InquisitiveQG (Ko et al., 2020),\nhave relied on crowd-sourced follow-up questions\nwritten for news articles rather than those naturally\ngenerated by humans, leading to a lack of depth\nand cognitive diversity.\nWe follow the setting of the FOLLOWUPQG\n(Meng et al., 2023), which formalizes information-\nseeking follow-up question generation. Based on\nquestions and answers from the ELI5 (explain like\nI'm 5) subreddit, follow-up questions in this dataset\nbuild upon-but are not answerable by the ini-\ntial question-answer pair, resembling real-world\ndialogues where follow-ups resolve ambiguities or\ndeepen understanding.\nMeng et al. (2023) found that models often pro-\nduce questions that are either repetitive or fail to\ntarget unexplored information, thus lacking the\ncognitive diversity and variability seen in human"}, {"title": "3 Data Augmentation", "content": "Effective follow-up question generation (QG) re-\nquires models to infer and target gaps between the\nprovided answer and the broader context of a con-\nversation. We follow the task definition presented\nby the FOLLOWUPQG (Meng et al., 2023): \"to\ngenerate follow-up questions that seek new infor-\nmation given the initial question and answer\u201d. For\nsimplicity, we denote the \u201cinitial question\u201d as IQ,\n\"initial answer\u201d as IA, and the \"follow-up question\"\nas FQ. Critical limitations in the training dataset\nare identified, including quality issues, which are\naddressed through dataset cleaning (Sec 3.1). The\nsmall scale (2,790 instances) and low diversity of\nthe dataset are tackled by a novel data augmentation\npipeline introduced in this paper (Sec 3.2). Specifi-\ncally, we augment the FOLLOWUPQG training set\nwith synthetic data generated by a pipeline that\nmimics human-like gap-driven questioning. This\napproach utilizes LLM-generated comprehensive\nanswers to systematically identify missing informa-\ntion in initial answers and generate follow-up ques-\ntions targeting those gaps. We demonstrate that the\naugmented data retains high quality (Sec 3.3).\n3.1 Data Cleaning\nThe FOLLOWUPQG dataset is limited by its small\nscale, comprising 3,790 samples: 2,790 for train-\ning, 500 for validation, and 500 for testing. Within\nthe 2,790 training instances, there are only 2,651\nunique (IQ, IA, FQ) triplets, indicating duplication.\nAdditionally, the number of 2,648 unique (IQ, IA)\npairs suggest minimal follow-up question diversity,\nas 99.8% of pairs have only one reference FQ. Fur-\nther analysis also uncovered data quality issues,\nlikely stemming from automated data collection\n(see Appendix A). To improve the data quality, we\ndid the following:\n\u2022 Deduplication. We removed 139 duplicate (IQ,\nIA, FQ) triplet instances.\n\u2022 Reference quality check. We manually filtered\nout 84 instances where the reference FQ diverged\nentirely from the initial question.\n\u2022 Sensitive content removal. We excluded 24 in-\nstances involving topics like self-harm or crime,\nwhich LLMs are likely to refuse to answer."}, {"title": "3.2 Augmentation Pipeline", "content": "As discussed in Section 3.1, the limited scale of the\ndataset and the lack of follow-up question diversity\nhinder the coverage of diverse questioning strate-\ngies, restricting model generalization. To address\nthis, we design a GPT-4-based pipeline that aug-\nments the original dataset by generating additional\nfollow-up questions. Our pipeline simulates hu-\nman reasoning through three interconnected stages:\ncomprehensive answer generation, information gap\nidentification, and follow-up question generation\u00b2.\nComprehensive answer generation. To identify\ngaps in the IA, we generate a comprehensive an-\nswer (CA) that represents a complete and thor-\nough response to the IQ. As shown in Figure 2,\nwe prompt GPT-4 iteratively to generate answers to\nIQ that target different perspectives, such as techni-\ncal, ethical, and practical, and synthesize a unified\nCA.\nInformation gap identification. The next step is\nto identify key concepts or details discussed in the\ncomprehensive answer (CA) but not covered in the\ninitial answer (IA). This is done by prompting GPT-\n4. For example, in the example shown in Figure 2,\nthe initial answer covers the topic of privacy issues\nbut does not cover areas of cyber security (i.e. an\ninformation gap).\nFollow-up question generation. Using the iden-\ntified information gaps, we prompt GPT-4 to gen-\nerate follow-up questions that address those gaps\nwhile maintaining contextual relevance to the IQ\nand IA. The generated questions must meet three\ncriteria: be (1) answerable by the CA, (2) unanswer-\nable by the IA, and (3) grounded in terminology\nand context from the IQ.\nTo match the format of the original follow-up\nquestions in the FOLLOWUPQG dataset, we auto-\nmatically reformat the generated FQs to remove\nartifacts such as bullets or numbering. Each (IQ,\nIA) pair is enriched with multiple follow-up ques-\ntions generated by our pipeline, averaging 10.95\nquestions per pair. The restructured dataset merges\nsynthetic questions with cleaned human-generated"}, {"title": "3.3 Augmented Data Validation", "content": "To assess the quality of the generated follow-up\nquestions, we conducted a human evaluation study\nusing Cloud Connect. To ensure high-quality\nannotations, we restricted participation to native\nEnglish-speaking annotators with a minimum of\n1,000 completed annotation tasks and an approval\nrating exceeding 90%. A randomly sampled subset\nof 100 (IQ, IA, FQ) triplets was evaluated based on\nthree key criteria: (1) whether the follow-up ques-\ntion was a valid question\u00b3, (2) whether any com-\nponent of the triplet contained sensitive informa-\ntion, and (3) the degree of relatedness between the\nfollow-up question and the initial question-answer\npair. The full survey format, including example\nannotations, is provided in Appendix C. The re-\nsults show that 94% of the follow-up questions are\nlabeled as valid, 92% as not sensitive, and 91% are\nrelated to the original (IQ, IA) pair. Inter-annotator\nagreement was moderate, with a Cohen's Kappa\nscore of $\\kappa$ = 0.73 (McHugh, 2012)."}, {"title": "4 Experiment Setup", "content": "Model Variants. To evaluate the effectiveness of\nour proposed pipeline and augmented dataset, we\ntrain several variants of the same model on differ-\nent data. We use BART-large (Lewis et al., 2020)\nas our base model, motivated by its strong perfor-\nmance on the work of FOLLOWUPQG (Meng et al.,\n2023). We report the performance of three varia-\ntions trained on FOLLOWUPQG training set. The\nORG variant follows the method presented in Meng\net al. (2023), trained on the 2,790 original instances\nfrom the training set, serving as the baseline model.\nIt employs a seq2seq model that conditions on IQ\nand IA (IQ <SEP> IA) to generate FQ.4"}, {"title": "5 Results", "content": "To thoroughly assess the quality of the generated\nfollow-up questions, we employ both automatic\nevaluation (\u00a75.1) and human evaluation (\u00a75.2).\nAs a first step for both evaluations, we automati-\ncally identify and remove ungrammatical questions\nbased on syntactic parsing (see Appendix E for a\ncomplete description of the filtering process). Ta-\nble 1 shows the percent of ungrammatical questions\nthat were filtered out for each model. AUG (3.58%)\nand FULL (6.31%) produce far fewer ungrammat-\nical FQs compared to ORG, demonstrating their\nability to generate more well-formed outputs. We\nfocus the rest of our evaluation on the grammatical\nquestions retained after the filtering."}, {"title": "5.1 Automatic Evaluation", "content": "Diversity. We measure the diversity of the set\nof follow-up questions generated for a particular\n(IQ, IA) input, averaged across the dataset. We\nreport Distinct-n (Li et al., 2016), which measures\nthe average distinct n-gram across all groups of\nfollow-up questions for a given (IQ, IA) pair. The\nresults in Table 2 show that AUG and FULL per-\nform similarly, and outperform ORG in generating\nmore diverse unigrams and bigrams.\nWe also apply agglomerative clustering to the\nsentence embeddings of the FQs (generated with\nall-mpnet-base-v2) and report the number of\nclusters formed at a distance threshold of 1.0. We\nnormalize it by the number of generated follow-\nup questions to quantify the diversity within that\nset. Again, Table 2 shows that adding the aug-\nmented data substantially improves the diversity of\nthe generated questions.\nAverage question length. We report the average\nquestion length in terms of the number of tokens.\nWe hypothesize that shorter questions are generally\nmore readable. Table 2 summarizes key statistics:\nthe average length, shortest and longest follow-ups,\nand standard deviation. The ORG model shows the\nhighest variation in question length (SD = 10.13)\ncompared to FULL (3.77) and AUG (2.98). No-\ntably, its longest follow-up (111 words) far exceeds\nthose from FULL (73) and AUG (24). In contrast,\nAUG maintains the most consistent length distribu-\ntion, with a lower standard deviation and a maxi-\nmum length of 24 words.\nExamining the generated follow-up questions,\nwe find that AUG and FULL generally produce con-\ncise, well-formed queries, while ORG sometimes\ngenerates unclear or uninformative short questions\n(e.g., \"So it's cultural?\"). Meanwhile, the longer\nquestions from ORG and FULL often include ex-"}, {"title": "5.2 Human Evaluation", "content": "We conducted a human evaluation to assess the\nquality of generated follow-up questions across\nfour key aspects: validity, complexity (the level\nof reasoning required), relevance, and informative-\nness (Table 4). We randomly sampled 30 (IQ, IA)\npairs from the FOLLOWUPQG testing set and veri-\nfied the generated FQs from all models.\nThe evaluation was carried out using Cloud Con-"}, {"title": "6 Analysis", "content": "To further understand the strengths and limitations\nof our method, we present a qualitative comparison\nof follow-up questions generated by all models\nfor the same (IQ, IA) pair (\u00a76.1), as well as an\nanalysis of the expected information gain from the\ngenerated follow-up questions (\u00a76.2).\n6.1 Qualitative Analysis\nIn Table 6, we compare follow-up questions gen-\nerated by the ORG, AUG, and FULL models for\na given (IQ, IA) pair. The questions produced by\nthe ORG model are often either redundant, such\nas \"What is a heuristic?\", or tangential, like re-\nsponding with \"How do you know if you can\nELI5?\" to the original responder that mentioned\nthey didn't know if they could explain it to a 5-year-\nold (ELI5)-diverging from the target concept of\nheuristics. While the FULL model generates a vari-\nety of relevant questions, excelling in diversity, it\nsometimes includes tangential ones or redundant\nphrasing, such as \u201cHow do heuristics help in seg-\nmenting and segmenting information for specific\ntasks?\u201d, which can affect clarity. In contrast, the\nAUG model offers the best balance of informative-\nness and diversity, producing focused and insight-"}, {"title": "6.2 Quantifying Information Gain", "content": "In Sec. 5.2 we asked annotators to rate how in-\nformative each follow-up question is. Here we\npropose an alternative, automated method for effec-\ntively evaluating informativeness without human\nannotators-by leveraging the \u201ccomprehensive an-\nswers\" generated from GPT-4 (CA; Sec. 3.2). We\nconsider CA as a proxy for the set of all and only\nthe relevant information that can be discussed in\nthe context of (IQ, IA) pair. Thus, an informative\nFQ cannot be answered by IA (otherwise, the an-\nswer to FQ would add no new information); but\nshould be answerable from CA (otherwise, it may\nbe irrelevant). Motivated by this, we prompt GPT-4\nto evaluate the answerability of each model's gen-\nerated follow-up questions using each IA and CA.\nTable 7 reassess the findings from the human\nevaluation that AUG produced the most informa-\ntive follow-up questions (36%), followed by FULL\n(35%) and ORG (25%).\nComparison between the GPT-4 predictions and\nthe human-annotated informativeness scores (\u00a75.1)\nvalidates this automated approach by showing that\nannotators assigned slightly higher scores to ques-\ntions classified by GPT-4 as informative (1.29) than\nto those classified as not informative (1.07). A T-\ntest (p-value = 0.0011) confirmed statistical signif-\nicance, however, with a small magnitude (Cohen's\nd = 0.215) (Cohen, 2013)."}, {"title": "7 Conclusion", "content": "In this work, we proposed a novel approach to\nenhance the diversity and informativeness of gener-\nated follow-up questions by directly modeling in-\nformation gaps through a generated comprehensive\nanswer. Training data was generated using GPT-4\nand distilled into a smaller model. Our experi-"}, {"title": "Limitations", "content": "We acknowledge several limitations in our work.\nFirst, while our CA-based pipeline is effective in\nknowledge-driven contexts, its applicability to non-\nknowledge-based conversations, such as opinion-\nbased questions (e.g., \u201cWhat would you do in such\na scenario?\u201d), remains unclear, as the subjective\njudgment required in these conversations can be\ndifficult for a generated CA to capture. Addition-\nally, although our pipeline prioritizes informative-\nness, follow-up questions do not always need to\nintroduce new information (Kurkul and Corriveau,\n2018)\u2014for example, requests for simpler expla-\nnations (e.g., \u201cCan you explain this in an easier-\nto-understand way?\u201d). In the future, we hope to\nextend this method to support various types of\nfollow-up questions and integrate it into down-\nstream dialogue-based applications."}, {"title": "Ethical Considerations", "content": "All annotators involved in the human evaluation for\nthis research were fairly compensated, with pay-\nment rates exceeding the local minimum wage to\nensure equitable remuneration for their time and\neffort. Prior to recruiting annotators, ethical ap-\nproval was obtained from the research ethics board\nat the authors' institution, ensuring that the human\nevaluation process adhered to ethical guidelines\nand that no harm was caused to any individual in-\nvolved. Additionally, the FOLLOWUPQG dataset\nis publicly available, and we commit to making our\ndataset, including the augmented data and gener-\nated comprehensive answers, accessible to promote\ntransparency and reproducibility in future work."}, {"title": "A Problematic and Unrelated Instance\nExamples", "content": "Tables 8 and 9 show examples of a problem-\natic instance and an unrelated instance from FOL-\nLOWUPQG training set, respectively."}, {"title": "B LLM Prompts", "content": "Table 10: Comprehensive Answer Generation Prompts:\nGPT-4 first generates an answer from a single perspec-\ntive, then iteratively provides non-overlapping answers\nfrom different perspectives, which are finally synthe-\nsized into a unified response."}, {"title": "C Augmented Data - Human Annotation\nGuideline", "content": "Table 12 presents the job description and annotation\nquestions for our human annotation task.\nC.1 Valid/Invalid Question Guideline\nThe follow-up question might contain multiple sen-\ntences but it should consist of at least one valid\nquestion. A valid question must be in a question\nformat and ask meaningful information, including\nWh-questions (what/why/where/etc.), open-ended\nquestions, probing questions and etc. Invalid ques-\ntions like \"10000 meters? really?\", are often used\nin conversational speech to express feelings instead\nof asking for new information. Table 13 contains\nexamples of valid and invalid follow-up questions.\nC.2 Inappropriate Question Guideline\nExamples of racist comments include: \u201cIt's credit\nto your race,\" \"Black people will not understand.\u201d\nExamples of hate speech include: \u201cHe should go\nback to where he comes from,\u201d \u201cAll Mexicans are\nrapists.\u201d Examples of offensive or rude comments\ninclude: \"Women are not suitable for working in\nthe IT field,\u201d \u201cGay will never understand.\u201d Table 14\ncontains an example of an inappropriate follow-up\nquestion."}, {"title": "C.3 Relevance Question Guideline", "content": "\u2022 Strongly Related: The follow-up question\nasks for specific definitions, particular reasons,\nor meanings directly from the original ques-\ntion and answer.\n\u2022 Related: The follow-up question primarily\nseeks information from the original question\nor answer but also brings in additional, new\ninformation.\n\u2022 Slightly Related: The follow-up question\nmainly addresses other cases but has some\nrelevance to the original question or answer.\n\u2022 Not Related: The follow-up question does\nnot relate to the original question or answer."}, {"title": "D Baseline Reproduce", "content": "To establish a baseline, we attempted to reproduce\nthe results of Meng et al. (2023) using the reported\nparameters, as the original implementation was un-\navailable. We use BART-large, consisting of 24\nlayers, 16 attention heads, and a hidden dimen-\nsion of 1024. The initial learning rate (5e-5) led to\ntraining instability, which we mitigated by adjust-\ning it to 2e-5 while keeping other hyperparameters\nunchanged (batch size: 8, epochs: 10, optimizer:\nAdam (Kinga et al., 2015)). The training was con-\nducted on an NVIDIA Tesla V100 GPU with 32GB\nmemory, taking approximately 6 hours per run. We\nset the random seed as 42. After multiple runs, our\nreproduced model yielded similar overall perfor-\nmance but with some variation-certain metrics\nimproved while others slightly declined (see Table\n16). This model served as the baseline for subse-\nquent experiments."}, {"title": "E Filtering Ungrammatical Follow-Up\nQuestions: Examples and Pseudocode", "content": "def is_valid_question(FQ, IQ, IA):\nreturn (\n)\n# confirm that FQ ends with '?'\ncontains_question_mark (FQ) and\n# use dependency parsing to\ncheck for\n# WH-Questions, Yes/No Questions\n# and Rhetorical Questions,\nis_question_dependency_parsing (\nFQ) and\n# checks for meaningless/invalid\nwords\n#e.g. <QUS>, <EQT >\nnot contains_invalid_word(FQ)\nand\n# checks for n-gram substring\nthat is\n# duplicated in both FQ and (IQ/\nIA)\nnot contains_duplicate_words(IQ,\nIA, FQ)"}, {"title": "F Model Evaluation - Human Annotation\nGuideline", "content": "Table 18 presents the job description and annotation\nquestions for our human annotation task.\nF.1 Error Question Guideline\nDoes the follow-up question contain any of the\nfollowing errors?"}, {"title": "F.2 Reasoning Question Guideline", "content": "Evaluate the level of reasoning needed to generate\nthe follow-up question.\n\u2022 Complex reasoning involves synthesizing\nmultiple ideas or deeply analyzing informa-\ntion.\n\u2022 Moderate reasoning requires interpreting the\ngiven content or slightly extending the discus-\nsion.\n\u2022 Minimal reasoning involves simple compre-\nhension or directly rephrasing information.\n\u2022 No reasoning applies to questions that are\ndirect repetitions or restatements without any\nthought process."}, {"title": "F.3 Informativeness Question Guideline", "content": "Evaluate whether the follow-up question enriches\nthe topic by providing or eliciting new information.\n\u2022 A Lot of New Information indicates a signif-\nicant amount of new knowledge is introduced.\n\u2022 Some New Information suggests moderate\nenrichment.\n\u2022 Little New Information implies minimal ad-\ndition.\n\u2022 No New Information means no new informa-\ntion is provided to the audience."}, {"title": "G Additional Examples", "content": "See Tables 22"}, {"title": "H Interface Examples", "content": "See Figures 3 and 4"}]}