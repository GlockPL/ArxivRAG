{"title": "Improving the Perturbation-Based Explanation of Deepfake Detectors Through the Use of Adversarially-Generated Samples", "authors": ["Konstantinos Tsigos", "Evlampios Apostolidis", "Vasileios Mezaris"], "abstract": "In this paper, we introduce the idea of using adversarially-generated samples of the input images that were classified as deepfakes by a detector, to form perturbation masks for inferring the importance of different input features and produce visual explanations. We generate these samples based on Natural Evolution Strategies, aiming to flip the original deepfake detector's decision and classify these samples as real. We apply this idea to four perturbation-based explanation methods (LIME, SHAP, SOBOL and RISE) and evaluate the performance of the resulting modified methods using a SOTA deepfake detection model, a benchmarking dataset (FaceForensics++) and a corresponding explanation evaluation framework. Our quantitative assessments document the mostly positive contribution of the proposed perturbation approach in the performance of explanation methods. Our qualitative analysis shows the capacity of the modified explanation methods to demarcate the manipulated image regions more accurately, and thus to provide more useful explanations.", "sections": [{"title": "1. Introduction", "content": "Over the last decade, there is an ongoing advancement and use of AI technologies for tackling a variety of tasks, such as natural language processing [17] and text classification [18], audio classification [40] and speaker diarization [24], as well as video understanding [20] and summarization [4]. Nevertheless, the \"black-box\" nature of AI models raises concerns about their use in several sensitive (in terms of human safety) applications and domains, such as autonomous cars, cybersecurity and medical decision-making [22, 39]. Transparency, accountability and explainability are crucial for building trust in AI, and over the last years there is a shift from just using the power of AI, to understanding and interpreting how AI has made a decision.\nSeveral methods have been proposed for explaining the output of AI models, e.g. [7, 9, 11, 19, 23, 25, 27]. According to [22], these methods can be categorized based on their scope (local or global), the stage they are implemented (during (\"ante-hoc\") or after (\u201cpost-hoc\u201d) the training), and the methodology (e.g. perturbation-based, gradient-based). In this paper, we focus on \u201cpost-hoc\" perturbation-based methods that do not require any access to the model's internal structure and design (i.e. are model-agnostic). These methods perturb the input data, observe the changes in the model's output, and analyze the extent to which the output is affected by each perturbation. Through a series of perturbations and corresponding outputs, they are able to infer the most important features for the model's decisions [27].\nVarious types of perturbations have been described in the literature on explainable image/video classifiers, such as the removal of input features, their masking with fixed or random values, and their replacement by blurred patches or Gaussian noise [6, 22]. Such perturbations are meaningful when explaining the output of models for e.g. image/video annotation. However, they are less suitable when the aim is to explain the decision of deepfake image/video detectors.\nAs documented in [12] and discussed also in [13], such perturbations may produce images that do not lie within the distribution of the data used for training the deepfake detector, giving rise to the out-of-distribution (OOD) issue and leading to unexpected model behavior. As a consequence, the applied explanation method cannot accurately detect whether the observed change in the deepfake detector's output is associated with the removal/modification of important input features or with the shift in the data distribution.\nTo overcome the aforementioned weakness of existing perturbation-based explanation methods, in this paper we propose the use of an adversarially-generated sample of the"}, {"title": "2. Related Work", "content": "Despite the ongoing interest in developing AI-based models for detecting deepfake images and videos, the explanation of the output of these models has been investigated only to a small extent, thus far. In one of the first attempts towards explaining a deepfake detector's decision, Malolan et al. [21], examined the use of the LIME [27] and LRP [5] methods to produce visual explanations (in the form of heatmaps) about the output of an XceptionNet-based [8] detector that was trained on a subset of the FaceForensics++ dataset [30]. Pino et al. [26] employed adaptations of SHAP [19], Grad-CAM [31] and self-attention methods, to explain the output of deepfake detectors (based on EfficientNet-B4 and B7 [34]) that were trained on the DFDC dataset [10]. Silva et al. [32], applied the Grad-CAM method [31] and focused on the computed gradients for the attention map to spot the image regions that influence the most the predictions of an ensemble of CNNs (XceptionNet [8], EfficientNet-B3 [33]) and attention-based models for deepfake detection. Xu et al. [38], utilized learned features to explain the detection performance of a trained linear deepfake detector on the FaceForensics++ dataset [30], with the help of heatmap visualizations and uniform manifold approximation and projection. Jayakumar et al. [15], employed the Anchors [28] and LIME [27] methods to produce visual explanations for a trained model (based on EfficientNet-B0 [33]) that detects five different types of deepfakes. Aghasanli et al. [2], used SVM and xDNN [3] classifiers to understand the behavior of a Transformer-based deepfake detector. Finally, Jia et al. [16] examined the capacity of multimodal LLMs in detecting deepfakes and providing textual explanations about their decisions.\nIn terms of evaluation, most of the aforementioned works evaluate the produced explanations qualitatively using a small set of samples [2, 15, 16, 21, 32, 38]. Towards a quantitative, and thus more objective, evaluation, Pino et al. [26] used tailored metrics that relate to low-level features of the obtained visual explanations, such as inter/intra-frame consistency, variance and centredness. Gowrisankar et al. [13] described a framework that applies a number of adversarial attacks, adding noise in regions of a deepfake image that correspond to the spotted salient regions after explaining the (correct) classification of its real counterpart, and evaluates"}, {"title": "3. Proposed Approach", "content": "The proposed approach aims to produce a visual explanation after a deepfake detector classifies an input image as a deepfake, providing clues about regions of the image that were found to be manipulated. The processing pipeline, shown in Fig. 2, contains an adversarial image generation step and a visual explanation generation step. The former aims to create an adversarial sample of the input image that can fool the detector to classify it as \"real\", and the latter uses this sample to form perturbation masks for inferring the importance of different parts of the input image and generate the visual explanation (e.g. in the form of a heatmap).\nAssuming a trained deepfake detector and an input im-"}, {"title": "Algorithm 1 Adversarial image generation", "content": "Parameters: Search variance $\\sigma$, Number of samples $n$, Image width/height $D$, Maximum number of iterations $M$, Maximum distortion $\\delta$, Learning rate $\\alpha$, Gaussian noise generator $N(mean, covar.)$, Identity matrix $I$\nInput: Input image $x$, Deepfake detector $F(x)$ with $F(x)_{real}$ being the probability to classify $x$ as \u201creal\u201d\nOutput: Adversarial image $x_{adv}$\n1: $x_{adv} = x$,\n2: for $i = 1 \\rightarrow M$ do\n3: \t if $F(x_{adv})$ is real then return $x_{adv}$\n4: \t $g = 0_{D,D}$ # Matrix that stores gradients\n5: \t for $j = 1 \\rightarrow n$ do\n6: \t\t $u_j = N(0_{D,D}, I_{D,D})$ # Random noise\n7: \t\t $g = g + F(x_{adv} + \\sigma \\cdot u_j)_{real} \\cdot u_j$\n8: \t\t $g = g - F(x_{adv} - \\sigma \\cdot u_j )_{real} \\cdot u_j$\n9: \t $g = \\frac{1}{n}g$\n10: \t $x_{adv} = x_{adv} + clip_\\delta (\\alpha \\cdot sign(g))$\n11: return $x_{adv$"}, {"title": "4. Experiments", "content": "We ran experiments on the FaceForensics++ dataset\n[30], which includes 1000 original videos and 4000 fake\nvideos. The fake videos are equally divided into four different classes (1000 per class) according to the type of the\napplied AI-based manipulation for generating them. More\nspecifically, the videos of the \"FaceSwap\" (FS) class were"}, {"title": "4.1. Explanation methods", "content": "We considered the following explanation methods:\n\u2022 LIME [27] creates visual explanations by replacing portions of the input image with the mean pixel value of the image to assess their influence on the model's output. It approximates the model's behavior locally by fitting the perturbation data and the corresponding model's outputs into a simpler model (e.g. a linear regressor).\n\u2022 SHAP [19] leverages the Shapley values from game theory. It constructs an additive feature attribution model that attributes an effect to each input feature and sums the effects, i.e., SHAP values, as a local approximation of the output.\n\u2022 SOBOL [11] generates a set of real-valued masks and applies them to an input image through blurring-based perturbations. By analyzing the relationship between masks and the corresponding model's prediction, it estimates the order of Sobol' indices and creates an explanation by highlighting each region's importance.\n\u2022 RISE [25] produces a set of binary masks to occlude regions of the input image and produce a set of perturbations. Then, it feeds these perturbations to the model, gets its predictions, uses them to weight the corresponding masks, and creates the explanation by aggregating the weighted masks together."}, {"title": "4.2. Dataset and evaluation protocol", "content": "We ran experiments on the FaceForensics++ dataset\n[30], which includes 1000 original videos and 4000 fake\nvideos. The fake videos are equally divided into four different classes (1000 per class) according to the type of the\napplied AI-based manipulation for generating them. More\nspecifically, the videos of the \"FaceSwap\" (FS) class were"}, {"title": "4.3. Implementation details", "content": "We used the pretrained deepfake detection model of [36],\ndownloaded from\u00b9. The deepfake detection model relies on\nthe second version of the EfficientNet architecture [35], that\nwas pretrained on ImageNet 1K and fine-tuned on Face-\nForensics++, as described in [36]. The generation of the\nvisual explanations was based on the following settings:\n\u2022 LIME: set the number of perturbations equal to 2K\nand used SLIC with number of segments equal to 50;\nused python package from: https://pypi.org/\nproject/lime\n\u2022 SHAP: set the number of evaluations equal to 2K and\nused a blurring mask with kernel size equal to 128,\nwhen applying the original data perturbation approach;\nused python package from: https://pypi.org/\nproject/shap\n\u2022 SOBOL: set the grid size equal to 8 and the num-\nber of design equal to 32, and kept all other param-\neters with default values; used implementation from\nhttps://github.com/fel-thomas/Sobol-\nAttribution-Method\n\u2022 RISE: set the number of masks equal to 4K and\nkept all other parameters with default values; used\nimplementation from: https://github.com/\nyiskw713/RISE\nWith respect to NES, similarly to [13] we set the learning\nrate $\\alpha$ equal to 1/255, the maximum distortion $\\delta$ equal to\n16/255, the search variance $\\sigma$ equal to 0.001, and the max-\nimum number of iterations $M$ during the evaluation stage\nequal to 50. Following [36], we set the number of sam-\nples $n$ equal to 40. Finally, the maximum number of itera-\ntions during the explanation production step was set equal\nto 80. Experiments were carried out on a NVIDIA RTX\n4090 GPU cards. Code and model are available at: https:\n//github.com/IDT-ITI/Adv-XAI-Deepfakes"}, {"title": "4.4. Quantitative results", "content": "The accuracy of the employed deepfake detector for the\ndifferent types of fakes, on the original set of images (sec-\nond row) and the generated variants of them after perform-\ning adversarial attacks on the image regions corresponding\nto the top-1, top-2 and top-3 scoring segments according to\nthe different explanation methods, is presented in Tab. 1.\nThese results show that LIMEadv is the top-performing"}, {"title": "4.5. Qualitative results", "content": "Our qualitative analysis is based on four pairs of real and\nmanipulated images of the FaceForensics++ dataset (one\npair per type of fake), that are shown in the top two rows\nof Fig. 5, and the produced visual explanations by the dif-\nferent methods, that are presented in the remaining rows of\nFig. 5. A comparison between LIME and LIMEadv shows\nthat LIMEadv: i) defines more completely the manipulated\narea in the case of the DF sample (including the region\naround the added eyeglasses), ii) spots more accurately the\nregions close to the eyes, mouth and chin in the case of\nthe F2F sample (that are typically altered when transfer-\nring the expressions from a source to a target video), iii)"}, {"title": "5. Conclusions", "content": "In this work, we presented our idea for improving the\nperformance of perturbation-based methods when explain-\ning deepfake detectors. This idea relies on the use of\nadversarially-generated samples of the input deepfake im-\nages, to form perturbation masks for inferring the impor-\ntance of input features. We integrated the proposed per-\nturbation approach in four SOTA explanation methods and\nevaluated the performance of the resulting modified meth-\nods on a benchmarking dataset. The conducted quantitative\nand qualitative analysis documented the gains in the per-\nformance of most of these methods, that can support the\nproduction of more accurately defined visual explanations."}]}