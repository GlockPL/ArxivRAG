{"title": "Examining Two Hop Reasoning Through Information Content Scaling", "authors": ["David Johnston", "Nora Belrose"], "abstract": "Prior work has found that transformers have an inconsistent ability to learn to answer latent two-hop questions - questions of the form \"Who is Bob's mother's boss?\" We study why this is the case by examining how transformers' capacity to learn datasets of two-hop questions and answers (two-hop QA) scales with their size, motivated by prior work on transformer knowledge capacity for simple factual memorization. We find that capacity scaling and generalization both support the hypothesis that latent two-hop QA requires transformers to learn each fact twice, while two-hop QA with chain of thought does not. We also show that with appropriate dataset parameters, it is possible to \"trap\" very small models in a regime where they memorize answers to two-hop questions independently, even though they would perform better if they could learn to answer them with function composition. Our findings show that measurement of capacity scaling can complement existing interpretability methods, though there are challenges in using it for this purpose.", "sections": [{"title": "1. Introduction", "content": "Many existing transformer interpretability methods, like probing (Belinkov, 2022) and sparse autoencoders (Huben et al., 2023) mostly involve extracting interpretable signals from the activations. These allow us to identify features that are relevant to the computation for particular data points, but they also necessarily ignore other parts of the computation. Another ubiquitous method for understanding models is to test how they generalize to situations absent from the training data - this is extremely useful, but so standard it would not typically be considered an \u201cinterpretability technique\". Both methods come with limitations: hidden state probing may fail to capture important features of the computation or fail to yield useful hypotheses about how the model is computing answers. It is usually infeasible to test a model's generalization performance on all tasks that might be of interest.\nWe investigate a new, complementary method for interpreting transformers: information content. Prior work by Allen-Zhu & Li (2024) found that sufficiently trained transformers can memorize facts up to a limit of around 2 bits per parameter, independent of architecture or dataset size. Let's consider carefully what this figure might represent. It cannot represent the maximum rate of compression of every trained transformer. Transformers are feedforward networks, so if a dataset benefits from recurrence, the best a transformer can do is learn several layers that all apply the same function (perhaps up to some symmetry). A good compression algorithm could recognise the repetition of layers, and encode a single layer along with the instruction to repeat it a sufficient number of times, but this affords no opportunity to the transformer to pack the computation into fewer parameters.\nWe hypothesize that the figure of 2 bits per parameter represents how a transformer can be compressed with respect to a model of computation that matches the transformer's architecture. This compression is very efficient for factual recall tasks, because they do not benefit from recurrence, but will be less efficient in tasks that do benefit from recurrence. We will call this imprecise notion the \u201cnative information content\" of a transformer, and we will study how this figure scales with model size for the task of two-hop question answering.\nThe native information content of transformers is interesting because it relates to how well transformers generalize at different tasks. Information compression is closely linked to learning in general \u2013 given a sequence of data ($X_1, X_2, ...$) sampled from a distribution P(X), the we can can model P, the more efficiently we can encode the data. Furthermore, there are numerous expressions of general principles that compact or simple representations of data generating processes are \"good\" ones (Solomonoff, 1964), and numerous researchers have expressed a general view that compression and intelligence are tightly linked (Del\u00e9tang et al., 2023).\nWe can also make specific predictions about generalization for different scaling rates of information content capacity. For example, a theorem proving system that proves everything from a fixed set of first principles will exhibit no"}, {"title": "1.1. Motivation", "content": null}, {"title": "1.2. Two hop question answering", "content": "In this work we measure the information content scaling of transformers trained to answer two-hop questions. Suppose we have a dataset of two-hop questions with answers (that is, strings of the form \"Who is Bob's boss's best friend? Mary\"). If we have a dataset that contains such a question for every person and pair of relationships, then a model that memorizes the answers h(Bob, boss, best friend) to all such questions independently will take $|N||R|^2 \\log |N|$ bits (this is the \u201chash table\u201d model of computation), where N is the set of people in the dataset and R the set of relationships. There is a much more efficient way to answer such questions: we learn a function f: N \u00d7 R \u2192 N that memorizes each person's relationships, taking $|N||R| \\log |N|$ bits, and then we apply it twice. That is, we would answer the original question with f(f(Bob, boss), best friend). Because f is reused, we reduce the memorization requirement by a factor of |R| - but, because f is reused, we believe this algorithm is impossible to implement without recurrence.\nBecause a transformer is purely feed forward, by the time an answer to f (Bob, boss) is obtained, it cannot \"go back\" and reapply f. Instead, it may need to learn two functions $f_1$ and $f_2$ with the only difference being that $f_2$ is stored in later layers than $f_1$, and compute $f_2(f_1 (Bob, boss), best friend). This is far from the only algorithm a transformer could learn that is compatible with its feed-forward architecture. The main point is that two-hop question answering requires answers (or partial answers) to factual lookups to be fed back into the same factual lookups, and this appears to necessitate that transformers replicate factual information across different layers."}, {"title": "1.2.1. GENERALIZATION PROPERTIES OF DIFFERENT TWO-HOP ALGORITHMS", "content": "We represent two-hop questions using the schema $(e_1, r, e_2, a, s)$, where $e_1$ is the first entity, r is the first relation, $e_2$ is the intermediate entity, a is the attribute in question (which may be another relation, or some other property of $e_2$), and s is the solution. For example, in the question \"Who is Bob's mother's boss?\", we have $e_1$ = Bob, r = mother, $e_2$ = Bob's mother, a = boss, and s = Mary. Note that specifying $(e_1, r, a)$ is sufficient to fully specify a two-hop question."}, {"title": "1.2.2. EMPIRICAL EVIDENCE REGARDING TWO-HOP GENERALIZATION", "content": "Wang et al. (2024) found evidence for the \"two function composition\" algorithm for answering latent two-hop questions. Specifically, they found:\n\u2022 Supervised probes in the middle layers can extract the answers to the first hop of two-hop questions in transformers trained to answer two-hop questions\n\u2022 Transformers trained to answer two-hop questions fail to generalize to unseen questions $(e_1, r_1, r_2)$ if $(e_1, r_1)$ and $(e_2, r_2)$ are only seen in the context of one-hop questions in the training data\nThis evidence is suggestive, but not decisive. The probing results suggest that one aspect of our intuition was sound that transformers require answers to the first hop to be made available early enough for subsequent processing to be feasible. However, absent proof, there is a large space of possible algorithms that could be used for this purpose. For example, it may be possible in principle to look up the answer to the first hop and encode both this answer and the entire function f(\u00b7, r2) : N \u2192 N this in activations, which may then be evaluated on e\u2081 using an evaluation map which can perhaps be encoded weight-efficiently.\nIn this work, we assess which of the proposed algorithms best matches the observed information content scaling of transformers trained to answer two-hop questions, which gives us an independent line of evidence regarding which algorithm transformers implement for this task."}, {"title": "1.3. Summary", "content": "\u2022 We trained transformers of a variety of sizes and depths on a collection of synthetic two-hop QA datasets of varying sizes\n\u2022 For one-hop QA, we reproduced previous results of a \"knowledge capacity\" of about 2 bits per parameter"}, {"title": "2. Method", "content": null}, {"title": "2.1. Data", "content": "We generated datasets of fictional profiles of \"people\" from $|N|$ = 1000 to $|N|$ = 250000 profiles. We call the set of people N. Each person had a randomly selected first, middle and last name, with $|N_o| = 4 \u00d7 10^{11}$ possible combinations. Name combinations were selected without replacement.\nEach person had 17 types of \"relations\" to other people. To make the problem easy to analyse, the relations had no structure - while some relations were called \"parents\" and others \"children\", these relations were independently uniformly randomly selected, so the \"child\" relation is not the inverse of the \"parent\" relation. We call the set of relations R. Each person also had 4 \u201cproperties\u201d. These could be queried at the last hop, but could not constitute a first hop (as the possible values were not people). Call the set of relations and properties \u201cattributes\", A. Each attribute $a_j \u2208 A$ has a set of possible values $V_j$.\nWe had 21 different attributes: 17 relations and 4 properties (birth city, birth date, employer and university).\nThe profiles were then used to generate questions from templates: one-hop questions took the form \"What was {person}'s {relation/attribute}? {answer}\", while two-hop questions took the form \u201cWhat was {person}'s {relation}'s {relation/attribute}? {answer}\u201d. An earlier version of this work employed diverse paraphrases, but this was abandoned to make the experiment simpler. Thus while the training data is English text, it does not feature any of the variety or structure typically associated with English text.\nIn every case, all facts were included in the dataset as one-hop questions. We created 7 different held out sets of two-hop questions to test generalization under different conditions. To create hold out sets, we selected particular components of two-hop questions and removed every two-hop question from the training data that featured this component. The held out components were:\n\u2022 First Entities ($e_1$)\n\u2022 First Relations (r)\n\u2022 Second Entities ($e_2$)\n\u2022 Attributes (a)\n\u2022 Entity 1-relation pairs ($e_1$,r)\n\u2022 Entity 2-attribute pairs ($e_2$, a)\n\u2022 Complete questions ($e_1$, r, a)\nFor example, holding out the relation \"mother\" means that we remove all two-hop questions containing the mother relation, but one-hop questions containing \"mother\" are still present in the training data.\""}, {"title": "2.2. Training", "content": "We trained transformers based on the Llama architecture (Dubey et al., 2024) from 250K to 15M parameters. We employed a custom tokenizer trained on our data with a vocabulary of 3K in order to facilitate very small parameter counts, since embedding parameters would take up a large fraction of the total parameters otherwise.\nWe used the \u00b5P parametrization (Yang et al., 2022) for our transformers to enable proper transfer of hyperparameters across different model widths. We used the schedule free AdamW optimizer (Defazio et al., 2024), which allowed us to train for as long as was necessary to achieve our convergence criterion without specifying a learning rate schedule in advance. All tokens but the answer tokens were masked.\nA batch size of 32 sequences was used to train all models, and training text was split into 500 token chunks, so that each batch contained 16K tokens (including special tokens). We trained until the loss decreased by less than $10^{-8}$ per step, this typically took about 10M steps (or 160B tokens) and did not depend strongly on the size of the dataset.\nUnless we were training on one-hop questions only, models were trained on a single one-hop question for every 10 two-hop questions."}, {"title": "2.3. Information Content Measurement", "content": "By dataset entropy, we mean the entropy of a random variable X := ($X_1, X_2, X_3, ...$) such that our entire dataset is considered one sample of it.\nWe define such a random variable as\nX = ($X_{names}, X_{birth dates}, X_{e1a1}, ..., X_{e1aA},..., X_{enaA}$)\nWhere $X_{names}$ is a random variable describing the uniform selection of N names from $N_o$ possibilities, and $X_{birth dates}$ a random variable describing the selection of |N| birth dates from $N_o$ possibilities. Conditioned on these two variables, the attribute values $X_{eiaj}$ are uniform IID:\n$P(X_{eiaj} = X_{eniaj} |A\\, \\; X_{names}, X_{birth dates}) = \\frac{1}{|V_i|}$\nHere we assume that the receiver already knows which tokens may be used for each answer type, though they don't know, for example, the actual set of names in the dataset. We neglected terms related to the selection of viable tokens from all possible tokens because this number does not scale with the total number of profiles. There are other terms that we can't easily quantify that also don't scale with the number of profiles (e.g. learning to attend to the relevant tokens in the question), and we therefore opted to focus on the scaling with the dataset size."}, {"title": "3. Results", "content": null}, {"title": "3.1. Information Content", "content": "We did not perfectly reproduce the existing result of a 2 bit per parameter information capacity for one-hop questions and answers. Instead, our one-hop training runs yielded an information capacity of around 1.6 bits per parameter (Figure 1). We did successfully replicate the figure in earlier training runs (Figure 8), but these had somewhat different hyperparameters to the two-hop training runs that appear in this paper - they did not employ \u00b5P parametrization and used 4 relations instead of 17 relations.\nWe observed that transformers exhibited \"stacked curve\" information content scaling for different dataset sizes (Figure 2). These curves generally began far below the hypothesized 2 bit per parameter capacity, and such transformers had not learned much more than uniform guessing of the answers. Subsequently, the content curve almost kisses the 2 bit per parameter line, before leveling off as it approaches the dataset entropy.\nUnder the 2 bits per parameter capacity hypothesis, these results fit the two function composition computational model far better than they fit either of the alternative models. This certainly isn't conclusive evidence for the hypotheses: we were only able to assess a limited range of dataset and parameter sizes, we are testing two hypotheses at once (2 bits per parameter capacity as well as the computational model) and the scaling behaviour is more complex than identified in Allen-Zhu & Li (2024) - here, the scaling curves seem to bend towards both the baseline and dataset entropy lines. It is, however, stronger support for the main hypothesis than any of the identified alternatives.\nThe curved dependence of information content on parameter count may be due to a few factors: first, when the models' performance on either individual hop (or both together) is too weak, it may be difficult to learn the composition algorithm. Second, models may choose to allocate some fraction of their information budget to memorization, and this fraction may increase as they approach perfect train set performance. Because they are still near their capacity limits, allocating budget to inefficient memorization may prevent them from achieving zero loss on the data. This may be weakly supported by the observation that the \u201cgeneralization gap\u201d (that is, the difference between train loss and loss on held out questions) grows with larger parameter counts (Figure 4), though this trend may reverse with sufficiently high parameter counts, see the final data point for N = 1000. Finally, we use a very crude approximation for the measuring the two-hop capacity when the variance in per-question loss is large, and it may be the case that this is responsible for the shape of the curve.\nAllen-Zhu & Li (2024) noted that information capacity did not seem to depend on the model architecture. We also trained deeper models \u2013 12 layers rather than 4 \u2013 and observed a similar but less consistent trend (Figure 9). This may be due to the fact that we tuned optimizer hyperparameters for the shallower model and did not adjust them for the deeper model."}, {"title": "3.2. Two Hop Generalization", "content": "We find the generalization performance of transformers with no chain-of-thought on two-hop problems precisely matches the prediction of Table 1. In short: without chain of thought, there was no generalization to any case where any component of the first or second hops was systematically excluded from the training data, but there was generalization to the case where complete questions were excluded from the training data (Figures 6 and 4 respectively). This matches prior work by Wang et al. (2024) and extends it by systematically excluding different parts of each hop of two-hop questions, whereas prior work systematically excluded facts from appearing anywhere in any two-hop question.\nRecall that training always presented every fact in the context of one-hop questions. Thus our work indicates that facts learned in the context of one-hop questions do not get learned as components of \u201cf1\u201d or \u201cf2\" unless they actually appear in the right position in a two-hop question.\nThe situation was somewhat more complicated for generalization with chain-of-thought. We never saw generalization to held out first entities, and always to held out entity 2-attribute pairs, but other held out elements showed generalization in some training runs but not others without a clear pattern (Figure 7. For generalization in chain-of-thought generations the model had to produce tokens (or sets of tokens) that were never seen in the same position during training; we speculate that the inconsistent generalization was the result of competing heuristics (i.e. low marginal probabilities of tokens appearing in some position vs applying the correct rules to answer a two-hop question). While we did not train many models of this type, it's possible that generalization on particular held out sets for this setup is seed dependent (Zhang et al., 2025). This does indicate that scaling \u2013 which was consistent with recurrent composition \u2013 is not a perfect proxy for generalization."}, {"title": "3.3. Two Hop independent memorization", "content": "We have speculated that models learn both independent memorization and the generalizing two function composition algorithms to carry out two-hop reasoning. We experimented with an alternative training regime, where models were incentivised to memorize two-hop answers independently at the expense of learning one-hop answers. We found that these models never learned to generalize, and exhibited significantly lower information content than generalizing models.\nSpecifically, we used a dataset where each person had 4 relations and 4 properties, rather than 17 relations, and we maintained the ratio of 1 one-hop question for every 10 two-hop questions. In this case, reducing the loss for an individual two-hop question by a fixed amount has $\\frac{10}{4}$ = 2.5 times the impact on the sum of loss for all questions as reducing the loss for an individual one-hop question. We observe that these models do not learn one-hop question answering much beyond the uniform distribution on possible answers, their capacity scaling approximately matches the expected scaling for independent memorization (Figure 5) and they do not generalize to any held out questions at all.\nWe hypothesize that the mechanism at work in this case is that the model first learns to memorize individual two-hop answers, because this has a larger impact on the average loss than memorizing one-hop answers, and transformers tend to learn simple rules first (see, for example Belrose et al. (2024)). However, due to the limited capacity of our small models, they continue memorizing answers until they do not have capacity to memorize further. This leads the models being trapped in a local minimum: improving performance on one-hop questions (which is necessary to learn function composition) requires making performance worse on the memorized two-hop questions due to capacity limitations. Thus they are \"trapped\" having learned the nongeneralizing function, even though in principle they have the capacity to perform substantially better if they learned to generalize."}, {"title": "3.4. Discussion and Conclusion", "content": "We set out to explore the viability of using information content measurement as a complementary method of transformer interpretability. We adopted a hypothesis driven approach - we guessed that transformers might implement a certain kind of function, deduced information content scaling principles for this kind of function, and tested whether the capacity for transformers to learn the dataset corresponded to these scaling principles.\nThere are significant challenges to be overcome in order to apply this to transformer interpretability in a scalable fashion. Even in relatively simple cases, it is nontrivial to go from a hypothesis to a capacity scaling curve, and the kinds of transformers we are most interested in are trained on complex and poorly understood data generating processes. We find that we do not reproduce the precise 2 bits per parameter empirical scaling curve on simple factual lookup data, and the fact that dataset choices may introduce uncertainty in this curve makes hypothesis testing more difficult.\nDespite this, we do find some support for our main hypothesis that transformers learn two-hop reasoning by memorizing all relevant facts twice by comparing empirical information contents to the 2 bits per parameter capacity figure. We also find, as expected, that models using chain of thought are much more efficient at learning two-hop reasoning. Finally, we saw that models with unusually low information content did not generalize, supporting the hypothesis that they learned an inefficient and non-generalizing representation of the data.\nA possible real-world example of information content efficiency is the strong performance in mathematical benchmarks for very small distilled \u201creasoning models\" found by DeepSeek-AI et al. (2025). Reasoning models are trained to rely much more on chain-of-thought than traditional language models. As our results suggest, transformers may have a much higher capacity to learn sequential reasoning with chain of thought than in a single forward pass, which seems to require redundant memorization of known facts. It is not clear whether non-reasoning models would actually be running into hard capacity limits here, and it is likely to be difficult to test.\nWhile it is not straightforward to apply this method to interpret models, it is plausible that it can be made to work if the application is compelling enough. The question is whether there are sufficiently compelling applications that cannot be addressed by other methods. At this stage, we are not sure if there are many such applications."}, {"title": "Contributions and Acknowledgements", "content": "David Johnston came up with the main ideas behind this project, did all experiments and analysis, and wrote nearly all of the paper. Nora Belrose gave advice and guidance throughout the project. David and Nora are funded by a grant from Open Philanthropy. We thank Coreweave for computing resources."}, {"title": "A. Information content estimation", "content": null}, {"title": "A.1. Two hop recurrent function composition", "content": "To simplify the following discussion, we consider only the relations that entities possess and not the additional properties.\nNext, we consider the case where we have a collection of two-hop questions and answers. We know that these are generated deterministically from a function f mapping one-hop questions to answers. There may be several such functions that lead to the same two-hop questions and answers: if \u2200j : f(ei,rj) = f(ek, rj) then defining g such that g(ei, rm) = f(ei,rm) unless f(ei, rm) = ei, in which case g(el, rm) = ek will lead to the same two-hop questions and answers. This is extremely unlikely in our generation scheme - we have N entities, with the probability of a perfect match between any two less than $\\frac{17}{N}$, which is very small compared to $\\frac{1}{CN}$.\nThere are unlikely to be other symmetries of f with respect to two-hop question answering. If we want g(ei, rj) = ek \u2260 f(ei, rj) = ep such that f(ep, ri) \u2260 f(ek,r\u0131) for some l, then we require \u2200m : g(ek,rm) = f(ek,rm) (to maintain the same two-hop answers), and also \u2200n : g(eq,rn) = f(ep,rn) for some q (because ep is an intermediate entity in some two-hop questions). If eq does not have the same relations as ep (which would be very unlikely in our setting), we need an additional entity to play the role of ep and so on. It is very likely that this exploding set of constraints eventually leads to a collision with the requirement to answer two-hop questions correctly: if, for example, g(ek,rm) = f(ek, rm) = eq then we require both that \u2200ng(eq, rn) = f(ep, rn) but also \u2200ng(eq, rn) = f(eq, rn) to keep the answers to the two-hop questions the same which, as we've already argued, is very unlikely.\nThus having f is sufficient to construct the set of all two-hop questions and it is also overwhelmingly likely to be necessary. For this reason, we treat the entropy of f \u2013 understood as a random variable whose distribution is given by our data generating process \u2013 to be equal to the entropy of the two-hop question and answer dataset.\nGiven this assumption, we can lower bound the information content of a recurrent function composer based on the loss of its answers to two-hop questions. The model reconstructs a one-hop probability P\u2081 such that\n$P_2(e_{112} |e_1, r_1, r_2) = P_1(e_{11} |e_1, r_1)P_1(e_{112}|e_{11}*, r_2) + \\frac{1 - P_1(e_{11} |e_1, r_2)}{|N|}$\nWhere $e_{x1} = f(e_1,r_1)$ and $e_{x112} = f(e_2,r_2)$. Let $q_{112} := P_2(e_{112}|e_1, r_1, r_2)$, $p_{11} := P_1(e_{11}|e_1, r_1)$ and $p_{112} := P_1(e_{112} |e_{11}*, r_2)$. We assume the $p_{ijs}$ are mutually independent.\n$\\frac{E_{i,j,k~U(N\u00d7R^2)}[q_{ijk}] = E_{i,j~U(N\u00d7R)} [p_{ij}]^2 + E_{i,j~U(N\u00d7R)}\\frac{1 - E[p_{ij}]}{|N|}}{E[p_{ij}] = \\frac{-1 + \\sqrt{1-4|N|(1 - |N|E[q_{ijk}])}}{2|N|}}$\nwhere we have taken the result between 0 and 1.\nWe are interested in the negative sum of log probabilities.\n$\\log E[p_{ij}] = \\log \\left ( \\frac{-1 + \\sqrt{1-4|N|(1 - |N|E[q_{ijk}])}}{2|N|} \\right )$\nWe make a very crude approximation, valid when Var(p) is small, that\n$E[log p_{ij}] \\approx log \\left ( \\frac{-1+\\sqrt{1-4|N|(1 - |N|e^{E[log q_{ijk}]})}}{2|N|} \\right ) $"}, {"title": "A.2. Two hop two function composition", "content": "Recall that two function composition answers two-hop questions using a pair of functions f1 and f2 such that $S_{ijk}$ = $f_2(f_1(e_i, r_j), r_k)$. This has a symmetry that is absent from recurrent function composition: if we permute all of the results of $f_1$ and apply the same permutation to the input entities to $f_2$, we preserve the same two-hop questions and answers (in the recurrent case, we cannot make these permutations independently). This means that the entropy of two function composition is approximately |N| log |N| less than double the entropy of one function composition though, because we train on one-hop question and answers as well, models probably cannot actually exploit this symmetry if they use the memorized facts $f_1$ and $f_2$ to answer one-hop questions.\nFinding a lower bound on two function composition information content is similar to the recurrent function composition case, except we now have two independent \u201cone-hop probabilities\u201d:\n$\\begin{aligned}E[q_{ijk}] &= E\\left [P_{i(j)k}^{hop 1}P_{i(j)k}^{hop 2} + \\frac{1 - P_{i(j)}}{|N|}\\right ]\\end{aligned}$\nwe assume that the model has a fixed budget that is split between first and second hops, and one unit of budget yields a fixed change in the expected log probability, in keeping with our overarching assumption about the connection between model budgets and loss. Thus we reparametrize: $\\epsilon^{hop 1}$ := $E[p_{ij}^{hop 1}]$,$\\epsilon^{hop 2}$ := $E[p_{ij}^{hop 2}]$, $u := \\frac{\\epsilon^{hop 1}}{\\epsilon^{hop 2}}, U := \\frac{\\epsilon}{u^{hop 1}u^{hop 2}}$\n$\\begin{aligned}E[q_{ijk}] &= E\\left [\\frac{u^2 + \\frac{1 - u\\epsilon}{|N|}}{U}\\right ]\\end{aligned}$\nChoosing e to maximise the right hand side gives us the minimal u that satisfies this equation, and the right hand side is clearly maximised when e is minimized.\nThe minimal feasible value of e is either the value that sets $\\epsilon^{hop 1}$ to $\\frac{1}{\\sqrt{N}}$ (because the model \u201calready knows\u201d that the correct answer has at least this probability) or that sets $\\epsilon^{hop 2}$ to 1. The latter is preferred if $u^2$ > $\\frac{1}{\\sqrt{N}}$, the former otherwise.\nThus, following the derivation above, and noting that we want the summed losses for both hops, we have\n$\\frac{e^{-E[log P_{i(j)}^{hop 1}} + -E[log P_{i(j)}^{hop 2}]}{2} \\approx \\begin{cases}\\frac{e^{-2}}{2}\\frac{(1+Var(loss)/2) - \\frac{N}{1-\\sqrt{N}}}}{(1 - \\sqrt{N})}\\\\frac{e^{-2}}{2}\\frac{(1+Var(loss)/2) - (1 - \\frac{1}{\\sqrt{N}})}{(\\sqrt{N} \\epsilon)^2}&(\\sqrt{N} \\epsilon)^2 > \\frac{1}{\\sqrt{N}}\\\\end{cases}$\nthis is our effective loss for information content calculations."}, {"title": "B. Generalization to held out hop elements", "content": null}, {"title": "C. Additional Information Content Scaling Plots", "content": null}]}