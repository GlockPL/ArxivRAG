{"title": "The Decoy Dilemma in Online Medical Information Evaluation", "authors": ["JIQUN LIU", "JIANGEN HE"], "abstract": "Can AI be cognitively biased in automated information judgment tasks? Despite recent progresses in measuring and mitigating social and algorithmic biases in AI and large language models (LLMs), it is not clear to what extent LLMs behave \"rationally\", or if they are also vulnerable to human cognitive bias triggers. To address this open problem, our study, consisting of a crowdsourcing user experiment and a LLM-enabled simulation experiment, compared the credibility assessments by LLM and human judges under potential decoy effects in an information retrieval (IR) setting, and empirically examined the extent to which LLMs are cognitively biased in COVID-19 medical (mis)information assessment tasks compared to traditional human assessors as a baseline. The results, collected from a between-subject user experiment and a LLM-enabled replicate experiment, demonstrate that 1) Larger and more recent LLMs tend to show a higher level of consistency and accuracy in distinguishing credible information from misinformation. However, they are more likely to give higher ratings for misinformation due to the presence of a more salient, decoy misinformation result; 2) While decoy effect occurred in both human and LLM assessments, the effect is more prevalent across different conditions and topics in LLM judgments compared to human credibility ratings. In contrast to the generally assumed \"rationality\" of AI tools, our study empirically confirms the cognitive bias risks embedded in LLM agents, evaluates the decoy impact on LLMs against human credibility assessments, and thereby highlights the complexity and importance of debiasing AI agents and developing psychology-informed Al audit techniques and policies for automated judgment tasks and beyond.", "sections": [{"title": "1 Introduction", "content": "Credibility assessment has been a central theme in information seeking and retrieval (IS&R) and Machine Learning (ML) research, and plays a key role in online information evaluation and decision-making activities [6, 20, 30, 78]. Facilitating and enhancing information credibility assessments is"}, {"title": "2 Related Work", "content": "This section discusses the theories and research progress from the areas related to our study, which will clarify where our interdisciplinary research contributions are situated in the field."}, {"title": "2.1 Cognitive Bias and Decoy Effect", "content": "The decoy effect, also termed the asymmetric dominance effect, has been widely examined and is well-recognized in the disciplines of cognitive psychology [47, 74]. The concept was initially introduced by [66] through controlled experiments and characterizes the phenomenon that the inclusion of a decoy option can significantly influence the perceived attractiveness of the available choices, despite no changes to the actual options. For example, consider a consumer choosing between two smartphones: \"A: a basic model priced at $300\" and \"B: a premium model priced at $600.\" The decision might be difficult to predict. However, when a third option, \"C: a mid-range model priced at $599,\" is introduced as a decoy, option B may become more appealing, even though the decoy option C is not intended to be chosen. The presence of the mid-range model makes the premium model appear to be a better deal in comparison.\nBuilding on [66]'s early work, subsequent research has delved into the decoy effect and sought to validate its influence across various domains, topics, and decision-making scenarios, such as consumer purchasing decisions [25, 76, 79], medical treatment choices [7, 60], and children's behavior [86]. Beyond observable behavioral changes, [24] investigated the neural correlates associated with the decoy effect, and found that choice sets with decoys activated the occipital gyrus while deactivating the inferior parietal gyrus. One possible explanation for the decoy effect is the concept of salience, or the extent to which an option stands out. The presence of a decoy option as a reference point can shift the relative salience of the original ones, and increase the perceived attractiveness of one option [15, 57]. Chen et al. [10, 12] examined the impact of decoy results on document relevance judgments and demonstrated the presence of decoy effects in IR evaluation. Evidence confirming the decoy effect contradicts the \"context-invariant\" assumption and offers an alternative explanation for seemingly irrational choices. Meanwhile, investigating decoy effects, along with other related cognitive biases, may offer us a more realistic, psychology-informed basis for characterizing and evaluating the process of human judgments.\nIn contrast to the controlled and simplified decision-making tasks often employed in behavioral experiments, evaluating medical information in web searches is a much more complex, context-dependent process. This evaluation is shaped by various contextual factors, including the rank position of search results, the nature of the topics being explored, and users' pre-existing beliefs and biases about the information they encounter [12, 54, 85]. These factors significantly influence how users assess the credibility and relevance of medical information, making them more vulnerable to cognitive biases like the decoy effect. The decoy effect, which skews user preferences by introducing an inferior but comparable option, can distort judgments in high-stakes areas such as healthcare, where misinformation can have serious consequences. Studying this bias in the context of medical"}, {"title": "2.2 Information Retrieval Evaluation", "content": "Evaluation has been a central them in IR research, and understanding user assessments and preferences has always been a cornerstone of IR evaluation experiments [21, 23, 31]. Among the various types of judgments, relevance judgment-assessing how topically relevant a document is to a given query-has traditionally served as the foundation for decades of system-centered IR evaluation [27, 53, 68]. The offline evaluation metrics, based on query-document relevance, have set the benchmarks for differentiating high-performing retrieval systems from less effective ones and have been critical in developing scalable and reproducible evaluation of ranking algorithms [33].\nWhile the relevance-based Cranfield paradigm has been fundamental in advancing ad hoc factual retrieval techniques [68], its limitations are increasingly apparent when applied to the complexities of real-world information-seeking tasks [5, 38]. The paradigm's emphasis on isolated, relevance-based judgments is insufficient for capturing the nuanced, iterative nature of modern search behavior, particularly in multi-query tasks aimed at resolving complex or exploratory information needs [4]. Real-world information retrieval is rarely confined to a single query-response cycle; rather, it involves users continuously refining queries and synthesizing information across multiple documents as their understanding evolves. Topical relevance alone does not account for the broader range of judgments users make, which are influenced by factors such as information quality, credibility, and trust. Consequently, there is a growing recognition within the IR community of the need to extend evaluation frameworks beyond relevance and to incorporate a more comprehensive view of information-seeking behavior, particularly through the lens of sessions, where users engage in multi-query interactions over time [37]. A session-based evaluation approach offers a more realistic and holistic view of user interaction with IR systems, capturing not only the relevance of individual results but also the temporal and cognitive processes that underpin users' judgments and decision-making in more complex search scenarios.\nRecent research has increasingly focused on expanding the scope of IR evaluation by examining dimensions beyond topical relevance, with an emphasis on user-centered factors that are critical in real-world contexts. Key dimensions such as usefulness [14, 38], trustworthiness [48, 71, 81], and credibility [22, 36, 77, 78] are essential in shaping user judgments, particularly in domains where information quality and reliability are essential. In health and medical information seeking and retrieval, for instance, credibility judgments are crucial as users navigate conflicting information from potentially unreliable sources [61, 73]. These dimensions are often inadequately captured by traditional relevance-based metrics, which highlights the need for more robust evaluation"}, {"title": "2.3 LLM in Information Judgment Tasks", "content": "The integration of LLMs into IR evaluation frameworks offers significant potential to enrich and advance the field by simulating more sophisticated and behaviorally realistic search interactions [52, 65]. Unlike traditional interaction models, which often rely on simplified or formalized representations of user behavior, LLMs possess the ability to process and generate natural language in ways that closely mimic human communication and decision-making processes. This capability allows for the creation of richer, more authentic synthetic data that better reflects the complexity of real-world search scenarios [56, 72]. By leveraging LLMs, researchers can explore a wider range of user behaviors, such as query reformulation, information synthesis, and the negotiation of conflicting information, all of which are essential for understanding user judgments in dynamic and multifaceted information environments. Furthermore, LLMs have the potential to enhance the simulation of multi-query sessions, enabling a deeper examination of how users assess the credibility and relevance of information over time. This is particularly relevant for complex IR tasks, where users may need to evaluate contradictory sources or synthesize information from diverse domains [1]. By offering a more nuanced understanding of credibility assessments and other critical dimensions of user interaction, LLMs can contribute to the development of more comprehensive evaluation frameworks that capture the full breadth of user judgments and experiences in IR systems [56, 80]. These advancements may ultimately lead to more effective, trustworthy IR systems capable of meeting the demands of increasingly complex and information-rich environments.\nDriven by recent research progresses in Natural Language Processing (NLP), LLM has been increasingly applied and assessed in tasks that are traditionally completed by human assessors, including information judgment tasks in IR and recommender systems evaluation [19, 87]. The adoption of LLM in these tasks are usually motivated by LLMs' flexibility in prompting, scalability of automated labeling, and the capability in processing customized natural language inputs and outputs [28, 65, 72]. Despite the strengths of LLMs, challenges remain in the application of LLMs for information judgments, especially in IR settings [16]. Issues such as model bias and the need for domain-specific training continue to be significant concerns. For example, Chen et al. [8] addressed the bias in LLM outputs and proposed methods to measure fairness and reliability in"}, {"title": "3 Research Questions", "content": "With the research challenges and open problems identified above, our study on investigating the credibility assessments from both LLM and human judges seeks to answer two research questions:\nRQ1: To what extent are LLM agents vulnerable to decoy effects compared to human judges in evaluating the credibility of online medical information in Web search?\nRQ2: To what extent do LLM agents' vulnerability to decoy effects vary across different topics and assessment contexts?"}, {"title": "4 Methodology", "content": "To answer the research questions, our project collected credibility assessment labels on online medical information from both AI and human judges in Web search session contexts, and examined and compared the extent to which their judgments are cognitively biased under potential decoy effects. The data collection phase of our work consists of two components: 1) Crowdsourcing user experiment where we observed users' behavioral patterns and credibility judgments on COVID treatment medical information under decoy and baseline conditions in Web search. 2) LLM-based evaluation experiment that mimics the crowdsourcing user study and collects credibility judgments from multiple LLM agents under the same setting (e.g. topics, Web pages with associated queries and tasks, search result ranking/judgment sequence, decoy and baseline conditions). Data collected from these two studies allows us to assess the vulnerability of varying LLM agents to decoy bias in information credibility assessments and also compare their degree of bias against human judgments as a baseline. The following sections will present the details of study design and data analysis."}, {"title": "4.1 Crowdsourcing User Experiment", "content": "4.1.1 Participants. We utilized Prolific \u00b9 for our online crowdsourcing study, as it ensures access to a diverse and pre-screened participant pool while maintaining high data quality through transparent participant approval processes and fair compensation [45]. Five prescreeners were applied: 1) Native English speakers, 2) No professional education in medicine or related fields, 3) At least 50% of studies completed with approval, and 4) No literacy difficulties. A total of 40 participants completed the pilot, and 617 completed the full experiment: 157 in Control A, 148 in Control B, 158 in Treatment Group A, and 154 in Treatment Group B. Of the participants, 52.7% were female, 46.0% male, and 1.3% preferred not to disclose their gender. Ages ranged from 18 to 83 years (M=27.8, SD=8.5). 41.0% had an undergraduate degree, 29.0% a high school diploma, 17.8% a graduate degree, and 9.7% a community college degree. Participants primarily joined from the United Kingdom (60.6%), the United States (25.3%), Canada (8.4%), and Australia (4.7%). Our experiment was approved by the University of Oklahoma Institutional Review Board (IRB #: 13527)."}, {"title": "4.1.2 Topics and Sessions", "content": "To simulate the experience of evaluating pages within sessions, we selected three COVID-19 treatment topics that demand a relatively high level of domain knowledge, compared to common COVID topics that are widely debated in general topics: Monoclonal Antibodies, ACE and ARB inhibitors, and Hydroxychloroquine. We focused on COVID-19 treatments because 1) this is a critical medical area with significant social implications, and 2) narrowing the scope helps control for variations in behavior due to different topics, thus reducing contextual effects on decoy measures. The questions corresponding to the tasks are: 1) Can monoclonal antibodies cure COVID-19? 2) Can ACE and ARBs worsen COVID-19? 3) Can hydroxychloroquine treat COVID-19?\nMonoclonal antibodies are lab-created molecules designed to mimic the immune system in combating pathogens, including viruses. These antibodies were developed to target SARS-CoV-2, the virus causing COVID-19, and were used as treatment for high-risk patients. ACE inhibitors and ARBs, commonly used to treat hypertension and heart failure, gained attention during the pandemic due to their interaction with the renin-angiotensin-aldosterone system (RAAS), which also regulates blood pressure and fluid balance. Hydroxychloroquine, an antimalarial drug, was initially considered for COVID-19 treatment due to its antiviral effects seen in vitro, though later studies provided mixed and inconclusive results. We chose these specific topics from the TREC Health Misinformation track dataset 2 for two main reasons: 1) The questions related to these topics have YES/NO answers validated by external experts, which provide a reliable ground truth for assessing information credibility and distinguishing correct information from misinformation. 2) These topics involve specialized COVID-19 treatment knowledge, making them less familiar to most participants. The ground truth labels helped us reliably evaluate user judgments. We collected 40 web pages for each topic, using four predefined query-SERP combinations with ten organic search results per SERP."}, {"title": "4.1.3 Decoy Design", "content": "To examine how the decoy effect can occur on SERPs, we strategically de-signed decoy search results by modifying the title and short abstract of snippets within the treatment groups' SERPs. Each COVID treatment topic featured a decoy result and a target misinformation result that shared closely related subtopics relevant to the task question, while the correct information focused on a slightly different subtopic 3. The decoy result was intentionally crafted as an \"obviously false\" option to amplify the perceived credibility of the target misinformation. This was achieved by enriching the target's snippet with specific subject or entity information (e.g., details about the lab or university conducting the study) and/or statistics (e.g., the number of participants involved in the study). Research suggests that such detailed information leads users to perceive the content as higher quality and more trustworthy [e.g. 17, 85], thereby creating a clear contrast between the target and decoy options in both user and LLM experiments."}, {"title": "4.1.4 Experimental Flow and Data Collection", "content": "We collected data from 540 participants during the first phase, with 45 participants assigned to each of the 12 conditions (3 topics \u00d7 4 treatments). After excluding 60 participants who spent less than five minutes and 6 who provided identical ratings for all articles, data from 474 participants were obtained. To reach the target of 45 effective participants per condition, we conducted a second data collection session. Participants were recruited in batches to ensure that each condition had 45 participants, ultimately resulting in 540 effective data points. The data collection process is shown in Figure 2.\nTo reasonably simulate a comparable, multi-query/SERP search session experience, we asked every participant to complete four consecutive query and SERP combinations/iterations on a given topic and click and open a minimum of three articles from the ranked results on each SERP before they could answer the related questions and finalize their session. To ensure we gathered enough data on participants' assessments of credibility, they were required to rate the credibility level of each article after their reading and before they exited the page. While participants were allowed to revise their ratings after the first read, it was not mandatory. As a result, each participant provided ratings for at least 12 articles. Participants took a median of 12.26 minutes to complete the study, with an average completion time of 14.58 minutes (SD = 7.85 minutes). Participants were compensated two US dollars each."}, {"title": "4.2 LLM-based Evaluation Experiment", "content": "To assess the decoy effect and compare the credibility assessments between human and LLM judges, we replicated the crowdsourcing experiment using LLMs. The process flow of the LLM-based experiment is illustrated in Figure 3. The experiment introduces two types of prompts: single-query and multiple-query prompts.\n(1) Single-query prompts start with a system prompt, followed by three articles retrieved by the current query: a decoy article in the treatment group or a random article in the control group, a target article, and a correct article.\n(2) Multiple-query prompts start by providing a series of previous queries (each task includes four queries) and their corresponding credibility ratings produced by the LLM for those queries. This simulates a multi-query, session-wide context, where the current query is appended to the end of the sequence. This context gives the LLM access to the entire sequence"}, {"title": "5 Results", "content": "This section presents the results from our user and LLM experiments in response to the proposed RQs on human and AI assessments."}, {"title": "5.1 RQ1: Decoy effect on the Credibility Assessments of LLM and Human Judges", "content": "Since all the crowdsourcing participants complete four queries in their studies, we only compared results from LLM-based experiments using multiple-query prompts with human judges here.\n5.1.1 Credibility Assessment. The results in Figure 4 present the mean ratings by topic and article type for the control group across various models. Each subplot represents a different model, and the ratings are categorized by three types: random (blue), target (red), and correct (purple). The bars represent 95% confidence intervals.\nFor crowdsourcing participant ratings from the user study, the correct responses received the highest mean ratings for two of the three topics (hydroxychloroquine and ACE ARB), followed by target and random responses. This pattern suggests that the participants were generally able to differentiate between correct information and misleading information, though some inconsistencies were observed. More recently released GPT and Claude models, including GPT-4O, GPT-4O-mini, Claude-3.5-Sonnet, and Claude-3-Sonnet, demonstrated a significantly more consistent and effective ability to distinguish correct information from both random responses and target misinformation. In these models, there was a clear and noticeable gap in ratings between correct information and other types of content. However, other models also showed a clear distinction between correct"}, {"title": "5.1.2 Compare Decoy Effects between LLMs and Human Judges", "content": "Next, we compare the decoy effects between the results from the crowdsourcing experiment and the LLM-based experiment. Decoy effects can be assessed by examining the differences in ratings assigned to target articles with misinformation between the control group (where no decoy article was present) and the treatment group (where a decoy article with salient misinformation was presented alongside the target article). A t-test was conducted to determine if the observed differences in ratings between the two groups are statistically significant.\nTable 1 presents the decoy effects observed during multiple-query sessions, showing the mean ratings, standard deviations, and t-test statistics across various models and topics. The decoy effect can be detected by comparing the target article ratings in the treatment group against the control group. There is a decoy effect occurred only when the target article's rating in the treatment group is significantly higher than in the control group, i.e., an increase in target rating due to the presence of the decoy. We observe that target ratings in the treatment group are often significantly higher than in the control group, suggesting that the presence of decoy articles positively influences the perceived credibility of the target articles. This implies that decoy articles, which contain more salient misinformation than target ones, can manipulate LLMs' assessment, which makes misinformation in target articles appear more credible than it would be with the decoy.\nInterestingly, the models that are more likely affected by decoy effects tend to be the more recent and larger LLMs, such as GPT-4O and Claude-3-Sonnet, which are also the models that generally performed better in credibility assessments, as illustrated in Figures 4 and 5. This suggests that while these models are better at identifying correct information, they may also be more vulnerable to cognitive biases triggered by the presence of strongly misleading decoy articles. On the other hand, Llama models appear less affected by the presence of decoy articles, exhibiting smaller differences in target ratings between control and treatment groups. This suggests that while Llama models"}, {"title": "5.2 RQ2: Decoy Effect on LLM Agents under Varying Assessment Contexts", "content": "In Section 5.1.2, we analyzed the results of the LLM-based experiment using multiple-query prompts (see Figure 3), where multiple queries were presented in a single prompt, maintaining context across the queries (there are four queries for each topic). However, chat-based large language models (LLMs) can also process information without retaining conversational context between queries, which can affect their performance. Thus, we conducted another LLM-based experiment by using single-query prompts, which means articles in a query would be in separate query from other three queries for a topic and each query would be a single prompt to LLMs.\nAs shown in Table 3, the decoy effects were far less prevalent in this single-query setup compared to the multiple-query experiment (see Table 1). No model showed decoy effects for more than one topic in the single-query format. Decoy effects were notably reduced, suggesting that LLMs are less susceptible to decoy manipulation when context is not carried over between queries. While some decoy effects were still observed in isolated cases for certain models (e.g., Llama-3.1-70B and Claude3-haiku for Topic 1, and Llama models for Topic 2), overall, the single-query structure appeared to mitigate this vulnerability. This indicates that LLMs' ability to maintain context plays a substantial role in their susceptibility to decoy articles. Reducing contextual carryover between queries could be a useful strategy to minimize the impact of decoy effects.\nTo better understand how session contexts, with memory of previous queries, enhanced decoy effects, we analyzed the ratings for target articles by query/SERP sequence in both the single-query and multiple-query experiments. The comparison is shown in Figures 6 and 7. Each subplot in these figures represents a different model. By comparing the two figures, we observe that the ratings for target articles on Page 0 (i.e., the first SERP) are very similar between the single-query and multiple-query setups, since there is no interaction context or session memory in either scenario at this point. However, the credibility ratings on target results in the treatment group vary greatly between single-query and multiple-query prompts, while the ratings for target articles in the control group remain similar between using the two types of prompts. There are only two models present similar trends over pages between the two types of prompts: Llama-3.1-70B and Claude-3-5-sonnet. GPT-40, Claude-3-sonnet, and GPT-3.5-turbo present the most dramatic difference between the single-query and whole-session (multiple-query) prompts, which explained their different decoy effects presented in Table 1 (for multiple-query prompts) and Table 3 (for single-query prompts). These results suggest that session memory and interaction context in multiple-query prompts"}, {"title": "6 Discussion", "content": "6.1 Decoy Effect on Credibility Judgment\nOur study examined decoy effects on credibility assessments by both LLMs and human judges. Results from the two experiments showed that recent GPT and Claude models were generally effective at distinguishing credible information from misinformation, with correct articles receiv-ing higher ratings across most COVID-19 treatment topics. Notably, LLMs, especially advanced models such as GPT-40 and Claude-3-Sonnet, were more susceptible to decoy effects compared to human judgers, particularly when using multiple-query or session-aware prompts that retain session context. In contrast, human judges exhibited decoy effects differently across varying topics, influenced by contextual features, such as prior knowledge and the sequence of article interaction. These findings emphasize the need to characterize and measure decoy effects in both human and AI-based information assessments.\n6.2 Automated Judgment and AI Debiasing\nThe increasing reliance on Generative AI for information judgment causes important concerns regarding cognitive biases embedded within these systems. LLMs, while adept at processing and generating text, are influenced by biases present in training data, which can lead to outputs that reflect and potentially reinforce misinformation and skewed interpretations. Examples include confirmation bias, where LLMs may prioritize information aligning with dominant viewpoints, and anchoring bias, where initial information disproportionately shapes subsequent judgments. Additionally, the decoy effect poses a specific risk, as LLMs might unintentionally emphasize certain information over others due to contextual presentation, leading to distorted credibility judgments. These risks are particularly critical in areas such as health and medical evaluation and decision-making, where accuracy and fairness are essential.\nTo mitigate these risks, it is crucial to build and assess AI debiasing and auditing strategies. Debiasing could involve enhancing training datasets to better represent diverse perspectives and designing algorithms that detect and mitigate cognitive biases during the LLM's information synthe-sizing and judgment processes. The potential for the decoy effect to influence LLM outputs revealed in our study further highlights the need for rigorous auditing systems that monitor how information is presented and prioritized. This could include real-time auditing tools that flag potentially biased outputs and regular external audits of AI systems in sensitive domains. Collaboration between"}, {"title": "6.3 Limitation and Future Directions", "content": "Our study contributes valuable insights into the intersection of cognitive biases, credibility assess-ments, and AI systems, while also recognizing several limitations that present promising directions for future research. First, our experiments, which examined both crowdsourcing and large language model (LLM) systems, focused specifically on COVID-19 treatment information. While this focus allowed for an in-depth exploration of a highly relevant and critical area during the pandemic, it remains to be seen how the decoy effect on credibility judgments manifests across other domains. The decoy effect is a well-established cognitive bias in decision-making, and extending this research to domains such as legal, financial, and scientific information would offer a more comprehensive understanding of its broader impact.\nAdditionally, the LLMs used in our experiments were trained on different datasets with varying knowledge cut-off dates, much similar to the human assessors in crowdsourced studies who bring diverse backgrounds, expertise, and understanding of the judgment tasks. These variances suggest that different LLMs may exhibit differing levels of susceptibility to decoy manipulations. While this study did not directly compare these variations, the potential for LLMs trained on more up-to-date or specialized datasets to be more or less vulnerable to cognitive biases opens a rich avenue for future inquiry. Understanding these dynamics could significantly advance our understanding of how model training and data specificity affect AI performance in credibility judgments.\nDespite these limitations, our work addresses a highly relevant and timely issue. COVID-19 treat-ment information has been critical for global populations, particularly marginalized communities who have been disproportionately impacted by both the pandemic and the spread of misinformation. By focusing on this pressing challenge, our study sheds light on how cognitive biases like the decoy effect can compromise the ability of both human and AI systems to accurately assess the credibility of health-related information. In this way, our research provides crucial insights into the vulnerabilities of AI systems in the context of medical misinformation, a global challenge that remains unresolved. These findings could guide the development of strategies to better protect users from the cognitive manipulations that contribute to the spread of misinformation. In addition, our experimental framework, designed to assess the decoy effect on both human and AI judgments, offers a replicable and scalable methodology. This framework can be adapted for use in different application domains, model types, user populations and task scenarios, making it a valuable tool and research resource for future studies, especially on the effectiveness and fairness of AI in judgment tasks. By applying this approach to other domains and exploring a broader range of tasks and user characteristics, researchers can deepen our understanding of how cognitive biases like the decoy effect influence decision-making across varied contexts.\nMoreover, our work paves the way for future exploration of bias mitigation techniques that could be applied to LLMs at relatively low cost. Strategies such as prompt engineering and model fine-tuning hold promise for reducing AI systems' susceptibility to decoy effects and other cog-nitive biases, and future studies could evaluate the effectiveness of these interventions. Further, by extending our decoy effect designs, researchers could investigate the moderating effects of contextual factors, such as task complexity, question structures, or conversational interactions, on"}, {"title": "7 Conclusion", "content": "Can AI be cognitively biased, especially in critical judgment tasks? In other words: If AI can (partially) do human information processing tasks, would they be \"purely rational\", or would they also make human-like mistakes under cognitive bias triggers? Motivated by this broad research problem, our study, focusing on the potential decoy effect on both human and LLM judges, investigated the impact of decoy results on the credibility assessments on COVID medical (mis)information. In contrast to the generally assumed \"rationality\" of AI (especially by the general public who increasingly use AI in a variety of information retrieval, judgment and decision-making scenarios), our study empirically confirms the cognitive bias risks embedded in LLM agents, evaluates the decoy impact on LLMs against human assessors' behaviors, and highlights the complexity and importance of debiasing and auditing AI agents in judgment tasks. Our research reveals a new, cognitive bias dimension of AI bias identification and debiasing, which differs from widely discussed social and algorithmic biases, and can enhance our understanding of LLM's limitations in conducting and facilitating human judgment tasks. Our human-LLM dual experimental design can also inform future studies on exploring, evaluating, and mitigating LLM\u02bcs cognitive biases of varying types (e.g. reference dependence [11, 42], confirmation bias [39, 69, 75], expectation disconfirmation [70], threshold priming [55]) inherited from human-generated data, and facilitate the development of bias-aware LLM evaluation benchmarks in human-centered IR and AI research."}]}