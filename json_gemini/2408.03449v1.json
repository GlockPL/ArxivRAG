{"title": "EEGMobile: Enhancing Speed and Accuracy in EEG-Based Gaze Prediction with Advanced Mobile Architectures", "authors": ["Teng Liang", "Andrews Damoah"], "abstract": "Electroencephalography (EEG) analysis is an important domain in the realm of Brain-Computer Interface (BCI) research. To ensure BCI devices are capable of providing practical applications in the real world, brain signal processing techniques must be fast, accurate, and resource-conscious to deliver low-latency neural analytics. This study presents a model that leverages a pre-trained MobileViT alongside Knowledge Distillation (KD) for EEG regression tasks. Our results showcase that this model is capable of performing at a level comparable (only 3% lower) to the previous State-Of-The-Art (SOTA) on the EEGEyeNet Absolute Position Task while being 33% faster and 60% smaller. Our research presents a cost-effective model applicable to resource-constrained devices and contributes to expanding future research on lightweight, mobile-friendly models for EEG regression.", "sections": [{"title": "1. Introduction", "content": "Electroencephalography (EEG) signal analysis is a pivotal research subject contribut- ing to the advancement of Brain-Computer Interfaces (BCI). Furthermore, the applica- tion of Machine Learning (ML) and Deep Learning (DL) algorithms has become a key component for EEG analysis, which has only grown steadily across the years (Sun and Mou, 2023). The EEGEyeNet dataset has become the centerpiece in this field fusing advanced EEG data compilation with cutting-edge ML and DL techniques. (Murungi et al., 2023a; Dou et al., 2022; Wolf et al., 2022; Rolff et al., 2022; Kastrati et al., 2023; Farago et al., 2022). Its popularity is underscored by its large collection of high-quality EEG and eye-tracking (ET) recordings alongside baseline ML and DL models to bench- mark accuracy on a variety of ET tasks. The advent of EEGEyeNet has spawned further research into applications of new DL algorithms for EEG-ET tasks Yang and Modesitt (2023); Modesitt et al. (2024); Dou et al. (2022); Rolff et al. (2022). While much of this research is focused on improving predictive accuracy, for these methods to be prac- tical in real-world scenarios, there is a need to develop more computationally efficient models."}, {"title": "1.1 Research Question", "content": "Is a MobileViT-based model viable for faster, SOTA-comparable accuracy on the EEGEyeNet dataset?\nBy answering this question, we hope to contribute to research into the development of more memory-conscious models designed for resource-constrained devices, such as mobile phones. Our research contributions highlight the potential for expanding the ac- cessibility of EEG-based eye-tracking technology to a wider audience, further interpo- lating neuroscience and Human-Computer Interaction (HCI) for medical applications."}, {"title": "2. Related Work", "content": null}, {"title": "2.1 EEG and Deep Learning", "content": "Electroencephalography (EEG) is extensively used in various research domains, includ- ing neural engineering, neuroscience, biomedical engineering, and brain-like comput- ing, especially in the development of brain-computer interfaces (BCIs). The study of EEG signals is essential for the progress of BCIs, providing deep insights into the com- plex neural activities of the human brain.\nIn the past decade, a wide range of machine learning and deep learning algorithms have been applied to EEG data, leading to significant advancements in numerous appli- cations. These applications include emotion recognition, motor imagery, mental work- load assessment, seizure detection, Alzheimer's disease classification, and sleep stage scoring, among others (Craik et al., 2019; Kastrati et al., 2021a; Roy et al., 2019; Alta- heri et al., 2023; Qu, 2022; Gao et al., 2021; Hossain et al., 2023; Yi and Qu, 2022; Key et al., 2024; Li et al., 2024; Koome Murungi et al., 2023; Murungi et al., 2023b; Dou et al., 2022; Zhou et al., 2022; Qu et al., 2020b,c,a, 2018, 2019; Saeidi et al., 2021; Qu and Hickey, 2022; Rasheed et al., 2020; Dadebayev et al., 2022; Wang and Qu, 2022;"}, {"title": "2.2 Vision Transformers for EEG Regression", "content": "Vision Transformers are well regarded as being extremely adept at performing a wide variety of image tasks when pre-trained on large datasets Dosovitskiy et al. (2021). This is a result of the self-attention mechanism, which captures information across se- quences of pixel patches, allowing the model to build a more robust representation of the entire image. While ViTs have been nominally utilized in image analysis, recent re- search has unmasked the applicability of ViTs for EEG analysis. The study presenting the EEGViT model highlighted how transformers' strong global and sequential data"}, {"title": "2.3 Lightweight and Mobile Networks", "content": "The use of lightweight models is the standard for vision tasks on mobile or resource- constrained devices. Typically, these models are adapted from CNNs, which, due to their spatial inductive bias, yield better performance than ViTs at lower parameter sizes. Lightweight CNNs can also be more effortlessly adapted for downstream tasks and are generally easier to optimize than ViTs, which require extensive data augmentation and regularization Xiao et al. (2021). However, to learn global representations, ViTs need to be integrated in some manner. MobileViT introduces a hybrid, lightweight model that treats transformers as convolutions Mehta and Rastegari (2022a). They achieve this by modifying standard convolution operations to encode global features via a transformer, seen in Figure 2, permitting both local and global processing, resulting in better accu- racy than other lightweight networks. Further optimizations were made to MobileViT, in the subsequently named MobileViTV2. This primarily involved altering the self- attention mechanism from using the multi-headed standard to a separable approach. Using this method, rather than attention scores being computed with respect to each patch, they are computed with respect to a single latent token, reducing the time com- plexity from quadratic to linear with respect to the number of patches. Additionally, the removal of the skip connection within the MobileViT block provided a minor improve- ment to the model's task performance."}, {"title": "2.4 Knowledge Distillation for Lightweight Models", "content": "While lightweight models perform reasonably well across various image task bench- marks, they still fall short in terms of accuracy when compared to their larger counter- parts. Due to the necessity of a resource-conscious design, architectural modifications are more difficult to make, limiting their overall performance Gao and Zhou (2023). One proven way to improve the performance of lightweight models is through model compression, with knowledge distillation being one well-known technique to achieve this. The main methodology behind this procedure is to have a larger, more complex network (teacher) with a smaller, simpler model (student) and train the student model using a \"distillation loss\" calculated from the combined losses of the student and teacher models Hinton et al. (2015). This causes the student model to mimic the teacher model's behavior, effectively \"compressing\u201d the teacher into a smaller network. This is espe- cially good for lightweight networks, as they are able to enjoy enhanced predictive accuracy alongside their computational efficiency.\nKnowledge distillation is an incredibly popular and useful technique in designing deployable DL models for resource-constrained devices, as this training procedure can result in better accuracy without affecting the model's size or speed Cui et al. (2024). This makes it an attractive method to aid in developing enhanced lightweight networks for EEG analysis. These networks, with higher accuracy and efficiency, have the poten- tial to increase practical applications in medical contexts, where speed and accuracy are vital."}, {"title": "3. Methods", "content": "In our study, we concentrated on the Absolute Position task within the EEGEyeNet dataset, opting for the MobileViT architecture due to its proven superior performance relative to other similarly sized models. We incorporated a meticulously fine-tuned EEGVIT-TCNet as the teacher model in our knowledge distillation process, aimed at en- hancing the MobileViT student model's accuracy. The accuracy of our proposed model was rigorously evaluated using the Root Mean Square Error (RMSE) on the test set, in addition to measuring the computational speed during inference, and the model's parameter count."}, {"title": "3.1 Dataset", "content": "This study employed the EEGEyeNet dataset for the training and validation of our model. (Wang and Wang, 2022; Fuhl et al., 2023; Xiang and Abdelmonsef, 2022; Mod- esitt et al., 2023; Mishra et al., 2023) This dataset encompasses EEG and ET recordings from 356 adults, 190 of whom were female and 166 were male, ranging in age from 18 to 80 years old. Researchers obtained written consent from all participants prior to data collection and compensated participants monetarily. The EEG data was recorded on the EEG Geodesic Hydrocel system with 128 channels and a 500 Hz sampling rate. The impedance of each electrode was analyzed between recording sessions and kept at a maximum of 40 KOhm. Eye positions were concurrently recorded with an EyeLink 1000 Plus at the same sampling rate. The eye tracking was calibrated using a 9-point grid before each recording and validated to ensure an average error of less than 1\u00b0 for the measurement of all points. Participants were seated 68 cm from a 24-inch monitor with a resolution of 800x600 pixels, with their heads stabilized in a chin rest position.\nTo address artifacts present in the EEG recordings as a result of environmental and psychological noise, the necessary preprocessing steps were taken. This process in- volved the detection and correction of bad electrodes, along with running a 40 Hz high- pass filter and a 0.5 Hz low-pass filter on the data. The EEG data was then synchronized alongside the eye-tracking data to ensure time-locked analyses at the onset of relevant events with errors not exceeding 2 ms.\nIn the Large Grid Paradigm, participants were asked to focus on a sequence of 25 dots located in different positions on the screen. Dots were each presented for roughly 1.5 to 1.8 seconds, and their positions were selected to ensure maximal coverage of the screen area. This procedure was separated into five experimental blocks, each display- ing a series of 27 dots, with the center dot appearing three times in a pseudo-randomized order to reduce the predictability of subsequent dots. This entire experiment was then repeated six times.\nThe Absolute Position benchmark data was performed using the Large Grid Paradigm. This task involves determining the subject's gaze position as an XY coordinate pair. Each sample of one second describes a single fixation from a participant. The bench- mark contains 21464 samples from 27 participants. This task was specifically important for our research as it provided a diverse array of gaze positions combined with a high sample count, allowing for a more comprehensive analysis of EEG-ET patterns, integral to determining the XY coordinate positions (Kastrati et al., 2021b)."}, {"title": "3.2 Model Architecture", "content": "The architecture of our model can be seen in Figure 3. Our model design is adapted from the EEGVIT-TCNet architecture and includes three main components that construct the entire network architecture.\nTemporal Convolutional Network Block: The TCN block is initialized with a 129-dimension input layer, specifically attuned to the number of EEG channels with an extra dimension for encoding grounding information. Three additional layers with channel sizes 64, 128, and 256 are utilized to build a comprehensive representation of the temporal dependencies within the recorded EEG signals. A uniform kernel of"}, {"title": "3.3 Training and Evaluation:", "content": "Data from the Absolute Position tasks was split 70% for training, 15% for validation, and 15% for testing. To maintain data integrity, we ensured data points with eye posi- tions outside of the 800-by-600 pixel range were excluded. Our model was then trained for a total of 15 epochs, with each training iteration consisting of 64 sample batches"}, {"title": "4. Results", "content": "Results from our exhaustive evaluation illustrate EEGMobile's ability to deliver SOTA- comparable accuracy at a faster inferencing speed and smaller size, outperforming the other transformer-based models.\nTable 3 shows that our EEGMobile architecture attains an impressively low RMSE of 53.6 mm. This eclipses all EEGEyeNet baseline models, the lowest of which is the CNN with an RMSE of 70.4 (Kastrati et al., 2021b). Likewise, our model outperforms the highest-performing EEGViT model with an RMSE of 55.4 (Yang and Modesitt, 2023). EEGMobile also attains a similar accuracy to EEGViT-TCNet, just 3% shy of its 51.8 RMSE (Modesitt et al., 2024). This notable performance accuracy underscores the robustness of EEGMobile and its ability to capture and generalize key information within the training data."}, {"title": "5. Discussion", "content": "Our meticulous performance evaluation legitimizes EEGMobile as a robust model capa- ble of executing EEG-ET tasks efficiently when compared to other transformer-based models, reinforcing the viability of combining lightweight networks with distillation techniques to develop cost-effective models. Our RMSE results suggest that EEGMo- bile is capable of providing predictive capabilities comparable to the other transformer models on EEG gaze estimation tasks and can gain a robust interpretation of the EEG data despite its small size. Similarly, without the use of KD, EEGMobile has a much higher RMSE of about 76.8. This alludes to the idea that KD aids in guiding the model to learn better representations, enhancing its accuracy. This is further highlighted by the high temperature and lambda parameters, which greatly increased the strength and ef- fect of distillation. We also note that EEGMobile's memory and time efficiency are still very much below those of CNN. However, when examining the components of all three ViT-based models, a significant amount of the total parameter counts stem from the feature extraction layers, which contain over 50 million parameters. As such, there is a strong possibility that integrating feature extraction methods with significantly fewer parameters may further close the size gap between EEGMobile and CNN in terms of speed and size. Our analysis of EEGMobile's size, speed, and task performance illus- trates its potential as an efficient model for practical EEG signal analysis, especially on weaker devices, across various domains.\nThis improved efficiency is material to real-time EEG and eye-tracking applications, where speed and accuracy are important EL Menshawy et al. (2015). Our findings are also particularly relevant to the HCI community, such as in the integration of EEG recording devices into virtual reality and augmented reality technologies to improve interactivity Xiang and Abdelmonsef (2022); Rolff et al. (2022); Xu et al. (2023). Ad- ditionally, due to the model's low parameter count, there are also memory efficiency benefits when it is utilized by resource-constrained BCI devices, allowing the model"}, {"title": "6. Conclusion", "content": "This study highlights the efficacy of lightweight models for EEG-ET tasks. By integrat- ing the MobileViT architecture into a Hybrid Transformer model and utilizing knowl- edge distillation techniques, we present a model with enhanced speed and size with a minimal cost to accuracy compared to the SOTA. Our findings further validate the potential of lightweight networks on EEG regression tasks, but further expand the ac- cessibility of DL-based EEG analysis tools for real-world applications, especially on resource-constrained devices, with the potential to advance the unification of neuro- science and HCI."}]}