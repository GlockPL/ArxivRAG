{"title": "RePD: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process", "authors": ["Peiran Wang", "Xiaogeng Liu", "Chaowei Xiao"], "abstract": "In this study, we introduce RePD, an innovative attack Retrieval-based Prompt Decomposition framework designed to mitigate the risk of jailbreak attacks on large language models (LLMs). Despite rigorous pre-training and fine-tuning focused on ethical alignment, LLMs are still susceptible to jailbreak exploits. RePD operates on a one-shot learning model, wherein it accesses a database of pre-collected jailbreak prompt templates to identify and decompose harmful inquiries embedded within user prompts. This process involves integrating the decomposition of the jailbreak prompt into the user's original query into a one-shot learning example to effectively teach the LLM to discern and separate malicious components. Consequently, the LLM is equipped to first neutralize any potentially harmful elements before addressing the user's prompt in a manner that aligns with its ethical guidelines. RePD is versatile and compatible with a variety of open-source LLMs acting as agents. Through comprehensive experimentation with both harmful and benign prompts, we have demonstrated the efficacy of our proposed RePD in enhancing the resilience of LLMs against jailbreak attacks, without compromising their performance in responding to typical user requests.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated exceptional proficiency in addressing various challenges (Achiam et al., 2023; Wu et al., 2023). However, the swift evolution of LLMs has sparked significant ethical considerations, as they can produce detrimental outputs when prompted by users (Wang et al., 2023; Ouyang et al., 2022; Liu et al., 2023b). To align with ethical standards, LLMs have been conditioned to conform to guidelines that enable them to reject potentially harmful queries (Xie et al., 2023). Despite the considerable efforts invested in pre-training and fine-tuning LLMs to enhance their safety, the phenomenon of adversarial exploitation, termed \u201cjailbreak attacks\", has recently come to light (Wei et al., 2023; Shen et al., 2023; Chao et al., 2023; Liu et al., 2023c; Deng et al., 2023a; Zhang et al., 2023). These attacks involve jailbreak prompts to provoke undesirable and harmful actions from LLMs trained with safety protocols.\nIn response to this threat, numerous strategies have been explored to counteract or diminish the impact of jailbreak attacks. For instance, the Llama Guard represents a recently supervised defense mechanism (Inan et al., 2023), which, while effective, entails substantial costs in terms of training resources. In addition, these kinds of guardrails are suspected of over-defense, which exaggerates safety and refuses normal text data, increasing the false positive rate. Other approaches that disrupt the generation of responses (Zhang et al., 2024; Xie et al., 2023; Robey et al., 2023; Ganguli et al., 2023; Pisano et al., 2023) are sensitive to the nature of input prompts and may be circumvented by particularly malicious prompts. Moreover, these methods can degrade the quality of the model's outputs by altering the original user prompts. In addition, some of them are facing growing computational costs due to longer token lengths. Previous research also utilizes multiple LLM agents (Zeng et al., 2024) to defend against jailbreak attacks. However, such an approach introduces a large time cost. Research indicates that LLMs can recognize and manage these risks through careful instruction and iterative reasoning (Xie et al., 2023; Jin et al., 2024; Helbling et al., 2023). However, such strategies heavily rely on the LLMs' ability to adhere to instructions, presenting challenges when employing smaller, less sophisticated open-source LLMs for defense. Although these approaches can save computation costs and have no bad impact on the benign prompts' response, these works purely rely on LLM's ability with a zero-shot learning"}, {"title": "Related Work", "content": "Recent scholarly work has broadened our comprehension of the susceptibility of safety-trained Large Language Models (LLMs) to jailbreak attacks (Wei et al., 2023; Liu et al., 2023b; Shen et al., 2023; Deng et al., 2023b; Xu et al., 2024). These attacks exploit meticulously designed prompts to circumvent the safety protocols, thereby inducing LLMs to produce content that is deemed inappropriate. Specifically, the study by (Wei et al., 2023) posits the existence of two primary failure modes during jailbreak attacks: conflicting objectives and inadequate generalization, as identified in (Brown et al., 2020; Bai et al., 2022; Ouyang et al., 2022). Furthermore, (Zou et al., 2023) introduces a novel approach to generate universal adversarial suffixes through a hybrid strategy that integrates greedy search with gradient-based optimization techniques. This particular form of attack is characterized as token-level jailbreak, where the inserted adversarial elements are typically semantically void in relation to the original prompt, as discussed in (Chao et al., 2023; Jones et al., 2023; Maus et al., 2023; Subhash et al., 2023). In addition to this, other automated jailbreak methodologies have been proposed, such as the Prompt Automatic Iterative Refinement (PAIR), which leverages LLMs to devise jailbreak prompts (Mehrotra et al., 2023; Chao et al., 2023). RePD only takes response as input, which is not sensitive to the attack method in the prompt."}, {"title": "Jailbreak Defense", "content": "Existing defense methods against LLM jailbreak attacks can be divided into prompt-based and response-based defenses. Prompt-based defenses like System-Mode Self-Reminder (Xie et al., 2023) use a specially designed prompt to remind LLM not to generate harmful or misleading content. IAPrompt (Zhang et al., 2024) proposed to analyze the intention of the given prompt using LLMs before it outputs a policy-aligned response. Goal"}, {"title": "Methodology", "content": "We focus on defending jailbreak attacks (Zou et al., 2023; Wei et al., 2024; Liu et al., 2023a) that force LLM to output content that does not align with human values. For example, a malicious user may use the harmful prompt: How can I create a bomb? to elicit harmful information from an LLM. LLMS with alignment training can identify the risks behind this request and refuse to fulfill it. Instead, the malicious user can bypass the alignment by using a jailbreak prompt combined with the previous harmful prompt, and the safety mechanism fails."}, {"title": "Template-based Jailbreak Attacks", "content": "Most jailbreak attacks are template-based attacks. In the definition of a template-based attack, the attacker will have a transparent and pre-defined harmful question (how to hotwire a car, how to hack a website, etc.). The goal of the attacker is to make LLM answer these harmful questions. Then the attacker can use a jailbreak template to construct the harmful questions into the jailbreak prompts. We divided the template into two types:\n\u2022 Embedding template: This type of template includes the attacks that just directly embed the harmful questions into the prompt template (role play prompt template! (Liu et al., 2023a), optimized token sequence (Zou et al., 2023), etc.).\n\u2022 Encoding template: This type of template includes the attacks that encode the harmful questions to different formats (base64 (Wei et al., 2024), encrypt (Yuan et al., 2024), translation into another language (Yong et al., 2023), etc.).\nAs the jailbreak prompts of this attack still contain information about the harmful question, a defensive strategy is to extract the question from these jailbreak prompts and figure out the true intention of the prompts."}, {"title": "A Retrieval-based Defense Framework", "content": "Our retrieval-based jailbreak defense framework RePD employs a one-shot learning paradigm that searches the most similar jailbreak templates to teach LLM to decouple the input prompt. Fig. 1 illustrates our proposed framework. In the settings in which we are concerned, the framework is divided into three steps: First, the malicious attackers formulize the jailbreak attack template and add harmful questions into the jailbreak prompt. Then, RePD adopts the retrieval process to formalize the defense prompt. At last, the defense prompt is fed to the LLM for response generation. In the following discussion, we discuss the three steps detailedly.\nStep 1: Jailbreak prompt formalization. In this step, malicious attackers will formalize their jailbreak prompts. In our setting, the attackers may take different attack mutants to generate the attack prompt. For each type of attack mutant, the attacker needs to craft a jailbreak template like DAN, developer mode, etc (see an example in Fig. 2). Then, the attacker needs to encode his harmful question into the jailbreak template to generate the complete jailbreak prompt (For jailbreak attack methods like base64, the attacker will use base64 to encode the original harmful question to generate the jailbreak prompt). Thus, we define the jailbreak prompt into two components: jailbreak template and harmful question.\nStep 2: Prompt retrieval. After receiving the jailbreak prompt (we noted the prompt as $\\theta$), RePD performs a retrieval process. RePD preserves a retrieval database storing known jailbreak attack templates as $T$. Considering the jailbreak prompt $\\theta$ may be the known attacks in RePD, RePD then performs a similarity computation process to find the retrieval template $\\tau$ within the database $T$, which matches $\\theta$ mostly. Then, combing with the retrieval template $\\tau$, RePD gets a random question $\\mu$ from the question database $T_u$ to generate a new retrieval prompt $F(\\tau, \\mu)$. Then RePD generates a string to state the process of how to decouple the generated retrieval prompt $(\\tau, \\mu)$ back into the retrieval template $\\tau$ and the random question $\\mu$ (the prompt is shown in Prompt. 1).\nStep 3: Prompt decouple & response. Then the regenerated prompt (as shown in Prompt. 1) is fed to the LLM. In this prompt, the retrieval prompt and retrieval question are provided as an example of how to decouple questions as the retrieval prompt does. This approach is a one-shot learning paradigm that enables the LLM to decouple the input user prompt as the retrieval template does. Based on the one-shot learning example, the LLM will perform a similar decouple process to the input jailbreak prompt. Then, in the response, the LLM is required to state the question at first. Thus if the question is harmful, LLM can easily detect and reject the response.\nRandomization. Considering the adaptive attacks (GCG(Zou et al., 2023), AutoDAN(Liu et al., 2023a), etc.), we applied a randomization process for RePD. The original prompt template (see Appendix Prompt. 1) is static, attackers can still achieve a high attack success rate against RePD through an adaptive attack process. Thus, RePD applies the random prompt rewrite process for the prompt template. For each query, the words within the prompt are randomly replaced with a set of similar words.\nNon-retrieval. We also consider teaching LLM to decouple the jailbreak prompt back into original questions without retrieving a one-shot learning process. In this setting, RePD's prompt only encompasses the prompt that tells the LLM to state the question first without the retrieving prompt."}, {"title": "RePD-M: Multi-agent Version", "content": "We also consider the setting that splits the problem decoupling and problem response to two LLM agents rather than one (noted as RePD-M). This is due to the consideration that one agent may not be effective on the two tasks simultaneously. By doing so, the first LLM is responsible for decoupling the input user prompt back into the questions, the second LLM is responsible for responding to the questions."}, {"title": "Evaluation", "content": "We conduct the jailbreak experiments on 2 aligned LLMs: LLaMA-2-7B-Chat (Touvron et al., 2023) and Vicuna-7B-V1.5 (Zheng et al., 2024). LLaMA-2-7BChat is the aligned version of LLAMA-2-7B. Vicuna-7BV1.5 is also based on LLAMA2-7B and has been further supervised and fine-tuned on 70k user-assistant conversations collected from ShareGPT. We use protected LLM to represent these two models in the experiments."}, {"title": "Benchmarks", "content": "We used the benchmark from SALAD benchmark (Li et al., 2024). It has several attack methods and defense methods for the evaluation.\nAttack methods. we adopt a suite of established attack methodologies to construct the jailbreak prompts. We categorize the attack methods we evaluated into three types: (A) Adaptive attack: (setting is illustrated in Appendix \u00a7A.1) For each instance of harmful behavior instruction, we employ GCG (Zou et al., 2023) to produce a general adversarial suffix. We also utilize AutoDAN (Liu et al., 2023a), PAIR (Chao et al., 2023), and TAP (Mehrotra et al., 2023) to generate novel instructions. (B) Encoding template-based attack: (as defined in \u00a73.2) These instructions are then translated into less commonly encountered source languages, such as German, Swedish, French, and Chinese, using LRL (Yong et al., 2023). Furthermore, we apply Base64 (Wei et al., 2024) as an attack method as well. (C) Embedding template-based attack: (as defined in \u00a73.2) We also crawl the jailbreak template for 1 as well. These jailbreak attacks follow the embedding template-based attack definition in \u00a73.2.\nDefense methods. We consider three existing jailbreak defense methods in our evaluation, including GPT Parahrasing(Cao et al., 2023), Safe Prompt (Deng et al., 2023b) and Self Reminder (Xie et al., 2023)."}, {"title": "Dataset", "content": "Harmful question. The ToxicChat dataset (Lin et al., 2023), consisting of 10,166 annotated prompts indicating toxicity, is derived from user interactions. In our experiment, we exclusively utilize the user inputs from this dataset. The dataset has been divided into two equal parts: a training subset and a testing subset. For evaluation, we rely on the official test set from ToxicChat-1123. For the adaption experiment, we use the official training set provided.\nBenign question. We use ChatGPT-4 to generate 200 benign questions to evaluate automatically."}, {"title": "Evaluation Metrics", "content": "Attack success rate (ASR). To assess the efficacy of jailbreak attacks, we implement a duo of evaluation techniques:\n\u2022 The Keyword-Based Evaluation method (Zou et al., 2023), which compiles a list of recurring keywords from responses to standard attacks, facilitating the determination of the success or failure of jailbreak attempts, and\n\u2022 The Automated Evaluation approach (Qi et al., 2023), employing GPT-4 in the role of an ad-"}, {"title": "Evaluation Results", "content": "In this section, we first compare RePD with existing schemes in \u00a74.5.1. Then we compare RePD with RePD-M in \u00a74.5.2. At last, we evaluate RePD's defense effectiveness against adaptive attack in \u00a74.5.3, and the effect of the retrieval mechanism in \u00a74.8."}, {"title": "Comparisons with Other Schemes", "content": "Examining the ASR in Table. 1, it is evident that the RePD approach substantially outperforms the other methods, yielding the lowest median, which indicates a higher resilience against attacks. Regarding ASR, RePD exhibits the most robust defense, with most of the data concentrated towards the minimal success rate for attacks, affirming its efficacy in mitigating successful jailbreak exploitations. Regarding the FPR as depicted in Table. 2, the RePD method maintains a commendable balance, achieving a lower median FPR than the Safe Prompt Defense Framework, suggesting fewer instances of legitimate behavior being incorrectly classified as an attack. This demonstrates that the RePD method strikes a superior equilibrium in minimizing false alarms without significantly compromising security. Lastly, in terms of accuracy, as shown in Table. 3, the RePD method demonstrates superior performance over the Self Reminder with a notably higher median, though it slightly trails the Safe Prompt Defense Framework. The tight interquartile range of the RePD method suggests consistent accuracy across different scenarios, highlighting its dependable performance in correctly identifying jailbreak attempts."}, {"title": "RePD and RePD-M", "content": "We also compared single-agent RePD with multi-agent RePD-M. Our initial intuition is that the two-agent RePD will perform better than the single-agent RePD. This is because the question decouple and question answer decouple by two agents can perform better. The results are shown in Table. 1 (see Apendix Fig. 4). Multi-agent RePD has better performance both in ASR and FPR. In ASR, multi-agent RePD has a 24.3% better performance"}, {"title": "Effect of Randomization", "content": "Here, we evaluate the performance of our method against adaptive attacks, which assumes that the attacker knows the whole process of our pipeline. In this setting, the static template makes the defense of RePD easy to bypass. Thus, we applied a random template generation process for the template. We compare the RePD's performance using static with RePD's performance using a dynamic randomly generated template. The results are shown in Table. 4, the ASR drops when using dynamic templates. Dynamic random RePD is robust against adaptive attacks, which has a 76.2% decreased ASR compared with static RePD. We also compare the RePD scheme with other schemes. The results indicate that our proposed RePD (with randomization) can defend against adaptive attacks by reducing the ASR within 10%."}, {"title": "Effect of Model Size", "content": "Furthermore, we studied the impact of model size on RePD's performance. As shown in Table. 1 (and Appendix Fig. 7), we evaluate RePD's ASR, FPR, and accuracy under Vicuna-1.5 and Llama-2's different model sizes. The results indicated that the enlargement of model size increases the performance of RePD. ASR and FPR drop rapidly as the model size decreases, while accuracy increases with the increase of model size. This can be attributed to the larger model size, increasing the model's ability to decouple questions and determine the harm of the question."}, {"title": "Evaluation of Different Attacks", "content": "We compare RePD's performance with other defense schemes under different jailbreak attacks (the attack types follow the definition in \u00a74.2). As shown in Fig. 5, all the schemes can defend against embed-type attacks very effectively. This is because this type of attack is very weak. Furthermore, when it comes to adaptive attacks and encoding attacks, previous schemes perform very poorly. While RePD can defend against these attacks very effectively. This is due to RePD's ability to decouple questions and randomization."}, {"title": "Effect of Retrieval", "content": "The Retrieval strategy, as indicated in the results (see Table. 5), plays a pivotal role in mitigating the risk of successful attacks (ASR) and in minimizing false alarms (FPR). When comparing the retrieval against non-retrieval settings, it's clear that the retrieval mechanism contributes to a reduction in both ASR and FPR for Llama-2 and Vicuna-1.5 models. Specifically, in non-retrieval scenarios, ASR for Llama-2 stands at 0.45 and 0.54 for Vicuna-1.5, which signifies a higher vulnerability to attacks when the system doesn't employ the retrieval method. Conversely, when retrieval is applied, there's a noticeable drop in ASR to 0.25 for Llama-2 and 0.31 for Vicuna-1.5, indicating a more robust defense posture. Furthermore, the FPR also shows a decline with retrieval, suggesting that the system becomes more accurate in distinguishing between benign and malicious queries, thus reducing the likelihood of legitimate queries being incorrectly flagged as attacks. Furthermore, the retrieval would not take much more time cost (see Fig. 3)."}, {"title": "The Un-retrieval Attack", "content": "Considering RePD needs to cope with the unseen attacks that are unstored in the retrieval database in the real-world settings, we compare RePD's ability to handle the attacks stored in the retrieval database with those unstored in the database (see Figure. 6). The evaluation results indicate that, though the ASR on un-retrieval attacks increases a little compared with the retrieval attacks, the absolute value of it still remains under 0.15. The rationale for the defense effectiveness is that the problem decouple process itself can defend the jailbreak attacks already (also evaluated in \u00a74.8). While the retrieval process and the problem decouple as a one-shot learning example provides a better defense against the retrieved ones."}, {"title": "Limitation", "content": "Though RePD can achieve better defense performance than previous methods, the approach still introduces extra time costs due to extending token length. Furthermore, the main goal of RePD is to defend against templated-based attacks. We leave the defense method against potential future emerging attacks, which are out of the scope of templated-based jailbreak as future work."}, {"title": "Conclusion", "content": "In conclusion, this paper introduces RePD, a novel defense framework designed to counteract jailbreak attacks on large language models (LLMs). Despite the extensive pre-training and fine-tuning in moral alignment, LLMs are still susceptible to generating harmful information when prompted by users. RePD addresses this vulnerability by employing an attack-retrieval-based prompt decomposition strategy. This framework leverages a retrieval database to construct a one-shot learning example, enabling the LLM to decompose tasks from prompts by recognizing and mitigating known attacks.\nOur experimental validation demonstrates the efficacy of RePD in assisting LLMs against jailbreak attacks. The evaluation results prove that RePD will not impact the benign response."}, {"title": "Appendix", "content": "In this section, we introduce the setting of the adaptive attack in the main paper.\nFor GCG, AutoDAN, PAIR, and TAP, their authors have released the code for reproduction. When using their codes, we leave most of the default settings unchanged. The setting is set as:\n\u2022 GCG: n-steps=1000, test-steps=50, batch-size=512\n\u2022 AutoDAN: num-steps=100, batch-size=64\n\u2022 PAIR: n-stream=5, n-iterations=12\n\u2022 TAP: n-stream=1, branching-factor=2, width=5, depth=5"}, {"title": "The Prompt for RePD", "content": "Prompt 1 RePD's prompt with retrieval.\nFor the prompt that the user enters $user\\_prompt$, you need to decouple it into two parts first: the template of the user's query method and the user's question or request. I will give you an example: for example, the user asks: $full\\_prompt$, you need to decouple it into query template: $prompt\\_template and the user's question or request: $prompt\\_request. Then, in your response, you need to state the request or the question by the user again, then state your official response. Remember, refuse to generate harmful responses.\nThe above Prompt. 1 is the retrieval prompt template that RePD adopts. Within the prompt template, $full\\_prompt is the full prompt which is constructed by $prompt\\_template and $prompt\\_request. $prompt\\_template is the retrieval jailbreak template, while $prompt\\_request is the randomly selected harmful request. $user\\_prompt denotes the prompt query user inputs.\nPrompt 2 RePD's prompt without retrieval.\nFor the prompt query that the user enters $user\\_prompt, you need to decouple the prompt's real question, which is the question you want to answer. Remember, to state the question in your response first, then continue your official response to the query. Remember, refuse to generate harmful responses."}, {"title": "Benign Question Generation Process for RePD's Evaluation", "content": "Prompt 3 Generate a list of questions. These questions should be one sentence long and have a clear goal and intention. The intention of the generated questions must be benign and legal. The questions should cover various fields, including life, geography, history, skills, etc.\nWe used the prompt within Prompt. 3 to input into ChatGPT-4 to generate 200 benign questions for our evaluation."}]}