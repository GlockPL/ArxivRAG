{"title": "COAT: COMPRESSING OPTIMIZER STATES AND ACTIVATION FOR MEMORY-EFFICIENT FP8 TRAINING", "authors": ["Haocheng Xi", "Han Cai", "Ligeng Zhu", "Yao Lu", "Kurt Keutzer", "Jianfei Chen", "Song Han"], "abstract": "FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54\u00d7 compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43\u00d7 end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine's speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training.", "sections": [{"title": "1 INTRODUCTION", "content": "Foundation Models (FMs), such as Large Language Models (LLM) and Vision Language Models (VLM), have made significant breakthroughs in various tasks such as reasoning, understanding, and"}, {"title": "2 RELATED WORK", "content": "Low-precision Training Low precision training (Wang et al., 2018; Chen et al., 2020; Lin et al., 2022; Wortsman et al., 2023; Xi et al., 2024) has become a prominent technique in modern deep learning, offering reductions in both computational costs and memory requirements. FP16 (half-precision) training (Micikevicius et al., 2017) is the most prevalent low-precision method nowadays. It introduces loss scaling to address FP16's narrower representation range problem. BF16 training (Kalamkar et al., 2019) refines this approach, as BF16 has a larger representation range and is more stable for large-scale training. In these approaches, forward and backward passes are computed in FP16 or BF16 precision, while master weights, gradients, and optimizers are stored in FP32."}, {"title": "3 PRELIMINARIES", "content": "FP8 Quantization Quantization compresses a high-precision tensor to low-precision to achieve speedup and save memory footprint, at the cost of lower precision and lower representation range. FP8 format consists of two encodings - E4M3 and E5M2 (Open Compute Project, 2023). E4M3 has higher precision, while E5M2 has a larger representation range. We define E4M3's min and max values as $A_{min}^{E4M3} = 2^{-9}$ and $A_{max}^{E4M3} = 448$, while E5M2's min and max values are $A_{min}^{E5M2} = 2^{-16}$ and $A_{max}^{E5M2} = 57344$. To quantize an FP32 tensor X into E4M3 precision, we use a quantizer $Q(\\cdot)$ to map the tensor into FP8's representation range. This process can be formulated as\n$X_{FP8}, S_x = Q(X_{FP32}), \\text{where } X_{FP8} = \\left[\\frac{X_{FP32}}{S_x}\\right], S_x = \\frac{max(|X_{FP32}|)}{A_{max}^{E4M3}}$\nwhere $S_x$ is the scaling factor, $[\\cdot]$ refers to round-to-nearest. Quantize into E5M2 follows a similar procedure, where we only replace $A_{max}^{E4M3}$ with $A_{max}^{E5M2}$. To map the quantized tensor back to FP32 precision, the dequantize operation $DQ(\\cdot)$ can be formulated as $X_{FP32} = DQ(X_{FP8}, S_x) = S_xX_{FP8}$.\nOptimizer Update Rule Optimizers are widely used in deep learning to update parameters. The most common gradient-based optimizer is Adam/AdamW (Kingma, 2014; Loshchilov, 2017), which uses first-order m and second-order momentum v to achieve better convergence. The update rule of AdamW at time step t can be formulated as:\n$m_t = \\beta_1 m_{t-1}+ (1 - \\beta_1) g_{t-1}$ \n$\\hat{m_t} = \\frac{m_t}{1-\\beta_1}$ \n$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g^2_{t-1}$\n$\\hat{v_t} = \\frac{v_t}{1-\\beta_2}$ \n$w_{t+1} = w_t - \\eta \\left(\\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t} + \\epsilon}} + \\lambda w_t \\right)$  (1)"}, {"title": "4 DYNAMIC RANGE EXPANSION FOR ACCURATE OPTIMIZER QUANTIZATION", "content": "In subsequent sections, we explain how COAT utilizes FP8 quantization to achieve memory-efficient FP8 training without compromising accuracy. Section 4 focuses on optimizer states quantization, while Section 5 discusses activation quantization."}, {"title": "4.1 UNDERSTANDING THE ISSUE OF CURRENT OPTIMIZER STATES QUANTIZATION METHOD", "content": "Under per-group quantization, we find that one significant drawback of current quantization methods is that, they can not fully utilize the representation range of FP8 and therefore lead to a large quantization error. Take the E4M3 data format as an example, the ratio between E4M3's maximum representable value and minimum representable value is $A_{max}^{E4M3}/A_{min}^{E4M3} = 448:1 = 229376 \\approx 2\\times10^5$.\nTherefore, for a quantization group X, if we want to fully utilize the 256 representable value\u00b9 of FP8, we hope the dynamic range of the quantization group X should cover the entire span between $A_{min}^{E4M3}$ and $A_{max}^{E4M3}$.\nTo make it more formally, we define dynamic range as the ratio between the maximum absolute value and the minimum absolute value within a quantization group X:\nDefinition 1 (dynamic range) Given a set that consists of G real numbers $X = \\{x_1, x_2, ..., x_G\\}$, the dynamic range R is defined as\n$R_X = \\frac{max(|x_1|, |x_2|, ..., |x_G|)}{min(|x_1|, |x_2|, ..., |x_G|)}$\nwhere $|\\cdot|$ denotes the absolute value.\nThat is to say, E4M3's dynamic range is $R_{E4M3} = 448 \\times 512 = 229376 \\approx 2 \\times 10^5$. However, in practice, many quantization groups within the optimizer states fail to effectively map values across this wide range. We observe that optimizer states are highly sparse, with fewer than 1% of values having large magnitudes, while the majority are relatively small and closely clustered. Most groups exhibit low dynamic ranges since large values are so few."}, {"title": "4.2 DYNAMIC RANGE EXPANSION", "content": "To address this problem, we introduce a expand function $f(\\cdot)$ before quantization to expand the dynamic range of the quantization group and align it with E4M3, which can be formalized as $X_{FP8}, S_x = Q(f(X_{FP32}))$. The expand function we use is defined as\n$f(x) = sign(x)|x|^k$,\nwhere k is used to control the strength of the expansion.\nFor a quantization group X, after applying the expand function f to X, the dynamic range becomes\n$R_{f(x)} = \\frac{max(|f(X)|)}{min(|f(X)|)} = \\frac{max(|sign(X)X^k|)}{min(|sign(X)X^k|)} = \\left( \\frac{max(|X|)}{min(|X|)} \\right)^k = (R_X)^k$.\nTherefore, when k > 1, Rx will be enlarged and become closer to the ideal $R_{E4M3}$. The optimal k satisfy that $(R_X)^k = R_{E4M3}$, which means that $k = log_{R_X}(R_{E4M3})$. With this optimal k, f(X) can fully utilize the representation range of E4M3, while the original X can only utilize a small portion of it. As shown in Figure 2(c), the second-order momentum typically has a larger k value (5 ~ 15) compared to the first-order momentum's k (1 ~ 3). This corresponds to our previous observation, that second-order momentum usually has a smaller dynamic range compared with first-order momentum, so it requires a larger k to align well with E4M3's dynamic range.\nWe calculate k on-the-fly for every optimizer step and for every quantization group for accuracy consideration. When dequantize, we apply the inverse of expand function $f^{-1}(x) = x^{\\frac{1}{k}}$ after de-quantization to recover its original value, which can be expressed as $X_{FP32} = f^{-1}(DQ(X_{FP8}, S_x))$.\nWe apply regular quantizer and our dynamic range expansion method to both the first-order momentum m and second-order momentum v. As visualized in Figure 3(b), the distribution after expansion can fully utilize the FP8 (E4M3) representation range, which proves the effectiveness of our method. We further quantify the effectiveness of our method in Table 1. In AdamW optimizer step, as stated in Eq. 1, $\\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t} + \\epsilon}}$ is the actual effective term for weight update, so we report the MSE of $\\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t} + \\epsilon}}$ to quantify the performance of a quantization method. We find that E4M3 is more suitable for first-order momentum than E5M2. For second order momentum, although E4M3 better than E5M2, their quantization error is nearly the same after applying our expand function. Our Dynamic Range Expansion can effectively reduce the MSE by 1.63\u00d7."}, {"title": "5 MIXED-GRANULARITY ACTIVATION QUANTIZATION", "content": "5.1 DECOMPOSE THE ACTIVATION MEMORY FOOTPRINT\nIn the forward pass of neural networks, activations must be preserved for the backward pass to calculate gradients. As illustrated in Table 2, non-linear layers such as LayerNorm/RMSNorm (Ba, 2016; Zhang & Sennrich, 2019) and Activation Functions (Hendrycks & Gimpel, 2016; Shazeer, 2020)"}, {"title": "5.2 MIXED GRANULARITY FP8 PRECISION FLOW", "content": "To address the inefficiency and inaccurate problem, we propose to use mixed granularity FP8 precision flow to improve the accuracy without introducing too much overhead. FP8 precision flow requires the input and output of all linear and non-linear layers in FP8. By directly saving the input tensor in FP8 format for the backward pass, we eliminate the need for an extra quantization operation, which reduces the associated overhead. However, this method still suffers from accuracy degradation and necessitates further refinement.\nWe propose to vary the quantization granularity across different layers to balance precision and efficiency in a mixed-granularity manner. For non-linear layers, VS-Quant (Dai et al., 2021) or Per-Block Quant (Xi et al., 2024) methods are well-suited due to their fine-grained and precise nature. For linear layers, we apply per-tensor quantization to maximize the performance of Tensor Cores.\nWe observe that quantizing the input of layernorm across multiple token axes is detrimental to accuracy. As illustrated in Figure 4(a), when the number of elements that share a scaling factor is fixed, the quantization error increases significantly when quantization is performed across the token axis. Therefore instead of using per-block quantization with block size B \u00d7 B as proposed in (Xi et al., 2024), we propose to use per-group quantization with group size 1 \u00d7 G, where G = B2 to keep the granularity the same. This approach enhances the accuracy of non-linear layers while maintaining efficiency. Our precise FP8 precision flow is visualized in Figure 1(a), where we display the full precision flow for a Llama-style decoder layer, both forward and backward pass.\nWe also propose Group Scaling, an efficient per-tensor scaling method that balances the performance and precision. To perform per-tensor quantization, the maximum absolute value of the tensor needs to be calculated through max reduction, adding a lot of overhead.\nIn our Group Scaling, we address these problems by splitting the max reduction into two stages: (1) performing max reduction on each 1 \u00d7 G element and storing the results as intermediate values; (2) applying max reduction on the intermediate tensor to obtain the per-tensor max value. The first stage can be seamlessly fused with the previous operation, adding minimal overhead, while the second stage is more efficient than doing max reduction on the entire tensor, as the intermediate result is $\\frac{G}{x}$ smaller than the original tensor."}, {"title": "6 EXPERIMENTS", "content": "6.1 ACCURACY EXPERIMENTS\nSetups We compare COAT with BF16 training and TransformerEngine (NVIDIA, 2024b) baselines. For all experiments, we adopt the default hyperparameters in the official training recipe. We validate the effectiveness of our method on multiple tasks, including Large Language Model (LLM) pretraining and fine-tuning, and Vision Language model (VLM) training. For LLM pertaining, we report the perplexity on Wikitext 103 (Merity et al., 2016), C4 (Raffel et al., 2020), and Pile (Gao et al., 2020), and the accuracy on COPA (Gordon et al., 2012), ARC (Clark et al., 2018), SciQ (Welbl et al., 2017), and HellaSwag (Zellers et al., 2019). For LLM fine-tuning, we conduct experiments in math corpus, and evaluate on Mathmeticas (Davies et al., 2021), SVAMP (Patel et al., 2021), NumGLUE (Mishra et al., 2022), and GSM8K (Cobbe et al., 2021). For VLM training, we report the score on VideoMME (Fu et al., 2024), POPE (Li et al., 2023b), VizWiz (Gurari et al., 2018), GQA (Hudson & Manning, 2019), VQAv2 (Goyal et al., 2017), TextVQA (Singh et al., 2019), SEED (Li et al., 2023a), and MMMU Validation Set (Yue et al., 2024). We use 1 \u00d7 128 per-group quantization for optimizer states and 1 \u00d7 16 per-group quantization for non-linear layer activations."}, {"title": "6.1.1 LLM PRETRAINING", "content": "To evaluate our method when pretraining LLMs, We train OLMo-1B (Groeneveld et al., 2024) and OLMo-7B on Dolma (Soldaini et al., 2024). Following the official report, we use a global batch size of 4M tokens (2048 macro batch size, with a sequence length of 2048 tokens). We use PyTorch FSDP in our experiments. For OLMo-1B, we pretrain for 5500 steps, which corresponds to approximately 22B tokens. For OLMo-7B, due to the resource limitation, we perform continue pretraining for 1000 steps (about 4B tokens) and resume from the official checkpoint at 5000 steps.\nWe report the training curve in Figure 5, and report the perplexity and accuracy result in Table 3. The training curve and downstream task performance were consistent with BF16 training and TransformerEngine baseline, validating the effectiveness of COAT. For OLMo-7B experiment, we report the training curve in Figure 6, where COAT also aligns well with the baseline and is nearly lossless."}, {"title": "6.1.2 LLM FINE-TUNING", "content": "We further evaluate our method on LLM fine-tuning task. We focus on math corpus and fine-tune a Llama-2-7B model on the MAmmoTH (Yue et al., 2023) dataset. We train for 3 epochs, and report the downstream tasks performance in 4. After fine-tuning, COAT still performs consistently with the baselines on the downstream task performance, proving the accurateness of our method."}, {"title": "6.1.3 VLM TRAINING", "content": "We also evaluate COAT on vision language models. We conduct experiments on VILA (Lin et al., 2024) and perform stage-3 SFT of VILA1.5-7B using the same SFT data mixture employed by VILA's original paper (Chen et al., 2023; Xu et al., 2024) and set the global batch size to 1024. We pad the sequence length to a multiple of 4 for efficiency consideration. The training loss curve is visualized in Figure 7. We report the downstream task performance and their average in Table 5. We find that COAT performs on par with BF16 training and is better than the TransformerEngine baseline, which demonstrates the accurateness of our method. We further visualize the VLM captioning experiment in Appendix B to prove the effectiveness of COAT on generation tasks."}, {"title": "6.2 \u039c\u0395\u039cORY SAVING AND SPEEDUP", "content": "We test the memory saving and speedup result of COAT in two settings: The results on a single transformer layer help to accurately analyze the capabilities for memory saving and speedup, while the end-to-end results reflect the practical benefits of our method in real-world applications."}, {"title": "6.2.1 MEMORY SAVING AND SPEEDUP FOR A SINGLE TRANSFORMER LAYER", "content": "Table 6 highlights the speedup and memory reduction achieved for a single transformer layer. We conducted experiments with a batch size of 4, varying the hidden sizes between 2048 and 4096, and sequence lengths of 2048 and 4096.\nCOAT demonstrates better speedup compared with TE, and significantly better memory reduction ability compared with BF16 and TE. Our approach achieves up to 1.57\u00d7 speedup over BF16 and achieves a consistent 1.65\u00d7 memory reduction compared to BF16, which is very close to the theoretically 1.69\u00d7 reported in Table 2. The speedup ratio becomes larger with larger hidden sizes and longer sequence lengths."}, {"title": "6.2.2 SPEEDUP AND MEMORY SAVING FOR END-TO-END TRAINING", "content": "Table 7 presents a detailed comparison of end-to-end memory reduction and speedup results across different configurations for transformer models, specifically Llama-2-7B, Llama-2-13B, and Llama-30B, with variations in the number of GPUs used (1, 2, 4, and 8). It highlights COAT's effectiveness in reducing end-to-end memory footprint and the speedup compared to standard BF16 and TransformerEngine (TE) setups under varying conditions of batch size and context length.\nCOAT allows full-parameter training of Llama-2-7B on a single GPU, where BF16 and TE both out of memory (OOM). Similarly, for the Llama-2-13B and Llama-30B models, our method enables 2-GPU training for Llama-2-13B and 8-GPU training for Llama-30B when Batch Size = 1.\nIn all multi-GPU training setting, COAT can double the micro-batch size and therefore lead to even higher speedup. For example, our method can achieve 2.25\u00d7 speedup when training Llama-2-13B on 4-GPUs since we can effectively increase the batch size to 2.\nOverall, COAT significantly reduces end-to-end memory usage by up to 1.55\u00d7 and speeds up the end-to-end training by nearly 1.44\u00d7. This facilitates full-parameter training on fewer GPUs, which is particularly beneficial for larger language models."}, {"title": "6.3 ABLATION STUDIES", "content": "6.3.1 DYNAMIC RANGE EXPANSION'S COMPATIBILITY WITH OTHER DATA FORMATS\nDynamic Exponent Quantization (DE) was proposed in (Dettmers et al., 2021) to quantize the optimizer states since its representation range has a range of 7 orders of magnitude and is very suitable for optimizer states quantization. Therefore, they proposed to quantize both first-order and second-order momentum with 8-bit DE.\nWe report the quantization error of $\\frac{m}{\\sqrt{v + \\epsilon}}$ in Table 8, and find that applying our dynamic range expansion method to 8-bit DE can further reduce the quantization error by 1.41\u00d7. Specifically, the lowest quantization error is achieved when first-order momentum m is quantized with 8-bit DE + Dynamic Range Expansion, and second-order momentum v is quantized with E4M3/E5M2 + Dynamic Range Expansion. This proves the effectiveness of our method."}, {"title": "7 CONCLUSION", "content": "In this work, we present COAT, a memory-efficient FP8 training framework for foundation models by quantizing both optimizer states and activations into FP8 format. We observe that the FP8 format's representation range is not fully utilized when quantizing optimizer states, prompting us to propose Dynamic Range Expansion to align their dynamic range. We then identify the importance of quantizing non-linear layers, and propose mixed-granularity FP8 precision flow to quantize the activations accurately without introducing too much overhead.\nExtensive experiments on LLM and VLM training and fine-tuning demonstrate that COAT can achieve nearly lossless performance. In end-to-end training, COAT achieves a 1.54 memory reduction and 1.43 speedup compared to BF16, and is comparable or even faster to TransformerEngine's training speed. Our method also enables full-parameter training of billion-scale models on fewer GPUs and is able to double the batch size in realistic settings. These results highlight COAT's capability to enable memory-efficient, large-scale model training without sacrificing accuracy, providing a highly effective solution for memory-constrained environments. Future work could further explore combining our proposed approach with other low-precision gradient compression methods to reduce communication overhead."}, {"title": "A DETAILS ABOUT OPTIMIZER STATES QUANTIZATION", "content": "When performing optimizer.step(), We first dequantize the optimizer states into FP32, then update the optimizer states and weights in FP32 precision. In the end, we quantize the updated optimizer states back to FP8 and store it in GPU memory. This can be formulated as:\n$m_{t-1} = DQ(\\hat{m}_{t-1}, S_{\\hat{m}_{t-1}})$ (Dequantize to FP32)\n$v_{t-1} = DQ(\\hat{v}_{t-1}, S_{\\hat{v}_{t-1}})$ (Dequantize to FP32)\n$m_t = \\beta_1 * m_{t-1} + (1 - \\beta_1) * g_t$\n$v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2) * g_t^2$\n$\\hat{m_t} = \\frac{m_t}{1-\\beta_1}$\n$\\hat{v_t} = \\frac{v_t}{1-\\beta_2}$\n$w_{t+1} = w_t - \\eta \\left(\\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t} + \\epsilon}} + \\lambda w_t \\right)$\n$\\hat{m}, S_{\\hat{m}_t} = Q(\\hat{m}_t)$ (Quantize to FP8)\n$\\hat{v}, S_{\\hat{v}_t} = Q(\\hat{v}_t)$ (Quantize to FP8)"}, {"title": "C VISUALIZATION OF EXPAND FUCTION", "content": "We further visualize Figure 3 by flattening it. This helps to understand the effectiveness of our method since floating point numbers can be more easily understood in a binary manner."}, {"title": "D DETAILED EXPLANATION FOR TABLE 2", "content": "We mainly explain why the reduction ratio of linear layers is not exactly 50% here. 5.66U comes from: QKV Projection - 1U, Attention Projection - 1U, Up&Gate Projection - 1U, Down Projection - 2.66U. Among them, the attention projection's input is exactly the output of FlashAttention. However, FlashAttention will save its input (QKV) and its output itself, so FlashAttention and Attention Projection actually share the same tensor when saving it. In Python, this tensor will only be saved for 1 time. So we should not further store an FP8 version of the Attention Projection's input, since this will even increase the memory usage by 0.5U if we do not change the code of FlashAttention. Although we need to quantize the input to FP8 again in backward pass, we can store the scaling factor to reduce this additional overhead.\nTherefore, after FP8 quantization, the memory usage of linear layers comes from: QKV Projection - 0.5U, Attention Projection - 1U, Up&Gate Projection - 0.5U, Down Projection - 1.33U. They sum up to 3.33U. Our FP8 precision flow method is also compatible with other quantized attention method Shah et al. (2024); Zhang et al. (2024) by directly quantize the output of the RoPE function."}, {"title": "E DETAILED EXPLANATION FOR FIGURE 1(C)", "content": "We mainly discuss FP8-LM in this section. FP8-LM reduces the gradient communication precision from FP32 to FP8 and therefore greatly reduces the communication overhead. It also reduces the master weight's precision from FP32 to FP16/BF16, and reduces the optimizer states precision from FP32 to BF16/FP8. For optimizer states and master weights, the memory footprint reduction result is consistent with our bar. However, for gradient, although FP8-LM can quantize the gradient into FP8, it still needs to preserve the main gradient in FP32 for gradient accumulation. Therefore it can not reduce the memory footprint of the weight gradient."}]}