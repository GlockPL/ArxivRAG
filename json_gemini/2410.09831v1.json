{"title": "LoLI-Street: Benchmarking Low-Light Image Enhancement and Beyond", "authors": ["Md Tanvir Islam", "Inzamamul Alam", "Simon S. Woo", "Saeed Anwar", "IK Hyun Lee", "Khan Muhammad"], "abstract": "Low-light image enhancement (LLIE) is essential for numerous computer vision tasks, including object detection, tracking, segmentation, and scene understanding. Despite substantial research on improving low-quality images captured in underexposed conditions, clear vision remains critical for autonomous vehicles, which often struggle with low-light scenarios, signifying the need for continuous research. However, paired datasets for LLIE are scarce, particularly for street scenes, limiting the development of robust LLIE methods. Despite using advanced transformers and/or diffusion-based models, current LLIE methods struggle in real-world low-light conditions and lack training on street-scene datasets, limiting their effectiveness for autonomous vehicles. To bridge these gaps, we introduce a new dataset \"LoLI-Street\" (Low-Light Images of Streets) with 33k paired low-light and well-exposed images from street scenes in developed cities, covering 19k object classes for object detection. LoLI-Street dataset also features 1,000 real low-light test images for testing LLIE models under real-life conditions. Furthermore, we propose a transformer and diffusion-based LLIE model named \"TriFuse\". Leveraging the LoLI-Street dataset, we train and evaluate our TriFuse and SOTA models to benchmark on our dataset. Comparing various models, our dataset's generalization feasibility is evident in testing across different mainstream datasets by significantly enhancing images and object detection for practical applications in autonomous driving and surveillance systems.", "sections": [{"title": "1 Introduction", "content": "Low-light environments can pose significant challenges for various computer vision tasks in our daily lives. For most computer vision tasks, models are typically trained on datasets collected during the day with sufficient lighting, making them less effective in dark or low-light environments. This limitation poses a significant challenge as the underlying datasets do not account for the variations and complexities in real-world low-light conditions. Thus, as daylight fades into night, the reduced visibility can hinder the ability to perform even the most basic tasks for computer vision systems. This is a matter of convenience, safety, and efficiency. To address these practical challenges, advancements in computer vision technology are crucial. Such systems can significantly assist in low-light conditions, enhance the vision capabilities of autonomous vehicles [39,33], and improve safety and security measures [33]. The importance of computer vision in mitigating the effects of low-light conditions highlights its potential impact on a wide range of applications [46]. For instance, recent advancements in image processing and machine learning have led to sophisticated algorithms that enhance image clarity [31], detecting [40] and recognizing [49] objects in near-darkness.\nAdditionally, with the rise of deep learning [44,55,41], transformers [50,36,6,50], and diffusion methods [59,27,58,51], its feature representation capabilities led to the rapid adoption of LLIE. Moreover, researchers are exploring the latest transformer and diffusion-based methods for LLIE by utilizing synthetic datasets and reporting significant improvements in LLIE. However, the models struggle in practical applications, a huge gap that leaves the scope to develop robust methods for effective LLIE in real-world scenarios. Thus, the full potential of these methods for LLIE has not yet been fully explored and requires further research. Furthermore, these learning-based methods heavily rely on high-quality labeled data for training to perform accurately in real-world scenarios.\nIn literature, different datasets are available with different scene types of images under various low-light conditions [52,35,10,11,32,14]. Despite having several LLIE datasets, there is still a lack of datasets, especially for urban street image scene types, which can be used to train the LLIE models for autonomous vehicles to use navigation and surveillance cameras in urban street scenarios where accurate object detection, recognition, and navigation are crucial for safety.\nTo address these gaps in this paper, we contribute as follows:"}, {"title": "2 Related Works", "content": "2.1 LLIE Datasets\nThe ExDARK dataset [35] includes 7,363 annotated images across 12 classes, crucial for low-light object detection. The LLVIP dataset [26] provides 15,488 pairs of visible and infrared images, essential for image fusion and pedestrian detection. The MIT-Adobe FiveK dataset [3] offers 5,000 indoor and outdoor images for various enhancement tasks. The SICE [4] dataset synthesizes 589 images across varied illumination conditions, while the SID [10] dataset pairs 5,094 short-exposure images with long-exposure references. Additionally, the LIME [17] dataset features 10,000 images for LLIE in low-light conditions, and the DPED [24] dataset enhances mobile photo quality. The LOLv1 [53] and LOLv2 [56] datasets contain paired high and low-light images, and the LSRW dataset [19] includes paired low-light images. These datasets have indoor and outdoor scenes, as presented in Table 1a. To the best of our knowledge, there is no dataset that presents the street scene types, unlike our proposed LoLI-Street dataset, which is crucial for autonomous vehicles under real-world low-light street scenarios. Moreover, our LoLI-Street provides a test set of 1000 street scene-type images under real-life low-light conditions for testing the LLIE methods.\n2.2 LLIE Methods\nTransformer-based LLIE. Initially proposed for natural language process-ing [47], transformers have recently shown remarkable performance in computer vision tasks, such as image classification [1,2,13], semantic segmentation [7,54,63], and object detection [8,12]. They have also proven effective in low-level vision tasks like image restoration [6,50] and image synthesis [23,28,61]. Recent stud-ies highlight transformers' effectiveness in LLIE by utilizing illumination-guided multi-head self-attention mechanisms to improve interactions between regions of different exposure levels [6].\nDiffusion-based LLIE. Diffusion-based models have shown significant poten-tial in LLIE by leveraging their generative capabilities to handle various degradations, including noise [60], low contrast [37], color correction [9], and medical-image denoising [16,43]. Recent advancements include Diff-Retinex [58], which"}, {"title": "3 Methodology", "content": "3.1 Our Dataset: LoLI-Street\nWe introduce the benchmark dataset \u201cLow-light Images of Streets (LoLI-Street)\", containing three subsets: train, validation, and test. The train and validation sets consist of 30k and 3k paired low and high-light images, and the real low-light testset (RLLT) contains 1k images under real-world low-light conditions, totaling 33k images. We collected high-resolution videos (4K/8K at 60fps) from various cities under low-light conditions, extracting and manually reviewing frames to create the Real Low-light Testset (RLLT) of our LoLI-Street dataset, ensuring high quality and excluding any with motion blur. As shown in Table 1b, LoLI-Street encompasses three levels of low-light intensity, resulting in different quantitative metrics. Sample images are presented in Fig. 2a, and Fig. 2b\n3.2 Our Proposed Method\nOur proposed TriFuse integrates a custom vision transformer, wavelet-based con-ditional diffusion denoising, and an edge-sharpening module detailed as follows:\nDiscrete Wavelet Transformation (DWT). We use DWT to decompose a given low-light image $I^{low} \\in R^{H \\times W \\times C}$ in various low and high-frequency components. The 2D-DWT with Haar wavelets [18] decomposes the image into four sub-bands: $A^{low}$, $V^{low}$, $H^{low}$, and $D^{low}$, as illustrated in Fig. 3. The mathematical formulation for the 2D-DWT is provided in Eq. (1):\n${A^{low}, V^{low}, H^{low}, D^{low}} = 2D\\text{-}DWT(I^{low}),$ (1)\nwhere $A^{low}$ is the approximation coefficient presenting the low-frequency information, and $V^{low}$, $H^{low}$, and $D^{low}$ are the coefficients presenting the vertical, horizontal, and diagonal high-frequency information, respectively. Focusing the diffusion process on these components, especially the average coefficients, Tri-Fuse enhances the model's ability to handle global image structures effectively.\nTriFuse. TriFuse integrates a transformer, CNN, Encoder, and Decoder block, involving the diffusion process for predicting noise at each timestamp, form-ing the cornerstone of our conditional noise generation for diffusion denoising. This approach leverages the power of transformers to accurately predict and ad-just noise at each diffusion timestep of denoising diffusion probabilistic models (DDPM) [20], enhancing the denoising process, which eventually improves LLIE.\nIn the forward diffusion process in Eq. (2), the input image $x_0$ is progressively corrupted into a noisy version $x_T$ over T steps, governed by a variance schedule ${\\beta_1, \\beta_2,..., \\beta_T}$ as follows:\n$q(x_{1:T}|x_0) = \\prod_{t=1}^{T}q(x_t|x_{t-1}), q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t I),$ (2)\nwhere $x_t$ is noisy data at timestep t, and $\\beta_t$ is the variance schedule.\nThe reverse diffusion in Eq. (3) involves learning to denoise the noisy image $x_T$ back to a clean image $x_0$ through a series of Gaussian denoising transitions:\n$p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^{T}p_\\theta(x_{t-1} | x_t), p_\\theta(x_{t-1} | x_t) = N(x_{t-1}; \\mu_\\theta(x, t), \\sigma_t I).$ (3)\nHere, $\\mu_\\theta$ is the predicted mean, and $\\sigma_t$ is the variance, are learned parameters.\nConditional Noise Module (CNM) for Diffusion Denoising. The CNM is designed to predict the noise $\\epsilon_t$ at each timestep t, utilizing a transformer-based architecture to grasp the intricate patterns in noise and image details. Our model utilizes self-attention mechanisms to capture long-range dependencies and contextual information, unlike traditional diffusion models that rely on random Gaussian noise at each timestep. By conditioning the noise on the input image and the timestep, our CNM significantly enhances the denoising process.\nThe CNM architecture begins by encoding the input image into a higher-dimensional space using convolutional layers, which extract features. These encoded features are flattened and processed through a series of transformer blocks where the self-attention mechanism enables the model to assess the importance of different image parts, effectively predicting the noise to be added or removed. After transforming the features through self-attention and feed-forward layers, the output is reshaped to the original feature map dimensions and passed through a decoder, which reconstructs the predicted noise map, guides the diffusion.\nThe CNM's ability to model complex dependencies and incorporate contex-tual information results in superior image restoration, particularly in challenging low-light conditions. By accurately predicting and controlling the noise at each diffusion step, the CNM ensures an effective and precise denoising process, preserving fine details and maintaining contextual awareness.\nThis integration enhances image quality by preserving fine details, contextual awareness, and providing adaptive denoising. Mathematically, the noise prediction is expressed as $\\epsilon_\\theta(\\hat{I}_t, t) = CNM(x_T)$. After integrating our custom CNM with the process shown in Eq. (3), it can be expressed as Eq. (4) as follows:\n$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} CNM(x_T)) + \\sigma \\eta,$ (4)\nwhere $\\alpha_t$ and $\\bar{\\alpha}_t$ are predefined noise schedules, and $\\eta$ represents Gaussian noise. Overall, this novel approach ensures that the denoising process is both effective and precise by accurately predicting and controlling the noise at each diffusion step. The integration of the CNM enhances image quality by ensuring that the noise prediction is conditional on both the image content and the timestep, leading to superior restoration of image details in low-light conditions.\nEdge Sharpening Module (ESM). ESM plays a critical role in enhancing the sharpness and clarity of edges in the restored images. It focuses on the high-frequency components obtained from the DWT, ensuring that fine details and textures are well preserved during the restoration process.\nThe ESM comprises several sophisticated components designed to handle high-frequency information efficiently. Depthwise convolutions capture channel-wise spatial information effectively, ensuring that the model can focus on intricate details without increasing computational complexity. Dilated Residual Blocks (6) preserve the input's spatial resolution while capturing multi-scale features as provided in Eq. (5). Using dilated convolutions allows the network to have a larger receptive field, which is essential for capturing contextual information at multiple scales without losing fine details.\n$Y = X + Conv(ReLU(BN(Conv(ReLU(BN(X)))))),$ (5)\nwhere X denotes the input feature map that enters the Dilated Residual Block, and Y is the output feature map after processing through the block. Conv, ReLU, and BN denote convolution, Rectified Linear Unit, and Batch Normalization, respectively. Cross-attention mechanisms are used to align and integrate contextual information across different directions (vertical, horizontal, and diagonal). The cross-attention mechanism is defined in Eq. (6) as follows:\n$A_{attn} = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V,$ (6)\nwhere Q = Conv(X), K = Conv(X), V = Conv(X) are the query, key, and value matrices, and $d_k$ is the dimensionality of the key vectors. The ESM processes the high-frequency components as given in Eq. (7):\n$ESM(x) = x + Conv(Concat(\\phi_{HL}(x_{HL}), \\phi_{LH}(x_{LH}), \\phi_{HH}(x_{HH}))),$ (7)\nwhere $x_{HL}, x_{LH}, x_{HH}$ are the high-frequency components and $\\phi_{HL}, \\phi_{LH}, \\phi_{HH}$ are the corresponding dilated residual blocks, respectively. By integrating these components, the ESM enhances the sharpness of edges and preserves the fine details in the restored images, addressing one of the critical challenges in LLIE.\nOverall, our proposed TriFuse model produces high-quality, sharp images by combining the ESM and CNM modules in the diffusion denoising process, making it an efficient solution for LLIE and suitable for various real-world applications."}, {"title": "4 Experimental Setup", "content": "Datasets. We use the train set of 30k paired images from our LoLI-Street dataset to train the models and validate the models' performance on the synthetic validation set of 3k paired images. Furthermore, we evaluate the models on the real test set of the LoLI-Street dataset, including 1k unpaired images. We used the well-known LOLv1 and LOLv2 datasets to evaluate the pre-trained and trained weights of each model and compare the existing models' performance\nImplementation. We used PyTorch on a server with four NVIDIA RTX 2080 GPUs (24GB each). SOTA models were trained with default settings for fair comparison. Our TriFuse was trained with a batch size of 12 and a patch size of 256x256. The initial learning rate of 1 \u00d7 10-4 decayed by 0.8 every 5 \u00d7 103 iterations. For efficient restoration, the time step T was set to 200, and the implicit sampling step S was set to 5 for both the training and inference phases.\nEvaluation Strategy. We calculated full-reference metrics (PSNR, SSIM, MS-SSIM, MSE, and MAE) and no-reference metrics (BRISQUE, and NIQE) to evaluate existing models and our TriFuse model. We assessed SOTA LLIE models with pre-trained weights on our LoLI-Street dataset to evaluate their quality. Additionally, we trained these models on our dataset and tested them to determine its suitability for generalization. Finally, we quantitatively and qualitatively compared our proposed model with recent SOTA models."}, {"title": "5 Comparative Analysis", "content": "We compare our TriFuse with multiple SOTA LLIE methods, including Retinex-Former [6], RQ-LLIE [34], CUE [62], LLFormer [50], DiffLL [27], PairLIE [15], FourLLIE [48], and SCI [38], covering transformer and diffusion-based models.\nQuantitative Analysis. We present a quantitative analysis of SOTA models on the LoLI-Street and existing datasets. Table 2 shows the performance of these models against the validation set using pre-trained weights with full-reference metrics under various lighting conditions. LLFormer performs robustly across all subsets, achieving the highest PSNR of 28.67 for the dense variety of our validation set. Table 3 evaluates SOTA models on the LoLI-Street validation set using LoLI-Street-trained weights, showing significant performance improvements and model generalization. Our proposed TriFuse achieves the highest scores in various metrics, demonstrating its robustness and effectiveness in LLIE tasks.\nThe performance of the SOTA models is presented in Table 4 on the real low-light test set of LoLI-Street, using both pre-trained and trained weights for each model. The evaluation metrics include BRISQUE and NIQE. Our proposed model, TriFuse, stands out with the lowest BRISQUE and NIQE scores, indicating superior visual quality and naturalness of the enhanced images compared to the existing models. Table 5 provides a performance comparison of the SOTA models and our proposed TriFuse on existing datasets (LOLv1, LOLv2 (real), LOLv2 (synthetic), LSRW, SICE, ExDark, and LLVIP). As shown, we observed that our model consistently achieved either the best or second-best performance across multiple datasets, as indicated by both full-reference and no-reference metrics. This further validates the effectiveness of our model and emphasizes its ability to generalize well from the training dataset. Table 6 summarizes the\nQualitative Analysis. In addition to the quantitative analysis, we conducted a qualitative evaluation of the enhanced images produced by different models on various datasets. Figure 4 showcases enhanced images from the LoLI-Street dataset's synthetic validation set and real low-light test set, demonstrating that our model consistently provides clearer and more detailed visual enhancements, especially in shadowed and low-light areas. Figure 5 presents enhanced images from the LOLv1 and LOLv2 (both real and synthetic), LSRW, and SICE validation sets, where our model excels in color fidelity and enhancing image details, as evident in the close-up views, revealing well-maintained texture details and reduced artifacts. Overall, the comparison highlights TriFuse's robustness and superior performance in enhancing low-light images across multiple datasets.\nAlso, Fig. 6 illustrates the results of YOLOv10 inference on a randomly selected image from the LoLI-Street test set after enhancement by different models. Our model improves visual quality and enhances object detection accuracy, detecting additional objects such as traffic lights and cars with faster inference times compared to other approaches. This qualitative analysis demonstrates our model's effectiveness in enhancing low-light images, significantly improving visual quality and object detection performance in real-world conditions.\nNevertheless, S(5) maintains competitive performance with superior computational efficiency and clarity in real low-light conditions, achieving the lowest BRISQUE (10.32) and NIQE (10.61) scores on the RLLT among all."}, {"title": "6 Conclusion", "content": "Identifying the growing need for LLIE solutions, we introduced LoLI-Street, a novel benchmark dataset featuring street scenes under diverse lighting conditions designed to enhance images and improve object detection in low-light environments, which is crucial for autonomous systems. Our proposed LLIE model, TriFuse, incorporates a unique wavelet-based CNM approach to generate accurate input noise in the diffusion denoising process. This results in effective denoising for real-world LLIE in lower diffusion sampling steps. Comprehensive evaluations demonstrate TriFuse's superiority over existing SOTA models across multiple benchmarks, achieving top performance in visual quality and object detection under low-light conditions based on various metrics. Future directions include optimizing TriFuse for real-time applications and adapting it to diverse adverse scenarios [29,25] employing unsupervised techniques [30]."}, {"title": "7 Acknowledgments", "content": "This work was partly supported by Institute for Information & communication Technology Planning & evaluation (IITP) grants funded by the Korean government MSIT: (RS -2022-II221199, RS-2024-00337703, RS-2022-II220688, RS -2019-II190421, RS-2023-00230337, RS-2024-00356293, RS-2022-II221045, RS-2021-II212068, No.RS-2023-00231200, RS-2024-00437849). Lastly, this work was supported by the Korea Internet & Security Agency (KISA) grant funded by the Korean government (PIPC) (No. RS-2023-00231200, Development of personal video information privacy protection technology capable of AI learning in an autonomous driving environment)."}]}