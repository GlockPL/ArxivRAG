{"title": "CLAMS: A System for Zero-Shot Model Selection for Clustering", "authors": ["Prabhant Singh", "Pieter Gijsbers", "Murat Onur Yildirim", "Elif Ceren Gok", "Joaquin Vanschoren"], "abstract": "We propose an AutoML system that enables model selection on clustering problems by leveraging optimal transport-based dataset similarity. Our objective is to establish a comprehensive AutoML pipeline for clustering problems and provide recommendations for selecting the most suitable algorithms, thus opening up a new area of AutoML beyond the traditional supervised learning settings. We compare our results against multiple clustering baselines and find that it outperforms all of them, hence demonstrating the utility of similarity-based automated model selection for solving clustering applications.", "sections": [{"title": "Introduction", "content": "Clustering plays a significant role in data analysis. It can be useful for exploratory data analysis, user profiling, and medical analysis tasks (Liao et al., 2016). In the last few decades, many clustering methods and tools have been developed, each with their own strengths and weaknesses, making it difficult to select the best ones. For instance, the scikit-learn library (Pedregosa et al., 2011) contains 13 clustering algorithms, each with several hyperparameters to tune. This makes it very difficult for non-experts to select the best models and hyperparameters, further exacerbated by the fact that there is no golden metric to optimize for.\nIndeed, clustering by its very nature is unsupervised. The lack of a ground truth makes it difficult to use traditional AutoML techniques, as they often rely on known labels to evaluate candidate solutions. Previous automated clustering approaches evaluate clustering methods via multiple internal metrics (which do not require labels), such as Calinkski-Harbarnz or the Silhouette score, or external metrics (which require labels) such as adjusted mutual information (AMI) and rand index. These metrics are also called Cluster Validity Indices (CVIs). One can optimize internal CVIs and aim to achieve optimal performance over external CVIs, but to the best of our knowledge, there has not been any significant work proving a strong correlation between internal and external CVIs. We cannot optimize for an external CVI for a dataset where no labels are available, which makes model selection for a new dataset without labels an extremely hard problem.\nIn this work, we propose an entirely unsupervised, zero-shot model selection framework that automates clustering by 'pretraining' on previous tasks with internal and external CVIs. As such, it takes a significant step forward to enabling AutoML for clustering in real-world settings. We make the following contributions:\n1. CLAMS (Clustering with Automated Machine Learning System) is a standalone, open-source AutoML tool for automated clustering.\n2. CLAMS-OT: A general zero-shot model recommendation system leveraging dataset similarity based on optimal transport (OT).\nWe evaluate our system on a wide range of clustering problems and find that it significantly outperforms all baselines, as demonstrated by Bayesian-Wilcoxon signed-rank tests (Benavoli et al.,"}, {"title": "Background", "content": ""}, {"title": "AutoML for Clustering", "content": "Automated machine learning provides methods and tools to make machine learning solutions available to people without the time or ML expertise to finetune ML models. Some examples of AutoML systems include Auto-Sklearn (Feurer et al., 2020), GAMA (Gijsbers and Vanschoren, 2021a), and FLAML (Wang et al., 2021). These systems enable the automation of model selection and hyperparameter optimization (HPO) for supervised classification and regression tasks. However, very little research in current AutoML literature addresses automated clustering.\nEarlier studies focused on optimizing the number of clusters (Tseng and Yang, 2001; Liu et al., 2011; Das et al., 2007; Saha and Bandyopadhyay, 2013). Later work incorporates algorithm selection, sometimes through meta-learning, or hyperparameter optimization beyond just selecting the number of clusters. Table 1 provides an overview of the different methods. Most of them optimize a CVI metric or apply meta-learning techniques based on dataset similarity measured via meta-features such as Landmarkers, Clustering-oriented Meta-feature Extraction (CME), and Cluster Cardinality Estimation (CCE)."}, {"title": "Meta-learning", "content": "Meta-learning (or learning to learn) is often employed in AutoML to learn from the historical performance of machine learning models on a variety of tasks and use this knowledge to find better models for new tasks (Vanschoren, 2018). It can help to speed up the model selection process and find better architectures. Meta-learning is often used to warm-start the search for optimal models by choosing good initial hyperparameters or reducing the search space that AutoML algorithms have to explore. Multiple strategies have been proposed for meta-learning in AutoML including transferring priors in Bayesian optimization, K-nearest datasets (Feurer et al., 2015), portfolio selection (Feurer et al., 2020), and optimal transport (Rakotoarison et al., 2022).\nIn clustering, few studies have considered meta-learning for algorithm selection without hyperparameter optimization (S\u00e1ez and Corchado, 2019; Pimentel and De Carvalho, 2019). De Souto et al. (2008); Soares et al. (2009) require ground truth labels since they used external metrics during optimization. AutoML4Clust (Tschechlov et al., 2021) and AutoClust (Poulakis et al., 2020) leverage"}, {"title": "Optimal Transport", "content": "Optimal transport (OT) or transportation theory, also known as Kantorovich-Rubinstein duality, is a problem that deals with the transportation of masses from source to target. This problem is also called the Monge-Kantorovich transportation problem (Villani, 2008). In recent years, OT has gained significant attention from the machine learning community, as it provides a powerful framework for designing algorithms that can learn to match two probability distributions, which is a common task in image and natural language processing. In this section, we give an introduction to OT and distance measures related to our work.\nIn OT, the objective is to minimize the cost of transportation between two probability distributions. For a cost function between pairs of points, we calculate the cost matrix $C$ with dimensionality $n \\times m$. The OT problem minimizes the loss function $L(P) := \\langle C, P \\rangle$ with respect to a coupling matrix $P$. A practical and computationally more efficient approach is based on regularization and minimizes $L(P) := \\langle C, P \\rangle + \\epsilon \\cdot r(P)$ where $r$ is the negative entropy, computed by the Sinkhorn algorithm (Cuturi, 2013), and $\\epsilon$ is a hyperparameter controlling the amount of regularization. A discrete OT problem can be defined with two finite point clouds, ${x^{(i)}}_{i=1}^n, {y^{(j)}}_{j=1}^m, x^{(i)}, y^{(j)} \\in \\mathbb{R}^d$, which can be described as two empirical distributions: $\\mu := \\sum_{i=1}^n a_i \\delta_{x^{(i)}}$, $\\nu := \\sum_{j=1}^m b_j \\delta_{y^{(j)}}$. Here, $a$ and $b$ are probability vectors of size $n$ and $m$, respectively, and the $\\delta$ is the Dirac delta."}, {"title": "Gromov Wasserstein Distance", "content": "In this work, we are interested in the Gromov Wasserstein (GW) distance between these two discrete probability distributions. Gromov Wasserstein allows us to match points within different metric spaces, which is the case in AutoML problems where we have datasets of different dimensionalities. This problem can be written as a function of $(a, A), (b, B)$ between our distributions $A$ and $B$ (Villani, 2008; Scetbon et al., 2022):\n$\\text{GW}((a, A), (b, B)) = \\min_{P \\in \\Pi_{a,b}} Q_{A,B}(P)$ (1)\nwhere $\\Pi_{a,b} := {P \\in \\mathbb{R}^{r \\times m} | P1_m = a, P^T 1_n = b}$ is the set of all possible mappings of points from $A$ to $B$ and the energy $Q_{A,B}$ is a quadratic function of $P$ which can be described as\n$Q_{A,B}(P) := \\sum_{i,j,i',j'} (A_{i,i'} - B_{j,j'})^2 P_{i,j} P_{i',j'}$\nIn this work we are interested in the Entropic Gromov Wasserstein cost (Peyr\u00e9 et al., 2016):\n$\\text{GW}_{\\epsilon}((a, A), (b, B)) = \\min_{P \\in \\Pi_{a,b}} Q_{A,B}(P) - \\epsilon \\cdot H(P)$ (3)\nwhere $\\text{GW}_{\\epsilon}$ is the Entropic Gromov Wasserstein cost between our distributions $A$ and $B$, $H(P)$ is the Shannon entropy, and $\\epsilon$ a regularization constant. The problem with Gromov Wasserstein is that it is NP-hard and the entropic approximation of GW still has cubic complexity. For practical speedup, we use the Low-Rank Gromov Wasserstein (GW-LR) approximation (Scetbon et al., 2021; Scetbon and Cuturi, 2022; Scetbon et al., 2022), which reduces the computational cost from cubic to linear time. Scetbon et al. (2022) consider the Gromov Wasserstein problem with low-rank couplings,"}, {"title": "Problem statement", "content": "We first introduce some notation:\n\u2022 $D_{meta}$: A set of n datasets {$D_1, \u00b7\u00b7\u00b7, D_n$}, these are prior datasets with known labels.\n\u2022 A: Space of all possible algorithms applicable to the specific machine learning problem. Each algorithm $A \\in A$ has a hyperparameter configuration space $A_A$. An algorithm initialized with hyperparameter configuration $\\lambda \\in A_A$ is referred to as $A_\\lambda$.\n\u2022 A*: A set {$A^*_1, \u00b7\u00b7\u00b7 , A^*_n$} with the best algorithm configuration $A^*_i$ found for each dataset $D_i \\in D_{meta}$ according to a loss function $L$.\nProblem Statement: Given a new dataset without any labels, our meta-learner needs to select an optimal algorithm with associated hyperparameter configuration from a collection of previously evaluated algorithm configurations. Since we cannot further optimize the given model on the new dataset this is a zero-shot model recommendation problem, unless some (downstream) evaluation metric is available.\nFormally, given a new unlabeled dataset $D_{new}$, select an algorithm with configuration $A^*_i \\in A$ to employ on $D_{new}$, where $A^*_i$ is the optimal model with tuned hyperparameters for the labeled dataset $D_i$ that is most similar to $D_{new}$.\nProblem Formulation: For supervised tasks, this problem can be represented as a Combined Algorithm Selection and Hyperparameter optimization (CASH) problem (Thornton et al., 2013), shown in equation 5, where $A^*$ is the combination of the optimal learning algorithm from search space $A$ with associated hyperparameter space $A_\\lambda$ evaluated over $k$ cross-validation folds of dataset $D = {X, y}$ with training and validation splits for loss measure $L$.\n$A^*_{\\lambda} = \\underset{A \\in A \\\\ \\lambda \\in A_A}{\\text{argmin }} \\frac{1}{k} \\sum_{f=1}^k L (A_\\lambda, {X_{\\text{train}}}^f, {y_{\\text{train}}}^f}, {x_{\\text{val}}}^f, {y_{\\text{val}}}^f})$ (5)\nThe CASH problem from Equation 5 relies on the validation split to optimize for the optimal configuration, because labels are used during training. However, since unsupervised learning algorithms do not use labels while training, validation splits are not relevant and we can instead train on all unlabeled data and use the ground truth labels only for evaluation. Our modified CASH formulation to select the optimal unsupervised algorithm with access to labels during optimization is as follows:\n$A^*_{\\lambda} = \\underset{A \\in A \\\\ \\lambda \\in A_A}{\\text{argmin }} L (A_\\lambda, {X}, {y})$ (6)\nOur method can also be used with internal metrics $L_{\\text{internal}}$, which do not require labels, in which case only unlabeled data is needed for optimization:\n$A^*_{\\lambda} = \\underset{A \\in A \\\\ \\lambda \\in A_A}{\\text{argmin }} L_{\\text{internal }} (A_\\lambda, {X})$ (7)"}, {"title": "Proposed Framework", "content": "In this section, we describe the components of our framework. Our proposed framework has two components.\n1. We first propose CLAMS, Clustering with Automated Machine Learning System. CLAMS is a standalone AutoML tool for clustering with any metric. In this work, CLAMS is used for populating the meta-data.\n2. Secondly we propose a dataset similarity indicator with optimal transport distances, which is used for zero-shot model-recommendation"}, {"title": "CLAMS: Clustering with Automated Machine Learning System", "content": "In this work, we have devised an Automated Machine Learning framework that we called CLAMS. Our proposed framework comprises a diversified and well-defined search space that encompasses various preprocessing steps and learning algorithms, as well as optimizers that are adopted from GAMA (Gijsbers and Vanschoren, 2021b). CLAMS aims to identify the optimal pipeline that combines the most suitable preprocessing techniques and learning algorithms. CLAMS is one of the few tools which allow full pipeline selection on clustering algorithms. CLAMS supports both internal and external metrics (if labels are provided).\nSearch Space. By default, CLAMS contains all the scikit-learn based clustering algorithms with their hyperparameters as well as preprocessors for data cleaning and feature extraction. To the best of our knowledge, this is the first time preprocessing steps are included in the clustering pipeline in the context of automated clustering (see Table 1). For this work, we have edited the CLAMS search space to account for the limited time during meta-training and the crashes we faced during the evaluation of multiple datasets. Affinity Propagation (Frey and Dueck, 2007) is excluded from the search space due to its high computational time complexity, We removed spectral clustering (Von Luxburg, 2007) because of crashes on multiple datasets. Therefore, 7 clustering algorithms namely, k-Means (MacQueen, 1967), Agglomerative Clustering (Gower and Ross, 1969), OPTICS (Ankerst et al., 1999), MiniBatchKMeans (Sculley, 2010), DBSCAN (Ester et al., 1996), Mean Shift (Comaniciu and Meer, 2002) and BIRCH (Zhang et al., 1996) are selected. The detailed search space is presented in Table 2 in Appendix A.\nSearch Phase. CLAMS includes random search (RS) (Bergstra and Bengio, 2012), asynchronous successive halving (ASHA) (Li et al., 2020), and asynchronous evolutionary algorithm (ASEA) from the GAMA library for the optimization of the pipelines, based on the evaluation metric that is provided. In this work, we use the evolutionary algorithm to populate the meta-dataset. The pseudocode for the CLAMS search phase can be found in Algorithm 1."}, {"title": "Meta learning for Clustering via Dataset Similarity (CLAMS-OT)", "content": "Our intuition behind the CLAMS-OT method is \"If dataset A is similar to dataset B then the optimal algorithm on dataset A should perform well on dataset B\". As there is no ground truth with"}, {"title": "Experimental Setup", "content": "For our experiments, we use 57 datasets from OpenML (?) which are suitable for clustering. The full list of datasets and their dimensionalities are shown in Table 3 in Appendix B. These datasets are selected manually, we aim to select both synthetic and real-world datasets for our setting.\nWe cannot directly perform distance calculations on datasets with missing or non-numeric data, so we employ a preprocessing pipeline before computing distances. First, we convert our non-numeric data in our datasets using the TableVectorizer in the dirty-cat library (Cerda et al., 2018; Cerda and Varoquaux, 2019). The TableVectorizer encodes nominal values using one-hot"}, {"title": "Results", "content": "To evaluate the effectiveness of our approach, we report the average Adjusted Mutual Information (AMI) (Vinh et al., 2010) obtained from five runs of both the baselines and our approach (CLAMS-OT). We utilize the Bayesian Wilcoxon signed-rank test, or the region of practical equivalence (ROPE) test (Benavoli et al., 2017, 2014), to analyze the experimental results. The ROPE test defines an interval within which differences in model performance are considered equivalent to each other. It returns probabilities based on the measured performance. One model is considered to be better than the other based on the region of practical importance. By setting the ROPE value to 1%, we can make more practical comparisons between model performances. To run and visualize the analysis, we use the baycomp library (Benavoli et al., 2017). Our experimental results, as determined by the ROPE test, demonstrate the superiority of our approach over the baselines.\nWe compare our approach to default configurations of many different clustering algorithms and show the results in Figure 3."}, {"title": "Discussion", "content": ""}, {"title": "OT as an Indicator of Dataset Similarity", "content": "Our research on the dataset and task similarity builds on existing studies that utilize distance measures to quantify the similarity between datasets. Specifically, we draw inspiration from the work of Alvarez-Melis and Fusi (2020), who proposed the Optimal Transport Dataset Distance (OTDD), a similarity metric between datasets that utilizes optimal transport to learn a mapping over the joint feature and label spaces. There have been other studies exploring the space of dataset and task similarity with distance measures. Gao and Chaudhari (2021) propose \u201ccoupled transfer distance\" which utilizes optimal transport distances as a transfer learning distance metric on image data. Achille et al. (2021) explore connections between Deep Learning, Complexity Theory, and Information Theory through their proposed asymmetric distance on tasks. In this work, we explored"}, {"title": "Limitations", "content": "Our framework has limitations that should be taken into consideration. First, CLMASOT effective-ness depends on the presence of similar datasets to the meta-test dataset within the Dmeta. In cases where there are no similar datasets, such as with dataset id 42464 in our experiments, our suggested pipeline may not yield favorable results. Second, the time complexity of our system scales linearly with the number of datasets in the Dmeta. This means that as the number of datasets increases, CLAMS-OT may require more time to perform model selection. These limitations are important to note as they may impact the practical application of our approach in certain scenarios."}, {"title": "Future Work", "content": "This work proposed a system to automate clustering with zero shot recommendation via optimal transport distance. Optimal transport distance is still very expensive to compute. Though the low-rank computation decreases the time complexity, the system still takes around 30 minutes to compute similarity with other datasets. We believe this can be largely improved by using Wasserstein Embedding Networks (Courty et al., 2018) or MetaICNN (Amos et al., 2022) for faster computation, though these networks still do not support Gromov Wasserstein space. Another direction where we believe this system can be optimized is to use dynamic training time depending on datasets during the CLAMS search phase."}, {"title": "Conclusion", "content": "In this work, we have presented CLAMS, An AutoML system for selecting clustering algorithms, and CLAMS-OT, a zero-shot model recommendation system to recommend clustering models for datasets without labels. CLAMS allows performing automated clustering on a given dataset via a given CVI, CLAMS-OT is a meta-learner that uses optimal transport distances. Our system achieved the highest performance in the ROPE test and the lowest rank in the critical difference test. We discussed the motivation and analysis of our meta-learning technique."}]}