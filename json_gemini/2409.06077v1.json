{"title": "MTLSO: A Multi-Task Learning Approach for Logic Synthesis Optimization", "authors": ["Faezeh Faez", "Raika Karimi", "Yingxue Zhang", "Xing Li", "Lei Chen", "Mingxuan Yuan", "Mahdi Biparva"], "abstract": "Electronic Design Automation (EDA) is essential for IC design and has recently benefited from AI-based techniques to improve efficiency. Logic synthesis, a key EDA stage, transforms high-level hardware descriptions into optimized netlists. Recent research has employed machine learning to predict Quality of Results (QoR) for pairs of And-Inverter Graphs (AIGs) and synthesis recipes. However, the severe scarcity of data due to a very limited number of available AIGs results in overfitting, significantly hindering performance. Additionally, the complexity and large number of nodes in AIGs make plain GNNs less effective for learning expressive graph-level representations. To tackle these challenges, we propose MTLSO - a Multi-Task Learning approach for Logic Synthesis Optimization. On one hand, it maximizes the use of limited data by training the model across different tasks. This includes introducing an auxiliary task of binary multi-label graph classification alongside the primary regression task, allowing the model to benefit from diverse supervision sources. On the other hand, we employ a hierarchical graph representation learning strategy to improve the model's capacity for learning expressive graph-level representations of large AIGs, surpassing traditional plain GNNs. Extensive experiments across multiple datasets and against state-of-the-art baselines demonstrate the superiority of our method, achieving an average performance gain of 8.22% for delay and 5.95% for area.", "sections": [{"title": "1 INTRODUCTION", "content": "Logic synthesis optimization (LSO) is a critical step in the electronic design automation (EDA) process, responsible for transforming a high-level hardware description language (HDL) representation of a circuit into an optimized netlist of Boolean logic gates. This transformation is essential for ensuring the final integrated circuit (IC) meets design criteria such as area and delay. Given the complexity and scale of modern ICs, which can contain billions of transistors, manual design is infeasible. EDA tools are essential for managing this complexity, but as designs grow more intricate, the traditional heuristic-based methods used in logic synthesis face challenges in achieving optimal results [7]. This highlights the need for more efficient and effective optimization methods. In this context, machine learning (ML) offers a promising avenue for enhancing LSO by providing faster and potentially more accurate predictions of QoR [12].\nDespite the potential of ML in EDA [17, 22], applying these techniques to LSO presents several challenges, particularly related to data scarcity and overfitting. The limited availability of large, labeled datasets in this domain hampers the ability to train robust ML models [14], which in turn affects their generalization capability and prediction accuracy. This limitation impedes the practical deployment of ML models in production environments where they need to deliver consistent performance across diverse design scenarios.\nTo tackle overfitting, multi-task learning (MTL) has demonstrated significant promise in various domains such as image and text [5]. MTL improves model generalization and robustness by leveraging shared representations across related tasks. By jointly learning multiple tasks, MTL mitigates data scarcity effects, benefiting from additional sources of supervision and enhancing performance across numerous applications.\nTo address the aforementioned issues, we propose Multi-Task Learning for Logic Synthesis Optimization (MTLSO), an end-to-end, multi-task learning-based approach for logic synthesis. MTLSO introduces a novel task of binary multi-label graph classification alongside the primary task of QoR prediction, thereby optimizing the utilization of valuable yet limited data. We devised a novel approach to generate the required labels for this new task using existing data, eliminating the need for additional data. Furthermore, to enhance the efficiency of graph representation learning for large AIGs, we designed a hierarchical graph representation learning strategy. This strategy integrates GNNs with graph downsampling across multiple layers to facilitate learning more expressive graph-level representations based on multiple levels of abstraction. In summary, we present the following contributions in this work:\n\u2022 We design a novel task of binary multi-label graph classification alongside the conventional regression task. This multi-task training approach enables the model to learn from multiple sources of supervision, thereby improving its ability to predict the QoR for given pairs of AIGs and synthesis recipes.\n\u2022 We introduce a label construction process that generates the necessary ground truth classification labels from existing data for training the graph classifier, eliminating the need for additional data sources.\n\u2022 To enhance the expressiveness of the learned graph-level representations of large AIGs, we adopt a hierarchical strategy in our graph encoder, featuring multiple layers of successive graph encoding and downsampling.\n\u2022 We conduct extensive experiments on several datasets and compare against state-of-the-art approaches to demonstrate the effectiveness of our proposed method, achieving an average gain of 8.22% for delay and 5.95% for area."}, {"title": "2 RELATED WORK", "content": "In this section, we provide an overview of significant advancements related to our current work, organized into two subsections. First, we review the most notable machine learning-based approaches proposed to solve the LSO problem. Next, we delve into the accomplishments of multi-task learning across various domains, presenting a compelling rationale for adopting such a strategy to effectively address the LSO problem.\n2.1 Logic Synthesis Optimization\nGiven the extreme complexity of VLSI chip designs, there has been a recent trend toward employing machine learning techniques to expedite design closure [9]. In this context, Yu [23] examines the potential of machine learning models to enhance efficiency across various stages of EDA, including LSO. There are studies such as [10] that compute representations of netlists by employing traditional techniques to extract hand-crafted features from AIGs. In contrast, other approaches utilize Graph Neural Networks (GNNs) as more advanced graph representation learners. These methodologies typically begin by employing a simple plain GNN to encode the input AIG. They subsequently employ various techniques to learn the representations of synthesis recipes. Prediction of QoR values for pairs of AIGs and recipes is then conducted by leveraging both the computed graph representations and the recipe representations. For instance, Chowdhury et al. [3] learn representations of synthesis recipes by passing them through a set of 1D convolution layers. They then concatenate these representations with those learned for circuits by GNNs to predict QoR values. LOSTIN [18] and GNN-H [19] use LSTM to learn representations of synthesis recipes, capturing the relative ordering of logic transformations within them. Yang et al. [21] adopt a similar strategy but replace the LSTM with the self-attention mechanism of the Transformer to learn representations of the recipes. None of these methods account for the varying importance of different graph nodes in learning the final graph-level representation of each AIG. They treat all graph nodes with equal importance, resulting in inefficiencies and reduced expressiveness of the final representation, particularly for very large AIGs, which are common in the LSO problem. Additionally, they have not addressed the critical issue of overfitting caused by severe data scarcity, which is a significant challenge in ML-based LSO solvers.\n2.2 Multi-task Learning\nMulti-task learning is a machine learning paradigm designed to enhance model generalization by utilizing shared data across multiple related tasks. This approach is particularly advantageous in scenarios characterized by data scarcity. Multi-task learning has demonstrated success in various fields, including natural language processing [15], computer vision [6], and speech recognition [11], among others. However, despite the significant challenges posed by data scarcity in LSO, which impede effective model training and underscore the need for MTL, it remains underutilized in this domain."}, {"title": "3 METHODOLOGY", "content": "In this section, we first define the problem formulation. Next, we introduce the main components of our MTLSO approach. Subsequently, we delve into a detailed explanation of our multi-task learning strategy, including its tasks, components, and objectives. We visualize an overview of our approach in Figure 1.\n3.1 Problem Formulation\nGiven an And-Inverter Graph (AIG) $G \\in \\mathcal{G}$ and a set of K synthesis recipes $\\{r_i\\}_{i=1}^K$, the goal of LSO is to predict the QoR value for each pair $(G, r_i)$, which can then be further utilized to determine the best synthesis recipe for the graph G. Formally, the aim is to learn the mapping function:\n$f: \\mathcal{G} \\times \\mathcal{R} \\rightarrow \\mathbb{R}$  (1)\nwhere $\\mathcal{G}$ represents the set of AIGs and $\\mathcal{R}$ represents the set of all available synthesis recipes.\n3.2 Graph Encoder\nGiven a graph G with its node feature matrix X, the goal of the graph encoder is to learn a graph-level representation $h_G$ for the entire graph. This is typically accomplished by utilizing a Graph Neural Network (GNN), which initially learns a node representation matrix H. These node representations are then aggregated, often using simple pooling techniques such as mean or max pooling, to derive the graph-level representation. To address the logic synthesis optimization problem, we adopt a hierarchical approach for learning graph representations. This is motivated by the inherent complexity of AIGs, where conventional plain GNNs may encounter limitations. In the following sections, we elaborate on our Hierarchical Graph Representation Learning (HGRL) strategy, providing both its motivation and workflow.\nHierarchical Graph Representation Learning. For solving the logic synthesis optimization problem, learning high-quality representations of AIGs is essential. These graphs vary in size, with most of them being large, having thousands or even tens of thousands of nodes. Furthermore, the importance of nodes in AIGs is not necessarily the same; some may play more critical roles in determining the final QoR value than others. As an example, some nodes may be redundant, meaning that removing them would not change the final logical function. Due to these reasons, solely using typical GNNs that conduct message passing among all the graph nodes in a flat manner may be less efficient. To address this challenge, we propose utilizing a hierarchical graph representation learning (HGRL) approach, which computes graph-level representations of AIGs at multiple levels of abstraction.\nThe HGRL consists of L layers stacked sequentially. At each layer l, the graph $G^l$ is processed by a GNN, utilizing its node feature matrix $X^l \\in \\mathbb{R}^{N^l \\times F^l}$ and adjacency matrix $A^l \\in \\{0, 1\\}^{N^l \\times N^l}$. Here, $N^l$ represents the number of nodes in $G^l$ and $F^l$ denotes the dimensionality of the node features at layer l. The initial AIG is represented as $G^0$, with $X^0$ and $A^0$ denoting its node feature matrix and adjacency matrix, respectively. The GNN at the l-th layer computes node representations through message passing among the $N^l$ nodes:\n$H^{l+1} = \\text{GNN}(X^l, A^l)$ (2)\nwhere $H^{l+1} \\in \\mathbb{R}^{N^l \\times F^{l+1}}$ is the representation matrix learned by the GNN for the nodes of $G^l$, and $F^{l+1}$ is the size of each learned node representation. After computing the node representations, we need to identify the top $N^{l+1} = \\lceil a N^l \\rceil$ most important nodes, where a is a ratio hyperparameter. This choice is made by a graph downsampling module, which computes a score for each node based on the projection of the learned node representations $H^{l+1}$ and a learnable vector. Nodes with lower scores are then removed from the graph $G^l$. Thus, the pruned node representation matrix and the pruned adjacency matrix can be computed as follows:\n$A^{l+1}, X^{l+1} = \\text{GRAPHDOWNSAMPLE}(A^l, H^{l+1})$ (3)\nwhere $A^{l+1} \\in \\{0,1\\}^{N^{l+1} \\times N^{l+1}}$ is the pruned adjacency matrix at the end of the l-th layer, and $X^{l+1} \\in \\mathbb{R}^{N^{l+1} \\times F^{l+1}}$ is the representation matrix for the $N^{l+1}$ remaining nodes. These two matrices are then given as inputs to the next encoding layer. After passing $A^0$ and $X^0$ through L consecutive graph encoding blocks, where the output of each block serves as the input for the subsequent block, the final outputs $A^L \\in \\{0,1\\}^{N^L \\times N^L}$ and $X^L \\in \\mathbb{R}^{N^L \\times F^L}$ are computed. Here, $N^L$ represents the number of nodes remaining after all pruning iterations, while $F^L$ signifies the size of the final node representations. The final representation matrix $X^L$ is subsequently fed into a global pooling module to compute the graph. This process aggregates the representations learned for the most critical nodes after multiple steps of subsequent graph neural network operations and graph downsampling:\n$h_G = \\text{GRAPHPOOL}(X^L)$ (4)\nwhere $h_G \\in \\mathbb{R}^F$ denotes the graph-level representation of the input AIG, with F representing its dimensionality.\n3.3 Recipe Encoder\nA synthesis recipe $r_i$ consists of a sequence of n transformations, with each transformation falling into one of m distinct categories. The sequence is represented as:\n$r_i = [t_{i1}, t_{i2}, ..., t_{in}],$\nwhere $t_{ij} \\in \\{C_1, C_2, ..., C_m \\}$ for $j = 1, 2, ..., n$ (5)\nTo optimize logic synthesis by predicting the QoR for a given pair of AIG and recipe, it is essential to learn a meaningful representation of the recipe. In this subsection, we elucidate the steps to achieve this.\n3.3.1 Embedding. The process of representation learning for a recipe $r_i$ commences with the conversion of its categorical transformations, each denoted as $t_{ij}$, into dense vector representations, which are low-dimensional continuous vectors. This conversion facilitates the model in identifying patterns and dependencies within the data:\n$e_i = \\text{RECIPEEMBED}(r_i)$ (6)\nHere, $e_i \\in \\mathbb{R}^{n \\times p}$ denotes the resulting embedding, where p signifies the dimensionality of the learned embedding for each transformation $t_{ij}$.\n3.3.2 Convolutional Layers. Following the computation of the embedding $e_i$ for the recipe $r_i$, a series of M one-dimensional convolutional layers, each configured with a distinct kernel size, are employed. These layers serve to discern intricate relationships between adjacent transformations and extract features at multiple scales. The m-th convolutional layer is denoted as:\n$A_m = \\text{CONVLAYER}_m(e_i)$ (7)\nHere, $A_m \\in \\mathbb{R}^{a \\times d_m}$ represents the m-th feature map learned for the recipe $r_i$, with a dimensionality of $d_m$. The final representation of recipe $r_i$ is computed by concatenating all M feature maps associated with this recipe:\n$\\lambda_i = \\text{Concat}(A_1, A_2, ...., A_M)$ (8)\nwhere $\\lambda_i$ signifies the final representation learned for $r_i$.\n3.4 Multi-Task Learning\nAddressing the LSO problem using AI-driven approaches is significantly challenged by data scarcity, as most datasets only contain a limited number of graphs. This limitation can result in model overfitting. To address this issue, we propose MTLSO, a multi-task learning approach that, in addition to the main task of QoR value regression, introduces a new task: binary multi-label graph classification. This supplementary task not only aids in training the model's parameters by providing additional signals during the training step but also signifies the relevance of a recipe to an AIG during inference, further assisting the model in predicting the QoR value. Further details on each task are discussed below.\n3.4.1 Binary Multi-label Graph Classification. We have designed a novel task, binary multi-label graph classification, to address the LSO problem more effectively and to make the most usage out of the valuable yet limited training data. This task involves taking an AIG as input and determining whether each of the K recipes is performing well on this AIG or not. In the following, we elaborate on the process of generating labels to be used for training the model and then formulate the task.\nLabel Construction Process. For a given AIG G, the dataset provides QoR values for all K associated recipes, denoted by $\\{q_i\\}_{i=1}^K$. The label construction process starts with selecting [pK] recipes with the lowest QoR values (i.e., the best-performing recipes), where p is a hyperparameter determining the ratio of top-performing recipes to be selected. Subsequently, the labels for these selected recipes are set to 1, while the labels for the remaining recipes are set to 0:\n$\\begin{cases}\n1 & \\text{if } r_i \\text{ is among the top-performing recipes of G}\\\\\n0 & \\text{otherwise}\n\\end{cases}$\nTask Formulation. The graph classifier takes as input $h_G$, the graph-level representation learned for AIG G by the graph encoder module. It then outputs a probability for each of the K recipes, specifying whether each recipe is among the best for that AIG. Hence, the binary multi-label graph classification task is formulated as:\n$P_{\\text{classification}}^G = \\text{GRAPHCLASSIFY}(h_G)$ (9)\nHere, $P_{\\text{classification}}^G \\in [0, 1]^K$ represents the predicted probabilities associated with the graph G, and the graph classifier is implemented as a 2-layer MLP with ReLU nonlinearity between the layers. The classification loss is computed as follows:\n$\\mathcal{L}_{\\text{classification}} = \\text{BINARYCROSSENTROPY}(P_{\\text{classification}}^G, C_G)$ (10)\nwhere $C_G \\in \\{0, 1\\}^K$ represents the true class labels for AIG G.\n3.4.2 QoR Value Regression. In our MTLSO approach, the second and primary task entails predicting the QoR value for a given pair (G, $r_i$). This prediction incorporates both the graph representation and the recipe representation, as well as the relative importance of the current recipe to the current graph, compared to other recipes, which is learned by the graph classifier. Consequently, the final QoR value for the pair (G, $r_i$), denoted as $\\hat{y}_G^i$, is computed as follows:\n$\\hat{y}_G^i = \\text{DECODER}(\\text{Concat}(h_G, \\lambda_i, P_{\\text{classification}}^G))$ (11)\nThe DECODER in the formula above is implemented as a 3-layer MLP with ReLU nonlinearity. For computing the loss in this regression task, we employ the following function:\n$\\mathcal{L}_{\\text{regression}} = \\text{REGRESSIONLOSS}(\\hat{y}_G^i, y_i)$ (12)\nThe detail regarding the choice of REGRESSIONLOSS is provided in Section 4. Finally, the total loss function for training the model combines the classification loss and the regression loss:\n$\\mathcal{L} = \\mathcal{L}_{\\text{classification}} + \\gamma \\mathcal{L}_{\\text{regression}}$ (13)\nwhere $\\gamma$ is a weight hyperparameter to balance $\\mathcal{L}_{\\text{classification}}$ and $\\mathcal{L}_{\\text{regression}}$."}, {"title": "4 EXPERIMENTS", "content": "In this section, we first elaborate on the datasets used for the evaluation process. Then, we outline the state-of-the-art baselines with which we compare our method. Next, the evaluation metrics are explained, followed by a description of the experimental setup. Subsequently, we discuss the results of our proposed approach, as well as those of the competitor methods. Finally, we review the results of our ablation study on the role of each model component.\n4.1 Datasets\nTo assess the performance of our proposed methodology, we conducted experiments using three datasets: OpenABC-D [3], EPFL [1], and a proprietary dataset, which we refer to as Commercial Dataset (CD) in this paper. Detailed statistics for these datasets are provided in Table 1.\n4.2 Baselines\nWe compare our approach with several well-known state-of-the-art methods, which are explained below.\n\u2022 Chowdhury et al. [3]: They use GCN [13] for encoding AIGs and a set of 1D convolutional layers to learn representations of recipes. These two representations are then concatenated to predict the QoR.\n\u2022 LOSTIN [18]: This model employs GIN [20] to compute representations of circuits and uses an LSTM for learning representations of synthesis recipes. The concatenation of these two representations is utilized for downstream QoR prediction.\n\u2022 GNN-H [19]: It adopts a similar strategy for predicting QoR values as LOSTIN [18], except that it utilizes PNA [4] for circuit representation learning.\n\u2022 Yang et al. [21]: They replace LSTM in LOSTIN [18] and GNN-H [19] by a Transformer encoder. Moreover, they utilize GraphSage [8] as the GNN.\n4.3 Metrics\nTo evaluate the effectiveness of our proposed method compared to other competitor approaches, we present the results in terms of Mean Absolute Percentage Error (MAPE). MAPE quantifies the accuracy of predictions by calculating the average absolute percentage difference between the actual and predicted values. It is defined as:\n$\\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^n \\frac{|y_i - \\hat{y_i}|}{y_i} \\times 100$\nwhere $y_i$ denotes the actual value, $\\hat{y_i}$ denotes the predicted value, and n is the total number of observations.\n4.4 Implementation Details\nOur implementation is done using PyTorch [16]. The Graph Encoder module employed in our primary experiment comprises two layers of Graph Encoding/Downsampling (i.e., L = 2), a configuration further examined through an ablation study. We adopt a 2-layer GCN [13] as our Graph Neural Network, with a hidden layer size of 64. The dimensions of the learned node features in the first and second encoding layers, denoted as F1 and F2, respectively, are both set to 64.\nFor the Graph Downsampler, TopKPooling [2] is utilized with a node retainment ratio of 0.5 (i.e., a = 0.5) for the primary experiments, with additional values examined in the ablation study. A multi-readout strategy that integrates both mean and max aggregations is adopted for GRAPHPOOL, resulting in a final graph-level representation dimension of 128 (i.e., F = 128).\nThe recipe encoder's embedding dimensionality is set to 60 (i.e., p = 60), and we utilize a series of four one-dimensional convolutional layers (i.e., M = 4). For label construction, the parameter p is set to 0.5. Moreover, the Relative Squared Error (RSE) is used as the regression loss function. The hyperparameter $\\gamma$ is set to 1, assigning equal weight to both graph classification and regression losses.\n4.5 Results\nThe main results, reported in Table 2, are averaged over multiple experimental runs. These findings demonstrate that our proposed MTLSO method outperforms the four baseline models in nearly all scenarios, with average gains of 8.22% in delay and 5.95% in area across all baselines and datasets. This underscores the critical advantage of employing a multi-task learning approach combined with a hierarchical graph encoder, rather than relying on simpler alternatives, for effectively addressing this problem.\nIt is important to note that our main architecture shares similarities with the one proposed by Chowdhury et al., particularly in our decision to use less complex techniques such as GCN [13] as the core of our graph encoder and 1D convolution layers for encoding the recipes. This contrasts with the more advanced recipe encoders, like LSTM or Transformer, employed by LOSTIN [18], GNN-H [19], and Yang et al. [21], which consider the ordering of logic transformations within each recipe. We opted for simpler model components to demonstrate that multi-task learning powered by HGRL, even with these basic components, can outperform the baselines. This suggests that performance could be further enhanced by using more advanced GNNs like GIN, as utilized by LOSTIN [18], or by incorporating more sophisticated recipe encoders.\nIn the next subsection, we conduct an ablation study to analyze the individual contributions of the multi-task learning approach and the hierarchical graph encoder to the overall model performance.\n4.6 Ablation Study\nTo assess the importance of each part of our model, we conducted an ablation study. In this study, we first modified our MTLSO method by replacing our Hierarchical Graph Representation Learning (HGRL) module with a simpler version, referred to as Plain Graph Representation Learning (PGRL). PGRL consists of a single GCN layer without any downsampling layer. Next, we retained the HGRL module from our MTLSO method but eliminated the graph classification task, training the model in a Single-task Learning (STL) mode. We present the results in Table 3. As shown in the table, MTLSO achieves the best results, indicating that both the HGRL module and the multi-task learning strategy significantly contribute to the model's performance. When comparing the significance of multi-task learning and the HGRL module, the results demonstrate that the former is more critical than the latter. This is evidenced by the fact that PGRL (trained in a multi-task manner) outperforms the STL variant.\nWe performed an additional ablation study on the number of Graph Encoding/Downsampling layers, denoted as L. In addition to our main experiments with L = 2, we investigated the cases where L was set to 1 and 3. The results for delay minimization are presented in Figure 2. We assessed performance using graph classification metrics and MAPE to understand how the hyperparameter L influences both the quality of the learned graph-level representation and the efficiency of the regression task. The results indicate that performance consistently improves with more than one encoding layer. This underscores the necessity of adopting a hierarchical strategy for effectively learning representations of large AIGs. Moreover, as the results suggest, the optimal L is 2 for EPFL and CD, and 3 for OpenABC-D based on the graph classification metrics. This implies that we can further improve the quality of graph representations by using the optimal value of the hyperparameter L for each individual dataset.\nFinally, an ablation study was conducted on the ratio hyperparameter a, which determines the percentage of nodes retained by the graph downsampling module during each graph encoding block. The results, illustrated in charts within Figure 3, encompass three values of a: specifically, 0.5 alongside the extremes of 0.1 and 0.9. Across these experiments, optimal performance consistently favored a = 0.5, with both extreme values showing inferior results. This finding supports the importance of adopting such a hierarchical strategy for encoding AIGs, since a portion of nodes are less informative and should be filtered out (as evidenced by a = 0.9 performing worse than a = 0.5). Conversely, the underperformance of a = 0.1 highlights that some nodes have a direct influence on the quality of the final graph-level representation, and hence should not be pruned. This underscores the need to set an optimal value for this hyperparameter."}, {"title": "5 CONCLUSION", "content": "In this paper, we present a novel multi-task learning approach designed for LSO, focusing on mitigating overfitting caused by limited data availability, a critical issue of machine learning-based LSO solvers. Our approach involves jointly training a model for both multi-label graph classification and regression tasks, enhancing its ability to predict QoR values for pairs of AIGs and synthesis recipes. To further boost the effectiveness of our method, we employ a hierarchical approach with multiple layers of successive Graph Encoding/Downsampling to learn graph-level representations of AIGs, as due to the inherent complexity and large size of these graphs, simple plain GNNs often struggle to be efficient enough. Extensive experiments across multiple datasets, compared against four established baselines, demonstrate an average performance gain of 8.22% and 5.95% for delay and area minimization, respectively, underscoring the effectiveness of our proposed approach."}]}