{"title": "Towards Robust Multi-tab Website Fingerprinting", "authors": ["Xinhao Deng", "Xiyuan Zhao", "Qilei Yin", "Zhuotao Liu", "Qi Li", "Mingwei Xu", "Ke Xu", "Jianping Wu"], "abstract": "Website fingerprinting enables an eavesdropper to determine which websites a user is visiting over an encrypted connection. State-of-the-art website fingerprinting (WF) attacks have demonstrated effectiveness even against Tor-protected network traffic. However, existing WF attacks have critical limitations on accurately identifying websites in multi-tab browsing sessions, where the holistic pattern of individual websites is no longer preserved, and the number of tabs opened by a client is unknown a priori. In this paper, we propose ARES, a novel WF framework natively designed for multi-tab WF attacks. ARES formulates the multi-tab attack as a multi-label classification problem and solves it using the novel Transformer-based models. Specifically, ARES extracts local patterns based on multi-level traffic aggregation features and utilizes the improved self-attention mechanism to analyze the correlations between these local patterns, effectively identifying websites. We implement a prototype of ARES and extensively evaluate its effectiveness using our large-scale datasets collected over multiple months. The experimental results illustrate that ARES achieves optimal performance in several realistic scenarios. Further, ARES remains robust even against various WF defenses.", "sections": [{"title": "I. INTRODUCTION", "content": "Anonymous communication techniques are designed to prevent the content and metadata of network communications from being leaked and/or tampered by malicious activities, such as eavesdropping and man-in-the-middle attack. With millions of daily users [2], the Onion Router (Tor) is one of the most popular anonymous communication tools used to protect web browsing privacy. Tor hides user activities by establishing browsing sessions through Tor circuits with randomly selected Tor relays, where data communication in each Tor circuit is encrypted via ephemeral keys and forwarded in fix-sized cells [3].\nAlthough Tor mitigates the privacy threat to some extent, an adversary can still observe the encrypted traffic of a Tor browsing session and utilize its network traffic patterns (e.g., the packet size and interval statistics) to infer the websites visited by the Tor client. This technique is referred to as the Website Fingerprinting (WF) attack. The rationale behind the WF attack is that the content of each website results in a unique traffic pattern distinguishable from other websites.\nPrior works [4], [5], [6], [7], [8], [9] demonstrated the effectiveness of WF attack, with best attack accuracy exceeding 95%. In general, these works formulate the WF attack as a classification problem and solve it based on machine learning or deep learning algorithms, such as Support Vector Machine (SVM), Random Forest, and Convolutional Neural Networks (CNN).\nThe effectiveness of existing WF attacks relies on a common yet unrealistic assumption. In particular, they assume that the client only visits a single web page in one browsing session [10], [11], [12]. This single-page assumption does not always hold in practice since normal clients often open multiple browser tabs simultaneously (or within a very short period) [10], [11], [13]. A multi-tab browsing session contains the network traffic generated by different web pages such that their patterns are mixed and become more difficult to be identified. Prior work [10] shows that the performance of the traditional WF attacks decreases drastically on multi-tab browsing scenarios. To relax this assumption, a series of multi-tab WF attacks have been proposed [11], [12], [14], [15], [16], [17].\nMost existing multi-tab WF attacks (e.g., [11], [12], [14], [15]) share a similar design architecture: they first divide the whole browsing sessions into multiple clean traffic chunks, where each chunk only contains the traffic of a single website, and then infer the visited websites based on each chunk. However, this architecture has three critical drawbacks. (i) They require prior knowledge of how many tabs are opened by clients. Existing multi-tab WF models are trained given a fixed number of tabs, e.g., 2 tabs in [12]. Yet, their models are not generic enough to handle other tab numbers. Consequently, these methods often yield very limited accuracy in practice when the number of opened tabs is dynamic and unknown a priori. (ii) Even in such a restricted setting, these methods are not resilient to the WF defense mechanisms. WF defenses are designed to perturb the original network traffic patterns by either delaying packet transmissions or padding dummy packets. Prior work [15] shows that lightweight WF defenses [18], [19] can significantly limit the effectiveness of existing multi-tab WF attacks. (iii) Further, their effectiveness further decreases as clients open more browser tabs. The capability of existing multi-tab WF attacks depends on the quality of clean traffic chunks, such as the number of clean chunks and the amount of clean traffic in these chunks. As clients open more browser tabs, it is more difficult to extract clean chunks from a browsing session. Recent studies [16], [17] explore WF attacks without explicitly dividing the obfuscated traffic into individual chunks. However, these attacks still require prior knowledge of the maximum number of tabs and exhibit significant performance degradation under WF defenses.\nOur Work. To address these limitations, we propose a new multi-tab website fingerprinting attack mechanism, ARES. The core idea of ARES is formulating the multi-tab WF attack as a multi-label classification problem to fundamentally relax the required prior knowledge on the number of tabs opened in a browsing session. Towards this end, we design ARES based on a novel multi-tab WF attack framework containing multiple classifiers. Different from the existing end-to-end WF attacks, we transform the complex multi-label classification problems into the multiple binary classification problem, where each classifier is responsible for calculating the possibility that whether a specific monitored website is visited. Afterwards, ARES regularizes and ranks these possibilities, and then outputs the complete label set for all monitored websites based on a pre-determined threshold. Besides the architectural innovation, we also develop a new Transformer model, Trans-WF, as the robust individual classifier used in ARES, as described below.\nThe key observation for designing Trans-WF is that although a website's clean and holistic traffic pattern is no longer preserved in multi-tab browsing sessions (or simply due to the dummy packets padded by WF defenses), it is still possible to extract multiple local patterns for the website from multiple short traffic segments. Thus, Trans-WF can build signatures for different websites by analyzing the relevance among these local traffic patterns. In its design, Trans-WF employs a multi-level traffic aggregation module to divide a browsing session into multiple traffic segments, and separately extract packet-based and burst-based aggregation features from these segments. These aggregation features effectively capture the robust patterns of different websites within obfuscated traffic. Then Trans-WF utilizes a local profiling module to accurately extract the local patterns from aggregation features. Moreover, Trans-WF designs an improved attention mechanism to further reduce the impact of noises on calculating the relevance among local patterns.\nWe extensively evaluate ARES based on large-scale datasets from over 500 thousand multi-tab Tor browsing sessions collected from May 2021 to December 2021 and from June 2022 to November 2022. In addition to multi-tab browsing, we consider various real-world complexities in WF attacks, including (i) multiple Tor versions co-exist, (ii) clients may visit sub-pages beyond the main page in each website, and (iii) the vantage points for collecting traffic could vary (not just at client-side). To the best of our knowledge, our datasets are by far the largest multi-tab WF datasets."}, {"title": "II. BACKGROUND", "content": "In general, the fingerprint of a website is a combination of network traffic patterns, such as the statistics of packet sizes and intervals when accessing this website. The Website Fingerprinting (WF) attack is a technique that can identify the websites accessed by a client only by analyzing the client's browsing traffic, even in encrypted form. When applied by adversaries, the WF attack could compromise normal users' online privacy. Yet WF could also assist in crime tracking on the dark web.\nTechnically, the WF attack is formulated as a classification problem solvable using machine learning (ML) algorithms. The existing researches have developed various types of features, e.g., the data volume and packet intervals, to profile the encrypted traffic. A series of ML-based classifiers (e.g., SVM and Random Forest) are used to perform WF attack [4], [5], [6], [11], [20]. In particular, with the emergence of deep learning (DL), DL-based WF attacks achieve automatic feature extraction and higher accuracy [8], [9]. Further, a study [9] shows DL-based WF attacks can effectively bypass the existing WTF-PAD defense [18]. However, DL-based WF attacks require a large amount of training data. Sirinam et al. [21] proposed the triple networks based WF attack to solve this problem. Still, the above WF attacks assume the client's browsing traffic is purely generated by a single website. The multi-tab attacks [11], [12], [14], [15] relaxed this assumption. They propose to divide the network traffic to obtain clean traffic chunks to facilitate website fingerprinting. Moreover, the latest multi-tab attacks [16], [17] leverage Transformer to directly identify obfuscated traffic. However, existing multi-tab WF attacks are not resilient to the WF defenses. Even worse, they require prior knowledge of the number of tabs (or the maximum number of tabs) opened by the user, which is challenging in practice.\nWebsite fingerprinting defenses are designed as countermeasures against WF attacks. Existing WF defenses mainly fall into three categories: padding-based, mimicry and regulariza-tion defense. Padding-based defenses (such as WTF-PAD [18] and Front [19]) disorder the original traffic pattern by ran-domly adding dummy packets. Mimicry defenses confuse the traffic pattern, causing the classifiers of WF attacks to falsely identify a website as another one [22], [23], [24]. For example, Decoy [23] loads a decoy website along with the real website. Regularization defenses make the traffic pattern of all websites fixed by adding dummy packets and delaying packets [25], [26], yet these defenses typically impose high overhead."}, {"title": "III. THREAT MODEL", "content": "In our threat model, clients access websites using privacy-enhancing techniques like Tor to hide their online activities. Each client could open several browser tabs to load multiple pages from different websites simultaneously (or within a short period of time). As a result, a client's browsing session may contain encrypted network packets from multiple websites. Further, the client's browser or on-path Tor relay nodes could have deployed some WF defense mechanisms, such that the traffic patterns of individual websites are no longer preserved. \nWe consider a privacy-hungry adversary that primarily focuses on de-anonymizing a client's online activities by inferring the websites visited by the client through website fingerprinting. Therefore, the adversary could deploy multiple traffic mirroring points to record the client's encrypted network traffic, even before the traffic enters the entry node of the Tor network. Yet, actively delaying or even discarding the client's network traffic is out of the scope of our threat model.\nCompared with the original multi-tab WF threat mod-els [11], [12], [14], [15], [16], our model is more realistic, yet more challenging, for the following three reasons. (i) We consider that the client could have deployed existing WF defenses. As a result, the traffic pattern of individual websites could have been perturbed by these anti-WF techniques. (ii) We consider that the number of tabs opened by the client is dynamic and unknown a priori. Prior WF mechanisms assume that the clients always open a fixed number of tabs (e.g., two tabs in [15]) since their models have to be trained and tested under the same specific setting. This is restrictive and unrealistic. (iii) We consider critical real-world complexities in WF attacks. Existing WF attacks [6], [8], [9], [21], [30] are evaluated using over-simplified scenarios, where clients use the same version of Tor Browser, clients only browse the homepage of websites, and network traffic is collectible at the client-side, etc. These assumptions are largely incorrect in practice. Therefore, our design considers a more practical threat model, where multiple versions of Tor Browsers can co-exist, clients can visit the sub-pages of websites, and different vantage points for traffic collection other than at the client side are evaluated.\nSimilar to existing arts [6], [8], [9], [21], our model con-tains two attack scenarios: closed-world and open-world. The closed-world scenario assumes that clients will only visit a small set of websites (e.g., the Alexa Top 100 websites). In this case, the adversary has the resources to collect data from all these websites (referred to as monitored websites). In the open-world scenario, clients can visit arbitrary websites, and therefore the adversary may only possess training data for a subset of the websites."}, {"title": "IV. DESIGN OF ARES", "content": "In this section, we present the design detail of ARES. We start with an overview of ARES before delving into its individual components.\nAs discussed in Section I, prior multi-tab WF attacks require prior knowledge of the number of tabs opened in a browsing session. To fundamentally relax this limitation, ARES regards the multi-tab attack as a multi-label classification problem.\nIt is challenging to solve the multi-label classification problem because the traffic from different websites is mixed together and the number of visited websites is unknown and dynamic. In particular, due to the high-dimensional features, mixed website traffic, and noises generated by WF defenses, the performance of existing WF attacks degrades significantly. Therefore, ARES builds a multi-tab WF attack framework with multiple classifiers, and each classifier is utilized to calculate the possibility of whether a specific website is accessed. Then, ARES integrates the results of individual classifiers to generate the complete label set for all monitored websites without prior knowledge of the number of tabs.\nMoreover, we develop a novel Transformer [31] model called Transformer for Website Fingerprinting (Trans-WF), as the classifier used in ARES. The design of Trans-WF is based on a key observation that a website's local patterns are still extractable from multiple short traffic segments, even when the entire traffic pattern is no longer preserved in a multi-tab browsing session and under defenses. Thus, Trans-WF can build robust signatures for different websites based on these local patterns.\nIn Figure 2, we illustrate the architecture of ARES. At a high level, ARES consists of N Trans-WF, where N represents the number of monitored websites, and the i-th Trans-WF is used to identify the i-th monitored website. ARES fuses the outputs of all Trans-WF models to output a label set for all monitored websites. The Trans-WF model consists of three modules designed to robustly identify obfuscated traffic of the specific website, including multi-level traffic aggregation, local profiling, and website identification.\nMulti-level Traffic Aggregation. The multi-level traffic ag-gregation module extracts features containing local website information from obfuscated traffic. Although global website information within the obfuscated traffic is disrupted, sufficient local website information is retained within its sub-segments. Therefore, Trans-WF first divides the traffic and then extracts packet-level and burst-level aggregation features from each segment to preserve local website information.\nLocal Profiling. The local profiling module utilizes a convo-lutional neural network (CNN) to extract local traffic patterns that represent key elements of the specific website. The obfus-cated traffic generated by multi-tab browsing is dynamic, re-sulting in local traffic patterns having variable positions within the traffic. By leveraging the translation invariance of CNNs, Trans-WF can effectively extract local traffic patterns that appear in any position, thus supporting website identification.\nWebsite Identification. The website identification module robustly identifies obfuscated traffic through an improved self-attention mechanism. Noise packets generated by multi-tab browsing and WF defenses pose significant challenges for website identification. However, there are correlations among different local patterns within obfuscated traffic. Trans-WF utilizes the top-m self-attention mechanism that mitigates the interference of noise packets and effectively analyzes correlations between local patterns.\n\nThe multi-level traffic aggregation module aims to extract traffic features containing website information from multi-tab obfuscated traffic while maintaining robustness even under WF defenses. Extracting effective website features from obfuscated traffic is challenging because noise packets introduced by multi-tab browsing or WF defenses disrupt the global traffic patterns of websites. The motivation behind this module is that sub-segments of obfuscated traffic still contain sufficient local patterns associated with key elements of the website. Therefore, the multi-level traffic aggregation module focuses on extracting meaningful local features from short traffic segments. These local features preserve sufficient information to characterize individual websites while being resilient to the noise and variability introduced by multi-tab browsing and WF defenses.\nAs shown in Figure 3, we illustrate the process of multi-level traffic aggregation. We extend the traffic aggregation used in existing single-tab WF attacks [32], [33]. Specifically, Trans-WF first divides the obfuscated traffic into fixed-length sub-segments based on a uniform time interval t. Each sub-segment contains local traffic patterns that are relevant and consistent to the website. Through traffic division, this module mitigates the impact of global noise and facilitates further feature extraction. Next, Trans-WF extracts aggregation fea-tures separately for incoming and outgoing traffic from each sub-segment, including packet-level features and burst-level features. The packet-level features include the total number of packets as well as the average interval between consecutive packets. These features capture fine-grained packet behavior and temporal dynamics, which are unique to specific websites as they are associated with the key elements of those websites. Unlike packet-level features, burst-level features are coarse-grained representations of traffic transmission. A burst repre-sents a sequence of consecutive packets in the same direction. The burst-level features extracted by Trans-WF include the total number of bursts and the average burst size, effectively capturing higher-level traffic patterns that remain stable even under obfuscation.\n\nThe local profiling module is applied to profile the local patterns of a monitored website by extracting the local feature vectors from multi-level aggregation features. This is chal-lenging for the following two reasons: (i) the locations of the packet sequences representing different local patterns are not fixed; (ii) the irreverent packets in the same segment generated from other websites or WF defenses create non-trivial noises. To overcome this challenge, we design our local profiling module based on Convolutional Neural Networks (CNN). CNN has the characteristic of invariant translation [34], i.e., it can profile the input data into the same embedding vectors regardless of how the input data is shifted. Moreover, prior WF attacks have demonstrated that CNN is more resilient against noises [9], [35].\nAs shown in Figure 4, the local profiling module contains L blocks and each block consists of two one-dimensional convolution layers (Conv1d), two batch normalization layers (BN) with the ReLU activation function (ReLU) and a max-pooling layer. Besides, we introduce two additional regular-ization techniques to further enhance our module. (i) Residual connection. It propagates the intermediate output of lower layers to higher layers through skip connections to prevent gradient vanishing. (ii) Dropout. It randomly drops some units (along with their connections) from the neural network during training to alleviate over-fitting.\nIn each block, the input is first fed into two convolution layers and two batch normalization layers, to extract the local features. These local feature vectors (i.e., the output of the last batch normalization layer) are connected with their original input via the residual connection, and then they will be fed into the max-pooling layer, for the purpose of retaining the most representative features while progressively reducing their sizes. Thus, the small perturbations in the input traffic segments can be filtered by the max-pooling layer.\n\nThe website identification module is in charge of analyzing the relevance among local patterns to identify whether a monitored website is visited in the multi-tab browsing session. The self-attention mechanism proposed in the transformer model [31] is a reasonable choice for this goal. The self-attention mechanism is widely applied in natural language processing and computer vision [31], [36], [37], [38], which can capture the dependencies within a sequence. Therefore, the self-attention mechanism can effectively analyze the de-pendencies of multiple local patterns, and thus identify the target website. Since the number of tabs opened by the client is dynamic, we use a multi-head self-attention mechanism to capture the information of the target website under the different numbers of tabs. Furthermore, we design the top-m attention, an improved self-attention mechanism, to enhance the model robustness under WF defenses.\nThe attention mechanism is a function that computes the relevance between a query and a set of key-value pairs, where the query, key, and value are all vectors projected from the input data individually [31]. In particular, the attention function first calculates the weight of each value using a compatibility function of the query and its corresponding key, and then produces a weighted sum of all values as the output that represents the relevance between the query and key-value pairs. When we apply this mechanism to correlate different segments of the same sequence, namely the self-attention [31], it can convert the sequence into a new representation that reveals its internal relevance. Thus, we can take the local feature vectors as the input of the self-attention function, and utilize the corresponding output as the fingerprint of a monitored website.\nWe illustrate this procedure using the vanilla attention mech-anism [31] at first. Let Q, K, and V donate the query, key, and value matrices, respectively. As shown in Equation (1), these matrices can be achieved via linear projections of a batch of input data X (i.e., the local feature vectors), where\n$Q = XW^Q, K = XW^K, V = XW^V,$\nwhere $W^Q, W^K, W^V \\in R^{d_m \\times d}$ are the matrices for pro-jections and d is the dimension of an output vector. Note that these projection matrices will be learned during model training. Then, the output of this attention function can be computed via Equation (2):\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V.$\nIn general, this equation computes the dot products of each query with all keys, scales these results by dividing $\\sqrt{d}$, and applies a softmax function to obtain the weights of each value.\nHowever, when identifying a monitored website under multi-tab browsing scenarios, the vanilla self-attention mech-anism has a severe shortcoming in that it is not resilient to the traffic noises generated by other websites and WF defenses. In particular, this mechanism contains a fully-connected attention layer such that the output vector for an input vector (i.e., a local feature vector) depends on the relevance between this input with all other inputs (i.e., all other local feature vectors). As a result, the local features extracted from noisy traffic inevitably reduce the accuracy of the output.\nTo handle this issue, we design an improved attention layer, namely top-m attention, based on [39]. This layer calculates the output for an input vector based on the top-m weight values computed by its corresponding query and all keys, rather than all weight values. In general, the traffic of the monitored website is less correlated with the traffic generated by other websites or WF defenses than itself. This means that the monitored website's local features and the local features from other websites tend to have smaller attention-based weight values. Thus, we can filter out the interference from the traffic noises via the top-m selection strategy. Let Q, K, and V donate the query, key, and value matrices, respectively. We formally describe this new attention layer design in Equation (3):\n$Attention_{Top-m}(Q, K, V) = softmax(\\Gamma(\\frac{QK^T}{\\sqrt{d}}))V,$\n$[\\Gamma(A)]_{ij} = \\{\n    \\begin{array}{ll}\n        A_{ij}, & A_{ij} \\text{ is the top-m largest elements in row j,}\\\\\n        \\epsilon, & \\text{otherwise,}\n    \\end{array}\n    $\nwhere \u0393() defines a row-wise top-m selection operation and $\\epsilon$ is a small enough constant. In our website identification module, we replace the vanilla attention layer with our new design.\nAs the number of tabs opened by the client is unknown and dynamic, the correlation between local patterns varies with the number of tabs. Therefore, we parallel multiple top-m attention layers to compose a multi-head top-m attention layer. As shown in Figure 5, it allows Trans-WF to jointly capture the relevance among the local features even in the dynamic number of tabs, such that the relevance representations can be enriched to achieve even more accurate website identifications. For the i-th head, its output is computed via Equation (5):\n$head_i = Attention_{Top-m}(QW_i^Q, KW_i^K, VW_i^V),$\nwhere $W_i^Q, W_i^K, W_i^V \\in R^{d \\times d_h}$ are the weight matrices specific to this head, where $d_h$ is the dimension of the output vector of each head. Let h denotes the number of heads, and we set $d_h = d/h$. Note that each head performs its own task individually. Then, the results of each head will be concatenated and transformed by a linear projection. Let \u039b(X) denote the output of our multi-head top-m attention layer. Finally, we can produce \u039b(X) via Equation (6).\n$\\Lambda(X) = Concat(head_1, ..., head_h)W^O,$\nwhere $W^O \\in R^{hd_h \\times d}$ is the weight matrix. With the output of the attention layer, we utilize a batch normalization layer and a Multilayer perceptron (MLP) to identify the existence of a target website. Also, we apply the techniques of residual connection and dropout to avoid the problems of gradient van-ishing and over-fitting, respectively. The identification result $\\Phi(X)$ of a target website can be computed as follows:\n$\\Phi(X) = MLP(LN(X + Dropout(\\Lambda(X)))),$\n$LN(X) = \\frac{g(X - \\mu)}{\\sqrt{\\sigma^2 + \\epsilon}} + b,$\nwhere LN is the layer normalization, g,b are the gain and bias parameters, $\\mu, \\sigma$ are the mean and the variance of X, $\\odot$ is the element-wise multiplication between two vectors, and $\\epsilon$ is a small constant to prevent division by zero. The MLP utilizes the common softmax function.\nTo mitigate the potential over-fitting of Trans-WF, we thus use Droppath [40] in Trans-WF. The Droppath randomly drops some training instances in the residual connection during training, causing these instances to skip part of the training. In particular, the Droppath achieves differential model training, which can alleviate the over-fitting."}, {"title": "V. EVALUATION", "content": "In this section", "33": ".", "41": "thereby improving the practicality of ARES for real-world applica-tions.\nDatasets. We develop an automated Tor browsing tool with over 1", "42": ".", "dataset": "We selected the Alexa top 100 websites as monitored websites and collected multi-tab browsing traffic for different website combinations. The dataset contains over 230"}, {"dataset": "In addition to the 100 monitored websites", "defense": "Randomly padding dummy packets is a common defense strategy to minimize the data overhead of the defense. This dataset contains over 50"}, {"defense": "The WTF-PAD [18", "43": "."}, {"defense": "The Front [19"}, {"defense": "The RegulaTor [24", "Settings": "In realistic scenarios", "25": [44], "45": "but these defenses are not practically deployable due to the excessive overhead. High overhead could cause functionality issues in Tor relay nodes. Therefore", "9": [32], "33": "we select four representative defenses.\nBaselines. We use seven representative WF attacks as our baseline methods", "attacks": "We select two classical single-tab WF attacks: Var-CNN [46", "47": ".", "defenses": "We select three state-of-the-art (SOTA) single-tab WF attacks that are resilient to WF defenses: DF [9", "35": "and RF [32"}, {"attacks": "We choose two state-of-the-art (SOTA) multi-tab WF attacks: BAPM [16", "17": ".", "metrics": "AUC [48", "49": ".", "follows": "according to Equation (10).\n$MAP@k = \\frac{\\sum_{i=1"}], "as": "Precision = $\\frac{TP}{TP+FP}$, and Recall = $\\frac{TP}{TP+FN}$. We can compute the average results for all websites.\nWe first evaluate ARES in the closed-world scenario. Table II shows the AUC, P@k, MAP@k results for multi-tab WF attacks. ARES achieves the best performance across different multi-tab settings. Specifically, ARES achieves a P@2 of 0.904 in the 2-tab setting, outperforming NetCLR, BAPM, DF, DF, Tik-Tok, Var-CNN, and TMWF, whose P@2 are 0.345, 0.528, 0.601, 0.641, 0.647, 0.655, and 0.722, respectively. Even in the most challenging 5-tab setting, ARES achieves a P@5 of 0.869 and a MAP@5 of 0.909, representing an average improvement of 190.83% and 135.02% over the baselines, respectively. We observe that as the number of tabs increases, the performance of existing attacks declines signif-icantly, whereas ARES maintains greater stability. Compared to the baselines, ARES exhibits higher prediction probabilities for visited websites, effectively identifying all the visited sites within obfuscated traffic.\nWe further evaluate multi-tab attacks using the AUC metric. The results show that when the number of tabs is 2, 3, 4, and 5, ARES achieves an average AUC improvement of 6.25%, 14.48%, 18.6%, and 25.71% over the baselines, respectively. These findings demonstrate that ARES can more effectively differentiate obfuscated traffic from various web-sites, reducing misidentifications. Furthermore, as the number of tabs increases, the performance advantage of ARES over the baselines becomes more obvious.\nFigure 6 illustrates the precision-recall curves of all WF attacks under different multi-tab settings. The precision-recall curve shows the average precision and recall of identifying all websites at various thresholds, with curves closer to the upper-right corner indicating better attack performance. ARES achieves the best precision-recall curves, demonstrating its optimal identification precision and recall for multi-tab ob-fuscated traffic.\nNow we evaluate the performance of ARES in the open-world scenario. Recall that the open-world experiments regard all non-monitored websites as one website category, while each monitored website is viewed as an individual category. As a result, the number of instances in the non-monitored website category is much larger than that of each monitored category. To avoid the data imbalance problem, we follow the settings in the prior arts [8", "9": "that mix all closed and open-world instances collected from the same tab setting in our evaluation. For instance, we combine the 2-tab closed and open-world instances to run the 2-tab open-world experiment.\nWe measure the AUC scores for monitored and non-monit"}