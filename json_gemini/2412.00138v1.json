{"title": "Unleashing the Power of Data Synthesis in Visual Localization", "authors": ["Sihang Li", "Siqi Tan", "Bowen Chang", "Jing Zhang", "Chen Feng", "Yiming Li"], "abstract": "Visual localization, which estimates a camera's pose\nwithin a known scene, is a long-standing challenge in vi-\nsion and robotics. Recent end-to-end methods that directly\nregress camera poses from query images have gained at-\ntention for fast inference. However, existing methods of-\nten struggle to generalize to unseen views. In this work,\nwe aim to unleash the power of data synthesis to promote\nthe generalizability of pose regression. Specifically, we lift\nreal 2D images into 3D Gaussian Splats with varying ap-\npearance and deblurring abilities, which are then used as a\ndata engine to synthesize more posed images. To fully lever-\nage the synthetic data, we build a two-branch joint training\npipeline, with an adversarial discriminator to bridge the\nsyn-to-real gap. Experiments on established benchmarks\nshow that our method outperforms state-of-the-art end-to-end approaches, reducing translation and rotation errors by\n50% and 21.6% on indoor datasets, and 35.56% and 38.7%\non outdoor datasets. We also validate the effectiveness of\nour method in dynamic driving scenarios under varying\nweather conditions. Notably, as data synthesis scales up,\nour method exhibits a growing ability to interpolate and ex-\ntrapolate training data for localizing unseen views.", "sections": [{"title": "1. Introduction", "content": "Visual localization, the task of calculating a 6-DoF camera\npose-its translation and rotation-based on a query image\nwithin a given environment, is essential for wide applica-\ntions, including robotics [1], autonomous vehicles [19], and\nvirtual reality [11]. Besides traditional geometry-based ap-\nproaches, recent learning-based visual localization methods\neither adopt scene coordinate regression (SCR) [4, 5, 40,\n59] or absolute pose regression (APR) [6, 9, 22, 51]. SCR\nmethods focus on learning-based 2D-3D correspondences\nfollowed by subsequent Perspective-n-Point (PnP) for pose\nestimation. In contrast, APR methods employ a supervised\nframework to train a regression neural network on image-\npose pairs, enabling direct pose estimation during inference.\nAPR offers the advantage of faster runtime and better per-\nformance in challenging conditions, such as images with\nlow texture or repetitive patterns [30], making it a promis-\ning approach for advancing visual localization.\nDespite the promises, a well-known pivotal work [47]\ndiscovered that existing APR seemed to perform image-\nbased memorization, i.e., retrieving poses seen during train-\ning, more than generalization, i.e., interpolating between or\nextrapolating beyond training poses. Driven by this crucial\nfinding, to improve such memorization while avoiding the\nneed for denser real-world training samples, recent meth-\nods leverage Neural Radiance Field (NeRF) [37] to synthe-\nsize additional posed images for APR training [9, 30, 39].\nHowever, a critical question remains: why do previous\nAPR methods struggle to generalize effectively? Deep neu-\nral networks show impressive generalization in many other\ntasks [29, 60, 61, 63], and humans can infer poses from new\nviewpoints in known environments. So, what is missing?\nWe hypothesize two key gaps: (1) Data Gap: Prior methods\nlacked sufficient diversity and quality in synthesized data to\nlearn a robust feature space. (2) Learning Gap: Limita-\ntions in previous learning pipelines prevented effective use\nof large-scale data to enhance generalizability.\nTo address the two key gaps, we propose a novel robust\ntraining framework for absolute pose regression (RAP), as\nshown in Fig. 1. We address the data gap by adopting and\nextending 3D Gaussian Splatting (3DGS) [23] to allow ef-\nficient and diverse data synthesis with controllable varying\nappearance. We address the learning gap by designing a\ntwo-branch joint training. The first branch coarsely trains\nour Transformer-based APR with both real data and data\nsynthesized at the original real pose, together with an adver-\nsarial discriminator to reduce the syn-to-real domain gap.\nThe second branch progressively unleashes the power of\ndata synthesis with randomly perturbed poses and appear-\nances, providing additional supervision to the same APR\nTransformer. We systematically evaluate each component's\neffect and analyze how the synthesized data endows our\nAPR model with better generalizability in pose regression.\nThrough extensive experiments, we demonstrate that\nscaling up the training data significantly enhances repre-\nsentation learning in APR. Using only 20% real training\ndata with our data synthesis already leads to a representa-\ntion with better APR performance than using all real data, as\nshown in Fig. 1. Meanwhile, our results indicate that APR\nconsistently benefits from more diverse visual data, and we\nobserve clear signs of a generalizable APR emerging as the\nscale of data synthesis increases, and its localization per-\nformance cannot be explained merely by memorization. Our\ncontributions are summarized as follows:\n\u2022 We identify research gaps in studying APR model gen-\neralizability and demonstrate through comprehensive ex-\nperiments that large-scale data synthesis with effective\nlearning strategies makes APR more generalizable.\n\u2022 We leverage 3D Gaussian Splats as our data engine to\nefficiently synthesize novel views with new appearances.\n\u2022 We propose a robust two-branch joint training pipeline\nwith an adversarial discriminator to reduce the syn-to-real\ngap and fully unleash the power of synthetic data.\n\u2022 Our method sets a new state-of-the-art in visual localiza-\ntion on real-world indoor, outdoor, and driving datasets."}, {"title": "2. Related Works", "content": "Visual Localization. Visual localization aims to estimate a\ncamera's translation and rotation within a 3D scene. Tra-\nditional geometry-based methods [7, 15, 31, 33, 42, 44-46, 55] accomplish this by using point clouds and a refer-\nence image database, relying on stored descriptor and image\nretrieval to establish 2D-3D correspondences. In contrast,\nscene coordinate regression (SCR) methods [3-5, 59] em-\nbed map information within neural networks to directly pre-\ndict 2D-3D correspondences. Both approaches generally re-\nquire PnP [17] and RANSAC [16] to output camera poses at\ntest time, which adds additional computation cost. Alterna-\ntively, absolute pose regression (APR) [6, 8, 20, 22, 38, 50]\naims to directly regress the camera pose from a query im-\nage using neural networks. Although the performance is\nsuboptimal compared with geometry-based methods, APR\nremains a promising approach due to its fast inference.\nData Augmentation for Pose Regression. End-to-end\npose regression methods rely heavily on the amount and\ndiversity of training data. Previous work [47] shows that\nAPR implicitly learns image memorization in the given en-\nvironment, actually overfitting to the training set. There-\nfore, the following works LENS [39], DFNet [9] and PM-\nNet [30] enhance APR performance by enriching training\nviews with NeRF. However, these approaches fail to ad-\ndress the generalizability of APR models and exhibit sev-\neral limitations: (1) The efficiency of training and novel\nview synthesis (NVS) in NeRF is severely restricted, hin-\ndering scalability. (2) They limit NVS to geometric (pose)\ntransformations while neglecting photometric (appearance)\nvariations, thereby decreasing APR robustness to changes\nin visual appearance. (3) The augmented data is underuti-\nlized in their learning frameworks, leaving its potential for\nimproving APR largely untapped. Differently, our frame-\nwork switches to 3DGS [23] as the scene representation\nto efficiently generate novel posed images with arbitrarily\nblended appearances and introduce an adversarial mecha-\nnism to unleash the power of synthetic data.\nHandling and Synthesizing Challenging Scenarios. Vi-\nsual localization often encounters unstructured photo col-\nlections [54], where visual appearance varies due to mov-"}, {"title": "3. Method", "content": "3.1. Pre-Processing with 3DGS\nA robust pose regressor should focus on intrinsic scene\nattributes, not appearance variations. Therefore, we first\nsynthesize diverse visual data for training. We leverage\n3DGS [23], representing scenes with explicit ellipsoids, to\nmodel diverse appearances (see Sec. B for more on 3DGS).\nFollowing GS-W [65], we assume the scene contains K\nGaussians and represent the independent intrinsic material\nattributes using positions $\\mu \\in \\mathbb{R}^{K \\times 3}$, spherical harmonics\n$Y \\in \\mathbb{R}^{K \\times 16 \\times 3}$, and other parameters \u2013 including rotation\n$q \\in \\mathbb{R}^{K \\times 4}$, scaling $s \\in \\mathbb{R}^{K \\times 3}$, and opacity $a \\in \\mathbb{R}^{K}$. To\ncapture the dynamic appearance influenced by environmen-\ntal factors, we extract features from the input image and\nassign each Gaussian its own feature using a learnable sam-\npler $S \\in \\mathbb{R}^{K \\times 2}$, forming features $E \\in \\mathbb{R}^{K \\times 16 \\times 3}$. We\nalso incorporate the camera's view direction @ to account\nfor viewpoint-dependent effects. The final color of Gaus-"}, {"title": "3.2. Architecture of Pose Regressor", "content": "Given a set of images and their associated camera poses\n${(I_i, P_i)}_1^n$, our goal is to train a neural network to di-\nrectly output a homogeneous camera pose $P \\in \\mathbb{R}^{3 \\times 4}$ for a\nquery image $I \\in \\mathbb{R}^{H \\times W \\times C'}$. Our network architecture is\nshown as the pose regressor in Fig. 2.\nFeature Extractor. Pose regression networks typically\nextract features using a common backbone 4, such as\nVGG [53] or EfficientNet [56], leveraging multiple deeper\nlayers for translation and rotation regression:\n$\\varphi(I) = \\{F_0(I), ..., F_{N-1}(I), F_N(I)\\},$ (3)"}, {"title": "3.3. Two-Branch Joint Training Paradigm", "content": "3.3.1 Branch-1: Aligning Features via Discriminator\nSynthetic images from 3DGS provide novel viewpoints and\nappearances but often contain artifacts, leading to a syn-to-\nreal domain gap. To align features from rendered and real\nimages of the same pose, we introduce an adversarial train-\ning mechanism besides the basic pose regression training.\nPose Regression Loss. For basic training, we render the\nsynthetic image I' with the same pose label P as the real\nimage I, both used as supervision for the pose regressor.\nThe training objective consists of translation loss $L_t$ and\nthe rotation loss $L_r$, which are measured by the Euclidean\ndistance between the ground truth pose $P = \\{t, r\\}$ and the\nestimated pose $\\hat{P} = \\{\\hat{t}, \\hat{r}\\}$: *We only present the translation regression for simplicity."}, {"title": "4. Experiments", "content": "4.1. Evaluation Setup\nDatasets. For evaluation, we follow previous works [9, 30]\nto use four scenes in the Cambridge Landmarks dataset [22]\nwith spatial extents from 875 m\u00b2 to 5600 m\u00b2. We also em-\nploy the 7-Scenes dataset [52], which provides seven indoor\nscenes with volumes spanning 1 m\u00b3-18 m\u00b3, and follow the\noriginal training and testing splits with more accurate SfM\npose annotations [5, 10]. Moreover, we evaluate our method\non MARS [26], a self-driving dataset featuring challenges\nlike moving objects, lighting changes, and motion blur.\nBaselines. We first compare our proposed RAP against\ncommon single-frame APR approaches on the three\ndatasets, where PMNet [30] and DFNet [9] are the most\nrelated and advanced methods based on data augmenta-\ntion. We split the remaining methods into two categories\nbased on whether they rely on extra novel view synthesis\nin test time, including SCR [4, 5, 59] and NRP (Neural /\nRendering-based Post refinement) [10, 18, 32, 40, 57, 66,\n67], which involves rendering or querying features in novel\nviews by the initial pose for the following one-shot or iter-\native refinement. MCLoc [57] is a pose refinement method\nusing 3DGS, while others use NeRF.\nImplementation Details. We first optimize our 3DGS\nfor each scene without masking moving objects. We\nthen train our RAP network, which uses an Efficient-BO\nbackbone [34] pre-trained on ImageNet [13], optimized\nwith Adam [24] at a learning rate of 1 \u00d7 10-4. Only\nthe features from the third (reduction_3) and fourth\n(reduction_4) layers are used respectively for transla-\ntion and rotation regression, and both layers are utilized for\nnarrowing the domain gap via the discriminator, which is\nalso optimized with Adam [24], using a learning rate of"}, {"title": "4.2. Benchmark Results", "content": "7-Scenes [52]. As shown in Table 1, our RAP achieves su-\nperior performance, with a 50% reduction in translation er-\nror (0.10 \u2192 0.05) and a 21.91% reduction in rotation error\n(3.24\u00b0 2.51\u00b0) averagely compared with previous state-\nof-the-art single-frame APR methods. Only in heads the\nrotation error is suboptimal, which consists of only two se-\nquences, one for training and the other for testing. This re-\nstriction may limit the ability of our augmentation method\nto fully capture the scene's variability. Meanwhile, RAPref\nachieves a substantial improvement, significantly reducing\npose estimation errors with one-shot refinement using our\n3DGS. Notably, our RAPref is the first to achieve an aver-\nage translation error below 1 cm and a rotation error below\n0.25\u00b0. Qualitative examples are displayed in Fig. 3.\nCambridge Landmarks [22]. In the more challenging out-\ndoor Cambridge Landmarks dataset, as shown in Table 2,\nour RAP demonstrates a significant performance advan-\ntage across all scenes, achieving over 30% improvements in\nboth translation and rotation error compared to other single-\nframe APR methods. The visualization in Fig. 4 shows that\nour method produces fewer outliers than DFNet. In the two\nlarger-scale scenes, i.e., College and Church, the improve-\nment in rotation error even doubled. Table 2 also highlights\nthe effectiveness of our RAPref in further reducing pose\nerrors through the refinement phase. Specifically, RAPref\nwith one-shot optimization significantly outperforms Cross-\nFire [40] and DFNet + NeFeS [10], which require 30 and"}, {"title": "4.3. Ablation Study", "content": "We conduct ablation studies on the validation set of Shop in\nthe Cambridge Landmarks dataset to investigate the impact\nof all the components in our RAP. Setup I, our baseline,\nconsists of the same components as in PoseNet [22] and\nhas been retrained for our experiments. In Setup II, we re-\nplace the feature extraction from VGG16 [53] to Efficient-\nBO [34], which enhances performance due to its superior\nfeature representation, while they both exhibit poor per-\nformances due to the lack of data synthesis. In Setup III\nand IV, we explore the effectiveness of the designed pose\naugmentation and appearance augmentation, which bring\nnotable improvements: translation error reduces from 103\ncm to 75 cm, and rotation error from 3.52\u00b0 to 3.14\u00b0. In\nSetup V and VI, we add regular convolutional layers and\nPose Transformer between feature extraction and pose re-\ngression. Both improve performance due to the increasing\nparameters, but the Transformer achieves superior results by\neffectively handling long-term dependencies through atten-"}, {"title": "4.4. Discussion on Data Synthesis", "content": "Emerging Interpolation and Extrapolation Capability.\nPreviously, APR has been understood to implicitly learn im-\nage retrieval [47], lacking the ability to successfully interpo-\nlate between training samples and generalize beyond them.\nTo investigate how APR training is affected by increasing\nsynthetic data, we experimented on Shop using only 20%\nof the real training set with our synthetic data, as shown\nin Fig. 6. We can observe that the training set with syn-\nthetic data, represented by the red hollow spheres, does not\nfully cover the test set spatially. Despite this, the model\nstill closely predicts the test camera poses, demonstrating\ninterpolation capability between training positions and even\nextrapolation capability beyond the original spheres.\nAnalyzing Generalization Boundaries. To evaluate the\nmodel's generalization capability, we designed an experi-\nment introducing a \u201cvoid zone\" centered on the test camera,\nwhere all real and synthetic data within this zone were ex-\ncluded. The void zone was progressively expanded to deter-\nmine the critical threshold at which the localization perfor-\nmance declines most significantly. Specifically, for Shop,\nwe used 100% of the training set to ensure complete scene\ncoverage, with void zone ranges set as [10/0.5, 20/1, 30/1.5,\n50/2, 80/2.5, 100/3] (cm/\u00b0). The results in Table 5 demon-\nstrate a stepwise decline in performance. Initially, expand-\ning the void zone has minimal impact on localization accu-\nracy. However, at 30 cm / 1.5\u00b0, a sharp decrease in perfor-\nmance marks the model's generalization boundary."}, {"title": "5. Conclusion", "content": "Summary. We address absolute pose regression with a ro-\nbust two-branch joint training framework based on Trans-\nformer, coupled with an efficient data synthesis pipeline\nleveraging 3D Gaussian Splats (3DGS) to synthesize nu-\nmerous posed images with diverse appearances as addi-\ntional supervision. Our RAP achieves state-of-the-art lo-\ncalization performance, even under challenging appearance\nvariations. Moreover, we thoroughly investigate the impact\nof scaling synthetic data and present a novel perspective\non APR: interpolation and extrapolation capabilities can\nemerge if the data and learning gaps in APR are effectively\naddressed. We believe our RAP could be a promising start-\ning point, and the experiments presented in the paper can\nprovide useful insights for future research in this field.\nLimitations and Future Works. Similar to other APR ap-\nproaches, our method does not yet achieve better accuracy\nthan geometry-based techniques, and our per-scene train-\ning is relatively time-consuming. Future work includes effi-\nciently training stronger APR models with geometric pri-\nors, leveraging temporal information, or powerful vision\nfoundation models [43]. Furthermore, achieving general-\nization in dynamic environments with fewer training sam-\nples presents a promising research direction."}, {"title": "Appendix", "content": "A. Pipeline Workflow\n\u2022 Stage 1: Appearance-Varying 3DGS (Sec. 3.1)\n* Input: Sequences of RGB images I and corresponding\ncamera poses P.\n* Output: 3D appearance-varying Gaussians with de-\nblurring capability.\n* Loss: L1, LD-SSIM, LLPIPS, Ls, and optionally, Ldepth.\n\u2022 Stage 2: Two-Branch Joint APR Training (Sec. 3.2)\nBranch-1\n* Input: Real image-pose pair (I, P) and the cor-\nresponding synthesized image with the same pose\n(I', P').\n* Output: Adjusted translation features (Adj(Ft(I),\nAdj' (Ft(I'))), adjusted rotation features\n(Adj(Fr(I)), Adj' (Fr(I'))), and predicted poses\n(P,\\hat{P})\n* Loss: Pose loss Lpose, generator loss LGen, and ad-\nversarial loss LDis.\nBranch-2\n* Input: Synthesized images with randomly blended\nappearances and perturbed poses (Isyn, Psyn).\n* Output: Predicted poses $\\hat{P}_{syn}$.\n* Loss: Pose loss $L_{pose}$\n\u2022 Stage-3: Post-Refinement (Sec. 4.1)\n* Input: Rendered image from 3DGS using the initial\npose predicted by the trained pose regressor above and\nthe query image.\n* Output: Final refined pose.\n* Loss: RANSAC-PnP [16, 17] solver on pixel-level\nmatching between the rendered and query images.\nB. 3D Gaussian Splatting Preliminary\nGaussian Splatting [23] is a promising real-time novel view\nsynthesis approach. By representing scenes with a set of 3D\nGaussians, this method preserves the differentiable proper-\nties of volumetric radiance fields while allowing efficient\noptimization and rendering. The scene is defined through\nparameters such as position $\\mu \\in \\mathbb{R}^{K \\times 3}$, covariance de-\ncomposed as rotation $q \\in \\mathbb{R}^{K \\times 4}$ and scaling $s \\in \\mathbb{R}^{K \\times 3}$,\nanisotropic color $c \\in \\mathbb{R}^{K \\times 3}$ modeled by sphere harmonics\n$Y \\in \\mathbb{R}^{K \\times 16 \\times 3}$, and opacity $a \\in \\mathbb{R}^{K}$. During optimiza-\ntion, the scene representation is optimized by iteratively ad-\njusting parameters through stochastic gradient descent, en-\nabled by a differentiable rasterizer. This process is com-\nbined with adaptive density control to dynamically adds or\nremoves Gaussians based on the gradient of screen-space\npoints corresponding to the Gaussians and opacity reset to\nreduce overfitting caused by floaters. The rendering pro-\ncess involves projecting 3D Gaussians onto the 2D image\nplane, sorting them by depth, and then applying a-blending\nto generate the final image. The render equation is:\n$C = \\sum_{i=1}^{N} T_i a_i c_i, T_i = \\prod_{j=1}^{i-1} (1 - \\alpha_j),$ (I)\nwhere $C \\in \\mathbb{R}^3$ is each pixel's color and $T_i$ is the transmit-\ntance. This approach significantly speeds up optimizing and\nrendering while achieving state-of-the-art visual quality.\nC. MASt3R Preliminary\nThis section further elaborates on the background knowl-\nedge of MASt3R [25] mentioned in Sec. 4.1. MASt3R\ngrounds image matching tasks in 3D space to improve ro-\nbustness and accuracy in challenging scenarios. Building on\nthe DUSt3R [60] framework, MASt3R incorporates a new\nfeature-matching head and a fast reciprocal matching algo-\nrithm, significantly enhancing performance for dense cor-\nrespondences and camera pose estimation. It addresses the\nlimitations of traditional 2D-based methods by leveraging\ndense 3D pointmaps and a coarse-to-fine matching strategy.\nExtensive evaluations demonstrate substantial gains in ac-\ncuracy, computational efficiency, and generalizability, mak-\ning MASt3R a robust solution for visual localization tasks.\nD. Architecture Details\nThis section provides additional details regarding the net-\nwork structure mentioned in Sec 3.2 of the main text.\nD.1. Feature Extraction\nOur RAP pipeline first downsamples the input real images\nby a scale factor of 2 to enhance computational efficiency.\nThe downsampled images are then normalized and passed\nthrough the backbone network. For feature extraction, we\nutilize EfficientNet-B0 [34] as the backbone for multi-scale\nfeature extraction. Translation feature Ft and rotation fea-\nture Fr are extracted from the third ('reduction_3') and\nfourth ('reduction_4') layers, with the number of feature\nchannels being $C_t = 40$ and $C_r = 112$, which then are pro-\njected via 1 \u00d7 1 convolutions to align with the input channel\ndimension D = 128 of the proposed Pose Transformer.\nD.2. Pose Transformer\nRelying on fine-grained local features, as done in previ-\nous works [9, 30], can hinder invariant feature learning\ndue to image noise caused by dynamic objects and illu-\nmination changes. To overcome this, we leverage Trans-\nformer's robust ability to capture long-range dependencies,\nas illustrated in Fig. II. Taking the Cambridge Landmarks\ndataset [22] as an example, the original image resolution\nis 854 \u00d7 480. After downsampling and feature extraction,\nthe resulting translation feature map (identical for the ro-\ntation feature map) has a shape of [B, 112, Ht, Wt], where"}]}