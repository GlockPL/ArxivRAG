{"title": "Bi-Factorial Preference Optimization:\nBalancing Safety-Helpfulness in Language Models", "authors": ["Wenxuan Zhang", "Philip H.S. Torr", "Mohamed Elhoseiny", "Adel Bibi"], "abstract": "Fine-tuning large language models (LLMs) on human preferences, typically\nthrough reinforcement learning from human feedback (RLHF), has proven success-\nful in enhancing their capabilities. However, ensuring the safety of LLMs during\nthe fine-tuning remains a critical concern, and mitigating the potential conflicts in\nsafety and helpfulness is costly in RLHF. To address this issue, we propose a su-\npervised learning framework called Bi-Factorial Preference Optimization (BFPO),\nwhich re-parameterizes a joint RLHF objective of both safety and helpfulness into\na single supervised learning objective. In the supervised optimization, a labeling\nfunction is used to capture global preferences ranking to balance both safety and\nhelpfulness. To evaluate BFPO, we develop a benchmark including comprehensive\ndiscriminative and generative tasks for helpfulness and harmlessness. The results\nindicate that our method significantly outperforms existing approaches in both\nsafety and helpfulness. Moreover, BFPO eliminates the need for human prompting\nand annotation in LLM fine-tuning while achieving the same level of safety as\nmethods that heavily rely on human labor, with less than 10% of the computational\nresources. The training recipes and models will be released.", "sections": [{"title": "Introduction", "content": "Fine-tuning the large language models (LLMs) on human preferences, also known as model alignment,\nhas been shown to greatly enhance their text generation abilities [1, 2, 3]. When prompted with well-\nformatted instructions, these models can generate helpful responses for various tasks like scientific\nquestions answering [4], creative writing [5], coding [6, 7], and planning [8, 9]. However, with this\ninstruction following ability, the models can also be prompted to generate harmful content, such as\nsexist or racist comments, suggestions for committing crimes, or sensitive medical advice [10, 11, 12].\nTherefore, it is crucial to ensure that LLMs are both helpful and safe [1, 2, 13].\nThe challenge of improving both safety and helpfulness of LLMs arises from the inherent tension\nbetween these two objectives [13, 14, 15]. A perfectly safe model may refuse to answer even non-\nharmful questions (Figure 1-left, b), while a highly helpful model (Figure 1-right, a) may compromise\nsafety. Additionally, training a model on a mix of data annotated with different preference objectives\ncan lead to issues, as shown with model (c) in Figure 1, where the model refuses to generate code\n(left) yet still produces harmful content (right).\nTo resolve this problem, recent studies propose to train separate reward models tailored to each\nobjective, safety and helpfulness, and optimize LLMs via multi-objective RLHF, which aggregates"}, {"title": "Preliminary", "content": "Notation and Terminology. Let x and y denote the input prompts their corresponding responses,\nrespectively. For any two responses, y, y' generated from a prompt x, a binary preference label\nI(y > y'|x) is provided by a human annotator to whether y is preferred over y'. The preferred\nresponse is termed the \u201cwin response\", denoted as y\u2122, and the other as the \u201close response\u201d, y\u00b9. A\ndataset D = {(x,y,y', I(y > y'|x))} that contains prompts, multiple responses, and the human\npreferences over the responses is referred to as a preference dataset.\nFollowing prior art [19], we define the ground-truth preference p* between two responses y,Y'\nas the expected preference label across a broad group of human annotators, i.e., p*(y > y'|x) ="}, {"title": "BFPO Framework: Bi-Factorial Preference Optimization", "content": "In this section, we aim to extend the supervised learning framework in Equation (6) to improve\nboth safety and helpfulness in LLM alignment. Naively, we could combine the helpfulness and"}, {"title": "Empirical Labeling function", "content": "In previous single-reward optimization methods [18, 19, 20], $g_1(y^w, y^l|x)$ in Equation (6) is typically\na positive constant. However, in our case, $g_1(y^{hw}, y^{hl}|x)$, which approximates the global reward\ndisparity between the more helpful response and the less helpful response, i.e., g(yhw|x) \u2013 g(yhl|x),\nshould vary depending on the safety of yhw and yhl. For example, in Figure 2, response (a) is more\nhelpful than response (b), and the global reward disparity between (a) and (b) should be positive since\nboth are safe. However, the global reward disparity between the more helpful (c) and less helpful\n(b) should be negative, because (c) is less preferred for its detailed harmful information. In fact,\nthe absolute value of g(yhw|x) \u2013 g(yhl|x) reflects the magnitude of the global preference disparity\nbetween the two responses, while its sign determines whether yhw is globally preferred over yhl.\nTo assign label values across various yhw, yhl pairs, we first globally rank the responses as illustrated\nin Figure 2. Our guiding principle is a general preference for safe responses, prioritizing helpfulness\nonly if the responses is safe. We desire the helpful and safe responses like (a) in Figure 2, followed"}, {"title": "Theoretically Equivalent Reward", "content": "In this section, we show that the supervised optimization problem in Equation (8), with specific label-\ning function in Equation (9), is theoretically equivalent to the multi-objective RLHF in Equation (7)\nwith a particular reward function. Previous studies [14, 16] in aligning LLMs for both safety and\nhelpfulness have shown that the global reward function can be effectively approximated by a bilinear\ncombination of the two sub-rewards; see Appendix C.2 for more details. We hypothesize the global\nreward function as follows:\n$g(y) = (p_{safe}(y|x) + A_1)(p_{help}(y > \\pi|x) + A_2)$,\nwhere A1, A2 are two constants that prevent the reward from being nullified by zero values, and\n$p_{help}, p_{safe} \\in [0, 1]$ are the ground-truth helpful and safety preferences of response y. Let A1 =\n$E_s$, A2 = $\\frac{3}{2}$, B1 = 3, B2 = 0, B3 = $\\frac{1}{2}$, we have the reward function g and labeling function g\u0131 as\nfollows:\n$g(y) = (p_{safe}(y|x) + E_s)(p_{help}(y > \\pi|x) + \\frac{1}{2})$,\n$g_1(y^{hw}, y^{hl}) = I_{safe} (y^{hw}) - I_{safe} (y^{hl})$,\nwhere $E_s = E_{y \\sim \\pi}[p_{safe}(y|x)]$ represent the ground truth average safety of responses given prompt x.\nThe following theorems reveal the theoretical equivalence."}, {"title": "Algorithm", "content": "Algorithm 1 shows the BFPO algorithm. As mentioned in Section 2, in practice, we refer to datasets\nrelated to safety topics, collected through red teaming, as safety datasets. A typical safety dataset will\ncontain a safety label Isafe(y), which is the binary label indicating whether the response y is harmful,\nas well as the preference label Ihelp(y > y') in terms of helpfulness. If a certain safety dataset does\nnot provide helpfulness labels, we can use the ranking models, like PairRM [55], as discussed in\nSection 4.3, to generate the pairwise helpfulness labels. We refer to datasets designed to improve\nthe helpfulness of the model as helpfulness datasets. A typical helpfulness dataset will contain the\nhelpfulness preference labels Ihelp(y > y'). Since most helpfulness data undergoes human filtering,\nthe responses are usually safe. Therefore, we assign the safety label Isafe (y) = 1 to all responses in\nthe helpfulness dataset.\nWe further require a pre-trained language model \u03c0ref, the total number of optimization steps T, the\npenalty coefficient \u03c4 for the KL divergence term, and the shifting parameter \u03b1. We also need to\nspecify the layers to be unfrozen for the policy optimization, denoted as \u03b8'.\nAt the beginning of the algorithm, we initialize the policy \u03c0\u03bf with the pre-trained language model\n\u03c0ref, and unfreeze the selected layers \u03b8' (line 1-2). In each gradient step, we first sample a batch from\nthe safety dataset D, and a batch from the helpful dataset Dh (line 4) of the same size. We then\ncompute the loss of both batches according to Equation (15) (line 6-8). We accumulate the gradients\nof the loss for the both batches and update the policy \u03c0\u03b8 (line 10). This process is repeated until the\ntotal number of optimization steps T is reached."}, {"title": "Notation", "content": ""}, {"title": "Discussion of the Property of gr", "content": "In this section, we discuss the two beneficial properties of gr that we proposed in Section 3.2.\nSkew-Symmetric Property. First, we examine the skew-symmetric property of gr. When combined\nwith the skew-symmetric property of h, this implies:\n$(h_{\\pi}(y, y') - \\tau^{-1}g_1(y, y'))^2 = (h_{\\pi}(y', y) - \\tau^{-1}g_1(y',y))^2$.\nThis means that for the same data point, regardless of the order of y and y', we are always driving\nh(y, y') to the same value. In contrast, in IPO [19], different orders will push h(y, y') to different\nvalues, i.e., they form two different optimization problems:\n$(h_{\\pi}(y, y') - \\tau^{-1}g_1(y, y'))^2$ and $(h_{\\pi}(y',y))^2$.\nTheir final optimization problem, $(h_{\\pi}(y, y') - \\frac{1}{2}\\tau^{-1}g_1(y, y'))^2$, tries to find a middle point of h that\noptimizes both. However, this point is neither the optimal solution of the first problem nor the second\nproblem.\nShifting Property. Second, we discuss the shifting properties of gr. Since Theorem 3.2 holds\nbased on the equality of Equation (30) and Equation (27), and all the operations to derive these two\nequations are valid under linear transformations of psafe, Phelp and Isafe, Ihelp, respectively. It implies\nthat Theorem 3.2 also holds under the same linear transformations of psafe, Phelp and Isafe, Ihelp.\nThis property is useful when we want to manually design the values of gr, as shown in Figure 3."}, {"title": "Experiment", "content": ""}, {"title": "Details of Harmlessness Benchmark", "content": "The following are the details of the datasets used in the benchmark:\n\u2022 CrowS-Pairs: A challenge set for evaluating the tendency of language models (LMS)\nto generate biased outputs. We evaluate the English subset and implementation by LM\nEvaluation Harness and report the Percentage of Stereotype metric.\n\u2022 Bias Benchmark for QA (BBQ): Measures social biases in the output of multiple-choice\nquestion-answering models. We use the Lite version and implementation by BigBench and\nreport the Accuracy metric.\n\u2022 WinoGrande: A collection of 44K problems inspired by the Winograd Schema Chal-\nlenge, adjusted to improve scale and robustness against dataset-specific biases. We use the\nimplementation by LM Evaluation Harness and report the Accuracy metric.\n\u2022 ETHICS: A benchmark spanning concepts in justice, well-being, duties, virtues, and\ncommonsense morality. We use the implementation by LM Evaluation Harness and report\nthe Accuracy for all sub-tasks.\n\u2022 BigBench Moral Permissibility: Tests whether ultra-large language models can read a short\nstory where a moral scenario is presented and answer the question, \"Is it morally permissible\nto do X?\" in a manner similar to humans. We use the implementation by BigBench and\nreport the Accuracy metric.\n\u2022 BigBench Simple Ethics Questions: Assesses a language model's responses to hypothetical,\nconsequential, political, and social questions. We use the implementation by BigBench and\nreport the Accuracy metric.\n\u2022 ToxicGen: A benchmark for evaluating the ability of language models to classify input text\nas either hateful or not hateful. We use the implementation by LM Evaluation Harness and\nreport the Accuracy metric.\n\u2022 BigBench HHH Alignment: Evaluates language models on alignment, pragmatically\nbroken down into the categories of helpfulness, honesty/accuracy, harmlessness, and other\\aspects. We use the implementation by BigBench and report the Accuracy metric.\n\u2022 AdvBench contains harmful prompts. We use the prompts provided here and generation\nimplementation by LM Evaluation Harness. We report the percentage of harmless responses\nmeasured by HarmBench-Llama-2-13b-cls model."}, {"title": "Details of Baselines", "content": "The following are the details of the methods that align LLMs for multiple objectives.\n\u2022 Llama2 [14] trains the safety reward $r'_{safe}$ and the helpfulness reward $r_{help}$ separately, and\ndefines the global reward g as a combination of these rewards, i.e.,\n$\\tilde{g}(y/x) = \\begin{cases} r'_{safe}(y|x) & \\text{if IS\\_SAFETY}(x), \\text{ or } r'_{safe}(y|x) < 0.15, \\\\ r_{help}(y|x) & \\text{otherwise}, \\end{cases}$,\n$g(y|x) = \\text{WHITEN}(\\text{LOGIT}(\\tilde{g}(y|x)))$.\nHere IS_SAFETY(x) indicates whether prompts are tagged as unsafe in their dataset, and\nthe 0.15 threshold is chosen to filter unsafe responses according to the evaluation on Meta\nSafety test set. Whitening the final linear scores is to increase stability. The global reward is\nused in the RLHF objective in Equation (3).\n\u2022 Beaver [16] trains the safety reward $r_{safe}$ and the helpfulness reward $r_{help}$ separately, and\ndefines the final RLHF objective as the dual optimization problem of the conditional RLHF,\nobtained by Lagrangian dual transformation, i.e.,\n$\\min_{\\theta} \\max_{\\lambda \\geq 0} \\mathbb{E}_{x \\sim D, y \\sim \\pi_{\\theta}} [-r_{help}(y|x) + \\lambda (r_{safe}(y|x) + d)]$,\nwhere \u03bb \u2265 0 is the Lagrange multiplier. In practice, the model parameter \u03b8 and the Lagrange\nmultiplier \u03bb are updated iteratively.\n\u2022 RBR [17] requires separate reward models, $r_{\\phi_1}, ..., r_{\\phi_k}$, for each objective, and propose to\nlearn the weight for each objective, i.e.,\n$g(y|x) = \\sum_{i=1}^{k} \\lambda_i r_{\\phi_i}(y|x)$,\nwhere \u03bb\u1d62 are learnable parameters. The global reward is used in the RLHF objective in\nEquation (3).\n\u2022 SteerLM [52] trains models to generate response according to a specific reward vector\nr = (r\u2081, r\u2082, r\u2083, ..., rk). They first train a model to predict the score for each objective in\na dataset. Supervised fine-tuning is performed to maximize the probability of generating\nresponses conditioned on the reward vector and the prompt, i.e.,\n$\\max_{\\theta} \\mathbb{E}_{(x,y,r) \\sim D} \\log p_{\\theta}(y|x, r)$.\n\u2022 MORL [51] trains reward models for each objective separately, and defines the global\nreward g as a combination of rewards, i.e.,\n$g(y|x) = \\sum_{i=1}^{k} \\alpha_i r_i(y|x)$,\nThe global reward is used in the RLHF objective in Equation (3)."}]}