{"title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models", "authors": ["Wenxuan Zhang", "Philip H.S. Torr", "Mohamed Elhoseiny", "Adel Bibi"], "abstract": "Fine-tuning large language models (LLMs) on human preferences, typically through reinforcement learning from human feedback (RLHF), has proven successful in enhancing their capabilities. However, ensuring the safety of LLMs during the fine-tuning remains a critical concern, and mitigating the potential conflicts in safety and helpfulness is costly in RLHF. To address this issue, we propose a supervised learning framework called Bi-Factorial Preference Optimization (BFPO), which re-parameterizes a joint RLHF objective of both safety and helpfulness into a single supervised learning objective. In the supervised optimization, a labeling function is used to capture global preferences ranking to balance both safety and helpfulness. To evaluate BFPO, we develop a benchmark including comprehensive discriminative and generative tasks for helpfulness and harmlessness. The results indicate that our method significantly outperforms existing approaches in both safety and helpfulness. Moreover, BFPO eliminates the need for human prompting and annotation in LLM fine-tuning while achieving the same level of safety as methods that heavily rely on human labor, with less than 10% of the computational resources. The training recipes and models will be released.", "sections": [{"title": "1 Introduction", "content": "Fine-tuning the large language models (LLMs) on human preferences, also known as model alignment, has been shown to greatly enhance their text generation abilities [1, 2, 3]. When prompted with well-formatted instructions, these models can generate helpful responses for various tasks like scientific questions answering [4], creative writing [5], coding [6, 7], and planning [8, 9]. However, with this instruction following ability, the models can also be prompted to generate harmful content, such as sexist or racist comments, suggestions for committing crimes, or sensitive medical advice [10, 11, 12]. Therefore, it is crucial to ensure that LLMs are both helpful and safe [1, 2, 13].\nThe challenge of improving both safety and helpfulness of LLMs arises from the inherent tension between these two objectives [13, 14, 15]. A perfectly safe model may refuse to answer even non-harmful questions (Figure 1-left, b), while a highly helpful model (Figure 1-right, a) may compromise safety. Additionally, training a model on a mix of data annotated with different preference objectives can lead to issues, as shown with model (c) in Figure 1, where the model refuses to generate code (left) yet still produces harmful content (right).\nTo resolve this problem, recent studies propose to train separate reward models tailored to each objective, safety and helpfulness, and optimize LLMs via multi-objective RLHF, which aggregates"}, {"title": "2 Preliminary", "content": "Notation and Terminology. Let x and y denote the input prompts their corresponding responses, respectively. For any two responses, y, y' generated from a prompt x, a binary preference label I(y > y'|x) is provided by a human annotator to whether y is preferred over y'. The preferred response is termed the \u201cwin response\", denoted as yw, and the other as the \u201close response\u201d, y\u00b9. A dataset D = {(x,y,y', I(y > y'|x))} that contains prompts, multiple responses, and the human preferences over the responses is referred to as a preference dataset.\nFollowing prior art [19], we define the ground-truth preference p* between two responses y,Y' as the expected preference label across a broad group of human annotators, i.e., p*(y > y'|x) =\""}, {"title": "3 BFPO Framework: Bi-Factorial Preference Optimization", "content": "In this section, we aim to extend the supervised learning framework in Equation (6) to improve both safety and helpfulness in LLM alignment. Naively, we could combine the helpfulness and safety datasets, treating safer response in safety dataset and more helpful response in the helpfulness dataset as the win response yw in Equation (6). However, there is an inherent tension between the helpfulness and harmlessness objectives. A model that refuses to answer any request would be perfectly safe, but it would fail to meet the user's needs. Conversely, a highly responsive model that attempts to address all requests, including potentially harmful ones, may compromise safety in favor of helpfulness [28]. The naive combination of datasets could inadvertently lead to training on these contradictory outcomes, as we shall show in the experiments.\nOn the other hand, previous work [14, 16] has developed successful multi-objective RLHF methods to resolve this tension, with the objective\n$\\max_{\\pi_{\\theta}} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta}(y|x)} [g(y|x) - \\tau KL [\\pi_{\\theta}(y|x)||\\pi_{ref}(y|x)]],$ \nwhere g(x,y) = g(rhelp(x,y), rsafe(x,y)) is a function that combines the helpfulness reward rhelp(x, y) and safety reward rsafe(x, y). Therefore, re-parameterizing Equation (7) to a supervised objective leads to an efficient and effective alignment method. The target objective is:\n$\\min_{\\theta} \\mathbb{E}_{(x,y^{hw}, y^{hl})\\sim \\mathcal{D}} [\\mathcal{L}(h_{\\pi_{\\theta}} (y^{hw}, y^{hl}), \\tau^{-1}g_{1}(y^{hw}, y^{hl}|x))],$ \nwhere yhw and yhl are the more helpful and less helpful responses. Similarly to Equation (6), g1 is the label function that leverages the safety labels Isafe(yhw), Isafe(yhl) to approximate the value g(yhw|x) - g(yhl|x)), where g is the global reward function in Equation (7).\nIn Section 3.1, we first develop an empirical labeling function g1 that accurately represents the global reward of responses based on both helpfulness and harmlessness. We then establish the theoretical equivalence between Equation (8) with this g1 and Equation (7) in Section 3.2. Next, we present the algorithm in Section 3.3 and provide a sample illustration in Section 3.4."}, {"title": "3.1 Empirical Labeling function", "content": "In previous single-reward optimization methods [18, 19, 20], g1(yw, y\u00b9|x) in Equation (6) is typically a positive constant. However, in our case, g1(yhw, yhl|x), which approximates the global reward disparity between the more helpful response and the less helpful response, i.e., g(yhw|x) \u2013 g(yhl|x), should vary depending on the safety of yhw and yhl. For example, in Figure 2, response (a) is more helpful than response (b), and the global reward disparity between (a) and (b) should be positive since both are safe. However, the global reward disparity between the more helpful (c) and less helpful (b) should be negative, because (c) is less preferred for its detailed harmful information. In fact, the absolute value of g(yhw|x) \u2013 g(yhl|x) reflects the magnitude of the global preference disparity between the two responses, while its sign determines whether yhw is globally preferred over yhl.\nTo assign label values across various yhw, yhl pairs, we first globally rank the responses as illustrated in Figure 2. Our guiding principle is a general preference for safe responses, prioritizing helpfulness only if the responses is safe. We desire the helpful and safe responses like (a) in Figure 2, followed"}, {"title": "3.2 Theoretically Equivalent Reward", "content": "In this section, we show that the supervised optimization problem in Equation (8), with specific labeling function in Equation (9), is theoretically equivalent to the multi-objective RLHF in Equation (7) with a particular reward function. Previous studies [14, 16] in aligning LLMs for both safety and helpfulness have shown that the global reward function can be effectively approximated by a bilinear combination of the two sub-rewards; see Appendix C.2 for more details. We hypothesize the global reward function as follows:\n$g(y) = (p_{safe} (y|x) + A_{1})(p_{help}(y > \\pi|x) + A_{2}),$ \nwhere A1, A2 are two constants that prevent the reward from being nullified by zero values, and phelp, psafe \u2208 [0,1] are the ground-truth helpful and safety preferences of response y. Let A1 =\nEs, A2 = 1/2, B1 = 3, B2 = 0, B3 = 1/2, we have the reward function g and labeling function g1 as follows:\n$g(y) = (p_{safe} (y|x) + E_{s})(p_{help}(y > \\pi|x) + \\frac{1}{2}),$ \n$g_{1}(y^{hw}, y^{hl}) = I_{safe} (y^{hw}) - I_{safe} (y^{hl}),$ \nwhere Es = $E_{y\\sim \\pi} [p_{safe} (y|x)]$ represent the ground truth average safety of responses given prompt x.\nThe following theorems reveal the theoretical equivalence."}, {"title": "3.3 Algorithm", "content": "With discussions in the previous sections, the loss function in the optimization problem in Equation (8) can be expressed as\n$\\mathcal{L}_{BFPO}(\\theta) = \\mathbb{E}_{(x,y^{hw},y^{hl})\\sim \\mathcal{D}} \\left[ \\log \\left( \\frac{\\pi_{\\theta} (y^{w}|x)\\pi_{ref}(y^{l}|x)}{\\pi_{\\theta} (y^{l}|x)\\pi_{ref}(y^{w}|x)} \\right) - \\frac{1}{\\tau} \\left( \\frac{3}{2}I_{safe} (y^{hw}) - I_{safe} (y^{hl}) - \\alpha  \\right) \\right]^{2}.$ \nIn practice, we directly use the above supervised loss to fine-tune the LLMs for both helpfulness and harmlessness. yw and y can be sampled from a public preference dataset D instead of being self-generated [18]. The safety labels Isafe (yhw), Isafe (yhl) are either provided in the dataset or obtained by a safety classifier. The probability \u03c0(y|x) of generating the response y given prompt x is obtained by forwarding the prompt and response through the LLM \u03c0\u03b8. \u03c0\u03b8 is the language model we are optimizing, and ref is a reference model that can be the model at the beginning of the optimization. We further employ two techniques to balance safety and helpfulness. First, we only unfreeze the selected layers \u03b8' in the policy \u03c0\u03b8 to avoid catastrophic forgetting, as described in [29]. Second, we implement buffered training in line 5 of Algorithm 1, where we sample batches of the same size from the safety dataset and the helpful dataset, inspired by [30]. The overall algorithm is summarized in Algorithm 1."}, {"title": "3.4 Illustrative Examples", "content": "Following [19], we conduct illustrative experiments on a synthetic dataset to demonstrate that our method can accurately recover the global preference using paired preferences. For simplicity, we consider a discrete action space with four actions, Y = {y1, y2, y3, y4}, without context. We define the safety labels and helpfulness ranking as\nSafety: Isafe (y1) = 1, Isafe (y2) = 0, Isafe (y3) = 1, Isafe (y4) = 0,\nHelpfulness: y1 > y2 > y3 > y4.\nConsequently, our proposed global preference, as in Figure 3, is y1 > y3 > y4 > y2. We consider the policy \u03c0\u03b8(yi) = softmax(\u03b8), where \u03b8 \u2208 R4 and i = 1, 2, 3, 4. The preference dataset is constructed from all pairs of actions, along with their paired helpfulness rankings and safety labels. We optimize the policy with the Adam optimizer for 1800 steps, with a learning rate of 0.01, batch size of 32 sampled with replacement, \u03c4 = 1, and \u03b1 = 0.5. We compare the supervised optimization objective proposed in Equation (15) as well as DPO [18] and IPO [19], where we take the more helpful response is taken as the win response. Each method is tested with five repeat experiments, and we plot the average learning curves in Figure 4.\nFor all \u03c4, we observe that only with our proposed method does \u03c0(yi), i.e., the probability of generating action yi, converges to the desired ranking, y1 > y3 > y4 > y2. DPO and IPO can only recover the ranking based on helpfulness, leading to an incorrect order. While IPO helps prevent the policy from being deterministic, our method retains this beneficial property while also achieving the correct ranking."}, {"title": "4 Experiment", "content": "4.1 Evaluation Setup\nHarmlessness Benchmark. To evaluate the harmlessness, we first construct a benchmark including both discriminative tasks and generative tasks based on previous benchmarks [31, 32, 33, 12]. The discriminative tasks measure the models' recognition of multiple safety topics, including\n\u2022 Bias: CrowS-Pairs [34], BBQ [35], WinoGrande [36].\n\u2022 Ethics: ETHICS [37], Moral Permissibility [31, 38, 39, 40], Simple Ethics Questions [37, 39].\n\u2022 Toxicity: ToxicGen [41], BigBench HHH Alignment [31]\nIn the generative tasks, we prompt the models to generate harmful content using the prompt dataset, AdvBench [12], Real Toxicity Prompts [42], ALERT [33], and adversarial harmful prompt dataset ALERT Adversarial [33]. We report percentage of harmless responses based on the safety classifier HarmBench-Llama2-13B-Chat [43]. Details of the benchmark can be found in Appendix C.1. We apply this benchmark to publicly available 7B-level models that have shown strong helpfulness scores in previous studies [32, 44], and present the performance in Figure 5 and in Appendix C.3.\nOverall Evaluation Metrics. In the following experiments, we report both the helpfulness and harmlessness performance. Helpfulness is measured using AlpacaEval 2.0 (Alpaca) [45, 46, 44]. Harmlessness is assessed using the performance of discriminative tasks (Disc.), generative tasks (Gen.) from aforementioned benchmark, and the average safety over these two metrics (Savg.)."}, {"title": "4.2 Alignment with BFPO Objective", "content": "From the evaluation on the open model in Figure 5, we observe that Zephyr-7b-beta [47], an open-sourced model fine-tuned over Mistral-7B-v0.1 [48] with DPO algorithm [18], exhibits a low score in harmlessness, particularly in generative tasks. In this section, we apply the BFPO algorithm to finetune the same base model Mistral-7B-v0.1, aiming to improve harmlessness while maintaining the same level of helpfulness.\nTraining Details. Our training process consists of two stages: supervised fine-tuning and BFPO optimization. The supervised fine-tuned model is used as the reference model \u03c0ref in the BFPO stage. We set \u03c4 = 0.01, \u03b1 = \u22120.5, and selected layer \u03b8' as the second MLP layers in each transformer block. All other hyperparameters remain the same as in the original Zephyr training [47].\nDataset Details. In the supervised fine-tuning stage, we follow previous work [47, 16] to use a mix of helpfulness data from UltraChat [49] and safety data from PKU-SafeRLHF [16]. In the BFPO stage, we use 30K helpfulness data from UltraFeedback [50] and 30K safety data from PKU-SafeRLHF. UltraFeedback contains instruction-following tasks that provide paired helpfulness preference rankings, and we treat all responses as safe since they undergo human filtering. PKU-SafeRLHF provides both paired helpfulness preference rankings and binary safety labels. Details are in Appendix C.3.\nBaselines. We first compare our method to the supervised method DPO [18] using different datasets., which directly leads to the Zephyr-7b-beta model, only uses the helpfulness dataset, UltraChat. DPO-S only uses the safety dataset, PKU-SafeRLHF. We also compare our method to existing approaches, DPO [18], IPO [19], and MORL [51], when using a naive mix of the helpfulness and safety datasets. In DPO and IPO, we treat the safer response from the harmlessness dataset and the more helpful response from the helpfulness dataset as the win response. MORL, representing the line of multi-objective reinforcement learning methods using PPO optimization [14, 16, 51, 52, 27], requires reward models. Following [27], we use a single highly-ranked [53], publicly available reward model, ArmoRM-Llama3-8B-v0.1 [54], to provide reward scores for both helpfulness and harmlessness. Refer to Appendix C.2 for more details. All methods use the same pre-trained model.\nResults and Comparisons. The results are presented in Table 1. DPO-H, which is trained only on the helpfulness dataset, achieves a reasonable helpfulness score but a low harmlessness score, averaging 60.99%. Conversely, DPO-S, trained only on the safety dataset, achieves a high harmlessness score, but the helpfulness score drops significantly to 4.34%.\nTraining with a naive mix of the helpfulness and safety datasets tends to bias the model toward learning more from the helpful data, resulting in even lower harmlessness scores, as shown by DPO. This aligns with previous findings that the mix ratio of helpfulness and harmlessness data is difficult to control, and training often focuses on a single perspective [14, 13]. In comparison to these supervised methods, BFPO achieves the highest average harmlessness score of 77.16% and significantly improves the generative tasks score from 39.71% to 95.24%.\nMORL, the multi-objective reinforcement learning method, shows a relatively small improvement in the harmlessness score. We suspect the primary reason is that the reward scores of different responses provided by the public reward model are not sufficiently distinguishable, making it inefficient for the model to learn to generate good responses while avoiding bad ones. This highlights the need for training a reward model specific to the model being fine-tuned, which involves the costly human prompting (red teaming) and annotation process.\nAt the same time, we maintain the same level of helpfulness as the model trained only with the helpful dataset and even improve it by incorporating the safety dataset. Full results are in Appendix C.3."}, {"title": "4.3 Improve Pre-aligned Models with Red Teaming Data", "content": "In this section, we apply our method as an additional safety alignment stage for existing pre-aligned models with a few thousand red teaming data. We compare our method with DPO [18], IPO [19], MORL [51] as in Section 4.2.\nData Preparation. We first use 9K harmful prompts from the PKU-SafeRLHF dataset [16] and have the Zephyr-7b-beta [47] model generate two responses for each prompt. We then use the HarmBench-Llama2-13B-Chat [43] classifier to determine whether the generated responses are harmful. For prompts that result in harmful responses, we use PairRM [55] to rank the responses in terms of helpfulness. This process results in 1.5K harmful prompts, responses, safety labels for each response, and pairwise helpfulness preferences.\nResults. Table 2 shows the results. Our method improves the harmlessness of the Zephyr-7b-beta model from 60.99% to 73.90%, while preserving the helpfulness. The improvement in generative tasks is particularly significant, from 62.94% to 88.79%. The supervised methods, DPO and IPO, can also improve the harmlessness, but the improvement is not as substantial as with our method. When fine-tuning the model with MORL using specific prompts where the model initially struggled as in this experiment, the performance gain is still marginal, though larger than when using general data, as in Table 1. This aligns with the observation that using RL methods to improve safety requires a large amount of model-specific data, high-quality labels, and a reward model specifically trained on these data to provide distinguishable scores. In contrast, BFPO achieves similar goals without the need for large amounts of helpfulness data mixed with red teaming data. Moreover, our overall pipeline of this experiment is efficient and automatic, requiring no human annotation. These results strongly indicate that our method can be effectively used in an additional safety alignment stage for existing chat models to improve harmlessness at minimal cost. Full results are in Appendix C.3."}, {"title": "4.4 Ablations", "content": "We validate the technical design of our algorithm in Table 4, showing that the shift parameter \u03b1 and buffered training are effective in improving harmlessness.\nIn the BFPO \u03b1 = 0 experiment, we set the shift parameter \u03b1 to 0. The results indicate that, as illustrated in Section 3.4, the shift parameter \u03b1 is useful in distinguishing unsafe data, and thus improves performance on generative tasks in harmlessness slightly. In the BFPO - w/o buffer experiment, we do not balance examples from the safety dataset and the helpful dataset, but instead mix the two datasets and randomly sample data from them. The lower harmlessness performance provides the evidence that buffered training helps mitigate the tension between helpfulness and harmlessness. Full results are provided in Appendix C.3."}, {"title": "5 Related Work", "content": "Alignment with Diverse Preferences. Traditional language model alignment methods [56, 22, 37] typically use a single reward or unified preference model.However, recent work suggests that human preferences are diverse and cannot be adequately represented by a single reward model. To address this, Chakraborty et al. [26] propose learning a mixture distribution for the reward using the EM algorithm, which they then apply in their MaxMin RLHF approach. Other research [52, 51, 27] ex- plores training multi-objective reward models for the alignment stage. These methods primarily focus"}, {"title": "6 Limitations and Discussion", "content": "In this paper, we propose a novel supervised optimization method, Bi-Factorial Preference Optimization (BFPO), to balance the safety and helpfulness during the alignment of LLMs. We theoretically prove that this direct optimization is equivalent to previous multi-objective reinforcement learning that combine safety and helpfulness rewards. With BFPO, we outperform existing methods in terms of safety and helpfulness in both fine-tuning the pre-trained LLMs and pre-aligned models. Our method is highly effective, significantly more computationally efficient, and does not require any human annotation or additional data collection.\nFurthermore, our approach is versatile and does not rely on any specific property of harmlessness itself. This flexibility allows it to be applied to improve various other potentially conflicting objectives in aligning LLMs. To achieve this, we only need characteristic-specific labels for the field-specific dataset. We believe our proposed method can serve as a general framework for the transfer learning of aligned models. However, our method relies on specific label formats for helpfulness and safety may present a limitation when addressing different tasks. Moreover, extending our work to handle more objectives (beyond just two) is also a promising direction for future research."}, {"title": "B.1 Notation", "content": ""}, {"title": "B.2 Proof of Theorem 3.1", "content": "We begin by restating Theorem 3.1 with the notation system y, y'. Note that the different notation systems will only affect the presentation of the reward function g and the labeling function g1, which we will discuss in the proof.\nTheorem B.1. Let \u03c4 > 0 be a real number, \u03c0\u03b8, \u03c0ref be two policy. Then\n$\\pi^{*}(y) = \\frac{\\pi_{ref}(y) exp (\\tau^{-1}g(y))}{\\sum_{s \\in S} \\pi_{ref}(s) exp (\\tau^{-1}g(s))}$ \nis an optimal solution to the optimization problem\n$\\max_{\\pi_{\\theta}} \\mathbb{E}_{y \\sim \\pi_{\\theta}} (y) [g(y) - \\tau KL [\\pi_{\\theta}(y)||\\pi_{ref}(y)]],$ \nand \u03c0* (y) is the optimal unique solution of\n$\\min_{\\pi_{\\theta}} \\mathbb{E}_{y,y' \\sim \\pi_{\\theta}} (y) [h_{\\pi}(y, y') - \\frac{g(y) - g(y')}{\\tau}]^{2},$ \nwhere\n$h_{\\pi}(y, y') = log (\\frac{\\pi_{\\theta}(y)\\pi_{ref}(y')}{\\pi_{\\theta}(y')\\pi_{ref}(y)}).$\nTo establish optimal solution, we follow Azar et al. [19] to leverage the following lemma.\nLemma B.2 (Rafailov et al. [18], Azar et al. [19]). Let\n$\\mathcal{L}_{\\tau}(\\delta) = \\mathbb{E}_{s \\in S}[f(s)] - \\tau KL[\\delta||\\eta],$\nwhere s \u2208 S and S is a finite set, f \u2208 RS is a function mapping elements of S to real numbers, \u03b4 \u2208 \u2206(S) is a probability distribution over S, \u03b7 \u2208 \u2206(S) is a fixed reference distribution, and \u03c4 \u2208 R* is a strictly positive number. Then the argmax problem with the regularized criterion\n$\\arg \\max_{\\delta \\in \\Delta(S)} \\mathcal{L}_{\\tau}(\\delta)$\nhas an optimal solution \u03b4*, where\n$\\delta^{*} (s) = \\frac{\\eta(s) exp(\\tau^{-1} f(s))}{\\sum_{s^{'} \\in S} \\eta(s^{'}) exp(\\tau^{-1} f(s^{'}))} \\forall s \\in S$.\nTo establish the uniqueness of the solution in Equation (16) for the optimization problem in Equation (18), we leverage the following lemma.\nLemma B.3 (Theorem 2 in Azar et al. [19]). Let\n$L(\\pi_{\\theta}) = \\mathbb{E}_{y,y' \\sim \\pi_{\\theta}} (y) [h_{\\pi}(y, y') - \\frac{g(y) - g(y')}{\\tau}]^{2},$\nthen min\u03c0\u03b5\u03c0 L(\u03c0\u03b8) has a unique optimal solution \u03c0* expressed in Equation (16), and no other local or global minima exist."}, {"title": "B.3 Proof of Theorem 3.2", "content": "In this section, we prove the Theorem 3.2. We begin by rewriting the formula in Equation (12) into a function of y, y'.\n$g_{1}(y, y') = B_{3} (B_{1} (I_{safe} (y) I_{help} (y + y') + I_{safe} (y') I_{help} (y' >y))\n- (I_{safe} (y) I_{help} (y' > y) + I_{safe} (y') I_{help} (y + y')) + B_{2}). (2I_{help} (y - y') \u2013 1),$\nHere, Ihelp(y > y') determines whether y is the win response or lose response. In other words,\n$I_{safe} (y^{hw}) = I_{safe}(y) I_{help}(y > y') + I_{safe} (y') I_{help}(y' \u27a2 y),$\nand the same applies to Isafe (yhl). To enable the reordering of the variables, we further multiply the formula by 2Ihelp(y > y') \u2013 1, since h\u03c0(y, y') = \u2212h\u03c0(y', y) By organizing the terms, we have\n$g_{1}(y, y') =(B_{1}B_{3} - B_{3})I_{help}(y > y')I_{safe}(y) + (B_{1}B_{3} \u2013 B_{3})I_{help}(y > y')I_{safe}(y')\n- B_{1}B_{3}I_{safe} (y') + B_{3}I_{safe}(y) + 2B_{2}B_{3}I_{help}(y > y') \u2013 B_{2}B_{3}$\nWe first establish the equivalence of the two optimization problems in Equation (22) and Equation (23) under the specific choice of constants, and then provide the general relation of constants for the equivalence.\nHere, we use the following constants:\n$A_{1} = \\mathbb{E}_{s}, A_{2} = \\frac{1}{2}, B_{1} = 3, B_{2} = 0, B_{3} = \\frac{1}{2}$\nTheorem B.4. The optimization problem\n$\\min_{\\pi_{\\theta}} \\mathbb{E}_{x \\sim \\rho, y, y' \\sim \\pi_{\\theta}(y)} [h_{\\pi} (y, y') - \\frac{g (p_{safe}(y), p_{help} (y) - g (p_{safe}(y'), p_{help}(y'))}{\\tau}]^{2},$\nwhere $g(y) = (p_{safe} (y) + \\mathbb{E}_{s}) (p_{help} (y > \\pi) + \\frac{1}{2})$, is equivalent to the optimization problem\n$\\min_{\\pi_{\\theta}} \\mathbb{E}_{x \\sim \\rho, y, y' \\sim \\pi_{\\theta}(y), I \\sim Bernoulli} [h_{\\pi} (y, y') - \\frac{g_{1} (y, y')}{\\tau}]^{2};$\nwhere\n$g_{1}(y, y') = I_{help}(y > y') I_{safe}(y) + I_{help} (y + y') I_{safe}(y') + \\frac{3}{2}I_{safe} (y) - \\frac{1}{2}I_{safe}(y')$\nHere, I ~ Bernoulli denotes the Bernoulli variables Isafe(y) and Isafe (y')."}, {"title": "B.4 Relation of the Constants", "content": "In this section, we derive a more general form of Theorem B.4, where, with specific relations between the constants in g and g1, the optimization problem in Equation (22) is equivalent to the optimization problem in Equation (23).\nWe restate g and g1 here with the notations used in the Appendix for convenience.\n$g = (p_{s}(y) + A_{1})(p_{h}(y) + A_{2}),$\nand\n$g_{1}(y, y') =(B_{1}B_{3} \u2013 B_{3})I_{help}(y > y')I_{safe}(y) + (B_{1}B_{3} - B_{3})I_{help}(y > y')I_{safe}(y')\n- B_{1}B_{3}I_{safe} (y') + B_{3}I_{safe}(y) + 2B_{2}B_{3}I_{help}(y > y') \u2013 B_{2}B_{3}$\nAs discussed in the proof of Theorem B.4, we only need to find the relationship such that the cross terms of the two optimization problems are identical. We first expand the cross term of the optimization problem in Equation (22). As in Equation (26), it can be written as\n$(24) = \\mathbb{E}_{x \\sim \\rho, y \\sim \\pi} [( \\pi_{y} \u2013 \\pi_{l})\u00b7 2 (g(p_{s}(y), p_{h}(y)) - \\mathbb{E}_{y^{'} \\sim \\pi} [g(p_{s}(y^{'}), p_{h}(y^{'}))])]$\nUsing the same strategy of obtaining Equation (29), we have\n$(28) = \\mathbb{E}_{x \\sim \\rho, y \\sim \\pi} [( \\pi_{y} \u2013 \\pi_{l}) (2B_{3}(B_{1} \u2013 1)p_{s}(y)p_{h}(y)\n+2B_{3}((B_{1} \u2013 1)\\mathbb{E}_{s} + 2B_{2})p_{h}(y) + 2B_{3}p_{s}(y) \u2013 2B_{1}B_{3}\\mathbb{E}_{s} - 2B_{2}B_{3})]$\nAligning the coefficients of each term in Equation (31) and Equation (32), we derive the following set of equations:\n$B_{3}(B_{1} - 1) = 1,$\n$\\mathbb{E}_{s}+2B_{3}-B_{3} = A_{1},$\n$B_{3} = A_{2}.$\nSolving these equations gives us the specific forms of g and g1. Here B2 is a shifting value that we define to align with our intuition. B3 is a scaling factor that is related to the penalty \u03c4."}, {"title": "B.5 Discussion of the Property of g1", "content": "In this section", "implies": "n$(h_{\\pi"}, "y, y') \u2013 \\tau^{-1}g_{1}(y, y'))^{2} = (h_{\\pi}(y', y) \u2013 \\tau^{-1}g_{1}(y',y))^{2}.$\nThis means that for the same data point, regardless of the order of y and y', we are always driving h\u03c0(y, y') to the same value. In contrast, in IPO [19"], "problems": "n$(h_{\\pi}(y, y') \u2013 \\tau^{-1}g_{1}(y, y'))^{2}$ and $(h_{\\pi}(y',y))^{2}.$\nTheir final optimization problem, $(h_{\\pi}(y, y') \u2013 \\frac{1}{2}\\tau^{-1}g_{1}(y, y'))^{2}$, tries to find a middle point of h that optimizes both. However, this point is neither the optimal solution of the first problem nor the second problem.\nShifting Property. Second, we discuss the shifting properties of g1. Since Theorem 3.2 holds based on the equality of Equation (30) and Equation (27), and all the operations to derive these two equations are valid under linear transformations of psafe, phelp and Isafe, Ihelp, respectively. It implies that Theorem 3.2 also holds under the same linear transformations of psafe, phelp and"}