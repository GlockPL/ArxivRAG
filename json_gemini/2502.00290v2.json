{"title": "Estimating LLM Uncertainty with Logits", "authors": ["Huan Ma", "Jingdong Chen", "Guangyu Wang", "Changqing Zhang"], "abstract": "In recent years, Large Language Models (LLMs) have seen remarkable advancements and have been extensively integrated across various fields. Despite their progress, LLMs are prone to hallucinations, producing responses that may not be reliable if the models lack sufficient grounding knowledge. To mitigate this issue, methods for estimating uncertainty have been adopted, with a focus on critical tokens as indicators of reliability. However, probability-based approaches have shown limitations in assessing token-level reliability due to the loss of evidence strength information acquired during training. In this paper, we introduce Logits-induced Token Uncertainty (LogU), a novel framework designed to estimate token-specific uncertainty in LLMs in real time, without the need for multiple sampling rounds. By leveraging evidence modeling for the implementation of LogU, we utilize the derived uncertainty measures to steer downstream tasks. Our experimental findings highlight the substantial effectiveness and potential of LogU, marking a significant advance in addressing the challenge of model hallucinations.", "sections": [{"title": "1. Introduction", "content": "Over the past few years, Large Language Models (LLMs) have developed rapidly and LLM-driven systems are deployed across various domains. Despite their remarkable performance, LLMs remain prone to hallucinations (Banerjee et al., 2024), which causes them to generate unreliable responses when the models lack the corresponding knowledge. Hallucinations critically undermine the reliability of LLMs, particularly in professional applications such as medical and legal consultations (Shah, 2024; Dahl et al., 2024). Hallucinations have been considered as a major barrier to the broader deployment of LLMs (Huang et al., 2023; Liu et al., 2024; Perkovi\u0107 et al., 2024; Zhou et al., 2024).\nUncertainty estimation has shown promise in the identification of hallucinations in LLMs (Xiao & Wang, 2021; Huang et al., 2024b). High uncertainty often indicates the need for caution from users, as it suggests that the model is likely to be influenced by hallucinations (Zhang et al., 2023; Yoffe et al., 2024). In other words, high uncertainty signals unreliable responses. However, existing methods for uncertainty estimation in LLMs have limitations in estimating the inherent uncertainty of LLMs and lack efficiency. Including discussions of various heuristic methods, such as self-reflection (Ji et al., 2023), LLM uncertainty estimation can be categorized into sampling-based and token-based methods. Sampling-based methods estimate uncertainty by multiple sampling (Liu et al., 2024), perturbations (Zhang et al., 2024), or contrastive analysis (Huang et al., 2024a). Semantic Entropy (SE) (Kuhn et al., 2024) is a representative technique among these sampling-based methods. The core of these methods is estimating the consistency of LLM's multiple guesses, and the response will be marked as unreliable if inconsistency is detected. However, sampling-based methods suffer from several limitations: (1) it cannot assess the reliability of a single response; (2) it requires multiple sampling iterations, making it inefficient and impractical for deployment in real-world applications; and (3) it fails to account for the model's inherent uncertainty, such as consistently incorrect responses due to a lack of knowledge.\nToken-based methods can estimate the uncertainty of a single sentence without requiring multiple samplings as deterministic approaches (Gupta et al., 2024; Fadeeva et al., 2024). However, due to the lack of effective token-level uncertainty estimation, these methods often fail to achieve satisfactory results. For example, many tokens are correct but exhibit low probabilities (high entropy) in the model response. Many works have pointed out that the importance of different tokens in a sentence is not uniform (Lin et al., 2024a; Duan et al., 2024a), and the reliability of the model response depends mainly on a few critical tokens (Duan et al., 2024b; Bigelow et al., 2024), so they only focus on critical tokens when estimating reliability. However, as shown in Fig. 1(a), although both \"Barack Hussein Obama\" and \"Abraham Lincoln\" could be correct answers, their maximum probability of the critical"}, {"title": "2. Related Work", "content": "Sampling-based uncertainty estimation. These methods evaluate the randomness in the LLM generation process. Specifically, they allow LLMs to guess multiple times and evaluate their consistency. (Cole et al., 2023) introduce repetition and diversity into the measurement of consistency. (Zhang et al., 2024) introduce a mechanism that perturbs semantically equivalent questions to evaluate the consistency of LLM responses across variants of the same question. (Huang et al., 2024a) inject correct and incorrect labels into the prompt during sampling, and the uncertainty level is"}, {"title": "3. Logits-induced Token Uncertainty", "content": "3.1. Notations\nConsider that given a pre-trained LLM noted as M and its corresponding tokenizer dictionary Y = {1, 2, ..., |Y|}, and |Y| indicates the size of the vocabulary dictionary. Specifically, the user inputs an instruction, then the instruction is transformed into a prompt by applying a chat template (for example, \"[INST] Could you give me one name of president?[\\INST]\". The prompt is encoded by the corresponding tokenizer as a vector q and input into LLM to perform the next token prediction under certain basic sampling strategies. The model continuously generates the next token at based on the q and tokens that have been generated at-1 = a1a2\uff65\uff65\uff65 at\u22121 (for example, at-1 is a generated token vector can be decoded into \"Sure, here is a historical American president: **\") until they meet the stop rule (for example, meeting [EOS]),"}, {"title": "3.2. Failures in Traditional Uncertainty Estimation", "content": "Traditional discriminative models typically use probability to estimate reliability. Probability, in fact, is a normalization of the strength of evidence for different categories. Due to the mutually exclusive properties of the different categories, the relative strength between different categories can accurately indicate the reliability of the prediction. However, in the case of LLMs, the situation is different. Although LLM's next token prediction can still be viewed as a classification task with |Y| categories, these categories are no longer mutually exclusive. Even different tokens that are mutually exclusive in a conversation may no longer be mutually exclusive in a different context. This is why recent LLM research suggests shifting from token-to-token training to a concept-to-concept training paradigm. There may be more than one suitable next token, so the relative relationship between different tokens to no longer reflect the reliability of the response. Therefore, the information of the strength of the evidence before normalization becomes important. During the LLM training process, the suitable token accumulates evidence (logits increase, similar to the discriminative model (Wei et al., 2022)), specifically, the higher the strength of the evidence, the more similar scenarios the model has encountered during training.\nConsider the two situations shown in Fig. 1(b): (1) left: LLM has encountered this question 3 times during training, with answers a, b, and c; (2) right: LLM has encountered this question 3,000 times during training, and the answers can be summarized into three situations, a, b, and c.\nThe accumulation of evidence in these two scenarios is significantly different, and their reliability is completely"}, {"title": "3.3. LogU: Four-quadrant Framework", "content": "The reason why probability-based methods fail to identify reliability is that probability is normalized. After logits are normalized, only the relative strength between different answers (should be either \u201cBarack\u201d or \u201cGeorge\") is retained, while the original strength information of the logits is discarded, which results in the loss of the ability to indicate reliability (distinguish between \"I do not know\" and \"\u0406 know more than one answer\"). To address this limitation, we propose a Logits-induced Token Uncertainty framework termed LogU. In addition to considering the relative relationships among tokens (AU), LogU also takes into account the strength of the model's response (EU). With the information of EU, \"I do not know\" and \"I know more than one answer\" can be characterized separately. As shown in Fig. 2(a), the four quadrants of uncertainty are described as:\nQuadrant I: high AU, high EU. This quadrant indicates that the model exhibits a low strength of evidence for all tokens, potentially due to lack of relevant knowledge. For example, as shown in Fig. 2(b), the model might recommend an unfamiliar medication.\nQuadrant II: low AU, high EU. In this quadrant, the model shows a low evidence strength for most tokens but there is a higher strength for one particular token, indicating a lack of diversity despite producing a relatively high probability token. For example, the model repetitively suggests a drug name that was recently mentioned. Failure of probability-based methods: Probability-based methods may regard this quadrant as highly reliable. However, it still involves a certain degree of risk due to the lack of knowledge, and the high probability only indicates the model recommendation.\nQuadrant III: low AU, low EU. Here, the model exhibits very high strength for one specific token while maintaining a low strength of evidence for all other tokens. This reflects a strong certainty about a particular token, such as the fixed phrase \"has been\u201d.\nQuadrant IV: high AU, low EU. This quadrant indicates that the model assigns a high strength of evidence to multiple tokens. Although none of them achieves a high probability, these tokens collectively demonstrate strong evidence. For example, this situation might arise when predicting nouns or punctuation marks that can be expressed in multiple valid ways. Failure of probability-based methods:"}, {"title": "3.4. Considering Logits as Evidence", "content": "In this subsection, we present a viable implementation of the LogU framework, specifically by modeling AU and EU using a Dirichlet distribution. Inspired by Evidential Deep Learning (Sensoy et al., 2018), we treat logits as evidence for each token and model them into a Dirichlet distribution. A naive approach is using the logits of all non-negative values as evidence, while setting those with negative values to no evidence by applying the ReLU activation function. However, unlike conventional classification networks, the number of candidates generated by an LLM (that is, tokenizer size) is significantly large, with a considerable proportion of tokens having extremely low logits, and these tokens should be discarded as noise (Tang et al., 2024). Therefore, this paper focuses on the distribution of main candidates with high logits. Specifically, we sample the top K tokens with the largest logits to model a Dirichlet distribution:\n\u03b1k = M(Tkq, at-1), \u03b1\u03bf = \u03a3 \u03b1k,\nwhere Tk is the token with the k-largest prediction logit, and \u03b1o is the total evidence of the Dirichlet distribution (the sum of the largest k logits).\nAleatoric (data) uncertainty. To measure the data uncertainty, we evaluate the expected entropy of the data distribution. Since entropy captures the \u201cpeakiness\u201d of the output distribution, a lower entropy indicates that the model concentrates most of the probability mass on a single class, while a higher entropy characterizes a more uniform distribution, indicating that the model is undecided about the prediction. For Dirichlet networks, this quantity has a closed-form solution:\nAU(at) =  \u03b1\u03ba (\u03c8(\u03b1\u03ba + 1) \u2013 \u03c8(\u03b1\u03bf + 1)),\nwhere \u03c8 denotes the digamma function, defined as f(x) = log \u0393(x).\ndx\nEpistemic (model) uncertainty. We define the EU by:\nEU(at) = K/\u03a3(\u03b1\u03ba + 1),\nand the underlying intuition is that larger \u03b1k produces a sharper density, and thus it indicates increased confidence in a prediction. For more information on Dirichlet distribution, please refer to the introduction from (Ulmer et al., 2023)."}, {"title": "4. Application I: Dynamic Decoding Strategy", "content": "4.1. LogU-guided Decoding\nIt is necessary to ensure that the generated response to be of diversity, especially in diversity-driven fields such as LLM-guided discovery researches (Peng et al., 2024). However, higher diversity often means that the LLM's responses are more prone to hallucinations. For example, during the sampling process, a larger temperature tends to generate more unreliable responses from the model. Therefore, ensuring both the diversity and precision of LLM-generated results is a key challenge.\nLogU offers a potential solution to this challenge. In this subsection, we propose a dynamic decoding strategy that can adjust its sampling approach according to LogU during response generation. The dynamic decoding strategy ensures diverse answers when the LLM has adequate knowledge (i.e., low model uncertainty), while adopting a more cautious sampling strategy when the model's knowledge is insufficient (i.e., high model uncertainty), thus maintaining both diversity and accuracy in the generated responses. Specifically, we hope that the sampling diversity and the LLM's EU about the next token denoted as EU(at) are negatively correlated, which means that the higher EU, the less chance for sampling tokens with lower scores. Taking the temperature sampling strategy as an example, there is a smaller temperature when the model has larger EU.\n4.2. Experimental Analysis\n4.2.1. SETTINGS\nIn this paper, we use the multi-label evaluation benchmark SemEval (Mohammad et al., 2018), which is a multi-tag NLP analysis task on tweet text. We evaluate models of different sizes, including LLaMA2 (7B), LLaMA2 (13B), LLaMA3 (3B), LLaMA3 (8B) and LLaMA3 (70B). As shown in Fig. 3, after providing an answer with the first class label, LLM dynamically decides whether to give the second class label based on the uncertainty indicator. We can evaluate the ability of uncertainty to guide the model in rejecting incorrect answers (the ability to avoid hallucinations in generating diverse responses).\nTo evaluate the effectiveness of the dynamic decoding strategy, we verify whether it can enhance diversity while maintaining accuracy. Specifically, we evaluate whether the model can select as many correct answers as possible with the guidance of uncertainty on the multi-label LLM benchmark SemEval (Mohammad et al., 2018). As shown in Fig. 3, for all test samples in the entire data set, when generating responses, the LLM selects the class with the highest output probability or the two tokens with the highest"}, {"title": "5. Application II: Reliability Estimation", "content": "5.1. LogU-guided Response Uncertainty Estimation\nAnother benefit that LogU brings is the estimation of the reliability of the response. In traditional probability-based token uncertainty estimation, a large number of uncritical tokens exhibit high uncertainty, making it difficult to map token uncertainty to sentence uncertainty. This issue requires manually applying weights to different tokens to overcome the problem caused by uncritical tokens being estimated with high uncertainty. LogU naturally overcomes this problem. As shown in the case study in Fig. 2(b), for uncritical tokens such as commas, the probability-based method labels them as unreliable tokens due to their low predictive probability (high entropy). In contrast, LogU accurately classifies them into the fourth quadrant, marking them as \"I know more than one answer\". Therefore, LogU can more easily use token uncertainty to represent the uncertainty of sentences. Inspired by (Duan et al., 2024a), we use the most uncertain tokens in a sentence to represent the overall reliability of the sentence. The response reliability can be represented as the averaged reliability on tokens with the\nK-lowest reliability values:\nRresponse =  \u03a3R(at),\nwhere Rresponse indicates the reliability of the response and R(at) represents the reliability of token at, and Tk represents the set of K tokens with the lowest reliability values (rel(at)).\nFollowing the uncertainty combination of AU and EU in discriminative models (Abdar et al., 2021), in the experiments, we simply represent the reliability of the token as:\nR(at) = 1/AU(at)EU(at),\nwhich indicates that the reliability of the token is low when both the estimated AU and EU are high. In this paper, we simply identify the response reliability in QA task according to the scenarios classified in Fig. 2(a): (1) high AU, high EU: LLM lacks knowledge of the question and has no idea of a suggested answer (unreliable); (2) low AU, high EU: The LLM lacks knowledge of the question, but it knows what should be an appropriate answer (reliable); (3) low AU, low EU: LLM knows precisely what is the most appropriate answer (reliable); (4) high AU, low EU: LLM has encountered many similar samples during training and knows more than one suitable response (reliable)."}, {"title": "6. Theoretical Analysis", "content": "Different correct answers are competitor. For any LLM trained with cross-entropy loss, different correct answers are competitors in terms of probability 3. Continuing with the example of proposing a president, suppose \u03c4a (\u201cBarack\u201d) is the label of a sample whose qis\u201c[INST]Could you give me one name of president?[\\INST]\" and a generated token vector at-1 can be decoded into \"Sure, here is a historical American president: **\", the loss of the next token at this position during supervised fine-tuning can be written as:\nLa = -log exp(M(raq,at-1)) / \u03a3 exp(M(rm|q,at-1)),\nwhere La is the loss on the sample with the next token label \u03c4a. Consider cases where multiple distinct answers to the same question appear in the training set, the situation becomes different. For example, \u03c4b (\u201cGeorge\u201d) is the label in another sample with the same question. When the model\n3The \"same question\" refers to questions that are semantically equivalent but do not need to be identical."}, {"title": "7. Conclusion", "content": "In this paper, we find that the main reason for the failure of probability-based methods in estimating LLM's uncertainty is that the information regarding evidence strength is lost during the normalization process, making probability cannot accurately reflect reliability. Therefore, we propose LogU to model the evidence for generating the next token in LLMs, a real-time token uncertainty estimation framework. We use LogU to guide two downstream tasks of LLMs, dynamic decoding and response reliability estimation, achieving significant performance improvements. These downstream tasks demonstrate the simplicity, effectiveness, and immense potential of LogU."}]}