{"title": "A Hybrid Framework for Reinsurance Optimization: Integrating Generative Models and Reinforcement Learning", "authors": ["Stella C. Dong", "James R. Finlay"], "abstract": "Reinsurance optimization is critical for insurers to manage risk exposure, ensure financial stability, and maintain solvency. Traditional approaches often struggle with dynamic claim distributions, high-dimensional constraints, and evolving market conditions. This paper introduces a novel hybrid framework that integrates Generative Models, specifically Variational Autoencoders (VAEs), with Reinforcement Learning (RL) using Proximal Policy Optimization (PPO). The framework enables dynamic and scalable optimization of reinsurance strategies by combining the generative modeling of complex claim distributions with the adaptive decision-making capabilities of reinforcement learning.\n\nThe VAE component generates synthetic claims, including rare and catastrophic events, addressing data scarcity and variability, while the PPO algorithm dynamically adjusts reinsurance parameters to maximize surplus and minimize ruin probability. The framework's performance is validated through extensive experiments, including out-of-sample testing, stress-testing scenarios (e.g., pandemic impacts, catastrophic events), and scalability analysis across portfolio sizes. Results demonstrate its superior adaptability, scalability, and robustness compared to traditional optimization techniques, achieving higher final surpluses and computational efficiency.\n\nKey contributions include the development of a hybrid approach for high-dimensional optimization, dynamic reinsurance parameterization, and validation against stochastic claim distributions. The proposed framework offers a transformative solution for modern reinsurance challenges, with potential applications in multi-line insurance operations, catastrophe modeling, and risk-sharing strategy design.", "sections": [{"title": "1 Introduction", "content": "The insurance and reinsurance industries play a pivotal role in managing financial risks and ensuring economic stability. Reinsurance, which involves the transfer of risk from insurers to reinsurers, is a cornerstone of risk management strategies aimed at maintaining solvency and optimizing financial performance. However, designing effective reinsurance strategies remains a highly complex challenge due to the stochastic nature of claims, multi-dimensional constraints, and the dynamic interplay between risk retention, profitability, and regulatory compliance [4,59].\n\nTraditional approaches to reinsurance optimization, such as the classical Cram\u00e9r-Lundberg model, have provided foundational insights into surplus dynamics and ruin probabilities. These models, while mathematically rigorous, rely on static assumptions about premium rates and claim distributions, limiting their applicability to modern reinsurance practices. Extensions to these models, including proportional and layered reinsurance structures, address some of these limitations but often remain computationally intensive and insufficiently adaptable to high-dimensional, real-world scenarios [5, 27].\n\nIn recent years, advances in machine learning and artificial intelligence (AI) have demonstrated transformative potential in addressing challenges across the insurance domain. Generative A\u0399 models, such as Variational Autoencoders (VAEs), have shown promise in capturing complex data distributions and generating synthetic data reflective of rare and catastrophic events that are un-derrepresented in historical datasets [30,42]. Concurrently, reinforcement learning (RL) techniques, particularly Proximal Policy Optimization (PPO), have emerged as powerful tools for sequential decision-making and dynamic optimization in uncertain environments [61].\n\nThis paper introduces a novel hybrid framework that integrates generative AI with reinforce-ment learning to optimize reinsurance strategies dynamically and adaptively. By leveraging VAES to model claim distributions and generate synthetic scenarios, the framework overcomes challenges associated with data scarcity and variability. The PPO algorithm, on the other hand, dynam-ically adjusts reinsurance parameters\u2014such as retention rates and layer boundaries based on evolving claim distributions, market conditions, and regulatory constraints. This synergy enables the framework to evaluate and optimize complex reinsurance strategies in real time, addressing high-dimensional uncertainties and ensuring financial stability [5, 16].\n\nThe proposed hybrid framework is validated through comprehensive simulations, including stress-testing scenarios such as high-frequency claims, catastrophic tail events, and pandemic-induced impacts. These evaluations demonstrate the framework's robustness, scalability, and adaptability, significantly outperforming traditional optimization methods such as Dynamic Programming, Monte Carlo simulations, and Multi-Objective Optimization. Furthermore, sensitivity analyses and scalability tests highlight the framework's resilience under diverse claim environments and varying portfolio sizes.\n\nKey contributions of this work include:\n\n1.  A hybrid framework for reinsurance optimization: The integration of generative AI models (VAEs) and reinforcement learning (PPO) to address multi-dimensional, stochastic optimization challenges in reinsurance.\n\n2.  Dynamic parameterization of reinsurance strategies: Incorporation of adaptive reten-tion rates and layer boundaries to ensure flexibility in risk-sharing mechanisms under evolving market conditions.\n\n3.  Comprehensive validation and benchmarking: Empirical evaluation against established optimization techniques, demonstrating superior performance in scalability, computational efficiency, and robustness.\n\nThe remainder of this paper is structured as follows: Section 2 presents the mathematical foundations of the surplus process, reinsurance structures, and optimization objectives. Section 3 introduces the hybrid computational framework, detailing the integration of generative AI and reinforcement learning. Section 4 describes the experimental setup, results, and benchmarking against alternative methods. Section 5 discusses the implications and limitations of the proposed framework, and Section 6 concludes with key findings and future research directions.\n\nBy bridging the gap between traditional actuarial methods and cutting-edge AI techniques, this study provides a transformative framework for reinsurance optimization, addressing pressing challenges in financial risk management and setting a foundation for future innovations in the insurance industry."}, {"title": "2 Model Description", "content": "This section introduces a robust and adaptable framework for modeling an insurer's operations over a finite planning horizon, T. The model addresses key challenges in risk management, dynamic claim processes, and financial stability by incorporating discrete-time modeling, a generalized surplus process, and dynamic reinsurance mechanisms. This approach provides a comprehensive foundation for optimizing decision-making under uncertainty."}, {"title": "2.1 Discrete-Time Framework", "content": "The planning horizon T is divided into n discrete time intervals, denoted as $t_1,..., t_n$, where $t_1 = 0$ and $t_n = T$. Each interval represents a time step during which the insurer manages risk portfolios, generates premium income, and incurs claims. This temporal structure reflects real-world insurance practices, where financial performance and risk exposure are periodically reviewed and adjusted [4]. The discrete-time formulation enables granular evaluation of risk and financial metrics, allowing the integration of stochastic factors into operational decisions. This granularity is crucial for analyzing the dynamic interplay between claims, premiums, and reinsurance."}, {"title": "2.2 Modeling the Surplus Process", "content": "The financial surplus, defined as the difference between an insurer's assets and liabilities, evolves over time based on claim occurrences and premium collections. The surplus process is modeled using an enhanced Cram\u00e9r-Lundberg framework, a foundational approach in actuarial science.\n\nLet $N_i$ represent the number of claims during the i-th interval $[t_{i-1},t_i)$, modeled as a Poisson random variable with intensity $\\Lambda \\Delta t_i$, where $\\Delta t_i = t_i - t_{i-1}$. Each claim $X_{ij}$ is assumed to be independent and identically distributed (i.i.d.). The surplus process is defined as:\n\n$S_{i+1} = S_i + c\\Delta t_i - \\sum_{j=1}^{N_i}X_{ij}$, (2.1)\n\nwhere:\n\n\u2022 $S_i$: Surplus at time $t_i$,\n\n\u2022 $c$: Premium rate per unit time, calculated as:\n\n$c = (1 + \\theta)\\lambda E[X]$, (2.2)\n\nwith $\\theta > 0$ representing the safety loading factor to ensure profitability and solvency [59].\n\nThis formulation provides a dynamic representation of surplus evolution, allowing for rigorous analysis of financial stability and risk mitigation strategies."}, {"title": "2.3 Incorporating Reinsurance Mechanisms", "content": "Reinsurance is a critical tool for risk-sharing, enabling insurers to transfer portions of their liabilities to reinsurers. This framework incorporates multiple reinsurance arrangements tailored to diverse risk profiles and operational needs."}, {"title": "2.3.1 Proportional Reinsurance", "content": "In proportional reinsurance, a fixed fraction $\\alpha$ of each claim is retained by the insurer, with the remainder covered by the reinsurer. The surplus process is modified as:\n\n$S_{i+1} = S_i + c\\Delta t_i - \\sum_{j=1}^{N_i}\\alpha X_{ij}$, (2.3)\n\nwhere $\\alpha \\in [0, 1]$ represents the retention rate. This straightforward approach balances risk retention and cost efficiency [59]."}, {"title": "2.3.2 Layered Reinsurance", "content": "Layered reinsurance divides claims into predefined layers with variable retention rates. The retained loss for a claim $X_{ij}$ is expressed as:\n\n$L_{ij} = \\sum_{k=1}^{K}\\alpha_k min(max(X_{ij} - a_k, 0), b_k \u2013 a_k)$, (2.4)\n\nwhere:\n\n\u2022 $[a_k, b_k]$: Boundaries of layer k,\n\n\u2022 $\\alpha_k$: Retention rate for layer k,\n\n\u2022 $K$: Total number of layers [5].\n\nThis structure enables strategic risk-sharing, optimizing cost-effectiveness while mitigating high-severity losses."}, {"title": "2.3.3 Dynamic Reinsurance Adjustments", "content": "To address evolving market conditions and regulatory requirements, dynamic adjustments to re-tention rates and layer boundaries are incorporated:\n\n$a_k(t_i) = a_{k}^{base} + \\delta_k(t_i)$, (2.5)\n\n$\\alpha_k(t_i) = \\alpha_{k}^{base} + \\Delta a_k(t_i), b_k(t_i) = b_{k}^{base} + \\Delta b_k(t_i)$, (2.6)\n\nwhere $\\delta_k(t_i)$, $\\Delta a_k(t_i)$, and $\\Delta b_k(t_i)$ are time-dependent adjustments informed by reinforcement learning agents. These dynamic capabilities enhance the framework's adaptability to stochastic and temporal uncertainties [62]."}, {"title": "2.4 Optimization Objectives", "content": "The primary objective is to maximize the expected utility of the terminal surplus $S_n$, accounting for premiums, claims, and reinsurance costs:\n\n$max \\mathbb{E}[U(S_n)]$. (2.7)\n$\\alpha_k,a_k,b_k$\n\nThis optimization is subject to the following constraints:\n\n1.  Ruin Probability Constraint: The probability of financial ruin must not exceed a prede-fined threshold [4]:\n\n$P(S_i < 0 \\text{ for any } i = 0, . . ., n) \u2264 V_{target}$. (2.8)\n\n2.  Budget Constraint: Total reinsurance premium costs must remain within a specified limit:\n\n$P = \\sum_{k=1}^{K}(1 + \\theta_k) \\beta_k \\mathbb{E}[r_k(X)] \u2264 P_{max}$, (2.9)\n\nwhere $\\beta_k = 1 -\u2013 \\alpha_k$ and $r_k(X)$ represents claims covered by layer k [3].\n\n3.  Layer Structure Constraint: Layer boundaries must remain non-overlapping:\n\n$a_{k+1} \u2265 b_k, \\forall k$. (2.10)\n\n4.  Retention Rate Bounds: Retention rates must satisfy:\n\n$0 \u2264 \\alpha_k \u2264 1, \\forall k$. (2.11)\n\nThese constraints ensure financial stability, regulatory compliance, and efficient resource allo-cation."}, {"title": "3 A Hybrid Framework for Generative Models and Reinforcement Learning in Reinsurance Optimization", "content": "The management of reinsurance portfolios requires balancing financial stability, regulatory compli-ance, and risk mitigation under uncertain conditions. This section introduces a hybrid framework that integrates generative AI models, specifically Variational Autoencoders (VAEs), with Rein-forcement Learning (RL) using Proximal Policy Optimization (PPO). The proposed framework addresses challenges such as data scarcity, stochastic claim dynamics, and evolving market condi-tions by combining synthetic data generation and sequential decision-making. This hybrid approach enables adaptive and dynamic reinsurance optimization while ensuring scalability and robustness in managing diverse risk profiles."}, {"title": "3.1 Generative Claim Model Using Variational Autoencoders (VAE)", "content": "Variational Autoencoders (VAEs) form the generative backbone of the proposed framework, de-signed to model the statistical properties of historical insurance claims. VAEs address the challenge of data scarcity by generating synthetic claim scenarios that capture both observed data patterns and potential extreme events [30,42]."}, {"title": "3.1.1 Machine Learning Architecture and Components", "content": "The VAE architecture comprises three core components, as illustrated in Figure 1:\n\n1.  Encoder: The encoder maps high-dimensional historical claims data to a lower-dimensional latent space. This neural network extracts salient features while reducing noise, providing a probabilistic representation parameterized by mean (\u03bc) and log variance (log \u03c3\u00b2). This latent representation ensures flexibility in modeling diverse data distributions [42].\n\n2.  Latent Space: The latent space encodes each claim as a probabilistic distribution. By sampling from this space, the VAE generates synthetic claims that extrapolate beyond the observed dataset while adhering to its statistical properties. This capability is critical for stress-testing and exploring rare, high-severity loss scenarios.\n\n3.  Decoder: The decoder reconstructs claims from the latent space, ensuring that the generated data aligns with the statistical characteristics of the historical claims. This process preserves realism while introducing controlled variability, enabling comprehensive policy testing under diverse scenarios."}, {"title": "3.1.2 Training Objectives and Loss Function", "content": "The VAE training process balances two critical objectives:\n\n\u2022 Reconstruction Fidelity: The reconstruction loss ensures that the decoder-generated syn-thetic claims closely approximate the input claims. This fidelity is quantified using the mean squared error (MSE):\n\n$L_{reconstruction} = \\mathbb{E} [||X \u2013 \\hat{X} ||^2]$.\n\n\u2022 Latent Space Regularization: The Kullback-Leibler (KL) divergence term regularizes the latent space, promoting smoothness and diversity in the generated claims:\n\n$L_{KL} = -\\frac{1}{2} \\sum_{j}(1 + log \\sigma^2 ; \u03c3^2 \u2013 \u03bc^2 \u2013 \u03c32)$.\n\nThe overall loss function is:\n\n$L_{VAE} = L_{reconstruction} + \u03b2\u00b7 L_{KL}$,\n\nwhere \u03b2 adjusts the relative weight of the regularization term [33]."}, {"title": "3.1.3 Application to Reinsurance Optimization", "content": "The VAE-generated synthetic claims enrich the RL training environment by introducing diverse and extreme scenarios. This expanded dataset enables the RL agent to develop robust policies that perform effectively under high-dimensional, stochastic conditions. By incorporating rare events, the framework ensures resilience against catastrophic losses and enhances decision-making in dynamic markets [5,16]."}, {"title": "3.2 Reinforcement Learning for Sequential Decision-Making", "content": "Reinforcement Learning (RL) is the decision-making core of the framework, optimizing reinsur-ance strategies through sequential learning. Built on the OpenAI Gym framework [12], the RL environment simulates realistic insurance operations, including stochastic claim dynamics, surplus evolution, and reinsurance contract adjustments."}, {"title": "3.2.1 State Observations and Action Space", "content": "At each timestep t, the RL agent observes a state vector $s_t$, which captures key operational metrics:\n\n$s_t = (S_t, \\Lambda, {\\alpha_k}_{k=1}^{K}, {a_k, b_k}_{k=1}^{K})$,\n\nwhere $S_t$ is the financial surplus, $\\Lambda$ denotes claim frequency, {$\\alpha_k$} are retention rates, and {$a_k,b_k$} define reinsurance layer boundaries [4,34].\n\nThe agent adjusts these parameters by selecting actions $a_t$ to optimize risk-sharing and financial stability over time."}, {"title": "3.2.2 Reward Structure and Policy Optimization", "content": "The reward function incentivizes solvency and profitability while penalizing adverse outcomes:\n\n$r_t = log(S_t + \\epsilon)$,\n\nwhere $\u03f5 > 0$ ensures numerical stability. The RL agent refines its policy $\u03c0(a_t|s_t)$ using Proximal Policy Optimization (PPO), maximizing the cumulative discounted reward:\n\n$J(\\pi) = \\mathbb{E}_{\\pi} [\\sum_{t=0}^{T} \\gamma^t r_t]$,\n\nwhere $\u03b3 \\in [0, 1]$ balances short- and long-term objectives [61]."}, {"title": "3.2.3 Integration with VAE-Generated Scenarios", "content": "The VAE-generated claims introduce variability and stress-test the RL agent's policies. By exposing the agent to extreme and rare events, this integration ensures robustness in high-dimensional and uncertain environments [33,42]."}, {"title": "3.3 Interaction Workflow", "content": "Figure 2 illustrates the interaction between the VAE and RL components. The iterative process combines synthetic claim generation, state observation, action execution, and reward feedback to optimize reinsurance strategies dynamically."}, {"title": "4 Comprehensive Evaluation of Optimization Frameworks", "content": "This section presents a comprehensive evaluation of the proposed hybrid reinsurance optimization framework, focusing on simulation setup, training metrics, and benchmark performance against established methods."}, {"title": "4.1 Simulation Configuration and Initial Parameters", "content": "The simulation environment was designed to mimic realistic reinsurance scenarios. Table 1 summa-rizes the initial parameters. The insurer's starting surplus was set at $20,000, with claims modeled using a Poisson process with an average frequency of \u03bb = 10 claims per year [51]. Claim sizes were sampled from a lognormal distribution \u03bc = 3.5 and \u03c3 = 1.0 [1]. These synthetic claims were used to train the Variational Autoencoder (VAE), which generated realistic scenarios for reinforcement learning."}, {"title": "4.2 Training Metrics and Surplus Trajectory Analysis", "content": "The training metrics and surplus trajectory emphasize the PPO agent's ability to optimize reinsur-ance strategies. Table 2 outlines key metrics, and Figure 3 illustrates the surplus trajectory across 6,144 timesteps."}, {"title": "4.3 Benchmark Performance and Comparative Analysis", "content": "The framework's performance was benchmarked against established methods, including Dynamic Programming [6], Monte Carlo simulations [29], Hybrid Deep Monte Carlo [60], Multi-Objective Optimization [21], and Hybrid RL with Generative Models. Table 3 summarizes the results.\n\nAnalysis of Results:\n\n\u2022 Dynamic Programming: Achieved a final surplus of $12,487.71 with zero ruin probability and high efficiency (1,568.63). However, its scalability is limited by the \"curse of dimension-ality\" [6].\n\n\u2022 Monte Carlo Simulation: Delivered a surplus of $12,803.21 but suffered from low efficiency (30.91) due to extensive computational demands [29].\n\n\u2022 Hybrid Deep Monte Carlo: Improved surplus ($12,973.67) but retained low efficiency (31.54), limiting its applicability for real-time optimization [60].\n\n\u2022 Multi-Objective Optimization: Balanced performance with a surplus of $12,467.12 and strong efficiency (1,462.96). However, its static nature limits adaptability in dynamic scenarios [21].\n\n\u2022 Hybrid RL with Generative Models: Outperformed all methods with the highest sur-plus ($14,280.64), strong efficiency (1,802.60), and realistic budget utilization ($259.99). Its dynamic adaptability and ability to simulate complex scenarios are key strengths [61].\n\nThe results underscore the hybrid framework's superiority in managing high-dimensional, stochas-tic environments while maintaining financial stability and computational efficiency."}, {"title": "5 Applicability to Reinsurance Optimization", "content": "Reinsurance optimization is a complex and dynamic field that requires robust, scalable, and adap-tive models to manage financial risks effectively. The hybrid framework proposed in this study integrates generative modeling and reinforcement learning to address these challenges. This section evaluates the framework's applicability in reinsurance by analyzing its performance across different claim distributions, testing its adaptability through out-of-sample and sensitivity analyses, and examining its resilience under extreme conditions via stress testing and catastrophic event sim-ulations. Additionally, the scalability of the framework is assessed across varying portfolio sizes to understand its limitations and potential for practical implementation in large-scale reinsurance operations.\n\nThe analysis highlights the framework's ability to maintain surplus stability and avoid ruin under typical operational scenarios, while also identifying areas for improvement, particularly in modeling tail events and managing large portfolios. This evaluation provides valuable insights into the framework's real-world applicability and offers directions for future enhancements."}, {"title": "5.1 Analysis of Generative Model Performance Across Distributions", "content": "The performance of the generative claim model was evaluated across Lognormal, Pareto, and com-bined Lognormal-Pareto distributions, focusing on its ability to replicate key statistical proper-ties. Using the Kolmogorov-Smirnov (KS) test and visual comparisons, we highlight the model's strengths in capturing central tendencies and its limitations in modeling tail behavior. Accurate tail modeling is critical for reinsurance applications due to the disproportionate impact of extreme claims [24]."}, {"title": "5.1.1 Overall Model Performance", "content": "The KS test results indicate significant discrepancies between the training and generated datasets, with a KS statistic of 0.6264 and a p-value of 0.0000. The maximum difference location (D) of 14.7174 highlights the model's difficulty in capturing extreme claims, which dominate risk assess-ments in reinsurance. Figure 4 illustrates these discrepancies, showing an underrepresentation of high-severity claims in the generated data. Addressing these gaps is essential for improving the reliability of tail-event modeling."}, {"title": "5.1.2 Lognormal Distribution", "content": "For the Lognormal distribution, the KS statistic of 0.5896 and p-value of 0.0000 reveal signifi-cant differences between the training and generated datasets, particularly in the tail regions. The maximum difference location (D) of 12.0666 emphasizes the model's challenges in replicating the distribution's long-tailed nature. Figure 5 shows that while the central tendencies are accurately reproduced, large claims are underrepresented. Strategies such as tail-prioritized loss functions and data augmentation focusing on rare events may enhance the model's performance [11]."}, {"title": "5.1.3 Pareto Distribution", "content": "For the Pareto distribution, the KS test results show a statistic of 0.6230 with a p-value of 0.0000, highlighting the model's inability to adequately represent the heavy-tailed characteristics of the data. Figure 6 reveals these limitations, particularly for rare, high-severity claims critical in rein-surance applications. Incorporating custom-tailored loss functions and oversampling the tail regions during training could mitigate these deficiencies."}, {"title": "5.1.4 Combined Lognormal and Pareto Distribution", "content": "The combined Lognormal-Pareto distribution provides further insights into the model's limitations. The KS statistic of 0.4438 and a p-value of 0.0000 confirm discrepancies in the tails, as shown in"}, {"title": "5.2 Out-of-Sample Performance and Sensitivity Analysis", "content": "The generative claim model's performance was evaluated through out-of-sample testing, sensitiv-ity analysis, and visualization of results. This comprehensive assessment highlights the model's robustness, its ability to generalize to unseen data, and its limitations in real-world applications.\n\nThe out-of-sample testing revealed a mean surplus of 16, 686.73 with a ruin probability of 0.00%, demonstrating the model's capability to effectively manage surplus over time within a simulated insurance environment. Figure 8 illustrates the surplus dynamics, where the model maintains stability without breaching the ruin threshold of -100. This stability indicates the model's success in balancing premium collection and loss management under varying conditions."}, {"title": "6 Conclusion and Future Work", "content": "The proposed hybrid framework integrating generative models and reinforcement learning repre-sents a significant advancement in the field of reinsurance optimization. By combining Variational Autoencoders (VAEs) [42] for modeling claim distributions with Proximal Policy Optimization (PPO) [61] for dynamically adjusting reinsurance strategies, the framework effectively addresses key challenges in managing high-dimensional and stochastic claim environments. The experimental results demonstrate the framework's robustness under typical operational scenarios, its adaptability to evolving claim distributions, and its scalability across varying portfolio sizes.\n\nThe framework's capability to manage financial stability and minimize ruin probability was evident through out-of-sample testing and sensitivity analyses. It consistently maintained surplus stability under moderate stress conditions and adapted to diverse claim distributions, underscoring its potential for real-world applications [4, 20]. Stress-testing scenarios\u2014including high-frequency claims, pandemic-like conditions, and catastrophic events\u2014highlighted the framework's strengths in handling typical claims and short-term financial shocks. However, its limitations became apparent under sustained catastrophic events, underscoring the need for tailored reinsurance structures and dynamic premium adjustments [5,24].\n\nThe scalability analysis further revealed the framework's challenges in managing larger port-folios. While the model performed robustly for smaller portfolios, its ability to sustain surplus diminished with increasing portfolio sizes due to amplified risk exposure. These findings emphasize the importance of adaptive reinsurance strategies and risk-sharing mechanisms to ensure scalability and financial stability [59].\n\nDespite the promising outcomes, the framework faces limitations in modeling extreme tail events\u2014an essential aspect of reinsurance applications. Discrepancies in tail regions of the gen-erative model outputs suggest the need for refinements. Potential improvements include the de-velopment of custom loss functions that prioritize tail accuracy [33], oversampling techniques, and enhanced latent space representations [11]. Additionally, integrating advanced optimization tech-niques, such as dynamic parameter tuning and multi-agent reinforcement learning, could further enhance the framework's adaptability and resilience [62].\n\nFuture research will focus on addressing these limitations by designing more sophisticated meth-ods for tail modeling and extending the framework to multi-line insurance operations. Incorporat-ing external factors such as market volatility, regulatory dynamics, and macroeconomic conditions could further enhance the framework's real-world applicability [29,51]. Furthermore, deploying and validating the framework on large-scale, real-world insurance datasets in real-time settings will be essential for assessing its scalability and practical relevance [21,60]."}]}