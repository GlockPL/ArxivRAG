{"title": "IE-Bench: Advancing the Measurement of Text-Driven Image Editing for Human Perception Alignment", "authors": ["Shangkun Sun", "Bowen Qu", "Xiaoyu Liang", "Songlin Fan", "Wei Gao"], "abstract": "Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding results different editing methods, and total 3,010 Mean Opinion Scores (MOS) provided by 25 human subjects. Furthermore, we introduce IE-QA, a multi-modality source-aware quality assessment method for text-driven image editing. To the best of our knowledge, IE-Bench offers the first IQA dataset and model tailored for text-driven image editing. Extensive experiments demonstrate IE-QA's superior subjective-alignments on the text-driven image editing task compared with previous metrics. We will make all related data and code available to the public.", "sections": [{"title": "1. Introduction", "content": "Text-driven image editing [1, 2, 28, 40, 45] has attracted significant attention in recent years. However, there is currently no well-established metric for evaluating the results of image editing. Objective metrics such as CLIP score [25], DINO score [23], LPIPS score [42], and SSIM [35] tend to evaluate image quality from a single perspective, such as text-image consistency or the correlation between the pre- and post-editing images. These metrics, however, do not provide an overall evaluation, nor do they align well with human visual perception. Previous studies [15, 19, 37] have shown that these metrics can significantly differ from human judgment in practical applications.\nIn recent years, some metrics aligned with human perception, such as HPS scores [36, 37], Pick score [15], and ImageReward [38], have made effective progress in evaluating text-to-image generation tasks by collecting human visual feedback. However, these methods focus only on individual images and text, which differs from the setting of image editing tasks. Unlike text-driven image generation, text-driven image editing also takes a source image as input. The edited result is expected to differ from the source image, but there is also a certain degree of correspondence. Modeling this relationship is crucial for evaluating the editing result: in some cases, the edited result is expected to retain semantic information related to the original image. If only the edited image output is considered, this aspect would be missed. This is an issue that previous methods have not addressed in depth.\nHowever, modeling this relationship is a challenging task. The connection between source and target images changes dynamically depending on the text context. For example, a stylistic instruction like \"make it a claymation style\" may drastically alter the structure, texture, and lines of the original image, whereas a replacement instruction like \"replace the cat with a dog\" will directly alter the semantic content, and thus a large difference between the source and target images is expected. On the other hand, an instruction like \"remove her earrings\u201d is expected to retain most of the identity information of the original character. Therefore, a multi-modal method that can dynamically model the source-target relationship is urgently needed.\nIn this work, we propose the Text-driven Image Editing Benchmark (IE-Bench) to improve the alignment between evaluation metrics for text-driven image editing and human perception. We first introduce IE-DB, a database containing various source-prompt-target cases and their corresponding Mean Opinion Scores (MOS). We collect diverse real-world, CG, AIGC, and art painting images from different sources, as shown in Figure 2. Following previous"}, {"title": "2. Related Work", "content": "2.1. Image Quality Assessment\nIn recent years, researchers have introduced numerous image quality assessment (IQA) methods [11, 18, 20, 24, 29, 34, 43, 44]. These methods can be categorized into full-reference IQA (FR-IQA) [17, 32, 41] and no-reference IQA (NR-IQA) [20, 34, 43, 44], based on the presence or absence of a reference image during the assessment process. Full-reference methods generally demonstrate higher prediction accuracy than no-reference methods, as a reference image enables the extraction of more effective features. Many classical IQA models initially utilized manually extracted feature-based methods [17, 41]. However, with the rapid advancement of convolutional neural networks, methods that employ deep learning for feature extraction [11, 20, 24, 29, 44] have significantly enhanced performance. [12] is the early adopter of deep convolutional neural networks for no-reference image quality assessment (NRIQA), employing a CNN to derive image quality metrics directly from raw image data instead of relying on traditional hand-crafted features or a reference image. [43] pioneered the use of a deep bilinear convolutional neural network specifically for blind image quality assessment, merging two CNN pathways to separately evaluate synthetic and natural image distortions. Meanwhile, [29] developed a self-adaptive hyper network, which innovatively determines the quality of images with authentic distortions using a methodical three-stage approach: analyzing the content, learning perception rules, and predicting image quality. However, these methods often do not account for the differing source-target relationships caused by changes in multimodal contexts.\n2.2. Metrics for Image Editing\nCurrently, the evaluation methods commonly used in text-driven image editing include several objective metrics [5, 25, 35, 42], as well as some IQA methods [15, 36-38] aligned with human feedback. CLIP-V [25] calculates the cosine similarity between each edited image and the source image, while CLIP-T measures the relationship between the result and the given text prompt. MSE and SSIM [35] represent the variance in pixels and overall structure between the edited image and the source image. FID [5] calculates the Fr\u00e9chet Distance between two images. The DINO score [23] measures semantic consistency and calculates feature variance. However, these individual metrics often assess the editing results only from a single dimension. For instance, they either measure only the source-target relationship or the visual-text connection, without aligning with human subjective perception. PickScore [15] estimates alignment with human preferences via a CLIP-style model fine-tuned on human preference data. ImageReward [38], and HPS scores [36, 37], evaluate natural images from aesthetic and technical distortion perspectives. Despite effective scoring based on human feedback training, these methods do not consider the inherent relationship between edited results and the source image, and some traditional IQA methods like [43] do not model the alignment between text and image. Currently, there is still a lack of an appropriate metric to evaluate edited results based on the source image and the editing prompts.\n2.3. Datasets for Image Editing\nIn previous studies, a common approach to evaluating edited images was to assemble human annotators to conduct subjective preference experiments [9, 22]. However, the results of such subjective experiments are often difficult to reproduce, and there can be significant variability in the data and prompts selected when comparing different methods. Recently, some research [1, 45] has attempted to address this by curating high-quality image editing prompt pairs through diverse data collection and prompt design, aiming to standardize community assessments. Nevertheless, these efforts still face two challenges:\n1. These datasets do not include subjective experimental feedback corresponding to the image data (such as mean opinion scores, MOS). Therefore, when using these datasets, others may still need to rely on objective metrics or conduct new subjective experiments.\n2. The scenarios covered by these datasets could potentially be expanded further.\n2.4. Methods for Image Editing\nPre-trained text-to-image diffusion models [3, 6, 26] have been proven to be highly effective in image editing tasks. Instruction-based image editing [1, 40] requires users to provide an instruction to transform the original image into a new one that conforms to the given instruction. Some methods can achieve this goal without fine-tuning. For example, Prompt-to-Prompt [33] suggests modifying cross-attention maps by comparing the descriptions of the original input with those of the modified version. MasaCtrl [2] transforms the self-attention mechanism within diffusion models into mutual self-attention, allowing the model to query relevant local content and textures from the source image, thereby enhancing consistency. Furthermore, due to the scarcity of paired image-instruction editing datasets, pioneering work InstructPix2Pix [1] introduced a large-scale vision-language dataset created by fine-tuning GPT-3 [4] and Prompt-to-Prompt [33] with stable diffusion. This method further fine-tuned the UNet architecture to enable the model to edit images based on simple instructions. To improve the editing performance of InstructPix2Pix on real-world images, MagicBrush [40] provided a large-scale manually annotated dataset for instruction-guided real image editing. SmartEdit [9] leverages large language models (LLMs) to enhance text understanding during the editing process, while SINE [45] and DAC [28] further boost the model's representation ability for specific samples through test-time fine-tuning. However, these methods typically rely on objective metrics [23, 25] for evaluation, which do not fully align with human subjective perception or only conduct one-off subjective experiments, leading to poor comparability and reproducibility. There remains a lack of comprehensive evaluation metrics that align well with human subjective feedback."}, {"title": "3. Text-driven Image Editing Database", "content": "The collection of IE-DB involves four primary stages: source image collection, prompt selection and execution of image editing methods, and subjective experiments conduction. We will elaborate on each part in subsequent sections with corresponding analyses.\n3.1. Source Image Collection\nIE-DB compiles a rich and varied collection of source images to facilitate a more robust evaluation of image editing quality. In addition to real-world scenes, the dataset incorporates computer-generated imagery, text-driven generated images, and artistic creations. Real-world scenes, given their widespread application, constitute the largest portion of the dataset. Unlike previous efforts, IE-DB refrains from random sampling of images from large datasets to avoid complications related to copyright, watermarks, and image resolution. Instead, to cover as many different content subjects, action categories, and scenarios as possible, IE-DB manually selected 301 images from four datasets: ADE [46], WIKIArt [31], COCO [21], ReasonEdit [9], and other Internet sources. Instead of sampling from large-scale dataset, We first confirmed the data sources (CG, AIGC, real-world datasets, and artist-created works), then iterated through each sample to tag attributes such as \"[Landscape/Object/Animal/Human]\u201d\" and \"[action type]\". Samples of the same type (e.g., Landscape/Object/Animal or Human action) or with insufficient resolution were skipped until the dataset was fully reviewed or the relevant categories were sufficiently populated. Smaller datasets were prioritized for review, and additional samples were drawn from larger datasets. Finally, suitable images were selected from the internet for supplementation. For instance, while many datasets included landscapes like grasslands and snowy mountains, scenes such as auroras, lava flows, and lightning were less represented. Similarly, although current datasets provide a rich variety of action categories, there remains a limited number of actions with significantly varied patterns. Ultimately, we collected over 100 human action types, as illustrated in the Figure 2. For simplicity, the remaining categories were grouped under \"others,\" with detailed explanations provided in the supplementary materials. Ultimately, a diverse set of 301 source images was gathered, encompassing a wide range of content. The origins, contents, and category distributions of these images are illustrated in Figure 2. Each image was resized to ensure the shorter side was 512 pixels, preserving the original aspect ratios.\n3.2. Prompt Selection\nBuilding upon previous research [8], we categorize image editing prompts into three primary types: (1) Style editing, involving adjustments to color, texture, or overall ambiance. (2) Semantic editing, which encompasses background modifications and localized edits such as adding, replacing, or removing specific objects. (3) Structural editing, including changes to object size, pose, or motion. To guarantee the variety and specificity of the prompts, we created tailored prompts for each image, with the distribution detailed in Figure 3.\n3.3. Image Editing\nWe selected five diverse image editing techniques. To achieve a balanced quality distribution among the edited images, our selection includes both state-of-the-art models and earlier methods. Additionally, we incorporated approaches based on various foundational models, from SD 1-4 to SD2-1, to enhance the diversity of the editing outcomes. Moreover, to diversify the edited content, we included both zero-shot methods and those that necessitate fine-tuning. We also opted for models that employ different editing paradigms, such as Instruct-P2P [1], Prompt-to-prompt [33], and MasaCtrl [2], among others. The specifics of these methods are outlined in Table 1."}, {"title": "3.4. Subjective Study", "content": "In accordance with ITU standards [27], subjective experiments necessitate a minimum of 15 participants to ensure result variance remains within acceptable limits. For this study, we enlisted 25 participants with diverse professional backgrounds. These individuals were tasked with evaluating edited images based on text-image consistency, sourcetarget fidelity, and overall quality, relying on their subjective judgments.\nAll participants were over 18 years old, held at least an undergraduate degree, and had varied professional experiences, including business, engineering, science, and law, ensuring their ability to make independent judgments. Prior to the experiment, participants underwent in-person training, where they were shown examples of high-quality and poor edits not included in the dataset. During the assessment, each participant evaluated all image samples, with a mandatory 5-minute break every 15 minutes to minimize fatigue. Our procedures were consistent with those in previous subjective studies, such as [13, 16, 18].\nText-image consistency was defined as the extent to which the edited content aligns with the provided prompt. Source-target fidelity measures how well the edited image retains a connection to the original. Participants rated these aspects on a 1 to 10 scale during their evaluations.\nConsistent with prior research [16, 30, 44], we employed Z-score normalization for the raw MOS values. Following the collection of raw Mean Opinion Scores (MOS), we applied Z-score normalization to account for inter-subject variability, using the formula:\n$Z_{m,i} = \\frac{X_{m,i} \u2013 \\mu(X_i)}{\\sigma(X_i)},$ (1)\nwhere $X_{m,i}$ denotes the raw MOS and $Z_{m,i}$ the Z-score for the m-th image evaluated by the i-th participant. Here, $\\mu(\\cdot)$ and $\\sigma(\\cdot)$ represent the mean and standard deviation, respectively, and $X_i$ is the set of all MOS scores from participant i. We also utilized the outlier filtering method from BT.500 [10]. Figure 4 illustrates the distinction between raw and normalized scores."}, {"title": "3.5. Statistes Analysis", "content": "We performed a comprehensive analysis of the IE-DB dataset and examined the evaluation scores for the image editing outcomes produced by each model on the gathered images, as depicted in Fig. 4. The models, labeled from 1 to 5, are MagicBrush [40], Instruct-Pix2Pix [1], Inf-Edit [39], MasaCtrl [2], and Prompt-to-Prompt [33]. Our observations reveal that the median scores for these models range between 4 and 8, with most scores clustering around 6. Notably, some edits received scores as low as 2, while others achieved scores above 9. This variability suggests that there is significant potential for enhancing the models' performance. The observed limitations may stem from the models' inadequate comprehension of prompts and their limited capability to capture fine visual details."}, {"title": "4. Method", "content": "4.1. Text-driven Image Editing Quality Assessment\nUtilizing the IE-DB, we developed the IE Quality Assessment (IE-QA) network, designed to mirror human subjective evaluations in assessing the quality of edited images, as depicted in Figure 5. This network evaluates edited images based on three key aspects: image-text alignment, source-target relationship, and visual quality.\n4.2. image-Text Alignment\nConventional image evaluation techniques often overlook the alignment between image content and the associated text prompt, a limitation that hampers their effectiveness in assessing AI-generated imagery. To address this, we integrated a text branch into our model, inspired by a successful IQA approach, to capture the alignment between the generated content and the corresponding text prompt. This can be expressed as:\n$e_{bv} = F_{bv}(I^*),$ (2)\n$e_{bt} = F_{bt}(p, F_{ca}(e_{bv}))$ (3)\nHere, $F_{bv}$ and $F_{bt}$ represent the CLIP visual and text encoders, respectively. p denotes the prompt, and the visual feature $e_{bu}$ is interacted with the text encoder through cross-attention, denoted as $F_{ca}$.\n4.3. Source-Target Relationship\nAssessing the consistency between original and edited images is inherently complex due to their latent connections and pronounced pixel-level discrepancies. To tackle this, we developed a multi-modal feature extractor that projects both the original and edited images into a latent space. By concatenating these features and processing them through a feed-forward network, we obtain a reliable measure of their relevance. This process is formulated as:\n$f = F(I),$ (4)\n$f^* = F^*(I^*),$ (5)\n$o_s = H_s(Concat(f, f^*))$ (6)\nHere, F and $F^*$ represent the pre-trained visual encoders for the original and edited images, respectively. The output vector $o_s$ captures the relevance between the source and edited images, with $H_s$ denoting the lightweight feed-forward network. In practice, we experimented with various backbones and ultimately selected the CLIP visual encoder for its effectiveness.\n4.4. Visual Quality\nTo evaluate the quality of the edited image, we adopt perspectives from the top-performing IP-IQA method [24], which assesses images based on aesthetics and technical distortion. Initially, the pre-trained visual encoder's parameters are frozen, and only the regression head is fine-tuned. In the subsequent stage, all parameters of the visual quality branch are updated to refine the assessment.\n4.5. Supervision\nFollowing previous research, we employ a combined loss function comprising the Pearson Linear Correlation Coefficient (PLCC) loss and rank loss, weighted by a, to train the entire network. This comprehensive loss function is formulated as:\n$L = L_{plcc} + \\alpha L_{rank},$ (7)\nwhere a is set to 0.3 in practice."}, {"title": "5. Experiments", "content": "5.1. Implementation Details\nAll models were developed using PyTorch and trained on NVIDIA V100 GPUs. Following the 10-fold cross-validation approach used in previous studies [24, 29, 43], all models were trained on the IE DB dataset with an initial learning rate of 1e-3, a batch size of 8, and a total of 60 epochs. Building upon the approach outlined in IP-IQA [24], we initially fine-tuned the model head using linear probing for 40 epochs, followed by training all model parameters for an additional 20 epochs. During training, the Adam [14] optimizer was employed in conjunction with a cosine learning rate scheduler. In line with previous research, the base visual-text interaction block utilizes attention pooling, leveraging the CLIP visual and text encoders.\n5.2. Evaluation Metrics\nConsistent with prior studies [16, 24, 43], we use four metrics as our evaluation metrics: Spearman's Rank Order Correlation Coefficient (SROCC), Pearson's Linear Correlation Coefficient (PLCC), Kendall rank-order correlation coefficient (KRCC).\n5.3. Quantitative Results\nOur results were compared against advanced evaluation metrics in image editing, including objective methods [5, 15, 25, 36, 37] and human-aligned image Quality Assessment (VQA) methods [24, 29, 43], as shown in Table 2. From these, it can be seen that compared to previous traditional VQA methods [24, 29, 43] and commonly used objective metrics, IE-QA outperforms previous traditional VQA methods and commonly used objective metrics in aligning with human subjective perception, achieving improvements of 10.46%, 9.02%, 10.19%, 11.08% in SROCC, PLCC, KLCC, and RMSE, respectively. Compared to the baseline, IE shows significant performance gains, with increases 0.1396, 0.1435, 0.096, and 0.243 in SROCC, PLCC, KLCC, and RMSE, respectively. Among the 0-shot objective measurements, performance tends to be lower compared to learning-based methods, a trend observed in previous studies such as [5, 23]. This is likely due to the difficulty in aligning objective quantitative metrics with human perceptions.\nAmong the 0-shot methods, HPSv2 [36], which is pre-trained with a reward model incorporating human feedback, achieved the highest overall performance. CLIP-V [25] showed comparable SROCC and KRCC metrics to CLIP-T but failed to align well with human subjective perception, resulting in lower scores."}, {"title": "5.4. Qualitative Results", "content": "We further conducted a qualitative comparison for different score levels in IE, as illustrated in Figure 6, where we present several image examples in IE DB with varying predicted sores."}, {"title": "5.5. Ablation Study", "content": "To further validate the results of each module in IE QA, we performed detailed ablation experiments on each module, as shown in Table 3. All results were obtained through 10-fold validation training on the IE DB with the same experimental hyper-parameter design. The settings we adopted in our final model are underlined. We first explore different ways of image-text alignment. Here, we experimented with CLIP and fine-tuned the regression head composed of Feed-Forward Networks, which learn the alignment from the cosine similarity of its visual and text Backbone outputs. Experiments demonstrate that, the text-visual branch is of importance to enhance network performance. Furthermore, we explored how to effectively model the relevance between the source image and the edited image. We first validate the effectiveness of applying source-target relationship modeling. Additionally, we explored effective ways to fuse features from the source image and the edited image, as presented in Table 3. \"Attention\" denotes the mutli-head cross-attention. We found that concatenation along the dimension is a simple and effective design for the assessment. We further ablate the effect of additional parameter, which demonstrates the improvements are not from more parameters. In specific, the introduction of the new source branch brought additional parameters. We need to demonstrate that the model's improvement is due to the correct design (i.e., establishing a connection between the destination and source images) rather than merely from the accumulation of these additional parameters. Therefore, we conducted an experiment where we removed the source branch and instead widened the structure of the destination branch to match the parameters added by including the source branch. This adjustment helps eliminate the influence of additional parameters introduced by the source branch. During these experiments, we could learn that the design of image-text similarity and the focus on the source-target image relationship modeling is of importance to the overall performance."}, {"title": "6. Conclusion", "content": "In this research, we present IE-Bench, a comprehensive framework for the evaluation of text-driven image editing techniques. This framework encompasses two key components: IE-DB and IE-QA. IE-DB is a carefully curated, subjectively aligned dataset specifically designed to address the unique challenges in assessing text-driven image editing. It includes an extensive array of video content, meticulously categorized editing prompts, and a diverse collection of edited images generated by various state-of-the-art image editing models. Notably, IE-DB stands out as the first image quality assessment (IQA) dataset that is purpose-built for evaluating the outcomes of text-driven image editing, to the best of our knowledge. Extensive experiments demonstrate the effectiveness of IE-QA. Unlike conventional metrics, IE-QA is designed to better reflect human judgment and perception, offering superior alignment with subjective evaluations of image quality and editing efficacy."}]}