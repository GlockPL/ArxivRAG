{"title": "CONTINUAL LEARNING WITH EMBEDDING LAYER SURGERY AND TASK-WISE BEAM SEARCH USING WHISPER", "authors": ["Chin Yuen Kwok", "Jia Qi Yip", "Eng Siong Chng"], "abstract": "Current Multilingual ASR models only support a fraction of the world's languages. Continual Learning (CL) aims to tackle this problem by adding new languages to pre-trained models while avoiding the loss of performance on existing languages, also known as Catastrophic Forgetting (CF). However, existing CL methods overlook the adaptation of the token embedding lookup table at the decoder, despite its significant contribution to CF. We propose Embedding Layer Surgery where separate copies of the token embeddings are created for each new languages, and one of the copies is selected to replace the old languages embeddings when transcribing the corresponding new language. Unfortunately, this approach means LID errors also cause incorrect ASR embedding selection. Our Task-wise Beam Search allows self-correction for such mistakes. By adapting Whisper to 10 hours of data for each of 10 unseen languages from Common Voice, results show that our method reduces the Average WER (AWER) of pre-trained languages from 14.2% to 11.9% compared with Experience Replay, without compromising the AWER of the unseen languages.", "sections": [{"title": "1. INTRODUCTION", "content": "Recent advancements in training speech models involve utilizing millions of hours of multilingual ASR and translation labeled data, leading to the development of models supporting Massively Multilingual ASR (MMASR) which can transcribe over 50 languages [1]. Additionally, newer models such as Whisper [2] and MMS [3] are language-agnostic [4], where a single model is able to perform both language identification (LID) and ASR seamlessly so that users do not need to manually specify the language they are transcribing. Nevertheless, even the most diverse MMASR models are not able to support the vast number of languages spoken globally [5], and the additional requirements for language-agnostic MMASR increases the difficulty of adding support for new languages after the initial pre-training.\nTo expand ASR support to additional languages, a naive solution is to adapt MMASR using only the new datasets containing the new languages. However, this causes catastrophic forgetting (CF) [6], where a model has degraded ASR performance on previously learned languages after adapting to new ones. Therefore, this necessitates Continual Learning (CL), a class of adaptation methods that aims to mitigate CF. There are numerous CL methods for model adaptation which includes Prototype-based [7], Regularization-based [8-10], Replay-based [11,12], Optimization-based [13] and Dynamic-architecture-based [14] methods.\nSpecifically in the audio domain, prototype-based [15], replay-based [16], and regularization-based [17] methods are examined. In addition, model averaging [17, 18] and layer-wise regularization [19] approaches have also shown promise in CL for audio. However, limited focus has been given to CL for MMASR. While some works has shown success in mitigating CF by incremental fine-tuning [20], training a map-"}, {"title": "2. PROPOSED METHOD", "content": "We use Whisper [2] as the pre-trained MMASR model and adapt it to the ASR and LID datasets of the new languages. LID adaptation is needed as it allows the model to transcribe in a language-agnostic setting [4], where instead of manually prompting Whisper to control the transcribed language, the model can use LID to automatically determine what language it should be transcribing.\nTo prevent catastrophic forgetting (CF) on previously learnt (old) languages while adapting to new languages, Experience Replay (ER) is used where a subset of the datasets containing the old languages are mixed with the new language adaptation dataset so the model can rehearse on old languages ASR tasks.\nAdditionally, we propose to further mitigate CF by using a separate text embedding lookup table [24] for each new languages and splitting the lookup table into two parts as shown in the bottom-right part of Figure 1.\nFinally, we propose task-wise beam search as shown in Figure 3 to retain a list of candidates of different languages during beam search to enhance the ASR and LID performance in the langauge-agnostic setting."}, {"title": "2.1. Whisper Model for Language-agnostic MMASR", "content": "Whisper [2] is used as the pre-trained MMASR model for adaptation. It supports 75 languages and also supports LID. It uses the Transformer attention-based encoder-decoder architecture and decodes in an auto-regressive manner. We select this model because while [25] shows that Whisper performs similarly with XLS-R [26] in terms of averaging both the seen and unseen languages WER, [23] shows that Whisper surpasses WavLM [27] overall in MMASR CL and [28] shows that Whisper performs better than wav2vec 2.0 models [29] and WavLM [27] for LID. The Whisper decoder is preferred over the Large Language Models (LLM) based decoder [30] for ASR as the latter does not support LID to the best of our knowledge."}, {"title": "2.2. Experience Replay", "content": "A naive approach to mitigate CF is to retrain a model from scratch on a combination of all the previously learnt tasks' datasets and the new task's dataset. However, this is computationally expensive and it is not always possible to acquire all"}, {"title": "2.3. Separate Token Embedding", "content": "The adaptation of the token embedding lookup table [24] at the decoder is prone to forgetting [31]. This is because Whisper shares its tokens between languages. When the model is adapted to new languages, the token embeddings [32] are also updated with word semantic information of the new language. This may overwrite the old languages semantics and cause forgetting.\nSpecifically, a token embedding lookup table is a matrix \\(A \\in \\mathbb{R}^{E \\times U}\\) where E is the embedding dimension and U is the number of tokens in the model's vocabulary. Given that a Transformer decoder takes in the embeddings of the K previously decoded tokens to predict the next token and \\(V = [v_1, ..., v_K]\\) contains K columns of U-dimension one-hot vector [33] of the decoded tokens. Then the token embeddings input \\(Z \\in \\mathbb{R}^{E \\times K}\\) feeding to the Transformer decoder is:\n\\(Z = A X V\\)\nTo mitigate forgetting in the token embeddings, we propose to create a separate copy of the token embeddings for each new language as shown in Figure 1B and keep the original token embeddings for the old languages, such that the new languages embeddings can be adapted without modifying the original ones. The token embeddings for a new langauge are stored in a separate lookup table \\(\\hat{A} \\in \\mathbb{R}^{E \\times J}\\) where J < U is the number of tokens used by the new language. The proportion of Whisper's tokens used by each new language is shown in Table 1"}, {"title": "2.4. Language-agnostic Dynamic Architecture", "content": "Although using language-specific token embeddings reduces forgetting, it is not language-agnostic. As shown in Figure 1A, the ASR decoder takes in a sequence of token embeddings to predict the next token. Figure 1B shows that a different token embedding lookup table is used for different languges. We further propose to split the lookup table into a language-shared Special Token (ST) part and a language-specific vocabulary part as shown in 1C. This is becuase the top-left part of Figure 2 shows that normally if a language-specific text embedding layer is used, manual selection of the language-specific lookup table is required before transcription and the MMASR system is not language-agnostic. The right part of the figure shows that if the lookup table is split, the model can first perform LID using the language-shared ST part and then automatically select the language-specific vocabulary part for transcription. As such, the decoding is language-agnostic as the model can infer the language label by itself."}, {"title": "2.5. Task-wise beam search", "content": "The language-agnostic ASR performance highly relies on the accuracy of LID [28, 34-36] as wrong LID predictions may lead to transcription in the wrong language. To improve the LID and the language-agnostic ASR performance, we propose task-wise beam search, where we retain a list of candidates of different languages during beam search [37] to improve its diversity [38].\nSpecifically as shown in Figure 3, an attempt to decode an audio in three languages is shown in decoding paths (DP) 1-3 respectively. First, the model performs LID by generating an LID token. Then DP1 is pruned as the LID token \u201cZH\u201d gives a low score of 0.1. Only languages DE and EN, which have the top N = 2 LID scores are kept and transcribed in DP2 and DP3. Next, the log probabilities of the transcribed tokens are summed to produce an ASR score for each language. Finally, the hypothesis in DP 3 is preferred over DP2 as its ASR score is higher.\nIn addition, we empirically find that Whisper may sometimes output blank hypothesis or output hypothesis in a different language than the one specified in the model's prompt. To increase the stability of our decoding method, it is important that we disable the task-wise beam search for a specific audio if we find that a decoding path has fewer than \\(M_{len}\\) words, or has more than \\(M_{overlap}\\) overlapped words with other paths."}, {"title": "3. EXPERIMENT AND DISCUSSION", "content": "We implement our CL methods based on the popular Speech-Brain [39] toolkit and CL-MASR [23].\nFollowing previous works [21, 23], we evaluate our method on a subset of the widely used large-scale Common Voice dataset [40]. We follow CL-MASR [23] to extract the data subsets. They consists of ten languages unseen by Whisper and ten seen languages. Each language contains 10 hours of data for training, 1 for validation, and 1 for testing. For Chinese, traditional Chinese characters are converted to simplified Chinese. We adapt small and large-v2 variants of Whisper in two CL settings: 1) Adapt to one unseen language and test forgetting on one seen language, 2) Adapt to ten unseen languages sequentially and test forgetting on ten seen languages, and a maximum of six days are required utilizing an NVIDIA A40 GPU.\nFor each unseen language, we adapt the 2 variants of Whisper for 2 epochs with a train batch size of 4. Only the weights of the Whisper decoder is updated and the encoder is frozen [23]. For ER, the replay data size is one hour for each new language. We use AdamW [41] as the optimizer and a variant\u00b9 of the ReduceLROnPlateau\u00b2 learning rate (LR) scheduler. Validation is done at an interval of 1/32 epoch. We sweep through the hyper-parameters to tune them for all methods.\nWe refer to our methods as 1) ER-E-B, which is ER with separate token embeddings and task-wise beam search, and 2) ER-E, which is ER with separate token embeddings only. We set N = 2, \\(M_{len}\\) = 5 and \\(M_{overlap}\\) = 3."}, {"title": "3.2. Results and Discussion", "content": "We present the results for both whisper-small and whisper-large-v2 models. Table 2 shows the result of adapting Esperanto and Interlingua for whisper-small. Full fine-tuning (FT) can improve WER for the target languages, but incurs CF. In contrast, all Continual Learning (CL) baseline methods significantly reduce CF, reducing AWER by 27.2-66.9% relatively compared with FT. Among these baseline methods, ER performs better in overall performance, though EWC and MAS has better results for English and Esperanto for language-aware results. However, AVG performs less well relative to ER, as task-specific layers are not used as in [22].\nResults show that our methods outperform all CL baselines and improve upon FT result by 61.7 \u2013 69.6%. Forgetting is further mitigated compared with all CL baselines while our method can maintain similar WER for the newly adapted languages. Furthermore, our ER-E-B method has almost no forgetting in English and German even in the more difficult language-agnostic setting.\nLastly, we adapt whisper-large-v2 sequentially to the 10 new languages of varying difficulties and plot the results in Figure 4. Our method ER-E-B outperforms the best CL baseline ER and reduces the AWER of pretrained languages from 14.2% to 11.9% without compromising the AWER of new languages. We do not compare with dynamic-architecture-based methods as they are not language-agnostic."}, {"title": "3.3. Ablation study", "content": "We further perform ablation studies for Task-wise Beam Search and Separate Token Embedding. For the experiments, we adapt whisper-small to a new language Esperanto (eo) and test forgetting on a pretrained language English (en). Table 3 shows that for Task-wise Beam Search, it consistently improves performance for ER and ER-E methods in the language-agnostic setting by up to a relative 8.3% AWER."}, {"title": "4. CONCLUSION", "content": "To conclude, we present ER-E-B, a CL method that outperforms ER methods in mitigating CF and ablation study has shown the effectiveness of our proposed language-agnostic dynamic architecture, task-wise beam search and separate token embedding."}]}