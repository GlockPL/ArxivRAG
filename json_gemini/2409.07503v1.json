{"title": "AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs", "authors": ["Lijia Lv", "Weigang Zhang", "Xuehai Tang", "Jie Wen", "Feng Liu", "Jizhong Han", "Songlin Hu"], "abstract": "Jailbreak vulnerabilities in Large Language Models (LLMs) refer to methods that extract malicious content from the model by carefully crafting prompts or suffixes, which has garnered significant attention from the research community. However, traditional attack methods, which primarily focus on the semantic level, are easily detected by the model. These methods overlook the difference in the model's alignment protection capabilities at different output stages. To address this issue, we propose an adaptive position pre-fill jailbreak attack approach for executing jailbreak attacks on LLMs. Our method leverages the model's instruction-following capabilities to first output pre-filled safe content, then exploits its narrative-shifting abilities to generate harmful content. Extensive black-box experiments demonstrate our method can improve the attack success rate by 47% on the widely recognized secure model (Llama2) compared to existing approaches.", "sections": [{"title": "I. INTRODUCTION", "content": "Significant advances in Large Language Models (LLMs), such as OpenAI's GPT series [1] and Meta's Llama series [2], have marked a leap forward in artificial intelligence. However, the integration of these models into real-world applications, particularly in the realm of content generation, where numerous chatbots and applications such as RAG have emerged, still carries the risk of generating various unsafe content [3], including illegal activities, bias, and discrimination. This problem severely hinders the development of LLMs.\nLLM jailbreak attacks are currently a primary means of discovering security vulnerabilities in LLMs. The essence of these attacks lies in the distributional shift of data within the model's internal hidden states [13], which allows attackers to alter the model's output through specific inputs. However, existing research has mainly focused on semantic-level attack methods, such as gradient optimization [4], [5], jailbreak prompt search [6], [7], and red team search [8], [9] algorithms. These methods are easily detected by the model because they focus primarily on the semantic aspects and do not adequately account for the potential difference in the model's protection capabilities across different output positions. This difference presents us with a new and highly potential avenue for attack."}, {"title": "II. THE DESIGN OF ADAPPA", "content": "Overview. Figure 3 shows the overview of AdaPPA, which includes three sequential steps: \u2460 firstly, conducting low-rank training on the pre-filling content generation model with some pre-trained safe and harmful data, as well as on the question rewriting model with artificially designed partial rewrites, \u2461 and then inputting the question into both the question rewriting model and the pre-filling content generation model to obtain the rewritten question and the pre-filled content, which are then combined in various ways to attack the target model, \u2462 finally, evaluating the results to ascertain whether the attack was successful."}, {"title": "A. Problem Rewrite", "content": "This section corresponds to the Problem Rewrite module in Figure 3, which aims to enhance the effectiveness of the attack by transforming the similarity of the original question. Existing research [8], [9] has shown that original harmful questions are often easily identified and defended against by the model; hence, we employ similarity transformation techniques to bolster the attack capability. Specifically, we designed a prompt to direct the model's focus on two aspects of rewriting: first, generating a statement describing a particular security research background, as illustrated in the red section of Figure 4; second, requiring the model to provide relevant attack methods, as illustrated in the blue section of Figure 4. The success criteria for rewriting are based on the cosine similarity of TF-IDF [12], where the similarity S(q,r) between the original problem and the rewritten problem is quantified by calculating the cosine similarity of their TF-IDF vectors Vq and Vr, with q and r representing the original problem and the rewritten problem, respectively. The specific formula is:\n$S(q,r) = \\frac{V_qV_r}{||V_q||||V_r||}$\nThe measure of similarity itself is a hyper-parameter, and through empirical experimentation, we have determined a threshold of 0.6 after several trials. This threshold indicates that if the similarity calculation using the above formula yields a result not less than 0.6, we consider the rewritten question to be similar to the original question. After obtaining some high-quality rewritten problems, we fine-tuned the Vicuna model [14] through low-rank training [15], targeting the attention mechanism's q and v components, with rank r = 256. Ultimately, a rewriting model specifically tailored for problem rewriting is produced."}, {"title": "B. Pre-fill Generation", "content": "This section corresponds to the Content Segmentation module and the Pre-fill Generation Model module, which aim to adaptively generate pre-filled content for the model, including both safe and harmful responses. In this method, the filler content is categorized into two types: safe filler and harmful filler. The generation of safe fillers can be achieved by designing a universal safe answer $R_{generic}^{safe}$ or by inputting the original question Q into the Llama2 model to generate targeted safe answers $R_{model}^{safe} = Llama2(Q)$, which are then segmented into different texts based on their length. The generation of harmful fillers primarily relies on the autoregressive properties of the large model. While it is feasible to design a universal malicious response $R_{generic}^{harmful}$, its effectiveness is unsatisfactory and thus discarded. Instead, we embed the original question Q into the prompt of the Llama2 model and predefine a positive harmful response $R^{harm}$ in the response section to generate the harmful response $R_{model}^{harm} = Llama2(INST Q / INSTR)$, where the model relies on the preceding token to continue writing. After obtaining the data, we conduct low-rank training on the pre-fill generation model, which ultimately enables the adaptive generation of both safe and harmful responses. We then perform longitudinal segmentation on the pre-filled content, using the specific segmentation method detailed in the following subsection."}, {"title": "C. Prompt Combination", "content": "This section corresponds to the Prompt Combination module, which aims to combine different pre-filled contents to identify the optimal attack prompt. According to the observational experiments depicted in Figure 1, the combination of different lengths and pre-filled contents significantly impacts the effectiveness of the attack. Our analysis indicates that excessively long pre-filled content may be detected by the model as malicious, thereby triggering the model's adaptation mechanism. Consequently, we have designed various combinations of attack prompts to optimize the model's attack power. However, simulating all possible combinations of different lengths is impractical. Therefore, based on experimental experience, we identify six representative combinations:\n1) $X + 1/3 R_{model}^{harm}$\n2) $X + 2/3 R_{model}^{harm}$\n3) $X +  R_{model}^{harm}$\n4) $X + R_{generic}^{safe} + T + 1/3 R_{model}^{harm}$\n5) $X+ R_{generic}^{safe} + T + 2/3 R_{model}^{harm}$\n6) $X + R_{generic}^{safe} + T + R_{model}^{harm}$\nwhere X represents the original problem or the rewritten problem, $R_{model}^{harm}$ denotes model-generated harmful fillers, $R_{generic}^{safe}$ stands for generic safe responses, and T represents transitional phrases. We conduct experiments to determine the optimal combination of prompts. However, the above combinations are based on heuristic rules to identify the best combinations. To enhance performance, we integrate successful cases into the training database for adversarial fine-tuning of the model. This approach enhances the model's attack capability, adaptability, and overall performance."}, {"title": "D. Attack and Judge.", "content": "After generating attack requests, these are input into the target model, strictly adhering to the fine-tuning protocol specified for each model to ensure optimal defense capabilities, thus simulating a scenario akin to a proprietary black-box model as encountered in official deployments. Upon receiving the model's responses, we employ the widely utilized automated analysis tool, Llama Guard 2 [16], for an initial assessment. This assessment is subsequently augmented by a manual review process to validate the outcomes of the automated analysis, thereby confirming the precision and reliability of the results."}, {"title": "III. EVALUATION", "content": "Experimental Setup"}, {"title": "A. Experimental Setup", "content": "Model Selection. In this study, we selected ten different models and their variants, including ChatGLM3-6B [11], Vicuna-7B [14], Vicuna-13B [14], Llama2-7B [2], Llama2-13B [2], Llama3-8B [17], GPT-40-Mini [1], and GPT-40 [1]. During the model inference process, we configured several critical parameters to optimize the effectiveness of the attack. First, we strictly adhered to the instruction formats provided by the official models to fully exploit their defensive capabilities and closely mimic the performance of the official APIs. Second, we disabled the sampling feature of the models and used a greedy generation strategy to ensure the stability and reproducibility of each attack.\nSelection of Discriminators. The ranking of results is a crucial stage in the entire research process. We utilized three automated discriminators, including Llama-Guard-2 [16], OpenAI's Moderation [18], and Google's Perspective API [19]. Once the automated discrimination is complete, the results undergo a secondary manual review to ensure their accuracy and reliability.\nDataset Selection. We utilized the PKU BeaverTails dataset [20], filtering 14,000 entries for training and observation across various risk scenarios. Additionally, we used the AdvBench [4] dataset for testing our methodology."}, {"title": "IV. RELATED WORK", "content": "With the widespread application of LLMs in practical scenarios, research on jailbreak attacks has surged, which can be broadly categorized into three types [9]: optimization-based, minority language-based, and distribution-based methods.\nOptimization-based techniques are the most traditional and among the earliest developed, primarily comprising gradient optimization methods [4], [21], which manipulate the calculation of parameters during the model inference process to steer the model towards outputting harmful content. Genetic algorithm-based methods [7], [22] employ mutation and selection to identify effective attack prompts. Edit-based methods [23] affect model tuning by refining and enhancing adversarial prompts through pre-trained models.\nMinority language-based attacks primarily capitalize on the cognitive deficiencies of models, such as cryptography [24] and translation into less common languages [25], [26], to boost the success rate of jailbreak attacks. Distribution-based methods primarily encompass jailbreak templates [6], [27] and contextual examples [13], [28], [29], enabling models to acquire harmful knowledge from prompts and consequently generate harmful information."}, {"title": "V. CONCLUSION", "content": "This paper introduces a method for jailbreaking LLMs to reveal security vulnerabilities. It uses the model's instruction-following to generate pre-filled content, then leverages the model's narrative shifting to output malicious content. Experiments show that this adaptively improves the attack model's effectiveness. Applied to black-box models, it outperforms baseline methods, proving effective for uncovering LLM vulnerabilities."}, {"title": "VI. ACKNOWLEDGMENT", "content": "This paper utilizes ChatGPT for checking unclear expressions and spelling errors in the text, as well as for writing some simple functions to save human effort, with no other use of AI."}]}