{"title": "XGEN-MM-VID (BLIP-3-VIDEO): YOU ONLY NEED\n32 TOKENS TO REPRESENT A VIDEO EVEN IN VLMS", "authors": ["Michael S. Ryoo", "Honglu Zhou", "Shrikant Kendre", "Can Qin", "Le Xue", "Manli Shu", "Silvio Savarese", "Ran Xu", "Caiming Xiong", "Juan Carlos Niebles"], "abstract": "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables BLIP-\n3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token Turing\nMachines. We experimentally confirm that BLIP-3-Video obtains video question-\nanswering accuracies comparable to much larger state-of-the-art models (e.g.,\n34B), while being much smaller (i.e., 4B) and more efficient by using fewer visual\ntokens.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Vision-Language Models (VLMs), benefiting from large-scale image-text training, have been\ndominating the field of computer vision. Recently, open-source VLMs are obtaining strong results,\ndespite having much smaller size than the commercial models (e.g., 4B vs. Trillions).\nFurther, in addition to such VLMs trained with images, VLMs for videos are becoming increas-ingly popular. The key component in a VLM for videos is the temporal abstraction of tokens\nover multiple frames. Models like Video-ChatGPT (Maaz et al., 2024) and PLLaVA (Xu et al.,\n2024a) rely on a simple spatial/temporal pooling on top of image frame-level tokens to repre-sent the entire video. Some models rely on a separate video encoder to capture temporal infor-mation in videos (Lin et al., 2023). Similarly, some models use additional convolutional lay-ers (or Transformer layers) over frames to reduce their representation size (e.g., Video-LLaMA\n(Zhang et al., 2023), Kangaroo (Liu et al., 2024)). Approaches that simply collect all the vi-sual tokens from all the frames (e.g., MiniGPT4-video (Ataallah et al., 2024), LLaVA-NeXT (Li\net al., 2024b), Tarsier (Wang et al., 2024a) and LLaVA-OneVision (Wang et al., 2024a)) also have\nbeen very popular recently, as they allow capturing all the details from the frame-level tokens.\nHowever, this often makes the number\nof tokens for video to be very huge\n(e.g., thousands even for 8 frames).\nSuch large number of video tokens\ncould be critical for longer videos as\nthe LLM computation is quadratic to\nthe number of total tokens.\nIn this paper, we introduce xGen-MM-Vid (BLIP-3-Video), which is an effi-cient compact vision-language model\nwith an explicit temporal encoder, de-signed particularly for videos. BLIP-3-Video particularly focuses on incor-"}, {"title": "2 BLIP-3-VIDEO", "content": "porating a learnable 'temporal encoder' within it. We explore different types of temporal encoder,\nand demonstrate that the model can abstract each video into much fewer visual tokens (e.g., 16) while\nbeing successful in open-ended question-answering and captioning tasks. We include a space-time\nattentional pooling as well as a sequential model as our temporal encoder, relying on token operations\nto iteratively abstract a series of frame-level tokens into a learnable memory.\nThere has been prior work investigating the role of pooling (Jin et al., 2024), convolutions, and cross\nattention layers (Zhang et al., 2023; Liu et al., 2024; Li et al., 2024c), but study on full space-time\nattentional pooling or sequential model to this extent has been limited in the past. Our objective\nin this paper is to provide a fundamental alternative to more brute-force way of collecting all the\nvisual tokens which have been increasing popular recently. We experimentally confirm that 16 ~ 32\nvideo tokens abstracted by the temporal encoder is often sufficient to represent the entire video for\nquestion-answering (Figure 1).\n2.1 MODEL ARCHITECTURE\nWe build the BLIP-3-Video model based on the image-based vision-language model (VLM), BLIP-3\n(Xue et al., 2024).\nThe model architecture is composed of the following four components: (1) the vision encoder (ViT)\ntaking each frame input, (2) the frame-level tokenizer to reduce the number of tokens, (3) the temporal\nencoder to build video-level token representations, and (4) the autoregressive LLM generating output\ntext captions based on such video tokens and text prompt tokens. Figure 2 shows an overview.\nFirst, we apply a pretrained SigLIP as the vision encoder, designed to take one single image frame at\na time. Perceiver-Resampler is then applied to map such visual tokens into N = 128 visual tokens\nper frame, independently. Once the model has such visual tokens over time (i.e., over multiple frames\nin the video), they are provided to an explicit 'temporal encoder'. The role of the temporal encoder is\nto build a video-level token representation from such sequence of image-level tokens, serving as a\nmapping function between a set of NT image tokens to M video tokens where T is the number of\nframes and M is a constant number of tokens. We explore various forms of the temporal encoder,\nincluding temporal pooling as well as sequential models, which we discuss further in the following\nsubsection. The resulting tokens are given to the LLM together with the encoded text tokens in a\nprefix manner, as in many standard VLMs.\nFor computational efficiency, the model takes uniformly sampled 8 frames per video. As a result, in\nour model, ViT first maps a video into 8 * 729 visual tokens, which is then mapped to 8 * 128 visual\ntokens using Perceiver-Resampler, and then to 16 ~ 128 video tokens using the temporal encoder.\nWe use Phi-3 (Abdin et al., 2024) as our LLM backbone taking such video tokens in addition to the\ntext prompt tokens. This enables the model to take text+video as an input and generate text sentences\nas an output."}, {"title": "2.2 TEMPORAL ENCODERS", "content": "A temporal encoder is a function of tokens, taking NT tokens as an input and returning M tokens\nas an output: X1,...,M = f(v(1,1),...,v(N,T)).\nWe explore different types of encoders as part of our model. The simplest form of the temporal encoder\nwill be temporal pooling, e.g., summating per-frame tokens over time: x1,..., M = {\\sum_{t}v((i,t))}_{i=1}^{M}\nwhere M is always restricted to be identical to N, which was also used in (Maaz et al., 2024). Another\npossible implementation would be the use of a temporal Transformer, modeling the entire token\nsequence and selecting the last m tokens similar to Mirasol3B (Piergiovanni et al., 2024):\nX1,...,M = {Transformer(v)}^{N.T-M+1}\nIn addition to the straightforward temporal encoders mentioned above, we explore two important\ntemporal encoders considering space-time nature of tokens: spatio-temporal attentional pooling and\nsequential models (Figure 3).\nSpatio-temporal attentional pooling: Attentional pooling allows learnable 'soft selection' of\nmultiple tokens given a larger set of tokens. Such attentional pooling have been previously developed\nfor Transformers (e.g., Perceiver (Jaegle et al., 2022) and TokenLearner (Ryoo et al., 2021)), and also\nused in earlier foundation models (e.g., CoCa (Yu et al., 2022)) for images.\nIn our model, we use TokenLearner (Ryoo et al., 2021), making it explicitly serve as our space-time\naware temporal encoder. Unlike previous per-image-frame usage of poolings where spatial pooling\nand temporal pooling are applied separately (e.g., Video-ChatGPT), our temporal encoder directly\ntakes all NT tokens and \u2018learns' to soft-select M informative tokens spatio-temporally. Here, N\ntokens could be viewed as spatial representations of a frame and we have T of them, suggesting it is\na spatio-temporal representation selection.\nOur attentional pooling in its simplest form is expressed as:\nXi = A(V) \\cdot V = softmax (a(VT)) \\cdot V\nwhere V is a matrix formed by concatenating input tokens v(1,1),...,v(N,T). The function A(\u00b7) computes\nthe summation weights for V, performing soft selection of tokens. This is further decomposed to the\nsoftmax and the function a(\u00b7). In Perceiver, a matrix multiplication with a latent query tokens (i.e.,"}, {"title": "2.3 TRAINING RECIPE", "content": "cross attention where |Q| = m) have been used to implement this: a(V) = Q \\cdot VT /\\sqrt{c}. TokenLearner\nuses a convolution/MLP on top of V: a(V) = MLPm(VT), which we use in our model. This allows\nselecting a smaller number of tokens (e.g., M = 32 tokens).\nWe experimentally confirm that such learnable spatio-temporal attentional pooling has advantages\nover the conventional approach of non-learnable spatial pooling and temporal pooling, in Section 3.3.\nSequential Model: We also deploy Token Turing Machines (TTM) (Ryoo et al., 2023) as a temporal\nencoder, which is a sequential model capable of taking any number of frames to generate a video-level\ntoken representation (e.g., M = 32 regardless the number of frames). Our use of TTM is similar to\nits usage in Mirasol3B (Piergiovanni et al., 2024), except that our model uses TTM directly to encode\na sequence of image tokens while Mirasol3B uses TTM to encode a sequence of sets of video tokens.\nHere, we also further extend TTM by adding time-stamped positional encodings to embded the frame\nindex of each token in the latent space. This enables the tokens in the 'memory' of TTM to preserve\nthe temporal ordering information, which is crucial when representing complicated or long video\nscenes. In addition, we use TTM temporal encoder in a 'grouped' fashion, maintaining a separate\nmemory of size G = 4 for each of N = 128 tokens over time. The memory is maintained to have\nthe size is N. G, and the final output from the sequence model is attentionally pooled from the final\nmemory to give M tokens.\nBLIP-3-Video follows a three-stage curriculum learning: (1) image caption pretraining, (2) video\ncaption pretraining, and (3) video instruction tuning. In all its training we freeze the vision encoder,\nonly training the model parameters after the vision encoder. First, we directly use the pretrained\nweights from BLIP-3 (Xue et al., 2024). BLIP-3 is for images and it does not contain weights for the\ntemporal encoder, so we randomly initialize those weights.\nAs its 2nd stage, the model is then trained on LLaVA-Hound-DPO's video caption data (Zhang et al.,\n2024b), featuring over 900k video captions. Instead of directly using the text captions provided\nin LLaVA-Hound-DPO, we used GPT-4 to rephrase such text captions so that they become more\nGPT-style captions.\nFinally, we tuned the model using a mix of video question-answering datasets, including VideoChat-GPT's 99k-sample video instruction tuning data (Maaz et al., 2024), along with the training splits of\nthe MSVD-QA (Xu et al., 2017), MSRVTT-QA (Xu et al., 2017), ActivityNet-QA (Yu et al., 2019),\nTGIF-QA (Jang et al., 2017), and NEXT-QA (Xiao et al., 2021) datasets, which contain 30k, 149k,\n32k, 71k, and 34k samples, respectively. For TGIF-QA, we only used the training data associated\nwith the Repeating Action and State Transition tasks. In our video instruction tuning recipe, we\nemploy both open-ended and multiple-choice video QA formats for TGIF-QA and NExT-QA. For\nthe open-ended video QA training data sourced from the MSVD-QA, MSRVTT-QA, TGIF-QA, and\nNEXT-QA training sets, we used GPT-3.5 to rephrase the original single-word or single-phrase answer\ninto a natural language sentence, providing the question in the LLM prompt context. For open-ended\nTGIF-QA and NExT-QA, we also double the sample size by using both the original short-phrase\nanswers and the rephrased sentence-based answers. In addition, we added a filtered version of the\nMira caption dataset (Ju et al., 2024) for our video instruction tuning. That is, we are using both\nvideo question-answering and video captioning for our final training. We excluded captions for Mira\nvideos longer than one minute, totaling 935k video caption samples.\nWe trained our model with 8 \u00d7 H100 GPUs. For the video caption pretraining, we use the batch size\nof 16 per GPU, 500 warmup steps, and the learning rate of 2e-5 with the cosine decay. We trained the\nmodel for 1 epoch. The video QA sft (i.e., instruction tuning) was done with the batch size of 4 per\ngpu, 500 warmup steps, and the learning rate of le-5 with the cosine decay. We trained the model for\n1 epoch in this case as well. The entire training (combining both video pretraining and the sft) takes\naround 12 hours, confirming the efficiency of our model."}, {"title": "3 EXPERIMENTS AND RESULTS", "content": "3.1 MODEL IMPLEMENTATION DETAILS\nWe share the model details with BLIP-3 (4B), except that BLIP-3-Video has the new temporal encoder\ncomponent in its architecture. This model takes the video with the input resolution of 384\u00d7384, using\nSigLIP encoder to map it to 729 tokens per frame with the channel size 1152. Perceiver-Resampler is\nimplemented with multiple cross-attention layers with the same channel dim, which is then given to\nthe temporal encoder.\nTokenLearner serving as the spatio-temporal attentional pooling was implemented using a MLP as\nthe attention function. The size of its inner dim was the number of target tokens * 2. The grouped\nTTM serving as the sequential model temporal encoder was implemented using 4 Transformer layers\n(with the channel dim of 1152) as the processor module while using TokenLearners for read/write\nmodules. Memory size was set to N * 4 = 512 tokens total.\nThe resulting 16 ~ 128 tokens are mapped to the text embedding with the channel dimension of\n3072, before given to the LLM (Phi-3).\n3.2 PUBLIC BENCHMARKS\nWe conducted experiments measuring video question-answering accuracies on multiple public\ndatasets. This includes open-ended answer generation tasks like MSVD-QA, as well as multiple\nchoice questions like NExT-QA. We follow their standard settings in all cases.\nWith the temporal encoder, BLIP-3-Video was able to retain the performance with much fewer tokens,\nwhich we discuss more in the following subsection. Our results suggest that not too many visual\ntokens are really necessary to be successful on these open-ended question answering benchmarks, as\nlong as we have a carefully designed temporal encoder.\n3.3 ABLATIONS\nWe conducted an ablation comparing different temporal encoders. These include: (1) the base single\nframe model (i.e., BLIP-3 trained with videos), (2) mean pooling similar to Video-ChatGPT, and\n(3) transformer temporal encoder similar to Mirasol3B. We also tried the (4) vanilla Token Turing\nMachines, which is not the grouped version we use as our temporal encoder.\nTable 3 shows the result, comparing the question-answering accuracies of different types of temporal\nencoders when abstracting a video into 128 tokens. We are able to observe that they all do a reasonable\njob, while some temporal encoders are more effective.\nIn addition, we compared different pooling approaches similar to the ones tried in prior works, when\nthey are required to select a much smaller number of tokens (e.g., 32) from a large set of visual\ntokens. We compare our spatio-temporal attentional pooling as well as the sequential model against\nits alternatives, including (1) fixed-window (non-learnable) space-time pooling and (2) learnable\n'per-frame' pooling. In particular, (2) is similar to the approach taken in LLaMA-VID (Li et al.,\n2024c), which independently selected a fixed number of tokens (e.g., 2) per frame.", "1 FRAMEs": ""}, {"title": "5 CONCLUSION", "content": "We introduce BLIP-3-Video, which is an efficient, compact vision-language model for videos with 4B\nparameters. BLIP-3-Video incorporates a temporal encoder in its architecture, which allows the model\nto abstract the entire video with as few as 16 or 32 tokens. In contrast to many state-of-the-art video\nVLMs taking advantage of thousands of visual tokens to represent a video (e.g., 4608), BLIP-3-Video\nshows a competitive performance while utilizing much fewer visual tokens (e.g., 32)."}]}