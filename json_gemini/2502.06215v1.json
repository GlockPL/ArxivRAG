{"title": "LessLeak-Bench: A First Investigation of Data Leakage in LLMs Across 83 Software Engineering Benchmarks", "authors": ["Xin Zhou", "Martin Weyssow", "Ratnadira Widyasari", "Ting Zhang", "Junda He", "Yunbo Lyu", "Jianming Chang", "Beiqi Zhang", "Dan Huang", "David Lo"], "abstract": "Large Language Models (LLMs) are widely utilized in software engineering (SE) tasks, such as code generation and automated program repair. However, their reliance on extensive and often undisclosed pre-training datasets raises significant concerns about data leakage, where the evaluation benchmark data is unintentionally \u201cseen\u201d by LLMs during the model's construction phase. The data leakage issue could largely undermine the validity of LLM-based research and evaluations. Despite the increasing use of LLMs in the SE community, there is no comprehensive study that assesses the extent of data leakage in SE benchmarks for LLMs yet. To address this gap, this paper presents the first large-scale analysis of data leakage in 83 SE benchmarks concerning LLMs. We systematically investigated whether, and to what extent, popular SE benchmark datasets were included in a LLM's pre-training data. Our approach involved using an efficient near-duplicate data detection algorithm, MinHash+LSH, to identify potential duplicate pairs between the SE benchmarks and LLM's pre-training dataset. Subsequently, we conducted extensive manual labeling on these potential duplicates to identify true duplicates. Those true duplicates reveal and confirm the data leakage of SE benchmarks. Our results show that in general, data leakage in SE benchmarks is minimal, with average leakage ratios of only 4.8%, 2.8%, and 0.7% for Python, Java, and C/C++ benchmarks, respectively. However, some benchmarks exhibit relatively higher leakage ratios, which raises concerns about their bias in evaluation. For instance, QuixBugs and BigCloneBench have leakage ratios of 100.0% and 55.7%, respectively. Furthermore, we observe that data leakage has a substantial impact on LLM evaluation. On the APPS benchmark, StarCoder-7B achieves a Pass@1 score that is 4.9 times higher on leaked samples than on non-leaked samples, highlighting how leaked benchmark data can lead to inflated metrics. We also identify key causes of high data leakage, such as the direct inclusion of benchmark data in pre-training datasets and the use of coding platforms like LeetCode", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, Large Language Models (LLMs) have been extensively applied to software engineering (SE) tasks [32], such as code generation [101] and automated program repair [93], leading to notable advancements in the SE domain. The remarkable capabilities of LLMs arise from the extensive knowledge within their large pre-training data, which encompasses various data types, including code, texts, or other modalities.\nDespite their effectiveness, many advanced LLM providers-both open-source and commer-cial-often do not disclose their pre-training datasets [57, 64]. This lack of transparency raises a critical concern in evaluating LLM-related approaches: the risk of data leakage. Data leakage occurs when LLMs are exposed to SE benchmark datasets during their pre-training phase, compromising the evaluation's validity. The impact of data leakage is twofold. First, it complicates the assessment of whether the notable performance of LLM-based approaches arises from true innovation or inflated effectiveness metrics due to prior exposure to SE benchmark data. Second, it leads to an unfair comparison between LLM-based methods and non-learning-based techniques, such as traditional program analysis approaches, which do not rely on training data and have no opportunity to learn from leaked data. The data leakage issue in LLMs is becoming increasingly recognized and significant due to LLMs' growing usage [54]. However, there is a lack of comprehensive studies investigating whether, and to what extent, SE benchmarks have been leaked into the pre-training data of LLMs.\nTo address this gap, we conduct the first large-scale investigation into data leakage of SE bench-marks. Our study investigates 83 diverse SE benchmarks\u00b9 across three widely used programming languages: Java, C/C++, and Python. Our study covers various SE tasks, such as code generation, code editing, program repair, fault localization, API recommendation, code translation, clone de-tection, test generation, vulnerability detection, vulnerability repair, debugging, log statement generation, and automatic code review. By conducting an extensive analysis of these benchmarks, we aim to understand the comprehensive landscape of data leakage issues within the SE domain. Regarding the studied LLMs, we focus on data leakage within the StarCoder [45] family. We choose StarCoder for three key reasons: (1) we require fully open-source LLMs\u2014with publicly available pre-training data-to identify concrete evidence of data leakage; (2) StarCoder demonstrates competitive performance among fully open-source models; and (3) it serves as the foundation for derivative LLMs such as WizardCoder [53], OctoPack [59], CodeShell [94], and DeepSeek-Coder [22].\nStudying data leakage in SE benchmarks requires detecting duplicate data between the SE benchmark datasets and the pre-training data of LLMs. To rigorously examine data leakage in SE benchmarks, we proposed a multi-phase approach named DetectLeak, which combines automated techniques with manual labeling to identify overlaps between LLM pre-training data and SE benchmark datasets. Initially, DetectLeak employs an automated near-duplication detection tool namely MinHash+LSH, to identify potential duplicate pairs. This involves comparing approximately 1.7 trillion pairs of LLM pre-training and SE benchmark data. Next, several experienced developers collaboratively label the potential duplicate pairs flagged by the automated tool, to accurately identify true duplicates and thus confirm the existence of data leakage of SE benchmark data.\n\u00b9If a benchmark has multiple variants, we treat each variant as a separate dataset or benchmark."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 Data Leakage Definition", "content": "Data leakage (also called Data Contamination) in LLMs refers to the unintentional inclusion of evaluation data during the model's construction phase (e.g., pre-training or training) [9]. This issue can lead to an inaccurate assessment of a model's true capabilities. To formally define data leakage, we denote \\(D_{construct}\\) as the pre-training/training dataset used for model construction and \\(D_{eval}\\) as the evaluation dataset. Data leakage occurs if either of the following two conditions is met:\n1. Exact Leakage. At least one evaluation sample exactly matches a data sample in the pre-training or training set:\n\\(D_{eval} \u2229 D_{construct} \u2260 0\\)\n2. Semantic Leakage. At least one evaluation sample is semantically equivalent to a data sample in the pre-training or training set:\n\\(\u2203x_{eval} \u2208 D_{eval}, \u2203x_{construct} \u2208 D_{construct}: f(x_{eval}, x_{construct}) = 1\\)\nHere, \\(f(x_{eval}, x_{construct})\\) is a function that measures the semantic equivalence between two data samples. If the samples are semantically equivalent, the function returns 1; otherwise, it returns 0. This condition indicates that at least one evaluation sample is semantically equivalent to a pre-training/training sample.\nData Leakage Regarding LLMs. For approaches based on LLMs, leakage can occur in two distinct phases: (1) during pre-training, where evaluation data is inadvertently included in the pre-training dataset, and (2) during fine-tuning or prompting, where evaluation data is accidentally incorporated into the training set. Detecting the second type of data leakage, which occurs between training and evaluation data, is straightforward. Since the training data is usually available, researchers"}, {"title": "3 METHODOLOGY", "content": "This section describes our framework, DetectLeak, for detecting duplicate data between SE benchmarks and the LLM's pre-training corpus. Figure 2 presents our DetectLeak framework, which consists of three main steps:\nStep 1: Automatic Detection of Potential Duplicates (1 of Figure 2): Given an LLM pre-training corpus and an SE benchmark, we use an automated near-duplicate detection tool to identify potential duplicate pairs between the SE benchmark data and the LLM's training data.\nStep 2: Manual Verification on Potential Duplicates (\u2461 of Figure 2:) Experienced developers collaboratively review and annotate the potential duplicates identified in Step 1. This manual validation ensures the accurate identification of true duplicates.\nStep 3: Quantifying the Extent of Data Leakage (\u2462 of Figure 2): In this step, we first identify the leaked benchmark data samples from the manually verified duplicate pairs between the SE benchmark data and the LLM's training data. Then, we quantify the extent of data leakage for each studied SE benchmark and compute their corresponding leakage ratios."}, {"title": "3.1 Automated Data Leakage Detection", "content": "While automated methods may not achieve the precision of manual labeling, manually reviewing ex-tensive pre-training datasets containing millions of samples is both impractical and cost-prohibitive. Therefore, in the first phase of DetectLeak, we adopt a cost-effective and efficient automated ap-proach to detect potential data leakage. For instance, in this study, we conduct the comparison of 1.7 trillion pairs of LLM pre-training and SE benchmark data, based on our studied LLM and SE benchmarks, which will be introduced in Sections 4.1 and 4.2. The sheer scale of this computational effort highlights the challenge posed by the extensive size of LLM pre-training datasets, emphasizing the complexity and resource-intensive nature of studying data leakage regarding LLMs. This also supports our decision to use the efficient automated tool as the initial step before manual labeling.\nAutomated Duplicate Detection Tool Selection. We specifically choose MinHash+LSH [3, 81], a scalable technique for identifying potential duplicates in large datasets. This tool has been widely adopted by many LLMs such as Qwen [7], CodeParrot [81], SantaCoder [3], StarCoder [45], and StarCoder v2 [51] to detect and filter potential duplicate data within their pre-training corpora. While the authors of these LLMs use MinHash+LSH for filtering duplicates in their pre-training datasets, we adopt it for a different purpose: detecting potential duplicates between LLM pre-training data and SE benchmark datasets. We also considered other automated tools for duplicate data detection except for MinHash+LSH but decided against using them for several reasons.\nFirst, Exact Match is effective for detecting exact duplicates but fails to identify semantic du-plicates (i.e., those that are semantically equivalent but not identical), which are crucial for our analysis. Second, while many code clone detection approaches are available in the literature [60], many effective tools are learning-based and rely on training with data that closely matches the distribution of the test set. These tools are also typically evaluated on Java-based benchmarks, (e.g., BigCloneBench [73]), and focus primarily on code snippets. In contrast, our study spans three different programming languages, and we lack training data for duplicates between SE benchmarks and LLM pre-training data. Additionally, benchmarks such as SWE-Bench [38] contain not only code but also other types of content, such as GitHub issue reports (similar to bug reports), which require a tool that can handle both code and texts. MinHash+LSH, on the other hand, does not require training data, supports multiple programming languages, and is capable of handling both code and text. These features make it a more suitable choice for our study. Furthermore, we have observed the widespread use of MinHash+LSH in deduplicating LLM pre-training data for models such as Qwen, CodeParrot, SantaCoder, StarCoder, and StarCoder v2. However, we have not no-ticed code clone detection tools applied to LLM' huge pre-training data to detect duplicates yet. For these reasons, we chose to follow the established practice of many LLM developers to select MinHash+LSH for our analysis."}, {"title": "MinHash+LSH Method Details", "content": "As shown in \u2460 of Figure 2, the method consists of several steps to detect potential duplicate pairs. For a given pair of data samples, the method first decomposes each sample into n-gram tokens. Then, the MinHash [12] module is applied to generate hashes for each sample. Next, Locality-Sensitive Hashing (LSH) [89] is used to group these hashes into bands and hash the bands. By comparing the hashes of bands, the method identifies similar data samples: if a pair of data samples share the same band hash, they are considered candidate duplicate pairs. For each candidate pair, the method calculates the Jaccard similarity coefficient [88] between the two data samples in this pair. If the coefficient exceeds the threshold, the pair is flagged as a potential duplicate by the automated tool. In this study, we set the n-gram size to 2 and the Jaccard similarity coefficient threshold to 0.7, based on initial small-scale trials."}, {"title": "3.2 Manual Labeling of Potential Duplicates", "content": "To improve detection accuracy, our approach includes a manual verification step of potential dupli-cate pairs flagged by the automated tool. This large-scale manual labeling process ensures accurate classification of duplicate pairs, confirming the presence of leaked SE benchmark data samples and revealing the extent of data leakage in software engineering benchmarks. This annotation process is performed by a team of eight experienced data annotators, including three post-doctoral researchers, four PhD students, and one Master's student, each with a minimum of four years of experience in programming. Figure 2 (\u2461) illustrates the annotation process.\nAnnotation Scheme. Annotators are tasked with categorizing potential duplicate pairs into one of the following four classifications:\nNot Related: The two data samples in the pair are dissimilar.\nRelated but Not Duplicates: The two data samples address similar tasks but are designed for distinct purposes.\nSemantically Equivalent: The code may feature renamed identifiers (e.g., variables, constants) while preserving the same logic. The structure may remain largely unchanged with minor additions or deletions. Essentially, the code achieves the same functionality but uses different syntax or implementation.\nExact Copies: The pairs are identical except for variations in formatting or comments.\nIn this categorization scheme, the first two categories represent the \u201cNon-duplicate\u201d class, while the latter two correspond to the \u201cDuplicate\u201d class. We adopt this four-class categorization to gain more detailed insights from annotators, compared to a simple binary classification (non-duplicate or duplicate).\nAnnotation Process. In our study, the automated tool MinHash+LSH identified 6,643 potential duplicate pairs, based on the selected LLM and SE benchmarks, which will be discussed in Sec-tions 4.1 and 4.2. Given the large volume of data to label, it is impractical for all annotators to review every pair. To address this, each potential duplicate pair is independently reviewed by two annotators, who evaluate the similarity between the data samples and assign one of the four annotation categories (ranging from \u201cNot Related\u201d to \u201cExact Copies\u201d). After completing the labeling, we calculate the Cohen's Kappa Score [87] between the two annotators based on their labels. The average Cohen's Kappa Score is 0.9424, indicating almost perfect agreement [42]. Notably, when we simplify the four-class categorization (ranging from \u201cNot Related\u201d to \u201cExact Copies\u201d) into a binary classification (\u201cDuplicate\u201d or \u201cNon-duplicate\u201d), the Cohen's Kappa Score increases to 0.9591, reflecting even higher agreement on determining whether pairs are duplicates or not. Although the agreement between the two annotators is high, there are still some pairs where they disagree on the label. In such cases, a third annotator will resolve the conflict by reviewing the labels assigned by the previous two annotators and re-assigning the most appropriate label for the pair."}, {"title": "3.3 Quantifying the Extent of Data Leakage", "content": "In the previous steps, we identified the actual duplicate pairs between the SE benchmark data and the LLM's training data. Using these duplicate pairs, we can easily identify the leaked benchmark data samples. If a pair is marked as a duplicate, the SE benchmark data sample in that pair is considered leaked, as it has been found to have a semantically equivalent or identical counterpart in the LLM's pre-training data.\nUsing the confirmed leaked samples, we quantify the extent of data leakage for each studied SE benchmark. To achieve this, we employ two simple and clear metrics: leakage count and leakage ratio. These metrics are defined as follows: (1) Leakage Count: the total number of leaked benchmark data samples, i.e., \\(N_{leak}\\) ; (2) Leakage Ratio: the proportion of leaked data samples within the benchmark, calculated as: \\(N_{leak}/N_{total}\\). Here, \\(N_{leak}\\) denotes the number of identified leaked benchmark samples, and \\(N_{total}\\) represents the total number of data samples in the benchmark. A higher leakage count or ratio indicates a more significant degree of data leakage within the benchmark, raising concerns about its reliability for evaluating LLMs."}, {"title": "3.4 LessLeak-Bench", "content": "After identifying leaked samples in SE benchmarks using our DetectLeak approach, we introduce LessLeak-Bench to mitigate data leakage risks. Specifically, we remove all identified leaked samples from each studied SE benchmark, producing cleaned versions that are free from known data leakage. This curated collection, LessLeak-Bench, serves as a more reliable and comprehensive benchmark for evaluating LLMs across diverse SE tasks."}, {"title": "4 EXPERIMENTAL SETUP", "content": "In this section, we discuss the selection process for the studied LLM and SE benchmarks. Additionally, we outline the implementation details and define the research questions in this study."}, {"title": "4.1 LLM Selection", "content": "Selection Criteria. Selecting an appropriate target LLM was a crucial step in this study due to the extensive manual annotation involved, which makes repeated experiments across multiple models infeasible. To guide our selection, we established the following selection criteria:\n(1) Full Open-Source Availability. The selected LLM must be fully open-source, encompassing both model parameters and pre-training data. However, since re-training the model is outside the scope of our study, open-source pre-training algorithms and scripts are not required.\n(2) High Effectiveness. The model should demonstrate strong effectiveness on widely used SE benchmarks, such as HumanEval [17] and MBPP [6].\n(3) Influence and Adoption. We prioritized models with significant influence, particularly those that have inspired or laid the groundwork for the development of newer/better LLMs.\nTarget LLM Selection Process. Table 1 presents our analysis of various code-related LLMs in chronological order, till April 2024, when this study was initiated. Models highlighted in light green meet our full open-source criteria, while those marked in light red represent the most suitable candidates based on our selection criteria. After thorough analysis, we selected StarCoder [45] as the research LLM for this study."}, {"title": "4.2 Benchmark Data Selection", "content": "The whole SE community has developed numerous high-quality benchmarks. However, the exten-sive manual annotation required for each makes it impractical to cover all valuable SE benchmarks. To investigate the leakage status of SE benchmarks in relation to LLMs, we focus on selecting benchmarks that have been previously used for LLM evaluation. Our selection is further constrained to benchmarks within three widely used PLs: Python, Java, and C/C++.\nStudied SE Benchmark Selection. To identify relevant benchmarks, we leverage a recent and comprehensive survey on LLMs for SE tasks [33], which analyzed 395 research papers from January 2017 to January 2024. From the papers referenced in this survey, we selected benchmarks and datasets used for evaluating LLMs. We further expanded our selection by examining the citations and related works of these papers, excluding those lacking replication packages or inaccessible datasets. In building our benchmark collection, we prioritized including datasets across a variety of SE tasks, rather than concentrating on any one task with extensive datasets.\nTo ensure clarity in our analysis, benchmarks with multiple variants\u2014such as those involving different programming languages or scenarios\u2014were assigned distinguishing tags appended to their original names. For example, the CodeEditorBench [27] benchmark includes variants in three programming languages: Python, Java, and C++. Additionally, it features four scenarios: Code Debug, Code Translate, Code Polish, and Code Requirement Switch. Since each variant contains distinct benchmark data samples, we assign a unique name to each variant. The naming convention combines the following components: (1) benchmark original name, (2) scenario name, and (3) programming language name. Name tags for the scenario and programming language are included only if there are multiple variations across different scenarios or programming languages. For example, the CodeEditorBench [27] variant containing Python data samples for the Code Debug"}, {"title": "4.3 Implementation Details", "content": "We retrieve StarCoder's pre-trained data from its official HuggingFace homepage. We execute computations on an NVIDIA GeForce A5000 GPU with 24 GB of memory. We acquire the data for each SE benchmark dataset directly from its replication packages or official websites. For the Minhash+LSH method, we adopt the BigCode Team's implementation [76], which leverages the DataSketches [78] library."}, {"title": "4.4 Research Questions", "content": "Our work aims to mainly answer four Research Questions (RQs).\n\u2022 RQ1: To what extent does data leakage exist in the studied SE benchmarks? In RQ1, we evaluate an extensive set of 83 SE benchmarks to investigate potential data leakage into an advanced LLM StarCoder [45].\n\u2022 RQ2: What factors contribute to high leakage rates in SE benchmarks? In RQ2, we analyze the top benchmarks with high leakage rates, discussing the potential reasons behind their high leakage ratios.\n\u2022 RQ3: How does the leakage of benchmark data affect the effectiveness of LLMs? In RQ3, we measure the effectiveness differences between leaked and non-leaked portions of the data to explore the impact caused by data leakage.\n\u2022 RQ4: How effective is the automated metric in detecting data leakage when lacking access to LLM pre-training data? Since pre-training data for many LLMs is inaccessible, this RQ investigates whether SE benchmark leakage can be inferred solely from LLM behaviors (i.e., without access to pre-training data), such as the Perplexity scores of LLMs."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "In this section, we first present the overall results of the data leakage detection analysis. Following that, we provide detailed results and answers to each research question (RQ).\nOverall Results. Before diving into the specific RQs, we first present an overview of the experi-mental results. The selected LLM's pre-training datasets consist of 12M samples for Python, 20M samples for Java, and 14M samples for C/C++. In comparison, the diverse SE benchmarks we studied collectively comprise 46k samples for Python, 42k samples for Java, and 21k samples for C/C++. To investigate potential data leakage, each SE benchmark sample was compared against all pre-training data samples for its corresponding programming language. This process resulted in an astounding total of over 1.7 trillion comparisons. The sheer scale of this computational effort highlights the complexity and resource-intensive nature of studying data leakage regarding LLMs.\nFrom an overall perspective, as depicted in Figure 3, only 2% of the benchmark samples from all the SE benchmarks studied were flagged by the automated tool MinHash+LSH as potentially forming at least one duplicate pair with the pre-training data of StarCoder. Moreover, of the pairs"}, {"title": "5.1 RQ1: Data Leakage Status in SE Benchmarks", "content": "Table 2 to Table 4 present the data leakage status of the studied SE benchmarks for Python, Java, and C/C++, respectively. The column \u201c#Auto\u201d represents the number of potentially duplicate pairs identified by the automated tool MinHash+LSH. The column \u201c#Manual\u201d shows the number of actual duplicate pairs verified through manual labeling. The column \u201cLeaked Count\u201d refers to the Leakage Count, which is the number of actual leaked SE benchmark samples. Lastly, the column \u201cLeaked Ratio\" refers to the Leakage Ratio, which shows the proportion of benchmark data identified as leaked to an LLM. Both the Leakage Count and Leakage Ratio serve as key metrics for assessing the extent of data leakage in the benchmarks. A higher value for these metrics indicates a more severe data leakage problem in the given SE benchmark. It is important to note that a single SE benchmark sample may appear multiple times in an LLM's pre-training dataset. For instance, an identical function may be present in different software repositories used in LLMs's pre-training. These occurrences can result in multiple duplicate pairs being identified, even though they correspond to a single leaked SE benchmark sample. As a result, we may observe that the Leakage Count is occasionally smaller than the number of duplicate pairs identified during manual labeling.\""}, {"title": "5.1.1 Data Leakage in Python Benchmarks", "content": "Table 2 reveals that data leakage in Python bench-marks is generally minimal, with an average leakage ratio of only 4.8%. Our results indicate that many Python benchmarks have low or negligible leakage rates, making them suitable for reliable evaluation of LLMs. As shown in Table 2, several benchmarks, such as BigCodeBench-py, BioCoder, CanItEdit, ClassEval, CodeBenchGen, CodeEditorBench-translate-py, CodeReview-py, CodeReviewNew-py, CodeScope-py, DS-1000, G-TransEval-py, LiveCodeBench series, Mconala-es, Mconala-ja, PythonSaga, and SVEN-py, exhibit no evidence of the leakage issue (i.e., Leakage Ratio=0%) in our study, highlighting that the threat of data leakage is relatively small when using those benchmarks to evaluate LLMs.\nHowever, a few Python benchmarks demonstrate much higher leakage rates, raising concerns about their bias in evaluation. Notable benchmarks include APPS (10.8%), BugsInPy (11.0%), CodeEditorBench-debug-py (10.7%), CodeEditorBench-switch-py (7.2%), EvoCodeBench (6.5%), QuixBugs (100%), SWE-Bench (8.7%), and SWE-Bench-verified (10.6%). These elevated leakage ratios suggest that a relatively large portion of the data samples in these benchmarks"}, {"title": "5.1.2 Data Leakage in Java Benchmarks", "content": "Data leakage in Java benchmarks is also generally low, with an average leakage ratio of just 2.8%, as shown in Table 3. This is notably lower than the average leakage ratio observed in Python benchmarks. The results reveal that many Java benchmarks exhibit low or negligible leakage rates, making them suitable for reliable evaluation of LLMs. Many Java benchmarks, including AixBench-auto, AixBench-manual, and several others, show no evidence of leakage issues (i.e., Leakage Ratio=0%). This finding highlights that the risk of data leakage is relatively small when using these benchmarks."}, {"title": "5.1.3 Data Leakage in C/C++ Benchmarks", "content": "Data leakage in C/C++ benchmarks is minimal, with an average leakage ratio of only 0.7%, as shown in Table 4. This represents the lowest average leakage ratio among the Python, Java, and C/C++ benchmarks analyzed. The results indicate that most C/C++ benchmarks exhibit low or negligible leakage rates, making them suitable for reliable evaluation of LLMs. However, two C/C++ benchmarks demonstrate relatively higher leakage rates: CodeEditorBench-debug-c (4.5%) and CodeEditorBench-switch-c (8.3%). These elevated leakage rates emphasize the importance of addressing data leakage by removing identified duplicate benchmark data when using these two benchmarks. In contrast, the threat of data leakage for other C/C++ benchmarks remains significantly lower."}, {"title": "5.1.4 Data Leakage Status in Specific SE Tasks", "content": "We will now discuss the leakage rates across different tasks. We first focus on Code Generation and Program Repair, as these are both highly popular tasks and constitute the majority of the benchmarks analyzed. For Code Generation, out of 33 benchmarks, 26 have a leakage rate of 0%, and the average leakage rate across all benchmarks is just 0.62%. This indicates that the issue of data leakage in the Code Generation benchmarks we studied is relatively minor. In contrast, for Program Repair, only 1 out of 9 benchmarks has a"}, {"title": "5.1.5 LLM Evaluation with LessLeak-Bench", "content": "Our experimental results indicate that while the average leakage rate in SE benchmarks is low, certain benchmarks have been significantly impacted by data leakage. To support researchers, we have removed the leaked samples and developed LessLeak-Bench, a cleaned version of SE benchmarks. We recommend using LessLeak-Bench in future studies to ensure more reliable LLM evaluations, as many leaked benchmark samples are no longer suitable for this purpose."}, {"title": "Answer to RQ1", "content": "In general, data leakage in SE benchmarks is minimal, with average leakage ratios of only 4.8%, 2.8%, and 0.7% for Python, Java, and C/C++ benchmarks, respectively. However, some benchmarks exhibit relatively higher leakage ratios. For instance, QuixBugs and BigCloneBench have leakage ratios of 100.0% and 55.7%, respectively. Additionally, the Program Repair task generally shows higher leakage ratios, with an average leakage rate of 12.5%. Researchers should exercise caution when applying LLMs to program repair tasks, as these benchmarks may require the removal of duplicate data samples to ensure unbiased evaluations."}, {"title": "5.2 RQ2: Understanding High Data Leakage in SE Benchmarks", "content": ""}, {"title": "5.2.1 Which Types of Software Repositories Are Likely Contributing to Data Leakage?", "content": "We conducted an investigation into which software repositories in the pre-training data of the LLM contribute most significantly to data leakage in SE benchmarks. To do this, we grouped all identified duplicate pairs, where the SE benchmark samples are the leaked benchmark data, and the pre-training data in these pairs indicate the source repositories in pre-training data. By analyzing the characteristics of these repositories, we can offer actionable recommendations.\nTable 5 highlights the software repositories associated with the most duplicate pairs. One key finding is that 92 duplicate pairs originated from the inclusion of the \u201cPatrickShaw/QuixBugs\u201d repository. This accounts for the high data leakage ratio observed in the QuixBugs benchmark, as the pre-training data seems to contain a cloned version of the QuixBugs repository. Additionally, we observed a high frequency of algorithm-related keywords, such as \u201cleetcode,\u201d \u201cprogramming,\u201d and \u201cdata-structures,\u201d in repository names. For instance, \u201cleetcode\u201d appears 8 times in the top 20 repositories.\nBased on these observations, we offer several actionable recommendations. For LLM developers, it is crucial to scan repository names during the preparation of pre-training data and filter out repositories that include common SE benchmark names, such as \u201cPatrickShaw/QuixBugs\u201d listed in Table 5. This will help prevent the inadvertent inclusion of benchmarks through cloned reposi-tories, thereby introducing benchmark data into the pre-training data. Additionally, repositories containing common coding website names, such as 'leetcode', or algorithm-related keywords like \"data-structures,\" should be excluded. Many benchmarks are sourced from these platforms or algorithm-related repositories, which increases the likelihood of data leakage. For LLM users and researchers evaluating LLMs, we recommend avoiding the direct use of benchmark data primarily sourced from coding websites like LeetCode or from algorithm-related tutorial repositories. This approach will help reduce the risk of data leakage during evaluations."}, {"title": "5.2.2 Why Do Certain SE Benchmarks Exhibit High Data Leakage?", "content": "This subsection explores bench-marks with high data leakage rates, aiming to investigate the potential underlying causes. While most benchmarks exhibit minimal data leakage, this research question focuses specifically on the top 10 benchmarks with the highest leakage. By identifying the reasons and characteristics behind their high leakage rates, we aim to equip researchers with insights to assess whether their chosen benchmarks may face similar issues.\nIt is important to clarify that the issue does not stem from the benchmarks themselves. Many of these benchmarks were developed and published prior to the advent of LLMs. As a result, the high leakage rates are not due to any flaws in the benchmarks, but rather an unintended consequence of the overlap between the historical data used to create the benchmarks and the data included in LLM training.\nTable 6 provides an overview of the top 10 benchmarks with the highest leakage rates, detailing their data sources, basic descriptions, and the potential causes behind their high leakage. Specifically, QuixBugs exhibits significant leakage due to the inclusion of the \u201cPatrickShaw/QuixBugs\u201d repository in the pre-training dataset. BigCloneBench's high leakage is attributed to the overlap between its source data and the \u201ccragkhit/elasticsearch\u201d repository included in the pre-training data. This overlap likely results from BigCloneBench's reliance on IJaDataset 2.0 [25], which contains the Elasticsearch project. A similar issue affects BugsInPy, where six source projects used in its dataset construction were also identified in LLM's pre-training data. For other benchmarks, the primary cause of high data leakage lies in their dependence on widely used coding platforms. Benchmarks such as APPS, CodeEditorBench-switch-java, CodeEditorBench-debug-py, CodeEditorBench-switch-c, and CodeEditorBench-switch-py are mainly derived from LeetCode. Since data from LeetCode is"}, {"title": "Answer to RQ2", "content": "We identified four potential causes of high data leakage: 1) the direct inclusion of benchmark data in the pre-training dataset, 2) overlap between the repositories used to create the benchmarks and the pre-training dataset, 3) reliance on coding platforms like LeetCode to construct the benchmarks, and 4) the shared use of data sources, such as GitHub issues, in both the benchmarks and the pre-training dataset. We also offer several actionable recommendations to reduce the risk of benchmark leakage in future studies."}, {"title": "5.3 RQ3: Impact of Data Leakage for LLM Evaluation", "content": "Motivation. This research question investigates whether data leakage leads to unfair evaluation of LLMs, specifically whether LLMs perform better on leaked data compared to non-leaked data."}, {"title": "Answer to RQ3", "content": "The experimental results show that data leakage has a significant impact on the evaluation of LLMs, with models performing better on leaked data compared to non-leaked data. For example, StarCoder-7b achieves a Pass@1 score that is 4.9 times higher on the leaked samples than on the non-leaked samples. This underscores how the presence of leaked benchmark samples can lead to significantly inflated metrics, emphasizing the critical need to address the data leakage issue in SE benchmarks to ensure fair evaluation."}, {"title": "5.4 RQ4: Automatic Data Leakage Detection Without Knowledge of Pre-training Data", "content": "Motivation and Setup. Since pre-training data for many LLMs is inaccessible, another key question is whether SE benchmark leakage can be inferred solely from LLM behaviors, such as the LLM's confidence in its generated content, without knowledge of the LLM's pre-training data.\nA benchmark dataset with reliable labels (i.e., leaked or non-leaked) is crucial for evaluating how effectively data leakage in LLMs can be detected when their pre-training data is unavailable. To address this, we utilize our manually labeled dataset created through our data leakage detection approach DetectLeak (as shown in Figure 2). In RQ1 and RQ2, we focus exclusively on leaked samples. In contrast, in RQ4, we include both leaked and non-leaked SE benchmark data samples to form a new dataset, named AutoDetectLeak-Bench. Specifically, we extract the manually labeled"}, {"title": "Answer to RQ4"}]}