{"title": "MA-GTS: A Multi-Agent Framework for Solving Complex Graph Problems in Real-World Applications", "authors": ["Zike Yuan", "Ming Liu", "Hui Wang", "Bing Qin"], "abstract": "Graph-theoretic problems arise in real-world applications like logistics, communication networks, and traffic optimization. These problems are often complex, noisy, and irregular, posing challenges for traditional algorithms. Large language models (LLMs) offer potential solutions but face challenges, including limited accuracy and input length constraints. To address these challenges, we propose MA-GTS (Multi-Agent Graph Theory Solver), a multi-agent framework that decomposes these complex problems through agent collaboration. MA-GTS maps the implicitly expressed text-based graph data into clear, structured graph representations and dynamically selects the most suitable algorithm based on problem constraints and graph structure scale. This approach ensures that the solution process remains efficient and the resulting reasoning path is interpretable. We validate MA-GTS using the G-REAL dataset, a real-world-inspired graph theory dataset we created. Experimental results show that MA-GTS outperforms state-of-the-art approaches in terms of efficiency, accuracy, and scalability, with strong results across multiple benchmarks (G-REAL 94.2%, GraCoRe 96.9%, NL-Graph 98.4%). MA-GTS is open-sourced at https://github.com/ZIKEYUAN/MA-GTS.git.", "sections": [{"title": "1 Introduction", "content": "Graph-theoretic problems have extensive applications in domains such as logistics scheduling, communication networks, production planning, and traffic optimization(Li et al., 2023b). These problems typically involve a large number of nodes and edges, coupled with complex constraints and dynamic variations, making their solution highly challenging(Bondy and Murty, 2008). Despite significant advancements in graph theory and algorithmic design, traditional approaches remain computationally expensive and inefficient when handling large-scale, high-complexity problems. Existing methods, including exact algorithms, greedy strategies, and dynamic programming(Bellman, 1966), perform well on small-scale instances. However, as problem size increases, their computational complexity and memory requirements grow exponentially, rendering them impractical for real-world applications. While heuristic methods(Kokash, 2005) can improve performance under specific conditions, they often suffer from local optima and require extensive parameter tuning and model selection. Therefore, developing efficient and scalable solution frameworks capable of addressing the computational demands and structural variability of complex graph-theoretic problems remains a critical research challenge.\nRecent advancements in large language models (LLMs) have spurred interest in their applications for graph-theoretic problems. Leveraging their natural language processing (NLP) capabilities, LLMs can serve as scene interpreters (mapping real-world problems to graph models), graph extractors (identifying graph structures from unstructured data), and graph algorithm invokers (assisting in solving and optimizing graph-based problems), addressing certain limitations of traditional algorithms. However, significant challenges remain. Firstly, LLMs rely on statistical pattern matching rather than strict mathematical computations, limiting their reasoning accuracy and making them unreliable for NP-hard problems(Hochba, 1997). Secondly, their ability to handle large-scale graphs is limited by the Transformer(Vaswani, 2017) architecture's context window and computational complexity, which restricts their capacity to capture global information. Finally, LLMs lack the ability to decompose and map real-world graph theory problems, which often contain complex textual noise and implicit graph structures. As a result, LLMs struggle with accurate denoising and may mismap node information, reducing reasoning interpretability. These limitations highlight the inadequacy of single-agent LLM frameworks for solving complex graph-theoretic problems in real-world applications and underscore the need for more efficient and scalable paradigms.\nTo tackle these challenges, we propose \u041c\u0410-GTS(Multi-Agent Graph Theory Solver), an innovative multi-agent framework designed to address complex graph-theoretic problems through agent collaboration and competition. Figure 1 illustrates the framework, which incorporates a multi-agent coordination mechanism allowing agents to perform local searches independently while sharing information and cooperating, thus improving solution efficiency and accuracy. MA-GTS analyzes the original real-world problem textual data, filters out noise, and extracts key graph data and problem-specific details, reducing the text length that LLMs must process and enhancing reasoning efficiency. This coordination mitigates the limitations of LLMs in implicit graph structure modeling, ensuring efficient solutions for complex graph tasks. Additionally, dynamic agent interactions enable the framework to address large-scale problems and adapt to complex constraints and dynamic changes.\nTo validate the effectiveness of the multi-agent framework, we introduce the G-REAL dataset, designed to simulate complex graph theory problems relevant to real-world scenarios. Unlike traditional datasets that rely on simple textual descriptions of graph structures, G-REAL better reflects practical applications for large-scale models. Experiments comparing MA-GTS with state-of-the-art open-source and closed-source LLMs\u2014three closed-source and two open-source models\u2014using both direct and Chain of Thought (CoT) reasoning, show that MA-GTS significantly outperforms existing LLMs in terms of efficiency and accuracy. Notably, it excels in solving large-scale problems with complex constraints, offering superior scalability, robustness, and cost-effectiveness. The primary contributions of this study are as follows:\n\u2022 First, we propose an innovative multi-agent framework, MA-GTS, which overcomes the limitations of traditional graph theory algorithms in large-scale complex problems, achieving state-of-the-art performance in our tests.\n\u2022 Second, we constructed a real-world graph theory dataset, G-REAL, that aligns with practical needs, providing the necessary data support for validating the effectiveness of the algorithm.\n\u2022 Finally, by introducing novel collaboration mechanisms and strategies, we achieve efficient and precise graph theory problem-solving within the multi-agent system, demonstrating its substantial potential in real-world application scenarios."}, {"title": "2 Related Work", "content": "LLMs for Graph: Recent advancements in LLMs for graph tasks have led to significant contributions in methodology and evaluation. These tasks are often classified into Enhancer, Predictor, and Alignment types (Li et al., 2023b). Notably, (Pan et al., 2023) presents a roadmap for unifying LLMs with Knowledge Graphs (KGs), while (Chai et al., 2023) proposes an end-to-end method for solving graph-related problems,(Cao et al., 2024) improves LLMs' understanding of graph structures by addressing positional biases and incorporating an external knowledge base. On the evaluation front, several benchmarks have been introduced. NLGraph (Wang et al., 2024) offers a simple test dataset for graph tasks, and GPT4Graph (Guo et al., 2023) evaluates LLM capabilities on semantic tasks. GraCoRe(Yuan et al., 2025) comprehensively verifies the graph understanding and reasoning capabilities of LLM. Other notable works include (Liu and Wu, 2023), which assesses LLMs in graph data analysis, and (Perozzi et al., 2024), which designs a hint method for graph tasks.\nLLM Agents: In recent years, several multi-agent frameworks have been proposed to enhance the coordination and efficiency of language models in complex tasks. MetaGPT(Hong et al., 2023) reduces hallucinations in complex tasks by embedding human workflows into language models. CAMEL(Li et al., 2023a) promotes autonomous cooperation among agents, guiding them to align with human goals and studying their interactions. AutoGen(Wu et al., 2023) is a flexible framework that allows developers to customize agent interactions using both natural language and code, suitable for various fields. In addition, (Li et al., 2024a) can be used to solve simple graph problems, (Li et al., 2024b) is an autonomous agent that uses LLMs to create animated videos from simple narratives."}, {"title": "3 MA-GTS", "content": "The MA-GTS framework adopts a hierarchical processing paradigm, comprising three layers: the Information Extraction Layer(IEL), the Knowledge Integration Layer(KIL), and the Algorithm Execution Layer(AEL). These layers interact through a hierarchical collaborative communication mechanism, enabling an end-to-end pipeline that processes unstructured data and solves complex graph-theoretic problems. Additionally, to support the knowledge base of MA-GTS, we have constructed the Graph Theory Knowledge Base and Graph Theory Algorithm Library. More information about them in the Appendix A.\nThe IEL parses textual and structured data, extracts graph-related information, and identifies problem types to provide standardized inputs. The KIL constructs structured graph data, applying graph theory and optimization strategies to improve accuracy and scalability. The AEL calls specified algorithms for computation and performs self-checking to solve complex graph problems efficiently. Figure 2 illustrates the functionality of each agent in their respective layers.\nBy leveraging agent collaboration, MA-GTS ensures efficient problem-solving, high scalability, and adaptability to complex constraints, offering a novel solution for real-world graph-theoretic challenges. The specific functionalities of each agent are detailed as follows:"}, {"title": "3.1 Information Extraction Layer (IEL)", "content": "The IEL extracts relevant information from text and unstructured data, converting it into a structured format for downstream processing. It also filters out irrelevant content, refining problem-specific information to enhance LLM reasoning. Additionally, it captures implicit graph-structural information, improving problem-solving efficiency and mitigating the impact of text length on model comprehension and inference.\nTextual Information Extraction Agent (TIEA): The TIEA analyzes real-world graph-theoretic problems, extracting key textual information not directly related to the graph structure or solution objectives. Using NLP techniques, it identifies and structures contextual descriptions, background information, entities, concepts, and definitions. Its goal is to organize context, terminology, and related concepts, providing semantic support for subsequent analysis and problem solving. The extracted information is then output in a standardized format for downstream processing.\nGraph Structure Information Extraction Agent (GSIEA): The GSIEA extracts implicitly embedded graph-structural information from text, particularly structured formats like tables, lists, adjacency matrices, or edge lists. It parses these inputs to identify nodes, edges, weights, and other topological properties, converting them into standardized graph representations (e.g., adjacency matrices or lists). This transformation enables downstream agents to efficiently use the extracted data for problem solving.\nProblem Information Extraction Agent (PIEA): The PIEA leverages LLMs' problem classification capabilities to analyze real-world graph-theoretic problems, identify their types, and extract key components. It classifies problems (e.g., shortest path, network flow, graph matching), extracts relevant constraints and objectives, and outputs the information in a structured format. This guidance improves the accuracy and efficiency of downstream problem-solving agents.\nFormally, the operation of IEL is:\n$$T\u2190 A_{TIEA}(P),$$\n$$P\u2190 A_{PIEA}(P),$$\n$$G\u2190 A_{GSIEA}(P)$$\n$$IEL_{output} = (T,P,G)$$\n(1)"}, {"title": "3.2 Knowledge Integration Layer (KIL)", "content": "The primary objective of this layer is to construct structured graph data with high representational capacity and integrate graph-theoretic principles for advanced modeling, thus enhancing the efficiency of the solution and the quality of optimization.\nStructured Graph Information Agent (SGIA): The SGIA standardizes the graph structure data extracted by the GSIEA, ensuring efficiency, consistency, and usability. It converts raw data into standard formats compatible with various environments, while performing data cleaning, deduplication, and format optimization to maintain accuracy. Additionally, it optimizes data storage and indexing based on algorithm requirements, enhancing computational efficiency for large-scale graphs. Without this agent, data inconsistencies, redundancy, and unoptimized structures could hinder algorithm performance. As a key component of MA-GTS, it ensures data standardization and optimization for efficient, scalable problem-solving.\nGraph Theory Agent (GTA): The GTA integrates information from the TIEA and PIEA to analyze graph-theoretic problems and determine optimal solution strategies using a pre-stored Graph Theory Knowledge Base, enhancing LLM inference efficiency. It models the input problem by extracting key features such as type, constraints, and structural complexity, then queries the Graph Theory Knowledge Base to select the most suitable solution method from classical algorithms (e.g., shortest path, maximum flow, graph matching)(Gallo and Pallottino, 1988; Goldberg and Tarjan, 1988) and heuristic techniques. By matching problems to algorithms, it reduces inefficient exhaustive searches, cutting computational costs and improving solution quality. Additionally, it provides guidance for multi-agent collaboration, enabling the AEL to invoke the optimal algorithm directly, ensuring efficiency and scalability. Without this agent, LLMs may face suboptimal strategy selection, excessive computation, and reduced efficiency. As a core component of MA-GTS, it is vital for algorithm selection and inference optimization in complex graph-theoretic tasks.\nFormally, the operation of KIL is:\n$$L_p \u2190 A_{GTA}(T,P,K_G),$$\n$$Alg^* arg opt_{Alg_i \u2208 L_p} A_{GTA} (Alg_i,T),$$\n$$G' \u2190 A_{SGIA}(G),$$\n$$KIL_{output} = (G', Alg^*)$$\n(2)\nwhere $L_p$ represents the set of graph theory algorithms selected by GTA based on textual and problem-specific information, $K_G$ denotes the Graph Theory Knowledge Base, $Alg^*$ refers to the algorithm most suitable for the given graph size, and $G'$ stands for the normalized graph structure data."}, {"title": "3.3 Algorithm Execution Layer (AEL)", "content": "The primary goal of this layer is to integrate multiple algorithmic paradigms, ensuring efficient, scalable, and robust solutions under various constraints. Without it, the MA-GTS framework would rely solely on LLM-based inference, leading to high computational costs, instability, or suboptimal outcomes. As the computational core, the AEL enables the efficient solution of complex graph-theoretic problems across varying scales and complexities.\nAlgorithm Solving Agent (ASA): The ASA is the core computational unit of the AEL, responsible for solving problems by executing algorithmic functions based on the optimal strategy selected by the GTA and the structured graph data processed by the SGIA. It utilizes a Graph Theory Algorithm Library that integrates exact algorithms(Noto and Sato, 2000) and heuristic approaches, ensuring optimal or near-optimal solutions across various problem scenarios. After computation, the agent performs result integration and verification through cross-validation, error analysis, and constraint checking to ensure correctness. It also provides explainable reasoning, offering inference paths, key decision points, and optimization steps to enhance transparency. As the computational core of MA-GTS, the ASA enables efficient, robust, and scalable solutions for complex graph-theoretic problems.\nFormally, the operation of AEL is:\n$$Code_{Alg^*} \u2190 A_{ASA}(Alg^*, L_{code}),$$\n$$S_{code} \u2190 A_{ASA} Coding(Code_{Alg^*}, G'),$$\n$$S^0 \u2190 A_{ASA}(S_{code}, Alg^*, G'),$$\n(3)\nwhere $L_{code}$ represents the Graph Theory Algorithm Library, $Code_{Alg^*}$ denotes the code obtained after optimal algorithm matching by ASA, $S_{code}$ refers to the output generated by running the code, and $S^0$ represents the interpretable output obtained by combining the code output with problem review. Finally, ASA undergoes n rounds of self-checking, ultimately producing the final suitable result, $S^n$."}, {"title": "4 G-REAL", "content": "Existing datasets for evaluating LLMs' understanding and reasoning on graph-structured data are explicitly constructed. However, real-world graph-theoretic problems often involve rich textual semantic information and implicitly structured representations. To assess the performance of the MA-GTS framework on practical problems, we introduce G-REAL, a dataset that captures real-world graph problems. This dataset comprises three commonly encountered graph-theoretic challenges: (1) the optimization of logistics and delivery routes, (2) wireless network channel allocation, and (3) network monitoring optimization. These correspond to three fundamental graph problems: the Traveling Salesman Problem (TSP), the Minimum Graph Coloring Problem, and the Minimum Vertex Cover Problem, respectively(Hoffman et al., 2013; Jensen and Toft, 2011; Hochbaum, 1982). The composition of G-REAL can be seen briefly in Figure 2. In this section, we provide a detailed description of the dataset's composition and construction methodology. More detail about G-REAL in Appendix C."}, {"title": "4.1 Data Collection", "content": "To mitigate the risk of data contamination in LLMs, which could lead to biased test accuracy due to prior exposure to training data, G-REAL employs several techniques, including randomized node naming, synthetic node descriptions, added textual noise, and randomly structured graph representations. Node names are generated by randomly combining the 26 letters of the alphabet, and synthetic node descriptions are created with arbitrary textual representations. For example, a node may be described as: \"Amber Plaza: A bustling central square surrounded by cafes, boutiques, and street performers.\" These fictional descriptions ensure that LLMs cannot leverage prior knowledge, maintaining the integrity of the evaluation.\nTo improve dataset realism and obscure graph structure, we introduce textual noise to each instance, simulating real-world graph problems embedded in unstructured text. Graph structures are randomly generated, with each node assigned a unique name to reduce prior LLM exposure. Optimal and approximate solutions are generated for each problem type using established algorithms, providing benchmarks for evaluating both LLM and MA-GTS performance."}, {"title": "4.2 Data Statistics", "content": "To evaluate our framework's effectiveness in real-world graph-theoretic problems, we construct test datasets with graph sizes from 8 to 25 nodes for each problem type. Each sub-dataset includes 50 instances with distinct structures, offering both optimal and approximate solutions for a comprehensive assessment of robustness and generalization. A statistical summary is provided in Table 1."}, {"title": "4.3 Evaluation", "content": "For the TSP, Minimum Graph Coloring Problem, and Minimum Vertex Cover Problem, the unique nature of their outputs requires a dual evaluation approach. Specifically, the results consist of both a set of selected nodes and the final computed solution. To comprehensively assess the graph reasoning capabilities of LLMs, we consider both types of outputs as evaluation metrics. The model's performance is measured by verifying the accuracy of both the selected node set and the computed solution. The methodology for calculating the final accuracy is as follows: $ACC_{ALL} = 0.5 \u00b7 ACC_{nodes} + 0.5 \u00b7 ACC_{result}$, where $ACC_{nodes}$ and $ACC_{result}$ represent the accuracy of the node set and the predicted values, respectively, with a value of 1 for correct predictions and 0 for incorrect ones."}, {"title": "5 Experiments Setup", "content": "5.1 Datasets\nTo evaluate the reasoning capabilities of the MA-GTS framework across various graph-theoretic problem types, complexities, and domains, we used the G-REAL dataset alongside two benchmark datasets, GraCoRe(Yuan et al., 2025) and NL-Graph(Wang et al., 2024), covering six distinct graph-theoretic tasks. We selected three sub-tasks for evaluation: the TSP, shortest path problem, and Cycle problem in GraCoRe and NLGraph. Notably, both GraCoRe and G-REAL include TSP instances; however, the G-REAL TSP is more complex and reflects real-world scenarios with implicit graph structure data. By comparing performance on these two TSP instances, we assess the model's ability to handle more intricate problems. The simpler tasks in NLGraph evaluate the generalization and robustness of the MA-GTS framework. A summary of the differences between these datasets is provided in Table 1."}, {"title": "5.2 Baselines and Foundation Model", "content": "We compared three of OpenAI's latest closed-source models: 03-mini, GPT-40-mini, and GPT-3.5(Achiam et al., 2023). Additionally, we evaluated two of the most recent open-source models: Llama3-7b(Touvron et al., 2023) and Qwen2.5-7b(Bai et al., 2023). For the evaluation methodology, we adopted both direct inference and CoT reasoning approaches. For the foundation model, we selected the GPT-40-mini model. Regarding the final test results, for each task, we used the accuracy of the final computed solution as the primary evaluation metric. More details about models in Appendix B."}, {"title": "6 Results and Analysis", "content": "In this section, we evaluate the performance of our framework against other LLMs on graph theory problems, with results presented in Table 2. MA-GTS outperforms all baselines, achieving state-of-the-art results and matching the performance of the leading o3-mini model on simpler problems. We also assess the MA-GTS framework from multiple perspectives."}, {"title": "6.1 Performance on real-world problems", "content": "As shown in Table 2, G-REAL provides three real-world graph theory problems, with the TSP being the most complex. Based on the results from these three problems, MA-GTS demonstrates superior performance, achieving an accuracy rate exceeding 90% across all tests. Notably, in the case of the TSP, MA-GTS outperforms the o3-mini model by 82%. Furthermore, when compared to the GPT-40-mini model, MA-GTS significantly improves its performance from 3.1% to 94.9%, marking a substantial increase. This clearly underscores the effectiveness of our framework. Additionally, it is evident that, aside from the o3-mini model, other models exhibit subpar performance on the G-REAL dataset. It is particularly interesting that the performance gap between the two open-source and two closed-source models is minimal, suggesting that the complexity of the problems may lead to a consistent decline in performance, an issue that warrants further investigation. Overall, MA-GTS stands out for its advanced capabilities and generalization when handling complex graph theory problems."}, {"title": "6.2 Performance on simple problem", "content": "Table 2 shows that for simpler graph theory problems, such as the Shortest Path and Cycle problems from the NLGraph dataset, the o3-mini model performs exceptionally well, with MA-GTS also showing strong results. Specifically, for the Shortest Path problem, the gap between MA-GTS and o3-mini is just 2.2%, and MA-GTS performs equally well on the Cycle problem. In contrast, other models perform less satisfactorily. The MA-GTS framework, based on the GPT-40-mini model, significantly enhances the accuracy of the 40 model, bringing it on par with the o3-mini. Overall, MA-GTS demonstrates excellent performance across diverse textual descriptions and graph structures, highlighting its remarkable generalization capabilities."}, {"title": "6.3 G-REAL effectiveness analysis", "content": "To evaluate the performance of LLMs and MA-GTS on real-world graph theory problems, we constructed the G-REAL dataset. As shown in Table 2, the performance of existing LLMs on the G-REAL dataset is suboptimal. To validate the effectiveness of this dataset, we compared it with the TSP problem from the GraCoRe Benchmark, testing problems with node sizes ranging from 8 to 25, consistent with the scale of G-REAL. From this comparison, we observe that on the G-REAL dataset, which includes text complexity, added text noise, and node name shuffling, the o3-mini model performs poorly, with its accuracy dropping from 79.7% in GraCoRe to 11.8%. In contrast, the MA-GTS framework appears unaffected by the complexities of real-world graph theory problems, maintaining performance above 90%. This result indirectly supports the validity of the G-REAL dataset and demonstrates the stability of the MA-GTS framework."}, {"title": "6.4 Impact of Node Size", "content": "To evaluate the impact of node scale on LLMs in complex graph theory problems, we tested the performance of MA-GTS and the o3-mini model on four complex graph problem datasets, with node sizes ranging from 8 to 25. The results, shown in Figure 3, clearly demonstrate that as the number of nodes increases, the performance of the o3-mini model deteriorates, particularly in the TSP problem from G-REAL. For node sizes greater than 20, the o3-mini model is unable to produce correct answers. In contrast, under the MA-GTS framework, the effect of node size is less pronounced. Even with more than 20 nodes, MA-GTS maintains high prediction accuracy and stability. This highlights both the effectiveness and superiority of our framework."}, {"title": "6.5 Cost Analysis", "content": "Since MA-GTS requires multiple agent calls to model APIs for inference, cost considerations arise. To address this, we compared the inference costs of MA-GTS based on the GPT-40-mini model with the o3-mini model, as shown in Table 3. Surprisingly, MA-GTS incurs significantly lower costs than the o3-mini model. The o3-mini model, in contrast, has hidden reasoning tokens during inference, leading to long, concealed reasoning processes even in direct inference scenarios. As shown in the table, the inference cost of MA-GTS is about one-tenth to one-twentieth of the o3-mini model, requiring far fewer inference tokens. Moreover, MA-GTS achieves far better results than o3-mini, demonstrating its high cost-effectiveness in delivering more accurate outcomes at a lower cost."}, {"title": "6.6 Ablations Studies and Analyses", "content": "To validate the effectiveness of each layer in MA-GTS, we conducted ablation experiments, with results shown in Table 4. The table demonstrates that each layer is crucial, and removing any layer significantly affects the final results. Although the IEL layer has the smallest impact on accuracy, its absence leads to a substantial increase in error rate (19%), highlighting its role in maintaining stability. The absence of the AEL layer results in the greatest accuracy loss. Even when a module is removed, MA-GTS still improves the accuracy of the base model, validating the framework's effectiveness. Additionally, when inference is performed using only the GPT-40-mini model with the constructed algorithm library, accuracy improves, but the error rate remains high (75%). For graph sizes larger than 10 nodes, the model struggles to correctly invoke algorithms, further demonstrating the robustness and generalizability of MA-GTS."}, {"title": "7 Conclusion", "content": "This paper introduces MA-GTS, a Multi-Agent Framework for solving real-world graph theory problems, validated using the G-REAL dataset. Performance comparisons across various LLM models show that MA-GTS achieves high accuracy, stability, and cost-effectiveness, excelling in both complex and simpler graph problems. With accuracy consistently above 90%, MA-GTS outperforms traditional LLM approaches, maintaining stability across different problem scales and being well-suited for larger graphs. Future work will focus on scaling to even larger problems and improving cost-efficiency."}, {"title": "Limitations", "content": "Although the MA-GTS framework demonstrates significant advantages in addressing complex graph-theoretic problems, several limitations remain. First, while the G-REAL dataset provides valuable support for validating the framework's effectiveness, it may not fully capture the diversity of real-world graph problems, thus limiting the generalizability of the framework. Second, the MA-GTS framework may still require substantial computational resources when handling large-scale problems, particularly in resource-constrained environments. Moreover, despite the improvements made in enhancing LLMs' graph structure modeling capabilities, LLMs may still encounter performance bottlenecks when dealing with graphs that exhibit highly dependent relationships or specialized structures. Finally, the current capabilities of open-source model invocation tools are insufficient, which may impact the stability of the MA-GTS framework."}]}