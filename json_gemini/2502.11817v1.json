{"title": "AAKT: Enhancing Knowledge Tracing with Alternate Autoregressive Modeling", "authors": ["Hao Zhou", "Wenge Rong", "Jianfei Zhang", "Qing Sun", "Yuanxin Ouyang", "Zhang Xiong"], "abstract": "Knowledge Tracing (KT) aims to predict students' future performances based on their former exercises and ad-ditional information in educational settings. KT has received significant attention since it facilitates personalized experiences in educational situations. Simultaneously, the autoregressive mod-eling on the sequence of former exercises has been proven effective for this task. One of the primary challenges in autoregressive modeling for Knowledge Tracing is effectively representing the anterior (pre-response) and posterior (post-response) states of learners across exercises. Existing methods often employ complex model architectures to update learner states using question and response records. In this study, we propose a novel perspective on knowledge tracing task by treating it as a generative process, consistent with the principles of autoregressive models. We demonstrate that knowledge states can be directly represented through autoregressive encodings on a question-response alternate sequence, where model generate the most probable representation in hidden state space by analyzing history interactions. This approach underpins our framework, termed Alternate Autoregressive Knowledge Tracing (AAKT). Additionally, we incorporate supplementary educational information, such as question-related skills, into our framework through an auxiliary task, and include extra exercise details, like response time, as additional inputs. Our proposed framework is implemented using advanced autoregressive technologies from Natural Language Generation (NLG) for both training and prediction. Empirical evaluations on four real-world KT datasets indicate that AAKT consistently outperforms all baseline models in terms of AUC, ACC, and RMSE. Furthermore, extensive ablation studies and visualized analysis validate the effectiveness of key components in AAKT.", "sections": [{"title": "I. INTRODUCTION", "content": "It is imperative for intelligent tutoring systems and on-line learning platforms to effectively depict the evolving knowledge status of students. To address this requirement, the fundamental task of Knowledge Tracing (KT) has been introduced [1]. KT involves tracing the mastery level of indi-vidual students over time and generating predictions regarding the accuracy of their responses to diverse questions based on their knowledge status. This process entails collecting the historical exercise sequences of students, computing their mastery levels, and forecasting the likelihood of correct re-sponses to subsequent exercises. By executing Knowledge Tracing, educators gain insights that facilitate the provision of personalized learning materials, identification of weaknesses, and targeted exercise recommendations.\nConventional methodologies within this domain mainly in-clude Bayesian Knowledge Tracing (BKT) employing Hidden Markov Models [1], [2], [3], and Deep Knowledge Tracing (DKT) utilizing Deep Neural Networks [4] along with its derivative models [5], [6]. While the initial Knowledge Tracing models, such as BKT, exhibit ease of implementation and enhanced interpretability, their efficacy is hindered by their inability to capture the dynamic nature of knowledge status due to oversimplified assumptions concerning the learning process.\nThe initial DKT models use RNNs, particularly LSTM and GRU architectures, to represent students' knowledge status [4], [7], [8]. However, RNN-based models concentrate on nearby exercises, limiting the ability to model long-term dependen-cies. To address this limitation, Sequential Key-Value Mem-ory Networks (SKVMN) [9] incorporate LSTM with hops to capture extended dependencies within exercise sequences. Despite this improvement, it does not fully overcome the challenge posed by the limited attention span. The Transformer architecture [10], known for its effectiveness in sequence-to-sequence prediction tasks with the attention mechanism, influences certain KT models. These models integrate atten-tion by incorporating a self-attention mechanism into Deep Knowledge Tracing (DKT) [11], [12], [13]. The key advantage of self-attention-based models lies in their ability to predict individual interactions by considering the entire historical exercise sequence. This addresses concentration imbalances and enhances the holistic perspective during predictions.\nDespite advancements in DNNs, current KT models still exhibit limitations which can be attributed to three factors. Firstly, some prior knowledge tracing models encode question sequence and history interaction sequences separately [11], [14], [12], [15]. These models initially encode the information of history interactions and the next question into hidden states separately. This late fusion method neglects shallow interac-tions and combines the two types of information only after they have been extracted into highly abstract states, resulting in a loss of bilateral information. Additionally, empirical findings reveal that, despite having more parameters, the self-attention-based AKT model fails to outperform the RNN-"}, {"title": "II. RELATED WORK", "content": "Corbett and Anderson introduced Bayesian Knowledge Tracing (BKT) as the initial knowledge tracing model, utilizing binary variables to predict a student's mastery or non-mastery of skills within a set [1]. BKT relies on assumptions about variable distributions, including response accuracy and ques-tion difficulty [17], [18]. The model considers four factors (i.e., initial knowledge status, learning rate, guessing probability, and slipping probability) to predict knowledge status based on the most recent response. Subsequent research expanded BKT by introducing factors like problem difficulty or individual learning ability [19], [20]. Additionally, Knowledge Tracing Machine (KTM) [21] employs factorization machines [22] to enhance the interpretability and performance of BKT. How-ever, the interpretability of BKT models relies on simplified as-sumptions, limiting their ability to capture dynamic knowledge status and provide comprehensive explanations of students' learning processes, thereby negatively impacting overall model performance.\nTo address the limitations of traditional KT models, Piech et al. [4] pioneered the integration of deep neural networks with knowledge tracing, introducing the DKT model. DKT employed Long Short-Term Memory recurrent neural net-works (LSTM) [23] to model students' knowledge status, sur-passing the performance of BKT and its extensions. Seeking enhanced prediction consistency in exercise sequences, DKT+ [7] improved upon DKT by introducing regularization on the LSTM's hidden state, smoothing the evolution of knowledge status. Subsequent to DKT, researchers delved further into the application of neural networks in knowledge tracing. Zhang et al. [8] proposed a Dynamic Key-Value Memory Network (DKVMN) employing external memory matrices to store knowledge memory and update corresponding mastery levels. Building on DKVMN, Abdelrahman and Wang [9] introduced a modified LSTM, hop-LSTM, in their model. Zhu et al. [24] first employs causal inference for explanatory analysis of knowledge tracing, then proposes a learning algorithm for stable KT based on the analysis outcomes. Guo et al. [16] utilized adversarial training in Adversarial Knowledge Tracing (ATKT) to enhance model robustness and mitigate overfitting in small datasets. Wang et al. [25] integrated educational priors and RNN in a KT model to improve interpretability.\nBuilding on the success of attention mechanisms and trans-formers [10] in natural language processing and computer vision, self-attention-based knowledge tracing models have emerged. Pandey and Karypis [11] were pioneers in intro-ducing the self-attention mechanism to Knowledge Tracing models (SAKT). This innovation enables SAKT to capture long-term dependencies within historical exercises. Pu et al. [15] proposed a KT model based on the vanilla transformer. Additionally, Ghosh et al. [12] presented a context-aware attentive KT model featuring monotonic attention and an exponential decay mechanism. More recently, Cui et al. [26] introduced a multi-relational transformer designed to facilitate fine-grained interaction modeling between question-response pairs. Qiu et al. [27] proposed the optimized pretraining deep knowledge tracing (OPKT) method, which enhances knowledge state assessment by self-supervised learning, com-prehensive contextual encodings and extracting latent features from learning sequences.\nDespite notable advancements, self-attention-based KT models encounter challenges in effectively integrating infor-mation from questions and responses. While various design features attempt to model both types of information, the interactive relationship between them is partly neglected. In SAKT [11], history interactions, including questions and responses, and future questions are encoded into different sequences, leading to inconsistencies in semantic spaces. AKT [12] calculates self-attention within question sequences and response sequences separately, employing a knowledge re-triever to partially combine the two types of information. However, the information from responses does not contribute to calculating attention weights in knowledge retrievers, re-sulting in inadequate information interaction. MRT-KT [26] establishes multiple relations between different exercises based on their question skill sets and response results. However, it omits including response information as encoded input due to the lack of an effective autoregressive sequence construction method.\nIn response to the identified limitations of self-attention-based KT models, we advocate for better integration of ques-tions and responses. Subsequently, we conducted an analysis of the causal relationships inherent in knowledge tracing tasks. Our proposal introduces an alternative autoregressive modeling approach based on a novel sequential representation that encompasses both question and response information."}, {"title": "III. METHODOLOGY", "content": "In this section, we present the details of our AAKT frame-work. The overall architecture is depicted in Fig. 2. The initial segment encompasses Alternate Sequence Construction, a process that amalgamates the question sequence with the question and response sequences to form a distinctive se-quence. This is succeeded by the Sliding Window mechanism, which selects subsequences from the alternate sequence for both training and prediction purposes. The second segment introduces Additional Information into the sequential em-beddings, capturing inter-sequence dependencies through an autoregressive transformer. This component predicts responses to each question in the sequence, employing Binary Cross En-tropy (BCE) loss calculated based on the prediction outcomes and response label sequences. The third and final segment conducts Auxiliary Task and focuses on compelling the question embeddings to assimilate skill information. This is achieved by predicting the skill distribution for each question and subsequently minimizing the Kullback\u2013Leibler divergence with the actual skill distribution.\n\\subsection{A. Problem Formulation}\nThe notations needed to formulate the target of knowledge tracing task and the principle of AAKT are listed in Table I.\nGiven the exercise sequence I of a certain student, knowl-edge tracing seeks to monitor the student's evolving knowl-edge status by predicting the correctness of future questions."}, {"title": "B. Preliminaries", "content": "As presented in the first contribution in Section I, the generative perspective of knowledge tracing task aligns with the setting of autoregressive modeling. We initiate the detailed interpretation by introducing the concept of autoregressive models."}, {"title": "C. Alternate Autoregressive Modeling", "content": "\\paragraph{1) Alternate Sequence Construction:} Some current KT models split the raw datasets into two kinds of sequences, namely history sequence and query sequence [11], [16], [31]. In accordance with the notations outlined in Table I, the exer-cise sequence I = {Ex1, Ex2,... } is divided and transferred into History Sequence and Query Sequence:\n$Hi = {h1, ..., hi}$\n$= {fh(91, t1, \u010d1), ..., fh (\u011fi, ti, \u010di)},$\n$Qi = {qry1,..., qry\u017c}$\n$= {fqry(91, t1),..., fqry (qi, ti)}.$\nIn Eq. 3, fh(\u00b7) and fqry(\u00b7) denote functions that embed ar-guments into a vector. history sequence takes every (question, skills, response) tuple in the history interactions, which is the full sequence of student's interactions in the learning process, and transform every tuple into a vector. Query sequence takes every (question, skills) tuple and does similar transformation. The differences between them lie in the choice of tuple and the transformations. These models learn from students' history exercise sequences and transfer them into hidden knowledge status of every step first. Subsequently, the next element in query sequence is combined with history sequence, generating a predictive result for the correctness of answering the next exercise:\n$\u0108i+1 = Predict(Hi, qryi+1)$\nwhere Predict() denotes prediction layers of models. Fol-lowing the guideline of Eq. 3 and Eq. 4, certain previous KT models developed two main kinds of modeling strategy (refer to Fig. 3(a) and Fig. 3(b)).\nTo address the above-mentioned issues, CL4KT [32] and LBKT [33] utilize the method demonstrated in Fig. 3(b), simultaneously considering knowledge states and question-integrated knowledge states. Here, the models capture stu-dents' knowledge states after completing n exercises and when encountering the (n + 1)th question. While this method mitigates generalization issues, it imposes computational over-head and introduces inconsistency by encoding the same information (i.e., q<n and r<n) twice."}, {"title": "2) Sliding Window Technique", "content": "To optimize information utilization within the datasets, we employ the sliding window technique during preprocessing, permitting overlaps in both the training and testing datasets. However, owing to consider-ations of validity in the testing process and metric calculations, there are slight variations in the specific steps for training and testing. The sliding window technique is illustrated in Fig. 4. In this depiction, two key hyperparameters are set: max sequence length Lmax, representing the maximum length in a data batch, and overlap ratio ro, denoting the extent to which the next sliding window covers the current one (refer to Table I).\nFor the training dataset, an individual student's alternate exercise sequence (refer to Eq. 5) is partitioned into subse-quences. The window initiates its movement from the com-mencement of the raw sequence and progresses towards the end. At each step, it moves forward by Lmax\u00b7(1-ro) elements, where Lmax and ro represent the maximum sequence length and overlap ratio, respectively. Additionally, the value of Lmax (1-ro) must be even, as a complete interaction involves two elements in the sequence (question and answer). If the range of the last window extends beyond the raw sequence, the remainder is masked, ensuring it is not considered when computing the loss. This technique effectively augments the dataset volume by introducing overlaps:\n$Dataset' \u2248  Dataset.$\nHandling the testing dataset and computing metrics involves some additional processes. In testing, all overlaps in the alternate sequence must be masked at corresponding positions (refer to Fig. 4(b)). This precaution is taken to prevent the same interaction from being counted multiple times during metric calculations, thereby avoiding inconsistency and inaccuracies. The efficacy of the sliding window technique in testing has been validated and shown to enhance prediction performance (refer to Section IV)."}, {"title": "D. Auxiliary Task in Embedding", "content": "To effectively integrate information from both questions and related skills, we introduce an auxiliary task in our model. We"}, {"title": "E. Processing of Additional Information", "content": "In the context of online education platforms, interaction records encompass diverse attributes, including timestamps and student gender. Considering these attributes in knowl-edge tracing tasks can be advantageous, as relying solely on question indices and skill sets may inadequately capture the nuances of provided exercises. In addition to semantic information, where some knowledge tracing models leverage textual question descriptions [34], [35], interaction-related information can be categorized into four distinct groups, as illustrated in Fig. 5.\nIn the AAKT framework, we introduce a viable approach to incorporate additional information, both anterior and posterior, into alternate sequences. As illustrated in Fig. 3, a compre-hensive representation of a specific interaction involves two elements in the input sequence. Essentially, anterior informa-tion should be combined with qi, while posterior information should be combined with ri at step i.\nIn our model, we have focused on utilizing one category: posterior continuous information, specifically the 'total time spent'. This choice is informed by the significance of time spent on exercises in the context of online education and the nature of the datasets. The total time spent values range from 0 ms to approximately 4 \u00d7 105 ms. To bring them into a standardized range (0 ms to 2 \u00d7 105 ms), we applied clipping. Additionally, a constant called Time Factor (\u03c4 = 60000 ms) was introduced to partially normalize these values:\n$timenorm = $\nAfter normalization, these values become dimensionless and can be integrated into the model. More precisely, the normal-ized time values undergo multiplication by a trainable vector, denoted as vtime, followed by addition to the embedding vectors of ri (refer to Eq. 5):\n\u00e3i = timenorm,\n$fq(\u00e3i, \u010di) = $.\nHere, embright and embwrong are two trainable vectors."}, {"title": "IV. EXPERIMENTS", "content": "In this section we evaluate the proposed model's perfor-mance through a series of experiments on real-world datasets\u00b9.\nThe ablation studies and visualizations are also conducted to further validate the effectiveness of specific components of AAKT.\n\\subsection{A. Datasets}\nTo evaluate the performance of AAKT, we utilize four real-world datasets, each containing information on the total time spent on every question. The statistics of four datasets are shown in Table II and Fig. 6."}, {"title": "B. Baselines and Evaluation Metrics", "content": "We compare AAKT with the following advanced KT mod-els. Specifically, we classify the baseline into two categories according to the modeling strategy illustrated in Fig. 3.\nBaselines using knowledge states and question representa-tions:\nDKT [4], the first knowledge tracing model to leverage deep neural networks, employs a long short-term memory recurrent neural network (LSTM) for sequential predic-tion of student performance. By extracting the hidden state of the LSTM, the model can deduce the student's mastery level across various skills.\nDKT+ [7] is an improvement of DKT. It adds a regular-ization term on the hidden states of students, smoothing the evolving of knowledge status.\nDKVMN [8] extends DKT by introducing extra memory-augmented neural networks. Two matrices (namely key matrix and value matrix) are used to better trace students' hidden knowledge status. Moreover, DKVMN utilizes the 'read' and 'write' processes to emulate the process of learning.\nSAKT [11] is the initial knowledge tracing model to incorporate a self-attention mechanism. Its overall archi-tecture closely mirrors that of the transformer.\nAKT [12] models the influence of history interactions on students' current learning status by adopting an innova-tive monotonic attention. It also introduces the difficulty coefficient when encoding skills.\nKQN [31] introduces the concept of probabilistic skill similarity, which links specific metrics of distance be-"}, {"title": "C. Implementation", "content": "1) Details of Autoregressive Transformer: In our model, we utilize GPT-J [40] instance (without pre-trained weights) as the autoregressive transformer. The design of GPT-J closely follows GPT-3 Curie [41]. However, two minor architectural improvements have been made:\nRotary Positional Embedding (RoPE) [42] is adopted for better performance.\nThe attention layer and the feedforward layer are placed in parallel for decreased communication.\n2) Other Details: We conduct 5-fold cross-validation for each combination of models and datasets. For training, we use 80% of the student sequences, while the remaining 20% are utilized for model evaluation.\nOur AAKT model is implemented with PyTorch and we use Adam optimizer to train our model, setting the learning rate to 0.001 for all datasets. All experiments are conducted on a server with one NVIDIA A5000 (24GB) graphic card. Balancing time cost and performance, we set the overlap ratio ro to 0.5 both in training and testing. Moreover, the implementations of baselines are based on open-source code provided by corresponding authors and pyKT [43], a com-prehensive python-based benchmark platform for knowledge tracing tasks.\nDetails of other hyperparameters are listed below. For hyperparameters that are not fixed, we utilized grid searching to find the optimal choice.\nBatch size is chosen from {32, 64, 128, 256}.\nDimension of embedding is chosen from {64, 96, 128, 256}.\nMax sequence length is chosen from {20, 50, 100, 300, 500, 1000, 2000} according to the distribution of sequences in each dataset.\nThe number of heads in multi-head attention is set to 8. This configuration is borrowed from vanilla Transformer [10].\nThe number of GPT-J blocks is chosen from {2,3,4}.\nThe dimension of rotary position embedding is set to demb where demb and nhead denotes embedding di-mension and number of heads in multi-head attention respectively. This configuration is borrowed from the GitHub repository ChatGLM-Finetuning 5."}, {"title": "D. Results and Discussions", "content": "In this section, we list the average results of cross-validation in Table III (- means that the model does not fit the dataset), where the highest results in every column are dis-played in bold and the second highest results are underlined. From the data shown in Table III, the following observations regarding baselines and AAKT can be made:\nAAKT outperforms the state-of-the-art models on four datasets (i.e., EdNet-KT1, ASSISTments2009, ASSIST-ments2017, and Junyi) and three metrics, validating its effectiveness in knowledge tracing tasks.\nDKT and DKT+ have relatively high performances on EdNet-KT1 and ASSISTments2017, outperforming mod-els with more complicated structures like DKVMN and SAKT. DKT and DKT+ are simple models that merely utilize the LSTM structure. This implies that more skillful techniques should be explored in modeling the learning sequences of students for knowledge tracing and com-plexity does not necessarily lead to accuracy.\nTransformer-based models in baselines (i.e., AKT and DTransformer) perform well on four datasets. Notably, DTransformer achieves the second highest metric on EdNet-KT1, ASSISTments2009, and ASSISTments2017. The utilization of the diagnostic transformer enables DTransformer to outperform SAKT and AKT which use simpler attention techniques and training paradigms.\nContrastive learning is applicable in knowledge tracing and demonstrates great potential. Models with contrastive learning (i.e., CL4KT and DTransformer) have relatively high performance compared to all chosen baselines.\nMoreover, to further investigate the inter-metric relationship in Table III, we calculate the correlation coefficient of every"}, {"title": "E. Ablation Study", "content": "To further investigate the contribution of key components in AAKT, we conducted a series of ablation studies. Variants of"}, {"title": "F. Visualization", "content": "In this subsection, we conduct some visualizations to get deeper insights into some key components of AAKT.\n1) Case Study: In this part, a simple case study is con-ducted to reinforce the validation of alternate sequence con-struction and autoregressive modeling. We select a specific al-ternate sequence from EdNet-KT1 where our model predicted the first 8 answers correctly. Moreover, the attention weights of the first attention layer in our model are extracted when"}]}