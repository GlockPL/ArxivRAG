{"title": "IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos", "authors": ["Yunong Liu", "Cristobal Eyzaguirre", "Manling Li", "Shubh Khanna", "Juan Carlos Niebles", "Vineeth Ravi", "Saumitra Mishra", "Weiyu Liu", "Jiajun Wu"], "abstract": "Shape assembly is a ubiquitous task in daily life, integral for constructing complex 3D structures like IKEA furniture. While significant progress has been made in developing autonomous agents for shape assembly, existing datasets have not yet tackled the 4D grounding of assembly instructions in videos, essential for a holistic understanding of assembly in 3D space over time. We introduce IKEA Video Manuals, a dataset that features 3D models of furniture parts, instructional manuals, assembly videos from the Internet, and most importantly, annotations of dense spatio-temporal alignments between these data modalities. To demonstrate the utility of IKEA Video Manuals, we present five applications essential for shape assembly: assembly plan generation, part-conditioned segmentation, part-conditioned pose estimation, video object segmentation, and furniture assembly based on instructional video manuals. For each application, we provide evaluation metrics and baseline methods. Through experiments on our annotated data, we highlight many challenges in grounding assembly instructions in videos to improve shape assembly, including handling occlusions, varying viewpoints, and extended assembly sequences.", "sections": [{"title": "Introduction", "content": "The autonomous assembly of complex 3D structures requires an understanding at multiple levels of abstraction. The top level is task planning-decomposing the task into subtasks and computing their dependency and ordering, as outlined in an instruction manual; the middle level is visual grounding-registering each part with perceptual input at the pixel level, identifying their geometry and pose, so that the manual instructions get translated to actionable steps; the bottom level is motion planning and control-executing the steps based on the instructions and the identified states, avoiding collisions given the specific embodiment. A successful assembly requires solving all of these problems together, which is difficult. Thus, assembly remains a significant challenge in AI and robotics.\nWhen developing assembly benchmarks, researchers often choose IKEA furniture as target objects due to its ubiquity and standardization. However, existing benchmarks and datasets focus only on part of the assembly problem. Classic work on designing assembly instructions provides step-by-step guidance for task planning, but they are not visually grounded [1]; recent work has attempted to ground assembly steps to 3D part models and register them at the pixel level [2], but it does not include action trajectories of how the assembly may actually happen; datasets that provide 3D groundings in the form RGB-D videos are collected in controlled lab environments [3, 4]; a video dataset of IKEA furniture assembly from the Internet includes more diverse demonstrations [5], but it lacks correspondence with 3D part models and alignment with the instruction manuals.\nTo address these limitations, we introduce the IKEA Video Manuals dataset, a multimodal dataset with high-quality, spatial-temporal alignments of step-by-step instructions, 3D models, and real-world video demonstrations from the Internet. IKEA Video Manuals provides 34,441 annotated video framesfrom 98 assembly videos for 6 furniture categories. We provide extensive video annotations, including 2D-3D part correspondences, temporal step alignments, and part segmentation. The key contributions of our work include:\n\u2022 A novel multimodal dataset built on top of Internet videos to capture the complexity and diversity of real-world furniture assembly;\n\u2022 Comprehensive annotations, including 2D-3D part correspondences, temporal step alignments, and part segmentation;\n\u2022 Extensive experiments on plan generation, part segmentation and pose estimation, video object segmentation, and part assembly with the annotated data."}, {"title": "Related Work", "content": "Instructional Video Datasets and Procedural Understanding. Instructional video datasets are essential for advancing the understanding and learning of procedural tasks. Existing datasets such as YouCook2 [6], COIN [7], and EPIC Kitchens [8] focus on cooking and daily activities, while datasets such as IKEA ASM [3], IKEA-FA [9, 10], and Assembly101 [11] target shape assembly. However, these datasets are predominantly annotated at the coarse level with action labels, limiting their utility for grounding procedural tasks in 3D. Instructional datasets have led to the development of various methods for procedural understanding, including action recognition, video classification, action segmentation, localization, and prediction [7, 12, 13]. To capture the hierarchical nature of procedural tasks, some methods utilize hand-centric features and script data [14]. Temporal and semantic relationships between actions in complex activities have been explored using graph-based representations [15\u201317]. Techniques such as unsupervised learning from narrated instruction videos aim to identify key procedural steps [18], and cross-task weak supervision has been introduced to improve transfer learning of step localization [12]. Despite advances in procedural understanding, current efforts are mostly limited to 2D video data without grounding in 3D. This absence of 3D context restricts the learning of spatial relations and object interactions essential to real-world task understanding. In this paper, we address these limitations by focusing on the 4D grounding of"}, {"title": "Grounding Assembly Instructions on Internet Videos", "content": "In this section, we formally define the spatio-temporal data involved in grounding assembly instruc-tions. We then present the features and analysis of our IKEA Video Manuals dataset."}, {"title": "Definition", "content": "As illustrated in Fig. 1, we provide a dataset of grounded assembly instructions for IKEA furniture. Each piece of furniture $S$ in the dataset consists of a set of 3D parts $\\{P_1, ..., P_N\\}$, where $N$ denotes the number of parts. The 6-DoF poses of the 3D parts are denoted by $\\{\\xi_1, ..., \\xi_n\\}$. The furniture is assembled by transforming the 3D parts according to the poses, i.e., $S = \\{\\hat{\\xi}_1 (p_1), ..., \\hat{\\xi}_N (p_N)\\}$. In our dataset, each video consists of a sequence of frames $\\{f_1, ..., f_T\\}$ and demonstrates a physically realistic assembly process (Fig. 1b). In the assembly process, 3D parts are combined, and new sub-assemblies are formed. We denote each sub-assembly by $A$, which can be an individual part or a set of previously combined parts. In each frame, we identify the sub-assemblies that are being constructed (Fig. 1e). We further localize each sub-assembly in each frame of the video with a segmentation mask, which assigns pixels to their associated sub-assembly (Fig. 1c). A 6-DoF pose of each sub-assembly in the camera's coordinate frame is included along with the camera intrinsic parameters (Fig. 1d). Combining the poses of the furniture parts throughout the video gives rise to the 4D grounding of the assembly video.\nTo provide high-level guidance of the assembly procedure, we also include the instruction manual for each piece of furniture in our dataset. Each instruction manual consists of a sequence of $L$ images $\\{m_1, ..., m_L\\}$ (Fig. 1a). Similarly to the videos, each image from the manual is also annotated with the identities, masks, and poses of the appeared sub-assemblies. We observe that instruction manuals often provide higher-level illustrations of assembly procedures than the assembly videos and omit details of the part trajectories. To associate the two types of instructions for 3D assembly, a temporal alignment between them is established as a mapping $\\phi(m_i) \\rightarrow \\{f_j, .., f_k\\}$, where $j \\leq k$."}, {"title": "Key Features", "content": "Multiple Instruction Forms for Assembly. Shape assembly is a complex task that requires geometric reasoning and planning; our dataset provides different types of instructions to facilitate the process, including high-level assembly tree, instruction manuals, and assembly videos. The high-level assembly tree omits geometric and spatial information about parts but provides the decompositions of the assembly process into smaller steps. Manuals provide pictorial illustrations of the assembly process. These illustrations provide both a coarse breakdown of the assembly process and relative poses between object parts in 2D. Compared to instruction manuals, assembly videos provide a more fine-grained assembly process where parts are assembled one at a time, and the whole trajectory of the object movement till the construction of each sub-assembly is shown.\nTemporal Alignment of Instruction and Assembly Process. Different instruction forms provide different temporal decomposition of the process. Instruction manuals often provide high-level decomposition, while how-to videos demonstrate more detailed steps of each part assembly. Our dataset aligns each step from the instruction manual with a sequence of substeps, in which sub-assemblies are formed (Fig. 1a and Fig. 1b). These substeps are further mapped to segments of the how-to videos, which provide a frame-by-frame demonstration of the assembly.\nSpatial Alignment of Instruction and Assembly Process. Our dataset further provides spatial details of the whole assembly process in 3D observed from the instruction manuals and videos. These details are provided in the form of 6-DoF pose trajectories of the furniture parts. Specifically, for each video frame, the parts being assembled are annotated with a 2D image mask. The pose of each object part in the camera frame is provided, while the relative poses between parts that are being assembled are detailed. With the additional camera intrinsics we provide, the 3D parts can also be projected into 2D image space and aligned with their corresponding 2D mask.\nDiversity in Assembly. As shown in Fig. 2, our dataset captures a wide range of furniture assembly scenarios from the Internet videos, encompassing various furniture types, designs, and assembly processes. The dataset includes six main furniture classes. Different instances of the same furniture class are also provided to present the difference in assembly due to designs, sizes, and structures. Furthermore, the dataset also includes multiple assembly videos for each instance. These videos capture various perspectives, environments, and individuals performing the assembly, allowing an in-depth analysis of the variability and commonalities in assembly processes.\nComplexity in Real World Videos. By including assembly videos from the Internet, our dataset captures the complexity inherent in real-world data. The assembly videos present visual challenges such as changes in camera parameters, camera movements, diverse environment backgrounds, and heavy occlusions. These challenges are representative of the difficulties encountered in practical applications and provide a meaningful benchmark for evaluating the performance and robustness of assembly understanding algorithms."}, {"title": "Comparison with Existing Datasets", "content": "Our dataset is the first to provide 6-DoF pose annotations for furniture assembly from internet videos, capturing real-world complexity across diverse settings. We compare it to existing assembly datasets in Table 1. Unlike prior video datasets with 3D information, our dataset features a significantly wider variety of objects (36 furniture types comprising 268 parts) and environments (over 90 different settings). Our dataset uniquely captures variations in the assembly process for each piece of furniture, with 25% of items having multiple valid assembly sequences, the Laiva shelf having the highest number with eight variations. Our data collection pipeline addresses key challenges in real-world video annotation, notably ensuring consistent camera parameters and maintaining accurate relative poses between parts across frames. Additional comparisons, including action labels and human pose information, are discussed in Appendix I."}, {"title": "Dataset Statistics and Analysis", "content": "Statistics. Our dataset comprises 98 RGB videos. For each video, we annotated 1 frame per second, resulting in a total of 34441 annotated frames. On average, 316 frames are annotated for each video. In total, the dataset contains 137 high-level assembly steps from instructional manuals and 1120 detailed substeps from videos. The dataset provides assembly instructions for 36 unique IKEA furniture models, including 20 chairs, 8 tables, 3 benches, 1 desk, 1 shelf, and 3 other categories.\nAnalysis of Assembly Process. Our dataset includes the assembly process for complex 3D structures. On average, each piece of furniture has seven parts. The dataset also captures the variability in"}, {"title": "Data Collection and Annotation", "content": "The IKEA Video Manuals dataset is a comprehensive multimodal dataset that aligns 3D furniture, step-by-step instructions, and real-world video demonstrations. We collect data from various sources and perform extensive annotations to provide a rich resource for grounding furniture assembly instructions. In this section, we describe the data sources, temporal annotations, mask annotations, and pose annotations. We provide details and illustrations of our interface in Appendix C and E."}, {"title": "Collecting 3D Models and Assembly Videos", "content": "Building on the IKEA-Manual dataset [2] and IAW dataset [32], we collect 36 segmented 3D furniture models from the IKEA-Manual dataset and 98 assembly videos associated with them in the IAW dataset, providing temporal alignment between instruction steps and video segments. as illustrated in Fig. 4a. We focus on collecting fine-grained temporal segmentation and pose annotations for videos."}, {"title": "Annotating Temporal Segmentation and Part Identity", "content": "In our dataset, we provide fine-grained temporal segmentation of the videos based on the construction of each sub-assembly in the assembly processes. To annotate such data, we first extract video segments for each manual step from the IAW dataset. As a single step in the instruction manual often involves combining multiple furniture parts, we further decompose each video segment into smaller intervals, which we call substeps (shown in Fig. 4b). In these substeps, a new sub-assembly can be constructed or deconstructed. For each substep, we sample video frames at 1 FPS. As illustrated in Fig. 4c, we manually annotate the identities of the furniture parts in the first frame for each substep. This annotation is essential for ensuring consistent mask and pose annotations in the following stages because many furniture parts can be similar in appearance (e.g., the legs for a table)."}, {"title": "Annotating Segmentation Masks", "content": "To track the identities of the furniture parts throughout an assembly video, we annotate 2D image segmentation masks for 3D parts in the sampled frames. To facilitate the annotation process, we develop a web interface that displays auxiliary 2D and 3D information and enables interactive mask annotation based on the Segment Anything Model (SAM) model [33]. For each target part, an"}, {"title": "Annotating 2D-3D Correspondence", "content": "Inferring relative poses between 3D furniture parts from 2D videos is important for extracting grounded assembly knowledge from videos. We annotate 3D poses of furniture parts in the sampled video frames. Manual annotation is essential as real-world assembly videos often feature occlusions, challenging viewpoints, and partial visibility - these are difficult cases for recovering poses directly from depth estimation. A key challenge is to ensure the 3D trajectories of object parts respect the geometric constraints enforced by the physical assembly process (e.g., the final relative pose between two parts inferred from a video needs to align with their poses in the 3D furniture model). Besides minimizing the projection error of 3D parts in 2D images, we additionally emphasize the accuracy of relative poses between furniture parts and cross-frame consistency in the annotation process.\nA prerequisite for achieving spatially and temporally accurate pose annotation is a correct estimation of camera parameters from the video. As illustrated in Fig. 4f, we first identify video frames where potential changes in camera intrinsics occur (e.g., due to focal length adjustments or switching between multiple cameras). We then annotate 2D-3D point correspondences between the 3D models and their 2D projections in the video frames. Using a combination of these two types of annotations, we estimate camera intrinsics for each video. In particular, for each estimated candidate intrinsics, we use the Perspective-n-Point (PnP) algorithm [34] to estimate the pose of the object. We then select the intrinsic that minimizes the reprojection errors for frames between two camera changes. To further refine the camera intrinsics, we apply the Random Sample Consensus (RANSAC) [35] algorithm to filter out outliers in the annotated keypoints. From the resulting top ten intrinsics in each video segment, we choose the one that provides a minimal set of camera intrinsics.\nWe develop an interactive interface for refining pose annotations initially estimated from the 2D-3D point correspondences. The interface allows annotators to control the virtual camera using axis-"}, {"title": "Assembly Plan Generation", "content": "Assembly plan generation aims to predict a hierarchical assembly plan from a sequence of video frames $\\{f_1,..., f_T\\}$ depicting a furniture assembly process. The predicted plan is represented as a directed acyclic graph $G = (V, E)$, where each node $v \\in V$ corresponds to a subset of $K$ parts $\\{P_1,P_2,...,P_K\\}$, and each edge $e \\in E$ indicate assembly order and parent-child relationships. The root node $v_r$ represents the final assembled shape. IKEA manuals present assembly instructions as diagrams, often combining multiple parts in one step (Fig. 5a). Our IKEA Video Manuals presents physically realistic assembly plans extracted from Internet videos (Fig. 5b).\nExperiment Setup. We consider two heuristic baselines from IKEA-Manual [2]. The first baseline, SingleStep, constructs an assembly tree with all parts directly connected to the root node, correspond-ing to assembling all parts in a single step. The second baseline, GeoCluster, uses a pre-trained DGCNN [36] to extract 3D features for each part and constructs the assembly tree iteratively by grouping geometrically similar parts in individual steps. We report the precision, recall, and F1 score based on two matching criteria between the predicted and ground truth plans, as proposed in [2]. Simple Matching considers a predicted node as correct if it matches a ground truth node based on the primitive parts. Hard Matching requires the predicted node to match the ground truth node based on both the parts and parent-child relationships."}, {"title": "Part-Conditioned Segmentation", "content": "This task leverages the diverse videos in the IKEA Video Manuals dataset to evaluate the performance of part segmentation methods in real-world scenarios. The part-conditioned segmentation task requires the models to predict pixel-wise segmentation masks for the furniture parts seen in the assembly videos. Formally, given a frame $f$ and a sub-assembly $A$, the goal is to predict a binary segmentation mask for the sub-assembly.\nExperimental Setup. We test pre-trained CNOS [37] and SAM-6D [38]. Both models can segment novel objects given their 3D models. In total, we evaluated on 12296 examples from the dataset. We only include sub-assemblies that are unique in shape to remove ambiguities. We report Intersection-over-Union (IoU) and Top-5 IoU.\nResults and Analysis. As shown in Table 3, both CNOS and SAM-6D obtain relatively low performance on the IKEA Video Manuals dataset. We hypothesize that SAM-6D outperforms CNOS by considering additional geometric features, including shape and size. Common failures of both models stem from heavily occluded parts, visually complex backgrounds, and textureless 3D shapes. The results highlight the existing challenges in detecting object parts in Internet videos."}, {"title": "Part-Conditioned Pose Estimation", "content": "Estimating 3D poses of furniture parts from each video frame is essential for grounding the assembly process. Given a video frame $f$ and a furniture sub-assembly $A$, the goal of part-conditioned pose estimation is to predict the 6-DoF pose of the sub-assembly $A$ in the frame $f$.\nExperimental Setup. We sample 7795 annotations from the IKEA Video Manuals dataset and use ground truth masks for evaluation. We evaluate four methods: SAM-6D [38], MegaPose[39], and two differentiable rendering-based methods. SAM-6D requires a depth image in addition to the RGB image, which we obtain using the depth estimation model MiDaS [40]. The first differentiable rendering method uses Mean Squared Error (MSE) loss as the optimization objective, while the second incorporates an occlusion-aware silhouette re-projection loss proposed in PHOSA [41]. Both differentiable rendering methods use 20 random initial poses, refine the top five candidates with the lowest initial loss for 500 iterations, and output the pose with the lowest final loss. We report the ADD and ADD-S metrics, which are commonly used in the 6D pose estimation literature [42].\nResults and Analysis. Table 4 presents the quantitative results of all four methods on the IKEA Video Manuals dataset. While MegaPose achieves better performance overall, all four methods struggle with real-world challenges such as partial visibility and occlusions. MegaPose particularly struggles with symmetric parts and challenging viewpoints, while SAM-6D's performance is limited by the accuracy of the depth estimation in complex scenes. In Appendix H, we provide detailed error analysis and examples of failure cases. The high ADD and ADD-S scores indicate that the predicted poses are far from the ground truth poses, highlighting the challenges posed by the IKEA Video Manuals dataset."}, {"title": "Video Object Segmentation", "content": "Video object segmentation in our dataset focuses on tracking individual furniture parts within assembly substeps. Given a video sequence $\\{f_1, ..., f_T\\}$ representing a single substep and an initial segmentation mask $M_1$, the goal is to predict masks $\\{M_2,..., M_T\\}$ for subsequent frames where the part's identity remains constant (i.e., before it connects with other parts or becomes part of a new sub-assembly). The task evaluates models' ability to track parts despite occlusions, varying viewpoints, and similar-looking parts in real-world scenarios.\nExperimental Setup. We evaluate SAM2 [43] and Cutie [48] on video segments corresponding to assembly substeps with at least 20 frames. We closely follow the experiment design of standard video object segmentation, ensuring a fair comparison with other benchmark datasets. For each test example, we initialize the mask of the target part with ground truth in the first frame of the substep. We evaluate the performance on subsequent frames. We report the standard J&F metric for video object segmentation [45].\nResults and Analysis. Table 5 shows that both models perform worse on our dataset compared to existing benchmarks. SAM2's performance drops moderately, while Cutie shows a more significant decrease compared to existing benchmarks. The results highlight challenges presented by our dataset, including camera movements, the presence of parts with similar appearances and small parts, frequent occlusions, and extended assembly sequences."}, {"title": "Shape Assembly with Instruction Videos", "content": "Given a set of 3D parts $\\{p_1, ..., p_N \\}$ and an instruction video $\\{f_1, ..., f_K \\}$, the goal of this task is to predict the 6-DoF poses $\\{\\xi_1, ..., \\xi_N \\}$ for the 3D parts to assemble them into complete furniture. We decompose this problem into several sub-tasks, including key frame detection, assembled part recognition, pose estimation, and iterative assembly.\nMethod. We propose a modular video-based shape assembly pipeline that consists of the following steps: First, a keyframe detection model should be employed to identify the frames in which two parts or sub-assemblies are being combined. These frames typically provide a clear view of how the parts are being connected. Next, a segmentation and part identification model should be used to identify which 3D parts from the set of all 3D parts are being assembled in the frame and to determine their 2D locations in the image. Third, starting with the first keyframe, we estimate the poses of the parts being assembled and combine them into a sub-assembly $A_i$, which is a set of transformed parts $\\{S_i(p_i)\\}_{i=1}^K$, where $K$ is the number of parts in the sub-assembly. When moving to the next keyframe, we estimate the pose of the sub-assembly $A_i$ from the previous keyframe and the part to"}, {"title": "Conclusion", "content": "In this paper, we introduce the IKEA Video Manuals dataset, a large-scale multimodal dataset with high-quality, spatial-temporal alignments of step-by-step instructions, 3D furniture models, and real-world assembly videos from the Internet. In total, our dataset provides 34,441 video frames annotated with part segmentations and 6-DoF poses for 98 assembly videos and 36 different IKEA furniture models from 6 furniture categories. Experiments on the dataset highlight significant challenges in grounding instructional assembly videos, including extracting part segmentations and poses, constructing high-level assembly plans, and detecting key assembly steps in videos.\nLimitations. Currently, the dataset's limited size prevents large-scale training. The dataset focuses on visual and 3D information; including other data modalities such as audio or textual data is an important future direction. While manual annotation currently limits dataset scale, our framework for aligning Internet videos with 3D models and instructions provides a foundation for future expansion. The data collection process still requires manual annotation and verification, therefore presenting challenges for collecting data at a significantly larger scale. Current baselines demonstrate challenges in grounding 4D assembly but are yet to utilize this dataset for advanced model development. Since the dataset focuses on furniture assembly, whether models developed for this domain transfer to other assembly domains is left to be investigated. Future work could augment the dataset with additional modalities and develop algorithms leveraging instructional videos for 3D-grounded assembly plans. Broader implications include potential assistive technologies for individuals with disabilities, while internet-sourced data necessitates robust privacy and fair use methods."}, {"title": "Dataset Details", "content": "IKEA Video Manuals is a multimodal dataset with high-quality, spatial-temporal alignments of step-by-step instructions, 3D object representations, and real-world video demonstrations from the Internet. IKEA Video Manuals provides 34,441 annotated video frames, aligning 36 IKEA manuals with 98 assembly videos for six furniture categories. Fig. A1 shows all 3D furniture models included in the dataset. An example of the annotations associated with each frame is shown in Fig. A2. We provide details of the data and annotations associated with each frame below.\nFurniture-level information\n\u2022 Category: The category label of the furniture (e.g., Bench).\n\u2022 Name: The furniture name (e.g., applaro).\n\u2022 Furniture IDs: A list of IKEA product IDs for the furniture.\n\u2022 Variants: A list of furniture variants, if applicable.\n\u2022 Furniture URLs: A list of IKEA product page URLs for the furniture.\n\u2022 Furniture Main Image URLs: A list of URLs for the main product images on the IKEA website.\nVideo-level information\n\u2022 Video URL: The URL of the video.\n\u2022 Additional Video URLs: A list of additional video URLs for the same furniture.\n\u2022 Title: The title of the video.\n\u2022 Duration: The duration of the video (in seconds).\n\u2022 Resolution: The resolution of the video (e.g., 1920x1080).\n\u2022 FPS: The frame rate of the video (e.g., 30).\n\u2022\nPeople Count: The number of people in the video.\n\u2022 Person View: The view of the person in the video (e.g., front, side).\n\u2022 Camera Fixation: The fixation of the camera in the video (e.g., static, moving).\n\u2022 Indoor/Outdoor Setting: The setting of the video (e.g., indoor, outdoor)."}, {"title": "Datasheets", "content": "Dataset description. The datasheet for the IKEA Video Manuals dataset, available at https:\n//github.com/yunongLiu1/IKEA-Manuals-at-Work/blob/main/datasheet.md, including\nkey aspects of data collection and annotation:\n\u2022 Consent: The dataset is built upon two existing datasets, IKEA-Manual and IAW, which are\npublicly available for research purposes under the Creative Commons Attribution 4.0 International\n(CC-BY-4.0) license.\n\u2022 Personally Identifiable Information and Offensive Content: The dataset does not contain any\npersonally identifiable information or offensive content, as it focuses on furniture objects and\nassembly instructions.\n\u2022 Annotation Process and Compensation: The data annotation process was outsourced to an\nannotation company. The annotators were compensated based on the work they provided, with the\nestimated hourly pay being above the minimum wage.\nPlease refer to the datasheet for more detailed information on the dataset.\nLink and license. The dataset is available for public access under the CC-BY-4.0 license: https:\n//github.com/yunongLiu1/IKEA-Manuals-at-Work\nMaintenance. The dataset is hosted on GitHub and will be maintained by the authors. The repository\ncan be found at: https://github.com/yunongLiu1/IKEA-Manuals-at-Work. The dataset has\nthe following DOI: https://doi.org/10.5281/zenodo.11623997"}, {"title": "Details for Data Annotation", "content": "To create the IKEA Video Manuals dataset, we identified 36 IKEA objects from the IKEA-Manual\ndataset [2] that have corresponding assembly videos in the IAW dataset [5]. We matched the unique\nIDs of the instruction manuals to ensure correct correspondence between the datasets. We provide\nadditional details for each of the annotation steps below.\nC.1 Annotating Assembly Steps\nFor each assembly step annotated in the IKEA-Manual dataset [2], we identify matching video\nsegments in the IAW dataset [5]. We manually adjust the start and end time of each video segment to\ninclude a more complete assembly process, from picking up a part to positioning and tightening. The\nadjustment ensures better alignment with the physical assembly actions.\nC.2 Annotating Assembly Substeps\nIn the IKEA instruction manuals, each single step may involve the assembly of multiple parts (as\nshown in Fig. A3). We provide a more fine-grained assembly process by introducing substeps. A\nsubstep is labeled when 1) a new part appears in the video or 2) a new sub-assembly is created\nthrough positioning and/or fastening of parts. On average, each assembly step contains 7.59 substeps.\nIn total, the IKEA Video Manuals dataset contains 1120 substeps.\nC.3 Annotating Part Identities\nIn our dataset, each part of the 3D furniture model is assigned a unique ID consistent with the IKEA-\nManual dataset. However, locating an individual 3D furniture part in the frame can be challenging\ndue to several ambiguities, as illustrated in Fig. A4:\n(a) Wrongly assembled parts that are initially placed incorrectly and later relocated, causing\nconfusion for the annotator (Fig. A4a).\n(b) Similar or identical-looking parts, such as chair legs, that are difficult to distinguish and\nlabel accurately (Fig. A4b).\n(c) Parts that are heavily occluded, making it challenging to recognize the parts and their\nboundaries (Fig. A4c-d).\nTo ensure accurate part tracking throughout the video, we manually label the parts in the first frame\nof each substep after watching the entire video. This annotation is crucial for maintaining consistency\nwhen annotators only see individual frames in subsequent annotations. This approach ensures\nconsistent part identities throughout the video, addressing challenges posed by heavy occlusions,\nsimilar-looking parts, and assembly mistakes.\nC.4 Annotating Segmentation Mask\nTo efficiently generate segmentation masks for furniture parts in video frames, we utilize the Segment\nAnything Model (SAM). When SAM fails to generate accurate masks (e.g., Fig. A5), we use a brush\ntool built into our annotation interface to refine the masks manually."}, {"title": "Annotating 2D-3D Keypoints", "content": "To establish correspondence between the 3D parts and their 2D projections in the video frames,\nwe annotate keypoints on both the 3D parts and the 2D images. Our annotation interface is shown\nin Fig. A6. The annotation interface computes the part poses and camera parameters using the\nPerspective-n-Point (PnP) algorithm and visualizes the 2D projection in real-time. Based on the\nvisualization, the annotator can interactively refine the keypoint annotations to maximize the overlap\nbetween the 2D projection and the part seen in the 2D image.\nC.6 Robust Camera Parameter Estimation\nA key challenge in annotating real-world assembly videos is accurately estimating camera parameters,\nwhich can vary due to factors such as focal length adjustments and switching between multiple\ncameras. Our data collection pipeline incorporates several steps to address these issues:\nCamera Change Detection: We manually annotate points in the video where camera changes occur.\nThis allows us to segment the video into regions with consistent camera parameters.\nPer-Segment Intrinsic Estimation: For each video segment between camera changes, we estimate a\nset of intrinsic camera parameters. This approach allows for consistent parameters within a segment\nwhile accommodating changes between segments."}, {"title": "Part Identification and Annotation Consistency", "content": "To ensure accurate part identification, we assign unique IDs to 3D model parts, matching the IKEA-\nManual dataset. Annotators watch the entire video before starting, determining part IDs based on 3D\nmodel correspondence. For similar parts (e.g., table legs), we reference IKEA-Manual's assembly\norder. If leg_3 is first in IKEA-Manual, we label the first assembled leg as 3 in our video, with other\nsimilar parts identified by their relative positions.\nTo maintain annotation accuracy and consistency, our interface requires annotators to focus on one\npart throughout the video before moving to the next. We conduct multiple rounds of checks and\nre-annotations, with pose annotations beginning only after the mask is verified. Poses are initialized\nfrom previous frames to maintain consistency. This comprehensive approach ensures reliable part\ntracking, even for visually similar components, throughout the assembly process.\nIn our dataset, we create new substeps whenever a new part appears or a sub-assembly is formed.\nThis approach ensures all relevant parts or sub-assemblies are visible in the first frame of each substep.\nIf a part and all its connected components (i.e., the whole sub-assembly) disappear after being visible\nin the first frame, it will still be kept in the assembly process. However, we mark its mask and pose as\n\"not visible.\" There are no instances in our dataset where a part remains unclear throughout an entire\nsubstep.\nC.8 Details on locating 3D parts\nWe first segment each video into substeps when a new part appears or a new subassembly is formed.\nFor the first frame of each substep, we annotate part identities that are consistent throughout the\nvideo. The annotators then annotate masks for each part across all frames, which are verified for\nconsistency manually. After mask verification, we proceed to 3D pose annotation with a custom\ninterface that allowed annotators to view the parts from multiple angles to ensure accurate relative\nposes. We pay particular attention to coplanarity, inter-part distances, and correct relative locations\n(right/left, front/back, up/down). This multi-step approach, detailed in Sections 4.2-4.4 of our paper,\nallows us to accurately locate and pose 3D furniture parts in real-world assembly videos.\nC.9 Pose Refinement\nWhile initial estimates of the part poses can be obtained from the annotated 2D and 3D keypoints,\nthese estimates are often inaccurate, particularly in terms of the relative positions and orientations of"}, {"title": "Details for Quality Control", "content": "To ensure the accuracy and consistency of the annotations in the IKEA Video Manuals dataset, we\nanalyze the common errors in the annotations and perform extensive verifications.\nD.1 Common Errors\nFor mask annotations, common errors include incorrect part segmentation, missing parts, and noisy\nmasks. Incorrect part segmentation occurs when annotators misidentify the boundaries of a part due\nto similar colours or complex shapes. Missing parts occur when certain parts are not segmented,\noften due to occlusions. Noisy masks often occur when the SAM model fails to generate accurate\nmasks, leading to incomplete or inaccurate segmentation.\nFor pose annotations, common errors include incorrect"}]}