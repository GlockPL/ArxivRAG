{"title": "RecoveryChaining: Learning Local Recovery Policies for Robust Manipulation", "authors": ["Shivam Vats", "Devesh K. Jha", "Maxim Likhachev", "Oliver Kroemer", "Diego Romeres"], "abstract": "Model-based planners and controllers are commonly used to solve complex manipulation problems as they can efficiently optimize diverse objectives and generalize to long horizon tasks. However, they are limited by the fidelity of their model which oftentimes leads to failures during deployment. To enable a robot to recover from such failures, we propose to use hierarchical reinforcement learning to learn a separate recovery policy. The recovery policy is triggered when a failure is detected based on sensory observations and seeks to take the robot to a state from which it can complete the task using the nominal model-based controllers. Our approach, called RecoveryChaining, uses a hybrid action space, where the model-based controllers are provided as additional nominal options which allows the recovery policy to decide how to recover, when to switch to a nominal controller and which controller to switch to even with sparse rewards. We evaluate our approach in three multi-step manipulation tasks with sparse rewards, where it learns significantly more robust recovery policies than those learned by baselines. Finally, we successfully transfer recovery policies learned in simulation to a physical robot to demonstrate the feasibility of sim-to-real transfer with our method.", "sections": [{"title": "I. INTRODUCTION", "content": "A robot trying to tidy up a house is confronted with a myriad of possible failures. It might pick up a half-read 'Planning Algorithms' book from the table and try to place it in the top shelf but fail to see the objects already inside. How does it respond when the book bumps into the clutter and starts slipping out of its hand? One potential recovery behavior would be to first fix the slip by pushing the book against the shelf and then repositioning it for another attempt. Humans can quickly come up with robust strategies to deal with such failures, often with just partial information. For example, we often use our sense of touch to extract objects from a bag when we don't have a clear view of its contents, and we use our quick reflexes to recover from slipping on a patch of ice.\nHowever, popular model-based planning approaches [1], [2] struggle to generate such behaviors on-the-fly because of their reliance on an approximate model of the environment. This reliance leads to failures when the robot is faced with large inaccuracies in the model during deployment [3]. Hence, robots are usually deployed with recovery behaviors to gracefully handle potential failures. Common recovery strategies include retrying the previous step [4], backtracking [5] and hand-designed corrective actions [6]. These heuristic strategies can be sub-optimal and require significant manual engineering effort. In this paper, we propose to use reinforcement learning (RL) [7], [8] to automate the discovery of robust recovery behaviours for multi-step manipulation. RL is capable of discovering and learning complex robot skills [9], [10] but is limited by the twin problems of (1) high sample complexity and (2) significant reward shaping.\nTo address this, we propose a novel hierarchical reinforcement learning (HRL) formulation RecoveryChaining for recovery learning that is much more sample efficient than flat RL and can solve challenging manipulation problems even with a sparse reward. Our main idea is to use a hybrid action space which consists of primitive robot actions and"}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Recovery Learning", "content": "Recent works have explored the idea of learning recovery policies using offline datasets for safe exploration [11], [12] and for recovery from execution failures [13], [14]. For example, [12] learn a safe set using an offline dataset and [13] learn skill preconditions by executing the skills from different initial states. The learned preconditions (or safe sets) are then used as the goal for recovery learning. This approach is highly sensitive to the quality of the learned preconditions and is pessimistic as it tries to stay close to the offline dataset. To address these issues, our approach uses online RL with temporally extended actions to better explore the state space and discover robust policies.\nAnother common approach for learning reactive policies is to learn from human demonstrations [15], [16]. However, these policies are prone to failure if the robot visits out-of-distribution states during execution. Online data collection [17] by an expert human is often required to learn recoveries which makes this approach quite expensive. By contrast, our approach does not rely on human demonstrations. [18] propose to recover by modulating dynamical systems learned from segmented demonstrations. However, this assumes accurate detection of manually specified modes which we do not require."}, {"title": "B. Hierarchical Reinforcement Learning", "content": "Hierarchical reinforcement learning (HRL) [19] is an approach for solving long-horizon decision making problems. HRL focuses on decomposing a problem into smaller sub-tasks to enable easier policy learning and better generalization. This decomposition allows decisions to be made at higher levels of abstraction without having to deal with the low-level details. Skill chaining [20] is a popular approach to discover skills that allow an agent to solve any task in the state space. However, such approaches have primarily been evaluated in navigation domains and transfer to high-dimensional domains such as manipulation remains an open area of research. Our recovery learning approach utilizes the HRL framework, wherein recovery policies from failures are learned to connect them to the nominal policies. Our approach is inspired by previous works which show that manipulation policies can be learned more efficiently by using structured action spaces [21], [22], [23] such as, object-centric controllers and parameterized primitives."}, {"title": "III. BACKGROUND", "content": "Markov Decision Process (MDP). A Markov Decision Process (MDP) [7] is defined by the tuple (S, A, T, R, \u03b3, \u03bc) where S is the state space, A is the action space, T is the transition function, R is the reward function, \u03b3 is the discount factor and \u03bc is the initial state distribution. The MDP framework has been used extensively to model tasks in manipulation for planning and reinforcement learning. A key assumption made by MDPs is that the system state is always accurately known. This may not be true for some state variables, for example, due to errors in pose estimation of objects or some objects being out of the sensor's field of view. The Mixed Observable MDP (MOMDP) [24] accounts for this uncertainty by factoring the state into two types of state variables: fully observable variables represented by the variable x and partially observable variables y. The tuple (x, y) fully specifies the system state. For example, a robot's end-effector pose is usually known quite accurately and hence should belong to x, while poses of occluded objects would belong to y.\nOptions Framework. We model each robot skill as an option as per the options framework [25]. Each option consists of three components: (a) a robot control policy \u03c0 (b) an initiation set I, also sometimes called a precondition, which defines the states from which the option can be executed and (c) a termination condition \u03b2 which defines the states in which the option must terminate. Once an option terminates, the robot may choose to initiate another valid option. This decision is made by a separate high-level policy over options which treats the available options as its action space.\nSkill Chaining. Skill chaining (SC) [20] is a popular approach to solve long horizon RL problems by decomposing it into shorter sub-problems. A sequence of options is learned backwards from the goal, such that in each iteration a new option is learned to reach the precondition of the previously learned options. The precondition is usually learned using binary classification and describes the states from which the option policy can be successfully executed. During the initiation period of an option it is executed from different states in the environment to collect data for training the precondition. The precondition classifier is then frozen and used to generate success or failure rewards for the option that is trying to reach it."}, {"title": "IV. PROBLEM STATEMENT", "content": "Consider a long-horizon manipulation task defined by a distribution of start states and a binary goal function $f_{goal} : S \\rightarrow \\{0,1\\}$. As shown in figure 3, we assume we are given a nominal plan $\\pi^{nom} = (\\pi_1^{nom}, ..., \\pi_k^{nom})$ made up of k nominal controllers (or policies) that is capable of solving the task with non-zero probability. The nominal plan sequentially executes each nominal policy until the policy's termination condition is met. We use a fixed maximum number of steps as the termination condition for all the policies in our experiments unless the goal or failure condition is met. Due to state and actuation uncertainty or model inaccuracy, the system may deviate from the intended plan. We assume that we have a failure detector that can detect impending irrecoverable failures and hence ensure that the robot is always in a recoverable state. However, the actual nominal initiation sets are unknown. Our goal is to robustify the system by efficiently learning a separate recovery policy that allows the robot to complete the task after failure detection. In the following section, we describe our proposed approach and present solutions to some of the key challenges associated with this learning problem."}, {"title": "V. APPROACH", "content": "We model the system as a MOMDP [24], wherein the robot maintains an estimate $\\hat{s} := (x, y)$ of the true state s and acts based on $(\\hat{s}, o)$, where o \u2208 O corresponds to sensory observations such as proprioception. Our approach, visualized in fig. 2, involves two steps: (1) Failure Discovery. Nominal policies are executed under various conditions in simulation to induce and record failures. We leverage privileged information in simulation to record both the true state s and the corresponding observations associated with"}, {"title": "A. Failure Discovery", "content": "Failure Detection. We utilize a failure detector that monitors the execution of the system and raises a flag fail-condition if unsafe or unexpected conditions are met, for example, high end-effector forces, dropping an object, or slip. We assume that the failure detector can prevent the robot from encountering irrecoverable failures. While we hand-design the failure detectors in our experiments, prior works have shown how they can be learned from"}, {"title": "B. Recovery Learning", "content": "Our goal is to learn a policy that reliably takes the robot to the precondition of one of the nominal policies from all the failures $D_{fail}$. For example, if the book in a robot's hand slips while putting it on a shelf, recovery should regrasp the book such that the place controller can be executed. Since the true preconditions $I^{nom}$ are usually unknown, prior works [11], [12], [14] in recovery learning first estimate the nominal precondition $\\hat{I}^{nom}$ offline. The RL agent then computes actions to maximize the estimated precondition $r(s, a) = \\hat{I}^{nom}(s)$. However, accurate estimation of preconditions is highly dependent on the quality of the offline dataset making these approaches brittle and pessimistic. We discuss these issues in more detail in section V-C.\nTo address these drawbacks, we propose an online RL approach that does not rely on known preconditions. Instead of using learned preconditions, our main observation is that we can compute a monte-carlo estimate of the precondition of a nominal controller $\\pi_i^{nom}$ by executing the nominal plan suffix $\\pi_{i:k}^{nom} := (\\pi_i^{nom}, ..., \\pi_k^{nom})$ at the query state s. If the plan terminates in a state $s'$ inside the goal, then the precondition is satisfied at s, i.e., the precondition can be estimated as $I_{MC}(s) = f_{goal}(s')$. A single monte-carlo simulation is sufficient in deterministic MDPs but an average of multiple simulations may be required in stochastic domains. We use only a single simulation in our experiments.\nA Straw-man Approach. A straw-man RL approach to leverage this observation would be to replace the learned $I^{nom}$ with $I_{MC}$. However, this is highly inefficient as the environment would have to execute long sequences of nominal controllers, potentially multiple times, in every step to compute the reward. Ideally the environment need not have to repeatedly compute the monte-carlo estimates from states well outside the precondition.\nRecovery Chaining. Our key insight is to let the RL agent decide when to estimate the precondition. We provide the RL agent with temporally extended nominal options $\\pi_{i:k}^{nom}$ which"}, {"title": "RecoveryChaining MDP", "content": "Let A be the original action space of the agent consisting of primitive actions and S be the state space. S contains an absorbing goal state $s_g$ and an absorbing failure state $s_f$. If the robot satisfies $f_{goal}$, then it transitions to $s_g$ with a reward of 1. If fail-condition is triggered instead, then it transitions to $s_f$ with a reward of 0. The RecoveryChaining MDP is defined by the tuple $(S_{rc}, A_{rc}, T_{rc}, r_{rc}, \u03b3, \u03bc_{rc})$ where\n$S_{rc} = S \u222a \\{s_d\\}$, where $s_d$ is a new absorbing state.\n$A_{rc}$ is a hybrid action space $A \u222a \\{\\pi_{1:k}^{nom}, ..., \\pi_{k:k}^{nom}\\}$ consisting of primitive actions and terminal nominal options that transfer control to the nominal policies.\nThe agent transitions to $s_d$ after executing $\\pi_i^{nom} \\forall i$ if it does not transition to the goal.\n$r_{rc}(s \u2208 S) = f_{goal}(s), r_{rc}(s_d) = 0$.\n$\u03bc_{rc} = D_{fail}$\nThe hybrid action space is visulialized in figure 4. Intuitively, when the agent is far away from the precondition $I^{nom}$ and executes $\\pi_i^{nom}$, it gets no reward. After a few trials and errors, the agent identifies states that lie outside the precondition and stops executing the nominal option from those states. On the other hand, as the agent gets closer to the precondition, it receives higher rewards upon executing $\\pi_i^{nom}$ and implicitly learns that it is inside the desired precondition.\nThis approach has three key advantages:\n1) The agent implicitly learns the nominal preconditions through trial and error and stops computing monte-carlo estimates for nominal controllers that are not applicable.\n2) The ability to try all the nominal policies allows the agent to potentially discover novel ways to reuse them.\n3) The agent is not obligated to recover to any precondition. If the correct recovery is to completely avoid the nominal policies then it can discover such strategies.\nState Representation. To recover from failures due to inaccurate and partial state information, it is necessary that the robot has access to multiple sensing modalities, such as vision, proprioception and tactile sensing. We use an asymmetric learning approach, where we leverage full state information available in simulation for failure discovery but train the recovery policy to use only those state variables that will be available at deployment. To bridge the sim-to-real gap, we convert end-effector force signals into a binary signal using a threshold. Further, we also provide the recovery policy with the number of actions it has taken so far in partially observable domains which is critical for it to learn a robust recovery."}, {"title": "C. RecoveryChaining vs Pretrained Preconditions", "content": "Most previous recovery learning works learn the nominal precondition [13] or the safe set [12] using an offline dataset. The recovery policy is then trained to satisfy the leaned precondition. While this approach is efficient as it decouples recovery learning from the overall task, it suffers from two issues in practice:\nBrittleness. Recovery learning is highly sensitive to the quality of the learned preconditions. RL is a reward maximizer that is known to find and exploit loopholes even in carefully hand-designed reward functions [27]. A learned reward function aggravates this issue as the agent is likely to explore states that are quite different from the states where data for learning the reward function was collected. It is a challenge in manipulation, in particular, because the complex and non-linear dynamics induced by contact make generalization from limited data difficult. Hence, there is a risk that the agent may learn an undesirable behavior using a pre-trained\nPessimistic Bias. Learning with pretrained preconditions is pessimistic as the agent needs to stay close to the offline data. While this is effective at dealing with small perturbations it cannot discover complex recovery behaviors that require the agent to visit an out-of-distribution state. Recovery sometimes requires novel use of existing policies which is not possible with a pessimistic bias in learning."}, {"title": "VI. EXPERIMENTS", "content": "We evaluate our proposed approach in three challenging multi-step manipulation environments. The purpose of these experiments is to understand (1) whether our approach can learn robust and composable recoveries and (2) how well the hybrid approach of combining model-based and model-free policies works. While a model-based planner can be used to compute nominal plans, we design them manually in our experiments as planning is not the focus of our work.\nBaselines. We compare our method RecoveryChaining (RC) with three baselines. (a) Nominal: completes the task using just the model-based controllers; (b) Pretrained Pre-conditions (PP): learns a recovery policy using primitive actions to reach preconditions learned from offline data; (c)"}, {"title": "A. Pick-Place Domain", "content": "Our first domain is the pick-place task from robosuite [30]. To solve this task, the robot needs to pick a small loaf of bread from the source bin and place it in the target bin. The robot gets a 46 dimensional observation consisting of object poses, end-effector pose, etc. The bread is initialized in a different location in the source bin in every episode.\nNominal Skills. We designed four nominal controllers: GOTOGRASP skill takes the robot to a pre-grasp pose over the object, PICK skill picks up the object, GOTOGOAL moves the robot to the drop location and PLACE skill places the object at the drop location. Each controller is implemented as a state machine that selects a target pose for the robot end-effector in the Cartesian space based on its current state. The target end-effector pose is then achieved using task-space impedance control with fixed impedance. The nominal skills achieve a success rate of 70% on their own in this task.\nFailures. Most of the failures in this task occur when the initial location of the bread is close to one of the walls. The robot needs to reach inside the bin to grasp the object due to its small size. This causes the end-effector to collide with the wall leading to a failure that the nominal skills aren't capable of handling. Collision is detected using a threshold on the end-effector forces. We collect a total of 100 failures for learning recovery.\nRecovery. The learned policy finds two different strategies to recover from these failures. The first strategy rotates the end-effector along the z axis so that it does not collide with the wall when the robot reaches inside the bin. The second, and perhaps more interesting, strategy uses the gripper fingers to push the object away from the walls before picking it up. RC uses a combination of these two strategies to improve the success rate from 70% to 90%. On the other hand, PP quickly plateaus and converges to a locally optimal policy (fig 5). It uses preconditions learned from around 300 nominal trajectories as the reward function. RLR is not able to learn the recovery at all due to sparse reward. We summarize the success rates in table I."}, {"title": "B. Shelf Domain", "content": "Our second domain is a shelf environment with state uncertainty also implemented using robosuite. In this task, the robot needs to pick a box from a table and place it inside a shelf in an upright position. The robot observes a noisy estimate of the position of the box, where, the noise is sampled from a zero-mean Gaussian distribution with standard deviation of 1 and 2 cm along the y and z axes, respectively. We also provide the robot the number of actions taken so far as an observation. This allows it to learn open-loop policies if needed. The dimensions of the shelf and the box and the position of the shelf are sampled randomly in every episode.\nNominal Skills. We designed three nominal skills for the task assuming that the observations are accurate. PICK skill: the robot goes to the observed position of the box, closes the gripper, and picks up the box; MOVE skill: the robot moves to a pre-placement position conditioned on the given position of the shelf; PLACE skill: the robot places the box on the shelf and retracts. These nominal skills can complete the task reliably if the state estimates are accurate but can fail when the state estimates are wrong. All the nominal skills control the robot using task-space impedance control with fixed impedance.\nFailures. We collect failures by executing the nominal policies in simulation under state uncertainty. The failure conditions are given by the end-effector forces Fe exceeding a predefined threshold. The nominal controllers assume perfect pose of the box. However, the robot may grasp the box with an offset due to a wrong position estimate. This leads to mainly two types of failures: (1) collision: robot collides with the shelf or the table (2) collision-slip: collision with the shelf leads to in-hand rotation of the object if there is a delay in stopping the robot.\nRecovery. The recovery policy learns to move up and inside the shelf before switching to the nominal skill because most collisions happen below the center of mass of the object. We found that providing the number of past actions was crucial to learn a recovery in this task because of unreliable state estimates. In our ablation studies with different amounts of state uncertainty, we found that the policy tends to be more conservative and learns relatively open-loop policies under high uncertainty. Overall, RC did significantly better than PP on this task by improving task success from 50.8% to 83.2%. RC was not able to recover from failures involving significant in-hand rotation of the box as it was not provided with its orientation observation. Additional sensing, for example, slip detection using a tactile sensor can further improve the recovery policy. The reward model for PP was learned using 232 nominal trajectories."}, {"title": "C. Cluttered Shelf Domain", "content": "We consider a more complex version of the shelf domain with two objects randomly placed on the shelf. The robot needs to avoid them while putting the box on the shelf. We use the same nominal skills used in the shelf domain.\nFailures. In addition to monitoring end-effector forces for collision detection, we also use vision-based failure detection to avoid toppling objects on the shelf. The failure detector is triggered if the robot topples or rotates any object by more than a predefined threshold. Such failures can be detected in the real world by using object detectors and pose estimation"}, {"title": "D. Analysis of Learned Recoveries", "content": "RC can identify the best nominal controller to recover to. We do not provide the agent any prior knowledge about where it should recover to and which nominal controller it should switch to. To understand the exploration behavior of RC we plot the number of times the agent chooses each nominal option in a round of exploration in figure 8. Each round consists of 120 actions. We observe that the agent initially explores all the nominal controllers but quickly identifies the most suitable one and commits to recovering to its precondition. The RC policy implicitly learns the preconditions of all the nominal controllers through trial and error and avoids trying to switch to a nominal controller at states in which it is unlikely to succeed.\nRC can reuse the nominal controllers in novel ways. Prior approaches that use pretrained preconditions, like skill chaining, provide a pessimistic bias to the RL agent by freezing the preconditions after the initiation period. This prevents the agent from discovering novel ways to use the nominal controllers that may be quite different from the trajectories used to train the precondition. Sometimes, the robot needs to use the nominal controllers in novel ways not seen during nominal execution. We describe one such novel reuse of the PLACE skill discovered by our agent for the shelf task in figure 9. In this example, the box undergoes significant in-hand rotation due to a collision with the shelf below its center of mass. The robot needs to re-grasp the box before placing it on the shelf as it may fall over otherwise. The RC agent discovers that switching to the nominal PLACE skill from inside the shelf fixes the in-hand slip by aligning the box with the back of the shelf. This behavior is well outside the distribution of states visited by the nominal skills as the PLACE controller is designed to be triggered outside the shelf."}, {"title": "E. Transfer to a Physical Robot", "content": "We transfer the nominal and recovery policies to a real robot. Our real-world setup includes a 6-DoF Mitsubishi Electric Assista robot arm with a WSG-32 parallel-jaw gripper and a Mitsubishi F/T sensor 1F-FS001-W200 mounted on the wrist of the robot. We evaluate the feasibility of sim-to-real transfer of recoveries learned using our approach. We train the recovery in a simulated Assista robot with a box and then evaluate it on a real Assista with one box and two unseen objects- a mustard bottle and a can. Failures are induced on the real robot by providing incorrect position estimates of the shelf. Similar to the simulation, we implement a collision detector on the real robot by using observations from the F/T sensor on the robot. The robot was able to recover from both collision and slip (fig. 1) by using our recovery. The policy generalized well to the mustard bottle but performed slightly worse on the can because it is smaller than the box that was used for training and its curved surface is more prone to slip."}, {"title": "VII. CONCLUSION", "content": "We propose a hierarchical reinforcement learning approach to learn model-free recovery policies for robustifying nominal model-based controllers. Our approach, called RecoveryChaining, uses a hybrid action space to efficiently learn"}, {"title": "Robustness", "content": "rely policies that can be chained with model-based controllers. The action space contains temporally extended nominal options that transfer control to a specific nominal controller. These nominal options reduce the effective horizon of the task and enable recovery learning for multi-step manipulation. We evaluate our approach in three challenging domains and find that our approach can significantly improve task success using just a sparse reward. We also transfer a recovery trained in simulation to a physical robot to demonstrate the feasibility of sim-to-real transfer.\nA major limitation of our method is its reliance on a physics-based simulator for recovery learning. This limits its applicability to tasks that can be modeled well in simulation and introduces the challenge of sim-to-real transfer. Second, we assume that there exists an initiation set from which the nominal policies can be reliably executed and that this set can be reached from failures using a local corrective policy. Our method will not be effective if the nominal policies are unreliable everywhere since the recovery policy will not find any good state to switch to the nominal plan. Finally, more research is needed in efficiently learning recoveries in partially observable environments."}]}