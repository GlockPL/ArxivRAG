{"title": "GENERATIVE ADVERSARIAL REVIEWS: WHEN LLMS BECOME THE CRITIC", "authors": ["Nicolas Bougie", "Narimasa Watanabe"], "abstract": "The peer review process is fundamental to scientific progress, determining which papers meet the quality standards for publication. Yet, the rapid growth of scholarly production and increasing specialization in knowledge areas strain traditional scientific feedback mechanisms. In light of this, we introduce Generative Agent Reviewers (GAR), leveraging LLM-empowered agents to simulate faithful peer reviewers. To enable generative reviewers, we design an architecture that extends a large language model with memory capabilities and equips agents with reviewer personas derived from historical data. Central to this approach is a graph-based representation of manuscripts, condensing content and logically organizing information linking ideas with evidence and technical details. GAR's review process leverages external knowledge to evaluate paper novelty, followed by detailed assessment using the graph representation and multi-round assessment. Finally, a meta-reviewer aggregates individual reviews to predict the acceptance decision. Our experiments demonstrate that GAR performs comparably to human reviewers in providing detailed feedback and predicting paper outcomes. Beyond mere performance comparison, we conduct insightful experiments, such as evaluating the impact of reviewer expertise and examining fairness in reviews. By offering early expert-level feedback, typically restricted to a limited group of researchers, GAR democratizes access to transparent and in-depth evaluation.", "sections": [{"title": "1 INTRODUCTION", "content": "Assessing the quality of research is central to the advancement of scientific discovery. Peer review remains a cornerstone of scientific publication, ensuring that manuscripts meet standards of novelty, rigor, and significance. Although essential, this process faces several challenges, including biases Stelmakh et al. (2021), inconsistencies among reviewers Kravitz et al. (2010), and an urgent need for scalable solutions Liu & Shah (2023). Estimates suggest that researchers collectively invest millions of hours in reviewing activities annually American Journal Experts (AJE) (2024). Furthermore, access to high-quality feedback remains limited to a small fraction of researchers with established networks. Large language models (LLMs) hold considerable potential in relieving some of these issues in the scientific review process.\nRecent breakthroughs in LLMs have shown promise in human behavior modeling by enabling the creation of autonomous agents Hardy et al. (2023); Jansen et al. (2023); Argyle et al. (2023); Ziems et al. (2023); Li et al. (2023). A growing body of research has explored the application of these LLM-based agents in simulating diverse societal environments Park et al. (2023); Gao et al. (2023); T\u00f6rnberg et al. (2023); Liu et al. (2023a); Akata et al. (2023), with primary emphasis on agents' cooperation and collaboration behaviors, such as software engineering, playing games, and recommender system evaluation Wu et al. (2023); Xi et al. (2023); Abdelnabi et al. (2023); Anonymous (2024). However, studies specifically examining the application of LLM-based agents for academic peer review remain sparse.\nOnly a few approaches have explored the use of LLMs as tools to assist researchers at various stages of the scientific workflow, from data analysis to hypothesis generation. Yet, the peer review process remains a particularly challenging domain. For instance, ReviewerGPT has demonstrated how LLMs can identify errors, verify checklists, and select the best version of a paper Liu & Shah"}, {"title": "2 RELATED WORK", "content": "The use of artificial intelligence (AI), particularly large language models (LLMs), in scientific discovery has been explored in recent years, with a few studies investigating their potential to replicate and enhance traditional review mechanisms. While generative AI has made strides in areas like molecular modeling (Vignac et al., 2022) and protein structure prediction (Abramson et al., 2024), enabling more rapid experimentation, these applications focus primarily on scientific discovery rather than structured assessment and review. Thus, the use of AI in the scientific workflow, especially through large language models (LLMs), remains an emerging area with significant potential."}, {"title": "2.1 LLMS FOR MACHINE LEARNING RESEARCH", "content": "Recent advancements in artificial intelligence have introduced new methodologies that enhance the research process across various domains Xu et al. (2021). For example, Huang et al. (2024) introduces a benchmark to evaluate how effectively LLMs can generate code to solve various machine learning tasks. Lu et al. (2024a) utilize LLMs to propose, build, and test novel algorithms for preference optimization, achieving new performance milestones. Similarly, Liang et al. (2024) demonstrate that LLMs can generate feedback on research papers comparable to human reviewers, while Girotra et al. (2023) highlight LLMs' potential to produce more innovative ideas than humans. Furthermore, Wang et al. (2024a); Baek et al. (2024) use LLMs to generate research concepts inspired by literature without implementing them, and Wang et al. (2024b) automate survey writing through comprehensive literature analysis. Building on these foundations, our method introduces an affordable, on-demand system that can be applied to the field of scientific discovery to assess generated ideas and research papers with human-like depth and rigor."}, {"title": "2.2 LLMS FOR SCIENTIFIC REVIEW AND EVALUATION", "content": "With the capabilities of LLMs, AI has extended beyond idea generation and experimental design to tasks that closely align with the peer review process (Zheng et al., 2023; Wang et al., 2023; Miret & Krishnan, 2024). By leveraging the natural language understanding and contextual analysis afforded by these models, AI can be utilized to assist in manuscript evaluation by simulating aspects of a human reviewer's decision-making process (Liu et al., 2023b). Other initial investigations, such as those by Robertson et al. (Robertson, 2023), indicated that reviews generated by GPT models align closely with human reviewers' assessments. By evaluating reviews produced by both humans and LLMs for papers submitted to a prominent machine learning conference, early results illustrated that LLMs could play a valuable role in the peer review process. Further studies Liang et al. (2023) demonstrated that GPT-4's feedback shared notable similarities with human reviewer comments, with more than half of participants finding the LLM's feedback beneficial, highlighting its increasing impact in this area. Building on these findings, ReviewerMT Tan et al. (2024) reformulates the peer-review process as a multi-turn, role-based dialogue, encompassing distinct roles for authors, reviewers, and decision makers.\nRecent studies, such as Jin et al. (2023), simulate peer review through LLM agents to analyze factors like reviewer biases and decision-making but focus primarily on controlled experiments rather than structured content organization. In contrast, our work incorporates a graph-based representation that clusters key contributions, linking claims, technical details, and evidence to enhance coherence in evaluations. Similarly, Anonymous et al. (2023d) examines LLM reliability in review settings, focusing on feedback consistency rather than a comprehensive review simulation. Our approach presents a structured representation to enable detailed multi-dimensional assessments. Research such as Anonymous et al. (2023b) explores LLMs as evaluators in reinforcement learning, assessing their ability to judge AI outputs. However, this study does not address the peer review process comprehensively. Additionally, Anonymous et al. (2023c) shows that LLMs can provide useful feedback on academic documents but lack our structured approach, which systematically links clusters of content to improve interpretability AI-assisted peer review systems like Checco et al. (2020) focus on semi-automated tools for improving review speed and accuracy but are limited to assisting human reviewers. In contrast, GAR simulates the full review cycle, from the initial review to the meta-review stages, providing end-to-end automation.\nRecently, MARG D'Arcy et al. (2024) has extended this concept by deploying multiple LLMs in a collaborative framework where distinct sections of a paper are allocated to individual agents for focused review, enabling thorough analysis even for extensive documents that surpass the model's typical context limits. Unlike existing LLM research, which primarily centers on single-pass review generation, our work aims to replicate a holistic review experience by introducing a multi-round approach that connects historical reviews from similar papers with the agent's self-assessment. Finally, research in automated meta-review generation Anonymous et al. (2023a) is limited to synthesizing reviews from existing evaluations. Unlike previous methods, GAR differentiates itself by incorporating a memory-based reviewer equipped with granular personas and a graph-based paper representation to systematically connect evidence with arguments. Through memory-augmented multi-round evaluations and a meta-reviewer to synthesize final decisions, GAR offers a more comprehensive, faithful, and token-efficient simulation of the real-world peer review process."}, {"title": "3 METHODOLOGY", "content": "Generative reviewers aim to provide a framework for automated paper review. The system generates numerical scores (soundness, presentation, contribution, overall, confidence), lists weaknesses and strengths, and predicts the acceptance outcome (accept or reject).\nAt the center of this approach, we present a graph-based representation of manuscripts, designed to: 1) condense the document length for efficient processing, 2) establish logical links among ideas and novel concepts across different sections, 3) reduce token consumption.\nBuilding upon this representation, we introduce the agent's four key modules and describe how a meta-reviewer synthesizes the generated reviews into a final decision."}, {"title": "Task Formulation", "content": "Given a paper $p\\in P$ and a reviewer $r \\in R$, let $Y_{rp} = 1$ denote that reviewer $r$ has reviewer the paper $p$, and subsequently assigned a score $S_{rp}$ with $S_{rp} E \\{1, 2, 3, 4, 5, 6, 7, 8,9,10\\}$. The average score of each paper $p$ can be represented by $R_p = \\frac{\\sum_{r\\in R} Y_{rp}}{\\sum_{r\\in R}} = 1 \\cdot S_{rp} \\cdot Y_{rp}$. The simulator's goal is to faithfully distill the human genuine preferences such as $Y_{rp}$ and $s_{rp}$ of reviewer $r$ for an unseen paper $p$."}, {"title": "Generative Large Language Models", "content": "LLMs are trained to predict the most probable next token tk given the sequence of previous tokens $t_1...t_{k-1}$ by maximizing the likelihood function $P_{LLM}(t_k|t_1,...,t_{k-1})$. In this work, we use pre-trained LLMs without further finetuning them. Depending on the task, we generate one or more tokens given a task-specific context c(P) that describes the task to the language model and prompts it for an answer. Thus, we obtain generated tokens t by sampling from:\n$P_{LLM}(t/c(P)) = \\prod_{k=1}^{K}[P_{LLM}(t_k|c(P),...,c(P), t_1,..., t_{k-1})]$ (1)"}, {"title": "Review Process Design", "content": "GAR employs a 4-phase pipeline to simulate the peer review process. 1. Graph Construction: In this phase, the manuscript is structured into a knowledge graph that establishes connections between essential ideas, claims, technical details, and results. 2. Reviewer Selection: Next, three to six reviewers are selected and their profile module are initialized from historical data. 3. Reviewer Evaluation: In this phase, each manuscript undergoes a multi-round evaluation by the independent reviewers. Synthetic reviewers generate comprehensive reviews structured into four sections: significance and novelty, strengths, weaknesses, and suggestions for improvement. This format mirrors the conventional review frameworks utilized by major conferences. Additionally, each reviewer provides an overall score for the paper, assigning a numerical rating on a scale from 1 to 10. 4. Meta-Review: Finally, a meta-reviewer compiles the reviews to select the final decision."}, {"title": "3.1 GRAPH-PAPER REPRESENTATION", "content": "As mentioned above, parsing scientific manuscripts is challenging due to their length and the complex relationships among evidence and arguments. Typically, contributions and technical details are introduced in the early sections (e.g., Introduction and Method), while supporting results are often presented later. This raises several key questions:\n\u2022 Structuring Information: How can the diverse elements be effectively organized to enable LLM-based agents to cross-reference and analyze them?\n\u2022 Processing Lengthy Manuscripts: Conference papers face efficiency and cost issues with long sequences due to the self-attention mechanism's limitationsBeltagy et al. (2020).\n\u2022 Reducing Redundancy: How can redundant claims or findings be minimized to ensure an accurate and thorough assessment?"}, {"title": "To escape these pitfalls, we introduce a graph-based $G$ representation that organizes the content of an academic paper p into a structured graph $G(p)$. Constructing $G(p)$ consists of the following five steps:", "content": "\u2022 Acronym Extraction The initial step involves identifying and extracting acronyms and their definitions from the manuscript. Acronyms often represent key concepts and terminologies crucial for understanding the paper's content. The LLM parses the title, abstract, and introduction to retrieve a list of acronyms and their corresponding definitions, referred to as $R_{acr}$.\n* * *\n\u2022 Extraction of Core Elements: The second step is to identify and extract instances of graph nodes and edges from each chunk of the source paper. Let $C^* = \\{C_1, C_2,...,c\\}$ denote the set of chunks in the paper p (e.g., Introduction, Methods, Results). We leverage a multipart LLM prompt that first identifies all entities in the text, including ideas, claims, technical details, and supporting evidences, before identifying all relationships between clearly-related entities, including the source and target entities and a description of their relationship. Each extracted entity $e \\in E$ becomes a node in the graph $G(p)$, with relationships $r\\in R$ between these elements, such as \u201cproves\" or \"supports\u201d, represents an edge between nodes. The graph $G(p)$ is formally defined as: $G(p) = (E, R)$.\n= * * *\n\u2022 Concept Merging: To reduce redundancy, we merge nodes that represent the same or similar concepts but are phrased differently across the manuscript. We query the LLM to identify and merge such entities by defining the following merging function: $E' = LLM((Q_{merge}, E, R_{acr}))$, where $Q_{merge}$ represents the merging prompt, and $E'$ is the new set of entities after merging the entities E. Similarly, if two claims are merged, their technical details and supporting evidence edges, will then point to the newly merged entity, $R'$. The updated graph is hence defined as: $G'(p) = (E', R')$, and $G(p) \\leftarrow G'(p)$.\n\u2022 Community Detection: Given the homogeneous undirected weighted graph G(p), created in the previous step, a variety of community detection algorithms may be used to partition the graph into communities of nodes with stronger connections to one another than to the other nodes in the graph, with c referring to a community. In all our experiments, we use Leiden Traag et al. (2019) to partition the graph into modular communities of closely related nodes Edge et al. (2024). The modularity Q is defined as:\n$Q = \\frac{1}{2m} \\sum_{i,j}(A_{ij} - \\frac{k_i k_j}{2m})\\delta(c_i, c_j)$\nwhere $A_{ij}$ is the adjacency matrix, $k_i$ and $k_j$ are the node degrees, m is the total number of edges, and $\\delta(c_i, c_j)$ equals 1 if nodes i and j are in the same community, 0 otherwise. The Leiden algorithm iteratively optimizes modularity, producing stable and coherent communities of related ideas, claims, and results. This step is essential as it groups nodes into thematically related clusters, allowing the agent to process the manuscript in focused segments. For example, a community might encompass all elements related to a novel loss function, grouping together the key idea, associated claims, technical details, and ablation studies.\n\u2022 Community-Based Descriptor: The final step is to create report-like descriptors of each community in the Leiden hierarchy, $\\hat{C} = \\{\\hat{C}_1, \\hat{C}_2, . . ., \\hat{C}_k\\}$. The representation of the nodes and edges in the community serves to query the LLM, which produces a descriptor $\\hat{c}_i$ representing the community $c_i$. Each community in the graph is assigned its corresponding descriptor, $\\hat{c}_i = LLM((Q_{sum}, c_i, R_{acr}))$, where $Q_{sum}$ is a prompt instructing the LLM to describe the community, its structure, and cite the original text as much as possible to mitigate hallucination. These descriptors are attached to the graph $G(p)$."}, {"title": "The extracted graph $G(p)$ is passed to the reviewers. For simplicity, we assume that the agents share a unique graph. In future work, we anticipate conditioning the graph extraction with the reviewer's persona.", "content": ""}, {"title": "3.2 AGENT ARCHITECTURE", "content": "Leveraging this graph-based representation, GAR structures agents in terms of four specialized modules tailored for review scenarios: profile, memory, novelty, and review modules."}, {"title": "3.2.1 PROFILE MODULE", "content": "The profile module is essential for ensuring the alignment of synthetic agents with the diverse behaviors of genuine reviewers. Drawing inspiration from previous work in user modeling Harper & Konstan (2015); Rappaz et al. (2021), each reviewer persona consists of height core attributes: strictness, expertise level, focus areas, evidence focus, open-mindedness, ethic focus, tone, and attention to technical details.\nThe following attributes are derived from historical datasets:\n\u2022 Strictness reflects the degree to which a reviewer adheres to high standards in evaluating submissions, ranging from lenient to highly critical.\n\u2022 Evidence Focus describes how much importance the reviewer places on the evidence provided in the submission to support claims, highlighting their emphasis on empirical validation or theoretical soundness.\n\u2022 Open-mindedness measures the reviewer's willingness to consider unconventional or novel ideas. A higher score indicates more openness to creative methodologies or speculative hypotheses.\n\u2022 Tone refers to the overall style and approach taken by the reviewer in their feedback, ranging from highly critical to constructive.\n\u2022 Technical Focus reflects the extent to which the reviewer is detail-oriented in evaluating the technical correctness and methodological rigor of a submission.\nPredicting these characteristics is inherently challenging, as anonymization in blind review limits each reviewer to a single evaluation. To overcome this constraint, we introduce a technique called contrastive comparison, which conducts pairwise comparisons across inter-reviewer and intra-reviewer assessments. Specifically, we perform N comparisons in which the LLM assesses whether the reviewer's review, r, is stricter than another review from a different paper ri. To ensure fairness acknowledging that stricter reviews are often associated with lower-quality submissions, the LLM is also presented with inter-reviews (anchors) of the same target paper as context. The strictness scores of the reviewer r is formally defined as follows:\n$S_r = \\frac{1}{N} \\sum_{i=1}^{N}1(LLM (r > r_i|\\{Q_{comp},T,T_{int},r_i\\}))$ (2)\nwhere Qcomp is the comparison prompt, r is the target reviewer's review, $T_{int}$ represents the intra-reviews, and i refers to a review randomly sampled from another reviewer ri. Finally, strictness is categorized into low, medium, and high levels based on percentiles. Similar formulas are derived for the other characteristics. In detail, predicting evidence, open-mindedness, tone, and technical focus involves the same steps with Qcomp tailored to each characteristic.\nThe expertise level and focus areas are assigned as described below:\n\u2022 Expertise Level denotes the depth of the reviewer's knowledge in specific research areas, ranging from novice to expert. The score is derived from real reviews using their confidence scores \u2208 {1, 2, 3, 4, 5}.\n\u2022 Focus Areas defines the primary area of interest for the reviewer, such as theoretical contributions, empirical results, or novel applications. This trait is determined by extracting keywords from past reviews via a prompt $Q_{focus}$. For instance, clarity; technical depth; writing quality, illustrates the focus areas of a reviewer in the ICLR 2023 dataset."}, {"title": "3.2.2 NOVELTY MODULE", "content": "The novelty module, which, analogous to a human review, draws upon external knowledge sources to gauge the originality of the manuscript in comparison to prior research. It begins by extracting keywords from the introduction of the targeted paper, which are then employed in a semantic search to retrieve similar papers Ammar et al. (2018), $B_{sim}$. Retrieved documents are filtered to ensure that only prior work are included based on the year of submission."}, {"title": "The title, abstract, and introduction of the retrieved papers serve to analyze the extent of innovation, clarity of differences from past contributions, and adequacy of related work citation.", "content": "The LLM generates a novelty $S_{nov}$ score ranging from 1 (not novel) to 4 (highly novel), accompanied by a concise explanation $e_{nov}$. This scheme can be formalized as: $(S_{nov}, e_{nov}) \\leftarrow LLM((Q_{novel}, R_{acr}, B_{sim}, p))$, where, with a slight abuse of notation, p denotes the source paper (title, abstract, introduction). To ensure reliable outputs, the LLM is instructed to cite verifiable information, reducing hallucinations or uncertain references. This score $S_{nov} \\in \\{1,2,3,4\\}$ and explanation $e_{nov}$ are subsequently used during paper review to condition reviews."}, {"title": "3.2.3 MEMORY MODULE", "content": "We present a novel memory module designed to support agents. Assuming a benchmark dataset, each academic paper is structured as a graph G(p) = (V, E), as detailed in Section 3.1. However, here, we introduce an extra step. Given a community descriptor $\\hat{c} \\in \\hat{C}$, we query the LLM to determine whether the human review mentions $\\hat{c}$. If the descriptor is mentioned, the agent is instructed to cite the original review, otherwise, the LLM is prompted to output No specific mention was found in the review., denoted as $r_{c}$. Following this step, the memory is filled with pairs of community descriptor $\\hat{c}$ and their associated reviews as plain text, $\\{\\hat{c}, r_{c}\\}$. All descriptors $\\hat{c}$ are embedded using \u201cmxbai-embed-large\" Li & Li (2023) and used as index of the memory module, $h_{\\hat{c}}$.\nThis memory offers two retrieval schemes, serving at different stages of our framework."}, {"title": "\u2022 Community-level Retrieval", "content": "Retrieve most similar communities and their associated review $\\{\\hat{c},r_{c}\\}$, straightforwardly using the following similarity function: $sim(h_{\\hat{c}}, h_{\\hat{a}}) = \\frac{h_{\\hat{c}}h_{\\hat{a}}}{\\|h_{\\hat{c}}\\| \\|h_{\\hat{a}}\\|}$, where $\\hat{c}$ is the target community descriptor and $\\hat{a}$ represents other communities."}, {"title": "\u2022 Paper-level Retrieval", "content": "Retrieve similar papers based on node and edges overlap. This approach retrieves papers and their associated reviews based on underlying ideas and claims, enabling a more nuanced comparison. Unlike prior studies that rely on direct embedding similarity Cohan et al. (2020), this scheme compares manuscripts at a descriptor level. The similarly function $sim_{struct}$ between two papers $p_1$ and $p_2$ is expressed:\n$sim_{struct} (G(p_1), G(p_2)) = \\frac{|\\{\\hat{c} \\in \\hat{C}_1 | \\exists \\hat{e} \\in \\hat{C}_2 | (sim(h_{\\hat{c}}, h_{\\hat{e}}) > \\tau)\\}|}{\\max(|\\hat{C}_1|, |\\hat{C}_2|)}$ (3)\nwhere $\\tau$ is a scalar that determines whether two communities discuss similar concepts, and $\\hat{C}_1$ and $\\hat{C}_2$ are community descriptors of $p_1$ and $p_2$ respectively."}, {"title": "3.3 REVIEW MODULE", "content": "Equipping agents with profile and memory modules enables them to exhibit diverse behaviors akin to humans. We enhance the agent's ability for reasoning via Chain-of-Thought Wei et al. (2022). Namely, the review is initiated by the agent processing the paper and generating an initial review $R_{r,0}$ based on its persona and the preliminary novelty assessment $\\{S_{nov}, E_{nov}\\}$. In this stage, the agent evaluates each community descriptor $\\hat{C} \\in G(p)$, then outputs numerical scores (soundness, presentation, contribution, overall, confidence), weaknesses and strengths, as well as a preliminary binary decision (accept / reject). The prompt during the initial assessment is formulated as:\n$Q_{r,0} = (Q_{review}, Q_{novelty}, Q_{style}, S_{nov}, E_{nov}, R_{acr}, \\hat{C})$ (4)\nwhere the score, accompanied by a summary of supporting arguments, is formatted into plain text and passed on to subsequent review stages.\nTo refine the review and enhance its reliability, the agent then engages in multi-round refinement, where the reviewer r at turn k receives the review and thoughts from the previous response $R_{r,k-1}$. Agents are successively presented each community descriptor $\\hat{c}$ from the manuscript to review along with the M most similar communities $\\hat{c}_1, ..., \\hat{c}_m$ retrieved from the memory module and their associated reviews $r_{\\hat{c}_1}, \u2026, r_{\\hat{c}_m} \u00b7$ To reduce the cognitive workload of reviewers, the agent evaluates communities in blocks of size $\\frac{N}{K}$, where K is the number of review rounds. This retrieval-augmented scheme guides the agent in assessing summaries and discovering potential weaknesses and strengths."}, {"title": "By drawing inspiration from retrieved exemplars, the agent may decide to add strengths, weaknesses or correct potential mistakes made during the initial review.", "content": "Thus, the prompt at turn k is formally defined as:\n$Q_{r,k} = (Q_{check}, R_{r,0}, (R_{r,k-1}, (\\hat{c}_1, ..., \\hat{c}_m), (r_{\\hat{c}_1}, ..., r_{\\hat{c}_m})_1, ..., (\\hat{c}_1, ..., \\hat{c}_m), (r_{\\hat{c}_1}, ..., r_{\\hat{c}_m})_K ))$ (5)\nwhere $R_{r,0}$ denotes the initial review and $R_{r,k-1}$ is the k-th review. Given the prompt $Q_{r,k}$, each review agent generates a response $R_{r,k}$, sampled from a probability distribution $R_{r,k} \\sim P(\\cdot|Q_{r,k})$, as well as the thoughts/rationales behind their choices. The last review is then selected as the final review of reviewer r of the paper p."}, {"title": "3.4 \u039c\u0395\u03a4A-REVIEWER", "content": "After the individual reviews are completed, a meta-reviewer synthesizes the final decision. Each meta-reviewer is equipped with a memory module initialized from genuine meta-reviews, which is leveraged to retrieve the top-K2 most similar papers and their meta-reviews. The meta-reviewer is also provided with the scores and reviews provided by the individual reviewers, along with its own preliminarily feedback. It is tasked with synthesizing an overall assessment of the submission, generating a concise but structured summary that highlights both the strengths and weaknesses of the paper, with special attention to the methodological rigor, experimental validation, and impact of the work. This summary aims to consolidate key insights raised by the individual reviewers, presenting a balanced evaluation of the submission."}, {"title": "After T turns of self-reflection, the meta-reviewer generates the final decision $R_{meta}$, following this prompt structure:", "content": "$Q_{m,t} = (Q_{meta}, r_1, ..., r_{K_2}, S_t, (R_{j,K}))$ (6)\nwhere $r_1,..., r_{K_2}$ represent retrieved meta-reviews based on the similarity between the target paper and manuscript entries in the memory module, $Q_{meta}$ is a meta-review prompt, and $S_t$ is the meta-reviewer's summary of dialogues from turn t. The final acceptance decision consists of the last review produced at the last round, choosing from the following options: ACCEPT (ORAL), ACCEPT (POSTER), or REJECT.\nIn this work, we compare our method utilizing the above meta-reviewers, GAR, with meta-reviewers that accept or reject papers by comparing the review scores with a fixed threshold that reflects real conference acceptance ratio $R_{AG}$. We discuss the effectiveness these two strategies in the result section (see Sec. 5.3)."}, {"title": "4 ENVIRONMENT OVERVIEW", "content": "Our simulation involves two agent roles: reviewers and meta-reviewers. Each paper is evaluated by a committee comprising three to six reviewers and one meta-reviewer. The paper and review dataset for this simulation is sourced from OpenReview and includes submissions from conferences such as ICLR.\nA challenging problem in processing research papers lies in preserving their structural integrity, particularly complex elements like mathematical formulas. Unlike prior work such as AI-Scientist Lu et al. (2024b), which converts PDFs to plain text and may compromise document formatting, we utilize Nougat Blecher et al. (2023) to extract the Markdown (MMD) version of each manuscript, maintaining structural and formatting fidelity.\nWe argue that experimental results are indispensable for determining a paper's alignment with publication standards. Hence, we further graft figures into the paper representation. That is, figures that contain empirical findings, such as bar charts, are identified using Molmo-7b Deitke et al. (2024). Next, we prompt GPT-40 to generate detailed captions for these figures, describing the methods being compared, important findings, and key results. Each caption is placed immediately following the original figure title, providing LLM reviewers with direct access to experimental data, enhancing their ability to rigorously evaluate the paper's technical soundness."}, {"title": "5 EXPERIMENTS", "content": "Datasets. We primary conduct the experiments on the ICLR 2023 dataset, which consists of 3,797 papers obtained from Openreview. Each paper was retrieved by at least three reviewers. In some experiments, we also conducted experiments on the ICLR 2022, and NeurIPS 2023 Beygelzimer et al. (2021) datasets.\nBaselines We compare our method with AI-Scientist Lu et al. (2024b), OpenReviewer Tyser et al. (2024), ReviewerGPT Liu & Shah (2023), and AI-Review Chiang & Lee (2023).\nImplementation. All agents are powered by the GPT-40-mini version of ChatGPT OpenAI et al. (2024). In some experiments, we also use the following state-of-the-art LLMs as the backend of reviewer agents: GPT-40 OpenAI et al. (2024) and Llama-3.1 (8b and 70b) Grattafiori et al. (2024). GPT-40 is accessed through a public API, while the Llama-3.1 models, which are open-weight, are deployed via the Ollama framework (Contributors, 2023). For each experimental run with Llama-3.1 (8b), we utilize a single NVIDIA A100 40G GPU. Each run on Llama-3.1 (8b) takes approximately 20 minutes, and all results reported are averaged over 20 independent runs to ensure reliability and robustness of the findings. The prompts and other implementation details can be found in the Appendix."}, {"title": "5.1 LLM VS HUMAN REVIEWS", "content": "In order to assess the quality of reviews generated by large language models, five expert evaluators were given 200 papers, each with two anonymous reviews. As LLM Evaluators Chiang & Lee (2023) achieve comparable performance with human evaluators, we use GPT-40 to evaluate the generated reviews. For every paper, two reviewers were randomly assigned among the six possible reviewers: Human, GAR, AI-Scientist, OpenReviewer, ReviewerGPT, and AI-Review. The human reviews were obtained from OpenReview submissions. Evaluators were tasked with selecting their preferred review between the two provided for each paper.\nTherefore, this experiment measures and ranks reviewers based on match outcomes, using a win matrix, coefficients from the Bradley-Terry (BT) model, and logistic regression. The win matrix records the results of matchups between competitors. For N competitors, the matrix W is an N \u00d7 N grid where each element $w_{ij}$ indicates the probability that competitor i defeats competitor j, calculated as $W_{ij} = \\frac{\\text{# wins by i over j}}{\\text{total matches between i and j}}$. The matrix is constructed by processing a list of match results, updating both win counts and total match counts for each competitor pair. The win matrix generated in our experiment is displayed in Figure 2.\nThe Bradley-Terry model applies a parametric approach to estimate the relative strengths of competitors through pairwise comparisons. In this model, the probability P that competitor m prevails"}, {"title": "over competitor $m'$ is given by a logistic function:", "content": "$P(H = \\frac{1"}, {"function": "l(h, p) = -(h log(p) + (1 - h) log(1 - p))$. The optimization task can then be expressed as $\\xi = argmin_{\\xi} \\sum_{t=1} l(H_t, \\xi_t, \\frac{1}{1 + e^{m-Em}})$, while keeping $\\xi_1 = 0$ to anchor the scale. Once calculated, the BT coefficients $\\xi$ are used to rank competitors, ordering them from strongest to weakest by sorting the $\\xi$ values in descending order, as shown in Ranked Competitors = sort($\\xi$, descending). The results of the Bradley-Terry coefficient estimation, along with the corresponding rankings of reviewers, are displayed in Table 1.\nGAR leads with a score of 0.684, outperforming the human reviewer at 0.523. AI-Scientist and ReviewerGPT achieve scores of 0.242 and 0.000, respectively, while OpenReviewer records the lowest score of -0.632. Upon looking at the responses, GAR generated reviews stem from their depth, rigor, and reduced susceptibility to biases common among individual reviewers, resulting in high preferences. One key factor is the retrieval of relevant reviews for each community descriptor, helping the LLM to identify"}]}