{"title": "Offline-to-Online Multi-Agent Reinforcement Learning with Offline Value Function Memory and Sequential Exploration", "authors": ["Hai Zhong", "Zhuoran Li", "Xun Wang", "Longbo Huang"], "abstract": "Offline-to-Online Reinforcement Learning has emerged as a powerful paradigm, leveraging offline data for initialization and online fine-tuning to enhance both sample efficiency and performance. However, most existing research has focused on single-agent settings, with limited exploration of the multi-agent extension, i.e., Offline-to-Online Multi-Agent Reinforcement Learning (O2O MARL). In O2O MARL, two critical challenges become more prominent as the number of agents increases: (i) the risk of unlearning pre-trained Q-values due to distributional shifts during the transition from offline-to-online phases, and (ii) the difficulty of efficient exploration in the large joint state-action space. To tackle these challenges, we propose a novel O2O MARL framework called Offline Value Function Memory with Sequential Exploration (OVMSE). First, we introduce the Offline Value Function Memory (OVM) mechanism to compute target Q-values, preserving knowledge gained during offline training, ensuring smoother transitions, and enabling efficient fine-tuning. Second, we propose a decentralized Sequential Exploration (SE) strategy tailored for O2O MARL, which effectively utilizes the pre-trained offline policy for exploration, thereby significantly reducing the joint state-action space to be explored. Extensive experiments on the StarCraft Multi-Agent Challenge (SMAC) demonstrate that OVMSE significantly outperforms existing baselines, achieving superior sample efficiency and overall performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-Agent Reinforcement Learning (MARL) has achieved remarkable success across various domains, including mastering complex video games [1, 3, 28], optimizing warehouse logistics [25], enabling robotic soccer [17], and performing bi-manual dexterous manipulation [6]. However, these successes often come at the cost of low sample efficiency and high computational overhead, as MARL algorithms must explore a joint state-action space that grows exponentially with the number of agents. A promising approach to alleviate this computational burden is Offline-to-Online (O2O) Reinforcement Learning (RL). In recent years, O2O RL has achieved significant progress [10, 14, 15, 19, 29, 32-34], leveraging advances in Offline RL by utilizing offline datasets [7, 8] to provide strong initial policies and pre-trained value functions. By learning a high-quality pre-trained policy from offline data, single-agent O2O RL can significantly reduce the need for extensive exploration and further enhance performance through interaction during the online phase. However, existing results primarily focus on single-agent scenarios, and the important O2O MARL setting has received only very limited attention, e.g., [12].\nO2O MARL faces two major challenges. First, exploration during the transition from the offline-to-online phase introduces a distributional shift, that can result in significant unlearning of the pre-trained Q-values in the early stages of online learning. To demonstrate this phenomenon, we conducted an experiment analyzing how pre-trained Q-values evolve during online fine-tuning. We collected test trajectories for the offline pre-trained agent interacting with the environment and stored them into a replay buffer. During the online phase, we evaluated how online value estimations for these samples evolved. As shown in Figure 1, Multi-agent Conservative Q-Learning (MACQL) [13], Multi-agent Cal-QL (MACal-QL) [19], and the action proposal method from [9] exhibit clear signs of unlearning, as the fine-tuned Q-values exhibit a rapid drop during the initial stage of online learning. This suggests that the algorithms have forgotten the previously learned optimal actions. This unlearning behavior hinders the efficiency of online fine-tuning, as the policy has to relearn knowledge that was already acquired during the offline phase. In contrast, our proposed algorithm OVMSE preserves the offline knowledge and achieves fast online fine-tuning.\nSecond, efficient exploration is critical for O2O MARL. Exploration at the initial stage of online learning can lead to overestimation of Q-values for unseen state-action pairs. While this overestimation is necessary for agents to experiment with new actions and improve upon the offline pre-trained policy, inefficient exploration may result in assigning high Q-values to sub-optimal actions, requiring significant trial and error to correct. This issue is particularly pronounced in multi-agent systems, where the joint state-action space grows exponentially with the number of agents. However, unlike training from scratch, O2O MARL benefits from a pre-trained offline policy, which provides a much stronger starting point than a randomly initialized policy. Therefore, an effective exploration strategy for O2O MARL needs to focus on exploring more efficiently within a reduced joint state-action space, rather than exhaustively searching through the entire space.\nTo address these two challenges, we propose a novel O2O MARL algorithm named Offline Value Function Memory and Sequential Exploration (OVMSE). Specifically, OVMSE consists of two key components. The first component, OVM, introduces a novel way of computing target Q-values and solves the unlearning problem. With OVM, even when the offline pre-trained Q-values degrade due to distributional shifts during the early stages of online learning, the algorithm allows for the quick recovery of the offline pre-trained values. The second key component, Sequential Exploration, is designed to enable efficient exploration after transitioning to the online phase. Inspired by the sequential update mechanism [4, 30], SE restricts exploration to one agent at a time by allowing a single agent to select a random action while the others follow their respective policies. SE reduces the complexity of the joint state-action space and improves the quality of exploration, enabling more efficient fine-tuning and better performance. We further develop a decentralized SE for execution. We empirically validate OVMSE on a collection of easy, hard, and super-hard tasks in SMAC. Our results demonstrate superior performance compared to baseline methods.\nThe paper is organized as follows. Section 2 discusses related literature, Section 3 introduces the background knowledge for MARL and QMIX [23], Section 4 describes the proposed OVMSE algorithm, and Section 5 presents the experimental results and discussions. The main contributions of our paper are summarized as follows:\n(i) Key Challenges in O2O MARL: We identify and analyze two key challenges in O2O MARL: (1) Offline-learned Q-values can be unlearned during the initial stage of online learning, leading to the degradation of offline pre-trained policies; (2) Inefficient exploration in the exponentially large joint state-action space can result in slow online fine-tuning.\n(ii) OVMSE Algorithm: We propose OVMSE, a novel framework that combines Offline Value Function Memory (OVM) and Sequential Exploration (SE) to address both the unlearning and exploration issues. OVMSE is designed to facilitate robust and efficient online fine-tuning, reducing the sample complexity while achieving superior performance.\n(iii) Extensive Empirical Evaluation on SMAC: We conduct extensive empirical evaluations of OVMSE on a range of tasks-spanning easy, hard, and super-hard difficulties-in the StarCraft Multi-Agent Challenge (SMAC). Our results demonstrate that OVMSE significantly outperforms baseline methods. OVMSE shows significantly improved online performance, higher sample efficiency, and faster fine-tuning, underscoring its practical applicability to complex multi-agent environments."}, {"title": "2 RELATED WORK", "content": "O2O RL and O2O MARL. Many previous works on O2O RL focus on reusing offline data during online training [2, 10, 14]. [32] proposes retaining a copy of the offline policy and using a hybrid of the offline and online fine-tuned policies during online training, thereby preserving the performance achieved during the offline phase. A related approach to our work is presented in [9] which utilizes the offline policy to suggest actions for online target value estimation. In the context of O2O MARL, relatively little research attention has been given. [12] investigates O2O MARL by using offline data during the online phase and proposes metrics for sampling from offline data. In addition to our proposed value function memory, we employ a multi-agent sequential exploration strategy inspired by multi-agent sequential rollout [4, 30] to reduce the exploration space in cooperative multi-agent systems. This improves online learning efficiency, as the offline pre-trained policy can focus on targeted exploration rather than an exhaustive random search of the action space, which is typically required when training from scratch.\nOffline MARL. The principles used to address accumulated extrapolation error in offline single-agent RL also apply to offline MARL. A common strategy is to incorporate pessimism towards out-of-distribution (OOD) states and actions, which has proven effective in the offline MARL setting as well. Both [21] and [24] extend the Conservative Q-Learning (CQL) framework [13] to the multi-agent domain. [21] identifies that value functions in MARL are more prone to local optima as the number of agents increases and proposes using zeroth-order optimization to avoid being trapped in these local optima. [24] demonstrates that independently penalizing each agent's OOD actions can reduce the overall conservatism in value estimation, leading to improved empirical performance. [31] takes a different approach by ensuring that only actions present in the dataset are used for target Q-value estimation, thereby promoting conservatism for OOD states and actions. Another promising direction is to treat offline MARL problems as supervised learning tasks. For example, [27] and [18] frame offline MARL as sequence modeling problems, leveraging the power of decision transformers [5] to achieve strong empirical performance. Furthermore, [16] and [35] employ diffusion models as policy backbones, demonstrating good performance and the ability to learn diverse strategies."}, {"title": "3 PRELIMINARIES", "content": "3.1 Multi-Agent Reinforcement Learning\nFormulation\nA cooperative MARL problem can be described as a decentralized partially observable Markov decision process (Dec-POMDP) [20], which can be represented as a tuple (N, S, O, A, P, r, \u03b3, T).\n\u2022 N = {1, 2, ..., N} is the set of agents.\n\u2022 S is the set of states of the environment.\n\u2022 0 = 01\u00d702X...XON is the joint observation space, where Oi is the set of observations available to agent i.\n\u2022 A = A1 X A2 X ... X AN is the joint action space, where Ai is the set of actions available to agent i.\n\u2022 Given the current state s \u2208 S and the joint action a \u2208 \u0391, P(s' | s, a) and r(s, a) describe the probability of transitioning from s to the next state s', and the immediate reward shared by all agents, respectively.\n\u2022 \u03b3\u2208 [0, 1] is the discount factor, and T is the time horizon of the process.\nAt time step t, each agent maintains its observation-action history \u03c4\u00a1 \u2208 T\u2081 = {(0\u00a1,0, ai,0, \u00b7 \u00b7 \u00b7, 0i,t, ai,t)}, and the joint observation-action history is \u03c4 = (\u03c41, \u03c42, . . ., \u03c4\u03b7). The objective is to find a policy for each agent \u03c0\u2081 : Ti \u2192 A\u00a1 that maximizes the expected discounted return\n$\\sum_{t=0}^{T} \\gamma^t r(s_t, a_t)$,\nwhere st and at are the state and the joint action at time step t, respectively.\n3.2 QMIX\nIn this section, we present QMIX [22, 23], which is a popular MARL algorithm following the Centralized Training with Decentralized Execution (CTDE) paradigm. In our work, we utilize QMIX as the backbone for both the offline and online phases.\nUnder the CTDE framework, each agent has an individual action-value function Qi (ti, ai), based on its observation-action history. Additionally, QMIX introduces a joint action-value function Qtot (\u03c4, \u03b1), constructed using a mixing network fs, which combines individual agent action-values according to:\nQtot (t, a) = fs (Q1 (t1, a\u2081), ..., Qn (Tn, an)).\nThe mixing network satisfies the monotonicity constraint,\n$\\frac{\\partial Q_{tot}}{\\partial Q_i} \\ge 0 \\ \\forall i \\in N,$\nthereby maintaining that the joint action-value function Qtot is a non-decreasing function with respect to each individual action-value function Qi. Hence, QMIX satisfies the Individual-Global-Maximum (IGM) property [26], which guarantees that the joint optimal action can be constructed by combining each agent's individual optimal actions based on their local observations:\n$\\underset{a}{arg max} Q_{tot}(t, a) = (\\underset{a_1}{arg max} Q_1(T_1, a_1),..., \\underset{a_n}{arg max} Q_n(T_n, a_n)$"}, {"title": "4 OFFLINE VALUE FUNCTION MEMORY AND SEQUENTIAL EXPLORATION (OVMSE)", "content": "We now introduce our Offline Value Function Memory and Sequential Exploration (OVMSE) algorithm for O2O MARL. First, we present our online training mechanism, OVM. Next, we describe the designed decentralized sequential exploration mechanism, SE. Finally, we outline our offline training procedure.\n4.1 Offline Value Function Memory\nIn the online phase, we utilize QMIX as the backbone of our framework, replacing the Q target value with our Offline Value Function Memory (OVM) target. We describe the functionality of our proposed OVM training objective for online training. After offline training, we retain a copy of the pre-trained target value function, denoted as Qtot-offline. We introduce the OVM target, defined as:\n$Q_{OVM} = max ( Q_{tot-offline} (\\tau, a), r + \\gamma \\underset{a'}{max} Q_{tot}(\\tau', a')$,\nwhere Qtot is the online target Q-function, \u03c4' and a' are the joint observation-action history and joint action at the next time step, r + \u03b3 maxa' Qtot (t', a') is the online temporal difference target, and Qtot-offline (t, a) represents the offline memory of the agents with joint history t and joint action a. The OVM target selects the maximum value between the offline value function memory and the online temporal difference target.\nDuring the online phase, the value function is trained to minimize the mean squared error (MSE) between its estimation and the OVM target, as well as the MSE between its estimation and the online temporal difference target, i.e.,\n$L_{OVM} = (1 \u2013 A_{memory}) [Q_{tot} (\\tau, a) \u2013 (r + \\gamma \\underset{a'}{max} Q_{tot} (\\tau', a'))]^2 + A_{memory} [Q_{tot} (\\tau, a) \u2013 Q_{ovm}]^2,$\nwhich introduces a trade-off between the OVM target and the online temporal difference target. Here, Amemory is a memory coefficient that controls the balance between these two objectives.\nWe further propose an annealing schedule for Amemory, which gradually adjusts the influence of the offline value memory over time. The annealing schedule is defined as follows:\n$A_{memory} = min (1, A_{memory\\_end} + (1 - A_{memory\\_end}) \\frac{t}{T})$,\nwhere Amemory_end specifies the final value for Amemory, T is the annealing duration, and t represents the current environment step. The motivation for gradually decreasing Amemory is to maintain the memory of the offline value memory during the transition from offline-to-online learning. As the agent interacts with the environment and stores experiences in the replay buffer, the offline values are progressively adjusted towards the online temporal difference target values, facilitating further improvement.\nThis training objective ensures that the online value function preserves the pre-trained offline values while allowing the agent to follow the online temporal difference target when it is larger. As a result, the agent retains knowledge from the offline phase while effectively exploring new strategies and improving its performance during the online phase. The intuition behind OVM is as follows: the Q-values evolve based on the pre-trained offline Q-values. For unseen actions that were assigned low Q-values during offline training, their values can increase as they are explored online, allowing the agent to discover and exploit better strategies while also retaining offline knowledge.\n4.2 Multi-agent Sequential Exploration\ne-greedy exploration is one of the most widely used strategies for exploration in multi-agent reinforcement learning (MARL) algorithms [11, 23]. In this strategy, each agent independently selects a random action with probability et, which gradually decreases over environment steps:\n$\u20act = min (\u20acend, \\frac{\u20acstart - \u20acend}{T} t)$,\nwhere \u20acstart and \u20acend specify the start and terminal values for et, T is the annealing duration, and t is the current environment step. During the initial online phase, when e is high, multiple agents are likely to explore randomly and simultaneously, resulting in exploration that resembles a random search across the exponentially large joint state-action space. However, this approach is inefficient for O2O MARL, where a pre-trained offline policy already provides a strong starting point. Instead of exhaustively searching the entire joint state-action space, exploration should focus on a smaller, more targeted subset to refine the policy rather than starting from scratch. Inspired by the multi-agent sequential update scheme [4, 30], which sequentially updates different agents' policies, we introduce the concept of sequential exploration for O2O MARL. Specifically, we adapt this concept into a sequential e-greedy exploration strategy. At each time step, agents collectively decide whether to explore based on the probability et, which decays with the number of environment steps. If exploration is chosen, only one randomly selected agent performs a random action, while all other agents act greedily, following the current policy. This coordinated approach allows the system to efficiently leverage the pre-trained policy while facilitating targeted exploration to further refine and improve the policy during the online phase.\nDecentralized Sequential Exploration. However, this coordinated exploration requires communication between agents during execution, which may not always be feasible in decentralized execution settings. To address this limitation, we introduce a decentralized version of sequential exploration with a simple modification. We define a decentralized exploration probability, \u20acdec_t = \u20act/N, where N is the number of agents. Each agent independently decides to explore with probability Edec t, ensuring that, on average, the number of agents exploring at any given time matches that of the centralized version. This decentralized approach maintains the benefits of sequential exploration while ensuring compatibility with decentralized execution environments.\n4.3 Offline Training for OVMSE\nIn this section, we introduce the offline training procedure for OVMSE. During offline training, we utilize QMIX and Conservative Q-Learning (CQL) as our backbone algorithms. First, we minimize the squared temporal difference error as part of our offline training objective:\n$L_{QMIX} = [Q_{tot} (\\tau, a) - (r + \\gamma \\underset{a'}{max} Q_{tot} (\\tau', a')]^2,$\nwhere \u03c4' and a' denote the next observation-action history and joint action, respectively, and Qtot represents the target joint action-value function.\nIn addition to minimizing the temporal difference error, we incorporate the CQL training objective into the offline phase. CQL-based methods introduce a regularization term to the Q-value function's training objective, defined as:\n$L_{CQL} = \u0395_{\u03c4~D,a~\u03bc} [Q(\u03c4, \u03b1)] \u2013 E_{\u03c4~D,a~D} [Q(\u03c4,\u03b1)],$\nwhere u denotes the sampling distribution (e.g., sampling from the current policy or a uniform distribution). This regularization term intuitively penalizes the Q-values of actions not present in the dataset while promoting the Q-values of actions within the dataset.\nThe overall offline training objective combines the QMIX temporal difference error and the CQL regularization:\n$L_{offline} = L_{QMIX} + \u03b1L_{CQL},$\nwhere a is a hyperparameter that controls the strength of the CQL regularization."}, {"title": "5 EXPERIMENT", "content": "5.1 Setup\nCodebase, Offline Datasets, and Experiment Setup. We evaluate our proposed OVMSE algorithm using the StarCraft Multi-Agent Challenge (SMAC), a key benchmark for MARL algorithms, across four tasks: 2s3z (easy), 3s5z (easy), 5m_vs_6m (hard), and 6h_vs_8z (super hard). For the tasks 2s3z, 5m_vs_6m, and 6h_vs_8z, we use offline datasets provided by CFCQL [24]. We collected our own dataset for 3s5z, as it is not publicly available. Offline training leverages medium and medium-replay datasets [8]. During the online phase, we apply the CQL loss for OVMSE in 2s3z, as it stabilizes online training. However, for 3s5z, 5m_vs_6m, and 6h_vs_8z, we exclude the CQL loss because it hinders online learning by preventing the discovery of new strategies. Our implementation, including code and hyperparameters, is based on PYMARL2 [11] and CFCQL. Fine-tuning is conducted for 1 million environment steps in 2s3z and 3s5z, and for 3 million steps in 5m_vs_6m and 6h_vs_8z.\n5.2 Results\nFigure 2 and 3 present the training curves across four different tasks, pre-trained using both the medium and medium replay datasets. Our results show that OVMSE enables faster online fine-tuning and achieves superior overall performance. We compare OVMSE against baseline methods across three key aspects: (i) online performance, (ii) sample efficiency, and (iii) performance drop during the offline-to-online transition.\nTable 1 shows the final fine-tuned performance for all tasks (except for 2s3z, where all algorithms achieve near 100%. win rates). In the 6h_vs_8z medium replay task, OVMSE outperforms QMIX and Switch-CQL by more than 20%. in win rate. Similarly, in the 3s5z medium tasks, OVMSE leads other baselines by roughly 10% in win rate, and in the 3s5z medium replay, 6h_vs_8z medium, and 5m_vs_6m medium tasks, OVMSE demonstrates a lead of more than 5% in test win rate.\nIn terms of sample efficiency, OVMSE performs significantly better than the baselines. For instance, in the 6h_vs_8z medium and medium replay tasks, OVMSE achieves a 40% win rate approximately 1.5 million environment steps ahead of other baselines. In the 5m_vs_6m medium task, OVMSE achieves a 40% win rate about 0.5 million environment steps earlier than QMIX and Switch-CQL, with these baselines needing a total of 1.5 million additional steps to match OVMSE's performance. In the 2s3z medium and medium replay tasks, OVMSE achieves an 80% win rate with a lead of more than 100K environment steps over MACQL, MACal-QL, and QMIX.\nFurthermore, OVMSE experiences significantly less performance drop compared to all baselines during the offline-to-online transition. Notably, although other baselines typically use a higher mixing ratio than OVMSE, OVMSE still exhibits significantly less performance drop in the 6h_vs_8z medium, medium replay, 2s3z medium and medium replay, and 5m_vs_6m medium and medium replay tasks. This difference in performance drop is more evident in terms of mean test return, which is shown in Figure 3. These results show that our OVMSE algorithm efficiently utilizes offline data and avoids unlearning.\n5.3 Ablation Study\nIn this section, we conduct additional experiments to demonstrate the effectiveness of both OVM and Sequential Exploration. In the previous section, we established the superiority of the OVMSE approach. Through ablation studies, we aim to answer two key questions:\n(i) Does OVM preserve offline knowledge and lead to better performance?\n(ii) Does SE enable efficient exploration and lead to better performance?\nWe perform these ablation studies on the 2s3z medium task, 6h_vs_8z medium task, and medium replay tasks. As shown in Figure 4(a), (b), and (c), we set the mixing ratio to 0.0 for all tasks except for the 6h_vs_8z medium task, where a mixing ratio of 0.1 is applied. Additionally, we conduct a specific ablation study on the 2s3z medium task to examine the effect of reusing offline data during the online phase. As illustrated in Figure 4, we report the mean test returns as a function of environment steps, as this metric more effectively captures performance drops during the initial stages of online learning.\nOVM is effective. Our results demonstrate that OVM plays a crucial role in O2O MARL. In the 2s3z medium task, using SE alone without OVM results in a more significant performance drop during the initial stage of the online phase. Additionally, using OVM alone results in a much smaller performance drop compared to the action proposal approach [9]. In the 6h_vs_8z medium replay task, OVMSE significantly outperforms SE alone, due to the preservation of offline knowledge by OVM. Similarly, in the 6h_vs_8z medium task, OVMSE outperforms SE, with SE requiring 2 million steps to catch up.\nSE is effective. Our results also demonstrate that SE is highly effective. In the 2s3z medium task, OVMSE outperforms OVM alone by a large margin in the first 200k steps, due to SE's more efficient exploration. In the 6h_vs_8z medium task, using SE alone outperforms Switch CQL, where the only difference between the two algorithms is the use of e-greedy exploration versus decentralized SE. This clearly demonstrates the effectiveness of SE.\nOVMSE has minimal reliance on offline data. As shown in Figure 4(d), MACal-QL experiences a much larger performance drop when the mixing ratio is set to 0.0 compared to 0.5. This occurs because reusing offline data during online training helps reduce distribution shift. However, over-reliance on offline data can lead to slow online fine-tuning, especially when the offline data is of poor quality. In contrast, OVMSE with a mixing ratio of 0.0 achieves fast and stable online fine-tuning during the online phase, demonstrating that OVMSE is not sensitive to data distribution change and well preserves the learned Q-values.\n5.4 Discussions\nIn this section, we discuss the role of offline pre-trained knowledge in O2O MARL. Offline knowledge is beneficial in several ways. First, it helps guide online exploration much more efficiently than training from scratch. It also provides a strong initial policy. Furthermore, the offline pre-trained knowledge can facilitate faster online learning and lead to better performance, as clearly demonstrated in the 6h_vs_8z medium replay task shown in Figure 4 (b), where OVMSE outperforms SE by a large margin, thanks to its offline knowledge.\nHowever, we also observe that offline knowledge can hinder online learning in some cases. This issue likely arises because current offline training algorithms are not designed for the online phase. A good offline policy requires some degree of over-fitting to the offline datasets (e.g., achieved using pessimism or behavior cloning). However, for O2O MARL, this over-fitting can cause online learning to require many samples to correct or even result in the agent sticking to the offline policy, thereby preventing improvement. This observation motivated the introduction of the memory coefficient Amemory and its corresponding annealing schedule, which allow OVMSE to gracefully adjust offline knowledge to the level of online returns."}, {"title": "6 CONCLUSIONS", "content": "In this work, we investigate the O2O MARL. We identify two major challenges for O2O MARL: (i) During the initial stage of online learning, distributional shift can degrade the offline pre-trained Q-values, leading to unlearning of previously acquired knowledge; (ii) Inefficient exploration in the exponentially large joint state-action space can slow down online fine-tuning. To address these challenges, we introduce Offline Value Function Memory (OVM) to preserve offline knowledge and a decentralized sequential exploration strategy for efficient online exploration. Our approach, OVMSE, enables stable and fast online fine-tuning and leads to improved fine-tuned results. Experiments across various tasks in SMAC demonstrate the superior performance of OVMSE."}]}