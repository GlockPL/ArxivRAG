{"title": "Heterogeneous Graph Transformer for Multiple\nTiny Object Tracking in RGB-T Videos", "authors": ["Qingyu Xu", "Longguang Wang", "Weidong Sheng", "Yingqian Wang", "Chao Xiao", "Chao Ma", "Wei An"], "abstract": "Abstract\u2014Tracking multiple tiny objects is highly challenging\ndue to their weak appearance and limited features. Exist-\ning multi-object tracking algorithms generally focus on single-\nmodality scenes, and overlook the complementary characteristics\nof tiny objects captured by multiple remote sensors. To enhance\ntracking performance by integrating complementary informa-\ntion from multiple sources, we propose a novel framework\ncalled HGT-Track (Heterogeneous Graph Transformer based\nMulti-Tiny-Object Tracking). Specifically, we first employ a\nTransformer-based encoder to embed images from different\nmodalities. Subsequently, we utilize Heterogeneous Graph Trans-\nformer to aggregate spatial and temporal information from\nmultiple modalities to generate detection and tracking features.\nAdditionally, we introduce a target re-detection module (ReDet)\nto ensure tracklet continuity by maintaining consistency across\ndifferent modalities. Furthermore, this paper introduces the\nfirst benchmark VT-Tiny-MOT (Visible-Thermal Tiny Multi-\nObject Tracking) for RGB-T fused multiple tiny object tracking.\nExtensive experiments are conducted on VT-Tiny-MOT, and\nthe results have demonstrated the effectiveness of our method.\nCompared to other state-of-the-art methods, our method achieves\nbetter performance in terms of MOTA (Multiple-Object Tracking\nAccuracy) and ID-F1 score. The code and dataset will be made\navailable at https://github.com/xuqingyu26/HGTMT.", "sections": [{"title": "I. INTRODUCTION", "content": "ULTI-OBJECT tracking (MOT) plays a crucial role\nin various applications such as autonomous driving,\nairspace surveillance, and motion prediction [1]\u2013[3]. With the\nadvancement of UAV technology, drone-borne remote sensing\nsystems have become an important tool for data collection, and\nserved as a supplement to manned aircraft and satellite remote\nsensing systems. However, the detection results obtained from\nsingle-modality sensors are often unreliable in challenging\nsituations such as low illumination, heavy occlusion, and haze\n[4]. To address these issues, aggregating complementary infor-\nmation from multiple modalities has shown promising results.\nAmong the various sensor combinations available for drones,\nvisible and thermal cameras are popular choices that provide\nhigh-resolution visible and thermal videos for detecting and\ntracking tiny objects in the far distance.\nRecently, several single object tracking (SOT) methods [5]\u2013\n[8] have been proposed for tracking targets in visible-thermal\nvideos, and have made significant advancements. However,\nthe tracking of multiple tiny objects in RGB-T videos remains\ninadequately explored due to the following challenges:\n1) The tracking methods that can exploit complementary\ninformation from visible and thermal modalities are\nmainly SOT methods, which overlook the interaction\nbetween different targets. It is non-trivial to transfer the\nSOT method to MOT method.\n2) The absence of paired RGB-T video datasets hinders\nthe progress in MOT research. While there are datasets\navailable for object detection and single object tracking\nin RGB-T videos, datasets with annotated target IDs\nsuitable for multiple tiny object tracking are still lacking.\n3) Maintaining tiny object trajectory is challenging due to\nthe limited detection and data association accuracy.\nAlthough significant progress has recently been made in\nMOT, the leading methods can not handle the RGB-T videos\nwell. MOT methods typically follow the tracking-by-detection\n(TBD) or the joint-detection-and-tracking (JDT) paradigm.\nThe TBD methods [9]\u2013[13] heavily rely on detection accuracy,\nmaking the tracking process susceptible to unstable detections\nin challenging scenarios. By using a bi-modal detector [14],\n[15], the TBD methods can achieve a minor improvement\ndue to the neglection of temporal information. Some JDT\ntechniques [16], [17] have emerged to exploit the target\ntemporal information. However, these trackers' performance\nis limited in RGB-T videos as they are tailored to integrate\nhomogeneous information and do not account for the domain\ngap between the visible and thermal modalities.\nAs illustrated in Fig. 1, the target state may differ in different\nmodalities under diverse situations. For example, under low\nillumination (LI) scenarios, targets may be visible in the\ninfrared spectrum and invisible in the visible spectrum, posing\na challenge we refer to as 'mismatch' denoted by 'MM'.\nDue to the discrepancies in target states and sensor systems,\nsimply merging data from two modalities could hinder tracking\nperformance.\nMotivated by the success of Heterogeneous Graph Trans-\nformer in modeling web-scale heterogeneous data, we seek\nto use Heterogeneous Graph Transformer [18] to handle the\ntargets discrepancies in different modalities. In this paper, we\npresent a novel approach called HGT-Track (Heterogeneous\nGraph Transformer based Multi-Tiny-Object Tracking) that\nutilizes Heterogeneous Graph Transformer to construct the\ncollaborative target representation. Unlike previous methods,"}, {"title": "II. RELATED WORK", "content": "In this section, we briefly review three research streams that\nare closely related to our works."}, {"title": "A. Multi-modal tracking", "content": "With the increasing availability of various sensors, many\nmulti-modal tracking methods [19]\u2013[22] have been proposed\nto enhance the efficiency of tracking systems. Lukezic et\nal. [23] investigate the utilization of depth information in\nvisual object tracking. However, depth sensors are limited to a\nconfined range of 4-5 meters. To overcome this limitation, [24]\nemploys LiDAR and radar to capture accurate distance and\nangle information in autonomous driving scenarios. Compared\nwith the sensors above, thermal sensors are more economically\naffordable and suitable for remote sensing applications.\nRGB-T fusion tracking can be categorized into pixel-level,\nfeature-level, and decision-level fusion, depending on when\nthe dual-modal information is fused. Li et al. [5] propose an\nRGB-T fusion tracking method based on collaborative sparse\nrepresentation at the decision level within a Bayesian filtering\nframework. To improve the efficiency of RGB-T object track-\ning, Zhai et al. [25] introduce a low-rank constrained tracker\nto perform RGB-T tracking in the correlation filter framework.\nRecently, several deep learning-based methods [26]\u2013[31] have\nbeen proposed to enhance the information fusion in RGB-T\ntracking, which leverage the powerful capabilities of neural\nnetworks. MDNet [26] learns discriminative target represen-\ntation with multiple domain-specific branches. Zhu et al. [27]\npropose a method to reduce the noise and redundancy after\nthe deep aggregation of multi-modality features. Li et al. [31]\npropose an adaptive fusion module to improve information\nfusion efficiency. Though progress has been made, the deep-\nlearning models still face problems such as large model sizes\nand high computation costs. Additionally, existing methods,\nwhether traditional or deep learning-based, are all focused\non single object tracking without considering target number\nuncertainy. Therefore, there is a necessity to explore efficient\nmethods for multiple object tracking (MOT) in RGB-T videos."}, {"title": "B. Multi-object tracking", "content": "Current methods for Multiple Object Tracking (MOT) can\nbe broadly categorized into two approaches: Tracking-by-\nDetection (TBD) and Joint Detection and Tracking (JDT).\nTBD treats tracking as a two-stage process involving detection\nfollowed by tracking. While modern detectors [32]\u2013[34] per-\nform well in normal scenes, accurate tracking remains chal-\nlenging in remote-sensed scenes due to weak target signals.\nTo improve tracking performance in satellite videos, Xiao et\nal. [11] propose DSFNet, which generates accurate detections\nusing 3D convolution to process video clips at the cost of pro-\ncessing speed. Some trackers [12], [13], [35] optimize tracker"}, {"title": "III. PROBLEM FORMULATION", "content": "Multiple object tracking in RGB-T videos aims at inte-\ngrating complementary information from both modalities to\nachieve robust tracking performance.\nAssume that the state of object in the visible and thermal\nimages is denoted by a set $X_k = [X_k^v, X_k^t, q_k]$, where the\nsuperscript $v$ and $t$ represent the visible and thermal modality,\nrespectively. The subscript $k$ represents time. Since the target\nmay be neglected by detector, the target visibility represented\nby variable $q$ may have four possibilities: sensed by both,\nmissed by both, or sensed by one certain modality. The\nstate vector sets $X_k^v$ and $X_k^t$ represent the objects state in\ncorresponding modalities. The state vector of the $i$th object at\ntime $k$ is defined as follows,\n$X_{k,i}^{v/t} = [x_{k,i}, y_{k,i}, w, h]^T$,\nwhere $(x, y)$ represent the 2D position of the object in the\ncamera image, $(w, h)$ is the width and height of the target\nbounding box.\nThe main task is to estimate $X_k$ with the sequence of image\npairs $(I^v_k, I^t_k)_{k=1}^K$,\n$(I_0^v, I_0^t), (I_1^v, I_1^t), ..., (I_k^v, I_k^t) \\Rightarrow X_k$,\nwhere the state set at time k is estimated by processing the\nsequence of dual modality image pairs.\nFor multi-modal JDT methods, the prior for detection is\nprovided by both modalities:\n$Prior = Agg(I_k^{v/t} | X_k^{v/t})$,\nwhere $Prior$ represent the aggregated prior information to\nenhance object detection at time k, and function $Agg(\\cdot)$ is used\nto integrate multi-modal information. Then, for each modality,\nthe object can be detected with the prior information.\n$Z_k^{v/t} = Det(I_k^{v/t} | Prior)$,\nwhere $Det(\\cdot)$ represents the detect function, $Z_k^v$ and $Z_k^t$\nrepresent the detections at time k in the visible and thermal\nmodalities, respectively.\nThe tracking module takes the detection results from both\nmodalities at time k and previous target state at time k \u2212 1 as\ninput, and outputs the estimation of target state vector at time\nk.\n$X_{k,i}^{v/t} = Track(Z_k^{v/t}, I_{k-1}^{v/t}, I_k^{v/t})$,\nwhere function $Track(\\cdot)$ estimates the state vector $X_k^v$ and\n$X_k^t$. The tracking module considers both temporal information\nand cross-modal information to improve the accuracy of state\nestimation. The tracking result can be used to aid in the\nassociation of detections with tracklets. Finally, the object state\n$X_k$ updates with the matched detections."}, {"title": "IV. METHODOLOGY", "content": "In this section, we present our HGT-Track (Heterogeneous\nGraph Transformer based Multi-Tiny-Object Tracking), which\naims to effectively integrate spatial and temporal information\nfrom both modalities for joint online object detection and\ntracking. To achieve this, HGT-Track utilizes a Heterogeneous\nGraph Transformer (HGT) to aggregate target-specific infor-\nmation and enhance detection capabilities. Then, the genera-\ntion of tracklets are conducted with the complementary feature.\nAn overview of the HGT-Track framework is illustrated in Fig.\n2.\nWe first introduce the Heterogeneous Graph Transformer-\nbased encoder (Section IV-B). The HGT encoder fuses in-\nformation by concurrently encoding four types of nodes and"}, {"title": "A. Feature Embedding and Graph Generation", "content": "Given an RGB image $I_k^v \\in \\mathbb{R}^{W \\times H \\times 3}$ and a thermal\nimage $I_k^t \\in \\mathbb{R}^{W \\times H \\times 1}$ obtained at time $k$, we initially project\nthe images into modal-specific feature spaces using vanilla\nconvolution. Here, W and H represent the width and height\nof the images, respectively. Next, we employ the parameter-\nshared Pyramid Vision Transformer (PVTv2) [49] to extract\nmultiple scale features $Mult_k^{v/t} \\in \\mathbb{R}^{\\frac{W}{r} \\times \\frac{H}{r} \\times h}$ for each modality.\nIn this context, r denotes the downscale ratio and h denotes\nthe feature dimension. Taking each pixel as potential detection,\nwe reshape $Mult_k^{v/t}$ by the operation 'Flatten' to generate\nimage dense detection queries $D_k^{v/t} \\in \\mathbb{R}^{oh}$ for the two\nmodalities.\nFor the tracking feature, we treat the target center at\nprevious frame as the target node. The tracking feature\n$T_{k-1}^{v/t} \\in \\mathbb{R}^{n_{k-1} \\times h}$ are sampled from $Mult_{k-1}^{v/t} \\in \\mathbb{R}^{\\frac{W}{r} \\times \\frac{H}{r} \\times h}$,\ncorresponding to the target nodes saved in $X_{k-1}^{v/t}$, namely\n'Nodes Sampling'. Here, $n_{k-1}^v$ and $n_{k-1}^t$ denote the number\nof targets at time k \u2212 1 in corresponding modalities.\nThe dense detection queries and sparse tracking queries\nserve as four types of graph nodes. Considering the bias\nbetween the two modalities, we use a heterogeneous graph\n$G^a$ to model relationships among the various types of nodes.\nWe enable sparse interactions between the two modalities on\nnodes connected with $T_{k-1}^v$ and $T_{k-1}^t$. The spatial distance"}, {"title": "B. Information Integration", "content": "The defined $G^a$ with nodes and edges is then fed into\nthe Heterogeneous Graph Transformer (HGT) encoder and\ndecoder for information integration.\n1) Heterogeneous Graph Transformer encoder: The Het-\nerogeneous Graph Transformer encoder, as illustrated in Fig. 3\n(a), is used to model the spatial and temporal correlations and"}, {"title": "C. Object Detection and Tracking", "content": "For each modality, the decoded queries $D_k$ are post pro-\ncessed to generate detections. First, $D_k$ are reshaped back to\n$\\mathbb{R}^{\\frac{W}{r} \\times \\frac{H}{r} \\times h}$, and then used as inputs to several branches consist\nof convolution layers to generate the object center heatmap\n$C_k \\in \\mathbb{R}^{\\frac{W}{r} \\times \\frac{H}{r} \\times \\tau}$, bounding box $S_k \\in \\mathbb{R}^{\\frac{W}{r} \\times \\frac{H}{r} \\times 2}$, and refine\noffset $R_k \\in \\mathbb{R}^{\\frac{W}{r} \\times \\frac{H}{r} \\times 2}$. Here, $S_k$ contains the height and width\nof the object's bounding box prediction map, which has a\nresolution that is $\\frac{1}{r}$ of the original resolution. The target center\n$C_k$ are obtained by applying a threshold to $C_k$. The size of each\nobject is extracted from $S_k$ corresponding to each position $c_k$.\nThe set of detections is denoted as $Z_k = \\{C_{k,i}, S_{k,i}\\}_{i=1}^{N_z}$.\nThe tracking queries $T_{k-1}$ are input into the tracking branch,\nwhich consists of two linear layers with ReLU activation, to\nregress the tracking offset $t_k \\in \\mathbb{R}^{N_{k-1} \\times 2}$, where $N_{k-1}$ represents the\nnumber of targets at time k - 1. Subsequently, the predicted\npositions of the objects $C_k = \\{C_{k-1,i} + t_{k,i}\\}_{i=1}^{N_{k-1}}$ can be\ncomputed using the predicted tracking offsets. These predicted\npositions are further used to assist in the detection assignment\nprocess."}, {"title": "D. Tracklet Generation", "content": "1) Cross-modal detection matching: Compared to normal\nsingle modality methods for MOT, HGT-Track performs cross-\nmodal detection matching before detection-tracklet association\nin each modality. Given the detections $Z_k^v$ and $Z_k^t$, the distance\nDis between cross-modal detections is calculated using the\nIntersection over Union (IoU) metric:\n$Dis(i, j) = 1 \u2013 IOU(z_i^v, z_j^t)$,\nwhere the function IOU computes the Intersection over Union\nbetween the $i$th detection $z_i^v$ in $Z_k^v$ and the $j$th detection $z_j^t$\nin $Z_k^t$. The detections are then matched using the Hungarian\nalgorithm [53]. The variable $q_k$ of $X_k$ is updated based on the\ncross-modal matching result.\n2) Detection and tracklet association: Then, we assign the\ndetections to existing tracklets based on the edge affinity\nmatrix A. The edge weights of A are determined through\nregression using the edge feature. Specifically, the detection\nfeature $U \\in \\mathbb{R}^{N \\times h}$ is sampled from $D_k \\in \\mathbb{R}^{\\frac{W}{r} \\times \\frac{H}{r} \\times h}$ at the\ncorresponding detection positions. On the other hand, the\ntracking feature $V \\in \\mathbb{R}^{\\tilde{N} \\times h}$ corresponds to\u00ce exactly. The\nedge feature $E$ is defined as:\n$E_{ij} = U_i - V_j$,\nwhere $U_i$ is the feature of the object at the $i$th detection node\nand $V_j$ is the feature of the $j$th tracking node. The affinity\nmatrix A of dimension N \u00d7 N \u00d7 1 is then regressed from E.\n$A = Sigmoid(Conv2(ReLU(Conv1(E))))$,\nwhere Conv1 and Conv2 represent two convolutional layers\nwith a kernel size of 1, and Sigmoid function is used to output\nthe values of A between 0 and 1. The matrix A can be used"}, {"title": "E. Loss Function", "content": "The loss function consists of a detection loss term and a\ndata matching loss term.\n1) Detection Loss: The detection loss is composed of 4\nparts including the center focal loss $L_{cf}$, the box size loss\n$L_{bs}$, the refinement loss $L_{r}$ and the tracking displacement\nloss $L_{td}$. The aforementioned four loss terms correspond\nto the classification branch and three regression branches,\nrespectively.\nThe center focal loss [54], [55] $L_{cf}$ is used to train the\nheat map branch, which simultaneously penalize the classifi-\ncation error and localization error. The ground-truth heatmap\n$Y \\in [0,1]^{\\frac{W}{r} \\times \\frac{H}{r} \\times \\tau}$, where $\\tau$ is the number of object classes.\n$L_{cf}$ is composed of the center focal loss $L_{cf}^v$ and $L_{cf}^t$ in two\nmodalities, and $L_{cf}^v$ is computed as follows,\n$L_{cf}^v = \\frac{1}{N^v} \\sum_{xyc} \\begin{cases}\n(1 - Y_{xyc})^{\\alpha} log(Y_{xyc})  & \\text{if } Y_{xyc} = 1 \\\\\n(1 - (1 - Y_{xyc}))^{\\alpha} Y_{xyc} log(1 - Y_{xyc}) & \\text{otherwise}\n\\end{cases}$\nLoss $L_{cf}^t$ is computed similarly to $L_{cf}^v$.\nFor the remaining branches, losses are computed sparsely\nat the position of targets, i.e. the $i^{th}$ object at location $p_i$ with\nthe bounding box size $s_i$, the loss is computed as follows:\n$L_{bs} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\delta_v}{s_v^{p_i}} + \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\delta_t}{s_t^{p_i}}$,\nAnalogously to $L_{bs}$, the refinement loss $L_r$ and the tracking\ndisplacement loss $L_{td}$ are also computed at the center position\nof targets. The overall detection loss is the weighted sum of\nthe four loss items."}, {"title": "V. VISIBLE-THERMAL TINY MULTIPLE OBJECT\nTRACKING BENCHMARK", "content": "In this section, we introduce the newly collected visible-\nthermal tiny multiple object tracking (VT-Tiny-MOT) bench-\nmark, including dataset construction with statistics analysis\nand baseline methods with both single-modality and dual-\nmodality inputs."}, {"title": "A. Data Collection and Annotations", "content": "The visible and thermal videos were simultaneously\nrecorded using visible and thermal cameras mounted on a\nprofessional UAV (DJI Mavic 2). Each video pair consists\nof a visible video and a thermal video. To ensure accurate\nalignment between the two modalities, a homography matrix\nis computed using Zhang's method [56] and applied to align\nthe remaining frames of each video pair. However, due to two\nmain factors, the registration accuracy is limited, and there\nexists bias between each video pairs.\nFirst, the vertical arrangement of the cameras on the UAV\nleads to variations in the depth of field (Dof), which violates\nthe assumptions made in [56]. Second, the Zhang's method\n[56] is ineffective in handling the disparity variations in stereo\nimage pairs [57], especially in cases of texture-lacking images.\nDespite these limitations, the dataset has been refined to\nachieve a frame rate of 15 FPS and a resolution of 512 \u00d7 640\npixels."}]}