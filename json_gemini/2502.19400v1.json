{"title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding", "authors": ["Max Ku", "Thomas Chong", "Jonathan Leung", "Krish Shah", "Alvin Yu", "Wenhu Chen"], "abstract": "Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the 03-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.", "sections": [{"title": "1 Introduction", "content": "A key objective of AI systems is to assist humans in solving complex problems, particularly in domain-specific challenges. To achieve this, AI must go beyond surface-level pattern matching to achieve deeper conceptual understanding to effectively address these problems. Recent research has proposed evaluating AI performance on theorem-driven datasets through multiple-choice question answering (Zhang et al., 2024) and open-ended short question answering (Chen et al., 2023b). However, these approaches primarily assess textual reasoning and may not fully capture an AI system's ability to grasp theorem concepts at a deeper level. Studies have shown that AI models can be sensitive to superficial cues, such as the order of answer choices in multiple-choice questions (Pezeshkpour and Hruschka, 2023; Keluskar et al., 2024). This raises concerns about the robustness of such evaluations in truly measuring comprehension. Moreover, current theorem-focused datasets are predominantly text-based, overlooking how complex concepts are often best understood through structured visualizations.\nTheorem reasoning is inherently multimodal, particularly in areas such as geometry, topology,"}, {"title": "2 Related Works", "content": "The rapid advancements in large language models (LLMs) and large vision-language models (VLMs) have unlocked unprecedented capabilities in understanding multimodal content. Models such as GPT-4 (OpenAI, 2023), Gemini (Gemini-Team et al., 2024), Claude-3.5 Sonnet v1 (Anthropic, 2024), and DeepSeek (DeepSeek-AI et al., 2024) have demonstrated strong abilities in processing complex textual information and analyzing visual inputs within a unified framework (Zhang et al., 2023b). These breakthroughs have enabled transformative applications across various domains, including visual content understanding (Hu et al., 2023; Ku et al., 2023), code generation (Nijkamp et al., 2023; Jimenez et al., 2024; Yang et al., 2024a), and reasoning over structured data. To tackle complex tasks, researchers have explored LLM agents: AI systems that leverage LLMs to autonomously reason, plan, and execute tasks by interacting with structured environments or external tools. These agents have been deployed in various goal-oriented applications, such as scientific discovery (Lu et al., 2024; Si et al., 2024; Schmidgall et al., 2025), coding solutions (Abramovich et al., 2024), multimodal visual generation (He et al., 2024), and computer environment interaction (Xie et al., 2024). In this work, we extend the use of LLM agents into the domain of theorem explanation and visualization."}, {"title": "2.1 LLM and Agents", "content": null}, {"title": "2.2 LLM in Theorems Understanding", "content": "LLMs have demonstrated remarkable capabilities in solving complex mathematical problems, including formal theorem proving and symbolic reasoning. To evaluate these abilities, researchers have introduced multiple benchmark datasets, primarily consisting of multiple-choice and short-answer question answering (QA) tasks (Zhang et al., 2024; Amini et al., 2019; Hendrycks et al., 2021). Early studies centered on elementary to high school-level mathematics, leading to datasets such as Math23K (Zhou et al., 2023), GSM8K (Cobbe et al., 2021), and GeoQA (Chen et al., 2022a). As LLM capabilities advanced, more domain-specific benchmarks emerged, extending evaluation to fields like science reasoning (ScienceQA) (Lu et al., 2022), financial reasoning (FinQA) (Chen et al., 2022b), and theorem comprehension (TheoremQA) (Chen et al., 2023b). These datasets collectively assess LLMs' ability to solve mathematical and scientific problems up to the university level. However, existing benchmarks remain predominantly text-based, overlooking the role of visual intuition in mathematical reasoning. Many mathematical concepts are best understood through structured diagrams and dynamic representations, which current LLM evaluations fail to capture. To address this gap, we introduce an AI framework to generate theorem explanations in long-form videos, integrating symbolic derivations with structured visualizations to enhance comprehension."}, {"title": "2.3 LLM in Visualizations", "content": "Recent advancements in AI-driven visualization have enabled AI systems to generate structured visual content from textual descriptions (Li et al., 2024). These models typically process text-based inputs and produce programmatic representations, which are then converted into visual outputs (Ritchie et al., 2023; Goswami et al., 2025). This approach has been applied across various domains, including scientific visualization (Yang et al., 2024b), data representation (Galimzyanov et al., 2024), and motion graphics (Zhang et al., 2023a). Efforts such as Drawing-Pandas (Galimzyanov et al., 2024) have introduced benchmarks for evaluating code-based plotting in Matplotlib and Seaborn. Follow-up works like MatPlotAgent(Yang et al., 2024b) demonstrated that agentic approaches outperform agentless methods in visualization generation, while PlotGen (Goswami et al., 2025) incorporated multimodal feedback for iterative refinement, further improving visualization quality. Our work is the first to explore AI-driven visualization for generating animated theorem explanations, seamlessly integrating step-by-step symbolic derivations with structured motion graphics, bridging the gap between mathematical reasoning and visual comprehension."}, {"title": "3 Method", "content": "We develop TheoremExplainAgent (TEA), an agentic pipeline designed to automate the generation of videos using multiple specialized agents as shown in Figure 3. The process begins with the planner agent, which creates a high-level video plan according to the specified theorem. This plan consists of multiple scenes, each corresponding to a key segment of the resulting video. Once the initial plan is created, the planner agent refines the details of each scene, breaking them down into smaller components that define the specific visual elements, animations, and transitions needed. These detailed"}, {"title": "3.1 Task Definition", "content": "Model Input. The model receives a theorem along with a short description that provides context, which helps the model identify the theorem.\nModel Output. The model is to output a video that combines animations, structured derivations, and voiceover narration to provide a multimodal and comprehensive explanation of the theorem. The video is expected to be longer than a minute, featuring long animations across different scenes, with narration guiding the viewer through step-by-step proofs and real-world applications."}, {"title": "3.2 TheoremExplainAgent (TEA)", "content": null}, {"title": "3.3 TheoremExplainBench (TEB)", "content": "We curate an evaluation dataset comprising 240 theorems from various disciplines, including Computer Science, Chemistry, Mathematics, and Physics. Each entry includes the theorem name and a contextual description, sourced from OpenStax (Baraniuk, 2025) and LibreTexts (Larsen, 2025). To facilitate structured assessment, the the-"}, {"title": "4 Experimental Results", "content": "For the agent candidates in TheoremExplainAgent, we experimented with GPT-40 (OpenAI, 2023), Gemini 2.0 Flash (DeepMind, 2025), Claude 3.5 v1 (Anthropic, 2024), and 03-mini (OpenAI, 2025). Each candidate was used for both the planner agent and coding agent, ensuring consistency across configurations. We evaluated all agents across 240 theorems from TheoremExplainBench, comparing their performance under different setups. Our findings indicate that an agentless approach fails to generate videos longer than 20 seconds, whereas TheoremExplainAgent successfully produces videos of up to 10 minutes. Consequently, all experimental results presented below are based on the agentic approach.\nTable 1 reveals that the success rate in generating long-form theorem explanation videos varies significantly across difficulty levels and subjects. Overall, 03-mini consistently outperforms other models, maintaining high success rates across both easy and hard tasks, as well as across different STEM domains. In contrast, GPT-40 performs moderately well but show a declining success rate as complexity increases, suggesting difficulties in handling longer and more structured explanations. Gemini 2.0-Flash struggles the most, with notably lower success rates across all conditions. Across subjects, Mathematics tends to have the highest success rates, whereas Chemistry appear to be the most challenging domain. This observation may be attributed to the fact that complex objects in Chemistry, such as flask shapes and atoms, are more challenging to illustrate than simpler primitives in Mathematics, like triangles."}, {"title": "4.1 Correlation Study", "content": "From Table 3, we observe that our proposed metrics show strong alignment with human ratings in Visual Relevance and Element Layout, while demonstrating weaker correlations in Accuracy & Depth, Logical Flow, and Visual Consistency. This suggests that humans are particularly sensitive to visual aspects, such as spatial layouts, but may struggle with evaluating long-form text or audio-based content in detail. Visual Consistency appears to be more subjective, which may explain its relatively lower correlation with human ratings. Additionally, Accuracy & Depth and Logical Flow exhibits the weakest correlation with human judgments, likely due to differences in how LLM and humans assess coherence. Humans can tolerate informal flow, while LLMs may penalize it. On the other hand, human ratings across all dimensions show moderate inter-rater agreement, as indicated by Krippendorff's alpha values. Notably, text-based dimensions achieve slightly higher agreement than visual-based ones, suggesting that textual evaluations are more consistently interpreted among raters."}, {"title": "4.2 Error Analysis", "content": "We analyzed the error logs from unsuccessful runs in the TheoremExplainAgent video generation process and identified three primary failure categories. The most common issue was Manim code hallucinations, which accounted for the majority of failures. These errors involved nonexistent functions, modules, object properties, or image assets, as well as incorrect function signatures with invalid parameter types and numbers, reflecting a misunderstanding of the Manim API. The second major issue stemmed from LaTeX rendering errors, primarily due to syntax mistakes and improper handling of special characters in mathematical expressions. Lastly, general coding errors were observed, including missing imports, undefined variables, and computational mistakes in NumPy-based operations. These findings reveal key challenges across LLMs, underscoring the need for better code reliability and API understanding in AI-generated videos."}, {"title": "4.3 Case Study", "content": "We included representative video outputs in Figure 6. This figure demonstrates that TheoremExplainAgent is capable of generating high-quality exploratory videos. For example, in Mathematics, the model effectively visualizes concepts such as Riemann sums, using animated grids and function plots to illustrate integral approximations. In Chemistry, the system successfully explains the Octet Rule, leveraging atomic models to depict electron sharing and bonding interactions. In Physics, it generates electromagnetic wave simulations, showcasing wave propagation and spectral analysis. In Computer Science, it produces a clear demonstration of Run-Length Encoding, using side-by-side comparisons of raw and compressed data representations. We examined more generated videos carefully and observed that videos in Mathematics, Physics, and Computer Science generally exhibit higher visual quality and coherence compared to those in Chemistry. One notable observation is that Chemistry-related visualizations often rely on simple geometric primitives to depict complex lab apparatus and molecular structures, which can limit their clarity and effectiveness. Additionally, most of the generated videos exhibit minor element layout issues, such as overlapping texts, inconsistent sizes, or suboptimal object positioning, which slightly affects the overall presentation quality, as illustrated in Figure 7.\nWe also found that visual explanations more effectively reveal reasoning errors than text, facilitating error diagnosis. From Figure 5, we observe that while the text-based explanation allows us to detect that the model's answer is incorrect, it does not provide insight into why the mistake occurred. It seems the model understand the chain code theorem, but it applies it incorrectly. Such ex-"}, {"title": "5 Conclusion", "content": "This paper introduces TheoremExplainAgent, a novel agentic approach for generating multimodal theorem explanations through structured video content. Our study demonstrates that integrating visual explanations significantly enhances the clarity and interpretability of theorem reasoning, surpassing text-based methods alone. To systematically evaluate AI-generated explanations, we present a benchmark spanning multiple disciplines with five automated evaluation metrics. Our experiments reveal that agentic planning is crucial for producing long-form, coherent explanations, with 03-mini achieving the highest success rate and overall performance. However, challenges remain in visual element layout, emphasizing the need for improved spatial reasoning and refinement in AI-generated animations. Additionally, our findings underscore the importance of multimodal explanations in identifying reasoning flaws that text-based assessments often miss, reinforcing the role of structured visual communication in AI-driven theorem understanding. Looking ahead, future work should focus on enhancing visual structuring techniques, improving agent coordination, and advancing video understanding to further refine multimodal explanations for LLM-driven theorem comprehension."}, {"title": "6 Limitations", "content": "While our approach demonstrates the potential of AI-generated multimodal theorem explanations, several limitations remain. AI models still struggle with complex visual structuring, particularly in consistent elements layout in long-form explanations. Retrieval-augmented generation (RAG) also requires more tokens, increasing computational costs and inference time, which may impact scalability."}, {"title": "7 Potential Risks", "content": "AI-generated explanations have the potential to mislead users if errors go undetected, leading to false confidence in incorrect reasoning. This poses a risk where unverified AI-generated content could propagate misconceptions or misinformation if widely disseminated without proper validation. Ensuring the accuracy and reliability of AI-generated explanations remains a critical challenge."}, {"title": "8 Artifacts", "content": "We experimented TheoremExplainAgent with GPT-40 (OpenAI, 2023), Gemini 2.0 Flash (DeepMind, 2025), Claude 3.5 v1 (Anthropic, 2024), and 03-mini (OpenAI, 2025). We are releasing the TheoremExplainBench on Huggingface dataset with MIT licence. It features 240 theorems across Computer Science, Physics, Chemistry and Math subjects."}, {"title": "9 Computational Experiments", "content": "All the experiments were conducted on a NVIDIA A100-SXM4-80GB GPU. Approximately 1500 US dollars were spent on API call for closed-source model experiments."}, {"title": "10 Acknowledgement", "content": "We express our gratitude to Votee AI for sponsoring API calls from closed-source models. We also thank Xueguang Ma, Dongfu Jiang, Zhi-Rui Tam, Chiu-Wai Yan, and Kelly Chiu for their insightful discussions."}, {"title": "A Gallery", "content": "In Figure 6 we present the high-quality videos generated by TheoremExplainAgent across four STEM domains. The images are extracted from different scenes in the videos, showing the consistency of the topic. In Figure 7 we present the poorly generated videos from TheoremExplainAgent and examine their artifacts. In Figure 8 we compare a high quality animation and a low quality animation, and how they were rated with our proposed metric."}, {"title": "B Prompt Templates", "content": "We adapt Chain-of-Thoughts (CoT) (Wei et al., 2023) and Program-of-Thoughts (PoT) (Chen et al., 2023a) when we design the prompt for TheoremExplainAgent. We present our prompts templates in the end of the Appendix."}, {"title": "C Supplementary Information", "content": null}, {"title": "C.1 Human Annotation Process", "content": "We recruited 12 student volunteers in our annotation process. We explained to the annotators that their annotations were to be used in our study only and would not be released publicly.\nWe show the user interface of our annotation website in Figure 9, including the instructions presented to our annotators. We supplement each of the dimensions with guiding questions to clarify what the annotators should score."}, {"title": "C.2 Runtime Statistics", "content": "We report the runtime and cost statistics in Table 4, assuming 4 fixed codes and 7 scenes per video, we evaluate the cost, inference time, and latency of different language models, and find that the Claude 3.5-Sonnet v1 model has the longest inference time (2240-2380s), while the Gemini 2.0-Flash and GPT-40 are the fastest (around 1120s).The RAG integration increases the number of input tokens significantly. RAG integration significantly increases the number of input tokens, with Claude 3.5-Sonnet v1 + RAG being the most used (1,050,000). Output tokens are less variable, with the 03-mini model generating the most tokens (154,000). The Gemini 2.0-Flash model is the most cost-effective ($0.10-$0.16), while the Claude 3.5-Sonnet v1 + RAG is the most expensive ($4.67)."}, {"title": "C.3 Potentials for Future Research", "content": "Recent community efforts (Shah et al., 2024; Gatekeep, 2024; GenerativeManim, 2024) have explored AI-driven Manim-based video generation for educational purposes. However, no scientific studies have systematically evaluated the effectiveness and robustness of these approaches. Our work introduces a novel agentic framework for generating multimodal theorem explanations and demonstrates that AI-generated videos can achieve performance comparable to human-made content, although the robustness is still limited. Nevertheless, further research is needed to assess their impact on Al's reasoning capabilities, visualization quality, and learning outcomes. Future directions include establishing benchmarks for AI-generated educational videos (within EdTech), integrating interactive elements to enhance engagement (within HCI/Visualization), and refining evaluation metrics to assess LLMs' multimodal explanation abilities (within NLP)."}]}