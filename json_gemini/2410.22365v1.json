{"title": "Vascular Segmentation of Functional Ultrasound Images using Deep Learning", "authors": ["Hana Sebiaa", "Thomas Guyet", "Micka\u00ebl Pereira", "Marco Valdebenito", "Hugues Berry", "Benjamin Vidal"], "abstract": "Segmentation of medical images is a fundamental task with numerous applications. While MRI, CT, and PET modalities have significantly benefited from deep learning segmentation techniques, more recent modalities, like functional ultrasound (fUS), have seen limited progress. fUS is a non invasive imaging method that measures changes in cerebral blood volume (CBV) with high spatio-temporal resolution. However, distinguishing arterioles from venules in fUS is challenging due to opposing blood flow directions within the same pixel. Ultrasound localization microscopy (ULM) can enhance resolution by tracking microbubble contrast agents but is invasive, and lacks dynamic CBV quantification. In this paper, we introduce the first deep learning-based segmentation tool for fUS images, capable of differentiating signals from different vascular compartments, based on ULM automatic annotation and enabling dynamic CBV quantification. We evaluate various UNet architectures on fUS images of rat brains, achieving competitive segmentation performance, with 90% accuracy, a 71% F1 score, and an IoU of 0.59, using only 100 temporal frames from a fUS stack. These results are comparable to those from tubular structure segmentation in other imaging modalities. Additionally, models trained on resting-state data generalize well to images captured during visual stimulation, highlighting robustness. This work offers a non-invasive, cost-effective alternative to ULM, enhancing fUS data interpretation and improving understanding of vessel function. Our pipeline shows high linear correlation coefficients between signals from predicted and actual compartments in both cortical and deeper regions, showcasing its ability to accurately capture blood flow dynamics.", "sections": [{"title": "1. Introduction", "content": "Blood flow and volume are closely linked to neuronal activity changes in the brain, a phenomenon known as neurovascular coupling. This relationship has enabled functional neuroimaging techniques, such as functional ultrasound imaging, to successfully probe brain activity at the mesoscopic scale in both animals and humans. Functional ultrasound imaging (fUS) is a neuroimaging technique that leverages hemodynamic signals to map brain activity, achieving high spatial resolution down to 100 \u00b5m and temporal resolution as low as 400 ms [1, 2, 3]. This method captures changes in cerebral blood volume related to neuronal activity through ultra-fast Power Doppler imaging. These images are analyzed to understand brain functions and pathologies [4, 5, 6]. Accurately identifying whether signals originate from arterioles or venules in these images is crucial to improve our understanding of brain vascular dynamics, as these distinct vascular compartments are differently involved in spontaneous fluctuations of vessel tone and functional hyperemia [7, 8, 9, 10] as well as energy supply [11] and waste clearance [12]. Unfortunately, while the spatial resolution of fUS is relatively high for in vivo mesoscopic brain imaging compared to fMRI, it is insufficient to resolve individual blood vessels. Consequently, a single fUS pixel may encompass multiple vessel types, each responding differently to neurovascular changes, making it challenging to distinguish between these compartments. Although Color Doppler sequences in ultrasound imaging can determine blood flow direction \u2013 which helps in differentiating opposing flows in cortical arterioles and venules \u2013 this method still struggles to clearly identify these compartments. This limitation arises because upward and downward flows often coexist within the same 100-micron voxels, canceling each other out and obscuring clear compartmental distinction. To overcome this, ultrasound localization microscopy (ULM) is an advanced imaging modality that also employs ultra-fast sampling techniques to significantly enhance spatial resolution to just a few microns [13, 14]. This advanced approach not only improves spatial resolution but also enables the determination of blood flow direction. Nonetheless, it requires the intravenous injection of micro-bubble contrast agents and the tracking of their position along a protracted time period to create a detailed vascular map. This map differentiates between upward and downward flows based on the movement direction of the micro-bubbles. While ULM provides comprehensive insights into cerebral blood flow dynamics, it lacks the temporal resolution of functional ultrasound. Moreover, its deployment is complex and invasive, necessitating the injection of contrast agents and presenting significant challenges in managing large data volumes required for image reconstruction.\nThis study leverages deep learning to enhance fUS image analysis by circumventing the conventional use of ULM. Specifically, we aimed to differentiate between upward and downward blood flows, corresponding to arterial and venous structures in the cortical region, directly from fUS images, without relying on ULM or requiring a particular data processing of the intermediate raw data. Our approach focused on automating an image segmentation task by training a deep learning model on a dataset specifically annotated to identify these vascular compartments. The process of constructing annotations, based on available ULM images, is detailed in Section 3.3. Once the model is trained, it can be applied to new fUS images to accurately retrieve vascular structures without requiring ULM sessions or manual annotation. The challenge was to identify a deep learning architecture that could achieve high accuracy. We focused our investigation on UNet architectures, known for their effectiveness on medical images, even when trained on small datasets [15]. To address this, we proposed a benchmarking module that compares multiple segmentation architectures as detailed in Section 3.4.2. Our results, summarised in Section 4.4.1, demonstrated a segmentation quality comparable to that achieved for other imaging modalities (MRI, OCT,...) on vascular and tubular structures. Additionally, we investigated, in Section 4.4.2, how incorporating different temporal frames from fUS data affects the segmentation quality, exploring potential improvements in accuracy. Finally, we showed, in Section 4.4.3, that training a model on fUS images acquired in a resting state generalize to images acquired under visual stimulation with the same precision, without requiring re-training. In Section 4.4.4, we illustrated how this segmentation approach can be used in a practical way to improve fUS signal interpretation."}, {"title": "2. Related work", "content": null}, {"title": "2.1. Different applications of Deep Learning on functional ultrasound", "content": "Despite the potential benefits, fUS has seen few studies that make use of advanced computational methods, particularly for image segmentation task. The existing research primarily focuses on other aspects, such as improving image reconstruction from sparse data and enhancing sensitivity in neuroimaging applications. For instance, Di Ianni and Airan [16] have developed a deep learning platform known as Deep-fUS, which significantly reduces the amount of ultrasound data required while maintaining image quality. Their work demonstrates the potential for deep learning to significantly streamline fUS imaging using portable devices and resource-limited settings. This approach also highlights the innovative use of CNN in reconstructing power Doppler images from under-sampled sequences. Similarly to fUS, ULM has also seen limited exploration with deep learning. van Sloun et al. [17] have leveraged CNN to achieve super-resolution in ULM, significantly improving the precision of micro-bubble localization, while Milecki et al. [18] have developed a spatio-temporal framework that refines the process of micro-bubble tracking. While the aforementioned works have made substantial advancements in improving the quality of images produced by fUS and ULM, they primarily focus on enhancing image reconstruction and resolution."}, {"title": "2.2. UNets for medical image segmentation", "content": "Medical image segmentation has witnessed major advancements, particularly through the use of deep learning models [19, 20]. The UNet architecture has demonstrated remarkable success, providing precise segmentation across diverse imaging modalities such as Ultrasound [21], PET [22], MRI [23] and CT [24]. Its design, characterized by a symmetric encoder-decoder structure and extensive use of skip connections [25, 26], facilitates significantly the segmentation of various organs and tumors such as skin lesions [27], liver and rectal tumors in CT and MRI scans [28, 29], or breast tumors in ultrasound imaging [30]. Enhancements to UNet have also adapted it for volumetric data from MRI and CT, extending its structure to better analyze spatial relationships in 3D space [31, 32, 33], which is essential for organs and vascular segmentation [34]. Numerous reviews continue to discuss these advancements and their contributions to the field [35, 15, 36]."}, {"title": "2.3. Vessel segmentation", "content": "Medical imaging segmentation has also expanded to encompass more complex and subtle structures, such as blood vessels. This task has primarily focused on the segmentation of cerebral and retinal vessels, crucial for diagnosing and monitoring neurological and ocular diseases [37, 38]. This field is broadly categorized into two main areas: 2D and 3D segmentation. Volumetric imaging like Magnetic Resonance Angiography (MRA) allows for constructing detailed 3D vascular tree structures, with recent advances using enhanced UNet architectures [39, 40]. However, given the specific task we aim to address, our research focuses particularly on 2D segmentation. This emphasis on 2D analysis led us to explore domain-specific optimizations that significantly improve segmentation outcomes. Notably, Shit et al. [41] introduced the centerlineDice metric, which measures similarity across segmentation masks and their morphological skeletons, ensuring topological continuity. Furthermore, Zhou et al. [42] has advanced this area by embedding vessel density and fractal dimensions directly into the loss function, enabling refined multi-class vessel segmentation that categorizes pixels into veins, arteries, background or uncertain regions. This novel regularization has also improved biomarker quantification, directly contributing to downstream clinical tasks."}, {"title": "3. Method", "content": null}, {"title": "3.1. fUS acquisition", "content": "For the acquisition of functional ultrasound imaging, we performed experiments on the brain of 35 anesthetized rats. A few days before imaging, the rats underwent a skull-thinning procedure to enhance the signal-to-noise ratio, as described in [4]. The rats were anesthetized using a combination of subcutaneous medetomidine and isoflurane [43]. Imaging was performed using a preclinical ultrasound imaging system (Iconeus V1, Paris, France). Doppler vascular images were obtained using the Ultrafast Compound Doppler Imaging technique [44]. Each frame corresponded to a Compound Plane Wave frame [45] resulting from the coherent summation of backscattered echoes obtained after successive tilted plane waves emissions. Stacks acquired at a 500 Hz frame rate, were processed with a dedicated spatiotemporal filter based on Singular Value Decomposition (SVD) [46] to isolate the blood volume signal from tissue signals, thus generating Power Doppler images. These final images have a temporal resolution of 0.4 s, a spatial resolution of 100 \u00b5m and are directly proportional to the cerebral blood volume.\nIn the initial phase, we conducted a 10-minute resting-state recording. This generated a 2D + time image stack consisting of 3000 frames, each with a spatial resolution of 112 \u00d7 128 pixels (100 microns/pixel). This first stack details the cerebral activity signals associated with hemodynamic changes at rest. Following the initial data collection, a second acquisition was conducted immediately. During this phase, we implemented visual stimulation by maintaining the anaesthetised rat in the dark and exposing it to periodic light flashes, as described in [5]. Stimulation runs consisted of black and white flickering on a screen placed in front of the animal (flickering at a frequency of 3 Hz and continuous black screen for rest). The stimulation pattern consisted in 30 s of initial rest followed by runs of 30 s of flicker and 45 s of rest, repeated four times for a total duration of 330 s. This session was shorter than the first, yielding an image stack of 825 frames at the same spatial resolution and acquisition frequency. This second stack captures therefore cerebral activity linked to hemodynamic changes during visual stimulation."}, {"title": "3.2. ULM acquisition", "content": "Immediately following the initial fUS scans, we conducted an ultrasound localization microscopy session using the same equipment (Iconeus V1). For this, we injected contrast-enhancing micro-bubbles into the bloodstream of the rat via the tail vein and performed imaging over 5 minutes. The raw data collected using a dedicated Iconeus software, based on the methodology described by Errico et al. [13], were then used to construct three ultra-high resolution imaging modalities. These images, illustrated in Figure 1, detail the cerebral vascular tree at a fine scale with different types of information: (A) micro-bubble density, (B) velocity, and (C) axial velocity along the Z-axis. The resolution is 2 microns per pixel (7011 \u00d7 5 494 pixels) for micro-bubble density and 10 microns per pixel (7008 \u00d7 5490 pixels) for velocity. The lower resolution for velocity results from applying a filter to remove outliers, based on the individual tracking of micro-bubbles. The construction of these modalities is performed by the Iconeus software and requires approximately 6 hours of computation per experiment."}, {"title": "3.3. fUS Automatic Annotation", "content": "The above experiments provided us with two stacks of fUS (resting state and visual stimulation), paired with one ULM image for each animal. We used the ULM image for the vascular annotation within the fUS stack, as it provides the necessary information. More specifically, the ULM Z-axis velocity modality is used to automatically distinguish different vascular compartments: in the cortex, positive velocity values indicate the movement of micro-bubbles through arterioles, while negative values reflect their movement through venules. For the sake of simplicity, we use the terms arteries to refer to downward flow compartments and veins to refer to upward flow compartments. This intentional oversimplification applies primarily to the cortical region, where this relationship can be easily made. We acknowledge that it may not apply in other regions. To automatically differentiate these vascular compartments, we applied thresholding to the Z-axis velocity image, as illustrated in Figure 2-A. The images were then resized using bilinear interpolation to match the spatial resolution of the fUS stack. Since our primary focus is on identifying vascular structures rather than quantifying velocity, the resulting images were converted into binary masks. To ensure robust annotations, we set a conservative threshold of 0.05 to exclude ultra-fine structures that fUS is unlikely to detect, thereby producing more accurate and useful vascular masks. These masks served as ground truth annotations for both resting and visual stimulation states, as they represent the anatomical structure of the brain vascular tree and do not contain functional information. It is important to note that resizing ultra-high resolution images introduced a small proportion of mixed pixels belonging to both artery and vein masks. However, since these mixed pixels constituted less than 1%, they were retained to avoid artifacts that might arise from selective pixel exclusion."}, {"title": "3.4. Segmentation framework", "content": "We approached our problem of automatic compartmentalization of fUS imaging as a multi-class segmentation task where each pixel in the fUS image must be classified as vein, artery, or background. To this end, we trained a deep neural network to generate vein and artery masks for each fUS image stack input. These generated masks are then compared to the ground truth derived from ULM to adjust the weights of the network. This pipeline is detailed in Figure 2-B. We opted for a UNet architecture and some of its variants for the segmentation. Additionally, we evaluated various loss strategies to optimize feature and pixel-level classification. Our aim was to determine the most effective model and the most appropriate loss function for accurate fUS image segmentation. Once trained, the model is capable of generating masks based on new fUS images. Those masks are then projected on the fUS stack, enabling the visualization of cerebral blood volume evolution along the temporal axis while distinguishing the concerned compartment, whether it originates from a vein or an artery."}, {"title": "3.4.1. UNet as a backbone architecture", "content": "The UNet architecture, introduced by Ronneberger et al. [47], has become a leading approach in the field of biomedical image segmentation due to its robustness and efficiency. The architecture is fundamentally a convolutional neural network that features a \"U-shaped\" design. It consists of a contracting path (downsampling) to capture context and a symmetric expanding path (upsampling) that enables precise localization. The multiple convolutions, pooling layers, and concatenation steps within the UNet help capture multi-scale contextual information without the need for separate processing pathways. In addition to its specific architecture, the UNet benefits from skip connections that connect layers in the contracting path with corresponding layers in the expanding path. These connections are crucial in recovering spatial information lost during downsampling in order to achieve high accuracy in segmentation tasks. Originally designed to work well with very few training images, the UNet leverages extensive data augmentation techniques such as random rotations and elastic deformations, making it particularly suited for medical imaging scenarios where annotated data can be scarce."}, {"title": "3.4.2. Selected models", "content": "As discussed in Section 2.2, there have been significant advances and enhancements in the UNet architecture. We selected a benchmark reviewed in [15] from their Awesome-U-Net\u00b9 GitHub repository. In addition to the classic UNet architecture [48], four architectures were chosen for their ability to address various complexities encountered in medical image segmentation:\n\u2022 Residual Net [49] combines the strengths of UNet with ResNet, allowing for deeper networks without the risk of vanishing gradients. It uses recurrent convolutions for improved feature representation.\n\u2022 UNet++ [48] introduces nested, dense skip pathways. These additional connections help in better feature propagation across the network, reducing the semantic gap between the feature maps of the encoder and decoder stages.\n\u2022 MultiresNet [50] employs a multi-resolution convolutional approach to handle diverse image scales effectively within the segmentation process. It integrates layers that process multiple scales simultaneously, which helps in capturing both global and local contextual details more effectively.\n\u2022 Attention UNet [51], by incorporating attention gates, selectively emphasizes important features and suppresses irrelevant neuron activation, which is crucial in medical images where the focus area is often surrounded by a large amount of noise.\nThe selected architectures are originally designed to take 2D images as input and to generate 2D segmentation. However, to leverage the temporal richness of our fUS data, we opted to feed the UNet with multiple frames from a stack as many input channels. This approach allowed us to exploit the inter-frame dynamics while preserving the 2D structure of the existing architectures, avoiding the added complexity of adapting the networks. This trade-off ensures optimal capture of spatio-temporal information while maintaining the efficiency of the pre-existing models.\nIn the repository, additional implementation of architectures such as TransUNet [52], UC-TransNet [53], and MissFormer [54] were also provided. While these architectures have demonstrated their effectiveness on medical image segmentation, we did not train them on our data due to their requirement for square images. In our specific case, we already resize ULM images. Introducing further pre-processing by resizing also fUS images could add more biases, making it difficult to ensure the accuracy and reproducibility of the segmentation."}, {"title": "3.4.3. Selected losses", "content": "In this section, S represents the segmentation mask produced by the model from fUS, and T represents the ground truth from ULM. T consists of a ternary mask derived from vein and artery masks. Specifically, when needed, it is expressed as $T_c$ where c \u2208 {a,v,b} \u2013 with a indicating arteries, v indicating veins, and b representing the background. S can be specified in a similar manner when necessary. The dimensions of the image are given by the height h and width w.\nFor the first loss function, we proposed to use a combination of Cross-Entropy and Dice loss. Cross-entropy is effective for pixel-wise segmentation tasks. For each pixel i and class c, the Cross-Entropy Loss is defined as:\n$CE_i(S,T) = - \\sum_{c=1}^{C} T_{ic} log(S_{ic})$                                                                                    (1)\nwhere $T_{ic}$ is the ground truth label for class c at pixel i, represented as a one-hot encoded vector, and $S_{ic}$ is the predicted probability for class c at pixel i. This approach penalizes incorrect classifications at the pixel level. To obtain the final Cross-Entropy term, the $CE_i$ is averaged over all pixels in the image:\n$Cross-Entropy Loss(S, T) = \\frac{1}{P}\\sum_{i=1}^{P} CE_i(S, T)$                                                                     (2)\nwhere P is the total number of pixels in the image.\nDice loss is particularly useful for addressing class imbalances by focusing on the overlap between the predicted and ground truth masks. This is especially important in our case, as we have a significant class imbalance with far more background pixels compared to vein and artery pixels. Specifically, the Dice coefficient for a single class c is defined as follows:\n$Dice(S, T) = \\frac{2 \\sum_i S_{i,c}T_{i,c}}{\\sum_i S_{i,c} + \\sum_i T_{i,c} + \\epsilon}$                                                                      (3)\nwhere $S_{ic}$ is the predicted probability for class c at pixel i, $T_{i,c}$ is the ground truth label for class c at pixel i, and e is a small constant added to avoid division by zero. The Dice Loss for each class is then calculated as:\n$Dice Loss(S, T) = 1 \u2013 Dice(S, T)$                                                                                    (4)\nTo obtain the final Dice term, we compute the Dice Loss for each class and then take the average across all classes C:\n$Dice Loss(S, T) = \\frac{1}{C} \\sum_{c=1}^{C} Dice Loss(S, T)$                                                                                 (5)\nThe final formulation of the combined loss function is given by a weighted sum of the previously detailed terms, as follows:\n$LOSSDICE\\_CE(S,T) = \\alpha Cross-Entropy Loss(S,T) + \\beta Dice Loss(S,T), \\qquad \\alpha,\\beta \\in [0,1]$          (6)\nwhere \u03b1 and \u03b2 are weighting coefficients that balance the contribution of each loss term, and they are not required to sum to 1."}, {"title": "For the next loss functions, in combination with Cross-Entropy, we incorporate a regularization approach introduced in [42]. This approach includes two key terms: vessel density and fractal dimension.", "content": null}, {"title": "The vessel density measures a ratio of vessel area to a whole area as follows:", "content": "$Loss_V(S, T) = \\frac{|\\sum_i S_{i,a} - \\sum_i T_{i,a}|}{h \\times w} +  \\frac{|\\sum_i S_{i,v} - \\sum_i T_{i,v}|}{h \\times w}$ \nIn the above equation, $\\sum_i S_{i,a}$ and $\\sum_i S_{i,v}$ represent the counts of pixels classified as arteries and veins in the segmentation mask S, respectively, while $\\sum_i T_{i,a}$ and $\\sum_i T_{i,v}$ represent the corresponding counts from the ground truth T.\nTo determine the fractal dimension $d_f$ of vessels, we rely on the Minkowski-Bouligand dimension [55], commonly referred to as the box-counting dimension. This measure evaluates the complexity of vessel morphology by covering the image with a grid of boxes of a given size and counting how many boxes contain a part of the vessel structure. As the box size \u03f5 decreases, more boxes are required to cover the vessels, and the scaling behavior $N \\propto \\epsilon^{-d_f}$ with N the number of required boxes, defines the fractal dimension $d_f$. More explicitly, we set the box sizes $\\epsilon = {2^i | i \\in Z, 2 \\leq 2^i \\leq min{h, w}}$ and count the box number $N_S(\\epsilon)$ needed to cover the segmentation mask and $N_T(\\epsilon)$ to cover ground truth mask. The loss is then calculated as:\n$LOSS_B(S, T) = \\sum_i \\frac{1}{\\sqrt{\\epsilon_i}} \\bigg|\\frac{N_T(\\epsilon_i) - N_S(\\epsilon_i)}{N_T(\\epsilon_i)}\\bigg|^2$\n$\\epsilon_i$ was empirically configured to weight more on the error of large-size box.\nTo achieve accurate multi-class segmentation masks, we construct the CF-Loss (Clinically-relevant Feature-optimised Loss) by combining the feature-based loss functions, $Loss_V$ and $Loss_B$, with the pixel-wise Cross-Entropy Loss. Following the approach proposed in [42], we formulate the CF-Loss in three configurations to balance clinical relevance with segmentation accuracy:\n$LOSSCF_B(S,T) = \\alpha Cross-Entropy Loss(S, T) + \\gamma Loss_B(S,T)$          (7)\n$LOSSCF_V(S,T) = \\alpha Cross-Entropy Loss(S, T) + \\beta Loss_V(S,T)$           (8)\n$LOSSCF(S,T) = \\alpha Cross-Entropy Loss(S, T) + \\gamma Loss_B(S,T) + \\beta Loss_V(S,T)$           (9)\nwhere \u03b1, \u03b2 and \u03b3 are the loss weights for Cross-Entropy, vessel density and box count terms respectively. The values used for the weights are given in Section 4.3 below."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Dataset", "content": "As mentioned in the Method Section, our data were derived from experiments on 35 different rats, providing us with 35 fUS stacks in both resting state and under visual stimulation, annotated as described in Section 3.3. No pre-processing was applied to these fUS images To improve the robustness and generalizability of our models, we employed data augmentation techniques, including random horizontal and vertical flips and random rotations. This augmentation was applied in real-time during the training phase, where each frame of the input stack was modified with a random transformation at each training iteration. The corresponding ULM masks were augmented with the same transformations to maintain precise alignment with the fUS frames. To ensure accuracy in rotation, these flips and rotations were performed after resizing. Finally, we conducted a 7-fold cross-validation to ensure the reliability of our results, using data from 30 experiments randomly chosen for training and reserving 5 for testing."}, {"title": "4.2. Evaluation metrics", "content": "To provide a comprehensive evaluation of the segmentation performance, we selected the metrics detailed above:\n$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}, \\qquad Specificity = \\frac{TN}{TN + FP}$\n$Precision = \\frac{TP}{TP + FP}, \\qquad Recall = \\frac{TP}{TP + FN}, \\qquad F1 = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$\nIn this multi-class setting, TP (True Positives), TN (True Negatives), FP (False Positives), and FN (False Negatives) denote the counts for each pixel classification category.\nIn addition, we also used the Jaccard Index, also known as Intersection over Union (IoU). It is calculated for each class c individually, and then averaged to provide an overall score:\n$Jaccard_c = \\frac{T_c \\cap S_c}{T_c \\cup S_c}, \\qquad Jaccard Index = \\frac{1}{c}\\sum_{c=1}^c Jaccard$\nT and S denote the ground truth and segmentation masks respectively, as introduced before."}, {"title": "4.3. Hyperparameter Setup", "content": "For the configuration of the selected deep learning models, we used the hyperparameters recommended in the GitHub repository from which the models were sourced. The only changes made were to the input and output channels; the output was consistently set to three to predict a ternary mask. For the input, as mentioned in Section 3.4.2, we treated the temporal dimension of the fUS stack as an image channel, setting it to 3 000 or 825 depending on the training set (resting state or under visual stimulation respectively). We chose weights for different losses as follows: \u03b1 and \u03b2 at 0.5 in $LOSSDICE\\_CE$, \u03b1 at 1 for all CF losses, \u03b2 and \u03b3 at 1 in $LOSSCF\\_B$ and $LoSSCF\\_V$ respectively, and \u03b3 set at 0.5 for $LossCF$ as per the default settings in the reference paper. After various trials and errors, we settled on using the Adam optimizer with a learning rate of 10\u22123 across 200 epochs for all training sessions."}, {"title": "4.4. Results & Discussion", "content": null}, {"title": "4.4.1. Comparison among models and losses", "content": "In this experiment, we trained the selected models using the selected four loss functions on the resting state dataset, with the aim of identifying the most efficient model and the most appropriate loss function. We reported the average metrics across a 7-fold cross-validation in Table 1, page 13.\nThe highest performance metrics achieved were an accuracy of 89%, an F1 Score of 69%, and a Jaccard Index (IoU) of 0.56, aligning with results typically seen in vessel segmentation [42, 41]. The best results on average are predominantly achieved by the Attention UNet and UNet++, which seem better suited for this segmentation task due to their ability to focus on relevant regions within the image through attention mechanism. The UNet and ResNet performed slightly worse yet remained competitive, whereas the MultiResNet was noticeably less effective. This can be explained by its inability to effectively capture the fine details due to its more complex and less targeted architecture. In terms of loss functions, CF generally outperformed DICE_CE for Attention UNet, UNet++, and UNet, with negligible differences among the CF variants, suggesting that both fractal dimension and vessel density allowed the models to better identify vascular structures and distinguish vessel boundaries, improving segmentation accuracy.\nFor illustration, we presented predictions for six images in Figures 3 and 4 for arteries and veins, respectively. These images were selected to showcase different cerebral vascular structures. The predictions from Attention UNet and UNet++ align well with the performance metrics, closely matching the ground truth in the six examples. However, they struggle slightly with fine arterial structures at the cortical level and horizontal vessels, as seen in example 4. UNet and ResNet predictions respect the overall shape of the six images, but there are many missing structures, indicating that these models may not capture all the intricate details effectively. Finally, the segmentation of MultiresNet is notably imprecise, yet it shows a slightly better performance in segmenting veins, which have a simpler structure.\nIn the above experiment, we used the full fUS stack of 3 000 frames as input for training. We then repeated the experiment, this time using a single image per fUS stack, generated by averaging the 3 000 frames captured at rest. No statistically significant difference in overall performance was observed. Detailed results are provided in the appendix."}, {"title": "4.4.2. Evaluating the Impact of fUS stack depth on segmentation performance", "content": "The primary goal of this experiment was to investigate how the depth of the fUS stack, defined by the number of frames used during training, impacts prediction quality. To address this question, we focused on the Attention UNet, one of the top-performing models from previous experiments, and trained it using CF loss on fUS stacks with varying numbers of frames. This analysis was conducted both at rest and under visual stimulation conditions, given the different signal evolution of cerebral blood volume observed in each state. We maintained the same sampling frequency for the stacks, varying only the number of frames, n, selected from the sets {1, 100, 825, 3000} for the resting state and {1, 100, 825} for visual stimulation. If the number of selected frames, n, was smaller than the total available frames, N (i.e., 3000 for resting state or 825 for visual stimulation), we randomly selected n contiguous frames from the full stack. Specifically, a random index i was drawn from the distribution $i \\sim Uniform(1, N \u2013 n)$ and the $i \\dots i + n$ frames were used. This random selection was performed at every training step, meaning that different frames were sampled from each stack in every epoch of the training process. The results for the F1 score and Jaccard Index are reported in Figure 5.\nThese results show that both metrics - F1 score and Jaccard Index - exhibited consistent trends across training scenarios in resting state and under visual stimulation, suggesting that signal evolution does not significantly affect segmentation quality. In particular, reducing the number of frames remarkably improved these metrics, achieving maximal Jaccard Index of 0.60 and an F1 score above 0.72. A paired Wilcoxon test confirmed these improvements as statistically significant (p-value < 0.05) for the depths 1, 100 or 825 compared to 3000. This indicates that the model becomes easier to train with less parameters. Among the frame sets tested \u2013 1, 100, and 825 - the boxplot using 100 frames ranks highest, particularly under visual stimulation. This suggests that a single frame lacks adequate information and using 825 frames might overly complicate the model, reducing its effectiveness. Thus, it appears that using 100 frames provides sufficient data for accurate segmentation without overburdening the model."}, {"title": "4.4.3. Cross-condition efficacy of fUS-based models: Training on resting state for visual stimulation segmentation", "content": "We found above that brain signal changes captured by fUS during visual stimulation did not significantly influence the quality of segmentation, as similar results were achieved with or without visual stimulation. This led us to explore whether a model trained on fUS data from a resting state could effectively segment images acquired during visual stimulation. To this end, we trained the Attention UNet model on fUS images at rest (with 100 frames) and tested it on images from visually stimulated sessions (also with 100 frames). To ensure a clear separation between training and testing data, we used different groups of rats for each condition: fUS images from resting state sessions were used for training, while only fUS images from visual stimulation sessions of separate rats were used for testing. The goal was to ensure that the model was evaluated on new structural data.\nThe results for various metrics are presented in Table 2, revealing an accuracy of 90%, an F1 score of 71%, and a Jaccard Index of 0.59. These outcomes are noteworthy as they align with previous results. This is particularly interesting because it shows that our model can be effectively trained on shorter fUS stacks in a resting state, and still perform well on data collected during visual stimulation without requiring retraining for the new conditions. This highlights the model applicability for accurately tracking differentiated activity in different vascular compartments."}, {"title": "4.4.4. Visualizing vascular structures in fUS imaging", "content": "In this section, we illustrated that the UNet based inference of masks for veins and arteries (upward and downward compartments respectively) can be effectively used to enhance the interpretation of fUS signals.\nFirstly, for a more visual rendering of our results, we overlaid real and model-predicted masks on an fUS image captured during visual stimulation (Figure 6, left). Arterial and venous signals are distinguished by red and blue colors, respectively, while the background is rendered in grayscale. The color intensity corresponds to the strength of the Power Doppler signal, indicative of Cerebral Blood Volume (CBV). Figure 6 illustrates the outcome of this visualization process. We surrounded regions where the model predictions were relatively accurate in green and areas where it struggled in yellow. It should be noted that the cortex is located at the top of the images. In the cortical region, the predicted mask accurately delineates arteries but does not capture all the veins, although it does detect a fair number of them. This difference in performance may be due to the denser presence of arteries compared to veins in this region. In contrast, the model performs particularly well in the lower regions, where the signal appears more visible, and it effectively identifies both downward and upward compartments.\nThis visualization was complemented by extracting the signal"}]}