{"title": "Combining AI control systems and human decision support via robustness and criticality", "authors": ["Walt Woods", "Alexander Grushin", "Simon Khan", "Alvaro Velasquez"], "abstract": "AI-enabled capabilities are reaching the requisite level of maturity to be deployed in the real world. Yet, the ability of these systems to always make correct or safe decisions is a constant source of criticism and reluctance to use them. One way of addressing these concerns is to leverage AI control systems alongside and in support of human decisions, relying on the AI control system in safe situations while calling on a human co-decider for critical situations. Additionally, by leveraging an AI control system built specifically to assist in joint human/machine decisions, the opportunity naturally arises to then use human interactions to continuously improve the AI control system's accuracy and robustness.\nWe extend a methodology for adversarial explanations (AE) to state-of-the-art reinforcement learning frame- works, including MuZero. Multiple improvements to the base agent architecture are proposed. We demonstrate how this technology has two applications: for intelligent decision tools and to enhance training / learning frameworks. In a decision support context, adversarial explanations help a user make the correct decision by highlighting those contextual factors that would need to change for a different AI-recommended decision. As another benefit of adversarial explanations, we show that the learned AI control system demonstrates robustness against adversarial tampering. Additionally, we supplement AE by introducing strategically similar autoen- coders (SSAs) to help users identify and understand all salient factors being considered by the AI system. In a training / learning framework, this technology can improve both the AI's decisions and explanations through human interaction. Finally, to identify when AI decisions would most benefit from human oversight, we tie this combined system to our prior art on statistically verified analyses of the criticality of decisions at any point in time.", "sections": [{"title": "1. INTRODUCTION", "content": "Reinforcement learning (RL) can now produce artificial intelligence (AI) agents capable of superhuman perfor- mance in a wide range of tasks, as evidenced by the recently developed MuZero\u00b9 and its more sample-efficient successor EfficientZero.\u00b2 Despite these advances, current state-of-the-art AI agents can make very costly mistakes in real-world scenarios. For example, an AI-controlled car might misinterpret its environment for a few seconds, a mistake that has directly caused loss of life. While these technologies are constantly improving, it is not clear when or if they will ever reach a level of autonomy where they can no longer benefit from collaborating with human decision makers, particularly in environments with decisions on a time scale of hours or days. Effectively, we posit that there will always be critical situations where human/machine teaming will result in more effective decisions than decisions made solely by human or AI elements.\nA traditional take on human/machine teaming for decision making considers a spectrum of automation, from zero machine involvement to zero human involvement. The automotive industry has codified this idea for autonomous automobiles, where levels are defined as increasing sets of capability that require less human attention.4 We instead look at providing a spectrum of support for human decisions, from minimal decision assistance to maximal decision assistance. This new framing focuses on using AI techniques in support of the"}, {"title": "2. METHODS", "content": "To assemble our IDT prototype, we developed and combined the methods described below. These techniques were designed specifically to produce a general purpose IDT for helping human decision makers fully leverage the decision-specific knowledge captured by the RL agent. To accomplish this, there were four main thrusts to our work:\n1. General RL extensions for building a flexible agent architecture that could be applied to any kind of reinforcement learning problem, while both retaining good learning performance and revealing as much information about the environment as possible.\n2. Extending adversarial explanation (AE)6 to the RL domain to help users better understand different interpretations of available sensor data that would lead to different decision recommendations.\n3. The creation of strategically similar autoencoders (SSAs) to help users identify which data is critical to understanding the strategic scenario observed by the IDT.\n4. Helping users understand the long-term consequences of a decision through criticality (which is defined informally as the reward that an agent can be expected to lose if it makes a certain number of mistakes in a particular situation), and safety margins (the number of mistakes that an agent can afford to make before the expected reward loss is significant).7,8\nCombining all of these methods resulted in an IDT that could help human users better understand key decision factors and the possible consequences of different decisions in any given situation.\nDue to the number of techniques developed as part of this work, many of the techniques will only be briefly described. Much of the presented work is exploratory; further research would be required to fully determine the empirical advantages of our approaches. Relevant citations are provided that will help the reader better understand those parts of the system that are of interest to them."}, {"title": "2.1 Building a Flexible Agent Architecture", "content": "MuZero is a high-performing RL agent that holds many superhuman performance records.\u00b9 While promising, that particular agent posed problems for some real-world applications for which, even through simulation, it would be prohibitively expensive to gather the needed 200 million or more data points to train the agent. Fortunately, an agent called EfficientZero presented a few improvements and was also able to surpass human capabilities, using only 100,000 data points.\u00b2 While this is still a significant amount of required data, it made MuZero-style agents feasible for helping to make decisions in many more real-world environments.\nAt its core, MuZero and EfficientZero both implement explicit world models (WMs). Unlike the original WM work, which used game state data at time t to predict the game state at t + 1, MuZero-style agents use information at time t to predict only the agent's latent representation of that game state at time t + 1. This prediction is done implicitly, by optimizing the difference in expected immediate and future rewards from such a rollout. This means that the model can simulate future states, but only through its own strategic, latent interpretation of the world. These latent states are also parameterized by the action selected by the model at each time step, at. By combining this WM simulation capability with Monte-Carlo tree search (MCTS), a MuZero-style model can explore the consequences of different sequences of actions across multiple steps in time.\u00b9 While not perfect, the performance of these agents speaks to the method's utility.\nWhile MuZero performs very well, it has a number of limitations. The limitations that we thought were most important to address in pursuit of our IDT, and our explorations into addressing those limitations, are discussed in appendix A. Briefly, the resulting improvements include: a new method for balancing pre-LayerNorm activations; a ranked Gaussian method for distributional RL; loss segment scaling for less sensitive hyperparameters in different environments; the handling of complex action spaces via a pairwise policy update; direct control of the exploration/exploitation trade-off via a pairwise policy update; a particle swarm tree search alternative to MCTS; and a modified MuZero value representation for easier model learning.\nWith these enhancements in hand, the IDT's agents successfully learned to execute tasks in a variety of environments."}, {"title": "2.2 Finding Relevant Decision Boundaries with Adversarial Explanations", "content": "AEs are a technique for exploring a large number of counterfactuals to help better understand any given decision from an AI system. This is accomplished by conditioning neural networks (NNs) to be queried for decision boundaries that are close to a given input. For a full description of AE, see the work which established the technique,6 and applied it to neural networks that were trained in a supervised way.\nIn this work, we extended AE to RL. As discussed in section 2.1, this was done in the context of a MuZero- style agent, meaning that we also wanted explanations to work in the context of the dynamics function. Adapting the conditioning step of AE worked as follows: as in prior work, one parameter was chosen for each example in the training batch, using a random rollout time \u03c4, and a random choice from the available Q and policy network outputs. The gradient of this parameter with respect to the inputs was computed, and some function of that gradient was added to the overall loss function, as in previous approaches.6\nMultiple additions to the basic approach helped optimize performance in an RL setting. These were gradient scale correction, optimizer scale correction, a different approach for controlling the strength of the regularization, and a sparsity-promoting gradient-minimization function. Details of these methods may be found in appendix B.\nTogether, these improvements successfully allowed AE to be applied to RL."}, {"title": "2.3 Identifying Critical Data with Strategically Similar Autoencoders", "content": "In the original paper on WMs, Ha and Schmidhuber used autoencoders to compress the agent's sensors into a latent representation that could be re-expanded to the original sensor representation. This classic use of autoencoders is a great way for identifying the most visually significant varying factors in sensory space, and compressing them. However, qualities of the input are retained based on visual similarity instead of their use in the task being handled by the AI system. We note that MuZero's adaptation of the WM work stopped propagat- ing this explicitly reconstructable representation, instead focusing on propagating enough state to predict action probabilities and corresponding reward values.\u00b9 From an agent efficacy point of view, this is advantageous, as exactly reconstructing the sensor inputs is irrelevant to task performance.\nFor an IDT whose goal is to help human/machine teams make good decisions, reconstructing some version of the sensor input might be very useful. Specifically, reconstructing the parts of the input that the AI con- siders strategically important to any potential decisions would allow for a human to better understand which environmental factors the AI considers salient, and which ones it ignores. To address this, we developed SSAs.\nThe core insight for SSAs is that an autoencoder does not necessarily need to reconstruct the input. Instead, the same architecture may be used to reconstruct the latent decision space that MuZero uses to make decisions."}, {"title": "2.4 Flagging Long-Term Consequences with Criticality and Safety Margins", "content": "From an IDT point of view, WM-style simulation provides the ability to explore the potential long-term effects of different decisions. The SSA allows these simulated counterfactuals to be cast back into a sensory space, allowing for easier understanding of which strategic elements are being tracked. AEs allow for such sensory space interpretations to be modified, to better understand how minor initial differences in the scenario might lead to wildly different long-term outcomes. However, while these tools allow for the investigation of long-term"}, {"title": "3. RESULTS", "content": "We focus on qualitative results, walking through a limited number of examples from the Atari game Beamrider. Additional non-public examples exist that apply these ideas to, e.g., scenarios in the AFSIM simulation frame- work. We encourage prospective partners who are interested in using IDTs for human/machine decision teaming to contact us."}, {"title": "3.1 Model Criticality", "content": "Criticality requires one measurement that is dependent on the model: proxy criticality.7,8 For our modified MuZero-style agents, we use  E[reward|policy] \u2013 E[worst reward|1 wrong step], i.e., the difference between the"}, {"title": "3.6 Model Robustness", "content": "As with traditional AEs, the RL-enabled AEs presented in this work provide protection against adversarial input manipulations. To get reasonable RL agent performance, we used lower regularization strength than in the original AE paper. While we have not yet measured quantities comparable to the original work's accuracy- robustness area (ARA) metric, we did measure the root-mean-squared error (RMSE) of the perturbation (the difference between the perturbed input and the original input) to achieve a given change in policy for both a plain network and one treated with AE methodology. In our experiments, the modified network had roughly identical performance to the original network (9135 vs 9274 average episode performance, respectively), while the required RMSE was 1.81\u00d7 higher for the modified network. This shows that the modified network should be significantly more resistant to adversarial tampering, also implying that the ARA would be significantly higher, an exercise we leave for future work."}, {"title": "4. DISCUSSION AND FUTURE WORK", "content": "The provided capabilities should be a significant aid for humans trying to debug the AI system's decisions or understand them in the context of a human/machine teaming scenario. We note that this work covered qualitative, case-study results only; specific human trials for quantitative results are left for future work.\nAs with classification-based AE, the resulting explanations could also be passed to human subject matter experts for labeling, which in turn could improve the AI model's robustness and decision making capabilities. 6 This is a promising future direction for this work, which would allow human feedback to improve the AI's decisions and explanations."}, {"title": "5. RELATED WORK", "content": "Outside of RL, a comparison of AE to other state-of-the-art explanation methods has been presented in previous work. For context within the field of explainable reinforcement learning (XRL), we refer to a few more recent"}, {"title": "6. CONCLUSION", "content": "As both the quantity of data available to decision makers and the complexity of decision environments increases, tools are needed to help humans meaningfully understand those environments and make effective decisions. Traditional decision assistance tools - DSTs - have focused on using data to help users answer questions they already know to ask. Instead, we proposed IDTs that use AI methods to help users find the answer to a much broader set of questions, relying less on user knowledge and giving a broader perspective on the available data. These capabilities were provided through: RL extensions for a MuZero-style agent for better learning and to reveal information about the environment to users; the extension of AE to RL to help users better understand different interpretations of available sensor data that would lead to different decisions; SSAs to help users identify which data the AI considers critical to understanding the current strategic scenario; and criticality and safety margins to help users understand the long-term consequences of different decisions. By combining all of these innovations, IDTs become a viable approach for joint human/machine decision making, and in particular, help to uncover unknown known/unknown factors. By focusing on not just AI methods that autonomously make good decisions, but also in using those methods to expose available information to human decision makers, a platform is provided for making better decisions for critical missions."}, {"title": "APPENDIX A. RL AGENT DIFFERENCES FROM MUZERO", "content": "In this section are the modifications to MuZero that were implemented and explored as part of this work."}, {"title": "A.1 Balancing Pre-LayerNorm Activations", "content": "Layer normalization, or LayerNorm,17 is a way of normalizing the activity of different neurons, to ensure that any resulting activations are well-conditioned for learning. LayerNorm has a number of benefits, including similar training and test time behavior. However, in some contexts, LayerNorm does not perform as well as batch normalization (or BatchNorm).18 One theory for this that we explored is that there is no function forcing each data channel to actually be used. That is, LayerNorm enforces statistics over all data channels, whereas BatchNorm enforces statistics over each data channel independently. Other work has explored forcing the pre-LayerNorm activations of a NN to ensure that each channel gets used via Wasserstein normalization.19 However, we found that constraint to be too restrictive and difficult to balance; other optimization objectives were negatively impacted by the condition.\nInstead, we take the idea of using gradients to condition the pre-LayerNorm activations19 and use that idea to implement a variant of BatchNorm. To implement this, the forward pass is identical to LayerNorm, but during the backward pass, gradients pre-LayerNorm are modified. This is accomplished by swapping the channels (features) dimension with the batch dimension (assuming batch, channel, height, width ordering for images) and then computing statistics across the newly located batch dimension and all subsequent dimensions. Notably, at this step, we compute dsd as the standard deviation of each channel, and then multiply all of these by a scalar such that the new variance across all channels would be 1. We found this osa normalization to be important, as otherwise, the use of this method in, e.g., transformers, resulted in saturation of the attention layers. Finally, the rank of each value is computed within its statistics group (along the batch and spatial dimensions), and these ranks are converted to locations on a Gaussian via the inverse error function:\n\\(target = dsd\\sqrt{2}erf^{-1}\\left [\\frac{2R}{1 + N} - 1 \\right ]\\),\nwhere R is the rank and N is the number of samples in the statistics group. Finally, the gradient is modified such that all values are pulled toward their corresponding target by a constant (we used 1e \u2212 2) times the distance to target.\nThe result, loosely named LayerNormRebalanced, ensures that each feature channel is fully used across different inputs, while also retaining train/test computational similarity and not saturating attention layers affected by the change in gradient."}, {"title": "A.2 Gaussian Mixture Models for Distributional RL", "content": "MuZero only tracked expectations of rewards. For environments with multiple distinct reward states (e.g., a player ship destroying a target, merely surviving, or itself being destroyed), it can be helpful for a human co-decider to understand the risks of these different reward states. To capture this information, we turn to distributional RL methods.\nBriefly, distributional RL involves capturing the entire reward distribution, instead of just an expectation. The original implementation that the authors are aware of was by Bellemare et al., and used point masses to approximate the distribution over a fixed range. 20 That technique has since been expanded with Implicit Quantile Networks21 and Gaussian mixture model (GMM) approaches. 22, 23\nWe appreciated the computational simplicity and representational power of the GMM approaches, but found difficulties with the optimization of those representations. 22, 23 This tended to manifest in environments with large reward values - e.g., a reasonably skilled Beamrider agent might have an expected total discounted reward range of [0,800], whereas a Pong agent might only have a range of [-2,2]. Instead, we found success with a novel, rank-based GMM update rule. Briefly, our representation and update rules are as follows.\nFor each distribution to be captured, decide a fixed number N gaussian of Gaussians to track, and output 3N gaussian parameters from the corresponding value/reward network for the mean, standard deviation, and log"}, {"title": "A.3 Loss Segment Scaling", "content": "For maximum diversity in different environments, and to increase interpretability for the user, we opted to keep reward units in the environment's scale (e.g., the number of points in a game), without clipping or other transformations. In a similar spirit as the gradient corrections from appendix A.2, we found that environments with large reward ranges needed additional loss tweaking from the standard MuZero format to support e.g. the SimSiam loss from EfficientZero.\u00b2 This is a direct result of the gradients for the value/reward/action networks scaling with the observed reward magnitudes, while other gradients do not scale with those quantities. The best solution we found for this was to estimate the L1 or L2 distance between actions chosen uniformly at random. This essentially is a measure of how sensitive the reward space was to different action choices, a quantity that roughly scales with the loss magnitudes used by the core MuZero mechanics; by dividing any reward-space updates (such as value/reward network updates) by this quantity, we found that the other network hyperparameters generalized across multiple environments."}, {"title": "A.4 Complex Action Spaces via a Pairwise Policy Update", "content": "The original MuZero paper provided policy update and policy search methods that could only handle discrete action spaces. While an extension was developed for complex action spaces,24 we felt that that approach had a number of potential deficiencies. First, their inner policy update still uses visit counts. What happens when very similar actions (due to, e.g., a continuous action space) are visited twice as a result of the outer random sampling? Then, each has its own visit count from the MCTS process, despite being virtually identical, biasing toward over-sampling (and thus preferring) those values. This can be corrected via techniques such as importance sampling,25 but we wondered if there is a more straightforward way to produce policy updates.\nDrawing inspiration from prior work on RL for grammatical inference (RL-GRIT),26 we leveraged a pairwise policy update approach. This has a number of potential advantages: (1) as with appendix A.2, loss scales according to changes in expected reward, (2) it works the same for any kind of action space, and (3) as discussed in appendix A.6, it lends itself very well to an alternative of MCTS that is faster to compute while conferring similar benefits.\nGenerally, the pairwise policy update works as follows: sample N update potential actions from the cur- rent action distribution, and for each i \u2208 1..N update compute both log P i,update, the log probability of select- ing that action, and Q i,update, the expected Q value for taking that action. Then, compute dlog P i,update = \\( \\frac{1}{N_{update}} \\sum_{j=1}^{N_{update}} Q_{j,update} - Q_{i,update}\\)\nThis formulation of the pairwise policy update has a few interesting properties. Gradients added to each log probability scale with P(1 \u2013 P)|\u2206Q|, meaning both that updates are rare for saturated actions, and that the scale of these updates is proportional to the expected difference in reward for making such a policy update. Additionally, because this is a symmetric loss, two actions with stochastic Q update values that have the same expectation will stay at exactly the same relative probability to one another. Since we expect the reward and value networks of a MuZero-style agent to be noisy (via optimization, not explicitly as in RainbowDQN 27), we find this property to be advantageous.\nThe downside to this update rule is that it gives probability mass to actions that already have probability mass, and thus does not do a great job on its own with the exploration/exploitation trade-off. We address this below."}, {"title": "A.5 Directly Controlling the Exploration/Exploitation Trade-Off via a Pairwise Policy Update", "content": "The exploration/exploitation trade-off is a classic concern with RL agents.28 Essentially, the agent does not begin its learning process understanding the world, and thus a certain amount of exploration is required. However, as the agent learns those world dynamics, it must shift to exploitation to achieve a high score. Often, in doing so, it reaches increasingly complex world states, which again necessitates some level of exploration until those new states are more fully learned. This teeter-totter is difficult to balance, with a number of successful approaches like Random Network Distillation29 and the more recent Generalized Data Distribution Iteration (GDI)28 tackling this through novelty rewards and/or bandit optimization.\nDue to our pairwise policy update, we explored methods of handling this trade-off while also preventing excessive saturation of log probabilities. The final approach works as follows: N update,uniform actions are sampled uniformly at random from the entire action space, and corresponding log P i,update,uniform values are computed. Then, the probability mass is given from the policy samples to these uniform samples. Importantly, the magnitude of this transfer is controlled by an integrating controller such that the frequency of policy samples associated with a sampling probability that is less than that of a uniform sampling probability is equal to P target = 0.1. For easily testing different P target values, we scale the speed of the integrator update by 0.5/p target.\nIntuitively, this pushes states with little difference in Q values between actions toward a uniform distribution, while allowing those states with significant value differences to saturate. We found that this setup is very flex- ible, with the same hyperparameters working well in multiple environments. While the exploration/exploitation trade-off would likely perform better by using additional techniques like GDI,28 we found its performance in isolation to be satisfactory for many use cases."}, {"title": "A.6 Particle Swarm Tree Search", "content": "MCTS, used by MuZero, works well but depends heavily on tracking visit counts. In complex action spaces, this presents issues, as noted in appendix A.4. Furthermore, MCTS is necessarily iterative in nature, and cannot benefit from the full parallelization offered by modern graphics processing units (GPUs).\nTo work around this, we devised a novel approach inspired by particle filter30 and particle swarm optimiza- tion31 approaches.\nTo begin, N ps particles are sampled, with each particle following the agent's learned policy. Each of these particles are simulated for T ps time steps, at each step again following the learned policy from the newly com- puted world state according to the dynamics function. Observed immediate rewards and future Q values are accumulated as in MuZero's implementation of MCTS. This yields a set of particles which have both an action sequence and an expected Q value for following that action sequence from the current point in time.\nHaving collected N ps particles, we have N ps potential action trajectories and Q values. Like the original MCTS implementation, we need a means of determining an action distribution that combines both the estimated Q value and the likelihood of each action into an upper confidence bound. Unfortunately, in a similar issue to that mentioned in appendix A.4, we do not have access to the frequencies of combined samples. That is, while we can use the log probabilities of each action sample, those samples will also come from that same distribution, meaning that direct use of the probability information has a sort of doubling effect. Instead, we can use a simple trick: if we add uniform noise, scaled by c 1 (max ps Q \u2013 min ps Q) over all particles, to each particle's estimated Q value, and then take the top N ps << N ps of particles sorted by the modified Q values, then we have a distribution captured from the confidence bound. Here, c 1 = 0.2 is a constant determining the size of this effect. By adding uniform random noise, action samples with a larger sample probability are also more likely to receive a larger random sample. Thus, the probability of each sample is implicitly rolled into the newly sampled distribution, creating an altered action distribution in the same style as the MCTS approach.\nThis approach is certainly faster than MCTS (with a speedup factor of approximately 10), and in our limited trials, equally effective."}, {"title": "A.7 Modified MuZero Value Representation", "content": "Our final modification to the MuZero family of RL agents is to always have the Q network's result include the reward network's result for a single dynamics step. While our interpretation of MuZero is that it always has the Q network predict future rewards after the dynamics step, we found this much more difficult to train. The most likely explanation for this is that the dynamics function begins as nonsense, and when learning the initial WM, it is easiest to learn a Q function that does not fully rely on an accurate dynamics model."}, {"title": "APPENDIX B. ADDITIONS TO ADVERSARIAL EXPLANATIONS", "content": "As mentioned in the main text, several changes to AE were required to achieve satisfactory performance with RL. These are described below."}, {"title": "B.1 Gradient Scale Correction", "content": "Gradient scale correction required two considerations: first, the massive difference in scale for outputs associated with rewards and action probabilities, and second, considerations for the magnitude of the second derivative required for AE conditioning.\nTo handle the difference in output scales, we note that the input to any layer (including the final layer) has roughly standard normal statistics, as a result of the LayerNorms in the network. For large magnitude outputs, such as reward, this means that the final layer's weights are rather large. However, as mentioned earlier, this means that any gradients propagated backwards from this final layer would also be magnified, conflating the importance of different factors within the network. To fix this, we divide the gradient propagated backward from these final output layers by the expected standard deviation from the layer's weight matrix (the square root of the sum of squares). This expected standard deviation can be grouped over multiple output channels where they"}, {"title": "B.2 Optimizer Scale Correction", "content": "Our experiments exclusively use the Adam optimizer for its fast convergence properties.32 However, when used in combination with AE for RL, we noticed that learning was often unstable, with the occasional massive exploding gradient. To fix this, we used a variant of Adam that was step-size aware; that is, in the \"Update Parameters\" step, if \\(|\\frac{m_t}{(\\sqrt{v_t + \\epsilon})} | > (C_{maxstep} = 100)\\) for any parameter update, then all parameter updates were scaled down so that the max step size was 100 (which is then multiplied by the learning rate). This solved such instabilities, and retained good optimizer quality."}, {"title": "B.3 Controlling Regularization Strength", "content": "The original AE work controlled the AE regularization loss strength by targeting a given training loss. We did not find this to be effective for RL, as training loss is unpredictable and varies significantly by environment. One approach would have been to target an average agent reward, but that is a complex function of both the environment and the agent regularization itself. Instead, we target an expected L1 magnitude of the gradient for the chosen action's log probability with respect to the inputs. This effectively bounds the Lipschitz constraint from AE to enforce a given rate of change, which we found to be both easy to adjust and surprisingly consistent across environments."}, {"title": "B.4 Sparsity-Promoting Gradient-Minimization Function", "content": "We used z = 1.25 for the  L adv,z,q  function from Section III-B of the original AE paper. 6 This encouraged sparser gradients, which was convenient for human interpretation in many of the environments we tried. During evaluation (not training), we also found it useful to adjust the manner in which AEs were generated. By adding an additional L1 loss term back toward the original image, the explanations were even sparser, allowing for easier interpretation."}, {"title": "APPENDIX C. ADDITIONAL FIGURES", "content": ""}]}