{"title": "Combining AI control systems and human decision support\nvia robustness and criticality", "authors": ["Walt Woods", "Alexander Grushin", "Simon Khan", "Alvaro Velasquez"], "abstract": "AI-enabled capabilities are reaching the requisite level of maturity to be deployed in the real world. Yet, the\nability of these systems to always make correct or safe decisions is a constant source of criticism and reluctance\nto use them. One way of addressing these concerns is to leverage AI control systems alongside and in support of\nhuman decisions, relying on the AI control system in safe situations while calling on a human co-decider for critical\nsituations. Additionally, by leveraging an AI control system built specifically to assist in joint human/machine\ndecisions, the opportunity naturally arises to then use human interactions to continuously improve the AI control\nsystem's accuracy and robustness.\nWe extend a methodology for adversarial explanations (AE) to state-of-the-art reinforcement learning frame-\nworks, including MuZero. Multiple improvements to the base agent architecture are proposed. We demonstrate\nhow this technology has two applications: for intelligent decision tools and to enhance training / learning\nframeworks. In a decision support context, adversarial explanations help a user make the correct decision by\nhighlighting those contextual factors that would need to change for a different AI-recommended decision. As\nanother benefit of adversarial explanations, we show that the learned AI control system demonstrates robustness\nagainst adversarial tampering. Additionally, we supplement AE by introducing strategically similar autoen-\ncoders (SSAs) to help users identify and understand all salient factors being considered by the AI system. In\na training / learning framework, this technology can improve both the AI's decisions and explanations through\nhuman interaction. Finally, to identify when AI decisions would most benefit from human oversight, we tie this\ncombined system to our prior art on statistically verified analyses of the criticality of decisions at any point in\ntime.", "sections": [{"title": "1. INTRODUCTION", "content": "Reinforcement learning (RL) can now produce artificial intelligence (AI) agents capable of superhuman perfor-\nmance in a wide range of tasks, as evidenced by the recently developed MuZero\u00b9 and its more sample-efficient\nsuccessor EfficientZero.2 Despite these advances, current state-of-the-art AI agents can make very costly mistakes\nin real-world scenarios. For example, an AI-controlled car might misinterpret its environment for a few seconds,\na mistake that has directly caused loss of life. While these technologies are constantly improving, it is not clear\nwhen or if they will ever reach a level of autonomy where they can no longer benefit from collaborating with\nhuman decision makers, particularly in environments with decisions on a time scale of hours or days. Effectively,\nwe posit that there will always be critical situations where human/machine teaming will result in more effective\ndecisions than decisions made solely by human or AI elements.\nA traditional take on human/machine teaming for decision making considers a spectrum of automation,\nfrom zero machine involvement to zero human involvement. The automotive industry has codified this idea\nfor autonomous automobiles, where levels are defined as increasing sets of capability that require less human\nattention.4 We instead look at providing a spectrum of support for human decisions, from minimal decision\nassistance to maximal decision assistance. This new framing focuses on using AI techniques in support of the\nbest overall decisions, regardless of the autonomy capabilities of the AI system in isolation; the underlying goal\nis to maximally empower human decision making, rather than to automate the system as much as possible.\nMaking good decisions relies, in part, on a good understanding of all available data. This is sometimes called\n\"evidence-based\" decision making. Decision support tools (DSTs) are an existing class of tool where a user can\nwork on justifying any particular decision by asking specific queries of available data. The answers to those\nqueries can in turn provoke additional questions from the user, and the user can repeat this data interaction\nloop indefinitely, until they are satisfied that they understand the situation enough to make a well-informed\ndecision. This approach constitutes a relatively minimal level of decision assistance - it is purely driven by the\nuser, meaning that the user can only explore questions that they know to ask. Colloquially, this means that\nDSTs help with known knowns and known unknowns, rather than unknown known/unknown factors. Such an\napproach has clear limitations when it comes to impactful decision making; notably, there are limitations to the\nidea of \"evidence-based\" decisions that result from complex environment interactions and unforeseeable events. 5\nStill, with increasing computing power and availability of high-fidelity simulations, new possibilities arise for\nproviding decision makers with tools that offer much more comprehensive decision assistance.\nWe propose a new class of tools leveraging RL agents that have been trained to make decisions based on\navailable data, and then explaining them through explainable artificial intelligence (XAI) and related techniques.\nThis approach results in tools that can consider complex environment interactions that might result from decisions\nmade, as opposed to purely data-centric tools like DSTs. The proposed tools can function as a fully autonomous\ndecision system if desired, but importantly, focus on using those decision capabilities to quickly inform human\ndecision-making partners about a situation's full context. This results in a framework that supports both urgent\nand slow decisions, and can even identify when a human's input in a decision is likely to make a significant\ndifference in mission outcomes. We call this new class of tools intelligent decision tools (IDTs).\nIn this work, we describe methods and techniques for building IDTs by leveraging prior work6\u20138 and developing\nnew techniques to produce highly detailed and accurate XAI for state-of-the-art, MuZero-style RL agents. An\nagent training and analysis framework was developed, providing a suite of tools to assist human decision makers\nin rapidly understanding a scenario and making decisions that fully leverage available data, while also supporting\nfull automation. A brief illustration of the capabilities provided by this system is shown in fig. 1, with additional\nsupporting examples in section 3."}, {"title": "2. METHODS", "content": "To assemble our IDT prototype, we developed and combined the methods described below. These techniques\nwere designed specifically to produce a general purpose IDT for helping human decision makers fully leverage\nthe decision-specific knowledge captured by the RL agent. To accomplish this, there were four main thrusts to\nour work:\n1.  General RL extensions for building a flexible agent architecture that could be applied to any kind of\nreinforcement learning problem, while both retaining good learning performance and revealing as much\ninformation about the environment as possible.\n2.  Extending adversarial explanation (AE)6 to the RL domain to help users better understand different\ninterpretations of available sensor data that would lead to different decision recommendations.\n3.  The creation of strategically similar autoencoders (SSAs) to help users identify which data is critical to\nunderstanding the strategic scenario observed by the IDT.\n4.  Helping users understand the long-term consequences of a decision through criticality (which is defined\ninformally as the reward that an agent can be expected to lose if it makes a certain number of mistakes\nin a particular situation), and safety margins (the number of mistakes that an agent can afford to make\nbefore the expected reward loss is significant).7,8\nCombining all of these methods resulted in an IDT that could help human users better understand key\ndecision factors and the possible consequences of different decisions in any given situation.\nDue to the number of techniques developed as part of this work, many of the techniques will only be briefly\ndescribed. Much of the presented work is exploratory; further research would be required to fully determine\nthe empirical advantages of our approaches. Relevant citations are provided that will help the reader better\nunderstand those parts of the system that are of interest to them."}, {"title": "2.1 Building a Flexible Agent Architecture", "content": "MuZero is a high-performing RL agent that holds many superhuman performance records.\u00b9 While promising,\nthat particular agent posed problems for some real-world applications for which, even through simulation, it\nwould be prohibitively expensive to gather the needed 200 million or more data points to train the agent.\nFortunately, an agent called EfficientZero presented a few improvements and was also able to surpass human\ncapabilities, using only 100,000 data points.2 While this is still a significant amount of required data, it made\nMuZero-style agents feasible for helping to make decisions in many more real-world environments.\nAt its core, MuZero and EfficientZero both implement explicit world models (WMs). Unlike the original\nWM work, which used game state data at time t to predict the game state at t + 1, MuZero-style agents use\ninformation at time t to predict only the agent's latent representation of that game state at time t + 1. This\nprediction is done implicitly, by optimizing the difference in expected immediate and future rewards from such\na rollout. This means that the model can simulate future states, but only through its own strategic, latent\ninterpretation of the world. These latent states are also parameterized by the action selected by the model\nat each time step, at. By combining this WM simulation capability with Monte-Carlo tree search (MCTS), a\nMuZero-style model can explore the consequences of different sequences of actions across multiple steps in time.1\nWhile not perfect, the performance of these agents speaks to the method's utility.\nWhile MuZero performs very well, it has a number of limitations. The limitations that we thought were most\nimportant to address in pursuit of our IDT, and our explorations into addressing those limitations, are discussed in\nappendix A. Briefly, the resulting improvements include: a new method for balancing pre-LayerNorm activations;\na ranked Gaussian method for distributional RL; loss segment scaling for less sensitive hyperparameters in\ndifferent environments; the handling of complex action spaces via a pairwise policy update; direct control of\nthe exploration/exploitation trade-off via a pairwise policy update; a particle swarm tree search alternative to\nMCTS; and a modified MuZero value representation for easier model learning.\nWith these enhancements in hand, the IDT's agents successfully learned to execute tasks in a variety of\nenvironments."}, {"title": "2.2 Finding Relevant Decision Boundaries with Adversarial Explanations", "content": "AEs are a technique for exploring a large number of counterfactuals to help better understand any given decision\nfrom an AI system. This is accomplished by conditioning neural networks (NNs) to be queried for decision\nboundaries that are close to a given input. For a full description of AE, see the work which established the\ntechnique,6 and applied it to neural networks that were trained in a supervised way.\nIn this work, we extended AE to RL. As discussed in section 2.1, this was done in the context of a MuZero-\nstyle agent, meaning that we also wanted explanations to work in the context of the dynamics function. Adapting\nthe conditioning step of AE worked as follows: as in prior work, one parameter was chosen for each example in\nthe training batch, using a random rollout time \u03c4, and a random choice from the available Q and policy network\noutputs. The gradient of this parameter with respect to the inputs was computed, and some function of that\ngradient was added to the overall loss function, as in previous approaches.6\nMultiple additions to the basic approach helped optimize performance in an RL setting. These were gradient\nscale correction, optimizer scale correction, a different approach for controlling the strength of the regularization,\nand a sparsity-promoting gradient-minimization function. Details of these methods may be found in appendix B.\nTogether, these improvements successfully allowed AE to be applied to RL."}, {"title": "2.3 Identifying Critical Data with Strategically Similar Autoencoders", "content": "In the original paper on WMs, Ha and Schmidhuber used autoencoders to compress the agent's sensors into\na latent representation that could be re-expanded to the original sensor representation. This classic use of\nautoencoders is a great way for identifying the most visually significant varying factors in sensory space, and\ncompressing them. However, qualities of the input are retained based on visual similarity instead of their use in\nthe task being handled by the AI system. We note that MuZero's adaptation of the WM work stopped propagat-\ning this explicitly reconstructable representation, instead focusing on propagating enough state to predict action\nprobabilities and corresponding reward values.\u00b9 From an agent efficacy point of view, this is advantageous, as\nexactly reconstructing the sensor inputs is irrelevant to task performance.\nFor an IDT whose goal is to help human/machine teams make good decisions, reconstructing some version\nof the sensor input might be very useful. Specifically, reconstructing the parts of the input that the AI con-\nsiders strategically important to any potential decisions would allow for a human to better understand which\nenvironmental factors the AI considers salient, and which ones it ignores. To address this, we developed SSAs.\nThe core insight for SSAs is that an autoencoder does not necessarily need to reconstruct the input. Instead,\nthe same architecture may be used to reconstruct the latent decision space that MuZero uses to make decisions.\nThat is, rather than using the classic autoencoder setup of inputs \u2192 latent \u2192 inputs, we instead follow latent\n\u2192 inputs \u2192 latent. This problem is fundamentally underconstrained; that is, by design, multiple inputs often\nmap to a single latent code, as the differences between those inputs are not relevant to the RL agent's task.\nImportantly, this function can be completely detached from the main RL agent's learning process, giving us\ninsight into the model without the need to affect the model's learning.\nTo implement this, we combined ideas from StarGAN, 10 WassersteinGAN, 11 and VEEGAN. 12 Briefly, the\nstandard generator/discriminator relationship is trained as normal. However, using VEEGAN, we concatenate\nadditional values to the latent code specifically for capturing variances not represented by the latent code. These\nvariances are trained to not be useful in real/fake determination by making real examples look fake (similar\nto VEEGAN, 12 but we found it easier to optimize this objective when using predicted variance for both real\nand fake examples). To ensure that these variances capture significant visual variety within the input, the style\ndiversification loss from StarGAN is included.10 With the latent code being included as input to VEEGAN,\nwe noticed that VEEGAN could have a difficult time learning useful variances with the variance vector, mostly\nrelying on the latent code. While the style diversification loss helps with this, we also introduced a version of\n$L2$ loss that supports Brownian motion of $L2$-style predictions (by default, $L2$ predicts the mean, which for the\nvariance vector would be all zeros). We used a simple bias term to translate between $L1$- and $L2$-style loss, as\n$\\frac{\\beta(a - b)^2}{\\beta^2 + d(a - b)^2}$, with d being the detachment operator. Finally, this loss was divided at each point by the likelihood\nof the sampled fake variance, allowing for the definition of each variance number to drift when those variances\ncannot effectively be predicted, but also allowing the network to learn to reconstruct specific variance numbers\nonce they can be predicted.\nThe result is that the strategic scenario - and variations of that scenario - can be reconstructed for the user\nwithin the IDT. These reconstructions allow the user to explicitly see which environmental factors are recognized\nas potentially strategically significant by the RL agent. This is illustrated in fig. 3."}, {"title": "2.4 Flagging Long-Term Consequences with Criticality and Safety Margins", "content": "From an IDT point of view, WM-style simulation provides the ability to explore the potential long-term effects\nof different decisions. The SSA allows these simulated counterfactuals to be cast back into a sensory space,\nallowing for easier understanding of which strategic elements are being tracked. AEs allow for such sensory\nspace interpretations to be modified, to better understand how minor initial differences in the scenario might\nlead to wildly different long-term outcomes. However, while these tools allow for the investigation of long-term\nconsequences through the agent's WM, such an investigation is often expensive in terms of the human decision\npartner's time.\nTo help focus human decision maker time on only those decisions that are truly critical, we propose using\ncriticality and safety margins, which we derive using recently published techniques. 7,8 These approaches use\nstatistics to derive bounds for any decision the agent must make. Essentially, safety margin bounds say that an\nagent might make up to $N_{safety}$ random (potentially erroneous) actions before there is a reasonable chance that\ntask performance might be negatively affected beyond some user-defined threshold.\nPrior work shows that, in the Atari game Beamrider for example, 47% of agent losses could potentially be\nprevented by looking at only the lowest 5% of safety margins. 7,8 In other words, when a human decision maker's\ntime is limited, safety margins allow them to spend more time looking at a smaller number of critical decisions."}, {"title": "3. RESULTS", "content": "We focus on qualitative results, walking through a limited number of examples from the Atari game Beamrider.\nAdditional non-public examples exist that apply these ideas to, e.g., scenarios in the AFSIM simulation frame-\nwork. We encourage prospective partners who are interested in using IDTs for human/machine decision teaming\nto contact us."}, {"title": "3.1 Model Criticality", "content": "Criticality requires one measurement that is dependent on the model: proxy criticality. 7,8 For our modified\nMuZero-style agents, we use $E[reward_{policy}] \u2013 E[worst\\ reward|1 \\ wrong\\ step]$, i.e., the difference between the"}, {"title": "3.6 Model Robustness", "content": "As with traditional AEs, the RL-enabled AEs presented in this work provide protection against adversarial\ninput manipulations. To get reasonable RL agent performance, we used lower regularization strength than in\nthe original AE paper. While we have not yet measured quantities comparable to the original work's accuracy-\nrobustness area (ARA) metric, we did measure the root-mean-squared error (RMSE) of the perturbation (the\ndifference between the perturbed input and the original input) to achieve a given change in policy for both a\nplain network and one treated with AE methodology. In our experiments, the modified network had roughly\nidentical performance to the original network (9135 vs 9274 average episode performance, respectively), while\nthe required RMSE was 1.81\u00d7 higher for the modified network. This shows that the modified network should be\nsignificantly more resistant to adversarial tampering, also implying that the ARA would be significantly higher,\nan exercise we leave for future work."}, {"title": "4. DISCUSSION AND FUTURE WORK", "content": "The provided capabilities should be a significant aid for humans trying to debug the AI system's decisions\nor understand them in the context of a human/machine teaming scenario. We note that this work covered\nqualitative, case-study results only; specific human trials for quantitative results are left for future work.\nAs with classification-based AE, the resulting explanations could also be passed to human subject matter\nexperts for labeling, which in turn could improve the AI model's robustness and decision making capabilities. 6\nThis is a promising future direction for this work, which would allow human feedback to improve the AI's\ndecisions and explanations."}, {"title": "5. RELATED WORK", "content": "Outside of RL, a comparison of AE to other state-of-the-art explanation methods has been presented in previous\nwork. For context within the field of explainable reinforcement learning (XRL), we refer to a few more recent"}, {"title": "6. CONCLUSION", "content": "As both the quantity of data available to decision makers and the complexity of decision environments increases,\ntools are needed to help humans meaningfully understand those environments and make effective decisions.\nTraditional decision assistance tools - DSTs - have focused on using data to help users answer questions they\nalready know to ask. Instead, we proposed IDTs that use AI methods to help users find the answer to a much\nbroader set of questions, relying less on user knowledge and giving a broader perspective on the available data.\nThese capabilities were provided through: RL extensions for a MuZero-style agent for better learning and to\nreveal information about the environment to users; the extension of AE to RL to help users better understand\ndifferent interpretations of available sensor data that would lead to different decisions; SSAs to help users identify\nwhich data the AI considers critical to understanding the current strategic scenario; and criticality and safety\nmargins to help users understand the long-term consequences of different decisions. By combining all of these\ninnovations, IDTs become a viable approach for joint human/machine decision making, and in particular, help\nto uncover unknown known/unknown factors. By focusing on not just AI methods that autonomously make\ngood decisions, but also in using those methods to expose available information to human decision makers, a\nplatform is provided for making better decisions for critical missions."}, {"title": "APPENDIX A. RL AGENT DIFFERENCES FROM MUZERO", "content": "In this section are the modifications to MuZero that were implemented and explored as part of this work."}, {"title": "A.1 Balancing Pre-LayerNorm Activations", "content": "Layer normalization, or LayerNorm, 17 is a way of normalizing the activity of different neurons, to ensure that\nany resulting activations are well-conditioned for learning. Layer Norm has a number of benefits, including\nsimilar training and test time behavior. However, in some contexts, LayerNorm does not perform as well as\nbatch normalization (or BatchNorm).18 One theory for this that we explored is that there is no function\nforcing each data channel to actually be used. That is, LayerNorm enforces statistics over all data channels,\nwhereas BatchNorm enforces statistics over each data channel independently. Other work has explored forcing\nthe pre-LayerNorm activations of a NN to ensure that each channel gets used via Wasserstein normalization. 19\nHowever, we found that constraint to be too restrictive and difficult to balance; other optimization objectives\nwere negatively impacted by the condition.\nInstead, we take the idea of using gradients to condition the pre-LayerNorm activations 19 and use that idea to\nimplement a variant of BatchNorm. To implement this, the forward pass is identical to LayerNorm, but during\nthe backward pass, gradients pre-LayerNorm are modified. This is accomplished by swapping the channels\n(features) dimension with the batch dimension (assuming batch, channel, height, width ordering for images) and\nthen computing statistics across the newly located batch dimension and all subsequent dimensions. Notably, at\nthis step, we compute $d_{sd}$ as the standard deviation of each channel, and then multiply all of these by a scalar\nsuch that the new variance across all channels would be 1. We found this $d_{sd}$ normalization to be important, as\notherwise, the use of this method in, e.g., transformers, resulted in saturation of the attention layers. Finally, the\nrank of each value is computed within its statistics group (along the batch and spatial dimensions), and these\nranks are converted to locations on a Gaussian via the inverse error function:\n$target = d_{sd}\\sqrt{2er f^{-1}[\\frac{2R}{1 + N} - 1]}$,\nwhere R is the rank and N is the number of samples in the statistics group. Finally, the gradient is modified such\nthat all values are pulled toward their corresponding target by a constant (we used 1e \u2212 2) times the distance to\ntarget.\nThe result, loosely named LayerNormRebalanced, ensures that each feature channel is fully used across\ndifferent inputs, while also retaining train/test computational similarity and not saturating attention layers\naffected by the change in gradient."}, {"title": "A.2 Gaussian Mixture Models for Distributional RL", "content": "MuZero only tracked expectations of rewards. For environments with multiple distinct reward states (e.g., a\nplayer ship destroying a target, merely surviving, or itself being destroyed), it can be helpful for a human\nco-decider to understand the risks of these different reward states. To capture this information, we turn to\ndistributional RL methods.\nBriefly, distributional RL involves capturing the entire reward distribution, instead of just an expectation.\nThe original implementation that the authors are aware of was by Bellemare et al., and used point masses\nto approximate the distribution over a fixed range. 20 That technique has since been expanded with Implicit\nQuantile Networks21 and Gaussian mixture model (GMM) approaches. 22, 23\nWe appreciated the computational simplicity and representational power of the GMM approaches, but found\ndifficulties with the optimization of those representations. 22, 23 This tended to manifest in environments with\nlarge reward values - e.g., a reasonably skilled Beamrider agent might have an expected total discounted reward\nrange of [0,800], whereas a Pong agent might only have a range of [-2,2]. Instead, we found success with a\nnovel, rank-based GMM update rule. Briefly, our representation and update rules are as follows.\nFor each distribution to be captured, decide a fixed number $N_{gaussian}$ of Gaussians to track, and output\n$3N_{gaussian}$ parameters from the corresponding value/reward network for the mean, standard deviation, and log"}, {"title": "A.3 Loss Segment Scaling", "content": "For maximum diversity in different environments, and to increase interpretability for the user, we opted to\nkeep reward units in the environment's scale (e.g., the number of points in a game), without clipping or other\ntransformations. In a similar spirit as the gradient corrections from appendix A.2, we found that environments\nwith large reward ranges needed additional loss tweaking from the standard MuZero format to support e.g. the\nSimSiam loss from EfficientZero.2 This is a direct result of the gradients for the value/reward/action networks\nscaling with the observed reward magnitudes, while other gradients do not scale with those quantities. The\nbest solution we found for this was to estimate the L1 or L2 distance between actions chosen uniformly at\nrandom. This essentially is a measure of how sensitive the reward space was to different action choices, a\nquantity that roughly scales with the loss magnitudes used by the core MuZero mechanics; by dividing any\nreward-space updates (such as value/reward network updates) by this quantity, we found that the other network\nhyperparameters generalized across multiple environments."}, {"title": "A.4 Complex Action Spaces via a Pairwise Policy Update", "content": "The original MuZero paper provided policy update and policy search methods that could only handle discrete\naction spaces. While an extension was developed for complex action spaces, 24 we felt that that approach had\na number of potential deficiencies. First, their inner policy update still uses visit counts. What happens when\nvery similar actions (due to, e.g., a continuous action space) are visited twice as a result of the outer random\nsampling? Then, each has its own visit count from the MCTS process, despite being virtually identical, biasing\ntoward over-sampling (and thus preferring) those values. This can be corrected via techniques such as importance\nsampling, 25 but we wondered if there is a more straightforward way to produce policy updates.\nDrawing inspiration from prior work on RL for grammatical inference (RL-GRIT),26 we leveraged a pairwise\npolicy update approach. This has a number of potential advantages: (1) as with appendix A.2, loss scales\naccording to changes in expected reward, (2) it works the same for any kind of action space, and (3) as discussed\nin appendix A.6, it lends itself very well to an alternative of MCTS that is faster to compute while conferring\nsimilar benefits.\nGenerally, the pairwise policy update works as follows: sample $N_{update}$ potential actions from the cur-\nrent action distribution, and for each $i \\in 1..N_{update}$ compute both $log\\ P_{i, update}$, the log probability of select-\ning that action, and $Q_{i, update}$, the expected Q value for taking that action. Then, compute $dlog\\ P_{i, update} = \\frac{1}{N_{update}} \\sum_{j=1}^{N_{update}} Q_{j, update} - Q_{i, update}$.\nThis formulation of the pairwise policy update has a few interesting properties. Gradients added to each\nlog probability scale with $P(1 \u2013 P)|AQ|$, meaning both that updates are rare for saturated actions, and that\nthe scale of these updates is proportional to the expected difference in reward for making such a policy update.\nAdditionally, because this is a symmetric loss, two actions with stochastic $Q_{update}$ values that have the same\nexpectation will stay at exactly the same relative probability to one another. Since we expect the reward and\nvalue networks of a MuZero-style agent to be noisy (via optimization, not explicitly as in RainbowDQN27), we\nfind this property to be advantageous.\nThe downside to this update rule is that it gives probability mass to actions that already have probability\nmass, and thus does not do a great job on its own with the exploration/exploitation trade-off. We address this\nbelow."}, {"title": "A.5 Directly Controlling the Exploration/Exploitation Trade-Off via a Pairwise Policy\nUpdate", "content": "The exploration/exploitation trade-off is a classic concern with RL agents. 28 Essentially, the agent does not begin\nits learning process understanding the world, and thus a certain amount of exploration is required. However, as\nthe agent learns those world dynamics, it must shift to exploitation to achieve a high score. Often, in doing so,\nit reaches increasingly complex world states, which again necessitates some level of exploration until those new\nstates are more fully learned. This teeter-totter is difficult to balance, with a number of successful approaches like\nRandom Network Distillation29 and the more recent Generalized Data Distribution Iteration (GDI)28 tackling\nthis through novelty rewards and/or bandit optimization.\nDue to our pairwise policy update, we explored methods of handling this trade-off while also preventing\nexcessive saturation of log probabilities. The final approach works as follows: $N_{update, uniform}$ actions are sampled\nuniformly at random from the entire action space, and corresponding $log\\ P_{i, update, uniform}$ values are computed.\nThen, the probability mass is given from the policy samples to these uniform samples. Importantly, the magnitude\nof this transfer is controlled by an integrating controller such that the frequency of policy samples associated\nwith a sampling probability that is less than that of a uniform sampling probability is equal to $P_{target} = 0.1$.\nFor easily testing different $P_{target}$ values, we scale the speed of the integrator update by $0.5/p_{target}$.\nIntuitively, this pushes states with little difference in Q values between actions toward a uniform distribution,\nwhile allowing those states with significant value differences to saturate. We found that this setup is very flex-\nible, with the same hyperparameters working well in multiple environments. While the exploration/exploitation\ntrade-off would likely perform better by using additional techniques like GDI, 28 we found its performance in\nisolation to be satisfactory for many use cases."}, {"title": "A.6 Particle Swarm Tree Search", "content": "MCTS, used by MuZero, works well but depends heavily on tracking visit counts. In complex action spaces, this\npresents issues, as noted in appendix A.4. Furthermore, MCTS is necessarily iterative in nature, and cannot\nbenefit from the full parallelization offered by modern graphics processing units (GPUs).\nTo work around this, we devised a novel approach inspired by particle filter30 and particle swarm optimiza-\ntion31 approaches.\nTo begin, $N_{ps}$ particles are sampled, with each particle following the agent's learned policy. Each of these\nparticles are simulated for $T_{ps}$ time steps, at each step again following the learned policy from the newly com-\nputed world state according to the dynamics function. Observed immediate rewards and future Q values are\naccumulated as in MuZero's implementation of MCTS. This yields a set of particles which have both an action\nsequence and an expected Q value for following that action sequence from the current point in time.\nHaving collected $N_{ps}$ particles, we have $N_{ps}$ potential action trajectories and Q values. Like the original MCTS\nimplementation, we need a means of determining an action distribution that combines both the estimated Q\nvalue and the likelihood of each action into an upper confidence bound. Unfortunately, in a similar issue to that\nmentioned in appendix A.4, we do not have access to the frequencies of combined samples. That is, while we can\nuse the log probabilities of each action sample, those samples will also come from that same distribution, meaning\nthat direct use of the probability information has a sort of doubling effect. Instead, we can use a simple trick: if\nwe add uniform noise, scaled by $c_1(max_{ps}\\ Q \u2013 min_{ps}\\ Q)$ over all particles, to each particle's estimated Q value,\nand then take the top $N_{ps'}\\\\ N_{ps}$ of particles sorted by the modified Q values, then we have a distribution\ncaptured from the confidence bound. Here, $c_1 = 0.2$ is a constant determining the size of this effect. By adding\nuniform random noise, action samples with a larger sample probability are also more likely to receive a larger\nrandom sample. Thus, the probability of each sample is implicitly rolled into the newly sampled distribution,\ncreating an altered action distribution in the same style as the MCTS approach.\nThis approach is certainly faster than MCTS (with a speedup factor of approximately 10), and in our limited\ntrials, equally effective."}, {"title": "A.7 Modified MuZero Value Representation", "content": "Our final modification to the MuZero family of RL agents is to always have the Q network's result include the\nreward network's result for a single dynamics step. While our interpretation of MuZero is that it always has the\nQ network predict future rewards after the dynamics step, we found this much more difficult to train. The most\nlikely explanation for this is that the dynamics function begins as nonsense, and when learning the initial WM,\nit is easiest to learn a Q function that does not fully rely on an accurate dynamics model."}, {"title": "APPENDIX B. ADDITIONS TO ADVERSARIAL EXPLANATIONS", "content": "As mentioned in the main text, several changes to AE were required to achieve satisfactory performance with\nRL. These are described below."}, {"title": "B.1 Gradient Scale Correction", "content": "Gradient scale correction required two considerations: first, the massive difference in scale for outputs associated\nwith rewards and action probabilities, and second, considerations for the magnitude of the second derivative\nrequired for AE conditioning.\nTo handle the difference in output scales, we note that the input to any layer (including the final layer) has\nroughly standard normal statistics, as a result of the LayerNorms in the network. For large magnitude outputs,\nsuch as reward, this means that the final layer's weights are rather large. However, as mentioned earlier, this\nmeans that any gradients propagated backwards from this final layer would also be magnified, conflating the\nimportance of different factors within the network. To fix this, we divide the gradient propagated backward from\nthese final output layers by the expected standard deviation from the layer's weight matrix (the square root of\nthe sum of squares). This expected standard deviation can be grouped over multiple output channels where they"}, {"title": "B.2 Optimizer Scale Correction", "content": "Our experiments exclusively use theAdam optimizer for its fast convergence properties.32 However, when used in\ncombination with AE for RL, we noticed that learning was often unstable, with the occasional massive exploding\ngradient. To fix this, we used a variant of Adam that was step-size aware; that is, in the \u201cUpdate Parameters\u201d\nstep, if $|m_t/(\\sqrt{i_t} + \\epsilon)|$ > (Cmaxstep = 100) for any parameter update, then all parameter updates were scaled\ndown so that the max step size was 100 (which is then multiplied by the learning rate). This solved such\ninstabilities, and retained good optimizer quality."}, {"title": "B.3 Controlling Regularization Strength", "content": "The original AE work controlled the AE regularization loss strength by targeting a given training loss. We\ndid not find this to be effective for RL, as training loss is unpredictable and varies significantly by environment.\nOne approach would have been to target an average agent reward, but that is a complex function of both the\nenvironment and the agent regularization itself. Instead, we target an expected L1 magnitude of the gradient for\nthe chosen action's log probability with respect to the inputs. This effectively bounds the Lipschitz constraint\nfrom AE to enforce a given rate of change, which we found to be both easy to adjust and surprisingly consistent\nacross environments."}, {"title": "B.4 Sparsity-Promoting Gradient-Minimization Function", "content": "We used z = 1.25 for the $L_{adv, z, q}$ function from Section III-B of the original AE paper.6 This encouraged\nsparser gradients, which was convenient for human interpretation in many of the environments we tried. During\nevaluation (not training), we also found it useful to adjust the manner in which AEs were generated. By adding\nan additional L1 loss term back toward the original image, the explanations were even sparser, allowing for easier\ninterpretation."}]}