{"title": "Evaluating Environments Using Exploratory Agents", "authors": ["Bobby Khaleque", "Mike Cook", "Jeremy Gow"], "abstract": "Exploration is a key part of many video games. We investigate the using an exploratory agent to provide feedback on the design of procedurally generated game levels, 5 engaging levels and 5 unengaging levels. We expand upon a framework introduced in previous research which models motivations for exploration and introduce a fitness function for evaluating an environment's potential for exploration. Our study showed that our exploratory agent can clearly distinguish between engaging and unengaging levels. The findings suggest that our agent has the potential to serve as an effective tool for assessing procedurally generated levels, in terms of exploration. This work contributes to the growing field of AI-driven game design by offering new insights into how game environments can be evaluated and optimised for player exploration.", "sections": [{"title": "1. Introduction", "content": "Exploration in video game environments, is an area of study to understand what constitutes engaging and immersive experiences for players. The process of navigating through these virtual environments is often driven by the design of the levels themselves, which can either encourage or hinder exploration. As level designers strive to create more compelling and interactive worlds, understanding the factors that contribute to a good exploratory experience becomes increasingly important.\nThis study seeks to address the question of what makes certain game levels more conducive to exploration than others. Specifically, it investigates whether levels generated by 2 different procedural content generators (generator A and B) in their ability to facilitate exploration. Generator A is designed to produce levels that are generally engaging, with a balanced navigable area and a variety of interactive elements, while Generator B generates levels that might be considered unengaging, characterised by their lack of objects and object placement throughout a level. Both generators use Wave Function Collapse (WFC) to generate levels. More information about WFC can be found in 2.3.\nTo evaluate the effectiveness of these generators, an exploratory agent, modelled after those used in our previous study [1], was employed. The agent's behaviour was analysed based on several key metrics: environment coverage, inspection of unique objects, a custom novelty measure, entropy of the agent's path and average motivation experienced by the agent on its path. These metrics were chosen to quantify the quality of exploration in a way that balances the novelty and familiarity of the environment, the diversity of pathways, the unpredictability of the agent's movements, and if the agent is actually finding motivation to explore the environment.\nWe introduced the concept of an exploratory agent as \"a type of agent which traverses a level and explores it in accordance to it's features. It surveys an environment, to observe which features are available in the level, and moves in the direction towards the closest interesting target(s) or direction(s).\".\nExploratory agents have previously been used to evaluate levels in generative systems. For example, in Stahkle et al.'s PathOS framework [2] for assisting designers in level and world design. Cook also investigated evaluating levels with agents that used a vision-based approach [3], although their project was abandoned.\nOur experiment focuses on the role that exploratory agents can play as part of a PCG framework for differentiating between levels generated through various algorithms, and assessing their suitability for exploration. The key question this study tries to answer is whether exploratory agents are an effective means of filtering procedurally generated levels for exploratory experiences. These agents, by being given possible motivations for exploration, make it possible for developers to get valuable information about the quality and the engagement a level may provide, hence offering a systematic way of optimising PCG processes.\nIn this paper, we will evaluate the quality of the levels generated with respect to exploratory behaviors of agents by running a number of experiments; these experiments use coverage, entropy, and novelty metrics to see how well these levels support exploration. The long-term aim of the work is to demonstrate that EAs could be both reliable and efficient in filtering and improving the quality of procedurally generated content to ensure that the generated levels are varied, engaging, and support player exploration."}, {"title": "2. Background", "content": null}, {"title": "2.1. Curiosity Based Exploration", "content": "Pathak et al [4] provide an investigation of curiosity-driven learning in artificial agents, with a particular focus on agents operating without any external rewards. The presented work belongs to the area of autonomous agents that learn to control their behaviour in various simulated environments including games and physics simulations, purely driven by intrinsic motivation opertationalised through curiosity. The authors investigated the use of different feature spaces in estimating the prediction error and established that, while random features are enough in some cases, learned features can offer better generalisation. The research pointed to the possible failure of prediction-based rewards, more precisely in the stochastic case, and suggested that additional research on the efficient handling of such environments should be conducted.\nPathak et al's techniques are different from ours because our agent is not meant to be general in the sense that it would explore many different environments using intrinsic motivations. Our agent is given motivations, like the agent in our previous work [1], and it will explore in different ways in different environments."}, {"title": "2.2. Agents to Assist Game Design", "content": "Stahlke et al [2] introduces PathOS, which predicts player navigation in games that are digital. Level and world design can be improved by the collected data of how the agents navigate the world. The system was aimed at reducing the burden of playtesting, offering accessibility to devs, being easy to use for designers, and having generalisability. These are similar to the goals we propose with our Exploratory Agents.\nA study with 10 participants was carried out using the system, by applying and assessing it; the participants were pre-interviewed, introduced to the system and assigned to create two levels by using the system.\nThe authors reported that in the post-task interview, impressions of the system as a design tool were quite positive, although the participants mentioned that the behaviour of the agents was very different from how a player would behave.\nNova et al [5] present PathOS+ as an extension of the basic PathOS framework to complement expert assessments through the simulation of player data by AI. In this way, the problem stated by this strategy aims to deal with the subjectivity of expert assessments by using objective simulated player behaviour data. This would further help improve the reliability of expert assessments among games user research.\nThese are exemplified through the potential of PathOS+, by applying it in a gameplay analysis for navigation and player behaviour.\nFurthermore, we [1] explored the use of exploratory agents as a method for evaluating hand made game levels based off popular exploratory experiences, particularly in how well these levels support exploration. They pruposed a framework where agents are given motivations in the form of metrics to model motivations for exploration. As we do in this research project. The study showed that different combinations of metrics resulted in distinct exploratory behaviors, which aligns with expectations based on the design of the levels being tested. The researchers demonstrated that such agents could provide valuable feedback for level designers, potentially evaluating and guiding a generative process to create more engaging and exploratory-rich environments."}, {"title": "2.3. Wave function Collapse", "content": "The WFC algorithm, initially inspired by quantum mechanics, has become a prominent tool in the field of procedural content generation (PCG). This algorithm, first used to generate tilemaps, introduced by Maxim Gumin in 2016 \u00b9, operates on the principle of constraint solving and is primarily used to generate patterns or textures that resemble a given input, ensuring that the output adheres to the rules derived from the input data. The WFC algorithm functions by taking an initial grid where each cell can adopt multiple states, much like the quantum superposition, and systematically collapsing these possibilities based on local constraints until a consistent pattern is formed.\nWFC, in particular, has gained immense popularity within PCG because of its unique ability to produce coherent and intricate structures from inputs that are relatively simple. This makes it highly relevant in many applications, particularly in game level generation and general scene generation both in 2D and 3D spaces. It excels at the creation of content that remains structurally logical and is thus very suitable when developers need creativity but also coherence in their PCG.\nWFC has been applied to a number of Game Development cases in order to automate creation of complex environments with huge variability, hence drastically reducing the burden of manual level design. For instance, it has been used to generate tiling patterns for textures, layouts for dungeon-like environments, and even the infrastructures of virtual worlds. The flexibility provided by WFC can accommodate all these very different content generation scales, small detailed textures versus expansive game worlds, by adjusting the input parameters and the size of the grid used during the generation process.\nWFC has been used to generate 3D levels. For example in Bad North 2 and by Kleineberg 3. BorisTheBrave also provides a Unity3D package of which allows 3D generation of levels using WFC called Tessera 4, which we have decided to use in this research project.\nWFC is a powerful and efficient way to implement content creation automation within a videogame. This helps in generating large amounts of content in little time. For this reason, we have chosen to have our generators use WFC."}, {"title": "3. Agent Metrics", "content": "We [1] introduced object and direction based metrics in their framework. We use the same versions of these metrics. Some of them were modified. In this section, we give a brief overview of each metric we used and our modifications."}, {"title": "3.1. Direction-Based Metrics", "content": "Elevation change: This metric checks if the given direction hits any point in the terrain. If the terrain hit point is higher than the agent's y position (depending on how much higher the hit point is) a maximum value of 1 is given. 0.1 is added until the max of 1 is reached for every unit the terrain hit point is above the agent y. This metric was not modified from our previous version.\nOpenness: Takes a direction and measures how \"open\" it is by raycasting checking if there are any objects within a certain distance. Though, boundless space, a raycast hitting nothing is given a score of 0. We previously returned a value of 1 if a raycast hit nothing. However, from a perceptual and gameplay perspective, an environment with no perceivable boundaries might not actually encourage exploration, as there is no clear structure or point of interest to guide the agent's movement. This can create a sense of aimlessness rather than promoting exploratory behavior.\nWe have modified this metric to take into account how far an object is, according to the raycast, if the object is as far as the length of view, then 1 is returned otherwise, a fraction representing the percentage of how far the object is in proportion to the view distance (between 0 and 1) is returned.\nWe have decided to omit the light and shadow, and anticipation direction metrics used by us. This is because measuring varying light intensities was not a goal when generating our levels, and anticipation direction is essentially the same metric as Anticipation object detection, and we decided to go with the object detection version (to which we have renamed Anticipation detection)."}, {"title": "3.2. Object-Based Metrics", "content": "Anticipation Detection: Is given an object and checks the umbra and penumbra size of the object. It returns a maximum value of 1 and minimum value of 0. This metric was not modified from our previous version.\nLarge Object Detection: compares any object with the biggest one it had seen in the course of its run. It returns a value between 0 and 1, indicating how big, in percentage terms, an observed object is relative to the biggest one our agent had ever seen. In case the object is larger than the largest seen so far, then 1 is returned and the largest object seen so far is updated to the most recently seen object. This measure was not modified from our previous version.\nGroup Detection: Takes an object and checks if there are any other objects in a certain radius (in this case 40 units) of that object. Each object that is close adds a 0.1 to the score, to a maximum of 1. This metric was not modified from our previous version, apart from increasing the radius to check for other objects to account for the size of the assets in our experiment.\nWe have decided to omit the simple detection metric used in our previous work. We felt this was an overly simplistic metric which did not measure any object properties, as the other metrics do."}, {"title": "4. Exploratory Agent Framework", "content": "Our agent framework is very similar to the framework used by in our previous study [1]. It uses a system similar to context steering [6], in which context maps are formed for each measured direction (36 in total). A context map is a projection of the decision space of the entity onto a 1D array. Like our previous study's agents there are multiple adjustable parameters:\nLength of View: The maximum distance the agent can observe in units\nField of View: The maximum angle of which the agent can observe, independent of the camera attached\nDecision Time: The time step to recalculate the interest map and direction to move in for the agent.\nIn this framework, interest maps are formed from a list of objects which are in view of the agent. A camera is used to detect which objects are in view and only samples directions within view of its camera. There are 36 directions sampled in total. The highest scoring direction is chosen to be moved in.\nThere are three main stages to the pipeline, explained below.\nStage 1: Selecting a subset of objects A camera is used to survey the surrounding area of the agents. Every object within the camera frustrum is added to a list representing the objects of interest. The output from this stage is a list of objects of interest.\nStage 2: Making Interest Judgments The list from stage 1 is taken and an interest map, consisting of a score associated with a direction is formed. For each direction, a direction based metric is applied to calculate the directions interest score. Also, object based metrics are applied, each object has it's direction taken and rounded to the closest direction in the direction interest map (and added to the direction interest map) before the direction score is updated.\nStage 3: Making a Navigation Decision The direction map of Stage 2 is used to make a navigation decision. The direction of highest interest is chosen. If there is an object which is associated with the highest scoring direction, that is chosen to be navigated towards using A* pathfinding 5. Objects are only associated with directions when an object based metric is being used. A direction multiplied by 50 steps is chosen to be moved in. In our previous work [1] use the unity navmesh system and move 10 steps in the direction of the highest scoring direction we chose to make these changes because moving in larger steps, the agent commits more strongly to the direction identified as the most promising based on its internal metrics. This commitment ensures that the agent fully explores the opportunities presented by high-scoring directions, maximizing the benefits of moving towards areas that offer the greatest potential for novelty, object interaction, or other desirable outcomes. If there are multiple directions that are scored as the highest, a random one is chosen."}, {"title": "5. Generator Details", "content": "As mentioned before, our generators use WFC to generate level. We made these generators in the Unity game engine 6, using Tessera 7. Details of each generator are given in the following subsection.\nEach generator has 35 tiles, each of these 35 tiles have a chance (a float between 0 and 1) of being spawned. Each generator can generate a level of 350x350 units, each tile is 50 x 50, so 49 tiles can be generated to form a level. Each of these tiles can have 4 x rotation(0, 90, 180 and 270). The theoretical number of possible levels is (35x4)49 levels for both of these generators.\nHowever, because each tile has a probability associated with it, the effective expressive range is influenced by these probabilities. This means that the actual number of levels that can be meaningfully generated might be less.\nGenerator A uses slightly higher probabilities for decorated tiles and tiles with elevation/slopes, increasing the chance for a large variety of objects in our engaging levels.\nGenerator B uses slightly lower probabilities for decorated tiles and tiles with elevation/slopes, decreasing the chance for a large variety of objects in our engaging levels, leading to emptier levels/levels decorated with the same types of objects and/or less elevated positions. Generator A and B both use the same tileset.\nExploratory agents are capable of assessing how the spatial arrangement and layout of the levels influence exploration. This includes the distribution of objects which might attract players to explore, or the complexity of paths. Differentiating levels through other means, e.g. simply object counting, does not account for these spatial relationships, which are crucial for understanding the navigability and engagement level of a game environment. So, even though generator B, on average, produces levels with fewer objects, simple methods to determine whether a level is engaging or not would not necessarily be effective.\nIn addition, simpler methods of evaluating levels provide a raw measure of quantity but do not account for the spatial distribution, contextual significance, or interactive potential of these objects. Such evaluation criteria are limited in their ability to reflect the complexity of the player experience, where placement, accessibility, and interaction opportunities within the level are crucial determinants of exploration quality. Exploratory agents, on the other hand, offer a dynamic evaluation of levels by simulating motivations for exploration, allowing them to assess not just what is present in the environment, but how it might be experienced during exploration."}, {"title": "6. Evaluating Generated Levels", "content": "In order to demonstrate how well an environment might support exploration, we decided to use the evaluation criteria we used previously (Coverage, object inspection and novelty) as well as expand on it by adding our own. Our additions to the evaluation criteria include; Entropy, modifications to the novelty measure and measuring agent motivation over time. Also, like our previous work, we measured agent trajectory for each metric and spawnpoint 8. Using the mentioned evaluation criteria, we created a fitness function to give a score (between 0 and 1) for each level."}, {"title": "6.1. Coverage", "content": "Coverage serves as a measure of inspective exploration, as described in our previous work [1], we used the same technique to derive coverage as they did. Simply by counting how many of the 50x50 regions the agent had visited with their respective metrics.\nWe expect less coverage on average in our unengaging levels than in our engaging levels. Our unengaging levels lack a lot of the stimuli that drive an agent to explore the environment fully. Engaging levels often have more of these interactive elements, such as a wider variety of objects and more large objects, which motivate the agent to traverse the entire space. In contrast, unengaging levels are more repetitive, offering little to no reward for thorough exploration. As a result, the agent may not cover a lot of the area of the unengaging levels, leading to reduced coverage."}, {"title": "6.2. Inspection", "content": "Another measure of inspective exploration. This was a measure (in terms of percentage) of how many objects were seen and visited by the agent. We also used the same technique as Khaleqe et al. We measured the percentage of objects the agent came within 10 units of. A higher inspection score suggests that the agent had a \"want\" to learn about the objects in the environment whereas a lower one suggests the opposite.\nWe expect our agent to have a lower inspection score in our engaging levels than in our unengaging levels. Our unengaging levels lack a lot of the diversity and complexity found in engaging levels, which can lead to a higher concentration of the few available objects in the agent's field of view. Since there are fewer distinctive or appealing areas to explore, the agent might spend more time interacting with the objects it encounters, leading to a higher object inspection score. In contrast, in engaging levels, the agent might be more drawn to explore the environment as a whole rather than focusing on individual objects."}, {"title": "6.3. Entropy", "content": "This is a measure of Shannon entropy [7]. Shannon entropy, a foundational concept in information theory introduced by Claude Shannon in 1948, quantifies the uncertainty or unpredictability of a random variable. For a discrete random variable X with possible outcomes {x1, x2,..., Xn} occurring with probabilities {p1, p2, ..., pn}, the Shannon entropy H(X) is defined as:\n$$H(X) = - \\sum_{i=1}^n p(x_i) \\log_2 p(x_i)$$\nHere:\n\u2022 p(xi) is the probability of outcome xi,\n\u2022 log2 p(xi) is the logarithm of the probability in base 2, giving the entropy in bits.\nShannon entropy achieves its maximum value when all outcomes are equally likely, which corresponds to maximum uncertainty. Conversely, if one outcome is certain (i.e., p(xi) = 1 for some i and 0 for others), the entropy becomes zero, indicating no uncertainty.\nIn our study, Shannon entropy is used to measure the diversity and unpredictability of the agent's exploration path within the generated levels. The core reason for using Shannon entropy in this context is its ability to quantify the randomness of the agent's movements, which reflects the variety of choices the agent makes during exploration.\nBy calculating the entropy over the grid locations visited by the agent, we can determine whether the agent's path was too predictable (low entropy) or too random (high entropy). An ideal exploration path strikes a balance, showing neither excessive randomness nor predictability, which is crucial for maintaining engagement in exploratory experiences.\nWe expect our engaging levels to have less entropy, on average, than our unengaging levels. This is because our engaging levels have more meaningful object placements with a wider spread and a variety of objects that we expect will attract the agent's attention with any given metric causing more focused exploration in the engaging levels reducing the randomness of the agent's path."}, {"title": "6.4. Novelty", "content": "We used novelty measured over each time step to help evaluate the generated environments. Novelty is a measure of the stimuli experienced by the agent in it's path. We use a very similar measure to our previous work, though instead of measuring the novelty at each 50x50 region, we measure the novelty experienced by the agent at each time step (1 second).\nThe novelty score can be explained as follows:\nNt represent the novelty score at time t for a given type of object.\nSt represent the total novelty score t.\nAt represent the time interval, where At = 0.1 seconds.\nr represent the rate of novelty score recovery, where r = 0.03 per second.\nM represent the maximum novelty score an object type can recover to, where M = 0.1.\nP represent the penalty applied to the novelty score when an object type is seen, where P = 0.1.\nvt represent the visibility flag at time t, where vt = 1 if the object type is seen and vt = 0 otherwise.\nInitial Condition\nWhen a type of object is encountered for the first time:\nNo M 0.1\nand it is marked as \"seen\".\nNovelty Score Update\nThe novelty score at time t is updated as follows:\nIf the object type is not seen (vt = 0):\nNt+At = min(Nt + r \u00b7 \u2206t, M)\nIf the object type is seen (vt = 1) and it is \"new\":\nNt+At = Nt - P\nIf the object type is seen (vt = 1) and it is not \"new\":\nNt+At = Nt + r. At\nWhen the object type is seen at time t, the total novelty score is updated:\nSt = St + Nt\n\u2022 The novelty score Nt for an object type starts at 0.1 when the object is first seen.\nEach time step At = 0.1 seconds, the score either recovers at 0.01 per second (if the object type is not seen), or is penalized by 0.1 (if it is seen for the first time).\n\u2022 The tile score St accumulates the novelty score Nt of each object type seen at that time.\nWe expect novelty to be lower in our unengaging levels due to a lack of diverse elements and a more simplisitic level design. When the environment lacks diversity, the agent quickly becomes familiar with the surroundings, leading to a decrease in perceived novelty."}, {"title": "6.1. Motivation", "content": "Motivation is a measure of the highest scoring direction, according to the agent's attached metrics, at a given time step (1 second). The motivation metric serves as an indirect measure of how well the environment supports exploration. If the agent consistently finds high motivation, it suggests that those areas are well designed to encourage exploration. Conversely, if the agent's motivation decreases, it may indicate that the environment lacks sufficient stimuli or that the design is too predictable, leading to disengagement."}, {"title": "6.2. Fitness Function for Evaluating Levels", "content": "To evaluate the exploratory potential of generated levels, we designed a fitness function that integrates all the evaluation criteria mentioned above, each reflecting different aspects of the agent's exploration. The fitness function is structured as follows:\nF is the overall fitness score of a level, calculated as:\n$$F = \\sum w . f_m$$\nwhere:\nw is the weight given to a respective metric\nfm is the fitness score for the metric.\nThe fitness score for each metric is determined by the following criteria:\n1. Coverage: The fitness score fm is set to 0 if the average coverage over all spawns falls outside the range of 20% to 80%. If the coverage is within this range, fe is multiplied by the product of average motivation (Mavg) and average novelty (Navg). Ensuring that coverage is between 20% and 80% prevents the extremes where too little exploration indicates a sparse or uninteresting level, and too much coverage might suggest that the level is too small or lacks sufficient depth to sustain interest.\n2. Entropy: The fitness score fm is set to 0 if the average entropy over all spawns exceeds 0.9. Limiting entropy to 0.9 ensures that the agent's exploration is too chaotic. High entropy might indicate that the level design is overly complex or lacks clear direction, which could detract from the player experience.\n3. Object Inspection: The fitness score fi is set to 0 if the average object inspection over all spawns exceeds 80%. If the inspection rate is 80% or lower and greater than 10%, fi is multiplied by Mavg. Navg. Capping object inspection at 80% prevents the agent from being overly focused on objects, which might indicate that the level is too cluttered or lacks broader exploratory opportunities and having at least 10% of objects inspected makes sure there are at least some objects that are worth being investigated. This balance ensures that the level is engaging both at the micro and macro levels.\n4. Motivation and Novelty: Both average motivation over all spawns and average novelty over all spawns are directly factored into the multiplication to emphasize the importance of these metrics in evaluating the exploratory potential of the level. Multiplying each metric's score by the average motivation and novelty ensures that the fitness function favors levels that actively encourage exploration and offer new experiences. High motivation suggests that the level engages the agent effectively, while high novelty indicates that the level provides new stimuli and avoids repetition.\nThe final fitness score F therefore balances the contributions of individual exploration metrics with the comprehensive assessment provided by the agent loaded with all metrics. The thresholds and multipliers ensure that levels only receive a high fitness score if they meet essential criteria for meaningful exploration, such as balanced coverage, manageable entropy and appropriate object inspection."}, {"title": "7. Experiment Setup", "content": "To evaluate procedurally generated environments using exploratory agents, we conducted a study with our agent exploring 5 levels generated from generator A, considered engaging and 5 levels from generator B, considered unengaging. We looked at trajectories of the agent using the above metrics as well as an agent with a combination of all the metrics previously mentioned to gather data on how multiple metrics explore both engaging and unengaging levels. All levels were the same size (350x350) units.\nWe ran our agent with each metric for 3 minutes at 3 different spawn points for each of the levels. We also had a random control agent for which we measured coverage, novelty, entropy and object inspection for. We did not measure motivation for this agent as it didn't have the capability to have a metric loaded onto it.\nThe limited length and field of view does mean the spawn point will likely greatly affect the agent paths; to obtain a broader sample, we tested 3 different spawn points on 3 levels.\nIn our experiment, we tested all singular metrics before loading an agent with every combined metric. This approach was taken to ensure a comprehensive understanding of how each metric influences the agent's behaviour and the overall exploration process.\nBy first testing each metric individually, we observed the specific influence each one had on the agent and how it influenced the agents decision-making and exploration patterns.\nWe decided to load an agent with all metrics simultaneously to create a comprehensive and balanced exploratory agent that we thought might effectively navigate complex, procedurally generated environments and provide the most useful data in identifying engaging and unengaging levels.\nFor the experiment the following agent parameters were set to the values described below:\nLength of View: 115. Previously we set this to 80. Through preliminary testing of our agents we found this was too low of a value to observe many of the objects in these types of generated levels. 115 was a much better value in this regard\nField of View: 90. This was observed to be an appropriate value. Like our previous study, we thought a 90 degree FOV provided a good angle to perceive objects. It is standard in many first person games and does not allow an excessive view of the environment\nFor our fitness function, every metric was given a weight of 0.1, except for our agent which had all the metrics loaded in, this combination of metrics was given a weight of 0.5 This was because the combined metrics represent a holistic evaluation of the level's exploratory potential, integrating the insights provided by all the individual metrics. The agent that incorporates all these metrics is likely to offer a more accurate and comprehensive assessment of the level's quality for exploration, capturing the interplay between different motivations for exploration that might be missed when metrics are considered in isolation.\nBy giving more weight to the combined metrics, the fitness function emphasises the importance of a balanced exploration experience, where all factors are considered in tandem rather than in isolation. This approach ensures that levels which perform well across all dimensions of exploration are more highly rated, reflecting the belief that such levels are more likely to provide a rich, engaging experience for exploration."}, {"title": "8. Results", "content": "The motivation histogram for the unengaging levels vs the engaging levels shows much higher motivation frequencies, on average, for every metric tested, on the engaging levels rather than the unengaging levels. This shows that the paths followed by each metric were considered interesting, at least, much more interesting than our unengaging levels. This also suggests that there were more objects/phenomena that were interesting compared to the unengaging levels.\nThere were larger friquencies of low or 0 motivation (along with frequencies to high motivation due to how the metric works), particularly for large object detection and openness, in the engaging levels compared to the unengaging levels; this is due to the unengaging levels having a smaller spread of objects throughout their levels, so the agent was spending more time in particular areas of the map where most of the objects were placed. This is also evidenced by the trajectory plots 9.\nThe novelty histograms for the engaging levels compared to the unengaging levels shows significant differences for each metric. They are much higher frequencies with a low novelty, on the unengaging levels (this is particularly evident for openness, large object detection, anticipation detection and all metrics), where not much/nothing that is novel is being observed by the agent. This suggests that the engaging levels have more types of objects, spread out more evenly across the levels, so what the agent is viewing can be considered less boring. Even the random agent shows a lower overall novelty score with less peaks and dips.\nThe average coverage does not show significant differences between all metrics at all levels. All metrics does show slightly higher average coverage on the engaging levels than the unengaging levels, as well as the random agent. This goes against our expectations. Since both the engaging and unengaging levels have roughly the same navigability, there are no physical barriers or obstacles that would prevent the agent from moving through the space. This uniformity in navigability means that the agent can traverse the entire level without being hindered, leading to similar coverage metrics across different levels, irrespective of their design or engagement factors.\nThe average entropy does not show significant differences in entropy between the engaging and unengaging levels. Though all metrics shows slightly higher entropy in the https://github.com/BKhaleque/Evaluating-Environments-using-Exploratory-Agents engaging levels than the unengaging levels, which goes against our expectations. The reason why there This could be due to the agents explore the environment based on predetermined metrics and decision-making algorithms that systematically cover the space. Since the entropy measures the randomness and unpredictability of the agent's path, the systematic approach to exploration, where the agent randomly selects one of the highest scoring direction (even if those directions score 0), can lead to similar levels of entropy regardless of the level's engaging features. The agent's behavior might inherently limit the variation in its path, leading to similar entropy values across different environments\nThere are large differences between inspection, particularly for Anticipation detection, large object detection and all metrics, the unengaging levels show much higher inspection percentages (that's probably because there are less objects to investigate in the unengaging levels) However, all metrics shows much higher inspection on the engaging levels than the unengaging levels, this suggests that there was more motivation to investigate objects in the engaging levels than the unengaging.\nThe fitness scores for both engaging and unengaging levels, demonstrate a clear distinction in the exploratory potential of the levels generated by the two different procedural content generators. As shown in Table 1, the fitness scores for the engaging levels are consistently higher for every level, ranging from 0.703 to 0.916, with an average fitness score across all engaging levels of approximately 0.808. This suggests that the engaging levels are well-suited for exploration, offering a rich and varied environment that aligns with the exploratory behaviors of the agents.\nTable 2 shows significantly lower fitness scores for the unengaging levels, with values ranging from 0.168 to 0.673 and an average fitness score of approximately 0.494. The lower scores in these levels indicate that they are less appropriate for engaging exploration, due to a lack of engaging features and/or a more repetitive and predictable structure. The other data (histograms and measurements of average inspection) also support this. The stark difference in average fitness between engaging and unengaging levels (0.808 vs. 0.494) highlights the effectiveness of our classifier in distinguishing between levels with high and low exploratory potential, further supporting the utility of exploratory agents in procedural content generation."}, {"title": "9. Future Work", "content": "Future work could explore a broader range of PCG techniques beyond the current WFC constraint-based systems. For example, incorporating evolutionary algorithms with the fitness function used in this experiment to determine whether they are high-quality exploratory experiences could provide insight into how different generation strategies impact the exploratory behaviour of agents. This would allow for a more comprehensive understanding of the strengths and weaknesses of various PCG approaches. Testing a wider variety of levels, bigger or smaller in area, with a wider range of assets, could also help with this.\nAlso, though agent-based exploration is valuable for evaluating PCG environments, incorporating human/player feedback could enhance the understanding of how these environments support real player experiences. Conducting user studies where human players interact with the generated levels and comparing their exploration patterns and experiences with those of the agents could provide deeper insights into the alignment between agent-based metrics and actual player engagement."}, {"title": "10. Conclusion", "content": "In conclusion, there is evidence to suggest that our exploratory agent can distinguish between engaging and unengaging levels that have been generated procedurally. Future work should aim to test a larger sample size of generated levels and to have agents provide feedback on a generation process to produce higher quality PGC."}]}