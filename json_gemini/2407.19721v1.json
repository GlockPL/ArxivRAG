{"title": "Rina: Enhancing Ring-AllReduce with In-network Aggregation in Distributed Model Training", "authors": ["Zixuan Chen", "Xuandong Liu", "Minglin Li", "Yinfan Hu", "Hao Mei", "Huifeng Xing", "Hao Wang", "Wanxin Shi", "Sen Liu", "Yang Xu"], "abstract": "Abstract\u2014Parameter Server (PS) and Ring-AllReduce (RAR) are two widely utilized synchronization architectures in multi-worker Deep Learning (DL), also referred to as Distributed Deep Learning (DDL). However, PS encounters challenges with the \"incast\" issue, while RAR struggles with problems caused by the long dependency chain. The emerging In-network Aggregation (INA) has been proposed to integrate with PS to mitigate its incast issue. However, such PS-based INA has poor incremental deployment abilities as it requires replacing all the switches to show significant performance improvement, which is not cost-effective. In this study, we present the incorporation of INA capabilities into RAR, called RAR with In-Network Aggregation (Rina), to tackle both the problems above. Rina features its agent-worker mechanism. When an INA-capable ToR switch is deployed, all workers in this rack run as one abstracted worker with the help of the agent, resulting in both excellent incremental deployment capabilities and better throughput. We conducted extensive testbed and simulation evaluations to substantiate the throughput advantages of Rina over existing DDL training synchronization structures. Compared with the state-of-the-art PS-based INA methods ATP, Rina can achieve more than 50% throughput with the same hardware cost.", "sections": [{"title": "I. INTRODUCTION", "content": "The field of Deep Learning (DL) has seen remarkable advancements in recent years, driving transformative break-throughs across various domains. The advent of Artificial General Intelligence (AGI) and large language generation models, such as the Generative Pre-trained Transformer (GPT) [1], have significantly advanced machine intelligence and Natural Language Processing (NLP) [2], [3]. Furthermore, models like Segment Anything (SA) [4] have enriched DL's capability in image segmentation.\nAs model complexity and dataset sizes continue to expand exponentially, Distributed Deep Learning (DDL) has emerged as a pivotal approach for efficient training. Data parallelism, a strategy for managing vast datasets, divides the dataset among distinct processing units for concurrent training. This technique enables DL models to accommodate substantially larger datasets, thereby significantly enhancing training efficiency."}, {"title": "II. BACKGROUND", "content": "A. Distributed Deep Learning\nThe mathematical goal of DL can be defined as the following optimization problem (Equation 1), where $d_i$ is a data sample of dataset D, and $w$ represents all the parameters of the model and $y_i$ is the associated label with $d_i$. $f$ takes an input and outputs a prediction. $loss$ is the objective function. The goal is to minimize the average loss across the dataset.\n$\\min_{d_i \\in D}loss(f(w, d_i), Y_i)$ \nWith the rapid growth of the size of datasets and models, DDL has gained lots of research interest and has become the primary method to improve the training efficiency and throughput to meet the demands both industrially and academically.\nThere are two prominent parallelism schemes of DDL, data parallelism [10] and model parallelism [11], [12]. Data parallelism duplicates training models across all computing workers. In a single iteration, each computing worker processes different mini-batches of data to calculate the local gradient updates which are exchanged with other workers later before updating the model parameters [13]. When comes to data parallelism, Equation 1 changes into Equation 2, where N denotes the number of workers. The synchronization in data parallelism is the main optimization objective of this study.\n$\\min \\frac{1}{N} \\sum_{i=1}^N E_{d_i \\in D} loss(f(w, d_i), y_i)$ \nModel parallelism splits the model parameters to multiple workers to make it possible to train large-size models. Each worker holds a subset of model parameters or layers. In every iteration, the sampled mini-batch of datasets is copied to all workers, and different parts of the DL model are computed on different workers. Model parallelism is also an important area of study, which is orthogonal to the data parallelism emphasized in this paper [14]."}, {"title": "B. Synchronization Architectures", "content": "1) Parameter Server Architecture: The PS architecture [15] is a straightforward method for parallel computing across multiple workers (Figure 1(a)). In this architecture, a PS node maintains and manages a global model. During each training iteration, each worker computes its local gradients based on its own mini-batch and communicates these results to the PS. The PS updates the global model and synchronizes it with each worker. Typically, there are two synchronization models: Bulk Synchronous Parallel (BSP) [16] and Asynchronous Parallelism (ASP) [17]. In BSP, workers must await a synchronization barrier before initiating the next iteration. Conversely, ASP removes this synchronous barrier. Generally, BSP tends to yield higher accuracy, while ASP significantly increases training throughput. Regardless of the synchronization model, the PS architecture remains a prevalent choice in large-scale training clusters.\n2) Ring-AllReduce Architecture: AllReduce (AR) is a de-centralized architecture proposed to alleviate communication bottlenecks in the PS architecture (Figure 1(b)). AR treats all machines as workers, thus eliminating the need for PS. Ring AllReduce (RAR) stands out among AR algorithms due to its superior bandwidth performance. RAR splits communication phases into ScatterReduce and AllGather. In the ScatterReduce phase, each of the N workers divides their local gradients into N chunks. Each worker, in every iteration, transmits a chunk to its neighbor, receives one, and adds it to the corresponding chunk. The chunks transmitted and received in each iteration are different, with each worker forwarding the chunk received in the previous iteration. After N 1 iterations, each worker possesses a globally updated chunk. For instance, in Figure 2(a), worker 1 forwards chunk A to worker 2 in the first iteration. Worker 2 adds it to its local chunk and passes it to worker 3 in the subsequent iteration. After the ScatterReduce phase, worker 4 will possess a fully updated chunk A. During the AllGather phase, each worker transmits its complete chunk to the next worker and obtains one from the previous. Like in ScatterReduce, each worker forwards the chunk it received in the previous iteration. After N 1 iterations, every worker has fully updated gradients. As illustrated in Figure 2(b), worker 4 sends its fully updated chunk A to worker 1, who then forwards it to the next worker (i.e., worker 2). Thus, at the end of the AllGather phase, each worker possesses a fully updated result for all chunks. Unlike PS, RAR operates solely in BSP mode. While RAR achieves optimal bandwidth performance, it suffers from issues of extensive dependency chains and vulnerability to a single point of failure [18]."}, {"title": "C. Bring INA into DDL", "content": "In recent years, advancements in programmable networks have driven a surge of research employing INA techniques to expedite DDL training. INA leverages the computational power within programmable switches to aggregate gradients from multiple nodes, reducing network traffic and accelerating DDL training. Notable works in this domain include SwitchML [6] and ATP [7], both of which aim to enhance overall training speed by offloading the gradient aggregation task to switches. In SwitchML, all gradients are aggregated within the switches, thus training speed hinges significantly on the switches' aggregation capabilities. ATP adopts a best-effort strategy for gradient aggregation, where gradients not aggregated at the switch level are relayed to the PS for aggregation.\nNevertheless, these optimization efforts are tailored specifically for INA within the PS architecture. The RAR architecture, renowned for its efficient communication performance, is gaining increased attention. To our knowledge, no existing research explores INA utilization within the RAR architecture, presenting a distinct set of challenges and opportunities at the core of this study."}, {"title": "III. MOTIVATION AND CONCEPTS OF RINA", "content": "In this section, we first propose to analyze the issues with RAR methods, specifically their long dependency chains. Next, we present the Bandwidth-Occupation Model (BOM) for all existing PS-based INA methods to illustrate their problem: the lack of incremental deployment capability. Finally, we present the design concepts and architecture of Rina, briefly elaborating on its advantages over both the state-of-the-art PS-based INA methods and traditional RAR methods.\nA. Long Dependency Chain Problem in Ring-AllReduce\nCompared to PS-based INA, RAR does not have communication bottlenecks, which are determined by the communication mode of RAR. The following provides quick proof.\n1) We can view a network as a connected undirected graph. Let G = (V, E) be a connected undirected graph.\n2) Transform G into a directed graph D = (V, D) by replacing each $u, v \\in E$ with two directed edges (u, v) and (v, u) in D.\n3) By this transformation, for each vertex $v \\in V$, the in-degree equals the out-degree.\n4) According to the Eulerian path and circuit conditions [19] in directed graphs, since the in-degree equals the out-degree for all vertices in D, there exists an Eulerian circuit.\n5) Hence, a path in D starts from any worker, visits every worker once, and finally ends at the starting worker. This guarantees the RAR's requirements for communication without bottleneck.\nAlthough RAR has been proven to be free of bandwidth bottleneck, it still suffers from the issue of long dependency chains [20]. The most significant problem caused by long dependency chains is that throughput performance becomes affected by the increasing number of nodes. Take Figure 3 as an example. The figure shows the workflow of an RAR pipeline. The same part of the gradients shares the same color. Each node sends its computed gradient $G_1$ to the next worker. However, according to the implementation of the latest MPI libraries such as NCCL [21] and OpenMPI [22], each round of communication has a barrier to global synchronization. Interference caused by load fluctuation, interrupts, garbage collection, or background tasks during Worker 3 processing $G_6$ will defer the global completion time of this step, Indicating that single-point failure will directly slow down the entire training process. This is the fundamental problem for the long dependency chain.\nSuppose the whole cluster has N workers. The system overhead of sending gradient (including network protocol, memory movement, et al.) chunk i is O($G_i$), while the computation and communication time is C($G_i$). For j-th round synchronization, the time consumption of worker n will be O($G_u$) + C($G_u$),u = (i + n)%N. Since the existence of barriers, the estimated time consumption of the ScatterReduce phase will be $\\frac{1}{N} Max(O(G_u) + C(G_u))$.\nIn practical scenarios, O($G_u$) can be considered a fixed overhead independent of N. Typically, the distribution of C($G_u$) is proportional to N in a linear fashion. We assume C($G_u$) follows a normal distribution, where its mean is proportional to N and the variance is a fixed value, expressed as C($G_u$) ~ $N (k\u00b7\\frac{N}{4}, \u03c3^2)$. Here, k is a constant. The standard deviation \u03c3 is also assumed to be a constant \u03c3. For a random variable X that follows a normal distribution $N(\u03bc, \u03c3^2)$, the expected maximum value $M_n$ (taken from n independent and identically distributed samples) can be approximated as $E[M_n] \u2248 \u03bc + \u03c3\\sqrt{2\\ln{n}}$. Thus, the time consumption T of RAR during the ScatterReduce phase can be expressed as:\n$T =N - O(G_u) + \\underset{u=1}{\\overset{N}{\\sum}}Max(C(G_u))$\n$=NO(G_u) + E[max(C(G_u))]$\n$\u2248N\u00b7 O(G_u) + k + N\u00b7 \u03c3\\sqrt{2 \\ln N}$ \nFrom Equation 3, it can be seen that the value of T increases with the size of N, which demonstrates that the synchronization completion time of RAR increases as the number of nodes increases. It is also noteworthy that an increase in \u03c3 will also lead to an increase in T, which means that if the nodes' performance is unstable, the synchronization completion time of RAR will be further prolonged. This phenomenon is common [15], [23], [24], thus reducing the long dependency chain of RAR is highly necessary.\nAs a widely used AllReduce method in the industry, Hybrid Allreduce (H-AR) [25] has been proposed to address the issue of long dependency chains through a multi-step AllReduce process. H-AR first performs a ScatterReduce within the ToR, then an AllReduce between ToRs, and finally an AllGather within the ToR. This approach indeed mitigates the long dependency chain problem and achieves better performance than RAR. However, Rina's utilization of INA switches can achieve even higher throughput compared to H-AR, for Rina not only mitigates the dependency chain but also provides in-network computation capabilities. A detailed comparison is provided in \u00a7 VI-B."}, {"title": "B. Modeling PS-based INA with BOM", "content": "A major advantage of the PS-based INA approaches in improving the throughput of DDL training tasks is its ability to reduce network traffic [9]. In this section, we quantify this through the BOM model and discuss their weakness in incremental implementation.\nAssumptions: The DDL training cluster uses the BSP synchronization algorithm. All nodes need to send the gradients of their local models to the PS synchronously, followed by a broadcast generated by the PS. The INA switch can fully aggregate incoming traffic (as proven to be feasible in INAlloc [8] under the single-job scenario). If the corresponding PS-based INA method does not require a PS server, the farthest INA switch is treated as the PS. The entire topology is homogeneous, with a link bandwidth of $B_0$. There is no multipath scenario in the topology, that is, there is exactly one path from all nodes to the PS.\nLemma 1: For a topology only containing regular switches and n workers, the worker throughput is 1/n.\nAs shown in the sub-topology $T_1$ in Figure 4, this topology does not include any INA switches. Assuming the throughput of its outbound switch 2 is $B_1$, the worker throughput in this topology is $B_1/4$. The proof is as follows.\nAssume a complex topology T. From the topology T, we select the PS node working as the root to build a Directed Acyclic Graph (DAG) tree, which can be used to represent the network traffic during the gradient aggregation phase.\nGiven a DAG tree G = (V, E), where V is the set of vertices (or nodes) and E is the set of directed edges. G is a subtree of T. The root node is denoted as r and L is the set of leaf nodes. The output rate of a node v, denoted as OR(v), is defined as the number of outgoing edges from v.\nWe aim to prove that \u2200l \u2208 L, OR(l) is determined by the output rate of the root node r, OR(r). To do this, we use the principle of mathematical induction.\nBase case: When |V| = 1 (i.e., the tree only contains the root), it's trivial that OR(l) depends on OR(r) since they are identical.\nAdditional case: We select a non-leaf node v in inductive step. When |V| = 2, no leaf nodes exist, thus only one of the leaf nodes can be selected arbitrarily. In this scenario, OR(1) = OR(r) is also evident.\nInductive step: Assume the proposition holds for any tree with |V| = n, i.e., for any tree with n nodes, Vl \u2208 L, OR(1) is determined by OR(r).\nWe need to prove that for any tree with |V| = n+1, \u2200l \u2208 L, OR(1) still depends on OR(r).\nConsider a tree G with |V| = n + 1. Select a non-leaf node v (except r) and consider the sub-tree G' formed by removing one of v's child nodes c (and edges attached to c). Now G' has n nodes.\nBy the induction hypothesis, VI' \u2208 L' (leaf nodes in G'), OR(l') depends on OR(r). Since removing the child c of v doesn't change OR(r), it still holds that \u2200l' \u2208 L', OR(l') depends on OR(r).\nFor the removed node c, since it was an outgoing edge from v and eventually from r, OR(c) also depends on OR(r).\nHence, we have shown that for any tree G with |V| = n+1, Vl\u2208 L, OR(l) is determined by OR(r). We can conclude that for any topology only containing regular switches, the output rate of each worker is determined by the number of outgoing bandwidth from the root, which is OR(r)/W. W represents the number of workers\nLemma 2: The INA switch and its children can be viewed as one worker.\nINA switches can aggregate the received gradients and output dataflow without redundant positional gradients. In this way, to the parent node of this INA switch (no matter the other INA switch or PS), its behavior is manifested as an independent worker.\nLemma 3: For an INA switch, the actual throughput depends on the worst-performing child.\nTake Figure 4 as an example. For the outbound INA switch 3 in topology T2, even though it has sufficient INA capabilities to allow workers 1 and 2 to function at 100% throughput, it is still limited by the slowest child node (topology T\u2081). This implies that the actual outbound bandwidth of topology T2 is $B_0/4$, which is also the actual global throughput of this example.\nAt this point, given the topology, nodes, and placement of INA switches, we can calculate the actual throughput of all"}, {"title": "C. Incremental Deployment is Challenging for PS-based INA", "content": "The incremental deployment capability of PS-based INA methods is poor. Using BOM, we evaluate the changes in DDL training throughput in a specific topology scenario, starting from \"all switches are regular switches\" to replacing all switches in the topology with INA switches.\nThe training task we selected is ResNet50 [26], using the CIFAR-10 [27] dataset. We have chosen two topologies, namely the standard Fat-tree [28] (k=4) and standard Dragonfly [29] (a=4, g=9, and h=2) topologies, which are commonly used in data centers. The corresponding results are shown in Figure 5. As can be seen, to achieve significant throughput improvements, PS-based INA methods need to replace regular switches in the entire network with high-performance programmable switches as much as possible. If only a part of the switches is replaced, the effect of INA cannot be well-utilized. Therefore, designing an incremental deployment-friendly DDL synchronization architecture is one of the important design principles of Rina."}, {"title": "D. Rina Design Concepts and Challenges", "content": "Before introducing details of Rina, we summarize the existing PS-based INA methods from a higher perspective. The existing PS-based INA involves the INA device wrapping all its attached devices into a unified external device. From the viewpoint of other devices after the INA device, the INA and its workers are combined as a larger-scale worker."}, {"title": "IV. DESIGN DETAILS OF RINA", "content": "Rina is designed to incorporate the INA switch into DDL training tasks using the RAR synchronous architecture as a basis. Rina not only retains the benefits of the RAR architecture, including the absence of communication bottlenecks, but it also effectively mitigates the issue of the long dependency chain. Essentially, Rina upholds the RAR workflow pattern, which includes the ScatterReduce and AllGather phases, and optimizes these stages to leverage the capabilities of INA devices. Moreover, Rina introduces a new workflow based on the agent-worker model, discussed in \u00a7 IV-A. This model allows INA devices to manage all workers within each rack, as detailed in \u00a7 IV-B. We implement a lightweight congestion control protocol to meet the unique requirements of Rina and its utilization of INA capabilities. Furthermore, when compared to PS-based INA, Rina provides superior incremental deployment capabilities. We discuss in detail in \u00a7 IV-D how Rina achieves incremental deployment capability and how incremental deployment is carried out.\nA. Agent-worker Model\nFigure 6 presents a comprehensive illustration of the agent-worker model. Rina adopts the rack as its operative unit (i.e., an abstracted worker), superseding each rack's ToR switch with an INA switch to enable the INA capability. When viewed from an individual rack's perspective, once its ToR switch is supplanted by an INA switch, the worker of the lowest rank within that rack assumes the responsibility of managing the rack's INA switch and its workers. This low-rank worker is termed the \u201cagent\u201d. In addition to performing computational tasks, the agent also performs several additional functions, such as initiating Rina, assigning tasks to the INA switch, and provocation of synchronizations for other workers. For real-world implementations, these agent roles are executed via an extra daemon program that runs on one of the workers in the group (usually the first worker).\nB. Rina's Architecture, Workflow, and Dataflow\nAs illustrated in Figure 7, Rina is an architecture facilitating the harmonious operation of regular and INA switches. If a rack with an INA-enabled ToR switch exceeds two nodes, Rina can be deployed, designating such a rack as an abstracted worker and Rina-enabled rack. Rather than assigning tasks individually, Rina adopts a group-based approach, considering Rina-enabled racks as an abstracted worker. Conversely, for Rina-disabled racks, each worker is regarded as an autonomous worker or an autonomous group.\n1) Model and Dataset Partitioning: Data-parallel DDL training usually necessitates dataset partitioning to attain parallelism, with the addition of model splitting by RAR to guarantee synchronization throughput. Consequently, Rina calls for meticulous consideration in the partitioning of data and models. In both PS and RAR architectures, when all workers share equivalent computational capacity, an equal dataset segment is allocated to each worker to approximate global computation time. Conversely, with workers possessing heterogeneous computational capabilities, strategies such as batch-size tuning [30], [31] are employed to synchronize computation time.\nRegarding model partitioning during synchronization, the conventional RAR strategy divides the model evenly across workers to ensure near-equal synchronization time per step (refer to \u00a7 II-B). However, Rina necessitates model partitioning in line with the number of groups. This stipulation arises primarily because each Rina-enabled rack, represented as an independent worker by its respective agent and INA switch, must be accounted for.\nAt the onset of training, both the dataset and model partitions are disseminated to ensure uniform initial parameter states across all workers. We designate a global control node, which will be the agent of the Oth group or Oth autonomous worker. As training commences, this node randomizes all parameters of the target model and transmits the partition data of both the dataset and model to all groups (Figure 7-(0). To expedite the distribution of control messages within a Rina-enabled rack, Rina utilizes multicast. Once this preparatory phase is complete, all workers embark on their training tasks.\n2) Synchronization Process: Before each round of DDL training, synchronization is necessitated for all workers to synchronize training results based on each node's respective dataset. This periodic process aligns well with near-equal computation times across nodes, enabling all workers to commence synchronization approximately simultaneously. As in the RAR system, upon completion of computation, each node promptly transmits its model partition's gradient to the subsequent worker while concurrently receiving the gradient from the prior worker and conducting local aggregation. This attribute is retained in Rina. For autonomous workers, their actions mirror those in RAR. Meanwhile, within Rina-enabled racks managed by the agent, the agent handles task delegation.\nSimilar to RAR, Rina is divided into two phases: ScatterReduce and AllGather. During the ScatterReduce phase, all nodes undergo a synchronization round, ensuring each node acquires the final computation result for a model gradient segment. This gradient portion is then broadcasted to all nodes in the AllGather phase. Participation of the INA switch in aggregation is necessary for ScatterReduce, while AllGather necessitates switch support in multicast.\nIn a cluster comprising N nodes, these two phases would necessitate 2(N \u2013 1) steps in the RAR system. In contrast, in Rina with G groups, each phase demands 2G \u2013 1 steps. Given that a group may include several workers, in a cluster where each rack hosts eight computing nodes, G would equate to N/8. Thus, the synchronization steps demanded by Rina are notably fewer than those required in RAR, which contributes to a higher throughput.\n3) ScatterReduce Phase: Given the inability of prevalent P4 programmable switches such as Tofino-1 [32] to autonomously generate packets, and considering synchronization requirements, the synchronization signals of workers are uniformly triggered by the agent for each group.\nReferring to Figure 7, upon the agent's completion of its current computation round, it relays aggregation task data to the switch (1). This information comprises the model range and size destined for the next group, the reserved memory space in the switch, and the ID of the node currently engaged in the aggregation. On receipt of this data, the switch converts this packet into a data pull message (\u2461), which is multicast to all workers (including the agent) under this rack. Triggered by the pull message, all nodes initiate the transmission of corresponding gradients to the ToR switch (\u2462). These parameters are then aggregated at the ToR switch and dispatched to the agent of the subsequent group (4 and 5). When the next"}, {"title": "D. Incremental Deployment", "content": "Rina's agent-worker model can integrate all nodes under an in-network computing switch into a single worker. This means that replacing a conventional ToR switch connecting N nodes with an INA switch can reduce the length of the RAR dependency chain by N. Therefore, in the initial deployment phase, we should prioritize replacing normal switches with the most connected workers with INA switches.\nAs deployment progresses, we should consider replacing other normal switches in the topology that are not TOR switches. Refer to \u00a7 III-B, constructing a minimum spanning tree rooted at a specific worker node that connects all worker nodes. Then, by treating an INA switch and all its downstream worker nodes as a single worker, we can similarly replace the conventional switch with the most downstream nodes with an INA switch.\nThrough the gradual replacement of standard switches with those developed using our method, we can achieve effective incremental deployment. Each instance of switch replacement leads to a noticeable enhancement in the throughput of DDL tasks. A thorough evaluation of our incremental deployment capabilities is provided in Section VI-C."}, {"title": "V. IMPLEMENTATION", "content": "We implement the Rina prototype on both P4 programmable switches [32] and workers. The deployment on the switches emphasizes INA capabilities, whereas the implementation on worker nodes principally aims to enhance the processing performance of small packets.\n1) P4 Programmable Switch: Given the absence of floating-point computation capabilities in P4 switches, Rina adopts a method analogous to ATP [7], where all floating-point numbers are multiplied by an integer and then converted to integers at the worker nodes. This conversion enables the P4 switch to transform floating-point addition into a simpler integer addition operation. Moreover, considering hash-based memory allocation algorithm may cause collisions [7], Rina allocates contiguous memory space directly for INA tasks. To augment the INA throughput, we utilize the P4 switch's recirculate feature to extend the length of each INA packet.\n2) Worker: We implement a Rina prototype as middleware, seamlessly integrated into PyTorch. By utilizing UDP, we develop a customized transport protocol that leverages Mellanox (NVIDIA) Raw Ethernet Programming [35] to expedite the processing of user-space data packets. To enhance packet processing performance, we also employ TCP segmentation offload (TSO) [36]."}, {"title": "VI. EVALUATION", "content": "In this section, we evaluate the advantages of Rina compared to commonly used DDL training synchronization architectures, through simulation experiments and testbed experiments. The evaluation includes throughput, incremental deployment capabilities, and robustness.\nA. Evaluation Setup\n1) Simulator: We use the popular NS3 [37] simulator as our simulation tool. We developed real worker nodes and PS in NS3 and implemented the logic of PS and RAR synchronization architectures. For switches, we use the switch component to simulate regular switches and use nodes to implement the simulation logic of INA switches. In the simulation, we additionally evaluate the incremental deployment capabilities of various methods in popular data center topologies. These topologies include standard Fat-tree [28] (k=4) and standard Dragonfly [29] (a=4, g=9, and h=2).\n2) Testbed Configuration: We evaluate Rina using an 8-node cluster. The nodes are separated into 2 racks, for each rack has 4 nodes. They are interconnected through two Intel Tofino-1 P4 programmable switches (with 32x100Gbps ports) as the ToR switch. Each node has one AMD Epyc 7643 CPU (48 cores, 96 threads), 128GB RAM, and one Mellanox ConnectX-6 Ethernet Network Adapter with 2 100Gb ports. Additionally, each node has one NVIDIA RTX3090 GPU. The NVIDIA driver version is 460.91.03, and the CUDA [38] version is 11.2. The operating system is Ubuntu 20.04.2 with kernel version 5.15.0-75-generic. We use these 2 switches and 8 workers to build a spine-leaf simple topology for the evaluation of Rina.\n3) Workload: In the evaluation, we conduct several experiments to evaluate the performance of 5 DL models and 4 datasets. The workloads include training ResNet50 [26] and VGG16 [39] models on the CIFAR10 [27], InceptionV3 [40] on the CIFAR100 [27], ResNet101 [26] on the ImageNet1K [41], and the BERTbase [42] model on the SQUAD1.1 [43]. The task for the BERTbase model is the fine-tuning task on the SQUAD1.1 dataset. The batch sizes for all image classification models are set as 64, while BERT is 12. In all results, the throughput unit for BERTbase is one question-answer pair per 10 seconds. All other hyper-parameters stay as default. These workloads are chosen to enable a thorough understanding of the strengths and limitations of Rina.\n4) Baseline and Metric: We compare Rina with regular PS, RAR, H-AR [25], and PS-based INA (ATP [7]). The PS-based approaches use co-located PS for better performance. H-AR is a widely used method in the industry, achieving improved parallel performance than RAR. The two INA methods ATP and Rina ensure that the INA switches used have no memory bottlenecks and have similar aggregation throughput. We do not present the evaluation of SwitchML [6] since its throughput is consistently inferior to ATP. We primarily compare the performance differences in throughput of these four schemes, as well as the incremental deployment capabilities of ATP and Rina under different topologies. Specifically, we evaluate their gains in throughput by gradually replacing the regular switches in the corresponding topology with INA switches.\nB. Evaluation on Throughput\nWe assess Rina's throughput using five distinct DL models across both Fat-tree and Dragonfly topologies. Initially, we establish PS and RAR as baselines and juxtapose ATP and Rina at 50% and 100% INA switch replacement rates. In the Fat-tree topology, 50% replacement entails using 6 INA switches in ATP and 4 in Rina, while for Dragonfly, it implies employing 18 INA switches in both methods. As depicted in Figure 10, Rina significantly exceeds the common baselines of PS and RAR. H-AR outperforms RAR in terms of performance, but Rina can achieve better throughput than H-AR by replacing only half of the network switches. Compared to ATP, Rina can significantly enhance throughput by replacing 50% of the network switches with INA switches. Furthermore, after replacing all switches with in-network computing switches, Rina performs comparably to ATP and even surpasses ATP in the ResNet50 model. This indicates that the integration of INA switches in Rina yields superior benefits and elevates overall performance. For ease of presentation, we express the throughput of the BERTbase model as Questions and Answers (QAs) every 5 seconds.\nC. Evaluation on Incremental Capability\nWe further evaluate the incremental deployment capabilities of ATP and Rina using ResNet50, featuring a model with 98MB parameters. In both topologies, we progressively replace all switches with INA switches and measure their throughput. As illustrated in Figure 11, the throughput of Rina gradually increases as the count of INA switches rises. On the other hand, ATP, due to its lack of incremental deployment capabilities, only witnesses a throughput boost after a significant quantity of INA switch replacements. This indicates that with Rina, DDL training operators can experience performance enhancements proportional to their investment, thus offering substantial hardware cost-effectiveness.\nD. Testbed Verification\nWe assess Rina's performance on a testbed, with the outcomes depicted in Figure 12. The results reveal that Rina mirrors the performance advantages observed in the simulation. Its performance remains closely matched with ATP when all switches are replaced by INA switches. However, when only a fraction of switches are replaced, Rina can attain a training throughput surpassing ATP. This suggests that Rina preserves comparable performance benefits in real-world situations and provides enhanced incremental deployment capabilities compared to PS, RAR, and other PS-based INA methodologies."}, {"title": "VII. DISCUSSION", "content": "1) Improving Robustness: Rina confers a centralized control mechanism, enhancing the robustness of the cluster. However, specific design details and comprehensive evaluations are still needed. We are currently conducting experiments to make Rina to provide increased reliability, expedited error recovery, and dynamic scalability, further strengthening the robustness of DDL training clusters. Moreover, Rina does not address the issue of node heterogeneity. The heterogeneity in node computational capabilities should be managed through other methods such as batch-size tuning [31].\n2) Combined with Model Parallelism: Although this paper primarily discusses data parallelism and does not address model parallelism, it is worth noting that RAR is also frequently used in widely-used model parallel synchronization strategies. Consequently, if we attempt to introduce INA into model parallelism, Rina can be seamlessly integrated. We are continuing to investigate the potential challenges and solutions associated with incorporating Rina into model parallelism."}, {"title": "VIII. RELATED WORK", "content": "1) Reduce the Communication Size in DDL: Communication compression approaches are proposed to reduce communication costs. Stochastic Rounding [44", "45": "generalizes stochastic rounding to stochastic quantization and proposes multi-level gradient quantization schemes to further lower the transmission costs. Gradient sparsification can also reduce the communication size. A representative method of gradient sparsification [46", "5": "utilizes the loss-tolerant transmission to reduce the communication time. Deep gradient compression [47", "Synchronization": "Synchronization models greatly affect the performance of DDL training. BSP [16", "48": "aims to alleviate the straggler problem of BSP without losing synchronization by allowing faster workers to do more updates without waiting for slower ones, but still guarantees a staleness-bounded barrier. Compared with the SSP, ASP [17", "31": "uses a 2-stage synchronization to reduce communication and speed up the training throughput. Local SGD [49"}]}