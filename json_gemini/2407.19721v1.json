{"title": "Rina: Enhancing Ring-AllReduce with In-network Aggregation in Distributed Model Training", "authors": ["Zixuan Chen", "Xuandong Liu", "Minglin Li", "Yinfan Hu", "Hao Mei", "Huifeng Xing", "Hao Wang", "Wanxin Shi", "Sen Liu", "Yang Xu"], "abstract": "Parameter Server (PS) and Ring-AllReduce (RAR) are two widely utilized synchronization architectures in multi-worker Deep Learning (DL), also referred to as Distributed Deep Learning (DDL). However, PS encounters challenges with the \"incast\" issue, while RAR struggles with problems caused by the long dependency chain. The emerging In-network Aggregation (INA) has been proposed to integrate with PS to mitigate its incast issue. However, such PS-based INA has poor incremental deployment abilities as it requires replacing all the switches to show significant performance improvement, which is not cost-effective. In this study, we present the incorporation of INA capabilities into RAR, called RAR with In-Network Aggregation (Rina), to tackle both the problems above. Rina features its agent-worker mechanism. When an INA-capable ToR switch is deployed, all workers in this rack run as one abstracted worker with the help of the agent, resulting in both excellent incremental deployment capabilities and better throughput. We conducted extensive testbed and simulation evaluations to substantiate the throughput advantages of Rina over existing DDL training synchronization structures. Compared with the state-of-the-art PS-based INA methods ATP, Rina can achieve more than 50% throughput with the same hardware cost.", "sections": [{"title": "I. INTRODUCTION", "content": "The field of Deep Learning (DL) has seen remarkable advancements in recent years, driving transformative break-throughs across various domains. The advent of Artificial Gen-eral Intelligence (AGI) and large language generation models,such as the Generative Pre-trained Transformer (GPT) [1],have significantly advanced machine intelligence and NaturalLanguage Processing (NLP) [2], [3]. Furthermore, models likeSegment Anything (SA) [4] have enriched DL's capability inimage segmentation.\nAs model complexity and dataset sizes continue to expandexponentially, Distributed Deep Learning (DDL) has emergedas a pivotal approach for efficient training. Data parallelism, astrategy for managing vast datasets, divides the dataset amongdistinct processing units for concurrent training. This tech-nique enables DL models to accommodate substantially largerdatasets, thereby significantly enhancing training efficiency."}, {"title": "II. BACKGROUND", "content": "A. Distributed Deep Learning\nThe mathematical goal of DL can be defined as the fol-lowing optimization problem (Equation 1), where d_i is a datasample of dataset D, and w represents all the parameters ofthe model and y_i is the associated label with d_i. f takes aninput and outputs a prediction. loss is the objective function.The goal is to minimize the average loss across the dataset.\n$\\min _{w} \\frac{1}{N} \\sum_{d_{i} \\in D} loss(f(w, d_{i}), Y_{i})$\nWith the rapid growth of the size of datasets and models,DDL has gained lots of research interest and has become\nthe primary method to improve the training efficiency andthroughput to meet the demands both industrially and aca-demically.\nThere are two prominent parallelism schemes of DDL,data parallelism [10] and model parallelism [11], [12]. Dataparallelism duplicates training models across all computingworkers. In a single iteration, each computing worker pro-cesses different mini-batches of data to calculate the localgradient updates which are exchanged with other workers laterbefore updating the model parameters [13]. When comes todata parallelism, Equation 1 changes into Equation 2, whereN denotes the number of workers. The synchronization in dataparallelism is the main optimization objective of this study.\n$\\min \\frac{1}{N} \\sum_{i=1}^{N} E_{d_{i} \\in D} loss(f(w, d_{i}), y_{i})$\nModel parallelism splits the model parameters to multipleworkers to make it possible to train large-size models. Eachworker holds a subset of model parameters or layers. In everyiteration, the sampled mini-batch of datasets is copied to allworkers, and different parts of the DL model are computedon different workers. Model parallelism is also an importantarea of study, which is orthogonal to the data parallelismemphasized in this paper [14]."}, {"title": "B. Synchronization Architectures", "content": "1) Parameter Server Architecture: The PS architecture [15]\nis a straightforward method for parallel computing acrossmultiple workers (Figure 1(a)). In this architecture, a PS nodemaintains and manages a global model. During each trainingiteration, each worker computes its local gradients based onits own mini-batch and communicates these results to the PS.The PS updates the global model and synchronizes it witheach worker. Typically, there are two synchronization models:Bulk Synchronous Parallel (BSP) [16] and AsynchronousParallelism (ASP) [17]. In BSP, workers must await a synchro-nization barrier before initiating the next iteration. Conversely,ASP removes this synchronous barrier. Generally, BSP tendsto yield higher accuracy, while ASP significantly increasestraining throughput. Regardless of the synchronization model,the PS architecture remains a prevalent choice in large-scaletraining clusters.\n2) Ring-AllReduce Architecture: AllReduce (AR) is a de-centralized architecture proposed to alleviate communicationbottlenecks in the PS architecture (Figure 1(b)). AR treats allmachines as workers, thus eliminating the need for PS. RingAllReduce (RAR) stands out among AR algorithms due to itssuperior bandwidth performance. RAR splits communicationphases into ScatterReduce and AllGather. In the ScatterReducephase, each of the N workers divides their local gradientsinto N chunks. Each worker, in every iteration, transmitsa chunk to its neighbor, receives one, and adds it to thecorresponding chunk. The chunks transmitted and receivedin each iteration are different, with each worker forwardingthe chunk received in the previous iteration. After N\u22121iterations, each worker possesses a globally updated chunk.For instance, in Figure 2(a), worker 1 forwards chunk A toworker 2 in the first iteration. Worker 2 adds it to its localchunk and passes it to worker 3 in the subsequent iteration.After the ScatterReduce phase, worker 4 will possess a fullyupdated chunk A. During the AllGather phase, each workertransmits its complete chunk to the next worker and obtain-sone from the previous. Like in ScatterReduce, each workerforwards the chunk it received in the previous iteration. AfterN\u22121 iterations, every worker has fully updated gradients.As illustrated in Figure 2(b), worker 4 sends its fully updatedchunk A to worker 1, who then forwards it to the next worker(i.e., worker 2). Thus, at the end of the AllGather phase,each worker possesses a fully updated result for all chunks.Unlike PS, RAR operates solely in BSP mode. While RARachieves optimal bandwidth performance, it suffers from issuesof extensive dependency chains and vulnerability to a singlepoint of failure [18].\nC. Bring INA into DDL\nIn recent years, advancements in programmable networkshave driven a surge of research employing INA techniquesto expedite DDL training. INA leverages the computationalpower within programmable switches to aggregate gradientsfrom multiple nodes, reducing network traffic and acceler-ating DDL training. Notable works in this domain includeSwitchML [6] and ATP [7], both of which aim to enhanceoverall training speed by offloading the gradient aggregationtask to switches. In SwitchML, all gradients are aggregatedwithin the switches, thus training speed hinges significantlyon the switches' aggregation capabilities. ATP adopts a best-effort strategy for gradient aggregation, where gradients notaggregated at the switch level are relayed to the PS foraggregation.\nNevertheless, these optimization efforts are tailored specif-ically for INA within the PS architecture. The RAR architec-ture, renowned for its efficient communication performance,is gaining increased attention. To our knowledge, no existingresearch explores INA utilization within the RAR architecture,presenting a distinct set of challenges and opportunities at thecore of this study."}, {"title": "III. MOTIVATION AND CONCEPTS OF RINA", "content": "In this section, we first propose to analyze the issues withRAR methods, specifically their long dependency chains.Next, we present the Bandwidth-Occupation Model (BOM) forall existing PS-based INA methods to illustrate their problem:\nthe lack of incremental deployment capability. Finally, wepresent the design concepts and architecture of Rina, brieflyelaborating on its advantages over both the state-of-the-art PS-based INA methods and traditional RAR methods.\nA. Long Dependency Chain Problem in Ring-AllReduce\nCompared to PS-based INA, RAR does not have commu-nication bottlenecks, which are determined by the communi-cation mode of RAR. The following provides quick proof.\n1) We can view a network as a connected undirected graph.\nLet G = (V, E) be a connected undirected graph.\n2) Transform G into a directed graph D = (V, D) by\nreplacing each u, v \u2208 \u0395 with two directed edges (u, v)\nand (v, u) in D.\n3) By this transformation, for each vertex v \u2208 V, the in-degree equals the out-degree.\n4) According to the Eulerian path and circuit condi-tions [19] in directed graphs, since the in-degree equalsthe out-degree for all vertices in D, there exists anEulerian circuit.\n5) Hence, a path in D starts from any worker, visits everyworker once, and finally ends at the starting worker. Thisguarantees the RAR's requirements for communicationwithout bottleneck.\nAlthough RAR has been proven to be free of bandwidthbottleneck, it still suffers from the issue of long dependencychains [20]. The most significant problem caused by longdependency chains is that throughput performance becomesaffected by the increasing number of nodes. Take Figure 3as an example. The figure shows the workflow of an RARpipeline. The same part of the gradients shares the same color.Each node sends its computed gradient G\u2081 to the next worker.However, according to the implementation of the latest MPIlibraries such as NCCL [21] and OpenMPI [22], each roundof communication has a barrier to global synchronization.Interference caused by load fluctuation, interrupts, garbagecollection, or background tasks during Worker 3 processing G_6\nwill defer the global completion time of this step, Indicatingthat single-point failure will directly slow down the entiretraining process. This is the fundamental problem for the longdependency chain.\nSuppose the whole cluster has N workers. The systemoverhead of sending gradient (including network protocol,memory movement, et al.) chunk i is O(G_i), while thecomputation and communication time is C(G_i). For j-th roundsynchronization, the time consumption of worker n will beO(G_u) + C(G_u),u = (i + n)%N. Since the existence ofbarriers, the estimated time consumption of the ScatterReducephase will be 1\\frac{N}{\\sum_{u=1}} Max(O(G_u) + C(G_u)).\nIn practical scenarios, O(G_u) can be considered a fixedoverhead independent of N. Typically, the distribution ofC(G_u) is proportional to N in a linear fashion. We assumeC(G_u) follows a normal distribution, where its mean is pro-portional to N and the variance is a fixed value, expressed asC(G_u) \u223c N (k\u00b7N, \u03c3\u00b2). Here, k is a constant. The standarddeviation \u03c3\u03b5 is also assumed to be a constant \u03c3. For a randomvariable X that follows a normal distribution N(\u03bc, \u03c3\u00b2), theexpected maximum value M_n (taken from n independentand identically distributed samples) can be approximated asE[M_n] \u2248 \u03bc + \u03c3\u221a2lnn. Thus, the time consumption T ofRAR during the ScatterReduce phase can be expressed as:\n$T = N \\cdot O(G_{u}) + \\sum_{u=1}^{N} Max(C(G_{u}))$\n$=N \\cdot O(G_{u}) + E[max(C(G_{u}))]$\n$\u2248N \\cdot O(G_{u}) + k + N \\cdot \u03c3\\sqrt{2 ln N}$\nFrom Equation 3, it can be seen that the value of Tincreases with the size of N, which demonstrates that thesynchronization completion time of RAR increases as thenumber of nodes increases. It is also noteworthy that anincrease in \u03c3will also lead to an increase in T, which meansthat if the nodes' performance is unstable, the synchronizationcompletion time of RAR will be further prolonged. Thisphenomenon is common [15], [23], [24], thus reducing thelong dependency chain of RAR is highly necessary.\nAs a widely used AllReduce method in the industry, HybridAllreduce (H-AR) [25] has been proposed to address the issueof long dependency chains through a multi-step AllReduceprocess. H-AR first performs a ScatterReduce within the ToR,then an AllReduce between ToRs, and finally an AllGatherwithin the ToR. This approach indeed mitigates the longdependency chain problem and achieves better performancethan RAR. However, Rina's utilization of INA switches canachieve even higher throughput compared to H-AR, for Rinanot only mitigates the dependency chain but also provides in-network computation capabilities. A detailed comparison isprovided in \u00a7 VI-B.\nB. Modeling PS-based INA with BOM\nA major advantage of the PS-based INA approaches inimproving the throughput of DDL training tasks is its abilityto reduce network traffic [9]. In this section, we quantifythis through the BOM model and discuss their weakness inincremental implementation.\nAssumptions: The DDL training cluster uses the BSP syn-chronization algorithm. All nodes need to send the gradientsof their local models to the PS synchronously, followed bya broadcast generated by the PS. The INA switch can fullyaggregate incoming traffic (as proven to be feasible in INAl-loc [8] under the single-job scenario). If the correspondingPS-based INA method does not require a PS server, thefarthest INA switch is treated as the PS. The entire topologyis homogeneous, with a link bandwidth of B_0. There is nomultipath scenario in the topology, that is, there is exactlyone path from all nodes to the PS.\nLemma 1: For a topology only containing regular switchesand n workers, the worker throughput is 1/n.\nAs shown in the sub-topology T\u2081 in Figure 4, this topologydoes not include any INA switches. Assuming the throughputof its outbound switch 2 is B_1, the worker throughput in thistopology is B_1/4. The proof is as follows.\nAssume a complex topology T. From the topology T, weselect the PS node working as the root to build a DirectedAcyclic Graph (DAG) tree, which can be used to representthe network traffic during the gradient aggregation phase.\nGiven a DAG tree G = (V, E), where V is the set of vertices(or nodes) and E is the set of directed edges. G is a subtree ofT. The root node is denoted as r and L is the set of leaf nodes.The output rate of a node v, denoted as OR(v), is defined asthe number of outgoing edges from v.\nWe aim to prove that \u2200l \u2208 L, OR(l) is determined by theoutput rate of the root node r, OR(r). To do this, we use theprinciple of mathematical induction.\nBase case: When |V| = 1 (i.e., the tree only containsthe root), it's trivial that OR(l) depends on OR(r) since they areidentical.\nAdditional case: We select a non-leaf node v in inductivestep. When |V| = 2, no leaf nodes exist, thus only one ofthe leaf nodes can be selected arbitrarily. In this scenario,OR(1) = OR(r) is also evident.\nInductive step: Assume the proposition holds for any treewith |V| = n, i.e., for any tree with n nodes, Vl \u2208 L, OR(1)is determined by OR(r).\nWe need to prove that for any tree with |V| = n+1, \u2200l \u2208 L,OR(1) still depends on OR(r).\nConsider a tree G with |V| = n + 1. Select a non-leaf nodev (except r) and consider the sub-tree G' formed by removingone of v's child nodes c (and edges attached to c). Now G'\nhas n nodes.\nBy the induction hypothesis, VI' \u2208 L' (leaf nodes in G'),OR(l') depends on OR(r). Since removing the child c of vdoesn't change OR(r), it still holds that \u2200l' \u2208 L', OR(l')depends on OR(r).\nFor the removed node c, since it was an outgoing edge fromvand eventually from r, OR(c) also depends on OR(r).\nHence, we have shown that for any tree G with |V| = n+1,Vl\u2208 L, OR(l) is determined by OR(r). We can conclude thatfor any topology only containing regular switches, the outputrate of each worker is determined by the number of outgoingbandwidth from the root, which is OR(r)/W. W representsthe number of workers\nLemma 2: The INA switch and its children can be viewedas one worker.\nINA switches can aggregate the received gradients andoutput dataflow without redundant positional gradients. In thisway, to the parent node of this INA switch (no matter theother INA switch or PS), its behavior is manifested as anindependent worker.\nLemma 3: For an INA switch, the actual throughputdepends on the worst-performing child.\nTake Figure 4 as an example. For the outbound INA switch3 in topology T2, even though it has sufficient INA capabilitiesto allow workers 1 and 2 to function at 100% throughput, itis still limited by the slowest child node (topology T\u2081). Thisimplies that the actual outbound bandwidth of topology T2is B_0/4, which is also the actual global throughput of thisexample.\nAt this point, given the topology, nodes, and placement ofINA switches, we can calculate the actual throughput of all"}, {"title": "C. Incremental Deployment is Challenging for PS-based INA", "content": "The incremental deployment capability of PS-based INAmethods is poor. Using BOM, we evaluate the changes inDDL training throughput in a specific topology scenario,starting from \"all switches are regular switches\" to replacingall switches in the topology with INA switches.\nThe training task we selected is ResNet50 [26], usingthe CIFAR-10 [27] dataset. We have chosen two topolo-gies, namely the standard Fat-tree [28] (k=4) and standardDragonfly [29] (a=4, g=9, and h=2) topologies, which arecommonly used in data centers. The corresponding results areshown in Figure 5. As can be seen, to achieve significantthroughput improvements, PS-based INA methods need toreplace regular switches in the entire network with high-performance programmable switches as much as possible. Ifonly a part of the switches is replaced, the effect of INAcannot be well-utilized. Therefore, designing an incrementaldeployment-friendly DDL synchronization architecture is oneof the important design principles of Rina.\nD. Rina Design Concepts and Challenges\nBefore introducing details of Rina, we summarize the ex-isting PS-based INA methods from a higher perspective. Theexisting PS-based INA involves the INA device wrapping allits attached devices into a unified external device. From theviewpoint of other devices after the INA device, the INA andits workers are combined as a larger-scale worker."}, {"title": "IV. DESIGN DETAILS OF RINA", "content": "Rina is designed to incorporate the INA switch into DDLtraining tasks using the RAR synchronous architecture as abasis. Rina not only retains the benefits of the RAR architec-ture, including the absence of communication bottlenecks, butit also effectively mitigates the issue of the long dependencychain. Essentially, Rina upholds the RAR workflow pattern,which includes the ScatterReduce and AllGather phases, andoptimizes these stages to leverage the capabilities of INAdevices. Moreover, Rina introduces a new workflow basedon the agent-worker model, discussed in \u00a7 IV-A. This modelallows INA devices to manage all workers within each rack,as detailed in \u00a7 IV-B. We implement a lightweight congestioncontrol protocol to meet the unique requirements of Rina andits utilization of INA capabilities. Furthermore, when com-pared to PS-based INA, Rina provides superior incrementaldeployment capabilities. We discuss in detail in \u00a7 IV-D howRina achieves incremental deployment capability and howincremental deployment is carried out.\nA. Agent-worker Model\nFigure 6 presents a comprehensive illustration of the agent-worker model. Rina adopts the rack as its operative unit(i.e., an abstracted worker), superseding each rack's ToRswitch with an INA switch to enable the INA capability.\nWhen viewed from an individual rack's perspective, once itsToR switch is supplanted by an INA switch, the worker ofthe lowest rank within that rack assumes the responsibility ofmanaging the rack's INA switch and its workers. This low-rank worker is termed the \u201cagent\u201d. In addition to performingcomputational tasks, the agent also performs several additionalfunctions, such as initiating Rina, assigning tasks to the INAswitch, and provocation of synchronizations for other workers.For real-world implementations, these agent roles are executedvia an extra daemon program that runs on one of the workersin the group (usually the first worker).\nB. Rina's Architecture, Workflow, and Dataflow\nAs illustrated in Figure 7, Rina is an architecture facilitatingthe harmonious operation of regular and INA switches. If arack with an INA-enabled ToR switch exceeds two nodes,Rina can be deployed, designating such a rack as an abstractedworker and Rina-enabled rack. Rather than assigning tasksindividually, Rina adopts a group-based approach, consider-ing Rina-enabled racks as an abstracted worker. Conversely,for Rina-disabled racks, each worker is regarded as an au-tonomous worker or an autonomous group.\n1) Model and Dataset Partitioning: Data-parallel DDLtraining usually necessitates dataset partitioning to attainparallelism, with the addition of model splitting by RARto guarantee synchronization throughput. Consequently, Rinacalls for meticulous consideration in the partitioning of dataand models. In both PS and RAR architectures, when allworkers share equivalent computational capacity, an equaldataset segment is allocated to each worker to approximateglobal computation time. Conversely, with workers possessingheterogeneous computational capabilities, strategies such asbatch-size tuning [30], [31] are employed to synchronizecomputation time.\nRegarding model partitioning during synchronization, theconventional RAR strategy divides the model evenly acrossworkers to ensure near-equal synchronization time per step(refer to \u00a7 II-B). However, Rina necessitates model partitioningin line with the number of groups. This stipulation arisesprimarily because each Rina-enabled rack, represented as anindependent worker by its respective agent and INA switch,must be accounted for.\nAt the onset of training, both the dataset and model par-titions are disseminated to ensure uniform initial parameterstates across all workers. We designate a global control node,which will be the agent of the Oth group or Oth autonomousworker. As training commences, this node randomizes allparameters of the target model and transmits the partition dataof both the dataset and model to all groups (Figure 7-(0). Toexpedite the distribution of control messages within a Rina-enabled rack, Rina utilizes multicast. Once this preparatoryphase is complete, all workers embark on their training tasks.\n2) Synchronization Process: Before each round of DDLtraining, synchronization is necessitated for all workers tosynchronize training results based on each node's respectivedataset. This periodic process aligns well with near-equalcomputation times across nodes, enabling all workers tocommence synchronization approximately simultaneously. Asin the RAR system, upon completion of computation, eachnode promptly transmits its model partition's gradient to thesubsequent worker while concurrently receiving the gradientfrom the prior worker and conducting local aggregation. Thisattribute is retained in Rina. For autonomous workers, theiractions mirror those in RAR. Meanwhile, within Rina-enabledracks managed by the agent, the agent handles task delegation.\nSimilar to RAR, Rina is divided into two phases: Scat-terReduce and AllGather. During the ScatterReduce phase, allnodes undergo a synchronization round, ensuring each nodeacquires the final computation result for a model gradientsegment. This gradient portion is then broadcasted to all nodesin the AllGather phase. Participation of the INA switch inaggregation is necessary for ScatterReduce, while AllGathernessitates switch support in multicast.\nIn a cluster comprising N nodes, these two phases wouldnecessitate 2(N \u2013 1) steps in the RAR system. In contrast, inRina with G groups, each phase demands 2G \u2013 1 steps. Giventhat a group may include several workers, in a cluster whereeach rack hosts eight computing nodes, G would equate toN/8. Thus, the synchronization steps demanded by Rina arenotably fewer than those required in RAR, which contributesto a higher throughput.\n3) ScatterReduce Phase: Given the inability of prevalent P4programmable switches such as Tofino-1 [32] to autonomouslygenerate packets, and considering synchronization require-ments, the synchronization signals of workers are uniformlytriggered by the agent for each group.\nReferring to Figure 7, upon the agent's completion of itscurrent computation round, it relays aggregation task data tothe switch (1). This information comprises the model rangeand size destined for the next group, the reserved memoryspace in the switch, and the ID of the node currently engagedin the aggregation. On receipt of this data, the switch convertsthis packet into a data pull message (\u2461), which is multicast toall workers (including the agent) under this rack. Triggered bythe pull message, all nodes initiate the transmission of corre-sponding gradients to the ToR switch (\u2462). These parametersare then aggregated at the ToR switch and dispatched to theagent of the subsequent group (4 and 5). When the next"}, {"title": "C. Congestion Control and Reliability", "content": "1) Congestion Control: The implementation of effectivecongestion control can mitigate network congestion and con-trol the memory bottleneck of INA switches. Our Rina designsdistinct congestion control mechanisms for ScatterReduce andAllGather. Given that the INA switch and the node maintain aone-hop distance in Rina, the congestion control scheme canremain straightforward. Nonetheless, it necessitates cross-rackcongestion prevention measures to avert substantial packetloss.\nAs illustrated in Figure 7, congestion control initiates when(\u2461) prompts workers in the Rina-enabled rack to deliver gradi-ent shards to the INA switch. During the ScatterReduce phase,workers transmit at full speed 1, paralleling DCQCN's con-gestion control [34]. Once the aggregate message reaches thefollowing rack's agent, it returns an ACK. At this stage, con-gestion control employs an Additive Increase/MultiplicativeDecrease (AIMD) approach, ceasing window expansion uponreaching \"full speed\".\nDuring the AllGather phase, as other gradient shards'ScatterReduce phase might not have concluded, step (4)in Figure 7 monopolizes the agent's downstream bandwidth.Consequently, the sending rate at this juncture starts from zero.The ACK is issued either by the autonomous worker or theagent of the abstracted worker.\n2) Reliability: Despite recent methods' inability to manageworker errors, the PS-based INA method's central managementnode offers a significant advantage. It allows for active nodeexclusion upon error detection, enhancing overall reliability.Such a feature is lacking in the RAR approach due to its Peer-to-Peer (P2P) architecture. The P2P nature of RAR results indistifficulties in making node replacement upon error detectionchallenging. However, the architecture of Rina is designed toeffectively manage errors. We classify node errors into two cat-egories: agent errors and worker errors. Upon the occurrenceof an agent error, the other workers in the corresponding rack"}, {"title": "D. Incremental Deployment", "content": "Rina's agent-worker model can integrate all nodes underan in-network computing switch into a single worker. Thismeans that replacing a conventional ToR switch connecting Nnodes with an INA switch can reduce the length of the RARdependency chain by N. Therefore, in the initial deploymentphase, we should prioritize replacing normal switches with themost connected workers with INA switches.\nAs deployment progresses, we should consider replacingother normal switches in the topology that are not TORswitches. Refer to \u00a7 III-B, constructing a minimum spanningtree rooted at a specific worker node that connects all workernodes. Then, by treating an INA switch and all its downstreamworker nodes as a single worker, we can similarly replace theconventional switch with the most downstream nodes with anINA switch.\nThrough the gradual replacement of standard switches withthose developed using our method, we can achieve effectiveincremental deployment. Each instance of switch replacementleads to a noticeable enhancement in the throughput of DDtasks. A thorough evaluation of our incremental deploymentcapabilities is provided in Section VI-C."}, {"title": "V. IMPLEMENTATION", "content": "We implement the Rina prototype on both P4 programmableswitches [32", "Switch": "Given the absence offloating-point computation capabilities in P4 switches", "7": "where all floating-point numbers are multiplied by an integer and then convertedto integers at the worker nodes. This conversion enables theP4 switch to transform floating-point addition into a simplerinteger addition operation. Moreover"}, {"7": "Rinaallocates contiguous memory space directly for INA tasks.To augment the INA throughput", "Worker": "We implement a Rina prototype as middleware,seamlessly integrated into PyTorch. By utilizing UDP, we"}]}