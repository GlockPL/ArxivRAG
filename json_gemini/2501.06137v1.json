{"title": "Supervision policies can shape long-term risk management in general-purpose AI models", "authors": ["Manuel Cebrian", "Emilia G\u00f3mez", "David Fern\u00e1ndez Llorca"], "abstract": "The rapid proliferation and deployment of General-Purpose AI (GPAI) models, including large language models (LLMs), present unprecedented challenges for AI supervisory entities. We hypothesise that these entities will need to navigate an emergent ecosystem of risk and incident reporting, likely to exceed their supervision capacity. To investigate this, we develop a simulation framework parametrised by features extracted from the diverse landscape of risk, incident or hazard reporting ecosystems including community-driven platforms, crowdsourcing initiatives, and expert assessments. We evaluate four supervision policies: non-prioritised (first-come, first-served), random selection, priority-based (addressing the highest-priority risks first), and diversity-prioritised (balancing high-priority risks with comprehensive coverage across risk types). Our results indicate that while priority-based and diversity-prioritised policies are more effective at mitigating high-impact risks particularly those identified by experts they may inadvertently neglect systemic issues reported by the broader community. This oversight can create feedback loops that amplify certain types of reporting while discouraging others, leading to a skewed perception of the overall risk landscape. We validate our simulation results with several real-world datasets, including one with over a million ChatGPT interactions, including more than 150K conversations identified as risky. This validation underscores the complex trade-offs inherent in AI risk supervision and highlights how the choice of risk management policies can shape the future landscape of AI risks across diverse GPAI models used in society.", "sections": [{"title": "INTRODUCTION", "content": "The rapid adoption of GPAI models\u2014including but not limited to LLMs- -across various applications has introduced significant opportunities and unprecedented challenges. These models, capable of generating human-like text and handling a wide range of tasks across diverse data types, are increasingly integrated into everyday systems, raising key concerns related to cybersecurity, bias, privacy violations, misinformation, and the misuse of AI to generate harmful content [1-5].\nIn response to these risks, a multi-layered ecosystem of risk, incident and hazard reporting mechanisms has emerged, encompassing community-driven platforms, crowdsourcing initiatives, and expert-led assessments. Community-driven platforms, such as Reddit and Discord, allow users to collectively identify and flag potential issues in real time, offering diverse perspectives on emerging risks. Crowdsourcing initiatives, like those featured in DEF CON's Generative Red Team AI and OpenAI's Preparedness Challenge, mobilise large groups of individuals to rigorously test AI systems for vulnerabilities. For instance, the AI Village at DEF CON 31 hosted the largest-ever public Generative AI Red Team event, engaging over 2,000 participants to evaluate models from leading AI organisations [6]. Similarly, OpenAI's Preparedness Challenge invited the public to identify potential risks in AI systems, offering incentives such as API credits for top submissions [7]. These initiatives not only enhance the robustness of AI models but also foster a collaborative approach to AI safety, integrating insights from diverse stakeholders to proactively address potential threats.\nWe hypothesise that supervisory entities will need to increase their focus on AI risk management as the volume of risk reports on GPAI models continues to grow. This influx of reports could, over time, challenge the capacity of these entities to address them comprehensively, making the development of efficient strategies for prioritising and addressing risks essential. A comparable situation has been documented in the context of General Data Protection Regulation (GDPR) enforcement in the European Union. The Irish Council for Civil Liberties has reported on the cost and work load linked to GDPR enforcement [8]. Dreschsler signals that the empowerment of individuals is thus to an extent stunted by the ability of the enforcement structure to cope with the requests [9]. While the circumstances surrounding GDPR enforcement are varied and nuanced, this example helps to illustrate the kinds of challenges that supervisory authorities might encounter as they work to ensure comprehensive oversight of growing AI-related risks.\nOur study examines how supervisory entities can process and prioritise these incoming AI risk reports. In the context of this paper, the terms 'risk' and 'risk reporting' are used broadly to encompass not only potential risks, but also incidents and hazards [10] associated with GPAI models. As the number of reports likely exceeds the capacity for thorough investigation and mitigation, the policies guiding the selection and prioritisation of risks"}, {"title": "FORECASTING FUTURE AI INCIDENTS", "content": "To project the trajectory of AI incidents, we analysed data from the OECD AI Incidents Monitor (AIM) [16], which covers AI incidents and hazards from January 2018 to October 2024, including incidents caused by GPAI models as well as other types of AI systems or AI application contexts. Initial incident levels averaged approximately 20 per month in 2018, rising sharply in parallel to the proliferation of GPAI models. By late 2023, monthly incidents exceeded 500, with peaks reaching 844 in early 2024, underscoring the rapid escalation of AI-related risks alongside intensified deployment (the code used for these projections is available in the Code and Data Availability section).\nUsing exponential smoothing, we developed three scenarios to forecast incidents through 2030. In the worst-case scenario, unchecked AI adoption could lead to exponential growth, with incidents potentially surpassing 2,500 per month by 2030, posing significant regulatory challenges. The average-case scenario suggests moderate growth, with incidents reaching around 1,000 per month by 2030, indicating that regulation can slow growth but may not fully contain it. The best-case scenario projects a decline in incidents post-2025 under strong regulatory oversight, potentially reducing incidents below current levels by 2030. These figures correspond to global incidents, so the values for a specific AI supervisory entity should be adapted to the specific jurisdictional scope of that entity.\nIn addition to overall incident rates, we examined the projected evolution across key AI principles, such as Privacy, Robustness, Transparency, and Fairness. Incidents related to Robustness & Digital Security are expected to see the sharpest increase, potentially exceeding 1,000 monthly cases by 2030, underscoring significant security concerns. Incidents associated with Transparency & Explainability and Accountability are also projected to grow steadily, highlighting the need for governance frameworks that prioritise these principles. Meanwhile, issues related to Privacy and Human Rights are projected to grow moderately, suggesting on-going but contained challenges as AI systems integrate with sensitive societal contexts.\nTo further understand the dynamics of AI risks, we examined the evolution of jailbreak prompts, a specific category of adversarial prompts used to bypass LLM safeguards. Our analysis is based on data collected by Shen et al. [17], which includes over 1,400 jailbreak prompts spanning December 2022 to December 2023. The study reveals that these prompts, originating from various online communities, have grown in sophistication and frequency, often exploiting vulnerabilities through prompt injection and privilege escalation techniques.\nA pivotal event occurred in June 2023, when an update to OpenAI's ChatGPT made it more challenging for users to execute jailbreak prompts [15]. This intervention can be seen as a form of corporate regulatory action, analogous to how policy-driven regulations might constrain AI misuse. Figure 2 shows two scenarios: one projecting the upward trajectory of jailbreak prompts if no intervention had occurred and another reflecting the observed decrease after the June update. Without the intervention, jailbreaks might have continued to proliferate, illustrating the tangible impact that regulatory measures-whether corporate or governmental-can have on curbing AI misuse.\nThese projections, segmented by scenario and princi-"}, {"title": "SIMULATION FRAMEWORK", "content": "Our study employs a simulation framework to compare four supervision policies for processing GPAI models risk reports: (1) non-prioritised, (2) random, (3) priority-based, and (4) diversity-prioritised. The non-prioritised policy serves as a baseline, representing a first-come, first-served approach. While simple to implement, it may not efficiently address critical risks.\nThe random policy, which processes reports in a randomised order, serves as an important control condition. This approach helps to isolate the effects of structured prioritisation strategies by providing a comparison point that avoids systematic biases while still differing from the chronological processing of the non-prioritised policy. The random policy can be particularly useful in assessing whether observed differences in outcomes are due to intentional prioritisation or simply the result of processing reports in a non-chronological order.\nThe priority-based policy, inspired by traditional risk management practices, strategically addresses the most significant risks (in terms of accessibility or damage) first.\nThe diversity-prioritised policy aims to balance prioritisation with comprehensive coverage across different risk types, addressing the interconnected nature of GPAI risks and preventing potential blind spots in mitigation efforts [13, 18, 19].\nFull details of the simulation framework, including"}, {"title": "Report Prioritisation", "content": "We define a set of reports R = {r\u2081,r\u2082,...,r\u2099} generated over the simulation period, where each report r\u1d62 captures essential attributes relevant to AI risk supervision.\nEach report has an arrival time t\u1d62, indicating when the risk report becomes available for review. The supervision cost s\u1d62 represents the resources (e.g., time, expertise, computational power) needed to assess and address the reported risk adequately, with higher costs reflecting more complex risks requiring specialised attention.\nAccessibility a\u1d62 reflects how easily a reported risk could be encountered or exploited, with values ranging from 0 to 1 [20]. Accessibility values differ by source, following distinct Beta distributions that characterise the accessibility patterns of each source. Community reports typically yield higher accessibility values, as they are generated from a Beta(5, 2) distribution, indicating issues frequently encountered by general users. Crowdsourced reports, with a Beta(3,3) distribution, provide a more balanced accessibility range, while expert reports, following a Beta(2,5) distribution, tend to have lower accessibility values, reflecting more specialised, less commonly encountered risks.\nThe potential damage d\u1d62, which quantifies the severity of the negative impact if the risk were to materialise, has a broader range due to its distribution and scaling based on the source. Potential damage is modelled using a Pareto distribution, with different scaling factors to capture the expected impact levels from each source. Community reports are scaled by 100, representing lower typical damage, while crowdsourced reports are scaled by 200, allowing for a broader potential damage spectrum. Expert reports, scaled by 500, reflect the higher potential for serious consequences in risks identified by experts, emphasizing their systemic importance even if encountered less frequently. The Pareto distribution for potential damage mirrors the long-tailed severity profiles noted in large-scale red-teaming and crowdsourcing initiatives [21, 22], where a small fraction of discovered issues accounts for disproportionately large risks.\nTogether, accessibility and potential damage inform the prioritisation of each report, as captured by the priority score p\u1d62, calculated as:\np\u1d62 = log(1 + a\u1d62d\u1d62)\nThis scoring function combines accessibility and potential damage, giving prominence to high-accessibility,"}, {"title": "Report Generation", "content": "Report generation is modeled as a Poisson process for each source:\nN\u209b\u1d63c(t) \u223c Poisson(\u03bb\u209b\u1d63ct)\nwhere the parameters were empirically set to \u03bbcom = 25, \u03bbcrd = 12, and \u03bbexp = 5 per month as explained below (an illustrative figure showing the Poisson distributions for each source type is available in Figure 12 in the supplementary information). We chose a Poisson process as we assume events (in this case, reports) occur independently of each other, which is a reasonable assumption for diverse sources of GPAI model risk reports. The specific rates (\u03bbsrc) for each source were chosen based on careful consideration of the nature of each reporting channel. For community sources, we set a high rate (\u03bbcom = 25) to reflect the large number of users of GPAI models and the ease of submitting reports through platforms like social media or dedicated forums [17, 23, 24]. This high volume is expected to cover a wide range of issues from user experience to observed biases.\nCrowdsourced reports are assigned a moderate rate (\u03bbcrd = 12), representing organised efforts to identify risks but with fewer participants than the general community [21, 22]. These reports are expected to be more focused than community reports but more numerous than expert analyses.\nFor expert sources, we assume a lower rate (\u03bbexp = 5), considering the limited number of AI safety experts and the time-intensive nature of their analyses [25, 26]. While fewer in number, these reports are likely to be more comprehensive and to address more complex, systemic risks [27, 28]."}, {"title": "Parameter Distributions", "content": "The supervision cost s\u1d62 is modeled using a log-normal distribution because it naturally captures the right-skewed nature of resource requirements typically seen in risk assessment tasks. The parameters are set to reflect increasing complexity from community to expert sources. Community reports (\u00b5com = 1.5, \u03c3com = 0.5) are assumed to require less time and expertise to process, while expert reports (\u00b5exp = 3.0, \u03c3exp = 0.7) are more resource-intensive due to their typically more complex and technical nature. Crowdsourced reports fall in between (\u00b5crd = 2.0, \u03c3crd = 0.6).\nAccessibility a\u1d62 is modeled using a beta distribution. Community reports are given parameters (\u03b1com = 5, \u03b2com = 2) that skew towards higher accessibility, reflecting the idea that issues reported by general users are often more readily observable or exploitable. Expert reports, with parameters (\u03b1exp = 2, \u03b2exp = 5), tend towards lower accessibility, representing more obscure or complex vulnerabilities that might be harder to exploit but potentially more serious. Crowdsourced reports (\u03b1crd = 3, \u03b2crd = 3) are modeled with a symmetric distribution, representing a balance between the two extremes. The Beta distributions chosen to represent accessibility reflect skewed patterns observed in community-driven vulnerability identification efforts, such as those studied by Shen et al. [17], where frequently reported exploits cluster in more accessible regions.\nPotential damage d\u1d62 follows a Pareto distribution, which is commonly used to model the distribution of large events in complex systems. This choice reflects the \"long-tail\" nature of AI risks, where most issues might have relatively low impact, but a few could have extreme consequences. The parameters are set to model increasing potential damage from community to expert sources. Community reports have the highest shape parameter (\u03b1com = 3) and lowest scale (kcom = 100), representing mostly lower-impact issues with fewer extreme cases [23]. Expert reports have the lowest shape parameter (\u03b1exp = 1.5) and highest scale (kexp = 500), modeling a higher likelihood of identifying severe, high-impact risks [29]. Crowdsourced reports again fall in the middle (\u03b1crd = 2, kcrd = 200)."}, {"title": "Source-Based Risk Type Assignment and Distribution", "content": "In our framework, risk type assignment is closely tied to the report source, capturing the different perspectives and priorities of community users, crowdsourced contributors, and expert assessors. For community sources, we assign higher probabilities to risk types likely to be encountered and recognised by general users. For example, issues related to \"user experience\" have a high probability (0.3) for community-sourced reports, reflecting the tendency of everyday users to notice and report problems that directly impact their interactions with GPAI models, such as interface difficulties, unexpected model responses, or perceived biases in outputs [23].\nIn contrast, crowdsourced reports are given a more balanced distribution across risk types but with slightly higher probabilities for categories that align with common concerns within the AI ethics community [21]. This balance reflects the nature of crowdsourcing efforts, which typically involve individuals with varying levels of expertise and specific interests in AI safety [22]. This distribution approach recognises that crowdsourced contributors may bring a mixture of general observations and more targeted insights into AI-related risks.\nExpert sources, on the other hand, are assigned higher probabilities for technical and long-term risk categories. For instance, the probability that an expert report will address AI alignment issues is set to 0.2, capturing the tendency of AI researchers and ethicists to focus on fundamental challenges related to ensuring AI behavior aligns with human values and intentions [30]. This higher likelihood of focusing on alignment and other deep technical issues underscores the unique insights that experts bring to understanding complex, systemic AI risks. Full details of the risk type assignment probabilities by source are available in the simulation code, as noted in the Code and Data Availability section.\nThe risk types in our framework cover key areas vital to comprehensive AI risk management. Privacy risks address concerns around misuse or unauthorised exposure of personal data in interactions with the GPAI model. Security risks focus on vulnerabilities that could disrupt or compromise AI systems. Bias risks involve potential unfair or discriminatory outputs from language models, which may mirror societal biases. Ethical and alignment risks emphasise ensuring AI behaviour aligns with human values and goals [30]. Robustness evaluates the system's resilience across various conditions, while content moderation and misinformation risks concern the generation of harmful or misleading content. Each risk type has a source-specific distribution, supporting a realistic simulation of community, crowdsourced, and expert reporting patterns. Full probabilistic details for these distributions are available in the simulation code."}, {"title": "Simulation Parameters", "content": "The simulation parameters are designed to model the operations of an AI supervisory entity over an extended period. During each time step t, which corresponds to a month, the entity processes reports generated by various sources. The total simulation duration is T, with an initial observation period Tobs. During this observation period, the entity collects data to understand the volume, complexity, and nature of incoming reports. This initial phase helps the entity calibrate its processing and resource allocation strategies.\nThe processing capacity per month, Cm, is calculated based on the average total supervision cost of the reports received during the observation period. The reports received in each month of the observation period are denoted as Rt, where Rt is the set of reports generated at time t. Therefore, Cm is defined as:\nCm = Co * (1/Tobs) * \u03a3 [\u03a3 s\u1d62 for t=1 to Tobs, r\u1d62 \u2208 R\u209c]\nSecondly, setting Co < 1 creates a challenging scenario where efficient prioritisation becomes crucial for managing the inevitable backlog, mirroring the resource pressures often faced by regulatory bodies. This scenario is reminiscent of the backlog issues encountered in the implementation of the General Data Protection Regulation (GDPR), where regulatory agencies frequently face limited resources relative to the volume of compliance reports and complaints [8, 9]. Such constraints necessitate efficient prioritisation to address high-impact cases effectively."}, {"title": "Supervision Policies", "content": "We formalise the four distinct supervision policies:\nNon-Prioritised (NP):\nPNP = {r\u1d62 \u2208 Rt : \u03a3 s\u1d62 \u2264 Cm for r\u1d62 \u2208 R\u209c}\nRandom (RD):\nPRD = RS(Rt, nt), nt = max \u03b7: \u03a3 s\u1d62 \u2264 Cm for r\u1d62 \u2208 R\u209c\nPriority-based (PB):\nPPB = arg max \u03a3p\u1d62, for S \u2286 R\u209c, \u03a3 s\u1d62 \u2264 Cm\nDiversity-prioritised (DP):\nPDP = arg max \u03a3p\u1d62\u03b4\u1d62(S), for S \u2286 R\u209c, \u03a3 s\u1d62 \u2264 Cm"}, {"title": "ILLUSTRATION OF A NO-PRIORITISATION RUN", "content": "To better understand the dynamics of GPAI model risk report processing under different policies, we first examine a typical run using the no-prioritisation approach. This policy processes reports in the order they are received, without considering their potential impact or urgency. Figure 3 presents a view of the simulation results over a 15-month period, with the first 3 months serving as an observation period to calibrate the monthly processing capacity.\nThe top four graphs in Figure 3 illustrate the characteristics of processed reports over time. The Supervision Cost graph shows significant variability, with occasional spikes indicating reports that require substantial resources to process. This variability highlights the challenges in resource allocation when reports are processed without prioritisation. The Accessibility and Potential Damage graphs similarly display considerable fluctuations, suggesting that high-impact or easily exploitable risks are not consistently addressed in a timely manner under this policy.\nThe Priority graph, which combines Accessibility and Potential Damage, further emphasises the lack of strategic processing. High-priority reports (indicated by peaks in the graph) are interspersed with lower-priority ones, reflecting the chronological processing approach of the no-prioritisation policy.\nThe fifth and sixth graphs depict the distribution of"}, {"title": "ILLUSTRATION OF A PRIORITY-BASED RUN", "content": "To contrast with the non-prioritised approach, we now examine a typical run using the priority-based policy for GPAI model risk report processing. This policy selects reports based on their calculated priority scores, aiming to address the most critical risks first. Figure 4 presents the simulation results over the same 15-month period as the non-prioritised example.\nThe top four graphs in Figure 4 reveal several key differences. First, the Supervision Cost shows a broader range of values with more frequent spikes compared to the non-prioritised run. This indicates that the priority-based policy often processes reports with higher supervision costs, likely focusing on more complex issues.\nIn the Potential Damage graph, there is a noticeable trend towards processing reports with higher potential damage, which is less apparent in the non-prioritised run. This reflects the policy's effectiveness in prioritising high-risk reports, addressing them earlier and more consistently.\nThe Priority graph also shows a stark difference, as the priority-based approach consistently focuses on higher-priority reports throughout the simulation. Unlike the non-prioritised run, where reports were processed based on arrival time, here we see a clear bias towards high-priority items, with very few low-priority reports being processed.\nThe Source Distribution remains similar, but there is a slight increase in the proportion of expert and crowdsourcing reports being processed, compared to the dominance of community reports in the non-prioritised run. This shift suggests that the priority-based policy gives more weight to reports from sources that may identify higher-priority risks."}, {"title": "COMPARISON ACROSS PRIORITISATION POLICIES", "content": "We present results from one batch of 100 simulations for each of the four policies: non-prioritised, random, priority-based, and diversity-prioritised. Each simulation was run with standard parameters: a 15-month duration (including a 3-month initial observation period) and a monthly processing capacity set to 50% of the average total supervision cost observed during this period. Qualitatively similar results were observed in a second batch of simulations.\nFigure 5 illustrates the distribution of key report characteristics across the four policies. The figure consists of four histograms, each representing one of our key metrics: Supervision Cost, Accessibility, Potential Damage, and Priority.\nThe Supervision Cost histogram (top left) shows a right-skewed distribution for all policies, with the priority-based and diversity-prioritisedapproaches showing longer tails towards higher costs. The mean Supervision Cost was highest for the priority-based approach (11.18), followed by diversity-prioritised (9.67), non-prioritised (8.60), and random (8.17). The Kruskal-Wallis H-test confirmed a statistically significant difference in Supervision Cost across policies (H=54.42, p < 0.01), indicating that prioritisation methods tend to select more complex or time-consuming reports.\nIn terms of Accessibility (top right), differences between policies were more subtle. The random approach had the highest mean accessibility (0.6150), while the priority-based approach had the lowest (0.5741). The Kruskal-Wallis H-test revealed a statistically significant difference across policies (H=15.94, p < 0.01), although the variation is less pronounced than in other metrics.\nThe Potential Damage histogram (bottom left) reveals"}, {"title": "FEEDBACK LOOPS IN REGULATORY PRIORITISATION", "content": "We can now model the feedback loops that influence the incentives for reporting and the deterrence of risks over time. These feedback loops help forecast the long-term prioritisation dynamics under different supervision policies, particularly the priority-based and diversity-prioritised approaches. These policies may unintentionally amplify expert reporting while under-addressing other sources and risks, driven by mechanisms of incentives and deterrence. For instance, most of the obligations of the EU AI Act focus on high-risk AI systems and GPAI models with systemic risks. This focus may result in increased reporting from experts in areas related to those systems and models, which could potentially lead to fewer resources being allocated to monitor lower-risk applications that might also pose significant concerns.\nWe start by modelling the incentives for each reporting source-community, crowdsourcing, and expert-which adjust based on the proportion of their reports that are processed. The incentive level for a source Is(t) at time t evolves as follows:\nIs(t + 1) = clip (Is(t) + \u03b3\u03b9 (\u03c0s(t) \u2013 \u03b21) \u00b7 Is(t), Imin, Imax)\nwhere \u03c0\u03c2(t) represents the processing rate of reports from source s, \u03b3\u03b9 controls how quickly incentives adjust, and \u03b2r is the expected processing rate. The minimum and maximum incentive values, Imin and Imax, constrain incentive levels within realistic bounds. Specifically, Imin = 0.5 ensures incentives do not drop to a discouraging level, while Imax = 1.5 prevents incentives from escalating unsustainably. Setting these bounds maintains a balanced reporting system by keeping incentives within manageable limits.\nSimultaneously, the occurrence rate of each risk type Ort(t) changes depending on how effectively it is addressed. Processed reports reduce the occurrence rate,"}, {"title": "APPLICATION TO REAL-WORLD DATA", "content": "To validate our simulation framework and explore its practical implications, we applied our methodology to the WildChat dataset [15], which comprises one million conversations between users and ChatGPT (versions 3.5 and 4). The dataset compilation spanned from April 9, 2023, to April 29, 2024, capturing over a year of real-world interactions with prominent large language models. WildChat includes more than 2.5 million interaction turns in 68 different languages, encompassing diverse user intents and interactions.\nFor this analysis, we focused on the subset of conversations identified as containing toxic content, totalling 152,296 conversations. These instances are particularly relevant for studying risk reporting and mitigation mechanisms in GPAI models, as they represent potential risks that supervisory entities need to manage.\nWe mapped the dataset's attributes to our simulation parameters supervision cost (si), accessibility (ai), potential damage (di), priority score (pi), and risk type (rti) as follows.\nSupervision Cost (si), representing the resources required to assess and address a reported risk, was calcu-"}, {"title": "CONCLUSIONS", "content": "The findings of our simulation study underscore the profound impact that supervision policies can have on the management of risks associated with GPAI models. By systematically comparing non-prioritised, random, priority-based, and diversity-prioritised policies, we revealed how different strategies lead to markedly divergent outcomes in terms of risk mitigation efficiency and the coverage of diverse risk types.\nOur results indicate that the priority-based and diversity-prioritised policies are more effective at addressing high-impact risks. Specifically, these policies processed reports with significantly higher potential damage and overall priority scores compared to the non-prioritised and random policies. For example, the average potential damage mitigated per report was substantially higher under the priority-based approach, demonstrating its effectiveness in focusing resources on the most critical issues. However, this focus on high-priority reports introduces certain trade-offs. The priority-based policy tended to favour reports from expert and crowdsourced sources, potentially overlooking valuable insights from the broader community.\nThe diversity-prioritised policy, while offering a more balanced coverage across different risk types and sources, still prioritises high-impact risks, leaving room for overlooked community-driven reports that often cover emergent behaviours or user experience issues. The influence of feedback loops between incentives and deterrence further accentuates these trends. As more attention is given to expert-driven reports, the incentives for experts to continue reporting increase, reinforcing their influence. Meanwhile, community and crowdsourcing sources, receiving less attention, experience reduced incentives, leading to a decline in their reporting. This dynamic highlights the challenge of balancing short-term prioritisation with long-term risk diversity.\nTo validate these findings, we applied the simulation framework to the WildChat dataset, which comprises over one million conversations with ChatGPT, including over 150K conversations flagged as risky. This real-world data provided empirical support for our simulations, re-"}, {"title": "CODE AND DATA AVAILABILITY", "content": "The simulation code for this study is available in the GitHub repository:\nhttps://github.com/manuelcebrianramos/LLM_\nsupervision_policies\nThis repository contains the following:\n\u2022 The Jupyter Notebook (Simulation_Code/\nPrioritizationLLMsupervision.ipynb) imple-\nmenting the Report class and associated simulation\nfunctions.\n\u2022 All simulation data organised into batch folders in\nSimulation_Runs/.\nThe folder Simulation_Runs/ contains CSV files for\neach simulation type, named according to the following\npatterns:\n\u2022 simulation.YYYYMMDD_HHMMSS.csv for general simula-\ntions,\n\u2022 non-prioritized_simulation.XX_YYYYMMDD_HHMMSS.csv\nfor non-prioritised simulations,\n\u2022 random_fairness_simulation.XX_YYYYMMDD_HHMMSS.csv\nfor random fairness simulations,\n\u2022 prioritized_simulation.XX_YYYYMMDD_HHMMSS.csv\nfor priority-based simulations,\n\u2022 diversity_prioritized_simulation.XX_YYYYMMDD_HHMMSS.csv\nfor diversity-prioritised simulations.\nEach file contains detailed results of an individual sim-\nulation run. Simulation results are further organised into\nbatch folders, such as First_batch and Second_batch,\nfor ease of access and analysis based on different exper-\nimental waves. The results discussed in this paper are\nderived from the Second_batch, which contains the most\nup-to-date experimental data.\nIn addition to these simulations, we integrated datasets\nthat further enrich our analysis of incident trends. These\ndatasets include:\n\u2022 ToxicChat Dataset, a crowdsourced dataset from\nthe Vicuna online demo, containing 10,165 anno-\ntated reports for toxicity, jailbreaking, and Ope-\nnAI moderation scores, along with human an-\nnotations. This dataset served as the primary\nfoundation for our prioritisation analysis. It is"}]}