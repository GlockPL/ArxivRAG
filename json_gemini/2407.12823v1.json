{"title": "WTU-EVAL: A Whether-or-Not Tool Usage Evaluation Benchmark for Large Language Models", "authors": ["Kangyun Ning", "Yisong Su", "Xueqiang Lv", "Yuanzhe Zhang", "Jian Liu", "Kang Liu", "Jinan Xu"], "abstract": "Although Large Language Models (LLMs) excel in NLP tasks, they still need external tools to extend their ability. Current research on tool learning with LLMs often assumes mandatory tool use, which does not always align with real-world situations, where the necessity for tools is uncertain, and incorrect or unnecessary use of tools can damage the general abilities of LLMs. Therefore, we propose to explore whether LLMs can discern their ability boundaries and use tools flexibly. We then introduce the Whether-or-not tool usage Evaluation benchmark (WTU-Eval) to assess LLMS with eleven datasets, where six of them are tool-usage datasets, and five are general datasets. LLMs are prompted to use tools according to their needs. The results of eight LLMs on WTU-Eval reveal that LLMs frequently struggle to determine tool use in general datasets, and LLMs' performance in tool-usage datasets improves when their ability is similar to ChatGPT. In both datasets, incorrect tool usage significantly impairs LLMs' performance. To mitigate this, we also develop the finetuning dataset to enhance tool decision-making. Fine-tuning Llama2-7B results in a 14% average performance improvement and a 16.8% decrease in incorrect tool usage. We will release the WTU-Eval benchmark.", "sections": [{"title": "1 Introduction", "content": "LLMs like GPT4 (OpenAI, 2023) excel in general-purpose NLP tasks, which however struggle in tasks requiring calling external tools, such as fetching up-to-date stock prices and booking flights (Qin et al., 2023a,b; Patil et al., 2023; Ruan et al., 2023). How to improve LLMs' interaction with tools is a current hot topic. Recent studies have explored tool usage fine-tuning (Qin et al., 2023b; Patil et al., 2023) and reinforcement learning (Li et al., 2023) techniques, showing promising results in areas like math reasoning and web search.\nDespite notable progress, prior studies (Patil et al., 2023; Zhuang et al., 2023) mainly focused on scenarios mandating tool use by LLMs. However, in a real-world application, the necessity for tool usage is uncertain. Moreover, we observe that inappropriate tool invocation can lead to errors, adversely affecting outcomes. For example, Figure 1 provides an example of using ChatGPT (0613) to answer a question. Despite the context hinting at the answer: a quarter pounder's weight has been increased to 120.5g, not a quarter pound (113.4g), ChatGPT still invokes an external tool, Calculator, and due to incorrect parameter settings, it produces an erroneous response and redundant response time.\nWith the above observations, we want to explore an intriguing question: whether LLMs can discern their ability boundaries, and if LLMs have the option to decide whether to use tools, would their"}, {"title": "2 Related Work", "content": "Integrating tool calls into LLMs spans three critical areas: API collection and search, tool assistant strategy, and performance evaluations.\nAPI Collection and Search. APIBench (Patil et al., 2023), featuring APIs from HuggingFace, TorchHub, and TensorHub, assesses its proficiency. ToolBench (Qin et al., 2023b) features 16000+ real-world APIs across 49 categories from RapidAPI Hub, and develops a depth-first search decision tree (DFSDT), improving LLMs' search and reasoning capabilities.\nTool Assistant Strategy. SelfAsk (Press et al., 2022) simplifies tasks into sub-questions for tool invocation, akin to DemonstrateSearch-Predict (Khattab et al., 2023). Similarly, Toolformer (Schick et al., 2023), ART (Paranjape et al., 2023), and others (Gao et al., 2023; Lyu et al., 2023; Chen et al., 2022) using specific tokens to guide tool usage, halting to invoke tools, and incorporating their outputs for continued generation. But they only focus on tool-usage tools, and can not apply to real-world scenarios.\nTool Usage Evaluation. Jacovi et al. (2023) focuses on mathematical reasoning and reveals the influence of tool use versus non-use is less pronounced in larger LLMs compared to smaller ones. MetaTool (Huang et al., 2023) assesses LLMs' decisions on whether to utilize external tools and which tool to use, but does not address the effects of incorrect or unnecessary tool usage.\nDifferent from previous works, WTU-Eval aims to bridge this gap by investigating whether models recognize the need for tool use in real-world scenarios and how improper tool integration might affect the foundational efficiency of LLMs, as detailed in \u00a73."}, {"title": "3 The WTU-Eval Benchmark", "content": "The desired diagram of WTU-Eval is shown in Figure 2. In R1, the user asks a real-time question, but LLM cannot access this information without the search engine, so it fails to answer. In R2, when faced with the same question, LLM has access to tool pools and knows that the tool usage is necessary, so it decides to call Search Engine to find the real-time information and gives the correct answer. In R3, the user asks a general question, and LLM answers it with its knowledge. In R4, when presented with the same question, the LLM can access tool pools. Recognizing that tool usage is unnecessary, it decides to provide an answer directly.\nBy comparing the results between R1 and R2, we can determine whether LLMs recognize when a question exceeds their capabilities and thus requires the use of tools, and quantify the impact of using tools. By comparing the results of R3 and R4, we can determine whether the LLMs, when given the option to use tools, recognize that the current question can be answered without tools. Additionally, we can quantify the damage when they choose to use tools unnecessarily."}, {"title": "3.1 Evaluation Settings", "content": "We show WTU-Eval settings from datasets, tool pools, LLMs, and evaluation metrics.\nDatasets. We partition the datasets into the tool datasets (for tasks requiring specific tools), and the general datasets (for tasks solvable with LLMs' own ability). The tool datasets include MLQA (Lewis et al., 2019), ASDiv (Miao et al., 2021), GSM8K (Cobbe et al., 2021), MathQA (Amini et al., 2019), HotpotQA (Yang et al., 2018), and RealtimeQA (Kasai et al., 2022), focusing on machine translation, math reasoning, Wikipedia search, and web search. The general datasets contain BoolQ (Clark et al., 2019), RACE (Lai et al., 2017), PIQA (Bisk et al., 2020), RTE (Dagan et al., 2005), and HellaSwag (Zellers et al., 2019), focusing on reading comprehension, commonsense reasoning, and sentence completion. More details about the datasets are discussed in the Appendix A.\nTool Pools. Following BMTools (Qin et al., 2023a), we select the tools used in the evaluation, where machine translator and calculator are single-action tools, and search engine and Wikipedia search are multiple-action tools.\nLLMs. We test LLMs from both commercial and open-source sectors for a broad evaluation, including Text-Davinci-003, ChatGPT (0613), Llama2, ChatGLM3-6B, and Zephyr-7B. ChatGLM3-6B is notable for its unique agent-tuning with tool interaction insights. Zephyr-7B, evolved from Mistral-7B, employs Direct Distilled Preference Optimization (DPO) to better align with user preferences in language tasks.\nEvaluation Metrics. In WTU-Eval, we prioritize accuracy using advanced methods beyond exact matches, categorizing datasets into numerical and free-text responses. We check numerical answers with specific data and transform free-text responses into labels. For example, in PIQA (which provides two solutions for a given task), we label these two solutions as 1 and 2. When we cannot match labels or text content, we manually check the responses.\nAdditionally, tool usage is marked incorrect in the general dataset and a correct example is shown in Figure 2 R4. To balance comparisons, we introduce the Call Rate, considering the initial use of a tool as a call, ensuring a thorough evaluation."}, {"title": "3.2 Evaluation Prompt", "content": "In WTU-Eval, we utilize ReACT (Yao et al., 2022) for zero-shot and few-shot experiments in scenarios with access to tool pools (R2 and R4). The ReACT is structured into four stages: Thought, Action, Observation, and Final Answer, performed in a limited loop. To ensure fairness, all LLMs are evaluated under the same settings during the assessment. Further details on prompts are provided in the Appendix D.\nZero-shots. We introduce tool names, descriptions, and parameters to guide the LLMs to use tools.\nFew-shots. We introduce tool names, descriptions, parameters, and examples of: a) one tool usage scenario, and b) one general scenario where tools are not used."}, {"title": "4 Experiments and Results", "content": "When LLMs can determine whether to use tools and LLMs' ability is similar to ChatGPT, their performance in tool-usage datasets improves.\nIn Table 1 R2, when LLMs have access to tools, Llama2-13B's zero-shot performance on most tool questions drops to 0, while ChatGPT and Text-Davinci-003 exhibit significant improvements (by up to 25% in GSM8K), exceeding their performance in R1. It is observed that the use of tools does not unconditionally enhance LLMs' performance and the enhancement depends on LLMs' ability. Considering the scale gap between ChatGPT, Text-Davinci-003, and Llama2, we believe that properly using tools demands models' ability to deal with complex and extensive tool prompts without demonstrations.\nThis trend alters a little with the adoption of the few-shot methodology. In R2, ChatGPT and Text-Davinci-003's performance also improve (by up to 40% in GSM8K) with the few-shot setting, exceeding their performance in R1. In contrast, Llama2 only shows improvement on a small portion of tool datasets, with performance declining on the rest compared to R1. It is concluded that the efficacy of tool invocation in augmenting performance is contingent upon the ability of the model."}, {"title": "4.2 Impact of Different Tools on LLMs' Performance in Tool Datasets", "content": "In most tool-usage datasets, the proficiency of LLMs diminishes as the complexity of tools increases. In Table 1 R2, we especially introduce the Translator to MLQA, the Calculator to ASDiv, GSM8K, and MathQA, the Search Engine to RealtimeQA, and the Wikipedia Search to HotPotQA.\nTool usage impact is closely linked to tool complexity. LLMs efficiently manage translation tasks due to the Translator's simplicity. However, when faced with complex tools like the WolframAlpha Calculator, Llama2's performance drops significantly. Similarly, tasks using BingSearch and WikipediaSearch see only modest improvements due to more complex tool instructions, particularly in the few-shot setting across all LLMs.\nMoreover, the few-shot setting remarkably out-"}, {"title": "4.3 LLMs' Performance in General Datasets", "content": "LLMs' performance in general datasets declines when they can determine whether to use tools, indicating LLMs do not know their ability boundary. By comparing R4 to R3 in Table 1, we can observe that LLMs' performance decreases in all general datasets. Analyzing the incorrect answer, we note that LLMs tend to use tools, and due to wrong tool invocation, their performance declines. The whole incorrect answer study will be discussed in \u00a75.2."}, {"title": "4.4 Impact of Different Tools on LLMs' Performance in General Datasets", "content": "In general datasets, LLMs experience a decline in performance when various external tools are introduced, with the impact order being (Wikipedia Search, Search Engine) more significant than (Translator, Calculator). We test LLMs by introducing all tools and tool usage instructions. In the zero-shot setting, the collective impact of the tool pool is evaluated by introducing information for all tools simultaneously in the instructions. In the few-shot setting, due to the input length constraints of LLMs, we present each tool's name, description, and demonstration in individual prompts. The average accuracy across all tools reflects the cumulative effect of the tool pools.\nAs illustrated in Table 3, it is observed that due to the increased complexity introduced by the addition of tool instruction, zero-shot performance is"}, {"title": "5 Discussion", "content": "For a deeper understanding of the results, we explore different finetuning methods' impacts, conduct the error analysis, and make some improvements on WTU-Eval."}, {"title": "5.1 Quantitative Results on Different Fine-tuning Methods", "content": "We conduct R2 and R4's evaluation in ChatGLM3-6B and Zephyr-7B for their different fine-tuning methods from Llama2."}, {"title": "5.2 Error Analysis", "content": "In this section, we make a deep sampling of the failure cases. Besides the wrong answer, we set five error types of these cases, show examples and analysis in Table 4, and more details in Appendix C.\nThe proportions of these error types vary across different models, datasets, and settings. For instance, Figure 4 shows the distribution of error types in math-solving questions (ASDiV, GSM8K and MathQA) and commonsense reason questions (BoolQ) with Llama2-7B in R2 and R4. It can be inferred that incorrect/unnecessary tool invocation is preferred to appear in general datasets, while the other error types about tool invocation steps appear in tool-usage datasets."}, {"title": "5.3 Supervised Fine-Tuning for Tool-Usage Decision-Making", "content": "Based on our findings, LLMs' indecision on tool usage not only undermines their overall performance but also adversely affects their effectiveness on general datasets. To mitigate this, we curate a specialized dataset with a size of 4000 from the general datasets' training sets. Based on observation of step Thought's importance for the decision on tool usage in Table 4, we train the first Thought and second Action steps, aiming at improving decision-making ability regarding tool usage. We apply GPT-4 to generate the first Thought step and select the correct action for the general questions.\nAfter supervised fine-tuning, Llama2-7B's performance improves by an average of 14%, and incorrect tool use drops by 16.8% in general datasets. Specifically, in the PIQA, accuracy in the Search Engine improves by 40%, and the Calculator call rate decreases by 74%, as detailed in Table 7."}, {"title": "6 Conclusion", "content": "In this paper, we explore whether LLMs can discern their ability boundaries and use tools flexibly. We introduce the WTU-Eval to assess LLMs with eleven datasets and four tools. The results of WTU-Eval reveal that LLMs frequently struggle to determine tool use in general datasets, and their performance in tool-usage datasets improves when their ability is similar to ChatGPT. In both datasets, incorrect tool usage significantly impairs LLMs\u2019 performance. After detailed analysis, we also introduce a dataset focused on improving decision-making in tool usage, which successfully enhances Llama2-7B's performance and reduces unnecessary tool invocations.\nOur work points out the overlooked shortcomings in tool usage by LLMs, i.e., they struggle to recognize their capability boundaries and lack"}, {"title": "Limitations", "content": "This study's limitations arise from computational constraints, limiting our model selection to exclude larger variants like Llama2-70B, and from the models' slow processing of tool directives, leading us to evaluate a sampled subset of the test set, potentially causing result discrepancies with other studies."}, {"title": "Ethics Statement and Broader Impacts", "content": "This study exclusively utilized datasets and toolsets that are publicly available and previously published, ensuring they contain no offensive or harmful content. We rigorously adhere to ethical standards, including a thorough review of materials to safeguard privacy and integrity.\nThis study is pivotal for the practical application of LLMs, as it aims at reducing unnecessary tool invocations, thereby enhancing the efficiency of tool usage. This optimization in tool interaction not only advances the development of AI but also ensures more effective and streamlined AI operations, leading to smarter and more efficient AI systems that better serve the needs across different sectors and research disciplines."}, {"title": "A Hardware Configuration and Datasets", "content": "In this study, the hardware configuration comprises an NVIDIA GeForce RTX 3090 GPU with 20 GB of memory for a tool-usage task that costs several minutes, and an NVIDIA A100 GPU with 80 GB of memory for the fine-tuning task, requiring six hours to complete nine epochs tuning."}, {"title": "B Examples of Successful Types", "content": "Given a context and related question, try your best to answer the question. Context:\nBeing his favorite, he saved checking on the grapevines for his last stop. He was told by one the pickers that they fill 324 drums of grapes per day.\nQuestion: How many drums of grapes would be filled in 9 days?\nThought: I should use calculator for Multipli-cation to finish this task.\nAction: Calculator\nAction Input: 324*9"}, {"title": "C Examples of Failure Type", "content": "Context:\nEnglish breakfast is a very big meal-eggs, tomatoes, tea, coffee... For many people, lunch is a quick meal. In cities, there are a lot of sandwich bars. People can buy sandwiches there. Students can have a hot meal at school, but many just take a sandwich, a drink, and some fruit from home. \"Tea\" means two things. It is a drink and a meal! Some people have afternoon tea, with sandwiches, cakes, and a cup of tea. They usually have dinner quite early, between 6:00 and 8:00(......), and often all the family eat together.\nPeople often get take-away meals-they buy the food outside\nQuestion:\nWhen they get a takeaway meal, they often eat it ()?\nOptions:\nA. \"at home\"\nB. \"in the school\"\nC. \"outside\"\nD. \"in the bars\"\nThought: There is no answer in the passage.\nAction: BingSearch\nAction Input: Where do people often eat takeaway meals?\nError Raise: It's general data, you can not use the tools."}, {"title": "D Prompts", "content": "The prompts used in the evaluation are shown in this section.\nYou are AutoGPT, you can use many tools (functions) to do the following task if you need.\nFirst I will give you the task description, and your task start.\nAt each step, you need to give your thought to analyze the status now and what to do next, with a function call to actually excute your step.\nAfter the call, you will get the call result, and you are now in a new state.\nThen you will analyze your status now, then decide what to do next...\nAfter many (Thought-call) pairs, you finally perform the task, then you can give your finial answer.\nRemember:\n1.the state change is irreversible, you can't go back to one of the former state, if you want to restart the task, follow this:\nAction: Finish\nAction Input: I give up and restart\n2.All the thought is short, at most in 5 sentence."}, {"title": "E Tool Impact on Model Performance across General Datasets", "content": ""}]}