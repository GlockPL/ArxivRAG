{"title": "WTU-EVAL: A Whether-or-Not Tool Usage Evaluation Benchmark for Large Language Models", "authors": ["Kangyun Ning", "Yisong Su", "Xueqiang Lv", "Yuanzhe Zhang", "Jian Liu", "Kang Liu", "Jinan Xu"], "abstract": "Although Large Language Models (LLMs) excel in NLP tasks, they still need external tools to extend their ability. Current research on tool learning with LLMs often assumes mandatory tool use, which does not always align with real-world situations, where the necessity for tools is uncertain, and incorrect or unnecessary use of tools can damage the general abilities of LLMs. Therefore, we propose to explore whether LLMs can discern their ability boundaries and use tools flexibly. We then introduce the Whether-or-not tool usage Evaluation benchmark (WTU-Eval) to assess LLMS with eleven datasets, where six of them are tool-usage datasets, and five are general datasets. LLMs are prompted to use tools according to their needs. The results of eight LLMs on WTU-Eval reveal that LLMs frequently struggle to determine tool use in general datasets, and LLMs' performance in tool-usage datasets improves when their ability is similar to ChatGPT. In both datasets, incorrect tool usage significantly impairs LLMs' performance. To mitigate this, we also develop the finetuning dataset to enhance tool decision-making. Fine-tuning Llama2-7B results in a 14% average performance improvement and a 16.8% decrease in incorrect tool usage. We will release the WTU-Eval benchmark.", "sections": [{"title": "1 Introduction", "content": "LLMs like GPT4 (OpenAI, 2023) excel in general-purpose NLP tasks, which however struggle in tasks requiring calling external tools, such as fetching up-to-date stock prices and booking flights (Qin et al., 2023a,b; Patil et al., 2023; Ruan et al., 2023). How to improve LLMs' interaction with tools is a current hot topic. Recent studies have explored tool usage fine-tuning (Qin et al., 2023b; Patil et al., 2023) and reinforcement learning (Li et al., 2023) techniques, showing promising results in areas like math reasoning and web search.\nDespite notable progress, prior studies (Patil et al., 2023; Zhuang et al., 2023) mainly focused on scenarios mandating tool use by LLMs. However, in a real-world application, the necessity for tool usage is uncertain. Moreover, we observe that inappropriate tool invocation can lead to errors, adversely affecting outcomes. For example, Figure 1 provides an example of using ChatGPT (0613) to answer a question. Despite the context hinting at the answer: a quarter pounder's weight has been increased to 120.5g, not a quarter pound (113.4g), ChatGPT still invokes an external tool, Calculator, and due to incorrect parameter settings, it produces an erroneous response and redundant response time.\nWith the above observations, we want to explore an intriguing question: whether LLMs can discern their ability boundaries, and if LLMs have the option to decide whether to use tools, would their"}, {"title": "2 Related Work", "content": "Integrating tool calls into LLMs spans three critical areas: API collection and search, tool assistant strategy, and performance evaluations.\nAPI Collection and Search. APIBench (Patil et al., 2023), featuring APIs from HuggingFace, TorchHub, and TensorHub, assesses its proficiency. ToolBench (Qin et al., 2023b) features 16000+ real-world APIs across 49 categories from RapidAPI Hub, and develops a depth-first search decision tree (DFSDT), improving LLMs' search and reasoning capabilities.\nTool Assistant Strategy. SelfAsk (Press et al., 2022) simplifies tasks into sub-questions for tool invocation, akin to DemonstrateSearch-Predict (Khattab et al., 2023). Similarly, Toolformer (Schick et al., 2023), ART (Paranjape et al., 2023), and others (Gao et al., 2023; Lyu et al., 2023; Chen et al., 2022) using specific tokens to guide tool usage, halting to invoke tools, and incorporating their outputs for continued generation. But they only focus on tool-usage tools, and can not apply to real-world scenarios.\nTool Usage Evaluation. Jacovi et al. (2023) focuses on mathematical reasoning and reveals the influence of tool use versus non-use is less pronounced in larger LLMs compared to smaller ones. MetaTool (Huang et al., 2023) assesses LLMs' decisions on whether to utilize external tools and which tool to use, but does not address the effects of incorrect or unnecessary tool usage.\nDifferent from previous works, WTU-Eval aims to bridge this gap by investigating whether models recognize the need for tool use in real-world scenarios and how improper tool integration might affect the foundational efficiency of LLMs, as detailed in \u00a73."}, {"title": "3 The WTU-Eval Benchmark", "content": "The desired diagram of WTU-Eval is shown in Figure 2. In R1, the user asks a real-time question, but LLM cannot access this information without the search engine, so it fails to answer. In R2, when faced with the same question, LLM has access to tool pools and knows that the tool usage is necessary, so it decides to call Search Engine to find the real-time information and gives the correct answer. In R3, the user asks a general question, and LLM answers it with its knowledge. In R4, when presented with the same question, the LLM can access tool pools. Recognizing that tool usage is unnecessary, it decides to provide an answer directly.\nBy comparing the results between R1 and R2, we can determine whether LLMs recognize when a question exceeds their capabilities and thus requires the use of tools, and quantify the impact of using tools. By comparing the results of R3 and R4, we can determine whether the LLMs, when given the option to use tools, recognize that the current question can be answered without tools. Addition-"}, {"title": "3.1 Evaluation Settings", "content": "We show WTU-Eval settings from datasets, tool pools, LLMs, and evaluation metrics.\nDatasets. We partition the datasets into the tool datasets (for tasks requiring specific tools), and the general datasets (for tasks solvable with LLMs' own ability). The tool datasets include MLQA (Lewis et al., 2019), ASDiv (Miao et al., 2021), GSM8K (Cobbe et al., 2021), MathQA (Amini et al., 2019), HotpotQA (Yang et al., 2018), and RealtimeQA (Kasai et al., 2022), focusing on machine translation, math reasoning, Wikipedia search, and web search. The general datasets contain BoolQ (Clark et al., 2019), RACE (Lai et al., 2017), PIQA (Bisk et al., 2020), RTE (Dagan et al., 2005), and HellaSwag (Zellers et al., 2019), focusing on reading comprehension, commonsense reasoning, and sentence completion. More details about the datasets are discussed in the Appendix A.\nTool Pools. Following BMTools (Qin et al., 2023a), we select the tools used in the evaluation, where machine translator and calculator are single-action tools, and search engine and Wikipedia search are multiple-action tools.\n\u2022 Machine Translator: We select Baidu Translator\u00b9, as a current mainstream translation API with good performance, for testing.\n\u2022 Calculator: We choose the WolframAlpha API\u00b2 as our calculator.\n\u2022 Search Engine: We choose the Bing Search\u00b3 API as the web search tool for LLMs to browse current events, fiction stories, history facts, etc.\n\u2022 Wikipedia Search: Besides a simple Wikipedia API, as WikiSearch and WikiLoadPage are designed, we define an additional action \u2013 WikiDisambiguation. When the search entity cannot return the expected result, the model can access the interface to get a similar entity to the current search result and self-correct the search parameters."}, {"title": "LLMs", "content": "We test LLMs from both commercial and open-source sectors for a broad evaluation, including Text-Davinci-003, ChatGPT (0613), Llama2, ChatGLM3-6B, and Zephyr-7B. ChatGLM3-6B is notable for its unique agent-tuning with tool interaction insights. Zephyr-7B, evolved from Mistral-7B, employs Direct Distilled Preference Optimization (DPO) to better align with user preferences in language tasks."}, {"title": "Evaluation Metrics", "content": "In WTU-Eval, we prioritize accuracy using advanced methods beyond exact matches, categorizing datasets into numerical and free-text responses. We check numerical answers with specific data and transform free-text responses into labels. For example, in PIQA (which provides two solutions for a given task), we label these two solutions as 1 and 2. When we cannot match labels or text content, we manually check the responses.\nAdditionally, tool usage is marked incorrect in the general dataset and a correct example is shown in Figure 2 R4. To balance comparisons, we introduce the Call Rate, considering the initial use of a tool as a call, ensuring a thorough evaluation."}, {"title": "3.2 Evaluation Prompt", "content": "In WTU-Eval, we utilize ReACT (Yao et al., 2022) for zero-shot and few-shot experiments in scenarios with access to tool pools (R2 and R4). The ReACT is structured into four stages: Thought, Action, Observation, and Final Answer, performed in a limited loop. To ensure fairness, all LLMs are evaluated under the same settings during the assessment. Further details on prompts are provided in the Appendix D.\nZero-shots. We introduce tool names, descriptions, and parameters to guide the LLMs to use tools.\nFew-shots. We introduce tool names, descriptions, parameters, and examples of: a) one tool usage scenario, and b) one general scenario where tools are not used."}, {"title": "4 Experiments and Results", "content": "When LLMs can determine whether to use tools and LLMs' ability is similar to ChatGPT, their performance in tool-usage datasets improves. In Table 1 R2, when LLMs have access to tools, Llama2-13B's zero-shot performance on most tool questions drops to 0, while ChatGPT and Text-Davinci-003 exhibit significant improvements (by up to 25% in GSM8K), exceeding their performance in R1. It is observed that the use of tools does not unconditionally enhance LLMs' performance and the enhancement depends on LLMs' ability. Considering the scale gap between ChatGPT, Text-Davinci-003, and Llama2, we believe that properly using tools demands models' ability to deal with complex and extensive tool prompts without demonstrations.\nThis trend alters a little with the adoption of the few-shot methodology. In R2, ChatGPT and Text-Davinci-003's performance also improve (by up to"}, {"title": "4.1 LLMs' Performance in Tool Datasets", "content": "40% in GSM8K) with the few-shot setting, exceeding their performance in R1. In contrast, Llama2 only shows improvement on a small portion of tool datasets, with performance declining on the rest compared to R1. It is concluded that the efficacy of tool invocation in augmenting performance is contingent upon the ability of the model."}, {"title": "4.2 Impact of Different Tools on LLMs' Performance in Tool Datasets", "content": "In most tool-usage datasets, the proficiency of LLMs diminishes as the complexity of tools increases. In Table 1 R2, we especially introduce the Translator to MLQA, the Calculator to ASDiv, GSM8K, and MathQA, the Search Engine to RealtimeQA, and the Wikipedia Search to HotPotQA.\nTool usage impact is closely linked to tool complexity. LLMs efficiently manage translation tasks due to the Translator's simplicity. However, when faced with complex tools like the WolframAlpha Calculator, Llama2's performance drops significantly. Similarly, tasks using BingSearch and WikipediaSearch see only modest improvements due to more complex tool instructions, particularly in the few-shot setting across all LLMs.\nMoreover, the few-shot setting remarkably out-"}, {"title": "4.3 LLMs' Performance in General Datasets", "content": "LLMs' performance in general datasets declines when they can determine whether to use tools, indicating LLMs do not know their ability boundary. By comparing R4 to R3 in Table 1, we can observe that LLMs' performance decreases in all general datasets. Analyzing the incorrect answer, we note that LLMs tend to use tools, and due to wrong tool invocation, their performance declines. The whole incorrect answer study will be"}, {"title": "4.4 Impact of Different Tools on LLMs' Performance in General Datasets", "content": "In general datasets, LLMs experience a decline in performance when various external tools are introduced, with the impact order being (Wikipedia Search, Search Engine) more significant than (Translator, Calculator). We test LLMs by introducing all tools and tool usage instructions. In the zero-shot setting, the collective impact of the tool pool is evaluated by introducing information for all tools simultaneously in the instructions. In the few-shot setting, due to the input length constraints of LLMs, we present each tool's name, description, and demonstration in individual prompts. The average accuracy across all tools reflects the cumulative effect of the tool pools.\nAs illustrated in Table 3, it is observed that due to the increased complexity introduced by the addition of tool instruction, zero-shot performance is"}, {"title": "5 Discussion", "content": "For a deeper understanding of the results, we explore different finetuning methods' impacts, conduct the error analysis, and make some improvements on WTU-Eval."}, {"title": "5.1 Quantitative Results on Different Fine-tuning Methods", "content": "We conduct R2 and R4's evaluation in ChatGLM3-6B and Zephyr-7B for their different fine-tuning methods from Llama2.\nPoor Performance of ChatGLM3-6B in Decision on Whether to Use the Tool. The above analysis reveals that LLMs' performance drops in general datasets with tool access but improves in tool usage datasets, dependent on LLM ability. LLMs lacking effective tool-usage training show weaker decision-making on tool employment. Thus, we experiment with ChatGLM3-6B in general and tool-"}, {"title": "5.2 Error Analysis", "content": "In this section, we make a deep sampling of the failure cases. Besides the wrong answer, we set five error types of these cases, show examples and analysis in Table 4, and more details in Appendix C.\nThe proportions of these error types vary across different models, datasets, and settings. For instance, Figure 4 shows the distribution of error types in math-solving questions (ASDiV, GSM8K and MathQA) and commonsense reason questions (BoolQ) with Llama2-7B in R2 and R4. It can be inferred that incorrect/unnecessary tool invocation is preferred to appear in general datasets, while the other error types about tool invocation steps appear in tool-usage datasets."}, {"title": "5.3 Supervised Fine-Tuning for Tool-Usage Decision-Making", "content": "Based on our findings, LLMs' indecision on tool usage not only undermines their overall performance but also adversely affects their effectiveness on general datasets. To mitigate this, we curate a specialized dataset with a size of 4000 from the general datasets' training sets. Based on observation of step Thought's importance for the decision on tool usage in Table 4, we train the first Thought and second Action steps, aiming at improving decision-making ability regarding tool usage. We apply GPT-4 to generate the first Thought step and select the correct action for the general questions.\nAfter supervised fine-tuning, Llama2-7B's performance improves by an average of 14%, and incorrect tool use drops by 16.8% in general datasets. Specifically, in the PIQA, accuracy in the Search Engine improves by 40%, and the Calculator call rate decreases by 74%, as detailed in Table 7."}, {"title": "6 Conclusion", "content": "In this paper, we explore whether LLMs can discern their ability boundaries and use tools flexibly. We introduce the WTU-Eval to assess LLMs with eleven datasets and four tools. The results of WTU-Eval reveal that LLMs frequently struggle to determine tool use in general datasets, and their performance in tool-usage datasets improves when their ability is similar to ChatGPT. In both datasets, incorrect tool usage significantly impairs LLMs\u2019 performance. After detailed analysis, we also introduce a dataset focused on improving decision-making in tool usage, which successfully enhances Llama2-7B's performance and reduces unnecessary tool invocations.\nOur work points out the overlooked shortcomings in tool usage by LLMs, i.e., they struggle to recognize their capability boundaries and lack"}, {"title": "Limitations", "content": "This study's limitations arise from computational constraints, limiting our model selection to exclude larger variants like Llama2-70B, and from the models' slow processing of tool directives, leading us to evaluate a sampled subset of the test set, potentially causing result discrepancies with other studies."}, {"title": "Ethics Statement and Broader Impacts", "content": "This study exclusively utilized datasets and toolsets that are publicly available and previously published, ensuring they contain no offensive or harmful content. We rigorously adhere to ethical standards, including a thorough review of materials to safeguard privacy and integrity.\nThis study is pivotal for the practical application of LLMs, as it aims at reducing unnecessary tool invocations, thereby enhancing the efficiency of tool usage. This optimization in tool interaction not only advances the development of AI but also ensures more effective and streamlined AI operations, leading to smarter and more efficient AI systems that better serve the needs across different sectors and research disciplines."}, {"title": "B Examples of Successful Types", "content": "\u2022 Correct Tool Usage Example of Calculator\nTask Description:\nGiven a context and related question, try\nyour best to answer the question. Context:\nBeing his favorite, he saved checking on the\ngrapevines for his last stop. He was told by\none the pickers that they fill 324 drums of\ngrapes per day.\nQuestion: How many drums of grapes would\nbe filled in 9 days?\nThought: I should use calculator for Multipli-\ncation to finish this task.\nAction: Calculator\nAction Input: 324*9\nObservation: {\u2018@title': \u2018Input', 'subpod':\n{\u2018@title': '', 'img': {\u2018@src':{given link},\n'@alt': '324 * 9', '@title': '324 * 9',\n'@contenttype': 'image/gif'}, 'plaintext':\n'324 * 9'}}, {\u2018@title': 'Result', '@pri-\nmary': 'true', 'subpod': {'@title': '', 'img':\n{'@src':{given link}, \u2018@alt': '2916', '@ti-\ntle': '2916', '@contenttype': 'image/gif'},\n'plaintext': '2916'}, \u2018states': {'@count': '1',\n'state': {'@name': 'Step-by-step solution',\n'@input': 'Result_Step-by-step solution, '}}\nThought: I think I got the final answer.\nAction: Finish\nAction Input: 2916 (drums)\nFinal Answer: 2916 (drums)\"\n\u2022 Correct Tool Usage Example of Machine\nTranslator\nTask Description:\nGiven an English context and related Arabic\nquestion (you can translate the question into\nEnglish), answer the question in English.\nEnglish Context: In 1884, a paper was signed\nat Togoville with the King Mlapa III, whereby\nGermany claimed a protectorate over a stretch\nof territory along the coast and gradually extended its control inland. Its borders were defined after the capture of hinterland by German forces and signing agreements with France and Britain. In 1905, this became the German colony of Togoland. The local population was forced to work, cultivate cotton, coffee and cocoa and pay high taxes. A railway and the port of Lom00e9 were built for export of agricultural products. The Germans introduced modern techniques of cultivation of cocoa, coffee and cotton and developed the infrastructure.\nArabic Question:\n\u0645\u062a\u0649 \u062a\u0645 \u062a\u0648\u0642\u064a\u0639 \u0627\u0644\u0623\u0648\u0631\u0627\u0642\u061f\nThought: I need to translate the question\nAction: Translator\nAction Input:\n\u0645\u062a\u0649 \u062a\u0645 \u062a\u0648\u0642\u064a\u0639 \u0627\u0644\u0623\u0648\u0631\u0627\u0642\u061f\nObservation: When were these documents\nsigned?\nThought: I think I got the final answer\nAction: Finish\nAction Input: 1884\nFinal Answer: 1884\n\u2022 Correct Tool Usage Example of Search\nEngine"}, {"title": "C Examples of Failure Type", "content": "\u2022 Incorrect or Unnecessary Tool Invocation\nModel: ChatGPT\nContext:\nEnglish breakfast is a very big meal-eggs,\ntomatoes, tea, coffee... For many people,\nlunch is a quick meal. In cities, there are a lot\nof sandwich bars. People can buy sandwiches\nthere. Students can have a hot meal at school,\nbut many just take a sandwich, a drink, and\nsome fruit from home. \"Tea\" means two\nthings. It is a drink and a meal! Some people\nhave afternoon tea, with sandwiches, cakes,\nand a cup of tea. They usually have dinner\nquite early, between 6:00 and 8:00(......), and\noften all the family eat together.\nPeople often get take-away meals-they buy\nthe food outside\nQuestion:\nWhen they get a takeaway meal, they often\neat it ()?\nOptions:\nA. \"at home\"\nB. \"in the school\"\nC. \"outside\"\nD. \"in the bars\"\nThought: There is no answer in the passage.\nAction: BingSearch\nAction Input: Where do people often eat\ntakeaway meals?\nError Raise: It's general data, you can not\nuse the tools.\n\u2022 Null Content\nModel: Llama2-13B-hf\nThought Null\nContext:\nOlivia picked up sixteen pieces of paper from\nthe floor. If Edward picked up three pieces,\nQuestion: how many did they pick up in\ntotal?\nThought: I should try to calculate the total\nnumber of pieces of paper picked up.\nAction: Calculator\nAction Input: 16 + 3\nObservation: {'@title': 'Input', 'sub-\npod': {'@title': \", 'img': {'@src':{given\nlink}, '@alt': '16 + 3', '@title': '16 +\n3', '@contenttype': 'image/gif'}, 'plain-\ntext': '16 + 3'}}, {'@title': 'Result',\nAction Input:\n\u0645\u0627\u0630\u0627 \u0643\u0627\u0646 \u0627\u0644\u0627\u0633\u0645 \u0627\u0644\u0623\u062e\u064a\u0631 \u0644\u0623\u0647\u0631\u0648\u0646\u061f\nObservation: Please choose a valid tool in\nTranslator!\nThought: I can translate the question into English and then use the translator to answer the\nquestion.\nAction: Translate the question into English\nAction Input:\n\u0645\u0627\u0630\u0627 \u0643\u0627\u0646 \u0627\u0644\u0627\u0633\u0645 \u0627\u0644\u0623\u062e\u064a\u0631 \u0644\u0623\u0647\u0631\u0648\u0646\u061f\n\u2022 Recall invalid tool after reminding\nModel: Zephyr-7B\nEnglish Context:\n\"Das aschkenasische Rabbinat: Studien \u00fcber\nGlaube und Schicksal\" (Julius Carlebach)\n/ Die Rolle des Rabbiners in Deutschland\nheute (Ahron Daum)\nArabic Question:\nWhat is the last name of Aaron?\nThought: I can translate the question into En-\nglish and then use the translator to answer the\nquestion.\nAction: Translate the question into English.\nAction Input:\n\u0627\u0644\u0627\u0633\u0645 \u0627\u0644\u0623\u062e\u064a\u0631 \u0644\u0623\u0647\u0631\u0648\u0646\u061f \u0645\u0627\u0630\u0627 \u0643\u0627\u0646 \u0627\u0644\u0627\u0633\nError Raise: Continue to use the invalid\naction tool, the model refused to answer.\n\u2022 Similar invalid parameter recycle\nModel: Llama2-13B-Chat-hf\nContext:\nUncle Dave bought 143 ice cream sandwiches.\nIf he wants to give them to his 11 hungry\nnieces,\nQuestion:\nhow many can each niece get?\nThought: Hmm, I wonder if I can use the cal-\nculator to calculate the number of ice cream\nsandwiches each niece can get.\nAction: Calculator\nAction Input: 143 ice cream sandwiches / 11\nnieces\nObservation: WolframAlpha API cannot\nparse the input query.\nThought: Oh no! Maybe I need to rephrase\nthe question.\nAction: Calculator\nAction Input: 143 ice cream sandwiches / 11\nnieces = how many ice cream sandwiches per\nniece?\nObservation: WolframAlpha API cannot\nparse the input query.\nThought: Hmm, maybe I need to convert the\nnumber of ice cream sandwiches to a more\nmanageable unit.\nAction: Calculator\nAction Input: 143 ice cream sandwiches / 11\nnieces = how many ice cream sandwiches per\nniece (in dozens)?\nObservation: WolframAlpha API cannot\nparse the input query.\""}, {"title": "D Prompts", "content": "The prompts used in the evaluation are shown in this section."}]}