{"title": "An End-to-End Approach for Chord-Conditioned Song Generation", "authors": ["Shuochen Gao", "Shun Lei", "Fan Zhuo", "Hangyu Liu", "Feng Liu", "Boshi Tang", "Qiaochu Huang", "Shiyin Kang", "Zhiyong Wu"], "abstract": "The Song Generation task aims to synthesize music composed of vocals and accompaniment from given lyrics. While the existing method, Jukebox, has explored this task, its constrained control over the generations often leads to deficiency in music performance. To mitigate the issue, we introduce an important concept from music composition, namely chords, to song generation networks. Chords form the foundation of accompaniment and provide vocal melody with associated harmony. Given the inaccuracy of automatic chord extractors, we devise a robust cross-attention mechanism augmented with dynamic weight sequence to integrate extracted chord information into song generations and reduce frame-level flaws, and propose a novel model termed Chord-Conditioned Song Generator (CSG) based on it. Experimental evidence demonstrates our proposed method outperforms other approaches in terms of musical performance and control precision of generated songs.", "sections": [{"title": "1. Introduction", "content": "Music, as a ubiquitous art form, plays a significant role in people's lives. Music that includes both accompaniment and vocals is referred to as songs, where well-designed songs necessitate a harmonious blend of vocals and accompaniment. The task of Song Generation, which synthesizes songs from lyrics, can play a critical role in the entertainment industry.\nEarly endeavors in music generation leveraged symbolic representations to produce score parameters [1], which were then rendered into music. However, symbolic music is confined to fixed instrumental timbres and lacks expressiveness. Recent years have witnessed an emergence of End-to-End Music Generation models that generate musical audio through text prompts [2, 3, 4, 5] and melody control [6]. Yet, End-to-End Music Generation often struggles to produce meaningful vocals, frequently resulting in gibberish. Conversely, the Singing Voice Synthesis (SVS) field focuses on generating singing voices from lyrics and scores, with existing efforts [7, 8, 9, 10] capable of producing high-quality vocals. However, SVS-generated singing often lacks accompaniment and requires users to provide music scores, revealing a deficiency in the models' song composition capabilities.\nTo address the challenges faced by Music Generation and Singing Voice Synthesis, Jukebox [11] introduces lyrics as a control condition on top of text prompts, enabling autonomous song generation. However, it primarily models based on acoustic feature sequences, which impedes its ability to assimilate high-level music theory knowledge. Moreover, it exhibits limited control over the music generation process, often resulting in outputs that often lack musicality.\nIn this work, we present an innovative end-to-end Chord-Conditioned Song Generator (CSG), which introduces chord condition for generating condition-compliant songs. Chord, as an important concept of the song, forms the foundation of accompaniment and provides vocal melody with associated harmony [12]. Even simple chords can sustain the basic auditory sensation of accompaniment and, combined with vocals, create melodious songs. For instance, guitar playing and singing involve coupling guitar chords with matching vocals. Given the integral relationship chord shares with both accompaniment and vocals, it serves as a straightforward and effective control condition for generating both components. To our knowledge, this is the first instance where chords have been used as a control condition in the domain of End-to-End Music Generation. Previously, only a portion of Symbolic Music Generation efforts utilized chord control [13, 14, 15]. Chord control simplifies manipulation significantly over melody and score control. Through CSG, even non-expert users, lacking formal musical theory knowledge, can employ standard chord progressions like '6451', '4536', or custom sequences, facilitating the creation of unique, harmonious songs. Following [16, 2], CSG employs a Self-Supervised Learning (SSL) model to extract semantic tokens, serving as substitutes for acoustic features. Moreover, given that existing methods for automatic chord extraction suffer from low precision issues, merely incorporating chords does not enable the model to effectively learn the relationship between chords and music, thereby impacting the musicality of the generated songs. To address this issue, we propose an innovative Attention with Dynamic Weights Sequence (DWS) that, while integrating chords with lyrics and songs, also assesses the correctness of chords frame by frame. By reducing interference from erroneous data and increasing the model's confidence in accurate chord data, this approach simultaneously enhances the musicality and control precision of the generated songs.\nThe main contributions of our paper are: (1) We introduce chords as the control condition for song generation, effortlessly and efficiently enhancing the musicality of the generated songs. (2) We propose an innovative Attention with DWS to improve the precision of control and the musical performance of the generated songs."}, {"title": "2. Methodology", "content": "As shown in Figure 1a, given chord and lyric tokens as inputs, CSG generates frames of songs in an autoregressive manner.\nFirst, we map the input tokens to high-dimensional space. Then the chord and lyric embeddings are combined with vocal embeddings as well as song embeddings from previous frames, and fed into the Attention with DWS for semantics fusion. After that, the fusion embedding gets populated into a GPT module [17] to generate vocal and song embeddings for the current frame, which are finally decoded by two decoders for token prediction. At the training stage, the GPT and Attention with DWS get initialized from random weights and are trained together, while the lyric tokens are extracted by a tokenizer in pre-trained BERT2 [18]. Vocal and song tokens are extracted by pre-trained BEST-RQ\u00b3 [19, 20]. Chord token extraction will be explained in Section 2.1. During inference, we keep all the model weights fixed and get the chord&lyric tokens from users. At the end of inference, a diffusion vocoder, adjusted based on Stable Audio [21], facilitates the restoration from tokens to audio. The following sections elaborate on the modules."}, {"title": "2.1. Chord Token Extraction", "content": "From the song data, we separate the background music and employ Autochord to extract chord progressions from it. The extracted chord progressions consist of three components: chord roots, interval relations, and durations. Specifically, there are twelve possible chord roots: C, C#, D, D#, E, F, F#, G, G#, A, A#, B and four possible interval relations: Major, Minor, Augmented, and Diminished. The 48 possible combinations derived from them can cover the vast majority of chords. We further quantified the chord progressions into sequences, analyzing them at intervals corresponding to a 50Hz rate, with each frame quantized into one of 48 possible tokens. These tokens, determined by their root note and interval relation combination, are encoded as integers ranging from 0 to 47."}, {"title": "2.2. Attention with Dynamic Weights Sequence (DWS)", "content": "To integrate chords with lyrics and songs, the simplest method is to concatenate the chord sequence with the audio sequence in the embedding dimension or to use cross-attention. However, concatenation allows the autoregressive prediction of the song to only see the preceding chord sequence, not the subsequent chords. Additionally, the measured chord extraction accuracy of the Autochord is 67.33%, which means our chord data inevitably contains some noise or even errors. These inaccuracies can interfere with the model's learning, whether through concatenation or cross-attention. To address this issue and improve the precision of control, we propose Attention with Dynamic Weights Sequence (DWS).\nAs illustrated in Figure 1b, Attention with DWS employs a dual-path architecture in which each path incorporates causal or non-causal transformer blocks to facilitate temporal alignment learning within sequences. Subsequently, a cross-attention mechanism is harnessed to synchronize chord embeddings with lyric-audio embeddings, resulting in the alignment output C:\n\nC = \\text{softmax}(\\frac{Q_{\\text{lyric-audio}}K_{\\text{chord}}^T}{\\sqrt{d_k}})V_{\\text{chord}}\n\nAs a weighted average to chord embedding values, C is inevitably affected by the inaccuracies in chord data. This means that \\(c_t\\), representing elements of C at specific time points, may undergo varying degrees of perturbation. To mitigate the impact of such inaccuracies, we introduce a temporally adaptive weighting network. This network is designed to assess the correlation between chord embeddings and audio embeddings sequentially, on a frame-by-frame basis, leading to the generation of the weight sequence W:\n\nW = \\sigma(M([C; A]))\n\nHere, A denotes the lyric-audio embeddings computed by causal transformer blocks, and M is a mapping function that calculates temporal weights for each frame. \\(\\sigma\\) is the sigmoid activation function providing normalization. The generated weight sequence W then evaluates chord utilization across different moments. Consequently, the fusion embedding sequence F, which integrates both global chord information and preceding audio data, is represented as follows:\n\nF = \\sqrt{W}C + \\sqrt{1-W}A\n\nCompared with concatenation and cross-attention, Attention with DWS enhances the robustness of the Fusion Embedding Sequence F towards chords containing inaccuracies while having access to global chord information. By employing a dynamic weight sequence to discern between correct and incorrect chords frame by frame, Attention with DWS ensures that the presence of incorrect chords during training does not diminish confidence in the correct chords. Thus, this enhances the precision of chord-conditioned control over the model. Furthermore, by minimizing the interference from incorrect chords, Attention with DWS enables the model to more effectively learn the musical correlation between chords and songs, thereby improving the musicality of chord-controlled song generation."}, {"title": "3. Experiments", "content": "Given the lack of a large-scale, open-source lyrics-to-song dataset, we apply CSG to a proprietary dataset containing 554,467 English songs across various genres such as country, pop, rock, and rap. For preprocessing, the dataset is segmented into approximately 3 million vocal-only and accompaniment-only segments, each of which is annotated with the corresponding lyrics. We randomly sample 5% of the dataset and reserve it for validation and testing, while the rest is used for training."}, {"title": "3.2. Experiment Setup", "content": "We convert the chord, lyric, and audio tokens into 1024-dimensional embeddings. In Attention with DWS, both the self-attention block and the causal self-attention block consist of 2 layers, while the Adaptive Weight module is comprised of a single linear layer. The GPT model is constructed from 12 transformer blocks, each with a dropout rate of 0.1. Training of the proposed model and ablation baseline models are conducted for 500,000 steps on seven NVIDIA GeForce RTX4090 GPUs, with a batch size of 4 per GPU, utilizing the Adam optimizer and a learning rate warm-up scheduler with a target learning rate of 8 \u00d7 10-5 and a warm-up period of 32,000.\nJukebox is the sole work in the domain of song generation to date. We compare our model with the samples publicly available on the official Jukebox website. Moreover, to demonstrate how incorporating a basic chord condition can markedly enhance musicality, we trained a GPT-only model without the chord condition as a deterministic baseline for comparison, employing the same training methodology as our proposed model."}, {"title": "3.3. Results", "content": "We employ two Mean Opinion Scores (MOS) to evaluate the capability of different models. 1) Musical Performance (MP): Evaluate the musicality of generated songs. 2) Chord Alignment (CA): Evaluate the correlation between the chords in the generated songs and the actual chords. For MP, we randomly select 15 sets of lyrics written by Jukebox researchers, which are not present in any existing dataset, to act as lyric inputs for each model. Additionally, we devise conventional chords, such as '6451', '4536', and '2516', as chord controls. Twenty-two participants are invited to rate these 15 song segments with identical lyrics, providing comprehensive scores for the songs' musicality. For CA, six sets of chords, including both conventional (e.g., '6451', '4536', '2516', '1564') and unconventional (e.g., '1111', '1234') chords, are used to generate 12 samples. Twenty participants evaluate the correlation between the chords in the generated songs and the actual chords, considering auditory perception."}, {"title": "3.3.2. Objective Evaluation", "content": "In terms of generation fidelity, we employ the Fr\u00e9chet Audio Distance (FAD) [22] utilizing the VGGish model [23], where a lower FAD indicates higher audio fidelity. We randomly extract 103 10-second song segments with unseen lyrics from the Jukebox website, and generate 1000 12-second song segments using unseen lyrics and chords by both GPT-only and CSG, respectively, to calculate FAD.\nFurthermore, control precision serves as a critical metric for assessing the effectiveness of control. We specify random chords and pair them with unseen lyrics to generate 400 song segments. Utilizing Autochord, we extract the chords from these 400 segments and compare them with the specified chords to calculate the Similarity Index (SIM) for relative pitch accuracy, which calculates the proportion of the correct chords throughout the music. Meanwhile, we allow for a global shift in the key of the chords depending on the mode. Additionally, we compute the SIM for songs generated by the GPT-only model without chord conditions, serving as a reference baseline for uncontrolled generation.\nThe results for different methods are presented in Table 1. Both the GPT-only model and our proposed model exhibit lower FAD than Jukebox, attributable to the utilization of a vocoder based on Stable Audio, which enhances the fidelity of the generated audio. Besides, due to the introduction of chord control, our model exhibits a slight increase in FAD compared to GPT-only, while simultaneously demonstrating a significant improvement in SIM relative to conditions without chord control.\nThe observed increase in FAD when incorporating control conditions is a typical phenomenon in music generation, and the improvement of SIM indicates that our model substantially enhances control over chords at the cost of a slight decrease in generation fidelity."}, {"title": "3.3.3. Ablation Study", "content": "We conduct ablation studies to further investigate the effectiveness of the attention mechanism with DWS. As shown in the last two rows of Table 1, simple integrated methods including concatenation and cross-attention fail to identify inaccurate chords during training. This failure leads to a disruption in recognizing correct chords during inference, resulting in diminished control precision, as indicated by SIM and CA. Additionally, this failure also hampers the model's capacity to accurately determine the relationships between chords and music, leading to a reduction in the musicality of the generated songs, as evidenced by MP. The inaccuracy also slightly decreases the generation fidelity, as shown by FAD."}, {"title": "3.3.4. Case Study", "content": "Beyond the quantitative metrics introduced earlier, the control precision of the proposed method can be directly demonstrated through qualitative analysis of spectrograms synthesized by various models under the same input conditions (Figure 2). When using chord condition as \"6451\", i.e., \"A:min-F:maj-G:maj-C:maj\", our proposed model generates a song with accurate chords, where the chord \"1\" concurrently resides in both notes \"C2\" and \"C3\". With the same chord and lyrics conditions, the concatenation model generates a song with chords \"6651\", and the cross-attention model generates a song with chords \"6454565\". Without using chords for control, controllable chord information is hard to be seen in the spectrogram."}, {"title": "4. Conclusion", "content": "In this work, we introduce a novel chord-conditioned song generation method, termed CSG, featuring our innovative Attention mechanism with DWS. This mechanism not only integrates chords with lyrics and songs but also reduces the impact of inaccurate chord data at the frame level. Experimental results demonstrate that CSG surpasses competing methods in musical performance through effective utilization of chord information, and Attention with DWS significantly enhances the musicality and control precision of generated songs."}]}