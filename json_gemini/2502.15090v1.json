{"title": "Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans", "authors": ["Masha Fedzechkina", "Eleonora Gualdoni", "Sinead Williamson", "Katherine Metcalf", "Skyler Seto", "Barry-John Theobald"], "abstract": "Modern large language models (LLMs) achieve impressive performance on some tasks, while exhibiting distinctly non-human-like behaviors on others. This raises the question of how well the LLM's learned representations align with human representations. In this work, we introduce a novel approach to the study of representation alignment: we adopt a method from research on activation steering to identify neurons responsible for specific concepts (e.g., \"cat\") and then analyze the corresponding activation patterns. Our findings reveal that LLM representations closely align with human representations inferred from behavioral data. Notably, this alignment surpasses that of word embeddings, which have been center stage in prior work on human and model alignment. Additionally, our approach enables a more granular view of how LLMs represent concepts. Specifically, we show that LLMs organize concepts in a way that reflects hierarchical relationships interpretable to humans (e.g., \u201canimal\u201d-\u201cdog\u201d).", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) exhibit impressive performance on a variety of tasks from text summarization (Basyal and Sanghvi, 2023; Jin et al., 2024) to zero-shot common-sense reasoning (Park et al., 2024; Shwartz et al., 2020), and are increasingly deployed as a human proxy (Just et al., 2024; Klissarov et al., 2023; Cui et al., 2024; Peng et al., 2024). At the same time, there is a growing body of evidence suggesting that LLMs exhibit patterns of behavior distinctly different from humans - for instance, hallucinating information (Bubeck et al., 2023; Lin et al., 2022) or memorizing complex patterns to solve reasoning tasks (Ullman, 2023). Such behaviors raise the question of how closely the conceptual representations learned by these models align with the conceptual representations in humans as safe and trustworthy deployment of LLMs requires such alignment. Overall, unveiling aspects of representation alignment and understanding how to foster it can help us identify and mitigate misaligned LLM behaviors, thus increasing trust in and safety of models (OpenAI et al., 2024; Shen et al., 2024).\nPrior work has examined the relationship between human-perceived similarity among concepts (i.e., word/image meaning) and various LLM-derived measures of similarity, such as confidence (Shaki et al., 2023) or the embedding distance (Bruni et al., 2012; Digutsch and Kosinski, 2023; Muttenthaler et al., 2023). While these approaches have significantly advanced our understanding of how conceptual representations align between humans and models, they suffer from a major limitation: they do not reveal where in the model the concepts are stored and make it difficult to draw conclusions beyond coarse alignment. For example, the cosine distance between embeddings might indicate that \"animal\" and \"dog\" are more similar than \"animal\" and \"daffodil\", but it can not tell us if \"dog\" and \"animal\" are processed with similar neural pathways or architectural components, limiting our ability to understand the existence of structures such as hierarchical relationships in the model.\nHere, we propose a novel way to study human - LLM alignment in concept representation. We borrow a method from activation steering (Suau et al., 2023, 2024; Rodriguez et al., 2025), to identify which neurons are most responsible for processing and understanding of a particular concept, so-called expert neurons. This approach enables us not only to measure alignment between human and model representations, but also to explore additional questions, such as whether LLMs organize concepts in a hierarchy interpretable to humans (e.g., \"dog\", \"cat\u201d, and \u201ccheetah\u201d being categorized as \"animal\"). We also track how alignment evolves during training for different model sizes, shedding light on the impact of model capacity on the development of aligned representations - an aspect largely overlooked in previous work on text-based models (Shen et al., 2024; Wei et al., 2022). Ultimately, understanding these internal structures and factors that lead to mis-alignment can provide valuable insight for designing interventions targeted at guiding model behaviors towards human-like solutions and enhancing their transparency (Fel et al., 2022; Peterson et al., 2018; Toneva, 2022).\nIn our experiments, we focus on causal LLMs using the Pythia models (70m, 1b and 12b) for which multiple training checkpoints are publicly available (Biderman et al., 2023). Given a diverse set of concepts across multiple domains (see Sec. 3.2), we identify each LLM's corresponding expert neurons. We measure their similarity at the LLM level as the amount of overlap between the expert neurons. We then evaluate the alignment between human and LLM representations by testing whether the similarity between neural activations correlates with human-perceived concept similarity, and whether the LLMs learn hierarchical structures similar to those observed in human category systems (Rosch, 1978). Finally, we identify the location of the model's concept representations and the point they form during training.\nOur results show that LLM representations are generally aligned with humans. Crucially, expert neurons capture human alignment significantly better than the single word embeddings used in prior work. Moreover, such alignment emerges early in training, with model size playing only a small role: a 70m LLM is less aligned than a 1b or 12b LLM trained on the same data, but there is no difference between the larger models. Finally, patterns in expert neurons reveal that the LLMs show a human-like hierarchical organization of concepts."}, {"title": "Related work", "content": "Representation alignment Studies on the kinds of representations used by humans and machines have been of interest to many fields (e.g., cognitive science, neuroscience, and machine learning; Hebart et al., 2020; Khosla and Wehbe, 2022; Muttenthaler et al., 2023; Tian et al., 2022). Studies on representation alignment (Sucholutsky et al., 2024) look specifically at the extent to which the internal representations of humans and neural networks converge on a similar structure. Across vision and text domains, models show notable alignment with human similarity judgments - typically used as a window into human representational structures. Peterson et al. (2018) report significant alignment between human similarity judgments and representations of object classification networks, while Digutsch and Kosinski (2023) report similar alignment with GPT-3's (Brown et al., 2020) embeddings. However, Shaki et al. (2023) finds that GPT-3's concept alignment is highly sensitive to prompt phrasing and Misra et al. (2020) show that alignment in BERT (Devlin et al., 2019) is very context-dependent. Investigating general factors that can cause mis-alignment, Muttenthaler et al. (2023) conclude that the training dataset and objective function impact alignment, but model scale and architecture have no significant effect. Of note, alignment and performance are not inherently tied: mis-aligned models can exhibit significant capabilities (Sucholutsky and Griffiths, 2023; Dess\u00ec et al., 2022).\nActivation steering refers to a class of methods that intervene on a generative model's activations to perform targeted updates for controllable generation (Rodriguez et al., 2025; Li et al., 2024; Rimsky et al., 2024). Suau et al. (2023) propose a method to identify sets of neurons in pre-trained transformer models that are responsible for detecting inputs in a specific style (Suau et al., 2024, e.g., toxic language) or about a specific concept (Suau et al., 2023, e.g., \"dog\"). Intervening on the expert neuron activations, successfully guides text generation into the desired direction. In a similar spirit, Turner et al. (2024) use a contrastive prompt (one positive and one negative) to induce sentiment shift and detoxification, while Kojima et al. (2024) steer multilingual models to produce more target language tokens in open-ended generation. Finally, Rodriguez et al. (2025) introduce a unified approach to steer activations in LLMs and diffusion models based on optimal transport theory."}, {"title": "Methods", "content": "We adopt the finding experts approach introduced by Suau et al. (2023) for activation steering, to study representational alignment. The motivation is two-fold: a) this approach has been successfully applied to detect neurons responsible for everyday concepts like \u201cdog\u201d, which is the focus of this work; b) it is able to distinguish the different senses of a homophone (e.g., \u201capple\u201d as a fruit or company), suggesting that this method is able to pick up fine-grained semantic distinctions.\nTo identify experts neurons for a given concept, each neuron is evaluated in isolation as a binary classifier: a neuron is considered an expert if its activations effectively distinguish between input data where the concept is present (henceforth positive set) and input data where the concept is absent (henceforth negative set). The performance of each neuron as a classifier for the concept (i.e., its expertise) is measured as the area under the precision-recall curve (AP). We consider neurons with an AP score above a given threshold, T, for a concept to be expert neurons for that concept. T can be thought of as quality of an expert neuron the larger the T values the greater a neuron's expertise for a given concept. In our experiments, we consider a range of values for \\( \\tau \\in [0.5,0.9] \\) ranging from a low (classification accuracy above chance) to a high level of expertise."}, {"title": "Data", "content": "To understand the alignment between human and model representations, we examine how patterns in expert neurons relate to perceived concept similarity in humans. We obtain human similarity judgments from the MEN dataset (Bruni et al., 2014), which contains 3,000 word pairs annotated with human-assigned similarity judgments crowdsourced from Amazon Mechanical Turk.\nFor each concept under consideration, we generate a set of sentences containing that concept. To ensure dataset diversity, half of each positive dataset is generated with a prompt eliciting story descriptions and half of the dataset is generated with a prompt eliciting factual descriptions of the target concept (the prompts, along with sample generations, are provided in App. A). The negative sets are sampled from the datasets for the remaining non-target concepts (e.g., if we are considering 1000 concepts, one of which is \u201ccat\u201d, the negative set is sampled from 999 concepts excluding \"cat\").\nTo study whether the LLMs represent concepts hierarchically (Sec. 5), we manually generate lists of ten domains, organized in human-interpretable hierarchies with four concepts per domain (e.g., the domain \u201canimal\u201d containing concepts \u201ccat\u201d, \u201cdog\u201d, \"cheetah\", and \"horse\"; the full set of domains and concepts is provided in App. C). We choose not to use WordNet (Miller, 1994) a lexical database of English annotated with a hierarchical structure because of drawbacks identified in its hierarchical structure, which often make the hierarchical relationships it presents unintuitive (for a discussion, see Gangemi et al., 2001).\nFor dataset generation, we experiment with three models of different performance levels: GPT-4 (OpenAI et al., 2024), Mistral-7b-Instruct-v0.2 (Jiang et al., 2023), and an internal 80b-chat model."}, {"title": "Models", "content": "We use GPT-2 (Radford et al., 2019) to select hyperparameters (e.g., the size of a positive and negative datasets) and validate that our data identifies a stable set of experts (see Sec. 4 for details). For all other experiments, we use models from the Pythia family (Biderman et al., 2023), specifically focusing on model sizes 70m (smallest), 1b, and 12b (largest), to understand the impact of model size on representational alignment. The size of each model is connected to its performance. The mean accuracy and standard error across eight benchmarks (Table 1) is 0.27 (0.01) for the 70m, 0.28 (0.01) for the 1b model, and 0.32 (0.02) for the 12b model at the end of training."}, {"title": "Can we reliably identify experts?", "content": "While the success of expert-based methods at steering model activations is well-documented (Suau et al., 2023, 2024), our interest is in studying model representations through the patterns in experts. Given the novel application of the method, we conduct a pilot study to explore the impact of dataset size, the model used to generate the dataset, and the exact sentences used to represent a concept on the stability of the discovered expert sets.\nFor the pilot study, we sample 50 word pairs from the training split of the MEN dataset. For each concept in the word pair, we generate a positive set containing 7000 sentences from three models: GPT-4, Mistral-7b-Instruct-v0.2, and an internal 80b-chat model. We sweep over positive set sizes of 100, 200, 300, 400, and 500 sentences, and negative set sizes of 1000 and 2000 sentences. For each positive and negative set combination, we repeat expert extraction eight times (folds) with the sets randomly sampled from the full pool of sentences.\nWe examine how sensitive the discovered experts are to the specific slice of the positive and negative sets (the 8 folds). We measure sensitivity in terms of the stability in experts across the folds, where high stability occurs when there is large overlap in the experts across folds. To assess overlap, we look at Jaccard similarity between expert sets across folds, using a range of thresholds \u03c4.\nThe findings are shown in Fig. 1 for each dataset configuration (subplot) and value of 7 (x-axis). The expert neurons discovered across different data configurations and folds (indicated by the error bars) are stable as indicated by a high (~ 0.8) overlap proportion and show little sensitivity to our manipulations. Interestingly, the LLM (line color) used to generate the probing dataset matters little while stronger models generate more diverse datasets (mean type/token ratio of 0.34, 0.21 and 0.18 for GPT-4, internal 80b-chat, and Mistral-7b-Instruct-v0.2 respectively), resulting in a somewhat higher expert overlap, the gain is too small to warrant their increased cost. Expert overlap increases with every increase in the size of the positive set but the increases are small beyond 300 sentences, and performance for 400 sentences is virtually indistinguishable from 500 sentences. Interestingly, a larger negative set results in lower expert overlap at higher T values and an increased variability across folds. One reason could be that as the size of the negative set increases so does the probability of the negative set containing sentences related to the target concept. For example, a sentence about \u201ccats\" may also talk about \u201cdogs\u201d. A second explanation could be that the larger negative set activates more polysemous neurons. Based on these findings, we conduct all subsequent analyses with a positive set of 400 sentences and a negative set of 1000 sentences, all generated with Mistral-7b-Instruct-v0.2."}, {"title": "Are model and human representations aligned?", "content": "Having determined the appropriate hyperparameters to capture a stable set of experts, we turn to the first main question of our study whether expert neurons capture semantic information meaningful to humans. We measure the alignment between LLM and human representations as the correlation between the human versus the LLM's similarity score for a each pair of concepts in the test split of the MEN data (1000 pairs). The LLM's similarity score is the Jaccard similarity between expert sets for \\( \\tau \\in \\{0.5,0.6, 0.7, 0.8, 0.9\\} \\). In App. B, we consider cosine similarity between the raw AP values as an LLM similarity score, finding very similar correlations to those obtained with Jaccard similarity (7 = 0.5), suggesting that what matters most for alignment is not the magnitude of the AP value, but rather whether it is above or below 0.5 (i.e., whether the neuron is positively or negatively associated with the concept).\nExpert neuron overlap is highly aligned with human similarity judgments We find that model representations are closely aligned with humans, with the highest alignment occurring at T = 0.5. At the final checkpoint, the Spearman correlations between expert overlap (r = 0.5) and MEN similarity are: 0.70, 0.77, 0.79 for 70m, 1b, and 12b respectively. For reference, agreement between humans has a Spearman correlation of 0.84. Interestingly, model size has a small impact on this alignment (in line with findings from Muttenthaler et al., 2023): the 1b and 12b models are virtually indistinguishable, with the 70m model slightly less aligned. The models start diverging in how well aligned they are with humans as t increases, with larger models being more aligned. The reason for this is that smaller models have fewer experts compared to larger models (see Fig. 5) resulting in a lot of empty expert set intersections for higher levels of \u0442.\nWord embeddings are less aligned than expert sets Prior work has focused on the analysis of embeddings when considering alignment in LLM and human representations (Digutsch and Kosinski, 2023). We hypothesize that expert sets are more correlated with human representations than word embeddings as they disambiguate different word senses (Suau et al., 2023). To test this, we extract the embeddings for each word in the MEN test split from the final hidden layer of the three Pythia models at each checkpoint and compute cosine similarity between the embeddings for each word pair in the MEN test split. We then correlate the cosine similarity with the corresponding human similarity judgement (Digutsch and Kosinski, 2023). We find statistically significant correlations (p<0.05) between the cosine similarity of the embeddings for a given concept pair and their similarity in the MEN dataset, at all checkpoints except for the first (see Fig. 3), consistent with prior work (Digutsch and Kosinski, 2023). However, as expected under our hypothesis, the correlations with human similarity are significantly lower for single word embeddings compared to the experts neurons (highest correlations are 0.25 vs. 0.79 for the embeddings vs. experts). Single word embeddings exhibit more variability in alignment, as indicated by larger confidence intervals within each checkpoint, and their pattern of alignment is less stable across checkpoints compared to that of the experts."}, {"title": "Do models organize concepts in hierarchies?", "content": "Some domains within the human conceptual system are organized in hierarchies, where broader categories include more specific categories. For example, the concept \"dog\" falls under \u201canimal\u201d, meaning that all dogs are animals (Graf et al., 2016; Murphy, 2004; Rosch, 1978). This raises the question of whether models organize concepts in a human-like hierarchy. We propose that, if the model organizes domains in a hierarchical fashion interpretable to humans, concepts from related sub-categories should share a set of experts (e.g., \u201cdog\u201d, \u201ccat\u201d, \u201chorse\u201d, and \u201ccheetah\u201d under the concept \"animal\"). Additionally, some of these shared experts should also be associated with the broader concept (\u201canimal\u201d in our example), suggesting that the model recognizes it as an overarching concept that includes its sub-categories.\nTo assess this, we consider the list of hierarchically organized domains we generated (see Sec. 3.2 and App. C), the experts associated with each concept in the list (7=0.5), and their reciprocal overlap. We discuss the final training checkpoint of Pythia 12b in the main text and present other model sizes and checkpoints in App. D.\nLet super be a super-ordinate concept (e.g., \"animal\") and let sub\u2081, sub2, ...subi be sub-ordinate concepts falling under it (e.g., \"dog\", \"cat\", \"horse\", \"cheetah\"). We call E(c) the set of experts specialized for a concept c. For each domain, Table 2 reports the percentage of experts shared among the sub-ordinate concepts that are also shared with the super-ordinate concept:\n\\begin{equation}\n\\frac{|E(\\text{super}) \\cap ( \\cap_{i=1}^{n} E(\\text{sub}\\_i))|}{| \\cap_{i=1}^{n} E(\\text{sub}\\_i)|} \\times 100.\n\\end{equation}\nFor example, Table 2 indicates that 42.46% of the experts shared by \"dog\u201d, \u201ccat\u201d, \u201chorse\", and \"cheetah\" are also shared with \"animal\".\nOur results confirm that the model captures hierarchical domain structures that characterize human conceptual systems. Within each domain, a large portion of experts for sub-ordinate concepts is shared with the super-ordinate concept. This suggests that the model recognizes the sub-ordinate concepts as part of the super-ordinate concept (e.g., that dogs are animals).\nTo rule out the possibility that expert sets overlap by chance, we compare the values we have obtained against two baselines (see Table 2, numbers in gray). In the first baseline, we randomize the super-ordinate concepts in our dataset by assigning each of them to N randomly selected sub-ordinates (e.g., associating \u201canimal\u201d with a random list of concepts like \"jacket\u201d, \u201cliver\u201d, \u201cdoctor\u201d, and \u201cred\u201d). In the second baseline, we shuffle the associations across concept categories, assigning each superordinate concept to a random set of internally related sub-ordinates (e.g., associating \u201canimal\u201d with \u201csock\u201d, \u201cshirt\", \"jeans\u201d, and \u201cjacket\u201d).\u00b9 The results show that, when the domain structure is randomized, no hierarchical pattern emerges, reinforcing the robustness of our findings.\nInterestingly, the patterns observed for the largest model do not largely differ from those found for smaller models. Additionally, the model seems to converge on a stable hierarchical representation around checkpoint 4k (App. D). This finding aligns with our later analyses, highlighting this checkpoint as a crucial transition point during training.\nHaving examined the overall structure of hierarchical"}, {"title": "Characterizing model knowledge", "content": "We conclude by characterizing the differences in experts as a function of model size and stage of training by reanalyzing the data from Sec. 5.\nLarger models have more experts Larger models allocate more experts to a given concept (see Fig. 5; the pattern does not change after scaling the raw number of experts by the number of neurons in the model). As t increases, fewer experts are identified and the drop is more pronounced for smaller models. Overall, larger models have a greater capacity to learn a higher number of experts and a higher number of more specialized experts. This increased specialization may contribute to finer-grained concept representations and ultimately better performance on downstream tasks.\nMore specialized experts take longer to learn We next look at the dynamics of learning experts across checkpoints. We calculate expert overlap (Jaccard similarity) for each concept across subsequent checkpoints in our data. As shown in Fig. 6, the stability of the discovered expert set grows as training progresses. Early in training (prior to step 36k), the expert overlap between subsequent checkpoints is low across model sizes, suggesting that semantic knowledge has not been acquired yet. The more T increases (corresponding to higher expert specialization), the more checkpoints it takes for the expert set to stabilize, suggesting that higher-quality experts take longer to learn.\nMore experts are found in MLPs and deeper layers Pythia models consist of intertwined self-attention and MLP layers (Biderman et al., 2023; Vaswani et al., 2023), each serving different functions (Geva et al., 2021; Jawahar et al., 2019; Liu et al., 2019). We analyze the distribution of experts within these layers. Fig. 7a shows the patterns for Pythia 12b (T=0.5). Larger numbers of experts are located in the MLP layers compared to attention layers with the allocation of experts to different layer types stabilizing at checkpoint 4k. We see the same trend in smaller models (App. F.1) after controlling for the number of neurons in the respective layers. Moreover, the mean number of experts generally increases with layer depth in MLPs, with checkpoint 4k again displaying the first recognizable structure (see Fig. 7b and App. F.2). For attention layers, high numbers of experts are located in deep layers and, interestingly, in the first layer (see App. F.3). Of note, if we focus our analysis on highly specialized experts only (r=0.9), we find higher numbers of experts in earlier layers (see App. F.8 and F.9), recovering the same patterns as identified in Suau et al. (2020). Our findings align with prior research on the role of layers at different depths, identifying deeper layers as responsible for processing higher-level semantic knowledge captured by expert neurons (Geva et al., 2021; Jawahar et al., 2019)."}, {"title": "Conclusion", "content": "We present a novel approach to study alignment between human and model representations based on the patterns in expert neurons. Representations captured by these neurons align with human representations significantly more than word embeddings, and approach the levels of alignment between humans. Consistent with prior work (Muttenthaler et al., 2023), we find that model size has little influence on alignment. Moreover, our approach reveals that models generally organize concepts into human-interpretable hierarchies. However, some domains are more structured than others, and this pattern remains consistent across model sizes. We leave it to future work to investigate the factors that could give rise to this pattern, such as the frequency of each domain in the training data."}, {"title": "Limitations", "content": "We consider only a simple case of similarity Consistent with prior work (Digutsch and Kosinski, 2023; Shaki et al., 2023; Misra et al., 2020), we study alignment between human and model representations, which we operationalize as the similarity between two concepts. We find that model size does not play a large role in alignment: even models as small as 70m excel in this alignment test. While this finding is consistent with previous literature (Muttenthaler et al., 2023), it is also possible that our task is too simple to distinguish between the models. This is supported by the observations that semantic relationships studied here start emerging early in training (around checkpoint 4k out of 143k). Future work will consider more complex cases of alignment, such as value alignment.\nWe do not study patterns in expert neurons through activating these neurons Since the approach we are using was designed for activation steering (Suau et al., 2023), one obvious application is to examine the intersections between the expert sets for two concepts through the lens of controllable generation. For instance, we could have activated the shared experts between \u201canimal\" and \"dog\" and examined model generations after the activation. We chose not to do this for the following reason: the approach we are using requires choosing the number of experts and the original work (Suau et al., 2023) has shown that this choice impacts the quality of generations and the degree to which a concept is expressed an effect that we also observed in our preliminary investigations. We leave such hyper-parameter search to future work: a priori, we do not have a clear hypothesis about whether activating more specialized experts vs less specialized ones within the intersection would lead to distinct generation patterns; or if any discernible pattern in those generations should be expected at all. Given these uncertainties, we did not feel confident that this analysis would yield reliable results. Other approaches do not require choosing the number of experts (Rodriguez et al., 2025), but these approaches are designed to change the activations of all neurons in the network and are thus not applicable for our use case.\nWe do not have access to training data To fully understand how knowledge develops in LLMs, we need to know what the model has seen at different points in training. Unfortunately, the Pile (Gao et al., 2020) that Pythia models were trained on is no longer available.\nModel choice Given the nature of our research question, it is crucial to be able to analyze multiple checkpoints from models of varying sizes, prioritizing interpretability over direct evaluations of model performance. For this reason, we rely on the Pythia family of models, publicly released in the interest of fostering interpretability research. We leave to future work the exploration of alignment and its emergence in alternative model families (e.g., the recent OLMo 2 family; Walsh et al., 2025)."}, {"title": "Prompts used for probing dataset generation and sample generations", "content": "Fact prompt: \"Generate a set of 10 sentences, including as many facts as possible, about the concept [concept name] as [a/an] [adjective/noun/verb] and defined as [WordNet definition]. Refer to the concept only as [concept name] without including specific classes, types, or names of [concept name]. Make sure the sentences are diverse and do not repeat.\"\nSample fact sentences for concept poppy defined as 'annual or biennial or perennial herbs having showy flowers':\nGPT-4: Gardeners often classify poppies as easy to care for due to their hardy nature.\nMistral-7b-Instruct-v0.2: As the farmer tended to his fields, he couldn't help but admire the poppies that grew among his crops, their beauty a welcome distraction.\nInternal 80b-chat model: Poppies have been used in traditional medicine for centuries, with various parts of the plant being employed to treat ailments like pain, insomnia, and digestive problems.\nStory prompt: \u201cGenerate a set of 10 sentences, where each sentence is a short story about the concept [concept name] as [a/an] [adjective/noun/verb] and defined as [WordNet definition]. Refer to the concept only as [concept name] without including specific classes, types, or names of [concept name]. Make sure the sentences are diverse and do not repeat.\"\nSample story sentences for concept poppy defined as 'annual or biennial or perennial herbs having showy flowers':\nGPT-4: As the wedding gift from her grandmother, a dried poppy was framed and hung on her wall.\nMistral-7b-Instruct-v0.2: Poppies are herbaceous plants that can grow annually, biennially, or perennially, depending on the specific species.\nInternal 80b-chat model: The poppy, a harbinger of spring, adorned the hillsides with a colorful tapestry, signaling the end of winter's slumber."}, {"title": "Computational budget", "content": "The concept dataset was parallelized over 8 A100 GPUs (80GB). Expert extraction took about 136 seconds per concept for the 12b Pythia model; about 27 seconds per concept for the 1b Pythia model; about 8 seconds per concept for the 70m Pythia model; and about 25 seconds per concept for GPT-2."}, {"title": "License and Attribution", "content": "The MEN dataset used in this work is released under Creative Commons Attribute license. The pretrained models are supported by public licenses the Pythia Scaling Suite (Apache), Mistral (Apache), and GPT-2 (MIT). GPT-4 is supported a proprietary license. We use an internal 80b-chat model and are unable to provide license information on it at this time."}]}