{"title": "ConceptExpress: Harnessing Diffusion Models for Single-image Unsupervised Concept Extraction", "authors": ["Shaozhe Hao", "Kai Han", "Zhengyao Lv", "Shihao Zhao", "Kwan-Yee K. Wong"], "abstract": "While personalized text-to-image generation has enabled the learning of a single concept from multiple images, a more practical yet challenging scenario involves learning multiple concepts within a single image. However, existing works tackling this scenario heavily rely on extensive human annotations. In this paper, we introduce a novel task named Unsupervised Concept Extraction (UCE) that considers an un-supervised setting without any human knowledge of the concepts. Given an image that contains multiple concepts, the task aims to extract and recreate individual concepts solely relying on the existing knowledge from pretrained diffusion models. To achieve this, we present ConceptExpress that tackles UCE by unleashing the inherent capabilities of pretrained diffusion models in two aspects. Specifically, a concept localization ap-proach automatically locates and disentangles salient concepts by lever-aging spatial correspondence from diffusion self-attention; and based on the lookup association between a concept and a conceptual token, a concept-wise optimization process learns discriminative tokens that rep-resent each individual concept. Finally, we establish an evaluation proto-col tailored for the UCE task. Extensive experiments demonstrate that Concept Express is a promising solution to the UCE task. Our code and data are available at: https://github.com/haoosz/ConceptExpress", "sections": [{"title": "1 Introduction", "content": "After observing an image containing multiple concepts, a skilled painter can recreate each individual concept within the complex scene. This remarkable cog-nitive ability prompts us to raise an intriguing question: Do text-to-image gener-ative models also possess the capability to extract and recreate concepts? In this paper, we try to provide an answer to this question by harnessing the potential of Stable Diffusion [54] in concept extraction.\nDiffusion models [23, 45, 50, 54, 57, 62] have exhibited unprecedented per-formance in photorealistic text-to-image generation. Although diffusion models"}, {"title": "2 Related Work", "content": "Text-to-image synthesis In the realm of GANs [8,20,29\u201331], plenty of works have gained remarkable advancements in text-to-image generation [52, 64, 77,78, 80,86] and text-driven image manipulation [1, 18, 47, 75], significantly pushing forward image synthesis conditioned on plain text. Content-rich text-to-image generation is achieved by auto-regressive models [51,79] that are trained on large-scale text-image data. Based on the pretrained CLIP [49], Crowson et al. [14] optimizes the generated image at test time using CLIP similarity without any training. Diffusion-based methods [23] have pushed the boundaries of text-to-image generation to a new level, e.g., DALLE 2 [50], Imagen [57], GLIDE [45], and LDM [54]. Based on the implementation of LDMs [54], Stable Diffusion (SD), large-scale trained on LAION-5B [59], achieves unprecedented text-to-image synthesis performance. Diffusion models are widely used for various tasks such as controllable generation [82,85], global [9,67] and local editing [3, 5, 13, 32, 46, 70], video generation [24,61,74] and editing [43,83], inpainting [41], and scene generation [4,6].\nGenerative concept learning Recently, many works [12,16,17,21,25,36,38, 42, 48, 55, 60, 65, 73] have emerged, aiming to learn a generative concept from multiple images. For example, Textual Inversion [16] learns an embedding vec-tor that represents a concept in the textual embedding space. Liu et al. [40]"}, {"title": "3 Unsupervised Concept Extraction", "content": "We aim to learn discriminative tokens that can represent multiple instance-level concepts from a single image in an unsupervised manner. Specifically, given an image I containing multiple salient instances, we use a pretrained text-to-image model to discover a set of conceptual tokens and their corresponding embedding vectors , which capture discriminative concepts from I. The concept number N is automatically determined in the discovery process. By prompting the i-th token , we can recreate the corresponding concept extracted from I. We present ConceptExpress to tackle this problem. Fig. 2 gives an overview of ConceptExpress.\n3.1 Preliminary\nText-to-image diffusion model [54] is composed of a pretrained autoencoder with an encoder E to extract latent codes and a corresponding decoder D to reconstruct images, a CLIP [49] text encoder that extracts text embeddings, and a denoising U-Net 6 with text-conditional cross-attention blocks. Textual inversion [16] represents a particular concept using a learnable embedding vector v*, which is optimized using a standard latent denoising loss with eo frozen, written as\n$L = E_{z~E(I),y,\u20ac~N(0,1),t} [||\u20ac \u2013 \u20ac\u0473(zt, t, C_{v*} (y))||_{2}^{2}]$,   (1)\nwhere t is the timestep, zt is the latent code at timestep t, e is the randomly sampled Gaussian noise, y is the text prompt, and cu is the text encoder pa-rameterized by the learnable v. ConceptExpress advances further by learning multiple embedding vectors in an unsupervised setting.\nFINCH [58] is an efficient parameter-free hierarchical clustering method. Given a set of n sample points in d dimensions, denoted as , we construct an adjacent matrix G for paired samples as\nG(i, j) = { 1 if k_{i} = j or k_{j} = i or k_{i} = k_{j} \n0 otherwise   (2)\nwhere ki represents the index of the closest sample to si \u2208 Sunder a specific dis-tance metric. To obtain a sample partition, we group the connected components within the undirected graph defined by the adjacency matrix G. Each connected component in the graph represents a cluster, and the centroids of the clusters are treated as super sample points for constructing a new adjacent matrix. This process enables iterative hierarchical clustering until all samples are grouped. As a result, multiple clustering levels of varying granularity are generated.\n3.2 Automatic Latent Concept Localization\nWe begin by locating instance-level concepts within the diffusion latent space. In pretrained diffusion models, self-attention possesses good properties of spa-tial correspondence which offers the inherent benefit as an unsupervised semantic"}, {"title": "3.3 Concept-wise Masked Denoising", "content": "We construct a token lookup table\nT_{lookup} := {[V_{i}]: (v_{i}, m_{i}, f_{i}) | i = 1,2,\u2026\u2026\u2026, N}   (7)\nwhere the i-th conceptual token [Vi] corresponds to a learnable embedding vec-tor vi, a latent mask , and a mean attention map ."}, {"title": "3.4 Implementation Details", "content": "We train the tokens in two phases for a total of 500 steps, with a learning rate of 5e-4. In the first 100 steps, we optimize the tokens using\n$L = \\frac{1}{gxN} \\sum_{i=1}^{N} \\sum_{j=1}^{g} (L_{i,j} + \u03b1L_{i,j}^{con} + \u03b2L_{i,j}^{reg}). $  (12)\nWe then merge the tokens, deriving [Vi] \u2208 Tlookup, and optimize them in the subsequent 400 steps using\n$L = \\frac{1}{N} \\sum_{i=1}^{N}(L_{i} + \u03b2L_{i}^{reg}). $  (13)\nWe use Stable Diffusion v2-1 [54] as our base model. We set \u03b1=1e-3, \u03b2=1e-5, \u03c4=0.07, and g=5. All experiments are conducted on a single RTX 3090 GPU. In our implementation, self-attention used in concept localization is computed using the unconditional text prompt at timestep 0, which induces minimal textual intervention and maximal denoising of the given image."}, {"title": "4 Experiments", "content": "4.1 Dataset and Baseline\nDataset In our work, we do not rely on predefined object masks or manually selected initial words for training images. This allows us to gather high-quality images from the Internet without human annotations to form our dataset. Specifically, we collect a set D1 of 96\u00b9 images from Unsplash\u00b2, ensuring that each\n1 96 is considerably large compared to the dataset sizes in the previous works, such as 30 in DreamBooth [55], 50 in Break-A-Scene [2], and 10 in DisenDiff [84].\n2 https://unsplash.com/"}, {"title": "4.2 Evaluation Metric", "content": "We establish an evaluation protocol for the UCE problem, which includes two tailored metrics described as follows.\nConcept similarity To quantify how well the model is able to recreate the concepts accurately, we evaluate the concept similarity, including identity sim-ilarity (SIM\u00b9) and compositional similarity (SIMC). Identity similarity mea-sures the similarity between each concept in the training image and the concept-specific generated images. We employ CLIP [49] and DINO [10] to compute the similarities. To ensure that the similarity is computed specifically for the i-th concept, we obtain concept-wise masks with SAM [33] by identifying the specific SAM mask associated with our extracted concept. Specifically, for each concept, we prompt SAM with 3 randomly sampled points on our extracted mask to produce SAM masks. The training image is then masked with the SAM mask corresponding to the i-th concept. The metric of identity similarity provides a crucial criterion for evaluating the intra-concept performance of unsupervised concept extraction. Compositional similarity measures the CLIP or DINO sim-ilarity between the source image and the generated image, conditioned on the prompt \"a photo of [V1] and [V2] ... [VN]\". This metric quantifies the degree to which the source image can be reversed using the extracted concepts.\nClassification accuracy To assess the extent of disentanglement achieved for each concept within the full set of extracted concepts, we establish a benchmark that evaluates concept classification accuracy. Specifically, we first employ a vision encoder, such as CLIP [49] or DINO [10], to extract feature representations for each concept from the SAM-masked training images. In total, we obtain 264 concepts in D\u2081 and 19 concepts in D2. We use these concept features as prototypes to construct a concept classifier. We then employ the same vision"}, {"title": "4.3 Performance", "content": "Quantitative comparison We compare ConceptExpress with BaS\u2020 based on concept similarity and classification accuracy metrics. The quantitative compar-ison results on the two datasets are reported in Tabs. 1a and 1b, respectively with CLIP [49] and DINO [10] as the visual encoder. Notably, ConceptExpress outperforms BaS\u2020 by a significant margin on all evaluation metrics. It achieves higher concept similarity SIM\u00b9 and SIMC, indicating a closer alignment with the source concepts. It also achieves higher classification accuracy ACC\u00b9 and ACC\u00b3, indicating a more significant level of disentanglement among the individ-ually extracted concepts. These results highlight the limitations of the existing concept extraction approach [2] and establish ConceptExpress as the state-of-the-art method for the UCE problem.\nQualitative comparison We show several generation samples of Concept-Express and BaSt in Fig. 5. ConceptExpress presents overall better generation"}, {"title": "4.4 Ablation Study", "content": "We conduct a quantitative ablation study on the training components in Tab. 2. Effectiveness of split-and-merge strategy (SnM) By comparing Rows (0) and (1), we validate the benefit of the split-and-merge strategy to initializer-absent training. The split-and-merge strategy effectively improves identity sim-ilarity and classification accuracy while slightly sacrificing compositional sim-ilarity due to its strong focus on a single concept. In Fig. 6, we present the generated images at different training steps, which reveals how SnM rectifies the training direction. The results illustrate that SnM effectively expands the concept space, allowing learnable tokens to explore a wider range of concepts, ultimately resulting in a more faithful concept indicator.\nEffectiveness of regularization By comparing Rows (0) and (2) in Tab. 2, we observe that regularizing the attention map can enhance the generation per-formance of individual concepts. Row (3) is our full method which further im-proves the performance regarding all metrics compared to incorporating each component in Rows (1) and (2). The thorough ablation study indicates the ef-fectiveness of each training component in ConceptExpress."}, {"title": "4.5 Concept Localization Analysis", "content": "Self-attention clustering To validate the significance of our three-phase method for concept localization, we compare it with k-means and our base method FINCH [58]. Since k-means requires a predefined cluster number and FINCH requires a stopping point, we set a proper cluster number of 7 for them. After clustering, we apply the proposed filtering method for fair comparison. We"}, {"title": "4.6 Unsupervised vs. Supervised", "content": "Although ConceptExpress is an unsupervised model, it would be intriguing to compare ConceptExpress to some supervised methods. Motivated by this, we"}, {"title": "4.7 Text-prompted Generation", "content": "With the extracted generative concepts, we can perform text-prompted gener-ation. In Fig. 9, we showcase the results conditioned on various text prompts using both individual concepts and compositional concepts. The results demon-strate that the learned conceptual tokens can generate images with high text fidelity, aligning faithfully with the text prompt. Furthermore, the images gen-erated with the conceptual tokens also preserve consistent concept identity with the source concepts in both individual and compositional generation. Please refer to Appendix F for additional photorealistic results of text-prompted generation."}, {"title": "5 Conclusion", "content": "In this paper, we introduce Unsupervised Concept Extraction (UCE) that aims to leverage diffusion models to learn individual concepts from a single image in an unsupervised manner. We present ConceptExpress to tackle the UCE prob-lem by harnessing the capabilities of pretrained diffusion models to locate con-cepts and learn their corresponding conceptual tokens. Moreover, we establish an evaluation protocol for the UCE problem. Extensive experiments highlight Concept Express as a promising solution to the UCE task."}, {"title": "I Broader Impact", "content": "This paper presents a convenient and efficient concept extraction method that does not require external manual intervention. The extracted concepts can be used to generate new images. On one hand, this greatly facilitates the effort-less disentanglement of instances from an image and enables the generation of personalized images. It even allows for the creation of a vast concept library by processing a large batch of images, which can be archived for the use of swift generation. On the other hand, however, this technology can also be meticulously exploited to handle and generate sensitive images, such as violent, pornographic, or privacy-compromising content. Moreover, due to its unsupervised nature, or-ganizations with massive image datasets can easily perform concept extraction and build extensive concept libraries, which may include a substantial amount of harmful or sensitive content. We believe it is crucial to continuously observe and regulate this technology to ensure its responsible use in the future.\nLimitation ConceptExpress has the following limitations that remain to be addressed in future research. The first limitation is related to processing im-ages containing multiple instances from the same semantic category, such as two instances of a bird. In this case, self-attention correspondence struggles to dis-entangle these instances and tends to identify them as a single concept, rather than recognizing them as separate instances. The second limitation is that cer-tain concepts with a relatively small occurrence in the image may be discovered. This can result in poor concept learning due to the lack of sufficient information for reconstructing the concept in the latent space with a resolution of 64\u00d764. The third limitation is that our model requires a certain level of input image quality. In the future, it can be further enhanced to robustly handle uncurated natural data, making it more applicable to real-world scenarios."}, {"title": "A More Details on Implementation", "content": "Attention aggregation The self-attention maps in different layers have vary-ing resolutions. To aggregate them into a single map for further processing, we follow the approach used in [66]. Each self-attention map A1 [I, J, :, :] represents the correlation between the location (I, J) and all spatial locations. As a result, the last two dimensions of the self-attention maps have spatial consistency, and we interpolate them to ensure uniformity. On the other hand, the first two dimen-sions of the self-attention maps indicate the locations to which attention maps refer. Therefore, we duplicate these dimensions. By interpolation and duplica-tion, we align the self-attention maps in all layers to a common latent resolution (i.e., 64\u00d764). Finally, we compute the average of all attention maps to obtain the aggregated map. This aggregation step allows us to create a unified attention map combining all maps from different layers.\nConceptual token learning We utilize the split-and-merge strategy in train-ing conceptual tokens. In the training after splitting, we not only sample prompts of individual tokens but also sample a compositional prompt \u201ca photo of [V1] and [V2] ... [VN]\u201d for training. This approach enhances the compositionality of the learnable tokens. However, unlike [2], we refrain from using all possible composi-tions and instead only use the full composition. This decision is made to ensure that the proportion of using single tokens remains high during training. This, in turn, facilitates effective learning of each individual conceptual token.\nEarth mover's distance (EMD) We penalize attention alignment using the location-aware EMD. The EMD is formulated as an optimal transportation prob-lem. Suppose we have supplies of n, sources S = {si}=1 and demands of na des-tinations D = {dj} 1. Given the moving cost from the i-th source to the j-th destination cij, an optimal transportation problem aims to find the minimal-cost flow fij from sources to destinations:\n$minimize \\sum_{i=1}^{ns} \\sum_{j=1}^{nd} cijij$  (14)\n$subject to \\quad fij \u2265 0, i = 1, ..., ns, j = 1, ..., nd$  (15)\n$\\sum_{j=1}^{nd} fij = s_{i}, i = 1, ..., ns $  (16)\n$\\sum_{i=1}^{ns} fij = d_{j}, j = 1, ..., Nd$  (17)\nwhere the optimal flow fij is computed by the moving cost cij, the supplies si, and the demands dj. The EMD can be further formulated as (1 \u2013 cij) fij. In our problem, the cross-attention map c[v] represents the supply, while the target mean attention fi represents the demand. The moving cost is calculated as the Euclidean distance between spatial locations. Unlike the MSE, the EMD considers differences not only between elements at the same location but also between elements at different locations. This means that the EMD takes into account both spatial alignment and the magnitude of differences, providing a more comprehensive measure of dissimilarity."}, {"title": "B Concept Localization Benchmark", "content": "In the main paper, we present a novel benchmark to evaluate concept local-ization performance. This benchmark effectively assesses our model's concept localization capability in two aspects: (1) concept discovery accuracy and (2) concept segmentation efficacy. Here, we offer further details on the benchmark, including the dataset and evaluation metrics.\nDataset curation To assess concept localization, the benchmark dataset must meet two criteria: (1) clear and distinct concept definition, and (2) accurate ground-truth concept masks. Natural images lack these characteristics. Instead, we source our images from CLEVR [27], a dataset known for its well-defined objects, diverse in colors, materials, and shapes, set against a uniform grey back-ground. We collect a total of 25 images, each containing 3 to 5 concepts, along with their corresponding ground-truth segmentation masks. We show image sam-ples from the dataset alongside the generated images of our extracted concepts in Fig. 10 (left).\nEvaluation metrics We further introduce evaluation metrics tailored for con-cept localization. The process of concept localization incorporates two parts: (1) concept discovery and (2) concept segmentation. For these two parts, we devise three metrics: recall and precision to assess concept discovery, and av-erage intersection over union (IoU) to assess concept segmentation. Specifically, let P = denote the set of the N concept segments discovered by the model, and let Q = denote the set of the M ground-truth concept seg-"}, {"title": "C User Study", "content": "To ensure the assessment of generation quality aligns with human preference, we conduct a user study comparing the generated results from BaSt and Concept-Express. We asked 14 users to vote between our method and BaSt by viewing the generated images of 19 concepts from 7 images in D2. For each concept, we presented the users with 8 images, randomly generated by ConceptExpress and BaS\u2020 respectively, along with the masked image of the source concept. They were then asked to indicate which model produced images that better resembled the source concept. Finally, we collected a total of 266 user votes, representing human preference. Among all the votes, 18.8% of the votes favored BaSt while 81.2% preferred our model. Detailed statistics of the votes for each concept are present in Fig. 11. The user study further indicates that ConceptExpress out-performs BaSt in generating concept images that align with human judgment."}, {"title": "D Additional Ablation Studies", "content": "D.1 Self-attention Clustering\nIn Fig. 12, we present additional self-attention clustering results, comparing our approach with k-means and FINCH [58]. Upon observation, we note that k-means and FINCH may separate a single concept (as seen in the 1st, 2nd, and 3rd rows) or include background regions (as seen in the 4th row) within their clusters. In contrast, our approach consistently demonstrates high accuracy in locating each concept, ensuring precise concept localization within the image.\nD.2 Split-and-merge Strategy\nVisualization In Fig. 13, we provide two additional examples illustrating how the split-and-merge strategy rectifies the token learning process. The split-and-merge strategy expands the search space for concepts during the splitting phase, allowing the merged token to exhibit concept characteristics that closely align with the source concept. This improves the ability to learn and represent unseen concepts without initialization, ultimately enhancing image generation quality.\nEffect of contrastive loss We further investigate the impact of the con-trastive loss during the splitting phase. The contrastive loss encourages split tokens representing the same concept to be closer together, promoting better concept agreement across all tokens. In Fig. 14, we employ PCA to visualize the embedding vectors with and without the contrastive loss. When the contrastive loss is used, the embedding vectors representing the same concept exhibit a more compact distribution, aligning with our goal of enhancing concept representation. We also report the quantitative comparison in Tab. 4. The use of contrastive loss can enhance the performance on all metrics, especially on classification accuracy.\nEffect of token merging To validate the necessity of merging multiple tokens midway through optimization, we additionally evaluate a variant that optimizes the multiple randomly initialized tokens separately and merges them at the end. We compare the results in Tab. 5. Our method significantly outperforms the variant, especially in compositional similarity and classification accuracy."}, {"title": "E Additional Quantitative Analysis", "content": "E.1 Unsupervised vs. Supervised\nWe present quantitative results for unsupervised methods, namely ConceptEx-press and BaSt, as well as methods augmented with different types of supervision. The comparison results are presented in Tab. 7. By comparing (5) to (1)-(3), we"}, {"title": "E.2 Text Guidance", "content": "We also explore the performance of subject-driven text-to-image generation. To do this, we utilize a set of prompts to generate text-conditioned images with all extracted concepts and their compositions. We expand the set of prompts used in [2] from 10 to 15 in Tab. 8. We evaluate the generated images by measuring their CLIP image similarity with the masked source image, as well as their CLIP text similarity with the corresponding text prompt in Tab. 8 (with the learnable token removed)."}, {"title": "E.3 Larger Classifier", "content": "In the evaluation of classification accuracy, we can increase the number of pro-totypes in the classifier by including a large codebook of concepts besides the concepts in the datasets. Specifically, we randomly sample one image per class from ImageNet-1k [56], obtaining 1,000 images in total. We encode them as addi-tional concept prototypes in the classifier. We report the results of classification accuracy by using the larger classifier in Tab. 9. We note that the accuracy values under the larger classifier are considerably lower compared to those under the original classifier for both models. Nevertheless, our model consistently demon-strates significant superiority over BaS\u2020 across all evaluation criteria, regardless of whether the larger or the original classifier is employed."}, {"title": "E.4 Initializer Analysis", "content": "In our unsupervised setting, initial words for training each concept are inacces-sible because the concepts are automatically extracted, and human examination of each concept requires costly labor. We introduce the split-and-merge strategy to address this problem.\nDespite this, we still aim to explore the performance of models trained with different types of initializers in supervised settings and compare them with"}, {"title": "F Additional Comparison and Our Results", "content": "As a supplement to the main paper, we provide additional comparison results between Concept Express and Bast in Fig. 19. Furthermore, we present a broader range of generation examples from ConceptExpress in Figs. 20 and 21, showcas-"}, {"title": "G Human Interaction with SAM", "content": "Although our task does not demand annotated concept masks, we can also seam-lessly incorporate SAM [33] into our model to enable interactive concept extrac-tion. We showcase human interaction with SAM through point or box prompts in Fig. 17. This experiment demonstrates that our model can be seamlessly integrated with SAM in practice, enabling human interaction in the concept extraction process through explicit point or box prompts."}, {"title": "H Unsatisfactory Cases", "content": "In our analysis, we have noticed two limitations of ConceptExpress. The first limitation is its difficulty in distinguishing instances from the same semantic category. The second limitation is its struggle to accurately learn concepts for instances with a relatively small occurrence. We have discussed these limitations in Sec. 5 in the main paper.\nTo further illustrate these limitations, we provide examples of unsatisfactory cases in Fig. 18. The first case arises because similar patches of instances from the same category (such as bird wings) exhibit close distributions in self-attention maps, leading to early grouping in the pre-clustering phase. As a result, we localize the two birds as a single concept, which may affect the number of instances shown in the generated image. We considered including spatial bias to address this, but we found it might impede the normal grouping of complete concepts. Therefore, we opt for our current approach. It is important to note that this limitation does not significantly impact the overall generation quality. Our model preserves the integrity of complete concepts, ensuring they reflect bird characteristics rather than generating creatures with multiple heads. Despite the segmentation containing multiple instances, the model can still generate a single instance, as shown in row 1 column 3. The second case is caused by the limited occurrence of the target concept. Due to the low resolution (64\u00d764) of the latent space, the small region captures limited information, making it challenging to train an accurate concept representation. As a result, the small \"marble pedestal\" is learned through the diffusion process and is transformed into a representation resembling a \"marble church\". We believe that future research will aim to address these limitations."}]}