{"title": "A New Type of Foundation Model Based on Recordings of People's Emotions and Physiology", "authors": ["David Gamez", "Dionis Barcari", "Aliya Grig"], "abstract": "Foundation models have had a big impact in recent years and billions of dollars are being invested in them in the current AI boom. The more popular ones, such as Chat-GPT, are trained on large amounts of data from the Internet, and then reinforcement learning, RAG, prompt engineering and cognitive mod-elling are used to fine-tune and augment their behavior. This technology has been used to create models of individual people, such as Caryn Marjorie. However, these chatbots are not based on people's actual emotional and physiological re-sponses to their environment, so they are, at best, surface-level approximations to the characters they are imitating. This paper describes how a new type of foun-dation model a first-person foundation model - could be created from record-ings of what a person sees and hears as well as their emotional and physiological reactions to these stimuli. A first-person foundation model would map environ-mental stimuli to a person's emotional and physiological states, and map a per-son's emotional and physiological states to their behavior. First-person founda-tion models have many exciting applications, including a new type of recommen-dation engine, personal assistants, generative adversarial networks, dating and recruitment. To obtain training data for a first-person foundation model, we have developed a recording rig that captures what the wearer is seeing and hearing as well as their emotional and physiological states. This novel source of data could help to address the shortage of new data for building the next generation of foun-dation models.", "sections": [{"title": "Introduction", "content": "Foundation models have had a big impact in recent years and billions of dollars are being invested in them in the current AI boom. The more popular ones, such as Llama, Chat-GPT and Dall-E, can generate plausible text and images in response to text and image prompts. These models are trained on large amounts of text and image data scraped from the Internet - often accessed through the Common Crawl repository. Researchers and technology companies are starting to realize that this data source could become exhausted soon, so they are looking for new sources of data to train the next generation of foundation models [1] [2]."}, {"title": "Background", "content": "Foundation models often use a transformer architecture [3] with hundreds of billions of parameters. They are trained on large amounts of data using considerable computer resources. For example, GPT-1 was trained on 5GB, GPT-2 on 40GB, GPT-3 on 45TB and MusicGen on 20,000 hours of audio [4, 5]. After training, foundation models are given a prompt, such as a text input, and they generate an output, such as text, code or images. \nThe behavior of foundation models is often fine-tuned through a combination of re-inforcement learning, retrieval augmented generation (RAG) and prompt engineering. In reinforcement learning the model generates multiple different outputs, which are ranked, typically by a human, and then it is trained to generate the desired output more frequently in the future. In retrieval augmented generation a vector database is created"}, {"title": "Personality Modelling with Foundation Models", "content": "Foundation models can imitate the conversational styles of individual people. With simple prompt engineering, Chat-GPT can generate text in the style of personalities, such as Donald Trump, whose speeches and tweets were included in the original train-ing data. More sophisticated models have been created by companies like Facebook (imitations of Snoop Dogg, Tom Brady and other celebrities) and UneeQ, who con-structs digital avatars with different personalities (https://www.digitalhumans.com/). For the most part, these chatbots appear to be generated by a combination of reinforce-ment learning, RAG and prompt engineering. The only potential exception that we are aware of is Soul Machines (https://www.soulmachines.com/), whose website claims that their AI characters are based on a combination of LLMs and multimodal cognitive models. Details are lacking, but our best guess is that cognitive models are used to simulate the agent's state, which is combined with previous knowledge (retrieved using RAG) to build the LLM prompt. Similar work has been carried out by Park et al. [7] and Kirk et al. [8]."}, {"title": "Emotions, Physiology and Decision-making", "content": "It is becoming increasingly recognized that emotions and physiological states play a central role in our decision-making and behavior. Consider a person who is sitting in front of a beef burger in a restaurant. Suppose they are experiencing hunger, and they predict that eating the burger will cause them to experience pleasurable sensations and a feeling of satiety. In this case their current and predicted physiological state explain their action of eating the burger. On the other hand, suppose that the person is feeling sick or cares deeply about animals. In this case they will not eat the burger and this behavior will, again, be explainable in terms of their current and predicted emotional"}, {"title": "Recommendation Systems", "content": "Recommendation systems typically use collaborative and content filtering. Collabora-tive filtering is based on similarities between users. Users rate the items (media, prod-ucts, etc.) and the similarity between users is calculated on the basis of the ratings that the users give to these items. So, users might be considered similar if they give the same ratings to ten films despite there being big differences in their age and regardless of the style and contents of the films that were rated. When similar users have been identified, their ratings are used to predict the rating that a person would give to an item that they have not consumed. For example, if similar users all highly rate the film Arrival (2016), Arrival will be recommended to a person who has not seen this film before.\nWith content-based filtering each item is associated with a set of features. For films, these could include the genre (action, romance, documentary, etc.), actors, director, plot, etc. Each user is modelled by looking at their history for patterns in the features of items that they consume. For example, Brian might have watched a lot of action films starring Dwayne Johnson. This model of past behavior is used to recommend new content. For example, a new action film starring Dwayne Johnson would be recom-mended to Brian; a new romcom with Kate Winslet would not. Content-based and col-laborative filtering use a variety of variety of machine learning algorithms \u2013 for exam-ple, statistics, clustering and deep neural networks - to identify similar users and rec-ommend items based on their contents. Many recommender systems use a hybrid ap-proach that combines content-based and collaborative filtering.\nCollaborative and content-based filtering both rely on historical data from users. So they cannot provide meaningful recommendations to new users who have just joined the system. This is known as the cold start problem. Collaborative filtering also cannot provide good recommendations for new items that have not been rated by users."}, {"title": "First-person Foundation Models (FPFMs)", "content": "Our hypothesis is that a foundation model that is trained from scratch on the stimuli and emotional and physiological states of a person will replicate human behavior more effectively than surface-level approximations built with LLMS, RAG, cognitive models and prompt engineering. This type of foundation model will be referred to as a first-person foundation model (FPFM). It could carry out the following mappings:\n1. Image/audio/text \u2192 Emotional/physiological state. How people's emotional and physiological states change in response to different stimuli. For example, I feel sad when I look at a picture of my deceased grandmother.\n2. Emotional/physiological state \u2192 External behavior. What people do or say when they are feeling a particular way. For example, I engage in food-seeking behavior when I am hungry.\n3. Image/audio/text + Emotional/physiological state \u2192 External behavior. What peo-ple do or say when they are feeling a certain way and experience a particular stimuli. For example, I eat a burger when I perceive a burger in front of me and I am hungry."}, {"title": "Training Data for First-person Foundation Models", "content": "We have developed a first-person recorder that stores what a person is seeing and hear-ing as well as their emotional and physiological reactions to these stimuli (see Section 4). To obtain training data for a FPFM one or more people could be paid to wear this recorder for an extended period of time. This could be accomplished through compa-nies, such as Surge AI (https://www.surgehq.ai/), who recruit people to generate train-ing data and fine tune AI models. Another option would be to release a cheap version of the recorder and launch a marketplace where people could be paid for their record-ings. People could also be motivated to use the recorder in exchange for benefits, such as life logging, personal assistance, enhanced recommendation, perfect memory, or generation of media content based on their lives.\u00b9 This would be similar to the way in which big technology companies give us free and useful services, such as email, calen-dars and social networking, in exchange for access to our personal data."}, {"title": "Modelling Individuals with FPFMs", "content": "Individual FPFMs could be created from data recorded from single people. However, model training is expensive, and it would take a significant amount of time to capture enough data from one person to train a FPFM (see Section 4.2). A more practical ap-proach would be to train a FPFM on data from multiple people and then use fine tuning and RAG to customize the model for a specific individual. Suppose we want to model"}, {"title": "Applications", "content": "Some potential applications of FPFMs are as follows:\n\u2022 Recommendation. FPFMs could be used to create a new form of recommendation engine that measures the customer's actual preferences for each product and recom-mends the ones with the highest net valence. For example, a FPFM based on my emotional reactions could watch every single film and TV series on Netflix and rec-ommend the ones that generate the most positive emotional states. This recommen-dation method does not depend on other users, so there would be no cold start prob-lem when new users joined the system, or new products were added to the system.\n\u2022 Focus groups. FPFMs based on target audiences could be used to evaluate films, products, political policies, etc. prior to their release.\n\u2022 Dialog in novels and scripts. Current foundation models, such as GPT-4, are already being used to generate novels and scripts. FPFMs could model characters more ef-fectively, leading to more realistic dialog. It would be possible to use FPFMs based on recordings of individual actors to generate scripts tailored to these actors. For example, a FPFM of Tom Cruise could be used to write the script for the next Mis-sion Impossible movie.\n\u2022 Personal assistants. A FPFM that understands a user's preferences could search for holidays, restaurants, etc. that appeal to the user. If a personal assistant had third-"}, {"title": "Digital Twins and Third-party Access to FPFMS", "content": "The benefits of FPFMs could be realized by streaming first-person data to a cloud ser-vice managed by a big technology company, such as Microsoft or Amazon, who would use fine tuning and RAG to build a FPFM of the person. However, many people will be unhappy about sharing this type of data with large companies. An alternative ap-proach would be to stream the first-person data to a local device, where it could be used to create an up-to-date digital twin of the person, which would remember everything that the person saw and heard as well as their emotional and physiological reactions. This digital twin could have access to other sources of data on the device, such as mail, photos, GPS, calendar, etc., which it could use to act as a personal assistant to the user. To support recommendation, advertising, dating and other applications, the FPFM could be exposed to authorized third parties through a web service. To protect users' privacy, third-party access could be limited to the FPFM's emotional and physiological reactions to visual and auditory data. Technology companies could pay a small fee to cover the costs of running the model, which would be a way in which users could benefit from money spent on advertising. Only users would have access to the behavioral output of the model."}, {"title": "First-person Recorder", "content": "We have developed a first-person recorder to capture training data for FPFMs. This records what the wearer is seeing and hearing, their emotional and physiological reac-tions to these stimuli and some aspects of their external behavior."}, {"title": "Hardware and Software", "content": "The recorder is based on a Raspberry Pi, worn around the user's neck, which is con-nected to a camera, microphone, GSR sensor and speaker. Data recorded by the Rasp-berry Pi is sent to a web service running on a laptop carried by the user, which has a WebSocket connection to the Epoc X EEG headset. Cloud services, such as AWS Rekognition, and the Emotiv Cortex API, are used to analyze the raw data for higher level properties, such as text contents, sentiment, cognition, facial expression, and ob-ject labels. A website hosted on the laptop enables the recorder to be configured and supports playback of recorded data. To reduce fraud a blockchain archi-tecture is implemented that sends a hash of the data to the cloud and receives a hash of the data plus a random number known only to the cloud service, which is added to the next file in the sequence. To ensure the privacy of other people, all faces are automati-cally blurred during the recording process. The data is stored in JSON files; schema definitions for these files are available on the project website. Version 1.0 of the re-corder is shown in Figure 2a."}, {"title": "Recorder Data", "content": "The data captured by the recorder is summarized in Table 2. This shows that the re-corder can capture ~40 GB of data (images, audio and text) in a 16-hour day. If the images, audio, raw EEG and raw GSR are excluded, this figure drops to ~1 GB per 16-hour day. Based on these figures, Table 3 gives some rough estimates of how long it would take to store enough data to train foundation models on the scale of GPT-1, GPT-2 and GPT-3. The data in Table 3 suggests that a purely text-based FPFM on the scale of GPT-2 could be created from ~50 days of data recorded from a single individual. Larger models are likely to require data recorded from several individuals.\nThe current version of the first-person recorder could be improved in several ways. For example, the backend could be moved to the cloud, the hardware could be up-graded, and an eye-tracking system, such as the Tobii Pro could be used to capture what the user is actually looking at, instead of their general field of view. Further work is also required to integrate the recorded signals into a single picture of the emotional and physiological state of the wearer. This could be done through a machine learning approach, in which the DES data is used as the labels and a deep network is trained to automatically generate the labels from the recorded data. Since people's emotional and physiological reactions vary widely, it is likely to be necessary to calibrate the recorder for each user. EEG headsets are expensive and uncomfortable to wear, so in the future, accurate measurement of the subject's emotional and physiological states could be used to find ways of capturing approximations to this data using cheaper hardware, such as the Fitbit and Apple watch."}, {"title": "Privacy and Legal Issues", "content": "The first-person recorder captures everything that the wearer is exposed to, including copyrighted books, music, and films. To reduce this problem, recordings could be au-tomatically screened for copyrighted content or GPS could be used to switch the re-corder off automatically in situations in which copyright is likely to be an issue, such as concerts and cinemas. However, the elimination of copyrighted content from the model would reduce the power of the FPFM as a recommendation engine, since it would no longer be storing the user's reactions to books, films, and music.\nThe automatic blurring of faces that the recorder uses to protect privacy would pre-vent a FPFM trained on the data from learning the strong emotional reactions that we have to familiar faces, such as friends, family, colleagues, and celebrities. To address this issue people could be asked to give their consent to have their faces recorded by the device. This could be at an individual level (I give my consent for my wife to record my face on her device) or globally (celebrities could give their consent to be recorded by anyone). Face recognition could then be used to record the faces of consenting peo-ple in an unblurred state.\nCompanies and government agencies with third-party access to a person's FPFM could assess whether that person has a liking for illegal products, such as drugs, sexual attraction towards children or emotional attachment to terrorist organ-izations. Like any foundation model, FPFMs will frequently hallucinate. So, if a per-son's FPFM has strong positive responses to illegal activities, this does not prove that they have committed any actual offences or are likely to do so in the future. Great care will have to be taken with the security of FPFMs, the terms and conditions under which they are shared with third parties, and the interpretation of their outputs."}, {"title": "Conclusions", "content": "The central role that emotions and physiology play in the selection and initiation of behavior is widely acknowledged in neuroscience and psychology, and it is starting to be recognized in artificial intelligence research. If this interpretation of the importance of emotion is correct, foundation models that are trained on text and image data from the Internet will only be able to create surface-level approximations to the behavior of individual people.\nThis issue could be addressed by a new type of foundation model that maps environ-mental stimuli to a person's emotional and physiological states, and maps a person's emotional and physiological states to their behavior. There are many exciting applica-tions of FPFMs, including recommendation, focus groups, dialog writing, personal as-sistance, GAN systems, dating and recruitment. Some of these applications could be realized by exposing a FPFM of the user's emotional and physiological reactions to trusted third parties. FPFMs could also be used to create digital twins that serve as personal assistants and are accessed privately by their owners.\nTo obtain training data for FPFMs we have developed a recording rig that stores what the wearer is seeing and hearing as well as their emotional and physiological"}]}