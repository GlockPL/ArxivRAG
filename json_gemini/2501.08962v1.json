{"title": "An analysis of data variation and bias in\nimage-based dermatological datasets for machine\nlearning classification", "authors": ["Francisco Mauro", "Emanoel Thyago", "Othon Vinicius", "Rodrigo Abreu", "Kelvin Cunha", "Jos\u00e9 Gabriel", "Rafael Barros", "Thales Bezerra", "Manoel Henriques", "Natalia Lopes", "\u00c9rico Moutinho", "J\u00e9ssica Guido", "Tsang Ing Ren", "Paulo Borba"], "abstract": "AI algorithms have become valuable in aiding professionals\nin healthcare. The increasing confidence obtained by these models is\nhelpful in critical decision demands. In clinical dermatology, classifica-\ntion models can detect malignant lesions on patients' skin using only\nRGB images as input. However, most learning-based methods employ\ndata acquired from dermoscopic datasets on training, which are large\nand validated by a gold standard. Clinical models aim to deal with clas-\nsification on users' smartphone cameras that do not contain the corre-\nsponding resolution provided by dermoscopy. Also, clinical applications\nbring new challenges. It can contain captures from uncontrolled environ-\nments, skin tone variations, viewpoint changes, noises in data and labels,\nand unbalanced classes. A possible alternative would be to use transfer\nlearning to deal with the clinical images. However, as the number of sam-\nples is low, it can cause degradations on the model's performance; the\nsource distribution used in training differs from the test set. This work\naims to evaluate the gap between dermoscopic and clinical samples and\nunderstand how the dataset variations impact training. It assesses the\nmain differences between distributions that disturb the model's predic-\ntion. Finally, from experiments on different architectures, we argue how\nto combine the data from divergent distributions, decreasing the impact\non the model's final accuracy.", "sections": [{"title": "1 Introduction", "content": "Skin cancer ranks among the neoplasias with the highest global incidence, yet\nfew patients actively seek or have access to specialized medical attention."}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Datasets in dermatology AI", "content": "There are two main types of datasets used for skin lesion classification: clinical\nimages (generated by common image-capturing devices) , and dermo-\nscopic images (based on an equipment consisting of a high-quality magnifying\nlens and an illumination) [1,18,10]. Despite the availability of various datasets"}, {"title": "2.2 Deep Learning on dermatology classification", "content": "Several deep learning models applied to skin lesion classification have been de-\nveloped to improve early diagnosis [8]. Ha et al. [7], the winning solution to the\nSIIM-ISIC Melanoma Classification Challenge, applied an ensemble of convo-\nlutional neural network (CNN) combining input images with dataset metadata,\nand reaches an AUC of 96% on ISIC 2020 dataset. The ISIC 2020 dataset has only\n1.76% of positive samples (i.e., malignant) out of 33,126 images, which makes\nthe evaluation of the model difficult. To address this problem, the ISIC 2018 and\n2019 datasets were utilized. Sadik et al. [14] proposed an analysis of different\nCNN architectures with transfer learning models pretrained on ImageNet, evalu-\nating architectures such as MobileNet, Xception, InceptionV3, Inception-ResNet,\nand DenseNet. They used two dermoscopic dataset in the study: Dermnet images\n(Atopic dermatitis, Eczema, Nevus, and Herpes) [11] and HAM10000 (Melanoma\nimages) [18]. Augmentation techniques were applied to improve the robustness\nof the models. Sui\u00e7mez et al. [16] applied processing techniques to remove hair\nfrom dermoscopic images from the HAM10000 and ISIC 2020 datasets. Subse-\nquently, a wavelet transform was utilized for noise removal and compression.\nMost works in literature employ dermoscopic dataset on evaluation [8]."}, {"title": "2.3 Interpretability on dermatology classification", "content": "Recently there are an effort to interpret image features that makes a model to\ndecide whether a lesion is benign or malignant [15]. Attention-based approaches\nfor melanoma recognition leverages attention maps to accentuate pertinent re-\ngions relevant to lesion classification [20]. Some approaches [7] inspect outcomes\nfrom multiple networks to identify the most proficient performers. Their tech-\nnique involves training various models independently and subsequently combin-\ning their outputs into an ensemble model. Despite the time-consuming nature of\nthis method, notable achievements are achieved in terms of results."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Motivation", "content": "According to Daneshjou et al. [3], dermatology classification approaches trained\non dermoscopic datasets prove ineffective in clinical evaluations. Public datasets,\nprimarily dermoscopic, exhibit biases and lack the necessary variation to han-\ndle real-world clinical classifications adequately. These datasets, dominated by\nspecific characteristics, notably fall short of representing variations in skin tones\nand patient ethnicity.\nWhile state-of-the-art approaches trained on datasets like HAM1000 [18] and\nDeepDerm perform well on dermoscopic data, they fail in classifying samples\nfrom the proposed clinical dataset (DDI). Fine-tuning the models improves their\nclinical classification results. However, while fine-tuning adjusts parameters for\na new target within the limited dataset, other concerns related to datasets con-\nception [12] need consideration.\nThe DDI dataset comprises images captured by different smartphones during\npatient evaluations, encompassing various skin tones, perspectives, scene illumi-\nnations, camera resolutions, and image noises. In contrast, dermoscopic datasets\nlike HAM10000 and DeepDerm feature images captured by dermoscopic devices,\nproviding higher resolution, controlled perspectives and illuminations, and gold-\nstandard annotations."}, {"title": "3.2 Dataset analysis", "content": "To comprehend the features influencing model predictions, we analyzed the fea-\ntures extracted by deep learning models from clinical and dermoscopic datasets,\nconsidering categorical (lesions ID) and binary (malignant, benign) classification.\nAdditionally, we analyzed features to visualize each lesion's characteristics,\nidentifying differences and similarities in class boundaries that pose challenges\nin each classification task. Dermoscopic dataset ISIC18, including data from\nHAM10000 [18] and BCN20000 [1], were used, along with the clinical dataset"}, {"title": "3.3 Models evaluation", "content": "Training and testing employed deep learning classification architectures, includ-\ning ConvNext (Tiny, Small, and Base) [9], DenseNet (121 and 161) [21], ResNet\n(50 and 152), and EfficientNet. Various setups were assessed to evaluate accuracy\nunder domain variability by altering training and testing distributions:\nFull Dermoscopic (FDerm): Models were trained on ISIC training set\nFull clinical (FClinic): Models were trained on PAD-UFES-20 training set5\nFine-tuned models (FineClinic): Models trained on dermoscopic data (ISIC)\nare fine-tuned on the clinical dataset (PAD-UFES-20), using only 30% of the\nclinical samples separated for training.\nAugmentations were incorporated into model training to mitigate image noise\nand class imbalance impacts. Transformations, such as RandomHorizontalFlip,\nRandom VerticalFlip, RandomRotation, ColorJitter, Random ResizedCrop, and\nRandomAffine, were applied to images representing a smaller percentage of the\ntotal training data. Models were implemented using the PyTorch library [13] on\na system with a 12th Gen Intel Core i7-12700H CPU, 16GB RAM, and a GPU\n3060 with 12GB VRAM. Starting with pre-trained weights from IMAGENET [4],\nwe trained each model for 100 epochs using the ADAM optimizer with a learning\nrate of 1 \u00d7 10-1 and a Cosine Annealing scheduler that decreased the learning\nrate every 10 epochs."}, {"title": "4 Results and Discussions", "content": null}, {"title": "4.1 Dermoscopic and clinical models", "content": "In the initial phase of training the model, we assessed various architectures by\nevaluating each model on dermoscopic and clinical data within the respective\ndomains. Following the operations outlined in Section 3.3, augmentation proce-\ndures were applied. While there are slight variations in the results across architec-\ntures, the classification behavior remains similar among models. The ConvNext\nmodel, yielding the best result, matches the state-of-the-art within the scope of\nCNN architectures.\nInitially, it can be stated that the models are adequate, demonstrating reason-\nable accuracy values. However, a closer examination of each class's performance\nreveals the impact of biases stemming from sample imbalances. Most predictions are directed towards nevi, constituting over 90% of the avail-\nable samples in ISIC. The number of available samples for malignant lesions"}, {"title": "4.2 Model adaptation", "content": "Table 3 displays the results obtained after fine-tuning the model architectures to\nadapt their parameters in clinical classification. It is evident that, before adjust-"}, {"title": "4.3 Clinical to clinical evaluation", "content": "Clinical scenarios pose a greater challenge than their dermoscopic counterparts,\nprimarily due to the diverse variations in data distribution. However, the limited\nnumber of images also constrains optimizing the model. To explore this limita-\ntion, we conducted a comparative analysis using two distinct clinical datasets. In\nthis experiment, we employed a model trained on PAD-UFES-20, which encom-\npasses the desired variations in patient skin tones, and evaluated its performance"}, {"title": "5 Conclusions", "content": "In this study, we evaluate the characteristics of both dermoscopic and clinical\ndatasets, exploring their features and analyzing their impact on model predic-\ntions. Through the application of samples across various CNN architectures, we\nassess the combination of clinical and dermoscopic data, highlighting the cru-\ncial role of domain differences in the evaluation process. Clinical features, such\nas patient skin tone, ethnicity, age, and lesion format, influence learning, while\nintrinsic factors within the domain distribution, including image resolution, dis-\ntortions, noise, and illumination, also impact the model's knowledge. Additional\nexperiments support this observation, especially when comparing two distinct\nclinical datasets captured in different settings. We anticipate that these experi-\nments offer valuable insights into the application of AI in clinical dermatology.\nWhile these models prove suitable for assisting doctors in preliminary diagnoses,\nthere remain gaps in methodological conception that need addressing for a reli-\nable application of this technology in real-world cases. By systematically evaluat-\ning biases within each type of data, we propose alternative evaluation approaches\nto enhance the reliability of models in both clinical and dermoscopic setups."}]}