{"title": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio", "authors": ["Ningyuan Xi", "Yetao Wu", "Kun Fan", "Teng Chen", "Qingqing Gu", "Peng Yu", "Jinxian Qu", "Chenxi Liu", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "abstract": "Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to obtain the unfamiliar language skill or adapt into new domains. The huge training cost of CPT often asks for cautious choice of key hyper-parameters such as the mixture ratio of extra language or domain corpus. However, there is no systematic study which bridge the gap between the optimal mixture ratio and the actual model performance, and the gap between experimental scaling law and the actual deployment in the full model size. In this paper, we perform CPT on Llama-3 8B and 70B to enhance its Chinese ability. We study the optimal correlation between the Additional Language Mixture Ratio (ALMR) and the Learning Rate (LR) on the 8B size which directly indicate the optimal experimental set up. By thorough choice of hyper-parameter, and subsequent fine-tuning, the model capability is improved not only on the Chinese-related benchmark, but also some specific domains including math, coding and emotional intelligence. We deploy the final 70B version of LLM on an real-life chat system which obtain satisfying performance.", "sections": [{"title": "Introduction", "content": "Continual Pre-Training (CPT) is a widely used technique to enhance the fundamental capability pre-trained Large Language Model (LLM) in which the pre-trained corpus is insufficient. CPT can be generally classified into two categories: to expand the LLM linguistic ability (Ke and Liu, 2023; Cui et al., 2023; Chen et al., 2024), and to enhance some domain-specific knowledge such as code, math and law (Gururangan et al., 2020; Ma et al., 2023; Wu et al., 2024; Liang et al., 2024). However, similar with the pre-training stage, CPT also suffers from enormous GPU consumption and tedious time cost. It is generally unclear what the optimal experiment configuration of CPT is since a grid-search is intractable. These issues call for the scaling law study of CPT, since the traditional heuristic manner is obviously unreliable and might be fail when the model size expands or the interested domain shifts. Furthermore, CPT usually results in the catastrophic forgetting of LLM with inappropriate experimental settings. There is also a gap between large-scale open-sourced foundational LLM (might be 70B or even larger) and a real-world industrial application requiring the LLM adapted with unfamiliar linguistic skills. This gap increases the challenge of CPT experiments, including the training corpus preparation, the hyper-parameter determination, and tuning and evaluation of downstream tasks.\nThe CPT methodology shares some basic considerations with pre-training. For example, OpenAI (Kaplan et al., 2020) first proposes the standardized Learning Rate (LR) strategy including warm-up, cosine schedule and decay. The famous Chinchilla scaling law (Hoffmann et al., 2022) further argues that the LR schedule should be correlated with the training tokens (therefore the training steps) with the batch size fixed. Inspire by this study, there are recently also some studies of CPT scaling laws considering the training data size. D-CPT Law (Que et al., 2024) proposes a scaling law of the optimal mixture ratio between the general corpus and the domain-corpus, as a function of different dataset sizes and model sizes. A subsequent study (Gu et al., 2024) further defines the maximum mixture ratio within the feasible region as Critical Mixture Ratio (CMR), and discovers its correlation with loss and training tokens. These studies provide some feasible manners to set up the CPT experiments, but also subject to some shortcomings. First, almost all of such studies indicate the model performance with the pre-training loss, which however, might not be equalized with the task performance in the CPT case, as indicated by our CPT experiments. Second, CMR assumes one can choose the largest feasible mixture ratio, which might be actually sub-optimal when evaluating with downstream task performance. Finally, their experiments often conduct on relatively smaller model size (up to 4B), while a typical industry-level application often requires the LLM to be 7B or even larger.\nTo address these issues and provide a insightful practice, in this paper, we focus on CPT of LLM on an additional linguistic knowledge which the pre-training dataset is relatively unfamiliar, and enhance its downstream linguistic and other domain-specific skills. We name the mixture ratio in this case by the Additional Language Mixture Ratio (ALMR), which is the under-determined key hyper-parameter. Different from previous studies, we instead study the optimal correlation between ALMR and LR during the CPT stage, with the proxy of benchmark performance. Similar with CMR, we also assume the training dataset size is fixed but in a much larger amount. The correlation is learned from CPT trajectories on Llama-3 8B, and is combined with the efficient frontier suggested by the CPT validation loss, to determine the final optimal ALMR and LR in the formal 8B experiment. We also conduct CPT of Llama-3 70B with the same ALMR and a smaller LR, and provide further alignment efforts with Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO). We find that model performances are increased for both model sizes, on both benchmarks related with the additional language, and some domain-specific metrics such as math, coding and emotional intelligence. We argue that appropriate choices of ALMR and LR can substantially help the LLM adapted with unfamiliar language and even been boosted for more domain-specific evaluations. To summarize, the main contributions of this paper include the following:"}, {"title": "Related Work", "content": ""}, {"title": "Continual Pre-Training", "content": "Continually Pre-Train (CPT) is generally employed to enhance LLM into two types of tasks, to expand the linguistic ability of LLM to a new language (Ke and Liu, 2023; Cui et al., 2023; Chen et al., 2024), or adapt to specific domains or tasks (Gururangan et al., 2020; Ma et al., 2023; Wu et al., 2024; Liang et al., 2024). For example, Cui et al., (Cui et al., 2023) developed a method to enhance LlaMA-2 13B's Chinese language capabilities by extending its vocabulary with 20,000 Chinese tokens followed by CPT on Chinese corpora with LoRA. This approach demonstrates how CPT can be applied to expand a model's linguistic abilities. The mixture ratio of Chinese corpus to general corpus is 2:8. Linly\u00b9 and Firefly-LLaMA2-Chinese\u00b2 also conducted CPT practices on LLama-2 7B, 13B and 70B. Based upon the recent LlaMA-3 (AI@Meta, 2024), Llama-3-Chinese conduct CPT on Llama-3 8B with LORA \u00b3, while Llama3-SynE (Chen et al., 2024) also perform CPT on 8B with synthetic data on scientific domains.\nFor the second category of CPT, Domain-Adaptive Pre-training (DAPT) (Gururangan et al., 2020) involves CPT on domain-specific corpora, and utilizes a corpus larger than task-specific datasets but smaller than the initial pre-training data. EcomGPT-CT (Ma et al., 2023) performs CPT on Bloom on e-commerce semi-structured data. TRAIT (Liang et al., 2024) adapt LLM to domains of advertisement and math by propose a data-augmentation framework. Llama Pro-8.3B (Wu et al., 2024) propose a relative different methodology which post-trains Llama-2 7B with block expansion, for programming and mathematics domains."}, {"title": "Scaling law of CPT", "content": "After the forerunner efforts of scaling law on pre-training LLMs (Kaplan et al., 2020; Hoffmann et al., 2022), recently there are also some attempts to discuss the scaling law of CPT. For example, D-CPT law (Que et al., 2024) proposes the scaling law of loss with mixture ratio, dataset size and model size, for general loss and 6 adapted domains. Their dataset sizes vary from 0.1 to 26B and model sizes vary from 0.5 to 4B. CMR (Gu et al., 2024) propose a scaling law for CPT to two domains, finance and science QA, given a fixed training tokens budget, 220B. Training tokens is 220B and training model sizes vary from 460M to 3.1B. Furthermore, they define the term of Critical Mixture Ratio which means the maximum of data mixture ratio within its feasible region. In comparison, here we study the scaling law between LR and the mixture ratio and argue that the optimal mixture ratio should be determined from the optimal averaged benchmark performance, not only the pre-training loss. Furthermore, our experiments are conducted on the actual 8B model size, which to the best of our knowledge, is the largest among such type of studies."}, {"title": "Methodology", "content": "Here we list our experimental methodologies, including three stages, CPT and SFT and DPO. We first recap their training objectives, then we briefly discuss the baselines we are compared with, the evaluation benchmarks we used, and finally the experimental configurations."}, {"title": "Training Objectives", "content": ""}, {"title": "Continual Pre-Training:", "content": "Pre-training of LLM employs the typical cross-entropy loss or the self-autoregressive loss over a sequence of tokens:\n$L_{PRE} = -\\frac{1}{L} \\sum_{i=1}^{L}log[p(x_i)] $ (1)\nwhere $L_{PRE}$ is the cross-entropy loss, L is the sequence length, x is the true label token in the data and i is the position in the sequence. p(\u00b7) is the softmax probability predicted by the LLM. CPT shares the same training objective while loading from the pre-trained model checkpoint."}, {"title": "Supervised Fine-Tuning:", "content": "Given the prompt-response samples, the loss of SFT can be express as follows:\n$L_{SFT} = \\frac{1}{L} \\sum_{i=1}^{L}log [p(y_i|x, Y_{1...i-1})] $(2)\nwhere L is the number of tokens in the response to the prompt, x is the prompt token sequence and y is the response token."}, {"title": "Direct Preference Optimization:", "content": "DPO aims to directly optimize a language model's policy to match human preferences without explicitly modeling a reward function, the loss function can be formulated as follows:\n$L_{DPO}(\\pi_{\\theta}; \\pi_{ref}) \\sim D[log \\sigma (\\beta log \\frac{\\pi_{\\theta}(y_{\\omega}|x)}{\\pi_{ref}(y_{\\omega}|x)} - log \\frac{\\pi_{\\theta}(y_{l}|x)}{\\pi_{ref}(y_{l}|x)})] $(3)\nwhere $ \\pi_{\\theta} $ is the policy being optimized, $ \\pi_{ref}$ a reference policy. (x, $y_{\\omega}$, $y_{l}$) are samples from a preferenced dataset D, with x as the prompt, $y_{\\omega}$ as the winning output, and $y_{l}$ as the losing output. $ \\beta $ is a temperature hyperparameter, while $ \\sigma $ is the sigmoid function."}, {"title": "Implementation Details", "content": "We employ the Llama-3 series as our pre-trained checkpoints. We primarily aim to enhance the Chinese understanding and generation capabilities since the original training corpus of Llama-3 contains limited fraction of Chinese tokens \u2074. We conduct the pipeline of CPT, SFT and DPO starting from Llama-3 8B and 70B, respectively. We therefore name the resulted versions of LLM as CPT, C-SFT and C-DPO, respectively.\nWe name the official Llama-3 8B and Llama-3 70B models as 'Base' which are the natural baselines for CPT models. We also abbreviate the official Llama-3 8B Instruct and Llama-3 70B Instruct as 'Instruct' as the baselines of our SFT and DPO models. For 8B CPT comparison, we also consider another baseline called Llama-3-Chinese \u2075 which also conduct CPT of Chinese corpus on Llama-3 8B.\nWe run CPT in Megatron \u2076 and finetuning in LlamaFactory (Zheng et al., 2024). The 8B experiments are run by 32 A100 GPUs with 2 TP and 1 PP, while the 70B experiments are run by 128 GPUs with 8 TP and 2 PP. We use the AdamW optimizer with the sequence window length of 2048. Table 2 lists other configurations in our experiments."}, {"title": "Evaluation Benchmarks", "content": "Here we brief introduce the evaluation benchmarks we use in this work, categorized by their domains. We primarily use the Chinese benchmarks to evaluate the model capabilities in the new adopted language, the English benchmark to ensure the robustness of general capability, and use the reasoning, math and coding benchmarks to indicate model improvement in domain-specific knowledge."}, {"title": "Disciplinary and Common Knowledge:", "content": ""}, {"title": "Reasoning, Math and Coding:", "content": ""}, {"title": "Classical NLP tasks:", "content": ""}, {"title": "Comprehensive Alignment", "content": ""}, {"title": "Study of Addition Language Mixture ratio", "content": "Experiment of pre-training often lasts several weeks or months therefore it is extremely difficult to find theoretical optimal hyper-parameters. Similar cases are encountered for CPT studies although its training cost might be smaller. In this work, we focus on the case of CPT with extra language addition, which immediately calls for the key hyper-parameter, ALMR, fraction of additional language corpus over the entire training corpus. In this work, we conduct different independent CPT experiments starting from Llama 8B, with different ALMR and LR combinations, to find out their optimal choices. Each training stops with nearly 100B tokens consumes, when the training loss become stable.\nFigure 1 depicts contours of two important factors, validation loss and averaged metrics, as functions of ALMR and LR. The left part of Figure 1 indicates that the larger LR is, the smaller the validation loss can reach. However, different from previous studies like D-CPT-law and CMR, here we argument that small loss does not necessarily correspond to better performance (consider the case when there is no additional language or extra domain knowledge injected in CPT, then the loss would be small since LLM is already familiar with training tokens.). Therefore, we also present the right part of Figure 1, in which we average the benchmarks listed in Table 3 and plot the contour. Apparently, larger mean metric means better performance, which corresponding to several peaks on the contours. We calculate their performance peaks and regress them to obtain the efficient frontier, which is the dash line in the right sub-figure of Figure 1\n$ALMR = 116.67 log(LR) + 1085.00$ (4)\nTo determine the final choice of our ALMR and LR in the final experiment, we also draw the efficient frontier Figure 1 (left), by calculating the gradient of LR and ALMR along which the loss decreases fastest\n$ALMR = -0.33 log(LR) + 29.67$ (5)\nEquation 4 and Equation 5 intersect at ALMR = 33% and LR = 1.0e \u2013 9, which are the final choices of our ALMR and LR, in the formal 8B experiment. For the 70B experiment, we use the same AMLR while decrease LR to 1.0e-10."}, {"title": "Experimental Results", "content": "In this section, we exhibit results of CPT, SFT and DPO results. We also conduct ablation studies and further alignment to enhance the emotional intelligence. Experiment results indicate that CPT assists substantial improvement of downstream task performances and validate the final formal deployment. One can refer to Appendix for further details."}, {"title": "Results of Continual Pre-Training", "content": "We mainly employ the benchmarks of disciplinary, common and specific domains knowledge, to evaluate the capability of pre-trained models. Table 3 lists results of the aforementioned benchmarks, in which we compare our CPT model with Base, as well as Llama-3-Chinese on size of 8B. For 8B, we achieve better results of all Chinese, Reasoning, Math and Coding benchmarks, while results of English benchmarks keep almost the same. It is also worthwhile to notice that Llama-3-Chinese fails to maintain the general and domain skills, with MMLU, GSM8K and HumanEval all apparently decreased from Base. Furthermore, our 70B CPT model outperforms Base in almost all the benchmarks, with only the exception of BBH.\nWe also visualize the early stages of several typical metric curves, to show the temporal tendency of model capability. Figure 2 shows the time trajectories of C-Eval, LCSTS, GSM8K and HumanEval. From the figure, we speculate that the disciplinary knowledge can be relatively quickly obtained but then improved slower by CPT, while NLU and domain knowledge are consistently improved along the entire CPT stage, and can be further improved given more high-quality data. We note that this conclusion is similar with some previous research (Ji et al., 2023)."}, {"title": "Results of Alignment", "content": "Table 4 lists results of C-SFT and C-DPO, with comparison with the baselines, the official instruct models. Again, our models outperform the baselines in all benchmarks, in which C-SFT is better in some benchmarks while C-DPO is better in others. We argue that the preference data used by C-DPO would steer LLM to its interested domain, in cost of degradation of some other knowledge.\nWe intentionally to keep LCSTS in both pre-training and alignment evaluations. Here we find the interesting phenomena that the downstream task performance might be extremely low when the pre-trained model has relatively weak support to the specific language. Recall that LCSTS of Llama3-8B in Table 3 is 9.64 but Llama3-8B-Instruct in Table 3 only has 0.34. Given the Chinese summarization instructions and demonstrations, Llama3-8B-Instruct only generates English responses which indicates its instructional fine-tuning data might neglect Chinese support and the catastrophic forgetting happens."}, {"title": "Ablation Study", "content": "To validate the effect of inclusion of Chinese samples either in CPT and fine-tuning stages, here we perform an ablation study on C-CPO 8B, in which without the CPT stage means starting the SFT and DPO stages from Llama-3 8B directly; without finetune-ch means only using non-Chinese samples during SFT and DPO. The standard C-CPO is still the best, which indicates both CPT with Chinese corpus and fine-tuning with Chinese samples help enhance the LLM's Chinese skills."}, {"title": "Emotional Chatting and Deployment", "content": "We aim to deploy the Chinese-enhanced LLM on Geely's free daily working agent and provide better native language support and behave as the emotional supporter. Therefore, we further examine the emotional intelligence of instruct models, including the emotion classification task like TAPTAP (Zhang et al., 2023), the emotional intelligence benchmark SECEU (Wang et al., 2023), and conduct the Turing Test \u2077 using the evaluation approach in BotChat (Duan et al., 2023). We conduct the Turing test using a propriety set consisting of 500 dialogues, covering our daily chatting topics.\nWe list the results of aforementioned benchmarks in Table 5. We add another open-sourced baseline, Qwen2-72B-Instruct (Team and Group, 2024), which has a similar size of model parameters, and is intentionally trained with Chinese corpus and achieve top performance in Chinese benchmark. Our C-SFT model surpasses two baselines,"}, {"title": "Conclusion", "content": "In this paper, we conduct the continual pre-training, the supervised finetuning, and direct preference optimization on Llama-3 8B and 70B, on the purpose of strengthen its capability on a specific additional language. We surprisingly find that the CPT of extra language also boost LLM's capability on not only the additional language understanding, but also some other specific domains. We also successfully finetune it with more dialogue-like samples and obtain state-of-the-art performance on emotional intelligence benchmarks. This version of model has been deployed on an industrial-scale application and provide real-life emotional chatting supports. We also conduct substantial experiments to find an optimal correlation between the learning rate and additional language mixture ratio, which sheds some lights on future continual pre-training researches."}, {"title": "Additional Results", "content": "Figure 3 show the training loss trajectories with respect to different learning rates. From these results, we find that the optimal Chinese corpus fraction should be around 30-40%.\nFrom these figures, it is evident that although the loss decreases most rapidly when the learning rate approaches zero, the number of tokens learned is relatively small due to the low proportion of Chinese in the corpus. Conversely, when the proportion of Chinese is high and the learning rate is relatively small, the loss decreases more slowly. This observation is further corroborated by subsequent pre-training experiments with more training steps. Therefore, from a long-term perspective, an optimal proportion of 30-40% for Chinese corpus fraction is suggested, and the metric performance in the main text also supports this conclusion.\nIn addition, in the subsequent SFT experiments, the corresponding loss curve in Figure 4 (left) is very normal, indicating a rapid decrease in loss without any adverse conditions such as gradient explosion.\nThe margin curve in Figure 4 (right) shows a growth that aligns well with expectations, demonstrating that CPT is highly effective as a buffer for model training on unfamiliar corpora."}]}