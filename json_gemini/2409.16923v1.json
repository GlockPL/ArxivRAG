{"title": "AI-assisted Gaze Detection for Proctoring Online Exams", "authors": ["Yong-Siang Shih", "Zach Zhao", "Chenhao Niu", "Bruce Iberg", "James Sharpnack", "Mirza Basim Baig"], "abstract": "For high-stakes online exams, it is important to detect potential rule violations to ensure the security of the test. In this study, we investigate the task of detecting whether test takers are looking away from the screen, as such behavior could be an indication that the test taker is consulting external resources. For asynchronous proctoring, the exam videos are recorded and reviewed by the proctors. However, when the length of the exam is long, it could be tedious for proctors to watch entire exam videos to determine the exact moments when test takers look away. We present an AI-assisted gaze detection system, which allows proctors to navigate between different video frames and discover video frames where the test taker is looking in similar directions. The system enables proctors to work more effectively to identify suspicious moments in videos. An evaluation framework is proposed to evaluate the system against human-only and ML-only proctoring, and a user study is conducted to gather feedback from proctors, aiming to demonstrate the effectiveness of the system.", "sections": [{"title": "Introduction", "content": "The adoption of online proctoring systems has grown in recent years. Online tests offer greater flexibility because test takers can take the test remotely without going to a specific test center. However, the problem of cheating is a threat to the validity of the test results. Therefore, security measures need to be built to detect and prevent cheating behaviors.\nOnline proctoring comes in various forms, including live synchronous proctoring where the proctor watches the test taker remotely during the test session, and asynchronous proctoring where video recordings of the test sessions are recorded and reviewed by proctors. In this study, we focus on the application of online proctoring in the Duolingo English Test (DET), which is an online, high-stakes English assessment test where a test taker's video is recorded with the test taker's webcam. The test taker's video, the screen recording, the responses, and other relevant information are collected, and proctors review each test session asynchronously.\nWe focus our study on the task of detecting if test takers are looking away from the screen suspiciously, as such a behavior could indicate that test takers are consulting external resources. There are two challenges proctors face when examining such behaviors. Firstly, when the exam video length is long, it could be tedious for the proctors to watch the entire video to find all moments where the test taker is looking away. In addition, as pointed out by Belzak, Lockwood, and Attali (2024), a test taker could also naturally look at arbitrary spots as part of their cognitive processing. With the limited amount of information available in the exam videos, different proctors' decisions could be less consistent for this task compared to other tasks such as detecting plagiarism.\nIn this study, we present an AI-assisted gaze detection system, where the predicted gaze direction of the test taker in each frame is shown on a scatter plot. Proctors can select regions on the gaze plot, and the related timestamps on the video player timeline will be highlighted. This allows proctors to navigate to relevant video frames more efficiently, which improves the proctoring experience. In addition, because the system enables a consistent view on the test taker's gaze directions, the quality of the proctoring could also be improved."}, {"title": "System Overview", "content": "Our system is designed for asynchronous proctoring of online exams. When a test taker takes a test, a video is recorded for the entire test session, and once the video is uploaded, a gaze detection model can be run on each frame of the video to predict the gaze direction in each frame. In practice, a suitable frame rate would need to be selected for inference according to the resource constraints.\nThe gaze direction predictions will be displayed to proctors as a scatter plot. In particular, the gaze angles predicted by the model can be represented as unit directional vectors originating from the origin, and these vectors are projected onto the 2D plane, with each point on the plot representing a frame and its associated gaze direction. Currently, our gaze plot only represents the gaze directions (i.e., the angles of the gazes), and not the exact location on the screen where the test taker is looking at. However, a similar plot can also be used for models that predict the exact screen location.\nProctors can select regions on the eye gaze plot and the corresponding frames will be highlighted on the video player timeline. This allows proctors to navigate to frames with similar gaze directions within the selected region. For instance, if a proctor observes a suspicious moment when the test taker is looking away from the screen, the proctor can consult the gaze plot to find the current video frame's location, and select a region around it. The system will highlight all other relevant timestamps, allowing the proctor to navigate to those timestamps and confirm whether the test taker is also exhibiting suspicious behaviors at those moments."}, {"title": "Evaluation Framework", "content": "To evaluate the effectiveness of the system in human-based asynchronous proctoring, we apply the concept of human-ML complementarity to define the evaluation goals and propose our experiment plans."}, {"title": "Human-ML Complementarity", "content": "In a hybrid decision-making system like AI-assisted proctoring, human-ML complementarity is the condition where the hybrid system outperforms both humans and ML models. Following the notation used by Rastogi et al. (2023), denote X as the set of all available features of a given test session, including video recording, responses, scores, etc. Denote the action space as A, where for a test session with T frames, $a \\in A$ is a binary sequence with length T, and $a_t$ indicates whether the t -th frame is labeled positive (i.e. looking away from the screen) or not. Then a decision-making system for labeling gaze direction in a test session can be written as a mapping $\\pi : X \\rightarrow A$. Denote $\\Pi$ as the set of all possible $\\pi$.\nIn this work, there are three systems of interest: (1) the human-only system $\\pi_H$, where human proctors label the test session mainly by watching the video; (2) the ML-only system $\\pi_M$, where binary predictions are made by thresholding predicted gaze directions on each frame; and (3) the hybrid system $\\pi_{H+M}$, where human proctors label the test session with additional access to predicted gaze directions."}, {"title": "Proposed Experiments", "content": "On a dataset with N test sessions, we define the evaluation function F as:\n$\\F(\\pi) = \\frac{1}{N} \\sum_{i=1}^{N} s(x^{(i)}, \\pi(X^{(i)}))$\nWhere $s: X \\times A \\rightarrow R$ is a scoring function of a labeling result for a given test session, regardless of where the labeling result comes from. Without access to ground truth labels, we use a labeling process with multiple proctors to generate high-precision labels to define s.\nSpecifically, given the i-th test session, $a_H^{(i)} = \\pi_H(X^{(i)})$ is the labeling result from a proctor without using the gaze plot, $a_M^{(i)} = \\pi_M(X^{(i)})$ is the labeling result made by thresholding the predicted gaze directions in each frame, and $a_{H+M}^{(i)} = \\pi_{H+M}(X^{(i)})$ is the labeling result from a proctor using the gaze plot. For the three binary vectors $a_H^{(i)}, a_M^{(i)}$, and $a_{H+M}^{(i)}$, we collect all the positive intervals, and present the intervals for a group of K proctors to label (without gaze plot), and take the majority opinion $a^{*(i)}$ as the reference for comparison.\nNote that we ensure high precision for $a^{*(i)}$ by using multiple proctors to reduce variance and selecting only positive intervals instead of the entire video to reduce tediousness. However, this also means that if an interval is labeled as negative by all three systems, it will not be labeled differently in this process.\nComparing $a_H^{(i)}, a_M^{(i)}$, and $a_{H+M}^{(i)}$ with $a^{*(i)}$, we can calculate the average precision and (upper-bounded) recall of each system as F(\u03c0H+M), F(\u03c0H), and F(\u03c0\u03bc). Conducting this experiment is the next step in this project."}, {"title": "User Study", "content": "We also conducted a user study, where we recruited 11 DET proctors to try out the AI-assisted gaze detection system on 300 test sessions sampled from DET. The proctoring results were not used for the official certification, but we collected the feedback from the proctors regarding the gaze detection system with a survey form.\nThe survey is based on a scale of 1-5, where 1 represents \"absolutely disagree\" and 5 represents \"absolutely agree\". Here we show the survey questions and the final averaged scores."}, {"title": "Conclusion", "content": "This paper presents an AI-assisted gaze detection system, which enables proctors to work effectively in finding the moments where a test taker is looking away from the screen. For the demo, we plan to show the gaze detection system on a laptop with an example test session, and the audience would be able to play with the system and give us feedback."}, {"title": "Limitations", "content": "We acknowledge that our system still has limitations, and future work will be needed to further improve the design. Firstly, the gaze plot only shows the gaze directions of the test takers, it doesn't show where on the screen the test taker is actually looking at. Therefore, the gaze plot should not directly be used alone to determine if the test taker is looking away. We expect the ML-only system to perform poorly because calibration will be needed to determine the exact relative positional relationships between the screen, the camera, and the test taker. Secondly, our system currently only works in an asynchronous proctoring environment, where the exam video is recorded. If synchronous proctoring is required, real-time prediction would be needed and the predictions need to be gradually added into the gaze plot. Finally, our proposed evaluation is still based on proctor decisions, and therefore is limited by the information that could be derived from the recorded information. To further improve the accuracy of the evaluation, we could have test takers taking the exams in a controlled environment where the camera and the screen are carefully calibrated. This will allow us to gather accurate measurements for test takers' eye gazes."}]}