{"title": "MATRYOSHKAKV: ADAPTIVE KV COMPRESSION VIA TRAINABLE ORTHOGONAL PROJECTION", "authors": ["Bokai Lin", "Zihao Zeng", "Zipeng Xiao", "Siqi Kou", "Tianqi Hou", "Xiaofeng Gao", "Hao Zhang", "and Zhijie Deng1*"], "abstract": "KV cache has become a de facto technique for the inference of large language models (LLMs), where tensors of shape (layer number, head number, sequence length, feature dimension) are introduced to cache historical information for self-attention. As the size of the model and data grows, the KV cache can quickly become a bottleneck within the system in both storage and memory transfer. To address this, prior studies usually focus on the first three axes of the cache tensors for compression. This paper supplements them, focusing on the feature dimension axis, by utilizing low-rank projection matrices to transform the cache features into spaces with reduced dimensions. We begin by investigating the canonical orthogonal projection method for data compression through principal component analysis (PCA). We observe the issue with PCA projection where significant performance degradation is observed at low compression rates. To bridge the gap, we propose to directly tune the orthogonal projection matrices with a distillation objective using an elaborate Matryoshka training strategy. After training, we adaptively search for the optimal compression rates for various layers and heads given varying compression budgets. Compared to previous works, our method can easily embrace pre-trained LLMs and hold a smooth tradeoff between performance and compression rate. We empirically witness the high data efficiency of our training procedure and find that our method can sustain over 90% performance with an average KV cache compression rate of 60% (and up to 75% in certain extreme scenarios) for popular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) like GPT-4 (OpenAI et al., 2024) and Claude3 (Enis & Hopkins, 2024) have shown great promise, finding applications in areas such as text generation (Brown et al., 2020; Raffel et al., 2023), code completion (Rozi\u00e8re et al., 2024), and sentiment analysis (Zhang et al., 2023a). The Key-Value (KV) cache, which is introduced to cache historical information for self-attention, is essential for maintaining context and accelerating the inference of LLMs. However, as the size of the model and data continues to grow (Fu et al., 2024; Ding et al., 2024; Chen et al., 2024), the KV cache can swiftly lead to system bottleneck in terms of storage and memory transfer (Shi et al., 2024).\nConsiderable efforts have been devoted to addressing such an issue. Noting that the KV cache contains tensors of shape (layer number, head number, sequence length, feature dimension), existing works have investigated compressing the KV cache from the axes of layer number (Brandon et al., 2024; Sun et al., 2024; Goldstein et al., 2024), head number (Ainslie et al., 2023; Shazeer, 2019; Yu et al., 2024), and sequence length (Wang et al., 2024; Zhang et al., 2023b; Li et al., 2024; Xiao et al.,"}, {"title": "2 RELATED WORK", "content": "KV Cache Eviction and Merging. KVMerger (Wang et al., 2024) and PyramidKV (Cai. et al., 2024) introduce innovative approaches to reduce KV cache memory consumption along sequence length dimension in long-context tasks. KVMerger merges KV by Gaussian weights and attention score, while PyramidKV uses a layer-wise approach with recent tokens occupying more weights. CLA (Brandon et al., 2024), YOCO (Sun et al., 2024), and GoldFinch (Goldstein et al., 2024), among others, exploit inter-layer KV cache reuse by sharing KV heads across layers. This significantly reduces the KV cache size along the head number dimension without compromising model capacity. GQA (Ainslie et al., 2023), MQA (Shazeer, 2019), and HeadKV (Yu et al., 2024), especially the last one, have demonstrated the effectiveness of compressing KV cache on the axis of head number due to their low-rank properties.\nKV cache Compression along the hidden size dimension. DeepSeekv2 (DeepSeek-AI et al., 2024) employs MLA techniques to reduce the feature dimension of keys and values within the attention mechanism, but this requires costly retraining from scratch. Concurrent advancements, however, have addressed this limitation. InfiniGen (Lee et al., 2024), Eigen-Attention (Saxena et al., 2024) and HeadKV (Yu et al., 2024) achieve a 40% reduction in the KV cache sizes using orthogonal projections parameterized by the SVD of the Q, K, and V matrices derived from a subset of samples. To mitigate performance degradation, LoRA (Hu et al., 2021) is employed to fine-tune model parameters. However, this compression approach on the axis of feature dimension results in a sharp decline in model performance when using less than 60% cache budget. Furthermore, fine-tuning the base model with LoRA may lead to catastrophic forgetting. In this paper, our method MatryoshkaKV circumvents these risks and achieves higher compression rate by directly fine-tuning orthogonal projections."}, {"title": "3 PRELIMINARY", "content": "This section provides a review of the KV cache mechanism and elucidates the implementation of PCA projection for KV cache compression."}, {"title": "3.1 KV CACHE", "content": "Consider the inference of an LLM p(x) with x as the prompt. It is a common practice to deploy the KV cache technique to each self-attention head in the model to store the key and value states for the present context, including both the prompt \u00e6 and the tokens that have already been generated. Given the KV cache for the context of length L 1 and dimension d in each head, the model generates a subsequent new token y with the attention states $\\text{softmax}(QK^T/\\sqrt{d})V$, where $Q \\in \\mathbb{R}^{1 \\times d}$ is the query vector for y and K, $V \\in \\mathbb{R}^{L \\times d}$ denote the concatenation of the KV cache and the KV vectors for y. This way, the computational complexity for one decoding step is reduced from $O(L)$ to O(1).\nHowever, the size of the KV cache can grow quickly w.r.t. that of the model and context, often causing system bottlenecks in terms of both storage and memory transfer during the inference phase. To address this, various KV cache compression techniques have been proposed, e.g., sharing the KV headers across layers inside LLMs (Brandon et al., 2024; Sun et al., 2024; Goldstein et al., 2024), merging heads that require caching KV (Yu et al., 2024; Ainslie et al., 2023), evicting or merging redundant tokens (Xiao et al., 2024; Li et al., 2024; Cai. et al., 2024; Zhang et al., 2023b). This work"}, {"title": "3.2 TRAING-FREE DIMENSION REDUCTION VIA PCA", "content": "A simple way to reduce the dimension of the KV cache is finding some matrices to project K, V as $K', V' \\in \\mathbb{R}^{L \\times r}$, $(r < d)$. Then, we can only cache K' and V', reducing the storage and memory transfer cost from O(d) to O(r). The rank r is desired to be adjustable based on the available compression budget: when the budget is sufficient, caching full KV states helps prevent information loss; in cases of limited budget, caching only the most essential information should be feasible.\nTo fulfill this, it is reasonable to introduce full-rank projection matrices $U \\in \\mathbb{R}^{d \\times d}$ and demand a hierarchy over the columns of U so that the optimal r-rank cache can result from the first r columns of U, denoted as $U_r \\in \\mathbb{R}^{d \\times r}$. In practice, U should be distinct for K and V and vary across attention heads and layers within the model, as these states commonly exhibit diverse distributions.\nDuring the forward pass of the model, we should be able to recover the original K and V from the reduced K' and V'. A natural choice is using $U^T$, the transposition of the projection matrices, where $UU^T \\approx I$ needs to be satisfied. Given that r can vary from 1 to d, we identify that U should be orthogonal matrices. It is known that the optimal orthogonal projections for compressing a set of high-dimension vectors can be their principal components, so we suggest constructing U based on the PCA results of the key or value states of a long sequence of tokens for each head separately.\nNevertheless, as the table displays, the PCA projections suffer from quickly degraded performance when further increasing the compression level. This is because despite principal components being optimal for key or value recovery in individual head layers, they may be inadequate for preserving the final output due to the non-linearity and compounding effects of the attention mechanism."}, {"title": "4 METHODOLOGY", "content": "To address the aforementioned issue, we propose to jointly tune the orthogonal projection matrices introduced to the LLM under an elaborate objective, to realize a more robust KV cache compression. This section provides the training and inference details of our approach."}, {"title": "4.1 MINIMIZE COMPRESSION LOSS BY KNOWLEDGE DISTILLATION", "content": "Recalling the objective for the compression is that the model outputs based on the compressed states should stay close to the original one. This implies a knowledge distillation objective (Hinton et al., 2015), which can be instantiated with the KL divergence:\n$\\mathcal{L}_{KD} = D_{KL}(p(\\bullet | x) || p'(\\bullet | x))$ (1)\nwhere we abuse p' to refer to the LLM equipped with low-rank projection matrices. As suggested by the literature (Kou et al., 2024), we also incorporate a language modeling loss to p' to prevent the generated text from deviating from the context distribution of the dataset, thereby ensuring high-quality generation. The tuning process involves only the update of U, which ensures that the model performance under the full-rank KV cache is maintained.\nOrthogonal constraint. We initialize the trainable orthogonal projections with the PCA ones due to their effectiveness. To confine the evolution of the projection matrices within the orthogonal matrix family throughout the tuning process, we employ Cayley parameterization to formulate the orthogonal matrix. Specifically, there is $U = (I + Q) (I \u2013 Q)^{-1}$ with Q as a skew-symmetric"}, {"title": "4.2 ACQUIRE HIERARCHICAL KV CACHE BY MATRYOSHKAKV TRAINING", "content": "The tuning process can destroy the hierarchical structures present in the orthogonal matrices inherited from the PCA ones because there is no prioritization given to the columns of the matrices U from the training objective. Consequently, we lose the flexibility to achieve a smooth transition between the level of compression and maintenance of the original performance.\nTo tackle this challenge, we draw inspiration for Matryoshka representation learning (Kusupati et al., 2022), introducing a Matryoshka strategy for training the projection matrices U. In particular, for each training iteration, we randomly sample r from a predefined schedule such as {4, 8, 16, ..., d/4,d/2,d} and use the first r columns of U, i.e., Ur, to construct the model p' for training. Note that the keys and values at different heads and layers use separately sampled r to avoid the entanglement of the compression effect. An illustrative explanation of this is given in Figure 4, and our approach is then called MatryoshkaKV for short."}, {"title": "4.3 FIND HETEROGENEOUS COMPRESSION RATES FOR VARIOUS LAYERS & HEADS", "content": "The Matryoshka training strategy enables the search for heterogeneous compression rates for various layers and heads in the model given a specific compression budget. Basically, we can first propose a compression level for the projection matrix at a particular position, assessing the deviation of the model output from the original on a predefined calibration dataset (measured by KL divergence), and determining whether to accept the proposal based on a predefined tolerance threshold for the deviation. Algorithm 1 exhibits a greedy algorithm for accelerating this based on accepting proposals in parallel. Note that this greedy algorithm also applies to the PCA projections.\nDiscussion. The recent KV cache compression approach on sequence length aspect (Cai. et al., 2024) also observes that compared to uniformly compressed KV cache using the same rate across all layers (Li et al., 2024), employing a distinct compression rate for each layer results in improved information utilization. Furthermore, as observed in (Wu et al., 2024), certain retrieval heads within an LLM consistently attend to crucial information, regardless of contextual variations. The indis-"}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct experiments on continual pre-training (CPT) and supervised fine-tuning (SFT) scenarios to demonstrate that our MatryoshkaKV can not only preserve the foundation knowledge of a base model but also be compatible with LoRA (Hu et al., 2021) for downstream tasks. Ablation studies in Section 5.3 validate the efficacy of our proposed method."}, {"title": "5.1 CONTINUAL PRE-TRAINING", "content": "Setup. We select LLaMA2-7B-base (Touvron et al., 2023) and Mistral-v0.3-7B-base (Jiang et al., 2023) as our base models. We adopt the Matryoshka training strategy detailed in Section 4.2 and fine-tune MatryoshkaKV projections with knowledge distillation loss in Equation 1 and language modeling loss, applying a 1:3 weighting ratio between the two losses. The projection rank rk and rv are randomly sampled from a predefined schedule set {d}=1 during training and are chosen dynamically with the greedy search for adaptive compression levels, as detailed in Section 4.3 during inference. We use Opencompass (Contributors, 2023) to test performance on several widely-used zero-shot benchmarks: PIQA (Bisk et al., 2019), ARC-challenge (ARC-C) (Clark et al., 2018), ARC-easy (ARC-E) (Clark et al., 2018), WinoGrande (WG) (Sakaguchi et al., 2019), HellaSwag (HLSG) (Zellers et al., 2019), and CommonSenseQA (CSQA) (Talmor et al., 2019).\nResults. Table 1 presents the results of our experiments. In zero-shot tasks, our MatryoshkaKV cache substantially reduces the cache footprint with minimal impact on performance. Specifically, our method retains 93.10% of LLaMA2-7B-base's average accuracy and 92.63% of Mistral-v0.3-7B-base's average accuracy, while utilizing only 37.5% of the original cache size. For simpler tasks like PIQA, it achieves 88.71% and 92.00% of the base model's performance with just a 25% cache budget. On more challenging tasks such as ARC-C, a larger cache budget is required, with 50% needed to retain 90% of the base model's performance. By contrast, PCA projection shows a sharp performance drop when the cache budget is reduced below 62.5%, achieving just 70.42% accuracy of LLaMA2-7B-base and 52.86% of Mistral-v0.3-7B-base. These results underscore the superior efficiency of our approach compared with PCA. We attribute PCA's performance decline"}, {"title": "5.2 SUPERVISED FINE-TUNING", "content": "Setup. We use LLaMA2-7B-base (Touvron et al., 2023) as our base model and verify the efficacy of our method on PIQA (Bisk et al., 2019), GSM8K (Cobbe et al., 2021), HellaSwag (Zellers et al., 2019), and OpenbookQA (OBQA) (Mihaylov et al., 2018) datasets. We design a two-stage training strategy to make Matryoshka training strategy compatible with LoRA (Hu et al., 2021) fine-tuning. Specifically, LoRA is firstly used to adapt the base model to downstream tasks, following standard SFT practices (Naveed et al., 2024; Zhao et al., 2024). In the second stage, we jointly fine-tune the MatryoshkaKV projections with the Matryoshka training strategy and the LoRA parameters. Further discussion on the superiority of this recipe is detailed in Appendix C.\nResults. We report accuracy on four zero-shot benchmarks at seven KV cache budgets in Table 2. As shown, our method demonstrates notable performance in the SFT scenario. It achieves 92.47%"}, {"title": "5.3 ABLATION STUDIES", "content": "We conduct ablation studies on various components of our method to verify their effectiveness.\nW/o greedy search for adaptive compression levels. We evaluate our trained models without our greedy search for adaptive compression levels. Figure 4 (Left) represents the average accuracy on four datasets mentioned in Section 5.2 as the cache budget varies. For the exact numerical values, please refer to Table 3 in Appendix D. To ensure that each head in the LLM plays its due role, we set 25% as our minimum cache budget for each head. At a 37.5% cache budget, the average accuracy improves by 1.92%, indicating the significance of our search algorithm for further KV cache compression. Furthermore, our MatryoshkaKV demonstrates robustness even when applying a uniform compression rate across all layers and heads, in contrast to PCA projection, which fails to handle this setting effectively.\nW/o Matryoshka training strategy. As discussed in Section 4.2, we point out that the tuning process w/o Matryoshka training strategy can destroy the hierarchical structures present in the orthogonal matrices inherited from the PCA ones. To validate this, we train MatryoshkaKV projections with a fixed KV cache budget of 50%. The result is displayed in Figure 4 (Right). We observe that fix-"}, {"title": "5.4 HETEROGENEOUS COMPRESSION RATES VISUALIZATION", "content": "Figure 1 shows the heterogeneous compression levels across all attention heads inside our MatryoshkaKV LLM distilled from the LLaMA2-7B-base. We acquire these results by leveraging the greedy search for adaptive compression levels on the ARC-C, ARC-E, and WinoGrande datasets. We observe that shallower layers require larger KV cache budgets, while in deeper layers, only a minority of specific heads require a relatively high budget. PyramidKV (Cai. et al., 2024) also observes that the model aggregates information globally from all available content in lower layers, indicating that KV cache inside lower layers can exert a substantial influence over the final output and should be allocated at a relatively high budget. Therefore, allocating more cache in lower layers and less in higher ones is superior to maintaining a uniform KV cache size across layers. Also, as Wu et al. (2024) point out, retrieval heads with high retrieval scores in LLaMA2-7B-base, are much more important than other heads and should be preserved in KV cache compression. These findings are consistent with our observations.\nMoreover, we observe that keys can be more compressed than values. As shown by the heatmaps in Appendix D, the compression of values affects downstream tasks more than keys. Specifically,"}, {"title": "6 CONCLUSION", "content": "In this study, we delve into how to compress the KV cache in LLMs by applying low-rank projection matrices to the feature dimension. We first investigate data compression using the canonical orthogonal projection method through PCA. We observe significant performance degradation at a relatively high compression rate, indicating that PCA projection is suboptimal for preserving global outputs due to LLMs' nonlinearity and compounding effects. To bridge the gap, we directly optimize orthogonal projection matrices for KV cache compression in LLMs with a distillation objective using an elaborate Matryoshka training strategy. After training, we show that adaptive compression rates for different layers and heads ensure optimal performance compared to uniform compression rates across all layers and heads in LLMs. Experimental results demonstrate significant performance gains and flexibility in achieving desired compression rates compared to traditional PCA projection."}, {"title": "7 FUTURE WORK", "content": "We directly tune orthogonal projections using the Matryoshka training strategy with a distillation objective. Experiments demonstrate that our method efficiently compresses the KV cache along the feature dimension. Having established the compatibility of MatryoshkaKV cache with GQA, we intend to enhance its versatility by integrating our feature compression techniques with contemporary token merging and eviction methods, such as H2O (Zhang et al., 2023b) and StreamingLLM (Xiao et al., 2024). This integration aims to optimize compression rates, especially for extended sequences, and ensure broader compatibility with existing KV cache methodologies."}, {"title": "A ERROR ANALYSIS", "content": "Consider in a decoder layer with key and value has same head dimension: $d = d_{k/v}$, our orthogonal projection $U^K, U^V \\in \\mathbb{R}^{d \\times d}$, error can be computed at a cache budget r/d as:\n$\\begin{aligned}\\mathcal{L} &= || \\text{Attention}(Q, K, V)W^O - \\text{Attention}(Q, K, V)W^O_V ||_F^2 \\\\&= || \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}})VW^O - \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}})VU^V_rU_r^V \\mathcal{T}W^O_V ||_F^2 \\\\&= || \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}})VU^V_rU_r^V \\mathcal{T}W^O - \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}})VU^V_rU_r^V \\mathcal{T}W^O_V ||_F^2 \\\\&= || \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}})VU^V_r(U_r^V)^\\mathcal{T} W^O - \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}})VU^V_r(U_r^V)^\\mathcal{T} U_r^V \\mathcal{T}W^O_V ||_F^2 \\\\&= || \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}}) - \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}})VU^V_r(U_r^V)^\\mathcal{T} + \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}}) VU^V_rU_r^V \\mathcal{T}W^O_V||_F^2 \\end{aligned}$ (2)\nConsider the original parameter $W^O_V$. By donating $\\mathcal{L}_{QK} = \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}}) - \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}})$ and $A = \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}})$ is a constant, we just need to minimize:"}, {"title": "B WEIGHT MERGING METHOD", "content": "Given that both $W^Q$ and $W^K$, as well as our orthogonal projection, operate on hidden states, consolidating parameters evidently reduces computational time. However, many LLMs utilize ROPE Su et al. (2023), introducing a relative position embedding between $W^Q$ and $W^K$, which complicates integrating the parameters with our unitary transform. This issue has been addressed in prior works Saxena et al. (2024); Yu et al. (2024). The approach in Saxena et al. (2024) involves maintaining the merged parameters and transforming the compressed dimension cache back to its original dimensions for reapplication of RoPE. This does not reduce peak memory usage for attention and necessitates ROPE for all past tokens. Alternatively, (Yu et al., 2024) compresses the key states post-RoPE, which prohibits the merging of $W^{Q/K}$ and $U^K$. However, as only a single new token requires orthogonal transformation and dimensionality reduction during inference, the time increase is merely slight as shown in (Yu et al., 2024). Consequently, our treatment of ROPE in the present study is influenced by (Yu et al., 2024)'s methodology. The integration of the weight parameters of $W^O$ and $U^{VT}$, given RoPE has no impact on value states, the details of our weight merging methods can be formulated as follows and in Figure 5\n$\n\\begin{aligned}\n\\text{MSA}(X) &= \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_H)W^O \\\\ &= \\text{Concat}(A_1V_1, A_2V_2, ..., A_HV_H)W^O \\\\ &= \\text{Concat}(A_1V_1U_V^YU_V^{YT}, A_2V_2U_V^YU_V^{YT}, ..., A_HV_HU_V^YU_V^{YT})W^O \\\\ &= \\text{Concat}(A_1V_1U_V^Y, A_2V_2U_V^Y, ..., A_HV_HU_V^Y) (U_V \\mathcal{T}W^O) \\\\&= \\text{Concat}(A_1V_1, A_2V_2, ..., A_HV_H)W^O_V\\end{aligned}\n$\n$\\text{where } A_i = \\text{Softmax}(\\frac{Q_iK_i}{\\sqrt{d_k}})$ is the attention weights of a given head in each layer (4)"}, {"title": "C TWO STAGE SFT", "content": "In this section, we provide a detailed discussion on our observations regarding fine-tuning with LORA and the orthogonal matrix. We elaborate on the issues stemming from calculating covariance on a limited sample subset and performing spectral decomposition, which may lead to suboptimal parameters. We hypothesize that larger gradients during training can arise from task-specific distributions, such as in GSM8K, affecting the alignment of LoRA weights with the base model.\nTo mitigate these issues, our two-phase training approach involves initially training only the LORA weights to ensure adequate adaptation to downstream tasks. In the second phase, we introduce simultaneous training of the unitary transformation matrix and the LoRA weights, focusing on maintaining performance while compressing the cache effectively. We also explore the impact of using separate learning rates for the LoRA and orthogonal matrix parameters to further investigate these phenomena. Extensive experimental results are provided to support our findings."}, {"title": "D EXPERIMENT", "content": "We present some experimental results using a uniform compression rate across all heads after CPT and SFT in our MatryoshkaKV LLM."}]}