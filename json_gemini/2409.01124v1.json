{"title": "Two-stage initial-value iterative physics-informed neural networks\nfor simulating solitary waves of nonlinear wave equations", "authors": ["Jin Song", "Ming Zhong", "George Em Karniadakis", "Zhenya Yan"], "abstract": "We propose a new two-stage initial-value iterative neural network (IINN) algorithm for solitary wave\ncomputations of nonlinear wave equations based on traditional numerical iterative methods and physics-informed\nneural networks (PINNs). Specifically, the IINN framework consists of two subnetworks, one of which is used to fit\na given initial value, and the other incorporates physical information and continues training on the basis of the first\nsubnetwork. Importantly, the IINN method does not require any additional data information including boundary\nconditions, apart from the given initial value. Corresponding theoretical guarantees are provided to demonstrate\nthe effectiveness of our IINN method. The proposed IINN method is efficiently applied to learn some types of\nsolutions in different nonlinear wave equations, including the one-dimensional (1D) nonlinear Schr\u00f6dinger equations\n(NLS) equation (with and without potentials), the 1D saturable NLS equation with PT-symmetric optical lattices,\nthe 1D focusing-defocusing coupled NLS equations, the KdV equation, the two-dimensional (2D) NLS equation\nwith potentials, the 2D amended GP equation with a potential, the (2+1)-dimensional KP equation, and the 3D NLS\nequation with a potential. These applications serve as evidence for the efficacy of our method. Finally, by comparing\nwith the traditional methods, we demonstrate the advantages of the proposed IINN method.", "sections": [{"title": "1 Introduction", "content": "Solitary waves, discovered and named by Russell in 1833-1834, play an important role in the study of nonlinear\nwave equations, which can describe shallow water wave mechanics and light propagation in nonlinear photonic\nlattices [1-4]. For example, the Korteweg-de Vries (KdV) equation, a mathematical physical model for waves on\nshallow water surfaces, demonstrates several characteristics expected of an integrable partial differential equation\n(PDE). It possesses a wide range of explicit solutions, particularly soliton solutions, which can be obtained analyti-\ncally by using the inverse scattering transform (IST) [5]. Moreover, many other nonlinear integrable PDEs can also\nbe analytically solved to find their solitons via the IST [6\u20138]. However, in the case of most non/nearly-integrable\nnonlinear PDEs, their analytical solution expressions are not available, and numerical computations are required to\nstudy these nonlinear waves (e.g., solitary waves) [3,9].\nVarious traditional numerical methods have been developed to tackle this challenge. One classical approach is the\nshooting method, which involves reducing a boundary value problem to an initial value problem [10,11]. It involves\nfinding solutions to the initial value problem for different initial conditions until one finds the solution that also sat-\nisfies the boundary conditions of the boundary value problem. While the shooting method is effective for solving\n1D problems, it is not applicable in higher dimensions. Another important class of methods are iterative methods,\nsuch as the Petviashvili method [9], imaginary-time evolution method (ITEM) [13\u201315], squared-operator iteration\nmethod (SOM) [16] and Newton's method [17,18]. In this category of methods, the solution is updated by a fixed\niterative scheme. For instance, in ITEM, a solitary wave with a specified power is sought by numerically integrating\nthe underlying nonlinear wave equation with the evolution variable t replaced by it [15], where i is the imaginary\nunit. In Newton's method, the solution is updated by solving a linear inhomogeneous operator equation, where the\ninhomogeneous term is the residue of the nonlinear wave equation [19]. Moreover, the conjugate-gradient method is\napplied in solving this linear equation not by direct methods as in the traditional Newton's method, which speeds up\nthe convergence considerably [18]. These numerical methods can achieve fast convergence. However, the choice of\ndiscretization scheme has a significant impact on the algorithm's accuracy and implementation difficulty. In general,"}, {"title": "2 Preliminaries", "content": null}, {"title": "2.1 Some notations and definitions", "content": "Notations: Let Rd and Cm be the d-dimensional real space and m-dimensional complex space, respectively. |\u00b7 | stands"}, {"title": "2.2 Problem statement", "content": "The problem we are interested in is the computation of solitary waves in a general nonlinear wave system in arbitrary\nspatial dimensions, which are special localized solutions that maintain their shapes as they propagate. The system\ncan be written in the following form:\n$L_0u(x) = 0,$\nwhere $L_0$ is a nonlinear operator, $x = (x_1, x_2,\\cdots, x_d) \\in \\mathbb{R}^d$ is a vector spatial variable, $u(x) \\in \\mathbb{C}^m$ is a complex-valued\nvector solitary wave solution, and $u \\to 0$ as $|x| \\to \\infty$. During practical computations, it is common to restrict x to a\nsufficiently large finite domain, that is, $x \\in \\Omega \\subseteq \\mathbb{R}^d$, and $u(x) \\to 0$ when $x \\to \\partial\\Omega$. For example, the d-dimensional\nscalar generalized nonlinear Schr\u00f6dinger (NLS) equation with a potential has the following form:\n$iU_t - \\Delta U + V(x)U + N(x, |U|^2)U = 0,$\nwhere $U = U(x, t) \\in \\mathbb{C}$ is a complex field of the d-dimensional spatial variable $x \\in \\mathbb{R}^d$ and time t, $\\Delta = \\sum_{i=1}^d \\partial_{x_i}^2$ is\nd-dimensional Laplacian, V(x) a real or complex potential, and $N(x, |U|^2)$ a function of x and intensity\n$|U|^2$. The stationary solitary waves (e.g., ground states and excited states) of this equation can be written in the form\n$U(x,t) = u(x)e^{i\\mu t},$\nwhere u(x) is a complex and localized function and $ \\mu \\in \\mathbb{R}$ is the propagation constant. Substituting it into Eq. (3)\nyields the stationary differential equation for u(x)\n$Lu = 0, \\text{ where } L = -\\Delta + V(x) + N(x, |u|^2) - \\mu.$\nNotice that when m = 1 (u(x) \u2208 C), we denote Lou as Lu, and the same applies in later examples. Eq. (5) admits\nsolitary waves of various forms for a large class of functions $N(x, |u|^2)$ and potential V(x). Especially, for the same\nequation, there may be different forms of solitary waves (such as trivial solution, degenerate state, symmetry break-\ning bifurcations, and so on). In the area of scientific computing, especially in solving forward problems of nonlinear\npartial differential equations, the discovery of solitary waves is still an open problem (see, e.g., Ref. [9] and reference\ntherein)."}, {"title": "2.3 Traditional numerical methods", "content": "Numerous numerical methods have been developed thus far to compute solitary waves of nonlinear wave euqations.\nOne is the shooting method, which is efficient for 1D problems but does not apply in higher dimensions [10,11]. Other\nmethods commonly used for solving these problems are iterative methods in the scheme $u_{n+1} = M_n u_n$ for given\ninitial state u0 with iterative operator $M_n$, including the Petviashvili method, accelerated imaginary-time evolution\n(AITEM) method, squared-operator iteration (SOM) method and Newton-conjugate-gradient (NCG) method [9, 12,\n15, 16, 18]. Among them, both the Petviashvili method and the AITEM can only converge to the ground states of\nnonlinear wave equations and would diverge for excited states. For multi-component equations, they may even\ndiverge for the ground states. Furthermore, these iterative methods require that the initial conditions are sufficiently\nclose to the desired exact solutions in order to guarantee algorithm convergence."}, {"title": "2.4 The PINNs method", "content": "Recently, being different from traditional numerical method, deep neural networks were introduced to approximate\nthe solution of partial differential equations (PDEs) with the aid of automatic differentiation methods, which reduce\nthe cost of constructing computationally-expensive grids. Especially, the physics-informed neural networks (PINNs)\napproach [35] was used to consider the important physical laws given by the PDEs to control the output solution\nof a deep neural network, which significantly reduces the required amount of data. The PINNs framework for the\ndata-driven solutions of nonlinear systems (2) can be introduced as follows.\nFirstly, a fully-connected neural network NN(x; \u03b8) with n hidden layers and m neurons in each layer is constructed\nto learn the solution u(x), where the parameters \u03b8 = {W, B} with W = {$w_j$}$_{j=1}^{n+1}$ and B = {$b_j$}$_{j=1}^{n+1}$ being the weight\nmatrices and bias vectors, respectively. Then the vector data of the hidden layers and output layer can be generated\nby following affine transformation Fj\n$A_j = \\sigma \\left(F_j(A_{j-1})\\right) = \\sigma(w_j\\cdot A_{j-1} + b_j), \\quad j = 1, 2, ..., n, \\quad A_{n+1} = F_{n+1}(A_n) = W_{n+1} \\cdot A_n + b_{n+1},$\nwhere \u03c3(\u00b7) denotes some nonlinear activation function, $w_j$ is a dim($A_j$)\u00d7dim($A_{j-1}$) matrix, $A_1$ = x, and $A_j$ =\n($a_{j1}$, ..., $a_{jm}$), $b_j$ = ($b_{j1}$, ..., $b_{jm}$)$^T$. Therefore the relation between input x and output $\\hat{u}(x;\\theta)$ is given by\n$\\hat{u}(x;\\theta) = A_{n+1} = \\left(F_{n+1} \\circ \\sigma \\circ F_n \\circ \\cdot \\cdot \\cdot \\circ \\sigma \\circ F_1\\right)(x),$\nwhere the activation function \u03c3 is chosen as the hyperbolic tangent function tanh(\u00b7) to ensure the smoothness of $\\hat{u}$.\nTo ensure that the output $\\hat{u}(x; \\theta)$ satisfies the equation (2), we utilize the total mean squared error (MSE) to define\nthe following loss function and optimize parameters \u03b8 to minimize the value of loss.\n$\\mathcal{L}_0 = \\text{MSE}_L + \\text{MSE}_\\Gamma = \\frac{1}{N_f} \\sum_{l=1}^{N_f} |L\\hat{u}(x_f^l)|^2 + \\frac{1}{N_b} \\sum_{l=1}^{N_b} |\\hat{u}(x_b^l)|^2,$\nwhere {$x_f^l$}$_{l=1}^{N_f}$ are connected with the randomly chosen sample points in $\\Omega$, and {$x_b^l$}$_{l=1}^{N_b}$ are linked with the randomly\nselected boundary points in $\\partial\\Omega$. With the aid of some optimization approaches (e.g., SGD, Adam & L-BFGS [53,54]),\nwe minimize the loss $\\mathcal{L}_0$ to make the learned solution $\\hat{u}(x; \\theta)$ satisfy Eq. (2).\nIt should be noted that before training an NN model, the parameters \u03b8 need to be initialized. In most cases, the bias\nterm is commonly initialized to zero. There are several effective methods available for initializing weight matrices,\nsuch as Glorot initialization and He initialization [55,56], which help to address the issue of improper initialization\nand can improve the performance and convergence of neural networks."}, {"title": "3 Methodology and applications", "content": null}, {"title": "3.1 Methodology: the IINN framework", "content": "When we try to apply the PINNs [35] to compute the solitary wave solutions of Eq. (2), the learned results are not\nsatisfactory. Especially, if we directly apply Eq. (8) as the loss function, the PINNs often converges to a trivial solution.\nEven if the network converges to a non-trivial solution, that solution may not be what we desire because the same\nequation (8) can admit different states. Inspired by traditional numerical iteration methods, we propose the following\ninitial value iterative neural network (IINN) algorithm to solve this problem and provide corresponding theoretical\nguarantees.\nIn the following, we will introduce the main idea of IINN method. Two identical fully connected neural networks\nNN1 and NN2, defined by Eq. (7), are employed to learn the desired solution $u^*$.\nNN\u2081-First, we choose an appropriate initial value $u_0$ such that it is sufficiently close to $u^*$. Then we randomly\nselect N training points {$x_i$}$_{i=1}^N$ within the region \u03a9 and train the network parameters \u03b8 by minimizing the mean"}, {"title": "3.2 Examples", "content": "In this section, we will demonstrate the performances of the IINN method by applying it to various examples of\nsolitary wave computations. For the following example, if not otherwise specified, we choose a 4-hidden-layer deep\nneural network with 100 neurons per layer, and set learning rate \u03b1 = 0.0001. In the case of certain specific systems,\nwe can find exact soliton solutions, which will serve as a benchmark to evaluate the performance of the network\nby calculating $E_1$. For general cases, we utilize numerical methods such as the Newton-conjugate-gradient (NCG)\nmethod [18] to obtain high-precision approximate solutions, which serve as a reference for comparison.\nGenerally speaking, due to the automatic differentiation algorithm, training NN2 usually takes much more time\nthan training NN1, especially for high-order equations and high-dimensional systems. Therefore, to ensure con-\nvergence speed, the number of training iterations for NN\u2081 needs to be large enough, or the threshold $\\varepsilon_1$ needs to be\nsmall enough. In the following, we set error threshold $\\varepsilon_1$=1e-07. Finally, we should mention that all computations are\nperformed by using a Lenovo notebook with a 2.30GHz eight-cores i7 processor and a RTX3080 graphics processor.\nExample 3.1 (Solitons of the 1D NLS equation with Kerr nonlinearity). The first example we consider is the 1D NLS\nequation with Kerr nonlinearity (where N(x, |U|\u00b2)U in Eq. (3) is taken as the Kerr nonlinear term g|U|2U):\n$iU_t \u2013 U_{xx} + V(x)U + g|U|^2U = 0,$\nwhere V(x) denotes the potential. The corresponding stationary Eq. (5) has the following form\n$Lu = 0, \\quad L = -d_{xx} + V(x) + g|u|^2 - \\mu.$\nIn the following, we consider three scenarios: V = 0, V taking the form of harmonic-Gaussian (HG) potential and\nV taking the complex Scarf-II potential.\nCase 1.\u2014Bright soliton of the 1D NLS equation with V = 0 and g = \u22121. In this case, Eq. (16) admits the bright soliton\nas follows\n$u(x) = \\sqrt{-2\\mu} \\text{sech}(\\sqrt{-\\mu}x), \\qquad \\mu < 0.$"}, {"title": "4.1 Limitations of PINNs method", "content": "Considering that the solitary wave for equation (2) is not unique, especially the equation has trivial solution, if we\ndirectly apply the PINNs method to the calculation of solitary wave, then we will almost certainly get the trivial\nsolution. For example, for the KdV equation (48) (Ref. Eq. (50)) considered in the previous section, the network will\nconverge to the trivial solution u = 0 eventually with loss function $\\mathcal{L}_0$ given by Eq. (8) (see Figs. 16(a1, a2)). Therefore,\nit is almost impossible to directly apply PINNs method to solve solitary waves unless additional information is given\nin the interior of the region.\nFurthermore, if we replace the PDE residual term in the loss function $\\mathcal{L}_0$ given by Eq. (8) in PINNs with $\\mathcal{L}_2$ given\nby Eq. (11), that is\n$\\mathcal{L}_3 := \\frac{1}{N_f} \\sum_{l=1}^{N_f} \\frac{|L\\hat{u}(x_f^l)|^2}{\\text{max}(|\\hat{u}(x)|)} + \\frac{1}{N_b} \\sum_{l=1}^{N_b} |\\hat{u}(x_b^l)|^2,$\nit may be that the network will converge to a non-trivial solution, but the solution may not be that we need. Similarly,\nwe consider the KdV equation by the presented IINN method. After 8000 steps of iterations with NN2 with randomly\ninitialized parameters, although a solitary wave solution is obtained, it is not the desired one (see Fig. 16(b1)). If we\nuse IINN method, we can obtain the solitary wave solution centered at any position (see the red dashed line in\nFig. 16(b1)). This is because the equation has infinitely many solutions. And we do not know which one the network\nwith randomly initialized parameters eventually converges to. According to loss-iteration plot (see Fig. 16(b2)), it\ncan be found that the network has converged."}, {"title": "4.2 Advantages over traditional numerical methods", "content": "IINN method has many advantages over traditional numerical methods (e.g, Ref. [20]). In general, in traditional\nnumerical methods, differentiation is approximated by difference, which requires the domain to be meshed, and the\nerror depends on the mesh size. Therefore, it is difficult to calculate the difference when dealing with complex region\nproblems. However, due to the automatic differentiation algorithm, we can easily deal with derivatives when apply-\ning IINN method. For example, when computing the ground state solution of 2D NLS equation with HO potential,\nwe only need to consider on the disk, and the rest of the region is useless. Therefore, we consider $\\Omega$ = {$x|d(x,0) \\leq 5$}\nwith N = 1000. With the same initial conditions as before, we can obtain the ground state solution through IINN\nmethod, whose intensity diagram on the disk is shown in Fig. 17(a1). The relative $L_2$ error $E_1$=2.340686e-03 compared"}, {"title": "5 Summary", "content": "We have proposed the initial value iterative neural network (IINN) algorithm for solitary wave computations. IINN\nmethod combines the ideas of traditional numerical iterative methods and the principles of physics-informed neu-\nral networks (PINNs), which consists of two subnetworks. One subnetwork is utilized to fit the given initial value\ncondition, while the other subnetwork incorporates physical information and continues training based on the first\nnetwork. Notably, the IINN approach does not require any data information including boundary conditions, except\nthe given initial value. Furthermore, we provide corresponding theoretical guarantees to demonstrate the effective-\nness of our method."}]}