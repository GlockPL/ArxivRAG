{"title": "Metareasoning in uncertain environments: a meta-BAMDP framework", "authors": ["Prakhar Godara", "Tilman Diego Alem\u00e1n", "Angela J. Yu"], "abstract": "In decision-making scenarios, reasoning can be viewed as an algorithm P that\nmakes a choice of an action a* \u2208 A, aiming to optimize some outcome such as\nmaximizing the value function of a Markov decision process (MDP). However,\nexecuting P itself may bear some costs (time, energy, limited capacity, etc.) and\nneeds to be considered alongside explicit utility obtained by making the choice in\nthe underlying decision problem. Such costs need to be taken into account in or-\nder to accurately model human behavior, as well as optimizing AI planning, as all\nphysical systems are bound to face resource constraints. Finding the right P can\nitself be framed as an optimization problem over the space of reasoning processes\nP, generally referred to as metareasoning. Conventionally, human metareason-\ning models assume that the agent knows the transition and reward distributions\nof the underlying MDP. This paper generalizes such models by proposing a meta\nBayes-Adaptive MDP (meta-BAMDP) framework to handle metareasoning in en-\nvironments with unknown reward/transition distributions, which encompasses a\nfar larger and more realistic set of planning problems that humans and Al sys-\ntems face. As a first step, we apply the framework to two-armed Bernoulli bandit\n(TABB) tasks, which have often been used to study human decision making. Ow-\ning to the meta problem's complexity, our solutions are necessarily approximate,\nbut nevertheless robust within a range of assumptions that are arguably realistic for\nhuman decision-making scenarios. These results offer a resource-rational perspec-\ntive and a normative framework for understanding human exploration under cog-\nnitive constraints, as well as providing experimentally testable predictions about\nhuman behavior in TABB tasks. This integration of Bayesian adaptive strategies\nwith metareasoning enriches both the theoretical landscape of decision-making\nresearch and practical applications in designing AI systems that plan under uncer-\ntainty and resource constraints.", "sections": [{"title": "1 Introduction", "content": "In decision making scenarios, reasoning can be viewed as an agent executing an algorithm P that\nselects an action a* \u2208 A that optimizes some outcome, for instance maximizing the value function\nof a Markov decision process (MDP). Similarly, metareasoning (Russell and Wefald [1991], Hay"}, {"title": "2 Background", "content": "For the sake of demonstrating contrast, we start with defining (finite horizon) MDP, BAMDP, and\nmeta-BAMDP in turn. An exact implementation of meta-BAMDP follows in the subsequent section."}, {"title": "2.1 Markov Decision Process - MDP", "content": "A Markov Decision Process (MDP) is formally defined by a tuple (S, A, P, R). Here S is the set\nof states of the environment, A the set of actions available to the agent, P : S \u00d7 A \u00d7 S \u2192 [0, 1]\nthe transition probability function and R : S \u00d7 A \u00d7 R \u2192 [0, 1] the reward distribution. The goal in\nan MDP is to find an optimal policy \u03c0*(s) that maximizes the expected cumulative reward from any\ngiven state. Formally, the objective is to maximize:\n$V^{\\pi}(s) = E\\left[\\sum_{t=0}^{T} \\int r R(r|s_t, \\pi(s_t)) dr | s_0 = s \\right]$"}, {"title": "2.2 Bayes-Adaptive Markov Decision Process - BAMDP", "content": "A Bayes-adaptive Markov decision process (BAMDP) extends the standard MDP framework\nby incorporating uncertainty about the reward and transition distributions, defined by the tuple\n$(S_B, A, P_B, R_B, b_0)$. Here, $S_B = S \\times B$ represents the augmented state space, where S is the\nphysical state space, and B is the belief space encapsulating probabilistic beliefs over parameterized\ndistributions P and R (parameters \u03b8, with belief b : \u0398 \u2192 [0, 1]). The action space A, transition\nmodel $P_B : S_B \\times A \\times S_B \u2192 [0, 1]$, and reward distribution $R_B : S_B \\times A \\times R \u2192 [0, 1]$.\nThe initial belief distribution over the models, $b_0$, sets the starting conditions. The goal of a BAMDP\nis to derive an optimal policy \u03c0*(s, b) that maximizes the expected cumulative reward, accounting\nfor model uncertainty. Formally, the maximization target is given by:\n$V^{\\pi} (s,b) = E\\left[ \\sum_{t=0}^T \\int r R(r | s_t, b_t, \\pi(s_t, b_t)) dr | s_0 = s, b_0 = b \\right]$"}, {"title": "3 Meta-Bayes-Adaptive Markov Decision Process - meta-BAMDP", "content": "We now present our definition of a meta-BAMDP. A meta-BAMDP is defined as a tuple\n($\\mathbb{S}_{MB}, A_{MB},P_{MB},R_{MB},K$), where:\n\u2022 The state space is given by $S_{MB} = S \u00d7 B \u00d7 \\mathcal{B}$. Here, S is the state of the environment,\nB is the set of beliefs representing the agent's state of knowledge regarding the transition\nand reward distributions. $B$ is the space of planning-beliefs. Abstractly, planning belief\n$b \\in \\mathcal{B}$ represents the intermediate computational states of a planning algorithm, thereby\nrepresenting the extent to which the agent has engaged in planning. Eg - if the planning\nalgorithm finds the optimal path on a DAG G, then its intermediate computational states\ncan be viewed as sub-DAG's of G.\n\u2022 The agent's action space $A_{MB} = A \u222a C$, where A corresponds to the physical actions as\nbefore and C represents the computational actions. Both these actions differ in the kinds of\ntransitions they cause. A physical action causes transitions in the environment state and, as\nin the BAMDP, the belief state b \u2208 B. A computational action causes transitions only in\nthe planning beliefs b.\n\u2022 The meta-BAMDP transition function $P_{MB}$ is given by\n$P_{MB} = \\begin{cases} P_A(s', b', \\hat{b}' | s, b, \\hat{b}, a), & a \\in A, \\\\ P_C(\\hat{b}' | \\hat{b}, a), & a \\in C. \\end{cases}$"}, {"title": "4 A meta-BAMDP for two armed Bernoulli bandit task", "content": "While the meta-BAMDP framework applies to a wide range of problems, we now apply it to a two\narmed Bernoulli bandit task (TABB). In a TABB task the state space S is a singleton set, making\nthe transition dynamics trivial. Therefore, B only incorporates beliefs over reward distributions. We\nbegin by considering how one might obtain the optimal solution to the underlying BAMDP. The\nstate space S in TABB is a singleton set, and can therefore be ignored (Sutton and Barto [2018]).\nAssuming that the agent performs Bayesian inference and starts with a uniform prior distribution,\nthe belief space of the TABB can be assumed to be\n$B = \\{ b = (\\alpha_1, \\beta_1, \\alpha_2, \\beta_2) \\in \\mathbb{N} : \\alpha_1 + \\beta_1 + \\alpha_2 + \\beta_2 \\leq T \\},$.\nIn order to obtain the optimal BAMDP policy, the agent can be imagined to construct the complete\ndecision-action graph (see Fig. 1 for a schematic) associated with TABB. The value of the terminal\nbeliefs (circles in Fig. 1) can be set to zero and then iteratively the values of all the non-terminal\nbeliefs (and actions) can be obtained via backward induction until the value of the root node $V(\\textbf{b}_0)$\nis found. Therefore, if given access to the entire graph, the agent makes use of backward induction\nto find the policy that maximizes $V(\\textbf{b}_0)$.\nBut what action should an agent take, if it does not have access to the entire graph? This would be\nakin to a situation where the agent has not considered the consequences of all of its actions until the\nend of the task. Let us say, that the agent only has access to a sub-graph (in solid lines in Fig. 1).\nU(b) = \\max \\left( \\frac{\\alpha_1 + 1}{\\alpha_1 + \\beta_1 + 2}, \\frac{\\alpha_2 + 1}{\\alpha_2 + \\beta_2 + 2} \\right) (T - \\sum_i (\\alpha_i + \\beta_i))"}, {"title": "5 Finding good approximations via pruning", "content": "We now make a series of arguments geared toward pruning the meta-graph. Let us start by writing\ndown the objective value function $V^{\\pi_{meta}}$ corresponding to the meta policy $\\pi_{meta}$ as\n$V^{\\pi_{meta}} (b, \\hat{b}) = \\mathbb{I}[\\pi_{meta}(a = \\| | b, \\hat{b}] [\\int r R_B(r|a=1,b)dr + \\int V^{\\pi_{meta}} (b', \\hat{b}') P_B(b'|b, \\hat{b}, a = 1)db'] \\+ \\\\ [\\int [V^{\\pi_{meta}}(\\hat{b}, \\hat{b}') P_C(\\hat{b} | \\hat{b}, a) - c] \\mathbb{I}[\\pi_{meta}(a \\neq \\|) | b, \\hat{b}] d\\hat{b}'$)"}, {"title": "5.1 Monotonic effect of node expansion on subjective value", "content": "Let us consider that the agent is in state b = (\u03b1\u2081, \u03b2\u2081, \u03b1\u2082, \u03b2\u2082) and \u0125 corresponds to just the root\nnode b. In the following we look at the impact of a single computational action on the subjective\nvalue V(b) = K(b)(b). The discussion will be easier if we look at the subjective Q values\nQ(a = 1, b) and Q(a = 2, b) instead. Prior to any computations we have Q(a = 1, b) = p\u2081\u03c4 and\nQ(a = 2, b) = p\u2082\u03c4, where $p_i = \\frac{\\alpha_i + 1}{\\alpha_i + \\beta_i + 2}$ for i \u2208 {1, 2} and \u03c4 is the remaining number of actions.\nWithout loss of generality, we expand a = 1 and obtain a new computational belief corresponding\nto b\u2081. Obviously, this transition leaves Q(a = 2, b) unchanged.\nLet the new subjective Q value corresponding to action a = 1 be Q(a = 1, b). Let the two child\nstates of a = 1 be called W (for win) and L, and the corresponding reward probability under action\na = 1 be $p_1^W$ and $p_1^L$ respectively. The subjective values of the child states $V^W$ and $V^L$, under the\nexpanded graph, are given by\n$V^j = \\max(p_1, p_2)(\\tau - 1),$.\nCorrespondingly, the Q value now becomes\n$Q(a = 1, b) = p_1 (1 + V^W) + (1 - p_1) V^L$."}, {"title": "6 Implications for human exploration behavior", "content": "We discuss below the characteristic properties of the meta-BAMDP solutions for varying computa-\ntional costs c, based on numerical solutions, for tasks of different lengths T < 14."}, {"title": "7 Conclusions", "content": "In this paper we developed a novel meta-BAMDP framework, which extends the scope of modeling\nmetareasoning in humans to also include situations with unknown transition/reward dynamics. We\npresent a theoretical instantiation of the framework in the TABB task, and provide an approach to\nmake the problem computationally feasible. Moreover, we show that solutions from our model qual-\nitatively explain recent experimental data on human exploration behavior in bandit tasks. Finally,\nour model also provides novel testable predictions for human behavior in bandit tasks. While addi-\ntional work is needed to validate model predictions, as well as expanding theoretical understanding\nand practical implementation of a broader class of meta-BAMDP problems, this work nevertheless\nrepresents a novel theoretical and algorithmic advancement in reinforcement learning and human\ncognitive modeling."}, {"title": "A Supplementary material", "content": "The complete\nalgorithm can be accessed via the url :\nhttps://github.com/Dies-Das/meta-BAMPD-data."}, {"title": "A.1 Pseudocode", "content": "The algorithm to find the solution to the meta-BAMDP involves two routines. First, to generate a\npruned meta-graph and second to perform backward induction on this meta-graph, to find the optimal\nmeta-policy. The latter is rather straight-forward, but the former is slightly complex to present and\ntherefore for ease of understanding we provide a pseudocode below.\nInput: maximum horizon T, initial beliefs bo, initial computational beliefs \u0125o, set M\nOutput: pruned meta-graph G\n1. Initialize the meta-graph G with a root node containing (bo, \u0125o).\n2. Initialize a queue Q and enqueue the root node.\n3. While Q is not empty:\n3.1. Dequeue a node (b, \u0125) from Q.\n3.2. Determine the terminal action a* for (b, \u0125) using Eq. 5.\n3.3. Calculate the subsequent belief states b' for action a*.\n3.4. Update \u0125 to be the subgraph of b starting from b'.\n3.5. Create new nodes (b', \u0125') for the meta-graph G if not already present.\n3.6. Add edges from (b, \u0125) to (b', \u0125').\n3.7. Enqueue new nodes (b', \u0125') into Q.\n3.8. If \u0125 is not in M:"}, {"title": "A.2 Robustness of solution", "content": "In order to test the robustness of the approximate solution, we loosen the restriction of myopic\napproximation from Sec. 5, in two directions. These two directions, correspond to two distinct\napproximation schemes (or terminal conditions X above) that we tested. First as in the main text,"}, {"title": "A.3 Comparing heuristics to meta-policies", "content": "For each computational cost c, we solve the meta-BAMDP problem and find the optimal meta\npolicy. Each meta policy implies a probability distribution over base policies. However, for\nthe myopic assumption, each meta-policy has a unique corresponding base-policy, which we call\n$\\hat{\\pi}_{base}(b)$. We define the squared distance between two base-policies (\u03c01, \u03c02) to be $d^2(\\pi_1, \\pi_2) =$\n$\\sum_{b} \\sum_{a} (\\pi_1(a|b) - \\pi_2(a|b))^2$. In order to find the best fit w (and simultaneously \u03b2), we minimize\nthe squared distance $d^2(\\pi_1, \\pi_2)$ where $\\pi_1 = \\hat{\\pi}_{base}$ and $\\pi_2 = \\frac{e^{-(\\beta \\mu + w \\sigma)}}{N}$, where N is the nor-\nmalization factor. As the optimal policies from the BAMDP are deterministic (except when ties are\nbroken), the estimated value \u03b2 turns out to be suitably large ($\\approx$ 100 in our case). We consider the\n\u03b2 \u00d7 w space to be bounded by the square (0, 100) \u00d7 (-10, 10), to remain broadly consistent with\nthe work of Brown et al. [2022]."}]}