{"title": "Metareasoning in uncertain environments: a meta-BAMDP framework", "authors": ["Prakhar Godara", "Tilman Diego Alem\u00e1n", "Angela J. Yu"], "abstract": "In decision-making scenarios, reasoning can be viewed as an algorithm P that makes a choice of an action a* \u2208 A, aiming to optimize some outcome such as maximizing the value function of a Markov decision process (MDP). However, executing P itself may bear some costs (time, energy, limited capacity, etc.) and needs to be considered alongside explicit utility obtained by making the choice in the underlying decision problem. Such costs need to be taken into account in order to accurately model human behavior, as well as optimizing AI planning, as all physical systems are bound to face resource constraints. Finding the right P can itself be framed as an optimization problem over the space of reasoning processes P, generally referred to as metareasoning. Conventionally, human metareasoning models assume that the agent knows the transition and reward distributions of the underlying MDP. This paper generalizes such models by proposing a meta Bayes-Adaptive MDP (meta-BAMDP) framework to handle metareasoning in environments with unknown reward/transition distributions, which encompasses a far larger and more realistic set of planning problems that humans and Al systems face. As a first step, we apply the framework to two-armed Bernoulli bandit (TABB) tasks, which have often been used to study human decision making. Owing to the meta problem's complexity, our solutions are necessarily approximate, but nevertheless robust within a range of assumptions that are arguably realistic for human decision-making scenarios. These results offer a resource-rational perspective and a normative framework for understanding human exploration under cognitive constraints, as well as providing experimentally testable predictions about human behavior in TABB tasks. This integration of Bayesian adaptive strategies with metareasoning enriches both the theoretical landscape of decision-making research and practical applications in designing AI systems that plan under uncertainty and resource constraints.", "sections": [{"title": "1 Introduction", "content": "In decision making scenarios, reasoning can be viewed as an agent executing an algorithm P that selects an action a* \u2208 A that optimizes some outcome, for instance maximizing the value function of a Markov decision process (MDP). Similarly, metareasoning (Russell and Wefald [1991], Hay\nPreprint. Under review."}, {"title": "1.1 Related work and contributions", "content": "Recent efforts in studying metareasoning in humans have focused on planning problems, i.e. the underlying, externally-defined problem is viewed as an MDP (Callaway et al. [2022], Lieder et al. [2018]). These works assume that the agent knows the true transition/reward distributions but not the optimal value function. The agent therefore engages in some reasoning to improve upon its current policy/value function (for instance via a policy iteration algorithm (D'Oro and Bacon [2021])). Metareasoning then concerns itself with finding the optimal resource allocation (time and space)"}, {"title": "2 Background", "content": "For the sake of demonstrating contrast, we start with defining (finite horizon) MDP, BAMDP, and meta-BAMDP in turn. An exact implementation of meta-BAMDP follows in the subsequent section."}, {"title": "2.1 Markov Decision Process - MDP", "content": "A Markov Decision Process (MDP) is formally defined by a tuple (S, A, P, R). Here S is the set of states of the environment, A the set of actions available to the agent, P : S \u00d7 A \u00d7 S \u2192 [0, 1] the transition probability function and R : S \u00d7 A \u00d7 R \u2192 [0, 1] the reward distribution. The goal in an MDP is to find an optimal policy \u03c0*(s) that maximizes the expected cumulative reward from any given state. Formally, the objective is to maximize:\n$V^{\\pi}(s) = E \\left[\\sum_{t=0}^{T} \\int r R(r|s_{t}, \\pi(s_{t})) dr \\mid s_{0} = s\\right]$\nwhere st represents the state at time t, \u03c0(st) specifies the action taken under policy \u03c0 when the state is st, and V\" (s) is the value function under policy \u03c0. The optimal policy \u03c0* is then defined as\n$\\pi^{*}(s) = \\arg \\max V^{\\pi}(s)$. If the problem an agent faces can be modelled as an MDP, the agent would need access to P and R in order to find \u03c0*. This process (of finding the optimal policy) is usually referred to as planning (as opposed to learning, which refers to learning the transition distributions from experience). In a more general setting, both P and R may be initially unknown to the agent, and it would also need to learn them from experience. The agent could, for instance, learn the transition and reward distributions via Bayesian inference. If so, we end up with a BAMDP.\n\u00b9As we will see later, a naive solution approach has exponential complexity."}, {"title": "2.2 Bayes-Adaptive Markov Decision Process - BAMDP", "content": "A Bayes-adaptive Markov decision process (BAMDP) extends the standard MDP framework by incorporating uncertainty about the reward and transition distributions, defined by the tuple (SB, A, PB, RB, bo). Here, SB = S \u00d7 B represents the augmented state space, where S is the physical state space, and B is the belief space encapsulating probabilistic beliefs over parameterized distributions P and R (parameters 0, with belief b : \u0398 \u2192 [0, 1]). The action space A, transition model PB : SB \u00d7 A \u00d7 SB \u2192 [0, 1], and reward distribution RB : SB \u00d7 A \u00d7 R \u2192 [0, 1].\nThe initial belief distribution over the models, bo, sets the starting conditions. The goal of a BAMDP is to derive an optimal policy \u03c0*(s, b) that maximizes the expected cumulative reward, accounting for model uncertainty. Formally, the maximization target is given by:\n$V^{\\pi} (s,b) = E \\left[\\sum_{t=0}^{T} \\int r R(r|s_{t}, b_{t}, \\pi(S_{t}, b_{t})) dr | s_{0} = s, b_{0} = b \\right]$\nwhere (st, bt) denotes the state and belief at time t. The optimal policy \u03c0* = arg max V\u2122 (s, b) is defined as the one that maximizes V\u201d for all (s, b) \u2208 S\u0432.\nThere are some crucial things to take note of here. A BAMDP is structurally distinct from a typical model-based RL algorithm like Dyna (Sutton [1991]). Not only is the agent updating its beliefs about (P, R) to then solve the implied MDP, but also, PB incorporates the evolution of the beliefs themselves. Therefore, while the agent might not know the environment dynamics, it could still make use of its belief update dynamics to guess the future status of its knowledge. Therefore, when moving from an MDP to a BAMDP, we \"loosen\" the restriction on the part of the agent - i.e. from requiring it to know (P, R), to requiring it to know PB. This apparent generality doesn't come for free: the state space of BAMDP is much larger than that of the underlying MDP, since an arbitrary probability distribution over continuous r.v.'s is effectively infinite-dimensional."}, {"title": "3 Meta-Bayes-Adaptive Markov Decision Process - meta-BAMDP", "content": "We now present our definition of a meta-BAMDP. A meta-BAMDP is defined as a tuple (\u0405\u043c\u0432, \u0410\u043c\u0432,\u0420\u043c\u0432,RMB,K), where:\n\u2022 The state space is given by SMB = S \u00d7 B \u00d7 B. Here, S is the state of the environment, B is the set of beliefs representing the agent's state of knowledge regarding the transition and reward distributions. B is the space of planning-beliefs. Abstractly, planning belief b\u2208 B represents the intermediate computational states of a planning algorithm, thereby representing the extent to which the agent has engaged in planning. Eg - if the planning algorithm finds the optimal path on a DAG G, then its intermediate computational states can be viewed as sub-DAG's of G.\n\u2022 The agent's action space AMB = AUC, where A corresponds to the physical actions as before and C represents the computational actions. Both these actions differ in the kinds of transitions they cause. A physical action causes transitions in the environment state and, as in the BAMDP, the belief state b \u2208 B. A computational action causes transitions only in the planning beliefs b.\n\u2022 The meta-BAMDP transition function PMB is given by\n$P_{MB} = \\begin{cases} P_{A}(s', b', \\b'|s, b, \\tilde{b}, a), & a \\in A, \\\\ P_{C}(\\tilde{b'}|\\tilde{b}, a), & a \\in C. \\end{cases}$\nHere' represents the corresponding states at the next time step. PA defines the transitions caused by physical actions and Pc the transitions caused by computational actions.\n\u2022 The reward distribution RBM :S \u00d7 B \u00d7 AMB \u00d7 R \u2192 [0, 1] is given by\n$R_{MB} = \\begin{cases} R_{B}(r|s,b,a) & a \\in A \\\\ R_{C}(r|a) & a \\in C \\end{cases}$"}, {"title": "4 A meta-BAMDP for two armed Bernoulli bandit task", "content": "While the meta-BAMDP framework applies to a wide range of problems, we now apply it to a two armed Bernoulli bandit task (TABB). In a TABB task the state space S is a singleton set, making the transition dynamics trivial. Therefore, B only incorporates beliefs over reward distributions. We begin by considering how one might obtain the optimal solution to the underlying BAMDP. The state space S in TABB is a singleton set, and can therefore be ignored (Sutton and Barto [2018]). Assuming that the agent performs Bayesian inference and starts with a uniform prior distribution, the belief space of the TABB can be assumed to be\n$B = \\{b = (\\alpha_{1}, \\beta_{1}, \\alpha_{2}, \\beta_{2}) \\in \\mathbb{N}: \\alpha_{1} + \\beta_{1} + \\alpha_{2} + \\beta_{2} \\leq T\\},$\nwhere ai is the number of successes after taking the action a i, and B\u2081 the failures.\nIn order to obtain the optimal BAMDP policy, the agent can be imagined to construct the complete decision-action graph (see Fig. 1 for a schematic) associated with TABB. The value of the terminal beliefs (circles in Fig. 1) can be set to zero and then iteratively the values of all the non-terminal beliefs (and actions) can be obtained via backward induction until the value of the root node V (bo) is found. Therefore, if given access to the entire graph, the agent makes use of backward induction to find the policy that maximizes V (bo).\nBut what action should an agent take, if it does not have access to the entire graph? This would be akin to a situation where the agent has not considered the consequences of all of its actions until the end of the task. Let us say, that the agent only has access to a sub-graph (in solid lines in Fig. 1).\n\u00b2It is crucial to note that this is the subjective value function of the agent and doesn't reflect the true value of being in a state."}, {"title": "5 Finding good approximations via pruning", "content": "We now make a series of arguments geared toward pruning the meta-graph. Let us start by writing down the objective value function Vmeta corresponding to the meta policy \u03c0meta as\n$\\begin{aligned} V^{meta}(\\tilde{b}, b) &= \\pi_{meta}(a = | |\\tilde{b}, b) \\left[ \\int r R_{B}(r|a=1, b) dr + \\int V^{meta}(b', \\tilde{b}) P_{B}(b'|b, \\tilde{b}, a =1) db'\\right] \\\\ & + \\left[ \\int V^{meta}(\\tilde{b}, b') P_{C}(\\tilde{b'}|\\tilde{b}, a) -c \\right] \\pi_{meta}(a(\\neq 1)| \\tilde{b},b) d\\tilde{b'} \\end{aligned}$"}, {"title": "5.1 Monotonic effect of node expansion on subjective value", "content": "Let us consider that the agent is in state b = (a1, \u03b21, a2, \u03b22) and b corresponds to just the root node b. In the following we look at the impact of a single computational action on the subjective value V(b) = K(b)(b). The discussion will be easier if we look at the subjective Q values\nQ(a = 1, b) and Q(a = 2, b) instead. Prior to any computations we have Q(a = 1,b) = p\u2081t and\nQ(a = 2,b) = p2T, where pi = , for i \u2208 {1,2} and 7 is the remaining number of actions.\nWithout loss of generality, we expand a = 1 and obtain a new computational belief corresponding to b\u2081. Obviously, this transition leaves Q(a = 2, b) unchanged.\nLet the new subjective Q value corresponding to action a = 1 be Q(a = 1,b). Let the two child states of a = 1 be called W (for win) and L, and the corresponding reward probability under action a = 1 bep and \u00f4, respectively. The subjective values of the child states VW and VL, under the expanded graph, are given by\n$V^{j} = \\max(\\tilde{p}_{1}, \\tilde{p}_{2})(\\tau - 1),$\nfor j \u2208 {W, L}. Correspondingly, the Q value now becomes\n$Q(a = 1,b) = \\tilde{p}_{1}(1 + V^{W}) + (1 - \\tilde{p}_{1})V^{L}.$\nOur task is to now compare Q(a = 1, b) and Q(a = 1, b). There are three cases which form.\nW\n1. Assume p > p\u00a6 > p2. In this case\n$Q(a = 1,b) = \\tilde{p}_{1}(1 + \\tilde{p}^{W}(\\tau - 1)) + (1 - \\tilde{p}_{1})\\tilde{p}^{L}(\\tau - 1).$\nBy substituting the expressions for, one obtains that in this case Q(a = 1,b) = Q(a = 1, b).\nW\n2. Assume p > p2 > p\u0142. In this case\n$Q(a = 1,b) = \\tilde{p}_{1}(1 + \\tilde{p}^{W}(\\tau - 1)) + (1 - \\tilde{p}_{1})\\tilde{p}_{2}(\\tau - 1).$\nClearly this is greater than the Q value from Eq. 11, further implying that Q(a = 1,b) > Q(a = 1, b).\n3. Assume p2 > p\u2122 > p\u0142. This implies that\n$Q(a = 1,b) = \\tilde{p}_{1} + \\tilde{p}_{2}(\\tau - 1).$\nYet again, this is greater than the expression in Eq. 12.\nIt can therefore be concluded that performing a computational (node-expansion) action can only have a monotonically increasing effect on the corresponding subjective Q value, and by extension the subjective value V (b). Notice that the subjective Q value for a given state-action pair is upper bounded by the optimal value Q* corresponding to the subjective value, when the entire reachable sub-graph has been expanded. This further implies that in states b, if there exists an i such that\nQ(a = i,b) \u2265 Q*(a \u2260 i,b), no further computation can change the agent's behavior. Let us call\n\u00b3Here the relation between V and Q is given by the usual relation V (b) = maxa (Q(a, b))"}, {"title": "6 Implications for human exploration behavior", "content": "We discuss below the characteristic properties of the meta-BAMDP solutions for varying computational costs c, based on numerical solutions, for tasks of different lengths T < 14.\nWhen comparing behavior in tasks of different lengths, we first consider the normalized expected reward (ignoring computational costs) under the optimal meta-policy as a function of computational cost (c.f. Fig. 2(a)). The presented values are averaged over a uniform distribution over all TABB environments. From Eq. 8 it is evident that the external rewards accrued by a meta-policy are lower bounded by the value of the greedy policy V\u00ba and upper bounded by the value of the opti- mal BAMDP policy, V*. Therefore we define the normalized expected reward as VN V*-V9. We observe from Fig. 2(a) that VN monotonically decreases with c. This offers a novel compu- tational explanation of the positive correlation observed between IQ and bandit task performance Steyvers et al. [2009], i.e individuals with higher working memory and attentional capacity have lower (opportunity-related) computational cost c and therefore are able to plan further ahead and make better decisions. Additionally, we observe a characteristic dependence of behavior on the task"}, {"title": "7 Conclusions", "content": "In this paper we developed a novel meta-BAMDP framework, which extends the scope of modeling metareasoning in humans to also include situations with unknown transition/reward dynamics. We present a theoretical instantiation of the framework in the TABB task, and provide an approach to make the problem computationally feasible. Moreover, we show that solutions from our model qual- itatively explain recent experimental data on human exploration behavior in bandit tasks. Finally, our model also provides novel testable predictions for human behavior in bandit tasks. While addi- tional work is needed to validate model predictions, as well as expanding theoretical understanding and practical implementation of a broader class of meta-BAMDP problems, this work nevertheless represents a novel theoretical and algorithmic advancement in reinforcement learning and human cognitive modeling."}, {"title": "A Supplementary material", "content": "The complete algorithm can be accessed via the url : https://github.com/Dies-Das/meta-BAMPD-data."}, {"title": "A.1 Pseudocode", "content": "The algorithm to find the solution to the meta-BAMDP involves two routines. First, to generate a pruned meta-graph and second to perform backward induction on this meta-graph, to find the optimal meta-policy. The latter is rather straight-forward, but the former is slightly complex to present and therefore for ease of understanding we provide a pseudocode below.\nInput: maximum horizon T, initial beliefs bo, initial computational beliefs bo, set M\nOutput: pruned meta-graph G\n1. Initialize the meta-graph G with a root node containing (bo, bo).\n2. Initialize a queue Q and enqueue the root node.\n3. While Q is not empty:\n3.1. Dequeue a node (b, b) from Q.\n3.2. Determine the terminal action a* for (b, b) using Eq. 5.\n3.3. Calculate the subsequent belief states b' for action a*.\n3.4. Update b to be the subgraph of b starting from b'.\n3.5. Create new nodes (b', b') for the meta-graph G if not already present.\n3.6. Add edges from (b, b) to (b', b').\n3.7. Enqueue new nodes (b', b') into Q.\n3.8. If b is not in M:\n3.8.1. Start searching through all possible computational expansion trajectories from (b, b) to (b, b') that end with a terminal action. This is done by a depth-first search, while making use of the restrictions presented in Sec. 5.\n3.8.2. For each trajectory in the above set:\n3.8.2.1. Determine the terminal action a** and the subsequent belief states after execut- ing a** in (b, b'), i.e., (b', b').\n3.8.2.2. If the a** differs from a*, add the new nodes (b', b') to G and Q.\n3.8.2.3. Terminate the search along a trajectory if:\n\u2022 No further computations along this trajectory can change the physical action taken under that computational belief (Sec. 5).\n\u2022 Some other terminal condition X is met (see Sec. A.2).\n4. Return the pruned meta-graph G.\nThere are two underdiscussed aspects of this pseudocode. First is that we only consider those com- putational action trajectories starting from (b, b) that can cause a change in the terminal action (as in 3.8.2.2. in the pseudocode above). There is no utility in performing computations if they cannot alter the terminal action, as any such computations could be done at a later state b'. Secondly, in the main manuscript, we assume that the terminal action leaves the bunchanged. It should be easy to see that the only relevant part of b in a state (b, b) is the reachable subgraph of b, i.e. only computations about the reachable future are of relevance to the agent. Therefore, keeping the b invariant to termi- nal actions or keeping only the relevant reachable subgraph are equivalent insofar as the solution to the meta-BAMDP is concerned."}, {"title": "A.2 Robustness of solution", "content": "In order to test the robustness of the approximate solution, we loosen the restriction of myopic approximation from Sec. 5, in two directions. These two directions, correspond to two distinct approximation schemes (or terminal conditions X above) that we tested. First as in the main text,"}, {"title": "A.3 Comparing heuristics to meta-policies", "content": "For each computational cost c, we solve the meta-BAMDP problem and find the optimal meta- policy. Each meta policy implies a probability distribution over base policies. However, for the myopic assumption, each meta-policy has a unique corresponding base-policy, which we call base(ab). We define the squared distance between two base-policies (\u03c01, \u03c02) to be d\u00b2(\u03c01,\u03c02) = \u03a3\u03b5 \u03a3\u03b1 (\u03c0\u03b9(a/b) \u2013 \u03c02(a|b))2. In order to find the best fit w (and simultaneously \u03b2), we minimize the squared distance d\u00b2(\u03c01,\u03c02) where \u03c0\u2081 = rbase and \u03c0\u2082 = e-(\u03b2\u03bc+w\u00f4), where N is the nor- malization factor. As the optimal policies from the BAMDP are deterministic (except when ties are broken), the estimated value \u1e9e turns out to be suitably large (\u2248 100 in our case). We consider the \u03b2 \u00d7 w space to be bounded by the square (0, 100) \u00d7 (-10, 10), to remain broadly consistent with the work of Brown et al. [2022]."}]}