{"title": "Position: Empowering Time Series Reasoning with Multimodal LLMs", "authors": ["Yaxuan Kong", "Yiyuan Yang", "Shiyu Wang", "Chenghao Liu", "Yuxuan Liang", "Ming Jin", "Stefan Zohren", "Dan Pei", "Yan Liu", "Qingsong Wen"], "abstract": "Understanding time series data is crucial for multiple real-world applications. While large language models (LLMs) show promise in time series tasks, current approaches often rely on numerical data alone, overlooking the multimodal nature of time-dependent information, such as textual descriptions, visual data, and audio signals. Moreover, these methods underutilize LLMs' reasoning capabilities, limiting the analysis to surface-level interpretations instead of deeper temporal and multimodal reasoning. In this position paper, we argue that multimodal LLMs (MLLMs) can enable more powerful and flexible reasoning for time series analysis, enhancing decision-making and real-world applications. We call on researchers and practitioners to leverage this potential by developing strategies that prioritize trust, interpretability, and robust reasoning in MLLMs. Lastly, we highlight key research directions, including novel reasoning paradigms, architectural innovations, and domain-specific applications, to advance time series reasoning with MLLMs.", "sections": [{"title": "1. Introduction", "content": "Time series analysis has long been a cornerstone of real-world applications across domains such as finance, healthcare, and energy. Prior to the rise of large language models (LLMs), research in this area predominantly focused on classic tasks such as forecasting and anomaly detection (Nie et al., 2022; Yang et al., 2023b). While tasks such as explainable time series and dependency analysis existed before the emergence of LLMs, they primarily relied on numerical data. The key shift with LLMs lies in their ability to incorporate rich contextual information beyond pure numerical representations (Wang et al., 2024b; Niu et al., 2024; Liu et al., 2024a; Aksu et al., 2024; Zhang et al., 2024b). Additionally, researchers continue exploring classic tasks leveraging LLMs to enhance these traditional approaches (Jin et al., 2023a;b). However, these efforts are often limited in scope, focusing narrowly on tasks like forecasting rather than advancing broader reasoning and inference capabilities based on extra contextual information (Jin et al., 2024; Zhou & Yu, 2024; Su et al., 2024).\nDeeper reasoning and contextual understanding in time series analysis are critical for identifying patterns, causal relationships, and subtle contextual dynamics (Hamilton, 2020; Fatemi & Gowda, 2024; Hu et al., 2024). These subtle contextual dynamics may include shifts in temporal dependencies, latent external influences, or evolving structural patterns that are not easily discernible through conventional numerical analysis. However, most current researches treat time series as purely numerical input, overlooking the inherently multimodal nature of real-world and time-dependent contexts (Zhang et al., 2025). In practice, time series are frequently accompanied by complementary data streams (e.g., text and images) that provide additional layers of information. Most existing systems do not fully exploit this multimodal richness, leaving a considerable gap in achieving robust reasoning for more complex time series tasks (Merrill et al., 2024; Wang et al., 2024c; Zhou et al., 2025).\nTo bridge this gap, we believe that it is crucial to develop the next generation of multimodal large language model (MLLM) frameworks that can integrate multiple sources of time-dependent data, thereby unlocking richer insights and more powerful decision-making abilities (Wang et al., 2024d). Given the growing need for advanced time series reasoning in real-world applications, we believe that time series reasoning with MLLMs can unlock more powerful and flexible inferences, support more informed decision-making, and drive tangible outcomes. We propose a framework that goes beyond traditional methods and addresses three key points: (1) A New Reasoning Paradigm \u2013 We define time series reasoning, highlight its essential components, and discuss both current and prospective architectures to enable deeper inference. (2) Beyond Traditional Tasks - We illustrate how time series reasoning, coupled with MLLMs, opens doors to novel tasks that go beyond the scope of classical tasks, demonstrating the broader real-world relevance. (3) Resources and Future Directions \u2013 We review existing resources, identify unresolved challenges among datasets, benchmarks, and evaluations, and emphasize the need for robust multimodal training strategies to further advance time series reasoning.\nContributions. The contributions of this work can be summarized in three aspects: (1) New Perspective on Time Series Reasoning \u2013 We move beyond traditional time series analysis tasks by emphasizing deeper inference and understanding. (2) A Multimodal Reasoning Framework \u2013 We propose a paradigm that integrates time-dependent data from various modalities, empowering MLLMs to derive richer insights and explanations. (3) Opportunities, Challenges, and Future Directions \u2013 We explore key research directions and technical challenges, propose solutions for advancing multimodal reasoning architectures, and highlight the importance of designing new datasets and evaluation methods to rigorously assess multimodal time series reasoning."}, {"title": "2. Time Series Reasoning", "content": "Time series reasoning refers to the open-ended ability of an MLLM to process and interpret time series with human-like logic. It captures temporal structures, trends, and patterns to generate precise and interpretable results across various time series tasks, delivering insights in clear and natural language. Unlike traditional time series methods focused on specific goals, time series reasoning unifies these tasks into an integrated framework. It combines context-awareness, time series characteristics, and advanced inference to provide deeper insights, enhanced interpretability, and the ability to handle complex tasks requiring external information beyond the time series itself. Please refer to the Appendix A for more definitions and references."}, {"title": "2.2. Types of Time Series Reasoning", "content": "The role of time series reasoning is to understand how sequence patterns change over time and to explain the mechanisms behind these changes for more informed decision-making. Numerous approaches exist to model temporal dependencies. However, for conducting time series reasoning, it is crucial to consider both how the reasoning process is structured and what the analysis aims to achieve. Therefore, we can naturally categorize time series reasoning based on two perspectives: Reasoning Structure and Task Objective.\nHere, we first formally define time series reasoning. Consider a univariate time series $x = (x_0, x_1,...,x_{T-1})$ of length $T$, where $x \\in \\mathbb{R}^{T}$. Let $M$ be an MLLM model that takes as input (i) the time series $x$ and (ii) a sequence of context tokens $c$, which may encode additional information such as domain knowledge or prompts. The model $M$ produces an output sequence $y$, which can include numeric tokens representing time series values, textual tokens providing explanations or descriptions of $x$, and tokens for other modalities. The model defines a probability distribution over the output sequence $y$, conditioned on the inputs $x$ and $c$, as $P_M(y \\mid x, c) = \\prod_{t=1}^{|y|} P_M(y_t \\mid y_1,..., y_{t-1}, x, c)$, where each token $y_t$ is predicted based on the preceding tokens $(y_1,..., y_{t-1})$ with time series $x$ and context $c$.\nA reasoning structure is a set of systematic steps that connects initial observations to final conclusions (Wei et al., 2022; Chu et al., 2023). In time series reasoning, it provides a transparent roadmap, showing how each inference is derived and how input factors interact to shape results. Reasoning structure can be categorized into four types. (1) End-to-end reasoning directly maps inputs to outputs, prioritizing efficiency over interpretability by skipping intermediate steps. (2) Forward reasoning adopts a bottom-up approach, building solutions step-by-step with explicit intermediate steps, making it suitable for tasks like math problems or trend prediction. (3) Backward reasoning uses a top-down approach, breaking problems into smaller sub-problems, often applied in diagnostic tasks to trace the causes of anomalies. Finally, (4) Forward-backward reasoning combines both approaches, employing forward reasoning to propose solutions and backward reasoning to validate or refine them, making it particularly useful for iterative tasks like anomaly detection. Detailed descriptions of the above are provided in the Appendix for further clarity.\nIn addition, these reasoning structures can be further organized into chain-, tree-, or graph-based formalisms to represent the reasoning path more explicitly. A chain-based approach arranges the reasoning steps in a sequential, linear fashion, making it straightforward to follow how each conclusion is derived. A tree-based approach expands reasoning into branches, allowing multiple concurrent paths and a hierarchical breakdown of complex problems. Meanwhile, a graph-based approach generalizes these connections and can accommodate interdependencies and cyclic references among different inference steps.\nTask Objective. In time series reasoning, the primary goal is to extract meaningful and actionable insights from temporal and other complementary multimodal data. Depending on the objective - such as explaining anomalies, forecasting, or identifying causal factors - different reasoning types can be applied. For instance, etiological reasoning identifies causes of sudden shifts, while deductive reasoning confirms hypotheses about periodicity. The task objective determines the appropriate reasoning type and its application."}, {"title": "2.3. Key Components for Achieving Time Series Reasoning", "content": "Robust time series reasoning often demands a comprehensive understanding of temporal patterns and the integration of relevant task-related contextual information. Accordingly, we propose four essential components for achieving effective time series reasoning: (1) Understanding Time Series Characteristics, (2) Contextual Guidance, (3) Reasoning Process, and (4) Iterative Feedback (illustrated in Figure 2).\nUnderstanding Time Series Characteristics. One essential aspect of time series reasoning is identifying patterns such as seasonality, trends, and abrupt fluctuations, which are vital for analyses and decision-making. In MLLMs, numerical time-series data are often converted into textual tokens, enabling integration with textual or multimodal inputs and leveraging the model's language capabilities. However, this can reduce the ability to recognize temporal patterns due to differences in how LLMs tokenize numerical data (Gruver et al., 2024). A key debate centers on using customized tokenization or encoding methods to represent numerical time-series data. Customized tokenization offers flexibility by aligning representation with the model architecture and integrating well with language tasks but may introduce repetitive symbols for numerical values. Encoding methods, while preserving local fluctuations and long-range dependencies, can disrupt the natural flow of language processing. Choosing between these approaches requires balancing interpretability, computational efficiency, and the preservation of temporal relationships. Addressing these challenges and improving methods remain critical areas for innovation.\nTo fully leverage multimodal integration in time-dependent data, it is crucial to understand correlated features across multimodal time series. In healthcare, electronic health records (EHR), wearable device data, and patient-reported outcomes often influence each other with time lags. For example, abnormalities in heart rate variability might precede changes in EHR metrics such as blood pressure by days, while self-reported fatigue might occur before alterations in wearable or EHR data. Capturing these lag effects is a key challenge and opportunity for MLLMs. Using mechanisms like temporal attention (Rosin & Radinsky, 2022) or dynamic temporal graph networks (Rossi et al., 2020), MLLMs can infer lagged relationships, enabling proactive decision-making and personalized insights across domains.\nContextual Guidance. Context plays a crucial role in guiding time series reasoning by providing additional knowledge for interpreting patterns and shaping forecasting outcomes. Time-series data rarely exists in isolation, and the same data can lead to vastly different predictions depending on the context (Requeima et al., 2024; Williams et al., 2024). This context may come from internal features like seasonality, trends, or anomalies, or external sources such as economic indicators, news events, or environmental factors. Since internal patterns often interact with external influences, incorporating contextual information could significantly improve data comprehension and decision-making. However, integrating external context presents challenges, as factors like policy changes, market shifts, or global events are often sporadic, unstructured, and difficult to quantify. Even when accessible, such data may be inconsistent or incomplete, potentially undermining analysis. Overcoming these obstacles is essential to advancing time series reasoning and enabling more informed decisions.\nReasoning Process. In Section 2.2, we introduced various approaches to time series reasoning, where the choice of method depends on the complexity and goals of the analysis. For instance, combining backward reasoning with etiological reasoning is effective for tracing the causes of anomalies, while forward reasoning with an inductive approach is better suited for identifying recurring seasonal patterns. Moreover, rather than focusing on a single task, time series reasoning aims to unify multiple objectives within an integrated framework, which introduces several challenges. First, maintaining consistent logic across reasoning steps is difficult, as earlier conclusions must remain valid when addressing subsequent goals. Second, incorporating external context without introducing spurious correlations can be complex. Lastly, as reasoning spans multiple tasks, articulating each inference step clearly becomes harder, especially when new information necessitates revising earlier conclusions.\nIterative Feedback. To tackle the above challenges in the reasoning process, iterative feedback and refinement become quite important. It allows the model to incrementally improve its reasoning by identifying inconsistencies, revising intermediate conclusions, and incorporating new information. This process can be facilitated through several methods: leveraging LLM agents to evaluate and critique reasoning steps, embedding self-evaluation mechanisms within the model to detect and resolve potential errors, or integrating a feedback loop directly into the model's architecture to allow it to adjust based on performance metrics."}, {"title": "2.4. Promising Model Design", "content": "There are generally four model design ideas for advanced reasoning tasks that either leverage the built-in reasoning capabilities of LLM/MLLMs, design a time-series MLLM, or fully utilize multimodal inputs and capabilities. We categorize these methods as Zero-Shot Inference, One-Stage Tuning-Based, Two-Stage Tuning-Based, and Multimodal Time Series approaches (illustrated in Figure 3).\nZero-Shot Inference. LLM/MLLMs inherently possess zero-shot reasoning abilities, enabling them to generate insights into temporal patterns through direct prompting using their built-in knowledge. Incorporating different types of reasoning structures (as discussed in Section 2.2) within prompts can enhance the quality of reasoning and interpretations. Additionally, the in-context learning approach \u2013 providing a small set of question-answer pairs with accompanying rationale \u2013 can further refine the model's temporal reasoning capabilities.\nOne-Stage Tuning-Based. A time-series MLLM is fine-tuned using a systematically compiled dataset of (instruction, response) pairs - often involving time-series data - to align model behavior with human objectives. This process, known as instruction tuning, mitigates issues such as untruthful, unhelpful, or unsafe responses by guiding the model to produce accurate and contextually relevant outputs. A key aspect of this strategy involves formulating clear instructions that capture user goals and providing detailed, precise answers. To enhance reasoning capability, the dataset's instructions may sometimes include different types of reasoning structures to guide the thinking process, and the corresponding responses may feature a detailed explanation of how the final answer was derived. The model parameters are updated under a supervised loss function, computed from the generated response tokens. This approach ensures the model can better generalize across diverse tasks as well as strengthen its reasoning and explanatory capabilities.\nTwo-Stage Tuning-Based. This approach begins by establishing an initial alignment between text and time-series modalities, followed by a supervised fine-tuning stage. In the first step, the model is trained to map textual descriptions to corresponding temporal attributes, ensuring a robust linkage between linguistic concepts and time-series features. Building on this foundation, the second step focuses on supervised fine-tuning, where the model is optimized for question-answering and reasoning tasks over the aligned modalities. By separating the alignment process from the fine-tuning phase, this two-stage approach aims to both equip the model with a strong multimodal representation of time-series data and facilitate contextually accurate, inference-driven responses (Xie et al., 2024).\nMultimodal Time Series. Multimodal time-series data include not only numerical sequences but also other temporal modalities. Tackling complex tasks that require robust reasoning demands a model design that integrates diverse modalities to enhance the capabilities of an MLLM. Addressing advanced reasoning capabilities in this domain involves three key components. First, a modality encoder transforms various raw inputs \u2013 such as numerical data, images, or audio - into meaningful embeddings. This step often leverages pre-trained models (e.g., CLIP for images) to efficiently capture domain-specific features (Radford et al., 2021). Second, a modality interface aligns these embeddings into a unified, text-like representation, ensuring seamless integration with the language-based reasoning engine. Finally, the LLM backbone serves as the central reasoning system, synthesizing the transformed inputs to perform advanced analysis and decision-making.\nOur position: Robust time series reasoning requires integrating external knowledge and incorporating iterative feedback. Future directions should refine data representation, integrate diverse modalities, enhance models, and develop self-correcting mechanisms to unify time series reasoning with MLLMs."}, {"title": "3. Beyond Classical Time Series Tasks", "content": "The integration of MLLMs and time series reasoning has inspired new tasks beyond traditional time series tasks (illustrated in Figure 4). These include question answering, causal inference & impact analysis, and time series generation & editing, which focus on reasoning and creative manipulation of time series. This section introduces these tasks, highlighting their distinct views and applications."}, {"title": "3.1. Question Answering", "content": "Time series-based Question Answering (QA) represents a shift from classical analysis to high-level reasoning, where a time series serves as the primary input, optionally enriched with multimodal data like text, images, or structured data for alignment (e.g., event timestamps) or context enrichment (e.g., supplementary details). The core of QA lies in answering open-ended questions posed by users based on input multimodal time series. Moreover, other modalities beyond the time series play an indispensable role in this novel task. This is particularly impactful in healthcare, where multimodal integration (e.g., clinical notes, imaging, wearable device data) facilitates holistic patient assessments, enabling real-time critical care insights or chronic disease trend analysis. Besides, integrating meteorological data with satellite imagery helps uncover patterns in extreme weather, while combining sensor data and maintenance logs to identify causes of system failures in industry. Leveraging MLLMs, this approach bridges raw time-series data and actionable insights by aligning inputs, modeling temporal dependencies, and interpreting patterns."}, {"title": "3.2. Causal Inference and Impact Analysis", "content": "Time series causal inference and impact analysis focus on uncovering causal relationships and quantifying the effects of specific events or interventions (Moraffah et al., 2021). This task often involves integrating time-series data with additional modalities, such as text or tabular data, which can provide alignment or supplementary context and uncover nuanced causal relationships that may not be apparent from time series alone. For instance, in finance, combining stock price sequences with company real-time financial reports enables analysis of how financial performance impacts market fluctuations (Kong et al., 2024a;b). Similarly, in healthcare, integrating electronic health records with external environmental data helps evaluate the causal effects of interventions, such as new drug treatments, on patient outcomes. Applications extend to marketing, where promotional timelines are analyzed alongside sales data to assess campaign effectiveness, and public policy, where the impacts of reforms on economic indicators like employment rates are quantified."}, {"title": "3.3. Time Series Generation and Editing", "content": "Time series generation and editing focus on synthesizing or modifying time series, with inputs optionally enriched by other modalities like text, images, or structured data to provide alignment or supplemental information (Narasimhan et al., 2024; Jing et al., 2024). This task is widely applicable in weather forecasting and urban management. For instance, generating synthetic weather data helps simulate extreme climate scenarios and explore the reasons behind extreme weather, while editing traffic flow data allows urban planners to assess the impact of infrastructure changes. Time series reasoning is fundamental to this process, ensuring that generated or edited data aligns with logical temporal dependencies and contextual relationships, such as maintaining seasonal weather patterns or capturing the cause-effect dynamics of urban systems. Multimodal inputs further enhance these tasks by providing additional context-satellite imagery that can guide the generation of weather time series, while map data or demographic statistics inform traffic simulations-ensuring the outputs are realistic and actionable. Moreover, we can leverage generation or editing techniques for improved time series imputation, utilizing conditions provided by information from other modalities. By incorporating reasoning from these modalities, we can achieve more realistic generation and imputation results.\nOur position: We advocate for continued innovation in this intersection, particularly in high-stakes domains like healthcare, finance, and industrial systems, where robust reasoning and multimodal integration are critical. Key challenges to address include data confidentiality, computational efficiency, and interpretability to ensure trustworthy and actionable outcomes. Future research should focus on domain-specific datasets, efficient training strategies, and evaluation metrics that assess both accuracy and reasoning quality, unlocking transformative capabilities across various sectors."}, {"title": "4. Resources and Challenges", "content": "Dataset and Benchmark. There is a notable shortage of publicly available datasets and codes in this area of research. To address this, we summarize several existing datasets in Table 2. However, several opportunities and challenges remain. Many datasets are artificially generated by GPT models or LLMs, and standard evaluation methods for these generated questions are lacking. Additionally, while most datasets pair numerical time series with textual descriptions, they often lack multimodal representations incorporating additional modalities, limiting their broader applicability. Furthermore, datasets that naturally merge time series into textual information are limited. Finally, existing datasets primarily focus on forward reasoning structure, such as chain-of-thought approaches, leaving opportunities to explore more diverse reasoning processes in future research.\nEvaluation Metrics. Reasoning is often intangible and highly subjective, making it relatively difficult to evaluate. Most existing research compares the outcomes of different LLMs and measures their accuracy, which is the approach commonly applied to multiple-choice or true/false questions (Chang et al., 2024). However, the methods for quantifying reasoning vary based on the specific tasks, datasets, and types of reasoning involved. For example, in QA tasks requiring inductive reasoning, answers are evaluated using RAGAS, a keyword-matching approach through LLM-based fuzzy matching (Es et al., 2023). To assess both forecast accuracy and the integration of contextual information, the Region of Interest CRPS (RCRPS) metric was introduced, which priorities context-sensitive windows in the prediction and accounts for constraint satisfaction (Williams et al., 2024). At present, there is no standard evaluation metric in the time series reasoning field. Future research should address this gap by designing task-specific metrics that can evaluate not only the accuracy of answers but also the underlying reasoning process.\nTraining Strategy. Current and potential model designs for trading strategies are detailed in Section 2.4. One potential area for improvement lies in the integration of explicit reasoning processes into the training phase. Currently, most approaches focus on including reasoning structure only within the question-and-answer pairs, without fully embedding reasoning mechanisms into the training process itself. This leaves open opportunities to explore whether the reasoning embedded in these pairs is optimal and how incorporating more detailed and high-quality reasoning could play a greater role in training. Such advancements could enhance model performance and decision-making capabilities, offering promising directions for future development in trading strategy models.\nOur position: Advancing time series reasoning requires progress across datasets, evaluation metrics, and training strategies. We need more realistic and multimodal benchmark datasets to move beyond artificial samples and support diverse reasoning structures. Specialized evaluation metrics should examine both outcomes and underlying reasoning processes. Integrating explicit reasoning into the training phase will strengthen model performance and decision-making, establishing a stronger foundation for interpretable time series reasoning systems."}, {"title": "5. Alternative Views", "content": "Is Single-Modality Data Sufficient to Advance Time-Series Reasoning in Real-World Applications? Single-modality numerical data can sometimes be sufficient - particularly in scenarios where the time series is extremely sparse and relies on simple, clear assumptions. However, this approach often fails to capture the rich contextual factors driving real-world phenomena. For instance, if we only observe a small set of yearly sales data for a product and neglect considerations like untapped markets or slowing innovation cycles, even the most sophisticated models are constrained by the narrow assumptions derived from these limited observations.\nIn contrast, LLMs excel at integrating diverse sources of information - including textual descriptions, background knowledge, and multimodal time-series data. By leveraging these varied inputs, LLMs can uncover deeper causal factors that go beyond the time series alone. This capability enables us to integrate richer contextual information and formulate more flexible assumptions. For example, consider how electric vehicles are shaped by shifting government incentives, rapid technological advancements, and evolving consumer attitudes. By factoring in these contextual elements, time-series analysis can yield more reliable predictions and insights. Hence, while single-modality data can suffice in tightly constrained scenarios, employing a multimodal approach and tapping into LLMs' broader reasoning capabilities enables richer, more accurate time-series analysis for complex, real-world applications.\nCould LLM/MLLMs Truly Contribute to Time Series Reasoning? While LLMs show promise in time series analysis, they have inherent limitations stemming from their design. As text-based sequence predictors, LLMs may lack an intrinsic understanding of mathematical logic, numerical precision, and temporal dynamics. Although they can mimic patterns from training data, they cannot perform true calculations or provide rigorous domain-specific temporal reasoning. Consequently, it is necessary to integrate explicit reasoning mechanisms - such as causal, analogical, and counterfactual reasoning - into MLLMs, as discussed in Sections 2.3 - 2.4.\nAnother concern is whether real-world time series might be inadvertently included in LLM pretraining datasets - an issue heightened by the opacity of training sources. Although textual-numerical pairs appear in specialized domains, multimodal time-series data are usually absent from standard pretraining corpora. Moreover, real-world deployments often rely on proprietary, domain-specific datasets (e.g., healthcare vitals with clinical annotations), which differ significantly from generic pretraining data. This specificity reduces the likelihood of overlap with undisclosed LLM training sets. However, given the lack of transparency in pretraining, practitioners cannot rule out overlaps entirely, emphasizing the need for rigorous evaluation frameworks for trustworthiness (see Section 4)."}, {"title": "6. Further Discussion", "content": "Hallucination. It is a persistent issue in MLLMs, leading to inaccurate results in time series analysis, especially in critical fields like finance and healthcare. Incorporating reasoning mechanisms into MLLMs offers a solution by enabling the model to understand causal relationships and context, allowing it to cross-check and validate outputs. This reasoning process can also flag and correct hallucinations, ensuring more reliable and accurate analyses.\nEnvironmental and Computational Cost. The integration of MLLMs into time series analysis may introduce challenges related to scalability and computational complexity. Critics highlight the environmental and computational costs, as training and deploying these models require substantial resources, particularly when handling large-scale, high-precision numerical data and in domains like finance and science. Addressing these challenges necessitates strategies to alleviate computational burdens, such as optimizing MLLMs, refining time-series data processing pipelines, and developing efficient alignment and inference mechanisms to enhance scalability while reducing overhead.\nData Confidentiality and Operational Constraints. MLLMs face challenges in time series analysis, including data confidentiality, as sensitive data like financial transactions or patient records cannot be shared with external services. Real-time forecasting, such as in wind power management, demands low latency (e.g., under 20 seconds for five-minute-ahead predictions). Additionally, cloud-based MLLMs also struggle in remote areas due to connectivity issues. Local deployment of open-source models ensures data control, real-time processing, and offline operation. Future research should focus on developing high-quality, accessible open-source MLLMs for time series analysis."}, {"title": "7. Conclusion", "content": "This position paper highlights the potential of time series reasoning with MLLMs for both researchers and practitioners. Our central position is that MLLMs can deliver more powerful and flexible reasoning capabilities for time series analysis, thereby improving decision-making and practical applications. To strengthen our position, we propose novel time series reasoning paradigms and introduce new task frameworks that leverage MLLMs to tackle complex temporal challenges. Despite current limitations - such as scarce datasets and the need for more sophisticated evaluation metrics - the integration of MLLMs with time series reasoning represents a noteworthy advancement. Looking ahead, we encourage researchers to focus on developing innovative architectures, refining training methodologies, and establishing comprehensive benchmarks to unlock MLLMs' potential in real-world time series contexts."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning, especially MLLM-based time series analysis and reasoning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "Appendix", "sections": [{"title": "A. Literature Review", "sections": [{"title": "A.1. MLLM-based Time Series Analysis", "sections": [{"title": "A.1.1. MULTIMODAL LARGE LANGUAGE MODEL DEFINITION", "content": "A Multimodal Large Language Model (MLLM) is an advanced AI system that extends the reasoning capabilities of Large Language Models (LLMs) by enabling them to process, interpret, and generate information across multiple modalities, including text, images, audio, and time-series data (Wu et al., 2023). Unlike traditional LLMs that rely solely on textual data, MLLMs integrate multimodal representations through sophisticated deep-learning architectures, allowing them to perceive and reason about complex relationships between different data types. This capability enhances their performance in tasks such as multimodal question answering, image and video captioning, and medical image analysis, where understanding information from multiple sources is essential (Yin et al., 2023). By leveraging advanced fusion mechanisms, MLLMs generate contextually rich and coherent outputs that go beyond text-based reasoning, making them highly effective in applications requiring comprehensive multimodal understanding (Zhang et al., 2024a)."}, {"title": "A.1.2. TIME SERIES DEFINITION", "content": "A time series refers to a collection of data points organized in chronological order, representing the progression of one or more variables over time. Formally, a univariate time series is denoted as $x = (x_0,x_1,...,x_{T-1}) \\in \\mathbb{R}^{T}$, where $T$ is the total number of time steps, and each $x_t \\in \\mathbb{R}$ represents the value of the series at time $t$. This structure captures the temporal evolution of a single variable. In contrast, a multivariate time series extends this definition to multiple dimensions and is represented as $X = (X_0, X_1,...,X_{T-1}) \\in \\mathbb{R}^{T \\times D}$. Here, $D$ represents the number of features, so each $x_t \\in \\mathbb{R}^{D}$ is a vector containing $D$ values at time $t$, reflecting the simultaneous evolution of multiple interrelated variables. Time series is fundamental in numerous domains, including finance, healthcare, and environmental monitoring, due to its ability to capture temporal dependencies and trends."}, {"title": "A.1.3. MULTIMODAL TIME SERIES", "content": "The increasing availability of heterogeneous data has highlighted the need to better capture the complexity of real-world phenomena. Multimodal time series address this challenge by integrating data from multiple modalities, where each modality represents a distinct type of information, such as images, text, audio, or structured numerical data. By extending traditional single-modal time series analysis, this approach enables the incorporation of diverse and complementary data sources, providing a more comprehensive understanding of complex systems. Formally, a multimodal time series can be represented as $X = \\{X^{(m)}\\}_{m=1}^{M}$, where $M$ denotes the total number of modalities, and each $X^{(m)}$ corresponds to the time series for modality $m$. For instance, $X^{(m)} = (x_0^{(m)}, x_1^{(m)},...,x_{T-1}^{(m)})$ represents the sequential data of modality $m$ over $T$ time steps, with $x_t^{(m)}$ varying in form depending on the modality, such as vectors for numerical data, matrices for images, or sequences for text and audio. Multimodal time series analysis enables a more comprehensive understanding of temporal patterns and interactions across modalities by integrating multiple data sources. This is particularly important in applications such as healthcare, autonomous driving, and multimedia analysis (Moor et al., 2023; Cui et al., 2024; Zhou et al., 2024)."}, {"title": "A.1.4. TIME SERIES CLASSICAL TASKS", "content": "Time series analysis and its various classical tasks are widely applied across real-world domains, such as financial forecasting, healthcare monitoring, traffic flow analysis, climate modeling, industrial predictive maintenance, and AIOps (Shumway et al., 2000; Nie et al., 2024; Yang et al., 2021a; 2023a; Liang et al., 2024). Also, time series analysis encompasses a diverse set of tasks aimed at extracting insights and addressing challenges in temporal data (Hamilton, 2020; Kirchg\u00e4ssner et al., 2012). Among them, common tasks include forecasting, which predicts future values based on historical trends and can be divided into short-term and long-term predictions (Wen et al., 2022), and anomaly detection, which identifies unusual patterns or deviations from expected behavior (Zamanzadeh Darban et al., 2024). Imputation addresses missing or corrupted data points to ensure dataset completeness (Du et al., 2024), while generation creates synthetic time series to replicate statistical properties for data augmentation or scenario simulation (Yang et al., 2024). Other tasks include classification, which assigns categorical labels based on patterns, and regression, which predicts continuous target values (Mohammadi Foumani et al., 2024; Yang et al., 2021b). In recent years, an increasing number of approaches have explored leveraging multimodal data to enhance classical time series analysis tasks, validating the effectiveness and rationality of these methods (Zhou et al., 2023; Liu et al., 2024b; Gruver et al., 2024; Chang et al., 2023; Cao et al., 2023; Yin et al., 2023). For instance, Time-LLM aligns and reprograms LLMs for time series forecasting through textual input alignment (Jin et al., 2023a), while Time-MMD incorporates additional textual data with Transformer-based models using weighted fusion to perform time series forecasting and potentially other tasks (Liu et al., 2024b). Beyond text modalities, medical data employs supplementary image or tabular data to enhance time series analysis using foundation models (Hollmann et al.,"}]}, {"title": "A.2. Reasoning in NLP and Time Series", "sections": [{"title": "A.2.1. REASONING IN NLP", "content": "In the field of Natural Language Processing (NLP), reasoning refers to the process of deriving conclusions from textual evidence and logical principles (Besta et al., 2025). It involves tasks such as understanding implicit information, performing logical inferences, and applying commonsense knowledge. Reasoning capabilities are crucial for addressing complex language tasks like natural language inference, multi-hop question answering, and commonsense reasoning (Yu et al., 2024). Types of reasoning include Chain-of-Thought (CoT), which breaks problems into intermediate steps for clarity, deductive reasoning which applies general rules to specific cases, and inductive reasoning which generalizes from observations (Xia et al., 2024). Abductive reasoning identifies the most plausible explanations, while analogical reasoning transfers knowledge based on similarities (Shi et al., 2024; Lewis & Mitchell, 2024). Others include commonsense reasoning, probabilistic reasoning, and causal reasoning (Yu et al., 2024). These approaches enhance the interpretability and performance of NLP systems."}, {"title": "A.2.2. REASONING IN TIME SERIES", "content": "Time series analysis tasks traditionally focus on narrower objectives \u2014 like forecasting or anomaly detection \u2014 each addressed by its own specialized model, often relying solely on numerical patterns within the data. In contrast, time series reasoning with logic integrates multiple tasks under a single, context-aware framework with human-like reasoning (Chow et al., 2024). It readily incorporates domain knowledge and external data sources, providing natural language explanations and causal insights rather than mere numerical outputs (Potosnak et al., 2024). This approach allows time series reasoning to adapt to shifting conditions and novel questions, delving into the \u201cwhy\u201d behind observed patterns and bridging the gap between automated analysis and real-world decision-making.\nFurthermore, reasoning in NLP"}]}]}]}]}