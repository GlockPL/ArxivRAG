{"title": "OCTCube: A 3D foundation model for optical coherence tomography that improves cross-dataset, cross-disease, cross-device and cross-modality analysis.", "authors": ["Zixuan Liu", "Hanwen Xu", "Addie Woicik", "Linda G. Shapiro", "Marian Blazes", "Yue Wu", "Cecilia S. Lee", "Aaron Y. Lee", "Sheng Wang"], "abstract": "Optical coherence tomography (OCT) has become critical for diagnosing retinal diseases as it enables 3D images of the retina and optic nerve. OCT acquisition is fast, non-invasive, affordable, and scalable. Due to its broad applicability, massive numbers of OCT images have been accumulated in routine exams, making it possible to train large-scale foundation models that can generalize to various diagnostic tasks using OCT images. Nevertheless, existing foundation models for OCT only consider 2D image slices, overlooking the rich 3D structure. Here, we present OCTCube, a 3D foundation model pre-trained on 26,605 3D OCT volumes encompassing 1.62 million 2D OCT images. OCTCube is developed based on 3D masked autoencoders and exploits FlashAttention to reduce the larger GPU memory usage caused by modeling 3D volumes. OCTCube outperforms 2D models when predicting 8 retinal diseases in both inductive and cross-dataset settings, indicating that utilizing the 3D structure in the model instead of 2D data results in significant improvement. OCTCube further shows superior performance on cross-device prediction and when predicting systemic diseases, such as diabetes and hypertension, further demonstrating its strong generalizability. Finally, we propose a contrastive-self-supervised-learning-based OCT-IR pre-training framework (COIP) for cross-modality analysis on OCT and infrared retinal (IR) images, where the OCT volumes are embedded using OCTCube. We demonstrate that COIP enables accurate alignment between OCT and IR en face images. Collectively, OCTCube, a 3D OCT foundation model, demonstrates significantly better performance against 2D models on 27 out of 29 tasks and comparable performance on the other two tasks, paving the way for AI-based retinal disease diagnosis.", "sections": [{"title": "Introduction", "content": "Optical coherence tomography (OCT) is a non-invasive imaging technique that enables 3D volumetric imaging of the microstructure of the retina.\u00b9 Because OCT images allow clinicians to see the distinctive layers of retina and quantify the thickness of these layers, OCT imaging has become critical for the diagnostic assessments and treatments of many retinal diseases,\u00b2 including glaucoma, diabetic macular edema, age-related macular degeneration, referable diabetic retinopathy, retinal neovascularization, as well as macular hole, central retinal artery, and vein occlusion. 10,11 Manual interpretation and analysis of each OCT image in a volume is not only time-consuming but also prone to human error, necessitating the development of automated algorithms and machine learning models to enhance accuracy and efficiency in clinical practice.12\nMeanwhile, since OCT images are being rapidly accumulated in routine clinical practice, it is possible to develop machine learning models to automate disease diagnosis from OCT images. In particular, image processing techniques, such as contrast-limited adaptive histogram equalization (CLAHE), 13 speckle noise reduction, 14 layer delineation, 15 and layer segmentation, 16,17 and deep learning approaches, such as convolutional neural networks, 5,18,19 generative adversarial networks20 and vision transformers, 21 have been exploited to develop supervised learning models for OCT-based diagnosis. Recently, foundation models have achieved state-of-the-art performance on various biomedical applications, especially on modeling biomedical images. 22-24 Foundation models exploit self-supervised learning to learn high-quality representations using large-scale unannotated images and then use these representations to train classifiers for various downstream applications using limited annotations. Recently, RETFound25 has been proposed as a retinal foundation model, which is trained on 1.6 million 2D retinal images including 736,442 2D OCT images (foveal and OCT volume center slice B-scans) and achieves state-of-the-art performance in the prediction of retinal diseases.26 In addition, this foundation model was able to predict several systemic diseases such as myocardial infarction and ischemic strokes. 26 However, despite their encouraging performance, existing approaches including RETFound focus on 2D OCT images, overlooking the 3D structure of an OCT volume. Modeling the 3D volume has shown to be more effective than 2D images in other medical imaging modalities,28\u201331 such as CT and MRI, by capturing continuous spatial patterns across nearby slices. The opportunity is particularly substantial for OCT imaging as the diseased macular areas might expand across the 3-dimensional spatial region across the fovea.32,33 Thus, modeling 3D OCT structure provides both front forward en face and depth information, holding the promise to not only improve retinal disease diagnosis and prognosis but also a wide variety of systemic diseases."}, {"title": "Results", "content": "Nevertheless, it remains unclear how to computationally model the 3D volume, as simply aggregating predictions slice-by-slice could lead to suboptimal results.34\nHere, we present OCTCube, the first 3D foundation model for OCT imaging. OCTCube is trained on 26,605 OCT volumes containing 1.62 million 2D OCT images. The key idea of OCTCube is to model the 3D structure holistically rather than aggregating the information from each 2D slice. Specifically, OCTCube is developed based on 3D masked autoencoders (MAE), 35-37 where a large proportion of 3D cubes are randomly masked from the OCT volume and then the model is trained to recover these cubes using an encoder-decoder structure (Fig. 1a). The encoder is used to initialize specific models for downstream tasks while the decoder is discarded and replaced by the task-specific decoders. In contrast to 2D MAE\u00b3\u201d for 2D OCT patches, 25 a 3D OCT volume introduces a much longer sequence of cubes, thus presenting much larger GPU memory consumption.38 To address this problem, we exploit FashAttention39,40 to reduce the computational cost, enabling us to efficiently train OCTCube using large-scale 3D volumes. We hypothesize that OCTCube is a broadly applicable and generalizable foundation model for various datasets, diseases and devices by effectively modeling 3D OCT volumes.\nWe evaluated OCTCube on cross-dataset, cross-disease, cross-modality and cross-device setting to validate its prediction accuracy and generalizability. In prediction of retinal diseases, OCTCube improved average AUPRC from 0.77 to 0.81 in the inductive setting and from 0.66 to 0.77 in the cross-dataset setting compared to 2D model RETFound. On a more challenging cross-device setting, OCTCube significantly outperformed 2D models in analyzing OCT volumes acquired with Topcon Maestro 2 devices while the pre-training data were acquired using Heidelberg Spectralis devices.41 Next, we found that OCTCube is also able to accurately predict seven different systemic diseases, including diabetes and hypertension. Finally, we extended OCTCube for a cross-modality analysis by developing a novel contrastive self-supervised learning framework named COIP that jointly aligns OCT and IR images, where OCT volumes are embedded using OCTCube. We found that OCTCube achieved better cross-modal retrieval results, serving as the foundation for multi-modal 3D retina modeling. Collectively, we present OCTCube, a 3D foundation model for optical coherence tomography, and demonstrate significant improvement over 2D models on 27 out of 29 tasks (Fig. 1b, Supplementary Fig. 1), paving the way for AI-based retinal disease diagnosis and prognosis."}, {"title": "Overview of OCTCube", "content": "OCTCube is a foundation model pre-trained using 26,605 3D OCT volumes. The key idea of OCTCube is to model the 3D structure holistically instead of aggregating the features from each"}, {"title": "Proof-of-concept evaluation of 3D modeling over 2D", "content": "2D slice (Fig. 1a). To achieve this, OCTCube exploits 3D masked autoencoders as the pre-training framework. In particular, OCTCube first splits an OCT volume into small 3D cubes. It then randomly masks 90% of cubes and uses an encoder-decoder architecture to reconstruct these masked cubes. By training this encoder-decoder architecture using 26,605 OCT volumes, OCTCube is able to obtain a high-quality encoder that can derive accurate representations for a new OCT volume. The decoder will be discarded for downstream applications while the parameters of the encoder will be updated for each downstream task according to the task-specific annotations, such as disease labels. We hypothesize that OCTCube is a broadly applicable and generalizable foundation model for various datasets, diseases and devices by effectively modeling 3D OCT volumes. We evaluate OCTCube on 29 tasks spanning retinal disease prediction, cross-dataset prediction, systemic disease prediction, cross-device prediction and cross-modality analysis (Fig. 1b, Supplementary Fig. 1). OCTCube attains the significantly best performance on 27 out of 29 tasks and comparable performance on the other two tasks.\nAs a proof-of-concept to show the benefits of 3D modeling over 2D modeling, we calculated the similarity between two slices in the same OCT volume. We found that nearby slices are more similar in terms of root mean square error (RMSE) (Supplementary Fig. 2a) and structural similarity index measure (SSIM) (Supplementary Fig. 2b), indicating the presence of repetitive patterns between nearby slices, suggesting the opportunities to exploit slices around the center slice to enhance the signal-to-noise ratio. To verify this, we used RETFound to assign a disease probability of Age-related Macular Degeneration (AMD) for each slice. We then aggregated the predictions from k slices around the center slice by averaging their predicted probabilities (Supplementary Fig. 3c). When k is 0, this aggregation approach is the same as a 2D model that only considers the center slice. We found that the best AUROC and the best AUPRC were achieved when k was 14 and 10 respectively, indicating that considering more slices can boost the prediction performance (Fig. 1c). We observed similar patterns on seven other retinal diseases (Supplementary Fig. 4). Moreover, because RETFound is pre-trained and fine-tuned only using center OCT slices, when all slices are considered (k=60), the prediction performance could be worse than when only considering center slices. Furthermore, the efficacy of a 3D model that simply averages the predicted probability of all slices is compromised, as many diseases develop and progress at different locations and rates. This variability highlights the need for a more advanced 3D approach that can adapt to different disease areas.\nTo further support this conclusion, we present case studies of two AMD patients (Fig. 1d,e, Supplementary Fig. 5a,b) who were both predicted incorrectly as non-AMD patients by RETFound using the center slice (slice 30). In contrast, OCTCube used other nearby slices to"}, {"title": "Retinal disease prediction in the inductive learning setting", "content": "highlighted as the source of prediction by the corresponding saliency maps (Fig. 1d,e, Supplementary Figs. 5c,d, 6-7).\nWe first evaluated OCTCube on the prediction of eight retinal diseases, including Primary Open-Angle Glaucoma (POAG), Diabetic Macular Edema (DME), Age-related Macular Degeneration (AMD), Epiretinal Membrane or Macular Hole (ERM/MH), Diabetic Retinography without Macular Edema (DR), Central Artery / Vein Retinal Occlusion (CRAO/CRVO), Posterior Vitreous Detachment (PVD) and Retinal Neovascularization (RNV). We exploited 5-fold cross validation settings with 20% data (Fig. 2a-b) and 80% data (Supplementary Figs. 8-9) to fine-tune our pretrained model for each disease. We set the pre-training patient collection, the fine-tuning patient collection, and the test patient collection to be independent from each other to ensure a practical inductive learning setting. We first compared OCTCube to supervised models that did not leverage unannotated images for pre-training and found that OCTCube substantially outperformed them with an average 53.8% AUPRC and 45.4% AUROC relative improvement, indicating the importance of pre-training. RETFound, which is pre-trained on 2D images, also outperformed supervised models, reemphasizing the effectiveness of pre-training. Next, we assessed the advantage of considering the entire 3D volumes by comparing OCTCube to RetFound (center), which uses the embedding of the center slice, and RETFound (all), which averages the embeddings of slices in the same volume (Supplementary Fig. 2a,b). We found that OCTCube significantly outperformed both variants of RETFound on 7 out of the 8 diseases (paired t-test p-value < 1e-3). Since our method and RETFound are both developed based on MAE, the superior performance of our method demonstrates the effectiveness of modeling the 3D volume. The improvement of OCTCube is more prominent on POAG among the eight diseases. Because POAG diagnosis relies on measuring the change of layer thickness throughout the entire macula, requiring the global context of the full volume at the same time to determine the diagnosis, 42 this larger improvement further reveals OCTCube's ability to leverage retinal patterns beyond the fovea. Interestingly, the supervised (all) model did not outperform the supervised (center) model, suggesting that the benefit of 3D modeling mostly arose from the pre-training stage. In conclusion, our experiments on eight retinal disease predictions in the inductive learning setting demonstrate the effectiveness of OCTCube by leveraging the 3D structure of OCT volumes.\nTo further understand the improvement of OCTCube, we visualized the saliency maps of an AMD patient on multiple slices around the center slice, reflecting the image regions that the model used to make the prediction (Fig. 2c-f). We first found that both OCTCube and RETFound capture relevant clinicopathologic features such as the drusen to predict AMD."}, {"title": "Cross-dataset and cross-device prediction", "content": "Nevertheless, OCTCube utilized consistent regions across slices, while RETFound used relatively different regions across slices, suggesting that OCTCube considers the 3D spatial fundus structure. We further visualized the saliency map of the OCT volume across the slow-scan dimension (Supplementary Fig. 10), and again observed clinically relevant saliency maps generated by OCTCube at the level of the retinal pigment epithelium in the perifoveal region. In contrast, RETFound demonstrated less meaningful saliency maps in this dimension partially due to its pre-training only using the slow-scan and depth dimension, indicating the broad applicability of OCTCube to analyze OCT images from different dimensions.\nThe superior performance on OCTCube motivates us to benchmark it on independent datasets that neither RETFound nor OCTCube has access to in the pre-training stage, enabling us to more rigorously examine the impact of the 3D model under potential population variance and batch effects. We evaluated different methods on five independent datasets (Fig. 3a-b, Supplementary Fig. 11). Similar to our observation on the UW Ophthalmology dataset, we found that OCTCube outperformed 2D models with a large margin (16.8% AUPRC improvement). RETFound (all) also outperformed Retfound (center), highlighting the effectiveness of considering the entire 3D volume. Importantly, our method still outperformed RETFound (all) by 8.5% AUPRC and 6.9% AUROC improvement, demonstrating the effectiveness of 3D pre-training. Collectively, OCTCube significantly outperformed both variants of RETFound on 4 out of the 5 datasets, demonstrating that OCTCube could be a broadly applicable tool for retinal disease prediction.\nSince independent datasets might have varying numbers of annotated data, we next investigated the performance of OCTCube under different ratios of training data (Fig. 3c,d). We found that while OCTCube achieved the best performance on different training data ratios, its improvement was larger when there are fewer annotations, indicating its superiority in the low-resource setting. For example, RETFound (center) achieves 0.86 AUROC that is comparable to OCTCube (0.87 AUROC) using 70% of training data, but its performance drops substantially to 0.61 AUROC when only using 20% of training data, whereas OCTCube still maintains a good AUROC of 0.77.\nFinally, we examined a more challenging task of cross-device prediction (Fig. 3e,f), where the test data at the fine-tuning stage are collected from Topcon Maestro 2 devices\u2074\u00b9 in the AI-READI dataset.43 RETFound was trained on Topcon 3D OCT-2000 SA devices and Topcon Triton devices, 41 while OCTCube was trained solely on the more difference Heidelberg Spectralis devices.45 A few examples of images acquired by these different devices are illustrated in Supplementary Fig. 12, showing clear visual distinctions between the same OCT volume from"}, {"title": "Prediction on systemic diseases", "content": "different devices. Even though RETFound was trained on Topcon images and OCTCube was not, we found that OCTCube significantly outperformed RETFound (center) by 29.3% AUPRC and RETFound (all) by 4.3% AUPRC on the Topcon test images, demonstrating its strong generalizability across datasets and across devices.\nAfter confirming the performance of OCTCube on predicting retinal diseases, we next investigated whether OCTCube can be used to predict systemic diseases related to retinal structure by exploiting OCT volumes. We selected 7 diseases based on International Statistical Classification of Diseases (ICD-946 and ICD-1047,48) codes that are frequently found among people with retinal conditions. Among them, hypertension was diagnosed in 3,440 patients, occurring in 35% of the patients with disease records included in our study, diabetes was diagnosed in 2,420 patients, occuring in 24.7% of the included patients. We found that OCTCube significantly outperformed RETFound on 5 out of 7 disease labels in terms of AUPRC (Fig. 4a) and 7 out of 7 in terms of AUROC (Fig. 4b) on predicting concurrent systemic diseases, demonstrating the advantage of modeling 3D OCT volumes for predicting systemic diseases.\nTo further understand how OCTCube successfully predicts systemic diseases, we examined the performance of the aggregation approach (Supplementary Fig. 3c) that averages the prediction scores over multiple slices (Fig. 4c, Supplementary Fig. 13). We found that aggregating more slices can improve the prediction performance on all seven diseases, necessitating the modeling of 3D structures. Moreover, the improvement of OCTCube over RETFound (center) and the improvement of the aggregation approach over RETFound (center) are highly correlated (Pearson correlation 0.86 for AUROC and 0.78 for AUPRC), indicating that both methods leverage similar 3D patterns to enhance the prediction performance (Fig. 4d,e). Nevertheless, OCTCube consistently outperformed the aggregation approach under various numbers of slices, demonstrating its effectiveness in modeling 3D structures.\nSince OCTCube achieved the largest improvement on predicting diabetes diagnosis, we have included a case study of a patient with diabetes to understand how OCTCube leverages OCT volumes for diabetes prediction (Fig. 4f-h). We observed many hard exudates, indicators of early diabetic retinopathy, present in various OCT scans of the right eye (Fig. 4f). However, hard exudates were not present in the center slice (slice 30). As a result, OCTCube was able to predict diabetic retinopathy but RETFound could not based on the OCT scans of the right eye. In contrast, a larger burden of hard exudates were found in the OCT scans of the left eye (Fig. 4g, Supplementary Fig. 14) as well as in both eyes during the 1 year follow up visit (Fig. 4h, Supplementary Fig. 15). The hard exudates are more severe in the left eye (OS) on the same acquired date and in the right eye acquired during the one-year follow-up visit, therefore both"}, {"title": "Cross-modality analysis of OCT and IR images through contrastive self-supervised learning", "content": "OCTCube and RETFound correctly predicted the diagnosis of diabetes in these instances. In conclusion, RETFound can only make correct predictions when patterns are visible and present on the center slice, whereas OCTCube is able to correctly predict diabetes when patterns are less visible on a single slice, leading to potential early disease detection.\nFinally, we investigated whether OCTCube as an OCT-based foundation model can be integrated with Infrared Retinal (IR) imaging modality. IR is usually taken together with the OCT imaging as the navigation map and provides a much larger 2D en face field of vision (FOV) of multiple fundus components, 45 such as optic disc, vessel direction, and the projected structure of macula. In practice, it is challenging and tedious to jointly analyze OCT volumes and IR pairs even for human experts. Therefore, we sought to investigate whether OCTCube could help learn a shared embedding space by aligning OCT and IR images. To this end, we collected 26,605 IR images for the same patient population and studied two retrieval tasks: find the most relevant OCT volume given an IR image and vice versa. Since 3D MAE pre-training does not directly provide cross-modal representation, we propose COIP, a contrastive self-supervised learning-based OCT-IR pre-training framework, to align these two modalities in a joint embedding space (Fig. 5a,b, see Methods). We found that OCTCube is able to retrieve relevant images across modalities more accurately with a 0.64 Recall@1, substantially higher than the 0.46 Recall@1 attained by RETFound, indicating the effectiveness of OCTCube on aligning OCT and IR images. RETFound (all) also outperformed RETFound (center), again demonstrating the benefits of modeling OCT data in the 3D space for multi-modal analysis (Fig. 5c,d, Supplementary Fig. 16a,b). Next, we studied the retrieval performance on AI-READI where the alignment of both RETFound and OCTCube were fine-tuned using the UW Ophthalmology dataset. OCTCube again outperformed RETFound (all) and RETFound (center) on both retrieving OCT to IR and IR to OCT, and the improvement was much larger on this cross-dataset setting, indicating its wide applicability to cross-modality analysis. Furthermore, we used laterality to evaluate this cross-modality alignment by examining whether OCTCube can retrieve images with the same laterality (right (OD) or left (OS) eye). OCTCube achieved the best performance on both IR to OCT and OCT to IR laterality prediction with an average 0.97 accuracy@1 (Fig. 5e,f, Supplementary Fig. 16c,d). Finally, we selected four case studies from the UW Ophthalmology (Fig. 5g,h) and AI-READI (Fig. 5i,j) datasets and found that the improvement of OCTCube came from successfully matching fundus structure between modalities. For example, OCTCube retrieved similar IR images for the OCT volume that is optic-disc centered (Fig. 5g), while both RETFound (all) and RETFound (center) failed to retrieve optic-disc centered IR images. Additional examples showed that OCTCube better understands macular changes and laterality (Fig. 5h), illumination difference (Fig. 5g,h) and"}, {"title": "Discussion", "content": "blood vessel structure (Fig. 5g,i,j), demonstrating its ability to capture spatial fundus structures through 3D modeling.\nOCTCube is related to previous efforts in computational ophthalmology, especially previous works that develop deep learning-based approaches for diagnosing and predicting retinal diseases from OCT images. In particular, previous studies have used deep convolutional neural networks,5,7,18,34 generative adversarial networks,10,20 multimodal feature learning and anomalies detection\u00b2 to predict diabetic retinopathy, diabetic macular edema, age-related macular degeneration and glaucoma. In contrast to these approaches, OCTCube has been developed as a foundation model using self-supervised learning, which consists of a pre-training stage and a fine-tuning stage. As a result, unlabeled images can be utilized by OCTCube to provide accurate initialization for the supervised fine-tuning stage. OCTCube is inspired by RetFound, the first foundation model for OCT images. In contrast to RetFound, OCTCube directly modeled the 3D structure of OCT volumes. Our extensive experiments show that OCTCube outperforms RetFound in various settings, demonstrating the importance of 3D volumes.\nThere are a few limitations we would like to address for OCTCube in future work. First, we hope to explore how to jointly train multi-modal4,52\u201354 3D foundation models for additional types of retinal images beyond OCT and IR, such as fundus autofluorescence (FAF),55 color fundus photography (CFP) and Fluorescein Angiography (FA)\u201d images. By jointly modeling all of the imaging modalities, especially in the 3D space, we could more comprehensively characterize the retina, thus deriving more robust performance for downstream applications. However, it remains unclear how to computationally integrate all of them given the computational cost and unmatched modality structures. Second, despite being equipped with the interpretable method, OCTCube could be more clinically useful if we are able to identify the most important cubes that contribute to the final disease predictions and filter out less important cubes to improve efficiency. We plan to explore advanced interpretable methods, such as SHAP58 and RELPROP,59 to identify such cubes and enable better interpretation and efficacy in the future. Third, a retinal patient could have multiple visits, presenting a longitude data containing multiple 3D volumes. We plan to incorporate such temporal information into OCTCube by extending it to 4D space where time is the fourth dimension. 60,61 While incorporating temporal information into medical imaging foundation models has not been well studied before, we plan to use more computationally efficient neural network architectures to allow the model to consider a time-series of volumes at the same time."}, {"title": "Methods", "content": "We have developed OCTCube, a 3D foundation model for optical coherence tomography trained from 26,605 3D OCT volumes spanning 1,622,905 2D OCT images. We demonstrate the advantage of modeling 3D volumes holistically instead of pre-training on 2D image sets by comparing OCTCube to 2D OCT foundation models on multiple independent datasets. We found that OCTCube outperformed comparison approaches consistently in all 29 tasks with significant improvement on 27 tasks (Fig. 1b, Supplementary Fig. 1), including retinal disease prediction, cross-dataset prediction, cross-device prediction, systemic disease prediction and cross-modality retrieval, indicating its accurate performance and strong generalizability. The strong predictive performance of OCTCube on both retinal diseases and systemic diseases indicates its potentially broad applicability. OCTCube may be used as a general tool for analyzing OCT data, paving the path for AI-based retinal diagnostic and prognostic. 50,51\nDetails of UW Ophthalmology dataset\nDataset overview. The UW Ophthalmology dataset contains 3D macula OCT volumes, paired Infrared Retinal images from the medical screening process of 17,214 patients, along with the diagnosis codes (ICD-9 and ICD-10 code) across the UW-medicine system. In this study, we utilize this dataset for pre-training, within-dataset ophthalmic disease prediction, systemic cross-disease prediction, and cross-modality prediction. We only include the first screening of each patient, to avoid distribution shifts brought by the longitudinal follow-up screening results. We include data samples from both eyes of a patient when available. This results in 33,262 3D OCT volume and IR image pairs. Each macula OCT volume and its paired IR images were extracted by the Heidelberg Spectralis (Heidelberg Engineering) imaging device45 from 2006 to 2023. The IR images are not used in the pre-training stage and only used for the cross-modality analysis. Each OCT volume is a composition of either 60 or 61 slices, and the default digital resolution of each slice is 496 by 768. The absolute pixel spacing varies for each dimension and each instance, and the average pixel spacing area is 7.51 by 1.88 by 8.66 mm. The paired IR image is taken together with the OCT volume acquisition, and the typical digital resolution is 768 by 768. This study was approved by the Institutional Review Board of the University of Washington (UW) and was in adherence with the tenets of the Declaration of Helsinki and the Health Insurance Portability and Accountability Act.\nData split. We split the dataset at patient level, with a ratio of 80% training and 20% test for the within-dataset evaluation. This results in 26,605 training and 6657 test sample volumes from 13,771 and 3,443 patients. For all the conducted experiments, the held out test set is only used for evaluation. In the pre-training stage, only the 3D macula OCT volumes are included and associated disease labels for these volumes are not seen by the model. In the within-dataset retinal and systemic cross-disease prediction, the OCT volumes are used to predict diagnosis information provided as label supervision. In the cross-modality prediction, both the OCT volumes and IR images are provided to perform alignment training and evaluation. Among all 17,214 patients, 12,830 patients had clinical records available. We thus use this subset to construct the data for the within-dataset retinal and systemic cross-disease prediction task.\nLabels for retinal disease prediction. We picked 8 retinal diseases based on ICD code (Supplementary Fig. 17, Supplementary Table 1): Primary Open-Angle Glaucoma (POAG, 1,248 patients), Diabetic Macular Edema (DME, 881 patients), Age-related Macular Degeneration (AMD, 1,888 patients),, Diabetic Retinopathy without Macular Edema (DR, 1,694 patients), Epiretinal Membrane or Macular Hole (ERM/MH, 1,482 patients), Central Retinal Artery or Vein Occlusion (CRAO/CRVO, 204 patients), Posterior Vitreous Detachment (PVD,"}, {"title": "Details of OCTCube", "content": "1,725 patients), Retinal Neovascularization (RNV, 299 patients). We then set up the rest of the recorded patients as the non-diseased cohort (3,936 patients). We also included patients with multiple retinal diseases, resulting in a multi-label and imbalanced dataset. We followed the train-test set split in the pre-training stage to hold out the test samples that are not seen in the pre-training stage for evaluation. For all the tasks being evaluated, we further split the training subset into train and validation subsets as 75% to 25%, resulting in a data split of 60% training, 20% validation, 20% test split.\nLabels for systemic cross-disease prediction. We aimed to examine if OCTCube is able to predict other systemic diseases using OCT volumes. We extracted the ICD code of systemic diseases for the recorded patients and counted their frequencies. We merged ICD-9 code to ICD-10 code and used the first level code of the ICD-10 code system. We then extracted all the level 1 disease codes with frequency larger than 100, resulting in 455 different diseases over 9,801 patients. The disease distribution can be seen at (Supplementary Fig. 18). We used these samples to construct the multi-label systemic cross-disease dataset. We followed the same train and validation split as we did in preparing data for retinal disease prediction, ensuring samples in the held out test set are never seen in the pre-training stage.\nDetailed overview of OCTCube. In this section we present the detailed architecture, design, and training of the OCTCube. OCTCube is based on attention-based Vision Transformer, which treats OCT volumes as a long sequence of continuous cube feature vectors and learns to generate summarized representations from the sequence based on stacked multi-head attention and non-linear transformations. It is composed of a heavy transformer encoder and a lightweight transformer decoder, and is trained with 3D masked autoencoder objectives.35 The encoder first indexes the spatial position of each cube, and then randomly selects most of the cube tokens and masks them out from the long sequences, and processes the rest feature sequence together with their indexed position information. After getting the output from the encoder, the lightweight decoder will insert a learnable embedding vector named <mask> as the placeholder token for each masked-out cube at its original indexed position and try to reconstruct the whole sequence. The reconstruction is guided by minimizing the mean squared error (MSE), thus not requiring label supervision at all. Prior works35-37 have shown that this process will lead to a good encoder for downstream tasks. After the training, the decoder will be discarded and the encoder is taken as the basic encoder for the downstream tasks.\nDecomposing the volume into the sequence of 3D cubes. Unlike a plain 2D Vision Transformer, 38 the OCTCube is a designated 3D-aware Vision Transformer model that can take 3D OCT volumes or 2D OCT slices with arbitrary sizes. To achieve this, OCTCube utilizes"}, {"title": "Details of cross modality alignment", "content": "CubeEmbed, a non-overlapped 3D convolution layer that can split the OCT volumes into small cubes and project each cube to an embedding token vector. Let (z, h, w) be the cube size pre-chosen in the CubeEmbed layer, applying this operation to the whole 3D OCT volume with Z slices and (H, W) resolution will result in a long sequence of cube embedding with length of (Z * H * W) / (z * h * w). It is thus crucial to pick an appropriate cube size in order to both maintain flexibility and avoid heavy computation. For the OCT modality, we propose to use a smaller size on the 3rd z-axis and a larger window size on the 2D slices for three reasons. First, depending on the scanning mode, the z-axis might have a much sparser sampling region; a smaller z will lead to similar pixel spacing to the other two dimensions. Second, a smaller z will provide flexibility to handle volumes with less number of slices. To the extreme, 2D slices can be treated as the volume with z=1, and since the slices will commonly be converted to a duplicated 3-channel rgb image, we set the cube size to be 3 in OCTCube. This makes OCTCube capable of taking both 2D slices and 3D volumes as inputs, improving its flexibility on downstream applications. We also make a 2D positional embedding for the h and w axis and a separate positional embedding for the 3rd z-axis, to help the model adapt to variant volume size. What's more, setting z=3 has another important benefit to accelerate the learning, which is being easy to utilize pre-trained 2D checkpoints to warm up the 3D-aware model, with simple conversion from PatchEmbed to CubeEmbed. This allows OCTCube to quickly adapt from 2D to 3D with much less computation effort.\nIncorporating FlashAttention into OCTCube. One big drawback of 3D modeling in medical imaging is the drastically increased computational cost. Compared to the pure convolutional neural network, the plain vision transformer suffers more on the increased sequence length, as the multi-head attention induces O(L\u00b2) space and time complexity, where L is the sequence length. As an example, an OCT volume with resize resolution 48 x 256 x 256 with cube size 3 x 16 x16 leads to a sequence with length of 4,096, and fine-tuning a ViT-large encoder with a linear projection head with this single sequence will used up more than 50 GB GPU memory, which exceeds the memory of most of the GPUs. To relieve this issue and make OCTCube more efficient and affordable to end users, we incorporate FlashAttention-239,40 into the Vision Transformer structure. FlashAttention-2 is an advanced technology that helps to reduce the GPU memory costs by 5~20 times and enable 2~4x training / inference speed for transformer structure without accuracy lost, by optimizing the computation of attention. Because FlashAttention-2 reduces the space complexity of attention from O(L\u00b2) to O(L), including FlashAttention-2 reduces the GPU memory usage of the above example to 10.52 GB, which is more affordable on modern GPUs. Moreover, the computing speed also achieves at"}]}