{"title": "Bridging Domain Gaps between Pretrained Multimodal Models and Recommendations", "authors": ["Wenyu Zhang", "Xinming Zhang", "Jie Luo", "Yuan Fang"], "abstract": "With the explosive growth of multimodal content online, pre-trained visual-language models have shown great potential for multimodal recommendation. However, while these models achieve decent performance when applied in a frozen manner, surprisingly, due to significant domain gaps (e.g., feature distribution discrepancy and task objective misalignment) between pre-training and personalized recommendation, adopting a joint training approach instead leads to performance worse than baseline. Existing approaches either rely on simple feature extraction or require computationally expensive full model fine-tuning, struggling to balance effectiveness and efficiency. To tackle these challenges, we propose Parameter-efficient Tuning for Multimodal Recommendation (PTMRec), a novel framework that bridges the domain gap between pre-trained models and recommendation systems through a knowledge-guided dual-stage parameter-efficient training strategy. In the first stage, we leverage features extracted from pre-trained vision-language models to facilitate multimodal recommendation training, which enables the recommendation model to adapt to the representation space of pre-trained features while allowing ID embeddings to capture personalized user-item matching patterns. This knowledge is then leveraged in the second stage to guide parameter-efficient fine-tuning of the pre-trained model, thereby achieving effective domain adaptation while maintaining computational efficiency. This framework not only eliminates the need for costly additional pre-training but also flexibly accommodates various parameter-efficient tuning methods. Experiments on three public datasets demonstrate that PTMRec significantly improves recommendation performance (average 10.6% gain in Recall@10) by training only x% of parameters compared to the original CLIP model. Our work provides a practical and general framework for addressing domain adaptation challenges in multimodal recommendation.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender systems play a crucial role in personalized services [33], with traditional ID-based methods [9, 13, 20] achieving significant success. However, as online platforms evolve [30], the prevalence of multimodal data challenges conventional approaches [34], driving the development of multimodal recommendation systems [1, 5, 7, 26, 27]. Recent advances in multimodal recommendation systems have spawned various approaches utilizing pre-trained models. From the perspective of leveraging pre-trained models, we categorize existing work into three paradigms: (1) Pre-extracted, (2) Pre-train/Fine-tuning, and (3) Pre-extracted with Parameter-Efficient Fine-tuning (PEFT).\nFirst, the pre-extracted paradigm uses pre-trained encoders to extract features offline, which, as shown in Fig. 1a, are used to initialize the feature embedding layer of recommendation models. VBPR [8] pioneered the introduction of pre-trained CNN features, MMGCN [25] enhanced feature interaction through modality-specific graph structures, and Freedom [35] improved performance via contrastive learning and cross-modal attention. However this paradigm mainly relies on early lightweight models (such as VGG-16 [21] and Sentence-BERT [19]), and the static nature of features limits the potential of recommendation models.\nSecond, as shown in Fig. 1b, the pre-training/fine-tuning paradigm employs pre-trained models such as BERT[3], ViT [4], and CLIP [18] as modal encoders and fine-tunes them during the recommendation training process [28]. One line of work directly fine-tunes pre-trained models as modal encoders (e.g., MoRec [32]). Another line of work addresses domain transfer through source domain pre-training and target domain fine-tuning (e.g., TransRec [22] and PMMRec [16]). Notably, AdapterRec [6] incorporates parameter-efficient adapters [10] during target domain fine-tuning to improve efficiency. Direct fine-tuning methods inevitably require substantial computational resources [29], while domain transfer approaches demand significant resources particularly in the source domain pre-training phase.\nThird, the pre-extracted with PEFT paradigm adapts to recommendation scenarios through parameter-efficient methods such as prompts [15], adapters [10], and LoRA [11]. As shown in Fig 1c, this paradigm primarily enhances pre-extracted features by introducing lightweight trainable modules. For instance, PromptMM [24] employs trainable prompt tokens and knowledge distillation to bridge modality content and collaborative signals, while MISSRec [23] adopts multimodal adapters to enhance sequence representation. Furthermore, if PEFT methods can be effectively utilized to transfer powerful pre-trained models to the recommendation domain, it would significantly enhance the performance of recommendation models. However, our experiments show that directly applying PEFT methods for joint training of pre-trained encoders and recommender systems leads to performance degradation (detailed results in Section 3.3 Ablation study), indicating that the substantial domain gap between pre-trained models and recommender systems cannot be effectively bridged through traditional PEFT methods.\nIn summary, directly applying pre-trained models in existing recommender systems still faces key challenges: while pre-extraction and PEFT-enhanced pre-extraction methods are computationally efficient, they struggle to fully unleash the representational potential of pre-trained models; the pre-train/fine-tuning paradigm shows superior performance but comes with enormous computational costs. The core dilemma lies in: how to construct a parameter-efficient paradigm that enables joint training of pre-trained models and recommender systems while maintaining efficiency?\nTo address these limitations, we propose a Parameter-efficient Tuning for Multimodal Recommendation framework. As illustrated in Fig. 1d, PTMRec introduces an innovative knowledge-guided dual-stage parameter-efficient training strategy that effectively bridges the domain gap between pre-trained models and recommendation systems while maintaining computational efficiency. In the first stage (Personalized Preference Learning), we train a lightweight recommendation model with learnable parameters using pre-trained features to capture task-specific knowledge about user preferences and item characteristics. In the second stage (Knowledge-guided Prompt Learning), we guide the tuning of pre-trained models through knowledge transfer optimization using personalized preference knowledge. This two-stage design not only eliminates the need for expensive additional pre-training but also provides a flexible framework that can accommodate various parameter-efficient tuning methods while maintaining the coupling between feature extraction and recommendation objectives."}, {"title": "2 METHOD", "content": "2.1 Preliminaries\nLet $U = \\{U_1, U_2,\\dots\\}$ denote the user set and $I = \\{I_1, I_2, \\dots\\}$ denote the item set. For each user u, their historical interactions form an item set $I_u$ representing positive feedback. Each item i contains visual and textual modalities ($X^v$ and $X^t$) besides ID information. For representations, we use embeddings: $e_{uid} \\in \\mathbb{R}^d$ and $e_{iid} \\in \\mathbb{R}^d$ for user and item IDs respectively, where d is the embedding dimension. Item multimodal information is represented by $e^v_i$ and $e^t_i$ from visual and textual features.\nFor optimization, recommendation systems typically employ BPR loss:\n\n$L_{BPR} = \\sum_{(u,i,j) \\in D} \\ln \\sigma(f_u(i) - f_u(j))$ (1)\n\nwhere $\\sigma(\\cdot)$ is the sigmoid function, $(u, i, j)$ represents a training triplet with user u, interacted item i, and non-interacted item j. Vision-language pre-training typically adopts InfoNCE loss for image-text alignment:\n\n$L_{NCE} = \\frac{1}{N} \\sum_{i=1}^N -\\log \\frac{\\exp(\\text{sim}(v_i, t_i)/\\tau)}{\\sum_{j=1}^N \\exp(\\text{sim}(v_i, t_j)/\\tau)}$ (2)\n\nwhere $v_i$ and $t_i$ are visual and textual features, $\\text{sim}(\\cdot,\\cdot)$ is cosine similarity, and $\\tau$ is temperature. These objectives reflect different goals: BPR focuses on learning personalized preferences through user-item interactions, while InfoNCE aims for general vision-language alignment.\n2.2 Personalized Preference Learning\nAs shown in Figure 2, in the first stage of our framework, we aim to learn basic recommendation patterns while preserving the general semantic knowledge from pre-trained models. Specifically, we utilize a frozen CLIP model for multimodal feature extraction to initialize and train the recommendation model.\nFor each item i, we first extract its visual and textual features using the pre-trained CLIP model, where $X_i^v$ and $X_i^t$ denote the raw image and text description of item i respectively. The image encoding divides the image $X_i^v$ into M fixed-size blocks, which are projected as image block embeddings $E^v_i = [e_{i1}^v, e_{i2}^v,\\dots, e_{iM}^v]$. A learnable class (CLS) token $c_i$ is initialized and input into the image encoder together with the image block embeddings. The image representation $e^v_i$ is obtained by mapping the class token $c_i'$ from the last Transformer layer through VisualProj:\n\n$[c_i', E_i'] = \\text{VisualEnc}([c_i, E^v_i])$. (3)\n$e^v_i = \\text{VisualProj}(c_i')$. (4)\n\nThe CLIP text encoder uses a tokenizer to project the text into token $E_i^t = [e_{i1}^t, e_{i2}^t, \\dots, e_{iN}^t]$. This is input into the text encoder TextEnc. The final text representation $e^t_i$ projects the last token output from the last Transformer layer through TextProj into the general multimodal embedding space.\n\n$E_i^{t'} = \\text{TextEnc}(E_i^t)$. (5)\n$e^t_i = \\text{TextProj}(e_{iN}^{t'})$. (6)"}, {"title": "2.3 Knowledge-guided Prompt Learning", "content": "Building upon the basic recommendation patterns learned in the first stage, we design a knowledge-guided parameter-efficient tuning strategy to enhance model performance. Our framework is flexible and can incorporate various parameter-efficient methods (e.g., LoRA, Adapter, or other methods). Through extensive experimental (Table 2) analysis balancing computational efficiency and model performance, we find that prompt learning is the most suitable choice for recommendation scenarios. Therefore, we focus on achieving efficient domain adaptation by introducing learnable prompts into CLIP encoders.\nAs shown in Fig. 2, we follow the design of MaPLe [12] and insert visual and textual prompts, denoted as $p^m = <p_1^m, p_2^m,\\dots, p_i^m>$, into the first i layers of the modality encoder, where $m\\in \\{t,v\\}$. Different from MaPLe, we remove the prompt coupling function for modality alignment since recommendation systems focus more on personalized perception.\n\n$[c_l', E_l^v, _] = \\text{VisualEnc}_l([c_l, E_l^v, p_l^v])$. $l = 1, \\dots, i$ (7)\n$[c_{l+1}', E_{l+1}^v, p_{l+1}^v] = \\text{VisualEnc}_l([c_{l+1}, E_l^v, p_l^v])$. $l = i+1, \\dots, L$ (8)\n\nFor the text encoder, it select the last token as the text representation. We insert $p^t$ before the text tokens.\n\n$[_, E_l^{t'}] = \\text{TextEnc}_l([p_l^t, E_l^t])$. $l = 1, \\dots, i$ (9)\n$[p_{l+1}^t, E_l^{t'}] = \\text{TextEnc}_l([p_l^t, E_l^t])$. $l = i+1, \\dots, L$ (10)\n\nIn the first i layers of the modality encoder, the modality prompt is only referenced within the current layer, and a new prompt is introduced for each layer. All prompts are initialized using the standard random initialization method. After the i-th transformer layer, the subsequent layers process the output of the previous layer."}, {"title": "2.4 Knowledge Transfer Optimization", "content": "To bridge the domain gap between pre-trained models and recommendation models, we propose an optimization strategy based on in-batch knowledge transfer. Specifically, in each training batch, we construct positive user-item pairs from interaction records while treating other items within the batch as negative samples. The user-item ID interaction distributions learned in the first stage contain core recommendation patterns. Through in-batch knowledge transfer, we effectively transfer these patterns to modal feature learning without additional computational overhead. The target distribution is defined as:\n\n$probs_{id} = \\text{stopgrad}(\\text{softmax}(e_u^T \\cdot e_{iid}))$, (11)\n\nwhere $e_u^T \\cdot e_{iid}$ computes similarity scores between user and item features, which are normalized into probability distributions through softmax. The stopgrad operation prevents gradients from flowing through ID features, thus maintaining a stable target distribution and avoiding representation collapse [2].\nFor modal features, we compute their interaction probability distributions as follows:\n\n$prob^m = \\log_\\text{softmax}(e_u^T \\cdot \\text{Linear}(e_i^m))$, where $m\\in \\{t, v\\}$, (12)\n\nwhere we obtain dimension-aligned feature representations by applying linear transformation to modal features $e_i^m$. Then, we minimize KL divergence to guide modal features in learning user-item interaction patterns. The specific computation is as follows:\n\n$L_{KT} = KL(prob^v, probs_{id}) + KL(prob^t, probs_{id})$. (13)"}, {"title": "3 EXPERIMENT", "content": "3.1 Experiment setting\n3.1.1 Baseline. To validate our framework's effectiveness, we conduct comparative experiments with two categories of representative models: traditional recommendation models including BPR (UAI'09) [20], LightGCN (SIGIR'20) [9], BUIR (SIGIR'21) [14], and multimodal recommendation models including VBPR (AAAI'16) [8], Freedom (MM'23) [35], MGCN (MM'23) [31], SMORE (WSDM'25) [17], comparing their performance both in original form and after integration with our framework.\n3.1.2 Implementation details. We selected three categories from the Amazon review dataset: Baby, Sports, and Clothing. All data is preprocessed through the MMRec framework [34] and image is download from link. Consistent with existing models, We sets\nthe embedding dimensions for users and items to 64 and employs the Adam optimizer with a learning rate of 0.001. To ensure model convergence during training, the number of epochs is set to 1000, and an early stopping strategy. The batch size for the first stage is set to 2048 (128 for the second stage with gradient accumulation steps of 12). The model evaluation was conducted on an NVIDIA Tesla V100 32 GB GPU.\n3.2 Performance Comparison\nAs shown in Tab. 1, we compared conventional recommendation methods with popular multimodal recommendation methods across three datasets. Consistent with existing multimodal recommendation studies, we adopted Recall@K and NDCG@K (K=10,20) as evaluation metrics. The PTM indicates that our framework was utilized in the model. The results reveal that significant improvements were achieved when the multimodal model was incorporated into our framework. The simpler the model, the greater the performance enhancement. Furthermore, the enhanced performance indicates that our framework not only improves the overall performance of the models but also narrows the performance gap between them. The experimental results validate our claim that our method can effectively facilitate the migration of basic models to recommendation domains with substantial domain gaps, solely with a limited number of prompts.\n3.3 Model Analysis\n3.3.1 Analysis of PEFT Methods. To select an appropriate Parameter-Efficient Fine-Tuning (PEFT) method, we conducted comparative experiments on model performance, computational efficiency, and memory consumption across three datasets. As shown in Table 2, Prompt Tuning demonstrates significant advantages in memory usage and computational efficiency while maintaining outstanding performance, making it our chosen implementation approach. The experimental results also verify that our framework can be flexibly integrated with different PEFT methods. (Note: Sports and Clothing datasets have more parameters than the Baby dataset due to their larger item counts.)\n3.3.2 Ablation Study. To validate the contribution of each component, we conducted ablation studies based on the Freedom model across three datasets. Specifically, we tested variants including CLIP feature initialization (CLIP-frozen), joint training with CLIP pre-prompt (CLIP-prompt), two-stage training without knowledge transfer (PTMRec w/o loss), and the complete framework (PTMRec). As shown in Fig. 3: (1) while CLIP-extracted modality features can enhance model performance, joint training with pre-prompt leads to performance degradation, indicating that simple prompts struggle to bridge the domain gap between recommendation and image-text matching; (2) although the two-stage training strategy partially alleviates the domain gap, optimizing prompts to obtain recommendation-suitable multimodal features remains challenging without domain knowledge guidance; (3) the complete framework, combining training decoupling and knowledge transfer loss, effectively mitigates domain differences, enabling the pre-trained model to transfer to domains with significant gaps."}, {"title": "3.4 Visualization", "content": "We visualized feature distributions on the Amazon Sports dataset using t-SNE and Gaussian kernel density estimation (see Fig. 4). The results show that: the original Freedom model's features exhibit high clustering but significant dispersion, with dramatic fluctuations in density curves indicating poor feature alignment; after introducing the CLIP encoder, the feature distribution becomes smoother with enhanced local cohesion; furthermore, the complete PTMRec framework strengthens both local cohesion and multimodal alignment of features, leading to tighter clustering of similar items' features and thus improved recommendation performance."}, {"title": "4 CONCLUSION", "content": "This paper proposes PTMRec, a framework addressing the high training costs and domain gaps when integrating pretrained multimodal models into recommendation systems. Based on the CLIP modality encoder, we employ parameter-efficient tuning to reduce training expenses, and enhance user preference perception through a two-stage training strategy with knowledge transfer regularization."}]}