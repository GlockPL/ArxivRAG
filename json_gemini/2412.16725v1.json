{"title": "Argumentation Computation with Large Language Models : A Benchmark Study", "authors": ["Zhaoqun Li", "Xiaotong Fang", "Chen Chen", "Mengze Li", "Beishui Liao"], "abstract": "In recent years, large language models (LLMs) have made significant advancements in neuro-symbolic computing. However, the combination of LLM with argumentation computation remains an underexplored domain, despite its considerable potential for real-world applications requiring defeasible reasoning. In this paper, we aim to investigate the capability of LLMs in determining the extensions of various abstract argumentation semantics. To achieve this, we develop and curate a benchmark comprising diverse abstract argumentation frameworks, accompanied by detailed explanations of algorithms for computing extensions. Subsequently, we fine-tune LLMs on the proposed benchmark, focusing on two fundamental extension-solving tasks. As a comparative baseline, LLMs are evaluated using a chain-of-thought approach, where they struggle to accurately compute semantics. In the experiments, we demonstrate that the process explanation plays a crucial role in semantics computation learning. Models trained with explanations show superior generalization accuracy compared to those trained solely with question-answer pairs. Furthermore, by leveraging the self-explanation capabilities of LLMs, our approach provides detailed illustrations that mitigate the lack of transparency typically associated with neural networks. Our findings contribute to the broader understanding of LLMs' potential in argumentation computation, offering promising avenues for further research in this domain.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have significantly improved their proficiency in addressing a range of symbolic computation tasks and complex reasoning problems, thereby demonstrating their adaptability across diverse domains [Fang et al., 2024; Huang et al., 2022; Alon et al., 2022; Pan et al., 2023]. Nevertheless, LLMs exhibit comparatively limited efficacy in formal logical reasoning tasks [Han et al., 2022; Eisape et al., 2024], frequently resulting in hallucinations and derivation errors. Integrating LLMs with symbolic methods remains a pivotal challenge in the advancement of contemporary artificial intelligence systems.\nArgumentation is a form of defeasible reasoning that plays a crucial role in various real-world applications, such as legal decision-making [Prakken and Sartor, 2023], policy compliance analysis [Dong et al., 2022], and ethical decision-making [Awad et al., 2020]. Combining argumentation with deep learning and gaining insights into how humans engage in argumentative reasoning is a vital aspect of artificial intelligence development. Previous research has pioneered this area by employing neural networks to compute acceptability semantics within the abstract argumentation framework (AAF) [Dung, 1995]. Notable contributions include AGNN and EGNN [Craandijk and Bex, 2020; Craandijk and Bex, 2022], which explores the use of graph neural network (GNN) to determine argumentation semantics, demonstrating that neural networks can effectively learn and predict the acceptability of arguments in various scenarios. These works highlight the promising intersection of neural networks and argumentation theory, however, the intrinsic black-box characteristic of neural networks leads to the lack of explainability in prediction. It is hard to show explicitly how their models predict the acceptability as their basis for decision-making, which is crucial in real applications.\nIn this work, we focus on exploring computational problems within the abstract argumentation framework, with an emphasis on leveraging LLMs to advance research in this domain. Our objectives are twofold: first, to train an LLM capable of both computing semantics and explaining the underlying algorithm process; second, to enhance the explainability of LLMs through the training, potentially overcoming some of their inherent limitations. To achieve these goals, we utilize the self-explanation capacity of LLMs to fine-tune language models on a curated data corpus. This approach enables LLM to interpret its behavior during the problem-solving process through text generation. Specifically, we establish an explainable AAF computation benchmark containing frameworks of varying difficulty levels. We explore two fundamental tasks: computing the grounded extensions and complete extensions based on labelling algorithms. In the dataset, each sample comprises an AAF described using a graph description language, with the execution information of a recursive algorithm and justification process attached as the explanation. The data organization embeds a post-"}, {"title": "2 Preliminaries", "content": "In the context of AAF, arguments are considered atomic entities devoid of internal structure and only a binary attack relation is considered, allowing for a highly adaptable approach to argumentation. The acceptability semantics of each model within the framework is determined based on the interactions among arguments.\nDefinition 1. An abstract argumentation framework F is a tuple F = (A,R) where A is a set of arguments and R is a relation $R \\subseteq A \\times A$.\nFor two arguments a, b \u2208 A, the relation (a, b) \u2208 R signifies that argument a attacks argument b. For a set S \u2286 A and an argument a, we say that S attacks a if there exists b \u2208 S such that (b, a) \u2208 R; similarly, a is said to attack S if there exists b \u2208 S such that (a, b) \u2208 R. Argument a is defended by set S if and only if for every b \u2208 A such that (b, a) \u2208 R there exists c\u2208 S such that c attacks b. We further state that S attacks a set P if there exist a \u2208 S and b\u2208 P such that a attacks b.\nAbstract argumentation frameworks can be concisely represented by directed graphs, where arguments are nodes and\nedges represent the attack relation. We use the AAF in Figure 1 (denoted as Example 1) as our illustration example in this paper. The status of a given argument is determined through argumentation semantics, often yielding results in the form of extensions [Dung, 1995] or labellings [Wu et al., 2010], which can be used interchangeably. In this work, we primarily use labellings, and the explanation of computation also involves concepts such as conflict-freeness.\nDefinition 2. A labelling L for an abstract argumentation framework F = (A,R) is a function $L:A \\rightarrow \\{IN, OUT, UNDEC\\}$.\nA labelling L assigns to each argument a \u2208 A one of the following values: IN, OUT, or UNDEC, indicating that the argument is accepted, rejected, or undecided, respectively. Let IN(L) = {a | L(a) = IN} and define OUT(L) and UNDEC(L) analogously. The set IN(L) for a labelling L under a semantics \u03c3 is a \u03c3-extension.\nDefinition 3. Let L be a labelling for F = (A, R) and a, b \u2208 R.\n\u2022 a is legally IN iff a is labelled IN and every b that attacks a is labelled OUT.\n\u2022 a is legally OUT iff a is labelled OUT and there is at least one b that attacks a and b is labelled IN.\n\u2022 a is legally UNDEC iff a is labelled UNDEC and not every b that attacks a is labelled OUT, and there is no b that attacks a such that b is labelled IN.\nThere are various semantics. In this work, we consider the complete, grounded, preferred, and stable semantics, denoted by com, grd, prf, and stb, respectively.\nDefinition 4. Let L be a labelling, we call L\n\u2022 an admissible labelling iff in L no arguments are illegally IN and no arguments are illegally OUT;\n\u2022 a complete labelling iff L is an admissible labelling without arguments that are illegally UNDEC;"}, {"title": "3 Dataset Construction", "content": "To construct a benchmark with algorithm explanations, we first generate randomly a variety of challenging argumentation frameworks following the methodology described in [Craandijk and Bex, 2020]. In generation process, duplicate samples are removed and the ground truth semantics are computed. The number of arguments in an AAF ranges from 6 to 25. To encompass various levels of difficulty, we generate 3,000 samples for the training set and 100 samples for the test set for each number of arguments n, resulting in a total of 60,000 training samples and 2,000 test samples. The grounded, preferred, stable, and complete semantics have an average of 1.0, 1.4, 1.1, and 2.4 extensions per AAF, respectively, with 4.3, 6.4, 7.6, and 6.0 arguments per extension, respectively.\nDesigned for LLM supervised fine-tuning, each data sample follows an instruction-tuning data template that includes four components: instruction, problem, explanation, and answer. During training, the instruction and problem serve as input text, while the explanation and answer constitute the output text. Figures 2 and 3 illustrate the specific data contents for solving the grounded extension and the complete extensions of Example 1, respectively. We define two basic tasks: computing the grounded semantics and the complete semantics. It should be noted that the complete labellings are computed under the assumption that the grounded labelling is already known. In practical applications, the computation involves a two-step process.\nAfter predicting all complete labellings, preferred and stable labellings can be straightforwardly inferred. In preferred labellings, the IN(L) sets are maximal, and stable labellings are complete labellings in which no argument is UNDEC."}, {"title": "3.1 Instruction and Problem", "content": "The instruction includes a basic introduction of AAF, the task description, and return format requirements. Since we are conducting formal reasoning, we ask LLMs to generate a JSON object at the end for clarity and concreteness. To render the abstract representation readable for both humans and LLMs, we adopt graph description languages to represent the original argumentation frameworks. We use three graph description languages: GraphML [Brandes et al., 2002], Graphviz DOT, and JSON. For each data sample, one format is randomly chosen during the problem statement generation."}, {"title": "3.2 Process Explanation", "content": "For the computation of extensions, we detail the process of executing labelling algorithms to identify the correct labellings of AAF semantics. The explanation includes a step-by-step description of how we solve the problem, reflecting the principles of argumentative computation."}, {"title": "Grounded Semantics", "content": "The labelling algorithm for generating the grounded labelling begins by assigning IN to all arguments that are not attacked. It then proceeds iteratively: OUT is assigned to any argument attacked by an argument that has just been made IN, and IN is"}, {"title": "\u2022 a grounded labelling iff there there does not exist a complete labelling L' such that $IN(L') \\subset IN(L)$;", "content": ""}, {"title": "\u2022 a preferred labelling iff there does not exist a complete labelling L' such that $IN(L') \\supset IN(L)$;", "content": ""}, {"title": "\u2022 a stable labelling iff $UNDEC(L) = 0$.", "content": "Grounded labelling always exists and is unique. In contrast, other semantics may exhibit dependency on the specific structure of the argumentation framework. We show all possible complete labellings of Example 1 in Table 1."}, {"title": "Complete Semantics", "content": "By definition, complete extensions are supersets of the grounded extension. Based on the grounded extension, an algorithm to compute all complete extensions involves changing the labels of some arguments in UNDEC(Lgrd) to IN or OUT. In this paper, we regard the computation of complete semantics as a two-step reasoning task that first computes the grounded semantics. Then, instead of using a forward algorithm, we allow the LLM to directly predict each complete labelling L. After that, we verify that the predicted labelling is a complete labelling by Definition 3.\nIn the explanation, we provide all ground truth complete labellings and allow the LLM to implicitly learn how to compute them based on the grounded labelling. This annotation approach enables the LLM to learn to infer each complete extension and then verify it."}, {"title": "3.3 Answer", "content": "In the answer, we give the final output labellings in a JSON object, which contains the sets IN(L), OUT(L), and UNDEC(L). The arguments in the sets are sorted by their number to avoid ambiguity."}, {"title": "3.4 Enhancing Data Diversity", "content": "To increase the data diversity and mitigate overfitting, we employ GPT-40 [OpenAI, 2023] to re-illustrate the entire text for both training and testing samples. As a necessary requirement, we maintain the formal part in each sample to ensure the validity and consistency of the data."}, {"title": "4 Chain-of-Thought Baseline", "content": "Prompting generally results in models making predictions with lower accuracy compared to models that have been fine-tuned with substantial training data [Zhang et al., 2023]. To validate this observation and establish a baseline method, we design a Chain-of-Thought (CoT) pipeline [Wei et al., 2022] that computes extensions based on a labelling algorithm. The argumentation computation is divided into multiple steps, framed as a reasoning task.\nFor computing grounded semantics, LLMs are taught to identify arguments and their relations, starting with those that aren't attacked. They follow steps to label arguments until no more changes are needed. As for complete Semantics, the models take arguments that are labelled UNDEC and decide which can be further add to set IN to form a complete extension, checking if their choices make sense. The concrete prompts and implementation steps can be found in Appendix in supplementary materials."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Implementation Details", "content": "In our training process, we employ the LoRA technique [Hu et al., 2022] to facilitate efficient fine-tuning. Throughout all experiments, we utilize the bfloat16 (bf16) computation datatype. The learning rate is set to 10e 4 and the maximum context length to 2048 tokens. The LLMs are trained for 8 epochs using a server with eight RTX 4090 24GB GPUs, which required approximately 20 hours.\nTo assess the performance of models, we consider two perspectives: the view of extension and the view of argument. The accuracy of extension prediction, denoted as ACC, is the proportion of samples where the predicted IN(L) exactly matches the ground truth extension. During inference, we calculate argument credulous/skeptical acceptability based on predicted labellings. For argument acceptability classification, we use the Matthews Correlation Coefficient (MCC) as the metric due to the imbalance between positive and negative samples, denoted as $MCC_{c}$ and $MCC_{s}$, respectively."}, {"title": "5.2 Main Results", "content": "In Table 2, we present the performance metrics of various language models, categorized by different semantics. For grounded semantics, the credulous and skeptical acceptance of arguments are identical, so they are combined into a single performance metric. Similarly, the credulous acceptances under both complete and preferred semantics are identical, resulting in the same MCC scores. It should be noted that complete semantics (and thus preferred/stable semantics) are computed with the assumption that grounded semantics is already known.\nThe upper section of Table 2 presents the zero-shot CoT approaches, encompassing the advanced GPT, LLaMA, and Qwen series models. These results highlight the models' initial performance without prior specific training. This baseline assessment is mainly for understanding their inherent capabilities in handling different semantic frameworks. In contrast, the lower section of the table showcases the outcomes after applying our training methodology. The final two rows demonstrate significant improvements in performance, underscoring the effectiveness of our approach. This enhancement illustrates the model's ability to adapt and optimize through training.\nIt is important to acknowledge that the accuracy of our models is comparatively lower than that achieved by GNN-based methods [Craandijk and Bex, 2020] which use trained multiple models for the prediction of accepted arguments under different semantics, since GNNs are inherently more suited to argumentation computation due to their architectural design. However, a key advantage of our method lies in its ability to directly compute extensions and provide corresponding explanations, offering a comprehensive understanding of the argumentation process. This capability underscores the practical applicability of our model in diverse semantic contexts."}, {"title": "5.3 Errors in Computing Grounded Labelling", "content": "In this section, we summarize the errors and failures encountered by the LLMs when computing the grounded labelling."}, {"title": "Hallucination", "content": "Hallucination typically refers to the generation of meaningless or unfaithful information that does not adhere to the provided source content. In abstract argumentation tasks, hallucinations in LLMs typically lead to the generation of information or text that is inconsistent with the input, thereby resulting in errors in the model's resolution process. For instance, in Figure 5 (a), the LLM incorrectly references the attack \"2 is attacked by 0 which is labelled IN\", while this attack actually does not exist."}, {"title": "Labelling Inconsistency", "content": "Although LLMs correctly generate arguments' labels, in subsequent steps, the model may erroneously reinterpret these labels, leading to incorrect solutions. As shown in Figure 5 (b), the argument 2 is mistakenly presumed to have been assigned OUT, when it should actually be assigned IN."}, {"title": "Early Termination", "content": "During the iterative computation phase, the model prematurely ends the computation process without assigning all arguments that could potentially be made IN or OUT. For example, in Figure 5 (c), the computation ceases after the IN step, specifically after assigning IN to 2, and fails to initiate the subsequent OUT step. This abrupt termination prevents the assignment of OUT to 3, of with the label remains UNDEC."}, {"title": "Wrong Label Assignment", "content": "The grounded extension is computed by alternately assigning IN and OUT to arguments. However, errors can occur if the sequence is not followed. In Figure 5, an error occurs when the LLM mistakenly assigns IN to 4, based on the incorrect assumption that 3 and 5, which attack 4, are assigned OUT. This error results from the incorrect assignment of OUT to 5, which is attacked by 4, incorrectly labeled as IN."}, {"title": "5.4 Errors in Computing Complete Semantics", "content": "In the computation of complete extensions, LLMs exhibit several critical inconsistencies:"}, {"title": "Incomplete Search", "content": "LLMs occasionally fail to identify all potential extensions, resulting in an incomplete coverage of the complete extensions. For example, in Figure 6 (1), the predicted complete extensions lack {0, 2, 4, 6}."}, {"title": "Conflict", "content": "During the inclusion of arguments labelled as IN, LLMs may incorrectly assess these arguments as being conflict-free. For example, in Figure 6 (2), 4 and 5 are added to the IN set even though they mutually attack each other."}, {"title": "Indefensibility", "content": "The complete extension generated by LLMs may not always defend all its arguments. For example, in Figure 6 (3), {0,2,6} does not defend itself from 5."}, {"title": "5.5 Effect of Explanations", "content": "In this section, we examine the effect of explanations on performance and the generalization capability of the model when applied to AAF with more arguments. To demonstrate the efficacy of explanations in data generation, as a comparison model, we additionally trained models, namely Llama3-noexp and Qwen2-noexp, on datasets without explanations,"}, {"title": "6 Related Work", "content": "Within the domain of formal logical deduction using language models, the primary focus has predominantly been on first-order logic [Tian et al., 2021]. Works such as Rule-Taker [Clark et al., 2020] and FOLIO [Han et al., 2022] exemplify the application of artificial intelligence in deducing logical conclusions from structured premises, thereby demonstrating models' capacity for formal logical reasoning. However, argumentative reasoning, which involves both logical reasoning and graph comprehension, has been largely overlooked [Guo et al., 2023].\nPrevious research has attempted to integrate argumentation frameworks or with neural networks for interpretation [Potyka, 2021] and semantics computation [Craandijk and Bex, 2022; Craandijk and Bex, 2020]. For instance, the work on interpreting neural networks as quantitative argumentation frameworks [Potyka, 2021] explores how neural networks can be understood through the lens of argumentation. Similarly, the AGNN [Craandijk and Bex, 2020] investigates the application of deep learning techniques to abstract argumentation semantics. Additionally, research in [Craandijk and Bex, 2022] proposes enforcement heuristics for argumentation with deep reinforcement learning, aiming to enhance the decision-making process in argumentation frameworks. In [Mileva et al., 2023], researchers aim to consolidate diverse approaches to learning argumentation semantics within a unified framework.\nRecent research has integrated argumentation techniques with large language models to enhance reasoning capabilities in various tasks. [Gorur et al., 2024] demonstrates that LLMs like Llama-2 and Mistral outperform traditional models in Relation-Based Argument Mining by effectively identifying support and attack relations between arguments. [Chen et al., 2024] conducts a comprehensive investigation into the potential of LLMs, such as ChatGPT, for computational argumentation. Their study focuses on argument mining and generation tasks, evaluating these models in both zero-shot and few-shot learning scenarios across diverse datasets. [Freedman et al., 2024] proposes an innovative approach to augment the reasoning abilities of LLMs through the incorporation of argumentative reasoning, which leverages LLMs to construct argumentation frameworks. The integration of these methodologies holds significant promise for advancing the development of robust artificial intelligence systems capable of sophisticated argumentation and reasoning."}, {"title": "7 Conclusion", "content": "In this paper, we examine the computation of abstract argumentation frameworks utilizing large language models. Our approach integrates argumentation computation with the capabilities of LLMs, providing enhanced explainability over traditional graph neural network methods. Experimental results demonstrate that LLMs can effectively learn and execute extension computation tasks. Key findings highlight the significant role of explanations in enhancing model performance and generalization. The self-explanatory capacities of LLMs address transparency challenges typically associated with neural networks. This research advances the understanding of LLMs in argumentation computation and paves the way for further investigation into defeasible reasoning and complex decision-making processes."}, {"title": "Appendix", "content": ""}, {"title": "A Chain-of-Thought Baseline", "content": "Prompting generally results in models making predictions with lower accuracy compared to models that have been fine-tuned with substantial training data [Zhang et al., 2023]. \u03a4\u03bf validate this observation and establish a baseline method, we design a Chain-of-Thought (CoT) pipeline [Wei et al., 2022] that computes extensions based on a labelling algorithm. The argumentation computation is divided into multiple steps, framed as a reasoning task."}, {"title": "Grounded Semantics", "content": "Initially, we employ LLMs to extract a list of arguments and identify attack relation to address the problem. We then task the LLMs with finding unattacked arguments to initialize the set IN(L). By following each step in the recursive labelling algorithm, the LLMs ultimately determine the grounded labeling when no further arguments can be added to IN(L) or OUT(L). The algorithm running prompt template is illustrated in Figure 8, where the special placeholders \u201c{PROGRAM}"}, {"title": "Complete Semantics", "content": "The subsequent task involves identifying complete labellings based on the grounded labelling. This process begins by selecting elements from UNDEC(L) and reassigning them to IN or OUT. Similar to the method used in model training, LLMs are directed to propose solutions and subsequently verify the legality of these choices, as shown in Figure 9. The special placeholder \u201c{GRD_LABELLING}\u201d will be replaced by ground truth grounded labelling."}]}