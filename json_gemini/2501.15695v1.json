{"title": "Contextual Knowledge Sharing in Multi-Agent Reinforcement Learning with Decentralized Communication and Coordination", "authors": ["Hung Du", "Srikanth Thudumu", "Hy Nguyen", "Rajesh Vasa", "Kon Mouzakis"], "abstract": "Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) has emerged as a pivotal approach for address-ing complex tasks in dynamic environments. Existing Multi-Agent Reinforcement Learning (MARL) methodologies typically assume a shared objective among agents and rely on cen-tralized control. However, many real-world scenarios feature agents with individual goals and limited observability of other agents, complicating coordination and hindering adaptability. Existing Dec-MARL strategies prioritize either communication or coordination, lacking an integrated approach that leverages both. This paper presents a novel Dec-MARL framework that integrates peer-to-peer communication and coordination, incor-porating goal-awareness and time-awareness into the agents' knowledge-sharing processes. Our framework equips agents with the ability to (i) share contextually relevant knowledge to assist other agents, and (ii) reason based on information acquired from multiple agents, while considering their own goals and the temporal context of prior knowledge. We evaluate our approach through several complex multi-agent tasks in environments with dynamically appearing obstacles. Our work demonstrates that incorporating goal-aware and time-aware knowledge sharing significantly enhances overall performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Cooperative Multi-Agent Reinforcement Learning (MARL) has emerged as a critical research area due to its potential to overcome the limitations of single-agent systems in addressing complex, real-world problems. While single-agent systems have demonstrated success in achieving human-like perfor-mance in specific scenarios [1], they often face limitations in terms of scalability, adaptability, and reliability, especially when dealing with complex tasks that require specialized agents [2]\u2013[4]. To address these limitations, the multi-agent system (MAS) architecture has gained prominence, enabling agents to communicate, coordinate, and tackle complex tasks in dynamic environments. MARL plays a key role in handling such dynamics [1], [5]. Among the various approaches within MARL, the Centralized Training and Decentralized Execution (CTDE) paradigm [6], [7] is popular for cooperative tasks [8]\u2013[14]. This approach employs a centralized critic during training to develop decentralized policies for agents, which are then executed independently. Although widely adopted, CTDE-based algorithms encounter significant difficulties in environments with large joint state-action spaces and inherent stochasticity. Moreover, these algorithms typically assume that agents share a common goal and depend on centralized control. However, many real-world situations involve agents with individual objectives and limited observability of others, leading to potential miscoordination, sub-optimal policies, and reduced adaptability.\nThe Decentralized Training and Decentralized Execution (DTDE) paradigm [15], [16] aims to address the limitations of the CTDE approach by relaxing the assumptions of full observability and centralized control. In a fully decentralized setting, each agent operates with its own goals and obser-vations, communicates with other agents within its obser-vation range, and coordinates during these communication sessions. The agent then uses the acquired observations and knowledge to optimize its objectives. This approach has the potential to enhance the robustness and adaptability of agents in handling uncertainties. However, DTDE-based algorithms can face significant challenges, including (a) exhaustive ex-ploration due to the absence of a centralized coordinator and limited observability, and (b) inefficient sharing of experience and knowledge caused by the growing number of agents and the rapid obsolescence of information.\nA promising approach to reducing the exhaustive explo-ration of independent agents in DTDE-based algorithms is to establish a communication protocol among agents. A straightforward communication scheme allows agents to share their local observations, which can then be used to optimize their local policies toward individual goals [16]\u2013[20]. While this approach reduces exploration time, it also introduces a significant amount of irrelevant information, which increases learning complexity and can degrade performance. Several strategies have been proposed to address this challenge. For example, agents can be instructed on when to communicate [21]\u2013[23]. In team settings, incentive-based communication schemes have been used to filter out trivial information and promote coordination toward a global objective [24]. Addition-ally, integrating Graph Neural Networks (GNNs) with agent feature embeddings and mutual information has been applied to eliminate irrelevant information [25]. Other methods focus on pruning irrelevant agents from communication sessions by leveraging agents' identities [26] and personalized commu-"}, {"title": "II. RELATED WORK", "content": "1) Decentralized Training and Decentralized Execution (DTDE): Approaches in cooperative MARL typically fall into two categories: Centralized Training and Decentralized Exe-cution (CTDE) [6], [7] and DTDE [15]. CTDE-based methods [8]\u2013[14] have demonstrated the stability of training multiple agents for cooperative tasks in complex environments. These"}, {"title": "III. PROBLEM PRELIMINARY", "content": "We formulate our framework as the Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) where the decision-making process of an agent follows Partially Observ-able Markov Decision Process (POMDP) [42]. This is defined as follows: $(n, S, {Ai}_{i=1}^n, T, {Ri}_{i=1}^n, {Oi}_{i=1}^n, P, \u03b3)$ where n is the number of agents, S is the set of states, ${A_i}_{i=1}^n$ denotes the set of action sets for each agent, $T : S \u00d7 A^n \u2192 S'$ is the state transition probability function following the joint actions $A^n = (a_1, a_2,...,a_n)$, ${R_i}_{i=1}^n$ is the set of rewards for each agent, ${O_i}_{i=1}^n$ represents the set of observations for each agent, $P: S \u00d7 A^n \u2192 O'$ is the observation probability function, and $\u03b3\u2208 [0,1]$ is the discount factor. Furthermore, a set of individual goals of agents is denoted by ${G_i}_{i=1}^n$ where goals can be defined in terms of states $G \u2286 S$ [43]. Notably, an agent can only access its own local observations and learn an independent policy $\u03c0_i$ to maximize its own goal in the decentralized setting.\nIn a fully decentralized environment, agents' observations can be limited and vary from one another (see also Figure 1). As a result, an agent only possesses knowledge of the states it has experienced, leaving other states unknown. Moreover, the value of acquired knowledge diminishes over time due to the environment's dynamics. This necessitates that agents consider the time factor associated with such knowledge when adjusting their policies. In our framework, we model this behavior by introducing a mental state for each agent, denoted by ${M_i}_{i=1}^n$. It is important to note that, within our framework, all agents share the same ontology and bounded environment. Consequently, the mental state of an agent encompasses all masked states in the environment, defined as follows: $M_i = {(s, m_t, d_t)}_{s\u2208S}$, where $m_t$ represents the masked label of state s at time step t (e.g., empty, obstacle, unknown, or other), and $d_t$ denotes the duration since the last visit. Additionally, $m_t$ dynamically changes in response to the environment."}, {"title": "IV. METHOD", "content": "In this section, we propose a novel Dec-MARL framework that equips agents with goal awareness and time awareness for addressing challenges of communication and coordination in full decentralized environments. Furthermore, detailed ex-planations of each component of our framework are provided below.\nTo facilitate generalization, our framework encodes the fol-lowing properties of the agent: (s, g, o, m, a). Specifically, we define $f(x) \u2192 e_x \u2208 R^k$ as the representation function, which could involve methods such as one-hot encoding, Multi-Layer Perceptron (MLP), categorical encoding, image-based encod-ing, or other representation techniques. Here, x represents one of the agent's properties, and k denotes the dimensionality of the embedding. It is important to note that both f and k can"}, {"title": "A. Representations and Value Approximation", "content": "vary depending on the specific agent property. Furthermore, the mental state of an agent is represented as follows:\n$e_{M_i} = \\bigcup_{(s,m)\u2208M_i} (e_s \\cup e_m)$\n(1)\nwhere $ \\cup $ is the concatenation operation between two embed-dings between $e_s$ and $e_m$, and $U$ is the aggregation function (e.g., summation, dot product, average pooling, or other). Our framework applies the average pooling. Note that the scheme of Equation 1 excludes the time factor.\nUnderstanding its current goal and recent mental state is essential for an agent to adjust its actions in two key ways: (a) moving toward the goal based on its belief about future states, or (b) exploring uncertain states that could be advantageous for achieving the current goal. As illustrated in Figure 2, there are situations where the agent must balance these two aspects to maximize rewards. Inspired by the Universal Value Function Approximator (UVFA) [43], our framework integrates both the agent's goal and mental state into the construction of the policy function as $\u03c0 : S \u00d7 G \u00d7 M \u2192 A$. The corresponding action-value function is then defined as $Q(s, a, g, M;\u03b8_2) \u2248 Q_{g,M}(s, a)$ where $\u03b8_2$ is learning parameters."}, {"title": "B. Time Awareness and Intrinsic Rewards", "content": "A reward provided by the environment is designed to guide an agent toward achieving its goal, commonly referred to as an extrinsic reward. In decentralized training, the agent does not have access to the global state. Therefore, exploring novel ob-servations that are based on the agent's local observations and potentially beneficial for future outcomes can be encouraged by using an intrinsic reward. The novelty of an observation is often estimated using a utility function with count-based mechanisms [35] as follows:\n$u_t(o) = \\frac{1}{N_o}$\n(2)\nwhere $u_t$ is dependent on the local observations of agent i at time t, and $N_o$ is the frequency of the observation. Additionally, $u_t$ can vary between agents. Equation 2 indicates that the novelty of an observation decreases as it occurs more frequently in the agent's experience. However, in many practical scenarios, an observation may become novel again despite its high frequency. This is due to the dynamics of the environment. Hence, instead of using count-based mecha-nisms, we introduce the time factor that measures the novelty of an observation as:\n$u_t(o) = e^{d_t}$\n(3)\nwhere e is the exponential function, $d_t, o \u2208 M_i$, and $t' < t$. In addition, $d_t$ is controllable according to the application do-mains. From our empirical analysis, we would suggest keeping $d_t$ as small as possible with the time increment less than 0.1 in situations where information is gradually changed (see also Figure 3). Equation 3 satisfies the following two conditions: (a) the value of the observation decays over time after being uncovered by the agent; and (b) the observation becomes novel again after being re-discovered by the agent. Furthermore, the value of $u_t$ can be integrated with embeddings in Equation 1 and convert $e_m$ into the time-aware scheme as follows:\n$e_{M_i} = \\bigcup_{(s,m)\u2208M_i} U(u_t(s). (e_s \\cup e_m))$\n(4)\nwhere $u_t(s) \u2208 R$ is a scalar value.\nIn addition to being integrated with the embedding of the mental state of an agent, we introduce a novel reward"}, {"title": "C. Integration of Communication and Coordination", "content": "In fully decentralized settings, it is essential for agents to communicate and share relevant observations and knowl-edge. While relevant observations can accelerate an individual agent's exploration, relevant knowledge can enhance their per-formance in achieving their goals. However, shared informa-tion can have both positive and negative impacts on an agent's policy and action value function [25], [29]. Therefore, it is important for agents to carefully evaluate the information they receive before incorporating it into their current policy and action value function. In our framework, we propose a strategy that integrates communication and coordination, incorporating goal awareness and time awareness. This strategy consists of three phases: Share-Reason-Aggregate. The details of each phase are specified below.\n1) Share: As the agent navigates the environment, it may encounter other agents within its observation range, allowing for the establishment of communication and coordination ses-sions. During a communication session, the agent broadcasts its goal to identify two types of agents: (a) agents who share the same goal, known as current peers, and (b) agents who have relevant knowledge from their experience but do not share the same goal, referred to as current advisors. It is worth noting that peers do not necessarily have prior experience of the given goal. Once this identification process is complete, the agents initiate the coordination session. Both peers and advisors retrieve observations relevant to the goal. The retrieval mechanisms can differ depending on the problem domain. In our framework, a peer retrieves both observations from its mental state and learning parameters such as $\u03b8$ and $\u03b8_2$. Additionally, inspired by [39], each agent in our framework is equipped with a heuristic planning capability that is activated only when the agent is in the role of an advisor. Specifically, in discrete observation spaces, such as a 2D map (x, y), an advisor estimates the shortest path comprising observations between the agent's current position and the given goal. Advisors do not share their learning parameters, as these parameters are optimized for different goals that may not align with the agent's current goal. It is important to note that agents in our framework share the same ontology and bounded environment, making observations and knowledge transferable among them.\n2) Reason: After the knowledge-sharing process, the agent activates its reasoning capability rather than blindly following the acquired observations and knowledge. To achieve this, our framework equips agents with a rule-based reasoning capability. First, the agent reflects on its mental state using the latest and novel observations shared by peers and advisors as follows:\n$M_i = \\bigcup_{j=1; s\u2208S}^K {(s, (s, m_t)_{i, dt})_{i, j}$ if $(d_t)_{i, j} < (d_t)_i$ \n \n\n\n$M_{t, dt})_{j}$, if $(d_t)_{j} > (d_t)_i \\cup M_i$ \n (6)\nwhere $ \\cup $ is the set union function, K is the total number of peers and advisors and j represents the index of a peer or an advisor. Second, to determine whether to update its learning parameters, the agent estimates the overlap ratio between its mental state and the observations shared by each peer. In our framework, this overlap ratio between discrete observations is calculated using the Jaccard similarity as:\n$J(M_i, M_j) = \\frac{|{(s, m_t)_i} \u2229 {(s, m_t)_j}|}{|{(s, m_t)_i} \u222a {(s, m_t)_j}|}$\n(7)\nwhere $J\u2208 [0,1]$, with J = 0 indicating that agents i and j have no overlapping observations, and J = 1 indicating a complete match between their mental states. Here, s represents the known state of an agent. It is important to note that this estimation takes place before agents update their mental states with the newly acquired observations. The primary objective is to encourage the agent to integrate novel knowledge obtained from its peers.\n3) Aggregate: After selecting peers based on the overlap ratio, the agent updates its learning parameters as follows:\n$\u03b8_i^t = (1 - \u03b2)\u03b8_i + \u03b2 \\frac{1}{K} \\sum_{j=1}^K \u03b8_j$\n(8)\n$\u03b8_{2_i}^t = (1 - \u03b2)\u03b8_{2_i} + \u03b2 \\sum_{j=1}^K \u03b8_2^j$\n(9)\nwhere K represents the total number of selected peers, and \u03b2 is the dampening factor that balances the agent's own learning parameters with those aggregated from its peers. Our empirical analysis indicates that Equations 8 and 9 can sometimes lead to situations where poor-performing agents negatively impact the performance of others during the coordination session. Therefore, we recommend keeping \u03b2 as low as possible."}, {"title": "V. EXPERIMENTS", "content": "To evaluate our framework, we designed a 2D map with dynamically appearing obstacles. The environment comes in two sizes: Base (10 x 10) and Large (20 x 20), to test the scalability of our framework. Each environment features 3 objects surrounded by obstacles, with the number and positions of these obstacles being static. We created two difficulty levels"}, {"title": "A. Environments and Tasks", "content": "for the environment: Easy and Hard. In the Easy environment, obstacles remain unchanged over time, while in the Hard environment, obstacles can appear and disappear dynamically. Specifically, in the Hard environment, an obstacle may appear at time t and disappear at a later time t', where t' > t, or vice versa. The Hard environment is designed to assess how well agents in our framework handle environmental dynamics.\nCombining the environment sizes with the difficulty levels results in four distinct environments: Base-Easy, Base-Hard, Large-Easy, and Large-Hard. In each environment, an agent starts at a predefined position far from its goal, with agents being placed in different areas far from one another. There are two scenarios regarding the agents' goals: (i) all agents pursue the same goal, and (ii) there are two distinct goals, with at least two agents pursuing the first goal and the remaining agents pursuing the second goal. Moreover, multiple agents can occupy the same cell. An agent's task is considered complete if it reaches its goal and remains in that position."}, {"title": "B. Implementation Details", "content": "We conducted our experiments a complex 2D environment with fully decentralized settings. Here, s = (x,y), G C S, m can be one of the following labels: empty, obstacle, object, agent, or unknown, and a can be one of the following options: left, right, up, down, or stay. Furthermore, we utilized categorical encoding functions to represent agent's properties (s, g, o, m, a) in their embeddings as: $e_s \u2208 R^{64}, e_g \u2208 R^{16}, e_o \u2208 R^{64}, e_m \u2208 R^{16}$, and $e_a \u2208 R^{16}$.\nWe implemented the Actor-Critic method [44] for each agent in our framework. This method includes an actor, which uses the policy with learning parameters $\u03b8$ to select an action in a given state, and a critic, which evaluates the chosen action using an action value function Q with learning parameters $\u03b8^w$. In addition, we followed the implementation details of the Deep Deterministic Policy Gradient (DDPG) algorithm [45]. Each agent's actor network consists of two fully-connected Multi-Layer Perceptrons (MLP), each layer containing 128 neuron units. This configuration is also used for the agent's critic network. Adam [46] is employed as the optimizer for learning the neural network parameters, with a learning rate of $10^{-4}$ for the actor and $10^{-3}$ for the critic. The discount factor $\u03b3$ is set to 0.99, and the soft target update rate $\u03c4$ is set to $10^{-3}$. Furthermore, the batch size of the relay buffer B is 64.\nWe also designed a sparse reward function of an agent as follows:\n$R(s) = \\begin{cases}\n1 & \\text{if } s = g_i \\\\\n-A_{stay} & \\text{if } (s_{t-1} = s) \u2227 (s \\neq g_i) \\\\\n-1  & \\text{if } (s_{t-1} \\neq s) \\\\ \n agg & \\text{otherwise} \n\\end{cases}$\n(10)\nThe reward value ranges between -1 and 1. An agent receives a reward of 1 if its position matches its goal. If the agent remains in a cell that is not its goal, it is penalized by $A_{stay} \u2208 (0, 1)$. In our experiments, we applied $A_{stay}$ = 0.5 to encourage"}, {"title": "C. Results and Discussion", "content": "1) Scenario 1: Table I presents the experimental results for the first scenario, where all agents pursue the same goal. The integration of both mental state and time-awareness in independent agents within a fully decentralized setting ($A^2$ through $A^5$) generally yields better outcomes compared to $A^1$. Specifically, in the Base-Easy environment, $A^5$ outperforms the other agents, completing tasks with 5% fewer steps on average. In the Base-Hard environment, not only does the performance of all agent types improve, but the number of steps taken is also reduced by 15% compared to the Base-Easy environment. Notably, $A^2$ outperforms the others in this scenario, potentially due to dynamic obstacles creating pathways that allow faster goal achievement. Furthermore, $A^4$ and $A^5$ excel in the Large-Easy and Large-Hard environments, respectively, highlighting the importance of time-awareness for effective exploration in larger observation spaces. Additionally, to achieve higher outcomes when dealing with dynamic environments, time-aware agents must communicate and coordinate with each other. Interestingly, we observed that agents only reached their goals in a few episodes within the large environments. A potential solution is to increase the number of episodes and the maximum steps per episode. As these numbers increase, it is also crucial to select an appropriate value for $d_t$ in Equation 3.\n2) Scenario 2: By comparing Table II with Table I, we observe that agents generally achieve higher rewards in the second scenario compared to the first. As illustrated in Table II, the overall performance of $A^2$ through $A^5$ continues to surpass that of $A^1$. Moreover, time-aware agents equipped with communication and coordination capabilities ($A^4$ and $A^5$) excel in three environments: Base-Easy, Base-Hard, and Large-Hard. Although $A^5$ does not outperform the independent agents in the Large-Easy environment, it is notable that $A^5$ tends to take fewer steps and reaches its goals in more episodes than the other types of agents.\n3) Ablation Study: We conducted an ablation study to assess the contribution of each additional feature for inde-pendent agents in a fully decentralized environment. The first feature examined was the mental state of an agent ($A^2$), which generally enhances the performance of $A^1$ in the Base envi-ronments across both scenarios. However, this feature alone is insufficient for agents operating in the Large environments. To address this limitation, time awareness was introduced as an additional feature ($A^3$). The results in Tables I and II highlight the improvement of agents equipped with both mental state and time awareness compared to $A^1$. When communication and coordination were integrated into $A^3$, performance im-provements were observed in three environments-Base-Easy, Base-Hard, and Large-Hard-across both scenarios. Further-more, enhancing agent performance in Hard environments is crucial for managing dynamics in a fully decentralized setting. The introduction of goal-awareness also led to per-formance gains in the Base environments. Our ablation study demonstrates significant improvements in the performance of independent agents within a decentralized setting when equipped with mental state, time awareness, goal awareness, and a strategy that integrates communication and coordination."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "We proposed a novel Decentralized Muti-Agent Reinforce-ment Learning (Dec-MARL) framework that aims to address two key challenges such as exhaustive exploration and inef-ficient knowledge sharing among agent in the fully decen-tralized settings. Our framework introduces several innova-tive aspects: (i) the incorporation of an agent's mental state with time awareness; (ii) time-aware intrinsic rewards that motivate agents to explore novel states, potentially aiding in the achievement of their individual goals; (iii) the integration of communication and coordination; and (iv) the inclusion of goal awareness within this integration to facilitate effi-cient knowledge sharing. Experimental results demonstrate that our framework progressively enhances the performance"}, {"title": "APPENDIX 1 - DESCRIPTION OF DATASETS", "content": "A. The Base Environment\nG3); and (ii) Agents 1 and 2 pursuing Goal 1 and Agent 3 pursuing Goal 2.\nB. The Large Environment\nFigure 5 shows the Large environment mentioned in the paper. A circle represents an agent, a diamond represents a goal of agents, a box filled by the red color represents an obstacle, and a box filled by zigzag lines represents an obstacle that is dynamically occurring. Furthermore, there are two settings such as: (i) all agents pursuing a single goal (i.e., G3); and (ii) Agents 1 and 2 pursuing Goal 1 and Agent 3 pursuing Goal 2."}]}