{"title": "FlexQuant: Elastic Quantization Framework for Locally Hosted LLM on Edge Devices", "authors": ["Yuji Chai", "Mujin Kwen", "David Brooks", "Gu-Yeon Wei"], "abstract": "Deploying LLMs on edge devices presents serious technical challenges. Memory elasticity is crucial for edge devices with unified memory, where memory is shared and fluctuates dynamically. Existing solutions suffer from either poor transition granularity or high storage costs. We propose FlexQuant, a novel elasticity framework that generates an ensemble of quantized models, providing an elastic hosting solution with 15x granularity improvement and 10x storage reduction compared to SoTA methods. FlexQuant works with most quantization methods and creates a family of trade-off options under various storage limits through our pruning method. It brings great performance and flexibility to the edge deployment of LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Since the introduction of ChatGPT, countless companies in the technology sector have been releasing personal AI assistant products. While OpenAI hosts ChatGPT as a purely in the cloud, these companies are transitioning to a future of AI services that are hosted locally on edge devices, aiming to deliver a truly personalized AI experience that lives entirely on personal devices. Microsoft markets their hybrid hosting concept called \u201cPC + Copilot,\" in which a language model runs locally on laptops [1]. Apple has integrated Apple intelligence, their personal assistant platform, on virtually their entire new hardware product line [2]. Google has released their Google AI Edge API for Android mobile application developer to deploy Gemini locally on supported Android devices, including their latest smartphone product line [3]-[6]. Several key factors motivate locally hosting LLMs. First, (Privacy Concerns) streaming personal information to the cloud for Retrieval Augmented Generation (RAG) greatly increases the risk of privacy breaches [7]. Second, (Connectivity Issues) cloud- based inference services rely on a stable internet connection, a privilege that users don't always have on edge devices. Finally, (Compute Scalability) the growing compute demand for AI services requires exponential growth in cloud compute capabil- ity. Equipping local inference capability could maximize data privacy protection, provide AI service without stable Internet connections, and offload fast-growing compute demand.\nDespite the emerging need for local deployment of LLMs on edge devices, it still remains technically challenging. This technical challenge comes from various aspects, including reducing first token latency, improving token generation speed, limiting energy consumption, etc. In this work, we focus our efforts on improving memory footprint elasticity of locally hosted LLMs. These LLMs power Gen-AI services that can be triggered at any time upon a user's request. Because the ma- jority of edge SoCs utilize unified memory architectures [8]\u2013 [10], the available memory space is dynamic at the time of a request, depending on memory usage from other applications. Thus, hosting a LLM with a fixed sized memory footprint is not a viable solution and LLM memory elasticity is a necessity.\nTo enable such elasticity, researchers have designed many compression methods, such as GPTQ [11], ExllamaV2\u00b9, SmoothQuant [12] etc. Those methods, especially ExllamaV2, offer system engineers a flexible design tool that can be used to generate an ensemble of quantized LLMs with various memory footprints. By switching between different models in that ensemble, it serves as an elastic hosting solution. However, this baseline elastic hosting method requires 100GB+ of storage to store all the models. A typical edge device, such as a cell phone, a tablet, or a laptop, usually only has few hundred GB of storage, so a 100GB+ footprint is very costly. A recent work, AnyPrecision [13], provided a better solution where different models could share their parameters resulting in an elastic policy requiring under 10GB of storage. However, it suffers from poor granularity of available model memory footprints and a worse memory-perplexity trade-off compared to ExLlamaV2, as demonstrated in Figure 1. The largest memory footprint gap between two options is larger than"}, {"title": "II. BACKGROUND", "content": "Reducing the computational cost and memory consumption of LLMs is a pressing challenge, particularly when deploying them on edge devices. Quantization, a method used to approx- imate higher-precision data using lower-precision representa- tions, has emerged as a standard solution to mitigating this issue. In this paper, we focus primarily on weight quantization methods that generally fall in one of two categories: quan- tization aware training (QAT) and post training quantization (PTQ). QAT methods simulate quantization during extensive retraining or finetuning phases to mitigate accuracy loss [14], [15]. Although these methods yield superior downstream ac- curacy, they are both time-consuming and incredibly taxing on computation and memory resources. As this burden is ex-acerbated with larger models, comparatively lightweight PTQ methods that require little to no training data are generally favored for quantizing LLMs [13], [14], [16]\u2013[18].\nOne such example is GPTQ, an exemplary one-shot, per- layer quantization method that iteratively quantizes parameters in blocks and uses approximate second-order information to update remaining, yet-to-be-quantized weights, mitigating quantization loss [11]. Other effective PTQ methods include SpQR [19], Activation-Aware Weight Quantization (AWQ) [17], SmoothQuant [12], ZeroQuant [18], AdaRound [20], and SqueezeLLM [16]. In these methods, regardless of weight precision, actual computations usually rely on standard pre- cision formats like as FP16 or FP8. Our elastic framework for hosting quantized LLMs on edge devices is incredibly versatile and can support any PTQ method to mix quantization levels as long as all computations use the same precision. In this paper, we mainly employ ExLlamaV2 for weight quantization. ExLlamaV2 is based on GPTQ and supports mixing quantiza- tion levels both across layers and within individual modules, making it particularly well suited for FlexQuant. ExLlamaV2 minimizes quantization error over a diverse calibration set while meeting a target average bitrate for the model. We find that this PTQ method offers the best trade-off between memory footprint, accuracy, and latency for our experiments."}, {"title": "III. FLEXQUANT FRAMEWORK", "content": "FlexQuant enables elastic hosting by generating an ensem- ble of Elastic Quantization Models (EQM). Models in the ensemble have gradually smaller memory footprint to enable memory elasticity. The maximum difference in footprint be- tween two adjacent models defines the ensemble's transition granularity. The total size of the entire ensemble determines its storage overhead. Existing ensemble design methods struggle to generate high granularity memory transition while main- taining low storage cost. FlexQuant automatically identifies EQMs to meet this need while improving accuracy-memory trade-off. Its flexible mechanism also allows adaptation to a wide range of quantization methods.\nFlexQuant leverages the interchangeability of quantized parameters in different bit-widths. Because an n-bit quan- tized LLM layer is a numerical approximation of its FP16 counterpart, representations of the layer in different bit-widths still share the same approximation target. Thus, a quantized LLM does not experience significant loss of accuracy if a module is replaced with a slightly lower bit-width counterpart. Empirical observation also supports this intuition. Based on experiments on a Llama 2 7B model, the perplexity drop of an 8-bit model due to replacing one of its module with a 3-bit version's is always under 0.02. This characteristic suggests that a smooth memory-accuracy trade-off, through gradually replacing quantized LLM's layers with their lower bit-width counterparts, is attainable. FlexQuant leverages this characteristic to design a high-granularity EQM ensemble with low storage cost."}, {"title": "Efficient Design Space Navigation", "content": "Although FlexQuant's EQM generation method brings multifold benefits, it still suffers from a major technical challenge: navigating its massive design space. For an EQM ensemble, formulated by m + 2 QMs, every element in EQM(now, nmid, ..., nmid, nup) has a candidate pool with the size of (m+2)#module, where #module indicates number of modules in a language model. As a lower bound example, a 2 model EQM ensemble for Llama-2 7B has a design space with the size of $2^{66}$ = 7.38 * 1019. To efficiently navigate a design space of this size, FlexQuant narrows down the available options by leveraging deployment constraints. In a realistic deployment scenario, in addition to transition granularity and storage cost, it is also important to limit the transitional memory cost due to module replacement. With this consideration, FlexQuant only allows for module transition from its current bit-with to its lower bit-width counterpart and forbids backward transition. This constraint minimizes the total memory IO cost during continuous transition.\nWith this design space constraint, the search for the next $EQM_k(n_{low}, n_{omid}, ..., n_{mid}, n_{mid}, n_{up})$ simplifies into selecting which module to replace with its lower bit-width version. The upper bound of design space's complexity dramatically reduced to (m + 1) * #module. In the case of 2 model EQM ensemble designed for Llama-2 7B, the complexity reduces to 66 for selecting the next configuration. However, this simplification is not enough. The total design space complexity of generating every configuration in the model ensemble, $EQM(n_{low}, n_{omid}, ..., n_{mid}, n_{up})$ could be as large as $\\prod_{i=1}^{(m+1)*\\#module}i$, which is still too large.\nWe use an approach that is inspired by Monte Carlo Tree Search [24], and the search for EQM generation could be viewed as a tree traversal process. The EQM search starts from the QM(nup), which could be viewed as the root of the tree structure. Stemming from the root, there are in total (m + 1) * #module options as the next configuration. All options would become their own subtree's root and the pattern goes on, until every stem reaches the end. All stems would end with the configuration reaching the QM(now) as leaf nodes. A complete tree traversal path from the root node to a lead node represents a valid EQM suite."}, {"title": "V. RESULTS & DISCUSSION", "content": "Figure 1 shows how FlexQuant, when applied to ExLla- maV2 (FQ-Ex) and to AnyPrecision (FQ-AP), compares to baseline elastic hosting framework using ExLlamaV2 (Base- Ex) and AnyPrecision (Base-AP). We compare these methods by measuring perplexity on the calibration set using Llama 2 7B. Both FQ-Ex and FQ-AP yield smooth transition trajec- tories with a granularity of ~100MB. In other words, every consecutive point along each FlexQuant curve represents a potential hybrid model that is only around 100MB larger than the previous one. In comparison, replicating this elastic tran- sition trajectory at 100MB granularity using Base-Ex requires a model ensemble using 177GB in storage, over 10x that of FQ-Ex. Replicating this 100MB granularity with Base-AP is not possible at all. Not only does FQ-Ex require an order of magnitude less storage, it also provides the best memory footprint-accuracy trade-off. Figure 1 shows that FQ-Ex is almost always at the Pareto frontier."}, {"title": "VI. CONCLUSION", "content": "Growing reliance on LLMs, coupled with the ever-present need for personalization and privacy, has led to increasing interest in locally hosted LLMs. One of the major challenges with locally hosting LLMs is memory elasticity. Existing elastic serving approaches either cannot dynamically adjust model sizes in small increments or fail to scale due to significant storage cost. Our FlexQuant work demonstrates a viable direction to greatly improve transition granularity and storage cost. Future system designers could leverage FlexQuant to have more flexibility and design options when deploying LLMs on edge devices. Additionally, we believe that a LLM's usage of other key hardware resources, such as energy or compute, also need to be adjusted elastically, a direction we aspire to explore in future works."}]}