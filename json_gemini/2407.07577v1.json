{"title": "IDA-VLM: Towards Movie Understanding via ID-Aware Large\nVision-Language Model", "authors": ["Yatai Ji", "Shilong Zhang", "Jie Wu", "Peize Sun", "Weifeng Chen", "Xuefeng Xiao", "Sidi Yang", "Yujiu Yang", "Ping Luo"], "abstract": "The rapid advancement of Large Vision-\nLanguage models (LVLMs) has demonstrated\na spectrum of emergent capabilities. Neverthe-\nless, current models only focus on the visual\ncontent of a single scenario, while their ability\nto associate instances across different scenes\nhas not yet been explored, which is essential\nfor understanding complex visual content, such\nas movies with multiple characters and intri-\ncate plots. Towards movie understanding, a\ncritical initial step for LVLMs is to unleash the\npotential of character identities memory and\nrecognition across multiple visual scenarios. To\nachieve the goal, we propose visual instruction\ntuning with ID reference and develop an ID-\nAware Large Vision-Language Model, IDA-\nVLM. Furthermore, our research introduces a\nnovel benchmark MM-ID, to examine LVLMs\non instance IDs memory and recognition across\nfour dimensions: matching, location, question-\nanswering, and captioning. Our findings high-\nlight the limitations of existing LVLMs in rec-\nognizing and associating instance identities\nwith ID reference. This paper paves the way for\nfuture artificial intelligence systems to possess\nmulti-identity visual inputs, thereby facilitating\nthe comprehension of complex visual narratives\nlike movies.", "sections": [{"title": "1 Introduction", "content": "Our real world contains a wide variety of informa-\ntion, such as texts, images, sounds, etc. Towards\nmultimodal comprehension (Guo et al., 2019; Lu\net al., 2023), inspired by the success of Large Lan-\nguage Models (LLMs) (OpenAI, 2023a; vicuna,\n2023; Touvron et al., 2023; Meta, 2024; Google,\n2023b), there is a surging interest in Large Vision-\nLanguage Models (LVLMs), such as LLaVA (Liu\net al., 2023a), Otter (Li et al., 2023a), GPT-\n4V (OpenAI, 2023b), Gemini (Google, 2023a), and\nothers. In the quest for Artificial General Intelli-\ngence (AGI), LVLMs serve as pivotal milestones,\nenhancing machines' capabilities in multimodal\nperception, reasoning, and knowledge.\nTypical LVLMs incorporate visual encoders\nwith LLMs, such as LLaVA (Liu et al., 2023a),\nMiniGPT4 (Zhu et al., 2023). These models project\nan image's visual features into the embedding space\nof language models and employ visual instruction\ntuning, allowing users to complete a variety of\nvisual tasks through language instructions, as de-\npicted in Figure 1(a). However, pure language\ninteraction makes VLMs hard to receive precise\nRegion-of-Interest references from users, thus hin-\ndering their capability to focus on specific regions\nof an image. To address this issue, a series of"}, {"title": "2 Related Work", "content": "2.1 Large Vision-Language Models\nConventional multimodal models consist of uni-\nmodal encoders and cross-modal fusion encoders.\nRelying on vision-language pre-training (Tan and\nBansal, 2019; Lu et al., 2019; Dou et al., 2022;\nWang et al., 2021; Ji et al., 2023b), they have\nshown an impressive cross-modal semantic align-\nment ability (Ji et al., 2023a; Tu et al., 2023), which\nbrings substantial advances on various downstream\ntasks (Goyal et al., 2019; Plummer et al., 2017;\nLin et al., 2014). However, due to the limitations\nof model size and training data scale, the perfor-\nmance of vision-language pre-training models is\nunsatisfied in open-ended scenarios.\nNowadays, Large language model (LLM) has ex-\nhibited remarkable abilities to understand, reason,\nand generate texts. Large Vision-Language Model\n(LVLM) (Chen et al., 2023c; Dai et al., 2023; Dong\net al., 2024) incorporates visual encoder and LLM,\naligning visual features to the embedding space of\nLLM. Leveraging strong generalization and emer-\ngent capability of LLM, LVLM realizes free multi-\nmodal interactions with human. Flamingo (Alayrac\net al., 2022) is a pioneering work on extending\nLLMs to vision-language pretraining by inserting\nadditional cross-attention layers for visual input.\nBLIP-2 (Li et al., 2023d) proposes Q-former to map\nthe visual features to the hidden space of language\nmodels. To date, various works have shown encour-\naging progress with instruction tuning, including\nMiniGPT-4 (Zhu et al., 2023), LLaVA (Liu et al.,\n2023a), Otter (Li et al., 2023a), which demonstrate\nimpressive results on natural instruction-following\nand visual reasoning capabilities. The perceptual\ncapabilities of LVLMs are evolving towards fine-\ngrained understanding. Numerous models (Zhang\net al., 2023; Liu et al., 2023a; Chen et al., 2023b;\nWang et al., 2023) focus on region-level under-\nstanding, using visual instruction tuning with Rol\nreference, so that users could ask questions about\nspecific instances within the content. Moreover,\nthere are some LVLMs equiped with Stable Diffu-\nsion, which can produce multimodal outputs (Koh\net al., 2023; Ge et al., 2023; Sun et al., 2023; Li\net al., 2023c). LVLMs hold the potential to per-\nform a wider array of functions and to understand\nmore intricate visual information. For instance,\nmovie is considered as one of the most intricate\nmediums for conveying visual information. This\npaper researches on movie understanding with ID"}, {"title": "2.2 Multimodal Benchmark", "content": "Previous evaluation for multimodal models\nmeasures some specific abilities, such as\nVQAv2 (Goyal et al., 2019), GQA (Hudson\nand Manning, 2019) for visual question an-\nswering, RefCOCO (Kazemzadeh et al., 2014)\nfor visual grounding, Visual7W (Zhu et al.,\n2016) for PointQA, VCR (Zellers et al., 2019)\nfor commonsense reasoning in an image. Re-\ncently, there is a surging interest in developing\ncomprehensive benchmarks for evaluating\nLVLMs. MMBench (Liu et al., 2023b) consists\nof multiple-choice questions and introduces a\nCircularEval strategy for evaluation framework.\nLLaVA-Bench (Liu et al., 2023a) employs GPT-4\nto assess responses from both GPT-4 and the\nmodel under evaluation, then provides scores\nand explanations for the answers. MM-Vet (Yu\net al., 2023) assesses LVLMs across six funda-\nmental visual-linguistic capabilities, utilizing a\nGPT-4-based evaluator for open-ended responses.\nMME (Fu et al., 2023) tests for perception and\ncognition competencies across 14 distinct subtasks,\nwhich requires models to provide simple 'yes'\nor 'no' answers. SEED-Bench (Li et al., 2023b)\ncovers 12 evaluation dimensions and adopts an\nanswer ranking strategy to evaluate LVLMs via\nmultiple-choice questions. POPE (Li et al., 2023e)\nis a dedicated benchmark for assessing object\nhallucination. Existing benchmarks evaluate\nmultifaceted performance of LVLMs, yet they\npredominantly focus on visual scenarios that lack\ncharacter identities and intricate plots."}, {"title": "3 Method", "content": "To understand complex visual input, for example, a\nmovie or animation, the model need memorize and\nrecognize character identities, linking characters in\ndistinct scenes. As shown in Figure 2, we simplify\nmovie understanding as given ID images of certain\ncharacters and keyframes from the movie or ani-\nmation served as test images, the model completes\ninstruction tasks. The ID images, test images, and\ninstruction text are sent to the model together for\ninference in an interleaved format of images and\ntext. The model need to memorize instance iden-\ntities in ID references and recognize them in test\nimages for providing responses."}, {"title": "3.1 Model Architecture", "content": "In this paper, we adopt Qwen-VL-chat (Bai et al.,\n2023) as our baseline model. In order to adapt\nthe model to the task of instance ID recognition,\nwe employ a dual-stage visual instruction tuning\nwith ID reference. The architecture of IDA-VLM\ncomprises three components: a visual encoder, ID-\nFormer consisting of cross-attention mechanisms,\nand a subsequent large language model.\nAs shown in Figure 2, the ID-Former is designed\nto project visual features into the input semantic\nspace of LLM and contribute to recognizing in-\nstance identities. This is achieved by two cross-\nattention modules. The first one interacts learn-\nable queries with the visual features through cross-\nattention, effectively compressing the visual seman-\ntics into a shorter, fixed-length feature encoding.\nThe second cross-attention utilizes queries of ID\nimages to modulate test image embeddings, acti-\nvating identity information of test images."}, {"title": "3.2 First-stage Tuning", "content": "In the first phase, we harness the off-the-shelf anno-\ntations available in existing datasets along with our\nproposed data configuration strategies, reducing\nthe cost for extra annotations about ID recogni-\ntion. Specifically, we utilize the public datasets\ncontaining instance spatial information, including\nVisual Commonsense Reasoning (VCR) (Zellers\net al., 2019), RefCOCO (Kazemzadeh et al., 2014),\nand Flickr30k (Plummer et al., 2017) datasets, to\nconstruct visual instruction tuning data with ID\nreference, which contains three instruction tasks:"}, {"title": "3.3 Second-stage Tuning", "content": "The second-stage fine-tuning data is based on the\nMovieNet (Huang et al., 2020) dataset. The origi-\nnal annotations in the MovieNet dataset encompass\nthe names of the characters present in each movie\nshot and their coordinate location information. In\nthe MovieNet, we select pictures that contain only\na single character to serve as ID images, while\nthose containing multiple characters are used as\ntest images. This approach allows us to naturally\ndevelop a dataset for location task. Moreover, we\ncompile ID images featuring the same character to\nconstruct data for matching task.\nFor Q&A and caption tasks, we adopt GPT-4V\nto convert annotations from the MovieNet dataset\ninto the question format that refers to specific roles.\nSpecifically, as illustrated in the lower part of Fig-\nure 3, we feed test images along with their character\nlocation information into GPT-4V, and encourage\nthe model to generate captions or question-answer\npairs via prompt engineering. The detailed prompts\nused for generating descriptions or question-answer\npairs are shown in Appendix A. Finally, we inte-\ngrate ID images, test images and results of GPT-\n4V into conversation templates, producing second-\nstage instruction tuning data."}, {"title": "4 MM-ID", "content": "4.1 Problem Definition\nTo measure ID recognition capability of models, we\npropose a new benchmark, MM-ID. We appraise\nthe capability of models to recognize IDs across\nfour incrementally complex levels. The first sub-\ntask we investigate is matching: given an instance\nimage, which could feature a person, an animal, or\na building, the model must choose an image from\nfour options that contains the same instance. The\nsecond sub-task involves localizing the instance\nwithin a test image based on the ID image, and\nproviding coordinates of the bounding box, with\nthe challenge coming from numerous similar dis-\ntractor objects or individuals present in the test"}, {"title": "4.2 Data Collection and Statistics", "content": "In the construction of MM-ID benchmark, we ini-\ntially collect images that meet the specified condi-\ntions. To evaluate the model's capability of recog-\nnizing instance-level information, the images used\nfor testing need to contain multiple characters. Fur-\nthermore, each character requires distinctiveness,\nsuch as different actions, clothing attributes, etc., to\nfacilitate the design of questions. Our data sources\nprimarily encompass three types: shot images of"}, {"title": "4.3 Evaluation Strategy", "content": "Quantitative evaluation for open-domain LVLMs\nhas always been challenging. Our MM-ID incor-\nporates four sub-tasks, each presenting its unique\nanswer format. For matching task, which resem-\nbles multiple-choice questions, accuracy is used\nas a metric. In localization task, we compare the\npredicted bounding box to the actual bounding box,\ncomputing the Intersection over Union (IoU) metric\nto measure accuracy. When IoU exceeds a thresh-\nold of 0.5, we consider the model recognizes in-\nstance identity accurately.\nFor question-answering and caption generation\ntasks, due to the open-ended nature of the generated\nresponses, it is challenging to employ rule-based\nevaluations. Hence, GPT-4 is used to score the re-\nsults. Under the condition of provided questions\nand correct answers, we design prompts to guide\nGPT-4 to focus on scoring the accuracy of charac-\nter roles, states, and activities within predictions.\nWe use both absolute and relative scoring strate-\ngies. Absolute scoring rates a model's prediction\ndirectly against the correct answer on a ten-point\nscale, while relative scoring pits two models' re-\nsults against each other, providing more immediate\ncomparative insights into the models' performance\nwith a fraction. The prompts used for GPT-4 scor-"}, {"title": "5 Experiments", "content": "5.1 Quantitative Results\nWe compare IDA-VLM with other open-source\nLVLMS (Zhao et al., 2023a; Ge et al., 2023; Bai\net al., 2023; Dong et al., 2024) and closed-source\nAPIs (Bai et al., 2023; Google, 2023a; OpenAI,\n2023b) on MM-ID. To meet the requirement of ID\nrecognition, the models selected for testing should\nsupport multiple images input. For each model,\nwe design appropriate prompts to guide them to re-\nspond according to character names. The prompts\nand training settings of our model can be found in\nAppendix C.\nAs illustrated in Table 1, our model achieves\nthe best performance on MM-ID. According to\nthe quantitative results, Gemini-pro exhibits best\nperformance on matching and Q&A sub-tasks\namong previous models. In the location sub-task,\nQwenVL-Chat gains the highest score, even higher\nthan other closed-source APIs. As for the caption\nsub-task, GPT-4V surpasses other models. Over-\nall, our model outperforms previous LVLMs by a"}, {"title": "5.2 Qualitative Comparison", "content": "As shown in Figure 5, we qualitatively compare the\nresults of IDA-VLM with GPT-4V and Gemini-pro.\nIn the first case, the model should determine which\nperson walks in front and match the person with\nthe given characters. Our model gives the right\nanswer 'Ariadne' while GPT-4V and Gemini-pro\nrecognizes inaccurately. In the second case, models\nare asked to provide a description for a movie scene\nwith three characters. GPT-4V describes that David\nand Elise sit at a table, ignoring Charlie standing.\nGemini-pro only gives a generic caption for the test\nimage without recognizing characters' identities"}, {"title": "5.3 Ablation Studies", "content": "Effect of Dual-stage instruction tuning. We re-\nport the separate effect of each instruction tuning\nstage in Table 3. When the second stage of tuning\nis removed, there is a significant drop in perfor-\nmance, suggesting that the second stage has a more\nsubstantial impact than the first. During the sec-\nond stage, the ID images are independent of the\ntest images, which enhances the quality of data for\nmodel learning. When the first and second stages\nof tuning are combined, our model achieves its\noptimal performance. It is worth noting that the\naccuracy of 'Matching' gets lower when adding\nthe first stage tuning, because the first stage tuning\ndata can't improve matching ability.\nEffect of ID-Former. We use ID-Former to project\nvisual features of ID images and test images to the\nsemantic space of LLM. As depicted in Table 4,\nsubstituting the ID-Former with a standard query\nformer leads to a reduction in all sub-task metrics."}, {"title": "6 Conclusion", "content": "In this paper, we focus on the capability to rec-\nognize and link instances across various scenes,\nwhich is significant for understanding complex vi-\nsual narratives, such as movies. We propose vi-\nsual instruction tuning with ID reference, which\nunleashes the potential of LVLM in ID recogni-\ntion, and develop an ID-aware LVLM, IDA-VLM.\nThe model memorizes the name and appearance of\neach character in ID reference, then recognize them\nacross disparate scenes correctly, linking charac-\nters across diverse images for accurate narrative\ninterpretation. To thoroughly assess the perfor-\nmance of instance recognition with ID reference,\na novel benchmark named MM-ID has been in-\ntroduced, which consists of four sub-tasks. Our\nmodel achieves best performance among previous\nmodels and closed-source APIs. Conclusively, this\nresearch contributes to broadening the horizons for\nfuture AI systems to efficiently understand multi-\nidentity visual content."}, {"title": "7 Limitations", "content": "As we employ QwenVL-chat as the baseline model\nfor our research, our proposed model inherits cer-\ntain limitations thereof. One such constraint is the\nupper limit on the number of input images. Within\na sample, the sum count of ID images and test im-\nages can not surpass eight. Consequently, we opt to\nuse keyframes as test images to effectively perform\ncomprehension of movie segments. By integrat-\ning our visual instruction tuning with ID reference,\nif we fortify the baseline, which is anticipated to\nenhance overall performance.\nOur model focuses on the capability to recog-\nnize and link instance IDs across various scenes\nfor accurate narrative interpretation. It should be\nacknowledged that during the visual instruction\ntuning with ID reference, there might be a slight\nreduction in the original capabilities of the model.\nHowever, our objective is to develop an ID-aware\nLVLM, rather than a versatile LVLM."}, {"title": "8 Ethical Considerations", "content": "Our method is related to person and instance ID, so\nwe pay much attention when collecting data. Our\ntuning data and benchmark data mainly come from\nMovieNet, which is a open-source movie dataset,\ncontaining numerous shot images of actors, neces-\nsitating careful consideration about the copyright.\nMoreover, there exists the potential risk that our\nmodel or dataset may be utilized by others to en-\ngage in activities concerning a particular individ-\nual."}, {"title": "A Visual Instruction Tuning Data Construction", "content": "We construct dual-stage instruction tuning data\nwith ID reference. The first stage data is con-\nstructed with predefined rules, cropping specific\ninstances from test images, while the second stage\ndata is generated by GPT-4V. Specifically, GPT-\n4V is utilized to produce four kinds of tuning\ndatasets: single-image caption, multi-image cap-\ntion, question-answer pair for a single image, and\nquestion-answer pair for multiple images. We\npresent prompts for producing data in Table 5 and\nTable 6. The constructed tuning data scale of two\nstages are shown in Table 7. We will produce more\ndata and open-source them.\nTo preserve the innate proficiency of the baseline\nmodel, we integrate the instruction tuning dataset\nfrom LLaVA with our compiled dataset. As de-\npicted in Table 8, we conduct an ablation study on\nthe proportion of LLaVA dataset. The capability\nfor 'Matching' reaches its best in the absence of\nLLaVA dataset, suggesting LLaVA data has no ben-\nefit for 'Matching'. The location score is highest\nat a 20% inclusion rate of LLaVA data. In contrast,\n'Q&A' and 'Caption' scores reach their optimal\nlevels when the inclusion rate is set at 10%. This\ncould imply that a judicious amount of LLaVA data\nserves to fortify foundational multimodal and text\ngeneration competencies. However, an excessive\ninfusion of LLaVA data seems to hinder the learn-\ning of ID recognition. Therefore, we select 10% as\nthe mixing rate of LLaVA data."}, {"title": "B MM-ID", "content": "The data composition of MM-ID is shown in Ta-\nble 9. During MM-ID construction, we annotate\nquestions and standard answers manually. The in-\nstructions for annotators are shown in Table 10.\nIt is a challenge to evaluate accuracy of the\nmodel on 'Q&A' and 'Caption' sub-tasks. We uti-\nlize GPT-4 to score the predictions under the con-\ndition of provided questions and correct answers.\nWe propose GPT-4 absolute and relative scoring\nstrategies with designed prompts, as depicted in\nTable 11.\nThe calculate process of relative score is as fol-\nlows: GPT-4 gives two scores (10 points) for pre-\ndictions from different models respectively. We\ncompute mean value of scores for each model, then\ncalculate the ratio of the average scores as the final\nrelative score."}, {"title": "C Experiment Settings", "content": "We use QwenVL-chat for model initialization,\nwhich has 9.6B parameters. In IDA-VLM train-\ning, we set learning rate as 1e-5 for the first stage\nand 5e-6 for the second stage. The model is trained\nfor 5 epochs in both the first and second stages.\nThe batch size with gradient accumulation is set to\n128. The visual encoder is fixed, while ID-Former\nand LLM are fine-tuned. It costs approximately 1\nday training on 8\u00d7A100-SXM-80GB for both the\nfirst stage and second stage tuning.\nTo promote the evaluation of models on MM-ID\ntasks, we craft specific prompts to instruct mod-\nels in accurately identifying character IDs. The\nprompts tailored for closed-source APIs are de-\ntailed in Table 12. and prompts for open-source\nmodels are designed similarly."}, {"title": "D More Visualization", "content": "In this section, we present more qualitative results\nof IDA-VLM, as shown in Figure 6, Figure 7, Fig-\nure 8, Figure 9."}]}