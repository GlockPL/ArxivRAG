{"title": "Point-LN: A Lightweight Framework for Efficient Point Cloud Classification Using Non-Parametric Positional Encoding", "authors": ["Marzieh Mohammadi", "Amir Salarpour", "Pedram MohajerAnsari"], "abstract": "We introduce Point-LN, a novel lightweight framework engineered for efficient 3D point cloud classification. Point-LN integrates essential non-parametric components such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN), and non-learnable positional encoding-with a streamlined learnable classifier that significantly enhances classification accuracy while maintaining a minimal parameter footprint. This hybrid architecture ensures low computational costs and rapid inference speeds, making Point-LN ideal for real-time and resource-constrained applications. Comprehensive evaluations on benchmark datasets, including ModelNet40 and ScanObjectNN, demonstrate that Point-LN achieves competitive performance compared to state-of-the-art methods, all while offering exceptional efficiency. These results establish Point-LN as a robust and scalable solution for diverse point cloud classification tasks, highlighting its potential for widespread adoption in various computer vision applications. For more details, see the code at: https://github.com/asalarpour/ Point_LN.", "sections": [{"title": "I. INTRODUCTION", "content": "In the field of computer vision (CV), the technology for processing two-dimensional images nears maturity [1] [2], and researchers are increasingly shifting their focus toward three-dimensional scenes, which more accurately represent real-world environments. Point clouds, consisting of large collections of tiny points in 3D space, play a crucial role in this process due to their rich informational content. These unordered sets of multidimensional points become meaningful when considered together, with point cloud classification being a key task in point cloud analysis. This task is essential across various domains, including security detection [3], [4] [5], target object detection [6], three-dimensional reconstruction [7], and autonomous vehicles [8]-[10].\nPoint cloud data, due to its inherent irregularity and sparsity, presents significant challenges for processing and analysis. Deep learning-based models for point cloud classification have gained widespread adoption due to their advantages in feature extraction and high classification accuracy. Early methods such as PointNet [11] and PointNet++ [12] introduced innovative frameworks for directly processing unordered point clouds, achieving remarkable performance. Subsequent approaches, including PointConv [13] and PointMLP [14], further advanced the field by incorporating more sophisticated feature aggregation mechanisms, leading to state-of-the-art results in various benchmarks.\nDespite their success, these parametric models have notable limitations. They often require large amounts of training data, substantial computational resources, and extensive hyperparameter tuning to achieve optimal performance. Moreover, their ability to generalize can be compromised in scenarios involving diverse datasets or varying conditions, limiting their adaptability in practice.\nTo address these challenges, non-parametric methods such as Point-NN [15] and Point-GN [16] have emerged as promising alternatives to parametric models in point cloud analysis. These methods eliminate the need for extensive parameter optimization and training, instead relying on techniques like nearest-neighbor search and similarity-based approaches. Their simplicity, flexibility, and ability to perform inference across diverse data distributions make them particularly suitable for scalable and robust solutions in tasks such as object detection and 3D reconstruction.\nHowever, despite their adaptability, non-parametric methods often struggle to achieve high accuracy in complex scenarios. To address these limitations, we propose a novel hybrid framework, Point-LN, which combines the strengths of non-parametric techniques with a learnable classifier. This approach improves classification accuracy while maintaining reduced reliance on learnable parameters, as illustrated in Figure 1. The proposed framework enhances versatility and effectiveness, making it highly applicable to real-world tasks like object detection and 3D reconstruction.\nIn this paper, we introduce an efficient network for point cloud classification that effectively captures and adaptively aggregates multivariate geometric features, without relying on learnable parameters for feature extraction. Additionally, we incorporate a classifier that uses a minimal number of learnable parameters, thereby enhancing both efficiency and performance.\nThe primary contributions of this paper are as follows:\n\u2022 We present a parameter-efficient derivative of non-parametric methods, delivering exceptional performance without relying on complicated operators.\nWe incorporate a learnable classifier that effectively maps the extracted features to target categories, allowing for fine-tuning and improved classification performance.\n\u2022 Despite its simple design philosophy, our approach demonstrates strong performance in 3D point cloud classification, achieving competitive results on popular benchmarks such as ModelNet40 [17] and ScanObjectNN [18]."}, {"title": "II. RELATED WORKS", "content": "Methods for 3D point cloud classification can be broadly categorized into Projection-based and Point-based approaches. The following subsections review these two categories in detail and discuss their key techniques, advantages, and limitations.\nA. Projection-based methods\nProjection-based methods address the irregularity and sparsity of point clouds by transforming them into grid-like representations, such as multi-view depth maps [19] [20] and 3D voxels [21] [22]. By utilizing sets of 2D views, which contain extensive information for recognizing 3D shapes, these methods, like MVCNN [23], use rendered 2D images to identify 3D objects. Similarly, VoxNet [24] introduced a convolutional neural network (CNN) architecture that represents 3D data using a volumetric occupancy grid. While effective in processing 3D data, projection-based methods have limitations. Transforming point clouds into 2D or voxelized grids can result in the loss of fine-grained spatial relationships, potentially affecting classification performance, especially for complex tasks that require preserving precise geometric details.\nB. Point-based Methods\nRecent advances have focused on directly processing point clouds without transforming them into grids. Point-based methods can be broadly categorized into Parametric (deep learning-based) and Non-parametric methods, as follows:\n1) Parametric Methods: They typically rely on deep learning techniques and can be subdivided into several groups:\n(i) Convolution-based methods [13], [25]-[27] use convolutional neural networks (CNN) to extract spatial features from point clouds. By applying convolutions over local neighborhoods of points, these methods capture spatial hierarchies and local geometric structures. This technique is highly effective for tasks such as classification, segmentation, and object recognition. For example, PointNet [11] and PointNet++ [12] employ convolutional layers to learn point-wise features, aggregating them to form a global representation of the point cloud. These models excel at handling unordered, irregular point clouds, making them suitable for a variety of 3D vision tasks.\n(ii) Graph-based methods [28], [29] treat point clouds as graphs, where each point is represented as a node, and edges denote relationships or distances between points. This representation is particularly effective for capturing connectivity and local structures within the point cloud. Notably, Dynamic Graph CNN (DGCNN) [29] dynamically constructs graphs during training, allowing the model to learn both local and global features. This dynamic graph construction improves the model's ability to capture complex geometric relationships, enhancing performance on tasks like classification and segmentation.\n(iii) Transformer-based methods [30] [31] leverage self-attention mechanisms to process point clouds, allowing the model to weigh the importance of different points based on their spatial relationships. This ability to capture long-range dependencies is particularly useful for tasks where global context is crucial. For example, Point Transformer [31] introduces ScorNet, a learning score-based focus module.\nPoint Transformer first extracts local and global features and then uses ScorNet to rank these local features. A local-global attention mechanism is applied to associate the ranked local features with global features, allowing the model to focus on the most relevant features for classification and segmentation. While parametric models have demonstrated high accuracy, they often come with high computational complexity, limiting their applicability in real-time scenarios.\n2) Non-parametric Methods: These methods, unlike parametric ones, do not require training or parameter tuning, which eliminates much of the computational overhead. These methods focus on directly capturing geometric features from point clouds, making them computationally efficient and adaptable to diverse data without the need for extensive training.\n(i) Point-NN [15] leverages non-parametric techniques to effectively capture geometric features by focusing on local neighborhood information. By minimizing reliance on learnable parameters, Point-NN enhances adaptability while maintaining simplicity.\n(ii) Point-GN [16] Similarly builds on these concepts by introducing a Gaussian embedding function that improves classification accuracy without adding significant computational overhead. Despite their simplicity and flexibility, non-parametric methods often fall short of the high accuracy levels seen in parametric models, especially for complex classification tasks. This trade-off between simplicity and performance underscores the need for further advancements in the field.\n(iii) Point-PN [15] effectively combines the strengths of both parametric and non-parametric methods to bridge this gap. By integrating a conventional learnable classifier and enhancing the embedding with parametric linear layers, Point-PN captures higher-level spatial patterns while maintaining simplicity. Additionally, it employs trigonometric positional encoding to better represent geometric relationships within the point cloud, enabling the model to extract rich spatial features. This approach demonstrates that a powerful and efficient 3D model can be developed from a non-parametric framework without relying on complex operators or an excessive number of parameters."}, {"title": "III. METHOD", "content": "In this section, we first introduce the fundamentals of 3D point clouds and review common classification techniques. Next, we explore the application of non-parameteric positional encodings in lightweight methods and how they enhance the processing of point cloud data. We then outline the architecture of our proposed network Point-LN, which consists of two key components: the feature encoder and the classifier. The feature encoder transforms the raw input point cloud into high-dimensional feature representations, while the classifier maps these encoded features to the target label space, producing classification logits. A detailed overview of the architecture is provided in Figure 2.\nA. Background\nPoint cloud classification aims to assign a label to an entire point cloud, or to specific regions within the point cloud, allowing for the identification of object characteristics. A 3D point cloud is a set of points $P = \\{P_1,P_2,...,P_N\\}$, where each point $p_i$ is represented by its 3D coordinates $(x_i, y_i, z_i)$, and optionally additional features $\\gamma(p_i) \\in \\mathbb{R}^{C \\times 3}$, where C is the feature dimension. The goal is to process this input point cloud and classify it into one of several predefined categories. The classification process consists of two main steps: the feature encoder and the classifier. First, the feature encoder extracts meaningful representations from the raw point cloud by aggregating spatial and geometric information across points. The encoded feature vector F represents the output of the feature encoder, which processes the input point cloud:\n$F = Encoder(\\{p_i\\}_{i=1}^N)$ (1)\nThe second step, the classifier, takes F as input to predict the point cloud's category, typically using fully connected layers followed by a softmax activation. The classification output y is given by:\n$y = Softmax(Classifier(F))$ (2)\nB. Positional Encoding\nPositional encoding (PE), first introduced in the transformer architecture [32], has proven successful in various domains such as natural language processing (NLP) and CV. For point cloud processing, positional encoding captures essential spatial relationships between points, enabling the model to understand their relative positions in 3D space. We explore two types of positional encoding commonly used in point cloud processing.\n(i) Trigonometric Positional Encoding (TPE)\nPoint-NN [15] employs TPE, taking advantage of the periodic nature of sine and cosine functions. This method effectively captures the spatial relationships between points in 3D space. For a point $p_i$ = ($x_i$, $y_i$, $z_i$), trigonometric functions are used to generate a $C_1$-dimensional positional embedding. The encoding for each axis is defined as follows:\n$\\begin{aligned}\n\\text{PETPE} \\begin{cases}\n y_x(x_i, 2n) = sin(\\alpha x_i /\\beta)\\\\\n y_x(x_i, 2n + 1) = cos(\\alpha x_i /\\beta)\\\\\n y_y(y_i, 2n) = sin(\\alpha y_i /\\beta)\\\\\n y_y(y_i, 2n + 1) = cos(\\alpha y_i /\\beta)\\\\\n y_z(z_i, 2n) = sin(\\alpha z_i /\\beta)\\\\\n y_z(z_i, 2n + 1) = cos(\\alpha z_i /\\beta)\n\\end{cases}\n\\end{aligned}$ (3)\nwhere $C_1$ is the initial feature dimension, $\\alpha$ controls the scale, and $\\beta$ governs the wavelength. The final positional encoding for each point is given by:\n$\\gamma^{TPE}(p_i) = Concat(y_x(x_i); y_y(y_i); y_z(z_i)) \\in \\mathbb{R}^{1 \\times C_1}$ (4)\n(ii) Gaussian Positional Encoding (GPE) GPE, used in Point-GN [16], enhances spatial awareness by encoding positional information into the feature representation without requiring learnable parameters. GPE uses a Gaussian function to map the raw point coordinates into higher-dimensional space, preserving both local and global spatial information. For each axis, the encoding is defined as:\n$\\begin{aligned}\n\\text{PEGPE} \\begin{cases}\n y_x(x_i, v_j) = exp(-\\frac{||x - v_j||^2}{2\\sigma^2})\\\\\\\n y_y(y_i, v_j) = exp(-\\frac{||y - v_j||^2}{2\\sigma^2})\\\\\\\n y_z(z_i, v_j) = exp(-\\frac{||z - v_j||^2}{2\\sigma^2})\n\\end{cases}\n\\end{aligned}$ (5)\nwhere $v_j$ represents reference points and $\\sigma$ is the standard deviation that influences the scale of local versus global features. The final positional encoding for each point is the concatenation of embeddings across all axes:\n$\\gamma^{GPE}(p_i) = [y_x(x_i, v_j), y_y(y_i, v_j), y_z(z_i, v_j)]_{j=1}^V$ (6)\nwhere V is the number of reference points along each axis.\nC. Lightweight Feature Encoder\nWe provide a lightweight feature encoder, and to better extract multi-scale hierarchy, we append simple linear layers to each stage of the encoder. In the following sections, we will outline the four steps of this feature encoder.\n1) Initial Embedding\n\u03a4\u03a1\u0395. To perform feature embedding we utilize TPE, which transforms the Cartesian coordinates of each point $p_i \\in \\mathbb{R}^d$ into a trigonometric representation, preserving the spatial relationships and helping the model maintain a consistent understanding of geometry. The resulting embeddings, $\\gamma^{TPE}(p_i)$, encapsulate geometric features derived from the point cloud.\nLinear Layer. The trigonometric embeddings $\\gamma^{TPE}(p_i)$ are then passed through a linear layer, which performs a learnable linear transformation. This operation is defined as:\n$f_i = Linear(\\gamma^{TPE}(p_i)) = W \\cdot \\gamma^{TPE}(p_i) + b$,\nwhere W is a learnable weight matrix, b is a bias vector, and $f_i$ represents the output embedded features for point $p_i$. This step embeds the TPE-transformed features, enabling the model to capture more complex patterns in the input data.\nFinal Output. The output of the initial embedding process includes the original points and their associated embedded features, represented as:\n$\\{p_i, f_i\\}_{i=1}^N$\nwhere $p_i$ are the original Cartesian coordinates of the points, and $f_i = Linear(\\gamma^{TPE}(p_i))$ are the embedded features derived from the TPE-transformed coordinates.\n2) Local Grouper\nSampling Layer. At each stage, the input point cloud and associated features from the previous stage is represented as $\\{p_i, f_i\\}$, the features could be TPE from intial embed step or GPE for different stages. To reduce computational complexity while retaining the global structure of the point cloud, we perform Farthest Point Sampling (FPS). This step selects a subset of N/2 points that are maximally spaced apart, ensuring they represent the center of the point cloud:\n$\\{p_j, f_j\\}_{j=1}^{N/2} = FPS(\\{p_i, f_i\\}_{i=1}^{N})$ (7)\nGrouping Layer. We then group the points into local neighborhoods using the K-Nearest Neighbors (KNN) algorithm. This step allows us to capture local geometric patterns:\n$idx_j = KNN(p_j, p_i)$ (8)\nwhere idxj represents the indices of the K nearest neighbors for point $p_j$. The retrieved coordinates and features are:\n$P_j = retrieve(\\{p_i\\}_{i=1}^{N}, idx_j) \\in \\mathbb{R}^{K \\times 3}$ (9)\n$F_j = retrieve(\\{f_i\\}_{i=1}^{N}, idx_j) \\in \\mathbb{R}^{K \\times (3 \\times 3)}$ (10)\nwhere, $P_j$ denotes the gathered coordinates, and $F_j$ represents the gathered features for the point $p_j$. The retrieved coordinates $P_j$ and features $F_j$ are then normalized using the mean and standard deviation of each point's neighbors. These normalized coordinates and features are subsequently forwarded to the next stage for further processing.\n3) Local Geometry Aggregation\nGPE Aggregation. The features from the Local Grouper are then fed into the Local Geometry Aggregation, Here, we use GPE to extract meaningful spatial information, that helps encode global and local geometric patterns, ensuring a rich feature representation without the need for learnable parameters. The spatially encoded features are combined with the neighborhood features through element-wise multiplication:\n$F_j - F_j + \\gamma(P_j) \\copyright \\gamma(P_j)$ (11)\nLinear Layer. Here, we insert two learnable linear layers for each stage, positioned right before and after the GPE Aggregation step. This addition aims to capture higher-level spatial information. Specifically, by placing these layers strategically, we enhance the encoder's ability to process and represent geometric features effectively.\n4) Pooling\nNeighbor Pooling. To aggregate local features, we use both mean and max pooling, ensuring that the final features are permutation-invariant and capture local geometry from multiple perspectives:\n$\\Phi_j = Mean(F_j) + Max(F_j), \\forall j \\in \\mathbb{Z}^k$ (12)\nwhere $Mean(F_j)$ and $Max(F_j)$ are permutation-invariant operations. These operations capture neighboring features from multiple perspectives and ensure that the order of neighbors has no effect on the final pooled features.\nAggregation Across Stages. The Lightweight feature encoder includes four stages, each producing pooled features $\\Phi$. After processing through all stages, global pooling is performed on the results from each stage. The final feature vector F for the lightweight feature encoder is obtained by concatenating the global mean and max features from all four stages:\n$F = [Mean(\\Phi) + Max(\\Phi)]_{s=1}^4$ (13)\nwhich captures and aggregates spatial and feature information across multiple levels by integrating the mean and max features from each stage.\nD. Classifier Architecture\nThe classifier maps the high-dimensional feature space, produced by the lightweight encoder, directly to the target label space. This mapping is performed by a simple neural network, which generates the output logits. The predicted probabilities are then obtained using the softmax function:\n$y = Softmax(Classifier(F))$, (14)\nwhere Classifier(\u00b7) represents the transformation from the feature space F to the logits.\nThe final predicted class is determined by selecting the class with the highest probability:\n$c = arg max(y)$. (15)"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate the performance of our method for point cloud classification through experiments conducted on two well-known datasets: ModelNet40 [17] and ScanObjectNN [18]. ModelNet40 consists of clean, synthetic 3D models, providing a controlled environment for assessing classification performance on ideal data. In contrast, ScanObjectNN presents a more challenging scenario with real-world 3D data that includes occlusions, clutter, and background noise, thus offering a more rigorous test of the robustness and versatility of our approach.\nTo thoroughly evaluate our method, we compare it against several state-of-the-art techniques, including both fully trained models and non-parametric methods. Specifically, we benchmark our approach against PointMLP [14], a recent fully trained model that has demonstrated impressive results in point cloud classification tasks. Additionally, we assess the performance of non-parametric methods such as Point-NN [15] and Point-GN [16], which have shown promising results in similar classification settings. Through these comprehensive comparisons, we aim to highlight the strengths of our method, demonstrating its effectiveness in handling both synthetic and real-world data, while also emphasizing its competitive edge in the context of fully trained and non-parametric models.\nA. Experimental Setup\nWe evaluate the performance of Point-LN on a system equipped with an NVIDIA RTX 4090 GPU. Although Point-LN is designed to be lightweight, the use of the high-performance GPU significantly accelerates both the training and inference processes. This allows for efficient model evaluation and rapid experimentation, particularly when dealing with large-scale and complex 3D datasets such as ModelNet40 [17] and ScanObjectNN [18].\nWhile Point-LN does not require a vast amount of training resources, the GPU's computational power enables faster convergence and efficient handling of the diverse real-world data. This ensures that we can conduct comprehensive benchmarking, rapidly assess the model's performance, and explore various hyperparameter configurations. The use of this hardware ensures our method can be evaluated effectively across both synthetic and real-world datasets, highlighting its strengths in terms of efficiency and scalability.\nB. Dataset Details\nThe ModelNet40 [17] dataset consists of 12,311 CAD models across 40 object categories, split into 9,843 samples for training and 2,468 for testing. This dataset is widely used for point cloud classification due to its clean, synthetic nature, providing a controlled environment for benchmarking.\nIn contrast, the ScanObjectNN [18] dataset presents a more challenging real-world scenario, with 2,902 samples across 15 object categories. Objects in ScanObjectNN are often occluded, cluttered, or contain background noise, providing a closer simulation to real-world 3D data. The dataset is divided into three official subsets: OBJ-BG, which contains objects with background noise, OBJ-ONLY, with objects without background, and PB-T50-RS, featuring partial occlusions and transformations. These subsets test the robustness of models under various degrees of complexity. For both datasets, we follow the common practice of sampling 1,024 points from each object, as used in prior works (e.g., PointNet++ [12], DGCNN [29]). Our model combines maximum pooling and average pooling to enhance feature aggregation, inspired by DGCNN [29].\nC. Shape classification on ModelNet40\nTable I evaluates the performance of Point-LN on the ModelNet40 [17] dataset. It achieves an accuracy of 94.0%, which is comparable to the best performing model, PointMLP [14], with an accuracy of 94.1%. However, PointMLP has a significantly larger model size (12.6M parameters). In contrast, our approach demonstrates the ability to capture both local and global geometric features while maintaining minimal model complexity, with only 0.8 million parameters. When compared to other methods, Point-LN shows competitive performance with models such as PointNet [11] (89.2%) and PointNet++ [12] (90.7%), while being much more lightweight.\nFor instance, Point-LN achieves a higher accuracy than Point-GN [16] (85.3%) and Point-NN [15] (81.8%), both of which have zero parameters but show lower classification accuracy. This demonstrates the effectiveness of Point-LN in extracting meaningful features with only a small number of parameters. Additionally, Point-LN ensures high efficiency for real-time applications. Despite its compact size, it provides performance on par with larger parametric models like PointMLP, but with substantially reduced computational overhead. This combination of competitive accuracy and exceptional efficiency makes Point-LN an ideal choice for resource-constrained environments, where both real-time performance and minimal model complexity are crucial.\nD. Shape classification on ScanObjectNN\nWhile the ModelNet40 [17] dataset is a well-established benchmark for point cloud analysis, its synthetic nature may limit the evaluation of methods under more challenging, real-world conditions. To address this, we also, we evaluate the performance of Point-LN on the challenging ScanObjectNN [18] benchmark, presenting our results in Table II. Point-LN achieves state-of-the-art accuracy across all subsets, with 92.2% on OBJ-BG, 92.1% on OBJ-ONLY, and 91.7% on the most challenging PB-T50-RS subset. These results highlight the robustness of Point-LN in handling real-world 3D data with noise, occlusions, and clutter.\nCompared to parametric methods, Point-LN delivers substantial accuracy improvements while maintaining a lightweight architecture. For instance, it outperforms PointNet [11] by +18.9% on OBJ-BG and PointNet++ [12] by +9.9% on OBJ-ONLY. Furthermore, Point-LN matches the performance of the larger PointMLP [14] (91.7% vs. 85.4% on PB-T50-RS) while requiring only 0.8 million parameters, compared to PointMLP's 12.6 million. Similar to non-parametric methods such as Point-NN [15] and Point-GN [16], Point-LN leverages positional encoding for feature extraction without relying heavily on trainable parameters.\nHowever, Point-LN introduces a novel lightweight architecture that significantly enhances performance, achieving a +10.3% improvement over Point-GN on PB-T50-RS while maintaining a small parameter count. These results demonstrate the effectiveness of Point-LN in combining the strengths of non-parametric techniques with enhanced feature extraction capabilities. Its ability to deliver high accuracy with minimal computational overhead makes Point-LN highly suitable for resource-constrained environments and real-time applications."}, {"title": "V. CONCLUSION", "content": "In this paper, we presented Point-LN, a lightweight framework tailored for efficient 3D point cloud classification. By integrating essential non-parametric components-such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN), and non-learnable positional encoding-with a streamlined learnable classifier, Point-LN achieves high classification accuracy with minimal parameters. This design ensures both computational efficiency and rapid inference, making it suitable for real-time and resource-constrained applications. Our experiments on benchmark datasets, ModelNet40 and ScanObjectNN, demonstrate that Point-LN matches or outperforms state-of-the-art methods while maintaining a significantly smaller model size.\nThe framework's robustness and adaptability are evident in its ability to handle diverse and noisy real-world 3D data effectively. Future work will explore enhancing Point-LN by incorporating additional geometric features and extending its application to more complex 3D tasks such as semantic segmentation and object detection. These advancements aim to further improve scalability and performance, solidifying Point-LN's role as a versatile tool in 3D point cloud analysis. In summary, Point-LN offers a balanced solution that combines high accuracy with exceptional efficiency, positioning it as a promising framework for various computer vision applications involving 3D point clouds."}]}