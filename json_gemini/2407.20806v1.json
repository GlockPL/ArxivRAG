{"title": "ARCLE: THE ABSTRACTION AND REASONING CORPUS\nLEARNING ENVIRONMENT FOR REINFORCEMENT LEARNING", "authors": ["Hosung Lee", "Sejin Kim", "Seungpil Lee", "Sanha Hwang", "Jihwan Lee", "Byung-Jun Lee", "Sundong Kim"], "abstract": "This paper introduces ARCLE, an environment designed to facilitate reinforcement learning re-\nsearch on the Abstraction and Reasoning Corpus (ARC). Addressing this inductive reasoning bench-\nmark with reinforcement learning presents these challenges: a vast action space, a hard-to-reach\ngoal, and a variety of tasks. We demonstrate that an agent with proximal policy optimization can\nlearn individual tasks through ARCLE. The adoption of non-factorial policies and auxiliary losses\nled to performance enhancements, effectively mitigating issues associated with action spaces and\ngoal attainment. Based on these insights, we propose several research directions and motivations for\nusing ARCLE, including MAML, GFlowNets, and World Models.", "sections": [{"title": "INTRODUCTION", "content": "We introduce ARCLE (ARC Learning Environment) as a reinforcement learning (RL) environment designed for the\nAbstraction and Reasoning Challenge (ARC) benchmark (Chollet, 2019a). This benchmark assesses agents' ability to\ninfer rules from given grid pairs and predict the outcome for a test grid, as illustrated in Figure 1. ARC is designed to\ntest abstraction and reasoning skills, making it a touch benchmark within the domain. Despite various attempts to con-\nquer ARC's complexities through program synthesis and reasoning using large language models, RL-based approaches\nare surprisingly rare (Section 2.1). We believe this scarcity is due to the lack of a dedicated RL environment tailored\nfor ARC. To fill this gap, we created ARCLE based on Gymnasium (Towers et al., 2023) to tackle the benchmark."}, {"title": "RELATED WORKS", "content": ""}, {"title": "SOLVING ARC", "content": "Since the unveiling of the ARC (Chollet, 2019a), approaches ranging from the development of similar benchmarks (Qi\net al., 2021; Kim et al., 2022; Xu et al., 2023) to domain-specific languages and program synthesis (Banburski et al.,\n2020; Acquaviva et al., 2022; Assouel et al., 2022; Alford et al., 2021; Witt et al., 2023; Ainooson et al., 2023) have\nbeen explored to extend its applicability and enhance learning strategies. These efforts have deepened our understand-\ning of ARC's challenges, highlighting the complexity of devising comprehensive solutions. The recent shift towards\nleveraging Large Language Models (LLMs), incorporating strategies from natural language processing to detailed task\ncontext integration (Camposampiero et al., 2023; Xu et al., 2024; Moskvichev et al., 2023; Mitchell et al., 2024; Lee\net al., 2024), underscores LLMs' potential in addressing ARC's intricacies.\nHowever, the performance of research on the ARC utilizing program synthesis and LLMs has not fully met expec-\ntations, often due to its logical flaw (Lee et al., 2024). This has prompted a pivot towards reinforcement learning as\na novel approach, albeit its application to ARC has been limited. Notable attempts include using RL strategies in\nprogram synthesis (Butt et al., 2024) and exploring imitation learning (Park et al., 2023). The introduction of ARCLE\nopens up new possibilities for advancing research on the ARC using RL."}, {"title": "RL ENVIRONMENTS SIMILAR TO ARCLE", "content": "Among the myriad RL environments, those featuring a vast action space similar to ARCLE's are prominently observed\nin game-based settings, such as PySC2 (Vinyals et al., 2017), where the diversity of actions, determined by mouse\nclick locations, mirrors the flexible action format in ARC. Similarly, environments designed for recommendation\nsystems (e.g., RecSim, RecoGym) and complex multi-step planning tasks (e.g., Super Mario Bros (Kauten, 2018),\nNLE (K\u00fcttler et al., 2020)) may not exhibit wide action spaces at each state but encapsulate the challenge of hard-to-\nreach goal through the necessity of sequential decision-making to achieve success. In parallel, the breadth of tasks\nwithin ARCLE resonates with the diverse objectives found in robotics environments like Meta-World (Yu et al., 2020),\nRLBench (James et al., 2020), and CALVIN (Mees et al., 2022), underscoring the complexity and variety of tasks\nthat ARCLE introduces to RL research."}, {"title": "ARCLE: ARC LEARNING ENVIRONMENT", "content": "ARCLE is a reinforcement learning (RL) environment package, implemented in Gymnasium (Towers et al., 2023),\ndesigned for RL approaches on Abstraction and Reasoning Corpus (ARC). RL agents on the ARCLE environments\nlearn to solve tasks by selecting actions to edit the grid (to be submitted) to the environment state. As Figure 2\nillustrates, ARCLE comprises three main components: envs, loaders, actions, and auxiliary wrappers which modify\nthe environment's action or state space. The following explanation is based on the terms in Table 1.\nThe envs component consists of a base class of ARCLE environments, and its three derivatives. AbstractARCEnv\ninherits Gymnasium's Env class to provide reinforcement learning environment features and defines the ARC-specific\nstructure of action and state space and user-definable methods. Its implementations, O2ARCEnv, ARCEnv and\nRawARCEnv provide embodied action and observation spaces. 02ARCEnv constructs the state and action space\naccording to the O2ARC interface (See Appendix A.3), and likewise, ARCEnv offers the testing web interface devel-\noped by Chollet (2019b). RawARCEnv restricts the action space to color modifications or grid size changes, providing\na more constrained and monotonic learning environment."}, {"title": "ACTIONS", "content": "Actions in ARCLE are defined to enable editing of the output grid for a given task, consisting of operation and\nselection. operation represents an integer that specifies the method of editing (functions contained in the\nactions component in Figure 2), and selection is a binary mask that denotes the area of the grid affected by the\nedit.\nBy defining ARCLE's actions through operation and selection as illustrated by the action in the middle of\nFigure 3, we have standardized various types of actions within the same structure. Notably, the actions in ARCLE\ncan affect a single pixel, contiguous multiple pixels, or even non-contiguous pixels, accommodating these possibil-\nities through employing the binary mask selection. Furthermore, by separating operation and selection, it\naccommodates the possibility of determining selection conditioned by the chosen operation autoregressively."}, {"title": "STATES & OBSERVATIONS", "content": "All environments included in ARCLE are designed with the assumption to be Markov Decision Processes (MDP).\nTherefore, every parameter used in changing the environment's state is given to agents in the environment, so obser-\nvations and states can be considered equivalent. The basic state space of an environment within ARCLE consists of\nthe input and grid. Input represents the test input grid of an ARC task, so it is fixed unless a new task is assigned\nto an environment. Grid is initially set as the test input grid of a task, and an agent edits this by selecting actions.\nDepending on which operations an environment adopts, the state of the environment can be different. For instance,\nif an environment includes Copy operation, the environment should include additional variables of the copied part:\nclip. Hence in O2ARCEnv, more variables are included in the state, to support Copy and object-oriented opera-\ntions such as Move. These object-oriented actions from the O2ARC interface are supplemented with selected,\nobject, object pos and background. Descriptions of these variables are depicted in 1. While the agent per-\nforms object-oriented operations in a row, object and background works as two layers; object is overlayed on\nthe background at object-pos. For the detailed mechanism described in Section A.5."}, {"title": "REWARDS", "content": "The built-in reward currently offered in ARCLE is the sparse reward. This reward grants 1 when the agent performs\nthe submit action and the state space's grid exactly matches the task's answer grid, and 0 if even a single pixel\ndiffers. This sparse reward approach can hinder the learning of an agent whose total reward sum remains 0 as there is\na unique answer per task. To counteract this, an auxiliary reward was designed and utilized in the subsequent Section\n4.1. This auxiliary reward adds a penalty term based on the ratio of the number of incorrect pixels to the total pixels,\nguiding the agent to learn in a direction that minimizes the number of pixels differing from the correct grid. Identifying\na reward setting superior to this auxiliary reward setup, i.e., one that can be universally applied across all ARC tasks\naware environment's action space (e.g., object-oriented operations), requires further research."}, {"title": "SOURCE CODE", "content": "Since the environments in ARCLE implemented based on Gymnasium (Towers et al., 2023) and are fully written in\nPython3, users who have used Gymnasium or its predecessor, OpenAI Gym (Brockman et al., 2016), can use it with\nfamiliarity. ARCLE is released on GitHub\u00b2 under the terms of the Apache-2.0 License, as well as uploaded to the PyPI\n(Python Package Index), so the ARCLE can be easily installed by the pip command.3 Without modifying the source\ncode, one can still create custom ARCLE-based environments by subclassing provided environments in ARCLE or\nwrapping with the wrapper classes. Please note that ARCLE is currently being continuously updated, so users may\nneed to check the version. In this paper, our descriptions and experiments are based on version 0.2.5."}, {"title": "API & SAMPLE USAGE", "content": ""}, {"title": "ARCLE BENCHMARKS", "content": "This chapter explains the process through which an agent learns to solve synthetic tasks using the ARCLE environment.\nTo ultimately solve ARC, the agent must acquire the ability to tackle unseen tasks through the learning process of tasks\nprovided in the training dataset. We speculate that approaches like meta-RL, generative models (e.g. GFlowNet), and\nmodel-based RL algorithms (e.g. World Models) may be necessary to solve tasks not observed during training. As\na preliminary step, we describe the initial results of learning an individual task. The PPO-based agent learns the\ninput/output grid pairs presented in one of the ARC tasks. If a method can be designed for the agent to understand and\nlearn from these tasks, we anticipate that it could be trained to solve unlearned tasks using the approaches mentioned\nabove with this agent."}, {"title": "SOLVING ARC WITH A GIVEN ANSWER: HANDLING THE LARGE DISCRETE STATE-ACTION SPACE", "content": "While we expect ARCLE agents to be better at imitating the cognitive process of human problem-solving, training\nan RL agent for ARCLE itself additionally becomes a difficult challenge due to its large discrete state-action space.\nIn this Section, we demonstrate the difficulty of obtaining highly performant agents within an ARCLE environment\neven when the state-action spaces are simplified and the answers are given, and we propose ARCLE-specific auxil-\niary loss functions and network architectures that can significantly improve agents' performance. Specifically, we use\noperations of 0\u20139 only with rectangular-shaped selection only (in a bounding box representation), and conse-\nquently, the sufficient information for decision making (i.e., the state s) becomes (grid, grid_dim, answer,\nanswer_dim) as we additionally assume answers to be given. We expect the methods introduced here to be used to\nhelp train ARCLE agents for the original ARC, where the answers are not provided and state-action spaces are more\ncomplex."}, {"title": "Proximal Policy Optimization (PPO)", "content": "We employed the well-known PPO algorithm (Schulman et al., 2017) to\ntrain the agents to solve ARCLE with the answers given. Due to the poor generalization ability (Kumar et al., 2021)\nand learning instability of value-based RL algorithms, recently, PPO has been widely adopted for tuning large neural\nmodels (Stiennon et al., 2020; Ouyang et al., 2022). It is an on-policy policy-gradient algorithm that aims to perform\na gradient update within the trust region. We gather trajectories and construct a dataset $D = \\{(s_i, a_i, R_i)\\}i$ consisting\nof state, action, and returns (sum of discounted rewards starting from the state). Then, the policy is updated according\nto the following losses ($L = L_{Baseline} + L_{PPO}$) with samples from D:\n$L_{Baseline}(V) = E_D [ (r - V_\\theta(s))^2 ]$\n$L_{PPO}(\\theta) = E_D [min(\\frac{\\pi_\\theta(a|s)}{\\pi_{old}(a|s)} (R - sg[V_\\varphi(s)]), clip(\\frac{\\pi_\\theta(a|s)}{\\pi_{old}(a|s)}, 1-\\epsilon, 1+\\epsilon) (R - sg[V_\\varphi(s)]))]$\nwhere $V$ is a value function that works as a baseline that reduces the gradient variance, $\\pi_{old}$ is a policy used to gather\nthe trajectories, and $sg[]$ is a stop-gradient operator. $r \\in [\u22121,0]$ is a reward from a dense reward function that\npenalizes the agent by the ratio of incorrect pixels of the next state."}, {"title": "LEARNING BETTER REPRESENTATION THROUGH AUXILIARY LOSS FUNCTIONS", "content": "Using an auxiliary loss function to predict important information has been a widely used approach for better gener-\nalizable representation and faster training (Jaderberg et al., 2016; Lample & Chaplot, 2017). We experimented three\ndifferent auxiliary losses, (1) $L_{rt\u22121}$ predicting the previous reward $r_{t\u22121}$ from the current state $s_t$, (2) $L_{rt}$ predicting\nthe current reward $r_t$ from the current state-action $(s_t, a_t)$, and (3) $L_{st+1}$ predicting the next state $s_{t+1}$ from the current\nstate-action $(s_t, a_t)$. All three functions are deterministic, and they are highly informative as they are correlated to\neither the value function or the action-value function. For policy architecture, we used the color-equivariant policy\narchitecture that will be detailed in Section 4.1.2.\nWhile the first auxiliary loss $L_{rt_1}$ can be easily adopted by additionally training a feed-forward network on top of the\nextracted state feature from the special token of $V_\\theta$, the other two auxiliary losses require the state-action feature that\nis not utilized in conventional PPO. We compute the state-action feature by performing a forward propagation again\nwith additional action embedding tokens after sampling an action from a policy. The prediction for the loss $L_{rt}$ is\ndone on top of the last action token that embeds $selection$, and the prediction for the loss $L_{st+1}$ is done on top of\ntokens that represent each pixel of $grid$."}, {"title": "NON-FACTORIZABLE POLICY ARCHITECTURE", "content": "It can be observed that the two main components of the action space of ARCLE, operation and selection, are\nintertwined with each other and cannot be separately decided. For example, the optimal selection for coloring a\npixel, or rotating an object will be completely different. This observation shows that the considerate choice of policy\narchitecture is necessary, as conventional factorized policy assuming $(operation | selection) | s$ will have\nlimited expressivity in representing such complex multimodal distributions. For all experiments in this section, we\nused all three auxiliary losses introduced in Section 4.1.1.\nTo demonstrate the expressivity of different policy architectures, we experimented with the following three architecture\ntypes: (1) Non-sequential policy assumes $(operation | selection) | s$. This policy is implemented by using\ntwo special tokens for operation and selection with two feed-forward networks on top of extracted features\nfrom those tokens. (2) Sequential policy does not assume conditional independence, by making the decision of\nselection dependent on sampled operation, similar to the RNN policy of Vinyals et al. (2019). This policy\nrequires two forward passes to sample an action, one for sampling operation from its special token and one for\nsampling selection from the token embedding the sampled operation, and therefore it is more computationally\ndemanding.\nOn the other hand, we also experimented (3) Color-equivariant policy that takes advantage of the ARCLE task that\nthe same permutation of colors applied to the task and the policy coloring actions results in the equivalent task, i.e.,\nthe color equivariance. We can achieve color equivariance of the policy by using several special tokens for policy\nequal to the number of operation to represent them, and by setting a special token of color-related operation\nas a function of color embedding used to represent grid. We can then use two different feed-forward networks on\ntop of extracted features of these operation tokens. One gives scalar output per token to be used as logits for\ndeciding the operation. The other one is used to get the operation-specific selection on top of the sampled\noperation token. This policy only requires one forward pass with a few additional operation tokens, and it is\ncomputationally efficient compared to the sequential policy."}, {"title": "ARCLE AS A CONTINUAL RL ENVIRONMENT", "content": "To address the inherent challenges of the ARCLE environment, it may be necessary to provide an agent with a curricu-\nlum. In such scenarios, an agent capable of continuously learning from a changing set of tasks would be beneficial.\nWe conducted a continual RL experiment to demonstrate the robust learning capabilities of the proposed policy ar-\nchitectures in response to task changes. In this experiment, the initial grids and answers were randomly generated as\nbefore, but the number of colors used increased periodically-specifically, across five learning phases with 2, 4, 6, 8,\nand 10 colors respectively."}, {"title": "FUTURE DIRECTIONS FOR RL RESEARCH IN SOLVING ARC WITH ARCLE", "content": "Based on Section 4.1, where we demonstrated the development and initial success of a PPO-based agent within AR-\nCLE, this section aims to show future RL research in addressing the challenges. Inspired by Chollet (2019a), we posit\nthat an effective ARC solver must possess advanced abstraction and reasoning abilities. Thus, we propose several\nresearch directions using ARCLE (e.g. MAML, GFlowNet, and World Model), with more details in Appendix A.6."}, {"title": "\u039c\u0395TA-RL FOR ENHANCING REASONING SKILLS", "content": "ARC is a multi-task few-shot learning problem: the whole dataset consists of multiple tasks, and each task has few\ndemonstration pairs (supporting set) to infer the output of a test input (query set). ARCLE is in the identical problem\nbut in the RL setting. To manage this, multi-task RL (Wilson et al., 2007) or meta-RL (Finn et al., 2017) algorithms\nthat foster an agent to experience over a task distribution could be applied. We have focused on developing ideas\nwith meta-RL rather than multi-task RL as it gives a richer optimization. In this setting, the meta-training set and the\nmeta-testing set are the training and evaluation sets given in the ARC dataset.\nMeta-RL algorithms on ARCLE should be capable of outputting an RL algorithm that rapidly reasons and produces a\npolicy for each ARC task, without exhaustive searching over actions. The policy is trained to generate valid trajectories\nfrom the input to the output grids simultaneously on multiple demonstration pairs in an ARC task. Then the policy\nis applied to the test input grid to generate output. Therefore, Meta-RL endows agents with essential reasoning skills\nfor ARC's diverse tasks, enabling them to quickly adapt to new tasks by autonomously developing learning strategies.\nIntegrating Meta-RL with ARCLE opens new pathways for researchers to devise techniques that allow AI to effectively\ngeneralize learning across various tasks, thus embodying the 'learning to learn' principle."}, {"title": "GENERATIVE MODELS AS SURROGATES FOR REASONING", "content": "Generative models, particularly GFlowNet (Bengio et al., 2023), offer a novel approach to tackling the reasoning\nchallenges presented by ARC. While an agent is equally given a set of grid operations on the ARCLE, many possible\ntrajectories can lead to a correct answer for an ARC task. Moreover, among demonstration pairs in an ARC task, the\ndetailed trajectories for each pair are varied, as each pair has its own input grid. GFlowNet establishes its policy as\na generative model that enables the sampling of actions from it, and the probability of sampling is proportional to\nthe reward-driven objective. Therefore, GFlowNet benefits from not only learning a posterior distribution to include\nhigh-reward modes but also from searching multiple modes of a solution space by leveraging probabilistic reasoning\nto generate diverse possible solutions, in the form of a directed acyclic graph (DAG). This supports a GFlowNet policy\nto solve the demonstration pairs in one ARC task, although its input grids are different from (but possess the same\nrule) one another. Moreover, its ability to identify multiple viable solutions for individual ARC tasks underscores its\nutility for data augmentation with correct solutions, further enhancing its value as a research tool in this domain."}, {"title": "MODEL-BASED RL FOR ABSTRACTION SKILLS", "content": "Encoding the demonstration pairs based on the core knowledge is a crucial point in establishing a plan to solve an\nARC task. Model-based RL, particularly World Models might be a solution to support abstraction in tackling the\nARC pairs. Among the ARC tasks, there are dissimilar common rules over all pairs in a task, although, there are\na few categories of core knowledge that a common rule in each task be derived from. Objectness, goal-directness,\narithmetic, geometric, and topology are part of them (Chollet, 2019a), and these can be infused in ARCLE's actions,\nlike Move and Rotate operations. Since World Models internalize the environment transitions caused by ARCLE's\nactions to learn an agent on its simulation, it would learn a joint representation of ARCLE's grid pair and actions\n(containing core knowledge). It encourages a controller in World Model agent to utilize flexible neural representation,\nrather than hard-coded operations in ARCLE to simulate. In short, World Models would provide neural abstraction\nskills of the pairs and operations in ARCLE, which supports a controller to search a rule efficiently on the flexible\nrepresentation. Hence, developing agents that can construct and utilize these models is a step towards equipping them\nwith the necessary abstraction skills for handling both trained and untrained tasks."}, {"title": "FURTHER RESEARCH QUESTIONS", "content": "Several research questions would advance while tackling ARC with ARCLE. First, the ARC task does not possess\nan explicit task distribution since individual ARC tasks include a unique rule and the current ARC dataset has only a\nfinite 800 training and evaluation tasks. Categorizing ARC tasks correspondingly to the core knowledge (Moskvichev\net al., 2023) and parameterizing tasks in each category similarly to XLand (Bauer et al., 2023) would be a worthwhile\ntopic that reinforces meta-RL and multi-task learning more promising methods to solve ARC with broader tasks.\nNext, ARCLE's action space consists of two sub-action spaces: an integer operation and a discrete binary mask\nselection. Handling with selection might entail an exponential size of search space: in particular, the size of\nDAG with the GFlowNet approaches grows enormously to degrade the efficiency of figuring out the correct output\ngrid. One probable setting is that we utilized a sequential policy in 4.1.2, maintaining two networks that produce\noperation and selection hierarchically. Then this GFlowNet may maintain a DAG of sampling operation\nonly by considering ARCLE as a probabilistic environment. However, the validity of this method is indeterministic.\nLastly, one might doubt the necessity of World Models in solving ARC to provide richer abstraction. The reason\nwould be that training agents directly in the environment is more straightforward since the environment's dynamics are\ndeterministic, rather than learning the World Models. Nevertheless, in ARC and ARCLE, there is a significant amount\nof auxiliary information to abstract more than a transition of observation: object information and their topology,\nsymmetry, and so forth. Previous studies have shown that it can capture information such as a hidden gravity parameter\nin an environment (Reale & Russell, 2022), and it is expected that additional useful information for ARC can be\nextracted as well. One open question brought here is what information a World Model can extract for ARC. In\nparticular, whether it can disentangle and extract information common to every ARC task (e.g., state transition) and\ntask-specific priors (e.g., objectness) in an interpretable form is a research question for the future."}, {"title": "CONCLUSION", "content": "In this paper, we introduced ARCLE, an RL environment designed for the ARC benchmark, using the Gymnasium li-\nbrary for direct engagement with ARC's challenges. Our development and application of a PPO-based agent, enhanced\nwith auxiliary losses and non-factorizable policies, have demonstrated ARCLE's possibility of learning and perfor-\nmance improvements in addressing ARC tasks. In detail, auxiliary losses improved learning outcomes, especially\nevident in random settings where the comprehensive application of all proposed strategies yielded the best perfor-\nmance. The success rate in these settings, and the superior outcomes from applying sequential and color-equivalent\npolicies, underline the importance of strategic operation and selection processes.\nThese experimental results show advanced RL methodologies such as meta-RL, generative models, and model-\nbased RL-to further enhance Al's reasoning and abstraction abilities. Specifically, meta-RL offers the potential\nto refine Al's reasoning skills by enabling adaptive learning strategies across varied tasks, suggesting a path toward\nmore generalized intelligence. Generative models, by simulating complex reasoning processes, could serve as links\nbetween data and sophisticated decision-making in ARC. Model-based RL model could strengthen AI's ability to\ndistill and apply abstract concepts from complex inputs. Thus, further research using ARCLE could elevate Al's\nlearning strategies and expand the boundaries of its current capabilities. We invite the RL community to engage with\nARCLE not just to solve ARC but to contribute to the broader endeavor of advancing AI research. Through such\ncollaborative efforts, we can unlock new horizons in Al's ability to learn, reason, and abstract, marking significant\nprogress in the field."}, {"title": "APPENDIX", "content": ""}, {"title": "ABSTRACTION AND REASONING CORPUS (ARC)", "content": "With increasing interest in human-like AI, attempts to measure intelligence are being made. However, how can intel-\nligence be quantified? Previous research has defined intelligence as the ability to solve various types of problems with\nlimited data and experience (Chollet, 2019a).\n$I^{Topt}_{IS,scope} = Avg_{\\tau,\\Theta} [\\omega_{\\tau,\\Theta} \\cdot \\Theta_{\\Sigma}  \\cdot \\Sigma_{C \\in Cur_{opt}}  [P_C \\frac{GDIS_{S,T,C}}{P_{IS,T} + E_{IS,T,C}}] ]$\nIn the above equation, $\\omega_{\\tau,\\Theta}$ represents the weights, $C$ denotes a single curriculum (training data), $P_c$ is the\nprobability of the curriculum occurring, $GDIS_{S,T,C}$ signifies the generalization difficulty of solved problems, $P_{IS,T}$\nrepresents prior knowledge, and $E_{IS,T,C}$ denotes the model's experience. Note that intelligence gets bigger when prior\nknowledge and experience get smaller and generalization difficulty gets bigger, just as the definition of intelligence\nproposed above. Therefore, it was argued that benchmarks for evaluating intelligence must meet three criteria: (1)\nsolvable with limited prior knowledge alone, (2) composed of diverse problem types, and (3) quantitatively measur-\nable.\nARC is a benchmark proposed to measure the intellectual capabilities of computers quantitatively. Each task in ARC\nconsists of 2-5 demo pairs with both inputs and outputs given, along with one test input grid. The goal is to infer the\nsolution to the test input grid by deducing rules from the examples. Demo inputs and outputs can vary in size from a\nminimum of 1 \u00d7 1 grid to a maximum of 30 \u00d7 30 grid, with each grid capable of being colored with 10 different colors.\nOne other property of ARC is that it is solvable with just four types of prior knowledge: objectness, goal-directedness,\ncounting, and geometry and topology (Chollet, 2019a). Thus, ARC is considered a fair intelligence assessment scale\nbecause it requires relatively simple rules and limited prior knowledge to solve tasks, while also necessitating the\ninference of various rules and enabling numerical evaluation of whether a task can be solved or not.\nOne of the key features of ARC is its requirement for high levels of abstraction and reasoning compared to other\nbenchmarks. Previous research comparing benchmarks for evaluating visual reasoning abilities noted that ARC stands\nout because it requires understanding abstract images and rules, evaluates by generating answers, and can present\nunseen tasks (Ma\u0142ki\u0144ski & Ma\u0144dziuk, 2023). Due to these characteristics, as of 2023, state-of-the-art models demon-\nstrate approximately 30% accuracy (Johnson et al., 2021). When compared to the fact that human performance is\naround 80%, it becomes evident how challenging the benchmark is. This is why ARC gathers attention in the pursuit\nof human-like AI research."}, {"title": "SIGNIFICANCE OF SOLVING ARC USING REINFORCEMENT LEARNING", "content": "Just as ARC has significant implications for the field of reinforcement learning (RL), RL also holds great importance\nfor the ARC and Artificial General Intelligence (AGI) research areas. This is because RL methodology 1) has char-\nacteristics suitable for solving ARC compared to other approaches, 2) has a high possibility of leading to research\non human reasoning, which is the aim of ARC, and 3) facilitates the utilization of research resources that have been\nemployed in other fields.\nThe suitability of RL for solving the ARC RL possesses suitable characteristics for solving ARC. ARC can be\nunderstood as a program synthesis benchmark that involves composing complex solutions from simple skills (Chollet,\n2019a). Previous attempts to solve ARC support this claim. Conventional deep learning approaches like autoencoders\nshow performance below 10% (Veldkamp et al., 2023), as they are specialized in learning a single skill for solving a\nspecific task. Similarly, large language models that have achieved success across various domains also exhibit around\n15% performance (Mirchandani et al., 2023), likely due to their limitation in finding combinations of step-by-step\nskills. The highest performance has been achieved by program synthesis methods that search for combinations of\nhuman-designed Domain Specific Languages (DSLs) (Hodel, 2023). While these results support that ARC indeed\nrequires program synthesis components, the current methods using manually designed DSLs are limited in their vul-\nnerability to unseen tasks and human bias. On the other hand, RL is a research field specialized in solving complex\nproblems by composing simple actions. Methods like MuZero (Schrittwieser et al., 2020) have successfully found\neffective action combinations in environments like the game of Go. The strong compositional capability makes RL\nmore suitable for solving ARC than other approaches.\nPotential for expanding into human reasoning research RL approach to solving ARC is expected to provide a\nsignificant foundation for theoretical inquiries into AGI, as RL methodologies are similar to how humans solve tasks.\nWhile there have been several attempts to solve ARC, existing methods differ from the strategies typically employed\nby intelligent agents for general problem-solving. Recent research methods, spearheaded by large language models,\nhave been argued to diverge from human reasoning processes (Mitchell et al., 2024). In contrast, RL has consistently\nyielded findings suggesting its biological similarity to human reasoning processes (Matsuo et al., 2022; Stachenfeld\net al., 2017). Additionally, the use of intuitive rewards and policies allows for a transparent examination and approach\nto tasks compared to other machine learning methods. This transparency in understanding how actual reasoning occurs\nis one of the crucial characteristics that enables research into the nature of reasoning. Therefore, research on ARC\nthrough RL will not only aim to solve the tasks but also present an opportunity to gain insights into how humans reason\nfrom a biological perspective.\nAcquisition of diverse research resources ARC is a benchmark created in 2019, and its solutions are currently\nunder research. Supporting an RL environment that can solve ARC has the effect of easily introducing resources from\nother machine learning research fields into ARC. RL allows the application of deep learning models and techniques\ndeveloped in other fields such as vision and natural language processing as its policy function, making it advantageous\nfor utilizing existing resources. These characteristics will further facilitate the application of recently spotlighted\nmethods such as meta-learning, continual learning, and multi-task learning to ARC. Consequently, rather than being\nlimited to finding RL-specific solutions, it provides a great opportunity to test various methodologies and resources\ntogether.\nDespite these advantages, efforts to tackle ARC tasks using RL have been limited, mainly due to a lack of appropriate\nRL environments. ARCLE maintains the inherent difficulty of ARC that requires solving general tasks with minimal\nprior knowledge and experience while preserving the strength of the RL approach in its similarity to human reasoning\nby incorporating the actions humans use when solving ARC. Furthermore, the action space consisting of low-level\nactions and the environment that allows for various variations offer high potential for utilization in meta-learning and\ncontinual learning research. Owing to these characteristics, ARCLE will not only contribute to the field of RL research\nbut also make significant contributions to research on ARC and reasoning intelligence."}, {"title": "OBJECT-ORIENTED ARC (O2ARC) WEB INTERFACE", "content": "Object-Oriented ARC (O2ARC) is a web interface that allows humans to directly solve ARC tasks and collect the\nprocess of solving them (Kim et al., 2022; Shim et al., 2024)4, an improvement upon the initial testing web interface\ndeveloped by Chollet (2019b). Initially, Chollet's testing web interface featured only a basic version involving color-\ning. However, the version of O2ARC has progressively improved to include object-oriented actions such as movement,"}]}