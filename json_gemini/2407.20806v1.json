{"title": "ARCLE: THE ABSTRACTION AND REASONING CORPUS LEARNING ENVIRONMENT FOR REINFORCEMENT LEARNING", "authors": ["Hosung Lee", "Sejin Kim", "Seungpil Lee", "Sanha Hwang", "Jihwan Lee", "Byung-Jun Lee", "Sundong Kim"], "abstract": "This paper introduces ARCLE, an environment designed to facilitate reinforcement learning re- search on the Abstraction and Reasoning Corpus (ARC). Addressing this inductive reasoning bench- mark with reinforcement learning presents these challenges: a vast action space, a hard-to-reach goal, and a variety of tasks. We demonstrate that an agent with proximal policy optimization can learn individual tasks through ARCLE. The adoption of non-factorial policies and auxiliary losses led to performance enhancements, effectively mitigating issues associated with action spaces and goal attainment. Based on these insights, we propose several research directions and motivations for using ARCLE, including MAML, GFlowNets, and World Models.", "sections": [{"title": "INTRODUCTION", "content": "We introduce ARCLE (ARC Learning Environment) as a reinforcement learning (RL) environment designed for the Abstraction and Reasoning Challenge (ARC) benchmark (Chollet, 2019a). This benchmark assesses agents' ability to infer rules from given grid pairs and predict the outcome for a test grid, as illustrated in Figure 1. ARC is designed to test abstraction and reasoning skills, making it a touch benchmark within the domain. Despite various attempts to con- quer ARC's complexities through program synthesis and reasoning using large language models, RL-based approaches are surprisingly rare (Section 2.1). We believe this scarcity is due to the lack of a dedicated RL environment tailored for ARC. To fill this gap, we created ARCLE based on Gymnasium (Towers et al., 2023) to tackle the benchmark."}, {"title": null, "content": "From RL's standpoint, ARC is considered highly challenging. The typical difficulties include (1) a vast action space, (2) a hard-to-reach goal, and (3) a variety of tasks. While other RL benchmarks (e.g., robotics, financial trading, rec- ommender systems, video games) might feature one of these challenges, ARC encompasses all, showing its difficulty. ARCLE is designed to help researchers navigate these challenges, offering a unique testbed for RL strategies.\nVast action space ARC stands out with its vast action space by allowing a variety of actions such as coloring, moving, rotating, or flipping pixels. This feature creates a large set of possibilities, complicating the development of optimal strategies for RL agents. Such a vast action space demands innovative approaches to navigate effectively.\nHard-to-reach goal ARC tasks are uniquely challenging because success is measured by the ability to replicate complex grid patterns accurately, not by reaching a physical location or endpoint. This requires a deep understanding of the task rules and an ability to apply them precisely. Designing effective reward systems is particularly challenging in this context, as progress is not easily quantified. Each ARC task demands not just strategic action but also a nuanced comprehension of the underlying patterns and rules.\nVariety of tasks ARC's wide array of tasks necessitates broad generalization, a stark contrast to benchmarks like Atari, which focus on mastering single games.\u00b9 This diversity calls for adaptive and varied strategies, highlighting ARC's demand for advanced RL methods.\nARCLE is an environment that helps overcome the challenges of ARC and paves new pathways for AI research, seam- lessly linking abstract reasoning in ARC with the adaptability in RL. Our initial experiments highlight the capability of RL to address specific tasks within ARC, indicating the potential necessity for advanced techniques such as meta-RL, generative models, or model-based RL algorithms. Thus, ARCLE stands out as a platform for testing RL solutions, prompting an in-depth exploration of the challenges ARC presents."}, {"title": "RELATED WORKS", "content": ""}, {"title": "SOLVING ARC", "content": "Since the unveiling of the ARC (Chollet, 2019a), approaches ranging from the development of similar benchmarks (Qi et al., 2021; Kim et al., 2022; Xu et al., 2023) to domain-specific languages and program synthesis (Banburski et al., 2020; Acquaviva et al., 2022; Assouel et al., 2022; Alford et al., 2021; Witt et al., 2023; Ainooson et al., 2023) have been explored to extend its applicability and enhance learning strategies. These efforts have deepened our understand- ing of ARC's challenges, highlighting the complexity of devising comprehensive solutions. The recent shift towards leveraging Large Language Models (LLMs), incorporating strategies from natural language processing to detailed task context integration (Camposampiero et al., 2023; Xu et al., 2024; Moskvichev et al., 2023; Mitchell et al., 2024; Lee et al., 2024), underscores LLMs' potential in addressing ARC's intricacies.\nHowever, the performance of research on the ARC utilizing program synthesis and LLMs has not fully met expec- tations, often due to its logical flaw (Lee et al., 2024). This has prompted a pivot towards reinforcement learning as a novel approach, albeit its application to ARC has been limited. Notable attempts include using RL strategies in program synthesis (Butt et al., 2024) and exploring imitation learning (Park et al., 2023). The introduction of ARCLE opens up new possibilities for advancing research on the ARC using RL."}, {"title": "RL ENVIRONMENTS SIMILAR TO ARCLE", "content": "Among the myriad RL environments, those featuring a vast action space similar to ARCLE's are prominently observed in game-based settings, such as PySC2 (Vinyals et al., 2017), where the diversity of actions, determined by mouse click locations, mirrors the flexible action format in ARC. Similarly, environments designed for recommendation systems (e.g., RecSim, RecoGym) and complex multi-step planning tasks (e.g., Super Mario Bros (Kauten, 2018), NLE (K\u00fcttler et al., 2020)) may not exhibit wide action spaces at each state but encapsulate the challenge of hard-to- reach goal through the necessity of sequential decision-making to achieve success. In parallel, the breadth of tasks within ARCLE resonates with the diverse objectives found in robotics environments like Meta-World (Yu et al., 2020), RLBench (James et al., 2020), and CALVIN (Mees et al., 2022), underscoring the complexity and variety of tasks that ARCLE introduces to RL research."}, {"title": "ARCLE: ARC LEARNING ENVIRONMENT", "content": "ARCLE is a reinforcement learning (RL) environment package, implemented in Gymnasium (Towers et al., 2023), designed for RL approaches on Abstraction and Reasoning Corpus (ARC). RL agents on the ARCLE environments learn to solve tasks by selecting actions to edit the grid (to be submitted) to the environment state. As Figure 2 illustrates, ARCLE comprises three main components: envs, loaders, actions, and auxiliary wrappers which modify the environment's action or state space. The following explanation is based on the terms in Table 1.\nThe envs component consists of a base class of ARCLE environments, and its three derivatives. AbstractARCEnv inherits Gymnasium's Env class to provide reinforcement learning environment features and defines the ARC-specific structure of action and state space and user-definable methods. Its implementations, O2ARCEnv, ARCEnv and RawARCEnv provide embodied action and observation spaces. 02ARCEnv constructs the state and action space according to the O2ARC interface (See Appendix A.3), and likewise, ARCEnv offers the testing web interface devel- oped by Chollet (2019b). RawARCEnv restricts the action space to color modifications or grid size changes, providing a more constrained and monotonic learning environment.\nNext, the loaders component provides functionalities to supply the ARC dataset to ARCLE environments. This component comprises the base Loader class defining interface requirements to ARCLE environments and their im- plementations. ARCLoader feeds the ARC dataset to any ARCLE environment and defines how the ARC dataset should be parsed from files and how the parsed dataset should be picked. Likewise, to load a similar dataset to the ARC, one can inherit the Loader class and specify how to parse and sample. We provide the MiniARCLoader which loads Mini-ARC dataset (Kim et al., 2022) upon an ARCLE environment, as an example usage of Loader class.\nLast, actions component includes a variety of functions capable of changing environment state, called operation. Each environment in ARCLE contains several operations to be used in an environment by agents on the environment. Since ARCLE currently implemented actions on the O2ARC interface, it contains more actions (e.g., Move, Rotate, Flip) than the original ARC testing interface (Chollet, 2019b).\nWe focus on explaining 02ARCEnv in the following sections, which encompasses most operations by ARCLE."}, {"title": "ACTIONS", "content": "Actions in ARCLE are defined to enable editing of the output grid for a given task, consisting of operation and selection. operation represents an integer that specifies the method of editing (functions contained in the actions component in Figure 2), and selection is a binary mask that denotes the area of the grid affected by the edit.\nBy defining ARCLE's actions through operation and selection as illustrated by the action in the middle of Figure 3, we have standardized various types of actions within the same structure. Notably, the actions in ARCLE can affect a single pixel, contiguous multiple pixels, or even non-contiguous pixels, accommodating these possibil- ities through employing the binary mask selection. Furthermore, by separating operation and selection, it accommodates the possibility of determining selection conditioned by the chosen operation autoregressively."}, {"title": "STATES & OBSERVATIONS", "content": "All environments included in ARCLE are designed with the assumption to be Markov Decision Processes (MDP). Therefore, every parameter used in changing the environment's state is given to agents in the environment, so obser- vations and states can be considered equivalent. The basic state space of an environment within ARCLE consists of the input and grid. Input represents the test input grid of an ARC task, so it is fixed unless a new task is assigned to an environment. Grid is initially set as the test input grid of a task, and an agent edits this by selecting actions.\nDepending on which operations an environment adopts, the state of the environment can be different. For instance, if an environment includes Copy operation, the environment should include additional variables of the copied part: clip. Hence in O2ARCEnv, more variables are included in the state, to support Copy and object-oriented opera- tions such as Move. These object-oriented actions from the O2ARC interface are supplemented with selected, object, object_pos and background. Descriptions of these variables are depicted in 1. While the agent per- forms object-oriented operations in a row, object and background works as two layers; object is overlayed on the background at object_pos. For the detailed mechanism described in Section A.5."}, {"title": "REWARDS", "content": "The built-in reward currently offered in ARCLE is the sparse reward. This reward grants 1 when the agent performs the submit action and the state space's grid exactly matches the task's answer grid, and 0 if even a single pixel differs. This sparse reward approach can hinder the learning of an agent whose total reward sum remains 0 as there is a unique answer per task. To counteract this, an auxiliary reward was designed and utilized in the subsequent Section 4.1. This auxiliary reward adds a penalty term based on the ratio of the number of incorrect pixels to the total pixels, guiding the agent to learn in a direction that minimizes the number of pixels differing from the correct grid. Identifying a reward setting superior to this auxiliary reward setup, i.e., one that can be universally applied across all ARC tasks aware environment's action space (e.g., object-oriented operations), requires further research."}, {"title": "SOURCE CODE", "content": "Since the environments in ARCLE implemented based on Gymnasium (Towers et al., 2023) and are fully written in Python3, users who have used Gymnasium or its predecessor, OpenAI Gym (Brockman et al., 2016), can use it with familiarity. ARCLE is released on GitHub\u00b2 under the terms of the Apache-2.0 License, as well as uploaded to the PyPI (Python Package Index), so the ARCLE can be easily installed by the pip command.3 Without modifying the source code, one can still create custom ARCLE-based environments by subclassing provided environments in ARCLE or wrapping with the wrapper classes. Please note that ARCLE is currently being continuously updated, so users may need to check the version. In this paper, our descriptions and experiments are based on version 0.2.5."}, {"title": "API & SAMPLE USAGE", "content": ""}, {"title": "ARCLE BENCHMARKS", "content": "This chapter explains the process through which an agent learns to solve synthetic tasks using the ARCLE environment. To ultimately solve ARC, the agent must acquire the ability to tackle unseen tasks through the learning process of tasks provided in the training dataset. We speculate that approaches like meta-RL, generative models (e.g. GFlowNet), and model-based RL algorithms (e.g. World Models) may be necessary to solve tasks not observed during training. As a preliminary step, we describe the initial results of learning an individual task. The PPO-based agent learns the input/output grid pairs presented in one of the ARC tasks. If a method can be designed for the agent to understand and learn from these tasks, we anticipate that it could be trained to solve unlearned tasks using the approaches mentioned above with this agent."}, {"title": "SOLVING ARC WITH A GIVEN ANSWER: HANDLING THE LARGE DISCRETE STATE-ACTION SPACE", "content": "While we expect ARCLE agents to be better at imitating the cognitive process of human problem-solving, training an RL agent for ARCLE itself additionally becomes a difficult challenge due to its large discrete state-action space. In this Section, we demonstrate the difficulty of obtaining highly performant agents within an ARCLE environment even when the state-action spaces are simplified and the answers are given, and we propose ARCLE-specific auxil- iary loss functions and network architectures that can significantly improve agents' performance. Specifically, we use operations of 0\u20139 only with rectangular-shaped selection only (in a bounding box representation), and conse- quently, the sufficient information for decision making (i.e., the state s) becomes (grid, grid_dim, answer, answer_dim) as we additionally assume answers to be given. We expect the methods introduced here to be used to help train ARCLE agents for the original ARC, where the answers are not provided and state-action spaces are more complex."}, {"title": "Proximal Policy Optimization (PPO)", "content": "We employed the well-known PPO algorithm (Schulman et al., 2017) to train the agents to solve ARCLE with the answers given. Due to the poor generalization ability (Kumar et al., 2021) and learning instability of value-based RL algorithms, recently, PPO has been widely adopted for tuning large neural models (Stiennon et al., 2020; Ouyang et al., 2022). It is an on-policy policy-gradient algorithm that aims to perform a gradient update within the trust region. We gather trajectories and construct a dataset $D = \\{(s_i, a_i, R_i)\\}_{i}$ consisting of state, action, and returns (sum of discounted rewards starting from the state). Then, the policy is updated according to the following losses ($L = L_{Baseline} + L_{PPO}$) with samples from D:\n$L_{Baseline} (V) = E_D [(r - V_\\theta(s))^2]$\n$L^{CPPO}(\\theta) = E_D \\left[ min\\left( \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)} (R - sg[V_\\varphi(s)]), clip\\left(\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)}, 1-\\epsilon, 1+\\epsilon\\right) (R - sg[V_\\varphi(s)]) \\right) \\right]$\nwhere $V$ is a value function that works as a baseline that reduces the gradient variance, $\\pi_{old}$ is a policy used to gather the trajectories, and $sg[]$ is a stop-gradient operator. $r \\in [-1,0]$ is a reward from a dense reward function that penalizes the agent by the ratio of incorrect pixels of the next state."}, {"title": "LEARNING BETTER REPRESENTATION THROUGH AUXILIARY LOSS FUNCTIONS", "content": "Using an auxiliary loss function to predict important information has been a widely used approach for better gener- alizable representation and faster training (Jaderberg et al., 2016; Lample & Chaplot, 2017). We experimented three different auxiliary losses, (1) $L_{r_{t-1}}$ predicting the previous reward $r_{t-1}$ from the current state $s_t$, (2) $L_{r_t}$ predicting the current reward $r_t$ from the current state-action $(s_t, a_t)$, and (3) $L_{s_{t+1}}$ predicting the next state $s_{t+1}$ from the current state-action $(s_t, a_t)$. All three functions are deterministic, and they are highly informative as they are correlated to either the value function or the action-value function. For policy architecture, we used the color-equivariant policy architecture that will be detailed in Section 4.1.2.\nWhile the first auxiliary loss $L_{r_{t-1}}$ can be easily adopted by additionally training a feed-forward network on top of the extracted state feature from the special token of $V_\\varphi$, the other two auxiliary losses require the state-action feature that is not utilized in conventional PPO. We compute the state-action feature by performing a forward propagation again with additional action embedding tokens after sampling an action from a policy. The prediction for the loss $L_{r_t}$ is done on top of the last action token that embeds selection, and the prediction for the loss $L_{s_{t+1}}$ is done on top of tokens that represent each pixel of grid.\nThe results are reported in Figure 5. Note that the vanilla PPO agent was not able to learn anything in the random setting despite the vastly simplified state-action space, demonstrating the difficulty of training an agent for ARCLE. While all of the experimented auxiliary losses improve the learning of the agent, it can be seen that adopting auxiliary features and adopting state-action feature-based auxiliary features make a significant difference in performance. Only with all three of these auxiliary losses, we were able to get three agents out of four that achieved a success rate larger than 95% in the random setting. On the other hand, in the ARC setting, auxiliary losses were able to help, but their effect was relatively less dramatic."}, {"title": "NON-FACTORIZABLE POLICY ARCHITECTURE", "content": "It can be observed that the two main components of the action space of ARCLE, operation and selection, are intertwined with each other and cannot be separately decided. For example, the optimal selection for coloring a pixel, or rotating an object will be completely different. This observation shows that the considerate choice of policy architecture is necessary, as conventional factorized policy assuming $(operation | selection) | s$ will have limited expressivity in representing such complex multimodal distributions. For all experiments in this section, we used all three auxiliary losses introduced in Section 4.1.1.\nTo demonstrate the expressivity of different policy architectures, we experimented with the following three architecture types: (1) Non-sequential policy assumes $(operation | selection) | s$. This policy is implemented by using two special tokens for operation and selection with two feed-forward networks on top of extracted features from those tokens. (2) Sequential policy does not assume conditional independence, by making the decision of selection dependent on sampled operation, similar to the RNN policy of Vinyals et al. (2019). This policy requires two forward passes to sample an action, one for sampling operation from its special token and one for sampling selection from the token embedding the sampled operation, and therefore it is more computationally demanding.\nOn the other hand, we also experimented (3) Color-equivariant policy that takes advantage of the ARCLE task that the same permutation of colors applied to the task and the policy coloring actions results in the equivalent task, i.e., the color equivariance. We can achieve color equivariance of the policy by using several special tokens for policy equal to the number of operation to represent them, and by setting a special token of color-related operation as a function of color embedding used to represent grid. We can then use two different feed-forward networks on top of extracted features of these operation tokens. One gives scalar output per token to be used as logits for deciding the operation. The other one is used to get the operation-specific selection on top of the sampled operation token. This policy only requires one forward pass with a few additional operation tokens, and it is computationally efficient compared to the sequential policy.\nFigure 6 summarizes the result. Overall, it can be observed that sequential policy and color equivariant policy outper- form non-sequential policy, showing that conditional dependence is crucial for learning in ARCLE. Sequential policy shows more stable and faster learning compared to color equivariant policy in terms of policy updates. However, con- sidering that sequential policy takes approximately 1.5x training time and 2x inference time, there is a clear trade-off and we can choose from two depending on the situation."}, {"title": "ARCLE AS A CONTINUAL RL ENVIRONMENT", "content": "To address the inherent challenges of the ARCLE environment, it may be necessary to provide an agent with a curricu- lum. In such scenarios, an agent capable of continuously learning from a changing set of tasks would be beneficial. We conducted a continual RL experiment to demonstrate the robust learning capabilities of the proposed policy ar- chitectures in response to task changes. In this experiment, the initial grids and answers were randomly generated as before, but the number of colors used increased periodically-specifically, across five learning phases with 2, 4, 6, 8, and 10 colors respectively."}, {"title": "FUTURE DIRECTIONS FOR RL RESEARCH IN SOLVING ARC WITH ARCLE", "content": "Based on Section 4.1, where we demonstrated the development and initial success of a PPO-based agent within AR- CLE, this section aims to show future RL research in addressing the challenges. Inspired by Chollet (2019a), we posit that an effective ARC solver must possess advanced abstraction and reasoning abilities. Thus, we propose several research directions using ARCLE (e.g. MAML, GFlowNet, and World Model), with more details in Appendix A.6."}, {"title": "\u039c\u0395TA-RL FOR ENHANCING REASONING SKILLS", "content": "ARC is a multi-task few-shot learning problem: the whole dataset consists of multiple tasks, and each task has few demonstration pairs (supporting set) to infer the output of a test input (query set). ARCLE is in the identical problem but in the RL setting. To manage this, multi-task RL (Wilson et al., 2007) or meta-RL (Finn et al., 2017) algorithms that foster an agent to experience over a task distribution could be applied. We have focused on developing ideas with meta-RL rather than multi-task RL as it gives a richer optimization. In this setting, the meta-training set and the meta-testing set are the training and evaluation sets given in the ARC dataset.\nMeta-RL algorithms on ARCLE should be capable of outputting an RL algorithm that rapidly reasons and produces a policy for each ARC task, without exhaustive searching over actions. The policy is trained to generate valid trajectories from the input to the output grids simultaneously on multiple demonstration pairs in an ARC task. Then the policy is applied to the test input grid to generate output. Therefore, Meta-RL endows agents with essential reasoning skills for ARC's diverse tasks, enabling them to quickly adapt to new tasks by autonomously developing learning strategies. Integrating Meta-RL with ARCLE opens new pathways for researchers to devise techniques that allow AI to effectively generalize learning across various tasks, thus embodying the 'learning to learn' principle."}, {"title": "GENERATIVE MODELS AS SURROGATES FOR REASONING", "content": "Generative models, particularly GFlowNet (Bengio et al., 2023), offer a novel approach to tackling the reasoning challenges presented by ARC. While an agent is equally given a set of grid operations on the ARCLE, many possible trajectories can lead to a correct answer for an ARC task. Moreover, among demonstration pairs in an ARC task, the detailed trajectories for each pair are varied, as each pair has its own input grid. GFlowNet establishes its policy as a generative model that enables the sampling of actions from it, and the probability of sampling is proportional to the reward-driven objective. Therefore, GFlowNet benefits from not only learning a posterior distribution to include high-reward modes but also from searching multiple modes of a solution space by leveraging probabilistic reasoning to generate diverse possible solutions, in the form of a directed acyclic graph (DAG). This supports a GFlowNet policy to solve the demonstration pairs in one ARC task, although its input grids are different from (but possess the same rule) one another. Moreover, its ability to identify multiple viable solutions for individual ARC tasks underscores its utility for data augmentation with correct solutions, further enhancing its value as a research tool in this domain."}, {"title": "MODEL-BASED RL FOR ABSTRACTION SKILLS", "content": "Encoding the demonstration pairs based on the core knowledge is a crucial point in establishing a plan to solve an ARC task. Model-based RL, particularly World Models might be a solution to support abstraction in tackling the ARC pairs. Among the ARC tasks, there are dissimilar common rules over all pairs in a task, although, there are a few categories of core knowledge that a common rule in each task be derived from. Objectness, goal-directness, arithmetic, geometric, and topology are part of them (Chollet, 2019a), and these can be infused in ARCLE's actions, like Move and Rotate operations. Since World Models internalize the environment transitions caused by ARCLE's actions to learn an agent on its simulation, it would learn a joint representation of ARCLE's grid pair and actions (containing core knowledge). It encourages a controller in World Model agent to utilize flexible neural representation, rather than hard-coded operations in ARCLE to simulate. In short, World Models would provide neural abstraction skills of the pairs and operations in ARCLE, which supports a controller to search a rule efficiently on the flexible representation. Hence, developing agents that can construct and utilize these models is a step towards equipping them with the necessary abstraction skills for handling both trained and untrained tasks."}, {"title": "FURTHER RESEARCH QUESTIONS", "content": "Several research questions would advance while tackling ARC with ARCLE. First, the ARC task does not possess an explicit task distribution since individual ARC tasks include a unique rule and the current ARC dataset has only a finite 800 training and evaluation tasks. Categorizing ARC tasks correspondingly to the core knowledge (Moskvichev et al., 2023) and parameterizing tasks in each category similarly to XLand (Bauer et al., 2023) would be a worthwhile topic that reinforces meta-RL and multi-task learning more promising methods to solve ARC with broader tasks.\nNext, ARCLE's action space consists of two sub-action spaces: an integer operation and a discrete binary mask selection. Handling with selection might entail an exponential size of search space: in particular, the size of DAG with the GFlowNet approaches grows enormously to degrade the efficiency of figuring out the correct output grid. One probable setting is that we utilized a sequential policy in 4.1.2, maintaining two networks that produce operation and selection hierarchically. Then this GFlowNet may maintain a DAG of sampling operation only by considering ARCLE as a probabilistic environment. However, the validity of this method is indeterministic.\nLastly, one might doubt the necessity of World Models in solving ARC to provide richer abstraction. The reason would be that training agents directly in the environment is more straightforward since the environment's dynamics are deterministic, rather than learning the World Models. Nevertheless, in ARC and ARCLE, there is a significant amount of auxiliary information to abstract more than a transition of observation: object information and their topology, symmetry, and so forth. Previous studies have shown that it can capture information such as a hidden gravity parameter in an environment (Reale & Russell, 2022), and it is expected that additional useful information for ARC can be extracted as well. One open question brought here is what information a World Model can extract for ARC. In particular, whether it can disentangle and extract information common to every ARC task (e.g., state transition) and task-specific priors (e.g., objectness) in an interpretable form is a research question for the future."}, {"title": "CONCLUSION", "content": "In this paper, we introduced ARCLE, an RL environment designed for the ARC benchmark, using the Gymnasium li- brary for direct engagement with ARC's challenges. Our development and application of a PPO-based agent, enhanced with auxiliary losses and non-factorizable policies, have demonstrated ARCLE's possibility of learning and perfor- mance improvements in addressing ARC tasks. In detail, auxiliary losses improved learning outcomes, especially evident in random settings where the comprehensive application of all proposed strategies yielded the best perfor- mance. The success rate in these settings, and the superior outcomes from applying sequential and color-equivalent policies, underline the importance of strategic operation and selection processes.\nThese experimental results show advanced RL methodologies such as meta-RL, generative models, and model- based RL-to further enhance Al's reasoning and abstraction abilities. Specifically, meta-RL offers the potential to refine Al's reasoning skills by enabling adaptive learning strategies across varied tasks, suggesting a path toward more generalized intelligence. Generative models, by simulating complex reasoning processes, could serve as links between data and sophisticated decision-making in ARC. Model-based RL model could strengthen AI's ability to distill and apply abstract concepts from complex inputs. Thus, further research using ARCLE could elevate Al's learning strategies and expand the boundaries of its current capabilities. We invite the RL community to engage with ARCLE not just to solve ARC but to contribute to the broader endeavor of advancing AI research. Through such collaborative efforts, we can unlock new horizons in Al's ability to learn, reason, and abstract, marking significant progress in the field."}, {"title": "APPENDIX", "content": ""}, {"title": "ABSTRACTION AND REASONING CORPUS (ARC)", "content": "With increasing interest in human-like AI, attempts to measure intelligence are being made. However, how can intel- ligence be quantified? Previous research has defined intelligence as the ability to solve various types of problems with limited data and experience (Chollet, 2019a).\n$\\underset{T_{IS, scope}}{Avg}\\left[ \\omega_{\\tau,\\Theta}. \\Theta(\\Sigma C) - \\Sigma PC \\frac{GD_{IS,T,C}}{P_{IS,T} + E_{IS,T,C}} \\right]$\nIn the above equation, $\\omega_{\\tau,\\Theta}\u00b7$ represents the weights, $C$ denotes a single curriculum (training data), $P_c$ is the probability of the curriculum occurring, $GD_{IS,T,C}$ signifies the generalization difficulty of solved problems, $P_{IS,T}$ represents prior knowledge, and $E_{RS,T,C}$ denotes the model's experience. Note that intelligence gets bigger when prior knowledge and experience get smaller and generalization difficulty gets bigger, just as the definition of intelligence proposed above. Therefore, it was argued that benchmarks for evaluating intelligence must meet three criteria: (1) solvable with limited prior knowledge alone, (2) composed of diverse problem types, and (3) quantitatively measur- able.\nARC is a benchmark proposed to measure the intellectual capabilities of computers quantitatively. Each task in ARC consists of 2-5 demo pairs with both inputs and outputs given, along with one test input grid. The goal is to infer the solution to the test input grid by deducing rules from the examples. Demo inputs and outputs can vary in size from a minimum of 1 \u00d7 1 grid to a maximum of 30 \u00d7 30 grid, with each grid capable of being colored with 10 different colors. One other property of ARC is that it is solvable with just four types of prior knowledge: objectness, goal-directedness, counting, and geometry and topology (Chollet, 2019a). Thus, ARC is considered a fair intelligence assessment scale because it requires relatively simple rules and limited prior knowledge to solve tasks, while also necessitating the inference of various rules and enabling numerical evaluation of whether a task can be solved or not.\nOne of the key features of ARC is its requirement for high levels of abstraction and reasoning compared to other benchmarks. Previous research comparing benchmarks for evaluating visual reasoning abilities noted that ARC stands out because it requires understanding abstract images and rules, evaluates by generating answers, and can present unseen tasks (Ma\u0142ki\u0144ski & Ma\u0144dziuk, 2023). Due to these characteristics, as of 2023, state-of-the-art models demon- strate approximately 30% accuracy (Johnson et al., 2021). When compared to the fact that human performance is around 80%, it becomes evident how challenging the benchmark is. This is why ARC gathers attention in the pursuit of human-like AI research."}, {"title": "SIGNIFICANCE OF SOLVING ARC USING REINFORCEMENT LEARNING", "content": "Just as ARC has significant implications for the field of reinforcement learning (RL), RL also holds great importance for the ARC and Artificial General Intelligence (AGI) research areas. This is because RL methodology 1) has char- acteristics suitable for solving ARC compared to other approaches, 2) has a high possibility of leading to research on human reasoning, which is the aim of ARC, and 3) facilitates the utilization of research resources that have been employed in other fields.\nThe suitability of RL for solving the ARC RL possesses suitable characteristics for solving ARC. ARC can be understood as a program synthesis benchmark that involves composing complex solutions from simple skills (Chollet, 2019a). Previous attempts to solve ARC support this claim. Conventional deep learning approaches like autoencoders show performance below 10% (Veldkamp et al., 2023), as they are specialized in learning a single skill for solving a specific task. Similarly, large language models that have achieved success across various domains also exhibit around 15% performance (Mirchandani et al., 2023), likely due to their limitation in finding combinations of step-by-step skills. The highest performance has been achieved by program synthesis methods that search for combinations of human-designed Domain Specific Languages (DSLs) (Hodel, 2023). While these results support that ARC indeed requires program synthesis components, the current methods using manually designed DSLs are limited in their vul- nerability to unseen tasks and human bias. On the other hand, RL is a research field specialized in solving complex problems by composing simple actions. Methods like MuZero (Schrittwieser et al., 2020) have successfully found effective action combinations in environments like the game of Go. The strong compositional capability makes RL more suitable for solving ARC than other approaches.\nPotential for expanding into human reasoning research RL approach to solving ARC is expected to provide a significant foundation for theoretical inquiries into AGI, as RL methodologies are similar to how humans solve tasks. While there have been several attempts to solve ARC, existing methods differ from the strategies typically employed by intelligent agents for general problem-solving. Recent research methods, spearheaded by large language models, have been argued to diverge from human reasoning processes (Mitchell et al., 2024). In contrast, RL has consistently yielded findings suggesting its biological similarity to human reasoning processes (Matsuo et al., 2022; Stachenfeld et al., 2017). Additionally, the use of intuitive rewards and policies allows for a transparent examination and approach to tasks compared to other machine learning"}]}