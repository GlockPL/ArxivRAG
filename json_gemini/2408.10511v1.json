{"title": "Single-cell Curriculum Learning-based Deep Graph Embedding Clustering", "authors": ["Huifa Li", "Jie Fu", "Xinpeng Ling", "Zhiyu Sun", "Kuncan Wang", "Zhili Chen"], "abstract": "The swift advancement of single-cell RNA sequencing (scRNA-seq) technologies enables the investigation of cellular-level tissue heterogeneity. Cell annotation significantly contributes to the extensive downstream analysis of scRNA-seq data. However, The analysis of scRNA-seq for biological inference presents challenges owing to its intricate and indeterminate data distribution, characterized by a substantial volume and a high frequency of dropout events. Furthermore, the quality of training samples varies greatly, and the performance of the popular scRNA-seq data clustering solution GNN could be harmed by two types of low-quality training nodes: 1) nodes on the boundary; 2) nodes that contribute little additional information to the graph. To address these problems, we propose a single-cell curriculum learning-based deep graph embedding clustering (scCLG). We first propose a Chebyshev graph convolutional autoencoder with multi-decoder (ChebAE) that combines three optimization objectives corresponding to three decoders, including topology reconstruction loss of cell graphs, zero-inflated negative binomial (ZINB) loss, and clustering loss, to learn cell-cell topology representation. Meanwhile, we employ a selective training strategy to train GNN based on the features and entropy of nodes and prune the difficult nodes based on the difficulty scores to keep the high-quality graph. Empirical results on a variety of gene expression datasets show that our model outperforms state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "The advent of single-cell RNA sequencing (scRNA-seq) technologies has enabled the measurement of gene expressions in a vast number of individual cells, offering the potential to deliver detailed and high-resolution understandings of the intricate cellular landscape. The analysis of scRNA-seq data plays a pivotal role in biomedical research, including identifying cell types and subtypes, studying developmental processes, investigating disease mechanisms, exploring immunological responses, and supporting drug development and personalized therapy. Cell annotation is the fundamental step in analyzing scRNA-seq data. In early research, various traditional clustering methods have been applied such as K-means, spectral clustering, hierarchical clustering and density-based clustering. However, scRNA-seq data are so sparse that most of the measurements are zeros. The traditional clustering algorithm often produces suboptimal results.\nSeveral clustering methods have been developed to address these limitations. CIDR [1], MAGIC [2], and SAVER [3] have been developed to initially address the issue of missing values, commonly referred to as dropouts, followed by the clustering of the imputed data. Despite the benefits of imputation, these methods encounter challenges in capturing the intricate inherent structure of scRNA-seq data. Alternative strategies, such as SIMLR [4] and MPSSC [5], utilize multi-kernel spectral clustering to acquire robust similarity measures. Nevertheless, the computational complexity associated with generating the Laplacian matrix hinders their application to large-scale datasets. Additionally, these techniques fail to account for crucial attributes of transcriptional data, including zero inflation and over-dispersion.\nIn recent years, deep learning has shown excellent performance in the fields of image recognition and processing, speech recognition, recommendation systems, and autonomous driving [6]-[9]. Some deep learning clustering methods have effectively emerged to model the high-dimensional and sparse nature of scRNA-seq data such as scziDesk [10], scDCC [11], and scDeepCluster [12]. These models implement auto-encoding architectures. However, they often ignore the cell-cell relationships, which can make the clustering task more challenging. Recently, the emerging graph neural network"}, {"title": "II. RELATED WORK", "content": "scRNA-seq clustering. With the advent of deep learning (DL), more recent works have utilized deep neural networks to automatically extract features from scRNA-seq data for enhancing feature representation. scDC [12] simultaneously learns to feature representation and clustering via explicit modeling of scRNA-seq data generation. In another work, scziDesk [10] combines deep learning with a denoising autoencoder to characterize scRNA-seq data while proposing a soft self-training K-means algorithm to cluster the cell population in the learned latent space. scDCC [11] integrates prior knowledge to loss function with pairwise constraints to scRNA-seq. The high-order representation and topological relations could be naturally learned by the graph neural network. scGNN [13] introduces a multi-modal autoencoder framework. This framework formulates and aggregates cell-cell relationships with graph neural networks and models heterogeneous gene expression patterns using a left-truncated mixture Gaussian model. scGAE [14] builds a cell graph and uses a multitask-oriented graph autoencoder to preserve topological structure information and feature information in scRNA-seq data simultaneously. However, the above clustering methods overlook the learning difficulty of different samples or nodes.\nCurriculum learning. Curriculum learning, which mimics the human learning process of learning data samples in a meaningful order, aims to enhance the machine learning models by using a designed training curriculum, typically following an easy-to-hard pattern [15]. The CL framework consists of two components: a difficulty measurer which measures the difficulty of samples and a training scheduler which arranges the ordered samples into training. The key to CL is how to define the promising measurer. SPCL [16] takes into account both prior knowledge known before training and the learning progress during training. CLNode [17] measures the difficulty of training nodes based on the label information. SMMCL [18] assumes that different unlabeled samples have different difficulty levels for propagation, so it should follow an easy-to-hard sequence with an updated curriculum for label propagation. scSPaC [19] utilizes an advanced NMF for scRNA-seq data clustering based on soft self-paced learning, which gradually adds cells from simple to complex to our model until the model converges. However, the above CL methods don't utilize the structural information of nodes in graph neural networks and don't consider the impact of difficult nodes on the graph."}, {"title": "III. PRELIMINARIES", "content": "In this section, we first introduce some notations, symbols, and necessary background. Then we present the Chebyshev graph convolution.\nA. Notations\nLet G = (V,E,X) be an undirected cell graph, where V = {V1, V2, ..., Unc } is a set of nc nodes associated with different"}, {"title": "B. Chebyshev Graph Convolution", "content": "Chebyshev graph convolution (ChebConv) is a variant of graph convolutional networks that uses Chebyshev polynomials to approximate the feature decomposition of graph Laplacian matrices, thereby achieving convolution operations on graph data. The theoretical foundation of ChebConv is graph signal processing and spectrogram theory, which introduces the concept of graph signal processing into graph convolutional networks. The ChebConv layer is defined as follows:\n$\u0397 = \\sum_{k=1}^{K} Z^{(k)} \u0398^{(k)}$\nwhere K represents the order of Chebyshev polynomials used to approximate graph convolution kernels. O is the layer's trainable parameter and $Z^{(k)}$ is computed recursively by:\n$Z^{(1)} = X$\n$Z^{(2)} = L X$\n$Z^{(k)}=2LZ^{(k-1)} - Z^{(k-2)}$\nwhere \u00ceL denotes the scaled and normalized Laplacian $\\frac{2L}{\\lambda_{max}} - I$. $\\lambda_{max}$ is the largest eigenvalue of L and I is the identity matrix.\nCompared with basic GCN, ChebConv effectively reduces the model's parameter count and computational complexity by transforming graph convolution operations into approximations of Chebyshev polynomials, while maintaining its ability to capture graph structures."}, {"title": "IV. PROPOSED APPROACH", "content": "In this section, we firstly present our idea of multi-decoder ChebConv graph autoencoder. Secondly, we introduce how the scCLG model parameters can be learned using a meaningful sample order. Finally, we elaborate the proposed scRNA-seq data clustering algorithm by combining the above two points."}, {"title": "A. Multi-Decoder ChebConv Graph Autoencoder", "content": "As shown in Fig. 2, to capture the cell graph structure and node relationships, we developed a variant of the graph convolution autoencoder that uses a stacked topology Chebyshev graph convolutional network as the graph encoder. We use three different decoders to map the encoded compressed vector from different perspectives and jointly optimize the modeling ability of the autoencoder. The gene expression matrix X and normalized adjacency matrix A are used inputs. Through the graph encoder, the feature dimension of each node will be compressed to a smaller size, and the compressed vector features will be decoded by three decoders: adjacency matrix decoder (Deca), ZINB decoder (Deczinb), and clustering decoder (Deccls). These decoders share encoder parameters to decompose an optimization objective into three optimization objectives for better capturing the cell-cell relationship:\n$L = L_{rec} + L_{zinb} + L_{cls}$\nMore detailed optimization information about $L_{rec}, L_{zinb}$ and Lcls is shown below.\n1) Reconstruction Loss: Given that the majority of the structure and information inherent in the scRNA-seq data X is conserved within the latent embedded representation generated by the scCLG encoder. The adjacency matrix decoder of the graph autoencoder can be defined as the inner product between the latent embedding:\n$Z = f_E(X)$\n$A_{rec} = \u03c3(ZZ^T)$\nwhere, fe represents the scCLG encoder function; Arec is the reconstructed adjacency matrix. Therefore, the reconstruction loss of A and Arec should be minimized in the learning process as below:\n$L_{rec} = ||A - A_{rec}||^2$\n2) ZINB Loss: In order to more effectively capture the structure of scRNA-seq data by decoding from the latent embedded representation Z, we integrate the ZINB model into a Chebyshev graph convolutional autoencoder to capture the global probability structure of the scRNA-seq data. Based on this foundation, we propose to apply the ZINB distribution model to simulate the data distribution to capture the characters of scRNA-seq data as follows:\n$NB(x|\\mu, \u03b8) = \\frac{\\Gamma(x+\u03b8)}{x!\\Gamma(\u03b8)} (\\frac{\u03b8}{\u03b8 + \\mu})^\u03b8 (\\frac{\\mu}{\u03b8 + \\mu})^x$\n$ZINB(x|\u03c0, \\mu, \u03b8) = \u03c0\u03b4_{0}(x) + (1 \u2212 \u03c0)NB(x|\\mu, \u03b8)$\nwhere \u00b5 and @ are the mean and dispersion in the negative binomial distribution, respectively. \u03c0 is the weight of the point mass at zero. The proportion $\\frac{\u03b8}{\u03b8 + \\mu}$ replaces the probability p. After that, to model the ZINB distribution, the decoder network has three output layers to compute the three sets of parameters. The estimated parameters can be defined as follows:\n$\\pi$ = sigmoid($W_{\\pi}f_{D_{ZINB}}(Z)$)\n$ \\mu$ = exp($W_{\\mu}f_{D_{ZINB}}(Z)$)\n$\\theta$ = exp($W_{\\theta}f_{D_{ZINB}}(Z)$)\nwhere fDzinb is a three-layer fully connected neural network with hidden layers of 128, 256 and 512 nodes. W represents the learned weights parameter matrices. \u03c0,\u00b5 and @ are parameters denoting the estimations of \u03c0,\u00b5 and \u03b8, respectively. The selection of the activation function depends on the range and definition of the parameters. In terms of the parameter \u03c0, the suitable activation function for it is sigmoid because the interval of \u03c0 is between 0 and 1. Due to the non-negative value of the mean \u00b5 and dispersion \u03b8, we choose the exponential activation function for them. The negative log-likelihood of the ZINB distribution can be used as the reconstruction loss function of the original data X, which can be defined as below:\n$L_{ZINB} = -log(ZINB(X|\u03c0, \\mu, \u03b8))$\n3) Clustering Loss: scRNA-seq clustering clustering as an unsupervised learning task, lacks guidance from labels, which makes it difficult to capture effective optimization signals during the training phase. To overcome this challenge, we apply a clustering module to guide the algorithm to adjust the cluster centers to ensure that the distribution of samples within each cluster is as consistent as possible while minimizing inter-cluster differences. The objective takes the form of Kullback-Leibler (KL) divergence and is formulated as follows:\n$L_{cls} = KL(P||Q) = \\sum_i\\sum_u P_{iu}log \\frac{P_{iu}}{Q_{iu}}$\nwhere qiu is the soft label of the embedding node zi which is defined as the similarity between zi and cluster centre \u00b5u measured by Student's t-distribution. This can be described as follows:\n$q_{ij} = \\frac{(1 + ||z_i \u2013 \\mu_j ||^2)^{-1}}{\\sum_{j'} (1 + ||z_{i} \u2013 \\mu_{j'} ||^2)^{-1}}$\nMeanwhile, piu is the auxiliary target distribution, which puts more emphasis on the similar data points assigned with high confidence on the basis of qiu, as below:\n$P_{ij} = \\frac{q^2_{ij}/\\sum_j q_{ij}}{\\sum_{j'} (q^2_{ij'}/\\sum_j q_{ij'})}$\nSince the target distribution P is defined based on Q, the embedding learning of Q is supervised in a self-optimizing way to enable it to be close to the target distribution P."}, {"title": "B. Curriculum Learning with Data Pruning", "content": "In this subsection, we first describe the proposed difficulty measurement method from both local and global perspectives and assign a difficulty score to each cell. Based on the difficulty score, we investigate the impact of nodes with higher difficulty on model optimization."}, {"title": "1) Hierarchical Difficulty Measurer", "content": "Our Hierarchical Difficulty Measurer consists of two difficulty measures from different perspectives. In this section, we present the definition of two difficulty measures and how to calculate them.\nLocal Difficulty Measurer. We introduce how to identify difficult nodes from a local perspective. Nodes located at the boundaries of multiple classes may reside in transitional regions within the feature space, leading to less distinct or consistent feature representations, thereby increasing the difficulty of classification. The first type of difficult node should have diverse neighbors that belong to multiple classes. Intuitively, features of nodes within the same class tend to be more similar. This is due to the influence of neighboring node features, resulting in nodes with similar connectivity patterns exhibiting comparable feature representations. In order to identify these difficult nodes, we calculate the diversity of the neighborhood's features:\n$D_{local}(u) = \\sum_{v\\in N(u)}S(u,v)$\n$S(u, v) = \\frac{\u03ba_v}{||u||\\cdot||v||}$\nwhere S(u, v) denotes the similarity between cell u and cell v. A larger Dlocal(u) indicates a more diverse neighborhood. As a result, during neighborhood aggregation, these nodes aggregate neighbors' features to get an unclear representation, making them difficult for GNNs to learn. By paying less attention to these difficult nodes, scCLG learns more useful information and effectively improves the accuracy of backbone GNNs.\nGlobal Difficulty Measurer. Then we introduce how to identify difficult nodes from a global perspective. Entropy plays a pivotal role in feature selection as a metric from information theory used to quantify uncertainty. In the process of feature selection, we leverage entropy to assess a feature's contribution to the target variable. When a feature better distinguishes between different categories of the target variable, its entropy value tends to be relatively low, signifying that it provides more information and reduces overall uncertainty. Consequently, in feature selection, lower entropy values indicate features that offer greater discriminatory power, aiding in the differentiation of target variable categories. We assume nodes that have lower entropy have fewer contributions to the graph. Therefore, this type of node is difficult to classify. Inspired by Entropy Variation [21], We consider the node contribution as the variation of network entropy before and after its removal.\nFor a node vi in graph G, we define p(v) as probabilities:"}, {"title": null, "content": "$p(v) = \\frac{D(v)}{\\sum_{u\\in V}D(u)}$\nwhere $\\sum_{v\\in V} p(v) = 1$.\nThe entropy of the graph is as follows:\n$Ent(G) = - \\sum_{v\\in V}p(v) log p(v)$\n$= - \\sum_{v\\in V}\\frac{D(v)}{\\sum_{u\\in V}D(u)} log (\\frac{D(v)}{\\sum_{u\\in V}D(u)})$\n$= log(\\sum_{v\\in V}D(v)) - \\sum_{v\\in V} \\frac{D(v)}{\\sum_{u\\in V}D(u)}log D(v)$\n$= log(\\sum_{v\\in V}D(v)) - \\sum_{v\\in V}log D(v)$\nwhere D(v) is the degree of node v. Ent(G) is the entropy of graph G with degree matrix.\nThe global difficulty of the node is as follows:\n$D_{global}(v) = 1 - \\frac{Ent(v)}{\\sum_{u\\in V}Ent(u)}$\n$Ent(v) = Ent(G) \u2013 Ent(\\hat{G}_v)$\nwhere Ent(v) is the change if one node and its connections are removed from the network. Gv is the modified graph under the removel of v. A lower Ent(v) indicates a lower influence on graph structure and is also more difficult. The global difficulty of node v is to subtract the normalized Ent(v) from 1. Considering two difficulty measurers from local and global perspectives, we finally define the difficulty of v as:\n$D(v) = \u03b2 * D_{local} + (1 \u2212 \u03b2) * D_{global}$\nwhere \u03b2 is the weight coefficient assigned to each difficulty measurer to control the balance of the total difficulty.\n2) Data Pruning: With the hierarchical difficulty measurer, we can get a list of nodes sorted in ascending order of nodes based on difficulty. The node at the end of the list is a nuisance for the overall model learning, so should it be retained? The sources of noise in graph neural networks can be varied, firstly, the attribute information of the nodes may contain noise, which affects the representation of the node features and hence the learning of the GNN. Secondly, the presence of anomalous data may cause the spectral energy of the graph to be \"right-shifted\", the distribution of spectral energy shifts from low to high frequencies. These noises will not only reduce the performance of the graph neural network but also propagate through the GNN in the topology, affecting the prediction results of the whole network. In order to solve this problem, we designed a data pruning strategy based on the calculated node difficulty. Specifically, we define a data discarding hyperparameter \u03b1. The value of \u03b1 is set while balancing data integrity and model generalization performance. As shown in Fig. 4, the scRNA-seq clustering performance of the scCLG improves after removing the node features with the highest difficulty which prove our hypothesis."}, {"title": "C. The Proposed scCLG Algorithm", "content": "Our model undergoes a two-phase training process. For the first phase, We pretrain the proposed GNN model ChebAE for discriminative feature learning with an adjacency matrix decoder and ZINB decoder. The number of first phase training rounds is T\u2081 epochs. The output of the encoder is a low dimensional vector which is used to calculate node difficulty using a hierarchical difficulty measurer. We retained the top 1 \u2013 \u03b1 of the data with high sample quality for subsequent training. For the formal training phase, we use the parameters pretrained and train the model for T2 epochs with pruned data. This phase is the learning of clustering tasks. Unlike the pre-training phase, we use all three decoders to optimize the model in more detail. We use the pacing function min(1, 2log2 do-log2 do*) mentioned in [17] to generate the size of the nodes subset. We illustrate the detailed information in Algorithm 1."}, {"title": "V. EXPERIMENTS", "content": "Dataset. For the former, we collect 7 scRNA-seq datasets from different organisms. The cell numbers range from 870 to 9519, and the cell type numbers vary from 2 to 9.\nBaselines. The performance of scCLG was compared with two traditional clustering methods (Kmeans and Spectral), and several state-of-the-art scRNA-seq data clustering methods including four single-cell deep embedded clustering methods (scziiDesk, scDC, scDCC and scGMAI) and three single-cell deep graph embedded clustering methods (scTAG, scGAE and SCGNN).\n\u2022 Deep soft K-means clustering with self-training for single-cell RNA sequence data (scziDesk) [10]: It combines a denoising autoencoder to characterize scRNA-seq data while proposing a soft self-training K-means algorithm to cluster the cell population in the learned latent space.\n\u2022 Model-based deep embedded clustering method (scDC) [12]: It simultaneously learns to feature representation and clustering via explicit modeling of scRNA-seq data generation.\n\u2022 Model-based deep embedding for constrained clustering analysis of single cell RNA-seq data (scDCC) [11] It integrates prior information into the modeling process to guide our deep learning model to simultaneously learn meaningful and desired latent representations and clusters.\n\u2022 scGMAI: a Gaussian mixture model for clustering single-cell RNA-Seq data based on deep autoencoder (scGMAI) [22] It utilizes autoencoder networks to reconstruct gene expression values from scRNA-Seq data and FastICA is used to reduce the dimensions of reconstructed data.\n\u2022 scGNN is a novel graph neural network framework for single-cell RNA-Seq analyses (scGNN) [13]: It integrates three iterative multi-modal autoencoders and models heterogeneous gene expression patterns using a left-truncated mixture Gaussian model.\n\u2022 A topology-preserving dimensionality reduction method for single-cell RNA-seq data using graph autoencoder (scGAE) [14] It builds a cell graph and uses a multitask-oriented graph autoencoder to preserve topological structure information and feature information in scRNA-seq data simultaneously.\n\u2022 Zinb-based graph embedding autoencoder for single-cell rna-seq interpretations (scTAG) [20] It simultaneously learns cell-cell topology representations and identifies cell clusters based on deep graph convolutional network integrating the ZINB model.\nImplementation Details. In the proposed scCLG method, the cell graph was constructed using the KNN algorithm with the nearest neighbor parameter k = 20. In the multi-decoders ChebConv graph autoencoder, the hidden fully connected layers in the ZINB decoder are set at 128, 256 and 512. Our algorithm consists of pre-training and formal training, with 1000 and 500 epochs for pre-training and formal training, respectively. Our model was optimized using the Adam optimizer, employing a learning rate of 5e-4 during pre-training and 1e-4 during formal training. The pruning rate \u03b1 is set to 0.11. For baseline methods, the parameters were set the same as in the original papers."}, {"title": "B. Clustering Result", "content": "Table II shows the clustering performance of our method against multiple state-of-the-art methods, and the values highlighted in bold represent the best results. Obviously, our method outperforms other baseline clustering methods for clustering performance. For the 7 scRNA-seq datasets, scCLG achieves the best NMI and ARI on all datasets. Meanwhile, we can observe that the general deep graph embedded models have no advantage and the clustering performance is not stable. Specifically, scGNN performs poorly on \"Wang_Lung\". The main reason is that the information structure preserved by the cell graph alone cannot address the particularities of scRNA-seq data well, and further data order is necessary, which again proves the superiority of scCLG. The performance of the deep clustering method and traditional clustering method exhibits significant fluctuations across different datasets. However, scCLG still has an advantage. This is because the scCLG could effectively learn the key representations of the scRNA-seq data in a meaningful order so that the model can exhibit a smooth learning trajectory. In summary, we can conclude that scCLG performs better than the other methods under two clustering evaluation metrics."}, {"title": "C. Parameter Analysis", "content": "2) Different Numbers of Variable Genes Analysis: In single-cell data analysis, highly variable genes vary significantly among different cells, which helps to reveal the heterogeneity within the cell population and more accurately identify cell subpopulations. To explore the impact of the number of selected highly variable genes, we apply scCLG on real datasets with gene numbers from 300 to 1500. Fig. 3 (B) shows the line plot of the average NMI and ARI on the 7 datasets selecting 300, 500, 1000 and 1500 genes with high variability, respectively. It can be seen that the performance with 500 highly variable genes is better, while the performance with 300 genes is much worse than the others. Therefore, to save computational resources and reduce running time, we set the number of selected high-variance genes in the model to 500."}, {"title": "1) Different Neighbor Parameter k Analysis", "content": "k represents the number of nearest neighbors to consider when constructing cell graph. In order to investigate the impact of k, we ran our model with the parameters 5, 10, 15, 25. Fig. 3 (A) shows the NMI and ARI values with different numbers of k. As depicted in Fig. 3 (A), we observe that the two metrics first increase rapidly from parameter 5 to 10, reach the best value at k = 20, and then decrease slowly from parameter 20 to 25. Therefore, we set the neighbor parameter k as 20 in our scCLG model."}, {"title": "3) Different Data Pruning Rate Analysis", "content": "In single-cell data analysis, data quality can be improved by pruning lower-quality samples thereby affecting the ability to generalize the model. To explore the impact of the selected data, we run our model with pruning rate parameters from 0.06 to 0.21 to drop difficult nodes. We also compared our pruning strategy with two different pruning strategies, namely pruning easy nodes and randomly pruning nodes. Fig. 4 shows the ARI and NMI values with different numbers of \u03b1 and pruning strategy. As depicted in Fig. 4, we can observe that the best performance is achieved when the \u03b1 is 0.11 and when difficult nodes are pruned. This indicates that the improvement of data quality can significantly improve the performance of the model. Compared to pruning easy nodes and randomly pruning nodes, pruning difficult nodes brings higher profit because difficult nodes have a negative impact on the representation of the graph. Furthermore, randomly pruning nodes is better than pruning easy nodes, indicating the effectiveness of our difficulty measurer which can assign reasonable difficulty scores to nodes."}, {"title": "D. Ablation Study", "content": "In this experiment, we analyzed the effect of each component of the scCLG method. Specifically, we ablated different components in no hierarchical difficulty measurer named Without CL. Table III tabulates the average ARI and NMI values on the 7 datasets with scCLG. As shown in Table III, it can be clearly observed that gene screening and extraction of scRNA-seq data from easy to hard patterns improves the clustering performance. For the 7 scRNA-seq datasets, scCLG achieve the best ARI and NMI on 5 of them. In summary, all components of the scCLG method are reasonable and effective."}, {"title": "VI. CONCLUSION", "content": "In this research, we propose a single-cell curriculum learning-based deep graph embedding clustering. Our approach first utilizes the Chebyshev graph convolutional autoencoder to learn the low-dimensional feature representation which preserves the cell-cell topological structure. Then we define two types of difficult nodes and rank the nodes in the graph based on the measured difficulty to train them in a meaningful manner. Meanwhile, we prune the difficult node to keep the high quality of node features. Our method shows higher clustering performance against state-of-the-art approaches for scRNA-seq data. Empirical results provide strong evidence that this performance is imputed to the proposed mechanisms and particularly their ability to tackle the difficult nodes."}]}