{"title": "FREE VIDEO-LLM: PROMPT-GUIDED VISUAL PERCEPTION FOR EFFICIENT TRAINING-FREE VIDEO LLMS", "authors": ["Kai Han", "Jianyuan Guo", "Yehui Tang", "Wei He", "Enhua Wu", "Yunhe Wang"], "abstract": "Vision-language large models have achieved remarkable success in various multi-modal tasks, yet applying them to video understanding remains challenging due to the inherent complexity and computational demands of video data. While training-based video-LLMs deliver high performance, they often require substantial resources for training and inference. Conversely, training-free approaches offer a more efficient alternative by adapting pre-trained image-LLMs models for video tasks without additional training, but they face inference efficiency bottlenecks due to the large number of visual tokens generated from video frames. In this work, we present a novel prompt-guided visual perception framework (abbreviated as Free Video-LLM) for efficient inference of training-free video LLMs. The proposed framework decouples spatial-temporal dimension and performs temporal frame sampling and spatial RoI cropping respectively based on task-specific prompts. Our method effectively reduces the number of visual tokens while maintaining high performance across multiple video question-answering benchmarks. Extensive experiments demonstrate that our approach achieves competitive results with significantly fewer tokens, offering an optimal trade-off between accuracy and computational efficiency compared to state-of-the-art video LLMs. The code will be available at https://github.com/contrastive/FreeVideoLLM.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, vision-language models (VLMs) have rapidly revolutionized multi-modal understanding and generation, enabling models to comprehend and produce responses from both visual and textual inputs (visual inputs include images, videos, etc). Due to the vast amount of image-text data and foundational models like CLIP Radford et al. (2021) and large language models (LLMs) such as GPT Brown (2020) and LLaMA Touvron et al. (2023), image-LLMs Zhang et al. (2024a); Liu et al. (2023); Achiam et al. (2023); Chen et al. (2024); Jie et al. (2024) have already made significant progress. InstructBLIP Dai et al. (2023) extends the capabilities on new visual tasks by incorporating instruction-aware features. LLaVA Liu et al. (2023) generate multimodal language-image instruction-following data using GPT-4 Achiam et al. (2023), demonstrating impressive multimodel chat abilities. The evolution of VLMs has also given rise to video-based LLMs that are specifically tailored for video understanding. These methods, as discussed by Tang et al. Tang et al. (2023), can be categorized into two primary approaches based on their training strategies: training-based video LLMs and training-free video LLMs.\nIn the training-based video LLMs, AV-LLM Shu et al. (2023) and Vid2Seq Yang et al. (2023) trained all the parameters in the LLM, which can be resource-intensive. Other main stream methods in this fine-tuning category either externally(e.g., Q-former) or internally fine-tune the bridge between the video encoder and the LLM, or adopt a phased fine-tuning approach for connective adapters and insertive adapters. Video-LLMs, such as Video-ChatGPT Maaz et al. (2024), Chat-UniVi Jin et al. (2024), and MovieChat Song et al. (2024), primarily focus on general question-answering (QA)"}, {"title": "2 RELATED WORKS", "content": "In this section, we briefly revisit the related works including image-LLMs, video-LLMs (especially training-free video LLMs)."}, {"title": "2.1 IMAGE-LLMS", "content": "The rapid advancement of image-language models (image-LLMs) can be attributed to two key factors: the foundational work of CLIP Radford et al. (2021), which introduced a shared representation space for vision and language, demonstrating strong zero-shot capabilities and robust performance across various computer vision benchmarks; and the emergence of powerful Large Language Models (LLMs) like GPT Brown (2020) and LLAMA Touvron et al. (2023), which can be further enhanced through instruction tuning Peng et al. (2023). Flamingo Alayrac et al. (2022) excels in few-shot learning by seamlessly integrating pre-trained vision and language models and high-quality interleaved multimodal data. BLIP-2 Li et al. (2023a) utilizes a lightweight Querying Transformer to connect modalities.\nFurther progress on image-LLMs has been achieved through multimodal instruction tuning. LLaVA Liu et al. (2023) uses GPT-4 Achiam et al. (2023) to generate robust multi-modal instruction data, pre-training on image-text pairs and fine-tuning for end-to-end multimodal understanding. InstructBLIP Dai et al. (2023) extends the capabilities of BLIP-2 with instruction-aware features, while mPLUG-Owl Ye et al. (2023) employs a two-stage modular approach to enhance task performance across different modalities. MiniGPT-4 Zhu et al. (2023) aligns a frozen visual encoder with a frozen LLM (Vicunna Peng et al. (2023)) using one projection layer and showcasing enhanced capabilities such as detailed image descriptions and creative storytelling. MiniGPT-5 Zheng et al. (2023) extends MiniGPT-4 to output text interleaved with images."}, {"title": "2.2 VIDEO LLMS", "content": "Building on the foundation of LLM and image-LLMs, video-language models (video-LLMs) ia also rapidly developed. FrozenBiLM Yang et al. (2022) leverages frozen bidirectional language models for zero-shot video question answering, achieving leading performance on zero-shot VideoQA without the need for manual annotations. VideoChat Li et al. (2023b) and Video-LLaMA Zhang et al. (2023) both utilize dual streams to handle audio and visual signals. Specifically, Video-LLaMA integrates Q-Former for these two streams, whereas VideoChat incorporates a video embedder alongside a perception tools for captions, and introduce a video-centric multimodal instruction fine-tuning dataset. Video-ChatGPT Maaz et al. (2024) utilizes a pretrained visual encoder to extract both spatial and temporal features from videos by averaging frame-level features. These features are then projected into the input space of large language models (LLMs). Additionally, it also contributes a high-quality dataset of 100,000 video-instruction question-answer pairs. Chat-UniVi Jin et al. (2024) use a set of dynamic visual tokens to uniformly represent images and videos tokens. PLLaVA Xu et al. (2024a) introduce a post-training weight fusion methods to alleviate forgetting phenomenon during multi-modality fine-tuning."}, {"title": "2.3 TRAINING-FREE VIDEO LLMS", "content": "In contrast to these tuning methodologies for Video LLMs, there is a growing interest in exploring training-free video LLMs, specifically how existing image model architectures can be minimally adapted to accommodate video inputs. IG-VLM Kim et al. (2024), as a pioneer in this exploration, transforms videos into single composite image grids, enabling the direct application of a high-performance VLM without the need for video-data training. FreeVA Wu (2024) also investigates the potential of leveraging offline image-LLMs as training-free video assistants by simply adding a parameter-free temporal aggregation, achieving strong performance in zero-shot video question-answering tasks Chen & Dolan (2011); Caba Heilbron et al. (2015); Xu et al. (2016), even surpassing video instruct-tuning models. Meanwhile, SLOWFAST-LLAVA Xu et al. (2024b) utilizes a dual-stream approach to efficiently capture spatial and temporal video features, demonstrating competitive performance on various video benchmarks without requiring additional training."}, {"title": "3 APPROACH", "content": ""}, {"title": "3.1 PRELIMINARIES", "content": "The image-LLM has a remarkable progress in the past two years due to the large amount of image-text pairs data and image instruction data. The representative open-sourced image-LLMs like LLaVA Liu et al. (2023) and InternVL Chen et al. (2024) obtain high performance on various image conversation, description and reasoning tasks. The image-LLMs take one image I as input, and the visual encoder gv (including the projector) extracts the image features and converts into language embedding tokens:\n$H_{I} = g(I),$  (1)\nwhere Hv \u2208 RN\u00d7D, N is the number of visual tokens per frame, and D is the token embedding dimension. Then the image tokens H\u2081 and the text tokens HT are fed into the LLM f for generating"}, {"title": "3.2 COMPUTATIONAL BURDEN OF VIDEO LLM", "content": "The training-free video LLM can leverage the well-trained image-LLMs for video without training on any data. Given a video, a number of frames are usually uniformly extracted and forms a sequence of images {I1, I2,\u2026\u2026\u2026, IT} where T is the number of frames. These images are fed into the visual encoder to obtain visual tokens:\n$Xv = gv ({I1, I2, \u2026\u2026\u2026, IT}),$ (3)\nXv \u2208 RT\u00d7N\u00d7D. Then the training-free video LLMs directly use LLMs to receive visual tokens Xv and text prompt to generate responses for video understanding:\n$Y = f([Hv, Hr]).$ (4)\nThis simple pipeline is intuitive yet effective Wu (2024); Kim et al. (2024).\nHowever, the video usually consists of many frames, that is, the number of visual tokens will be significantly large, proportional to the product of the frame number and the image size L \u00d7 TN. The input sequence length directly influences the computational cost and inference efficiency of LLM. In transformer architecture, the computational cost is mostly occupied by the self-attention and feed-forward network. The self-attention module performs token-to-token relation computation and requires a O(L2) computational cost. The feed-froward network makes nonlinear transformation of each token with the computation budget proportional to the sequence length. Compared to image-LLMs, the computational complexity of video-LLMs will increase by an order of magnitude, which will significantly reduce the inference speed and efficiency of video-LLMs, affecting the actual usage experience."}, {"title": "3.3 PROMPT-GUIDED VISUAL PERCEPTION", "content": "We introduce the prompt-guided visual perception for efficient video LLMs, by temporally and spatially sampling visual information respectively, as demonstrated in Figure 1.\nPrompt-guided Temporal Sampling The video is usually captured in a high frame rate, resulting in large redundancy in the neighbor frames. In order to reduce the number of frames and maintain the discriminative information, we propose to temporally sample the prompt-related frames. The previous video-LLMs take all the visual tokens generated by the visual encoder as inputs for LLMs. However, this approach does not consider the text context for different questions or tasks, which may introduce useless tokens during inference. For instance, given a video depicting two women shopping in a supermarket, one might ask how many apples they bought, while another person could ask whether they bought eggs. Different questions necessitate the model to focus on different periods of the video. On the other hand, different regions of the video are also required for different"}, {"title": "Prompt-guided Spatial Sampling", "content": "As mentioned above, different questions require the model to focus on a specific part of the spatial regions for answering. We propose to utilize the prompt information to guide the selection of regions of interest (RoI) in the video. For the visual tokens of a frame, we reshape them back to the spatial size as Xsi \u2208 RH\u00d7W\u00d7D where H \u00d7 W is the feature map size. For a feature vector in spatial position (h, w), its relation score with the prompt feature is calculated:\n$Sh,w=\\frac{XSi,h,w}{||XSi,h,w||2 ||FP||2},$ (7)\nwhere 1 \u2264 h < H and 1 < w < W. Suppose we need to box out a RoI with an area that is \u03b1\u00d7 the original area where 0 < \u03b1 < 1. The top-K tokens with the largest similarity scores are selected, and their positions are {(h1, w\u2081), (h2, w\u2082),\u2026\u2026,(h\u03ba,\u03c9\u03ba)} where K = \u03b1HW. The center coordinates of the RoI can be obtained by the mean of these positions:\nhc=1/k \u03a3 h\u03ba k=1\n(8)\nwc =  1/k \u03a3 w\u03ba k=1\n(9)\nThe height and width of the RoI is calculated by\nH' = \u221a\u03b1H,(10)\nW' = \u221a\u03b1W.(11)\nWith the center coordinates and box size, we can easily crop the RoI from the frame feature maps, and reshape them as a sequence of tokens as the video representation for the specific prompt. The obtained compact visual tokens are concatenated with the text prompt embeddings as the inputs of LLMs. In this way, the training-free video LLMs can process a video with fewer tokens and enjoy more efficient inference."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 BENCHMARKS AND IMPLEMENTATION DETAILS", "content": "Benchmarks As our method is training-free, we directly evaluate the proposed methods on open-ended video understanding and question-answering benchmarks, including MSVD-QA Chen & Dolan (2011), MSRVTT-QA Xu et al. (2016), ActivityNet-QA Caba Heilbron et al. (2015) and TGIF-QA Jang et al. (2017). The GPT APIs are utilized to assess the model accuracy and response quality. Following the previous works Wu (2024); Xu et al. (2024a;b), GPT-3.5-Turbo-0125 version is used for fair comparison."}, {"title": "4.2 MAIN RESULTS", "content": "In Table 2, we evaluate several models with a focus on efficiency, as reflected by the number of visual tokens (second column), which directly impacts the computational cost and memory requirements. Our method demonstrates notable efficiency, using only 1026 and 2600 visual tokens across two configurations, which is significantly fewer than competing models such as IG-VLM (3456 tokens) and SF-LLaVA (3680 tokens). Despite this reduced token count, our model consistently achieves competitive or superior performance across various QA benchmarks. For instance, in the MSVD-QA task, our method scores 76.8/4.0 with 1026 tokens, outperforming FreeVA (73.8/4.1 with 2304 tokens). Overall, the average accuracy and score of our method are comparable to other models with fewer inference tokens."}, {"title": "4.3 COMPARISON WITH SOTA METHODS", "content": "In Table 4, we present a comparison between our model and state-of-the-art (SOTA) methods, with a particular emphasis on the trade-off between accuracy and efficiency, as reflected by the number of visual tokens and performance on four video question-answering (QA) benchmarks. Our method utilizes 2600 visual tokens, which is more efficient than IG-VLM (3456 tokens) and SF-LLaVA (3680 tokens), while achieving highly competitive results across all evaluated tasks."}, {"title": "4.4 ABLATION STUDIES", "content": "Inference Speed We compare the inference speed of three representative training-free video LLMs in Table 5. Two important metrics for measuring the inference speed of large models are pre-filling latency and output speed. Pre-filling latency represents the time taken by the model to generate the first output token after receiving the input. A lower pre-filling latency means the model can begin producing results faster. Output speed refers to the rate at which the model generates subsequent tokens after the first one is produced. A higher TPS indicates that the model can output tokens more quickly once the generation process has started. The inference speed is evaluated on an NVIDIA V100 GPU with standard transformers framework. Our Free Video-LLM outperforms the others with fewer visual tokens (2,648), the fastest pre-filling latency (0.578 seconds), and the"}, {"title": "Prompt-guided Spatial RoI Cropping", "content": "We conduct experiments to evaluate the proposed prompt-guided spatial sampling for RoI cropping. The experiments involve uniform temporal sampling and prompt-guided temporal sampling, with and without the addition of RoI. Uniform temporal sampling and prompt-guided temporal sampling serves as baselines without RoI cropping, with 3 frames and 864 visual tokens resulting in MSVD accuracies of 71.7 and 75.0 respectively. When adding Rol to prompt-guided temporal sampling, with an RoI ratio of 0.6 and 3 frames, the number of visual tokens is reduced to 513 and the MSVD accuracy is 74.9. Replacing the proposed RoI method with adaptive average pooling will decrease the accuracy to 73.8 with the same number of visual tokens. These experiments demonstrate combining prompt-guided sampling with RoI can enhance performance, showing the effectiveness of prompt-guided spatial RoI cropping."}, {"title": "4.5 VIDEO QA EXAMPLES", "content": "We also show the video QA examples to analyze the advantages and disadvantages of the method more intuitively. Figure 2 summarizes example object detection results from the MSVD-QA benchmark, comparing the outputs of SF-LLaVA and our proposed method. For the first video, both models correctly identify that someone is folding a square green paper, with SF-LLaVA scoring slightly higher (5 vs. 4). In the second video, both models describe multiple men diving into a swimming pool, with neither able to accurately determine the number of divers, resulting in the same lower score of 2. Overall, while both models perform similarly on these tasks, our method is more efficient with much fewer tokens."}, {"title": "5 CONCLUSION", "content": "In conclusion, this paper introduces a novel framework for efficient inference of training-free video LLMs, named Free Video-LLM. The proposed method addresses the computational challenges associated with video understanding by introducing a prompt-guided visual perception approach that significantly reduces the number of visual tokens processed by the model. Through temporal and"}]}