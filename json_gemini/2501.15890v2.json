{"title": "Complexity in Complexity:\nUnderstanding Visual Complexity Through Structure, Color, and Surprise", "authors": ["Karahan Sar\u0131ta\u015f", "Peter Dayan", "Tingke Shen", "Surabhi S Nath"], "abstract": "Understanding human perception of visual complexity is cru-cial in visual cognition. Recently (Shen, et al. 2024) proposedan interpretable segmentation-based model that accurately pre-dicted complexity across various datasets, supporting the ideathat complexity can be explained simply. In this work, we in-vestigate the failure of their model to capture structural, colorand surprisal contributions to complexity. To this end, we pro-pose Multi-Scale Sobel Gradient which measures spatial in-tensity variations, Multi-Scale Unique Color which quantifiescolorfulness across multiple scales, and surprise scores gener-ated using a Large Language Model. We test our features onexisting benchmarks and a novel dataset containing surprisingimages from Visual Genome. Our experiments demonstratethat modeling complexity accurately is not as simple as pre-viously thought, requiring additional perceptual and semanticfactors to address dataset biases. Thus our results offer deeperinsights into how humans assess visual complexity.", "sections": [{"title": "Introduction", "content": "Visual complexity is a fundamental attribute of images thatreflects the level of detail, intricacy, and variation in visualelements within a scene (Snodgrass & Vanderwart, 1980a).This perceptual characteristic encompasses multiple dimen-sions, including the number of lines, density of elements(McDougall, Curry, & de Bruijn, 1999), quantity of objects(Olivia, Mack, Shrestha, & Peeper, 2004), clutter (Kyle-Davidson, Zhou, Walther, Bors, & Evans, 2023), symme-try (Kyle-Davidson, Solis, Robinson, Tan, & Evans, 2025),and variety of colors (Corchs, Ciocca, Bricolo, & Gasparini,2016). Understanding visual complexity is crucial across do-mains, from user interface design (Miniukovich, Sulpizio, &De Angeli, 2018; Ak\u00e7a & \u00d6mer \u00d6zg\u00fcr Tanri\u00f6ver, 2021) tocognitive psychology (Forsythe, 2009; Madan, Bayer, Gamer,Lonsdorf, & Sommer, 2018). While deep learning modelspredict complexity well, their black-box nature (Li et al.,2022) limits interpretability, which is essential for applica-tions like education (Stoesz, Niknam, & Sutton, 2020; Ghai &Tandon, 2022) and information visualization (Y. Zhu, Suo, &Owen, 2007b, 2007a). Developing interpretable, perception-aligned features remains a key research goal.\nA recent study by (Shen, Nath, Brielmann, & Dayan, 2024)bridged the gap between handcrafted features and deep learn-ing approaches by proposing a simple two-feature modelbased on outputs from deep segmentation models (the num-bers of segments and semantic classes in the image). Theirsimple, yet interpretable, model achieved superior perfor-mance compared to baseline on many naturalistic and artdatasets, supporting the idea that the numbers of segmentsand classes are generic features that explain complexity well.However, their model fails to take into account the structureand arrangement of components in the image.\nHere, we build upon the model from (Shen et al., 2024),by studying two of its major failure modes. First, we findthat datasets with high structural and color regularity re-quire two low-level features to explain complexity: Multi-Scale Sobel Gradient (MSG) and Multi-Scale Unique Color(MUC). MSG captures continuous spatial intensity variationsacross multiple scales, offering a richer representation of im-age structure. MUC quantifies color diversity also at multiplescales, and at different color resolutions. The latter featurebuilds on the concept of \"colorfulness\u201d from (Teresa, Ciocca,& Gagliardi, 2014), and provides a more robust and perfor-mant measure of chromatic complexity.\nWe discovered a second failure mode of (Shen et al.,2024) which concerns the rather underexplored contributionof whole-image, holistic or emergent information to visualcomplexity. Previous research by (Forsythe, Mulhern, &Sawey, 2008) demonstrated that participants in a visual com-plexity experiment exhibited a familiarity bias, perceivingfamiliar shapes as less complex compared to unfamiliar ornovel ones. Similarly, (Snodgrass & Vanderwart, 1980b)found a negative correlation between visual complexity andfamiliarity in their experiment. (Forsythe, 2009) suggeststhat complexity judgments are context-dependent and thatfamiliarity should be considered when developing a modelof visual complexity. Motivated by their findings, we showthat holistic, whole-image surprise judgments of scenes con-tribute significantly to the visual complexity of naturalisticimages. We define surprising images as those containing un-usual, or contextually novel elements, i.e., the opposite of fa-miliarity. (Meyer, Reisenzein, & Sch\u00fctzwohl, 1997; Ortony& Partridge, 1987). To this end, we introduce a novel dataset,SVG, containing both surprising and randomly sampled im-ages from the well-studied Visual Genome dataset (Krishna et"}, {"title": "Methods", "content": "In this work, we evaluate the effectiveness of sets of featuresto explain visual complexity on datasets using linear regres-sion models. We follow the procedure in (Shen et al., 2024),and fit M repetitions of 3-fold cross-validated linear regres-sion. M is determined by the size of the dataset to addressstatistical variability in smaller datasets. We measure per-formance using the mean Spearman correlation coefficientacross all test splits. We compare the performance of ourmodels to the same baselines in (Shen et al., 2024): hand-crafted features (Corchs et al., 2016; Kyle-Davidson et al.,2023) and deep learning approaches (Saraee, Jalal, & Betke,2020; Feng et al., 2023) for comparison. The code for ouranalysis, experimental setup, and dataset will be made pub-licly available upon acceptance."}, {"title": "Datasets", "content": "We evaluate our method on four publicly available datasetswith human-rated complexity scores: RSIVL (49 images)(Corchs et al., 2016), VISC (800 images) (Kyle-Davidson etal., 2023), Savoias (1400 images across 7 categories) (Saraeeet al., 2020), and IC9600 (9600 images across 8 categories)(Feng et al., 2022). We select image subsets to demonstratethe effectiveness of our new features to explain complexity -VISC and IC9600 architecture subset for MSG, Savoias Artand Suprematism subsets for MUC, and Savoias Interior De-sign and IC9600 Abstract for the combination of MSG andMUC. Finally, we use our novel dataset SVG to show thatsurprisal contributes significantly to complexity."}, {"title": "SVG: A Dataset of Surprising Images", "content": "To the best of our knowledge, there is currently no existingimage set with surprising images that can be used to study vi-sual complexity. To fill this gap, we introduce a new datasetcontaining 100 highly surprising images and 100 (on average,less surprising) images, the latter of which were sampled uni-formly at random from the Visual Genome, subject to strat-ification according to bins of complexity based on IC9600scores (Feng et al., 2023). To confirm that our subjectivelyselected images are more surprising than randomly sampledone, we compare the distributions of LLM-generated sur-prise scores between the two subsets. A histogram of thesescores shows clear separation: sampled images cluster atlower values, while handpicked images have higher surprisescores (mean: 39.45 for ordinary, 72.35 for surprising). AKolmogorov-Smirnov test confirms a significant difference(D = 0.67, p < 0.001), supporting our hypothesis.\nWe collect visual complexity ratings from humans using anonline experiment on Prolific (Palan & Schitter, 2018). Par-ticipants are shown pairs of images and asked to select the"}, {"title": "Multi-Scale Sobel Gradient", "content": "We propose the Multi-Scale Sobel Gradient (MSG) algorithm(Algorithm 1) to capture structure in images. MSG analyzesspatial intensity variations across multiple resolutions in anRGB image. Note that the Sobel operator is typically appliedto grayscale images (Kanopoulos, Vasanthavada, & Baker,1988), but we found in ablations that the grayscale versionof the algorithm (which first converts the colored images tograyscale) did not perform as well. Hence we use the colorversion of the algorithm for the rest of this work."}, {"title": "Multi-Scale Unique Color", "content": "We introduce MUC which intuitively counts the number ofunique colors present in an image. We start from the \"color-fulness\" feature introduced by (Teresa et al., 2014) (whichwas first applied to image indexing and content querying)and derive MUC by making colorfulness multi-scale. Note,(Teresa et al., 2014) and subsequent works did not providepseudocode for the colorfulness algorithm we are the firstto do so. MUC has two hyperparameters which control thecoarse-graining of spatial resolution and color resolution (bit"}, {"title": "Generating Surprise Scores using LLMs", "content": "In order to have an automated pipeline for predicting com-plexity, we prompt an LLM using zero-shot Chain-of-Thought (Wei et al., 2023) to get surprise scores for eachimage in our SVG dataset. Our prompt is shown in Algo-rithm 3. We selected GEMINI-1.5-FLASH for its ability toprovide rapid responses without compromising quality. Themodel generated surprise scores with step-by-step reasoning,producing structured output in the desired format. We alsocollect human surprise ratings on the SVG image set to val-idate our LLM-generated scores using a similar Prolific ex-periment to that described above."}, {"title": "Results", "content": "The features we introduced: MSG, MUC, and Surprise, ex-plain additional variance in perceived complexity beyond that"}, {"title": "Explaining the Failure Modes with a Data-Centric\nApproach: Colors and Structure", "content": "On the VISC dataset, the MSG feature improves the meanSpearman's rank correlation coefficient from 0.56 to 0.68,surpassing all previous handcrafted features. Similarly, itachieves a notable increase of 0.10 in mean Spearman'srank correlation coefficient on the IC9. Architecture dataset.These two datasets comprise of mostly real-world natural im-ages of buildings, and indoor and outdoor scenes which arehighly textured or regular - for example a grass field, the win-dows on a highrise, or rows of seats in an airport. A Sobeloperator applied to a spatially coarse-grained image can beinterpreted as an (a) symmetry detector. Hence MSG detects(a)symmetry and quantifies the regularity of local patches(e.g. windows on a highrise) at multiple spatial scales, mak-ing it a well-suited feature for predicting complexity on VISCand IC9. Architecture."}, {"title": "Memorability and Aesthetics", "content": "The existing literature onvisual complexity has largely concentrated on identifyinglow-level image features that define complexity, rather thanexamining its perceptual implications for the observer. How-ever, as noted by (Forsythe, 2009), complexity inherently relates to the difficulty of cognitive processing, suggesting thatsurprising images should be perceived as more complex. Weprovide first evidence in support of this hypothesis, showingthat surprise, as determined by a LLM, contributes signifi-cantly to visual complexity on our SVG dataset.\nPrior work links image memorability, a closely related con-cept to complexity (Kyle-Davidson & Evans, 2023), to dis-tinctiveness, finding that unique or atypical features enhanceretention. (Bruce, Burton, & Dench, 1994; Bartlett, Hurry,& Thorley, 1984) find that distinct facial features are morememorable. (Lukavsk\u00fd & D\u011bcht\u011brenko, 2017) find that im-ages that deviate from their neighbors in a CNN embedding"}, {"title": "Discussion", "content": "This work introduces three features to solve the inadequacyof the generic constructs proposed by (Shen et al., 2024)to explain complexity in certain datasets: multi-scale gradi-ent analysis, multi-scale unique color, and a surprise score.We experimentally show that complexity can be multifacetedrequiring both generic features (segmentation and objectcounts) and dataset specific features to predict accurately.Our results demonstrate that pixel-level structural and chro-matic features, MSG and MUC effectively capture key as-pects of visual complexity, with MSG quantifying continuousspatial intensity variations across scales and MUC capturingcolor diversity, both complementing object-level informationfrom deep segmentation models. Additionally, we introducesurprise, an image-level \u201ccognitive\u201d feature, as a previouslyunderexplored dimension of visual complexity, representingthe degree to which the image as a whole deviates from ex-pected objects or events. We introduce a new dataset SVGcontaining images with unexpected or novel elements to showthat surprise contributes significantly to perceived complex-ity. We constructed our SVG dataset by manually selectingsurprising images and hence acknowledge the limitations inscalability and the potential subjectivity of this approach. Wehope our preliminary yet confirmatory results will inspire fu-ture research to develop larger datasets using objectively de-fined measure to further validate the link between surpriseand complexity.\nMSG versus Patch Symmetry and Edge Density Patchsymmetry (Kyle-Davidson et al., 2023; Olivia et al., 2004;Rosenholtz, Li, & Nakano, 2007) and edge density (Guo,Qian, Li, & Asano, 2018; Ciocca, Corchs, & Gasparini,2015) are two commonly used low-level features for cap-"}]}