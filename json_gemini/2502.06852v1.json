{"title": "EAP-GP: Mitigating Saturation Effect in Gradient-based Automated Circuit Identification", "authors": ["Lin Zhang", "Wenshuo Dong", "Zhuoran Zhang", "Shu Yang", "Lijie Hu", "Ninghao Liu", "Pan Zhou", "Di Wang"], "abstract": "Understanding the internal mechanisms of transformer-based language models remains challenging. Mechanistic interpretability based on circuit discovery aims to reverse engineer neural networks by analyzing their internal processes at the level of computational subgraphs. In this paper, we revisit existing gradient-based circuit identification methods and find that their performance is either affected by the zero-gradient problem or saturation effects, where edge attribution scores become insensitive to input changes, resulting in noisy and unreliable attribution evaluations for circuit components. To address the saturation effect, we propose Edge Attribution Patching with GradPath (EAP-GP), EAP-GP introduces an integration path, starting from the input and adaptively following the direction of the difference between the gradients of corrupted and clean inputs to avoid the saturated region. This approach enhances attribution reliability and improves the faithfulness of circuit identification. We evaluate EAP-GP on 6 datasets using GPT-2 Small, GPT-2 Medium, and GPT-2 XL. Experimental results demonstrate that EAP-GP outperforms existing methods in circuit faithfulness, achieving improvements up to 17.7%. Comparisons with manually annotated ground-truth circuits demonstrate that EAP-GP achieves precision and recall comparable to or better than previous approaches, highlighting its effectiveness in identifying accurate circuits.", "sections": [{"title": "1. Introduction", "content": "In recent years, transformer-based language models (Vaswani et al., 2017) have achieved remarkable success"}, {"title": "2. Related Work", "content": "Neural networks can be conceptualized as computational graphs, where circuits are defined as subgraphs that represent the critical components necessary for specific tasks"}, {"title": "3. Preliminaries", "content": "To better illustrate our motivation and method, in this section we will revisit previous methods for circuit discovery."}, {"title": "Integrated Gradients Method.", "content": "Integrated Gradients (IG) (Sundararajan et al., 2017) is a gradient-based attribution method in explainable AI that aims to quantify how each input feature contributes to a deep neural network's output. Generally, the idea is to evaluate how the model performance will be changed if we change the target feature of the input to the baseline input (or counterfactual input). It does so by estimating the accumulated gradients along a path from the baseline input to the target input. The performance of IG largely depends on two key hyperparameters:"}, {"title": "Circuit discovery.", "content": "Given a model G, which can be represented as a computational subgraph, a circuit $C\\subseteq G$ is a subgraph, where it can be represented as a set of edges in the circuit (Olah et al., 2020)."}, {"title": "Definition 3.1 (Computational Graph (Hanna et al., 2024b)).", "content": "A transformer LM G's computational graph is a digraph describing the computations it performs. It flows from the LM's inputs to the unembedding that projects its activations into vocabulary space. We define this digraph's nodes to be the LM's attention heads and MLPs, though other levels of granularity, e.g., neurons, are possible. Edges specify where a node's output goes; a node v's input is the sum of the outputs of all nodes u with an edge to v. A circuit C is a subgraph of G that connects the inputs to the logits."}, {"title": "Definition 3.2 (Circuit Discovery).", "content": "Given a full model G and a subgraph C, for any pair of clean and corrupted input (prompt) z and z', denote T as the task distribution, $E_G$ as the activations of G with input z, $E_C(z, z')$ as the activations of the subgraph when z is input, with all edges in G not present in C overwritten by their activations on z'. Moreover, denote $L(A)$ as a loss on the logits for activations A (with input z), which is used to measure the performance of subgraphs. Formally, circuit discovery can be formulated as\n$\\\\\\arg \\min_C E_{(z,z')\\in T}|L(E_C(z, z')) - L(E_G(z))|$.\nIn practice, we always use logit difference or probability difference as the loss L."}, {"title": "4. Saturation Effects in Circuit Discovery", "content": "In this section, we revisit the above-mentioned gradient-based automatic circuit identification methods, EAP and EAP-IG. Both of them determine edge importance by computing the activation difference multiplied by the loss gradient. This product approximates the metric difference in (5) and is used to evaluate each edge's contribution.\nSpecifically, we analyze a circuit edge (u, v) with its clean activation $x_u$ in the IOI dataset and examine how different input choices influence the gradient of the loss in both EAP and EAP-IG. EAP in (5) evaluates an edge's importance by computing the product of the metric's derivative at $x_u$ and the change in the edge's activation. However, as illustrated in Figure 1 (black point), this approach can be misleading: a nearly zero derivative at $x_u$ suggests that the edge has minimal influence and will not contribute to the attribution, even if the activation has a non-zero gradient at $x_u'$ and the difference in activations is significant.\nWe also conduct experiments under the same setting for EAP-IG in (6) with k = 5. As illustrated in Figure 1, we find that the gradient remains close to zero when $x_u'$ falls within the ranges [0, 0.2] and [0.8, 1], where the IG score changes slowly. In contrast, slight variations in the score occur only within the range $\\in$ [0.2, 0.8]. While this approach partially mitigates the zero-gradient issue in EAP, the nature of the chosen straight-line path inevitably leads it into regions where the gradient remains nearly zero ([0, 0.2] and [0.8, 1]). The perturbed activation inputs within these"}, {"title": "Definition 4.1 (Saturation Effects and Regions).", "content": "For an edge (u, v) in a circuit discovery task, its saturation regions refer to segments of the integration path where the gradient of the loss, $\\frac{\\partial L}{\\partial x_u}$, remains close to zero, reducing the sensitivity of L to activation changes. Saturation effects occur when scores accumulate in these regions, reducing the attribution's responsiveness to activation variations. This distortion ultimately compromises the reliability of circuit analysis, leading to unfaithful circuit evaluations."}, {"title": "5. Mitigating Saturation Effect via EAP-GP", "content": "In the previous section, we discussed how EAP suffers from the zero-gradient problem and how EAP-IG is affected by saturation effects. Our previous experiments show that the main reason EAP-IG gets stuck in the saturation region is that the integration path is a direct line between the clean and baseline input, which is independent of the full model G. Such a model-agnostic way will unintentionally make the gradient nearly zero for some perturbed activations. Thus, we need to construct an integral path that depends on the model to avoid the saturation region.\nTo address these issues, we propose Edge Attribution Patching with GradPath (EAP-GP). Unlike the pre-fixed"}, {"title": "6. Experiments", "content": ""}, {"title": "6.1. Experimental Setup", "content": "Dataset We evaluate model performance using six datasets: Indirect Object Identification (IOI), Subject-Verb Agreement (SVA), Gender-Bias, Capital-Country, Hypernymy, and Greater-Than (Hanna et al., 2024b). IOI tests the model's ability to identify indirect objects, while SVA assesses subject-verb agreement. Gender-Bias examines gender bias in language models, and Capital-Country evaluates tasks."}, {"title": "6.2. Experimental Results", "content": "This section compares the three methods based on our primary faithfulness metrics, with all experiments conducted on GPT-2 Small (117M). Additional experiments on GPT-2 Medium (345M) (see Figure 6 and Table 5) and GPT-2 XL (1.5B) (see Figure 7 and Table 6), along with examples of circuits identified by EAP-GP (see Figure 8), are reported in Appendix A.2.\nCircuit Faithfulness. We compare the faithfulness of identified circuits for three gradient-based circuit identification methods (EAP, EAP-IG, and EAP-GP) across six tasks under edge sparsity levels ranging from 96.9% to 99.9%, as"}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Datasets", "content": "Indirect Object Identification (IOI): The IOI task ((Wang et al., 2023)) involves inputs such as: \"When Amy and Laura got a snack at the house, Laura decided to give it to\"; where models are expected to predict \"Amy\u201d. Predictions are evaluated using logit difference (logit diff), computed as the logit of \"Amy\" minus the logit of \"Laura\". Corrupted inputs replace the second occurrence of \u201cLaura\u201d with a third name (e.g., \u201cNicholas\u201d), making \u201cLaura\u201d and \u201cAmy\u201d roughly equiprobable. We generate a dataset using (Wang et al., 2023)'s dataset generator.\nGender-Bias: The Gender-Bias task is designed to examine gender bias in language models. It provides inputs such as \u201cThe banker wished that\", where biased models tend to complete the sentence with \u201che\u201d. Bias is measured using logit difference (logit diff), computed as the logit of \"he\" minus the logit of \"she\u201d, or vice versa if the profession is male-stereotyped. Corrupted inputs replace female-stereotyped professions with \"man\" and male-stereotyped professions with \"woman\u201d, such as transforming \u201cThe banker wished that\u201d into \u201cThe woman wished that\u201d, prompting the model to generate the opposite pronoun. This task originates from (Vig et al., 2020) and was later analyzed in a circuit-based context by (Chintam et al., 2023).\nCapital-Country: In the Capital-Country task, models receive inputs such as \u201cPort Vila, the capital of\u201d, and are expected to output the corresponding country (Vanuatu). Corrupted instances replace the correct capital with another one, such as changing \"Port Vila, the capital of\u201d to \u201cNiamey, the capital of\u201d. Performance is evaluated using the logit difference, defined as the logit of the correct country (Vanuatu) minus the logit of the corrupted country (Niger).\nSubject-Verb Agreement (SVA): In the Subject-Verb Agreement (SVA) task, models are given sentences such as \u201cThe pilot the assistant\u201d and must generate a verb that matches the subject's number (e.g., \u201cis\u201d or \u201chas\u201d for pilot). In corrupted inputs, the subject's number is modified, such as changing \u201cThe pilot the assistant\u201d to \u201cThe pilot the assistants\u201d, causing the model to produce verbs with the opposite agreement. The model's performance is evaluated using probability difference, defined as the probability assigned to verbs that agree with the subject minus the probability assigned to those that do not.\nHypernymy: In the Hypernymy task, models must predict a word's hypernym (or superordinate category) given inputs such as \u201c, second cousins and other\u201d, where the correct answer is \u201crelatives\u201d. Corrupted inputs replace the target word with an instance from a different category, such as changing \u201csecond cousins and other\u201d to \u201crobins and other\u201d. Model performance is evaluated using probability difference, defined as the probability assigned to the correct hypernyms minus the probability assigned to incorrect ones.\nGreater-Than: In the Greater-Than task, models receive a sentence containing a chronological sequence of years as input and must predict the next year that follows the pattern. Given a clean input, such as \"The contract lasted from the year 1352 to the year 13\", the expected completion is the next valid number in the sequence. Corrupted inputs replace the last number with an incorrect continuation that disrupts the numerical pattern, such as \u201cThe contract lasted from the year 1301 to the year 13\u201d. Model performance is evaluated using probability difference (prob diff), defined as the probability assigned to the correct next number minus the probability assigned to the incorrect one (e.g., \"52\" in the example above).\nEach dataset consists of both positive and negative examples. Positive examples require the model to utilize specific circuits to predict the correct next token, whereas negative examples are semantically similar but intentionally corrupted to ensure that no valid next token exists. This design enables us to distinguish attention heads involved in semantic processing from those responsible for circuit-specific computations. To ensure fair comparisons, the number of positive and negative examples in each dataset is kept consistent with (Hanna et al., 2024b). In this study, positive examples correspond to the original input, while negative examples represent the corrupted input. Table A.1 provides an example of each task. Overall, For IOI, Gender-Bias, and Capital-Country, we also use Logit Difference defined as:\n$\\log P(correct) - \\log P(misleading)$,\nwhich measures the difference in log probabilities between the correct and misleading name/pronoun. For SVA, Greater-Than, and Hypernymy, we use Probability Difference, defined as:\n$\\sum P(y_{correct}) - \\sum P(y_{incorrect})$,\nwhich compares the probability of the correct answer with the sum of the probabilities of incorrect answers."}]}