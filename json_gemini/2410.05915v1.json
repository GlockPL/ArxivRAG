{"title": "Give me a hint: Can LLMs take a hint to solve math problems?", "authors": ["Vansh Agrawal", "Pratham Singla", "Amitoj Singh Miglani", "Shivank Garg", "Ayush Mangal"], "abstract": "While many state-of-the-art LLMs have shown poor logical and basic mathematical reasoning, recent works try to improve their problem-solving abilities using prompting techniques. We propose giving \"hints\" to improve the language model's performance on advanced mathematical problems, taking inspiration from how humans approach math pedagogically. We also test the model's adversarial robustness to wrong hints. We demonstrate the effectiveness of our approach by evaluating various LLMs, presenting them with a diverse set of problems of different difficulties and topics from the MATH dataset and comparing against techniques such as one-shot, few-shot, and chain of thought prompting.", "sections": [{"title": "1 Introduction", "content": "The ability to reason and logically solve complex mathematical problems is essential for progress in nearly every field, whether it be modeling complex environments, developing new algorithms, or engineering new devices. Recent works have explored the mathematical capabilities of Large Language models and have found them lacking in logic and basic mathematical reasoning [1] [2]. In this work, we propose \"hinting\" LLMs, by giving subtle guidance or clues, as a method to improve problem-solving capabilities on mathematical tasks and then compare its effectiveness against other prompting methods, such as one-shot [3], few-shot [3], and chain of thought prompting [4]. We evaluate our approach using the MATH dataset [5], consisting of math problems categorized into distinct classes based on subtopics such as algebra, probability, geometry, etc., with different levels of complexity (1-5). We test a diverse set of LLMs, including base language models, instruction fine-tuned models, models specifically tuned for mathematical tasks, and closed source models such as GPT-40-mini [6] and Gemini Flash [7].\nWe further examine robustness to adversarial prompts, misleading hints and clues of varying levels. We investigate how sensitive the models are to incorporating these incorrect hints as context, which may degrade performance, versus their ability to reject such misleading information. Through these experiments, we seek to contribute to the ongoing research on improving current state-of-the-art language model's reasoning capabilities and their practical applications in solving mathematical tasks[8]."}, {"title": "1.1 Background and Related Work", "content": "Various prompting methods have been shown to increase the accuracy of LLMs in solving complex problems that require understanding and reasoning [9]. A few popular methods being -\n1. One shot prompting [3]: Giving a single example problem and its final answer to the model in-context to learn from."}, {"title": "2 Method", "content": "We extracted hints by providing the problems in the MATH dataset to Gemini-1.5-flash model and prompting it to generate a hint for that question. These hints are then provided in context to the model along with the target problem to help the model reason about the task. We believe this is more aligned with how humans solve math problems by getting hints instead of the complete solution for an example problem, as in Chain of Thought, or just the final answer, as in one/few-shot approaches. We also test the adversarial robustness of models to being provided either a adversarial misleading hint, or a hint from a random question. We also generate similar versions for one/few shot and CoT prompting as shown in Figure 1. More details about the prompts can be found in Appendix A.\nWe evaluate on a diverse ensemble of LLMs ranging from base models to instruct fine-tuned models and math-finetuned LLMs, including nine open-source models and two closed-source models. (More details in C). We use the MATH dataset [5], which contains problems of seven different classes (e.g., algebra, geometry, etc.) of varying difficulty levels from 1 (easy) to 5 (hard). More details about the difficulty levels are given in Appendix B."}, {"title": "3 Experiments and Results", "content": "We evaluate 11 models for our experiments, using open-source implementations where possible and public API offerings for closed-source models. We evaluate 17 different types of prompting techniques, ranging from baseline to hinting to adversarial, along with one/few shot and CoT as given in Appendix A. The problem set consisted of all seven topics from the MATH dataset [5], with 100 problems for each topic. Exact details about the data split are given in Appendix B. We report the comparison by checking the fraction of question that the model got correct."}, {"title": "3.2 Evaluating Hint based prompting", "content": "We observe that hinting improves the performance of models, as shown in Figure 2. We attribute the poorer performance of other approaches to a lack of generalization. Approaches like chain of"}, {"title": "3.3 Evaluating Adversarial Hinting", "content": "We find that giving Adversarial hints drastically reduces the model's performance dropping it below CoT which performed the worst in our non-adversarial approaches as shown in Fig 2 and Table 1. We also apply this to few-shot[3] and one-shot[3] settings and find that adversarial hinting affects the performance in those cases as well. Inferior performance of CoT and it's prompting variants can be attributed to over-fitting to various factors within irrelevant information that influence the model's sensitivity to distractions based on lexical similarity. [14]."}, {"title": "3.4 Model-wise performance", "content": "While comparing the performance of different models, we observe that closed-source models exhibit the highest performance, followed by models fine-tuned for mathematical tasks, instruct-tuned models and finally base models. Additionally, variations in model size impact performance, with smaller models generally performing worse. However, performance also depends on the extent of fine-tuning; for example, the Qwen-2-Math-Instruct model[15] achieves comparable results to GPT-40-mini[6] as shown in Appendix E. Among the 7B and 8B models, Qwen-2-Math-Instruct[15] performs best, while Mistral-Instruct[16] ranks the lowest. Our observations further reveal that base models struggle"}, {"title": "4 Conclusion", "content": "In this work, we have evaluated the mathematical reasoning abilities of various models and approaches. Our results indicate that providing hints is more effective than giving direct answers because it guides the model to the correct solution instead of restricting its search space like few-shot[3], one-shot[3], and chain of thought[4] which lack generalization and only perform well when the example and target problems are very similar. Hence, for math problems with unknown solutions, it is better to provide the user's knowledge about the problem as hints to improve its reasoning and problem-solving capabilities than a full solution for a similar problem. Sensitivity to random and adversarial prompting[18] techniques demonstrated performance loss due to adverserial/random hints in few-shot[3], one-shot[3], and chain-of-thought[4] settings."}, {"title": "5 Limitations and Future Work", "content": "Our work mainly focuses on prompting LLMs with problems as textual input. Testing these techniques in multi-modal models like VLMs, where geometrical problems can be passed as image inputs, is a possible future direction. Further, our experiments involved a small subset of problems to test the models on due to computational limitations.\n1. Our techniques are yet to be tested on larger models like the Llama 3.1 70B, 450B[19],\nFalcon 180B[20], OPT 175B[21], etc., and other closed source models like Claude[22],\nBard[23], etc due to computational limitations.\n2. We compare the generated answers with the solutions using LLMs, which can introduce a\ndegree of error.\n3. These techniques can be further evaluated on the entire MATH dataset and other datasets\nsuch as GSM-8k[24], etc. to ensure a more exhaustive analysis."}, {"title": "A Prompting", "content": "We compare 17 different prompting techniques, including base-prompting techniques such as few-shot, one-shot, and chain-of-thought, and their variants involving hinting and adversarial prompting techniques for rigorous and exhaustive evaluation. We group the approaches as follows:"}, {"title": "A.1 Base Prompting techniques", "content": "In our baseline approach, we provide the models with the target questions and then prompt them to solve them. For One-Shot[3] and Few-shot prompting [3], we provide the model with example problems from the same class as the target problem. Since the dataset is divided into difficulty levels, we provide one Level-3 problem for one-shot and 5 examples, one from each level for few-shot prompting. We only provide the final numerical answer without any explanation for these approaches. More details about the difficulty levels are given in B. For Chain-of-Thought prompting[4], the model is given the complete step-by-step solution of the example problem in context instead of the final numerical answer only as shown in (Figure 4)\nOur final baseline prompting techniques involve:\n\u2022 Baseline: Only target question given\n\u2022 One-shot: (Table 6) One example question and final answer pair (Difficulty level: 3) and then the target question given\n\u2022 Few-shot: (Table 5) Giving five example questions and final answer pairs, each corresponding to one of the 5 difficulty levels, and then the target question\n\u2022 Chain Of Thought: (Table 6) Giving one example QnA pair with step-by-step-solution (Difficulty level: 3) and then the target question"}, {"title": "A.2 Hinting", "content": "For our baseline approach, we only provide hints for the target problem. In the one-shot[3] and few-shot hinting cases[3], example problems of the same category and its hint (without the final answer) are provided to the model. (Figure 5)\n\u2022 Baseline + Hinting: Providing each question with its corresponding correct hint\n\u2022 One-shot-Hinting: Providing one example Question and Hint pair (Difficulty level: 3) and then the target question\n\u2022 Few-shot-Hinting: Providing five example Question and Hint pairs, each corresponding to one of the 5 difficulty levels, and then the target question"}, {"title": "A.3 Adversarial Prompts", "content": "To test the robustness of our approach, for one and few-shot, we adversarially prompt the model with an example problem and the incorrect final answer to test how much the models make use of the problem-answer pairs provided. We further extend the idea of adversarial prompting to chain of"}, {"title": "B Dataset", "content": "We evaluate on a subset of the MATH dataset [5]. The MATH dataset consists of problems from various popular math competitions, including the AMC 12, AIME, and more. Owing to the structure of mathematics, these problems have a particular method of solving them with multiple related steps. Humans generally use problem-solving techniques and \u201cheuristics\" to solve such problems, thus making them a good metric for assessing a model's problem-solving and reasoning skills.[5] The dataset categorizes the problems into various categories and difficulties. The seven categories are Pre-algebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Pre-calculus. The problems are divided into five levels: level 1, including more straightforward questions; and level 5, including advanced questions.\nWe enhanced the dataset with hints and adversarial hints for our experiments. To generate hints for each problem in the dataset, we used the Gemini-1.5-flash model, prompting it to generate hints and adversarial hints related to the question. Due to computational limitations, we used a subset of the dataset, 100 problems from each topic (20 questions of each difficulty), leading to a total sample set of 700 problems."}, {"title": "C Experiments", "content": "All the prompting techniques were tested on a set of models, chosen in manner to ensure diversity for our experimentation's and testing. Nine open-sourced models and two closed-source have been used models as listed below:\n\u2022 Gemma-2-2b-it\u00b9"}, {"title": "E Tables", "content": "Table 4: Few-Shot Prompting Examples for All Math Categories used for our experimentation"}]}