{"title": "Robust Dynamic Facial Expression Recognition", "authors": ["Feng Liu", "Hanyang Wang", "Siyuan Shen"], "abstract": "The study of Dynamic Facial Expression Recognition (DFER) is a nascent field of research that involves the automated recognition of facial expressions in video data. Although existing research has primarily focused on learning representations under noisy and hard samples, the issue of the coexistence of both types of samples remains unresolved. In order to overcome this challenge, this paper proposes a robust method of distinguishing between hard and noisy samples. This is achieved by evaluating the prediction agreement of the model on different sampled clips of the video. Subsequently, methodologies that reinforce the learning of hard samples and mitigate the impact of noisy samples can be employed. Moreover, to identify the principal expression in a video and enhance the model's capacity for representation learning, comprising a key expression re-sampling framework and a dual-stream hierarchical network is proposed, namely Robust Dynamic Facial Expression Recognition (RDFER). The key expression re-sampling framework is designed to identify the key expression, thereby mitigating the potential confusion caused by non-target expressions. RDFER employs two sequence models with the objective of disentangling short-term facial movements and long-term emotional changes. The proposed method has been shown to outperform current State-Of-The-Art approaches in DFER through extensive experimentation on benchmark datasets such as DFEW and FERV39K. A comprehensive analysis provides valuable insights and observations regarding the proposed agreement. This work has significant implications for the field of dynamic facial expression recognition and promotes the further development of the field of noise-consistent robust learning in dynamic facial expression recognition. The code is available from [https://github.com/Cross-Innovation-Lab/RDFER].", "sections": [{"title": "I. INTRODUCTION", "content": "THE role of facial expressions in human communication has been extensively studied [1]\u2013[10]. Understanding emotional states through facial expressions is crucial in social interactions, making automatic recognition of such expressions a critical challenge in various fields such as speech emotion recognition [11], mental health diagnosis [?], driver fatigue monitoring [12], and metahuman technology [13]. While impressive progress has been made in Facial Expression Recognition (FER) [14], [15], the recognition of Dynamic Facial Expressions (DFER) is gaining increasing attention.\nThe emergence of extensive large-scale in-the-wild datasets, such as DFEW [16] and FERV39K [17], has led to the development of several methods [18]\u2013[22] for DFER. Previous studies [18], [19] have primarily utilized general video understanding techniques for dynamic facial expression recognition. Later on, Zhang et al. [23] propose an erasing attention consistency method to suppress the noisy samples during the training process automatically, Li et al. [20] identify a significant amount of noisy frames in DFER and Li et al. [21] recognize that the expressions in DFER exhibit considerable intra-class and minimal inter-class differences. Some studies have also explored the physical structure of the face [24], [25] and the movement of facial muscles [26] from a number of perspectives. In addition, in our previous work M3DFEL [27], we also significantly improved dynamic facial expression recognition by utilizing multi-example learning for Bag-level design against noise in dynamic expression recognition. However, we argue that some reported task-specific issues can be formed as general issues, for example, the weakly supervised problem contains the noisy frame problem, and face recognition also faces the problem of large intra-class variance and small inter-class variance. As shown in Fig. 1, in this study, we explore the datasets and formalize the hard and noisy sample issue present in DFER.\nThe issue of noisy labels is prevalent in most crowd-sourced datasets. The employment of the 'small loss trick' is a common approach to mitigate the effects of label noise. This approach considers samples with low loss values as clean and employs them to train the model. Several techniques such as loss reweighting [28], [29], label refurbishing [30], sample selection [31] and co-teaching have been developed based on this approach. However, the noisy label problem in DFER is more complex as the high-loss samples involves not only noisy samples, but also hard samples. Avoiding the learning of noisy"}, {"title": "II. RELATED WORK", "content": "Dynamic Facial Expression Recognition. In contrast to SFER techniques that focus solely on spatial features within an image, DFER approaches must consider both spatial and temporal information simultaneously. Transformer-based networks have gained popularity in recent literature for their ability to extract spatial and temporal information. Zhao et al. [18] introduce the dynamic facial expression recognition transformer (Former-DFER), comprising a convolutional spatial transformer (CS-Former) and a temporal transformer (T-Former). Ma et al. [19] propose the spatial-temporal Transformer (STT) to capture discriminative features within each frame and model contextual relationships among frames. To reduce the impact of noisy frames on the dynamic facial expression recognition (DFER) task, a dynamic-static fusion module is used to extract more robust and discriminative spatial features from both static and dynamic features [20], [21]. Wang et al. [22] propose the Dual Path multi-excitation Collaborative Network (DPCNet) to learn critical information for facial expression representation from fewer key frames in videos. In addition, Zhang et al. proposed to try to improve dynamic expression recognition by means of increasing zero-shot generalization ability [35], sample feature diversification [36] and symmetric noisy labels [37].\nLong-tailed noisy label problem. The topic of long-tailed noisy label problem has gained prominence due to its relevance to real-world applications and its inherent complexity. The effectiveness of existing noisy label methods, which rely on small loss tricks, has been found to be limited when dealing with long-tail situations. Therefore, there is a need for a new framework to address this challenge. RoLT [33] has identified this issue and proposed a prototypical noise detection method that is robust to label noise by designing a distance-based metric. JA+SL [32] has demonstrated that self-supervised learning approaches are effective in handling severe class imbalance. Additionally, H2E [34] has defined long-tailed noises as 'hard' noises and proposed the Hard-to-Easy framework to convert 'hard' noises to 'easy' ones."}, {"title": "III. METHODOLOGY", "content": "A. The Agreement-Based Loss Reweighting\nInspired by decoupling [38], which leverages the disagreement between two networks to discern noisy labels, our aim is to employ the discordance of a single network across distinct frames to differentiate between samples. Given that the videos in DFER datasets are limited to a few seconds, we anticipate that facial movements between frames will be subtle and unlikely to have a significant impact on emotion recognition results. Nevertheless, much like how identical networks with different initializations can exhibit disagreement on the same input, a network may exhibit discordance on inputs with minor variations. By assessing such disagreement, we can determine whether a given sample is challenging or not.\nAs previously mentioned, the presence of hard and noisy samples presents a challenge in distinguishing between the two using the small loss trick. Consequently, the model must concentrate on learning from the hard samples while disregarding the noisy labels. In the context of DFER, the factors that contribute to the difficulty of a sample include complex facial movements, ambiguous expressions, and environmental distractions. In addition to these obstacles, temporal changes in the samples pose a significant challenge in comparison to those in SFER. The degree of disagreement between the temporal changes and the expression recognition results serves as a direct indicator of the sample's difficulty. Notably, facial movements typically do not have a substantial impact on the final outcome. Therefore, a strong disagreement is a clear indication of a hard sample.\nTo put this formally, given a video $V \\in \\mathbb{R}^{T \\times C \\times H \\times W}$ and split it into frames or clips $V_i$. By using a recognizer, we obtain the facial expression of them,\n$E_i = f(V_i)$.\n(1)\nThe agreement $A$ can be calculated by counting the proportion of the most predicted class. For example, when four clips sample from one video are predicted as Neutral, Surprise, Happy and Happy, the video is considered having two Happys in four clips and the corresponding agreement is 0.5.\nOur implementation employs fundamental techniques to establish and manage hard and noisy samples, thus enabling the straightforward observation of the agreement's impact. A hard sample is defined as a sample with minimal agreement, while a falsely labeled sample demonstrates substantial agreement but significant loss. To this end, we ascribe two ranks to all samples based on their agreement and loss, and subsequently set two thresholds. Specifically, the $t_h$ percent of samples with the minimum consistency are considered as hard samples, the $t_n$ percent of samples with the maximum agreement and maximum loss are considered as noisy samples. Any samples falling outside these thresholds are deemed ordinary. Consequently, the loss calculation can be derived using the following equation:\n$\\text{loss} = \\begin{cases} \\lambda_{\\text{noisy}} \\times \\text{Cross\\_Entorpy}(x, y) & x \\in \\text{noisy} \\\\ \\lambda_{\\text{ordinary}} \\times \\text{Cross\\_Entorpy}(x, y) & x \\in \\text{ordinary} \\\\ \\lambda_{\\text{hard}} \\times \\text{Cross\\_Entorpy}(x, y) & x \\in \\text{hard} \\end{cases}$\n(2)\nB. The Key Expression Re-sampling Framework\nDue to the presence of irrelevant movements in a proportion of the samples, the model may become confused. To address"}, {"title": "C. The Dual-Stream Hierarchical Network", "content": "stream and the emotion-movement stream. The movement-emotion stream analyzes the movement in the cropped clips first, followed by the emotional change in the processed clips. Conversely, the emotion-movement stream analyzes the emotional change through frames with stride first, followed by the analyzing the movement change. The output feature is the concat of the two streams and followed by a fully connected layer to predict the final expression."}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets and Metrics\nThe DFEW dataset [16] is a large-scale collection of dynamic facial expression samples obtained from over 1,500 movies worldwide and comprises more than 16,000 video clips. Methods are evaluated using the 5-fold cross-validation setting provided by DFEW, with the weighted average recall (WAR) and unweighted average recall (UAR) used as evaluation metrics.\nFERV39K [17] is currently the largest in-the-wild DFER dataset, containing 38935 video clips collected from four scenarios and subdivided into 22 fine-grained scenes. The training and testing sets divided by FERV39k are directly used for a fair comparison.\nB. Implementation Details\nOur research framework is constructed using PyTorch, leveraging Tesla V100 GPUs. The backbone network and sequence model are comprised of the ResNet50 pretrained on ImageNet and BiLSTMs. A subset of 20% of the samples are identified as hard samples, and their losses are augmented by a factor of 1.5. Additionally, 10% of the samples are identified as noisy samples and are not included in the backward propagation process. The models are trained for 100 epochs, with ten warm-up epochs, using the AdamW optimizer and cosine scheduler. The learning rate is set to 7e-4, with a minimum learning rate of 7e-6, and a weight decay of 0.05. The batch size is set to 128, and label smoothing is utilized with a value of 0.1. Augmentation techniques utilized include random cropping, horizontal flipping, and 0.4 color jitter. The evaluation metrics employed are the weighted average recall (WAR) and unweighted average recall (UAR). Further experimentation and analysis utilizes DFEW [16]."}, {"title": "C. Performance Comparison", "content": "We compare our method with the state-of-the-art methods on two in-the-wild datasets DFEW and FERV39K. The WAR is used as a primary measure of accuracy to demonstrate the comprehensive performance of the method. Meanwhile, the UAR denotes the mean accuracy across all classes, providing insight into balanced performance among various categories.\nIn this study, the results on DFEW are presented in Table I, where experiments are conducted using 5-fold cross-validation. Our proposed framework yield superior outcomes in both WAR and UAR, surpassing the performance of GCA+IAL [21] by 0.49%/1.22% for WAR/UAR. Additionally, Table I provides the results for each expression, with a detailed analysis of the suboptimal performance of Disgust and Fear presented in Section IV-G.\nTable II presents the results for the FERV39K dataset, which is known for its complexity and lower accuracy compared to DFEW. Our proposed framework demonstrates superior performance when compared to 3DResNet18 [43] and R18+LSTM [17], with improvements of 11.03% and 5.65% for WAR, and 9.80% and 5.55% for UAR, respectively, indicating its effectiveness. Furthermore, our approach outperforms the Transformer-based GCA+IAL [21]. It is noteworthy that FormerDFER stands out as a method that prioritizes UAR over WAR in FERV39K, resulting in higher accuracy for smaller classes such as Disgust and Fear, but lower overall performance."}, {"title": "D. Ablation Study", "content": "Evaluation of different network configurations. The present study assesses various network configurations and reports the findings in Table III. The elimination of the re-sampling framework results in the removal of the global summary. The ablation analysis shows that the re-sampling framework effectively detects key frames or clips and discards irrelevant frames, thereby enhancing performance by 1.57%/1.04% of WAR/UAR. Additionally, the global summary offers valuable insight into the context of the expression and enhances model performance. The ablation of the Dual-Stream Hierarchical Network involves using a single movement encoder and an emotion encoder consecutively, and results indicate that the dual-stream architecture facilitates better learning of short-term facial movements and long-term emotional changes. Moreover, the dual-stream hierarchical architecture outperforms the ensemble model of two sequence models without the hierarchical architecture, exhibiting an increase in performance by 0.51%/0.24% of WAR/UAR. Overall, the proposed architecture effectively disentangles short-term facial movements and long-term emotional changes, resulting in improved representation of facial expressions.\nEvaluation of loss reweighting strategies. The result are presented in Table IV. In our experimental approach, we aim"}, {"title": "E. Effectiveness and Scalability", "content": "The fundamental structure of RDFER is relatively straightforward and can be replicated with any video model as the underlying structure. Consequently, further studies were conducted to demonstrate the scalability of the proposed framework. Specifically, different video backbones were adopted for the framework, including R(2+1)D and MC3 provided by Torchvision, CNN-LSTM and the reproduced Former-DFER [18]. The experiments were conducted on the first fold of DFEW [16].For each method, the first line represents setting the bag size to one and removing the Instance Aggregation Module, and the second line represents setting the bag size to four. During the adoption of Former-DFER, it was found to be sensitive, often resulting in training failure and slightly lower results. As demonstrated in Table VI, the implementation of all methods has been observed to enhance WAR by approximately two points and UAR by one point within the designated framework. This outcome signifies the efficacy and scalability of the proposed framework.Notably, despite the augmented parameter count, the framework imposes a negligible computational cost of approximately 0.05G FLOPs. It is anticipated that subsequent research will enhance the framework in a manner analogous to the advancement in video understanding, SFER, and MIL."}, {"title": "F. Analysis", "content": "The proportion of samples with different agreements and expressions. The study is carried out using the first fold of DFEW. The findings are presented in Table V. The video data are divided into four clips, and the level of agreement varied from one-quarter to four-quarters. The results indicate that the model was able to achieve agreement in the majority of the samples (71.46%), with only 0.03% of samples being predicted as four different types of clips. Furthermore, 18.24% of samples have one clip that differed from the other clips. Additionally, different facial expressions exhibits varying degrees of agreement. Approximately 92% of Happy result in full agreement, while Sad, Neutral, Angry, and Disgust achieve agreement in around 65% to 75% of the samples. Only 55.10% and 53.59% of samples result in full agreement for Surprise and Fear, respectively. The results reveal notable differences in agreement among the facial expressions. Apart from the impact of class imbalance, classes with a comparable number of samples also display distinct agreement distributions. Specifically, Happy have approximately 26% more samples with full agreement than Neutral, implying that Neutral is more perplexing than Happy.\nThe accuracy of samples with different agreements and expressions. The study is carried out on the first fold of the DFEW dataset, and the outcomes are presented in Table VII. Notably, the findings indicate a conspicuous trend whereby samples with higher levels of agreement exhibit greater accuracy. This observation implies that agreement can serve as a reflection of sample difficulty, thereby supporting the notion that low/high agreement samples correspond to hard/easy samples. Similarly, Table V demonstrates that, in terms of expressions, Happy is the easiest expression to classify with high levels of agreement and accuracy. Conversely, Fear and Disgust exhibit low accuracy, with the former displaying approximately 55% full agreement, but significantly lower accuracy than Surprise. These results suggest that the confusion surrounding Fear may be attributed to spatial rather than temporal issues."}, {"title": "G. Visualization", "content": "In order to conduct a more thorough evaluation of the efficacy of our approach, we undertake studies involving visualization.\nSample Analysis. Fig. 3 displays visualizations of samples exhibiting varying degrees of agreement. In Fig. 3(a), four frames are grouped together to form one clip, resulting in four distinct predictions. The samples present a significant challenge, as the left sample exhibits notable variations in posture and the subject's dark complexion detracts from the clarity of facial characteristics, akin to shadowing, and the right sample showcasing multiple expressions such as Happy, Surprise, Angry, and Fear. Mislabeling such samples would have a limited impact on the model's performance. Samples in Fig. 3(b) exhibit significant changes in lighting and pose."}, {"title": "V. DISCUSSION", "content": "Our approach is inspired by the Co-teaching series, which leverages model disagreement to update networks. However, Co-teaching requires training two networks simultaneously, resulting in a doubling of parameters and training time. Therefore, we investigate whether we can update a single network using the disagreement of one model towards different sampled clips of the video instead.\nDecoupling [38] has demonstrated that samples with disagreement are valuable for learning. In the context of the DFER task, where facial movements vary across frames, our experiments have revealed that the model exhibits disagreement on different frames or clips within a single video. This characteristic implies that these samples are difficult to learn from due to their intricate facial movements and ambiguous expressions. Conversely, samples with high agreement are generally simpler. As the model confers high confidence to easy samples and low confidence to difficult ones, noisy-labeled easy samples carry more weight than noisy-labeled difficult samples. Thus, we define samples with high agreement and high loss as noisy samples that require less intensive learning, while samples with low agreement are considered difficult samples that necessitate more rigorous learning.\nOur proposed approach for agreement presents a novel dimension to the current methods for handling noisy labels. While the existing methods rely on the small loss trick, our method has the added advantage of being able to differentiate between hard and noisy samples. Furthermore, it can be used alongside the small loss trick. Our simple implementation has yielded satisfactory results, and we posit that more complex frameworks can be employed to enhance the learning of hard and noisy samples. This could potentially replace the manual definition of these samples and the hyper-parameters of losses with established theoretical frameworks, thereby unlocking the full potential of the agreement approach. However, it must be noted that the approach of agreement requires the acquisition of outcomes from each frame in a given sample, thereby limiting its feasibility in tasks resembling DFER."}, {"title": "VI. LIMITATION", "content": "Although RDFER did indeed achieve SOTA results, our current work is not without limitations that require further investigation.\nThe presence of hard and noisy samples represents a significant challenge that has been identified in our work, which warrants further investigation. Although notable results have been achieved by optimizing for these two sample types, there may still be other noisy samples, such as those caused by the inherent limitations of muscle motor units constrained by biological factors. Furthermore, additional statistical and systematic noise issues require further investigation.\nFurthermore, the representational learning was enhanced by the separation of short-term movements from long-term mood changes, which resulted in an improvement in the effect. Nevertheless, a potential weak relationship between movements and mood changes may still impact the efficacy of representational learning.\nIn conclusion, although experiments were conducted on mainstream datasets with favorable outcomes, there is still a considerable distance to be traversed before real-world applications can be considered viable. Furthermore, larger samples and data are required to validate the effect of robust dynamic expression recognition. It is imperative that these limitations are addressed if dynamic expression recognition techniques and their applications targeting the field of computational visual media are to advance."}, {"title": "VII. CONCLUSION", "content": "This article identifies the complex nature of the DFER problem, which involves hard and noisy samples that make the small loss trick ineffective. To address this issue, we propose a novel approach that involves discerning between hard and noisy samples using prediction agreement across various sample frames. This methodology strengthens hard sample learning while attenuating the learning of noisy samples. Additionally, we introduce a Key Expression Re-sampling Framework and a Dual-Stream Hierarchical Network to improve the model's representation learning ability and locate critical expressions in a video. We discuss the advantages and potential of the proposed agreement, and provide visualization of a set of samples with different agreements. Statistical analysis about the proportion and accuracy of the samples with different agreements and expressions are provided to discuss the property of agreement. Our experimental results on benchmark datasets demonstrate the superiority of our method over existing state-of-the-art approaches."}]}