{"title": "fairBERTs: Erasing Sensitive Information Through Semantic and Fairness-aware Perturbations", "authors": ["Jinfeng Li", "Yuefeng Chen", "Xiangyu Liu", "Longtao Huang", "Rong Zhang", "Hui Xue"], "abstract": "Pre-trained language models (PLMs) have revolutionized both the natural language processing research and applications. However, stereotypical biases (e.g., gender and racial discrimination) encoded in PLMs have raised negative ethical implications for PLMs, which critically limits their broader applications. To address the aforementioned unfairness issues, we present fairBERTs, a general framework for learning fair fine-tuned BERT series models by erasing the protected sensitive information via semantic and fairness-aware perturbations generated by a generative adversarial network. Through extensive qualitative and quantitative experiments on two real-world tasks, we demonstrate the great superiority of fairBERTs in mitigating unfairness while maintaining the model utility. We also verify the feasibility of transferring adversarial components in fairBERTs to other conventionally trained BERT-like models for yielding fairness improvements. Our findings may shed light on further research on building fairer fine-tuned PLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Pre-training and fine-tuning of language models (LMs) have become a new paradigm in natural language processing (NLP), which have brought NLP to a new era. Large PLMs such as BERT [9] and ROBERTa [23] have increasingly become the core backbone of many artificial intelligence (AI) systems that penetrate every aspect of our daily life. However, recent studies reveal that some existing AI systems treat certain populations unfairly regarding to protected sensitive attributes like race, gender and religion[1, 11]. Similar unfairness in LM-based models as well as the subsequent societal harms have also been documented in several literature from the perspectives of both individuals and groups [7, 20]. Specifically, a real-world case of decision discrimination existed in an extensively used BERT model deployed on HuggingFace\u00b9 is exemplified in Figure 1. Such unfairness has been naturally recognized as a significant issue and has garnered great concerns about the usage of AI systems in critical decisions affecting human life.\nThe unfairness in LMs can arise from any stage throughout the model life cycle. Concretely, in the pre-training stage, large-scale unlabeled corpora are collected from the web without manual censorship, which may include individual as well as social stereotypes, thus resulting in biased PLMs. Moreover, the biases in PLMs are usually hard to detect. In the fine-tuning stage, distributional bias in labeled dataset may mislead models to learn false correlations between data and labels, thus leading to unfair decisions. Considering the tremendous amount of practical applications even include the high-stakes applications (e.g., toxicity detection, spam filtering) deployed in real world, it is crucial to ensure that the decisions of LMs do not reflect discriminatory behaviors toward certain populations."}, {"title": "2 RELATED WORKS", "content": "Pre-trained Language Models (PLMs). Pre-training and fine-tuning of PLMs have basically become a new paradigm in NLP, which benefit various downstream NLP tasks and avoid training new models from scratch. Typically, Devlin et al. [9] has achieved a milestone by proposing BERT, a novel PLM pre-trained via the masked language modeling task, i.e. first corrupting the input by replacing some tokens with '[MASK]', and then training a model to reconstruct the original tokens. Liu et al. [23] improved BERT to ROBERTa by replacing the original static masking with dynamic masking, thus further boosted the model performance. Instead of masking the input, Clark et al. [5] introduced a replaced token detection pre-training task by corrupting the input via replacing some tokens with plausible alternatives sampled from a small generator network, which was proved to be more sample-efficient. Although these PLMs have both exhibited great superiority in promoting the performance on various downstream tasks, none of them have taken the issues of model bias or unfairness into consideration in the early stages of design, pre-training as well as fine-tuning.\nFairness and Adversarial Robustness in NLP. The issues of unfairness in Al systems have attracted significant attentions in the research community. Substantial efforts have devoted to the definition, measurement and mitigation of biases in machine learning [15, 21, 28]. As a result, a variety of definitions of algorithmic bias have been proposed, which can be summarized as individual fairness [12] and group fairness [14, 19] in terms of individual and group levels. Concretely, individual fairness is intuitively motivated by the similar treatment principle which requires that similar individuals should be treated similarly. Group fairness requires model decisions to be equal across different groups defined by sensitive attributes (e.g., gender, race). In other words, the outputs of models should be independent of the sensitive attributes.\nIn NLP domain, the bias is routinely understood in intrinsic bias and extrinsic bias. Bolukbasi et al. [3], Caliskan et al. [4] and Liang et al. [22] have made one of the first efforts to point out the intrinsic bias inherent in pre-trained word embeddings or representations and propose the corresponding debiasing algorithms. However, the extrinsic bias is generally associated with specific downstream tasks, measuring and mitigating intrinsic bias in the pre-trained representations may not necessarily solve the unfairness issues of models learned by fine-tuning LMs [30]. For extrinsic bias, Dixon et al. [11] introduced how to counteract the bias in dataset by strategically adding data through a novel data re-balancing technique. Park et al. [25] proposed to reduce gender bias by augmenting training data via identifying gender related tokens and swapping them with equivalent opposite ones. Similarly, Garg et al. [16] addressed the fairness concerns in text classification via counterfactual augmentation and sensitive tokens blindness. Qian et al. [26] proposed to fine-tune BPLMs on demographically perturbed data by collecting a large dataset of human annotated text perturbations and training a neural perturbation model. Although these methods are innovative, they are limited in practice due to at least one of the following reasons: (i) it is not trivial to accurately identify the sensitive attributes as well as distributional bias in data, while the effect will directly affect the efficacy of debiasing; (ii) swapping or blinding the sensitive attributes may drastically alter the semantics of inputs, which will"}, {"title": "3 METHODOLOGY", "content": "Given a text input $x \\in X$ with the ground-truth label $y \\in y$, a finetuned BLPM classifier $F: X \\rightarrow Y$ which maps from the sample space X to the label space Y. Denote by $h_c$ the latent representation of $x$ in F, the classification function $f(.)$ makes a prediction $\\hat{y}$ based on $h_c$, i.e., $\\hat{y}= f (h_c)$. Denote by $h_s$ the sequence output of $x$ in F, by $z \\in Z$ the sensitive attribute (e.g., gender, race and nationality) related to $x$, and by D a discriminator to predict $z$ based on $h_c$. In this paper, we aim to mitigate unfairness born with BPLMs by erasing sensitive attribute z from latent representation $h_c$ via semantic and fairnessaware perturbations $\\delta$ generated by a learned generator G on $h_s$, i.e., $\\delta=G(h_s)$, such that the classifier F would not correlate predictions with protected sensitive attributes. Therefore, our objective can be formalized as follows:\n$\\begin{aligned}\nD(h_c+\\delta) D(h_c)=z,\\\\\ns.t. f(h_c+\\delta)=f(h_c)=y.\n\\end{aligned}$                                                                                                                 (1)\nThe framework of fairBERTs is illustrated in Figure 2, which consists of three components, i.e., the BPLM, the adversarial debiasing component, and the task-specific classifier. In this part, we will detail the design of each component.\nBPLMs. In BPLMs, the final hidden state corresponding to a special token [CLS] is routinely used as the aggregate sequence representation for classification tasks. In this paper, we denote this hidden state as $h_c$ and denote the feature extractor for $h_c$ as $g_c ( . )$. Consequently, the latent representation $h_c$ is obtained by $h_c = g_c(x)$. The sequence representation consisting of the final state of each token in x is usually used for sequential tasks owing to the semantic-rich context information it encodes. Similarly, we denote this sequence representation as $h_s$ and denote the corresponding extractor as $g_s ()$, and $h_s$ is then obtained by $h_s = g_s(x)$. Intuitively, since $h_c$ is directly correlated with the final predictions, the core essence of bias mitigation in fairBERTs is to eliminate the false correlations between ground-truth labels and protected sensitive attributes by erasing sensitive information from $h_c$.\nAn intuitive idea to make $h_c$ indistinguishable from sensitive attribute z as well as preserving its classification utility is to destroy the sensitive information encoded in $h_c$ through adversarial perturbations generated by GANs. To guarantee the semantic and fairness properties of generated perturbations, we generate them based on the semantic-rich sequence representation $h_s$.\nAdversarial debiasing GANs. Different from the traditional GANs, there are two discriminators in our adversarial debiasing framework in addition to a conventional generator for generating semantic and fairness-aware perturbations $\\delta$. As shown in Figure 2, for each input text x, the generator G takes $h_s$ as input to generate a fairness-aware perturbation mask $\\delta=G(h_s)$ of the same dimension"}, {"title": "3.1 Problem Definition", "content": "3.1                                                                                                                                                  Method"}, {"title": "3.2 Method", "content": "Adversarial debiasing GANs. Different from the traditional GANs, there are two discriminators in our adversarial debiasing framework in addition to a conventional generator for generating semantic and fairness-aware perturbations $\\delta$. As shown in Figure 2, for each input text x, the generator G takes $h_s$ as input to generate a fairness-aware perturbation mask $\\delta=G(h_s)$ of the same dimension with $h_c$. The fair classification representation $h_c'$ is then obtained by superimposing $\\delta$ into $h_c$, i.e.,\n$h_c' = h_c + \\delta = h_c + G(h_s)\n= g_c(x) + G(g_s(x)).$                                                                                                         (2)\nThe first discriminator D tries to distinguish sensitive attribute z from the perturbed latent representation $h_c'$. The second discriminator F is the final classifier we train for the target downstream tasks, which maps from sample space X to target label space y based on $h_c'$ and guarantees that $h_c'$ is utility-preserving.\nConcretely, in fairBERTs, we implement the discriminator D with multiple layers of fully connected networks activated by Leaky ReLU [24]. In the arms race of the debiasing game, the goal of D is to predict z as well as possible. To achieve such adversarial goal, we let D make prediction mainly relied on $h_c'$. In the meantime, we also provide the primitive latent representation $h_c$ for D with a constraint to facilitate better learning of D. Therefore, the optimization objective of D can be formulated as\n$\\begin{aligned}\n\\mathcal{L}_D &= \\mathcal{J}(D(h_c'),z)+\\alpha\\mathcal{J}(D(h_c),z)\\\\\n&= \\mathcal{I}(D(g_c(x)+G(g_s(x))), z)\\\\\n &+\\alpha \\mathcal{J} (D(g_c(x)), z),\n\\end{aligned}$                                                                                   (3)\nwhere $\\mathcal{I}(.)$ is the cross-entropy loss and $\\alpha > 0$ is a hyper-parameter to balance the two parts.\nIn contrast to D, the generator G aims to make it hard for D to predict z, while also ensuring that the generated perturbations would not destroy the semantic and classification utility of the original representation. Therefore, there are also two parts in the optimization objective of G. The first part is for the fairness purpose, which can be defined as\n$\\begin{aligned}\n\\mathcal{L}_E &= -\\mathcal{I} (D(h_c'), z)\\\\\n&= -\\mathcal{I}(D(g_c(x)+G(g_s(x))), z).\n\\end{aligned}$\nThe second part is for the utility-preserving purpose, which is also viewed as the optimization objective of F. Thus, it can be formalized as\n$\\begin{aligned}\n\\mathcal{L}_T &= \\mathcal{I} (f(h_c')), y)+\\epsilon\\mathcal{J} (f(h_c), y)\\\\\n&= \\mathcal{I}(f(g_c(x)+G(g_s(x))), y)\\\\\n &+\\epsilon \\mathcal{I} (f (g_c(x)), y),\n\\end{aligned}$                                                                                      (5)\nwhere $\\epsilon > 0$ is a small value that controls the regularization of the classification loss. Hence, the final loss function $\\mathcal{L}_G$ of G is calculated by\n$\\mathcal{L}_G = \\mathcal{L}_E + \\beta\\mathcal{L}_T$                                                                                       (6)\nin which $\\beta > 0$ is another hyper-parameter that balances the adversarial goal and classification goal. Notably, we also implement the generator G with very simple structures to reduce the complexity of convergence in the training stage.\nTask-specific Classifier. Finally, the fair representation $h_c'$ can be employed for the downstream task-specific classification. Concretely, the final prediction result $\\hat{y}$ of x is determined by\n$\\hat{y} = f(h_c') = f(g_c(x) + G(g_s(x))),$                                                                                       (7)\nwhich would be independent of the protected sensitive information."}, {"title": "3.3 Learning of fairBERTS", "content": "The training algorithm of fairBERTs is detailed in Algorithm.1, in which the generator G plays a mini-max game with the discriminator D and they are updated alternatively during the whole training process. More specifically, the mini-max optimization objectives is formalized as\n$\n\\begin{aligned}\n\\mathop{\\arg \\max}_{D} \\mathop{\\min}_{G}  \\mathcal{I} (D(h_c'), z)+\\alpha\\mathcal{J} (D(h_c),z)-\\beta\\mathcal{L}_T,\n\\end{aligned} (8)\n$\ns.t. $h_c = g_c(x)$, $h_c' = g_c(x) + G(g_s(x))$,\nwhere D maximizes its ability to distinguish the protected sensitive attribute z while G optimizes reversely by minimizing the ability of the discriminator D.\nSpecifically, to assist in learning a better generator G, we propose counterfactual adversarial training by borrowing from conventional adversarial training schemes. In contrast to conventional adversarial training, our method aims to flip the sensitive labels by substituting"}, {"title": "4 EXPERIMENTS", "content": "In this section, we first detail our experiment setup. Then, we qualitatively and quantitatively evaluate the effectiveness of fairBERTs across different models and datasets. Finally, we verify the transferability of fairBERTs by transferring adversarial perturbations generated by G to other BERT-like models."}, {"title": "4.1 Experimental Setup", "content": "Dataset. We evaluate the proposed fairBERTs on two public real-world datasets, i.e., Jigsaw Toxicity2 and Yelp Sentiment [10].\n\u2022 Jigsaw Toxicity. It is commonly used in the toxicity detection tasks, which contains millions of public comments from the Civil Comments platform with fine-grained annotations for toxicity. Besides, a subset of samples have been labelled with a variety of identity attributes that are mentioned in the comment, and we take gender as the protected information. Since each sample could be labelled by multiple human raters, we take the majority of the annotations as the final class label and sensitive label in our experiment.\n\u2022 Yelp Sentiment. Each sample in this dataset contains a gender annotation with the corresponding confidence value. We use a released ROBERTa\u00b3 model which is trained on above 58M tweets and fine-tuned for sentiment analysis to annotate the sentiment of each sample as its target class label. Similarly, we reserve those samples with both sentiment and gender annotation confidence greater than 0.85 for the sentiment analysis task.\nBaselines. For comprehensive fairness comparison, we consider four classical baselines in our experiment:\n\u2022 Vanilla. It refers the models directly trained on the original dataset without bias mitigation. We adopt two widely used PLMs as the vanilla baselines, i.e., BERT-base (BERT) and ROBERTa-base (ROBERTa).\n\u2022 Fairness Through Unawareness (FTU). It is an intuitive method to achieve fairness by prescribing not to explicitly employ the protected sensitive features when making automated decisions [12]. We denote the scheme for blinding sensitive tokens during training as FTU-I, and denote the scheme that masks sensitive tokens in both the training and inference stages as FTU-II.\n\u2022 Counterfactual Logit Pairing (CLP). It is a kind of data augmentation through counterfactual generation by penalizing the norm of the difference in logits for pairs of training examples and their counterfactuals [16], which encourages the model to be robust to sensitive terms.\nMetrics. We adopt five metrics to evaluate the performance of fairBERTs and the compared baselines from the perspective of both model utility and fairness. Concretely, we first use the accuracy (Acc.) metric to evaluate the overall model utility on all test samples. We then leverage three commonly acknowledged fairness metrics, i.e., demographic parity difference (DPD), equal opportunity difference (EOD) and disparate impact ratio (DIR), for the fairness evaluation and comparison. The detailed definitions and calculation formulas can be found in [28]. We also utilize the counterfactual token fairness (CTF) proposed in [16] for measuring counterfactual fairness. In our paper, we modified it by calculating the ratio of prediction-holding samples after replacing the sensitive tokens from one group with the those counterparts in the opposite group.\nImplementation. We apply two different backbone BPLMs (i.e., BERT-base and RoBERTa-base) in fairBERTs and make comparisons with the corresponding vanilla BERT and RoBERTa, respectively. All experiments are conducted on a server with two Intel(R) Xeon(R) E5-2682 v4 CPUs running at 2.50GHz, 120 GB memory, 2 TB HDD and two Tesla P100 GPU cards. To fairly study the performance of fairBERTs and baselines, the optimal hyperparameters such as learning rate, batch size, maximum training epochs, and dropout rate are tuned for each task and each model separately. We repeat the main experiments three times and report the mean and variance of the performance."}, {"title": "4.2 Qualitative Evaluation of Fairness", "content": "We first qualitatively evaluate the effectiveness of fairBERTs in terms of model fairness by visualizing the interpretation results of model decision on several examples as well as the latent model representations used for downstream tasks.\nModel Interpretation. Concretely, we leverage LIME [29], a famous model-agnostic interpretation method, to interpret the decision basis of fairBERTs and the vanilla BERT model over two representative examples sampled from the two datasets. The interpretation results are illustrated in Figure.3.\nFor the first example on toxicity detection task, we obviously observe the existence of bias in the vanilla BERT model against the female group since the protected sensitive token \u201cwomen\u201d contributes positively to the toxicity in the model prediction. Observed from the case of sentiment analysis, the vanilla BERT model also relies heavily on a feminine name entity \u201cSally\u201d instead of putting attention to the real sentimental token \u201cgreat\u201d and \u201clush\u201d when making prediction. This indicates that the vanilla models conventionally trained on biased datasets indeed result in unfairness in decisionmaking. This also suggests that simply blinding sensitive tokens from the inputs does not necessarily guarantee the fairness constraints, because sensitive information can sometimes be reflected from some other entities (e.g., the gender information can be inferred from the name entity \u201cSally\u201d even though the sentence does not directly contains sensitive tokens).\nComparatively, the decisions of fairBERTs are truly made depending on on the real toxic words (i.e., \u201crapist\u201d and \u201cliar\u201d) and sentimental words (i.e., \u201cgreat\u201d and \u201clush\u201d), which seems to be more objective and fair, thus qualitatively demonstrating that fairBERTs makes more trustworthy decision independent of the protected sensitive information.\nComparison of Representation. We then visualize the intermediate model representations used for the downstream tasks to verify whether the protected sensitive information is still represented in the fine-tuned model. Concretely, we employ the t-SNE algorithm to map each test sample from a high dimensional representation space to a two-dimensional space from the perspectives of class label and sensitive attribute label, respectively. The comparison results are shown in Figure.4, in which samples with the same label are drawn in the same color. Assuming the models learn well on the downstream classification tasks, then the blue and green data points would"}, {"title": "4.3 Quantitative Evaluation of Fairness", "content": "Fairness and Utility. We then quantitatively evaluate the effectiveness of fairBERTs and the compared baselines in terms of model utility and fairness. The main evaluation results are summarized in Table.2. Notice that the values of these five metrics range from 0 to 1. A higher value of Acc. reflects a better model utility. A smaller value of DPD and EOD, and a higher value of DIR and CTF indicate a better fairness. All evaluation experiments are conducted on the whole testing datasets.\nIt can be obviously seen from Table.2 that all vanilla models have both achieved considerable high accuracy in the target classification tasks (i.e., the accuracy is above 0.9200 on toxicity detection and above 0.9900 on sentiment analysis), indicating good model utility. It also suggests that the unfairness issue in models trained on Jigsaw Toxicity is more severe than those on Yelp Sentiment. We speculate that this is mainly because people are often mixed with more social prejudices and personal stereotypes when expressing insulting comments (e.g., hate speech and racial discrimination), resulting in more unfair issues in the toxicity detection tasks. After applying FTU-I as the mitigation method, a slight increase in fairness is gained at the sacrifice of model accuracy. For instance, the accuracy of vanilla ROBERTa on Jigsaw Toxicity dropped from 0.9200 to 0.8955 with a subtle improvement of 0.0271 in DPD. When FTU-II is used for unfairness mitigation, the promotion in fairness grows further but along with a larger drop in model accuracy, which may result in a trade-off between model utility and fairness. Similarly, a trade-off between model accuracy and fairness is also observed when employing the CLP mitigation method although it obtains a relatively significant improvement in terms of the four fairness dimensions. We speculate this is mainly because that CLP is also subject to the drawbacks of missing consideration in correlations between sensitive attributes and non-sensitive attributes, and the added robust term in optimization objective has limited the model\u2019s ability in capturing the semantics of input.\nIn contrast, fairBERTs learnt without counterfactual adversarial training has achieved fairness gains comparable to the CLP method, and almost has no negative impact on the model utility. In most cases, the model accuracy has been promoted instead along with the improvement in model fairness. Specifically, the accuracy of BERT and RoBERTa on Yelp Sentiment has been promoted from 0.9920 and 0.9933 to 0.9953 and 0.9940, respectively. This is mainly attributed to the generative adversarial framework, which has also been proved in [6]. When learning with counterfactual adversarial training, fairBERTs has exhibited great superiority in mitigating the involved unfairness and outperforms the compared baselines by a significant margin. This demonstrates that fairBERTs is effective in mitigating the bias encoded in model while balancing model utility well in the meantime.\nReduction of Sensitive Words. We further analyze the change-ment of sensitive words in the top-3 important decision-making words before and after bias mitigation by taking the BERT backbone as an example. The quantitative results on the two datasets are shown in Figure.5, in which the top-3 important decision words are given by LIME through model-agnostic interpretation.\nIt is clearly observed that there is a significant decrease in the twelve representative sensitive gender words frequently occurred in the top-3 important decision words. Concretely, the amount of gender words decreases by above 30% on the toxicity detection task and a similar trend is shown on the sentiment analysis task. This suggests that fairBERTs can reduce the dependence of model decision on the protected sensitive information, which would help models focus on the real feature in decision-making."}, {"title": "4.4 Evaluation of Transferability", "content": "Finally, we further investigate the transferability of the generator G as well as the generated semantic and fairness-aware perturbation \u03b4. Specifically, we directly apply the corresponding perturbation \u03b4 generated by G to the classification representation of the conventionally trained vanilla models for each input text sample, and then reevaluate the utility and fairness of vanilla models from the five aspects on the testing sets. We take the toxicity detection task as an example to report the evaluation results and the main experimental results are summarized in Table.3.\nCompared to the vanilla BERT and ROBERTa, after applying the semantic and fairness-aware perturbations, the two models both attain a noticeable improvement in the four fairness metrics while maintaining most of the model utility. Specifically, the DIR of BERT and ROBERTa is promoted from 0.8716 and 0.7989 to 0.9206 and 0.9318 respectively with just a small drop of about 2% to 3% in"}, {"title": "5 CONCLUSION", "content": "To solve the problem that protected sensitive information leaks into intermediate model representations, we present fairBERTs, a general framework for building fair BERT series fine-tuned models by erasing sensitive information via semantic and fairness-aware perturbations. Through qualitative and quantitative empirical evaluation, we demonstrate that fairBERTs attains promising effectiveness in mitigating the unfairness involved in the model decision-making process while preserves the model utility well at the mean time. We also show the feasibility of transferring the adversarial debiasing component in fairBERTs to other conventionally trained models for yielding fairness improvements. A promising future work is to generalize fairBERTs to other tasks or research domains for building more trustworthy fine-tuned PLMs."}]}