{"title": "Contextual Bandit with Herding Effects:\nAlgorithms and Recommendation Applications", "authors": ["Luyue Xu", "Liming Wang", "Hong Xie", "Mingqiang Zhou"], "abstract": "Contextual bandits serve as a fundamental algorithmic frame-\nwork for optimizing recommendation decisions online. Though extensive\nattention has been paid to tailoring contextual bandits for recommen-\ndation applications, the \"herding effects\" in user feedback have been ig-\nnored. These herding effects bias user feedback toward historical ratings,\nbreaking down the assumption of unbiased feedback inherent in con-\ntextual bandits. This paper develops a novel variant of the contextual\nbandit that is tailored to address the feedback bias caused by the herd-\ning effects. A user feedback model is formulated to capture this feedback\nbias. We design the TS-Conf (Thompson Sampling under Conformity)\nalgorithm, which employs posterior sampling to balance the exploration\nand exploitation tradeoff. We prove an upper bound for the regret of\nthe algorithm, revealing the impact of herding effects on learning speed.\nExtensive experiments on datasets demonstrate that TS-Conf outper-\nforms four benchmark algorithms. Analysis reveals that TS-Conf effec-\ntively mitigates the negative impact of herding effects, resulting in faster\nlearning and improved recommendation accuracy.", "sections": [{"title": "1 Introduction", "content": "Contextual linear bandit is an important sequential decision making framework\nfor information retrieval applications [5]. It is also applied to optimize news\nrecommendations [10,19], movie recommendations [15, 16], advertising [21,31],\netc. Recently, a number of variants of contextual linear bandits were proposed\nto capture important factors of information retrieval applications. Such as the\nconversational contextual bandit which captures the contextual linear bandit\nto capture conversational feedbacks in recommendation applications [32], the\nimpatient contextual bandits which captures feedback delay in recommendation\napplications [12], contextual budgeting bandit which captures the multi-agent\nnature the budget allocation in online advertising [6], etc.\nThis paper tackles a critical challenge in the field of recommendation systems:\nthe herding effects in user feedback [1,23,27]. Randomized controlled experiments\n[1,14] proved the existence of herding effects. The herding effects states that users"}, {"title": "2 Related Work", "content": "Contextual linear bandits serve as a fundamental sequential decision making\nframework for information retrieval applications advertising, recommendation,\netc [5]. A number of variants of contextual linear bandits were proposed to\ncapture important factors of information retrieval applications [24-26,34]. Con-\nversational contextual bandit tailors the contextual linear bandit to capture\nconversational feedbacks in recommendation applications [32]. The contextual\nbudgeting bandit extend the contextual linear bandit to the multi-agent for the\npurpose of studying the budget allocation in online advertising [6]. The key dif-\nference to the above work is that our model captures the well-known herding\neffects in feedback. The new technical challenge is that this herding effects leads\nto confounded feedback with spurious correlation, which may result in linear\nregret.\nThrough controlled experiments [1,14,18], some researchers identified a rat-\ning bias influenced by historical ratings, observing that users tend to give higher\nratings after being exposed to higher historical ratings, a behavior termed as\nherding effects. Wang et al. [23] introduced an additive generative-based model\ndesigned to quantify herding effects. While it can capture the pattern of herding\neffects, it lacks the neatness required for analytical studies of evolving dynamics\nof aggregate ratings under herding effects. Krishnan et al. [8] developed a polyno-\nmial regression-based model to quantify herding effects. Xie et al. [27] proposed\na neater linear model for herding effects and supported analytical studies on the\nevolving dynamics of aggregate ratings. Other notable psychological effects that"}, {"title": "3 Model", "content": "We consider the sequential decision problem as one where the decision-maker\nmakes decisions over a finite number of $T \\in \\mathbb{N}^+$ rounds. The set of actions used\nin the decision-making process is fixed to be a finite set $A \\subset \\mathbb{N}^+$, where $|A| < \\infty$.\nConsider a scenario where the decision-maker acts as a movie recommendation\nsystem, and $A$ is the set of movies under consideration. In each round $t \\in [T] \\triangleq\n\\{1, ..., T\\}$, the decision-maker is presented with a finite set of choices $A_t \\subseteq A$ and\n$|A_t| = K$, from which it must choose one action $A_t \\in [K] \\triangleq \\{1, ..., K\\}$ to the\nuser. The user then receives the expected preference reward $E[R_t(A_t)]$ (positive\nor negative preference) for the recommending action, which is unobservable to\nthe decision-maker. Based on the reward $E[R_t(A_t)]$, each user provides feedback\n$V_t(A_t) \\in \\mathbb{R}$ about the action $A_t$ to decision-maker, where $R \\subset \\mathbb{R}$. The application\ndefines the metric for quantifying $V_t(A_t)$. In movie recommendation applications,\n$V_t(A_t)$ models the user's rating of the movie."}, {"title": "3.2 The User Feedback Model", "content": "Contextual reward model. In our study, we focus on the use of contextual\nfeatures to evaluate the rewards users receive from recommended items. For each\naction $a \\in A$, there is a feature vector $x_a \\in \\mathbb{R}^d$ associated with it that captures\ncontextual information between the user and the action with $d \\in \\mathbb{N}^+$. The pref-\nerence vector of the user, linked to $x_a$, is represented as $\\theta \\in \\mathbb{R}^d$. It should be"}, {"title": "3.3 Sequential Decision Making Model", "content": "Based on the feedback model, we use $V_t(A_t)$ to study the unknown parameters\nof the user feedback and to get the user's preference estimate. To provide a\nclearer explanation, we consolidate all unknown parameters within $V_t(A_t)$ into\n$\\Psi \\triangleq [\\theta, \\alpha, \\sigma]$. Here, $\\sigma \\triangleq [\\sigma_1,...,\\sigma_\\left| A \\right|]$ represents the noise standard deviation\nfor each action in the action set. After round $t$, the decision-maker possesses\ninformation regarding the decision action $A_t$, the associated historical feedback\n$h_{t, A_t}$, the feedback $V_t(A_t)$, and the observed context $x_a$. Let $H_t$ represent the\ndecision-making history up to decision round $t$ as $H_t \\equiv \\{[A_1, h_{1,A_1}, x_{A_1}, V_1(A_1)],\n..., [A_t, h_{t, A_t}, x_{A_t}, V_t(A_t)]\\}$. In the $t$-th round of decision-making, the decision-\nmaker must base decisions on the history $H_{t-1}$ of the preceding $t - 1$ rounds.\nTherefore, we propose using a sequential decision-making algorithm that lever-\nages historical dependencies. Specifically, this algorithm maps the decision his-\ntory to the current round's action probability distribution $F(H_{t-1})$. The action\n$A_t$ is consequently generated from this distribution, expressed as $A_t \\sim F(H_{t-1})$.\nIf the distribution $F(H_{t-1})$ targets a single action without variance, we have"}, {"title": "4 Algorithm", "content": "In the context of accumulating decisions by round $t$, we articulate the model's\nparameters, still to be inferred, as $\\Psi = [\\theta, \\alpha, \\sigma]$. The calculation for their pos-\nterior distribution, denoted $p(\\Psi \\mid H_t)$, is delineated in an ensuing lemma.\nLemma 1. Suppose the probability density function of the noise has the para-\nmetric form $f(\\cdot, \\sigma)$, where $\\sigma$ controls the tail property. Given the decision history\n$H_t$ up to round $t$, the posterior distribution $p(\\Psi \\mid H_t)$ can be derived as:\n$$p(\\Psi \\mid H_t) = \\frac{p(\\Psi)}{C} \\times \\prod_{\\tau=1}^{t-1} \\prod_{\\alpha \\in A} [f(\\eta_{\\tau, \\alpha}, \\sigma_\\alpha)]^{\\mathbb{I}\\{A_{\\tau}=a\\}} \\qquad,\n$$\n$$\\eta_{\\tau, \\alpha} = V_\\tau(a) - \\alpha h_{\\tau, a} - (1 - \\alpha) \\theta^T x_a,\n$$\nwhere $C$ represents the normalizing factor which is independent of the unknown\nmodel parameters $\\theta, \\alpha, \\sigma$.\nDrawing on the foundational lemma presented earlier, Algorithm 1 intro-\nduces a method for posterior sampling tailored to address the challenges of the\ncontextual bandit learning dilemma as discussed in Section 3. Each interaction\ncycle, or round $t$, commences with the identification of the model's parameters\n$\\Psi$, grounded in the posterior distribution outlined in Eq.(4). Subsequently, the\nalgorithm computes the expected reward for each viable action, with a prefer-\nence for selecting the action projected to offer the maximum return. Upon the\ndecision maker's implementation of the chosen action, the system garners feed-\nback $V_t(A_t)$ from the agent. This feedback is subsequently integrated into the\ndecision history, thereby facilitating the transition to the subsequent iteration.\nIn general, the posterior distribution derived in Eq. (4) is computationally ex-"}, {"title": "4.2 Regret Analysis", "content": "Given a parameter $\\Psi$, denote the estimator for estimating $a$ from $H_t$ as $\\hat{a}_t(\\Psi)$.\nNote that this estimator is defined to assist the proof of regret, we do not need\nto know how to construct it. We define the confidence bound $\\hat{a}_t(\\Psi)$ as:\n$$P[\\forall t, |\\hat{a}_t(\\Psi) - a| \\leq W_t(\\delta; \\Psi)] \\geq 1 - \\delta.$$\nDifferent instances of $\\hat{a}_t(\\Psi)$ have different confidence width. Let $W^*\\left(\\delta; \\Psi\\right)$ denote\nthe smallest possible confidence width attained by the optimal estimator $\\hat{a}_t(\\Psi)$.\nTheorem 1. The regret of Algorithm 1 satisfies:\n$$R_T(\\mathcal{D}) \\leq O\\left(\\frac{1}{1-\\alpha} \\sum_{t=1}^{T}W_t^*\\left(1/T;\\Psi\\right) + \\frac{1}{1-\\alpha} d\\sqrt{T \\ln T}\\right).$$\nAlgorithm 1 TS-Conf (Thompson Sampling under Conformity)\nAlgorithm 2 TS-ConfMCMC"}, {"title": "5 Experiments", "content": "Our approach to constructing the simulated datasets is aligned with\nestablished practices in the field, similar to those employed in related studies\n[13,30]. In our empirical evaluation using real-world data, we employ datasets\nsourced from four distinct platforms: Amazon Music\u00b3, MovieLens\u2074, Yelp\u2075, and\nGoogle Maps\u2076. Through the utilization of these varied datasets spanning multiple\ndomains, our objective is to rigorously assess and understand the real-world\napplicability and performance of the proposed algorithm. Based on the approach\nsimilar to [30, 33], we perform data preprocessing and assess the accuracy of\nalgorithm recommendations through regret values. Due to page limitations, more\ndetails on the data preprocessing for datasets can be found in the appendix.\nTo the best of our knowledge, limited research exists\non contextual bandit algorithms that specifically address the herding effects. In\nlight of this gap, we adapt mainstream bandit algorithms to incorporate the\nherding effects, thereby producing a suitable comparison algorithm. To ensure a\nfair and relevant comparison in our study, which focuses on the herding effects, we\nbenchmark our proposed algorithm against two sets of algorithms. The first set\nincludes established baseline algorithms, LinUCB [4] and Thompson Sampling\n(TS) [2], known for their accuracy in scenarios with unbiased user feedback.\nHowever, these algorithms do not specifically address herding effects, a gap in\nthe existing research. Recognizing this limitation, we developed LinUCBConf, an\nadaptation of the LinUCB algorithm. LinUCBConf is designed to provide a more\nappropriate baseline for our study by accounting for herding effects, which were\nnot explicitly considered in previous models. This adaptation allows for a more\nequitable comparison, enabling us to effectively demonstrate the strengths and\ninnovations of our proposed algorithm in the context of herding effects. When\nestimating the preference parameter $O_t$, LinUCBConf employs the same action"}, {"title": "5.2 Stability Analysis of Algorithm Parameters", "content": "In our study, we focus on analyzing\nthe influence of the number of iterations, denoted as N, on the regret metric\nof our algorithm. The parameter N is critical as it represents the iterations\nnecessary for the MCMC method to approximate samples for the posterior dis-\ntribution. For the purpose of this analysis, we standardize the dimensions of\nthe actions and the noise variance at $d = 10$ and $\\sigma^2 = 1.0$, respectively. This\nuniformity allows for a controlled assessment of the impact of N on the algo-\nrithm's regret. As illustrated in Figure 1, the regret values produced by TS-Conf\nconsistently fall within the range spanned by TS-ConfMCMC. This shows that\nthe approximate algorithm TS-ConfMCMC we proposed can obtain results sim-\nilar to precise sampling through a limited number of samplings, illustrating the\neffectiveness of the approximate algorithm."}, {"title": "5.3 Real-world applications", "content": "Figure 4 shows the regret $R_t$ produced by each algo-\nrithm in different dimensions and different noise variances. It can be observed\nthat the TS-Conf algorithm always has the lowest regret value across varying\ndimensions and noise levels. Consistent with the experimental observations on\nsynthetic data, the regret values for both LinUCB and TS (both of them ignore\nthe biased influence in user feedback) exhibit significantly higher regret than the\nTS-Conf algorithm and will increase linearly with t. Similarly, LinUCBConf also\nhas a greater regret compared with TS-Conf, and its regret values converge at a\nslower rate than our algorithm."}, {"title": "6 Conclusion", "content": "This paper presents a contextual bandit framework to address the herding ef-\nfects problem in recommendation applications. In this framework, we assume\nthat the user feedback on the action is biased and influenced by user preferences\nand the historical ratings of this action. We design a TS-Conf algorithm that\nleverages a posterior sampling technique to effectively balance the trade-off be-\ntween exploration and exploitation in this framework. Our theoretical analysis"}]}