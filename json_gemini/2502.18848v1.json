{"title": "A Causal Lens for Evaluating Faithfulness Metrics", "authors": ["Kerem Zaman", "Shashank Srivastava"], "abstract": "Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's internal reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, a unified evaluation framework remains absent. To address this gap, we present CAUSAL DIAGNOSTICITY, a framework to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of causal diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate a variety of faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that all tested faithfulness metrics often fail to surpass a random baseline. Our work underscores the need for improved metrics and more reliable interpretability methods in LLMs.", "sections": [{"title": "1 Introduction", "content": "Advancements in Large Language Models (LLMs) have enhanced possibilities for model explainability, making natural language explanations preferable over feature attribution methods. Most LLMS can provide explanations for their predictions at minimal cost (Wei et al., 2022). However, despite their plausibility, these explanations do not always reflect the model's actual reasoning, potentially misleading practitioners (Turpin et al., 2023). The idea of faithfulness aims to assess how accurately explanations reflect the true reasoning mechanism of the model. While numerous methods have been proposed to measure faithfulness for natural language-based explanations, they are criticized for not adequately considering the model's inner workings, relying instead on simplistic consistency measures (Parcalabescu and Frank, 2023). Furthermore, while many faithfulness metrics have been developed, currently there are no reliable evaluation frameworks for comparing them. To address this gap in the field, we introduce a new evaluation framework, CAUSAL DIAGNOSTICITY, along with a new benchmark for comparing various faithfulness metrics. Our framework extends the notion of diagnosticity (Chan et al., 2022b), which measures how often a faithfulness metric favors faithful explanations over unfaithful ones, and applies it to faithfulness metrics for natural language explanations. We investigate knowledge editing approaches for causally generating faithful and unfaithful explanation pairs and evaluate diagnosticity through three tasks. These tasks include (1) a fact-checking task, (2) an analogy task, (3) an object counting, and (4) a multi-hop reasoning task. Figure 1 shows an overview of our framework. We evaluate a diverse set of faithfulness metrics, including post-hoc explanation-based and chain-of-thought (CoT)-based metrics: Counterfactual Edits (Atanasova et al., 2023), Simulatability, metrics based on corrupting CoT explanations (Lanham et al., 2023), and CC-SHAP (Parcalabescu and Frank, 2023). Our evaluation shows that all tested faithfulness metrics often fail to surpass a random baseline or perform at near-random levels. These results underscore the need for a diagnosticity-first approach in the design of faithfulness metrics. Moreover, our analysis demonstrates that metrics producing continuous faithfulness scores are more diagnostic than those with binary scores.\nOur contributions include (1) a framework for evaluating faithfulness metrics for natural language explanations, (2) a new dataset spanning four tasks, and (3) benchmarking of prominent faithfulness"}, {"title": "2 Background", "content": "Faithfulness Faithfulness quantifies how well explanations reflect a model's true reasoning process. Let \\(M_{\\theta}\\) denote an LLM parameterized by \\(\\theta\\) and with a context c, operating on a token set V such that \\(M_{\\theta}(t_{in} | c) = t_{out}\\), where \\(t_{in} = (t^1_{in},...,t^{N_{in}}_{in})\\), \\(t_{out} = (t^1_{out},...,t^{N_{out}}_{out})\\) and \\(c = (c_1,...,c_{N_c})\\); \\(t_{in}, t_{out}, c_i \\in V\\); \\(N_{in}\\), \\(N_{out}\\) and \\(N_c\\) represent the lengths of the input, output and context sequences. The context c consists of instructions or prompts.\nFor brevity, we use M to denote a model parameterized by \\(\\theta\\) with context c. The input and output sequences can take many forms. For the simplest case \\(t_{in} = x\\) and \\(t_{out} = y\\) where (x, y) is an input-output pair for a task. With appropriate prompting, the output can take the form \\(t_{out} = y \\oplus \\epsilon\\) for post-hoc explanations or \\(t_{out} = \\epsilon \\oplus y\\) for chain-of-thought (CoT) explanations, where \\(\\epsilon\\) is the explanation and \\(\\oplus\\) denotes sequence concatenation. We define a faithfulness metric as \\(F(x, y, \\epsilon, M) \\in \\mathbb{R}\\), where F represents how faithfully explanation \\(\\epsilon\\) represents the reasoning process for input-output pair (x, y) for the model M."}, {"title": "2.1 Faithfulness Metrics", "content": "We focus on seven prominent faithfulness metrics: (1) Counterfactual Edits (Atanasova et al., 2023), (2) Simulatability, metrics based on CoT corruptions (Lanham et al., 2023) (including (3) Early Answering, (4) Adding Mistakes, (5) Paraphrasing, and (6) Filler Tokens), and (7) CC-SHAP (Parcalabescu and Frank, 2023). While Simulatability and Counterfactual Edits target post-hoc explanations, the others are tailored for CoT explanations. CC-SHAP is applicable to both types of explanations. Next, we review these metrics.\nCounterfactual Edits Atanasova et al. (2023) propose a faithfulness metric based on the principle that an explanation is unfaithful if the model prediction changes after a counterfactual edit to the input, but the explanation fails to reflect the edit. A limitation of this approach is the need to train a separate neural editor for each model-dataset pair to make such counterfactual interventions. Instead, we follow their random baseline based on the same rationale, where they insert a random adjective before a noun or a random adverb before a verb, as Parcalabescu and Frank (2023) did. In this approach, an explanation is considered unfaithful if the prediction changes after word insertion and the explanation fails to mention the inserted words.\nSimulatability Simulatability assesses faithfulness from the lens of the extent to which whether an explanation enables a simulator to predict the model's output (Doshi-Velez and Kim, 2017; Hase and Bansal, 2020; Hase et al., 2020; Wiegreffe et al., 2020; Chan et al., 2022a). We follow Chan et al. (2022a)'s definition of simulatability as \\(1_S(Y_i | X_i, E_i) - 1_S(Y_i | x_i)\\), where \\(1_S(b | a)\\) is the accuracy of S in predicting b given a.\nCorrupting CoT Lanham et al. (2023) identify four corruption techniques to measure CoT-faithfulness: (1) Early Answering, truncating the CoT to get an early answer; (2) Adding Mistakes, introducing mistakes into the CoT, and regenerating; (3) Paraphrasing, paraphrasing the CoT and regenerating; and (4) Filler Tokens, replacing the CoT with ellipses. An explanation is considered unfaithful if the corruption does not alter the original prediction (except for Paraphrasing, where prediction changes signify unfaithfulness). While these metrics were initially proposed as binary measures, we extend them by quantifying faithfulness in terms of changes to prediction scores."}, {"title": "2.2 Knowledge Editing", "content": "Our framework generates faithful-unfaithful explanation pairs by modifying facts within LLMs using knowledge editing. This is necessary as LLM knowledge can become outdated over time. Knowledge editing methods allow updates without altering unrelated knowledge (Cohen et al., 2024; Zhang et al., 2024; Patil et al., 2023; Geva et al., 2023; Gupta et al., 2023; Hartvigsen et al., 2023; Zheng et al., 2023; Meng et al., 2022; Mitchell et al., 2022a), using triplets consisting of subject s, object o, and relation r. For instance, they can update (s = Joe Biden, r = is the president of, o = the United States) to (s = Donald Trump, r = is the president of, o = the United States) while keeping other information unchanged. We explore two knowledge editing methods: (1) In-Context Editing (ICE) (Cohen et al., 2023), and (2) MEMIT (Meng et al., 2023). ICE is preferred since it requires no parameter updates. Unlike ICE, MEMIT relies on a rigid subject-object-target template, which limits its use in complex scenarios. Additionally, MEMIT-like methods are sensitive to hyperparameters making them less applicabile, even with reported optimal values for specific models (Wang et al., 2023). Finally, in-context approaches consistently surpass MEMIT in multi-step reasoning tasks (Cohen et al., 2023)."}, {"title": "3 Method", "content": "Our CAUSAL DIAGNOSTICITY framework is inspired by the idea of diagnosticity, which evaluates how well faithfulness metrics distinguish between faithful and unfaithful explanations. In 3.1, we summarize diagnosticityt as introduced by Chan et al. (2022b) for evaluating feature attribution methods. In 3.2, we introduce CAUSAL DIAGNOSTICITY, describing how it builds on diagnosticity and extends it to natural language explanations using causal interventions via edited models."}, {"title": "3.1 Diagnosticity", "content": "Faithfulness has been widely studied (Jacovi and Goldberg, 2020), leading to multiple metrics but no unified evaluation framework. We adopt diagnosticity, proposed by Chan et al. (2022b), which measures how often a faithfulness metric prefers faithful over unfaithful explanations. For example, if a model correctly answers \"No\" to the question, \"Is Rihanna a researcher?\" based on her being a singer, a faithful explanation should reflect this reasoning. An explanation that provides an irrelevant rationale (e.g., the number of albums she has sold) or false information (e.g., assigning her the wrong occupation) would be unfaithful.\nFollowing the notation from Chan et al. (2022b), let u and v be explanations (regardless of form, e.g., language, feature attributions, etc.), with \\(u > v\\) denoting that u is more faithful than v. A faithfulness metric F ranks the explanations as \\(u >_F v\\) if it assigns a higher faithfulness score to u than v. Then, the diagnosticity of the metric F is:\n\\(D(F) = P(u >_F v|u > v)\\)   (1)\nWe approximate this as:\n\\(D(F) \\approx \\frac{1}{|Z|} \\sum_{(u_i,v_i) \\in Z} \\mathbb{I}(u_i >_F v_i)\\) (2)\nwhere Z contains pairs of faithful(ui) and un-faithful (vi) explanations for input-output pairs (xi, yi). Since higher faithfulness scores represent more faithful explanations, we rewrite:\n\\(D(F) \\approx \\frac{1}{|Z|} \\sum_{(u_i,v_i) \\in Z} \\mathbb{I}(F_{p_i,M}(u_i) > F_{p_i,M}(v_i))\\) (3)\nwhere \\(p_i = (x_i, y_i)\\)."}, {"title": "3.2 Causal Diagnosticity", "content": "To obtain unfaithful explanations for measuring diagnosticity, Chan et al. (2022b) use random feature attribution scores. While random scores can work for structured explanations like feature attributions, this approach is not straightforward for natural language explanations. Random text cannot function as a meaningful explanation and cannot ensure un-faithfulness in a coherent way.\nTo address this, we introduce CAUSAL DIAGNOSTICITY, which generates unfaithful explanations using knowledge editing. Rather than in-"}, {"title": "4 Tasks", "content": "We evaluate faithfulness metrics using four controlled tasks in the CAUSAL DIAGNOSTICITY framework: (1) fact-checking, (2) analogy, (3) object counting, and (4) multi-hop reasoning. These tasks assess causal diagnosticity by using counterfactual models with faithful and unfaithful explanations. While the altered models should reason differently, their explanations may not always reference the modifications. To ensure valid faithfulness comparisons, we synthetically generate explanations that emphasize model differences. Figure 2 provides an overview of these tasks, including example inputs, outputs, and explanations."}, {"title": "4.1 Fact Check Task", "content": "Task This task focuses on simple fact-checking, where a fact is presented alongside two counterfactual answers. For any relation \\((s_i, r_i, o_i)\\), we present a question that checks its correctness, accompanied by two counterfactuals: \\((s_i, r_i, \\bar{o}_i)\\) and \\((s_i, r_i, \\tilde{o}_i)\\). These counterfactuals yield the same answer but are based on different reasoning. For instance, given the knowledge triplet \\((s_i = \\text{``Rihanna''}, r_i = \\text{``is''}, o_i = \\text{``a singer''})\\), the corresponding question would be \"Is Rihanna a singer?\" Let the counterfactual objects be \\(\\bar{o}_i = \\text{``researcher''}\\) and \\(\\tilde{o}_i = \\text{``lawyer''}\\). Both counterfactuals would result in the answer \"No,\" but for different reasons.\nDataset We construct our dataset from COUNTERFACT (Meng et al., 2022), which consists of knowledge triplets. We use Mistral-7B-Instruct-v0.2 to convert these triplets to yes/no questions. Then, for each object oi, we fetch sibling entities from WikiData to create counterfactuals. Finally, we generate synthetic explanations corresponding to each counterfactual. For example, the corresponding explanation oi would be \"Joe Biden is a researcher, not the president of the United States\" for \\(\\bar{o}_i\\). Further details about dataset generation can be found in Appendix D."}, {"title": "4.2 Analogy Task", "content": "Task This task is based on analogies exploiting hierarchies between two relations where \\(r_1 \\subset r_2\\) holds. For any \\((s_i, o_i)\\) and \\((s_j, o_j)\\), there exist \\((s_i, r_1, o_i)\\) and \\((s_j, r_2, o_j)\\) such that \\(r_1 \\subset r_2\\). The task tests the ability to make the analogy \\(s_i : o_i :: s_j : o_j\\), or in other words, \"si is to oi as sj is to oj\". We choose r1 and r2 as capitalof and cityof relations, respectively. For instance, we test \"Paris is to France as Berlin is to Germany.\" We corrupt one of the models so that the relation capitalof is no longer valid while the relation cityof still holds. Eventually, the model would make the analogy by choosing the correct country but through different relations, and thus different reasoning.\nDataset First, we collect a list of countries and cities, then select one capital and one non-capital"}, {"title": "4.3 Object Counting Tasks", "content": "Task Adapted from BIG-bench (Bench authors, 2023), this task tests object classification and counting. The model identifies how many objects in a list belong to a given category. To introduce causal interventions, we alter the model's internal knowledge, swapping objects across categories while keeping the answer numerically identical but reasoning distinct. For example, in How many of \"countertop,\" \"grape,\" and \"kiwifruit\" are fruits?, the correct answer is 2, since \"countertop\" is not a fruit. If the model is edited to classify \u201ccountertop\u201d as a fruit and \u201cgrape\u201d as furniture, the answer remains 2, but for different reasons.\nDataset We define five object categories, each with five types, as detailed in Table 7 (Appendix D). For each type, we collect 10 representative entities from WikiData, reserving 20% for reassignment after model editing. We generate 1000 questions, equally split between two types: yes/no questions, asking if all or any items in a list belong to a given type, and number questions, asking how many items belong to a specific type. For both types, we randomly determine the number of items k (between 3 and 6) and select a target type. For"}, {"title": "4.4 Multi-hop Reasoning Task", "content": "Task This task extends diagnostic evaluation to complex multi-step reasoning, and multiple facts. Like Fact Check, it ensures identical answers across counterfactual settings but requires multi-hop chains to reach conclusions. Unlike FactCheck, this task requires explanations grounded in multi-step reasoning chains, where even divergent explanation pairs share overlapping intermediate steps.\nDataset We construct this dataset using StrategyQA (Geva et al., 2021), a multi-hop QA benchmark that provides gold-standard fact decompositions for each example. We generate two counterfactual variants for one fact per question, preserving the original answer while altering the reasoning. When facts are interdependent, we propagate modifications to ensure consistency. Next, we generate explanations for each counterfactual set using the original decompositions. We use gpt-40 for generating counterfactuals and explanations, which we manually verify for logical coherence. The final data consists of 200 high-quality examples, balancing complexity with computational feasibility."}, {"title": "5 Experiments", "content": "In this section, we present our results and analyses for a series of experiments. These consist of: (1) evaluating diagnosticity scores for post-hoc and CoT-based metrics, (2) analyzing the reliability of knowledge edits, (3) studying the effect of replacing ICE with MEMIT, (4) assessing model-generated vs. synthetic explanations, (5) comparing binary and continuous faithfulness metrics, and (6) analyzing the effect of model size."}, {"title": "5.1 Diagnosticity of Faithfulness Metrics", "content": "Experimental Setup We evaluate the seven metrics described in Section 2 with two different LLMs: qwen-2.5-7b (Yang et al., 2024), and gemma-2-9b-it (Riviere et al., 2024). For our main experiments, we use ICE as the knowledge editing method and synthetic explanations to ensure faithfulness to the edited model.\nTable 1 reports diagnosticity scores across the four tasks for both models. Among post-hoc metrics, CC-SHAP outperforms Simulatability and Counterfactual Edits (p < 0.05, McNemar's test). The Multi-hop Reasoning task is an exception, where differences between Simulatability and Counterfactual Edits are not significant. Among CoT-based metrics, no single best performer emerges, but CC-SHAP excels in FactCheck, while Paraphrasing significantly leads in Object Counting for both models (p < 0.05).\nWhen examining discrepancies between models, notable differences emerge in Early Answering and Filler Tokens for the FactCheck task, as well as in CC-SHAP across post-hoc and CoT setups for the Analogy task. These inconsistencies may be attributed to the way these metrics operate. In Early Answering metric, Truncated explanations may result in incomplete or nonsensical sentences, which can be out-of-distribution (OOD) for the model. Consequently, the drop in prediction scores may not solely reflect the unfaithfulness of the altered explanation but rather the model's sensitivity to OOD inputs (Hooker et al., 2018).\nCrucially, none of the metrics exceeds the baseline value of 0.50 across all models and tasks. Paraphrasing is the most reliable, significantly outperforming (p < 0.05, Binomial test) the baseline in 5 out of 8 measurements. The Multi-hop Reasoning task is especially challenging, as all metrics fail to significantly surpass baseline performance."}, {"title": "5.2 Reliability of Edits", "content": "CAUSAL DIAGNOSTICITY assumes that one explanation in each pair is faithful to the evaluated model, while the other is unfaithful. While synthetic explanations in principle ensure faithfulness or unfaithfulness with respect to the edited model, their practical accuracy depends on the success of the editing method. We assess this by comparing the perplexities of the explanation pairs. The premise here is that faithful explanations should have a lower perplexity than unfaithful ones.\nFigure 3 shows the percentage of explanations deemed faithful that have lower perplexity than those deemed unfaithful, broken down by task and model. For FactCheck, the edits show strong success, with scores approaching 1.0, followed by Multi-hop Reasoning and Object Counting. In contrast, edits for the Analogy task underperform, with scores falling below 50%. This is likely due to conflicting information about widely known facts, such"}, {"title": "5.3 Effect of Knowledge Editing Method", "content": "We replace ICE with MEMIT (Meng et al., 2023), a locate-and-edit approach enabling bulk edits (details in Appendix B). Since Multi-hop reasoning edits do not align with MEMIT's format, this task is excluded. Figure 4 compares MEMIT and ICE, averaging diagnosticity scores across three tasks. ICE shows higher diagnosticity scores than MEMIT. A correlation analysis shows a strong agreement between the methods (p = 0.91 for FactCheck, p = 0.90 for Analogy, and p = 0.96 for Object Counting), indicating that the choice of editing method does not significantly alter results, though relative rankings between metrics shift. Full results for MEMIT are in Appendix A."}, {"title": "5.4 Effect of Model Generated Explanations", "content": "While our main results use synthetically generated explanations, we perform an ablation using model-generated explanations. We evaluate all faithfulness metrics using qwen-2.5-7b, limiting model-generated explanations to 100 tokens. Figure 5 compares model-generated and synthetic ex-"}, {"title": "5.5 Binary vs Continuous Metrics", "content": "In Table 1, low diagnosticity scores of faithfulness metrics that produce binary outcomes, such as Simulatability and Counterfactual Edits, are notable. To investigate this, we compare binary and continuous variants of CoT-based metrics across four tasks using qwen2.5-7b. We find that continuous variants consistently outperform binary ones across all tasks and metrics. Details of these experiments can be seen in Table 2, Appendix A. While Siegel et al. (2024) provide theoretical rationale for preferring continuous alternatives in Counterfactual Edits, we provide the first empirical confirmation that this trend holds across multiple metrics and tasks."}, {"title": "5.6 Effect of Model Size", "content": "Our main experiments are conducted on relatively small models with 7 billion to 9 billion parameters. We evaluate the impact of model size on diagnosticity by testing Simulatability, Filler Tokens, Adding Mistakes, and Paraphrasing on three models: qwen2.5-7b-instruct, qwen2.5-32b-instruct, and qwen2.5-72b-instruct. For the 32B and 72B models, we use the AWQ (Lin et al., 2024) versions. Since AWQ versions of the larger base models are unavailable, we instead use their instruction-tuned counterparts.\nFigure 6 shows no clear scaling trends in diagnosticity. Simulatability remains stable, while Adding Mistakes is lowest at 32B (for Analogy and Object Counting) highest in FactCheck. Paraphrasing scales well in FactCheck and Object Counting but follows an inverse trend in Analogy. Filler Tokens slightly improves in FactCheck but worsens in Object Counting. While Figure 8 in Appendix A suggests edit reliability improves with model size, our results indicate that diagnosticity scaling shows no uniform patterns across different configurations."}, {"title": "6 Conclusion", "content": "Our work here provides a testbed for faithfulness metrics, laying the groundwork for improvements in faithfulness metrics and natural language explanations. We benchmark popular post-hoc and CoT-based faithfulness metrics across tasks. Given the shortcomings of existing metrics, future research should prioritize diagnosticity-first approaches and explore contrastive methods that avoid OOD perturbations, such as Paraphrasing or continuous Coun-"}, {"title": "Limitations", "content": "CAUSAL DIAGNOSTICITY substantially depends upon the efficacy of the knowledge editing method. Our framework presupposes that the applied edits are capable of generalizing across diverse surface forms and reasoning processes while maintaining compositionality. Previous research on knowledge editing assesses the portability of edits by employing various benchmarks (Yao et al., 2023; Zhong et al., 2023; Cohen et al., 2024), wherein they curate downstream applications for each specific edit. Nevertheless, the creation of such benchmarks pertinent to our tasks necessitates substantial effort, which is not within the scope of this study. Consequently, we utilize the perplexity relationship between edits and synthetically generated explanations as an indicative measure of model editing success. Furthermore, while we undertake an ablation study employing MEMIT, the potential benefits of model-generated explanations and more extensive models employing alternative editing techniques remain largely unexamined. This oversight is primarily attributed to the considerable computational expense associated with resolving issues in model-generated explanations, which involve the use of parameter-updating editing methods or memory-based approaches that necessitate extended contexts. Additionally, our scaling experiments exclude CC-SHAP owing to its slow execution. Specifically, memory-based methods considerably extend the duration of experiments involving CC-SHAP as they increase context length."}, {"title": "B Knowledge Editing", "content": ""}, {"title": "B.1 MEMIT", "content": "MEMIT (Meng et al., 2023) is a locate-and-edit-based knowledge editing approach. Unlike pre-"}, {"title": "B.2 In-Context Knowledge Editing", "content": "In-Context Editing methods are memory-based approaches in which new knowledge is introduced to the model via context rather than modifying its parameters. While most memory-based methods, such as IKE (Zheng et al., 2023), MeLLo (Zhong"}, {"title": "B.3 Task-based Editing Templates", "content": "Table 6 shows the templates we use for editing models in each task. For the FactCheck task, there is a variety of prompts where the action or situation of the subject differs, but the target is always located"}, {"title": "C Faithfulness Metrics", "content": "Predictions and Explanations We use different prompts based on the explanation type, which can be either post-hoc or CoT, to generate predictions"}]}