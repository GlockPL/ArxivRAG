{"title": "Automated Test-Case Generation for REST APIs Using Model Inference Search Heuristic", "authors": ["Clinton Cao", "Annibale Panichella", "Sicco Verwer"], "abstract": "The rising popularity of the microservice architectural style has led to a growing demand for automated testing approaches tailored to these systems. EvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) to automatically generate test cases for microservices' REST APIs. One limitation of these EAs is the use of unit-level search heuristics, such as branch distances, which focus on fine-grained code coverage and may not effectively capture the complex, interconnected behaviors characteristic of system-level testing. To address this limitation, we propose a new search heuristic (MISH) that uses real-time automaton learning to guide the test case generation process. We capture the sequential call patterns exhibited by a test case by learning an automaton from the stream of log events outputted by different microservices within the same system. Therefore, MISH learns a representation of the system-wide behavior, allowing us to define the fitness of a test case based on the path it traverses within the inferred automaton. We empirically evaluate MISH's effectiveness on six real-world benchmark microservice applications and compare it against a state-of-the-art technique, MOSA, for testing REST APIs. Our evaluation shows promising results for using MISH to guide the automated test case generation within EvoMaster.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the microservice architectural style has gained significant traction among companies developing modern (web) applications. This shift is largely driven by the benefits provided by this architectural style: it enables developers to develop, maintain, and deploy the components (known as microservices) of their applications independently. Microservices interact with each other via REST APIs, and a request by a client may involve a sequence of REST API calls between microservices to get the corresponding response. Despite the benefits, microservices are notoriously known to be hard to debug once a fault has occurred due to their distributed nature. Different studies have shown that developers usually spend days debugging a fault, making it a time-consuming process [32], [55]. As with any software system, writing system-level tests could mitigate potential faults after deployment. However, manually crafting these tests for microservices is not only labor-intensive but also prone to incomplete coverage as developers might not cover all possible cases that could occur in the system due to their complexity [32], [40], [49], [50]. These challenges have lead to the growing need of tools to generate test cases for microservices automatically. EvoMaster is a state-of-the-art tool that uses Evolutionary algorithms (EAs) [8] to create test cases for microservice applications with web APIs. Specifically, the EA continuously generates and evolves a pool of test cases that can be executed against the system under test (SUT). Each test case represents a sequence of REST API calls for a particular type of resource. Test cases are evolved based on a set of objectives (e.g., line coverage, branch distances, etc.) used to define the fitness of the test cases. Typically, randomized operations such as mutation and crossover are used to evolve the test cases. Furthermore, different (randomized) operations are used to select test cases to keep within the pool. Despite their success, existing EA-based approaches rely heavily on unit-level search heuristics, such as branch distances, to guide the generation of test cases. Branch distance, for example, measures how close a test case is to cover a specific branch in the code using Korel's rules [29] for branching conditions. While effective for maximizing fine-grained code coverage within individual units or services, these heuristics provide no guidance on system-level behaviors. Specifically, they lack awareness of how microservices interact and how execution flows across services. This narrow focus on unit-level objectives can result in test cases that fail to explore distributed systems' intricate, interconnected behaviors, thereby missing faults that arise from complex inter-service interactions [46]. To address this limitation, we introduce the Model Inference Search Heuristic (MISH), a novel grey-box approach to guide the automatic test case generation process. Unlike white-box techniques that rely on code-level metrics such as branch distance, MISH leverages log statements printed by the SUT during test execution, which are inserted into the code by developers. MISH continuously collects log statements output by the SUT during test execution and uses them to learn a finite state automaton (or simply a state machine) in real time. Each test case is converted into a sequence of events derived from these logs, and the inferred state machine serves as a representation of the system's behavior. We learn a state machine using the online learning heuristic proposed by Baumgartner and Verwer for FlexFringe [13], a framework for learning state machines. Our approach uses a single objective"}, {"title": "II. BACKGROUND & RELATED WORK", "content": "This Section describes the concepts used in this work, this would help the reader understand the methods utilized in MISH. Additionally, we list related works for each concept.\nEAs are widely applied within the field of search-based software testing to automate different software testing tasks such as test case generation [24], [38], minimizing test suites [52], and prioritizing test cases [14]. Prior studies have shown that automating these tasks has significantly reduced the time that developers spend on testing and debugging their system [2], [45]. Several automated software testing tools also incorporate EAs to generate test cases for various level of testing, such as EvoSuite [24], EvoMaster [7], SUSHI [16], EXSYST [26], and Sapienz [1]. Inspired by the biological evolution process in nature, an EA starts with an initial population of randomly sampled test cases. In the context of software testing, a test case represents an individual in the population, and the statements in a test represent its genes. EA runs the test cases to determine their fitness values based on a predetermined fitness function (e.g. line coverage). The EA then generates offspring from the population of test cases using genetic operations such as crossover or mutation. The offspring are also run to determine their fitness values. Finally, EA selects the best candidates from the parent and offspring population to form the new population of test cases that survive to the next iteration of the EA. This process (evaluation, generation of offspring, and selection) is repeated until the stopping criterion is met (e.g., search budget time is exhausted).\nRepresentation State Transfer (REST) APIs are predominantly used for communication or requesting resources from a (micro) service. These are typically done by executing (HyperText Transfer Protocol) HTTP calls towards pre-defined endpoints. With the increasing adoption of the microservice architectural style, testing REST APIs automatically has also become an important topic of research. Testing REST APIs comes in two flavors: black-box or white-box. The former does not have access to the source/byte code of the system, meaning no information on the SUT's internal structure can be used to guide the test case generation. The latter approach has access to the internal structure and can use this information to generate test cases to trigger specific parts of the SUT. RESTTESTGEN [48], RESTest [36], and RapiTest [22] are methods developed to automatically test REST APIs that use black-box heuristics. EvoMaster also provides the option to generate test cases following in black-box mode [9]. These typically use the API specifications of the SUT to generate test cases. For the white-box approach, many state-of-the-art techniques are built as search algorithms within EvoMaster. The Many Independent Objective (MIO) algorithm is a state-of-the-art technique built within EvoMaster [6]. This algorithm treats each test target as an independent objective and tries to find a test case that covers the objective. The aim is to find a test suite that covers as many (independent) objectives as possible. MIO keeps track of a population of test cases for each objective and creates the final test suite by taking the union of the best test cases from all populations. The Many Objective Sorting Algorithm (MOSA) is another state-of-the-art technique built within EvoMaster. Proposed by Panichella et al. [38], this algorithm treats test case generation as a multi-objective optimization problem. Instead of optimizing each objective separately as with the MIO algorithm, all objectives are optimized at the same time in MOSA. Unlike MIO, MOSA keeps track of a single population of test cases. The final result is a minimal test suite containing the test cases covering as many objectives as possible.\nA state machine, also known as a finite state automaton, is a mathematical model for capturing the sequential behavior"}, {"title": "III. APPROACH", "content": "Our approach continuously learns a state machine model from event logs outputted by the SUT and uses the model to define fitness for the test cases that have been executed. This section details the internal workings of our novel algorithm, MISH, including (1) the transformation of log events outputted by the SUT into traces, (2) the interaction between EvoMaster and the model inference framework to update a state machine with these traces continuously, (3) the computation of the fitness for each test case, and (4) the evolution of test cases for future iterations/generations. Figure 2 depicts a high-level overview of MISH's workflow, and Algorithm 1 outlines the pseudo-code.\nAs described in Section II, a state machine is learned from traces of symbols. In the context of this work, each symbol in a trace represents an arbitrary log event outputted by the SUT, and each trace represents a sequential behavior exhibited by the SUT (based on the sequence of log events). We use a state machine to model these sequential behaviors within the SUT. To transform event logs into traces, we first group the log events based on the start and end timestamps of the test cases. This is done to capture the sequential behavior exhibited by the SUT after running each test case. MISH requests the test cases' start and end timestamps from EvoMaster and temporarily stores them in memory (see line 3 in Algorithm 1). We compare each log event's timestamp against each test case's start and end timestamp. If the log event occurred within this period, we add it to the group created for this test case. We"}, {"title": "IV. EMPIRICAL STUDY", "content": "This section details the empirical study we carried out to evaluate MISH's feasibility and effectiveness for generating system-level test cases. We first outline the research questions addressed in the study. Next, we detail the selection of applications and parameter settings used in EvoMaster for evaluation. Finally, we describe the metrics employed to compare the performance of MISH against MOSA.\nThe study aims to answer the following research questions:\nRQ1 focuses on evaluating the overall effectiveness of MISH in generating test cases that achieve high system-level coverage and reveal complex behaviors in REST APIs compared to MOSA. With RQ2, we aim to investigate in detail the scenarios in which MISH generates effective test cases and the scenarios in which MISH struggles with generating effective test cases. Finally, RQ3 aims to evaluate and compare the efficiency of MISH in achieving test coverage over time, in comparison to MOSA."}, {"title": "V. EVALUATION RESULTS", "content": "We now answer our research questions based on the results achieved from our empirical study. Each research question is answered separately in the following subsections."}, {"title": "VI. THREAT TO VALIDITY", "content": "Concerning threats to construct validity, we evaluated all evolutionary algorithms (EAs) using metrics widely employed in prior studies: the number of covered targets, the number of potential faults detected, and the number of potential 500 faults detected. For the stopping criterion, we utilized a commonly used approach based on a fixed search budget measured by runtime. Specifically, we set the search budget to 60 minutes per run, consistent with prior research on EvoMaster. A potential threat to internal validity could arise from the integration of state machine learning into the EA for test-case generation. The additional learning time for the state machine could negatively impact the runtime of the EA. However, based on the average statistics collected from the model inference framework, the state machine learning time is in the order of milliseconds. Thus, we consider this impact to be minimal. For threats to external validity, we selected six applications from the well-established EvoMaster Benchmark (EMB), which has been extensively used in prior research. Our study focused on real-world applications, excluding any artificial or synthetic applications. The selected applications vary in size, including differences in the number of source files, lines of code, and API endpoints, which helps assess the generalizability of our algorithm across diverse scenarios. To address threats to conclusion validity, we followed best practices from prior studies on EvoMaster. Each algorithm was run 20 times per application, using different random seeds to account for the stochastic nature of EAs. For statistical analysis, we employed the Wilcoxon rank-sum test to evaluate the significance of differences between algorithms and the Vargha-Delaney effect size measure to quantify the magnitude of these differences."}, {"title": "VII. CONCLUSION & FUTURE WORK", "content": "We presented MISH, a novel approach that uses real-time automaton learning as a search heuristic to guide the test case generation within EvoMaster. MISH continuously updates an automaton by gathering logs produced by the SUT after running a generation of test cases. MISH computes fitness for the test cases based on the path traversed by the log statements within the automaton. MISH uses the automaton to capture the sequence of REST API calls needed to trigger deep and connected behaviors between microservices. We implemented MISH within EvoMaster and empirically evaluated its effectiveness in generating test cases on six microservice applications from the EMB benchmark. Our evaluation results indicate that MISH performs comparably to MOSA, a state-of-the-art technique for generating test cases for microservices, and outperforms MOSA in certain scenarios. MISH achieves this performance while utilizing only a single objective to determine fitness for test cases, whereas MOSA utilizes multiple objectives. MISH provides promising results for the use of model inference as a search heuristic in test case generation. Furthermore, our results suggest that MISH is quicker at discovering unseen targets at the start of the search compared to MOSA. In future work, we plan to investigate the effect of utilizing MISH as part of a many-objective search algorithm (e.g., combining MOSA and MISH). We speculate that by incorporating MISH within a many-objective search algorithm, we can aid the many-objective search algorithm in discovering unseen targets quicker at the start of the search. Moreover, we can utilize different information gathered by the many-objective search algorithm to help MISH with its search when its performance reaches a plateau. In its current implementation, MISH communicates with the model inference framework via file-based interactions, which involve frequent disk I/O operations. Over many generations of test cases, this file-based communication negatively impacts performance. To address this limitation, we plan to replace the file-based communication with a specialized API. This new communication mechanism would not only improve efficiency but also enhance the robustness of MISH by mitigating potential communication failures."}]}