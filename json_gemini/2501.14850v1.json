{"title": "ON THE LOCALITY BIAS AND RESULTS IN THE LONG RANGE ARENA", "authors": ["Pablo Miralles-Gonz\u00e1lez", "Javier Huertas-Tato", "Alejandro Mart\u00edn", "David Camacho"], "abstract": "The Long Range Arena (LRA) benchmark was designed to evaluate the performance of Transformer improvements and alternatives in long-range dependency modeling tasks. The Transformer and its main variants performed poorly on this benchmark, and a new series of architectures such as State Space Models (SSMs) gained some traction, greatly outperforming Transformers in the LRA. Recent work has shown that with a denoising pre-training phase, Transformers can achieve competitive results in the LRA with these new architectures. In this work, we discuss and explain the superiority of architectures such as MEGA and SSMs in the Long Range Arena, as well as the recent improvement in the results of Transformers, pointing to the positional and local nature of the tasks. We show that while the LRA is a benchmark for long-range dependency modeling, in reality most of the performance comes from short-range dependencies. Using training techniques to mitigate data inefficiency, Transformers are able to reach state-of-the-art performance with proper positional encoding. In addition, with the same techniques, we were able to remove all restrictions from SSM convolutional kernels and learn fully parameterized convolutions without decreasing performance, suggesting that the design choices behind SSMs simply added inductive biases and learning efficiency for these particular tasks. Our insights indicate that LRA results should be interpreted with caution and call for a redesign of the benchmark.", "sections": [{"title": "1 Introduction", "content": "The Transformer architecture [Vaswani et al., 2017] has revolutionized sequence modeling, and has been the basis for many state-of-the-art breakthroughs in NLP, computer vision, and reinforcement learning. Despite its success, it is not without limitations. Being reliant on the attention mechanism, the Transformer suffers from quadratic complexity in the input sequence length, which makes it difficult to scale to long sequences and expensive to train and run. Addressing this issue has become a hot topic in the deep learning community, and many variations of the Transformer have been proposed to improve its scalability.\nIn this work, we focus our attention on the Long Range Arena benchmark [Tay et al., 2021], which was designed to evaluate the ability of models to capture long-range dependencies. This benchmark encompasses tasks in the image, text, and mathematical domains, and includes sequences of up to 16,384 elements. The Transformer architecture and its variants have been shown to perform poorly on these tasks, which has motivated the development of new architectures.\nThese new architectures, such as State Space Models (SSMs) [Gu et al., 2022a], were synthesized to a common underlying idea by Li et al. [2023]. They can all be reformulated as a long-convolution, with a kernel size matching the full sequence length. This kernel should be efficiently parameterized, that is, scale sublinearly with the input sequence length, and should include a time decay mechanism, reducing the weight of elements based on their distance to the current token."}, {"title": "2 Background", "content": ""}, {"title": "2.1 The attention mechanism", "content": "The attention mechanism, paramount in the Transformer architecture, suffers from quadratic complexity in the sequence length. Indeed, given queries $Q \\in \\mathbb{R}^{L_1 \\times D_{qk}}$, keys $K \\in \\mathbb{R}^{L_2 \\times D_{qk}}$, and values $V \\in \\mathbb{R}^{L_2 \\times D_v}$, where $L_1$ and $L_2$ are the sequence lengths, and $D_{qk}$ and $D_v$ are the dimensions of the query/key and value vectors, respectively, the output of the attention module is given by\n$Y = softmax(\\frac{QK^T}{\\sqrt{D_{qk}}})V$.\n(1)\nThe attention matrix $A = softmax(QK^T/\\sqrt{D_{qk}})$ is of size $L_1 \\times L_2$, yielding a computation time complexity of $O(L_1L_2D_{qk})$ and a naive space complexity of $O(L_1L_2)$. In reality, we can avoid quadratic memory complexity by recomputing the attention matrix during the backward pass [Dao et al., 2022].\nReducing the cost of running the Transformer and allowing it to process long sequences is a hot topic in Deep Learning. From hardware usage optimizations such as the I/O optimizations in Flash Attention [Dao et al., 2022] or precision reductions and quantization [Gholami et al., 2022], to architectural modifications like the Linear Transformer [Katharopoulos et al., 2020], the Reformer [Kitaev et al., 2020], blockwise attention [Qiu et al., 2020, Dai et al., 2019] or sliding-window attention [Zaheer et al., 2020], we have seen a large body of work on this very topic."}, {"title": "2.2 Long Range Arena", "content": "To evaluate the performance and efficiency of the different architectural proposals for long-sequence modeling, Tay et al. [2021] proposed a benchmark with tasks in the text, image, and math domains, called the Long Range Arena. These tasks were designed to include long sequences, ranging from 1000 to 16000 elements, while keeping the computational burden reasonably low. It includes six different datasets.\nListOps This synthetic task involves calculating the result of nested mathematical operations in parentheses, which form a tree-like structure. Both operands and results are always integers between 0 and 9, and sequences can be as long as 2000 tokens between operations and operands. Four different operations can appear: minimum (MIN), maximum (MAX), median (MED), and sum modulo 10 (SM). The following is a short example: MIN [MAX [1, 2, 3], 4, 5] = 3.\nText Classification The IMDB sentiment analysis task, involving the classification of movie reviews as positive or negative. In this benchmark, proposals are required to tokenize the text at the byte level, instead of the usual subword tokenization. They should also use sequence lengths of 1000, 2000, 3000 or 4000 bytes.\nDocument Retrieval This task involves finding a similarity score between two documents. The dataset is the ACL Anthology Network, where the model has to predict whether two papers have a citation link. The documents are tokenized at the byte level as well, this time requiring a fixed sequence length of 4000 bytes. Inter-token interaction between documents is not allowed for this task, this is, each document needs to be summarised to a fixed-size vector representation and then compared.\nImage Classification The CIFAR-10 image classification task. Images are 32 \u00d7 32 pixels, and the task is to classify them into one of ten classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. In this benchmark, images are given in grayscale, and they must be encoded as a one-dimensional sequence of individual pixels, totaling 32 \u00d7 32 = 1024 elements. No information about the two-dimensional structure of the image can be provided to the model."}, {"title": "2.3 New architectures", "content": "Motivated by the poor performance of the Transformer in the Long Range Arena, several works have been published proposing new architectures that greatly improved their performance and computational efficiency. In this section, we review some of the most relevant."}, {"title": "2.3.1 State Space Models", "content": "In 2022, Gu et al. [2022a] proposed a new architecture, S4, based on the discretization of a differential equation model called the State Space Model (SSM). For a 1D input signal u(t), the differential equation is the following:\n$\\begin{cases}\nx'(t) = Ax(t) + Bu(t)\\\\\ny(t) = Cx(t) + Du(t)\n\\end{cases}$,\n(2)\nwhere x is the hidden state, u is the input signal, and y is the output signal. The elements A, B, C, and D are the parameters of the model. The system is discretized with a fixed time step \u2206, and the D parameter is removed, as it can be seen as a skip connection. The discretized system is then:\n$\\begin{cases}\nX_{t+1} = \\overline{A}x_t + \\overline{B}u_t\\\\\nY_t = \\overline{C}x_t\n\\end{cases}$,\n(3)\nwhere $\\overline{A} = f(\\Delta, A, B)$ and $\\overline{B} = f(\\Delta, A, B)$ depend on the discretization method. This equation can be seen as a linear recurrent network without inter-token nonlinear activations. This greatly reduces expressiveness, as nonlinearities will only appear in token-wise operations. However, it also has great properties. In addition to the recurrent formulation that allows for very fast auto-regressive inference, the recurrence can be expanded to\n$x_{t+1} = \\sum_{i=0}^{t} \\overline{A}^i \\overline{B}u_{t-i}$,\nand thus be seen as a convolution operation, allowing for fast and parallelizable training.\nWhen the input signal is a vector, the parameters A, B, and C are matrices. In this case, the exponential of the matrix A is not straightforward, but the kernel can be efficiently computed by parameterizing the matrix A as the sum of a normal diagonal matrix minus a low-rank matrix. This architecture pushed the state-of-the-art average accuracy in the Long Range Arena from 59.37% to 86.09%."}, {"title": "2.3.2 Synthesizing SSMs", "content": "Posterior work has been published that synthesizes the key ideas that make the S4 model work. Orvieto et al. [2023] developed a similar linear recurrent model with a diagonal parameterization with complex eigenvalues in the unit disk. They thus removed the discretization step or the normal minus low-rank decomposition. Furthermore, independent processing of each dimension in the diagonalized space allows us to process the convolution in O(Llog L) by using the Fast Fourier Transform, where L is the sequence length. They achieved performance comparable in the long range arena to that of the S4 model.\nNote that S4 has a convolutional mode that uses a kernel of the same size as the input signal. Li et al. [2023] tried to generalize these architectures by creating a convolutional model directly and trying to pin down the properties that these long convolutions should have to achieve good performance. They found two sufficient conditions. First, the kernel should be parameterized in a way that makes the effective number of parameters scale sublinearly with sequence length. Second, the kernel should satisfy a decaying structure, that is, the weights assigned to distant tokens should be smaller in magnitude than those assigned to closer tokens. They developed the SGConv architecture, which is a simple convolutional model that satisfies these properties, and achieved comparable performance in the Long Range Arena. The authors showed that the S4 model fulfilled these properties, and the model developed by Orvieto et al. [2023] clearly does too, as the exponential of a matrix with complex eigenvalues of norm less than 1 will decay in magnitude, and the number of parameters is linear in the hidden dimension and independent of the sequence length."}, {"title": "2.3.3 MEGA", "content": "At the time of writing, the current state of the art in the Long Range Arena is MEGA [Ma et al., 2023], which alternates between the attention mechanism and an exponential moving average (EMA) across the sequence length. More formally, given input embeddings $x_1, ..., x_l$ in $\\mathbb{R}^D$, the output of the EMA component is\n$Y_t = a \\odot x_t + (1 - a \\odot \\delta) \\odot Y_{t-1}$,\n(4)\nwhere $\\delta \\in (0, 1)^D$ is a damping vector and $a \\in (0, 1)^D$ is a component-wise decay factor. The EMA operation is also a form of diagonal linear recurrence with eigenvalues of norm less than 1, like the one in Orvieto et al. [2023]. This yields a convolutional kernel that meets the properties discussed by Li et al. [2023]. The MEGA model achieved state-of-the-art performance in the Long Range Arena, with an average accuracy of 88.21%."}, {"title": "2.4 Improving the Transformer in the Long Range Arena", "content": "Amos et al. [2024] managed to achieve close to state-of-the-art performance in the Long Range Arena with the rotary Transformer by pretraining the models with a denoising objective, using the data from the downstream task. Similar to the pretraining of BERT [Devlin et al., 2019], the model is asked to predict randomly masked tokens in the sequence.\nUsing this pretraining procedure, they also achieved competitive performance with a simple diagonal linear recurrent model, without the need for any complex parameterization or initialization as in the S4 model or the model by Orvieto et al. [2023]."}, {"title": "2.5 gMLP", "content": "Liu et al. [2021] tried to achieve BERT's performance on standard text tasks using only MLP networks. To do this, they developed the gMLP architecture, alternating between channel-wise and spatial-wise or sequence-wise projections. Let $X \\in \\mathbb{R}^{L\\times D}$ be the input data, where L is the sequence length and D is the embedding dimension. The gMLP layer is defined as follows:\n$\\tilde{Z} = \\sigma(XU), Z_1, Z_2 = split(\\tilde{Z}, axis=\\\"D\\\"), \\overline{Z} = Z_1 \\odot (WZ_2 + b), O = \\overline{Z}V$,\n(5)\nwhere $U \\in \\mathbb{R}^{D\\times H}$ and $V \\in \\mathbb{R}^{H\\times D}$ are projections in the channel dimension, acting on each token independently, and $W \\in \\mathbb{R}^{L \\times L}$ and $b \\in \\mathbb{R}^L$ are the weights and biases of a linear projection across the sequence dimension. The function $\\sigma$ is a non-linear activation function such as the GELU.\nThey managed to achieve competitive performance with BERT on the GLUE benchmark. The authors observed that the learned spatial projection matrices W were, in fact, Toeplitz matrices, which resulted in a long convolutional operation. Unlike the long convolutions in the models described in the previous sections, the kernels in gMLP are fully learned and unrestricted."}, {"title": "3 Training method", "content": "As we hypothesized that the reasons for the poor performance of the Transformer in the LRA are the lack of inductive bias, and very high-dimensional and insufficient data, we apply techniques to avoid overfitting and prevent the model from learning spurious correlations.\nCIFAR-10. Images are amenable to very natural data augmentation techniques. In the CIFAR-10 classification task, we apply a strategy derived from the one discovered in for AutoAugment [Cubuk et al., 2019]. The specific techniques can be found in the source code files indicated in appendix A.3.\nPathfinder. The LRA dataset provides three sets of examples of different difficulty, depending on the path lengths and amount of distracting paths. The common procedure is to train only on the difficult set. We try training on all three sets, without augmentation. The advantage of this over using augmentation is that the dataset is more varied and that the model is able to find the signal faster in the easier sets. Further discussion is provided in Appendix C.\nTo make the comparison fair and maintain the number of training steps, we reduce the number of epochs from the usual 200 to 67. Validation and test sets are drawn from the difficult set, and we remove subsets of similar sizes from the easy and intermediate ones to have exactly three times the number of training samples.\nListOps. As each mathematical operation in the task is commutative, we can shuffle the order of the operands without altering the result. In other words, at each node of the operation tree, we can apply a random permutation to the children nodes. Using this technique, we can generate many more training samples from the same input data."}, {"title": "4 Transformer performance", "content": "To test whether the lackluster performance of Transformers stemmed from a lack of inductive biases and insufficient data, we train the Transformer using our mitigating strategies. Similarly to Amos et al. [2024], we use rotary embeddings [Su et al., 2024] to encode positional information. We compare our results with the original Transformer results in the Long Range Arena [Tay et al., 2021] and those obtained by Amos et al. [2024] using a denoising pre-training objective. We also include results from the state-of-the-art MEGA model [Ma et al., 2023], and replicate their experiments using our training techniques.\nThe results are shown in table 2. We achieved a comparable performance with Amos et al. [2024] without the need for a pretraining phase, reaching a higher average accuracy and outperforming in all nontextual tasks. Both sets of techniques manage to take the Transformer to near state-of-the-art results. We also observe that our techniques do not seem to improve the performance of MEGA. In fact, it gets slightly worse, possibly due to an inexact replication of their optimization process to match ours (see appendix A.2).\nOur results suggest that prior Transformer results were indeed due to a lack of inductive bias for the tasks and insufficient data, as we hypothesized. The MEGA model, on the other hand, was able to reach its peak performance without better training strategies, as its inductive biases make it very data-efficient in this benchmark."}, {"title": "5 Removing restrictions from long-convolution kernels", "content": "Next, we use the same techniques to train an unrestricted and freely parameterized long-convolution-based model, gMLP, and compare it with SSMs S4 [Gu et al., 2022b] and S5 [Smith et al., 2023]. The results are shown in table 3. The gMLP model achieves performance comparable to both SSM models and even outperforms them on average. The SSM models barely benefit from improved training strategies, suggesting that they were able to reach peak performance without them because of their inductive biases. Our results indicate that the design choices in SSMs merely added data efficiency for these particular tasks, and not better long-range dependency modeling."}, {"title": "6 Discussion on the positional and local biases of the Long Range Arena tasks", "content": "Our results indicate that the differences in performance between the Transformer and new architectures such as SSMs in the LRA are likely a byproduct of insufficient datasets and the greater inductive biases of the latter. If we look closely at each task, we can deduce that they are mainly positional. In particular, we expect a strong encoding of relative positions to be effective and more efficient to learn. Further, some of the tasks might benefit greatly from a bias towards locality. Both characteristics favor convolutional models with time decay mechanisms. This might also attribute some of the increased performance of the Transformer to the use of rotary embeddings, which adds similar biases.\nLet us first consider textual data. Tokenizing text at the byte level means that the model has to learn first how to form meaningful word embeddings from letters, a positional and local task. Something similar can be said for images, where we often want to detect patterns in small patches (local neighborhoods). This is made slightly more difficult by the fact that we encode the image as a 1D sequence, separating consecutive vertical pixels by the width of the image, but the argument is still valid. This does not necessarily mean that long-range dependencies are not important. For example, in the Pathfinder task, the two connected dots can be very far apart in the image. Regardless of this, modeling relative positions is likely to be better and more efficient, as it is necessary to find neighboring pixels. Finally, in appendix B we provide a discussion of some of the patterns that are learned in the ListOps task. The extent to which local patterns account for the increase in accuracy to 63% escapes our analysis and still needs to be studied empirically."}, {"title": "7 Ablation study: Transformer performance", "content": "Given the previous discussion, we decide to study the importance of correct positional encoding to increase the performance of the Transformer. Table 4 shows the results. We compare rotary, summed sinusoidal and summed learned embeddings. The learned embeddings are very inferior to the other two, and rotary embeddings are superior to the summed sinusoidal ones, with small margins except for the ListOps task. These results were expected because rotary and sinusoidal embeddings do not include trainable parameters, model relative positions, and add a time-decay mechanism. The superiority of rotary embeddings might come from the fact that they are applied only to key and query vectors, without distorting intermediate representations or value vectors. Summed embeddings add this distortion, and they get entangled with the rest of the information across layers. In table 4 we also find the impact of removing our training techniques. While rotary embeddings\u2014together with better hyperparameters\u2014clearly improve the originally reported performance of the Transformer, not using training techniques to mitigate overfitting causes a massive drop in accuracy."}, {"title": "8 Measuring the importance of locality in the Long Range Arena", "content": "The fact that a locality bias could be helpful in the LRA is concerning in a benchmark for long-range dependency modeling. To discern the importance of locality on each task, we train a convolutional model with small kernel sizes. This experiment should provide a baseline for the performance we can expect with a fixed bound on the range of dependencies that can be modeled. The details of the model can be found in appendix A.2, and the results are shown in table 1.\nIn general, we are able to get fairly close to state-of-the-art performance in each task with a small distance bound of 30 tokens per layer. The Pathfinder task appears to be the least reliant on very short-range dependencies. The extreme cases are the textual tasks, where a distance bound of 2 tokens per layer is sufficient to achieve state-of-the-art"}, {"title": "9 Conclusions", "content": "Our study provides critical insights into the nature of the tasks in the Long-Range Arena (LRA) benchmark, challenging its utility for long-range dependency modeling evaluation. Specifically, we identify that the tasks in the LRA benchmark largely benefit from positional and local dependencies. Our experiments demonstrate that small convolutional networks\u2014limited in the range of dependencies they model\u2014can closely match state-of-the-art results. This finding explains the success of models like MEGA and structured state-space models (SSMs) on the benchmark and raises questions about the adequacy of the LRA in truly testing long-range dependencies.\nWe also establish that the constraints on kernel design, as suggested by Li et al. [2023], are not strictly necessary to achieve strong performance. Instead, enriched data and optimized training strategies can yield similar benefits, with the proposed kernel constraints mainly providing inductive biases and learning efficiency in specific tasks. This opens new pathways for more adaptable model designs for long-range dependency modeling.\nOur analysis highlights that Transformers' historically weak performance on LRA tasks stems primarily from a lack of inductive biases, rather than an inability to model dependencies. By introducing rotary embeddings for local and positional biases and refining training to prevent overfitting, they also manage to achieve state-of-the-art results.\nOur findings point to a significant re-evaluation of both model architectures and the LRA benchmark for long-range dependency modeling. We underscore the need for benchmarks that genuinely assess long-range dependencies, but leave the design of such a benchmark for future work. As general guidelines, synthetic tasks such as Pathfinder seem to be the most robust to short-range dependencies and allow us to modulate the range of dependencies required to solve the tasks. Natural language seems to be an inconvenient modality, as it may prove difficult to create tasks that require long-range dependency modeling but low computational resources, which was one of the intentions behind the LRA. The restriction to simplified synthetic languages is probably the best choice. Finally, we also recommend standardization of training strategies that mitigate overfitting, so that data efficiency can be measured separately from long-range dependency modeling capabilities."}, {"title": "A Experiment details", "content": ""}, {"title": "A.1 Hardware and software", "content": "All experiments were run on a single NVIDIA GeForce RTX 3090 GPU with 24GB of memory or an NVIDIA GeForce RTX 3080 Ti GPU with 12GB of memory. We used PyTorch 2.1.0 with CUDA 12.2. The code for the experiments is available at https://github.com/pablomiralles22/paper-LRA-source. A Conda environment file can be found in the repository to reproduce the Python environment."}, {"title": "A.2 Hyperparameters", "content": "In tables 5 to 7 we show the hyperparameters used to train the Transformer, the gMLP model, and the convolutional model on the LRA. We used the AdamW optimizer, with a learning rate scheduler that reduces the learning rate on training plateau and during the last 10% of the training epochs. The parameters of the Transformer apply regardless of the positional encoding, except those specific to rotary embeddings. The base frequency for sinusoidal embeddings is the same as for rotary embeddings.\nWith regard to the MEGA and SSM experiments, we generally replicated their hyperparameters, with the following exceptions.\n\u2022 When the models did not converge, we reduced the learning rate.\n\u2022 The attention mechanism in MEGA is restricted to the classical softmax one.\n\u2022 We use the same optimizers, schedulers and training epochs that we used for the Transformer, gMLP and convolutional models.\n\u2022 For S5, we used the full-glu activation in Pathfinder and text retrieval, as we obtained better results. We also cut down the epochs in text retrieval due to lack of time during the rebuttal period.\nWith respect to the convolutional model, it consists of several equal layers with residual connections. Each layer starts with a convolution that maps the input embeddings to H channels, with kernel sizes varying between experiments to modify the bound of the distance between tokens that can interact with one another. After a non-linearity, a linear layer returns back to D channels. The code for each layer can be found in the file src/models/layers/res_conv.py."}, {"title": "A.3 Data augmentation", "content": "The augmentation techniques can be consulted in the source code."}, {"title": "A.4 Multi-task learning", "content": "In the multitask learning environment, we summed both the downstream task loss and the denoising loss with the same weight of 1. For the denoising task, we mask 30% of the tokens in the input sequence, of which a third are replaced by random tokens, and the rest are replaced by a special mask token."}, {"title": "B Some remarks on the ListOps task", "content": "Transformers achieved very poor results in the ListOps task, with different variants ranging between 17% and 36% accuracy. If we look at the distribution of labels depending on the root operation in fig. 1, we can see that the labels are not uniformly distributed except for the SM operation. When the root operation is a MIN, the distribution is strongly biased towards 0. The same happens with the MAX operation, but towards 9, and with the MED operation, but towards 4. If we always predict 0 or 9, we already get around 17% accuracy. If we predict 0, 4 or 9, based only on the root"}, {"title": "C Remarks on training for the Pathfinder task", "content": "For this task, we decided to learn from the easy and intermediate sets instead of using augmentation, or both. Of course, this is because we found better empirical results with this approach. The main consideration is that the Transformer seemed to need several passes over the most difficult examples to correctly learn them. The model seems to learn faster this way. However, not using any augmentation or other sets leads to overfitting. A good balance is having extra data or a finite set of possible augmentations. We recommend using a subset of the dihedral group D4 of reflections"}]}