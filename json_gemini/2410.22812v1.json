{"title": "Universality of the \u03c0\u00b2/6 Pathway in Avoiding Model Collapse", "authors": ["Apratim Dey", "David Donoho"], "abstract": "Researchers in empirical machine learning recently spotlighted their fears of so-called Model Collapse. They imagined a discard workflow, where an initial generative model is trained with real data, after which the real data are discarded, and subsequently, the model generates synthetic data on which a new model is trained. They came to the conclusion that models degenerate as model-fitting generations proceed. However, other researchers considered an augment workflow, where the original real data continue to be used in each generation of training, augmented by synthetic data from models fit in all earlier generations. Empirical results comparing discard and augment workflows on canonical datasets and learning procedures confirmed the occurrence of model collapse under the discard workflow and avoidance of model collapse under the augment workflow. Under the augment workflow, theoretical evidence also confirmed avoidance in particular instances; specifically, Gerstgrasser et al. (2024) found that for classical Linear Regression, test risk at any later generation is bounded by a moderate multiple, viz. \u03c0\u00b2/6 of the test risk of training with the original real data alone\u2014no matter how many generations of training with synthetic data take place. Some commentators questioned the generality of theoretical conclusions based on the generative model assumed in Gerstgrasser et al. (2024): could similar conclusions be reached for other task/model pairings? In this work, we demonstrate the universality of the \u03c0\u00b2/6 augment risk bound, across a large family of canonical statistical models, offering key insights into exactly why collapse happens under the discard workflow and is avoided under the augment workflow. In the process, we provide a framework that is able to accommodate a large variety of workflows (beyond discard and augment) thereby enabling an experimenter to judge the comparative merits of multiple different workflows by simulating a simple Gaussian process.", "sections": [{"title": "1 Introduction", "content": "Unbridled public access to text and image generative models (e.g. ChatGPT, Dall-E, Gemini, Claude, etc.) has brought about the concern that the Internet will soon be overwhelmed with artificially generated content. Since AI models are trained on public data, one imagines that future model trainings may soon be dominated by synthetic data. If synthetic data exhibit artifacts and distortions, these might compound across generations, leading to model degeneration, a concern termed as Model Collapse or Model Autophagy Disorder (Shumailov et al., 2024; Alemohammad et al., 2024a; Bertrand et al., 2024; Mart\u00ednez et al., 2023a,b; Hataya et al., 2023; Dohmatob et al., 2024; Feng et al., 2024b). Researchers voicing this concern mostly considered a discard workflow, where one trains with real data at the initial generation, but in all later generations, one each time generates synthetic data from the most recently trained previous model, and then trains on the resulting synthetic data to produce the next generation of trained model. In this way, the original real data are excluded from later generations of training. They showed that, for this workflow, trained model performance suffers unboundedly across successive generations, tending towards eventual degradation. More recently, other researchers have considered an augment workflow, where the original real data continue to be used in each generation of training, alongside synthetic data from models fitted in all previous generations. In specific instances, Gerstgrasser et al. (2024); Marchi et al. (2024); Seddik et al. (2024); Kazdan et al. (2024) have shown that model collapse is avoided by this workflow augmenting real data with synthetic data. In particular, Gerstgrasser et al. (2024) spotlight what we call a \u03c0\u00b2/6 argument, whereby they show that under the augment scenario, for the specific case of Linear Regression, the (excess) test risk is at most \u03c0\u00b2/6(\u2248 1.645) times that of the initial model trained only with real data.\nDespite substantial and accumulating empirical evidence demonstrating avoidance of model collapse in the augment scenario (e.g. Gerstgrasser et al. (2024) and follow-ups), published theoretical explanations of this phenomenon span only a small fraction of the cases where it has been observed empirically. The theoretical calculation leading to the \u03c0\u00b2/6 bound in Gerstgrasser et al. (2024) is no doubt correct; but is seemingly specific to linear regression. A natural question emerges:\nQuestion 1: Is there a theoretical explanation for the reported empirical advantages of the augment training workflow over the discard workflow, in particular avoidance of model collapse in the augment workflow?\nOf course, discard and augment are but only two of the potentially millions of workflows one may employ to iteratively fit models. In some sense, discard and augment are two pessimistic approaches in this regard, since they equally weigh all the (immediate past) synthetic data and all the (accumulated) real+synthetic data respectively, without accounting for data quality or data selection. Despite being pessimistic, that the augment workflow enjoys a bounded error with the bound close to 1, is certainly a pleasant surprise. Nonetheless, it is well known how daunting the challenge is in identifying real from synthetic data, and absent such a discriminatory tool, one would need to resort to these workflows.\nHaving said all this, it is indeed plausible that in the near future, high quality tools will emerge that will be increasingly able to differentiate between real and synthetic data."}, {"title": "2 Related works and our contributions", "content": "The original announcement of Shumailov et al. (2023) (that is where the term Model Collapse was introduced) awakened the world into realizing that naively using only synthetic data for iterative model training leads to eventual, inevitable degradation. Since then, a flurry of intense research has ensued, driven both by empirical and theoretical scientists, who have confirmed the doomsday predictions through a wide variety of experiments and mathematical models (Alemohammad et al., 2024a; Bertrand et al., 2023; Hataya et al., 2023; Mart\u00ednez et al., 2023a,b; Dohmatob et al., 2024; Feng et al., 2024b). Popular media outlets have followed on, highlighting the perils of real data scarcity in an era where the Internet, the most important source of all human knowledge, gets polluted by indiscriminate amounts of poor quality, artificially generated data depositions.\nInterestingly, much before the notion of model collapse became popular, in specific cases, mathematical scientists had already discussed the detrimental effects of iterative model fitting; Mobahi et al. (2020) have discussed the collapse of iteratively fit ridge estimators to 0,"}, {"title": "2.1 Prior works", "content": "The original announcement of Shumailov et al. (2023) (that is where the term Model Collapse was introduced) awakened the world into realizing that naively using only synthetic data for iterative model training leads to eventual, inevitable degradation. Since then, a flurry of intense research has ensued, driven both by empirical and theoretical scientists, who have confirmed the doomsday predictions through a wide variety of experiments and mathematical models (Alemohammad et al., 2024a; Bertrand et al., 2023; Hataya et al., 2023; Mart\u00ednez et al., 2023a,b; Dohmatob et al., 2024; Feng et al., 2024b). Popular media outlets have followed on, highlighting the perils of real data scarcity in an era where the Internet, the most important source of all human knowledge, gets polluted by indiscriminate amounts of poor quality, artificially generated data depositions.\nInterestingly, much before the notion of model collapse became popular, in specific cases, mathematical scientists had already discussed the detrimental effects of iterative model fitting; Mobahi et al. (2020) have discussed the collapse of iteratively fit ridge estimators to 0,"}, {"title": "2.2 Our contributions", "content": "We establish a unified theoretical framework that not only addresses both Questions 1 and 2 (as mentioned in Section 1) simultaneously but also shows that there is no need to perform model-specific calculations. In the appropriate coordinates, for any given workflow, all reasonable models exhibit the same behavior, particularly when the dataset size is large. For"}, {"title": "3 Background", "content": "Unless otherwise stated, P and E will stand for probability measure and expectation respectively. Po and E, will, in particular, denote probability and expectation computed under the real data generating distribution. For example, Eo(f(Z)) will denote the expectation of f(Z) where Z comes from the real data generating distribution Po. will denote convergence in probability, while will denote convergence in distribution. op(1) will denote a quantity that converges to 0 in probability, under P. Z will denote a generic data-point, often split into a feature X and response Y, and hence we will often visualize Z as a tuple Z = (X, Y).\nFor (possibly) random sequences an,bn, an \u2248 bn means an = bn(1+ op(1)) as n \u2192 \u221e. For a vector x \u2208 R\" and r > 0, B(x;r) will denote the ball {y \u2208 R\" : ||y \u2013 x||2 \u2264 r}."}, {"title": "3.1 Notations", "content": "Unless otherwise stated, P and E will stand for probability measure and expectation respectively. Po and E, will, in particular, denote probability and expectation computed under the real data generating distribution. For example, Eo(f(Z)) will denote the expectation of f(Z) where Z comes from the real data generating distribution Po. will denote convergence in probability, while will denote convergence in distribution. op(1) will denote a quantity that converges to 0 in probability, under P. Z will denote a generic data-point, often split into a feature X and response Y, and hence we will often visualize Z as a tuple Z = (X, Y).\nFor (possibly) random sequences an,bn, an \u2248 bn means an = bn(1+ op(1)) as n \u2192 \u221e. For a vector x \u2208 R\" and r > 0, B(x;r) will denote the ball {y \u2208 R\" : ||y \u2013 x||2 \u2264 r}."}, {"title": "3.2 Preliminiaries", "content": "The General Workflow. Workflow 1 describes a very general iterative model fitting procedure that subsumes all the procedures the literature so far has dealt with. We incrementally build up a chain of growing datasets Z1 C Z2 C \u2026 C ZG C\u2026\u2026. Z\u2081 contains the original pristine real data. Iteratively, at generation g, given Zg, we create a dataset Dg+1 which has data of two types: data from a fixed / known model, denoted by Xg+1, and data from a learned model (using, potentially, Xg+1 and parameter daug estimated from Zg), denoted by Ug+1. We combine the new data Dg+1 with all earlier data to get the enlarged dataset Zg+1 = ZgUDg+1. This workflow describes how the Internet evolves: the original real data still exists (since Z1 C Z, for all g) but gets increasingly contaminated with synthetic data Dg+1 produced at generation g. To the best of our knowledge, Gerstgrasser et al. (2024) first explicitly presented Workflow 1, albeit in the specific case of linear regression and with"}, {"title": "Exponential family model", "content": "For clarity of exposition, we focus on a canonical setting, the exponential family of generative statistical models, which encompasses the vast majority of theoretical distributions used in model-building in statistics and machine learning.\nAny exponential family of generative models can be described abstractly as follows. It comprises probability densities of the form\n\\begin{equation}\np(y/n) = exp(nT(y) \u2013 A(n))h(y), y\u2208R^{dy},\u03b7\u0395\u03a9\n\\end{equation}\nHere, \u03b7 is called the natural parameter, T : Rdy \u2192 Rdn is called the sufficient statistic, h(\u00b7) is a function independent of the parameter \u03b7,\n\\begin{equation}\n\u03a9 = {n\u2208R^{dn} : \\int_{R^{dy}} e^{nT(y)}h(y)d\u03bc(y) < \\infty}\n\\end{equation}\nand \u0391: \u03a9 R is the log-partition function defined by\n\\begin{equation}\nexp(A(n) = \\int_{R^{dy}} e^{nT(y)}h(y)d\u03bc(y), \u03b7 \u0395\u03a9,\n\\end{equation}\nwhere \u03bc is a\u03c3-finite probability measure on Rdy. Popular examples include normal, bi-nomial, poisson, exponential, gamma, etc. distributions. It is well known (Lehmann et al., 1986) that exponential families allow convenient mathematical operations, making them par-ticularly attractive for presenting the essential arguments without a barrage of assumptions or notations."}, {"title": "3.3 Assumptions", "content": "Assumption 1. At each generation G \u2265 1, the features XG,1,\u2026\u2026, XG,n are generated iid from a known distribution H, free of G. Also assume that H has finite moments of all orders.\nAssumption 2. At each generation G > 1, given feature XG,i and candidate parameter 0, the response-generating distribution p(\u00b7|\u03b7(XG,i,0)) comes from the exponential family model defined in Equation 2 with natural parameter \u03b7(XG,i, 0) = XG,\u20810 (identifying Rdx as Rdn \u00d7 Rde) and sufficient statistic T(\u00b7). That is,\n\\begin{equation}\np(y|n(X_{G,i}, 0)) = exp(0^T X T(y) \u2013 A(X_{G,i}d))h(y)\n\\end{equation}\nWe will also assume the following regularity condition. There exists r > 0 such that for any (possibly random) \u03b8\u2208 B(0, r), and for any X \u2208 Rdx,\n\\begin{equation}\n|\\frac{\\partial^3 A(\\eta)}{\\partial \\eta_i \\partial \\eta_j \\partial \\eta_k}|_{\\eta=X\\theta} \u2264h(X), \\text{ for all } 1 \u2264 i, j, k \u2264 d_n\n\\end{equation}\nfor a non-negative function h which has finite moments (under H) of all orders."}, {"title": "AAL estimators", "content": "Given data Z\u2081,\u2026\u2026, Zn, we consider weighted M-estimators \u00d4N:\n\\begin{equation}\n\u03b8_N \u2208 arg \\min_\u03b8 \\sum_{i=1}^N \u03c9_{N,i}L(Z_i; \u03b8)\n\\end{equation}\nwhere L(;) is a loss function and \u03b80 is the target parameter of interest. The optimization in equation 5 may in general yield multiple minimizers, but if the loss L(z; 0) is strictly convex in 0 for each z, the minimizer is unique. Under standard regularity conditions on L, and when Z1,\u2026, Zn are iid, \u00cen admits an asymptotic expansion of the form\n\\begin{equation}\n\\sqrt{N}(\u03b8_N - \u03b8_0) = (\\frac{1}{N}\\sum_{i=1}^N \u03c9_{N,i}\u2207^2L(Z_i; \u03b8_0))^{-1} (\\frac{1}{\\sqrt{N}} \\sum_{i=1}^N \u03c9_{N,i}\u2207L(Z_i; \u03b8_0)) + o_p(1)\n\\end{equation}\nWriting AN(0) := \\sum_{i=1}^N \u03c9_{N,i}\u2207^2L(Z_i; \u03b8)/N, we can equivalently express the above as\n\\begin{equation}\n\\sqrt{N}(\u03b8_N \u2013 \u03b8_0) = \\frac{1}{\\sqrt{N}} \\sum_{i=1}^N \u03c9_{N,i}\u03c8(Z_i; \u03b8_0) + o_p(1)\n\\end{equation}\nwhere \u03c8(z; 0) = AN(0)\u207b\u00b9\u2207L(z;0). Usually, AN(\u03b8) concentrates around a deterministic quantity, yielding an asymptotically linear form for the estimator ON. Such an estimator On satisfying Equation 7 will therefore be called asymptotically approximately linear (AAL). Common examples (under appropriate conditions on the model) include the (weighted ver-sions of) sample mean, the sample median, estimators in linear regression, logistic regression, probit regression, quantile regression, etc.\nAs discussed previously, choosing different weights for data points is becoming increasingly important in modern machine learning, especially when one aims to emphasize high-quality data to improve model performance or to mitigate the effects of distribution shift. Of course, WN,i = 1 for all i covers the case of usual unweighted M-estimators."}, {"title": "4 Comparison of Experiments and Statistical Efficiency", "content": "Classical statistical decision theory, formulated in the 1920's-1950's, delivered a theoretically airtight way to make statements of the form: \u201cthis estimator can achieve the same results as this other estimator, using only a fraction of f as much data\". The key heuristics go back to Sir RA Fisher, but today's final formalization is credited to Abraham Wald and Jacob Wolfowitz, Erich Lehmann and, in its most advanced conceptualization, Lucien Le Cam and Jaroslav Hajek; see the discussion in Lehmann et al. (1986). Suppose that estimator 01 achieves mean-squared error MSE(01, n\u2081) on a sample of size n\u2081 and estimator 02 achieves MSE(02,n2) on a sample of size n2. Consider 'proportionally growing' sequences where n1/n2 \u2192 fas each ni \u2192 \u221e, for some f \u2208 (0,\u221e). Suppose that:\n\\begin{equation}\n\\frac{MSE(\u03b8_2, n_2)}{MSE(\u03b8_1, n_1)} \u2192 1, \\quad n_2 \u2192 \u221e, \\quad n_1 \\sim f \\cdot n_2 \u2192 \u221e.\n\\end{equation}\nThen we say that the asymptotic relative efficiency of 12 relative to \u03b8\u2081 is f \u00d7 100 percent. Consider the particular case where the estimators are root-n consistent and asymptotically normal, i.e. obey:\n\\begin{equation}\n\\sqrt{n}(\u03b8_i \u2013 \u03b8_0) \\stackrel{approx.,n\u2192\u221e}{\u223c} N(0, V_i), i = 1, 2.\n\\end{equation}\nIn such cases MSE(\u00d4i, ni) ~ Vi/ni, so the required sample size ratio for equal performance MSE(01,71) MSE(02, n\u2082) obeys: N1/N2 \u2192 f = V2/V\u2081. The ARE of 12 relative to \u03b8\u2081 is, therefore, simply the inverse of the ratio of the asymptotic variances, viz.\n\\begin{equation}\nARE(\u03b8_2; \u03b8_1) = V_1/V_2.\n\\end{equation}\nIf V\u2081 > V2 we conclude that 02 is at least as good as 61, and can also consider ARE(01;02) reversing the order of arguments. Note that a wide collection of parameter estimation pro-cedures studied in machine learning and statistics are based on empirical risk minimization (ERM) and usually they admit a limiting Gaussian approximation as in (11).\nThe role of \u2081 in our comparisons is played by the traditional way of applying our con-sidered estimation rule applied only to the n real data. The role of 62 is played by that same estimation rule, however the rule applied to a nontraditional dataset, which may be of the same size or a larger size. In the discard and augment cases, we denote the resulting estimates by odis (discard) and baug (augment).\nOur main results consider a wide variety of canonical statistical models, and we show that, under the augment workflow, the asymptotic relative efficiency is always bounded below by 60%, no matter how many synthetic data generations take place. More precisely: at any generation G,\n\\begin{equation}\nARE(\u03b8^{aug}_G; \u03b8_1) \u2265 6/\u03c0^2 > 60\\%, \\quad \u2200G \u2265 1;\n\\end{equation}\nin contrast, under the discard workflow, efficiency degrades with increasing G:\n\\begin{equation}\nARE(\u03b8^{dis}_G; \u03b8_1) \u2192 0, \\quad G \u2192 \u221e.\n\\end{equation}"}, {"title": "5 Main results", "content": "The main result establishes an asymptotic equivalence between any sequence of model-fitting iterations and a Gaussian sequential experiment. In the large sample limit, the statistical properties of the model-fitting procedure align with those of a Gaussian process. Consequently, the specific complexities of the model-fitting process become irrelevant, allowing one to focus solely on the Gaussian process. This equivalence offers a powerful and unify-ing perspective, enabling researchers to analyze the Gaussian process alone and thus make confident predictions about their experiment."}, {"title": "5.1 Main Theorem", "content": "The main result establishes an asymptotic equivalence between any sequence of model-fitting iterations and a Gaussian sequential experiment. In the large sample limit, the statistical properties of the model-fitting procedure align with those of a Gaussian process. Consequently, the specific complexities of the model-fitting process become irrelevant, allowing one to focus solely on the Gaussian process. This equivalence offers a powerful and unify-ing perspective, enabling researchers to analyze the Gaussian process alone and thus make confident predictions about their experiment."}, {"title": "5.2 Implication on estimation", "content": "We now exhibit the power of Theorem 5.1 in theoretically analyzing two commonly studied workflows in the literature on model collapse. Several researchers have previously focused on the discard workflow and highlighted collapse in specific theoretical models (Shumailov et al., 2024; Alemohammad et al., 2024a; Dohmatob et al., 2024; Bertrand et al., 2024). By choosing a specific sequence of weights that correspond to using only the immediate past synthetic data and discarding the previous history, we establish that collapse is bound to happen in such a case for a wide variety of models, going significantly beyond the scope of the previous literature. We are not aware of any result at this level of generality."}, {"title": "Lemma 5.2 (Discard workflow)", "content": "Consider the sequence of weights\n\\begin{equation}\nW_{G,g,i}=\\begin{cases}1, & g = G, 1 \u2264 i \u2264 n,\\\\0 & \\text{otherwise}\\end{cases}\n\\end{equation}\nThen, Var(We(G)) = G \u00d7 Var(We(1)). As a result,\n\\begin{equation}\nARE(\u03b8^{dis}_G; \u03b8_1) = 1/G\n\\end{equation}"}, {"title": "Lemma 5.3 (Augment workflow)", "content": "Consider the sequence of weights\n\\begin{equation}\nW_{G,g,i} =\\begin{cases}1, & 1 \u2264 g \u2264 G, 1 \u2264 i \u2264 n,\\\\0, & \\text{otherwise}\\end{cases}\n\\end{equation}\nThen, Var(We(G)) = Var(We(1)) \u00d7 (\u03a3g=1^G1/g^2). As a result,\n\\begin{equation}\nARE(\u03b8^{aug}_G; \u03b8_1) = (\\sum_{g=1}^G 1/g^2)^{-1} \u2265 6/\u03c0^2 > 60\\%\n\\end{equation}"}, {"title": "5.3 Implication on prediction", "content": "Lemmas 5.2 and 5.3 describe the asymptotic MSE of the estimators dis and daug respectively. Can we also understand the behavior of test losses commonly used in machine learning tasks, as model fitting generations progress? To answer this, we consider a particular loss, the cross entropy (CE) loss, evaluated on a test point Z = (X, Y), when Z actually comes from Po but"}, {"title": "Lemma 5.4", "content": "Make Assumptions 1, 2 and 3. Under the discard workflow defined in Lemma 5.2, for any model fitting generation G \u2265 1, formally\n\\begin{equation}\n\\frac{E D_{KL}(\u03b8^{dis}_G || \u03b8_0)}{E D_{KL}(\u03b8_1 || \u03b8_0)} \\stackrel{n\u2192\u221e}{\u223c} G\n\\end{equation}"}, {"title": "Lemma 5.5", "content": "Make Assumptions 1, 2 and 3. Under the augment workflow defined in Le\u0442\u0442\u0430 5.3, for any model fitting generation G \u2265 1, formally\n\\begin{equation}\n\\frac{E D_{KL}(\u03b8^{aug}_G || \u03b8_0)}{E D_{KL}(\u03b8_1 || \u03b8_0)} \\stackrel{n\u2192\u221e}{\u223c} (\\sum_{g=1}^G \\frac{1}{g^2}) \u2264 \\frac{\u03c0^2}{6}\n\\end{equation}"}, {"title": "5.4 Implication on computation and comparison", "content": "Multiple researchers may propose multiple different workflows (that is, multiple different data weighing schemes). Some of them may be as simple as equally weighing or random subsam-pling, whereas others may be more complicated (e.g. applying data selection strategies to cherry-pick high quality data for model building). For only a handful of such procedures can one hope to obtain precise and aesthetic asymptotic characterization of the MSE of the estimator. Lemmas 5.2 and 5.3 describe the MSE of estimators under the discard and augment workflows respectively. However, an example of a workflow where getting an ex-plicit expression for the MSE seems to be challenging, is the so-called augment-subsample workflow. In this new workflow, at generation G, we randomly choose n out of nG data points in Zg to fit the model, and the weights reflect this selection: (WG,g,i)g,i is a uniformly random draw from the set of nG-dimensional binary (0/1) tuples each having exactly n 1's."}, {"title": "6 Examples", "content": "We now provide some explicit examples of common models that exhibit the power of our main result."}, {"title": "6.1 Linear Models", "content": "Linear models are perhaps the most ubiquitous models in statistics and machine learning. The linear model imposes a linear relation y = xT\u1e9e+e where e ~ N(0, \u03c3\u00b2). This corresponds"}, {"title": "6.2 Logistic Models", "content": "The logistic is another highly popular exponential family model used in the (top) softmax layer for modern neural nets to fit binary or multinomial responses. For the binary case, for example, it corresponds to the natural parameter \u03b7(\u03a7, \u03b8) = XT0 with X = x and sufficient statistic T(y) = y\u2208 {0,1}. Direct calculations are difficult in this model. The most popular method to estimate the parameter 0 is by maximum likelihood estimation, which is an M-estimator. As a result, the same \u03c0\u00b2/6 bound will hold in the augment workflow according to Lemma 4.3. This immediately extends the linear regression example considered in Gerstgrasser et al. (2024) to logistic regression for augment workflow, thereby presenting the \u03c0\u00b2/6 bound. Of course, it also immediately tells us that the discard workflow behaves identically as well - the variance of og explodes linearly in logistic regression too, extending the conclusion derived in linear regression by Dohmatob et al. (2024). Thus, key insights from the linear regression settings considered by previous authors transfer immediately to an apparently more complicated model as logistic regression without any further calculation."}, {"title": "7 Experiments", "content": "Classification on Real Datasets from UCI ML Repository. Till now, the empirical study of Model Collapse has been mostly limited to deep learning models (on language and image generation tasks primarily, hence on unstructured data). We explore the discard vs augment workflow to perform classification using iteratively fit logistic regression on four tabular datasets available on the UCI Machine learning Repository: Diabetes, Heart, Wisconsin Breast Cancer and Titanic. Figure 2 shows very clearly the benefit of augment over discard.\nClassification with self-supervised learned features. We obtain self-supervised learned (SSL) features on CIFAR-10 using a variety of models originally trained on ImageNet using ResNet50. The model checkpoints have been taken from Lightly-AI (2023), an open source GitHub repository. We train a (multinomial) logistic model using these features to clas-sify on CIFAR-10, and experiment under both the discard and augment workflows. Figure 3 shows four plots from four highly impactful SSL models: SimCLR (Chen et al., 2020), DINO (Caron et al., 2021), MoCoV2 (He et al., 2020) and SWAV (Caron et al., 2020). In all these cases, the test loss for augment rises more slowly across generations than does the test loss for discard. However, we would like to highlight that the high dimensionality of the features and large sample size made it computationally infeasible to use an off-the-shelf software (e.g. LogisticRegression(),glm()) for fitting the logistic models. Instead, we fit the model using mini-batch gradient descent on the logistic loss, and also stop the gradient descent iterations early, and hence the estimates obtained are not necessarily what would have been obtained if we had the resources to fit an honest logistic model. Still, the benefit of augment over discard is already manifested."}, {"title": "8 On the proof of Theorem 5.1", "content": "Motivation. On a fundamental level, proving Theorem 5.1 boils down to understanding the statistical properties of the dataset obtained after generating (and augmenting) synthetic data. Clearly, our synthetic data is distributionally different from the real data; for example, the real data may consist of iid points but our synthetic data are typically not iid (as they are generated conditional upon the a random parameter provided by a statistical estimate). Further, tracking the dependencies due to iterative model building and data generation presents significant theoretical complications.\nIn the literature, so far, we have seen approaches that attempt to directly work with the shifted distribution (Shumailov et al., 2024; Bertrand et al., 2024; Alemohammad et al., 2024a; Dohmatob et al., 2024; Gerstgrasser et al., 2024; Marchi et al., 2024; Seddik et al., 2024). This lead to involved and cumbersome calculations, hard to do, hard to interpret and, seemingly, hard to enivision where they might lead even in slightly more general models.\nWe develop here an alternate approach. It allows us to entirely avoid working with the complexity of the intergenerational dependence of our synthetic data distributions. Our approach exploits fundamental ideas of statistical decision theory developed by Lucien Le Cam. He showed that, in the locally asymptotically normal situation, if one knows the limit-"}, {"title": "9 Discussion", "content": "In this work, we seek to unify the theoretical literature on model collapse by clarifying first of all the (universal) benefit of augmenting real and synthetic data, and secondly providing a useful workhorse to the data scientist who may be daunted by the numerous possible ways to fit models. A central message that we deliver is that there is no need to do model-specific calculations, and simulating a Gaussian process is all that one needs. We use this workhorse to do a comparative analysis of the discard vs. augment vs augment-subsample workflows. A natural question that emerges at this point is how to optimize the weights (WG,g,i)g,i at generation G to achieve maximal performance. Thanks to Theorem 5.1, this amounts to, in principle, choosing weights that minimize the variance of the asymptotic Gaussian limit, We(G). To avoid obscuring the central message, we do not pursue this line of thought in the current manuscript. Future work will study this in greater depth."}, {"title": "10 Acknowledgement", "content": "The authors would like to thank X.Y. Han for pointing to the GitHub repository, Lightly-AI (2023), containing the checkpoints of SSL models."}, {"title": "A Proof Sketch of Theorem 5.1", "content": "In whatever follows, P will correspond to the (actual) distribution of the nG datapoints in the database Zg at generation G. This is an intractable distribution owing to complicated dependencies among the data points. Let us remind the reader that Pref denotes the reference distribution on the same nG data points, but which assumes all these data points are iid from the original real distribution. Following the Le Cam principle, the goal is to show contiguity of Pref to P and then LeCam's Third Lemma applies.\nFor every G > 1, we will find the asymptotic joint distribution of the quantity\n\\begin{equation}\nS_{n,G} := (\\frac{1}{\\sqrt{n}}\\sum_{i=1}^n X_{g,i} (T(Y_{g,i}) \u2013 \u2207A(X\u03b8_0)), \\sqrt{n}(\u03b8_G \u2013 \u03b8_0))^G_{g=1}\n\\end{equation}\nunder Pref.\nUsing Assumption 3 that\n\\begin{equation}\n\\sqrt{n}(\u03b8_G \u2013 \u03b8_0) = \\frac{1}{G \\sqrt{n}} \\sum_{g'\u2264g i<n} \u03c8(Z_{g',i}; \u03b8_0) + o_p(1)\n\\end{equation}\nwe conclude by the multivariate CLT (using the fact that the weights are independently chosen of the data) that Sn,G \u2192 SG weakly, for a multivariate Gaussian variable SG with mean zero and covariance matrix \u03a3G.\nFor convenience, define, for a generation G, the asymptotic Gaussian limits\n\\begin{equation}\nW_T(G) = asymp.lim. [\\frac{1}{G \\sqrt{n}} \\sum_{i=1}^n X_{g,i} (T(Z_{g,i}) \u2013 \u2207A(\u03b8_0)]\n\\end{equation}\n\\begin{equation}\nW_\u03b8(G) = asymp.lim. [\\sqrt{n}(\u03b8_G \u2013 \u03b8_0)]\n\\end{equation}"}, {"title": "A.1 Setting the stage", "content": "In whatever follows", "quantity\n\\begin{equation}\nS_{n,G}": "frac{1}{\\sqrt{n}}\\sum_{i=1}^n X_{g,i} (T(Y_{g,i}) \u2013 \u2207A(X\u03b8_0)), \\sqrt{n}(\u03b8_G \u2013 \u03b8_0))^G_{g=1}\n\\end{equation}\nunder Pref.\nUsing Assumption 3 that\n\\begin{equation}\n\\sqrt{n}(\u03b8_G \u2013 \u03b8_0) = \\frac{1}{G \\sqrt{n}} \\sum_{g'\u2264g i<n} \u03c8(Z_{g',i}; \u03b8_0) + o_p(1)\n\\end{equation}\nwe conclude by the multivariate CLT"}]}