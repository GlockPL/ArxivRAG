{"title": "SRA-MCTS: Self-driven Reasoning Aurmentation with Monte Carlo Tree Search for Enhanced Code Generation", "authors": ["Bin Xu", "Yiguan Lin", "Yinghao Li", "Yang Gao"], "abstract": "Large language models demonstrate exce\u0440tional performance in simple code generation tasks but still face challenges in tackling complex problems. These challenges may stem from insufficient reasoning and problem decomposition capabilities. To address this issue, we propose a reasoning-augmented data generation process, SRA-MCTS, which guides the model to autonomously generate high-quality intermediate reasoning paths. This creates a positive feedback loop, enabling continuous improvement. Our method operates entirely through the model itself without requiring additional supervision. By synthesizing natural language reasoning paths and translating them into executable code, the approach ensures analytical accuracy and enhances the success rate in solving complex tasks. Experimental results show that, even without additional supervisory signals, our method achieves performance improvements across different model scales, demonstrating the significant potential of self-improvement in small models. Furthermore, the method remains robust when traditional Chain-of-Thought (CoT) approaches exhibit performance degradation, with notable improvements observed in diversity metrics such as pass@10. We encourage further exploration of reasoning processes within training data to enhance the ability of language models to address complex problems.", "sections": [{"title": "1 Introduction", "content": "Large language models(LLMs) perform well on simple and medium-level problems in the field of code generation, but the performance on more complex problems is not satisfactory (Luo et al., 2024; Li et al., 2023). Some approaches enhance the question part of datasets by using GPT3.5 (Ouyang et al., 2022) to refine open-source datasets or summarize open-source code, thereby increasing the depth and breadth of the questions (Luo et al., 2024; Wei et al., 2023). However, merely improving the depth and breadth of the questions in the data may lead to high-quality questions, but without effectively enhancing the answers, it cannot ensure the correctness of the responses.\nSome work enhances the answers in the data by incorporating a natural language intermediary process as part of the response, guiding the model to generate results in the correct direction. Some use GPT3.5 to generate natural language solutions in a step-by-step Chain-of-Thought (CoT) format (Wang et al., 2024b). Some simply introduce guiding ideas to steer the process (Li et al., 2023). The previous work aims to guide the large model's reasoning process toward generating correct results through natural language as an intermediary. However, the inherent uncertainty in the model's generation process means that errors may still arise during the formulation of these natural language ideas.\nThe experiments conducted by ScaleAI provide concrete experimental validation for previous work on answers that providing LLMs with correct solutions in natural language as a part of the answer, even if incomplete (just 10-20 tokens), can substantially boost the performance on benchmarks (Wang et al., 2024a). In contrast, incorrect solutions result in a decline in solve rate. This demonstrates that providing solutions to large models can guide and inspire their reasoning process, and the correctness of the solution directly impacts the accuracy of the final result.\nWe enhance the natural language in the answer section of the data and reduce the likelihood of errors in the intermediate process. We mitigate the potential for errors in LLMs by generating multiple solutions and selecting the correct process, with diversity playing a key role. Borrowing an idea from ReST (Zhang et al., 2024), our SRA-MCTS approach leverages the diversity inherent in Monte Carlo Tree Search (MCTS) (Coulom, 2006). As in Figure 2, each node at a given layer represents a step in the solution process, our approach emphasizes the process more than CoT. For a branch, if a child node is deemed incorrect, its selection probability is significantly reduced. We not only adjust the temperature coefficient for generation but also allow the model to see the previous response to generate the next one, avoiding repetitive answers. Additionally, the reward score evaluated by the large model is used to compute the UCB1 formula (Auer et al., 2002), guiding the selection of the best response while maintaining the potential for exploration of other branches. As shown in Figure 1, using SRA-MCTS-generated data for fine-tuning results in a model that outperforms both the official version and the CoT fine-tuned model, achieving the best overall performance.\nOur contributions are as follows:\n\u2022 We introduce SRA-MCTS, a plug-and-play data generation method for reasoning augmentation that is simple, effective, and enhances diversity while promoting autonomous thinking in LLMs.\n\u2022 We propose a pipeline for the code domain, where problems are processed by SRA-MCTS to generate diverse natural language plans, followed by the LLM converting these plans into code. The generated data is used for model fine-tuning, bringing the model's output closer to human-like thinking and enabling it to learn the reasoning process.\n\u2022 Our method enables self-improvement in small models, for example, gemma2-2b, with experiments showing that the performance of self-improved small models surpasses that of small models trained with distilled data from a 70B model."}, {"title": "2 Related Work", "content": "Data distillation is an effective method for obtaining high-quality data, widely applied in the code generation field for question and answer distillation. For question distillation, CodeAlpaca (Chaudhary, 2023) uses self-instruct by modifying Stanford Alpaca's prompts and seed tasks (Taori et al., 2023), generating 20K instruction-following data points from GPT. Evol-Instruct (Luo et al., 2024) adds heuristic rules like reasoning difficulty using GPT to increase question's depth and diversity of CodeAlpaca. OSS-Instruct (Wei et al., 2023) leverages widely available open-source code snippets, designing prompts that enable GPT to extract corresponding questions from a few lines of code, further aligning the questions with real-world distributions. CodeOcean (Yu et al., 2024) utilizes an LLM-based generator-discriminator approach, using GPT-3.5 to distill questions from open-source code snippets and GPT-4 to filter the outputs, yielding high-quality and diverse data.\nFrom the perspective of answers, Phi-1 (Gunasekar et al., 2023) distills large amounts of textbook content from GPT-3.5 to enhance the problem-solving process, achieving strong performance with smaller models. DolphCoder (Wang et al., 2024b) generates step-by-step natural language CoT answers through multiple rounds of GPT generation, simplifying the learning process for large models. OpenAI 01 (OpenAI) uses CoT to simulate deep human-like thinking when solving problems. Previous methods typically generate responses either as direct answers or through Chain-of-Thought (CoT). Direct answers have proven ineffective for complex problems (Wei et al., 2022), while CoT suffers from irreversible errors in intermediate steps. Although step-by-step execution of CoT and adjusting temperature can alleviate some errors (Wang et al., 2024b), its efficiency still lags behind the diversity offered by a tree structure. The CoT-SC (Wang et al., 2023) method generates multiple answers and selects the most frequent one to mitigate errors, but the lack of diversity results in repetitive outputs, weakening error mitigation. Our SRA-MCTS approach not only generates step-by-step reasoning using a tree structure but also enables error"}, {"title": "3 Method", "content": "We propose a pipeline that leverages the model itself to enhance training data and improve performance, consisting of three stages: SRA-MCTS for plan generation, plan-to-code transformation, and training. By providing natural language descriptions for code, we enhance the model's performance in code generation, as shown in the Figure 3. For a set of problems, we first use the LLM with SRA-MCTS to generate natural language, step-by-step solutions for each problem (excluding code, except for necessary formulas). The LLM then generates specific code based on the problem and the generated solutions. Finally, we combine the problem, the natural language solution, and the code to create a fine-tuning dataset for training the LLM."}, {"title": "3.1 SRA-MCTS", "content": "We propose a Self-guided MCTS-based data generation method for Reasoning Augmentation, called SRA-MCTS. It consists of four phases: Selection, Expansion, Evaluation & Reflection, and Backpropagation. These phases are repeated multiple times, with each repetition representing one iteration that generates a specific reasoning step. The LLM plays a key role in the middle two phases, while the first and last phases are guided by formulas or rules. By introducing sufficient diversity through a tree structure, LLM is able to fully stimulate its reasoning capabilities using the MCTS method."}, {"title": "3.1.1 Selection", "content": "First, we need to select a leaf node from the tree structure as the context for the LLM's reasoning. Starting from the root node, we recursively select the node with the highest UCB1 (Auer et al., 2002) value at each branch (randomly if values are tied) until we reach a leaf node. It is important to note that, during the first iteration, the root node, which also serves as a leaf node containing the problem, is selected by default. The chosen node will be used in the next step for the LLM's plan generation. The selection criterion is based on the traditional UCB1 formula.\n$\\text{UCB1}_i = r_i + c \\cdot \\sqrt{\\frac{2 \\cdot \\log N}{N_i}}$\nwhere $r_i$ is the reward score of the current plan at this node (generated by the LLM during evaluation), c is the exploration constant, set to 0.5, $n_i$ is the number of visits to this node, N is the number of visits to its parent node. We recursively select the node with the highest UCB1 value (randomly in case of ties) until reaching a leaf node. The selected node will then be used for the next step of plan generation by the LLM."}, {"title": "3.1.2 Expansion", "content": "In the previous phase, the selected leaf node $N_d$ will be expanded by the LLM to generate a new node representing a new plan. The new node's state is formed by concatenating the $N_d$'s State and Action:\n$S_{d+1} = \\text{concatenate}(S_d, A_d)$\nwhere d is the depth of the node in the current tree, i represents the i-th branch. This concatenated state $S_{d+1}$ becomes the context for the LLM's reasoning, guiding the model by showing the previous generation and adjusting the temperature parameter to control the model's diversity in its generation. The process is described by the following formula:\n$A_{d+1}=\\begin{cases}\\text{LLM}(S_{d+1}; R), & i=1 \\\\ \\text{LLM}(S_{d+1}+A^i_{d+1}+ ... + A^2_{d+1}; A^1_{d+1}; R), & i>1\\end{cases}$\nIn this step, $A^i_{d+1}$ refers to the output generated by the LLM in the previous branch, serving as a part of prompt to guide the model to generate non-redundant ideas. R represents the Reflection phase mentioned in the next phase, used to prompt the model for its reasoning on the current generation. To manage computational costs, we set the number of expanded branches to 3. To avoid duplicate outputs, we perform a redundancy check. If any output is found to be repetitive, the generation process will be restarted. At this stage, code generation is not involved only the natural language reasoning process is generated."}, {"title": "3.1.3 Evaluation and Reflection", "content": "Evaluation As shown in Figure 4, We use the LLM to assign reward scores to all newly gener-ated nodes from several perspectives: single-step correctness, overall solution coherence, solution completeness, and overall correctness. If erroneous steps occur in the intermediate stages, the node will receive a lower reward score, reducing the likelihood of that branch being selected. This helps mitigate the loss of robustness commonly seen in chain-based reasoning due to mistakes in intermediate steps. The reward scoring is as follows:\n$r_{d+1} = \\text{LLM}(S_{d+1} + A_{d+1})$\nReflection During the Expansion phase, although the LLM generates new nodes, directly using them as input for the next step often results in unstable performance. To address this, we employ a reflection mechanism (Zhang et al., 2024). The model first generates a brief reflection based on the previous context, suggesting the approach for the next step. The LLM then generates the specific content for the next step based on this guidance. The reflection can also be used to assess whether the generated step can solve the problem. If the previous steps are deemed sufficient to solve the problem, the model outputs an <end> tag to indicate that the data generation process has concluded. The reflection value R = LLM($S_{d+1} + A_{d+1}$) is determined as follows:\n$R = \\begin{cases} <\\text{end}>, & \\text{problem solved} \\\\ \\text{short thought for next step}, & \\text{problem unsolved} \\end{cases}$"}, {"title": "3.1.4 Backpropagation", "content": "Backpropagation recursively transmits reward increments from the leaf node to its parent nodes, allowing for more accurate selection in the next iteration. Early in the process, a plan may not receive a high score, but after a few iterations, it may become more reliable and thus should have an increased probability of being selected. This is achieved by propagating the increments from child nodes upward. The formula is as follows:\n$\\Gamma_{\\text{increment}} = \\frac{\\Sigma_{\\text{branch}} \\text{Visits}_{\\text{children}} \\cdot r_{\\text{children}}}{\\Sigma_{\\text{branch}} \\text{Visits}_{\\text{children}}}$\n$\\Gamma_{\\text{parent}} = \\alpha \\cdot \\Gamma_{\\text{parent}} + (1 - \\alpha) \\cdot \\Gamma_{\\text{increment}}$\nwhere Visits represents the number of times a node is visited, used to balance the contribution of frequently visited nodes to the increment. $\\Gamma_{\\text{increment}}$ denotes the increment in the parent node's reward provided by the child node, and \u03b1 is used to balance the original reward of the parent node and the increment from the child node. We set \u03b1 to 0.7 manually.\nRepeat the four phases above until the  tag is output, or the iteration limit (either iteration count or time) is reached."}, {"title": "3.2 Plan to Code", "content": "Through SRA-MCTS, we obtain a batch of natural language plans. Now, we have the LLM act as a function implementer to strictly convert these plans into code, i.e., Code = LLM(Problem + Plan). Below is the prompt for implementing the conversion:"}, {"title": "3.3 Training", "content": "We have collected the Question, Plan, and Code triples, with the Question as input and the latter two as output, forming the training dataset. Since our method emphasizes self-driven generation, unless stated otherwise, the datasets used for model training are self-generated. For training, we use LLaMA-Factory (Zheng et al., 2024) for LoRA (Hu et al., 2022) fine-tuning."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Baseline", "content": "We select gemma-2-2b-it (Rivi\u00e8re et al., 2024), Meta-Llama-3.1-8B-Instruct (Dubey et al., 2024), and Qwen2.5-14B-Instruct (Team, 2024) as our baseline. The selection of these models is based on their strong instruction-following capabilities and their extensive pre-training on large-scale downstream code data. This makes them well-suited for leveraging reasoning-augmented approaches to activate autonomous problem-solving abilities in programming tasks."}, {"title": "4.2 Data", "content": "The challenge in collecting suitable problems lies in ensuring that the problems are not overly simple, but rather have enough depth to allow for thorough analysis and exploration. Therefore, we select the LeetCode dataset (Greengerong, 2023), focusing on medium and hard problems. We retain only the problem descriptions, discarding the solutions. To prevent overlap between the training and evaluation datasets, we perform decontamination on the dataset. This involved conducting a 10-gram-level duplication check for each problem, with a threshold set at 0.5. If the duplication rate exceeded this threshold, the corresponding problem was removed from the dataset. After decontamination, we obtain approximately 2,000 data samples."}, {"title": "4.3 Benchmark", "content": "To evaluate the effectiveness of our approach, we use commonly adopted benchmarks in the code generation field: Human-Eval (Chen et al., 2021) and MBPP (Austin et al., 2021). Additionally, for a more accurate assessment, we include Human-Eval+, which contains 80 times the test cases of the original Human-Eval, and MBPP+ (Liu et al., 2023), which includes 35 times the test cases of the original MBPP. The Human-Eval benchmarks involve code completion tasks, where the model is provided with the function name and comments and is tasked with completing the entire function. In contrast, MBPP benchmarks provide a brief functional requirement described in natural language, and the model must generate the function from scratch.\nIn code generation, Pass@k is a common metric that measures the probability of generating a correct solution in at least one of k attempts. It reflects the model's success rate across multiple attempts, which is especially useful given the uncertainty in the generation process. The Pass@k metric is defined as:\n$\\text{pass@k} = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{k} \\sum_{j=1}^k (1-\\prod_{j=1}^k (1-p_{i,j}))$\nwhere N is the total number of test cases, $p_{i,j}$ represents the probability of success for the j-th generation in the i-th test case, $\\prod_{j=1}^k (1 \u2013 p_{i,j})$ represents the probability that all k generations for the i-th test case fail."}, {"title": "4.4 Experiment Setup", "content": "Our experiments operate on the premise that training data is self-generated by the model. To test the validity of this approach, we use Meta-Llama-3-70B-Instruct (Dubey et al., 2024) as an external model to generate data, which is then used to train smaller models. We compare the performance of self-generated data with that of data distilled from a larger model to determine if the self-generation approach for small models can approximate the effectiveness of large-model distillation.\nTo compare the performance of our SRA-MCTS method with CoT, a popular approach for reasoning enhancement, we evaluate it on the benchmarks mentioned earlier. We compare the results with original models as well as those trained explicitly using CoT. Except for the original model, the training data is self-generated. In the CoT group, the data is generated by prompting the model to create CoT-formatted data in a single generation. In contrast, our method directly uses SRA-MCTS to generate the data."}, {"title": "5 Results and Analysis", "content": "SRA-MCTS enhances small models' reasoning, allowing self-generated data to outperform 70B-distilled data in improving performance. The results in Figure 5 indicate that self-synthesized data outperformed data synthesized by Meta-Llama-3-70B-Instruct across all model sizes and nearly all benchmarks. SRA-MCTS demonstrates significant improvements on the Human-Eval and Human-Eval+ benchmarks. In the 2B category, the SRA-MCTS method achieves an average increase of 2 points compared to data synthesized by the 70B model. The 8B category shows similar gains of over 2 points. Only at the 14B level does the data generated by the 70B model begin to show slight advantages on about half of the benchmarks.\nResearcher's exploration of small model potential is still limited. Although 70B models are near the data synthesis upper limit for most researchers, experiments show that self-synthesized data from small models not only costs less but also rivals or surpasses that of LLMs. This suggests that as model and data scale reach their limits, researchers should focus on unlocking small model potential and improving data quality.\nSRA-MCTS surpasses CoT in average performance and excels in pass@10, highlighting diversity. In Table 1, we set control groups by model size to compare SRA-MCTS with CoT and the official Instruct model. In the 2B group, our method generally outperforms the Instruct version but lags behind CoT on Human-Eval benchmarks until MBPP+, where it gains an advantage. This trend continues in the 8B group, where our method also gains an advantage on MBPP. Unlike the 2B group, however, the Instruct version performs well, especially on pass@1 and pass@10 for Human-Eval and Human-Eval+. Our method excels on pass@10 for Human-Eval+, MBPP, and MBPP+. At the 14B scale, our method outperforms the CoT approach across almost all benchmarks, except for MBPP+. Additionally, on Human-Eval+, it trails the Instruct model by less than 2 points. Calculating the average performance increments across all benchmarks, we observe that our method consistently improves performance at every model scale."}, {"title": "6 Ablation Study", "content": "To investigate the role of natural language in reasoning augmentation, we created a control group with and without the inclusion of natural language Plan in the training data. This allowed us to explore the impact of natural language on reasoning enhancement. Using regular expressions, we extract the code generated by SRA-MCTS. One dataset contained only the code results, while the other included both code and natural language explanations. We fine-tune the same model using these two datasets separately, resulting in two distinct fine-tuned models.\nWe believe natural language not only eases LLMs' understanding of code but also encourages proactive analysis and knowledge expression. ScaleAI has experimentally verified that providing both correct and incorrect natural language plans for the same problem during inference leads to a significant divergence in problem-solving outcomes (Wang et al., 2024a). This demonstrates that natural language plans greatly influence the model's reasoning path. Our goal is to enable models to generate correct plans and autonomously guide their reasoning paths to solve problems effectively through data generation and training. As shown in Figure 5, models trained without natural language data demonstrate a noticeable decline in performance across all three model size groups. On Human-Eval-related benchmarks, this difference is less pronounced, with only a 1-2 point variation. However, on MBPP-related benchmarks, the performance gap is significantly more evident. In particular, the 2B model shows a nearly 7-point drop on the MBPP+ benchmark and a 1-2 point average drop on other benchmarks. For the 8B model, the performance gap on MBPP benchmarks averages around 7 points. The largest performance gap appears in the 14B model on MBPP+ for pass@10, where a 13-point difference clearly demonstrates the significant role of natural language in guiding and stimulating the model's reasoning process. We attribute this difference to the nature of MBPP, where the benchmark data is largely instruction-based, containing clear functional requirements. This format encourages the model to utilize its reasoning capabilities to enhance both depth and breadth of understanding. In contrast, on the continuation-style Human-Eval benchmark, the model's reasoning abilities appear less utilized and are somewhat constrained."}, {"title": "7 Limitation and Future Work", "content": "Limited Self-Evaluation Capabilities of Small Models During the data generation process, the model is required to evaluate the generated content. However, small models often struggle with this task, failing to follow instructions and generating responses to the posed questions instead of evaluations. Even when scores are generated, they may be interspersed with irrelevant content, necessitating the use of manually crafted rules for extraction.\nTo address the evaluation challenges posed by small models, approaches such as the popular LLM-as-a-judge framework or the Process Reward Model (PRM) can be employed. These methods involve fine-tuning the evaluation model to standardize its output direction and format.\nDependence on Manual Hyperparameter Tuning in MCTS In the backpropagation of reward scores, the updates are propagated step by step to parent nodes based on manually defined proportions. During branch selection, manually set constants influence the exploration of lower-scoring branches. These constants need to be manually controlled within a reasonable range; significant deviations from this range may result in performance degradation.\nMCTS has been proposed for many years, and numerous improvements to its algorithmic structure have been made during this time. Addressing the dependency on hyperparameters is undoubtedly a critical aspect of these advancements. In the future, we aim to modify these formulas to reduce reliance on manually defined parameters.\nInefficiencies in Time Consumption In the data generation process, large models require multiple inferences for both generation and evaluation. To ensure non-redundant outputs, the model must reference previously generated content, making the entire process sequential and further impacting time efficiency.\nInference acceleration frameworks, such as VLLM (Kwon et al., 2023), can be employed to speed up the inference process. To address the issue of sequential processing, model diversity can be enhanced to avoid generating duplicate content, allowing the workflow to revert to a parallel process."}, {"title": "8 Conclusion", "content": "We introduce SRA-MCTS, a plug-and-play data generation method for reasoning augmentation that is simple, effective, and enhances diversity while promoting autonomous thinking in LLMs. This approach boosts performance across model scales, surpassing the official Instruct version and the traditional CoT method at the 8B and 14B levels.\nOur method enables self-improvement in small models, with experiments showing that the performance of self-improved small models surpasses that of small models trained with distilled data from a 70B model."}]}