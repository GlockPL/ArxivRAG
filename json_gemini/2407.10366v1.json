{"title": "Accessing Vision Foundation Models at ImageNet-level Costs", "authors": ["Yitian Zhang", "Xu Ma", "Yue Bai", "Huan Wang", "Yun Fu"], "abstract": "Vision foundation models are renowned for their generalization ability due to massive training data. Nevertheless, they demand tremendous training resources, and the training data is often inaccessible, e.g., CLIP, DINOv2, posing great challenges to developing derivatives that could advance research in this field. In this work, we offer a very simple and general solution, named Proteus, to distill foundation models into smaller equivalents on ImageNet-1K without access to the original training data. Specifically, we remove the designs from conventional knowledge distillation settings that result in dataset bias and present three levels of training objectives, i.e., token, patch, and feature, to maximize the efficacy of knowledge transfer. In this manner, Proteus is trained at ImageNet-level costs with surprising ability, facilitating the accessibility of training foundation models for the broader research community. Leveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the performance of the Oracle method DINOv2-L/14 (142M training data) across 15 benchmarks and outperforms other vision foundation models including CLIP-L/14 (400M), OpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M). Code is available at here.", "sections": [{"title": "Introduction", "content": "By leveraging extensive pre-training on diverse and massive datasets, vision foundation models [43, 54, 46, 11] represent a significant advancement in the field of computer vision, aiming to learn comprehensive and versatile visual features that can generalize well across various downstream tasks, such as classification, segmentation, etc. As a result, vision foundation models are becoming fundamental components in the toolkit of modern computer vision research and development.\nWhile those models have released their weights for public usage, training foundation models remains largely inaccessible for most researchers due to two primary factors: (1) the training data for these foundation models is seldom disclosed. While there have been attempts to reproduce CLIP [46] using alternative datasets [11], replicating the training of foundation models like DINOv2 [43] and SynCLR [54] remains less explored due to private data sources. (2) even if the training data were accessible, training on these enormous datasets necessitates substantial computational resources, which may be beyond the reach of most researchers. ImageNet-1K [13], which has long been the cornerstone for advancements in the supervised learning domain, is now less frequently utilized as a training set in the era of foundation models due to its relatively 'small' scale. In this work, we seek to address the following question: Can the success of vision foundation models be replicated on much smaller datasets, such as ImageNet-1K, without sacrificing their generalization ability?\nIntuitively, leveraging the pre-trained weights of these foundation models is essential to accomplish this task and there are two possible directions: (1) recent advances [38, 66] in Natural Language Processing (NLP) have validated that structure pruning could be a possible answer as it inherits most of the knowledge from the foundation model. However, they either sample subsets from the original"}, {"title": "Methodology", "content": "In this section, we present Proteus, a simple and general framework to access vision foundation models with 'limited' data, i.e., ImageNet-1K. We first introduce our efforts in mitigating the dataset bias of the proxy dataset so that Proteus can effectively transfer the general representation from the pre-trained foundation models by mimicking its behaviors. Then, we present our proxy task which encompasses multiple levels of learning objectives to promise the applications on various tasks."}, {"title": "Proxy Dataset", "content": "While we do not introduce any new dataset to try to reproduce the results of foundation models [11], we focus on leveraging publicly available resources, e.g., ImageNet-1K, to access the vision foun-dation models. This task is challenging because the distribution of the large-scale datasets of those foundation models is usually unknown and it is very likely that there exists distribution shifts between ImageNet-1K and those private datasets. Hence, it is critical to reduce the dataset bias [58] of ImageNet-1K so that our learned representation can be general enough.\nCombating Dataset Bias. Given a pre-trained teacher network $F' (\\cdot)$, a foundation model in our context, conventional knowledge distillation [26, 59] optimizes the student network $F (\\cdot)$ by two loss terms. The first one is Cross-Entropy loss which is calculated on the logits of student network p:\n$\\mathcal{L}_{C E} = -\\sum_{k=1}^K \\hat{y}_k \\log (p_k),$\nwhere $\\hat{y}_k$ is the one-hot label for class k and K denotes the number of total classes. While the second one is KL divergence loss [31] which enforces the prediction of student network p and teacher model p' to be as similar as possible:\n$\\mathcal{L}_{K L} = -\\sum_{k=1}^K p'_k \\log (\\frac{p_k}{p'_k})$\nCombining the two losses uniformly, the student network $F (\\cdot)$ can be updated by:\n$\\mathcal{L} = (1 - \\lambda) \\cdot \\mathcal{L}_{C E} + \\lambda \\cdot \\mathcal{L}_{K L},$\nwhere $\\lambda$ is a hyperparameter introduced to balance the two terms. Empirically, the default setting works well in the supervised learning setting as it delivers good performance on ImageNet-1K."}, {"title": "Proxy Task", "content": "Foundation models such as DINOv2 [43] are designed to learn general-purpose visual features, excelling not only in high-level classification tasks but also in dense prediction tasks like semantic segmentation. To maximize the knowledge transfer power and promise the application on various tasks, we conduct distillation across three different levels of training objectives, i.e., token-level, patch-level, and feature-level, to transfer the fruitful knowledge by emulating the teacher's behaviors.\nToken-level Objective. To learn the discriminative features for high-level understanding, we min-imize the L2 distance to align the classification token between the teacher and student model. Specifically, the student classification token $v_{cls}$ will first be mapped to a higher dimension to match the channel number of the teacher's classification token $v'_{cls}$:\n$\\hat{v}_{c l s} = \\mathcal{P}_{c l s} (v_{c l s}),$\nwhere $\\mathcal{P}_{c l s} (\\cdot)$ denotes the projection head for the classification token and we simply adopt the combination of Layer Normalization [5] and a linear layer. Then, we enforce the model to mimic the teacher's prediction $v'_{cls}$ via Mean Squared Error (MSE) loss:\n$\\mathcal{L}_{t o k e n} = ||\\hat{v}_{c l s} - v'_{c l s} ||^2$.\nFeature-level Objective. Although the token-level learning objective alone serves as a good proxy task to obtain the discriminative visual features, it cannot guarantee decent performance on dense prediction tasks like semantic segmentation or depth estimation (see analysis of Tab. 9). To mitigate this issue, we perform feature-level knowledge transfer in a similar manner:\n$\\mathcal{L}_{f e a t} = ||\\hat{v} - v'||^2,$\nwhere $\\hat{v}$ is obtained by the projection head $\\mathcal{P}_{f e a t} (\\cdot)$ for feature distillation.\nPatch-level Objective. To further uncover the hidden knowledge from the foundation model, we construct a patch-level learning objective inspired by the idea of masked image modeling [7, 20, 64, 74, 6]. Given an image x, an additional view $X_{mask}$ will be generated where the patches are randomly masked and it will be sent to the student network to create the intermediate feature $v_{mask}$. Following the previous procedures, we enforce the student to recover the masked regions by:\n$\\mathcal{L}_{p a t c h} = ||\\hat{v}_{p a t c h} - v'_{p a t c h} ||^2,$\nwhere the patch tokens of the foundation model $v'_{patch}$ is produced by the unmasked view x and $\\hat{v}_{p a t c h}$ is derived from the projection head $\\mathcal{P}_{patch} (\\cdot)$.\nCombining the three learning objectives, we construct the proxy task of Proteus as follows:\n$\\mathcal{L} = \\Lambda_{t o k e n} \\cdot \\mathcal{L}_{t o k e n} + \\Lambda_{f e a t} \\cdot \\mathcal{L}_{f e a t} + \\Lambda_{p a t c h} \\cdot \\mathcal{L}_{p a t c h},$\nwhere $\\Lambda_{t o k e n}, \\Lambda_{f e a t}, \\Lambda_{p a t c h}$ are introduced hyperparameters to balance the three terms and we simply set them to 1 in our implementations without additional fine-tuning."}, {"title": "Empirical Validation", "content": "In this part, we conduct pre-training on ImageNet-1K [13] and validate our methods under different setups. Distilling from DINOv2 [43], we first evaluate our method on the object recognition task and perform linear probing on ImageNet-1K and 12 fine-grained datasets. Further, we validate our approach on dense prediction tasks, including Semantic Segmentation and Depth Estimation. Moreover, we examine the scaling behavior of our method by switching backbones with different model capacities. In the end, we test the generalization ability of our method by distilling SynCLR [54] and CLIP [46], which are trained with quite different learning objectives on private datasets."}, {"title": "Experimental Setup", "content": "We conduct pre-training on the training set of ImageNet-1K [13], comprising approximately 1.2 million images distributed across 1,000 categories. We follow the training recipe of DeiT [59] for 300-epoch on ImageNet-1K training and conduct the experiments on 8 A100 GPUs with a total batch size of 1024, except the ViT-L experiment with a batch size of 256 due to GPU memory constraints. In default, Proteus is distilled from the foundation model at a larger scale with the same patch size. Following the setups in DINOv2 [43] and SynCLR [54], we evaluate our approach on classification tasks (ImageNet-1K and 12 fine-grained classification datasets) and dense prediction tasks (semantic segmentation and depth estimation)."}, {"title": "Accessing DINOv2", "content": ""}, {"title": "Target Model: ViT-S", "content": "In this part, we choose randomly initialized ViT-S [14] with the patch size of 14 as the backbone following DINOv2 [43] and utilize pre-trained DINOv2-B as the teacher network. Note that official DINOv2-S/B/L models are obtained by distilling from DINOv2-g which has stronger performance, it increases the training time significantly due to a very large teacher. Since the CLIP-series models [46, 11] do not offer ViT-S level options, we compare our method with ViT-B/32 level models, despite the latter having a larger number of parameters.\nImageNet Linear Evaluation. Following the design from DINOv2 [43], we concatenate features from multiple layers for linear probing on ImageNet-1K and rerun all the baseline methods in this setup. Shown in Tab. 1, Proteus outperforms all the CLIP [46, 11] models on ImageNet-1K significantly and even surpasses the official DINOv2 model. The possible explanation is that we conduct pre-training on ImageNet-1K training set which shares a similar distribution with its validation set. However, it is noteworthy that Proteus also surpasses other self-supervised approaches pre-trained on ImageNet-1K (e.g., DINO [9] reaches 78.2% with ViT-B/16, and iBOT [74] attains 81.0% with ViT-L/16). Additionally, Proteus outperforms supervised learning methods, such as DeiT [59], which achieves 81.2% with ViT-S/16 when using knowledge distillation."}, {"title": "Scaling Up", "content": "In this part, we scale up our experiments by utilizing DINOv2-L/g [43] as the teachers and compress them to ViT-B/L [14] respectively to examine the scalability of Proteus. We compare our method with DINOv2 [43], CLIP [46], OpenCLIP [11], and SynCLR [54], which is trained on 600M synthetic data. The scaling behavior of Proteus is validated in both high-level understanding tasks (Tab. 4) and dense prediction tasks (Tab. 5 and Tab. 6).\nClassification. For ImageNet linear evaluation, Proteus-B surpasses DINOv2-B and exhibits remark-able advantages over all the SynCLR and CLIP variants, including the ViT-Large models. When"}, {"title": "Comparison with Distillation in Supervised Learning", "content": "As Proteus only requires ImageNet-1K for pre-training, we extensively compare our method with the supervised training scheme DeiT [59] which is equipped with knowledge distillation from multiple perspectives. In terms of ImageNet accuracy, the advantage of Proteus increases when we scale up the model and this phenomenon correlates with the trend in robustness evaluation on four ImageNet variants. We then validate the two approaches on 12 fine-grained classification datasets to test their generalization ability and Proteus consistently outperforms DeiT at different scales as we effectively transfer the fruitful knowledge from the foundation model DINOv2 [43]. Moreover, Proteus exhibits remarkable advantages in dense prediction tasks and the improvement also increases when we scale up the models. In conclusion, Proteus constantly outperforms the conventional supervised learning setting across various dimensions, offering a novel training scheme enhanced by foundation models."}, {"title": "Accessing SynCLR and CLIP", "content": "We test the generalization ability of Proteus by leveraging other foundation models SynCLR [54] and CLIP [46] as the teacher networks. SynCLR is trained with the contrastive learning objective on the undisclosed 600M synthetic dataset, while CLIP is obtained by aligning images and corresponding text descriptions through contrastive learning on the private dataset WIT-400M. We utilize SynCLR-L/14 and CLIP-L/14 as the teachers, and we remove the patch and feature learning objectives for CLIP training following the original design. Shown in Fig. 4, Proteus exceeds SynCLR at ImageNet linear evaluation (81.4% versus 80.5%) and falls behind on 12 fine-grained datasets (87.4% versus 88.8%). A similar phenomenon is observed in Fig. 5, where Proteus outperforms CLIP on ImageNet (81.2% versus 78.7%) and performs similarly at fine-grained classification (85.7% versus 85.8%). Beyond the accuracy comparisons, we notice that the distribution of Proteus's results is very similar"}, {"title": "Analysis", "content": "In this section, we conduct analyses on the proxy task to verify the effectiveness of our design. We first examine the effectiveness of removing the designs that lead to dataset bias. Then, we conduct ablation to validate our proposed three different levels of learning objectives."}, {"title": "Proxy Task", "content": "To study the importance of mitigating dataset bias, we start with the conventional knowledge distillation setting of ViT [59], which leverages hard logits distillation and combines the Cross-Entropy loss with the KL-divergence loss [31]. Note that we do not introduce the extra distillation token like DeiT and keep the same structure as the teacher model. Shown in Tab. 8, the default distillation setting of DeiT delivers very good ImageNet accuracy. However, its generalization ability (fine-grained classification) remains at the same level as distillation from ImageNet pre-trained teacher (78.7% versus 77.8%), indicating that it does not fully utilize the fruitful knowledge of the foundation teacher model. We then empirically change hard logits to soft logits and remove the CE loss as the one-hot label will introduce the dataset bias. The results support our hypothesis as soft logits distillation without CE loss delivers the highest accuracy on fine-grained classification among the four choices. Further, we revise the design to distilling before the FC layer as it implicitly introduces the dataset bias, which brings significant improvement to generalizability."}, {"title": "Proxy Dataset", "content": "While we have already demonstrated that ImageNet-1K serves as a good proxy dataset because we successfully compress foundation models on it, we further study the importance of the proxy dataset by introducing multiple variants: (1) ImageNet-Merge: Following the idea of DINOv2 [43] and SynCLR [54], we simply concatenate all the training sets that we utilized for validation, including ImageNet-1K, 12 fine-grained datasets, ADE20K and NYU-Depth V2. It results in a training set of 1.4M images. (2) ImageNet-Single: We build a 1.2M training set where the images are generated from a single large image by performing extensive data augmentations [4, 3]. (3) ImageNet-sub-data: we randomly sample a portion of data at each class with the ratio of 20% and 50%. (4) ImageNet-sub-class: we randomly sample a portion of classes from the total 1000 classes with the ratio of 20% and 50%. Please refer to Sec. A.11 for more details.\nDataset Diversity. We first conduct ablation on ImageNet-Merge and ImageNet-Single to study the property of dataset diversity. Shown in Tab. 10, ImageNet-Merge brings almost 1% average improvement on fine-grained classification with merely 0.2M additional data, which suggests the importance of a diverse dataset. We further consider the extreme case where all the source information comes from a single image and it surprisingly delivers very decent performance. Specifically, 6 out of 12 datasets suffer from performance drop over 10% while the remaining ones exhibit acceptable results considering that most of the classes in those fine-grained datasets are missing in this single image. This demonstrates that Proteus is robust even under the extreme scenario."}, {"title": "Related Work", "content": "Supervised Learning approaches [22, 51, 30] used to dominate the research community of computer vision. These methods provide valuable experience in model architecture design, which benefit the research in different domains [71, 72] and lay a solid foundation for the introduction of the transformer architecture [60, 14]. Nevertheless, the reliance on extensively annotated datasets [13, 34, 17] inherent in supervised learning models presents significant limitations. Additionally, supervised learning can struggle with generalization outside of the training dataset [27, 48], especially in cases where the distribution of the training data does not accurately reflect real-world scenarios.\nSelf-supervised Learning has emerged as a powerful paradigm for leveraging unlabeled data, significantly reducing the dependency on extensive labeled datasets. Contrastive learning methods [10, 21, 42, 56, 18] foster invariance to different transformations of the same image while segregating the representations of distinct images [63]. On the other hand, masked image modeling [7, 20, 64, 74, 6] involves either pixel reconstruction [20] or local feature regeneration [6], often yielding impressive results when fine-tuning for dense prediction tasks [20]. In the landscape of vision foundation models, strategies such as DINOv2 [43] and CLIP [46] have set new benchmarks for versatility and robustness. However, both approaches conduct large-scale training on private gigantic datasets, presenting substantial challenges for training foundation models in a similar vein.\nModel Compression\ntechniques like Knowledge Distillation [26, 44, 2, 55, 59] and Pruning [19, 32, 40, 36] have been extensively studied for task-specific model compression. Howeverm task-agnostic model compression, which maintains the generalization ability, has become rather important in the era of foundation models [46, 43, 54]. Nevertheless, there remain two critical challenges: (1) The training data of those foundation models is rarely disclosed [46, 43, 54]. (2) Training those foundation models requires significantly large training resources and it is a common protocol to utilize the original dataset to perform task-agnostic model compression [65, 43]. To address these issues, we consider the setting of data-free model compression with limited data."}, {"title": "Discussion", "content": "Limitation. Due to computational constraints, we only validate our model on ImageNet-1K, and its efficacy at a larger scale dataset remains unverified. Besides, Proteus has to keep the same patch size as the teacher model because of the introduced patch and feature-level learning objectives.\nBroader Impact. Discussed in Sec. 3.2.3, Proteus comprehensively surpasses the supervised learning method across all metrics, demonstrating its potential to serve as a training paradigm for those supervised learning approaches. Besides, our work supports the research in model compression so that foundation models can be compressed at a much smaller cost. Further, we have shown in Sec. 4.2 that it is possible to access foundation models at even smaller datasets compared to ImageNet-1K, which may be a promising avenue for future exploration. Moreover, while our work mainly focuses on the pure vision foundation models with image modality, we hope our work can incentivize the exploration of this idea in Large Language Models (LLMs) and Large Multimodal Models (LMMs) to facilitate the research under the era of foundation models.\nConclusion. In this paper, we propose Proteus, a simple and general distillation framework to transfer the general-purpose visual representations from foundation models into the target network with 'limited' data (ImageNet-1K). Specifically, we remove the designs from conventional knowledge distillation that could lead to dataset bias and present three levels of training objectives, i.e., token, patch, and features, to promise the application across various tasks. We conduct extensive experiments to verify the scalability and generalization ability of Proteus and provide comprehensive analysis."}, {"title": "Implementation Details", "content": "We strictly follow the DeiT [59] training protocol on ImageNet-1K [13] except that we do not enable Mixup [70]. All models are training for 300 epochs with a batch size of 1024 on 8 A100 GPUs, except the ViT-L experiment with a batch size of 256 due to GPU memory constraints. We adopt the masking strategy in iBOT [74] for the patch-level learning objective."}, {"title": "ImageNet Linear Probing", "content": "For all ImageNet linear probing results, we follow the linear probing protocol outlined in DINOv2 [43]. Specifically, the linear layer is trained using the SGD optimizer for 25,020 iterations with a batch size of 512. We employ random-resized-crop data augmentation and conduct a grid search over the hyperparameters as defined in DINOv2. We report the best accuracy on the validation set in accordance with prior works."}, {"title": "Fine-grained linear classification", "content": "Following SynCLR [54], we train a regularized multinomial logistic regression model on the output classification token. During training and testing, no data augmentation is applied; images will be resized to 224 along the shorter side, followed by a center crop of 224x224. The training objective is minimized using L-BFGS with L2-regularization and the L2-regularization constant is chosen on the validation set from 45 logarithmically spaced values between 10\u20136 and 103. Unlike DINOv2 [43] and SynCLR, We reduce the maximum number of L-BFGS iterations from 1000 to 500 for faster evaluation."}, {"title": "Semantic Segmentation", "content": "We conduct experiments on ADE20K dataset [73] using the single-scale setup. We adopt the resolution of 512\u00d7512 for models that are trained with the patch size of 16\u00d716 and 518\u00d7518 for patch size of 14\u00d714. Linear: we follow the setup and hyperparameters in iBOT [74] and use FCN [37] as the linear layer to perform probing on the frozen backbone. Fine-tune: we adhere to the setup and hyperparameters in SynCLR [54] which utilizes UperNet [68] as the task adaptation layer."}, {"title": "Depth Estimation", "content": "We evaluate the methods on the NYU-Depth V2 [50] dataset. Similar with semantic segmentation, we utilize a resolution of 512\u00d7512 for models trained with a patch size of 16\u00d716, and a resolution of 518\u00d7518 for models trained with a patch size of 14\u00d714. We always adopt DPT [47] as the decoder under both evaluation protocols. Fine-tune: we follow to the setup and hyperparameters in the public codebase [33]. Linear: we utilize the same hyperparameters in the Fine-tune protocol except that we enlarge the learning rate from le-4 to 4e-4."}, {"title": "Weight Inheritance", "content": "Existing methods to compress foundation models often utilize the pre-trained weights either by structure pruning [38, 66] or weight inheritance [65, 69]. However, structure pruning requires complex setups and is not general enough to be adapted to arbitrary architectures. Weight Selection [69] proves that initializing smaller models with uniformly selected weights from a pre-trained larger model results in better performance. Besides, TinyCLIP [65] shows simply inheriting the weights from the teacher network, i.e., the foundation model, with a fixed pattern can improve the distillation efficiency. Following the design of Weight Selection, we conduct ablation on DeiT [59] and DINOv2 [43], where the pre-trained weights are learned with supervised learning objective and self-supervised learning objective, respectively. Table 11 demonstrates that weight inheritance is advantageous for DeiT training, whether knowledge distillation is applied or not. Conversely, as shown in Table 12, weight inheritance is less effective for DINOv2 and can even be detrimental when knowledge distillation is used. We conjecture that this phenomenon is caused by the mismatch between the downstream learning objective (supervised learning) and the pre-training learning objective (self-supervised learning), while both the teacher and student of DeiT are trained with the supervised learning objective.\nTo validate this point, we conduct ablation on weight inheritance and Tab. 13 shows that directly inheriting the pre-trained weights results in performance degradation at all classification benchmarks. Therefore, we simply leverage the randomly initialized network as the student to maintain the generalization ability of Proteus, so that it could be applied to different foundation models which are learned with various learning objectives."}, {"title": "Robustness Evaluation", "content": "We compare Proteus with DINOv2 on ImageNet variants for robustness evaluation. We utilize the models that are obtained by linear probing on ImageNet-1K and run inference on those datasets. Shown in Tab. 14, Proteus achieves similar results with DINOv2 on ImageNet-Sketch, ImageNet-R and outperforms it obviously on ImageNet-C. However, we fall behind DINOv2 at ImageNet-A at different scales which may be caused by the lack of training data."}, {"title": "More Diverse Proxy Dataset for CLIP", "content": "While we have shown that Proteus achieves similar average accuracy on fine-grained classification with CLIP [46] when leveraging CLIP-L/14 as the teacher, it is worth noting that the performance at certain datasets, e.g., StanfordCars and FGVC-Aircraft, is obviously worse than CLIP. We conjecture the limitation is caused by the diversity of ImageNet-1K so we reconduct the experiments with ImageNet-Merge. Shown in Tab.15, the accuracy of these datasets increases significantly, even outperforming CLIP on FGVC-Aircraft. This indicates that the performance of Proteus can be easily enhanced by a more diverse dataset."}, {"title": "Dense Prediction for SynCLR", "content": "In the main text, we have compared Proteus with SynCLR [54] on the classification tasks when utilizing it as the teacher network. As SynCLR is also trained with the patch-level objective iBOT [74], it exhibits strong performance on dense prediction tasks. We further fine-tune the pre-trained backbones on semantic segmentation dataset ADE20K [73] and utilize UperNet [68] as the task adaptation layer. Shown in Tab. 16, Proteus lags behind SynCLR only by 0.9% which is acceptable considering the small scale dataset that we use. It further demonstrates the generalization ability of Proteus as SynCLR is trained with a very different learning objective on the large synthetic dataset."}, {"title": "Generalization to out-of-domain Concepts", "content": "Following SynCLR [54], we evaluate the pre-trained models on two additionally fine-grained datasets: GTSRB [52] and Country211 [46]. These two datasets are not in the retrival list or concept list in DINOv2 and SynCLR, therefore considered as out-of-domain concepts. Tab. 17 shows that DINOv2 and SynCLR obtain obviously worse performance compared to CLIP, possibly because its training data included similar concepts. While Proteus achieves very similar results with its counterparts on both datasets by mimicking the teacher's pattern, indicating its strong generalization ability across different foundation models."}, {"title": "Teacher Choice", "content": "In the default setting of Proteus, we choose the pre-trained model at a larger scale to serve as our teacher network and we conduct ablation with teacher at different scales for analysis. Shown in Tab. 18, the average accuracy of Proteus decreases with the increased teacher scales. This may seem counterintuitive as a larger teacher contains more fruitful knowledge and has stronger ability. However, there exists a dimension gap between teacher and student, and a projection head is utilized to align the dimensions between them. Assumably, a larger teacher denotes a lager gap between the dimension and information may be lost during the projection. This facilitates the application of Proteus because smaller teachers will speedup the training process significantly compared with giant teacher models."}, {"title": "Training Efficiency", "content": "We compare with the supervised learning method DeiT [59] in terms of training efficiency as both methods are trained on ImageNet-1K for 300 epochs. We measure the time on the same platform, i.e., 8 A100 GPUs. Shown in Tab. 19, Proteus consumes similar training hours with DeiT as our training objectives are very easy. Moreover, we find the validation after each training epoch actually costs lots of time in DeiT. In contrast, we perform pre-training on ImageNet-1K and only conduct linear probing after the training, which can be finished within one hour."}, {"title": "Projection Head Design", "content": "Here we conduct ablation on the design of the projection head which is used to align the dimension between teacher and student model. Tab. 20 shows that different combination results in similar performance but Layer Normalization (LN) [5] leads to slightly better accuracy in terms of the two metrics. We adopt this design in all our experiments without fine-tuning for better strategy."}, {"title": "Visualization Implementations", "content": "Following the procedure in DINOv2 [43] and SynCLR [54], we perform PCA on the image patches from the same set and colorize them using their first three components. We resize the images to 518x518 to fit the patch size of 14 and the pictures are captured from the Internet."}, {"title": "Composition of Proxy Dataset", "content": "Shown in Tab. 21 and Fig. 8, we provide detailed information about the proxy datasets that we used in Sec. 4.2."}, {"title": "List of Benchmarks for Evaluation", "content": "We show the list of benchmarks used for evaluation in Tab. 22."}]}