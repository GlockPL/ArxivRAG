{"title": "Learning Mean Field Control on Sparse Graphs", "authors": ["Christian Fabian", "Kai Cui", "Heinz Koeppl"], "abstract": "Large agent networks are abundant in applications and nature and pose difficult challenges in the field of multi-agent reinforcement learning (MARL) due to their computational and theoretical complexity. While graphon mean field games and their extensions provide efficient learning algorithms for dense and moderately sparse agent networks, the case of realistic sparser graphs remains largely unsolved. Thus, we propose a novel mean field control model inspired by local weak convergence to include sparse graphs such as power law networks with coefficients above two. Besides a theoretical analysis, we design scalable learning algorithms which apply to the challenging class of graph sequences with finite first moment. We compare our model and algorithms for various examples on synthetic and real world networks with mean field algorithms based on Lp graphons and graphexes. As it turns out, our approach outperforms existing methods in many examples and on various networks due to the special design aiming at an important, but so far hard to solve class of MARL problems.", "sections": [{"title": "1. Introduction", "content": "Despite the rapid developments in the field of multi-agent reinforcement learning (MARL) over the last years, systems with many agents remain hard to solve in general (Canese et al., 2021; Gronauer & Diepold, 2022). Mean field games (MFGs) (Caines et al., 2006; Lasry & Lions, 2007) and mean field control (MFC) (Andersson & Djehiche, 2011; Bensoussan et al., 2013) are a promising way to model large agent problems in a computationally tractable manner and to provide a solid theoretical framework at the same time. While MFGs consider competitive agent populations, the focus of MFC are cooperative scenarios where agents optimize a common goal. The idea of MFC and MFGs is to abstract large, homogeneous crowds of small agents into a single probability distribution, the mean field (MF). While MFC and MFGs have been used in various areas ranging from pedestrian flows (Bagagiolo et al., 2019; Achdou & Lauri\u00e8re, 2020) to finance (Carmona & Delarue, 2018; Carmona & Lauri\u00e8re, 2023) and oil production (Bauso et al., 2016), the assumption of indistinguishable agents is not fulfilled in many applications.\nA particularly important class of MARL problems are those with many connected agents. Initially, these agent networks were modeled by combining the graph theoretical concept of graphons (Lov\u00e1sz, 2012) with MFGs, resulting in graphon MFGS (GMFGs) (Caines & Huang, 2019; 2021; Cui & Koeppl, 2022; Zhang et al., 2024) and graphon MFC (Hu et al., 2023b). Since GMFGs only model often unrealistic dense graphs, subsequently mean field models based on Lp graphons (Borgs et al., 2018b; 2019) and graphexes (Veitch & Roy, 2015; Caron & Fox, 2017; Borgs et al., 2018a) were developed, called LPGMFGs and GXMFGs, respectively (Fabian et al., 2023; 2024). While these models facilitate learning algorithms in moderately sparse networks, they exclude sparser topologies. Formally, (LP)GMFGs and GXMFGs are designed exclusively for graphs with expected average degree going to infinity which, for example, excludes power laws with a coefficient above two.\nThe learning literature contains various approaches to finding optimal behavior in MFGs and MFC, see Lauri\u00e8re et al. (2022a) for an overview. For example, Subramanian et al. (2022) develop a decentralized learning algorithm for MFGs where agents are able to independently learn policies, while Guo et al. (2019; 2023) focus on Q-learning methods for general MFGs. Various MFC learning approaches exist (Ruthotto et al., 2020; Carmona et al., 2023; Gu et al., 2023), but we are aware of only one work by Hu et al. (2023b) which learns policies for MFC on dense networks, but not on sparse ones.\nMany empirical networks of high practical relevance are considerably sparser than the topologies covered by (LP)GMFGs and GXMFGs. Examples of sparse empirical networks which at least to some extent follow power laws with coefficients between two and three include the internet (V\u00e1zquez et al., 2002), coauthorship graphs (Goh et al., 2002) and biological networks (Dorogovtsev & Mendes, 2002). These topologies are particularly challenging to analyze"}, {"title": "2. Locally Weak Converging Graphs", "content": "In the following, let $(G_N)_{N \\in \\mathbb{N}} = (V_N, E_N)_{N \\in \\mathbb{N}}$ be a growing sequence of random graphs where $V_N$ denotes the vertex set and $E_N$ is the edge set of the corresponding graph $G_N$.\nIn this paper, we focus on growing graph sequences where the expected average degree remains finite in the limit while the degree variance may diverge to infinity. To formalize the properties of the graph sequences we are focusing on, we first require a suitable graph convergence concept. We choose local weak convergence in probability which means that local node neighborhoods converge to neighborhoods in a limiting model. The definition below states local weak convergence, for details see e.g. Lacker et al. (2023).\nDefinition 2.1 (Local weak convergence in probability). A sequence of finite graphs $(G_N)_{N}$ converges in probability in the local weak sense to $G \\in G^*$ if for all continuous and bounded functions $f : G^* \\rightarrow \\mathbb{R}$\n$\\lim_{N \\rightarrow \\infty} \\frac{1}{N} \\sum_{i \\in [N]} f(C_{v_i}(G_N)) = E[f(G)]$ in probability,\nwhere $C_{v_i}(G_N)$ denotes the connected component of $v_i \\in G_N$ with root $v_i$ and $G^*$ is the set of isomorphism classes of connected rooted graphs.\nTo obtain meaningful theoretical results and hence practical approximations for large graphs in the next sections, we focus on graph sequences converging in the local weak sense, which we formalize with the next assumption.\nAssumption 2.2. The sequence $(G_N)_{N\\in\\mathbb{N}}$ converges in probability in the local weak sense to some $G \\in G^*$.\nThe class of random graph sequences fulfilling Assumption 2.2 covers many famous graph theoretical frameworks which are frequently used in the literature. We will briefly discuss three particularly important types of these models, namely configuration models, preferential attachment models and Chung-Lu graphs. We point to Van Der Hofstad (2024) for an extensive introduction and theoretical details and for more random graph models converging in the local weak sense.\nConfiguration models. The configuration model (Bender & Canfield, 1978; Bollob\u00e1s, 1980; Molloy & Reed, 1995; 1998) (CM) is arguably one of the most established random graph models. The basic mechanism of the CM is to start with a fixed and arbitrary degree sequence. Then, a multigraph is randomly generated with the prescribed degree distribution which means that the graph can contain self-edges and double edges between pairs of nodes.\nThe CM is known to converge under suitable and moderate assumptions in the local weak sense in probability (Van Der Hofstad, 2024, Theorem 4.1). However, the CM generates multigraphs instead of simple graphs and the number of multiedges increases drastically as the vertex degrees increase (Bollob\u00e1s, 1998). Consequently, the CM is suboptimal for generating simple graphs with a significant fraction of high degree nodes such as power law networks."}, {"title": "Preferential attachment models.", "content": "To model random graphs with power law features, Barab\u00e1si & Albert (1999) introduced the famous Barab\u00e1si-Albert (BA) model. The original BA model was subsequently extended in various ways for different applications, see Piva et al. (2021) for an overview. The BA model depicts graphs that grow over time by adding new nodes and edges to the topology. Since new nodes are more likely to be connected to highly connected nodes in the current graph, these models are often referred to as preferential attachment models.\nPreferential attachment models under suitable conditions converge in the local weak sense in probability, see Van Der Hofstad (2024, Theorem 5.8) for details. The BA model, for example, generates power law networks with a coefficient of exactly three (Bollob\u00e1s et al., 2001). In many applications, however, it is beneficial to also consider graphs with power law coefficient deviating from three to capture different empirical graph topologies."}, {"title": "Chung-Lu graphs.", "content": "The Chung-Lu (CL) random graph model (Aiello et al., 2000; 2001; Chung & Lu, 2002; 2006) provides an efficient way to model large, sparse networks (Fasino et al., 2021). To generate a random CL graph with $N\\in \\mathbb{N}$ nodes, first specify a weight vector $w \\in \\mathbb{R}^N$ with one weight $w_i\\in \\mathbb{R}_+$ for each node $i \\in {1, . . ., N}$. Then, two nodes i and j are connected with probability $\\frac{w_i \\cdot w_j}{w}$, independently of all other node pairs and with normalization factor $w := \\sum_{1<k<N} w_k$. Intuitively, a node with high weight is more likely to have many connections than a node with small weight.\nSequences of CL graphs fulfill Assumption 2.2 under mild technical assumptions, see Van Der Hofstad (2024, Theorem 3.18) for a formal statement. Most importantly, the average expected degree has to converge to a finite limit, which perfectly aligns with our goal to model very sparse networks. Throughout the paper, we use the running example of power law degree distributions with coefficient $\\gamma > 2$ observed in many real world networks to some extent (Newman, 2003; Kaufmann & Zweig, 2009; Newman et al., 2011).\nExample 1 (Power law). In our work, a power law is a zeta distribution with parameter $\\gamma > 2$ such that $P(deg(v) = k) = \\frac{k^{-\\gamma}}{\\zeta(\\gamma)}$, where $\\zeta(\\gamma)$ is the Riemann zeta function $\\zeta(\\gamma) := \\sum_{j=1}^{\\infty} \\frac{1}{j^{\\gamma}}$. A power law degree distribution has a finite expectation $E[deg(v)] = \\frac{\\zeta(\\gamma - 1)}{\\zeta(\\gamma)}$ for $\\gamma > 2$.\nLarge, sparse power law networks of the above form can be efficiently generated by the CL framework (Fasino et al., 2021). Note that our methods apply to all distributions meeting Assumption 2.2 and perform well on many empirical networks, as shown in the next sections."}, {"title": "Advantages over graphons and graphexes.", "content": "Local weak converging graph sequences such as those generated by CM, BM or CL can model sparser, and thus often more realistic topologies than those captured by Lp graphons and graphexes. Mathematically, both Lp graphons and graphexes are limited to graph sequences were the average degree diverges to infinity. Locally weak converging graph sequences, on the other hand, can capture sparse and often more realistic topologies. The usefulness of models like the CM, BA model and CL graphs is reflected in their frequent use in various research areas. However, formulating a mean field approach based on these graph theoretical models is challenging due to their high number of low degree nodes."}, {"title": "3. The Finite Model and Its Limit", "content": "Denote by $\\mathcal{P}(X)$ the set of probability distributions over a finite set $X$ and define $[N] := {1, ..., N}$ for any $N \\in \\mathbb{N}$.\nFinite model. Assume some finite state space $X$, finite action space $U$ and finite and discrete time horizon $T := {0,..., T - 1}$ with terminal time point $T$ are given. Furthermore, there are $N \\in \\mathbb{N}$ agents connected by some graph $G_N = (V_N, E_N)$ with vertex set $V_N$ and edge set $E_N$. Here, the random state of agent $i \\in [N]$ at time $t \\in T$ is denoted by $X_{i,t}^N$. All agents $V \\subseteq V_N$ with degree $k \\in \\mathbb{N}$ share a common policy $\\pi_k$ at all time points $t \\in T$. The empirical k-degree MF is defined as\n$\\mu_{t}^{N,k} := \\frac{1}{|V^k|} \\sum_{i\\in[N]:v_i\\in V^k} \\delta_{X_{i,t}^N} \\in \\mathcal{P}(X)$,\nfor all $t\\in T$ and $k \\in \\mathbb{N}$. Define the overall empirical MF sequence as $\\mu_t^N := (\\mu_{t}^{N,1}, \\mu_{t}^{N,2}, ...) \\in \\mathcal{P}(X)^N$. Each policy $\\pi_k \\in \\mathcal{P}(U)^{T\\times X \\times S^*}$ in the policy ensemble $\\pi = (\\pi_1, \\pi_2, ...) \\in \\mathcal{P}(U)^{T\\times X \\times G^* \\times \\mathbb{N}}$ takes into account the current state of the respective agent i with k neighbors and its neighborhood $G_{i,t} \\in G_k := {G \\in \\mathcal{P}(X) : k \\cdot G \\in \\mathbb{N}}$. Our learning algorithms also apply to other policy types, e.g., in our experiments we consider computationally efficient policies only depending on the current agent state. Then, the model dynamics are\n$U_{i,t}^N \\sim \\pi_k(U_{i,t}^N| X_{i,t}^N, G_{i,t})$\nand\n$X_{i,t+1}^N \\sim P(X_{i,t+1}^N| X_{i,t}^N, U_{i,t}^N, G_{i,t})$\nfor an agent i with degree k, t \u2208 T, i.i.d. initial distribution $\\mu_0 \\in \\mathcal{P}(X)$, and transition kernel $P : X \\times U \\times \\mathcal{P}(X) \\rightarrow \\mathcal{P}(X)$. Note that the theory and subsequent learning algorithms extend to degree dependent transition kernels $P_k$."}, {"title": "The policies are chosen to maximize the common objective", "content": "$J^N(\\pi) := \\sum_{t=1}^T r(\\mu_t^N)$\nwith reward function $r : \\mathcal{P}(X)^N \\rightarrow \\mathbb{R}$. Our model also covers reward functions with actions as inputs by using an extended state space $X \\cup (X \\times U) \\to (X \\times U)$ and splitting each time step $t \\in T$ into two.\nLimiting LWMFC system. In the limiting LWMFC system, the MF for each degree $k \\in \\mathbb{N}$ evolves according to\n$\\mu_{t+1}^{k} := \\mu_t P^{\\pi, \\mu', \\mathcal{W}}$\n$\\mathcal{W} := \\sum_{x\\in X} \\mu_t^k(x) \\sum_{G\\in G_k} P(G(\\mu_t) = G | x_t = x)$\n$\\cdot \\sum_{u\\in U} \\pi_k(u | x, G) P(\\cdot | x, u, G)$\nwith i.i.d. initial distribution $\\mu_0^k \\in \\mathcal{P}(X)$ and where $G_k$ is the set of k-neighborhood distributions as before. As in the finite system, define the limiting MF ensemble $\\mu_t := (\\mu_t^1, \\mu_t^2, ...) \\in \\mathcal{P}(X)^{\\mathbb{N}}$ and the corresponding reward in the limiting system is $J(\\pi) := \\sum_{t=1}^T r(\\mu_t)$.\nTheoretical results. Next, we show the strong theoretical connection between the finite and limiting LWMFC system. The following theoretical results built on the assumption that the underlying graph sequence converges in the local weak sense, formalized by Assumption 2.2. The proofs are in Appendix A. We first state empirical MF convergence to the limiting MFs.\nTheorem 3.1 (MF convergence). Under Assumption 2.2, for any fixed policy ensemble $\\pi$, the empirical MFs converge"}, {"title": "to the limiting MFs such that for all $k \\in \\mathbb{N}$ and all $t \\in T$", "content": "$\\mu_t^{N,k} \\longrightarrow \\mu_t^k$ in probability for $N \\rightarrow \\infty$.\nThe MF convergence from Theorem 3.1 enables us to derive a corresponding convergence result for the objective function under a standard continuity assumption on the reward.\nAssumption 3.2. The reward function $r : : \\mathcal{P}(X)^{\\mathbb{N}} \\rightarrow \\mathbb{R}$ is continuous.\nWith the above assumption in place, we establish the convergence of the objective function in the finite system to the one in the limiting LWMFC model.\nProposition 3.3 (Objective convergence). Under Assumptions 2.2 and 3.2 and for any fixed policy ensemble $\\pi$, the common objective in the finite system converges to the limiting objective, i.e.\n$J^N(\\pi) \\rightarrow J(\\pi)$ in probability for $N\\rightarrow\\infty$.\nWe leverage these findings to show that for a finite set of policy ensembles, the optimal policy for the limiting system in the set is also optimal in all sufficiently large finite systems. Therefore, if one wants to know the optimal ensemble policy for an arbitrary, large agent system, it suffices to find the optimal ensemble policy in the limiting system once which is formalized by Corollary 3.4.\nCorollary 3.4 (Optimal policy). Assume some set {$\\pi_1,..., \\pi_M$} of $M < \\infty$ policy ensembles is given and that w.l.o.g. $J(\\pi_1) > J(\\pi_i)$ for all $i \\in [M]$ with $i \\neq 1$. Under Assumptions 2.2 and 3.2 and for some $N^* \\in \\mathbb{N}$, $\\pi_1$ is optimal in all finite systems of size $N > N^*$ such that\n$J^N(\\pi_1) > \\max_{i\\in[M],i\\neq 1} J^N(\\pi_i).$"}, {"title": "4. The Two Systems Approximation", "content": "In limiting systems on sparse graphs, the state evolution and optimal policy of an agent potentially depend on the entire network (Lacker & Soret, 2022). Calculating $P_\\pi(G_k(\\mu_t) = G | x_t = x)$ at time t \u2208 T in the limiting system requires all possible t-hop neighborhood degree-state distributions where t-hop neighborhoods include all agents with a distance of at most t edges to the initial agent. Unfortunately, by Lemma 4.1 the number of t-hop neighborhoods grows at least exponentially with the degree k in important classes of locally weak converging graphs sequences, such as CL graphs with power laws above two.\nLemma 4.1. In the limiting system, the number of possible t-hop degree-state neighborhood distributions of agents with degree k \u2208 \\mathbb{N} at time t \u2208 T in the worst case, e.g. CL power law, is $\\Omega (2^{poly(k)})$.\nJust neglecting high degree nodes in the model might appear as a reasonable approximation to reduce computational complexity. However, the heavy tail of a degree distribution with finite expectation and infinite variance makes this approach highly inaccurate, as Example 2 illustrates.\nExample 2. In a power law graph with $\\gamma = 2.5$, around 96% of nodes have a degree of at most five. However, these 96% of low degree nodes only account for roughly two thirds of the expected degree, formally $\\sum_{h=1}^5 h^{1-\\gamma} /\\frac{\\zeta(\\gamma-1)}{\\zeta(\\gamma)} < 0.68$. Nodes with a degree of at most ten still only account for around 76% of the expected degree.\nTwo systems approximation. For the subsequent two systems approximation, we first require a heuristic on the neighbor degree distribution for a given node.\nHeuristic 1. For an arbitrary node $v' \\in V$ the degree distribution of its neighbor $v \\in V$ is approximately\n$P(deg(v) = k | deg(v') = k', (v', v) \\in E) = \\frac{k \\cdot P(deg(v) = k)}{\\sum_{k''\\in N} k'' \\cdot P(deg(v) = k'')}$.\nHeuristic 1 is a good approximation for some sequences of locally weak converging graphs, such as CL graphs (Jackson et al., 2008, Chapter 4), and thus reasonable in our setup. The idea of Heuristic 1 is the following: if one fixes any node $v' \\in V$ and considers its neighbors, high degree nodes are more likely to be connected to $v'$ than lowly connected ones. Instead of the overall degree distribution, we thus weight each probability by its degree and normalize accordingly. The result is an approximate neighbor degree distribution accounting for the increased probability of highly connected neighbors.\nTo address the complexity of the limiting system, we provide an approximate limiting system based on Heuristic 1 and"}, {"title": "the underlying sparse graph structure. Our two systems approximation consists of a system for small degree agents with at most k* neighbors and another one for agents with more than k* connections, where k* \u2208 \\mathbb{N} is some arbitrary, but fixed finite threshold. Define an approximate MF $\\hat{\\mu}^k$ for each $k \\in [k^*]$ and furthermore summarize all agents with more than k* connections into the infinite approximate MF $\\hat{\\mu}^{\\infty}$ and define $\\hat{\\mu} := (\\hat{\\mu}^1, \u2026\u2026\u2026, \\hat{\\mu}^{k^*}, \\hat{\\mu}^{\\infty})$. Based on Heuristic 1, we assume that all agents with more than k* neighbors observe the same neighborhood state distribution", "content": "$G^{\\infty}(\\mu) := \\frac{1}{E[deg(v)]} (\\sum_{h=1}^{k^*} h P(deg(v) = h) \\mu^h + \\sum_{k=k^*+1}^{\\infty} k P(deg(v) = k) \\mu^{\\infty}).$\nThe unified approximate neighborhood state distribution $G^{\\infty}$ allows us to state an approximate, simplified version of the MF forward dynamics for high degree agents given by\n$\\mu_t^{\\infty+1} := \\mu_t P^{\\pi} := \\sum_{x,u} \\hat{\\mu}_t^{\\infty}(x) \\pi_{\\infty}(u | x, G^{\\infty}(\\mu_t)) P(\\cdot | x, u, G^{\\infty}(\\mu_t))$,\nwhere all agents with more than k* connections follow the same policy $\\pi_{\\infty}\\in \\mathcal{P}(U)^{T\\times X \\times \\mathcal{P}(X)}$ and where the sum is over all (x, u) \u2208 X \u00d7 U. The approximate neighborhood of an agent with degree k \u2208 [k*] at each time t \u2208 T is sampled from $G(\\mu) \\sim Mult(k, G(\\mu))$, i.e. $G(\\mu)$ is multinomial with k trials and probabilities $G^{\\infty}(\\mu)(x)$ for each x \u2208 X. Using Heuristic 1, the approximation yields for each $k \\in [k^*]$ the MF forward dynamics\n$\\mu_{t+1}^{k} := \\mu_t P^{\\pi} := \\sum_{x,u,G} \\hat{\\mu}_t^k(x) P_{Mult}(G | k, G^{\\infty}(\\mu_t)) \\pi_k(u | x, G) P(\\cdot | x, u, G)$\nwhere the sum is over all (x, u, G) \u2208 X \u00d7 U \u00d7 $G^k$.\nExtensive approximation. In Appendix B we derive a second, extensive approximation\n$\\mathcal{P}_{\\pi,\\mu} (G_{t+1}^i (\\mu_t) = G, x_{t+1} = x)$\n$\\sim \\sum_{G' \\in G_k} \\sum_{x' \\in X} \\sum_{c \\in C_k} \\mathcal{P}_{\\pi,\\mu} (G_{t}^i (\\mu_t) = G', x_{t} = x')$\n$\\cdot \\sum_{u \\in U} \\pi^*_k (u | x') P (x | x', u, G')$"}, {"title": "5. Learning Algorithms", "content": "To solve the MARL problem of finding optimal policies for each class of k-degree nodes, we propose two methods based on reducing the otherwise intractable many-agent graphical system to a single-agent MFC MDP. The first approach in Algorithm 1 is based on solving the resulting limiting MFC MDP under the parameters of the real graph, using the previously established two systems approximation. The second approach in Algorithm 2 instead directly learns according to single-agent RL that solves the MFC MDP by interacting with the real graph.\nRL in MFC MDP. The two system approximation reduces the complexity of otherwise intractable large interacting systems on networks to the MFs of each degree. The system state at any time is then given by low-degree MFs $\\mu_t^{1}, \\mu_t^{2}, ..., \\mu_t^{k^*}$ and high-degree MF $\\mu^{\\infty}$, briefly $\\mu_t := (\\mu_t^1, \\mu_t^{2}, ..., \\mu_t^{k^*}, \\mu_t^{\\infty})$. Given a state $\\mu_t$, the possible state evolutions depend only on the analogous set of low-degree and high-degree policies at that time, $\\pi_t := (\\pi_t^{1}, \\pi_t^{2},...,\\pi_t^{k^*},\\pi_{\\infty})$. Therefore, choosing a fully defines the state transition of the overall system, and is thus considered as the high-level action in the MFC MDP. Introducing a high-level policy $\\pi$ to output $\\pi_t \\sim \\pi_{\\theta}(\\pi_t | \\mu_t)$ allows us to solve for an optimal set of policies by solving the MFC MDP for optimal $\\theta$, since the limiting MF dynamics are deterministic. Finally, the MFC MDP is solved by applying single-agent policy gradient RL, resulting in Algorithm 1. In practice, we use proximal policy optimization (Schulman et al., 2017). To lower the complexity of the resulting MDP, we parametrize policies as distributions over actions given the node state, $\\pi_{\\theta}^k \\in \\mathcal{P}(U)^X$."}, {"title": "Algorithm 1 LWMFC Policy Gradient", "content": "1: for iterations n = 1, 2, ... do\n2:   for time steps t = 0, ..., Blen - 1 do\n3:  Sample LWMFC MDP action $\\pi_t \\sim \\pi_{\\theta} (\\pi_t | \\mu_t)$.\n4:  Compute reward $r(\\mu_t)$, next MF $\\mu_{t+1}$, termination flag $d_{t+1} \\in {0,1}$.\n5:   end for\n6:  Update policy $\\pi_{\\theta}$ on minibatches $b = {(\\mu_t, \\pi_t, r_t, d_{t+1}, \\mu_{t+1})_{t>0}}$ of length $blen$.\n7: end for\nMARL on real networks. In addition to assuming knowledge of the model and computing the limiting MFC MDP equations, we may also directly learn on real network data without such model knowledge in a MARL manner. To do so, we still apply policy gradient RL to solve an assumed MFC MDP, but substitute samples from the real network into $\\mu_t$. At the same time, we let each node perform its actions according to the sampled $\\pi_t \\sim \\pi_t(\\pi_t | \\mu_t)$. This approach is well justified by the previous theory and approximation, as for sufficiently large networks the limiting system and therefore also its limiting policy gradients are well approximated by this procedure."}, {"title": "Algorithm 2 LWMFMARL Policy Gradient", "content": "1: for iterations n = 1, 2, ... do\n2:   for time steps t = 0, ..., Blen - 1 do\n3:  Sample LWMFC MDP action $\\pi_t \\sim \\pi_{\\theta} (\\pi_t | \\mu_t)$.\n4:  for node i = 1,..., N do\n5:  Sample per-node action $U_{i,t} \\sim \\pi_k (U_{i,t} | X_{i,t})$ with degree $k_i = \\infty$ if $k_i > k^*$.\n6:   end for\n7:  Perform actions, observe reward $r_t$, next MF $\\mu_{t+1}$, termination flag $d_{t+1} \\in {0,1}$.\n8:   end for\n9:  Update policy $\\pi_{\\theta}$ on minibatches $b = {(\\mu_t, \\pi_t, r_t, d_{t+1}, \\mu_{t+1})_{t>0}}$ of length $blen$.\n10: end for\nThe approach results in Algorithm 2 and has advantages. Firstly, the algorithm does not assume model knowledge and is therefore a true MARL algorithm, in contrast to solving the limiting MFC MDP. Secondly, the algorithm avoids potential inaccuracies of the two systems approximation, as we will see in Section 7, since it directly interacts with a real network of interest. Lastly, in contrast to standard independent and joint learning MARL methods, the method is rigorously justified by single-agent RL theory and avoids exponential complexity in the number of agents respectively."}, {"title": "6. Examples", "content": "For a general overview of many applications, see Lauri\u00e8re et al. (2022a). We consider four problems briefly described here. Problem details can be found in Appendix C.\nSusceptible-Infected-Susceptible/Recovered (SIS/SIR). The classical SIS model (Kermack & McKendrick, 1927; Brauer, 2005) is a benchmark in the MF learning literature (Lauri\u00e8re et al., 2022b; Zhou et al., 2024). Agents are infected or susceptible, resulting in the state space $X := {S, I}$, and decide to protect themselves or not. The infection probability increases without protection, and with the number of infected neighbors. Furthermore, there is a constant probability for recovering from an infection. The SIR model (Hethcote, 2000; Doncel et al., 2022) is an extension of SIS where agents can also be in a recovered state R where they are immune to a reinfection. Consequently, the state space for the SIR model is $X := {S, I, R}$.\nGraph coloring (Color). Inspired by graph coloring"}, {"title": "7. Simulation & Results", "content": "In this section, we numerically verify the two system approximation as well as the proposed learning algorithms by comparing them with baselines from the literature. The two systems approximation is compared with previous graph approximations such as graphex or Lp graphon MF equations, and the learning algorithms are verified against standard scalable independent learning methods such as IPPO (Tan, 1993; Papoudakis et al., 2021), due to the large scale of networks considered here. To generate artificial networks of different sizes we employ a CL-based graph sampling algorithm (Chung & Lu, 2002; Miller & Hagberg, 2011) from the Python NetworkX package.\nWe compare the accuracy of our model on different empirical datasets with Lp graphon and graphex based models and with our extensive approximation LWMFC*, where computationally feasible, to see how much information is lost"}, {"title": "8. Conclusion", "content": "We have introduced the novel LWMFC framework which can depict agent networks with finite expected degree and diverging variance. After a theoretical analysis, we provided a practical two systems approximation which was then leveraged to design scalable learning algorithms. Finally, we evaluated the performance of our model and learning algorithms for different problems on synthetic and real-world datasets and compared them to existing methods. For future work, one could extend the LWMFC model to various types of specific mean field models, e.g. to partial observability or agents under bounded rationality. We hope that LWMFC and the corresponding learning approaches prove to be a versatile and useful tool for researchers across various applied research areas."}, {"title": "A. Appendix: Proofs for the Theoretical Results", "content": "A.1. Proof of Theorem 3.1\nProof. We aim to eventually apply Lacker et al. (2023", "X_e": "X \\cup (X \\times U)$ which serves as the state space for the extended particle system for some fixed policy ensemble $\\pi$. The idea behind the extended state space $X_e$ is to define an extended particle system where the state transition in X and the choice of the next action $u_{t+1"}, "in U$ are separated into two different time steps.\nUsing the notations from Lacker et al. (2023), denote by $S^{\\lfloor}(X)$ the set of finite unordered sequences of arbitrary length with values in X and by$\\$\\Xi := X^{X \\times S^{\\lfloor}(X)} \\times U^{X \\times U \\times S^{\\lfloor}(x)}$ the set of possible noise values. Next, specify a transition function $F^+ : X_e \\times S^{\\lfloor}(X) \\times \\Xi \\rightarrow X$ for each $\\tau"]}