{"title": "UNIMATCH: UNIVERSAL MATCHING FROM Atom TO TASK FOR FEW-SHOT DRUG DISCOVERY", "authors": ["Ruifeng Li", "Mingqian Li", "Wei Liu", "Yuhua Zhou", "Xiangxin Zhou", "Yuan Yao", "Qiang Zhang", "Hongyang Chen"], "abstract": "Drug discovery is crucial for identifying candidate drugs for various diseases. However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem. Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures that determine different molecular properties. To address these issues, we introduce Universal Matching Networks (UniMatch), a dual matching framework that integrates explicit hierarchical molecular matching with implicit task-level matching via meta-learning, bridging multi-level molecular representations and task-level generalization. Specifically, our approach explicitly captures structural features across multiple levels\u2014atoms, substructures, and molecules-via hierarchical pooling and matching, facilitating precise molecular representation and comparison. Additionally, we employ a meta-learning strategy for implicit task-level matching, allowing the model to capture shared patterns across tasks and quickly adapt to new ones. This unified matching framework ensures effective molecular alignment while leveraging shared meta-knowledge for fast adaptation. Our experimental results demonstrate that UniMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and 6.52% in \u2206AUPRC. UniMatch also shows excellent generalization ability on the Meta-MolNet benchmark. The code is available at\nhttps://github.com/Lirain21/UniMatch.git", "sections": [{"title": "1 INTRODUCTION", "content": "Drug discovery is pivotal for human health, involving the screening and optimization of numerous compounds to identify potential drug candidates that satisfy both pharmacological efficacy and toxicological safety criteria. The traditional drug development cycle typically spans over a decade, incurs costs exceeding $1 billion, yet achieves a success rate of less than 10%. Artificial Intelligence-Driven Drug Discovery (AIDD) has emerged as a promising solution to address this challenge. Within AIDD, Quantitative Structure-Activity/Property Relationships (QSAR/QSPR) models are crucial for predicting the relationships between molecular structures and their activities. These methods rely heavily on extensive datasets due to the complexity of understanding and modeling molecular geometries. However, the lengthy durations, high costs, and low success rates of chemical wet experiments limit the availability of labeled experimental data.\nFew-shot learning has demonstrated substantial potential in addressing data scarcity by enabling models to generalize rapidly from minimal data to new tasks. Recently, several approaches have been proposed to address this challenge in few-shot scenarios. Most approaches are based on molecular graphs with atoms as nodes and chemical bonds as edges, leveraging Graph Neural Networks (GNNs) to capture molecular topologies. In particular, such models as IterRefLSTM, Meta-MGNN, PAR, ADKF-IFT, and Meta-GAT employ GNNs as encoders to learn molecular representations for label prediction. Additionally, several sequence-based methods, such as CHEF, MH-Nfs and CRA, utilize Multilayer Perceptrons (MLPs) as encoders to compress molecular fingerprints or descriptors for predictive modeling.\nHowever, existing approaches often overlook a crucial aspect: different levels of structural information-ranging from atoms to substructures to the entire molecule\u2014contribute to distinct molecular properties. Some properties are influenced by atomic composition, while others depend on substructures or the overall molecular configuration. Figure 1 illustrates this with examples: (a) fluorine and nitrogen affect molecular acidity and basicity, respectively; (b) hydroxyl groups influence the hydrophobic properties of ethanol and dodecane; and (c) the overall molecular structure affects boiling points. In graph-based methods, the use of multiple GNN layers may cause over-smoothing, where the receptive fields of nodes expand excessively, thus obscuring substructural details. As a result, GNNs are more suitable for predicting properties related to the overall structure of molecules. In contrast, fingerprint-based methods rely on only fragmented local features, potentially overlooking critical information about the overall molecular structure. Although CHEF introduces a representation fusion strategy, its reliance on ECFP6\u2014which is based on fixed local features\u2014limits its ability to capture hierarchical molecular structures. Therefore, effectively capturing different levels of molecular structures is crucial for accurately predicting a wide range of molecular properties.\nTo address this challenge, we propose Universal Matching Networks (UniMatch), a framework that facilitates universal matching across multiple levels from atoms to tasks-enhancing the few-shot molecular property prediction task. Our main contributions are summarized as follows:\n\u2022 To the best of our knowledge, we are pioneers to introduce a universal matching approach that spans from the atomic level to the task level. This framework employs explicit hierarchical molecular matching and implicit task-level matching at distinct levels to align molecular structures with tasks. The dual matching mechanism complements itself, forming a synergistic framework that enhances the model's adaptability and generalization across various tasks (Section 3).\n\u2022 We propose an explicit hierarchical molecular matching mechanism that integrates information from atoms to higher-level structures, capturing complex molecular features. By utilizing an attention-based matching module, the model aligns representations across multiple levels, selecting the most relevant features for improved prediction (Sections 3.1).\n\u2022 We incorporate a meta-learning strategy to achieve implicit task-level matching, learning shared parameters that generalize across tasks. This matching occurs at an abstract"}, {"title": "2 BACKGROUND", "content": "2.1 PROBLEM DEFINITION\nNt\nThe few-shot molecular property prediction problem, as defined by ADKF-IFT and MHNfs, involves training models on a set of tasks {T}1 sampled from the training set Dtrain to improve generalization to new tasks. Each task T, includes a support set ST = {(xr,i, yr,i)}i=1 and a query set QT = {(xr,j, yr,j)}j=1, where xr,i \u2208 Rd and xr,j \u2208 Rd represent molecular features, and yr,i, yr,j \u2208 {0, 1} indicate the molecular properties or activities. The support set ST provides a few labeled examples for task-specific adaptation, while the query set QT is utilized to evaluate the model's performance on unseen examples.\n2.2 PRELIMINARIES\nGraph neural networks (GNNs) are designed to handle graph-structured data (non-Euclidean data) by aggregating information from neighboring nodes to learn effective representations. Models such as GCN, GIN , and GAT are widely used for graph classification and other related applications. In a graph G = {V, E}, V represents the set of nodes, and E the set of edges. h(0) represents the initial features of node v, and bu,v denotes the features of the edge eu,v between nodes u and v. At the lth layer, the representation h(l)v of node v is updated in GNNs as follows:\nh(l)v = UPDATE(l)v(h(l\u22121)v , AGGREGATE(l)({(h(l\u22121)u , bu,v) | u \u2208 N(v)})), (1)\nwhere N(v) is the set of neighboring nodes of v. The AGGREGATE function combines features from neighboring nodes, and the UPDATE function updates the node features for the next layer."}, {"title": "3 METHOD", "content": "This section presents the model architecture (Section 4.1) and the meta-learning strategy (Section 4.2) of Universal Matching Networks, which respectively achieve explicit hierarchical molecular matching and implicit task-level matching. Figure 2 illustrates an overview of UniMatch framework.\n3.1 ARCHITECTURE: EXPLICIT HIERARCHICAL MOLECULAR MATCHING\nWe propose a novel architecture for explicit hierarchical molecular matching, which captures and aligns complex molecular structures across multiple levels (atomic, substructural, and molecular) via hierarchical pooling and matching, enabling fine-grained comparison and similarity assessment from local to global scales. It establishes the foundation for implicit task-level matching.\nEncoding Module. Following the mainstream graph-based few-shot molecular property prediction approaches, we adopt the widely used GIN as the backbone of our method. In GNNs, each layer aggregates local information from nodes and their neighboring hops. As the network depth increases, the model incrementally aggregates hierarchical information, progressing from individual nodes to substructures and ultimately capturing the entire molecule.\nTo capture molecular representations at different levels, we employ mean pooling to aggregate node representations at each layer of the GNN. For a given task T, we first obtain the node representations"}, {"title": "3.2 \u039c\u0395\u03a4A-LEARNING", "content": "We employ a meta-learning strategy to facilitate implicit task-level matching. To further clarify why meta-learning functions as implicit task-level matching, we introduce a task relationship matrix that captures task similarities at a higher level of abstraction. This matrix allows for efficient adaptation and enhances generalization through optimized learning.\n3.2.1 TRAINING AND INFERENCE\nFor simplicity, we define UniMatch as f\u03b8,w, where \u03b8 denotes the parameters of the molecular encoder, and w = {Wq, Wk, Wo} represents the parameters of the matching and fusion modules.\nTraining Phase. During the training phase, we employ a standard meta-learning process to improve the model's generalization ability using the training set Dtrain, as illustrated in Algorithm 1. Parameter learning involves a combination of inner and outer optimization. To facilitate dual optimization, we split the support set Strain,T for each task Train,T into S'T and Q'T. The training loss L(Q'T , f\u03b8,w) evaluated on Q'T , is defined as:\nL(Q'T , f\u03b8,w) = \u03a3 (x\u03c4,i, y\u03c4,i) \u2208QT \u2212yr,i log (\u0177r,i), (5)\nwhere yr,i \u2208 R2 is a one-hot vector representing the class of the sample, where a positive sample is denoted by [1, 0] and a negative sample is denoted by [0, 1].\n\u2022 Inner Loop. During the inner optimization, task-specific parameters wT are updated for each task Train,T, enabling the model to adapt quickly to the current task, as follows:\nwT = w \u2212 \u03b1\u2207wL(QT , f\u03b8,w), (6)\nwhere \u03b8 denotes the fixed parameters in the inner optimization and \u03b1 is the inner learning rate.\n\u2022 Outer Loop. The outer optimization aims to update the meta-parameter (\u03b8 and w) to improve generalization across all tasks. During training, this is achieved by minimizing the aggregated loss over all tasks:\nNt\n\u03b8\u2217 , w\u2217 = arg min \u03a3L(Qtrain,T, f\u03b8,w), (7)\nT=1\nwhere Nt is the total number of training tasks.\nInference Phase. After training, we evaluate the model on a set of test tasks Ttest drawn from the test set Dtest. The support set Stest,T is split into S''T and Q''T. With \u03b8 fixed, we fine-tune the task-specific parameters w using Q''T as defined in Eq. 6. After fine-tuning, the model is evaluated on Stest,T, which serves as the support set to predict the labels for the unknown query molecules.\n3.2.2 IMPLICIT TASK-LEVEL MATCHING\nIn our UniMatch framework, we consider meta-learning as an implicit task-level matching mechanism. During the training phase, the model autonomously captures and internalizes inter-task relationships and features, embedding them into its meta-parameters. As a result, even without explicitly modeling task relationships, the model can rapidly adapt to new tasks, demonstrating a form of implicit matching.\nImplicit matching primarily captures the model's adaptability across multiple tasks through latent relationships, rather than direct parameter updates. To quantify these relationships, we define a task relationship matrix M \u2208 RNt\u00d7Nt, where each element Mi,j represents the relationship (e.g., similarity or distance) between task Ttrain,i and task Ttrain,j, which is defined as follows:\nMi,j = g\u03b8,w (Ttrain,i, Ttrain,j), (8)\nwhere g\u03b8,w is a relation function based on the meta-parameter (\u03b8 and w), measuring the relationship between task Ttrain,i and task Ttrain,j."}, {"title": "4 EXPERIMENT", "content": "In this section, we evaluate the empirical performance of UniMatch, as outlined in Section 3. We validate our UniMatch on the MoleculeNet (Section 4.1) and FS-Mol (Section 4.2) benchmarks. Additionally, we perform an ablation study of UniMatch in Section 4.2. To demonstrate the generalization of UniMatch, we further test it on seven datasets from the Meta-MolNet benchmark in Section 4.3, covering both single-task and multi-task scenarios. Finally, we conduct visualization experiments in Section 4.4 to demonstrate the importance of the dual matching mechanism in UniMatch. Additional experiments and analyses are provided in Appendix E. All experiments are run on an NVIDIA RTX A6000 GPU.\n4.1 FEW-SHOT MOLECULAR PROPERTY PREDICTION ON THE MOLECULENET BENCHMARK"}, {"title": "4.2 FEW-SHOT MOLECULAR PROPERTY PREDICTION ON THE FS-MOL BENCHMARK", "content": "Benchmark and Baselines. FS-Mol, introduced by Stanley et al. (2021), serves as a benchmark for few-shot molecular property prediction. It comprises 5,120 tasks, partitioned into 4,938 for train-ing, 40 for validation, 157 for testing, covering a total of 233,786 compounds (see Appendix C.1).\nTo evaluate UniMatch, we compare it against four types of baselines: 1) Single-task methods: single-task GP with Tanimoto kernel (GP-ST), single-task GNN (GNN-ST), MixHop and CHEF; 2) Multi-task pre-training: Multi-task GNN (GNN-MT) ; 3) Self-supervised pre-training: Molecule Attention Transformer (MAT); and 4) Meta-learning methods: PAR, ProtoNet, GNN-MAML,"}, {"title": "4.3 CROSS-DOMAIN DRUG DISCOVERY ON THE META-MOLNET BENCHMARK", "content": "Benchmark and Baselines. Meta-MolNet sets a standard for evaluating generalization in computational chemistry by improving data quality and testing rigor. We evaluate our model on classification tasks including GSK3, JNK3, HIV, Tox21, ToxCast, PCBA, and MUV. For comparison, we consider four types of baselines: 1) Classical machine learning methods: support vector machine (SVM), extreme gradient boosting (XGBoost) , and random forests (RF). 2) Supervised learning methods: GCN, CMPNN , DMPNN, Attentive FP , and TrimNet. 3) Self-supervised learning methods: CDDD, Mol2Context-vec , MolBERT, N-gram, and Pre-GNN. 4) Meta-learning methods: ADKF-IFT and Meta-GAT . All baseline results are reproduced according to Lv et al. (2024). Due to the sub-task settings of Meta-MolNet, prototype-based methods are no longer applicable. Further details can be found in Appendix D.1.\nEvaluation Procedure. To evaluate the generalization ability of UniMatch, we follow a higher ratio of molecules/scaffolds by Lv et al. (2024). For classification tasks, we use AUROC and AUPRC as evaluation metrics. Specifically, AUROC is used to measure the performance of binary classification tasks (GSK3, JNK3, HIV, Tox21, and ToxCast), while AUPRC is more suitable for tasks with severely imbalanced distributions (PCBA, MUV). All experimental results are averaged over three independent runs with different random seeds, using a support set size of 2. Additional details on the evaluation metrics can be found in Appendix D.3.\nPerformance. Figure 4 presents a comparison of different methods across the seven classification datasets in Meta-MolNet benchmark. The results indicate that UniMatch performs exceptionally well on the GSK3, JNK3, Tox21, and ToxCast datasets, while showing less well on the HIV and PCBA datasets. Our method encounters significant challenges on the MUV dataset, likely due to distributional biases. Overall, UniMatch exhibits excellent generalization capabilities across most datasets for new molecular scaffolds, but struggles in specific cases, such as the MUV dataset."}, {"title": "4.4 VISUALIZATION", "content": "To validate the importance of hierarchical representations, we visualize 10 molecules from the tox21 dataset for the NR-AhR toxicity prediction task, as shown in Figure 5. In the second row, we select one molecule (SMILES: \u201cCCOc1ccc2nc(S(N)(=O)=O)sc2c1\u201d) to illustrate how each GNN layer captures distinct structural levels, ranging from atoms and substructures to the entire molecule. Additionally, PCA projections of these 10 molecules are used to examine the distribution of active and inactive molecules. This analysis enhances our understanding of the model's ability to distinguish molecular structures across layers, offering insights into the role of hierarchical feature extraction and its interpretability in toxicity prediction. Further details can be found in Appendix F."}, {"title": "5 RELATED WORK", "content": "5.1 GRAPH-BASED MOLECULAR PROPERTY PREDICTION\nGraph-based methods are a mainstream approach for the few-shot molecular property prediction task. PAR and ADKF-IFT employ GIN as the molecular encoder, while Meta-MGNN utilizes Pre-GIN . Meta-GAT adopts GAT to learn molecular representations. However, these methods typically focus on single-scale molecular features and overlook the hierarchical nature of molecular structures . In addition, several approaches combine the strengths of Large Language Models (LLMs) to tackle the few-shot problem, but these methods often incur high computational costs. Our method differs by incorporating molecular hierarchical structures through hierarchical pooling and matching, enabling more effective alignment of complex structures.\n5.2 MATCHING LEARNING\nTo address the few-shot learning problem, matching learning compares new instances with a small set of labeled examples to facilitate accurate predictions. Common methods include Matching Networks, ProtoNet, Relation Networks, and LGM-Net. While these methods perform well in Natural Language Processing (NLP) and Computer Vision (CV), they struggle with the inherent complexity of molecular graphs, which feature non-Euclidean structures and intricate relationships between nodes and edges. Hierarchical matching can mitigate this issue by capturing multi-level representations, but existing approaches still face limitations when applied to molecular data due to its unique topological complexity. Specifically, AMN and SSF-HRNet , despite their improvements in feature robustness and hierarchical relationships, struggle to fully represent global structural information and generalize across complex, varied molecular graphs. Similarly, VTM and HCL integrate hierarchical matching with patch-level techniques in CV, but their effectiveness diminishes when handling the structural diversity of molecular graphs. To overcome these challenges, our UniMatch combines explicit intra-molecular hierarchical learning with attention mechanisms at atomic, substructural, and molecular levels, along with implicit task-level hierarchical learning via meta-learning, enhancing the model's ability to capture task-specific molecular information and improve generalization."}, {"title": "6 CONCLUSION", "content": "We propose Universal Matching Networks (UniMatch) to address the limitations of existing few-shot learning methods in drug discovery. UniMatch employs a dual matching framework that integrates explicit molecular matching with implicit task-level matching. Explicit hierarchical molecular matching provides contextual representations that support implicit task-level matching, enabling better knowledge sharing across tasks. The complementary nature of these two mechanisms further enhances model performance and adaptability to new tasks. Experimental results show that UniMatch improves AUROC and \u2206AUPRC by 2.87% and 6.52%, respectively, on the MoleculeNet, and FS-Mol benchmarks and demonstrates excellent generalization on the Meta-MolNet benchmark. Future work will focus on improving the fusion mechanism of UniMatch by adopting advanced techniques such as attention fusion or multi-scale feature aggregation to better capture the complex relationships between structural levels. Additional details and discussions are provided in Appendix H."}, {"title": "A DETAILS OF TRAINING AND INFERENCE", "content": "A.1 ALGORITHM\nAlgorithm 1 Meta-training procedure for UniMatch.\nInput: The few-shot training tasks {T\u03c4 }Nt\n\u03c4=1 of molecular property prediction;\nOutput: Trained model f\u03b8,w;\n1: Randomly initialize \u03b8 and w;\n2: while not converged do\n3: Sample a batch B of tasks {T\u03c4 |\u03c4 \u2208 B};\n4: for all {T\u03c4 |\u03c4 \u2208 B} do\n5: Sample Ns and Nq molecules to form Strain,\u03c4 and Qtrain,\u03c4 ;\n6: Split Strain,\u03c4 into S'\u03c4 and Q'\u03c4 ;\n7: for l = 1, ..., L do\n8: Obtain node representations h(l)s,v, h(l)q,v , and h(l)q of lth GNN layer by Eq. 1;\n9: Obtain molecular representations z(l)s, z(l)q' , z(l)s , z(l)q , and z(l)q of lth GNN layer by Eq. 2;\n10: Evaluate prediction \u0177 (l)\u03c4,q' and \u0177 (l)\u03c4,q of lth GNN layer by Eq. 3;\n11: end for\n12: Evaluate the final prediction \u0177\u03c4,q' and \u0177\u03c4,q by Eq. 4;\n13: Evaluate training loss L (Q'\u03c4 , f\u03b8,w) by Eq. 5;\n14: Fine-tune \u03b8 as \u03b8\u03c4 by Eq. 6;\n15: Evaluate testing loss L (Qtrain,\u03c4 , f\u03b8,w) by Eq. 5;\n16: end for\n17: Update \u03b8, w by Eq. 7;\n18: end while\nAlgorithm 1 outlines the meta-training procedure for UniMatch, designed to optimize the model for few-shot molecular property prediction tasks. The algorithm starts by initializing the parameters \u03b8 and w and iteratively updates them through a combination of inner and outer optimization steps. Within each iteration, a batch of tasks T\u03c4 is sampled, and the support set Strain,\u03c4 is split into S'\u03c4 and Q'\u03c4 . For each GNN layer, node and molecular representations are computed, and predictions are evaluated using the specified loss functions. The meta-parameters are fine-tuned and updated based on the performance on the query sets, ensuring effective model generalization across tasks.\nA.2 DETAILS OF IMPLICIT TASK-LEVEL MATCHING\nTraining Phase.\n\u2022 Task Vector p\u03c4 Expansion and Analysis. Each task Ttrain,\u03c4 has its own internal task vector p\u03c4 , which can be represented as an expansion derived from the general parameter \u03b8, the task-specific parameter w, and their relationships with other tasks. The relationship matrix M governs parameter sharing and task matching.\n\u2022 Using p\u03c4 to Construct Task Relationship Matrix M. We want to use the task vector p\u03c4 to construct the task relationship matrix M. The following methods can be used to measure the similarity or relationship between task vectors:\n\u2013 Dot Product Similarity:\nMr,j = pT\u03c4 pj. (11)\nThe dot product similarity measures the inner product of task vectors p\u03c4 and pj in vector space. The larger the value, the higher the similarity between the two tasks.\n\u2013 Cosine Similarity:\nM\u03c4,j = pT\u03c4 pj /\n||p\u03c4 || ||pj|| (12)\nThe cosine similarity measures the cosine of the angle between task vectors p\u03c4 and pj. The value is in the range of [-1, 1], and the closer it is to 1, the more similar the two tasks are."}, {"title": "B DETAILS OF MOLECULENET BENCHMARK", "content": "In this section, we introduce the details of datasets that are included in the MoleculeNet benchmark in Section B.1. In addition, we show the details of the experimental setup B.2.\nB.1 DETAILS OF DATASETS"}, {"title": "C DETAILS OF FS-MOL BENCHMARK", "content": "In this section, we first introduce the details of FS-Mol benchmark (Stanley et al., 2021) in Section C.1. The subsequent discussion delves into the details of the compared baselines on FS-Mol benchmark in Section C.1. In addition, further details regarding the evaluation metric \u2206AUPRC is presented in Section C.3. Finally, the details of experimental setup on FS-Mol benchmark is presented in Section C.4.\nC.1 DETAILS OF BENCHMARKS\nThe Few-Shot Learning Dataset of Molecules (FS-Mol) is designed for machine learning applications in the Quantitative Structure-Activity Relationships (QSAR) field , specifically focusing on few-shot learning scenarios. It comprises a total of 5120 distinct assays, encompassing 233,786 unique compounds. The dataset is partitioned into three subsets: Dtrain for training, Dtest for testing, and Dvalid for validation purposes. Dtest contains 157 tasks, Dtrain includes 4938 tasks, and Dvalid is composed of 40 tasks. Notably, each task in the dataset contains an average of 94 compounds, a notably lower figure compared to other similar datasets. This characteristic reflects the high specificity of the protein targets and the corresponding assays, posing a significant challenge in the QSAR domain."}, {"title": "C.2 DETAILS OF BASELINES.", "content": "In the comparative analysis of the FS-Mol benchmark , four types of baselines have been chosen: Single-task methods, Multi-task pre-training methods, Self-supervised pre-training methods, and Meta-learning methods.\nSingle-task Methods. The single-task methods are single-task GP with Tanimoto kernel (GP-ST), single-task GNN (GNN-ST), MixHop and CHEF for context-enriched information.\nGP-ST, as delineated in the study by , encompassing the random walk kernel, shortest-path kernel, and subtree kernel, are employed to evaluate the resemblance between graphs of chemical compounds. Gilmer et al. (2017) introduces GNN-ST, particularly focusing on MPNNS for proficient learning from graph-based representations of molecules in quantum chemistry. CHEF leverages fingerprint-based features to capture chemical information, thereby enhancing the performance of molecular property prediction tasks. MixHop is a novel graph convolutional architecture that enables higher-order neighborhood mixing in Graph Neural Networks (GNNs). By incorporating multiple neighborhood feature mixing opera-tions, including neighborhood difference operators, the MixHop model can learn a broader range of graph structural representations without increasing computational complexity.\nMulti-task Pre-training Method. Multi-task GNN (GNN-MT) employs a 10-layer pre-trained GNN with 128 hidden dimensions and \"principal neighborhood message aggregation.\" Task-specific readout functions and an MLP with a 512-dimensional hidden layer produce activity label predictions. The model is fine-tuned on all tasks in Dtrain using multi-task learning.\nSelf-supervised Pre-training Method. The Molecule Attention Transformer (MAT) modifies the Transformer architecture by incorporating insights on inter-atomic distances and the molecular graph structure into the self-attention mechanism.\nMeta-learning Methods. Property-Aware Relation Networks (PAR), Proto-typical Networks (ProtoNet), GNN-MAML and ADKF-IFT are four typical meta-learning methods. Specifically, PAR introduces a property-aware embedding function that transforms generic molecular embeddings into a substructure-aware representation which relevant to the target property, and designs an adaptive relation graph learning module to jointly estimate the molecular relation graph and refine the molecular embeddings with respect to the target property. Schimunek et al. proposes MHNfs approach, utilizing a Modern Hopfield Network (MHN) to link molecules with an extensive array of reference molecules, thereby enhancing the covariance structure of the data and mitigating spurious correlations of molecules. ProtoNet, a simple approach to few-shot classification, learns an embedding where each class is represented by a prototype, computed as the mean of the embedded support examples for that class. Classification is then done by computing distances from the query example to each class prototype. GNN-MAML uses graph neural networks to learn molecular representations, and employs a meta-learning framework for model optimization. It also incorporates molecular structure, self-supervised modules, and self-attentive task weights to exploit unlabeled data and address task heterogeneity. ADKF-IFT combines the representational power of deep learning with the probabilistic modeling capabilities of gaussian processes, enabling efficient and uncertainty-aware molecular property prediction through meta-learning."}, {"title": "C.3 EVALUATION METRICS OF FS-MOL BENCHMARK", "content": "The \u2206AUPRC (Area Under the Curve for Precision-Recall) serves as a pivotal statistical measure utilized for assessing enhancements in the efficacy of classification models when confronted with imbalanced datasets due to targeted modifications, like algorithmic adjustments or alterations in data processing methodologies. By contrasting the precision-recall curve's area prior to and post adjustments, this metric adeptly elucidates the extent of enhancement in the capacity of model to identify minority classes, thereby supplying a quantitative foundation for optimizing the model and facilitating decision-making support."}, {"title": "C.4 DETAILS OF EXPERIMENTAL SETUP", "content": "In UniMatch, the hyperparameters used by UniMatch are reported in Table 3. What is more, on FS-Mol benchmark (Stanley et al., 2021), we set the batch task 21 and weight decay 5e-5. And we train the model for 10,000 epoches."}, {"title": "D DETAILS OF META-MOLNET BENCHMARK", "content": "In this section, we first introduce the details of Meta-MolNet benchmark in Section D.1. In addition, we provide the details of the baselines in Section D.2. Finally, the details of evaluation metric is provided in Section D.3.\nD.1 DETAILS OF BENCHMARKS\nMeta-MolNet is an innovative benchmarking platform designed to improve molecular machine learning models by integrating diverse datasets through multitask and transfer learning, spanning applications from drug discovery to materials science. In this paper, we use 7 classification tasks on Meta-MolNet benchmark to evaluate our UniMatch, which include GSK3, JNK3, HIV, Tox21, ToxCast, PCBA and MUV. The GSK3 dataset focuses on predicting the activity of compounds against the GSK3 enzyme, which is associated with diseases like diabetes and Alzheimer's. The JNK3 dataset assesses the inhibitory activity of compounds against JNK3, a kinase implicated in neurodegenerative diseases. The HIV dataset contains data for predicting the ability of compounds to inhibit HIV replication. Tox21 evaluates the toxicity of compounds across multiple biological pathways, while ToxCast predicts the toxic effects of environmental chemicals. The PCBA dataset measures compound activity across various bioassays from the PubChem database. Lastly, the MUV dataset provides a rigorous and unbiased benchmark for validating virtual screening methods. Together, these tasks offer a comprehensive evaluation framework for molecular machine learning models. The detailed description of datasets in Table 4."}, {"title": "D.2 DETAILS OF BASELINES", "content": "Four types of baselines-classical machine learning models, graph-based models, message passing neural networks, and self-supervised pre-training models are chosen for comparative analysis on the Meta-MolNet benchmark\nClassical Machine Learning Methods. Support Vector Machines (SVM), extreme gradient boosting algorithms (XGBoost) , and Random Forests (RF) are among the classical machine learning methods that utilize descriptors and/or fingerprints commonly found in traditional QSPR/QSAR models . Notably,"}, {"title": "D.3 EVALUATION METRICS OF META-MOLNET", "content": "In this paper, we use benchmark datasets with a higher ratio of molecules to scaffolds, presenting a significantly more challenging scenario compared to random cross-validation and datasets with a lower ratio (Lv et al., 2024), for evaluating generalization ability. For classification tasks, we use Area Under the Receiver Operating Characteristic Curve (AUROC) and Area Under the Precision-Recall Curve (AUPRC) as evaluation metrics. Specifically, AUROC measures the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) across different classification thresholds. AUROC ranges from 0 to 1, where 0.5 represents a random classifier and 1 represents a perfect classifier. A higher AUROC value indicates better classification performance, making it well-suited for evaluating binary classification tasks such as GSK3, JNK3, HIV, Tox21, and ToxCast. Meanwhile, AUPRC considers the trade-off between precision (positive predictive value) and recall (sensitivity). Like AUROC, AUPRC ranges from 0 to 1, with higher values indicating better performance. AUPRC is particularly useful for evaluating models on imbalanced datasets, making it more suitable for tasks such as PCBA and MUV, which have severely skewed distributions."}, {"title": "D.4 DETAILS OF EXPERIMENTAL SETUP", "content": "On the Meta-MolNet benchmark, we set the query set size to 8 and the support set size to 2. We employ the AdamW optimizer with a learning rate of 0.001 for meta-learning and an inner learning rate of 0.001 for fine-tuning the task-specific modules within each task. A weight decay of 5e-4 is applied. The model is trained for 100 epochs to ensure robust performance."}, {"title": "E FURTHER EXPERIMENTS RESULTS ON FS-MOL", "content": "E.1 OVERALL PERFORMANCE\nFigure 6 (a)~(e) show the performance of different methods in classifying 157 FS-Mol test tasks across various support set sizes via box plots. The box plots show the distribution of classification accuracies for each method, providing insight into their overall performance and effectiveness in handling varying support set sizes. Our HieMatch demonstrates superior performance compared to the state-of-the-art (SOTA) method across all metrics.\nE.2 SUB-BENCHMARK PERFORMANCE\nFS-Mol divides tasks into 7 sub-benchmarks using Enzyme Commission (EC) numbers , allowing for assessment across the entire benchmark. In classification tasks with a support set size of 16, Table E.2 illustrates the performance of the top methods across all sub-benchmarks. The results highlight that, while excelling in overall performance, HieMatch emerges as the top performer in half of the sub-benchmarks for classification tasks."}, {"title": "F VISUALIZATION EXPERIMENTS", "content": "The details of the ten molecules used in Section 4.4 are represented in Table 6."}, {"title": "GRELATED WORK", "content": "G.1 HIERARCHICAL REPRESENTATION LEARNING ON GRAPHS\nHierarchical representation learning is crucial for graphs as it captures multi-scale structures, enabling models to discern both local and global patterns more effectively"}, {"title": "H DISCUSSION, LIMITATION AND FUTURE WORK", "content": "Conclusion. In this paper", "module.\nLimitation": "Simple Fusion Design. The fusion mechanism in the proposed UniMatch model is relatively simplistic", "investment.\nLimitation": "Underfitting on Regression Tasks. Our UniMatch model exhibits underfitting on regression tasks, indicating that it may not be capturing all the necessary features and complexities required for accurate predictions. Experimental"}]}