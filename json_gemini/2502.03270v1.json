{"title": "When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning", "authors": ["Nikolaos Tsagkas", "Andreas Sochopoulos", "Duolikun Danier", "Chris Xiaoxuan Lu", "Oisin Mac Aodha"], "abstract": "The integration of pre-trained visual representations (PVRs) into visuo-motor robot learning has emerged as a promising alternative to training visual encoders from scratch. However, PVRs face critical challenges in the context of policy learning, including temporal entanglement and an inability to generalise even in the presence of minor scene perturbations. These limitations hinder performance in tasks requiring temporal awareness and robustness to scene changes. This work identifies these shortcomings and proposes solutions to address them. First, we augment PVR features with temporal perception and a sense of task completion, effectively disentangling them in time. Second, we introduce a module that learns to selectively attend to task-relevant local features, enhancing robustness when evaluated on out-of-distribution scenes. Our experiments demonstrate significant performance improvements, particularly in PVRs trained with masking objectives, and validate the effectiveness of our enhancements in addressing PVR-specific limitations.", "sections": [{"title": "1. Introduction", "content": "Performing robust and accurate robotic manipulation from visual inputs necessitates informative and stable visual representations. The traditional paradigm for training visuo-motor policies has involved learning visual encoders from scratch alongside policy models (Levine et al., 2016). Recently, however, the adoption of pre-trained visual representations (PVRs), i.e., computer vision models trained on large and diverse visual datasets, has emerged as a compelling alternative, moving away from the tabula-rasa approach (Parisi et al., 2022). This shift is driven by three key factors: the state-of-the-art performance of PVRs in computer vision tasks, their impressive generalisation capabilities derived from training on vast datasets, and the absence of robust robot-specific foundation models capable of addressing challenges unique to robotics, such as handling diverse embodiments.\nDespite the promising results of PVRs in downstream robotic applications, including affordance-based manipulation (Li et al., 2024), semantically precise tasks (Tsagkas et al., 2024), and language-guided approaches (Shen et al., 2023), their integration into visuo-motor policy learning for even basic pick-and-place tasks remains an open challenge. Crucially, training visual encoders from-scratch or fine-tuning them with in-domain data still leads to competitive performance compared to using raw PVR features or even adapted PVRs (Sharma et al., 2023; Hansen et al., 2023). Furthermore, no single PVR, or set of characteristics, has consistently delivered optimal performance across diverse tasks and environments (Majumdar et al., 2023; Hu et al., 2023). Notably, their generalisation capabilities remain underutilised, as small scene variations can destabilise policy models (Caron et al., 2021; Xie et al., 2024), c.f. Fig. 3. This limitation has reignited interest in training models from scratch with augmented datasets (Hansen et al., 2023), a strategy that is prohibitively expensive for many real-world applications.\nWe identify critical shortcomings in the current use of PVRs for visuo-motor policy learning, rooted in the inherent nature of PVR features. First, we observe that these features are temporally entangled, primarily because widely used PVRs are designed as time-invariant models. Additionally, imitation learning datasets often consist of frame sequences where only minor changes occur in the pixel domain between adjacent timesteps. As a result, the extracted features from these frames remain highly similar, even at transition points where the corresponding actions may differ significantly. This discrepancy forces policy models to map nearly identical inputs to divergent outputs, introducing a problematic one-to-many mapping that violates the Markov property (see Fig. 1). Second, policy networks tend to overfit to"}, {"title": "2. Related Work", "content": "2.1. PVRs in Visuo-motor Policy Learning\nIn (Parisi et al., 2022), frozen PVRs were evaluated across simulated environments, outperforming models trained from scratch. Similarly, (Hu et al., 2023) showed that the utility of PVRs depends on the policy training paradigm, with behaviour cloning and inverse reinforcement learning yielding robust results, while reinforcement learning exhibited higher variability. Furthermore, (Silwal et al., 2024) provided evidence that simulation experiments (e.g., Metaworld (Yu et al., 2020) benchmark) are indicative of real world performance for PVR-based trained policies.\nThe dataset(s) used for pre-training plays a pivotal role in PVRs. While it was hypothesised that pre-training with video data featuring egocentric human-object interaction would be highly effective for learning features suitable for robot learning (due to their emphasis on object manipulation), research indicates that the diversity of images within the dataset is a more critical factor in successful robot learning (Dasari et al., 2023; Majumdar et al., 2023). Indeed, PVRs pre-trained on static datasets such as ImageNet (Ridnik et al., 2021) have demonstrated competitive performance, underscoring the importance of dataset vari-"}, {"title": "2.2. Time-informed Policy Training", "content": "In PVR-based visuo-motor policy learning, the incorporation of temporal information remains underexplored. Augmentation with temporal perception can happen either at feature level or during training time.\nFeature Augmentation. While early fusion methods, such as stacking multiple frames before encoding (Karpathy et al., 2014), are common in training visual encoders from scratch, late fusion-processing frames individually and stacking their representations (Vaswani et al., 2017) has shown superior performance with fewer encoder parameters. Recent work (Shang et al., 2021) highlights that naive feature concatenation in latent space is insufficient; instead, approaches like FLARE (Shang et al., 2021) incorporate sequential embeddings and their differences, inspired by optical flow techniques. Nevertheless, concatenating sequential embed-dings as input to policy networks has become standard in visuo-motor policy learning (Parisi et al., 2022) and state-of-the-art generative policies (Chi et al., 2023). However, a gap remains in leveraging PVR features, which are primarily designed for vision tasks, within this temporal framework.\nLoss Function Augmentation. A major limitation of many PVRs is their inherit lack of temporal perception, as most are pre-trained on static 2D image datasets. Temporal perception can be added by employing loss functions that enforce temporal consistency during training (e.g., R3M (Nair et al., 2022) and VIP (Ma et al., 2023b)), when training with video data. However, there is no clear consensus on the superiority of this approach compared to alternatives like MIM (e.g., MVP (Xiao et al., 2022; Hu et al., 2023) and VC-1 (Majumdar et al., 2023)). This disparity suggests that existing temporal modelling strategies may be insufficient in isolation.\nIn subsequent experiments, we evaluate PVRs trained with temporal information and demonstrate that methods trained with a time-agnostic paradigm achieve comparable performance. We hypothesise that this limitation arises from a lack of task-completion perception, which we address by incorporating positional encoding-a fundamental mechanism in many machine learning approaches. This straightforward operation has been instrumental in the success of Transformers (Vaswani et al., 2017), implicit spatial representations (Mildenhall et al., 2020), and diffusion models (Ho et al., 2020)."}, {"title": "2.3. Task-Relevant Feature Extraction", "content": "Downstream vision tasks often make use of the output features of PVRs. However, these features typically encode a broad range of scene information, much of which may be irrelevant to the specific task. To address this challenge, attentive probing (Chen et al., 2024; Danier et al., 2024; Bardes et al., 2024) has emerged as a popular evaluation technique, leveraging local tokens. This approach leverages"}, {"title": "3. Methodology", "content": "In this section, we present our approaches to enhance the deployment of PVRs in visuo-motor policy learning. First, we introduce the general behaviour cloning framework commonly employed in this domain (Section 3.1). Then, we describe a method to enrich features with temporal information, aimed at mitigating issues of temporal ambiguity (Section 3.2). Finally, we introduce our approach to selectively attend to task-relevant components of PVR features, improving their utility for robot learning tasks (Section 3.3)."}, {"title": "3.1. Preliminaries", "content": "Imitation Learning via Behaviour Cloning. We consider an expert policy \\(\\pi^* : P \\times O \\rightarrow A\\), which maps the current proprioceptive observation \\(p \\in P\\) of a robot manipulator and a visual observation \\(o \\in O\\) to a corresponding robot action \\(a \\in A\\). This expert policy is used to generate a dataset of expert demonstrations, \\(D_{exp}\\), consisting of N trajectories \\(T_e = \\{(p, o, a)_{t=0}^T\\}_{i=1}^N\\), where each trajectory captures the sequence of observations, and actions recorded over T timesteps while solving a task."}, {"title": "4", "content": "The goal of behavioural cloning is to learn a policy \\(\\pi_\\theta\\), parameterised by \\(\\theta\\), that closely imitates the expert policy by minimizing the discrepancy between its actions and the expert's actions. This is formulated as a supervised learning problem, where the loss function measures this discrepancy across the dataset of demonstrations:\n\\\u0395_{(p,o,a)\\sim T_e}||a - \\pi_\\theta(f_{PVR}(o), p)||^2,  (1)\nwhere \\(f_{PVR}\\) represents a PVR that acts as a feature extraction function used to process visual observations \\(o\\).\nIn robot learning, the decision-making process is commonly assumed to satisfy the Markov property. This assumption implies that the current observation \\(x_t = (p_t, o_t)\\) encapsulates all the information necessary for predicting the subsequent state, i.e., \\(P(x_{t+1}|x_t) = P(x_{t+1}|x_t, x_{t-1},...,x_0)\\). Consequently, tasks are modelled as sequences of decisions, where each action depends solely on the current state, facilitating the application of behaviour cloning under this framework."}, {"title": "3.2. Temporal Disentanglement", "content": "We find that the assumption of Markovian decision-making in policies based on features extracted from frozen PVRs is frequently invalid. This arises because, at each timestep, the available information may be insufficient for the policy to confidently map the current observation to the appropriate action.\nConsider the example presented in Fig. 1, where PVR-features of the same pick-and-place trajectory are projected with PCA into 2D. Regardless of the PVR utilised, the extracted features seem to suffer from temporal entanglement. First, features extracted from the frames where the robot has stopped to pick up the box form a tight cluster, since the only change is the movement of the gripper fingers, which corresponds to a very small percentage of pixels. Second,\nas the gripper moves down and subsequently ascends, the"}, {"title": "5", "content": "primary visual change is the cube's vertical displacement relative to the table. Consequently, the visual features extracted from the descent and ascent frames may differ only marginally, and only in dimensions affected by the small pixel region of the cube.\nFig. 1 also hints that including proprioceptive data in the policy input is not necessarily adequate to resolve these ambiguities. This is either because the high-dimensionality of visual features often dominates the lower-dimensional proprioceptive input, or because the robot may follow nearly identical trajectories while performing a task, further compounding the difficulty of disentangling visually similar states. At the same time, prior methods, where features from successive images were concatenated alone (Parisi et al., 2022) or along with their differences (Shang et al., 2021), have been effective to some extent. However, if frame-to-frame appearance differences are minimal, the resulting features can be very similar, and thus their differences predominantly contain near-zero values.\nTraining a policy network to map \\((p_t, o_t)\\) to \\(a_t\\) becomes difficult under these conditions. When multiple observations are nearly indistinguishable, the mapping violates the functional requirement that each input must map to exactly one output. To address these challenges, we propose a simple yet effective method to augment each observation with a temporal component by encoding the timestep index of each frame as a high-dimensional vector, using Eq. 2. This augmentation can temporally disentangle similar \\((p_t, o_t)\\) pairs, introducing a task progression signal into the robot state, which we argue can substantially enhance policy per-formance."}, {"title": "6", "content": "of a tabletop). Processing such extraneous information not only dilutes the policy network's focus but also leads to overfitting to specific scene conditions.\nThis observation aligns with recent work on vision model evaluation (Chen et al., 2024), which argues that only specific image regions carry the necessary information for solving a task. Building on this insight, we hypothesise that incorporating local information is particularly effective in the context of robot learning, echoing findings in PVR distillation research (Shang et al., 2024), though this area remains empirically underexplored.\nRecognizing the importance of local information is only part of the solution; a data-driven mechanism is also required to filter irrelevant details, such as background patches, and prioritise task-relevant information. To this end, we adopt the attentive probing methodology to implement Attentive Feature Aggregation (AFA). Specifically, we append a cross-attention layer to the frozen PVR, modified to include a trainable query token that interacts with the sequence of local tokens produced by the model. These tokens correspond to the per-patch embeddings for ViTs and the channel embeddings for ResNets, both from the final layer.\nFollowing Eq. 3, the query token q computes dot products with the feature sequence, with length equal to #patches and dimension dk, organized as a matrix F. These dot products are passed through a softmax function to assign weights to the contributions of each local token to the final embedding. Also, our module consists of multiple heads, so that specific dimension groups that might be irrelevant to the policy can be filtered out. Gradients are allowed to flow through the cross-attention layer, updating the parameters of q as well as the key and value projection matrices, WK and Wv."}, {"title": "3.3. Attending to Policy Related Features", "content": "We posit that training policies using the global features of PVRs (i.e., CLS token for ViTs or average channel feature for ResNets) leads to overfitting to scene conditions that are irrelevant to the task at hand. The output features of these representations often capture visual characteristics of the scene that may be irrelevant to the policy (e.g., the texture"}, {"title": "7", "content": "Additionally, most models exhibit improved out-of-distribution performance with AFA, except for CLIP and ViT-B/16. This is reasonable, as these models are trained with objectives that emphasise global frame perception, unlike other models that incorporate supervision at the patch level. Notably, MIM-trained PVRs benefit the most from AFA, reflecting the alignment between AFA's design, which is inspired by attentive probing, and the training principles of MIM-based models. These findings highlight AFA's ability to enhance policy performance, particularly in challenging out-of-distribution scenarios, and underscore its compatibility with models that leverage local feature representations.\nThe average in-domain performance remains nearly unchanged, with a slight increase from 63.1% to 66.4% when using AFA. The minor performance boost observed with AFA in in-domain scenarios, especially when compared to its substantial improvements in perturbed scenes, sug-"}, {"title": "8", "content": "gests that AFA does not learn a new latent space for the PVR that is more suited to the task. Instead, it appears to refine the use of the existing latent space by learning to effectively leverage relevant information while discarding elements that are irrelevant to the policy. This distinction underscores AFA's role as a mechanism for better utilisation of pre-trained features rather than redefining or adapting the underlying feature space.\nWe explored how to effectively utilise PVRs for visuo-motor policy learning, identifying the issues of feature temporal entanglement and lack of robustness in scene visual changes. Furthermore, we proposed two approaches to address these limitations leading to a significant performance increase. Below we provide a set of questions with answers that we believe further highlight the contributions of our work.\nQ:Does TE help because expert demos are time synced?\nA: Even though the expert demos are generated by a heuristic policy, in Fig. A3 we show that they are actually not synchronised and the only common factor is that the task is always completed by T=175 steps.\nQ:Does AFA's robustness stem from the additional trainable parameters?\nA: AFA introduces approximately 1.5M additional parameters to a policy model that originally has around 0.5M parameters. To determine whether the observed performance improvement is simply due to the increased model capacity, we train a policy network using raw PVR features but design a deeper policy network to match AFA's parameter count. The results, shown for two tasks in Fig. A6, suggest otherwise. This indicates that the performance gains arise"}, {"title": "5. Discussion", "content": "from AFA's ability to effectively attend to local information,\nas is suggested by Fig. 7.\nQ:Does the simulation's lack of visual realism affect the\nperception of PVRs?\nA: Empirical evidence has shown that simulations serve as\na reliable proxy for PVR-based visuo-motor policy learning,\nparticularly in environments like MetaWorld (Silwal et al.,\n2024). To further validate this, we replicate the visualization\nin Fig. 1 for an expert demonstration in two real-world tasks.\nThe results in Fig. A2 and Fig. A1, reveal that the same\nfeature-related issues persist, underscoring the relevance of\nour findings across both simulated and real-world settings.\nQ:Why is there still a significant performance gap under\nscene perturbations?\nA: We posit that the remaining performance gap largely\nstems from inherent limitations of behaviour cloning. Its\nsupervised nature makes the policy network vulnerable to\ncompounding errors. Despite improved feature attention,\nthe models tend to overfit to specific visual settings, making\nit sensitive to domain shifts that progressively push the\npolicy out-of-distribution.\nQ:Can we get even better policy performance?\nA: Behaviour cloning performance is largely dependent\non how well the expert policy \\(\\pi^*\\) is sampled. Increasing\nthe number of demonstrations or utilising techniques like\nDAgger (Ross et al., 2011) could help mitigate this issue.\nLimitations. We conducted extensive experiments, evalu\nating multiple PVRs and training policies across different\nseeds and representative tasks. However, our work has cer\ntain limitations. First, while MetaWorld serves as a reliable\nand widely used proxy for robot learning experiments, we\nhave not tested our methods on a real robot. Evaluating their\neffectiveness in real-world scenarios remains an important\ndirection for future research. Second, in line with common\npractice, we use a simple MLP policy network to ensure\nour approach targets the feature space and remains agnos-\ntic to policy architecture. However, it would be valuable to\nexplore how our methods impact state-of-the-art policy mod-\nels, such as diffusion-based approaches, which are known\nto struggle when conditioned on PVRs (Chi et al., 2023).\nFuture work should address these limitations by assessing\nour approach with alternative policy architectures trained\nend-to-end with visual encoders."}, {"title": "6. Conclusion", "content": "The use of PVRs for visuo-motor policy learning is still in its early stages, and we believe our work paves the way for further exploration of key challenges. Our insights contribute to the development of PVR models specifically designed for robot learning, ultimately leading to a generalist robotic system powered by large-scale vision foundation models."}]}