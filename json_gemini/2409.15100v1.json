{"title": "Robust Federated Learning Over the Air: Combating Heavy-Tailed Noise with Median Anchored Clipping", "authors": ["Jiaxing Li", "Zihan Chen", "Kai Fong Ernest Chong", "Bikramjit Das", "Tony Q. S. Quek", "Howard H. Yang"], "abstract": "Leveraging over-the-air computations for model aggregation is an effective approach to cope with the communication bottleneck in federated edge learning. By exploiting the superposition properties of multi-access channels, this approach facilitates an integrated design of communication and computation, thereby enhancing system privacy while reducing implementation costs. However, the inherent electromagnetic interference in radio channels often exhibits heavy-tailed distributions, giving rise to exceptionally strong noise in globally aggregated gradients that can significantly deteriorate the training performance. To address this issue, we propose a novel gradient clipping method, termed Median Anchored Clipping (MAC), to combat the detrimental effects of heavy-tailed noise. We also derive analytical expressions for the convergence rate of model training with analog over-the-air federated learning under MAC, which quantitatively demonstrates the effect of MAC on training performance. Extensive experimental results show that the proposed MAC algorithm effectively mitigates the impact of heavy-tailed noise, hence substantially enhancing system robustness.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated learning (FL) [1]\u2013[4] is an emerging paradigm for collaborative data processing, enabling clients to benefit from high-quality model services while safeguarding the confidentiality of their private data. Nevertheless, significant challenges persist during the execution process. The frequent transmission of model information between clients and server consumes substantial network bandwidth, while the aggregation of a large number of parameters requires extensive computing resources. Moreover, although FL avoids directly aggregating user data, the exchange of model parameters could still pose risks to user privacy, particularly through inference attacks [5].\nA viable solution to this problem is by integrating over-the-air (OTA) computation [6]\u2013[9] into the FL system, leveraging the superposition property of a multiple-access channel to automatically aggregate the clients' gradient, significantly enhancing channel utilization while concurrently reducing computational overhead [10]. Furthermore, as the server receives aggregated gradients instead of individual ones from clients [11], the vulnerability to inference attacks is significantly reduced.\nHowever, the analog channel inherently introduces electromagnetic interference during the transmission [12]\u2013[15]. While such interference enhances privacy protection, it also compromises the reliability of channel transmission, especially when it manifests as impulse interference, rendering the noise exhibiting a heavy-tailed distribution (rather than Gaussian) [16]-this has been consistently demonstrated by both theoretical [17] and empirical evidence [18]. In heavy-tailed distributions, extreme values (i.e., very large or very small values) occur with high probability, which could lead to severe signal distortion, resulting in a gradient explosion in the FL system and thereby profoundly affecting the training process of OTA FL.\nNumerous methods have been proposed to combat the impact of strong channel noise, ranging from channel inversion [19], phase correction [20], [21], to amplitude correction and energy estimation [22]. However, these methods only enhance the channel quality and fail to cope with the heavy tail phenomenon at the algorithmic level. Gradient norm clipping (GNC) has been proposed for resolving gradient explosion problem [23], and has also been used for heavy-tailed gradient distribution problems [24], but there is a crucial limitation: Once the data statistical structure of the gradient is altered by noise, GNC struggles to maintain its effectiveness.\nTo enhance the robustness of OTA FL against heavy-tailed noise, we introduce a novel residual clipping technique named median anchored clipping (MAC). This method constrains the magnitude of signals received after centralization, adjusts the proportional relationships among gradients, maximizes gradient retention, and mitigates the impact of heavy-tailed interference on OTA FL. Our main contributions are summarized as follows:\n\u2022 We propose a novel robust gradient clipping method tailored for OTA FL systems to mitigate the impact of heavy-tailed noise present in the analog channel.\n\u2022 We derive the convergence rate of OTA FL gradient descent algorithm under MAC.\n\u2022 We conduct substantial experiments where the results show that our MAC algorithm effectively mitigates the impact of heavy-tailed noise in analog OTA FL."}, {"title": "II. SYSTEM MODEL", "content": "We consider the FL system depicted in Fig. 1, consisting of an edge server and N clients. Every client n possesses a local"}, {"title": "A. Setting", "content": "dataset $D_n$ that contains $m_n$ data samples $\\{(x_i, y_i)\\}_{i=1}^{m_n}$ where $x_i \\in \\mathbb{R}^d, y_i \\in \\mathbb{R}$. We assume the local datasets are statistically independent from each other. The edge server orchestrates with the clients to learn a statistical model from their datasets while preserving privacy.\nMore precisely, the clients need to collaboratively find a vector $w \\in \\mathbb{R}^d$ that minimizes the following loss function:\n$f(w) = \\frac{1}{N}\\sum_{n=1}^N f_n(w)$\n(1)\nwhere $f_n(w)$ is the local empirical risk of agent n. The solution of (1) is commonly known as the empirical risk minimizer, denoted by $w^* = \\arg \\min f(w)$. And we adopt the OTA FL for model training in this paper."}, {"title": "B. Federated Model Training Over the Air", "content": "The general procedure of analog OTA FL is detailed in [25]. We briefly describe it in this part for completeness.\nParticularly, at the k-th round of global communication, the edge server broadcasts the global parameter $w_k$ to all the clients. Then, each client n calculates its local gradient $\\nabla f_n(w_k)$, modulates this parameter onto the magnitude of a set of common waveforms that are orthogonal to each other, and simultaneously sends the resulting analog signals to the edge server. The edge server passes the received signal to a bank of matched filters, with each branch tuned to one of the waveform bases, and outputs the automatically aggregated (but distorted) gradient. Formally, the global gradient can be written as follows:\n$g_k = \\frac{1}{N}\\sum_{n=1}^N h_{n,k}\\nabla f_n(w_k) + \\xi_k$\n(2)\nin which $h_{n,k}$ represents the channel fading of client n at the k-th global iteration, assumed to be a random variable with unit mean and finite variance, independent across the clients, and varies over time in an i.i.d. manner; $\\xi_k$ results from the electromagnetic interference, modeled as a d-dimensional random vector where each entry follows an independent symmetrical $\\alpha$-stable distribution ($S\\alpha S$) [26] (with tail index $\\alpha$ and scale parameter $\\tau$), accounting for the heavy-tailed distribution of impulse noise.\nConsequently, the global parameter is updated as\n$w_{k+1} = w_k - \\eta g_k$\n(3)\nwhere $\\eta$ is the learning rate. Then the global parameter will be broadcasted to all clients for the next round of computations."}, {"title": "C. Unstable Training Performance", "content": "Normally, the above recursion is executed multiple rounds until convergence (if it occurs), upon which all participating entities have a common model close to $w^*$. However, the spectrum is, by nature, a shared medium, giving rise to potentially strong co-channel interference, which typically manifests as noise during training. A notable feature of analog channel noise is its heavy-tail characteristic [18], which is manifested by frequent occurrence of impulse noise, which could lead to gradient explosion.\nTo that end, the main thrust of the present paper is to develop a scheme to cope with the noise introduced by analog OTA parameter aggregation so as to stabilize the training process and improve the performance of the trained model."}, {"title": "III. MEDIAN ANCHORED CLIPPING", "content": "Our MAC algorithm strengthens the robustness of OTA FL against strong communication noise by performing three key steps for the aggregated gradients: 1) centralization, 2) clipping, and 3) recovery. The primary concept behind the MAC algorithm is to determine a datum point for a set of gradient entries and, anchoring on this point, recalibrate the magnitudes of entries."}, {"title": "A. Proposed Method", "content": "To begin with, we define the vector-median as follows:\nDefinition 1: For a vector $w \\in \\mathbb{R}^d$, $med(w)$ is the median entry of all entries of w, i.e., given $w = (w_1, w_2, ..., w_d)^T$,\n$med(w) = median\\{w_i, i \\in [d]\\}$\n(4)\nwhere $[d]$ stands for the set {1,...,d}.\n1) Centralization: Given a globally aggregated gradient $g_k$, we centralize it by subtracting the vector-median from each entry, namely,\n$\\tilde{g}_k = g_k - med(g_k) \\cdot 1$\n(5)\nwhere 1 represents an all-ones vector.\nThe rationale behind centralizing the global gradient at the median is that this operation minimizes the L-1 deviation of the entries (note that due to heavy-tailed noise, the L-2 deviation of the entries may be unbounded). As such, during the subsequent clipping procedure, it preserves the original information of entries that are close to the median while eliminates the extreme values introduced by the impulse noise.\n2) Clipping: Based on the centralized gradient $\\tilde{g}_k$, we perform value clipping to each entry, thereby constraining the range of individual entries within a specified threshold C. More concretely, for a generic entry $g_{k,i}$, $i \\in [d]$, we have\n$g_{k,i} \\coloneqq sgn(g_{k,i}) \\cdot \\min(|g_{k,i}|, C)$       (6)\nwhere $sgn(\\cdot)$ takes the sign of its input variable."}, {"title": "Algorithm 1: OTA FL with MAC algorithm", "content": "3) Recovery: After clipping the centralized gradient, we add back the median to each entry as follows:\n$g_k \\coloneqq g_k + med(g_k) \\cdot 1$.        (7)\nToward this end, we obtain a new global gradient with the detrimental effects of heavy-tailed noise effectively alleviated while retaining the useful information as much as possible. The details of this method are summarized in Algorithm 1."}, {"title": "B. Convergence Analysis", "content": "To facilitate the analysis, we make the following assumptions., which are widely adopted in machine learning research.\nAssumption 1: The objective function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is $\\gamma$-strongly convex, i.e., for any $w,v \\in \\mathbb{R}^d$, it is satisfied:\n$f(w) \\geq f(v) + (\\nabla f(v), w \u2013 v) + \\frac{\\gamma}{2}||w - v||^2$. (8)\nAssumption 2: The objective function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is L-smooth, i.e., for any $w, v \\in \\mathbb{R}^d$, it is satisfied:\n$f(w) \\leq f(v) + (\\nabla f(v), w \u2013 v) + \\frac{L}{2}||w - v||^2$. (9)\nAssumption 3: The gradients of each client are bounded, i.e., for $\\forall n \\in [N]$, there exists a constant G that\n$||\\nabla f_n(w)|| \\leq G$. (10)\nAt this stage, we are ready to present the main theoretical result of this paper as the following."}, {"title": "Theorem 1: If the learning rate is set as $\\eta \\leq \\frac{1}{L}$, then Algorithm 1 converges as", "content": "$\\mathbb{E}[||w_{k+1}-w^*||^2] \\leq \\left(1 - \\eta(\\gamma P_C+(1-P_C))\\right)^k ||w_0-w^*||^2 + \\frac{\\eta}{\\gamma P_C + (1-P_C)} \\left(\\frac{\\eta \\delta (C-2G)^2 P_C + dC^2 (1-P_C)}{1-\\eta L} \\right)$\n(11)\nwhere $0 < \\delta < 1$ and $p_c$ is\n$P_C = \\mathbb{P}\\{|\\xi_{k,i}| < C - 2G\\} \\sim 1 - \\frac{\\tau \\alpha}{C^\\alpha}$.\n(12)\nRemark 1: The result in (11) demonstrates that regardless of the heavy-tail index $\\alpha$ and scale parameter $\\tau$, running OTA FL in conjunction with MAC consistently achieves a linear convergence rate (where the residual error can be reduced by decreasing the learning rate). As such, the robustness of model training is substantially enhanced, making it resilient to the detrimental effects of heavy-tailed communication noise.\nRemark 2: The effects of noise characteristics (including tail index and scale) and clipping threshold are quantified by $P_C$, which is determined within a probabilistic range. As (12) shows, these factors jointly influence the algorithm's convergence rate."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "System Setting: We examined the effectiveness of our proposed MAC algorithm by employing the GNC as a baseline and compared their training performances under the same configurations of OTA FL systems. In our experiment, Rayleigh fading with a parameter setting of $\\mu = 1$ is utilized for modeling channel fading. Unless otherwise stated, we set $N = 50$, $\\eta = 0.03$, $E = 5$, $\\alpha = 1.5$, $\\tau = 0.1$, and local batch size is 10."}, {"title": "A. Experiment Setup", "content": "Dataset and Models: We assessed performance on CIFAR-10, CIFAR-100 [27], and FEMNIST (which is processed in a non-i.i.d manner) [28] using ResNet-18, ResNet-34 [29], and CNN architectures, respectively. As the neural networks have a multi-layer structure, the MAC algorithm is executed parallel to the gradient blocks in a layer-wise manner. We also use a Dirichlet distribution with a concentration parameter of $Dir = 0.3$ [30], [31] to characterize data heterogeneity."}, {"title": "B. Performance Evaluation", "content": "In Fig. 2, we compare the performance of OTA FL under a variety of model training tasks, where the automatically aggregated but noisy global gradient undergoes MAC, GNC, and no post-processing (which we dub as noisy transmission), respectively. We also display the ideal scenario without any channel corruption (including fading and noise) as the performance upperbound. This figure shows that the training process is severely compromised by heavy-tailed noise, leading to fluctuations in the test accuracy curve and a marked decrease"}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a new algorithm, named MAC, which significantly enhances the robustness of OTA FL systems against channel noise that often exhibits heavy-tailed distributions. MAC leverages the median of the global gradient entries as a datum plane, and applies value clipping to truncate the extreme values induced by impulse noise, thereby effectively alleviating the detrimental effects of channel noise while largely preserving the original gradient information.\nWe validated the effectiveness of MAC through convergence analysis and a set of empirical experiments, in which the MAC algorithm demonstrated consistent stability under various noise conditions. The MAC algorithm is effective, low-complex, and straightforward to be implemented, rendering it highly applicable in practical scenarios."}]}