{"title": "Rule-Bottleneck Reinforcement Learning: Joint Explanation and Decision\nOptimization for Resource Allocation with Language Agents", "authors": ["Mauricio Tec", "Guojun Xiong", "Haichuan Wang", "Francesca Dominici", "Milind Tambe"], "abstract": "Deep Reinforcement Learning (RL) is remarkably\neffective in addressing sequential resource allo-\ncation problems in domains such as healthcare,\npublic policy, and resource management. How-\never, deep RL policies often lack transparency and\nadaptability, challenging their deployment along-\nside human decision-makers. In contrast, Lan-\nguage Agents, powered by large language mod-\nels (LLMs), provide human-understandable rea-\nsoning but may struggle with effective decision\nmaking. To bridge this gap, we propose Rule-\nBottleneck Reinforcement Learning (RBRL), a\nnovel framework that jointly optimizes decision\nand explanations. At each step, RBRL generates\ncandidate rules with an LLM, selects among them\nusing an attention-based RL policy, and deter-\nmines the environment action with an explana-\ntion via chain-of-thought reasoning. The RL rule\nselection is optimized using the environment re-\nwards and an explainability metric judged by the\nLLM. Evaluations in real-world scenarios high-\nlight RBRL's competitive performance with deep\nRL and efficiency gains over LLM fine-tuning. A\nsurvey further confirms the enhanced quality of\nits explanations.", "sections": [{"title": "1. Introduction", "content": "Sequential resource allocation is a fundamental problem in\nmany domains, including healthcare, finance, and public pol-\nicy (Considine et al., 2025; Boehmer et al., 2024; Yu et al.,\n2024). This task involves allocating limited resources over\ntime while accounting for dynamic changes and competing\ndemands. Deep reinforcement learning (RL) is an effective\nmethod to optimize decision-making for such challenges,\noffering efficient and scalable policies (Yu et al., 2021; Ta-\nlaat, 2022; Xiong et al., 2023; Zhao et al., 2024). However,\ndeep RL policies generally provide action recommendations\nwithout human-readable reasoning and explanations. Such\nlack of interpretability poses a major challenge in critical\ndomains where decisions must be transparent, justifiable,\nand in line with human decision-makers to ensure trust and\ncompliance with ethical and regulatory standards.\nFor example, doctors may need to decide whether to prior-\nitize intervention for Patient A or Patient B based on their\ncurrent vital signs (Boehmer et al., 2024). An RL algorithm\nmight suggest: \u201cIntervene with Patient A \" with the implicit\ngoal of maximizing the value function. However, the un-\nderlying reasoning may not be clear to the doctors, leaving\nthem uncertain about the factors influencing the decision\n(Milani et al., 2024). For doctors, a more effective sug-\ngestion could be risk-based with specific information, e.g.,\n\"Patient A's vital signs are likely to deteriorate leading to\nhigher potential risk compared to Patient B, so intervention\nwith Patient A is prioritized\u201d (Gebrael et al., 2023; Boatin\net al., 2021).\nLanguage agents (Sumers et al., 2024) leverage large lan-\nguage models (LLMs) for multi-step decision-making using\nreasoning techniques like chain of thought (CoT) (Wei et al.,\n2022) and ReAct (Yao et al., 2023). They enable natural\nlanguage goal specification (Du et al., 2023) and enhance\nhuman understanding (Hu & Sadigh, 2023; Srivastava et al.,\n2024). However, LLMs struggle with complex sequential\ndecision-making, such as resource allocation (Furuta et al.,\n2024), making RL a crucial tool for refining them into ef-\nfective policy networks (Carta et al., 2023; Tan et al., 2024;\nWen et al., 2024; Zhai et al., 2024). Yet, fine-tuning LLMs\nfor policy learning is highly challenging due to the substan-\ntial computational costs and the complexity of token-level\noptimization (Rashid et al., 2024), which remains an open\nchallenge, particularly in sequential resource allocation.\nConsequently, aiming to combine the strengths of both deep\nRL and language agents, we pose the following question:"}, {"title": "2. Related Work", "content": "Our work intersects with three distinct areas within the RL\nliterature. We discuss related work in each of these domains.\nRL for Resource Allocation RL has been widely stud-\nied for constrained resource allocation across domains. In\nmaternal health, Boehmer et al. (2024) apply RL to a rest-\nless multiarmed bandit (RMAB) problem (Whittle, 1988)\nto compute patient-specific intervention probabilities. Also\nin an RMAB setting, Xiong et al. (2022) propose a model-\nbased RL approach that prioritizes users via an index and al-\nlocates resources under budget constraints. In public health,\nConsidine et al. (2025) propose RL to optimize extreme\nheat warnings under a budget on the number of possible\nalerts. Other works include multi-agent RL for robotic\nwarehouse allocation (Shen et al., 2023) and exogenous"}, {"title": "3. Preliminaries and Problem Formulation", "content": "Notations. A bold lowercase letter $a \\in \\mathbb{R}^d$ represents a\nvector with dimension d, while a regular lowercase letter\na denotes a scalar. An uppercase letter A denotes a map-\nping function, and a calligraphic letter A denotes a set; [h]\ndenotes the set of {1, . . ., h}; \u0394(A) denotes the space of\nprobability distributions supported in A.\n3.1. Resource-Constrained Allocation\nResource-constrained allocation tasks are usually formu-\nlated as a special Constrained Markov Decision Process\n(CMDP), which is defined by the tuple (S, A, P, R, C, h, \u03b3),\nwhere S denotes a state space and A denotes a finite action\nspace. The transition probability function, specifying the\nprobability of transitioning to state $s' \\in \\mathbb{R}^{d_1}$ after taking ac-\ntion $a \\in \\mathbb{R}^{d_2}$ in state s, is $P(s'|s, a) : S \\times A \\times S \\rightarrow \\triangle(S)$,\n$R(s, a) : S \\times A \\rightarrow R$ represents the reward function, defin-\ning the immediate reward received after taking action a in\nstate s, and we let $C'(s, a) : S \\times A \\rightarrow \\mathbb{R}^{d_3}$ be the immediate\ncost incurred after taking action a in state s. Often, each di-\nmension i \u2208 [d2] in a is either 0 or 1 in resource-constrained\nallocation tasks. In addition, h is the time horizon and\n\u03b3\u2208 [0, 1] denotes the discount factor, which determines the\npresent value of future rewards.\nThe goal of resource-constrained allocation tasks is to find a\npolicy \u03c0: S \u2192 \u25b3(A) that maximizes the expected cumula-\ntive discounted reward while satisfying the cost constraints:\n$\\pi^* = \\arg \\max_\\pi E[\\sum_{t=1}^{h} \\gamma^{t-1}R(s_t,a_t)]$\ns.t. \u2200t \u2208 [h]: C'(st, at) \u2264 B(st),\n(1)\nwhere B: S \u2192 $R^{d_3}$ is the budget function.\n3.2. Language-augmented RL with Rules\nWe outline the language-augmented RL framework, where\nthe action space includes internal language actions \u0100 =\nA\u222aL (Yao et al., 2023; Carta et al., 2023). Language"}, {"title": "3.3. Problem Statement", "content": "Our primary challenge is to enable LLMs to jointly optimize\na language policy that both solves the underlying optimiza-\ntion problem and enhances the quality of the explanations-\na task that has received little attention in the literature. We\naim to increase the quality of lexpl while also optimizing\ndecision-making. We aim to achieve this by selecting rules\nthat encourage both good quality explanations and high\nreward. In Section 4, we will describe our method for con-\nstructing a surrogate explainabilty \u201crule reward\u201d Rule(arule)\nusing an LLM as judge (Shen et al., 2024; Bhattacharjee\net al., 2024; Gu et al., 2024). We denote the joint environ-\nment/rule reward as \u00eft = R(st, av) + Rule(arule). Then,\nwe propose the following augmented optimization objective:\nh\nmax E[J(\u03c0):=\u2211trt s.t. constraint in (1). (6)\nt=1\n\u03c0"}, {"title": "4. Rule-based Reinforcement Learning (RBRL)", "content": "RBRL leverages the strengths of LLMs and RL to achieve\nboth interpretability and robust sequential decision-making.\nAlgorithm Overview The RBRL framework in ALgo-\nrithm 1 involves four steps: (1) RULE SET GENERATION\n(line 3), the LLM processes the state-task pt to create can-\ndidate rules Rt for action selection; (2) RULE SELECTION\n(line 4), an attention-based RL policy \u03c0\u03b8 selects the best rule\narule \u2208 R; (3) DECISION, RULE REWARD AND EXPLANA-\nTION (lines 5-8), the LLM generate an environment action\nanv and based on the chosen rule aule and gives a human-\nreadable explanation lexpl; (4) REINFORCEMENT LEARN-\nING (lines 10-12), collected samples update the policy \u03c0\u03b8\nand value networks with the SAC algorithm (Haarnoja et al.,\n2018) and the combined environment and rule reward \u00eft.\nAlgorithm 1 details the entire process. Further sections\nelaborate on these steps."}, {"title": "4.1. Rule Set Generation", "content": "The rule generation process seeks to to create interpretable\nand actionable guidelines for decision-making. Under this\nframework, a set of candidate rules Rt is generated ac-\ncording to Rt\nALLM(Rt pt, a thought). To enhance in-\nterpretability, each rule is accompanied by a rationale ex-\nplaining the reasoning behind the decision. The LLM is\ninstructed to generate rules as a JSON format, which is com-\nmon for integration of LLMs with downstream applications\n(Together AI, 2024). Examples of generated rules are given\nin Figure 2b. See Figure 10 in the Appendix for the prompt\ntemplates used to generate the rules.\nA higher number of rules is always preferable, however, it\nalso increases the generating costs and slows down each\niteration of the method. For our experiments, we found that\n|Rt = 5 provided achieved a reasonable trade-off."}, {"title": "4.2. Rule Selection", "content": "In this step, rules are converted from text to vector form,\nand a trainable attention-based policy network \u03c0\u03bf provides\nthe probability distribution for sampling a rule. Figure 3 il-\nlustrates the process, with a detailed procedure in Algorithm\n2 of the Appendix. Below are the major components of\nthe architecture of \u03c0\u03bf. We propose to base the architecture\non cross-attention layers (Bahdanau et al., 2015; Vaswani\net al., 2017), with the state acting as the keys and values,\nand the rules as the queries. This allows to learn from the\nembedding representations of rules, and efficiently handle\ndynamically changing number of rules if needed.\nState Representation. The numeric state is projected by a\nlinear layer: $k_t = Linear(s_t) \\in \\mathbb{R}^{d_h}$, with dh being to\ndenote the architecture hidden dimension.\nRule Candidate Embedding. Each rule in the list of rule can-\ndidates Rt = {p\u2081, p,..., p} is embedded into a numeric\nrepresentation using a Sentence Embedding language model\n(e.g., SentenceBERT (Reimers & Gurevych, 2019)) and\nfurther processed by a projection layer similar to the state\nrepresentation. This results in a query matrix Qt \u2208 Rqxdn.\nAttention-based Policy Network. The vector kt, serving\nas keys, engages with the rule embeddings Qt, acting as\nqueries, via a cross-attention mechanism to derive a hidden\nstate representation $h_t^{(1)} = Attention(Q_t, k_t,k_t) \\in$\n$R^{q \\times d_h}$, computed as\n$Attention(Q_t, k_t, k_t) = softmax(\\frac{Q_tk_t^T}{\\sqrt{d_h}})k_t$,\nwhich facilitates the rule candidate vector embeddings in\nattending to the environment state. Subsequently, we se-\nquentially apply self-attention layers to the hidden represen-\ntation $h_t^{(k+1)}$\nAttention(h, h, h), enabling"}, {"title": "4.3. Decision, Rule Reward, and Explanation", "content": "Upon selection of rule aule, the LLM determines the\naction to be applied within the environment anv\nALLM(anvarule, a thought, pt), ensuring concordance with the\nchosen strategy. Subsequently, the LLM formulates a post-\nhoc explanation lexpl~ LLM (lexpl|aenv, arule, a thought, pt)\ncontingent upon the rule. Figure 10 in the Appendix illus-\ntrates the prompt template employed to generate both the\naction and explanation.\nThis procedure concurrently produces the rule reward\nRule(rule), used for reinforcement learning (RL) in the\nnext step. This rewards is derived from using the LLM as a\njudge to answer the following three questions:\nER1. Without providing any, is aule sufficient to pre-\ndict/infer the optimal action?\nER2. Does a rule contain enough details about the applicabil-\nity of the rule to the current state?"}, {"title": "4.4. RL with SAC", "content": "Augmented state space Traditional RL frameworks fail\nhere due to intermediate steps: generating the rule set\nRt, mapping rules aule to actions anv in an LLM-driven\nenvironment. RBRL addresses this issue by creating an\naugmented state \u0161t := (st, Rt) with transition dynamics\nP(St+1 St, aule), integrating rules into the state space for\nreasoning over both the environment's dynamics and de-\ncision rules aule. This proposition explains the transition\ncomputation.\nTheorem 4.1. The state transition of the RBRL MDP can\nbe calculated as\nP(St+1 St, aule) = P(Rt+1|St+1)\u00d7\n/ a\nP(St+1 aenv, St) \u00b7 P(aenvarule, St)daenv, (7)\nwhere P(Rt+1 St+1) = \u03c0LLM(Rt+1|Pt,Tt) is the proba-\nbility of the LLM generating rule set Rt+1 provided the\nstate St+1, P(St+1|aenv, st) is the original environment dy-\nnamics, and P(aenv|aule, St) = LLM(aenv|pt, aule) is the\nprobability of the LLM selecting the environment action aenv.\nSAC step The attention-based policy network in Section\n4.2 is optimized using the SAC algorithm, which balances\nreward maximization with exploration by incorporating an\nentropy term in the objective function. The SAC update\nprocess is outlined as follows. First, we define an auxiliary"}, {"title": "5. Experiments & Human Survey", "content": "In this section, we evaluate RBRL and empirically show\nthat it can achieve a joint improvement in both reward and\nexplainability over comparable baselines. We briefly sum-\nmarize these environments here, with additional details in\nAppendix B.1.\nDomains We evaluate RBRL in three environments per-\ntaining to the following two distinct domains:\n\u2022 WearableDeviceAssignment: We use two envi-\nronments, Uganda and MimicIII, from the vital sign\nmonitoring domain introduced by Boehmer et al. (2024),\nmodeling the allocation of limited wireless devices among\npostpartum mothers as an restless multi-armed bandit prob-\nlem (RMAB). Each mother (\u201carm\") is represented by a state\nof vital signs (e.g., heart rate, respiratory rate, SPO2) and\ntheir variability. Action vector elements are binary: as-\nsigning a device to a mother or not. The aim is to prioritize\nhigh-risk patients since there are only a limited number of B\ndevices. Rewards penalize abnormal vital signs, with moni-\ntoring enabling health-improving interventions. Boehmer\net al. (2024) suggests to recast the RMAB problem as an\nMDP, deciding from which patient to remove a device when\na new patient arrives. A penalty is incurred when removing\nthe device from an active patient if there are free devices.\n\u2022 HeatAlerts: We use the weather2alert environ-\nment from Considine et al. (2025), which formulates issuing\nheat alerts as a constrained MDP. The action is binary, sub-\nject to a constraint on the total number of alerts that can\nbe issued. The state includes the heat index, the remaining\nbudget, and a history of previous alerts. Actions at are bi-\nnary and represent whether to issue an alert. Rewards are\nreductions in hospitalizations from the alert. A penalty is\nincurred when selecting to issue an alert when on budget.\""}, {"title": "5.1. Environment Reward Optimization", "content": "We discuss the main results and refer to Appendix B for the\ndetailed experiment setup. Unless otherwise specified, we\nuse gpt-40-mini (OpenAI, 2024b) as LLM due to its\nreasonable cost and high performance. All baselines were\ntrained on 3 seeds for 2000 environment steps each.\nQ1. Did RBRL optimize the reward function? RBRL\nis compared to Chain-of-thought (CoT) (Wei et al., 2022)\nfor language reasoning and Proximal Policy Optimization\n(PPO) (Schulman et al., 2017) for numeric states. Figure 4\nindicates RBRL outperforms CoT, showing RL-optimized\nrule selection improves decision-making. The results show\nthat RBRL exceeds PPO in all environments with equivalent\nenvironment steps, suggesting a higher performance when\nlearning in an online setting. For completeness, we also\ncompare against PPO trained at 100k steps, 50 times more.\nFor the HeatAlerts environment, RBRL exceeds such\nasymptotic performance, consistently with the findings of\nConsidine et al. (2025) noting that PPO and other standard\nRL algorithms struggle to handle the constraints and ex-\nploration in this domain. For the Uganda and MimicIII\nenvironments, we observe the trend of RBRL getting closer\nto 100k PPO, but not reaching the performance. As high-\nlighted in Section 2, various works have remarked that the\ntrade-off between interpretability and performance can be\njustified to increase system trust, robustness, and adaptabil-\nity in high-stakes environments.\nQ2. Did structured rules help optimization? We con-\nduct two ablation studies on structured rules. First, we\nbenchmark the use of structured rules without RL, called\nbaseline RulesLLMOnly, which is shown in Equation (2)-\n(5). Next, we compare RBRL with a variant optimizing\nunstructured thoughts, termed thoughts-based RL (TBRL).\nThe implementation mimics RBRL, utilizing a candidate\npool R with the CoT prompt. Results in Figure 5a show\nthat comparing RBRL with RulesLLMOnly highlights RL\ntraining gains, suggesting rule generation alone does not\nexplain RBRL's performance. Additionally, significant im-\nprovements over TBRL suggest optimizing structured rules\nis more effective than optimizing free reasoning traces.\nQ3. How does RBRL compare to token-level LLM finetun-\ning with RL? We implement LLM finetuning from the\nenvironment reward to a Llama 3.1 8B model (Meta AI,\n2024), termed FinetunePPO. We train a value head on\nthe last hidden states and use KL divergence from the refer-\nence model as a regularization reward (Ziegler et al., 2019).\nA simple CoT generation is used, followed by a call for\naction question, optimizing the full CoT and action trace.\nWe train 3 seeds for 18 hours on an Nvidia A100 40G GPU,\nachieving 1200 steps per seed. For compatibility, we also\ntrain RBRL on the same Llama 3.1 8B model. Figure 5b\ncompares results, showing a positive but relatively flat trend"}, {"title": "5.2. Human Survey and Explainability", "content": "Q4. Did RBRL increase the explainability of post-hoc expla- \nnations? A survey with 40 participants was conducted \nto assess explanation quality, detailed in Appendix E. Each \nprompt included the task, state, and action space as orig- \ninally given to the LLM, followed by actions and expla- \nnations from the CoT agent and the RBRL agent, without \ndisclosing agent types. Participants were asked to choose \npreference for explanation A, B, or none. Prompts were \nsplit between Wearable Device Assignment and Heat Alerts \ndomains. Figure 6 shows results, favoring RBRL's explana- \ntions in both domains, with a detailed breakdown in E. An \nadditional experiment with an LLM judge (Gu et al., 2024)"}, {"title": "6. Conclusion", "content": "The RBRL framework takes an important step towards ad- \ndressing the critical challenge of balancing decision efficacy \nand interpretability in constrained allocation tasks. By syner- \ngizing the generative capabilities of LLMs with the optimiza- \ntion power of RL, RBRL introduces a novel paradigm where \nstructured rules, generated dynamically by LLMs, guide \nRL policies to make transparent and actionable decisions. \nEvaluations across healthcare and public health domains \ndemonstrate RBRL 's competitive performance against tradi- \ntional RL baselines while significantly enhancing the quality \nof human-understandable explanations. Human and LLM- \njudged surveys further validate that RBRL's explanations \nimprove trust and clarity, contributing towards addressing \nthe longstanding barrier to deploying AI in high-stakes sce- \nnarios."}, {"title": "Impact and Ethics Statement", "content": "This work advances the development of transparent AI sys- \ntems for high-stakes decision-making in domains like health- \ncare and public policy. By enabling RL agents to generate \nhuman-readable rules and explanations, RBRL improves \ntrust and accountability, critical for ethical deployment in \nsettings where lives and resources are at stake. \nWhile the framework prioritizes alignment with human rea- \nsoning, potential risks include over-reliance on imperfect \nLLM-generated rules or explanations that may inadver- \ntently obscure biases in training data. Mitigation requires \nrigorous validation of rules by domain experts and ongo- \ning monitoring of LLM outputs. Additionally, RBRL 's \nreliance on LLMs raises computational and accessibility \nchallenges in resource-constrained environments. By ad- \ndressing these considerations, this research contributes to \nsafer, more equitable AI systems that empower-rather than \nreplace-human decision-makers. \nNotice that the Uganda and Heat Alerts datasets used in this \nstudy is derived from a simulator provided by Boehmer et al. \n(2024) and Considine et al. (2025). These simulators do"}, {"title": "A. Algorithmic and Mathematical Details", "content": "A.1. Algorithms\nIn this subsection, we present the detailed pseudocodes for Rule_Search in Algorithm 2 and the SAC for attention-based\npolicy network in Algorithm 3."}, {"title": "A.2. Proof of Theorem 4.1", "content": "In this section, we provide the detailed proofs for Theorem 4.1. We start with the following equation\nP(Rt+1 St+1)\na\n=\n \u222b\nP(St+1 aenv, St) \u00b7 P(aenvarule, st) daenv\nP(Rt+1 St+1)\u22c5\n \u222b\na\nP(St+1 aenv 1|aenv, St, Rt, aule) \u00b7 P(aenv|Rt, aule, St)daenv\nP(Rt+1 St+1)\u22c5\n \u222b\na\nP(St+1 aenv, St, Rt, aule) \u00b7 P(aenv|st, Rt, arule)daenv\nP(A|B,C).P(B|C)=P(A,B|C)\nP(Rt+1 St+1,St, Rt,aule)=P(Rt+1 St+1)\nP(A|B,C).P(B|C)=P(A,B|C)\nP(Rt+1 St+1)\u22c5\n \u222b\n P(St+1|St, Rt, aule)\nP(Rt+1|St+1, St, Rt, aule) \u00b7 P(St+1|St, Rt, aule)\nP(st+1, Rt+1|St, Rt, aule)\nP(\u0161t+1 St, aule),\n \u0161t:=(st, Rt)\n(12)\nwhere (a) follows from the fact that the transition to st+1 is fully determined by current state st and current action to the\nenvironment anv, i.e., independent on rule set Rt and selected rule aule; (b) holds since any is determined only by the\nselected rule aule and the state st; (c) is due to our designed rule generation procedure where Rt+1 is generated by the LLM\nfrom the the latest state st+1. This completes the proof."}, {"title": "B. Experiment Setup", "content": "B.1. Environments Details\nB.1.1. WEARABLE DEVICE ASSIGNMENT DOMAIN\nThe simulator for the Uganda domain is adapted from (Boehmer et al., 2024) with minor modifications to simplify the\nproblem. This section provides an overview of the environment, with additional details available in the original paper. In\nthis environment, they want to allocate vital sign monitoring devices to mothers arriving in a maternal unit in order to better\nmonitor mothers' health condition. Each mother's state is modeled by her vital signs (heart rate, respiratory rate, and blood\noxygen saturation) along with each vital sign's variability. The mother's vital sign transition is governed by a multivariate\nGaussian distribution defined over her vital signs at the current timestep and next timestep, learned from de-identified\nvital sign data collected from patients at Mbarara Regional Referral Hospital. MIMIC-III (Johnson et al., 2016) is another\nde-identified clinical vital sign dataset that includes the same set of vital signs as the Uganda domain. The key difference is\nthey have different data sources, as MIMIC-III's data comes from Beth Israel Deaconess Medical Center in Boston.\nWearing a monitoring device does not directly alter a mother's vital sign trajectory but has an indirect positive effect by\ntriggering alerts when vital signs deviate from the normal range. These alerts often lead to medical interventions that\nimprove the mother's condition. If no monitoring device is assigned (passive action), the mother's next state is sampled\nfrom the multivariate Gaussian distribution conditioned on the current state. If a monitoring device is assigned and the vital\nsigns remain within the normal range, the vital signs evolve as under the passive action. However, if any vital sign deviates\nfrom the normal range, there is a 30% chance the vital signs evolve as under the passive action, based on empirical evidence\nsuggesting that clinicians fail to respond in such cases 30% of the time (Boatin et al., 2021). Otherwise, vital signs are\nprobabilistically adjusted towards the normal range before sampling the next state, modeling the positive impact of medical\nintervention.\nThe algorithm's goal is to optimize monitoring device allocation to maximize the aggregate reward across all mothers. We\nsimplify the problem by requiring exactly one new mother to join the maternal unit at each timestep, starting with a single\nmother in the unit. The system has a budget of five monitoring devices. A device must be allocated to the new mother, and if\nall devices are already in use, one must be removed from a current user. Once removed, a device cannot be reassigned to the\nsame mother. Each mother remains in the maternal unit for 10 timesteps, after which her vital sign trajectory no longer\ncontributes to the reward. Once a device is taken from a mother, we directly sample her entire vital sign trajectory under\npassive action for the remaining timesteps she stays in the maternal unit and compute all her future rewards. We can directly\ncompute future rewards because the mother will not receive the device again, so she will only undergo passive action in the\nremaining time. This observation enables us to maintain a smaller observation space, as we only need to keep track of the\nstates of the mothers who own the device.\nIn this domain, the constraints can be written as $||a_t \\in \\mathbb{R}^{d_2}||_1 < B,\\forall t$, which d2 represents the number of patients in the\nsystem at each time slot, and the 1-norm of the action vector must remain within the budget B.\nB.1.2. HEAT ALERTS DOMAIN\nThe heat alert issuance problem can be modeled as an MDP in the context of RL (Considine et al., 2025). The state at any\ngiven time, denoted as st, encompasses both exogenous and endogenous factors. Exogenous factors include current weather\nconditions, such as the heat index, temperature, and humidity, which directly influence the risk of heat-related health issues.\nEndogenous factors include the history of issued alerts, such as the number and timing of past alerts, their effectiveness, and\nthe remaining budget for the season. Additionally, the day of the week is considered, as public responsiveness to alerts may\nvary between weekdays and weekends. The action space is binary, with at \u2208 Z2. The decision to issue a heatwave alert\nat = 1 or not at = 0 is constrained by the remaining alert budget. If the budget is exhausted, no further alerts can be issued.\nThe reward function is designed to reflect the reduction in heat-related hospitalizations, which depends on the effectiveness\nof the alert under current weather conditions. A Bayesian hierarchical framework could be employed to model the health\nimpact of alerts, capturing the uncertainty in their effectiveness. Importantly, consecutive alerts tend to lose effectiveness,\nintroducing a diminishing returns effect that must be accounted for in the decision-making process.\nThe transition dynamics, P(st+1|st, at), describe how the system evolves over time. The next state is influenced by weather\ntrajectories, the action taken, and public responsiveness to alerts. For instance, issuing an alert reduces the remaining budget\nand updates the history of issued alerts, while the weather conditions may change independently. Public responsiveness\nmay also vary based on the frequency and timing of past alerts. A key constraint in this problem is the limited alert budget,"}, {"title": "B.2. Gym environments an Language Wrappers", "content": "We implemented the WearableDevicesAssignment environments as gymnasium environments (Towers et al., 2024), while the HeatAlerts domain was already available in this format. We additionally created a LanguageWrapper Python class described in Table 1, which can be applied to any gymnasium environment. Our code implementations can be applied to any environment wrapped in this class."}, {"title": "B.3. RL implementations, hyperparameters and Settings", "content": "We implemented three main RL algorithms for the experiment sections: Attention-based SAC for RBRL, numeric PPO, and Finetuning-based PPO. We based our implementation on the single-file, high-quality implementations from the cleanrl project (Huang et al., 2022). For Attention-based SAC, we required significant changes to keep track of the rule-augmented state space, as described in Section 4.4. Other major changes to the baseline SAC implementation (originally designed for Atari) were more frequent target network updates and updating the actor and critic four times per iteration. This was done to improve sample efficiency and cope with the slow generation by the LLM. Numeric PPO was used practically without modification.\nFor the Finetuning-based PPO, we used low-rank adaptation (LoRA) (Hu et al., 2021) with the Transformers package and models hosted on Llama Hugging Face (Wolf et al., 2020). We set the rank to r = 1 and the adaptation weight to 2, resulting in only 0.8% trainable parameters (still an order of magnitude larger than the Attention-based policy)."}, {"title": "B.4. Computing environment", "content": "SAC attention can run on a regular laptop since most of the computation happens in the cloud through API LLM calls, while the RL module is small and can run on personal CPUs. Nonetheless, the process is bottlenecked by the speed of generation from the APIs. A full run of 2 million environment steps, with parallelized API calls across four environments, took approximately four hours to complete. One training cycle did not exceed $10 in API costs. However, all the experiments and development incurred approximately $1,500 in API costs. As described in the main text, the LLM fine-tuning experiments used an Nvidia A100 40GB GPU for each seed, training on three seeds for 18 hours each. Computations were performed on a Slurm-based high-performance computing cluster."}, {"title": "C. Additional Survey Results", "content": "Figure 7 illustrates the results of a human survey conducted to evaluate the quality of explanations generated by our method compared to alternatives. A total of 21 valid responses were collected for the HeatAlert environment (Figure 7a), and 20 valid responses were gathered for the Uganda environment (Figure 7b). As shown in the figures, our method was favored by the majority of participants across all cases. In the HeatAlert environment, the preference for our approach is evident, although there is a small percentage of tied and \u201cNot Preferred\" responses. In contrast, the preference for our method is even more pronounced in the Uganda environment, with a significantly higher number of participants selecting \"Ours Preferred.\u201d These results demonstrate the effectiveness of our approach in generating explanations that"}]}