{"title": "Stein Variational Evolution Strategies", "authors": ["Cornelius V. Braun", "Robert T. Lange", "Marc Toussaint"], "abstract": "Stein Variational Gradient Descent (SVGD) is a highly efficient method to sample from an unnormalized probability distribution. However, the SVGD update relies on gradients of the log-density, which may not always be available. Existing gradient-free versions of SVGD make use of simple Monte Carlo approximations or gradients from surrogate distributions, both with limitations. To improve gradient-free Stein variational inference, we combine SVGD steps with evolution strategy (ES) updates. Our results demonstrate that the resulting algorithm generates high-quality samples from unnormalized target densities without requiring gradient information. Compared to prior gradient-free SVGD methods, we find that the integration of the ES update in SVGD significantly improves the performance on multiple challenging benchmark problems.", "sections": [{"title": "1 Introduction", "content": "We consider the task of minimizing a function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$. Since most real world problems - such as neural network parameter search - involve highly non-convex functions, the optimization process is very sensitive to its initialization (Sullivan et al., 2022; Li et al., 2018). To address this challenge, a common approach is to generate multiple candidate solutions and select the best one (Hern\u00e1ndez-Lobato et al., 2014; Toussaint et al., 2024; Parker-Holder et al., 2020). This allows to cast the optimization task into an approximate inference problem which can be formulated as follows:\n$\\min_q D(q || p)$\n$p(x) = \\frac{e^{-f(x)}}{Z}$,\nwhere the normalization constant $Z = \\int_{\\mathbb{R}^d} e^{-f(x)}dx$ is typically untractable, p and q are probability distributions supported on $\\mathbb{R}^d$, and D is a suitable divergence, such as the Kullback-Leibler (KL) divergence.\nStein Variational Gradient Descent (SVGD) is a powerful method to approximate such a target distribution through a set of particles that is iteratively updated using gradient steps (Liu and Wang, 2016). This procedure makes SVGD a very sample efficient and flexible method that does not require long burn-in periods, nor makes assumptions about the family of the target distribution. Due to these properties, SVGD is a popular and powerful method for sampling and optimization (Lambert et al., 2021; Power and Berenson, 2024; Lee et al., 2023; Zhang et al., 2019).\nHowever, in many real-world problems - such as robotics and chemistry - the energy function f is non-differentiable (Lambert et al., 2021; Englert and Toussaint, 2018; Maus et al., 2023). This non-differentiability poses a challenge for SVGD due to its reliance on the gradient of $\\log p(x) \\propto f(x)$. To facilitate Stein variational inference in these scenarios, prior work introduced a gradient-free version of SVGD that uses analytical gradients from a surrogate distribution that is fitted to the particle set at each step (Han and Liu, 2018, GF-SVGD). While this method comes with theoretical guarantees, fitting the surrogate to the objective function is challenging in practice (cf. Fig."}, {"title": "2 Preliminaries", "content": "2). To circumvent these issues, other works instead used simple Monte Carlo (MC) gradients in the SVGD update (Liu et al., 2017; Lambert et al., 2021; Lee et al., 2023). However, the MC gradient estimate has high variance, which can lead to slow convergence on real-world problems.\nTo address the aforementioned shortcomings of existing gradient-free SVGD methods, we propose a novel approach, Stein Variational Evolution Strategies (SV-ES) that bridges the domains of Evolution Strategies (ES) and distribution approximation. The idea of SV-ES is to replace the score term in the SVGD update by the search distribution mean update of an ES in order to improve gradient-free SVGD. We motivate our work based on prior results that established ES as a competitive alternative to gradient-based optimization algorithms, achieving higher performance and robustness on difficult objectives due to their inherent exploration capabilities (Salimans et al., 2017; Wierstra et al., 2014). In particular, the Covariance Matrix Adaptation Evolution Strategy (Hansen and Ostermeier, 2001, CMA-ES) is one of the most popular ES across many domains (Hansen and Kern, 2004; Hansen et al., 2010; Jankowski et al., 2023), due to its adaptive and efficient search process, which leverages natural gradient-like updates of its search distribution (Tian et al., 2021; Akimoto et al., 2012). Thus, we present Stein Variational CMA-ES (SV-CMA-ES) as a particular instance of SV-ES in this paper.\nOur method, SV-CMA-ES, replaces the score term in the SVGD particle update by the search distribution mean update of CMA-ES. This means that our approach represents each SVGD particle by a separate ES search distribution, which is updated using the fitness values of all samples from the respective subpopulation and the estimated update steps of all other particles (cf. Figure 1). We validate the effectiveness of our proposed approach on challenging problems from multiple domains such as robot trajectory optimization and reinforcement learning. Our experimental results demonstrate that SV-CMA-ES improves considerably over prior gradient-free SVGD approaches. In particular, we show that our algorithm can be used to generate samples from challenging densities efficiently, as well as a blackbox optimizer of the underlying energy functions. We summarize our contributions as follows:"}, {"title": "2.1 Stein Variational Gradient Descent", "content": "Stein Variational Gradient Descent (Liu and Wang, 2016, SVGD) is a non-parametric inference algorithm that approximates a target distribution with a set of particles $X = \\{x_i\\}_{i=1}^n$ as $q(x) = \\sum_{x_i \\in X} \\delta(x - x_i)/n$, where we use $\\delta(.)$ to denote the Dirac delta function. Given an initial set of particles, the goal is to determine an optimal particle transformation $\\phi^* : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ that maximally decreases the KL divergence $D_{KL}(q || p)$:\n$x_i \\leftarrow x_i + \\epsilon \\phi^*(x_i), \\forall x_i \\in X$\ns.t. $\\phi^* = \\arg \\min_{\\Phi \\in \\mathcal{F}} \\left.\\frac{d}{d\\epsilon} D_{KL} \\left(q[\\epsilon \\phi] || p\\right)\\right|_{\\epsilon = 0}$ (1)\nwhere $\\epsilon \\in \\mathbb{R}$ is a sufficiently small step-size, $q[\\epsilon \\phi]$ denotes the distribution of the updated particles, and $\\mathcal{F}$ is a set of candidate transformations.\nThe main result by Liu and Wang (2016) is the derivation of a closed form solution to this optimization problem. By choosing $\\mathcal{F}$ as the unit sphere $\\mathcal{B}_k$ of a vector-valued reproducing kernel Hilbert space $\\mathcal{H}$, i.e. $\\mathcal{F}_k = \\{\\phi \\in \\mathcal{H} : ||\\phi||_{\\mathcal{H}} \\leq 1\\}$, with its kernel function $k(\\cdot, \\cdot) : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$, the authors show that the solution to Eq. (1) is:\n$\\phi(x) \\propto \\mathbb{E}_{x \\sim q} [\\nabla_x \\log p(x)k(x,\\cdot) + \\nabla_x k(x,\\cdot)].$ (2)\nThis result can be used to update the particle set iteratively using (1) and (2), where the expectation is estimated via MC approximation over the entire particle set X. Intuitively, the particle update balances likelihood maximization and particle repulsion. While the first term in Eq. (2) moves the particles towards regions of high likelihood, the latter counteracts this by repulsing particles based on the kernel function gradient, hence it has been called repulsive force (Ba et al., 2021; D'Angelo and Fortuin, 2021).\nBecause vanilla SVGD has been shown to be prone to the initialization of particles and mode collapse (Zhuo et al., 2018; Ba et al., 2021; Zhang et al., 2020), prior work proposed Annealed SVGD (Liu et al., 2017; D'Angelo and Fortuin, 2021). This extension of SVGD, reweighs the terms in the update based on the optimization progress (D'Angelo and Fortuin, 2021). Given the timestep-dependent annealing parameter $\\gamma(t) \\in \\mathbb{R}$,"}, {"title": "2.2 Covariance Matrix Adaptation Evolution Strategy", "content": "The Covariance Matrix Adaptation Evolution Strategy (Hansen and Ostermeier, 2001, CMA-ES) is one of the most popular ES algorithms. We therefore choose it as starting point for our ES-based SVGD method. The core idea of the CMA-ES algorithm is to iteratively optimize the parameters of a Gaussian search distribution $\\mathcal{N}(x, \\sigma^2C)$ from which the solution candidates are sampled. While the algorithmic intuition of CMA-ES is similar to MC gradient approaches (Salimans et al., 2017), CMA-ES updates the search distribution following natural gradient steps, which has been shown to produce more efficient steps than standard gradient descent on multiple problems (Martens, 2020; Akimoto et al., 2012; Glasmachers et al., 2010).\nGiven a population of candidate samples $\\xi_i \\sim \\mathcal{N}(x, \\sigma^2C)$, each iteration of CMA-ES updates the search parameters as follows. The mean of the search distribution is updated by recombining the current generation of candidates via a weighted sum:\n$x \\leftarrow x + \\alpha_{\\chi} \\sigma \\sum_{i=1}^{m} w_i y_i$, where $y_i = (\\xi_i - x)/\\sigma$. (4)\nIn this sum, the recombination weights $w_i$ are based on the rank of the fitness value $f(\\xi_i)$ relative to all other candidates. For details on their exact computation, we refer to Hansen (2016). Next, the step-size $\\sigma$ is updated based on the trace of prior steps. Given\n$\\hat{y} = \\sum_{i=1}^{m} w_i y_i$, $d_{eff} = (\\sum_{i=1}^{m} w_i^2)^{-1}$, and\n$p_{\\sigma} \\leftarrow (1 - \\alpha_{\\sigma})p_{\\sigma} + \\sqrt{\\alpha_{\\sigma}(2 - \\alpha_{\\sigma})} d_{eff} C^{-\\frac{1}{2}}\\hat{y}$,\nwe define\n$\\sigma \\leftarrow \\sigma \\times exp \\left(\\frac{\\alpha_{\\sigma}}{d_{\\sigma}} \\left(\\frac{||p_{\\sigma}||}{\\mathbb{E}[||\\mathcal{N}(0, I)||} - 1\\right)\\right).$ (7)\nHere m is the elite population size hyperparameter that specifies how many candidates are considered in the update of the distribution parameters, and $d_{\\sigma}$ is a smoothing parameter that controls the rate of step-size changes. This update guarantees that the step-size is increased iff the length of the optimization path $p_{\\sigma}$ exceeds what would be expected if steps were sampled randomly from a Gaussian. Finally, the covariance C is updated based on the covariance of the previous steps and current population fitness values:\n$C \\leftarrow (1 + c_1 d(h_o) - c_1 - c_{\\lambda} \\sum_{i=1}^{\\lambda} w_j)C\n+ c_1p_cp_c^T + c_{\\lambda} \\sum_{i=1}^{\\lambda} w_i y_iy_i^T,$ (8)\nwith $p_c \\leftarrow (1 - a_c)p_c + h_o\\sqrt{ac(2 - ac)} d_{eff} \\hat{y}_i$. (9)\nIn words, the covariance update performs exponential smoothing over the optimization path to update C based on the within- and between-step covariance of well performing solutions. Thus, Eq. (8) scales C along directions of successful steps, making the CMA-ES update follow a natural gradient direction to make the search converge faster (Akimoto et al., 2012). For completeness, we list the detailed update, including the definitions of $h_o, d(h_o)$ and $w_i$ in the Appendix."}, {"title": "3 Stein Variational Evolution Strategies", "content": "This section introduces a novel framework of using a multi-population ES gradient-free SVGD. We first outline how ES can generally be used for gradient-free SVGD approximations. Then, we proceed by introducing the CMA-ES-based SVGD as a particular instance of this framework. The full algorithm is listed in Alg. 1. In the following, we use $n \\in \\mathbb{N}^+$ to refer to the number of particles that are used for SVGD inference, i.e. the number of ES search distributions that are run in parallel, each used sampling subpopulations of size $m \\in \\mathbb{N}^+$. This amounts to a total population size of $nm$ for ES-based algorithms."}, {"title": "3.1 SVGD via Evolution", "content": "Consider the task of sampling from a density $p(x) = e^{-f(x)}/Z$ with the untractable normalization constant Z. The SVGD update in Eq. (1) depends on p solely via its score function:\n$\\nabla_x \\log p(x) = -\\nabla_x f(x).$\nWe thus make the (trivial) observation that any algorithm, which estimates a gradient approximation $\\nabla_x$, can be used to approximate the SVGD update (1) as follows:\n$x_i \\leftarrow x_i + \\epsilon \\mathbb{E}_{x \\sim q} [-\\nabla_x f(x)k(x, x_i) + \\nabla_x k(x, x_i)].$ (10)\nThe idea of this work is thus to represent each SVGD particle by the mean of an ES search distribution and use the estimated steps of the ES algorithm to approximate the gradient in the SVGD particle update. Since ES are easily parallelizable on modern GPUs (Lange, 2023; Tang et al., 2022), this approach comes at small additional runtime given that objective function evaluations are not prohibitively expensive, as it is the case for many real world problems such as logistic regression or modern reinforcement learning simulators (Lange, 2022; Freeman et al., 2021).\nOne of the most straightforward instances of such a Stein Variational ES is based on the update that was"}, {"title": "3.2 Stein Variational CMA-ES", "content": "Based on the evolutionary SVGD update in Eq. (10) and the CMA-ES update of the search distribution mean in Eq. 4, we now define Stein Variational CMA-ES (SV-CMA-ES). SV-CMA-ES is a multi-population version of CMA-ES, where n search distributions are updated in parallel, each representing an SVGD particle x via their distribution mean. In other words, for each particle $x_i$, there is a corresponding Gaussian search distribution that is centered at the particle and parametrized as $\\mathcal{N}(x_i, \\sigma C_i)$. This permits us to approximate the score function $\\nabla_xf(x)$ using the CMA-ES mean update in Eq. (4).\nGiven a particle $x_i$ and a sampled subpopulation $\\xi_{ij} \\sim \\mathcal{N}(x_i, \\sigma C_i)$, we approximate the SVGD update as follows:\n$x_i \\leftarrow x_i + \\mathbb{E}_{x \\sim q} [-\\nabla_x f(x)k(x, x_i) + \\nabla_x k(x, x_i)]$\n$= x_i + \\epsilon \\left[\\sum_{j=1}^n \\hat{y}_{j} k(x_j, x_i) + \\nabla_{x_j} k(x_j, x_i)\\right]$\n$= X_i + \\epsilon \\sum_{j=1}^n \\sum_{k=1}^m \\left[w_{jk} \\frac{(\\xi_{jk} - x_j)}{\\sigma} k(x_j, x_i) + \\nabla_{x_j} k(x_j, x_i)\\right]$\n$\\approx X_i + \\epsilon \\sum_{j=1}^n \\frac{1}{\\sigma} \\sum_{k=1}^m \\left[w_{jk} (\\xi_{jk} - x_j) \\right] k_{ji} + \\nabla_{x_j} k_{ji}$ (11)\nwhere $w_{jk}$ are the sample weights that are computed based on the fitness values $f(\\xi_{jk})$ following Hansen (2016). Note that in the last line, the SVGD learning rate hyperparameter $\\epsilon$ can easily be replaced by the CMA-ES step-size hyperparameter $\\alpha_{\\chi} \\sigma_i$.\nIt now remains to define the step-size control for CMA-ES from which the covariance update naturally follows. The CMA-ES step-size update (7) is based on the length of the step $\\hat{y}_i$ by which the particle, i.e. distribution mean, is shifted. Given the particle update in Eq. (11), we define $\\hat{y}_i$:\n$\\hat{y}_i = \\sum_{j=1}^n \\sum_{k=1}^m \\left[w_{jk} \\frac{(\\xi_{jk} - x_j)}{\\sigma} \\right] k(x_j, x_i) + \\nabla_{x_j} k_{ji}$ (12)\nBased on this quantity, the exponentially smoothened step $p_{\\sigma_i}$, is computed analogously to the CMA-ES optimization path update in Eq. (6). Using $p_{\\sigma_i}$, the particle step-size $\\sigma_i$ and covariance $C_i$ can be computed using (7) and (8) respectively."}, {"title": "3.3 Practical considerations", "content": "We now discuss some modifications to the algorithm that we found beneficial in practice. As noted earlier, the update of the particle in Eq. (11) smoothens the gradient approximation across all particles. As a result, the magnitude of the effective steps is reduced compared to standard CMA-ES. Since CMA-ES reduces the step-size $\\sigma$ automatically when small steps are taken, this may lead to premature convergence. An example that illustrates this problem is a bimodal distribution with both modes far apart, such that for most particles $k(x, y)$ is close to zero for all pairs x, y that are sampled from different modes. In this scenario, the driving force term of the SVGD update corresponds to the vanilla CMA-ES update scaled down by the factor of 1/n. Hence, the proposed steps in this scenario would shrink iteratively. To address this issue, we propose the following simplified particle update that we found to work well in practice:\n$X_i + \\epsilon \\sum_{k=1}^m w_{ik} \\frac{(\\xi_{ik} - x_i)}{\\sigma_i} + \\sum_{j=1}^n \\nabla_{x_j} k(x_j, x_i)$ (13)\nThis updates uses only particle $x_i$ to estimate the first term of the update, i.e., the driving force. We note that this corresponds to a hybrid kernel SVGD (D'Angelo et al., 2021; MacDonald et al., 2023) step $\\phi(x) = \\mathbb{E}_{x \\sim q} [\\nabla_x f(x)k_1(x, x_i) + \\nabla_x k_2(x, x_i)]$, with $k_1(x, y) = n\\delta(x = y)$. This kernel can be approximated by an RBF kernel with small bandwidth h $\\rightarrow$ 0. In the latter case, it follows from Theorem 4.1 in MacDonald et al. (2023) that Eq. (13) indeed approximates SVGD if $k_2$ is chosen to be an arbitrary RBF kernel.\nWhile the update in Eq. (13) does not possess the same capabilities of transporting particles \"along a necklace\" as the vanilla SVGD update (cf. Fig. 1 of Liu and Wang (2016)), it has been noted previously that these SVGD capabilities play a limited role for practical problems in the first place (D'Angelo and Fortuin, 2021). Instead, prior work proposed the annealed update in Eq. (3) to transport the particles to regions of high density (D'Angelo and Fortuin, 2021; Liu et al., 2017). In practice, we observe that by using an annealed version of the above update, i.e.\n$X_i + \\frac{\\epsilon}{n} \\sum_{k=1}^m w_{ik} \\frac{(\\xi_{ik} - x_i)}{\\sigma_i} + \\sum_{j=1}^n \\nabla_{x_j} k(x_j, x_i)$ (14)\nencourages sufficient mode coverage to efficiently sample from distributions."}, {"title": "4 Related Work", "content": "Stein Variational Gradient Descent Extensions SVGD is a popular method for sampling from unnormalized densities. As such, SVGD has been an active research topic and many extensions have been proposed. These include approaches to improve the performance in high dimensions, for instance using projections (Chen et al., 2019) or by adjusting the particle update to reduce its bias (D'Angelo and Fortuin, 2021; Ba et al.,\n2021). Other extensions include non-Markovian steps (Ye et al., 2020; Liu et al., 2022), learning-based methods (Langosco di Langosco et al., 2021; Zhao et al., 2023), and domain-specific kernel functions (Sharma et al., 2023; Barcelos et al., 2024). While our focus lies on gradient-free SVGD approaches, most of these ideas could be integrated into our approach, which would be an interesting direction of future research.\nGradient-free sampling Many gradient-free sampling methods, like those in the MCMC family, iteratively update a proposal distribution to match the target (Andrieu et al., 2003). A shortcoming of these approaches is their slower sampling procedure compared to SVGD, as they often require many burn-in steps and additional rejection steps to decorrelate the samples. Gradient-free SVGD (Han and Liu, 2018, GF-SVGD) addresses this by estimating the gradient for SVGD on a surrogate distribution, which allows for between-chain interactions and requires no rejections. Further work improved the computational efficiency of this method by fitting the surrogate to a limited set of points (Yan and Zou, 2021). However, surrogate-based methods require a well-chosen prior to initialize the surrogate, as they lack an explicit exploration mechanism. Thus, in practical scenarios, a different gradient-free SVGD approach has been presented which relies on MC estimates of the gradient (Liu et al., 2017). In this work, we present a novel perspective on gradient-free SVGD, which combines ideas from the literature on ES and variational inference. Different to prior work, we propose a particle update that is based on CMA-ES, a highly efficient ES (Hansen and Kern, 2004).\nEvolution Strategies ES are a specific class of BBO methods that iteratively improve a search distribution over solution candidates by implementing specific sampling, evaluation and update mechanisms (Rechenberg, 1978). While ES commonly use a single distribution (Salimans et al., 2017; Wierstra et al., 2014; Sehnke et al., 2010), it has been demonstrated that their efficiency can be improved by employing restarts or multiple runs in parallel (Auger and Hansen, 2005; Pugh et al., 2016). For instance, restarts with increasing population sizes have been demonstrated to improve the performance of CMA-ES (Loshchilov et al., 2012). A downside of restarting approaches is their sequential nature, which makes them slower and prohibits to exploit the benefits of modern GPUs. Our method is different as it uses the SVGD update to sample multiple subpopulations in parallel which naturally enables to explore multiple modes. In particular, our proposed SVGD-based update is simpler to compute than other distributed updates (Wang et al., 2019b), yet more informed than uncoordinated parallel runs."}, {"title": "5 Experiments", "content": "We compare SV-CMA-ES with the two existing gradient-free SVGD algorithms from literature: GF-SVGD (Han and Liu, 2018) and the MC gradient SVGD (Liu et al., 2017) that we refer to as SV-OpenAI-ES following the naming convention of the ES community. Furthermore, we compare against vanilla SVGD as a gradient-based method. All strategies have been implemented based on the evosax (Lange, 2023) library. To guarantee a fair comparison, we keep the number of function evaluations equal across all methods. In other words, if SV-CMA-ES is evaluated for 4 particles, each represented by a subpopulation of size 16, we evaluate GF-SVGD and SVGD with 64 particles. For each kernel-based method, we use the standard Gaussian RBF kernel. For GF-SVGD, we follow the setup of Han and Liu (2018) and use the same kernel function for the SVGD kernel k and the smoothing kernel kp, as well as an isotropic Gaussian prior $\\mathcal{N}(0, \\sigma^2I)$. The optimal scale $\\sigma^2$ of the prior is found via a hyperparameter grid search. For each method, we implement the annealed version of SVGD (cf. Eq. (3)) using a logarithmic schedule $\\gamma(t) = max(log(T/t), 1)$. Unless specified differently, this choice is followed in all experiments. For all optimizer-based methods, we use Adam (Kingma and Ba, 2015). For a fair comparison, we search the best hyperparameters for all algorithms separately. The full details of our experimental setup can be found in the Appendix B and in our codebase.\u00b9"}, {"title": "5.1 Sampling from Synthetic Densities", "content": "Setting We first evaluate or method on multiple synthetic densities to illustrate the quality of the generated samples. The closed form pdf for every problem is listed in the Appendix. We use the total population size of 400 across all methods. For the ES, this number is split across 100 particles, while for SVGD and GF-SVGD the total population size is equal to the number of particles. We additionally evaluate the scaling of all methods. For this analysis, we report the final performances after 1000 iterations across different numbers of particles, as well as subpopulation sizes for the ES based methods.\nResults The Figures 2 and 3 display the qualitative and quantitative sample properties. As expected, gradient-based SVGD generates high quality samples for all problems. We find that among the gradient-free methods, SV-CMA-ES performs the best across all problems. While GF-SVGD generates high quality samples for the Gaussian mixture, the variance of the"}, {"title": "5.2 Bayesian Logistic Regression", "content": "Setting Next, we evaluate our method on Bayesian logistic regression for binary classification. We follow the setup described by Langosco di Langosco et al. (2021), which uses a hierarchical prior $p(\\theta)$ on the parameters $\\theta = [\\alpha, \\beta]$, where $\\beta \\sim \\mathcal{N}(0, \\alpha^{-1})$ and $\\alpha \\sim \\Gamma(\\alpha_0, b_0)$. Given this prior, the task is to approximate"}, {"title": "5.3 Reinforcement Learning", "content": "Setting We further assess the performances of the gradient-based SVGD methods on six classic reinforcement learning (RL) problems. The goal of each RL task is to maximize the expected episodic return $J(\\theta)$, where each particle $\\theta$ now parameterizes a multi-layer perceptron (MLP) controller. The corresponding SVGD objective is to sample from the following posterior:\n$p(\\theta) \\propto exp(-J(\\theta)), J(\\theta) = \\mathbb{E}_{(\\mathcal{S}_t,a_t) \\sim \\pi_{\\theta}} \\left[\\sum_{t=1}^{T} R(\\mathcal{S}_t, a_t)\\right]$"}, {"title": "6 Conclusion", "content": "Summary We proposed a new gradient-free algorithm for Stein variational inference. The resulting method, SV-CMA-ES, combines elements from evolutionary computation and SVGD to sample from unnormalized densities without gradients. On several problems with different characteristics, we demonstrated that SV-CMA-ES outperforms prior gradient-free SVGD versions consistently. We could thus confirm our hypothesis that the incorporation of the CMA-ES update enables faster convergence than vanilla MC gradients, and better overall performance than GF-SVGD across multiple problems.\nLimitations For stable convergence, we selected a fixed kernel bandwidth via grid search, while prior work used the median heuristic. Selecting the kernel bandwidth via grid search is costly and thus constitutes a disadvantage. Furthermore, our approach can be computationally expensive due to the singular value decomposition required for each covariance matrix. The latter leads to a runtime complexity in $\\mathcal{O}(nd^3)$ in d dimensions with n particles. Future work could address this by exploring diagonal covariance matrices, which are commonly used to speed up CMA-ES (Ros and Hansen, 2008)."}]}