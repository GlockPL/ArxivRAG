{"title": "ViPro: Enabling and Controlling Video Prediction for Complex Dynamical Scenarios using Procedural Knowledge", "authors": ["Patrick Takenaka", "Johannes Maucher", "Marco F. Huber"], "abstract": "We propose a novel architecture design for video prediction in order to utilize procedural domain knowledge directly as part of the computational graph of data-driven models. On the basis of new challenging scenarios we show that state-of-the-art video predictors struggle in complex dynamical settings, and highlight that the introduction of prior process knowledge makes their learning problem feasible. Our approach results in the learning of a symbolically addressable interface between data-driven aspects in the model and our dedicated procedural knowledge module, which we utilize in downstream control tasks.", "sections": [{"title": "1 Introduction", "content": "Current advances in deep learning research [2,8,19] have shown the tremendous potential of data-driven approaches. However, upon taking a closer look, oftentimes domain-specific inductive biases in the training process or the architecture [8] play a critical role in making the most of the available data. While an end-to-end generalized architecture that is able to tackle a wide range of problems is elegant, we argue that this often leads to models that require large amounts of data to be feasible while still being unable to generalize well outside of the training distribution [16], leading to limited applicability for more specialized use cases where data are often scarce, such as, for instance, in the medical domain. Transfer of deep learning research into applications usually requires further fine-tuning of the model for the given use case, and this often corresponds to collecting specialized data. We believe that providing additional means for domain experts who may or may not be experts in machine learning to represent their knowledge besides data in deep learning models is crucial for driving AI adaption in more areas. Ideally, specializing the model to a domain should reduce the complexity of the learning task and thus, lead to leaner architectures that require less data than a domain agnostic variant, while providing better predictions for sparsely observed situations. Furthermore, as we show with our approach, such grey box modelling approaches also inherently increase the controllability of the model. Thus, developing a model that benefits from both domain knowledge and data samples together is our objective.\nThere are various types of domain knowledge that can be integrated, and ways how they can be integrated [24], ranging from using logic rules or differential equations to structure and expand both the loss function and the learning process [32,20], to architectural considerations that take into account the structure of the underlying problem, such as Graph Neural Networks (GNNs) [28] to model interactions, or more famously to use Convolutional Neural Networks (CNNs) for spatial data.\nWe propose to view the knowledge integration types from a different perspective and group them in either procedural or declarative knowledge. While the latter encompasses domain facts or rules (\u201cKnowing-That", "Knowing-How\"). A straightforward representative of this paradigm would be a mathematical formula, which can describe a certain relation between variables in a concise manner without risk of running into a long-tail problem for less frequently observed variable assignments as would be the case in data-driven approaches. Without a doubt, however, data-driven function approximators are immensely successful in the real world because we are often not able or it is impossible to describe the problem in such a precise way. However, we argue that currently data-driven approaches are often used without considering whether the underlying mechanisms of the domain could be described in a more efficient manner without resorting to arduous data collection.\nIn this work, we propose a novel architecture design that integrates such procedural knowledge as an independent module into the overall architecture. We apply it to video prediction, a task where state-of-the-art models often still struggle due to the high spatio-temporal complexity involved in scenes. Its environments often involve understanding complex domain processes that are hard to robustly learn from observations only and thus are likely to benefit from domain inductive biases. At the same time, this field is the foundation for many possible downstream tasks such as Visual Question Answering (VQA) [30], Model Predictive Control (MPC) [7], or system identification [7]. We create several scenarios which feature complex dynamics, and integrate the knowledge about these dynamics. We show that current deep learning models struggle on their own, but can thrive once enhanced with procedural domain knowledge. We verify that this is still possible even with very limited data, and further highlight that such an interface enables control in the target domain w.r.t. the integrated function parameters at test time, providing a potential basis for downstream control tasks and allowing flexibility in adjusting the model for novel scene dynamics.\"\n    },\n    {\n      \"title\": \"In summary, our contributions are:\",\n      \"content\": \"Specification and analysis of an architectural design for interfacing procedural knowledge with a data-driven model.\nIntroduction of novel challenging scenarios with complex dynamics for video prediction.\nApplication to a downstream control task by relying on the inherently achieved disentanglement w.r.t. the function parameters.\"\n    },\n    {\n      \"title\": \"The paper is structured as follows:\",\n      \"content\": \"First, relevant related work is shown in Section 2, after which our proposed procedural knowledge integration scheme is introduced in Section 3, followed by a description of our datasets in Section 4. In Section 5, we first describe our used setup including implementation details in Section 5.1 and continue by establishing baseline results in Section 5.2. We then analyze the latent state of the model and the resulting controllability in Section 5.3. We conclude by discussing limitations and potential directions for future work in Section 6. Our datasets and code are available at https://github.com/P-Takenaka/nesy2024-vipro.\"\n    },\n    {\n      \"title\": \"2 Related Work\",\n      \"content\": \"Predicting future video frames is a challenging task that requires certain inductive biases in the training process or model architecture in order to lead to acceptable prediction outcomes. The most commonly integrated bias is the modelling of the temporal dependency between individual frames, which assumes that future frames are dependent on past frames [25,3,26]. Some methods also exploit the fact that the scene is composed of objects by structuring the latent space accordingly [12,14,30,22], which improved scene reconstruction performance further compared to approaches that rely on a single global latent scene representation for predictions.\nSince many dynamics in video scenes are of a physical nature, there are also works that explore the learning of the underlying Partial Differential Equations (PDEs) to facilitate video reconstruction [13,4,33,29]. Another line of work\u2500most similar to our approach-considers a more explicit representation of dynamics knowledge in the model [7,9]. Here, discretized PDEs are integrated to calculate a physical state for each frame, which is decoded back into an image representation. These approaches were, however, limited to 2D dynamics of sprites, which allowed the dynamics model to operate directly in the screen space, making the learning problem much easier, while limiting applicability to more realistic settings. These methods also relied on the Spatial Transformer Network [6] for decoding purposes. Dynamical properties of the scene besides the object positions such as for instance changing lighting conditions or object orientations are not modelled with this approach, since it is based on sampling pixel predictions directly from the input reference frames. More recently, an architecture was proposed in a preliminary workshop publication [21] that could in theory handle such dynamic properties. It was, however, only applied to semantic segmentation in visually simple settings and we show that it fails at video reconstruction in our datasets.\nOur work continues in this line of research and increases the complexity of video scenarios that can be handled, while broadening the applicability by bridging the gap to control-based downstream tasks such as MPC.\"\n    },\n    {\n      \"title\": \"3 Proposed Architecture\",\n      \"content\": \"Our objective is to allow domain experts to integrate their knowledge of underlying domain processes in a data-driven architecture in a domain-independent way. As such, we embed this knowledge represented as a programmatic function $F$ within a distinct procedural knowledge module $P$ inside the overall architecture. Instead of learning the domain dynamics itself, we thus provide the means for the model to learn the interface between $F$ and its data-driven components. This is possible since we can directly execute the program code that is $F$ - as opposed to for instance natural language instructions that would need some kind of encoding first and make it part of the computational graph.\nWe opt for an auto-regressive frame prediction scheme in which the model is exposed to the initial $n$ video frames in order to learn the data sample dynamics, before it auto-regressively predicts the next $m$ frames on its own. Learning is guided by the reconstruction loss\n$\\begin{equation} L_{rec} = \\frac{1}{N} \\sum_{i=0}^{N} (V_i-\\hat{V}_i)^2\\tag{1} \\end{equation}$\nof all $N = m + n$ predicted RGB frames $\\hat{V}$ and the ground-truth $V$.\nOur architecture is thus composed of three main components: 1) An initial video frame encoder, which embeds the $n$ observed frames into a suitable latent representation, 2) our procedural knowledge module $P$ that transforms the frame's latent representation to the next time step, and 3) a final video frame decoder that transforms the latent representations back into an image representation, as depicted in Figure 1.\nThe core of our contribution resides in $P$ (see Figure 2): It combines the integrated programmatic function $F$ with a deep spatio-temporal prediction model $R$ to obtain the latent frame representation $z'$ of the next time step based on the latent representation $z$ of the current step. We implement this by transforming the latent vector $z$ into a representation that is separable into three components $z_a$, $z_b$, and $z_c$ via a latent encoder $P_{in}$ and a decoder $P_{out}$. Each part $a, b, c$ is responsible for encoding different aspects of the frame that are learned implicitly through our architectural design, which we now describe and also later verify in our experiments.\n$z_a$ represents features that are relevant for $F$ - i.e. which correspond to its input and output parameters in latent space. Since $F$ is a programmatic function, we can directly integrate it into the computational graph as an individual layer. However, since its input space is usually symbolic and not in a latent representation, $z_a$ is translated into and from its symbolic pendant $s$ through dedicated mapping layers $F_{in}$ and $F_{out}$, respectively. We further facilitate the transformation between latent and symbolic representation of the state by introducing a regularization loss $L_s$ that is based on the symbolic state $s$ and its auto-encoded version for all $N$ predicted frames:\n$\\begin{equation} L_s = \\frac{1}{N} \\sum_{i=0}^{N} (s_i - F_{in} (F_{out}(s_i)))^2\\tag{2} \\end{equation}$\nThe final loss is thus $L = L_{rec} + \\lambda L_s$, with $\\lambda$ being a constant weighting coefficient.\nThe second part $z_b$ contains residual scene dynamics that are not covered by $F$. It corresponds to the output of $R$, which is a typical auto-regressive frame predictor model [30] with the goal of modelling spatio-temporal scene properties. It takes into account the whole latent representation consisting of $z_a$, $z_b$, and $z_c$. Instead of predicting the full future latent state on its own, we show that when combined with $F$, this model $R$ focuses on residual scene dynamics only and does not interfere with dynamics handled by $F$. This property allows us to exert a certain level of control when predicting frames w.r.t. the inputs and outputs of $F$, as shown in Section 5.3.\nFinally, $z_c$ contains static scene features that do not change frame-to-frame, such as for instance parts of the background or object colors. It is thus realized as a residual connection directly mapping from $z_c$ to $z_c'$. Auto-regressive approaches often suffer from accumulating errors, as future predictions build on top of past prediction mistakes. We were able to alleviate this issue by allowing the model to store scene statics in $z_c$.\nThus, our model has three different pathways available to pass latent features from one video time step to the next. Each contains meaningful inductive biases that encourage the model to follow our interpretation of the latent vectors. $z_a$ contains only the encoded state required for $F$ and is limited by an information bottleneck induced by the symbolic state transformation for $F$. $z_c$ is a direct shortcut from the first encoded frame, and thus offers an easy to learn path for time-invariant features. Finally, $z_b$ offers the only unrestricted pathway that allows encoding time-variant features. Keeping representations intact over many frames in auto-regressive models such as $R$ is a challenging learning objective however, which 1) makes it less likely that the model will try to encode static scene features that are better handled by $z_c$, and 2) encourages the model to rely on $z_a$ for dynamics that are handled by $F$.\nObject-centric representations have shown to further improve video prediction performance over methods that work with a single latent state for the whole scene [30]. Since some of our datasets are object based, we also introduce a variant of our proposed architecture that operates on slot-based object representations. For our module $P$ the main difference is that the latent state of a single frame is now separated into multiple object representations. In practice, this amounts to an additional dimension in the tensors which can be handled in parallel by all components of $P$. During the burn-in phase, our module $P$ takes turns with a slot attention [15] module to assemble these frame object representations. For each frame, the latter refines slot-based object representations w.r.t. the encoded video frame of the current time step and the object representation prediction of our module $P$ from the last time step (cf. Figure 2). Afterwards, $P$ uses this refined representation to predict the next time step.\nWith our architecture we aim to concentrate the domain inductive biases within $P$, and keep the image encoder and decoder parts domain agnostic and in line with current state-of-the-art models in video prediction. As opposed to models that use domain-specific decoders such as neural renderers [17], we thus do not need to obtain a full symbolic representation of the scene and can keep parts not relevant for $F$ in the latent space of the model. At the same time, the architecture's modularity facilitates the integration of and interfacing with future developments in video prediction, such as better image encoder or decoders but also better predictors in place of $R$.\"\n    },\n    {\n      \"title\": \"4 Data\",\n      \"content\": \"We designed and introduce three new datasets which consist of videos of rendered 3D scenes using Kubric [5]. They feature complex, nonlinear physical dynamics and aim to provide suitable testing grounds for our approach. Other video dynamics datasets in the literature often either happened in 2D screen space only [7,4], or involved only short-term dynamics for which linear approximations of the underlying dynamics were sufficient [30,11]. As we also show in our experiments, such models fail once the underlying dynamics increase in complexity as in our datasets. Thus, our settings are an ideal candidate for which the introduction of domain knowledge can supplement or even enable purely data-driven approaches. We integrate the dynamics equations of each dataset as $F$ within the procedural knowledge module $P$ of our model.\nOur main and most challenging dataset \\\"Orbits\\\" is based on the three body problem [18], in which multiple objects attract each other, resulting in chaotic movements once three or more objects are involved. A similar dataset was introduced in previous related work [21], albeit with no background and only sphere object shapes.\nOur second dataset is a rendering of an Acrobot environment, a setting which is commonly used for benchmarking physics and control models [1]. It features a double pendulum where one end is fixed in space, resulting in dynamics that are inherently complex with chaotic movements without further actuation. As we show in our experiments these environments are easier to predict in pixel-space for data-driven models compared to our Orbits dataset, however stabilizing the pole by actuating the joint between the pendulums is a common control objective that we want to work with as a downstream task.\nOur third dataset features a non object-centric variant of the Acrobot setting. Instead of observing the double pendulum directly we mount the camera on it, while orienting it towards a static, texturized background, whose texture is the same for all data samples. We let the double pendulum move according to the dynamics defined in the Acrobot setting, and the now also moving camera observes different parts of the background texture in each frame. The prediction of future frames is thus only successful if the model is able to correctly estimate and utlize the indirectly observed pendulum dynamics. The overall dataset setup is otherwise the same as in the Acrobot setting. This dataset is related to visual Simultaneous Localization And Mapping (SLAM), in which models establish a map of the environment based on video input, since an internal representation of the whole always only partially observed scene is necessary. By applying our approach to this dataset we also show its potential for visual robotics navigation tasks as future work, for which we could exchange the pendulum dynamics for a robot dynamics model in $F$.\nWe use our object-centric architecture variant for the Orbits and Acrobot dataset, and the non object-centric variant for the Pendulum Camera dataset. We describe further details of these datasets in Appendix C and their underlying dynamics equations in Appendix F.\"\n    },\n    {\n      \"title\": \"5 Experiments\",\n      \"content\": \"In the following we first describe our experimental setup and continue by analyzing our proposed approach w.r.t. its performance in contrast to existing methods. Afterwards we study the feasibility of using our model for downstream control tasks.\"\n    },\n    {\n      \"title\": \"5.1 Setup\",\n      \"content\": \"All models observe the initial six video frames and where applicable - the symbolic input for $F$ for the very first frame. We evaluate the performance based on the prediction performance of the next 24 frames, however during training only the next twelve frames contribute to the loss in order to observe generalization performance for more rollouts.\nWe compare the performance with two groups of relevant state-of-the-art work in video prediction: 1) Purely data-driven approaches that do not rely on physical inductive biases in the architecture such as Slot Diffusion [31], SlotFormer [30] and PredRNN-V2 [26] and 2) approaches that include general physical inductive biases such as PhyDNet [13], Dona et al. [4], and Takenaka et al. [21]. We describe the details of the configurations of these models in Appendix B.\nWe measure the averaged reconstruction performance for three different random seeds with the Learned Perceptual Image Patch Similarity (LPIPS), which has shown better alignment with human perception than other metrics by relying on a pretrained image encoder. However, for completeness we also report the standard metrics Structural Similarity (SSIM) and Peak Signal-to-Noise Ratio (PSNR).\nImplementation Details Our used video frame encoder is a standard CNN with a subsequent position embedding. The modules $P_{in}$ and $P_{out}$ within $P$ are implemented as fully-connected networks including a single hidden layer with the ReLU activation function, whose non-linearity enables the model to learn the separable latent space that we require without relying on the learning capacity of the video frame encoder. We linearly transform $z_a$ from the symbolic to the latent space and back by implementing $F_{in}$ and $F_{out}$ as fully-connected layers without bias neurons. This enables easy auto-encoding between both spaces while within $P$, and forces the model to learn the complex transformation between the symbolic state for $F$ and its latent version that can be used by the video frame decoder in $P_{in}$ and $P_{out}$. The spatio-temporal prediction model $R$ within $P$ is implemented as a transformer [23] with temporal position embedding. Finally, we use a Spatial Broadcast Decoder [27] as our video frame decoder.\nFor the object-centric variant of our model we use Slot Attention [15] to obtain object representations in the latent space. For a scene with $M$ objects and $N$ rollout frames, we thus obtain $N \\times M$ latent representations, which are decoded individually into the image space by the video frame decoder. The final frame prediction is assembled by combining the $M$ object representations as in\"\n    },\n    {\n      \"title\": \"5.2 Video Prediction\",\n      \"content\": \"Here we establish and discuss the video prediction performance of our proposed architecture and compare it with related work. More qualitative results for all datasets can be found in Appendix D.\nOrbits For the Orbits dataset we are able to significantly outperform other approaches (cf. Table 1 and Figure 3). Our model is able to follow the correct object trajectories and render the objects accordingly. When comparing the symbolic state $s$ used by $F$ with its auto-encoded version $F_{in} (F_{out}(s))$ we observe a low Mean Absolute Error (MAE) of 0.005. This indicates that frame-to-frame object states can be accurately recovered from the latent representation without too much error accumulation.\nIn order to verify the impact of the integrated procedural knowledge, we train two variants of our model which replace the integrated function $F$ with an identity function, and use only the residual model $R$ in place of our procedural knowledge module $P$ (ablations 3) and 4) in Table 1, respectively). In both cases the performance decreases substantially and converges to the same performance as the related work, giving clear evidence that $F$ has a large positive impact on the performance.\nSince we expect that the integration of procedural knowledge should decrease the complexity of the learning problem, and as such require less data than a completely uninformed model, we also train our model with a very small dataset consisting of only 300 training samples instead of 10K to test this hypothesis. As can be seen in Table 1 the performance significantly decreases, but stays well above variants that do not include $F$. When looking at the predictions in Figure 3, the reason for this is a quality loss in object appearance prediction, however the objects are still rendered at the correct positions. As we will further show in Section 5.3, the latent vectors $z_b$ and $z_c$ contain such appearance features, both of which are purely based on the data-driven aspects of our model. These results indicate that while our data-driven parts suffer equally to regular data-driven models when trained with small amounts of data, the inclusion of a procedural knowledge path $z_a$ can enable the model to still provide meaningful predictions. Through this experiment we thus highlight that our model not only benefits from the integrated knowledge, but also still benefits from additional data, which conforms to our goal of providing domain experts with additional avenues to improve their models without removing options.\nWe also test our approach in the face of uncertainties in the integrated function. For this we let the model learn the environment constants object mass and gravitational constant present within $F$ in an unsupervised manner. We used a modified learning rate of $1e-2$ instead of the regular $2e-4$ for these parameters to improve and speed up convergence due to the larger magnitudes compared to regular network weights. The performance is comparable to our model without any learned parameters (see \u201cLearned Parameters in $F$": "n Table 1). In fact, the model converges on the ground-truth values of the learned parameters without ever supplying these as a supervision signal, highlighting the potential of our approach for system and parameter identification tasks as well.\nFinally, we analyze the impact of individual components of our approach. We train variants of our architecture that do not use the residual path $z_c$ and do not use a residual model $R$ and thus $z_b$ (see ablations 1) and 2) in Table 1). Removing either only has a slight negative influence on performance, which aligns with the finding earlier that using $F$ has the largest performance impact in the model. It shows however that predictions can further be improved by using a suitable model $R$, especially when there are environment processes not covered by $F$, such as dynamic lighting conditions.\nAcrobot & Pendulum Camera The Acrobot dataset features a smaller performance gap between our approach and purely data-driven methods such as SlotFormer (cf. Table 2 and Figure 4), indicating that this scenario is easier when relying on data alone. This is probably due to the restricted movements in the scene, whereas in the Orbits setting objects are able to move freely in space, making predictions more difficult. Still, the integration of procedural knowledge was beneficial and lead to improved performance across the board even when compared to Takenaka et al. with a similarly informed model. The challenging nature of modelling indirectly observed dynamics in the Pendulum Camera dataset is reflected in both the quantitative and qualitative performance observed in Table 2 and Figure 4. While our model is able to reconstruct the overall patterns present in each frame, all comparison models were unable to"}, {"title": "5.3 Control Interface", "content": "The model appears to have used the integrated function $F$ correctly in order to obtain better frame predictions. Since $F$ is based on symbolic inputs and outputs, the question arises whether we can use these to control the predictions in an interpretable manner. To evaluate this, we consider two scenarios: (1) We modify $z_b$ and $z_c$ - i.e. the appearance features - and observe whether the dynamics do not change; (2) we directly adjust $z_a$ and observe whether the rendered outputs correlate with our modifications of the symbolic state.\nFor (1), we leverage the object-centric representations in the Orbits scenario and simply swap $z_b$ and $z_c$ between different objects. Figure 5 shows the qualitative results. First, as we expected in Section 3 we can observe that $z_c$ contains static object properties, such as their size, color, and overall shape. At the same time, when modifying $z_b$ we can see changes in time variant features in the image such as object shading, occlusions, and shape changes due to the cameras perspective projection. Finally, all changes to $z_b$ and $z_c$ did not change the scene dynamics, as the objects simply continue their trajectory, albeit with new appearances.\nFor (2), we interface our model with MPC and by doing this show that our model not only renders objects at intended locations, but also inherently enables the integration of downstream tasks that operate in symbolic space. We use an off-the-shelf MPC controller that interacts with our model through $F$. We build on top of the Acrobot scenario and define a control task to swing up the double pendulum by allowing a torque to be applied to the joint between the poles. We"}, {"title": "6 Limitations and Future Work", "content": "Our work puts a spotlight on the benefit of procedural knowledge represented as programmatic functions and aims to answer some fundamental questions on how it could be utilized in data-driven video predictors. As such, there are still many follow-up questions and directions that can be the topic of future research.\nOne assumption that we made was to let the model observe the correct function input of the very first frame in order to give the model a hint of the magnitude and distribution of possible symbolic states. The prediction of initial values - especially in physics - is a known problem with a large body of research behind it. As our focus was on establishing our procedural knowledge module $P$ in the architecture in a domain-independent way, we excluded this aspect as it is part of the encoder module of the underlying model, which should be domain-specific. Still, the integration of a suitable encoder for predicting initial state values is interesting for future work.\nIt would also be interesting to combine the given integrated knowledge with synthesis approaches which might dynamically extend or repair the integrated function. Neural program synthesis is an active field of research which might give many insights in this regard. It could also be used to move towards having a dynamically extending library of domain functions available instead of only a single procedural knowledge function, greatly increasing the utility."}, {"title": "7 Conclusion", "content": "We have introduced a novel architectural scheme to join procedural knowledge in the form of programmatic functions with data-driven models. By applying it to video prediction, we show that our approach can enable models to handle tasks for which data-driven models alone would struggle. While our approach also works with very limited data, we highlight that it still benefits from more data, leaving it open for domain experts whether they want to refine their integrated knowledge, or to collect more data, essentially broadening the means by which performance improvements can be made. Furthermore, our grey-box modelling approach increases the transparency of the overall model and allows direct control of the model predictions through the learned interface to the integrated procedural knowledge, enabling easy interfacing with downstream tasks such as MPC."}, {"title": "A Further Implementation Details", "content": "In the following we describe the core components of our architecture in more detail."}, {"title": "A.1 Video Frame Encoder", "content": "The used video frame encoder is a standard CNN. The input video frames are encoded in parallel by merging the temporal dimension T with the batch dimension B. The CNN consists of four convolutional layers, each with a filter size of 64, kernel size of 5, and a stride of 1. In the non object-centric variant of our architecture, the output features are flattened and transformed by a final fully connected network, consisting of initial layer normalization, a single hidden layer with ReLU activation and a final output linear layer with C = 768 neurons each. The result is a latent vector of size $B \\times T \\times C$ that serves as input to $P$.\nIn the object-centric variant, a position embedding is additionally applied after the CNN, and only the spatial dimensions H and W are flattened before the transformation of the fully connected network, with C reduced to 128. The result is a latent vector of size $B T \\times C \\times H \\times W$. In each burn-in iteration of the object-centric variant, we use the Slot Attention mechanism [15] to obtain updated object latent vectors before applying P."}, {"title": "A.2 Procedural Knowledge Module", "content": "$P$ is responsible for predicting the latent vector of the next frame. It consists of the following submodules:\n$P_{in}$. Responsible for transforming the latent vector obtained from the image frame encoder into a separable latent vector $z$. It is implemented as a fully connected network with a single hidden layer using the ReLU activation function. All layers have a subsequent ReLU activation function. The number of neurons in all layers corresponds to $C$.\n$P_{out}$ Responsible for transforming $z$ back into the latent image space. It has the same structure as $P_{in}$.\n$F_{in}$. Responsible for transforming $z_a$ within $z$ into the symbolic space required for $F$. It is a single linear layer without bias neurons. In the object-centric case, its output size directly corresponds to the number of parameters required for $F$, $N_{param}$ for a single object. In the non object-centric case when there is no separate object dimension available, it instead corresponds to $N_{param} \\times N_{objects}$, where $N_{objects}$ corresponds to the (fixed) number of objects (if present in the dataset).\n$F$. Contains the integrated function directly as part of the computational graph. Details about $F$ for the individual data scenarios can be found in Appendix F.\n$F_{out}$. Same structure as $F_{in}$, with the input and output sizes reversed.\n$R$. Responsible for modelling residual dynamics not handled by $F$. We implement it as a transformer [23] with two layers and four heads. We set the"}, {"title": "Enabling Video Prediction using Procedural Knowledge", "content": "latent size to $C$ and the dimension of its feed-forward network to 512. It takes into account the most recent 6 frame encodings. Its output corresponds to $z_b$. A temporal position embedding is applied before the transformer.\nWe first transform the latent image vector into a separable latent vector $z$ by transforming it with $P_{in}$. We then split $z$ of size $C$ into the three equally sized components $z_a$, $z_b$, and $z_c$. We continue by obtaining their respective next frame predictions $z_a'$, $z_b'$, and $z_c'$ as follows: $z_a'$ by $F$, $z_b'$ by transforming $z$ with $R$, and $z_c'$ directly corresponds to $z_c$. All three components are merged back together and transformed into the image latent space with $P_{out}$ before decoding."}, {"title": "A.3 Video Frame Decoder", "content": "We implement the video frame decoder as a Spatial Broadcast Decoder [27]. We set the resolution for the spatial broadcast to 8, and first apply positional embedding on the expanded latent vector. We then transform the output by four deconvolutional layers, each with filter size 64. We add a final convolutional layer with filter size of 3 to obtain the decoded image. We set the strides to 2 in each layer until we arrive at the desired output resolution of 64, after which we use a stride of 1. In the object-centric variant, we set the output filter size to 4 and use the first channel as weights $w$. We then reduce the object dimension after the decoding as in [15] by normalizing the object dimension of $w$ via softmax, and using it to calculate a weighted sum with the object dimensions of the RGB output channels."}, {"title": "A.4 Training Details", "content": "We train all models for at maximum 500k iterations each or until convergence is observed by early stopping. We clip gradients to a maximum norm of 0.05 and train using the Adam Optimizer [10] with an initial learning rate of $2e-4$. We set the loss weighting factor $\\lambda$ to 1. We set the batch size according to the available GPU memory, which was 32 in our case. We performed the experiments on four NVIDIA TITAN Xp with 12GB of VRAM, taking on average one to two days per run."}, {"title": "B Details for Comparison Models", "content": "Takenaka et al. [21]. We apply the training process and configuration as described in their paper, and instead use RGB reconstruction loss to fit into our training framework. We integrate the same procedural function here as in our model.\nSlot Diffusion [31]. We use the three-stage training process as described in the paper with all hyperparameters being set as recommended.\nSlotFormer [30]. We use their proposed training and architecture configuration for the CLEVRER [34] dataset, as its makeup is the most similar to our datasets and follow their proposed training regimen."}, {"title": "C Further Dataset Details", "content": "In Table 3 we show further statistics of our introduced datasets."}, {"title": "5.3 Control Interface", "content": "For (2), we interface our model with MPC and by doing this show that our model not only renders objects at intended locations, but also inherently enables the integration of downstream tasks that operate in symbolic space. We use an off-the-shelf MPC controller that interacts with our model through $F$. We build on top of the Acrobot scenario and define a control task to swing up the double pendulum by allowing a torque to be applied to the joint between the poles. We"}, {"title": "E Orbits Control Validation Dataset Details", "content": "In the Orbits setting the object positions are part of the symbolic state, which are an integral factor of correctly rendering the output frame. However, it is not trivial to measure how well our model is able to decode \"hand-controlled\" 3D object positions into a 2D frame in a generalizable manner. Therefore we chose to setup an empirical evaluation framework by assembling variations of the Orbits dataset, ranging from different simulation parameters, over completely novel dynamics, up to non-physics settings such as trajectory following. For each validation set, we replace $F$ of a model trained on the default Orbits dataset with the respective version that handles these new dynamics, and then validate the model without any retraining."}, {"title": "E.1 Dataset Details", "content": "This section shows more details about the Orbits dataset variants that are used to empirically estimate the control performance. Table 5 describes each setting in more detail, and qualitative results are shown in Appendix E.2."}, {"title": "F Integrated Function Details", "content": "This section shows the functions integrated in our model. All functions first calculate the appropriate acceleration $a$ before applying it in a semi-implicit euler integration step with a step size of $\u0394t$.\nFor"}]}