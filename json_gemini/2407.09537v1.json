{"title": "ViPro: Enabling and Controlling Video Prediction for Complex Dynamical Scenarios using Procedural Knowledge", "authors": ["Patrick Takenaka", "Johannes Maucher", "Marco F. Huber"], "abstract": "We propose a novel architecture design for video prediction in order to utilize procedural domain knowledge directly as part of the computational graph of data-driven models. On the basis of new challenging scenarios we show that state-of-the-art video predictors struggle in complex dynamical settings, and highlight that the introduction of prior process knowledge makes their learning problem feasible. Our approach results in the learning of a symbolically addressable interface between data-driven aspects in the model and our dedicated procedural knowledge module, which we utilize in downstream control tasks.", "sections": [{"title": "1 Introduction", "content": "Current advances in deep learning research [2,8,19] have shown the tremendous potential of data-driven approaches. However, upon taking a closer look, oftentimes domain-specific inductive biases in the training process or the architecture [8] play a critical role in making the most of the available data. While an end-to-end generalized architecture that is able to tackle a wide range of problems is elegant, we argue that this often leads to models that require large amounts of data to be feasible while still being unable to generalize well outside of the training distribution [16], leading to limited applicability for more specialized use cases where data are often scarce, such as, for instance, in the medical domain. Transfer of deep learning research into applications usually requires further fine-tuning of the model for the given use case, and this often corresponds to collecting specialized data. We believe that providing additional means for domain experts who may or may not be experts in machine learning to represent their knowledge besides data in deep learning models is crucial for driving AI adaption in more"}, {"title": "2 Related Work", "content": "Predicting future video frames is a challenging task that requires certain inductive biases in the training process or model architecture in order to lead to acceptable prediction outcomes. The most commonly integrated bias is the modelling of the temporal dependency between individual frames, which assumes that future frames are dependent on past frames [25,3,26]. Some methods also exploit the fact that the scene is composed of objects by structuring the latent space accordingly [12,14,30,22], which improved scene reconstruction performance further compared to approaches that rely on a single global latent scene representation for predictions.\nSince many dynamics in video scenes are of a physical nature, there are also works that explore the learning of the underlying Partial Differential Equations (PDEs) to facilitate video reconstruction [13,4,33,29]. Another line of work\u2500most similar to our approach-considers a more explicit representation of dynamics knowledge in the model [7,9]. Here, discretized PDEs are integrated to calculate a physical state for each frame, which is decoded back into an image representation. These approaches were, however, limited to 2D dynamics of sprites, which allowed the dynamics model to operate directly in the screen space, making the learning problem much easier, while limiting applicability to more realistic settings. These methods also relied on the Spatial Transformer Network [6] for decoding purposes. Dynamical properties of the scene besides the object positions such as for instance changing lighting conditions or object orientations are not modelled with this approach, since it is based on sampling pixel predictions directly from the input reference frames. More recently, an architecture was proposed in a preliminary workshop publication [21] that could in theory handle such dynamic properties. It was, however, only applied to semantic segmentation in visually"}, {"title": "3 Proposed Architecture", "content": "Our objective is to allow domain experts to integrate their knowledge of underlying domain processes in a data-driven architecture in a domain-independent way. As such, we embed this knowledge represented as a programmatic function F within a distinct procedural knowledge module P inside the overall architecture. Instead of learning the domain dynamics itself, we thus provide the means for the model to learn the interface between F and its data-driven components. This is possible since we can directly execute the program code that is F-as opposed to for instance natural language instructions that would need some kind of encoding first and make it part of the computational graph.\nWe opt for an auto-regressive frame prediction scheme in which the model is exposed to the initial n video frames in order to learn the data sample dynamics, before it auto-regressively predicts the next m frames on its own. Learning is guided by the reconstruction loss\n$\\mathcal{L}_{rec} = \\frac{1}{N} \\sum_{i=0}^{N} (\\hat{V}_i - V_i)^2$\n(1)\nof all N = m + n predicted RGB frames V and the ground-truth V.\nOur architecture is thus composed of three main components: 1) An initial video frame encoder, which embeds the n observed frames into a suitable latent representation, 2) our procedural knowledge module P that transforms the frame's latent representation to the next time step, and 3) a final video frame decoder that transforms the latent representations back into an image representation, as depicted in Figure 1."}, {"title": "5 Experiments", "content": "In the following we first describe our experimental setup and continue by analyzing our proposed approach w.r.t. its performance in contrast to existing methods. Afterwards we study the feasibility of using our model for downstream control tasks."}, {"title": "5.1 Setup", "content": "All models observe the initial six video frames and where applicable-the symbolic input for F for the very first frame. We evaluate the performance based on the prediction performance of the next 24 frames, however during training only the next twelve frames contribute to the loss in order to observe generalization performance for more rollouts.\nWe compare the performance with two groups of relevant state-of-the-art work in video prediction: 1) Purely data-driven approaches that do not rely on physical inductive biases in the architecture such as Slot Diffusion [31], SlotFormer [30] and PredRNN-V2 [26] and 2) approaches that include general physical inductive biases such as PhyDNet [13], Dona et al. [4], and Takenaka et al. [21]. We describe the details of the configurations of these models in Appendix B.\nWe measure the averaged reconstruction performance for three different random seeds with the Learned Perceptual Image Patch Similarity (LPIPS), which has shown better alignment with human perception than other metrics by relying on a pretrained image encoder. However, for completeness we also report the standard metrics Structural Similarity (SSIM) and Peak Signal-to-Noise Ratio (PSNR).\nImplementation Details Our used video frame encoder is a standard CNN with a subsequent position embedding. The modules $P_{in}$ and $P_{out}$ within P are implemented as fully-connected networks including a single hidden layer with the ReLU activation function, whose non-linearity enables the model to learn the separable latent space that we require without relying on the learning capacity of the video frame encoder. We linearly transform $z_a$ from the symbolic to the latent space and back by implementing $F_{in}$ and $F_{out}$ as fully-connected layers without bias neurons. This enables easy auto-encoding between both spaces while within P, and forces the model to learn the complex transformation between the symbolic state for F and its latent version that can be used by the video frame decoder in $P_{in}$ and $P_{out}$. The spatio-temporal prediction model R within P is implemented as a transformer [23] with temporal position embedding. Finally, we use a Spatial Broadcast Decoder [27] as our video frame decoder.\nFor the object-centric variant of our model we use Slot Attention [15] to obtain object representations in the latent space. For a scene with M objects and N rollout frames, we thus obtain N \u00d7 M latent representations, which are decoded individually into the image space by the video frame decoder. The final frame prediction is assembled by combining the M object representations as in"}, {"title": "5.2 Video Prediction", "content": "Here we establish and discuss the video prediction performance of our proposed architecture and compare it with related work. More qualitative results for all datasets can be found in Appendix D.\nOrbits For the Orbits dataset we are able to significantly outperform other approaches (cf. Table 1 and Figure 3). Our model is able to follow the correct object trajectories and render the objects accordingly. When comparing the symbolic state s used by F with its auto-encoded version $F_{in} (F_{out}(s))$ we observe a low Mean Absolute Error (MAE) of 0.005. This indicates that frame-to-frame object states can be accurately recovered from the latent representation without too much error accumulation.\nIn order to verify the impact of the integrated procedural knowledge, we train two variants of our model which replace the integrated function F with an identity function, and use only the residual model R in place of our procedural knowledge module P (ablations 3) and 4) in Table 1, respectively). In both cases the performance decreases substantially and converges to the same performance as the related work, giving clear evidence that F has a large positive impact on the performance."}, {"title": "5.3 Control Interface", "content": "The model appears to have used the integrated function F correctly in order to obtain better frame predictions. Since F is based on symbolic inputs and outputs, the question arises whether we can use these to control the predictions in an interpretable manner. To evaluate this, we consider two scenarios: (1) We modify $z_b$ and $z_c$ i.e. the appearance features and observe whether the dynamics do not change; (2) we directly adjust $z_a$ and observe whether the rendered outputs correlate with our modifications of the symbolic state.\nFor (1), we leverage the object-centric representations in the Orbits scenario and simply swap $z_b$ and $z_c$ between different objects. Figure 5 shows the qualitative results. First, as we expected in Section 3 we can observe that $z_c$ contains static object properties, such as their size, color, and overall shape. At the same time, when modifying $z_b$ we can see changes in time variant features in the image such as object shading, occlusions, and shape changes due to the cameras perspective projection. Finally, all changes to $z_b$ and $z_c$ did not change the scene dynamics, as the objects simply continue their trajectory, albeit with new appearances.\nFor (2), we interface our model with MPC and by doing this show that our model not only renders objects at intended locations, but also inherently enables the integration of downstream tasks that operate in symbolic space. We use an off-the-shelf MPC controller that interacts with our model through F. We build on top of the Acrobot scenario and define a control task to swing up the double pendulum by allowing a torque to be applied to the joint between the poles. We"}, {"title": "6 Limitations and Future Work", "content": "Our work puts a spotlight on the benefit of procedural knowledge represented as programmatic functions and aims to answer some fundamental questions on how it could be utilized in data-driven video predictors. As such, there are still many follow-up questions and directions that can be the topic of future research.\nOne assumption that we made was to let the model observe the correct function input of the very first frame in order to give the model a hint of the"}, {"title": "7 Conclusion", "content": "We have introduced a novel architectural scheme to join procedural knowledge in the form of programmatic functions with data-driven models. By applying it to video prediction, we show that our approach can enable models to handle tasks for which data-driven models alone would struggle. While our approach also works with very limited data, we highlight that it still benefits from more data, leaving it open for domain experts whether they want to refine their integrated knowledge, or to collect more data, essentially broadening the means by which performance improvements can be made. Furthermore, our grey-box modelling approach increases the transparency of the overall model and allows direct control of the model predictions through the learned interface to the integrated procedural knowledge, enabling easy interfacing with downstream tasks such as MPC."}, {"title": "A Further Implementation Details", "content": "In the following we describe the core components of our architecture in more detail."}, {"title": "A.1 Video Frame Encoder", "content": "The used video frame encoder is a standard CNN. The input video frames are encoded in parallel by merging the temporal dimension T with the batch dimension B. The CNN consists of four convolutional layers, each with a filter size of 64, kernel size of 5, and a stride of 1. In the non object-centric variant of our architecture, the output features are flattened and transformed by a final fully connected network, consisting of initial layer normalization, a single hidden layer with ReLU activation and a final output linear layer with C = 768 neurons each. The result is a latent vector of size B\u00d7T \u00d7 C that serves as input to P.\nIn the object-centric variant, a position embedding is additionally applied after the CNN, and only the spatial dimensions H and W are flattened before the transformation of the fully connected network, with C reduced to 128. The result is a latent vector of size $B T\\times C \\times H\\times W$. In each burn-in iteration of the object-centric variant, we use the Slot Attention mechanism [15] to obtain updated object latent vectors before applying P."}, {"title": "A.2 Procedural Knowledge Module", "content": "P is responsible for predicting the latent vector of the next frame. It consists of the following submodules:\n$P_{in}$. Responsible for transforming the latent vector obtained from the image frame encoder into a separable latent vector z. It is implemented as a fully connected network with a single hidden layer using the ReLU activation function. All layers have a subsequent ReLU activation function. The number of neurons in all layers corresponds to C.\n$P_{out}$ Responsible for transforming z back into the latent image space. It has the same structure as $P_{in}$.\n$F_{in}$. Responsible for transforming $z_a$ within z into the symbolic space required for F. It is a single linear layer without bias neurons. In the object-centric case, its output size directly corresponds to the number of parameters required for F $N_{param}$ for a single object. In the non object-centric case when there is no separate object dimension available, it instead corresponds to $N_{param} \\times N_{objects}$, where $N_{objects}$ corresponds to the (fixed) number of objects (if present in the dataset).\nF. Contains the integrated function directly as part of the computational graph. Details about F for the individual data scenarios can be found in Appendix F.\n$F_{out}$. Same structure as $F_{in}$, with the input and output sizes reversed.\nR. Responsible for modelling residual dynamics not handled by F. We implement it as a transformer [23] with two layers and four heads. We set the"}, {"title": "A.3 Video Frame Decoder", "content": "We implement the video frame decoder as a Spatial Broadcast Decoder [27]. We set the resolution for the spatial broadcast to 8, and first apply positional embedding on the expanded latent vector. We then transform the output by four deconvolutional layers, each with filter size 64. We add a final convolutional layer with filter size of 3 to obtain the decoded image. We set the strides to 2 in each layer until we arrive at the desired output resolution of 64, after which we use a stride of 1. In the object-centric variant, we set the output filter size to 4 and use the first channel as weights w. We then reduce the object dimension after the decoding as in [15] by normalizing the object dimension of w via softmax, and using it to calculate a weighted sum with the object dimensions of the RGB output channels."}, {"title": "A.4 Training Details", "content": "We train all models for at maximum 500k iterations each or until convergence is observed by early stopping. We clip gradients to a maximum norm of 0.05 and train using the Adam Optimizer [10] with an initial learning rate of 2e-4. We set the loss weighting factor \u039b to 1. We set the batch size according to the available GPU memory, which was 32 in our case. We performed the experiments on four NVIDIA TITAN Xp with 12GB of VRAM, taking on average one to two days per run."}, {"title": "B Details for Comparison Models", "content": "Takenaka et al. [21]. We apply the training process and configuration as described in their paper, and instead use RGB reconstruction loss to fit into our training framework. We integrate the same procedural function here as in our model.\nSlot Diffusion [31]. We use the three-stage training process as described in the paper with all hyperparameters being set as recommended.\nSlotFormer [30]. We use their proposed training and architecture configu- ration for the CLEVRER [34] dataset, as its makeup is the most similar to our datasets and follow their proposed training regimen."}, {"title": "C Further Dataset Details", "content": "In Table 3 we show further statistics of our introduced datasets."}, {"title": "D Additional Prediction Visualisations", "content": "This section shows additional qualitative results of our model for the MPC task."}, {"title": "E Orbits Control Validation Dataset Details", "content": "In the Orbits setting the object positions are part of the symbolic state, which are an integral factor of correctly rendering the output frame. However, it is not trivial to measure how well our model is able to decode \"hand-controlled\" 3D object positions into a 2D frame in a generalizable manner. Therefore we chose to setup an empirical evaluation framework by assembling variations of the Orbits dataset, ranging from different simulation parameters, over completely novel dynamics, up to non-physics settings such as trajectory following. For each validation set, we replace F of a model trained on the default Orbits dataset with the respective version that handles these new dynamics, and then validate the model without any retraining."}, {"title": "F Integrated Function Details", "content": "This section shows the functions integrated in our model. All functions first calculate the appropriate acceleration a before applying it in a semi-implicit euler integration step with a step size of \u0394t.\nFor the Orbits dataset each objects state consists of position p and velocity v. The environmental constants correspond to the gravitational constant g and object mass m. Given N objects in the scene at video frame t, the object state of the next time step t + 1 for any object n is obtained as follows:\n$\\begin{aligned}\n    F_{t,n} &= \\sum_{\\substack{i=0\\\\ i\\neq n}}^{N} \\frac{g m (p_{t,i} - p_{t,n})}{|| (p_{t,i} - p_{t,n}) ||^3} \\\\(3)\\\\\n    a_{t,n} &= \\frac{F_{t,n}}{m} \\\\ (4)\\\\\n    v_{t+1,n} &= v_{t,n} + \\Delta t a_{t,n} \\\\ (5)\\\\\n    p_{t+1,n} &= p_{t,n} + \\Delta t v_{t+1,n} \\\\(6)\n\\end{aligned}$\nFor the Acrobot dataset the per-frame state consists of the pendulum angles \u03b8\u2081 and \u03b8\u2082 and their angular velocities $\u03b8\u2081\u0307$ and $\u03b8\u2082\u0307$. The environmental constants consist of the pendulum masses $m_1$ and $m_2$, the pendulum lengths $l_1$ and $l_2$, the link center of mass $c_1$ and $c_2$, the inertias $I_1$ and $I_2$, and the gravitational constant G. The pendulum state of the next time step t + 1 is calculated as follows:\n$\\begin{aligned}\n   \\delta_{1_{\\tau}} &=  m_1 c_1^2 + m_2(l_1^2 + c_2^2 + 2 l_1 c_2 cos(\\theta_{2_{\\tau}})) + I_1 + I_2 \\\\ (7)\\\\\n   \\delta_{2_{\\tau}} &= m_2 (c_2^2 + l_1 c_2 cos(\\theta_{2_{\\tau}})) + I_2  \\\\(8)\\\\\n   \\phi_{\\tau} &= m_2 c_2 G cos(\\theta_1 - \\frac{\\pi}{2}) \\\\ (8)\\\\\n   \\phi_{\\tau} &= -m_2 l_1 c_2 \\dot \\theta_2^2 sin(\\theta_{2_{\\tau}}) - 2 m_2 l_1 c_2 \\dot \\theta_1 \\dot \\theta_2 sin(\\theta_{2_{\\tau}}) \\\\(9)\\\\\n  &+ (m_1 c_1 + m_2 l_1) G cos(\\theta_1 - \\frac{\\pi}{2}) + \\phi_{2} \\\\(10)\\\\\n   \\dot \\theta_{1_{\\tau}} &= \\frac{- \\phi_2 l_{2_{\\tau}} + \\phi_{1_{\\tau}}}{\\delta_{1_{\\tau}}} \\\\(11)\\\\\n   \\dot \\theta_{2_{\\tau}} &= \\frac{m_2l_1c_2 \\dot \\theta_1 sin(\\theta_{2_{\\tau}}) - \\phi_{2_{\\tau}}}{m_2c_2 + I_2}  \\\\ (12)\\\\\n   \\theta_{1_{\\tau + 1}} &= \\dot \\theta_{1} + \\phi_{1} \\\\(13)\\\\\n  \\theta_{2_{\\tau + 1}} &= \\dot \\theta_{2}  \\\\(14)\\\\\n  \\dot \\theta_{1_{\\tau+1}} &= \\theta_{1_{\\tau}} + \\Delta t \\theta_{1_{\\tau}}  \\\\(15)\\\\\n  \\dot \\theta_{1_{\\tau+1}} &= \\theta_{1_{\\tau}} + \\Delta t \\theta_{1_{\\tau+1}}   \\\\ (16)\\\\\n    \\dot \\theta_{1_{\\tau+1}} &= \\theta_{1_{\\tau}} + \\Delta t \\theta_{2_{\\tau+1}} \\\\(17)\n\\end{aligned}"}, {"title": "G MPC Details", "content": "We set the control objective as the maximization of the potential energy-i.e. both pendulums oriented upwards and the minimization of the kinetic energy-i.e. resting pendulums. The system model corresponds to our integrated function F and due to already being discretized does not require further processing. We use a controller with a prediction horizon of 150 steps and store the predicted torque action sequence for the next 75 frames."}]}