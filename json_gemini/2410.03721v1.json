{"title": "Thematic Analysis with Open-Source Generative AI and Machine Learning: A New Method for Inductive Qualitative Codebook Development", "authors": ["Andrew Katz", "Gabriella Coloyan Fleming", "Joyce Main"], "abstract": "This paper aims to answer one central question: to what extent can open-source generative text models be used in a workflow to approximate thematic analysis in social science research? To answer this question, we present the Generative AI-enabled Theme Organization and Structuring (GATOS) workflow, which uses open-source machine learning techniques, natural language processing tools, and generative text models to facilitate thematic analysis. To establish validity of the method, we present three case studies applying the GATOS workflow, leveraging these models and techniques to inductively create codebooks similar to traditional procedures using thematic analysis. Specifically, we investigate the extent to which a workflow comprising open-source models and tools can inductively produce codebooks that approach the known space of themes and sub-themes. The problem motivating the workflow is simple: in many social science research settings, researchers and stakeholders generate large amounts of text. Key insights into phenomena might reside in that collection of text, but the volume of data is too large to analyze by hand. Examples of such data include hundreds of hours of audio recordings, thousands of written documents, and responses to open-ended questions on surveys. To address the challenge of gleaning insights from these texts, we combine open-source generative text models, retrieval-augmented generation, and prompt engineering to identify codes and themes in large volumes of text, i.e., generate a qualitative codebook. The process mimics an inductive coding process that researchers might use in traditional thematic analysis by reading text one unit of analysis at a time, considering existing codes already in the codebook, and then deciding whether or not to generate a new code based on whether the extant codebook provides adequate thematic coverage. We demonstrate this workflow using three synthetic datasets from hypothetical organizational research settings: a study of teammate feedback in teamwork settings, a study of organizational cultures of ethical behavior, and a study of employee perspectives about returning to their offices after the pandemic. We show that the GATOS workflow is able to identify themes in the text that were used to generate the original synthetic datasets. We conclude with a discussion of the implications of this work for social science research and the potential for open-source generative text models to facilitate qualitative data analysis.", "sections": [{"title": "1 Introduction", "content": "In social science research, one may want to answer questions about the experiences, beliefs, and attitudes of individuals. To do so, the researcher can collect quantitative and/or qualitative data. Qualitative and quantitative methodologies have their own underlying epistemological assumptions and traditions [1, 2]. They also come with tradeoffs. While there are many mathematically derived ways to analyze quantitative data, qualitative data has a different foundation and set of approaches for analysis. One form of qualitative data analysis involves a process called coding. Coding is the process of identifying themes, patterns, and insights in a corpus of text [3]. This process is often done manually, but it can be time-consuming and labor-intensive. Moreover, it does not scale well to large volumes of text. Whereas a small group of researchers can analyze tens or even hundreds of observations, thousands or tens of thousands present unique challenges for resources and consistency. These issues facing qualitative researchers in social research who want to work on a large scale are the central challenges motivating this paper. In our work, we aim to validate a method that uses modern machine learning techniques, especially open-source large language models, to analyze large volumes of text and ultimately generate qualitative codebooks.\nOur approach for doing this validation work is simple: compare our new method and its resulting codebooks with the themes and sub-themes that were intentionally built into synthetic data generated for this study. To do this, we simulate three datasets that imitate common data collection and contexts in organizational research. The first dataset is designed to mimic a study of teamwork dynamics, specifically teammate feedback, which is a common research topic in organizational behavior and management research [4-7]. The second dataset is motivated by studies of organizational cultures of ethical behavior [8-12]. The third dataset is inspired by more recent research on employee perspectives about returning to their offices after the COVID-19 pandemic [13-16]. In the following sections, we will show that our new method that leverages open-source models and tools is able to identify themes in these qualitative datasets reliably and efficiently. The ability to perform this kind of text analysis for inductively generative qualitative codebooks is a significant contribution to the organizational research literature and social science research more broadly. Before describing the method, we briefly review literature on qualitative coding and large language models in social science research."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Qualitative Codebooks and Coding", "content": "Qualitative coding has a long tradition in social science research. It is a process of identifying themes, patterns, and insights in a corpus of text [3].\nThematic analysis is one of the foundational methods in qualitative data analysis [17]. Benefits of thematic analysis are that it can be flexibly utilized in a broad range of approaches (e.g., theoretical, conceptual, epistemological) while providing detailed, complex findings. Following the Braun and Clarke tradition, thematic analysis has six phases: \u201c1) familiarizing yourself with the data, 2) generating initial codes, 3) searching for themes, 4) reviewing themes, 5) defining and naming themes, and 6) producing the report\" [17, page 87]. Codebooks begin to be developed in phase two, refined in phases three through five, and used in phase six. A final codebook shows all of the codes for an analyzed data set organized hierarchically to show the relationships between codes (e.g., themes and sub-themes) and descriptions and examples for each code [18]. Of course, this is just one example of qualitative coding. Many more are outlined in popular qualitative coding manuals [19]. Regardless of the tradition, there are non-trivial questions about what constitutes a code, differences between codes and themes, and which procedure is most advantageous for a given research question [20].\nThe coding process is often done manually, but it can be time-consuming and labor-intensive. Moreover, it does not scale well to large volumes of text; whereas a small group of researchers can analyze tens or even hundreds of units of text, thousands or tens of thousands present unique challenges for resources and consistency. These issues facing qualitative researchers working on a large scale are the central challenges motivating this paper. In this work, we aim to validate a method that uses modern machine learning techniques to code large volumes of text."}, {"title": "2.2 Natural Language Processing and Machine Learning in Thematic Analysis", "content": "There have been several recent attempts over the past 15 years to accomplish this task of identifying themes in large corpora of texts. These attempts have used a variety of machine learning techniques, including topic modeling, clustering, and deep learning. For example, [21] used a technique called latent Dirichlet allocation (LDA) to identify topics in a corpus of text. A more modern version of LDA that uses transformer-based language models is BERTopic [22]. Before these deep neural network approaches, however, computer-assisted qualitative data analysis software was around for decades [23, 24]. These tools, such as NVivo, ATLAS.ti, and Dedoose, have been used to facilitate the coding process by providing a digital interface for researchers to organize and analyze their data. Other tools like quanteda [25] or tidytext [26] in R have been used to analyze text data in a more programmatic way, but these are not designed for thematic analysis and instead focus more on text mining. Until the past few years, there were glimmers of potential for using machine learning techniques to facilitate thematic analysis, but the computational resources and models were not yet at a level of sophistication to approximate anything close to human-level coding.\nSince the release of ChatGPT in November 2022, large language models (LLMs) have pervaded global discourse. Unsurprisingly, qualitative researchers immediately began exploring the use of LLMs for qualitative data analysis (QDA). ChatGPT is one of the most commonly used LLMs for QDA and has been used to study a variety of topics, including universal basic income, PhD students' transitions to being independent researchers, and engineering student career interests [27-30].\nWhile some papers focus on the application of LLMs to specific use cases (i.e., data sets), others undertake establishing workflows for utilizing LLMs for QDA [29, 31, 32]. As this paper's methods employ thematic analysis, we center our literature review on that particular method of QDA. De Paoli (2024) and Katz et al. (2023) focused on human-in-the-loop LLM-assisted thematic analysis with an individual human colder, while Gao et al. (2024) developed an LLM-assisted collaborative coding platform for multiple researchers."}, {"title": "3 Method", "content": "This Methods section is divided into three subsections. The first subsection describes the data simulation process used to generate the synthetic datasets used in this study. The second subsection describes the central contribution of the paper: the GATOS workflow used to generate the codebooks for qualitative data analysis. Finally, the third subsection describes the evaluation process used to compare the themes generated by the GATOS workflow with the themes and sub-themes embedded in the data simulation process."}, {"title": "3.1 Data Simulation", "content": "Data for these validation studies came from three simulated datasets. The datasets were simulated to mimic the following three contexts that are encountered in organizational research:\n1. Teammate feedback\n2. Organizational cultures of ethical behavior\n3. Employee perspectives about returning to their offices after the pandemic\nThese three contexts might be pertinent to organizational research when understanding interpersonal teammate dynamics, how organizations' members perceive the organization's culture of ethical behavior, and the employee perspectives and attitudes toward policy shifts regarding workplace location.\nOur data simulation approach was inspired by common approaches to methods development studies in quantitative research wherein researchers generate synthetic data to test their methods [33, 34]. The philosophy is that if we control the data generating process then we know what the method should identify as the correct answer. For quantitative methods, that might be recovery of parameters used to generate the data. For qualitative methods such as we describe in this paper, the analog could be recovery of themes or sub-themes used to generate the text data.\nThe objective for generating synthetic data was to generate data that were as realistic as possible to data that one might actually encounter when running studies in organizational research. We achieved that objective in a multi-step process. Step one involved generating backstories and data generation criteria for the model to use. Step two involved using generative text models to actually simulate data according to those generation criteria. There were two categories of criteria used in this process: those generated by a generative text model and those manually specified by the researchers. The model-specified criteria included: personas, contexts, themes, and sub-themes. The manually-specified criteria included: data type, data collection context, writing style, and writing length."}, {"title": "3.1.1 Simulated Dataset 1: Teammate feedback", "content": "The first synthetic dataset comes from a hypothetical study of teammate feedback in organizations where teamwork is essential. The specified context that the model was given as:\nData Collection Context: Teammate feedback surveys instructing students to respond to the prompt \"Please provide constructive comments about your fellow teammates as well as yourself. The purpose of these comments is to give you the opportunity to explain how you rated your peers and if there was behavior or experience in particular that influenced you when doing your peer and self evaluations.\"\nAfter removing invalid responses, the simulated teammate feedback dataset consists of 854 written responses to that data collection context prompt. The distribution of length of the written responses is shown in Figure 2. The average number of words in the responses was 194 and the median was 180. As shown in the figure, the responses demonstrated a bimodal distribution. This distribution was likely a function of the way the data were generated by forcing variety in response length through the writing style and writing length criteria."}, {"title": "3.1.2 Simulated Dataset 2: Organizational Cultures of Ethical Behavior", "content": "The second synthetic dataset is from a hypothetical study of organizational cultures of ethical behavior. The context for this imagined study was:\nData Collection Context: A study asking people working for large companies the following question: \"What factors do you believe affect your organization's culture around ethical behavior?\"\nAfter removing invalid responses, there were 823 responses in this dataset. The distribution of the length of those written responses for this scenario is shown in Figure 3. The average number of words in the responses was 129 and the median was 95. The responses demonstrated a long tail distribution, with a few responses being much longer than the rest. This was a deviation from the bimodal distribution of first dataset."}, {"title": "3.1.3 Simulated Dataset 3: Employee Perspectives About Returning to Their Workplaces After the Pandemic", "content": "The third synthetic dataset is from a hypothetical study of employee perspectives about returning to their offices after the COVID-19 pandemic. The specified context for this third study was:\nData Collection Context: A study of worker perspectives on returning to the office after the pandemic.\nThe distribution of the length of the 1,110 written responses for scenario number three is shown in Figure 4. The average number of words in the responses was 131 and the median was 97. The responses demonstrated a more realistic long tail distribution similar to the second dataset but with fewer responses being much longer than the rest in comparison with dataset number two. This distribution is more similar to our observations from prior work [30] where we found that responses to open-ended survey questions often have a long tail distribution."}, {"title": "3.2 GATOS Workflow Overview", "content": "The general task we are trying to solve with this method is simple: identify recurring patterns in text data collected in organizational research settings. Our solution was to combine several NLP tools and techniques with generative text models into a multi-step workflow. In particular, we use an open-source generative text model Mistral-22b-2409 and modern NLP techniques (i.e., text embedding) to enable inductive qualitative data analysis at a large scale. By inductive, we mean a data-driven approach to generate a codebook rather than purely theory-driven. This is the kind of approach one may take when not knowing a priori what is discussed in the text. The generated codebook can then be used to label the original text units according to the themes they express. This codebook application step is beyond the scope of the current paper and will be described in future papers.\nWe break this inductive codebook generation process down into multiple parts. First, summarize the original text units of analysis. For example, with open-ended survey questions these units of analysis would be the individual written responses from each participant. Second, take all of the atomic summary points and use a text embedding model to generate high-dimensional numeric representations of each summary point. Third, reduce the dimensionality of those text embeddings to a lower dimensional space to enable step four - clustering the embedded summary points. In theory, these clusters should contain semantically similar summary points. For example, summary points such as \u2018frequent emails reminders' and \u2018constantly sent info to help stay on track' could be clustered together since they are about the same idea (i.e., team leader communication about progress). At this point in the process, we have gone from an initial set of raw text units to a smaller set of clusters of semantically similar text units; however, we still do not know what those are necessarily about - only that most of the clustered texts are similar in some way.\nUp to now, many of these steps have been explored in prior research. The novelty of this workflow comes in step five. The goals of step five (codebook generation) are two-fold and contend with each other: (1) to generate a code for each cluster and (2) try to avoid generating too many redundant codes. This step is intended to mimic how one might code data in a more traditional qualitative data analysis setting. For example, a human researcher might start by reading their data and generating codes as they encounter new ideas. However, when they see data that fits under a code that already exists in their burgeoning codebook then they would not generate a new code. Instead, they would simply move on to the next data point. At each data point, therefore, the researcher is doing multiple things: identify what is being discussed in the data; think about whether it warrants a code; check whether a code already exists that would describe it; and if not, create that new code and add it to the codebook.\nIn step five, we mimic that reflection process with the generative text model and retrieval augmented generation (RAG). In particular, we first take the cluster of text and find the k most similar matches for each summary point in the cluster (usually, k is on the order of 2 to 4) using the embeddings for the summary points, embeddings for the existing codes in the codebook, and calculating their cosine similarities. We then aggregate those nearest neighbors into a set, thereby removing redundancies, to generate a list of unique nearest neighbor codes for that cluster. Consider the scenario where there are six summary points. If there are six summary points in a cluster, we would find 6 * k nearest neighbor codes for that cluster, though this list may have multiple copies of the same codes. Staying with the same example as above, if the six summary points are all about the team leader sending email reminders, then we try to find the existing codes in the codebook that are most similar to this idea, but that may only result in two or three unique existing codes in the codebook that might be similar to those summary points. Those unique codes are then included in the prompt to the generative text model when it is deciding whether to generate a new code for that cluster.\nAt a philosophical level, at this point in the process, it is still an open question whether those 'most similar' codes actually do capture the idea(s) expressed in that cluster. To investigate this question, we instruct a generative text model to look at the cluster of (n) summary points, the existing codes in the codebook (as represented by the \u2264 n * k nearest neighbor codes), and decide whether or not a new code is needed based on whether the existing codes in the codebook provide sufficient thematic coverage of the cluster of summary points. If the model decides that a new code (or codes) is (are) needed, then it generates a new code and definition for that code, which are added to the codebook. If the model decides that a new code is not needed, then the process simply moves on to the next cluster of data points. The full prompt for this code consideration step is given in the Appendix 8.3.\nBy the end of step five, we have gone through each cluster of summary points and either identified a new code or decided that no new code was needed, thereby balancing the goals of identifying the recurring patterns in the summary points while also not generating too many redundant codes. In practice, there are still many near redundancies that exist, so we proceed to a final step in which we cluster these newly generated codes and prompt a model to identify the distinct themes. The process therefore culminates with a list of themes and codes belonging to those themes."}, {"title": "3.3 GATOS Workflow in Detail", "content": "This section presents the steps used in the GATOS workflow to generate a codebook inductively from raw data. At a high level, the goal is to mimic steps a human coder might take when conducting thematic analysis by first reading the raw data, summarizing them in multiple summary points, generative codes that capture recurring patterns, and then identifying themes among those codes. Ideally, new codes are generated when a key idea in the data is not yet represented by a code in the existing codebook. In practice, we instruct the generative text model to read a group of n summaries, compare it with \u2264 n *k of the nearest neighbor matches for the codes in the codebook, and then decide whether or not a new code should be generated to describe the idea(s) expressed in that cluster of summary points. The details of the workflow steps are described in the following subsections."}, {"title": "3.3.1 Step 1: Summarize the Original Data", "content": "The first step in the GATOS workflow is to summarize the original data into distinct ideas. This step is necessary because the raw data may contain multiple ideas in a single response. For example, a participant may write a long response that contains multiple ideas about their perspective of their organization's culture of ethics. The goal of this step is to extract the key ideas from the raw data to make them easier to analyze in the subsequent steps. One can use a relatively small language model (e.g., 7 to 14 billion parameters) for this step. Here, we used the mistral-nemo-12b model for this information extraction task because it is small enough to run quickly but performant enough not to miss parts of the original data to summarize (along with being an open-source model and released under an Apache-2.0 license). The prompt used for summarization is given in the Appendix 8.2."}, {"title": "3.3.2 Step 2: Clustering Semantically Similar Ideas", "content": "The second step in the workflow is to identify semantically similar summary points from step one. The goal of this step is to group together the instances when the same idea is expressed in different ways in the data. If we can do this grouping well, then we should be able to proceed through the clusters one-at-a-time to identify recurring patterns more easily than if there were several discrete ideas covered in each cluster. To accomplish step 2, we first embed all of the summary ideas using a text embedding model. In our case here we used the \u2018mxbai' model [37] because it is a lightweight performant model according to the MTEB leaderboard [38] released under an Apache-2.0 license. The next step is to reduce the dimensionality of these 1,024-dimensional embeddings because clustering in this high-dimensional space might suffer from the curse of dimensionality [39], where all points tend to be far from each other in high-dimensional spaces. We use principal component analysis (PCA) plus uniform manifold approximation and projection (UMAP) [40] to reduce the dimensionality of the embeddings to a lower-dimensional space. We first use PCA to reduce from 1,024 dimensions to D dimensions, where D is dynamically found by identifying whichever dimension number retains 90% of the variance in the data. In practice, we tended to find 100 < D < 120. From this intermediate embedding space, we then used UMAP to reduce down to five dimensions. We use UMAP here to try and preserve the global structure of the data manifold and improve clustering results. From this low-dimensional space, we then use agglomerative clustering with Euclidean distance to cluster the summary points. We use agglomerative clustering rather than K-means because our initial testing suggested that more homogeneous clusters were identified with agglomerative clustering."}, {"title": "3.3.3 Step 3.1: Create Set of Speculative Starter Codes", "content": "The next step in the GATOS workflow is to create the actual codebook by iteratively reading the clusters of summary points and deciding whether to generate a new code or not. To start the process, however, we prompt a generative text model to create 20 hypothetical codes that one might expect to appear in a study of whatever topic we simulated. For example, for the teammate feedback study, we prompted the model to generate 20 hypothetical codes one would expect to describe some data collected in a study of teammate feedback. These initial synthetic codes are used to help the process get started by providing the process with some initial codes to consider from the nearest neighbor matching when generating new codes."}, {"title": "3.3.4 Step 3.2: Inductive Codebook Generation", "content": "With the clusters of summary points and speculative starter codes generated, we can now begin the iterative inductive codebook generation. We accomplish this by embedding each summary point that belonged to a cluster. We once again used the mxbai embedding model as in Step 2. We then embed all entries in the codebook (at the start, this is 20 codes, but as the codebook grows this number also grows).\nAfter embedding the extracted information and the codes from the growing codebook, we find the k nearest neighbors codes for each of the extracted ideas in the cluster. We use cosine similarity for this matching. Cosine similarity is a measure of similarity between two non-zero vectors belonging to an inner product space. The similarity score is defined to equal the cosine of the angle between them, which is also the same as the inner product of the same vectors when normalized to both have unit length. In plain language, cosine similarity measures similarity between two vectors, e.g., embedding vectors. We use the cosine similarity to find the k nearest neighbors for each of the extracted ideas. In the present study, we set k = 5. We chose 5 because to balance between having enough potentially relevant codes in the codebook for the generative text model to check without giving the model all of the codes. Giving the model all of the codes might be a bad idea because it could distract or overwhelm the model's attention. The output of this neighbor matching step is a set of k nearest neighbors for each of the extracted ideas. We use these as part of the prompt to the generative text model to decide whether to generate a new code or not based on the extracted idea and the extant codes in the codebook.\nSimply put, the procedure is as follows. First, embed the new cluster of summary points that the model is going to analyze. Next, embed all the entries in the codebook. Then, calculate cosine similarity between the embedding for the new text and the codes in the codebook to find the k nearest codes in the codebook for the new piece of text. In our case, we set k as 5. Remove duplicate codes from this set. Finally, include only this small subset of the codes in the prompt to the generative text model when instructing it to consider whether or not to generate a new code based on the new cluster it is reading and the subset of the codebook it can reference."}, {"title": "3.3.5 Using Generative Text Model to Decide Whether to Generate a New Code", "content": "The next step in our process runs C many times, where C is the number of clusters from step two. As mentioned before, at a high level we instruct the generative text model to read the summary points in a single cluster, read the nearest neighbor codes from the codebook that might describe the ideas in that cluster, and then decide whether to generate a new code or not. We used Mistral-22b-2409 for this step because it is sufficiently large to be able to exhibit reasoning steps while being small enough to run quickly on common consumer hardware. \nThere are six specific steps that the model is instructed to complete as part of this decision-making process. First, the model is instructed to read the existing codebook. Second, the model is instructed to read the summary data points in cluster. Next, the model is instructed to try to use one or more of the existing codes to describe the summaries in the cluster. Fourth, the model is instructed to create a new code if needed. The model then must evaluate whether the suggested new code adheres to three evaluation criteria: parsimony, abstraction level, and non-redundancy. Finally, the model is instructed to make a final recommendation about whether to create a new code or not.\nThis workflow sounds complicated, but the actual philosophy is similar to what a traditional qualitative research might do when creating a codebook inductively: start reading data, create a code, and add that to the codebook. Then, when reading the next piece of data, consider the existing codes and see if the new data can be described by one of those existing codes in the codebook. If the new data point already has a code that describes it, then there is no need to generate a new code, so proceed to the next piece of data. On the other hand, if the new data does not already have a code that describe it, then create a new code. We repeat this process for each of the n pieces of data (i.e., text) in the dataset."}, {"title": "3.4 Step 4: Codebook Simplification Through Theme Identification", "content": "The final step in the method is to simplify the codebook by trying to identify themes. This step is necessary because the generative text model may generate redundant codes in the preceding step and codes that belong together at a more abstract level. To address these issues, we used a step in which we clustered similar codes together and gave those to the model along with the instructions to identify higher-level themes in the clusters of codes where possible. The final output of the GATOS workflow is a set of themes and codes that should describe the common semantic patterns in the data."}, {"title": "4 Results", "content": "In this results section, we will present the results of the simulation study across the three synthetic datasets. For each dataset, we compare the codebooks generated by the GATOS workflow with the specified codebooks used to generated the original data. Before reporting results from each case, we first give examples of input and output for each of the main prompts used in this process."}, {"title": "4.1 Example Input and Output for Each Generative Step in GATOS Workflow", "content": ""}, {"title": "4.1.1 Information Extraction with Generative Text Model", "content": "An example of the input and output from the information extraction step is given in table 4. We ran this step for each entry in the datasets to extract the key ideas from each response, which makes the data more manageable for the next steps in the method. As shown here, the input is a medium-length simulated response from the return-to-office dataset, and the output is a five-point list of key ideas extracted from that response. The specified theme and sub-theme for the original synthesized data were \u201cExpectations for Hybrid Work Arrangements\u201d and \u201cFears about Negative Impact on Career Advancement\u201d, respectively. Despite being given a single theme and sub-theme to discuss, the generative model included more than just those themes and sub-themes in the simulated response. This can be seen in the summary points extracted from the response, which include a mix of the specified theme and sub-theme as well as additional information that was not part of the original specified criteria (but was in the simulated data as a byproduct of the simulation)."}, {"title": "4.1.2 Inductive Codebook Generation", "content": "After extracting information from each response to generate summary points, we then clustered the summary points and prompted a generative text model to iteratively consider whether or not to create a new code to describe that cluster. In the codebook creation prompt, the codes in the existing codebook XML tag in the prompt were the nearest codes to the new cluster of summary points. At this point in the GATOS workflow, there are two types of possible outcomes from the generative text model: (1) a new code (or several) is generated and added to the codebook or (2) no new code is generated."}, {"title": "4.1.3 Theme Creation", "content": "An example of the input cluster of codes and output suggested themes from the theme creation step are given in table 5. The input codes contained some redundancies related to flexible work arrangements and work hours, so the generative text model identified those as part of the theme \u201cFlexibility in Work Arrangements\u201d. The model also identified themes related to uncertainty in work schedules and breaks, so those were combined into the theme \"Uncertainty in Work Schedules\". This example also illustrates how individual codes could also persist in the theme identification process as in the case of the code \u201cControl Over Work Hours\". The example also illustrates how there is a thin line between a code and a theme, as the code \"Control Over Work Hours\u201d could be seen as a theme in its own right, or it could have been subsumed into the theme \u201cFlexibility in Work Arrangements\"."}, {"title": "4.2 Codebook Creation Rates", "content": "One important objective of the GATOS workflow is to generate codes that capture the ideas in the data while avoiding redundancy in the codes. To track this, we noted the rate of code creation over time as each cluster was scrutinized by the generative text model. In the worst-case scenario, the model would create one (or more) new codes with each cluster it encountered. This would be unideal because there would almost certainly be thematically redundant clusters, which would lead to redundant codes being created and defeat the purpose of the codebook checking step. Instead, if the model actually were checking the codes in the codebook to decide to create a new code or not, we would expect to see the model generate fewer new codes over time as it encounters more clusters and the codebook becomes saturated. This would suggest that the model is finding existing codes in the codebook that can describe the ideas in the new clusters. The rate of code creation for the teammate feedback is shown in Figure 9, for the organizational culture of ethics dataset in Figure 10, and for the return to workplace dataset in Figure 11.\nAs the figures show, the GATOS workflow met the minimal success criterion of not always generating a new code for each cluster. This can be seen by the deviation of the blue line from the red 45-degree line in the figures. The blue line represents the cumulative number of new codes generated by the model, while the 45-degree line represents the trajectory one would expect to see the blue line follow if there were a new code created for each cluster. The blue line bending below the red line is a quick way to see that sometimes the model does not create a new code for a cluster."}, {"title": "4.3 Evaluation: Comparing GATOS-generated Themes with Original Sub-Themes", "content": "In this section we present the answer to the central question of this paper: how well do the codebooks generated through the GATOS workflow capture the themes and sub-themes used to generate the original synthetic data?. We present the answer to this question for each of the three datasets in their respective subsections. In each of those subsections, we present examples where the workflow nearly matches the original themes one-to-one and examples where the workflow only partially captures the original sub-theme. It is important to note that we are only presenting the comparisons for the GATOS workflow-generated themes and the original sub-themes. We do not present the comparisons for the individual codes generated by the GATOS workflow because the codes are not the final output of the workflow; however, we did find in our testing that those codes always matched an original sub-theme perfectly. One might expect that result given the number of codes generated by the workflow (in the range of 246 to 314, as shown in table 6) and the number of sub-themes used to generate the original synthetic data."}, {"title": "4.3.1 Synthetic Dataset 1: Teammate Feedback", "content": "There were 60 sub-themes (after removing a few redundant sub-themes) used to generate the original synthetic data for the teammate feedback dataset. The codebook generated by the GATOS workflow contained 249 codes and 89 themes. An example comparison of the codebook generated by the GATOS workflow with the original sub-themes is shown in Table 7. In this table, we have selected the best examples of how the GATOS workflow themes aligned with the sub-themes in the original synthetic data. There were many examples where the GATOS workflow-generated themes closely matched the original sub-themes. For example, the original sub-theme \u201ceffective use of feedback mechanisms\" was most closely matched with the GATOS workflow theme \u201cfeedback mechanism effectiveness\". Another near-perfect match was for the sub-theme \u201cembracing feedback as a learning opportunity\", which was matched with the GATOS workflow theme \u201copenness to feedback\". The latter example demonstrates how the GATOS workflow can capture the semantic meaning of a sub-theme without using the exact same words (i.e., openness and embracing).\nTo avoid the temptation to cherry-pick the data, we also present the worst matches or unmatched themes for the teammate feedback synthetic dataset in Table 8. In this table, we show the instances where there was no clear theme that perfectly aligned with a sub-theme used in the original data generation step. For example, the simulated sub-theme \"setting realistic goals for self-improvement\" was only matched by \u201cpersonal growth\" and \"self-reflection\" in the GATOS-generated themes. Those themes are missing an aspect of goal setting that was present in the original sub-theme. The worst match of these poor matches was for the sub-theme \u201cavoiding blame\u201d, which was matched with the themes \u201cpersonal bias management\u201d and \u201cnegative behaviors and their impact\". While these themes might indirectly relate to avoiding blame, they miss the essence of blame avoidance. It should be noted"}]}