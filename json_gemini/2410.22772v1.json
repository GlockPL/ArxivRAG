{"title": "Reliability Assessment of Information Sources Based on Random Permutation Set", "authors": ["Juntao Xu", "Tianxiang Zhan", "Yong Deng"], "abstract": "In pattern recognition, handling uncertainty is a critical challenge that significantly affects decision-making and classification accuracy. Dempster-Shafer Theory (DST) is an effective reasoning framework for addressing uncertainty, and the Random Permutation Set (RPS) extends DST by additionally considering the internal order of elements, forming a more ordered extension of DST. However, there is a lack of a trans- formation method based on permutation order between RPS and DST, as well as a sequence-based probability transformation method for RPS. Moreover, the reliability of RPS sources remains an issue that requires attention. To address these challenges, this paper proposes an RPS transformation approach and a probability transformation method tailored for RPS. On this basis, a reliability computation method for RPS sources, based on the RPS probability transformation, is introduced and applied to pattern recognition. Experimental results demonstrate that the proposed approach effectively bridges the gap between DST and RPS and achieves superior recognition accuracy in classification problems.", "sections": [{"title": "I. INTRODUCTION", "content": "UNCERTAIN information is ubiquitous in daily life, affecting decision-making in various domains. To address this, numerous theories have been developed, such as probability theory [1], intuitionistic fuzzy sets [2], Z-numbers [3], [4], and Dempster-Shafer Theory (DST) [5], [6].\nAmong these, DST stands out for its ability to manage uncertainty by representing and combining evidence from multiple sources [7], [8]. Unlike probability theory, DST allows for degrees of belief distributed over sets of possibilities, making it particularly suitable for situations with reliability analysis [9], [10]. This flexibility enables DST to integrate disparate pieces of evidence, offering a robust approach to decision- making under uncertainty [11], [12]. Evidence theory has been further developed across various fields, including complex evidence theory [13], [14] and generalized quantum theory [15]. It has also been employed to explore the information fractal dimension to assess the complexity of mass functions [16], [17], as well as to introduce new entropy measures, such as Deng entropy [18] and generalized information entropy [17]. However, when handling uncertainty involving ordered information, evidence theory exhibits certain limitations. To address this issue, the Random Permutation Set (RPS) was proposed [19]. By replacing combinations with permutations, RPS introduces Permutation Event Sets (PES) and Permutation Mass Functions (PMF), which correspond to the power set and mass function in evidence theory, respectively. In subsequent research, Deng defined a method for generating PMF and effectively determining the order of fusion [20]; Wang further extended the orthogonal sum method within RPS [21]; Chen proposed an RPS distance calculation based on the J-divergence measure [22]; meanwhile, RPS entropy was introduced to quantify the uncertainty within RPS [23].\nIn pattern recognition problems, if there is significant conflict among the fused information sources, DST may produce counter-intuitive results [24], [25]. To address this issue, two common approaches have been proposed. One approach is to directly modify the combination rule, such as Yager's [26] combination rule and Smets' unnormalized combination rule [27]. However, such methods often compromise certain desirable properties, such as commutativity and associativity. As a result, many researchers prefer to preprocess information sources based on varying reliability, as seen in Murphy's average evidence quality method [28] and Deng's weighted average method based on evidence distance [29]. Other approaches to reliability computation from different perspectives include Xiao's belief divergence [30], Liu's dissimilarity measurement [31], and Jiang's correlation matrix [32].\nIn this paper, we conduct an in-depth analysis of RPS and develop a method for calculating support based on the internal order of elements, enabling the transformation from DST to RPS. This allows DST to overcome the constraints of order and achieve more precise mass calculations. Further- more, considering the practical significance of permutation order in pattern recognition, we propose a Ranked Proba- bility Transformation specifically for RPS, emphasizing the varying influence of different internal sequences on decision- making. Based on this transformation, we design a method for calculating the reliability of RPS sources. Finally, extensive numerical examples and practical applications are provided to validate the effectiveness of the proposed algorithm. The main contributions of this paper are as follows: 1. A transformation from DST to RPS is achieved, enabling DST to overcome the constraints of internal element order and allowing for more precise mass calculations. 2. A Ranked Probability"}, {"title": "II. PRELIMINARIES", "content": "To better understand the subsequent sections, we first ex- plain some fundamental conceptsthat are essential for this paper."}, {"title": "A. Dempster-Shafer evidence theory", "content": "Dempster-Shafer theory (DST) is a mathematical framework designed to handle uncertainty by representing both belief and plausibility. It has been widely used in fault diagnosis [33], [34], clustering [35], [36], decision-making [37], [38], and pattern recognition [39], [40], especially when dealing with incomplete or ambiguous data. One of DST's key strengths is its flexibility, as it does not require prior probabilities and provides a robust method for combining evidence. This makes it particularly effective for enhancing decision-making reliability, even in conflicting information [41], [42].\nDefinition 1 (Frame of discernment). Let be the Frame of Discernment (FOD), which consists of a set of exhaustive and mutually exclusive elements, with each element representing a possible state of the variable, indicated by [5], [6]:\n$\\Theta = \\{x_1, x_2, x_3, ..., x_n\\}$\n(1)\nThe power set of O, which is denoted as $2^\\Theta$, consists of all subsets of O, and can be expressed as:\n$2^\\Theta = \\{\\emptyset, \\{x_1\\}, \\{x_2\\},\\cdots, \\{x_n\\},\\cdots, \\{x_1, x_2,\\cdots, x_n\\}, \\Theta\\}$\n(2)\nDefinition 2 (Basic probability assignment). The basic prob- ability assignment (BPA), also known as the mass function, is a mapping $2^\\Theta \\rightarrow [0, 1]$, and it satisfies the following conditions [5], [6]:\n$\\sum_{A \\in 2^\\Theta} m(A) = 1, \\quad m(\\emptyset) = 0$\n(3)\nwhere A is denoted as a focal element and it satisfies $m(A) > 0$.\nDefinition 3 (Pignistic probability transformation). The Pig- nistic Probability Transformation (PPT) successfully converts the Basic Probability Assignment (BPA) into probabilities for the final decision-making stage [43]. Its core concept is to evenly distribute the BPA of a focal element containing multiple elements among each of its internal elements, thereby"}, {"title": "B. Random permutation set theory", "content": "The Random Permutation Set (RPS) is an innovative exten- sion of DST that additionally considers the potential order of elements within a focal element. This internal order can repre- sent varying importance, recognition of internal possibilities, and other characteristics. Based on this internal predefined order, more detailed reasoning and decision-making can be achieved. Some fundamental concepts related to RPS will be explained below.\nDefinition 5 (Permutation Event Space). Considering a FOD $\\Theta = \\{x_1, x_2, x_3, ......, x_n\\}$ with an internal order, the corresponding PES is expressed as [19]:\nP E S(\\Theta) = \\{A | i = 0, 1, 2, \\ldots\\ldots, n; j = 0, 1, 2, \\ldots\\ldots, P(n, i)\\}\n= \\{\\emptyset, (x_1), (x_2),\\cdots, (x_n), (x_1, x_2), (x_2, x_1), \\ldots\\ldots,\\newline \\quad ((x_{n-1}, x_n), (x_n, x_{n-1}), \\cdots, (x_1, x_2,\\cdots, x_n),\\newline \\quad \\ldots\\ldots, (x_n, x_{n-1},...,x_1)\\}\\newline\n(6)\nwhere i is the number of elements in one focal and P(n, i) is the number of permutation in focal with i elements, calculated by $P(n,i) = \\frac{n!}{(n-i)!}$. j represents the index of the permutation for a given number of elements. For each A in the PES, it is referred to as a Permutation Event (PE).\nDefinition 6 (Random permutation set). Given a Frame of Discernment (FOD) $\\Theta = \\{x_1, x_2, x_3, ......, x_n\\}$, its RPS is a series of pair consisting of elements from the PES [19]:\nRPS(\\Theta) = \\{(A, \\mu(A)) | A \\in PES(\\Theta)\\}\\newline\n(7)\nwhere $\\mu(A)$ is defined as the Permutation Mass Function (PMF), which is a mapping PES(\\Theta) $\\rightarrow [0, 1]$, and it satisfies:\n$\\mu(\\emptyset) = 0, \\newline \\quad \\sum_{A \\in PES(\\Theta)} \\mu(A) = 1$\n(8)\nRPS introduces an internal order of elements within a focal element on the basis of the BPA. When a focal element of the BPA contains multiple elements, their permutation order can potentially reflect their relative importance, effectively subdividing the original focal element based on an internal"}, {"title": "III. PROPOSED METHODS", "content": "In this section, the transformation of DST and RPS will be introduced by incorporating the concept of support. Addi- tionally, considering the intrinsic order within the RPS, a new OPT algorithm will be proposed. Furthermore, the calculation of the reliability of the source in RPS will also be discussed.\nDefinition 12 (Internal orders ranking). Given a FOD $\\Theta = \\{x_1, x_2, x_3, , x_n\\}$, for any A \u2208 PES(\u0398), the internal order ranking of A is defined as:\n$\\beta(A) = \\{\\beta_1, \\beta_2, \\beta_3, ..., \\beta_n | n = |A|\\}, \\quad A \\in PES(\\Theta)$\n(22)\nwhere is the order of element in A. By utilizing internal order ranking, each element in the PEs can be represented by its corresponding $\\beta$, which facilitates the subsequent mass allocation.\nExample 1. Given a FOD Theta = {D, N, A}, its Permu- tation Events and corresponding internal order rankings are shown in Table 1.\nDefinition 13 (Ordered support degree). Given a FOD $\\Theta = \\{x_1, x_2, x_3,\\cdots, x_n\\}$, for any A \u2208 PES(\u0398), the order support of A is defined as:\nSord(A) = \\prod_{i=1}^{|A|} \\frac{BetP(\\beta_i)}{\\sum_{j=i}^{|A|} BetP(\\beta_j)}\n(23)\nwhere BetP($\\beta_i$) represents the probability distribution of element $\\beta_i$ obtained after applying the Pignistic Probability Transformation (PPT) on the initial BPA.\nCorollary 13.1. Since the RPS additionally considers the impact of order compared to DST, for different A \u2208 PES(\u0398), if they contain the same number of elements, their order support satisfies:\n$\\sum_{|A|=g} Sord(A) = 1, \\quad g \\in \\{1, 2, 3,..., n\\}$\n(24)\nwhere g denotes the number of elements contained in A.\nCorollary 13.2. If A contains only one element, then Sord(A) is always equal to 1.\nExample 2. Given a FOD $\\Theta = \\{D, N, A\\}$, assume that the probability distributions obtained after the Pignistic Probabil- ity Transformation are:\nBetP(D) = 0.2, BetP(N) = 0.3, BetP(A) = 0.5 (25)\n\u2022 For a PE containing only one element, Sord is always equal to 1.\n\u2022 For a PE containing two elements, such as (N, D), its Sord is:\nSord(N, D) = \\frac{BetP(\\beta_1)}{BetP(\\beta_1) + BetP(\\beta_2)} \\frac{BetP(\\beta_2)}{BetP(\\beta_2)} = 0.6\n(26)\nwhere $\\beta_1, \\beta_2$ denote N and D in (N, D) respectively.\n\u2022 For a PE containing three elements, such as (A, D, N), its Sord is:\nSord(A, D, N) = \\frac{BetP(\\beta_1)}{BetP(\\beta_1) + BetP(\\beta_2) + BetP(\\beta_3)} \\frac{BetP(\\beta_2)}{BetP(\\beta_2) + BetP(\\beta_3)} \\frac{BetP(\\beta_3)}{BetP(\\beta_3)} = 0.2\n(27)\nwhere $\\beta_1, \\beta_2$ and $\\beta_3$ denote A, D and N in (A, D, N) respectively.\nDefinition 14 (RPS transformation). Given a FOD $\\Theta$, for any A \u2208 PES(\u0398), the RPS transformation is defined as:\n$\\mu(A) = m(Element(A)) \\cdot Sord(A)$\n(28)\nwhere Element(A) represents A in the original DST's BPA, disregarding its order.\nNoting that RPS takes into account the additional factor of order compared to DST, the PMF essentially represents a finer division of the BPA. The rule for this finer division must consider the arrangement of the elements.\nExample 3. Given a FOD $\\Theta = \\{D, N, A\\}$, which satisfies:\nm(D) = 0.1, m(N) = 0.2, m(A) = 0.2,\\newline m(N, A) = 0.2, m(D, N, A) = 0.3\n(29)\nStep 1: Calculate the BetP corresponding to the objects in the original BPA based on the Pignistic probability transformation.\nStep 2: Calculate ordered support degree for PEs according to BetP.\nStep 3: Calculate transformed PMF based on Pord and original BPA."}, {"title": "Definition 9 (Ordered probability transformation)", "content": "To convert the PMF into a probability distribution, the Ordered Probabil- ity Transformation (OPT) is proposed. Given an RPS(\u0398) =\n{(\u0391, \u03bc(\u0391)) | \u0391\u2208 PES(\u0398)}, OPT is represented as [20]:\nOPT(xi) = \u03bc({xi})+ \\sum_{\\substack{x_i \\in A \\in PES(\\Theta) \\newline Last(A) \\neq x_i, |A|>1}} \\frac{\\mu(A)}{|A|-1} (15)\nwhere Last(A) \u2260 xi indicates that if xi is the last element of A, meaning it is the least important in the internal order, it will be ignored during the probability allocation process. The key of OPT is to redistribute the mass of multi-element PEs evenly while ignoring the least significant elements."}, {"title": "Definition 10 (RPS discounting rule)", "content": "Given an RPS(O) = {(\u0391, \u03bc(\u0391)) | \u0391\u2208 PES(\u0398)} with reliability a, its discounting rule is defined as [20]:\n\\mu^{\\prime}(A) = \\begin{cases} \\mu(A) \\cdot \\alpha, & |A| = 1 \\\\ \\frac{\\mu(A) \\cdot \\alpha + \\frac{1-\\alpha}{Perm(k)-1}}{Perm(k)}, & |A| > 1 \\end{cases} (16)\nwhere $Perm(k) = \\sum_{i=1}^k P(k, i)$ is the total number of all permutations in the PES of that RPS. Considering the impact of internal order, the discounting of the RPS evenly distributes the uncertainty (1 - reliability) among all possible permutations to ensure fairness."}, {"title": "Definition 11 (BPA generation using Gaussian discriminant Model)", "content": "Assume that the object to be recognized is Oi with j features, and its potential labels are {01,02,..., n}. During the training process, let the sample size be N. For the recognized object Oi, its membership degree based on the Gaussian Discriminant Model is given by [20]:\nf_i{O_i}(\\theta_n) = \\frac{1}{\\sqrt{2\\pi(\\sigma_i)^2}} exp{-\\frac{\\Vert x_i - \\theta_n \\Vert^2}{2(\\sigma_i)^2}} (17)\nwhere\n\\overline{x_i} = \\frac{1}{N} \\sum_{i=1}^{N} x_{i \\leftarrow \\theta_n} (18)\n\\sigma_i = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\overline{x_i})^2_{x_i \\leftarrow \\theta_n}} (19)\nf{Oi}(On) represents the membership of Oi to on based on the j-th attribute, and xi \u2190 On represents the value of the j- th attribute for all samples labeled as on during the training process.\nAfter obtaining the membership values of Oi for all labels, these membership values are normalized as follows:\nf'{O_i}(\\theta_n) = \\frac{f{O_i}(\\theta_n)}{\\sum_{\\theta_n \\in \\Theta} f{O_i}(\\theta_n)} (20)"}, {"title": "Definition 15 (Ranked probability transformation)", "content": "Given a FOD \u0398 = {X1,X2,X3,\u00b7\u00b7\u00b7,Xn}, the Ranked Probability Transformation (RPT) for the corresponding RPS is defined as:\nRpt(xi) = \\sum_{A \\in PES(\\Theta)} \\frac{e^{-rank(xi)}}{\\sum_{xi \\in A} e^{-rank(xi)}} \\cdot m(A) (30)\nwhere rank(xi) denotes the position of xi among the ele- ments of A.\nConsidering that in RPS, elements ranked higher should receive a larger portion of the PMF, a dispersion factor is introduced to achieve this effect. The factor e serves as the dispersion factor, enabling the redistribution of PMF according to different weights based on the rank of elements. The value of X ranges from [0,1], with larger values of A causing the rank to have a greater effect on PMF distribution, leading to more dispersed PMF. When X = 0, the RPT reduces to the standard PPT. In this paper, A is set to a default value of 0.67."}, {"title": "Definition 16 (Decision contribution)", "content": "In the train- ing phase of pattern recognition, given a FOD \u0398 = {X1,X2,X3,...,xn}, the decision contribution of a PRS source is defined as:\n$dc = Rpt(x^*) - \\frac{\\sum_{x_i \\in \\Theta - x^*} Rpt(xi)}{|\\Theta - x^*|}$ (31)\nwhere x* denotes the correct classification of the recognized object, j represents the index of the recognized sample, k is the index of the RPS source, and |\u0398 \u2013 {x*}| denotes the number of the remaining incorrect classes.\nIt is noted that d^j_k not only takes into account the Rpt of the correct classification, but also is related to the Rpt of incorrect classifications. If the average Rpt for the other incorrect categories is greater than that of the correct one, the RPS source is considered to have made a negative contribution to the decision."}, {"title": "Definition 17 (RPS reliability calculation)", "content": "In pattern recog- nition, given k RPS sources {RPS1, RPS2, ..., RPSk}, their reliability is defined as:\n$R_k = \\frac{max \\{DC\\} - min \\{DC\\}}{max \\{DC\\} - min \\{DC\\}} (32)$\n$DC = \\{DC_1, DC_2, ... , DC_k\\}$ (33)\nwhere\n$DC_k = \\sum_{i=1}^{N} dc$"}, {"title": "IV. NUMERICAL EXAMPLES", "content": "Example 4. Given a FOD $\\Theta = \\{x_1, x_2, x_3\\}$, during the training phase, assume the correct label exists in the form of RPS*. Given an RPS1, it satisfies the following condition with RPS*:\nRPS1 = {((x1),0.4), ((x1,x2), 0.2), ((A), 0.4)} (34)\nRPS* = {((x1), 1)} (35)\nwhere A \u2208 PES(\u0398), and the order of A is disregarded, then RPS1 will degenerate into the following mass function:\nm(x1) = 0.4, m(x1,x2) = 0.2, m(A) = 0.4 (36)\nTo better understand the relationship between the proposed distance calculation method and the internal order, in this example, the proposed method will be compared with the widely used J distance. The specific comparison is shown in Table 3. For convenience, the composition of A is represented using indices on the graph, as shown in Fig. 2."}, {"title": "Example 5. Given an FOD \u2295 = {X1,X2,X3}, m1 and m* satisfy:", "content": "m1(x1) = \u03b7, m1(x3) = 0.7 \u2013 \u03b7,\nM1(X2,X3) = 0.2, M1 (X1,X2, X3) = 0.1 (37)\nm*(x1) = 1 (38)\nwhere \u03b7 \u2208 [0,0.7], it is used to modify the mass function m1, with a step size of 0.01 for each change.\nSince the reliability of m1 is relative, in this example, we use m* and m2 (where m2(X2,X3) = 1) to determine the upper and lower bounds of m1. For comparative analysis, we employ Liu's dissimilarity measurement [31], Jiang's correlation co- efficient [32], Lefevre's adapted conflict [45], and Jousselme et al.'s distance [44] to compare with the proposed reliability calculation method. The specific results are shown in Fig. 3."}, {"title": "Example 6. Given a FOD \u2295 = {X1,X2,X3}, m1 and m* are respectively defined as follows:", "content": "m1(x1) = 0.1, m1(x3) = \u03b7,\nM1(X2, X3) = 0.7 - \u03b7, m1 (x1, X2, X3) = 0.2 (39)\nm*(x1) = 1 (40)\nwhere \u03b7 \u2208 [0, 0.7], with a step size of 0.01. Similarly, since the reliability of m1 is relative, in this example, we also use m* and m2, where m2(x2,x3) = 1, to determine the upper and lower limits of m1. The comparison results of the proposed reliability calculation method with other DS-based methods are shown in Fig. 4.\nIn this example, it can be observed that since x1 is the correct label, the reliability of m1 remains at a relatively low level regardless of how y changes. As shown in Figure 4, when \u03b7 continuously changes, the reliability of m1, calculated using the proposed method, does not exhibit significant fluctuations compared to other calculation methods. This is because, in the proposed method, the reliability of m1 is based on both the supporting and opposing Rpt for the correct decision. However, the composition of the opposing evidence is not given much emphasis, making it a calculation method more focused on the final outcome."}, {"title": "V. APPLICATION OF THE PROPOSED METHOD IN TARGET CLASSIFICATION", "content": "In this section, the method proposed in this paper will be ap- plied to real-world target classification problems. Several com- parative algorithms will be used to demonstrate the model's performance and effectiveness in handling classification tasks under varying conditions. These comparisons will highlight the advantages and limitations of the proposed method in relation to existing approaches."}, {"title": "A. Dataset", "content": "The datasets for this experiment are sourced from the UCI Machine Learning Repository, including Iris, Wine, Heart, Australian, Raisin, and Credit Card Clients (CCC). The UC Irvine repository is renowned for its comprehensive collection of datasets, which span various domains and serve as standard benchmarks in the field of machine learning. The specific details of each dataset are presented in Table 4."}, {"title": "B. Comparative models", "content": "To more comprehensively evaluate the effectiveness of the method proposed in this paper, we selected traditional machine learning models as well as several algorithms based on DS theory.\nIn the machine learning algorithms selected, we include Decision Tree (DT) [46], Support Vector Machine (SVM) [47], Naive Bayes (NaB) [48], K-Nearest Neighbors (KNN) [49], and Logistic Regression (LR) [50]. Each of these algorithms has distinct characteristics: DT is known for its simplicity and interpretability, SVM is effective in high-dimensional spaces, NaB is efficient for probabilistic classification, KNN is a simple and intuitive instance-based learning method, and LR is widely used for binary classification problems. Together, these algorithms provide a comprehensive compar- ison of performance, enabling us to evaluate the robustness and generalization of the proposed method across different classification approaches.\nIn the algorithms based on DS theory, we selected: the traditional DST algorithm [5], Liu's dissimilarity measurement [31], the method proposed by Murphy et al. [28], the method developed by Deng et al. [29], and the PCA algorithm [51]. These methods were chosen for their diverse approaches to handling uncertainty and evidence fusion. The traditional DST algorithm serves as a baseline for comparison, Liu's dissimi- larity measurement focuses on evaluating the distance between evidence sources, Murphy's method improves the handling of conflicting evidence, Deng's method introduces a more refined evidence combination strategy, and PCA helps highlight the most significant aspects of the evidence. Together, these DS-based methods provide a balanced perspective for compar- ing the proposed reliability calculation method in complex decision-making scenarios."}, {"title": "C. Implementation", "content": "Step 1: The dataset was randomly divided into training and testing sets using five-fold cross-validation.\nStep 2: In the training set, use the Gaussian discriminant model to generate labeled training BPA, and apply RPS transformation to convert the original BPA into RPS.\nStep 3: Based on the Ranked Probability Transformation, convert different PMFs into probability distributions for different labels.\nStep 4: Calculate the decision contribution of each evidence source toward the correct decision result based on the labels of the training samples, and thereby determine their respective reliability.\nStep 5: In the testing phase, use the Gaussian discriminant model and RPS transformation to obtain the initial BPA and the corresponding RPS for the test sam- ples.\nStep 6: Based on the reliability calculated during the train- ing phase, perform RPS discounting for each evi- dence source.\nStep 7: Determine the fusion order according to the re- liability from highest to lowest, and perform left intersection operations on the RPS sequentially to obtain the final fusion result.\nStep 8: Use the Ranked Probability Transformation to con- vert the fused PMF into probabilities, obtain the predicted label, and compare it with the correct result."}, {"title": "D. Result and discussion", "content": "In the testing phase, we use five-fold cross-validation to calculate the accuracy and standard deviation of different al- gorithms. To provide a clearer visualization of the performance of various algorithms across different datasets, we present this data in the form of box plots, as shown in Fig. 6 and 7. The specific data is detailed in Table 5.\nFrom Fig. 6, it can be seen that the proposed method demon- strates stable overall performance across different datasets, achieving an average accuracy of around 88.14%, which is higher than most other algorithms. Particularly in the Iris and Wine datasets, it achieves an accuracy exceeding 95% with a narrow error range, indicating reliability and consistency in these datasets. Although the accuracy is slightly lower in the Raisin and Australian datasets, it still maintains good performance and consistency. The proposed method shows notably superior performance in most datasets in compari- son with other algorithms, especially in Iris, Wine, and Australian. Moreover, the narrow error range reflects strong robustness across diverse datasets, with stable predictions and superior overall performance.\nFrom Fig. 7, the proposed method exhibits stable standard deviation across various datasets, with particularly low vari- ability in the Iris and Wine datasets, indicating consistent predictive performance. Although the standard deviation is slightly higher in the Raisin and Australian datasets, it remains within an acceptable range overall. Compared to other algorithms, the proposed method generally maintains lower standard deviation, demonstrating greater predictive consis- tency and robustness.\nIn summary, the proposed method demonstrates high effi- ciency and stability, achieving a relatively high classification accuracy of 88.14% and a low standard deviation of 1.91%, outperforming other classification algorithms.\nThe efficiency and stability of the proposed method are approximately due to the following factors: 1. The reliability calculation is outcome-driven rather than based on the simi- larity among evidence sources. This approach assigns higher reliability to evidence sources that contribute more to accurate decisions, effectively eliminating interference from highly sim- ilar sources that may hinder correct decision-making. 2. The reward and penalty mechanism is designed to ensure that the reliability calculation is not only related to correct decisions but also accounts for decisions that do not support the correct outcome. 3. The RPS transformation meticulously considers the impact of sequence on decision-making, while the Ranked Probability Transformation builds on the RPS transformation by emphasizing the influence of top-ranked predictions on decisions and reducing the interference from lower-ranked labels, thereby enhancing stability. Consequently, the proposed method exhibits strong performance in both accuracy and stability, further demonstrating its superiority."}, {"title": "VI. CONCLUSION", "content": "This paper introduces a novel approach for evaluating the reliability of evidence sources. The proposed method considers the influence of the internal order of elements within focal sets on decision-making by transforming the BPA into an order- sensitive RPS. Furthermore, based on the varying priorities inherent in the internal order of the RPS, a Ranked Probability Transformation is introduced to emphasize the impact of sequence. During the training phase, the reliability of different RPS sources is calculated based on their defined contribution to correct decision-making.\nIn the experimental section, several traditional machine learning algorithms and DS-based classification algorithms were selected to compare with the proposed method across multiple datasets. Five-fold cross-validation was used to eval- uate and compare the accuracy and standard deviation. The final results demonstrate that the proposed method exhibits strong accuracy and stability in classification tasks.\nThe main contribution of this paper is the proposal of an RPS transformation method based on BPA, allowing tra- ditional BPA to additionally account for the influence of sequence on decision-making, providing a more refined exten- sion to evidence theory. Meanwhile, the Ranked Probability Transformation is introduced to highlight the significance of sequence, setting it apart from the PPT algorithm. The proposed method is particularly well-suited for supervised learning on large datasets, effectively filtering out unreliable RPS sources that may interfere with decision-making, espe- cially when sample features exhibit high similarity.\nIn the future, attention will be paid on integrating the sim- ilarity among RPS sources with their contribution to correct outcomes to reduce dependence on the training process. Addi- tionally, establishing more appropriate standards in reliability calculation will be pursued to enhance the applicability of the proposed fusion algorithm."}]}