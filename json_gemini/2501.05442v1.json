{"title": "Progressive Growing of Video Tokenizers for Highly Compressed Latent Spaces", "authors": ["Aniruddha Mahapatra", "Long Mai", "Yitian Zhang", "David Bourgin", "Feng Liu"], "abstract": "Video tokenizers are essential for latent video diffusion models, converting raw video data into spatiotemporally compressed latent spaces for efficient training. However, extending state-of-the-art video tokenizers to achieve a temporal compression ratio beyond 4\u00d7 without increasing channel capacity poses significant challenges. In this work, we propose an alternative approach to enhance temporal compression. We find that the reconstruction quality of temporally subsampled videos from a low-compression encoder surpasses that of high-compression encoders applied to original videos. This indicates that high-compression models can leverage representations from lower-compression models. Building on this insight, we develop a bootstrapped high-temporal-compression model that progressively trains high-compression blocks atop well-trained lower-compression", "sections": [{"title": "1. Introduction", "content": "The emergence of diffusion models [15, 36] has transformed image [9, 27, 30, 32, 33] and video [4, 5, 10, 14, 16, 35, 40] generation. These models gradually transform random noise into coherent visual outputs, showcasing impressive capabilities in producing high-fidelity content [5, 8]. Latent diffusion models (LDMs)[32] have gained popularity for image and video generation [1, 33]. Unlike pixel diffusion models that operate directly on raw pixels, LDMs project pixels into a low-dimensional latent space using a variational autoencoder (VAE) [23], enabling diffusion in this reduced space. This dimensionality reduction enhances computational and memory efficiency, which is particularly crucial for video generation due to the higher dimensionality of video data compared to images.\nEarly latent video diffusion models (LVDMs) relied on latents extracted from each frame independently [2, 14, 35] by reusing the same VAE from image LDMs. This neglects the temporal dynamics inherent in video data, compromising the temporal consistency of the generated videos, and offers no temporal compression in the latent space. To address these limitations, the seminal work MagViT-v2 [41] pioneers a dedicated spatiotemporal video encoding that can jointly encode images and videos in the same latent space. Originally designed to encode video into discrete spatiotemporal tokens, MagViT-v2 has been widely adapted to continuous tokens for use in many state-of-the-art LVDMs [13, 24, 40, 46].\nState-of-the-art video diffusion models (VDMs) utilize diffusion transformers [27] as their backbone architectures, with computational costs scaling quadratically with input token lengths. Increasing latent space compression by a factor of two in both spatial and temporal dimensions significantly enhances model efficiency. While MagViT-v2 and other video tokenizers achieve substantial spatial compression (8\u00d7), their temporal compression remains limited to 4x. This work focuses on improving temporal compression while maintaining spatial compression at 8\u00d7. We found that extending MagViT-v2 by adding more temporal down/upsampling layers to boost temporal compression negatively impacts reconstruction quality. This underscores a key challenge in adapting convolutional video tokenizers for"}, {"title": "2. Related work", "content": "Latent video diffusion models. The pioneering LVDMs [2, 3, 10, 14, 26, 35] repurposed image VAEs to transform each frame of the video independently into the latent space. This caused temporal flickering in the generated video and limited the video length due to limited compression achieved by"}, {"title": "3. Method", "content": "Our goal is to create a continuous-token video tokenizer with high temporal compression ratios (8\u00d7 or 16\u00d7) and high reconstruction quality. In Section 3.1, we discuss the technical modifications we make to the MagViT-v2 implementation [41] to arrive at our (ProMAG) base model. In Section 3.2, we analyze why the base ProMAG, capable of doing 4x temporal compression, has difficulties extending to 8x or 16\u00d7 temporal compression. We discuss a method of progressively growing the ProMAG-4X model to achieve 8\u00d7 and consequently 16\u00d7 temporal compression in Section 3.2. Finally, in Section 3.3 we introduce our layer-wise spatial tiling technique during decoding to enable high-resolution encoding-decoding."}, {"title": "3.1. Base model: ProMAG", "content": "Following prior works [29, 40], we build our model based on the continuous-token variant of MagViT-v2 [41]. The MagViT-v2 tokenizer is composed of 3D causal convolution layers. The use of causal convolution not only enables full"}, {"title": "3.2. Progressive model growing", "content": "Motivation. To increase the temporal compression of ProMAG from 4\u00d7 to 8\u00d7 or 16\u00d7, the standard approach is to add additional downsampling and upsampling blocks in the"}, {"title": "Key-frame embedding.", "content": "A naive way to achieve 8\u00d7 temporal compression using the 4\u00d7 model, \\(E_{4x}\\), is to subsample the input video frames v by a factor of two, \\((v_{//2})\\), and then encode the frames with \\(E_{4x}\\). This can be referred to as learning the key-frame encodings \\(z_{key}\\), of the subsampled keyframes.\n\\[z_{key} = E_{4x} (v_{//2})\\)\nwhere \\(E_{4x}\\) is the encoder blocks without the bottleneck 1 \u00d7 1 \u00d7 1 layers."}, {"title": "5. Discussion", "content": "5.1. Progressive growing vs. progressive training\nWe want to investigate if the effectiveness of our method comes from our specific design towards reusing pretrained high-quality lower compression model and only learning the remaining information in the bottleneck layers, or simply just from progressive training of the model in stages by adding more bottleneck layers (w/o skip information flow).\nProgressive training. In this case, we start from a pretrained 4x model, and then add bottleneck downsampling and up-"}, {"title": "6. Conclusion and Limitations", "content": "In this work, we push the boundary to which we can perform temporal compression while keeping latent channel dimension constant. To this end, we propose a novel method of progressively growing our 4\u00d7 temporal compression video tokenizer, ProMAG, to 8\u00d7 and subsequently 16\u00d7 temporal compression. Through extensive evaluation of reconstruction and text-to-video generation, we show that (i) Our video tokenizer can perform much better reconstruction than baseline methods at very high temporal compression rates, (ii) our high compression latent space is suitable for DiT training and provides an immense boost in terms of efficiency for video generation. However, there still exists limitations, which offers opportunities for future research. In scenarios of very high and abrupt motion, our high-compression tokenizers, while perform significantly better than the directly trained baselines, also suffers from temporal artifacts in reconstruction. In addition, in DiT we find that training on increasingly more compressed latent space tends to slightly increase the time needed for convergence."}]}