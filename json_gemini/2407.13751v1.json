{"title": "Temporal Representation Learning for Stock Similarities and Its Applications in Investment Management", "authors": ["Yoontae Hwang", "Stefan Zohren", "Yongjae Lee"], "abstract": "In the era of rapid globalization and digitalization, accurate identification of similar stocks has become increasingly challenging due to the non-stationary nature of financial markets and the ambiguity in conventional regional and sector classifications. To address these challenges, we examine SimStock, a novel temporal self-supervised learning framework that combines techniques from self-supervised learning (SSL) and temporal domain generalization to learn robust and informative representations of financial time series data. The primary focus of our study is to understand the similarities between stocks from a broader perspective, considering the complex dynamics of the global financial landscape. We conduct extensive experiments on four real-world datasets with thousands of stocks and demonstrate the effectiveness of SimStock in finding similar stocks, outperforming existing methods. The practical utility of SimStock is showcased through its application to various investment strategies, such as pairs trading, index tracking, and portfolio optimization, where it leads to superior performance compared to conventional methods. Our findings empirically examine the potential of data-driven approach to enhance investment decision-making and risk management practices by leveraging the power of temporal self-supervised learning in the face of the ever-changing global financial landscape.", "sections": [{"title": "1. Introduction", "content": "The identification of similar stocks is an essential task in finance, with far-reaching implications for portfolio diversification and risk management. Traditionally, the similarity between stocks has been determined based on factors such as industry sector, geographical location, and market capitalization. However, in the era of rapid globalization and digitization, these conventional approaches have become increasingly inadequate in capturing the complex dynamics of the global financial landscape. The rapid pace of globalization has led to the emergence of intricate relationships among companies across different sectors and regions. Multinational corporations now operate in multiple countries, blurring the lines between traditional sector classifications. Additionally, the rise of digital technologies has given birth to new industries and business models that transcend geographical boundaries. As a result, the similarities between stocks can no longer be accurately captured by relying solely on regional or sector-based classifications. Moreover, the non-stationary nature of financial markets poses significant challenges to the identification of similar stocks. The statistical properties of financial time series, including returns and correlations, are subject to change over time, a phenomenon known as concept drift or temporal shift (Lu et al. 2018, Bai et al. 2022). Consequently, similarity measures based on historical data may become unreliable and fail to capture the evolving dynamics of the market. Indeed, (Wan et al. 2021) showed that news data can be useful for modelling evolving company relationships.\nRecent advances in deep learning-based methods in finance domain have shown promise in predicting the desired parameters from the given data (Zhang et al. 2020). However, financial time series data presents unique challenges. The parameters of interest, such as expected returns, are not only difficult to predict due to the presence of excessive noise, but also the labels that can help the model extract hidden patterns in the data are often not well-defined. Moreover, the complex interactions among various financial assets and the impact of external factors, such as macroeconomic variables and market sentiment, further complicate the estimation process.\nOne promising solution to tackle these challenges is to leverage representation learning techniques to extract meaningful embeddings from unlabeled financial time series data, taking into account for temporal distribution shifts. By learning robust and informative representations, we can capture the complex relationships between different assets and exploit the inherent structure in the data. Self-supervised learning (SSL) has emerged as a powerful paradigm for learning such representations from unlabeled data, with successful applications in various domains, including computer vision (Chen et al. 2020, He et al. 2020) and natural language processing (Devlin et al. 2018, Yang et al. 2019). SSL enables the model to learn meaningful representations by solving pretext tasks that do not require explicit labels, making it particularly suitable for scenarios where labeled data is scarce or expensive to obtain. However, while these methods are useful for exploiting inductive biases in data used in any domain, their use for financial time series data has been less explored.\nTime series data exhibits distinct characteristics, such as seasonality, trend, and interaction between stocks, which require careful consideration when designing SSL frameworks. Furthermore, the non-stationary nature of financial markets necessitates the development of techniques that can adapt to distribution shifts and generalize well to future time periods. Most existing SSL methods focus on learning invariant representations (Chen et al. 2020, He et al. 2020), assuming that the data distribution remains stationary. However, this assumption does not hold in the context of financial markets, where the underlying dynamics can change over time.\nTo address these challenges, we combine techniques from SSL and the field of temporal domain generalization. Our approach aims to learn general model representations that can adapt to temporal shifts over time, thereby enhancing the accuracy and robustness of financial parameter estimation using learned embeddings. By incorporating temporal domain generalization into the SSL framework, we enable the model to learn representations that are not only informative but also resilient to distribution shifts.\nThe main contributions of this paper are as follows:\n\u2022 We examine a novel temporal Self-Supervised Learning (SSL) framework, SimStock, that combines SSL with temporal domain generalization to learn robust and informative representations of financial time series data. SimStock leverages the power of SSL to capture the complex relationships between different financial assets while accounting for temporal shifts in the data distribution;\n\u2022 We introduce a new corruption method for SSL of stock data, termed dimension corruption, which integrates temporal patterns into the corruption process. By corrupting the input data along different dimensions, such as time, asset, and feature, SimStock learns representations that are robust to noise and non-stationarity in the data;\n\u2022 We conduct extensive experiments on four real-world benchmarks with thousands of stocks to demonstrate the effectiveness of SimStock in finding similar stocks. Our results show that SimStock achieves state-of-the-art performance, outperforming existing methods in terms of accuracy and robustness. We showcase the practical utility of SimStock in simplifying the process of screening potential investment opportunities;\n\u2022 We demonstrate the practical utility of SimStock in various financial applications, including pairs trading, index tracking, and portfolio optimization. We show that using the similar stocks identified by SimStock leads to superior performance compared to conventional methods in these applications, highlighting the potential of our approach to enhance investment strategies and risk management."}, {"title": "2. Related work", "content": "This section explores relevant research in two key areas: Self-Supervised Learning (SSL) for time series data and the estimation of financial parameters from historical data. We discuss the challenges and limitations of applying SSL techniques to time series and review various approaches aimed at improving the robustness and accuracy of parameter estimation in finance."}, {"title": "2.1. Self-supervised learning for time series data", "content": "Self-Supervised Learning (SSL) has emerged as a promising paradigm for learning robust and generalizable representations from unlabeled data, with successful applications in both computer vision and natural language processing (NLP) (Chen et al. 2020, Jing and Tian 2020, Zhai et al. 2019, Lee et al. 2019, Qiu et al. 2020, Ruder and Plank 2018, Song et al. 2020). SSL methods aim to overcome the limitations of traditional supervised learning, which requires large amounts of manually labeled data and can produce models that are non-robust and sensitive to small variations in the inputs.\nFirst and foremost, in the field of NLP, Self-Supervised Learning (SSL) methods, such as BERT (Devlin et al. 2018), GPT (Radford et al. 2019), and their variants (Mann et al. 2020, Touvron et al. 2023, Jiang et al. 2024), have achieved state-of-the-art results earlier than in other fields by pre-training on large corpora of unlabeled text data to learn to predict masked words or generate realistic text sequences. One reason for the success of SSL in NLP is the abundance of unlabeled text data. Later, SSL techniques were proposed to learn useful visual features by solving pre-tasks on unlabeled images, such as predicting the relative positions of image patches or identifying distorted versions of an image, in order to alleviate the label requirements and enable powerful feature extraction in computer vision as well (Wei et al. 2022, Fang et al. 2022). This sequential adaptation is not only due to the abundance of unlabeled text data in NLP, but also because language data is discrete and structured (i.e., words), while image data is high-dimensional, continuous, and amorphous, as discussed by (He et al. 2020), the success of SSL methodologies in NLP followed by success in computer vision is a natural result.\nIn the temporal domain, recent research on SSL methods has predominantly concentrated on video understanding (Jenni et al. 2020) or action classification (Qian et al. 2021). Video-based data is particularly suitable for SSL approaches because of the high correlation between frames, despite its temporal nature. Also, the temporal coherence and continuity present in videos provide a rich source of information that can be exploited to learn meaningful representations without the need for extensive manual labeling. Therefore, methodologies used in computer vision, such as contrastive learning and predictive modeling, can be easily adapted to the video domain. For instance, recent research leverage SSL techniques to learn patch-level (Yun et al. 2022, Caron et al. 2021) or region-level (Xiao et al. 2021) representations from videos. These approaches aim to capture the inherent structure and relationships within video frames, enabling the learning of meaningful features without relying on explicit labels. By exploiting the temporal coherence and spatial consistency present in videos, these self-supervised methods can discover emergent properties and capture rich semantic information."}, {"title": "2.2. Parameter estimation in non-stationary financial markets", "content": "In the field of investment management, estimating future returns and risk is a fundamental challenge due to the inherent uncertainty and non-stationary nature of financial markets. Traditionally, researchers and practitioners have relied on historical financial data to estimate various parameters, such as expected returns, volatility, and covariance matrices, which are crucial inputs for portfolio optimization, risk management, and asset pricing models.\nOne common approach is to use a rolling window of historical returns to estimate the expected returns and covariance matrix of a set of financial assets. For instance, in the classic mean-variance portfolio optimization (MVO) framework proposed by (Markowitz 1952), the optimal portfolio weights are determined based on the estimated expected returns and covariance matrix using a sample of historical returns. See (Kim et al. 2021) for more explanations. Similarly, the Capital Asset Pricing Model (CAPM) (Sharpe 1964, Lintner 1975) and the Fama-French factor models (Fama and French 1993, 2015) rely on historical data to estimate the beta coefficients and factor premiums.\nHowever, using historical data for parameter estimation has several limitations. First, financial markets are known to exhibit non-stationarity, meaning that the statistical properties of the data, such as the mean and variance, can change over time (Cont 2001, Lo 2017). This implies that the estimates based on historical data may not be representative of the future. Second, the sample size of historical data is often limited, leading to estimation errors and potential over-fitting (Kan and Zhou 2004, 2007). For instance, in the context of portfolio optimization, an inherent drawback of MVO is the high sensitivity of the optimal portfolio to estimation errors in the input parameters (Michaud 1989), particularly in the expected returns (Chopra and Ziemba 2013) and correlations (Chung et al. 2022). Small changes in the estimated expected returns can lead to significant shifts in the optimal portfolio weights, resulting in portfolios that may be suboptimal or unstable out-of-sample.\nTo mitigate these issues, researchers have proposed various techniques to improve the robustness and accuracy of parameter estimation. One popular approach is the shrinkage method, which combines the sample estimates with a structured estimator to reduce the estimation error (Ledoit and Wolf 2003, 2004b,a) Another technique is the use of robust estimators, such as the minimum covariance determinant (MCD) estimator (Rousseeuw and Driessen 1999) and the minimum volume ellipsoid (MVE) estimator (Van Aelst and Rousseeuw 2009), which are less sensitive to outliers and heavy-tailed distributions. More recently, (Gerber et al. 2021) proposed the Gerber statistic for estimating the covariance matrix between assets, a robust co-movement measure that extends Kendall's Tau by counting the proportion of simultaneous co-movements when their amplitudes exceed data-dependent thresholds, capturing meaningful co-movements while being insensitive to extreme values and noise.\nIn addition to these techniques, researchers have also explored various approaches to extrapolate past data and model future market dynamics. For example, (Barberis 2000) proposed a Bayesian approach that combines the sample estimates with prior beliefs about the asset returns to improve the out-of-sample performance of portfolio optimization. (Rapach et al. 2010) employed combination forecasts, which aggregate individual forecasts based on different predictors, to enhance the accuracy of out-of-sample stock return predictions. Also, (Welch and Goyal 2008) found that the historical average excess returns of stocks over bonds, which is often used as an estimate of the equity premium, is sensitive to the choice of the sample period and the assumptions about survivorship bias. Moreover, they showed that this historical average is a poor predictor of future returns, as the equity premium exhibits substantial time-variation and mean-reversion.\nDespite these advancements, the fundamental challenge of estimating future returns and risk from historical data remains. The assumption that future behavior will be similar to the past is often violated in practice, as financial markets are subject to regime shifts, structural breaks, and extreme events (Ang and Bekaert 2002, Guidolin and Timmermann 2007). See (Lee et al. 2023) for more detailed review of machine learning for asset management."}, {"title": "3. SimStock", "content": "In this section, we introduce SimStock\u00b9, a novel SSL framework that incorporates temporal domain generalization to learn robust and comprehensive representations of financial time series data. The various components of the model are graphically illustrated in fig. 1."}, {"title": "3.1. Preliminaries", "content": "We consider a self-supervised task where the stock data distribution evolves over time. In the training phase, we are given T observed source domains $D_{1:T} = {D_1, D_2, ..., D_T}$, which are sampled"}, {"title": "3.2. Training Dynamics of Temporal Domain Generalization", "content": "We are motivated by DRAIN (Bai et al. 2022), which first proposed the concept of temporal domain generalization. In each temporal domain Ds, the representation network $f_\u03b8$ can be trained by maximizing the conditional probability P($\u03b8_s$|$D_s$). Here, $\u03b8_s$ represents the state of the model parameters at timestamp ts. Given the dynamic nature of Ds, the conditional probability P($\u03b8_s$|$D_s$) will also change over time. The objective, in the context of temporal domain generalization, is to estimate $\u03b8_{T+1}$ utilizing all the training data from D1:T. From a probabilistic perspective, we can express this as:\nP($\u03b8_{T+1}$|$D_{1:T}$) = $\u222b_\u03a9$ P($\u03b8_{T+1}$|$\u03b8_{1:T}, D_{1:T}$) \u00b7 P($\u03b8_{1:T}$|$D_{1:T}$)d$\u03b8_{1:T}$,         (1)\nwhere \u03a9 denotes the space for model parameters $\u03b8_{1:T}$. In eq. (1), the first term inside the integral P($\u03b8_{T+1}$|$\u03b8_{1:T}, D_{1:T}$) represents the inference phase, which is the process of predicting the future state of the target representation network (i.e., $\u03b8_{T+1}$) given all historical states (i.e., $\u03b8_{1:T}, D_{1:T}$). The second term P($\u03b8_{1:T}$|$D_{1:T}$) signifies the training phase, which involves leveraging all training data D1:T to ascertain the state of the model on each source domain. More specifically, the training phase can decomposed as follows:"}, {"title": "3.3. Temporal Representation Learning", "content": "Our ultimate goal is to learn a representation model, $f_\u03b8$, which captures the stock data distribution that evolves over time. To achieve this, we develop an SSL framework for temporal representation learning of stock data.\nTemporal feature variant. Let $x_s \u2208 R^{d_m}$ be the price feature of a stock, where dm is the dimension of the feature. The time-varying patterns of stock prices are essential for identifying short- and long-term characteristics of stocks. To learn more rich representations, the price feature xs is processed by a temporal transformation module \u03bc. Specifically, the price feature xs is provided with k variations, denoted as:\n\u03bc(x) = CONCAT($\u03bc_1(x_s), \u03bc_2(x_s), ..., \u03bc_k(x_s)) \u2208 R^{d_mk}$,                                    (3)\nwhere dmk = dm \u00d7 k, and each $\u03bc_i : R^{d_m} \u2192 R^{d_m}$ for i \u2208 {1,2,..., k} is a temporal transformation function from the collection U. In other words, $\u03bc_1, \u03bc_2, \u2026, \u03bc_k \u2208 U$, where U denotes the collection of temporal transformations. This module is used to create temporal features that incorporate various time intervals. For example, $\u03bc_1(x_s)$ and $\u03bc_2(x_s)$ would reflect temporal patterns within a day and a week. Various methods, such as the moving average (Woo et al. 2022, Wu et al. 2021b), Fourier transform (Zhou et al. 2022), moving average convergence/divergence (MACD) features for momentum (Lim et al. 2019) and mixtures of experts (Zhou et al. 2022), can be utilized to create these temporal features. In this study, we use moving average, which is the most common choice. More specifically, for each temporal transformation $\u03bc_i$, we calculate the moving average of the price feature xs over a window of size wi:\n\u03bci(x) = $1/w_i * \u2211_{j=0}^{w_i-1}x_{i-j}$, i = 1,2,..., k. (4)\nwhere wi is the window size for the i-th transformation, and $x_{i-j}$ denotes the price feature at time t \u2212 j in domain s. In our implementation, we use five different window sizes: w\u2081 = 5, w2 = 10,"}, {"title": "3.3. Combined embedding with static metadata.", "content": "In our framework, static metadata cs, which can include a variety of data such as firm description, 3-statement financial information, and more, is handled in the static embedding layer. However, for the purpose of this study, we have only included sector information in c. As a result, an embedding Embed(cs) \u2208 Rdmk is obtained. Next, we create a combined embedding that incorporates both the temporal feature variant $\u03bc(x_s)$ and the embedded static metadata Embed(c). The resulting combined embedding is denoted as follows:\n$H^s = \u03bc(x_s) + Embed(c^s) \u2208 R^{d_mk}$. (5)"}, {"title": "3.4. Feature Tokenizer module.", "content": "We draw inspiration from the tokenizer approach Gorishniy et al. (2021), which transforms input features into token embeddings to obtain more meaningful representations. This method Gorishniy et al. (2021) do not reflect the temporal aspect, but we utilize temporal feature variants in the Feature Tokenizer module to capture the time-varying patterns in the stock price data. The feature-wise token embeddings TKEj for a given feature index j are computed as follows:\n$TKE_j^s = b_j + H^sW_j$(6)\nwhere $b_j\u2208 R^d$ is the j-th feature bias term and $W_j\u2208 R^d$ is the weight vector for the j-th feature. Consequently, the token embeddings $TKE^s \u2208 R^{d_m k\u00d7d}$ can be obtained by stacking all of the feature embeddings and adding a special [ST] token, which is known to possess the essence of information after training. This is represented as:\nTKE = STACK([ST], $TKE_1^s$, ..., $TKE_{d_mk}^s$) (7)\nwhere $R^{d\u00d7d} = R^{(d_{m k+1})\u00d7d}$ denotes the dimension of the combined token embeddings TKE. Therefore, the feature tokenizer plays a crucial role in learning meaningful temporal representations by transforming combined embedding."}, {"title": "3.5. Dimension corruption.", "content": "In self-supervised learning, the main objective is to learn an embedding space where positive pairs (or views) remain close to each other, while negative pairs (or views) are far apart. When generating views, mixup (Zhang et al. 2017) or cutmix (Yun et al. 2019) methods are most commonly used. These methods are suitable for invariant augmentation of static data (e.g., images), however, these are not suitable for time-series data (e.g., stocks). We generate views for temporal variants on the same instance, unlike conventional SSL methods that use invariant augmentation by using different instances together. For time-series data, mixing different sequences would break the entire temporal structure. Therefore, we propose a dimension corruption method for the augmentation of temporal data.\nFirst, we create positive and negative views, $H_{pos}^s$ and $H_{neg}^s$, by randomly shuffling the dimensions within the token embeddings TKE. Here, we define two permutation matrices, $P_{pos}^s$ and $P_{neg}^s$, both of size d x d. 1\n$H_{pos}^s = \u03bbTKE_s + (1 \u2212 \u03bb)TKE_s P_{pos}^s$\n$H_{neg}^s = (1 - \u03bb)TKE_s + \u03bbTKE_s P_{neg}^s$(8)\nIn this case, the formula (8Temporal Representation Learningequation.3.8) generates positive and negative views for self-supervised learning."}, {"title": "3.6. Representation module.", "content": "The representation module $f_\u03b8$ aims to characterize the shift between different domains by refining the parameters \u03b8s through the process described in Section 3.2. In order to effectively reflect temporal patterns of corrupted token embeddings ($H_{pos}^s$ and $H_{neg}^s$), we use the self-attention mechanism (Vaswani et al. 2017). The self-attention mechanism aggregates corrupted token embeddings with normalized importance as follows:\nAttention(Q, K, V) = Softmax($QK^T/\u221ad$)V.                                     (9)\nHere, Q = HWQ \u2208 Rd\u00d7dk, K = HWK \u2208 Rd\u00d7dk and V = HWv \u2208 Rd\u00d7dv represent queries, keys, and values, respectively. Note that H represents token embeddings (either positive or negative), while WQ, WK, and Wy are learnable matrices that share weights between positive and negative token embeddings. The output, which has a dimension of dv, is then transformed back into an embedding of dimension d through a fully connected layer. Finally, the outputs $ST_{neg}^s$ and $ST_{pos}^s$ are obtained."}, {"title": "3.7. Triplet loss.", "content": "For SimStock, we train it to minimize a triplet loss (Balntas et al. 2016), which is a popular choice in SSL. The key idea behind triplet loss is the use of triplets, each of which consists of an anchor, and positive and negative views. Here, the anchor is the embeddings for the unperturbed combined embedding.\nFor the triplet ($ST_{pos}^s, ST_{neg}^s, H^s$), where $ST_{pos}^s$ is the positive view, $ST_{neg}^s$ is the negative view, and Hs is the combined embedding (anchor), the triplet loss is defined as follows:\nLtriplet = max(0, sim($H^s$, $ST_{pos}^s$) \u2212 sim($H^s$, $ST_{neg}^s$) + \u03b1)(10)\nIn the above equation, sim(\u00b7,\u00b7) denotes a similarity measure (e.g., cosine similarity or Euclidean distance), and \u03b1 > 0 is a margin that is introduced to separate positive pairs from negative pairs.\nThe intuition behind this loss function is that we want to ensure that the anchor point gets closer to the positive sample than to the negative sample by at least the margin \u03b1."}, {"title": "3.8. Inference phase.", "content": "In our framework, the inference phase is particularly important. Unlike most existing contrastive representation learning studies (Chen et al. 2020, Grill et al. 2020), our model is specifically designed to be robust with respect to temporal distribution shifts. The inference phase consists of passing the target domain $D_{s+1}$ through the embedding module to obtain the combined embedding $H^{s+1}$ and further processed by the feature tokenizer module to obtain the token embeddings $TKE^{s+1}$. The stock representation is then obtained by feeding TKEs+1 into the representation model $f_{\u03b8s+1}$, which is updated with the optimal parameters $\u03b8_{s+1}$ generated by the TDG method described in Section section 3.2"}, {"title": "4. Experiment", "content": "Now we present experiment results to thoroughly demonstrate the performance of SimStock on real-world benchmark datasets. Our code and configurations will be publicly available on GitHub."}, {"title": "4.1. Implementation details", "content": "We present the details of tasks, datasets, baseline models, hyperparameter selection, evaluation metrics, and the experiment setting."}, {"title": "4.1.1. Tasks.", "content": "We conduct four main experiments to assess the effectiveness of SimStock in various financial applications:\n\u2022 Finding similar stocks: We evaluate how well SimStock can identify stocks that exhibit similar price movements, compared to baseline methods. Given a query stock, we consider both same exchange (finding similar stocks within the same exchange) and different exchanges (finding similar stocks in a different exchange from the query stock) scenarios;\n\u2022 Pairs trading: We evaluate the effectiveness of pairs trading based on the similar stocks identified by SimStock. We form pairs of stocks that are found to be similar, and then execute a pairs trading strategy;\n\u2022 Index tracking: Given an ETF as the query, we find a basket of similar stocks using SimStock and assess the performance of using these stocks for index tracking. The goal is to see if the selected stocks can closely mimic the returns of the target ETF;\n\u2022 Portfolio optimization: We investigate whether SimStock embeddings can enhance portfolio optimization. Specifically, we replace the correlation matrix of stock returns using the SimStock embedding as a similarity measure for mean-variance portfolio optimization. We compare the performance of the resulting portfolios with those obtained using conventional methods for estimating."}, {"title": "4.1.2. Datasets.", "content": "We collect daily stock price features (Open, High, Low, Close, and Volume) and sector information from Yahoo Finance for stocks listed on five major exchanges: NYSE (New York Stock Exchange), NASDAQ (National Association of Securities Dealers Automated Quotations), SSE (Shanghai Stock Exchange), SZSE (Shenzhen Stock Exchange), and TSE (Tokyo Stock Exchange). In our study, we combine NYSE and NASDAQ stocks (4,231 in total) and refer to them as the US exchange, while treating SSE (1,407 stocks), SZSE (1,696 stocks), and TSE (3,882 stocks) separately. The data spans three periods: a training period from January 1, 2018, to December 31, 2021; a reference period from January 1, 2022, to December 31, 2022; and a test period from January 1, 2023, to December 31, 2023. This timeline allows us to verify whether stocks identified as similar in the reference period maintain their similarity in the subsequent one-year test period.\nOur dataset is less susceptible to survivor bias because the source domain is organized as a sequence of domains {$D_1, D_2, ..., D_T$} over time. Each source domain Ds consists of a distinct set of stocks that existed at time step s. This allows the model to reflect the realistic entry and exit of stocks at each time step. For example, Ds-1 will contain stocks that existed at time s \u2212 1, but may not contain all stocks from Ds if some were delisted. Similarly, Ds can include newly added stocks that were not present in Ds\u22121. Furthermore, the price data used for each stock is adjusted to account for corporate actions such as splits and dividends.\nWe generate normalized input features describing the trend of a stock on day t. The variables $Z_{Open}$, $Z_{High}$ and $Z_{Low}$ represent the comparison values of the opening, highest, and lowest prices, respectively, relative to the closing price of the same day. Also, $Z_{Close}$ and $Z_{Volume}$ represent the comparative values of the closing prices and the volume values compared with day t-1, respectively. Refer to table 1 for the formulas used to calculate each feature. In addition, we calculated stock price features for 5, 10, 15, 20, 25, and 30-day intervals for the temporal feature variant as discussed in section 3.3."}, {"title": "4.1.3. Baseline models.", "content": "For the task of finding similar stocks, index tracking and pairs trading task, we compare SimStock with the following baselines:\n\u2022 Corr1: Calculates the correlation of stock returns using the past one-year returns (i.e., from January 1, 2022 to December 31, 2022).\n\u2022 Corr2: Calculates the correlation of stock returns using returns from the beginning of the test period (i.e., from January 1, 2018 to December 31, 2022).\n\u2022 Peer: Uses the list of similar stocks provided by Financial Modeling Prep.1\n\u2022 TS2VEC (Yue et al. 2022): A state-of-the-art method based on self-supervised learning for finding similar time-series data.\nFor the portfolio optimization task, we compare the performance of portfolios constructed using the SimStock similarity matrix with those constructed using the following conventional correlation matrices:\n\u2022 Historical covariance matrix(HC) (Jobson and Korkie 1980): The sample correlation matrix of the most recent past stock returns.\n\u2022 Shrinkage method(SM) (Ledoit and Wolf 2004a): A shrinkage estimator of the covariance matrix proposed by (Ledoit and Wolf 2004a).\n\u2022 Gerber statistic(GS) (Gerber et al. 2021): A robust correlation measure that counts the proportion of simultaneous co-movements between assets when their amplitudes exceed data-dependent thresholds."}, {"title": "4.2. Can SimStock find similar stocks?", "content": "In this section, we explore two distinct scenarios for identifying similar stocks using our proposed model: the same exchange scenario and the different exchanges scenario. The same exchange scenario focuses on finding similar stocks within the same exchange given a query stock. This approach allows for the identification of stocks with comparable characteristics and behaviors within a specific market. On the other hand, the different exchanges scenario involves finding similar stocks within another exchange given a query stock. This scenario leverages the concept of transfer learning, where the trained weights of a model from one exchange are applied to stock data from a different exchange. Transfer learning is a machine learning technique that leverages the knowledge gained from a model trained on a source task to improve the performance of a model on a related target task (Pan and Yang 2009). In our study, we apply transfer learning by utilizing a model trained on one stock exchange (source domain) to identify similar stocks in another exchange (target domain). For example, models trained on the US exchange can be used to find similar stocks in the SSE, SZSE, or TSE exchanges. Notice that the query is not restricted to individual stocks. It can be either sector indices or ETFs."}, {"title": "4.2.1. Evaluation metrics.", "content": "To evaluate the performance of models in finding similar stocks compared to other models, we employ two widely used metrics: Correlation and Dynamic Time Warping (DTW). DTW is a measure for measuring the similarity between two temporal sequences, allowing for non-linear alignments and capturing the similarities in the overall patterns of stock price movements (Berndt and Clifford 1994). Consider two time-series sequences X = {$x_1,..., x_m$} and Y = {$y_1, \u2026\u2026\u2026, y_n$}. The DTW between X and Y is defined as:\nDTW(X, Y) = $\u2211_{v(\u03be) \u2208\u03c0} ||x_i - y_j||^2$.             (11)\nHere, an alignment path of length K is a sequence of K index pairs (i, j)k, where max(m, n) \u2264 K\u2264m+n\u2013 1. Also, ||.|| is the Euclidean distance. DTW uses global path constraints while comparing two time-series sequences X and Y. That is, the pairs i and j are constrained so that |i - j| \u2264 r, where r is a predefined radius, in the case of the Sakoe-Chiba band.\nTo assess the effectiveness of our approach in capturing similarities and predicting future stock behaviors, we evaluate the model's performance using an out-of-sample period. The dataset used for inference is the year immediately preceding the out-of-sample period. By finding similar stocks in the previous year and comparing their performance to the target stock in the out-of-sample period, we measure the model's performance using Correlation and Dynamic Time Warping (DTW) measures. We consider the top 9, 7, 5, 3, and 1 similar stocks (namely, TOP@9, TOP@7, TOP@5, TOP@3, and TOP@1) for evaluation."}, {"title": "4.2.2. Quantitative evaluation: same exchange scenario.", "content": "The diagonal plots in fig. 3 and fig. 4 illustrate the performance (Correlation and DTW) of different models in same exchange scenario. The diagonal plots show that our proposed SimStock consistently outperforms the baselines (Corr1, Corr2, Peer, and TS2VEC) in finding similar stocks within the same exchange, across all four stock markets (US, SSE, SZSE, and TSE). For example, in the US to US scenario, SimStock achieves a correlation of around 0.8 for the top similar stock (TOP@1), substantially higher than the 0.6 correlation of the best performing TS2VEC.\nSimilarly, for the SSE to SSE and TSE to TSE scenarios, SimStock consistently shows superior performance compared to the baselines, with correlations close to 0.8 for TOP@1, but SimStock and TS2VEC gave equivalent results in SZSE to SZSE. Note that the performances of all the baseline models were not much different. It is interesting that the peer stocks picked by investment platforms"}, {"title": "4.2.3. Quantitative evaluation: different exchanges scenario.", "content": "The off-diagonal plots in fig. 3 and fig. 4 represents the outcomes of identifying for similar stocks in exchanges different from the exchange of the query stock. Note that Peer is not available for this scenario, because most trading platforms do not provide information on similar stocks in other exchanges. The off-diagonal plots demonstrate that SimStock outperforms the baselines in all different exchanges scenarios,indicating its effectiveness in leveraging temporal domain generalization to identify similar stocks across different markets. For instance, in the US to SSE, US to SZSE and US to TSE scenarios, SimStock achieves correlations around 0.3 for TOP@1, surpassing the performance of Corr1, Corr2, and TS2VEC. Similarly, for the SSE to US, SSE to SZSE and SSE to TSE scenarios, SimStock exhibits higher correlations compared to the baselines, showcasing its ability to capture similarities between stocks in these closely related exchanges.\nHowever, it is important to note that the overall performance of all models in the different exchanges scenarios is lower compared to the same exchange scenario. This can be attributed to the inherent differences in market dynamics, regulations, and economic factors across different exchanges. Despite these challenges, SimStock consistently outperforms the baselines in all cases, highlighting its robustness and adaptability in finding similar stocks across diverse market conditions.\nThe DTW results in the off-diagonal plots further support the superiority of SimStock in identifying similar stocks across exchanges. The DTW approach is a more suitable alternative to the correlation metric when considering different timestamps. SimStock consistently achieves lower DTW distances compared to the baselines, indicating its ability to capture the overall patterns and similarities in stock price movements, even when applied to different markets. These findings underline the potential of SimStock in enabling investors and analysts to identify similar stocks across different exchanges."}, {"title": "4.2.4. Qualitative evaluation.", "content": "The similarity between stocks should not be measured only on time-series distances. Similar stocks may be in different industries, but they should have similar fundamental information. Unfortunately, however, it is almost impossible to measure such similarity in a quantitative way. Therefore, we provide a qualitative analysis of those results.\nTo assess the effectiveness of SimStock in identifying fundamentally similar stocks, we conduct a nuanced analysis by examining the actual stocks recommended by each method for a diverse set of query stocks from various sectors (table 2). We compare the top@3 similar stocks identified by SimStock with those of the baseline methods, including TS2VEC, Corr1, Corr2, and Peer, to evaluate whether SimStock captures relevant business characteristics, financial attributes, and industry-specific similarities better than the baselines.\nWhen we begin by examining the technology sector, SimStock identifies highly relevant similar stocks for query stocks such as Apple (AAPL), Microsoft (MSFT), and Meta (META). For AAPL, SimStock's top@k similar stocks include Microsoft (MSFT), Tyler Technologies (TYL), and Intuit (INTU), all well-established companies in the software and technology domain. In contrast, the baseline methods identify a mix of technology and non-technology companies, such as Amazon (AMZN) for TS2VEC and Corr2, and Thermo Fisher Scientific (TMO) for Corr1. Similarly, for MSFT, SimStock identifies Cadence Design Systems (CDNS), Manhattan Associates (MANH), and Tyler Technologies (TYL) as similar stocks, while the baselines include manufacturing companies like Danaher (DHR) for Corr1.\nNext, we analyze the healthcare sector, where SimStock identifies relevant similar stocks for query stocks such as Pfizer (PFE) and Centene (CNC). For PFE, SimStock's top similar stocks include BioNTech (BNTX), Moderna (MRNA), and Johnson & Johnson (JNJ), all major players in the pharmaceutical and biotechnology industries. The baseline methods, however, identify less relevant stocks, such as NantKwest (NKNG) for TS2VEC and Metrocity Bankshares (MCBS) for Corr1. For CNC, SimStock identifies other health insurance and financial services companies like Bank of Montreal (BMO) and Morgan Stanley (MS), while the baselines include less related stocks, such as Humana (HUM) for TS2VEC and Stericycle (SRCL) for Corr2.\nIn the energy sector, SimStock identifies industry-specific similar stocks for the query stock Exxon-Mobil (XOM), such as Marathon Oil (MRO), Cenovus Energy (CVE), and Hess Corporation (HES). The baseline methods also identify some relevant energy stocks, such as MRO for TS2VEC and Murphy Oil Corporation (MUR) for Corr1 and Marathon Petroleum (MPC) for Corr2. That is, all methods successfully identified stocks that are similar to XOM.\nFinally, we examine the financial sector, where SimStock identifies relevant similar stocks for query stocks such as Wells Fargo (WFC) and Visa (V). For WFC, SimStock's top similar stocks include Bank of America (BAC), Fifth Third Bancorp (FITB), and F.N.B. Corporation (FNB), all well-known banking institutions. Similar, the baseline methods also identify relevant banking stocks, such as BAC for Corr2, JPMorgan Chase (JPM) for TS2VEC and Berkshire Hills Bancorp (BHLH) for Corr1. For V, SimStock identifies Mastercard (MA), Stifel Financial (SF), and Inter-continental Hotels Group (IHG) as similar stocks, while the baselines include less related stocks, such as Planet Fitness (PLNT) for Corr2.\nThe above provides further reassurance that SimStock robustly identifies relevant companies as similar to each other in a way intuitive to most human market participants."}, {"title": "4.3. Application to pairs trading", "content": "In this section, we investigate the practical application of the similar stocks identified by SimStock in the context of pairs trading. Pairs trading is a market-neutral trading strategy that exploits the relative mispricing between two highly correlated securities (Gatev et al. 2006). The strategy involves simultaneously buying the relatively underpriced security and selling the relatively overpriced security, with the expectation that the prices will converge to their long-term equilibrium. We assess the profitability of pairs trading using the top@k similar stocks identified by SimStock and compare its performance with benchmark models, namely TS2VEC, Corr1, and Corr2. We added Cointegration method (Coint) (Nelson and Plosser 1982, Enders 2004) to the benchmark model, which selected 3 stocks from the US exchange that exhibited the strongest cointegration relationships, as determined by their p-values in descending order."}, {"title": "4.3.1. Pairs trading procedure.", "content": "We employ the price ratio approach for pairs trading, which involves constructing a spread between two securities by computing the ratio of their prices. The spread is then normalized by its historical mean and standard deviation to obtain the Z-score, which measures the deviation of the current spread from its historical average in terms of standard deviations. The Z-score is calculated as follows:\nPRt = $P_{q,t}/P_{s,t}$\nZt = $(\u03bc_{PR} \u2212 PR_t)/\u03c3_{PR_t}$(12)\nwhere Pq,t and Ps,t are the prices of the query stock and similar stock in the pair at time t, and \u03bc1, \u03bc2, and \u03c32 are the rolling mean and standard deviation of the price ratio up to time t, calculated using lookback periods of L\u2081 and L2 trading days, respectively.\nTrading signals are generated based on the Z-score, with the opening of a long-short position when the Z-score exceeds a pre-specified threshold of \u00b11.25 SD, indicating a significant deviation from the long-term equilibrium. Specifically, when Zt > 1.25 SD, we sell the overpriced security and buy the underpriced security. Conversely, when Zt < -1.25 SD, we buy the overpriced security and sell the underpriced security. The position is closed when the Z-score reverts back within the range [-0.5 SD, 0.5 SD], suggesting price convergence. To limit potential losses, a stop-loss (Hwang et al. 2023b) mechanism is implemented, whereby positions are closed if the portfolio value drops below a predetermined threshold Lstop of 500 USD.\nTo determine optimal values for the hyperparameters L\u2081 and L2, we perform a grid search over the training period from 2022-01-01 to 2022-12-31. Different L\u2081 and L2 values are determined for each query stock paired with each of its top 3 similar stocks identified by SimStock and the benchmark models (TS2VEC, Corr1, Corr2), resulting in 3 pairs per query stock per model. Pairs trading is applied independently to each pair, with overall performance for each query stock and model calculated as the average wealth across the 3 pairs, starting with an initial trading capital"}, {"title": "4.3.2. Evaluation of pairs trading performance.", "content": "table 3 presents the average terminal wealth and maximum drawdown (MDD) achieved by applying pairs trading to the top@3 similar stocks identified by SimStock and the benchmark models (TS2VEC, Corr1, Corr2, Coint) for each query stock. The results of pair trading between query stock and TOP@1 stock can be found in Appendix B.\nSimStock achieves the highest average terminal wealth for 6 out of the 12 query stocks (AAP, CMG, MSFT, WFC, BA and CVS). Coint performs best for 2 query stocks (XOM and PFE), while Corr1, Corr2 and TS2VEC each perform best for 1 query stock (MA, AMZN and V respectively). For the remaining query stocks, SimStock ranks second in terms of wealth for 4 cases (V, XOM, META and MA).\nRegarding maximum drawdown, SimStock exhibits the lowest MDD for 7 out of the 12 query stocks (AAPL, CMG, MSFT, WFC, PFE, META and MA), indicating better risk management. TS2VEC, Corr1, Corr2 and Coint achieve the lowest MDD for 1 query stock each (AMZN, V, CVS and XOM respectively). For the remaining 5 query stocks (V, XOM, AMZN, BA and CVS), SimStock has the second-lowest MDD. The ability to generate higher returns while maintaining lower drawdowns suggests the robustness of the pairs identified by SimStock.\nIt is important to note that TS2VEC, Corr2, and Coint failed to generate buy/sell signals for certain stocks. Specifically, TS2VEC and Coint were unable to generate signals for AAPL and WFC, while Corr2 failed to do so for WFC. SimStock, on the other hand, generated a buy/sell signal for one out of the three stocks, specifically META.\nThe results demonstrate the superior performance of SimStock in identifying profitable pairs for trading compared to the benchmark models. SimStock's ability to achieve the highest average terminal wealth for the majority of query stocks while maintaining the lowest maximum drawdown for most cases highlights its effectiveness in capturing meaningful similarities between stocks that translate into successful pairs trading strategies."}, {"title": "4.4. Application to index tracking of thematic ETFs", "content": "Thematic ETFs have gained popularity among retail investors who wish to invest in specific themes or trends, such as innovative technologies. These ETFs are composed of assets that share a common theme, providing a contrast to traditional sector-based classifications, which can be slow and ambiguous in reflecting technological advancements and innovations. As thematic ETFs aim to identify innovative tech companies, many investors are interested in tracking these ETFs.\nIn this section, we compare the performance of SimStock and other baselines in identifying stocks to track four popular thematic ETFs. Our goal is to find similar stocks from the US, SSE, SZSE, and TSE exchanges that can effectively track the performance of these thematic ETFs. In other words, we use the thematic ETFs as queries to find similar stocks in each model.\nFor this analysis, we selected the four popular thematic ETFs: ARK Innovation ETF (ARKK), First Trust Cloud Computing ETF (SKYY), Global X Robotics & AI ETF (BOTZ), and Global X Lithium & Battery Tech ETF (LIT). The in-sample and out-of-sample periods are the same as those used in section 4.1."}, {"title": "4.4.1. Evaluation metrics.", "content": "To evaluate the performance of SimStock and the baselines in tracking thematic ETFs, we first find the top k similar stocks based on the thematic ETF queries. We then create equal-weighted portfolios using these similar stocks to track the thematic ETFs. We use two metrics to assess the tracking performance: Tracking Error (TE) and Tracking Error Volatility (TEV). (Focardi and Fabozzi 2004).\nThe end-of-the-period tracking error (TE) is calculated as follows:\nTE = $\u221a{1/n \u2211_{j=1}^n (R^i_j - R^p_j)^2 }$(13)\nwhere $R^i_j$ and $R^p_j$ are the cumulative returns of the query ETF and the tracking portfolio at period j, respectively, and n is the number of periods.\nTracking Error Volatility (TEV) measures the volatility of the difference in returns between the tracking portfolio and the target ETF. It is calculated as the standard deviation of the difference in returns:\nTEV = $\u221a{1/n \u2211_{j=1}^n ([R^i_j - R^p_j] - E[R^i - R^p])^2}$(14)\nwhere $E[R^i - R^p]$ is the mean difference in returns between the target ETF and the tracking portfolio over the n periods. For each thematic ETF, we find the top@k similar stocks on each exchange (US, SSE, SZSE, TSE) using SimStock and the baseline methods. We then create equal-weighted portfolios using these similar stocks to track the performance of the thematic ETFs."}, {"title": "4.4.2. Quantitative evaluation.", "content": "In this subsection, we quantitatively evaluate the performance of SimStock and the baseline methods in tracking the four thematic ETFs. table 4 report the tracking error (TE) for each ETF using the top@k similar stocks identified by each method across the four exchanges. The best performing method for each k and exchange is highlighted in bold, while the second best is in underline. The results of the evaluation with Tracking Error Volatility (TEV) can be found in Appendix B.\nA clear pattern emerges from the results: SimStock consistently achieves the best (bold) or second lowest (underline) tracking error for the majority of k values across three of the four ETFs (ARKK, SKYY, and BOTZ) and exchanges. This strong performance demonstrates that the similar stocks identified by SimStock are highly effective at tracking these thematic ETFs. In contrast to SimStock, while the Corrl and Corr2 baselines show reasonable performance in some cases, they lack consistency across different scenarios. TS2VEC, on the other hand, generally exhibits higher tracking errors, indicating that the stocks it identifies do not track the ETFs as closely.\nNotably, SimStock's performance on the US exchange is particularly impressive for ARKK, SKYY, and BOTZ. It substantially outperforms the other methods, especially for lower values of k (e.g., top 10-20 similar stocks), suggesting that SimStock excels at identifying a concentrated set of the most relevant stocks for tracking these ETF themes. SimStock's ability to identify highly relevant stocks in this market, even with a smaller number of stocks (low top@k values), is a testament to its effectiveness and precision in capturing the key components driving these thematic ETFs. While the performance gap between SimStock and the other methods is less pronounced on the other exchanges, SimStock still achieves the best or second-best performance in most cases for ARKK, SKYY, and BOTZ. However, SimStock's performance in tracking the Global X Lithium & Battery Tech ETF (LIT) is relatively weaker compared to the other ETFs.\nIn addition to the quantitative evaluation using tracking error and tracking error volatility, we can also visually assess the performance of SimStock and the baseline methods in tracking the thematic ETFs by examining their cumulative return curves. fig. 5 showcases the cumulative return curves for each thematic ETF (ARKK, SKYY, BOTZ, and LIT) and the corresponding tracking portfolios created using the top@10 similar stocks identified by each method on the US exchange.\nThe cumulative return curves provide a clear visual representation of how closely the tracking portfolios match the performance of their respective thematic ETFs over time. For ARKK, SKYY, and BOTZ, the SimStock portfolio (shown in blue) closely follows the trajectory of the thematic ETF (shown in dotted black), indicating that the top 10 stocks identified by SimStock effectively capture the performance of these ETFs. In contrast, the portfolios based on the other methods (TS2VEC in purple, Corr1 in green, and Corr2 in red) exhibit larger deviations from the ETF curves, suggesting that their selected stocks are less effective at tracking the ETFs.\nThe cumulative return curves represent SimStock's ability to identify a concentrated set of stocks that effectively track the performance of thematic ETFs, particularly for ARKK, SKYY, and BOTZ, based on the top@10 similar stocks from the US exchange."}, {"title": "4.4.3. Qualitative evaluation.", "content": "For the ARK Innovation ETF (ARKK), which focuses on disruptive innovation, SimStock identifies stocks of companies involved in cutting-edge technologies such as 3D computing (MLTS, DDD), a fintech (SQ), and artificial intelligence (SHOP) company. Similarly, for the First Trust Cloud Computing ETF (SKYY), SimStock finds stocks of leading cloud computing (NET, DBX, TYL, GWRE and GLOB) and software-as-a-service (INTU, ESTC and PAYC) providers. The qualitative assessment of SimStock's similar stock selections for all thematic ETFs can be found in Appendix C."}, {"title": "4.5. Application to portfolio optimization", "content": "In this section, we investigate whether SimStock embeddings can enhance portfolio optimization. Specifically, we construct the correlation matrix using the SimStock embedding as a similarity measure, and use it as an input for portfolio optimization. We compare the portfolio performance using the SimStock embedding with other covariance estimators. Notice that SimStock is designed to identify stocks with similar behavior, but it does not actively search for stocks that exhibit completely different dynamics compared to a given stock. This may limit its ability to ensure optimal portfolio diversification. While our approach uses deep learning to estimate the covariance matrix (\u03a3) for portfolio optimization, it differs from existing deep learning-based portfolio optimization methods. Most existing deep learning approaches for portfolio optimization focus on predicting optimal portfolio weights (Zhang et al. 2021, Uysal et al. 2023) or estimating portfolio parameters, particularly the expected returns (\u03bc) (Butler and Kwon 2023, Costa and Iyengar 2023). To the best of our knowledge, no prior deep learning-based portfolio research has specifically targeted the"}, {"title": "4.5.1. Optimization formulation.", "content": "We consider the Mean-Variance Optimization (MVO) framework (Markowits 1952) for portfolio optimization. The MVO approach determines the optimal asset weights (wi) in a portfolio, subject to constraints on risk and return, assuming the expected return (\u03bc\u2081), variance (\u03c32), and covariance (\u03c3ij) of each asset are known. This study adopts the experimental design and methodology established by (Gerber et al. 2021). We randomly sampled the asset classes 100 times with 10, 30 and 50 stocks each from the S&P500 and JPX Prime 150. The long-only MVO problem, accounting for transaction costs, is formulated as:\nMaximize: $w^T\u03bc - \u03c8 \u2211^N_{i=1} |w_i - w_{0i}|$ Subject to: $w^T \u03a3 w \u2264 \u03c3_{target}^2$,  \u2211_{i=1}^N} w_i = 1, 0 \u2264 w_k \u2264 1,\u2200k = 1,2,\u2026, N.(15)\nThe objective function seeks to maximize the portfolio's expected return ($w^T\u03bc$) while minimizing transaction costs, represented by the term $\u03c8 \u2211^N_{i=1} |w_i - w_{0i}|$. Here, \u03c8 denotes the fixed proportional transaction cost, set to 10 basis points (0.1%) in this study, and wo represents the previous portfolio weights. By including this term, we penalize deviations from the previous weights, effectively controlling portfolio turnover and reducing trading costs.\nThe optimization is subject to three constraints: (1) the portfolio variance must not exceed a predetermined risk target ($\u03c3_{target}^2$), (2) the sum of portfolio weights must equal unity, and (3) individual asset weights must be non-negative and not exceed one, prohibiting short selling. By solving this constrained optimization problem, we obtain the optimal portfolio weights w* for a given risk target \u03c3target and transaction cost \u03c8. The set of optimal portfolios across various risk levels constitutes the efficient frontier, with each point representing the highest achievable return for a specific level of risk.\nWe estimate the expected return \u03bcti for asset i at time t using the sample mean of its historical returns over a T-month lookback window. For the covariance matrix \u03a3, we compare the performance of historical covariance (HC), shrinkage method (SM) (Ledoit and Wolf 2004a), Gerber statistic (GS) (Gerber et al. 2021), TS(TS2VEC) (Yue et al. 2022) embedding and SimStock embedding (SS)."}, {"title": "4.5.2. Backtesting procedure.", "content": "To evaluate the performance of the different covariance estimators in the context of portfolio optimization, we employ the following backtesting procedure. Starting from Jan 2021, at the beginning of each month, we use the monthly returns of the current asset universe over the previous T = 12 months to estimate the expected return vector \u03bc and the covariance matrix \u03a3.\nWe then apply a quadratic optimizer to solve for the optimal portfolio weights w* given a specific risk target \u03c3target for MVO and the optimal portfolio weights w* that minimize the portfolio variance for MVP. The previous portfolio is rebalanced according to the new optimal weights, and this optimized portfolio is held for one month. At the end of the month, the realized portfolio return is calculated as $w^{*T}r$, where r represents the vector of realized asset returns for that month. This process is repeated by rolling the in-sample period forward by one month and computing the updated efficient portfolio for the next month. The rolling-window approach allows the portfolios to adapt to structural changes in the market and mitigates data mining bias. As the initialization of the first portfolio requires two years of monthly returns, our performance evaluation covers the period from Jan 2022 to Feb 2024."}, {"title": "4.5.3. Efficient frontiers.", "content": "In this subsection, to evaluate the performance of portfolios optimized using the SimStock embedding covariance matrix (\u2211ss), we compare their ex-post efficient frontiers with those of portfolios constructed using traditional covariance estimators, namely the historical covariance (HC), shrinkage method (SM), Gerber statistic (GS) and TS2VEC (TS), denoted as \u03a3HC, ESM, Egs and \u03a3\u03c4s, respectively. To obtain the covariance matrix Ess using the TS2VEC and SimStock embedding, we first compute the pairwise L2 distances between the embeddings of each stock pair. These distances are then scaled to the range [-1, 1] using max-min normalization, which brings the stocks closer to 1 in the correlation matrix for smaller L2 distances, indicating greater similarity between stocks. The resulting normalized distances are used to populate the correlation matrix, represents the correlation coefficient between stocks i and j as estimated by embeddings. We then compute the covariance matrix \u03a3\u03c4\u03c2 and Ess. The analysis is conducted on portfolios of varying sizes (10, 30, and 50 stocks) from both the S&P500 and JPX"}, {"title": "4.5.4. Detailed investment performances.", "content": "This section compares the performance of five portfolio construction methods (Simstock embedding, Historical Covariance, Shrinkage Method, Gerber Statistic, and TS2VEC embedding) for MVO, using 30-stock portfolios from the S&P 500 and JPX Prime 150 universes. See Appendix E for results for 10 and 50 stocks.\ntable 5 and table 6 present the performance metrics for the target risk optimizations in the S&P500 and JPX Prime 150 universes, respectively, at four different risk target levels (24%, 27%, 30%, and 33%). See Appendix E for results for 10 and 50 stocks.\nIn the S&P500 universe (table 5), the Simstock embedding (SS) method produced the highest returns at all risk target levels while maintaining risk measures comparable to the other methods. Returns and risk measures increased for all methods as the risk target rose. The Historical Covariance (HC), Shrinkage Method (SM), and Gerber Statistic (GS) methods performed similarly to each other, but slightly worse than SS, particularly at higher risk levels. For example, at the 24% risk target, SS achieved an 11.31% arithmetic return compared to 10.04%, 9.61%, and 10.14% for HC, SM, and GS respectively. At the 33% risk target, SS's return increased to 13.33%, while HC, SM and GS saw returns of 12.50%, 12.85% and 12.69%. The TS2VEC embedding (TS) method consistently underperformed the others, with a 9.40% return at the 24% risk level and 11.49% at the 33% level, below all other methods.\nThe JPX Prime 150 universe (table 6) showed similar patterns, with SS producing the best returns, followed by HC, GS, SM and TS in that order across risk levels. SS achieved a 20.12% return at the 24% risk target and 22.47% at the 33% target. In contrast, TS had the lowest returns at 14.60% and 18.45% for those risk levels. The HC, GS, and SM methods produced returns that were higher than TS but lower than SS at each corresponding risk level.\nDespite potentially higher volatility which is evident in the ex-post efficient frontiers in fig. 6, the performance of the SS method highlights its potential as a meaningful and effective stock embedding technique for portfolio optimization."}, {"title": "4.5.5. Characteristics of SimStock similarity matrix.", "content": "To investigate what distinguishes the SimStock embedding method from other portfolio optimization methods, we can compare the correlation matrices generated by different methodologies. Specifically, we propose the following mapping:\nIF = $|| MD - RC_{future} ||_F / || MD - RC_{past} ||_F< 1$(16)\nHere, MD refers to the correlation matrix obtained using a specific methodology (e.g., SS, SM, GS or TS), while $RC_{future}$ and $RC_{past}$ represent the realized correlation matrices for the future and past periods, respectively.\nIf the value of this mapping is less than or close to 1, it implies that $|| MD - RC_{future} ||_F \u2264 || MD - RC_{past} ||_F$. This means that the current methodology is tracking the future correlation matrix well, as the Frobenius distance between the method's correlation matrix and the future realized correlation matrix is smaller than the distance between the method's correlation matrix and the past correlation matrix.\nfig. 7 illustrates the Frobenius distances for different methodologies in the S&P500 and JPX Prime 150 universes. The figure compares the correlation matrices between the same months in consecutive years (e.g., Feb-2021 vs Feb-2022, Mar-2021 vs Mar-2022, etc.). For example, \"Feb-2021 vs Feb-2022\" means that the correlation series for the period from February 2021 to January 2022 is compared to the correlation series for the period from February 2022 to January 2023, representing the current year and the future year, respectively.\nFor the S&P500, the Frobenius distances for the SimStock embedding method (SS) are consistently lower than those of the shrinkage method (SM), Gerber statistic (GS) and TS2VEC embedding (TS) across all month pairs. This suggests that the SS method is better at tracking the future correlation matrix compared to the SM and GS methods.\nSimilarly, in the JPX Prime 150 universe, the SS method generally exhibits lower Frobenius distances than the SM, GS and TS methods for most month pairs. This indicates that the SS method is more effective at capturing the future correlation structure of the assets compared to the other methodologies."}, {"title": "5. Conclusion", "content": "In this paper, we examine SimStock, a novel temporal self-supervised learning framework that aims to learn robust and informative representations of financial time series data. By incorporating techniques from SSL and temporal domain generalization, SimStock captures the complex relationships between different financial assets while accounting for temporal shifts in the data distribution. The proposed dimension corruption method integrates temporal patterns into the corruption process, enabling SimStock to learn representations that are robust to noise and non-stationarity in the data.\nWe conduct extensive experiments on four real-world benchmarks with thousands of stocks to demonstrate the effectiveness of SimStock in finding similar stocks. Our results show that SimStock consistently outperforms existing methods in both same exchange and different exchanges scenarios, achieving state-of-the-art performance in terms of accuracy and robustness. A qualitative analysis further highlights the ability of SimStock to identify stocks with similar fundamental characteristics and industry-specific similarities, simplifying the process of screening potential investment opportunities.\nThe practical utility of SimStock is demonstrated through various financial applications, including pairs trading, index tracking, and portfolio optimization. In the pairs trading experiment, using the similar stocks identified by SimStock leads to superior profitability compared to traditional methods. For index tracking, SimStock exhibits lower tracking errors and better alignment with the underlying themes of thematic ETFs, particularly for ARKK, SKYY, and BOTZ. Finally, in portfolio optimization, the SimStock embedding approach slightly outperforms traditional covariance estimators, achieving higher risk-adjusted returns across different portfolio sizes and risk levels.\nHowever, our current analysis does not examine the stability of the embedding space over time. While recent generative models (Kim et al. 2024) and large language models (LLMs) (Rho et al. 2024, Nie et al. 2024) have investigated such issues, the case of continuous time series remains unexplored. To address this gap, future research could focus on a theoretical analysis of the embedding space in the context of financial time series. This analysis could potentially incorporate notions of regime change (Wood et al. 2023) to better understand how the embedding space evolves over time and adapts to different market conditions. Also, this study primarily focused on time series data, utilizing sector information as the only static data input. Future research could expand on this approach by incorporating additional textual data, such as firm descriptions, news articles, and financial statements, to develop a more comprehensive representation model for stock data. This expanded approach has the potential to provide a deeper understanding of the complex dynamics between stocks."}]}