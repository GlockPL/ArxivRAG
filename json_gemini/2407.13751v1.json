{"title": "Temporal Representation Learning for Stock Similarities and Its Applications in Investment Management", "authors": ["Yoontae Hwang", "Stefan Zohren", "Yongjae Lee"], "abstract": "In the era of rapid globalization and digitalization, accurate identification of similar stocks has become increasingly challenging due to the non-stationary nature of financial markets and the ambiguity in conventional regional and sector classifications. To address these challenges, we examine SimStock, a novel temporal self-supervised learning framework that combines techniques from self-supervised learning (SSL) and temporal domain generalization to learn robust and informative representations of financial time series data. The primary focus of our study is to understand the similarities between stocks from a broader perspective, considering the complex dynamics of the global financial landscape. We conduct extensive experiments on four real-world datasets with thousands of stocks and demonstrate the effectiveness of SimStock in finding similar stocks, outperforming existing methods. The practical utility of SimStock is showcased through its application to various investment strategies, such as pairs trading, index tracking, and portfolio optimization, where it leads to superior performance compared to conventional methods. Our findings empirically examine the potential of data-driven approach to enhance investment decision-making and risk management practices by leveraging the power of temporal self-supervised learning in the face of the ever-changing global financial landscape.", "sections": [{"title": "1. Introduction", "content": "The identification of similar stocks is an essential task in finance, with far-reaching implications for portfolio diversification and risk management. Traditionally, the similarity between stocks has been determined based on factors such as industry sector, geographical location, and market capitaliza-tion. However, in the era of rapid globalization and digitization, these conventional approaches have become increasingly inadequate in capturing the complex dynamics of the global financial land-scape. The rapid pace of globalization has led to the emergence of intricate relationships among companies across different sectors and regions. Multinational corporations now operate in multiple countries, blurring the lines between traditional sector classifications. Additionally, the rise of digi-tal technologies has given birth to new industries and business models that transcend geographical boundaries. As a result, the similarities between stocks can no longer be accurately captured by relying solely on regional or sector-based classifications. Moreover, the non-stationary nature of financial markets poses significant challenges to the identification of similar stocks. The statistical properties of financial time series, including returns and correlations, are subject to change over time, a phenomenon known as concept drift or temporal shift (Lu et al. 2018, Bai et al. 2022). Consequently, similarity measures based on historical data may become unreliable and fail to cap-ture the evolving dynamics of the market. Indeed, (Wan et al. 2021) showed that news data can be useful for modelling evolving company relationships.\nRecent advances in deep learning-based methods in finance domain have shown promise in pre-dicting the desired parameters from the given data (Zhang et al. 2020). However, financial time series data presents unique challenges. The parameters of interest, such as expected returns, are not only difficult to predict due to the presence of excessive noise, but also the labels that can help the model extract hidden patterns in the data are often not well-defined. Moreover, the complex inter-actions among various financial assets and the impact of external factors, such as macroeconomic variables and market sentiment, further complicate the estimation process.\nOne promising solution to tackle these challenges is to leverage representation learning techniques to extract meaningful embeddings from unlabeled financial time series data, taking into account for temporal distribution shifts. By learning robust and informative representations, we can capture the complex relationships between different assets and exploit the inherent structure in the data. Self-supervised learning (SSL) has emerged as a powerful paradigm for learning such representations from unlabeled data, with successful applications in various domains, including computer vision (Chen et al. 2020, He et al. 2020) and natural language processing (Devlin et al. 2018, Yang et al. 2019). SSL enables the model to learn meaningful representations by solving pretext tasks that do not require explicit labels, making it particularly suitable for scenarios where labeled data is scarce or expensive to obtain. However, while these methods are useful for exploiting inductive biases in data used in any domain, their use for financial time series data has been less explored.\nTime series data exhibits distinct characteristics, such as seasonality, trend, and interaction between stocks, which require careful consideration when designing SSL frameworks. Furthermore, the non-stationary nature of financial markets necessitates the development of techniques that can adapt to distribution shifts and generalize well to future time periods. Most existing SSL methods focus on learning invariant representations (Chen et al. 2020, He et al. 2020), assuming that the data distribution remains stationary. However, this assumption does not hold in the context of financial markets, where the underlying dynamics can change over time.\nTo address these challenges, we combine techniques from SSL and the field of temporal do-main generalization. Our approach aims to learn general model representations that can adapt to temporal shifts over time, thereby enhancing the accuracy and robustness of financial parameter estimation using learned embeddings. By incorporating temporal domain generalization into the SSL framework, we enable the model to learn representations that are not only informative but also resilient to distribution shifts.\nThe main contributions of this paper are as follows:\n\u2022 We examine a novel temporal Self-Supervised Learning (SSL) framework, SimStock, that combines SSL with temporal domain generalization to learn robust and informative repre-sentations of financial time series data. SimStock leverages the power of SSL to capture the complex relationships between different financial assets while accounting for temporal shifts in the data distribution;\n\u2022 We introduce a new corruption method for SSL of stock data, termed dimension corruption, which integrates temporal patterns into the corruption process. By corrupting the input data along different dimensions, such as time, asset, and feature, SimStock learns representations that are robust to noise and non-stationarity in the data;\n\u2022 We conduct extensive experiments on four real-world benchmarks with thousands of stocks to demonstrate the effectiveness of SimStock in finding similar stocks. Our results show that SimStock achieves state-of-the-art performance, outperforming existing methods in terms of accuracy and robustness. We showcase the practical utility of SimStock in simplifying the process of screening potential investment opportunities;\n\u2022 We demonstrate the practical utility of SimStock in various financial applications, including pairs trading, index tracking, and portfolio optimization. We show that using the similar stocks identified by SimStock leads to superior performance compared to conventional methods in these applications, highlighting the potential of our approach to enhance investment strategies and risk management."}, {"title": "2. Related work", "content": "This section explores relevant research in two key areas: Self-Supervised Learning (SSL) for time series data and the estimation of financial parameters from historical data. We discuss the challenges and limitations of applying SSL techniques to time series and review various approaches aimed at improving the robustness and accuracy of parameter estimation in finance."}, {"title": "2.1. Self-supervised learning for time series data", "content": "Self-Supervised Learning (SSL) has emerged as a promising paradigm for learning robust and generalizable representations from unlabeled data, with successful applications in both computer vision and natural language processing (NLP) (Chen et al. 2020, Jing and Tian 2020, Zhai et al. 2019, Lee et al. 2019, Qiu et al. 2020, Ruder and Plank 2018, Song et al. 2020). SSL methods aim to overcome the limitations of traditional supervised learning, which requires large amounts of manually labeled data and can produce models that are non-robust and sensitive to small variations in the inputs.\nFirst and foremost, in the field of NLP, Self-Supervised Learning (SSL) methods, such as BERT (Devlin et al. 2018), GPT (Radford et al. 2019), and their variants (Mann et al. 2020, Touvron et al. 2023, Jiang et al. 2024), have achieved state-of-the-art results earlier than in other fields by pre-training on large corpora of unlabeled text data to learn to predict masked words or generate realistic text sequences. One reason for the success of SSL in NLP is the abundance of unlabeled text data. Later, SSL techniques were proposed to learn useful visual features by solving pre-tasks on unlabeled images, such as predicting the relative positions of image patches or identifying distorted versions of an image, in order to alleviate the label requirements and enable powerful feature extraction in computer vision as well (Wei et al. 2022, Fang et al. 2022). This sequential adaptation is not only due to the abundance of unlabeled text data in NLP, but also because language data is discrete and structured (i.e., words), while image data is high-dimensional, continuous, and amorphous, as discussed by (He et al. 2020), the success of SSL methodologies in NLP followed by success in computer vision is a natural result.\nIn the temporal domain, recent research on SSL methods has predominantly concentrated on video understanding (Jenni et al. 2020) or action classification (Qian et al. 2021). Video-based data is particularly suitable for SSL approaches because of the high correlation between frames, despite its temporal nature. Also, the temporal coherence and continuity present in videos provide a rich source of information that can be exploited to learn meaningful representations without the need for extensive manual labeling. Therefore, methodologies used in computer vision, such as contrastive learning and predictive modeling, can be easily adapted to the video domain. For instance, recent research leverage SSL techniques to learn patch-level (Yun et al. 2022, Caron et al. 2021) or region-level (Xiao et al. 2021) representations from videos. These approaches aim to capture the inherent structure and relationships within video frames, enabling the learning of meaningful features without relying on explicit labels. By exploiting the temporal coherence and spatial consistency present in videos, these self-supervised methods can discover emergent properties and capture rich semantic information.\nHowever, limited research has been conducted regarding the application of such techniques to financial data reflecting its temporal characteristics. One of the main reasons for this is the chal-lenge of generating different views (both positive and negative), which play a key role in self-supervised representation, in non-stationary time-series data. In the case of financial price data, it is not straightforward to generate meaningful positive and negative views that capture the in-herent temporal dependencies and preserve the essential characteristics of the original time series. Unlike images or text, where various augmentation techniques can be applied to create different views (e.g., cropping, flipping, masking or word substitution) without significantly altering the core information, financial time series data is highly sensitive to temporal order and contextual rela-tionships. Previous studies have applied various methods to generate views for time series data in order to solve this problem. The work in (Yue et al. 2022) utilized timestamp masking and random cropping, while (Choi and Kang 2023) employed scaling and permutation techniques. However, these approaches may not be suitable for time series data that require fine-grained representations. Next, multivariate time series, which are common in most time series data, present a unique challenge for Self-Supervised Learning (SSL) methods. Although these time series contain multi-ple dimensions, the relevant and informative patterns are often confined to a limited number of dimensions. Consequently, directly applying SSL techniques that have been successful with other data types to extract meaningful information from time series data becomes difficult, as the sparse nature of useful information in multivariate time series hinders the effectiveness of these methods.\nThird, time series data possess distinct characteristics, including seasonality, trend, and frequency domain information (Wen et al. 2020, Wu et al. 2021a). Considering how SSL works, in sequential data, the hidden patterns of the data are used based on the \"current time\" that the data is being learned. In other words, this assumes that these characteristics will apply the same in the future. However, the non-stationary nature of time series data poses a significant challenge for SSL methods. The underlying patterns and relationships in the data can change over time, leading to a phenomenon known as concept drift or temporal shift. This means that the representations learned by SSL methods based on historical data may not generalize well to future time periods. To address this issue, techniques from the field of Domain Generalization (DG) can be employed. DG aims to learn general model representations that can adapt to temporal shifts over time. To improve the generalization ability of the model when there is temporal change, methodologies such as DRAIN (Bai et al. 2022) have been proposed, but they are only applicable to supervised learning.\nThis work extends an earlier prototype of SimStock which was introduced in (Hwang et al. 2023a) and explored the possibility of using SSL for identifying similar stocks based on their temporal dynamics. Here we show that such an architecture indeed finds applicability and effectiveness across various financial tasks, such as pairs trading, index tracking, and portfolio optimization, by conducting rigorous comparative analyses of different models within this framework."}, {"title": "2.2. Parameter estimation in non-stationary financial markets", "content": "In the field of investment management, estimating future returns and risk is a fundamental challenge due to the inherent uncertainty and non-stationary nature of financial markets. Traditionally, researchers and practitioners have relied on historical financial data to estimate various parameters, such as expected returns, volatility, and covariance matrices, which are crucial inputs for portfolio optimization, risk management, and asset pricing models.\nOne common approach is to use a rolling window of historical returns to estimate the expected returns and covariance matrix of a set of financial assets. For instance, in the classic mean-variance portfolio optimization (MVO) framework proposed by (Markowitz 1952), the optimal portfolio weights are determined based on the estimated expected returns and covariance matrix using a sample of historical returns. See (Kim et al. 2021) for more explanations. Similarly, the Capital Asset Pricing Model (CAPM) (Sharpe 1964, Lintner 1975) and the Fama-French factor models (Fama and French 1993, 2015) rely on historical data to estimate the beta coefficients and factor premiums.\nHowever, using historical data for parameter estimation has several limitations. First, financial markets are known to exhibit non-stationarity, meaning that the statistical properties of the data, such as the mean and variance, can change over time (Cont 2001, Lo 2017). This implies that the estimates based on historical data may not be representative of the future. Second, the sample size of historical data is often limited, leading to estimation errors and potential over-fitting (Kan and Zhou 2004, 2007). For instance, in the context of portfolio optimization, an inherent drawback of MVO is the high sensitivity of the optimal portfolio to estimation errors in the input parameters (Michaud 1989), particularly in the expected returns (Chopra and Ziemba 2013) and correlations (Chung et al. 2022). Small changes in the estimated expected returns can lead to significant shifts in the optimal portfolio weights, resulting in portfolios that may be suboptimal or unstable out-of-sample.\nTo mitigate these issues, researchers have proposed various techniques to improve the robustness and accuracy of parameter estimation. One popular approach is the shrinkage method, which combines the sample estimates with a structured estimator to reduce the estimation error (Ledoit and Wolf 2003, 2004b,a) Another technique is the use of robust estimators, such as the minimum covariance determinant (MCD) estimator (Rousseeuw and Driessen 1999) and the minimum volume ellipsoid (MVE) estimator (Van Aelst and Rousseeuw 2009), which are less sensitive to outliers and heavy-tailed distributions. More recently, (Gerber et al. 2021) proposed the Gerber statistic for estimating the covariance matrix between assets, a robust co-movement measure that extends Kendall's Tau by counting the proportion of simultaneous co-movements when their amplitudes exceed data-dependent thresholds, capturing meaningful co-movements while being insensitive to extreme values and noise.\nIn addition to these techniques, researchers have also explored various approaches to extrapolate past data and model future market dynamics. For example, (Barberis 2000) proposed a Bayesian approach that combines the sample estimates with prior beliefs about the asset returns to improve the out-of-sample performance of portfolio optimization. (Rapach et al. 2010) employed combi-nation forecasts, which aggregate individual forecasts based on different predictors, to enhance the accuracy of out-of-sample stock return predictions. Also, (Welch and Goyal 2008) found that the historical average excess returns of stocks over bonds, which is often used as an estimate of the equity premium, is sensitive to the choice of the sample period and the assumptions about survivorship bias. Moreover, they showed that this historical average is a poor predictor of future returns, as the equity premium exhibits substantial time-variation and mean-reversion.\nDespite these advancements, the fundamental challenge of estimating future returns and risk from historical data remains. The assumption that future behavior will be similar to the past is often violated in practice, as financial markets are subject to regime shifts, structural breaks, and extreme events (Ang and Bekaert 2002, Guidolin and Timmermann 2007). See (Lee et al. 2023) for more detailed review of machine learning for asset management."}, {"title": "3. SimStock", "content": "In this section, we introduce SimStock, a novel SSL framework that incorporates temporal domain generalization to learn robust and comprehensive representations of financial time series data. The various components of the model are graphically illustrated in fig. 1."}, {"title": "3.1. Preliminaries", "content": "We consider a self-supervised task where the stock data distribution evolves over time. In the training phase, we are given T observed source domains $D_{1:T} = \\{D_1, D_2, ..., D_T\\}$, which are sampled"}, {"title": "3.2. Training Dynamics of Temporal Domain Generalization", "content": "We are motivated by DRAIN (Bai et al. 2022), which first proposed the concept of temporal domain generalization. In each temporal domain $D_s$, the representation network $f_{\\theta_s}$ can be trained by maximizing the conditional probability $P(\\theta_s | D_s)$. Here, $\\theta_s$ represents the state of the model parameters at timestamp $t_s$. Given the dynamic nature of $D_s$, the conditional probability $P(\\theta_s | D_s)$ will also change over time. The objective, in the context of temporal domain generalization, is to estimate $\\theta_{T+1}$ utilizing all the training data from $D_{1:T}$. From a probabilistic perspective, we can express this as:\n$P(\\theta_{T+1} | D_{1:T}) = \\int_{\\Omega} P(\\theta_{T+1} | \\theta_{1:T}, D_{1:T}) \\cdot P(\\theta_{1:T} | D_{1:T})d\\theta_{1:T},$\\nwhere $\\Omega$ denotes the space for model parameters $\\theta_{1:T}$. In eq. (1), the first term inside the integral $P(\\theta_{T+1} | \\theta_{1:T}, D_{1:T})$ represents the inference phase, which is the process of predicting the future state of the target representation network (i.e., $\\theta_{T+1}$) given all historical states (i.e., $\\theta_{1:T}, D_{1:T}$). The second term $P(\\theta_{1:T} | D_{1:T})$ signifies the training phase, which involves leveraging all training data $D_{1:T}$ to ascertain the state of the model on each source domain. More specifically, the training phase can decomposed as follows:\n$P (\\theta_{1:T} | D_{1:T}) = \\prod_{s=1}^{T} P (\\theta_s | \\theta_{1:s-1}, D_{1:s}) = P (\\theta_1 | D_1) \\prod_{s=2}^{T} P (\\theta_s | \\theta_{1:s-1}, D_{1:s}).$"}, {"title": "3.3. Temporal Representation Learning", "content": "Our ultimate goal is to learn a representation model, $f_\\theta$, which captures the stock data distribution that evolves over time. To achieve this, we develop an SSL framework for temporal representation learning of stock data.\nTemporal feature variant. Let $x^s \\in \\mathbb{R}^{d_m}$ be the price feature of a stock, where $d_m$ is the dimension of the feature. The time-varying patterns of stock prices are essential for identifying short- and long-term characteristics of stocks. To learn more rich representations, the price feature $x^s$ is processed by a temporal transformation module $\\mu$. Specifically, the price feature $x^s$ is provided with k variations, denoted as:\n$\\mu(x^s) = CONCAT(\\mu_1(x^s), \\mu_2(x^s), ..., \\mu_k(x^s)) \\in \\mathbb{R}^{d_mk},$\\nwhere $d_{mk} = d_m \\times k$, and each $\\mu_i: \\mathbb{R}^{d_m} \\rightarrow \\mathbb{R}^{d_m}$ for $i \\in \\{1,2,..., k\\}$ is a temporal transformation function from the collection U. In other words, $\\mu_1, \\mu_2, \u2026, \\mu_k \\in U$, where U denotes the collection of temporal transformations. This module is used to create temporal features that incorporate various time intervals. For example, $\\mu_1(x^s)$ and $\\mu_2(x^s)$ would reflect temporal patterns within a day and a week. Various methods, such as the moving average (Woo et al. 2022, Wu et al. 2021b), Fourier transform (Zhou et al. 2022), moving average convergence/divergence (MACD) features for momentum (Lim et al. 2019) and mixtures of experts (Zhou et al. 2022), can be utilized to create these temporal features. In this study, we use moving average, which is the most common choice. More specifically, for each temporal transformation $\\mu_i$, we calculate the moving average of the price feature $x^s$ over a window of size $w_i$:\n$\\mu_i(x^s) = \\frac{1}{w_i} \\sum_{j=0}^{w_i-1} x_{i-j}, i = 1,2,..., k.$\nwhere $w_i$ is the window size for the i-th transformation, and $x_{i-j}$ denotes the price feature at time t-j in domain s. In our implementation, we use five different window sizes: $w_1 = 5, w_2 = 10, w_3 = 15, W_4 = 20, and w_5 = 25$. These window sizes correspond to weekly, bi-weekly, tri-weekly, four-weekly, and monthly moving averages, respectively.\nCombined embedding with static metadata. In our framework, static metadata $c^s$, which can include a variety of data such as firm description, 3-statement financial information, and more, is handled in the static embedding layer. However, for the purpose of this study, we have only included sector information in c. As a result, an embedding $Embed(c^s) \\in \\mathbb{R}^{d_mk}$ is obtained. Next, we create a combined embedding that incorporates both the temporal feature variant $\\mu(x^s)$ and the embedded static metadata $Embed(c^s)$. The resulting combined embedding is denoted as follows:\n$H^s = \\mu(x^s) + Embed(c^s) \\in \\mathbb{R}^{d_mk}.$\\nFeature Tokenizer module. We draw inspiration from the tokenizer approach Gorishniy et al. (2021), which transforms input features into token embeddings to obtain more meaningful repre-sentations. This method Gorishniy et al. (2021) do not reflect the temporal aspect, but we utilize temporal feature variants in the Feature Tokenizer module to capture the time-varying patterns in the stock price data. The feature-wise token embeddings $TKE_j^s$ for a given feature index j are computed as follows:\n$TKE_j^s = b_j^s + H^sW_j$\nwhere $b_j^s \\in \\mathbb{R}^d$ is the j-th feature bias term and $W_j \\in \\mathbb{R}^d$ is the weight vector for the j-th feature. Consequently, the token embeddings $TKE^s \\in \\mathbb{R}^{d_c \\times d}$ can be obtained by stacking all of the feature embeddings and adding a special [ST] token, which is known to possess the essence of information after training. This is represented as:\n$TKE^s = STACK([ST], TKE_1^s, ..., TKE_{d_mk}^s)$\nwhere $\\mathbb{R}^{d \\times d} = \\mathbb{R}^{(d_mk+1) \\times d}$ denotes the dimension of the combined token embeddings $TKE$. Therefore, the feature tokenizer plays a crucial role in learning meaningful temporal representations by transforming combined embedding.\nDimension corruption. In self-supervised learning, the main objective is to learn an embed-ding space where positive pairs (or views) remain close to each other, while negative pairs (or views) are far apart. When generating views, mixup (Zhang et al. 2017) or cutmix (Yun et al. 2019) methods are most commonly used. These methods are suitable for invariant augmentation of static data (e.g., images), however, these are not suitable for time-series data (e.g., stocks). We generate views for temporal variants on the same instance, unlike conventional SSL methods that use invariant augmentation by using different instances together. For time-series data, mixing different sequences would break the entire temporal structure. Therefore, we propose a dimension corruption method for the augmentation of temporal data.\nFirst, we create positive and negative views, $H_{pos}^s$ and $H_{neg}^s$, by randomly shuffling the dimensions within the token embeddings $TKE$. Here, we define two permutation matrices, $P_{pos}^s$ and $P_{neg}^s$, both of size d x d.\n$H_{pos}^s = \\lambda TKE^s + (1 - \\lambda)TKE^sP_{pos}^s$\n$H_{neg}^s = (1 - \\lambda)TKE^s + \\lambda TKE^s P_{neg}^s$\nIn this case, the formula (8Temporal Representation Learningequation.3.8) generates positive and negative views for self-supervised learning.\nRepresentation module. The representation module $f_\\theta$ aims to characterize the shift between different domains by refining the parameters $\\theta_s$ through the process described in Section 3.2. In order to effectively reflect temporal patterns of corrupted token embeddings ($H_{pos}^s$ and $H_{neg}^s$), we use the self-attention mechanism (Vaswani et al. 2017). The self-attention mechanism aggregates corrupted token embeddings with normalized importance as follows:\n$Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d}})V.$\nHere, $Q = HW_Q \\in \\mathbb{R}^{d \\times d_k}, K = HW_K \\in \\mathbb{R}^{d \\times d_k}$ and $V = HW_V \\in \\mathbb{R}^{d \\times d_v}$ represent queries, keys, and values, respectively. Note that H represents token embeddings (either positive or negative), while $W_Q, W_K$, and $W_V$ are learnable matrices that share weights between positive and negative token embeddings. The output, which has a dimension of $d_v$, is then transformed back into an embedding of dimension d through a fully connected layer. Finally, the outputs $ST_{neg}^s$ and $ST_{pos}^s$ are obtained.\nTriplet loss. For SimStock, we train it to minimize a triplet loss (Balntas et al. 2016), which is a popular choice in SSL. The key idea behind triplet loss is the use of triplets, each of which consists of an anchor, and positive and negative views. Here, the anchor is the embeddings for the unperturbed combined embedding.\nFor the triplet ($ST_{pos}^s, ST_{neg}^s, H^s$), where $ST_{pos}^s$ is the positive view, $ST_{neg}^s$ is the negative view, and $H^s$ is the combined embedding (anchor), the triplet loss is defined as follows:\n$\\mathcal{L}_{triplet} = max(0, sim(H^s, ST_{pos}^s) - sim(H^s, ST_{neg}^s) + \\alpha)$\nIn the above equation, $sim(\\cdot, \\cdot)$ denotes a similarity measure (e.g., cosine similarity or Euclidean distance), and $\\alpha > 0$ is a margin that is introduced to separate positive pairs from negative pairs.\nInference phase. In our framework, the inference phase is particularly important. Unlike most existing contrastive representation learning studies (Chen et al. 2020, Grill et al. 2020), our model is specifically designed to be robust with respect to temporal distribution shifts. The inference phase consists of passing the target domain $D_{s+1}$ through the embedding module to obtain the combined embedding $H^{s+1}$ and further processed by the feature tokenizer module to obtain the token embeddings $TKE^{s+1}$. The stock representation is then obtained by feeding $TKE^{s+1}$ into the representation model $f_{\\theta_{s+1}}$, which is updated with the optimal parameters $\\theta_{s+1}$ generated by the TDG method described in Section section 3.2"}, {"title": "4. Experiment", "content": "Now we present experiment results to thoroughly demonstrate the performance of SimStock on real-world benchmark datasets. Our code and configurations will be publicly available on GitHub."}, {"title": "4.1. Implementation details", "content": "We present the details of tasks, datasets, baseline models, hyperparameter selection, evaluation metrics, and the experiment setting."}, {"title": "4.1.1. Tasks", "content": "We conduct four main experiments to assess the effectiveness of SimStock in various financial applications:\n\u2022 Finding similar stocks: We evaluate how well SimStock can identify stocks that exhibit similar price movements, compared to baseline methods. Given a query stock, we consider both same exchange (finding similar stocks within the same exchange) and different exchanges (finding similar stocks in a different exchange from the query stock) scenarios;\n\u2022 Pairs trading: We evaluate the effectiveness of pairs trading based on the similar stocks identified by SimStock. We form pairs of stocks that are found to be similar, and then execute a pairs trading strategy;\n\u2022 Index tracking: Given an ETF as the query, we find a basket of similar stocks using SimStock and assess the performance of using these stocks for index tracking. The goal is to see if the selected stocks can closely mimic the returns of the target ETF;\n\u2022 Portfolio optimization: We investigate whether SimStock embeddings can enhance port-folio optimization. Specifically, we replace the correlation matrix of stock returns using the SimStock embedding as a similarity measure for mean-variance portfolio optimization. We compare the performance of the resulting portfolios with those obtained using conventional methods for estimating."}, {"title": "4.1.2. Datasets", "content": "We collect daily stock price features (Open, High, Low, Close, and Volume) and sector information from Yahoo Finance for stocks listed on five major exchanges: NYSE (New York Stock Exchange), NASDAQ (National Association of Securities Dealers Automated Quo-tations), SSE (Shanghai Stock Exchange), SZSE (Shenzhen Stock Exchange), and TSE (Tokyo Stock Exchange). In our study, we combine NYSE and NASDAQ stocks (4,231 in total) and refer to them as the US exchange, while treating SSE (1,407 stocks), SZSE (1,696 stocks), and TSE (3,882 stocks) separately. The data spans three periods: a training period from January 1, 2018, to December 31, 2021; a reference period from January 1, 2022, to December 31, 2022; and a test period from January 1, 2023, to December 31, 2023. This timeline allows us to verify whether stocks identified as similar in the reference period maintain their similarity in the subsequent one-year test period.\nOur dataset is less susceptible to survivor bias because the source domain is organized as a sequence of domains \\{D_1, D_2, ..., D_T\\} over time. Each source domain $D_s$ consists of a distinct set of stocks that existed at time step s. This allows the model to reflect the realistic entry and exit of stocks at each time step. For example, $D_{s-1}$ will contain stocks that existed at time s \u2013 1, but may not contain all stocks from $D_s$ if some were delisted. Similarly, $D_s$ can include newly added stocks that were not present in $D_{s\u22121}$. Furthermore, the price data used for each stock is adjusted to account for corporate actions such as splits and dividends.\nWe generate normalized input features describing the trend of a stock on day t. The variables $z_{Open}$, $z_{High}$ and $z_{Low}$ represent the comparison values of the opening, highest, and lowest prices, respectively, relative to the closing price of the same day. Also, $z_{Close}$ and $z_{Volume}$ represent the comparative values of the closing prices and the volume values compared with day t-1, respectively. Refer to table 1 for the formulas used to calculate each feature. In addition, we calculated stock price features for 5, 10, 15, 20, 25, and 30-day intervals for the temporal feature variant as discussed in section 3.3."}, {"title": "4.1.3. Baseline models", "content": "For the task of finding similar stocks, index tracking and pairs trading task, we compare SimStock with the following baselines:\n\u2022 Corr1: Calculates the correlation of stock returns using the past one-year returns (i.e., from January 1, 2022 to December 31, 2022).\n\u2022 Corr2: Calculates the correlation of stock returns using returns from the beginning of the test period (i.e., from January 1, 2018 to December 31, 2022).\n\u2022 Peer: Uses the list of similar stocks provided by Financial Modeling Prep.1\n\u2022 TS2VEC (Yue et al. 2022): A state-of-the-art method based on self-supervised learning for finding similar time-series data.\nFor the portfolio optimization task, we compare the performance of portfolios constructed using the SimStock similarity matrix with those constructed using the following conventional correlation matrices:\n\u2022 Historical covariance matrix(HC) (Jobson and Korkie 1980): The sample correlation matrix of the most recent past stock returns.\n\u2022 Shrinkage method(SM) (Ledoit and Wolf 2004a): A shrinkage estimator of the covariance matrix proposed by (Ledoit and Wolf 2004a).\n\u2022 Gerber statistic(GS) (Gerber et al. 2021): A robust correlation measure that counts the proportion of simultaneous co-movements between assets when their amplitudes exceed data-dependent thresholds."}, {"title": "4.2. Can SimStock find similar stocks?", "content": "In this section", "model": "the same exchange scenario and the different exchanges scenario. The same exchange sce-nario focuses on finding similar stocks within the same exchange given a query stock. This approach allows for the identification of stocks with comparable characteristics and behaviors within a spe-cific market. On the other hand, the different exchanges scenario involves finding similar stocks within another exchange given a query stock. This scenario leverages the concept of transfer learn-ing, where the trained weights of a model from one exchange are applied to stock"}]}