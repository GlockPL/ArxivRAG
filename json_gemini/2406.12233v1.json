{"title": "SyncVSR: Data-Efficient Visual Speech Recognition with End-to-End Crossmodal Audio Token Synchronization", "authors": ["Young Jin Ahn", "Jungwoo Park", "Sangha Park", "Jonghyun Choi", "Kee-Eung Kim"], "abstract": "Visual Speech Recognition (VSR) stands at the intersection of computer vision and speech recognition, aiming to interpret spoken content from visual cues. A prominent challenge in VSR is the presence of homophenes-visually similar lip gestures that represent different phonemes. Prior approaches have sought to distinguish fine-grained visemes by aligning visual and auditory semantics, but often fell short of full synchronization. To address this, we present SyncVSR, an end-to-end learning framework that leverages quantized audio for frame-level crossmodal supervision. By integrating a projection layer that synchronizes visual representation with acoustic data, our encoder learns to generate discrete audio tokens from a video sequence in a non-autoregressive manner. SyncVSR shows versatility across tasks, languages, and modalities at the cost of a forward pass. Our empirical evaluations show that it not only achieves state-of-the-art results but also reduces data usage by up to ninefold.", "sections": [{"title": "1. Introduction", "content": "Visual Speech Recognition (VSR), also referred to as lip-reading, constitutes the process of decoding spoken language through the observation of the visual cues, specifically the movements of the lips and facial dynamics. This technology holds critical importance in a variety of contexts, including the interpretation of lip movements from individuals with speech disorders [1], benefiting individuals with hearing disorders [2], recognizing spoken content within environments where acoustic signals are compromised [3,4], providing voiceovers to silent historical films, and fortifying security systems [5].\nThe primary challenge encountered in VSR stems from the inherent scarcity of information that can be extracted from visual cues alone [7]. Central to this issue is the presence of homophenes, wherein disparate sounds are visually manifested through identical or nearly identical lip movements [8]. Such phenomena represent significant ambiguity in the analysis of visemes, the fundamental units of visual speech recognition. This ambiguity poses considerable difficulties, as it muddles the clarity of speech interpretation through visual means alone.\nPrevious research to overcome such limitations has pre-dominantly revolved around aligning visual with auditory semantics, attempting to reduce the gap between audio models and visual models. Earlier techniques [9-11] harnessed knowledge distillation from pretrained Automatic Speech Recognition (ASR) systems. Subsequent studies [8, 12, 13] trained aux-"}, {"title": "2. Methodology", "content": "Crossmodal Audio Token Synchronization. Our work integrates audio reconstruction loss with VSR training objectives. Conventionally, word-level VSR employs a word classification loss, whereas sentence-level VSR utilizes the joint CTC-Attention loss [21]. The total loss is the weighted sum of the task-specific objective loss and our audio reconstruction loss. Let D be a training set that consists of a training sample (x, y, z). Let x and y be the input video and ground truth label. Let z = {zt}t<T be a discrete audio sequence corresponding to the input video x.\nWord Classification Loss. For word-level VSR, cross-entropy loss measures the difference between predicted class probabilities and the ground truth labels. Given that y represents the ground truth category for the input video x, the objective loss is formulated as follows:\n$L_{task} = -E_{(x,y,z) \\in D} log p(y|x)$\nwhere p(y|x) denotes the output probability from the model.\nJoint CTC-Attention Loss. For sentence-level VSR, we employ a combination of Connectionist Temporal Classification (CTC) [22] loss from the encoder and Language Modeling (LM) loss from the decoder, known as joint CTC-Attention loss. Let \u03c0 = {\u03c0t}t<T be intermediate CTC labels, and $(y)$ be a set of all possible intermediate labels for CTC loss. Using PLM for language modeling and pCTC for conditional indepen-"}, {"title": "3. Experimental Setup", "content": "Training Dataset. We employ the LRW [23] dataset for English and the CAS-VSR-W1K [24] for Chinese to evaluate word-level VSR tasks. The LRW dataset comprises 500 words, each represented by up to 1,000 training videos. The LRW-1000 dataset consists of 718,018 videos spanning 1,000 words. Our sentence-level experimental framework was anchored on the LRS2 [35] and LRS3 [6] datasets, representing the most extensive publicly available resources for audio-visual speech recognition in English. The LRS2 dataset, sourced from BBC programs, comprises 144,482 video clips, totaling 225 hours of video content. The LRS3 dataset, harvested from TED talks, encompasses 151,819 video clips, amassing 439 hours of footage. Additional training data was sourced from the English-speaking segments of the VoxCeleb2 [43] dataset, comprised of a training corpus totaling 1,323 video hours, complemented by transcriptions following the scheme of AutoAVSR [38].\nDataset Preprocessing. We used MediaPipe [44] to identify the region of interest with a size of 128 x 128 for video-based VSR, and the extracted landmark data served as input for a pointcloud-based VSR system. We used a data augmentation scheme of a resized random crop with a size of (96, 96) and a random horizontal flip and applied a center crop for inference similar to that of previous works [3,38].\nModel Architecture. For word-level VSR, an encoder is composed of a combination of 3D CNN, ResNet18, and Transformer [45] to extract video features following the previous works [8, 12, 13, 31]. On the other hand, Conformer [46] is used as a temporal backbone for sentence-level VSR, where we"}, {"title": "4. Results", "content": "Versatility Across Tasks, Languages, and Modalities. Our framework is comprehensively evaluated according to tasks, languages, and input modalities. In word-level tasks, shown in Table 1, SyncVSR marks state-of-the-art results in English and Chinese benchmarks. In sentence-level tasks, displayed in Figure 1 and Table 3, SyncVSR outperforms available methodologies when given a similar amount of video dataset. Notably, our method also advances a tier in model size, where our base-size model shows superior performance over other large-size models. Our method also achieves state-of-the-art performance in landmark-based VSR tasks shown in Table 2 and Table 4.\nDistinguishing Homophenes. Homophenes often closely resemble each other in their graphemes-the smallest functional units of a writing system. For example, homophene pairs, like (Million, Billion) or (Living, Giving), differ by just one grapheme. Although earlier research, notably by Kim et al. [8] has examined a subset of these pairs, a full-scale evaluation of every potential homophene pair has yet to be achieved. As a result, in Figure 3, we assess the relative F1-score gain of existing training methods over a vanilla setting that does not utilize the audio data, focusing on the grapheme edit distances. This suggests that the inclusion of an audio reconstruction loss objective assists in differentiating visemes that are mapped into similar graphemes, which is where homophene pairs are typically found."}, {"title": "5. Conclusion", "content": "We addressed the problem of homophenes with an improved crossmodal synchronization method, effectively bridging the divide between visual cues and their corresponding audio segments. The use of quantized audio tokens for direct frame-level supervision enables SyncVSR to achieve state-of-the-art performance on various benchmarks with a remarkable level of data efficiency. We believe SyncVSR is a step toward future developments in the field of multimodal speech recognition."}]}