{"title": "DipMe: Haptic Recognition of Granular Media for Tangible Interactive Applications", "authors": ["Xinkai Wang", "Shuo Zhang", "Ziyi Zhao", "Lifeng Zhu", "Aiguo Song"], "abstract": "While tangible user interface has shown its power in naturally interacting with rigid or soft objects, users cannot conveniently use different types of granular materials as the interaction media. We introduce DipMe as a smart device to recognize the types of granular media in real time, which can be used to connect the granular materials in the physical world with various virtual content. Other than vision-based solutions, we propose a dip operation of our device and exploit the haptic signals to recognize different types of granular materials. With modern machine learning tools, we find the haptic signals from different granular media are distinguishable by DipMe. With the online granular object recognition, we build several tangible interactive applications, demonstrating the effects of DipMe in perceiving granular materials and its potential in developing a tangible user interface with granular objects as the new media.", "sections": [{"title": "1 Introduction", "content": "Granular materials are commonly seen in our daily lives. As a continuous deformable media, granular materials such as sand or beads has been introduced to tangible user interface (TUI) to pre-view the landscape or adjust the stiffness of input devices [1-4]. While different rigid objects have been used in TUI to give flexible control of vir-tual content [5-9], to the most of our knowledge, the potential of interacting with different types of granular objects has not been exploited.\nTUI has proved to play a vital role in every-day tasks. It makes things more concrete and provides embodiment effects through physicality [10]. For example, typical studies such as Project Zanzibar [11], which showed a flexible mat to communicate with tangible objects placed on its surface. It also supports sensing a user's touch and hover hand gestures, which opened up the possibility of novel digital experiences. De Tinguy et al. [12] proposed that portable devices could be used to simulate the experience of interacting with different objects. Schmitz et al. [13] proposed a fabrication pipeline and sensing approach that enabled object recognition of tangibles on capac-itive touchscreens. Yan et al. use the LaserShoes [14] to achieve real-time inference, which cooper-ate with human under different circumstances. At the same time, they used the laser speckle imaging technique and the LaserShoes could distinguish the surface textures that appear.\nHowever, existing TUI with granular materi-als only involves a pre-specified type of granular media. Ishii et al. [1] proposed the concept of con-tinuous tangible user interface (Sandscape) using granular materials. They argued that granular materials could bridge the gap between physi-cal and digital forms because of their continuous physical properties. Users could interact with models made of sand and the shape of the gran-ular media was converted into a digital height field to preview land scapes. Kazi et al. [2] developed a digital canvas (Sandcanvas) to sim-ulate sand drawings. They studied the gestures on touchscreens and incorporated sand simula-tors to reproduce sand drawings on a virtual screen. Follmer et al. [3] proposed using gran-ular materials to build Jamming user interface. They exploited computer-controlled jamming of granular particles to show its ability in control-ling the stiffness of shape-changing objects. In this work, we will introduce dipping as a method to sense different types of granular media and use multiple types of granular media for interactive applications. If users would like to fully exploit the interaction experience or map different types of granular objects into the virtual world, existing solutions are not ready for users to interact with different granular objects. One of the missing fea-tures is to enable the input device to understand the type of granular media.\nTo understand the different types of media, various methods have been developed for object recognition based on vision [14, 15], inertia data [16-18], acoustic signals [19, 20], or electromag-netic signals [21, 22] for various human-computer interactions. Vision-based methods are typical ways for object recognition. However, during the interaction with the granular media, the shape changes of the material, the occlusion or the vary-ing lighting conditions may all affect the recogni-tion results. Furthermore, vision-based solutions require capturing the interaction scene, which may also bring concerns to users in terms of privacy. Haptic recognition is also an impor-tant modality for object recognition in the field of computer-human interaction (CHI). Force or tactile signals are used in previous works with machine learning techniques. For example, tac-tile information has been used for learning the grasp signature for object recognition [23] or human-environment interactions [24]. Wu et al. [25] proposed to encode contact information for object recognition on an interactive fabric. Vib-Eye [26] used the vibration passing through the object to determine the identity of an object. The vibrotactile information received by the fin-ger was represented with a spectrogram and used for object recognition. Researchers also proposed to install proximity sensors on objects to recognize the grasping events [27]. However, it is costly to customize different objects by embedding sensors inside them.\nWhile humans naturally learn to recognize types of granular material through physical inter-actions, it is challenging for computers to classify them, especially in user interaction scenarios. Soil, a typical granular medium, is commonly stud-ied in geoscience using the cone penetrometer test (CPT) to ascertain geotechnical properties and identify soil stratigraphy [28, 29]. Geolo-gists increasingly employ force data collected from experiments along with machine learning tech-niques for tasks such as soil spatial mapping, recognition, and classification [30, 31]. However, traditional CPT is costly and time-consuming, requiring heavy and precise tools and strict per-pendicular testing directions [32]. In contrast, DipMe provides an inexpensive and easily deploy-able alternative, allowing for testing with a casual dipping operation, making it more suitable for tangible interactions with particulate media.\nMachine learning has revolutionized object recognition in recent years, with techniques such as Time Series classification (TSC) [33] utiliz-ing inception modules with a fully convolution network (FCN) [34], a robust temporal feature network (RTFN) [35], and a convolution neu-ral network (CNN) [36]. Additionally, researchers have proposed Multivariate Time Series Classifi-cation (MTSC) [37]. These machine learning tools have also been leveraged for object recognition in the human-computer interaction community using multimodal signals [38-42]. We propose the use of a more advanced machine learning algorithm, based on the encoder part of the transformer model and multi-channel mechanism, to achieve autonomous recognition of force signal series, sur-passing conventional algorithms in performance.\nIn this work, we propose DipMe, a haptic solu-tion for recognizing different types of granular"}, {"title": "2 System design", "content": "2.1 System overview\nBased on the observation from literature, our design of DipMe as an input device will consider several aspects. First, the device should be com-patible with the probing operation. Second, the device should have the capabilities to acquire the normal force and lateral torque when it probes into the granular media. The force signals should be continuously sampled and the signal-to-noise ratio should be sufficiently large to distinguish the differences when interacting with different gran-ular materials. Lastly, the form of DipMe should be close to daily supplies so that it can be easily held and tracked for interactive applications.\nWith these design considerations, we proto-type a system to recognize granular materials during user interaction, as illustrated in Fig. 1. We develop an input device DipMe with the force-sensing ability. Users are allowed to hold and dip a pole-shaped device into the granular media."}, {"title": "2.2 Hardware design", "content": "The overall form of DipMe is designed to share similar structures with soil samplers. We design a pole-like shape to facilitate users to hold and probe DipMe into the granular media. The entire measurement system consists of five parts: 1) sens-ing side, which is directly contacted to four load cells of the force sensor, 2) a self-developed three-dimensional force sensor, 3) handle side, which is gripped by users, 4) a VIVE Tracker and 5) assis-tant modules. The hardware details of DipMe are shown in Fig. 2 and different parts are assembled using bolts and nuts. In our use scenario, a user grasps the upper part (handle side) of DipMe and pushes the lower part (sensing side) into granular media with different particle sizes to acquire force signals. A force/torque sensor is installed between the upper and lower part to acquire the force data online. In order to obtain the probing motion for both recognition and interaction, we also attach a VIVE Tracker at the top of DipMe and send real-time motion data to PC.\nHardware Configurations. The enclosure of the handle side (a2) and sensing side (a4) is 3D printed using nylon and ABS. For contact force (normal force and lateral torque) measurement, we used four small load cells (type: AhJCZN; BCS-M2) from the same batch to form the elas-tomer part of the combined multidimensional force sensor (a3), each with the same property parameters. The exploded view is shown in the left subfigure of Fig. 2. The combined force in the Z-axis results from the sum of the readings from all four cells. The pair of forces detected by the opposing cells constitutes a force couple, which is proportional to the torque in the respective direc-tion. Although the torque signals seem to be less prominent than the pressing force, the designed sensor is able to capture the torque with a decent precision. As a difference quantity, the noise of the individual signals will be reduced because the core step of torque measurement is based on the sub-traction of the two forces measured at the opposite sides. The force sensor is installed between the handle side and the sensing side. The switching power (type: AhJCZN; D-30F, a5) is used to safely supply power to the entire system. The trans-mitter (type: AhJCZN; LZ-JX4, a7) converts the input from the sensor into an electrical signal and amplifies it for remote measurement and control. All the sensors and other electronic components"}, {"title": "3 Methods", "content": "In this section, we introduce our software solu-tion for haptic recognition of granular media. The captured force and torque signals from the input device serve as input. Initially, we preprocess the raw signals before feeding them into a machine learning pipeline. Subsequently, the recognized label is generated to facilitate interaction with granular media, as depicted in Fig. 1.\n3.1 Data Pre-processing\nWhen a user pushes DipMe into the test granular media, the force will be applied to the probe of the sensing side and the signal will be sampled. This I/O force is fed to a series of signal processing methods, which is as shown in Fig. 4.\nThe processing method for raw signals is as follows. First, we convert the gravity of the sensing side to the the base coordinate system (BCS) [43] tracked by the base station. The gravity along the principle direction of DipMe as well as the torques contributed by the gravity are subtracted from the raw signals, so that the influence of the orientation of the device will be suppressed.\nSecond, a low-pass filter (LPF) is applied to eliminate the impact of high-frequency object vibration. The bandwidth of human voluntary motion with wrist motion during daily activities typically falls in the range of around 5 Hz [44]. For LPF, we design an 5-order Butterworth filter that has a flat passband with stopband frequency 10Hz (Fig. 3). Finally, we resample the signals accord-ing to the velocity extracted from the probing motion, and unify the data to approximate those collected from a constant-speed probing motion. In this case, the force and torque signals are fil-tered and sensitive to the tested granular media, as shown in Fig. 4.\n3.2 Multi-channel Encoder Model\nOur model is mainly based on the multi-channel encode (MCE) structure with attention mecha-nism [47]. In our model, the force signals are first divided into three channels (X-axis torque, Y-axis torque, Z-axis force) and sent to encoding blocks [48]. We then pass them through a Con-vBlock and a multi-head attention block (with a position embedding layer, a batch normalization (BN) layer and a full connection (FC) layer) sepa-rately. Finally, we concatenate the feature vectors obtained from the three channels into one vec-tor and use a multilayer perceptron (MLP) and a softmax layer to obtain the recognition results, as shown in Fig. 5.\nA personal computer (PC) with a CPU of 3.10 GHz Inter Core i5-10500 and a GPU of NVIDIA GeForce RTX 3080 is used for doing follow-up experiments. During the training period, we selected Weighted Logarithmic Loss [49] as the loss function and applied the Adam optimizer. The learning rate and batch size were set to 3e-4 and 16, respectively. We did not take a pre-trained model to initialize our parameters and used only 100 epochs for model training. In practice, we found that the settings were sufficient for our models to converge."}, {"title": "4 Evaluation", "content": "We selected 6 granular media with different par-ticle sizes for our study, as shown in Fig. 4. The single sampling duration is 2.51s, which is suffi-ciently long for accurate recognition of the types of granular media. These media could be classi-fied into four groups based on particle size: 1) clay (0~0.002mm), 2) silt (0.002~0.063mm), 3) sand (0.063~2mm), 4) gravel (2~63mm) [50]. For each type, we used a transparent crisper (MBL; MBL-3500, b3) with a volume of 1800mL to store the samples, experimented 40 times with DipMe, and then processed the raw sensor data as described in Section 5.1. This step resulted in a time-series matrix (40 times*6 types*251 length*3 channels),"}, {"title": "4.1 Recognition Length Selection", "content": "As presented in Section 5.2, the model's input is the multivariate-time series with dimension of 3 and length of N from the data pre-processing phase, and the output is the type of granular media. The Recognition Length is set to 128 in our granular media classification. To determine the effectiveness of this length, we use the sliding window approach [51] to segment the processed time series data with different lengths to train a set of MCE models (200 samples for training, 40 samples for testing). The tested Recognition Lengths included 32, 64, 128, and 251. Table 1 presents the mean accuracy and the inference time (includes a sampling time) for the classification of one trial. As anticipated, input series with larger length result in greater accuracy, yet they require significantly longer time for classification. Given that the accuracy improvement is moderate, we have opted to choose 128 as the recognition length for input series in our model. This decision has been taken while striking a balance between accu-racy and inference time for the online recognition task."}, {"title": "4.2 Method Comparison", "content": "We ran cross-validation test [52, 53] using four popular MTSC approaches (Generalized ran-dom shapelet forest (gRFS) [54]; Dynamic Time Warping with K-Nearest Neighbor (DTW+KNN) [55]; Multi-scale Convolutional Neural Network (MCNN) [56]; ResNet [57]) to demonstrate the effectiveness of the proposed method (MCE with the recognition length 128) for 10-fold cross vali-dation.\nIn order to better compare the different methods, we selected four classification met-rics (Accuracy, Precisionmacro, Recallmacro, F1-Scoremacro\u00b9) [58, 59] as evaluation metrics and the results are provided in Table 2. With both the self-attention mechanism and the multi-channel fusion mechanism, our method effectively identi-fied patterns within the multivariate time series with a higher performance than that of other MTSC methods."}, {"title": "5 User Study", "content": "We conducted a user study to assess the effec-tiveness of DipMe, for recognizing the types of granular media with actual users. We recruited 10 right-handed participants (8 males and 2 females), with ages ranging from 21 to 28 years old (mean = 23.3, SD = 2.31) for this user study via volunteer-ing. During the study, the participants held the handle side of DipMe with their dominant hands and they were told to move the DipMe casually and try to maintain a normal speed when testing the granular material.\n5.1 Experiment Details\nOur first experiment was to test whether the model could recognize the types of granular media when different participants conducted the tests with DipMe. To ensure there is no overlapping between the training set and the testing set, we first divided all the data (including the data in the Section 5) into seven folders and randomly selected three folders (180 samples) as the test-ing data set while ensuring a consistent number of samples in each type. The second experiment was to do cross-user validation using the leave-one-out evaluation method [52]. We retrained the model using data from 8 participants (144 sam-ples) and tested it on the remaining participants (36 samples).\nAll participants finished the two sessions, first with a short teaching and practice session, which can familiarize participants with the entire system and provide them with information on the exper-imental procedure (the time duration is 1~2min and practical data were not included in data set). For each type of the granular media, the sec-ond session was repeated for 3 times (the time duration is 3~4min). We randomly order the placement of granular media and calibrated to eliminate the influence of DipMe's gravity before each repetition. During the experiment, partici-pants followed the instructions displayed on the host computer. In total, we collected 180 samples on 6 kinds of granular media from the 10 partici-pants, and it took less than 1 hour to finish all the"}, {"title": "5.2 Results", "content": "A confusion matrix [60] for two experiments is shown in Fig. 6, where a decimal in the (i,j) cell signifies the percentage of the i-th media recog-nised as the j-th media. It is visible that the diagonal terms are dominant for almost all types.\nRecognition Accuracy of the Types of Granular Media with Actual Users. The average accuracy over all granular media and participants was 92.78% (Fig. 6 (a)), with the macro\u00b2-precision of 0.932 (SD = 0.072), the macro-recall of 0.928 (SD = 0.049), the macro-F1 score of 0.929 (SD = 0.052). This result substanti-ates the efficacy of our approach in recognizing the types of granular media penetrated by different participants.\nThe most significant errors occurred with the nutrition soil, which was misidentified as sand in 13.3% of cases (Fig. 7 (a)). Nutrition soil is par-ticularly prone to moisture-related changes during extended experimental cycles, lasting approxi-mately 2-3 weeks. This can lead to some of the soil samples used in the experiment acquiring mechan-ical properties similar to those of sand and will cause some recognition errors in our model. Indi-vidual precision and recall values are shown in Figure 7 (a) for all the granular media. These statistics reaffirm the effectiveness of DipMe.\nCross-User Validation Results. For cross-user validation, the recognition results are shown in Fig 6 (b). The average recognition accuracy of the retrained cross-user model is 83.32%. From the Fig. 7 (b) we can calculate that, the macro-precision is 0.840 (SD = 0.093), the macro-recall is 0.833 (SD = 0.105), the macro-F1 score is 0.832 (SD = 0.076). The average accuracy decreases during cross-user validation. The decrease in accu-racy may be attributed to variations in force patterns resulting from differences in penetra-tion motion pattern [61], leading to discrepancies between the training and testing datasets. It's important to note that the same type of granular media can correspond to multiple force patterns, contributing to reduced accuracy in cross-user validation.\nThe results also indicate that simulated soil and mung exhibit the higher accuracy among the six media with varying characteristics (see Table. 3). This is understandable as the structure of simulated soil and mung is simpler, leading to more distinguishable force patterns. Additionally, sand has the lowest average accuracy. We can also find that there is a decrease of the performance between trained-model and cross-user-model. One possible reason for this observation is that media with varying particle sizes had an amplifying effect on the differences in participants' probing motion, resulting in greater disparities in force patterns within the same type of media groups."}, {"title": "6 Application", "content": "Dip Me enables users to identify various types of granular materials through tangible props, result-ing in a straightforward and immersive tangible interaction experience. Following HCI commu-nity's design principles, we have designed three applications as shown in Fig. 8-10.\n6.1 Spatial Mapping Interface of Subsurface Granular Media\nIn geotechnical engineering [30], a common task is to map the spatial distribution of various sub-surface soil types. With the material recognition capability of DipMe, we propose to dip and esti-mate a map of the subsurface granular material."}, {"title": "6.2 Drawing Interface for VR", "content": "DipMe can be used to build a tangible interactive application for drawing and writing. As shown in Fig. 9, a user employed DipMe to recognize the type of granular media provided, with a gesture similar to a brush dipped in different colors of ink in the real world. He could then complete the switching of virtual brush colors and proceed to subsequent writing tasks in space. We equipped DipMe with a virtual reality (VR) interface as the input device. By tracking the 6D tracker (HTC; VIVE Tracker 3.0) and showing the writing pat-tern on the screen or see-through HMD, users were"}, {"title": "6.3 Shaking Virtual Maracas with Changeable Tones", "content": "As shown in Fig. 10, we developed an inter-face to simulate Maracas\u00b3 using DipMe. In the physical world, shaking a maraca filled with differ-ent materials will produce varying frequencies of sound, while changing the tones of the instrument needs to switch to another maraca with differ-ent designs. We propose to simulate Maraca with DipMe and an interface to change its tone by dip-ping into various granular media. As depicted in Fig. 10, a user probed DipMe into one granu-lar medium. The system recognized the type of the granular media and equipped DipMe with a similar sound from shaking a maraca filled with"}, {"title": "7 Discussion", "content": "Probing Motion. In this work, the recognition of the granular material is based on the haptic data collected from a probing operation. In our applications, a casual dipping motion works well in the interaction. However, if the granular mate-rial is too firm, e.g., cement flour will be hard to dip especially when it gets wet, we may need to twist or shake the device while probing it. As the model is trained from the signals generated by straightly probing the device into the granu-lar media, if the user would like to shake or twist the device to probe it, we need to classify those special motions and retrain the model. Neverthe-less, if the straight probing motion is allowed, our model still works when the motion is operated by different individuals, as validated in Section 7. Therefore, the recognition from active dip-ping is an easy operation for recognizing granular material."}, {"title": "Latency", "content": "As for the probing operation, we require the signals to be sufficiently long for object recognition. In our implementation, we need to take the force and torque signals whose length is about 1.28 seconds. Because the online inference stage of the learning model is fast (less than 0.2 seconds in our system), the latency for obtaining the recognition results is also about 1.48 sec-onds, similar to other tangible user interfaces with object recognition as the preprocess [26]. In our test, we find the latency does not break the user experience, while we will also work to see if we can reduce the latency of the model in the future.\nMultimodal. One possible solution to improve the sensing unit of the input device is to add the information from more channels. For example, the vibration sensed from accelerometers or the sound recorded when the device is dipped into the gran-ular media. We also note that for the simplicity of the design and the privacy of the interaction, we still recommend to use the haptic information. Data from other modes are not quite suitable for the interaction, e.g., the vibration will be easily affected by the signature of the user's motion, the acoustic data might be polluted by the sound in the open environment or the vision data will be expensive to capture and may reveal too much private information during the interaction. In our practice, the haptic recognition as well as the dip motion is a good balance of the design and work well in our tangible applications.\nMore Tangible Applications. We show several new tangible interactive applications with gran-ular material in this work enabled by DipMe."}, {"title": "8 Conclusion", "content": "We propose DipMe, a new input device for haptic recognition of different types of granular media. We show that the haptic signal from a dipping operation is a valid feature to help computer understand types of granular media with modern machine learning tools. The recognition perfor-mance during user interactions is evaluated and several applications enabled by DipMe are demon-strated to show its potential in developing new tangible interfaces with the proposed input device.\nOur work is a first step towards the recogni-tion of granular media types for human-computer interaction. In the future, we will explore how to design and utilize other information such as tac-tile signals that are sensitive to more properties of the granular material, such as humidity or fine-grained friction. Furthermore, we will work on the miniature design of the input device and explore more potential applications by integrating DipMe into VR or AR systems."}, {"title": "Appendix A Development details of the Force/Torque Sensor", "content": "Here we briefly describe the development details of the force/torque sensor introduced in the paper. As shown in the left subfigure of Fig. 2, we employed four compact one-dimensional force sen-sors as the load cells. Each load cell has its measuring range of \u00b140N and its accuracy is < 0.3%F.S.. By composing the readings from the cells, we obtain the force along the z-axis and the torques along the x-axis and y-axis. Given the symmetric layout of the loading cells, the composed force and torques are computed as\n{\nF = F1 + F2 + F3 + F4\nMx = kx(F3-F4)\nMy = ky(F2-F1)\nwhere Fi is the force reading of the ith cell, kx, ky denote the torque coefficients which are related to the distance between the opposite loading cells. Considering the fabrication errors, friction or other factors which may affect the measurements, we determine kx, ky and add other compensa-tion terms by calibrating the sensor with standard loads. We obtain the performance parameters of the developed sensor and list them in the following table."}]}