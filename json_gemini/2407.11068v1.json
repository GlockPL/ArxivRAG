{"title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay", "authors": ["Gon\u00e7alo Hora de Carvalho", "Robert Pollice"], "abstract": "The evaluation of Large Language Models (LLMs) often focuses on linguistic tasks, yet such assessments may not fully capture the models' general reasoning capabilities. We explore the hypothesis that LLMs, such as GPT-3.5 and GPT-4, possess broader cognitive functions, particularly in non-linguistic domains. Our approach extends beyond standard linguistic benchmarks by incorporating games like Tic-Tac-Toe, Connect Four, and Battleship, encoded via ASCII, to assess strategic thinking and decision-making. To evaluate the models' ability to generalize beyond their training data, we introduce two additional games. The first game, LEGO Connect Language (LCL), tests the models' capacity to understand spatial logic and follow assembly instructions. The second game, the game of shapes, challenges the models to identify shapes represented by 1s within a matrix of zeros, further testing their spatial reasoning skills. This \"show, don't tell\" strategy uses games to potentially reveal cognitive capabilities rather than simply querying the models. Our results indicate that despite their proficiency on standard benchmarks and temperature settings, GPT-3.5 and GPT-4's abilities to play and reason about fully observable games without pre-training is mediocre. Both models fail to anticipate losing moves in Tic-Tac-Toe and Connect Four, and they are unable to play Battleship correctly. While GPT-4 shows some success in the game of shapes, both models struggle with the assembly tasks presented in the LCL game. These results suggest that while LLMs like the GPT models can emulate conversational proficiency and basic rule comprehension, their performance in strategic gameplay and spatial reasoning tasks is limited in cognitive flexibility and generalization. Importantly, this reveals a blind spot in current LLM benchmarks that we highlight with our gameplay benchmark suite ChildPlay (GitHub Repository). Our findings provide a cautionary tale about claims of emergent intelligence and reasoning capabilities of LLMs that are roughly the size of GPT-3.5 and GPT-4.", "sections": [{"title": "1 Introduction", "content": "Typically, LLMs are transformer-based models that process input text and generate output text in a coherent and contextually appropriate manner. They utilize the self-attention mechanism to weigh the importance of different words in a sentence relative to each other [38, 7]. Input text is tokenized, converted into vectors using embeddings, and processed through transformer layers that calculate attention scores to dictate focus on relevant tokens [38, 7, 13]. The model then selects the next token based on learned distributions, iteratively generating an arbitrarily long sequence of text [38, 7, 13]. With their enormous parameter counts, from Alpaca with 7 billion parameters [33], to LLaMA with 65 billion [35] or even PaLM and its 540 billion parameters [12], these neural networks have learned to model complex linguistic abstractions, capturing patterns in syntax, semantics, pragmatics, and even elements of style and tone [7, 8, 25].\nBenchmarks for evaluating Large Language Models (LLMs) have been designed to assess compre-hension, generation, and adaptability across a wide range of language tasks. Datasets like SQUAD, GLUE, BIG-bench, and the Im-evaluation-harness offer various test types, including multiple-choice questions, reading comprehension exercises, and dialogue completion tasks. These benchmarks deploy metrics such as response correctness, language generation fluency, and the ability to maintain contextually relevant dialogue [26, 39, 3, 15]. Other benchmarks like SuperGLUE, ANLI, Truth-fulQA, and HellaSwag have been developed to evaluate different aspects of LLM performance, such as natural language understanding, commonsense reasoning, and factual knowledge about diverse topics [40, 24, 21, 42].\nRecent studies have explored alternative approaches to evaluate LLMs' reasoning abilities in non-linguistic modalities. Liga and Pasetto modeled the game Tic-Tac-Toe using ASCII characters, pitting LLMs against the minimax algorithm to observe emergent features, which, according to the authors, might be akin to consciousness [20]. The minimax algorithm is widely considered the optimal algorithm for playing tic-tac-toe, as it guarantees a win or draw against a perfect opponent [2, 1]. While LLMs performed well in some instances, they generally failed to win against the minimax algorithm, often resulting in a draw [20]. Topsakal and Harper [34] used Tic-Tac-Toe encoded with list and illustration prompts in their study. They found that while GPT-4 secured the most wins, it did not always win, indicating that GPT models cannot play Tic-Tac-Toe optimally. This contradiction raises the question: can we truly say the model knows how to play Tic-Tac-Toe if it can explain optimal strategies (see Appendix A.5) but does not consistently win? Or is its performance merely the result of probabilistic outcomes?\nSome critical studies have highlighted the need for caution in interpreting LLMs' capabilities through benchmarking. Lappin et al. assessed their strengths and weaknesses, finding that they excel at many language tasks but struggle with deeper reasoning, world knowledge integration, and context understanding beyond local co-occurrences [19]. And Ze\u010devi\u0107 et al. argued that LLMs may discuss causality but lack true causal reasoning based on interventions and counterfactuals [43].\nBender et al. argue that the lack of transparency and potential risks associated with these large, opaque models raise concerns about their trustworthiness and accountability [4]. While the criticism of Bender et al. focuses on the social dimension of the problem of interpretability and trustworthiness, recent work by Schaeffer et al. critics emergent capabilities and the perceived intelligence of LLMs. They suggest that some claimed \"emergent abilities\" of LLMs may be an artifact of the choice of evaluation metric, rather than fundamental changes in model behavior [27]. Their analyses demonstrate how the use of nonlinear or discontinuous evaluation metrics can create the illusion of emergent abilities, even when the underlying model performance changes smoothly and predictably with scale.\nThis critique of the evaluation metrics used in assessing LLMs invites a deeper exploration of general intelligence - specifically how it can be reliably measured and observed in AI through rigorous and realistic tests that extend beyond linguistic prowess to include broader cognitive functions. If we must define general intelligence (GI), one is to use the \"g factor,\" which refers to the ability to reason, plan, solve problems, think abstractly, and learn quickly across a wide range of domains [30, 5, 41, 10, 9]. GI then involves higher-order cognitive processes that go beyond specific skills or knowledge domains [16, 17].\nA critical issue that arises in analysing the reasoning capabilities of large and opaque models like the GPT series, is training-test set cross-contamination, which becomes increasingly problematic for the most advanced models [7]. The massive training datasets used, comprising extensive portions of the internet, are often untraceable and completely anonymous to researchers outside the initial developer groups, to some extent even to the developers themselves, making replication studies impossible [7, 14]. The exact amount and identity of data used to train models like GPT-3.5 or GPT-4 has not been publicly disclosed, posing a risk of rendering current benchmarking efforts meaningless due to cross-contamination.\nResearchers have attempted to counter the contamination problem using N-Gram Overlap as a metric for detection, by eliminating or withholding results for tests where answers were present in the training data [7]. However, this method has been criticized. Blodgett et al. point out, for example, that such heuristic approaches to mitigating biases in NLP systems can be problematic and may not fully address the underlying challenges [6]. The method is also limited in that it fails to consider the context in which N-Grams appear and may discount synonymous or analogous text worded differently."}, {"title": "2 Experiments", "content": "Specific tasks in the BIG-bench benchmark [3], among others, are categorized as either zero-shot, one-shot, or multi-shot [7]. Our tasks fit the zero-shot category, as models are given only a brief explanation at inference time with no examples for playing beyond the explained formalism. To demonstrate the reasoning capabilities of LLMs beyond their training data, we focus on a modality not explicitly trained for: spatial reasoning about ASCII sequences. An agent capable of true abstraction should be able to encode and interpret these sequences if the rules are explained or known.\nFor this purpose, we developed several tasks, including LEGO assembly, ASCII games of Tic-Tac-Toe, Connect-Four, and Battleship, as well as identifying simple geometrical shapes represented as 1s in 15-sided grids of Os. The same models were deployed over all experiments, namely gpt-3.5-turbo-1106, and gpt-4-1106-preview, which in this paper are referred to as GPT-3.5 and GPT-4, respectively. Every experiment was tested across different temperature settings (t) per model, namely t=0, t=0.5, t=1, and t=1.5. When asked about their understanding of the tasks, GPT-3.5 and GPT-4 were able to generate board states and explain the queried games, including their rules and optimal play. Thus, we consider the tests valid: if the models are truly capable of reasoning, they should be able to play these games optimally given that they \"know\" and are capable of explaining what playing optimally means (see Appendix A.5). Experiments ran over night, at minimum taking a couple of minutes and at most taking a few hours.\nLego Connect Language (LCL) We invented a formal language we call LEGO Connect Language (LCL). More specifically, we propose $LCL_2$ as a language to instruct assembly in 2D on the x and y axis (this can easily be generalised to $LCL_3$ - instructions along the x, y, and z axis). The models were given instructions and their output was fed through a visualizer script to generate the images contained in this work. Only 2x4 pieces were allowed. A piece P (see Fig 1) is then defined as a tuple $P = (l, w, (x, y), c, h)$. A construction, M, is then a valid construction in LCL2 if no pieces are overlapping and all pieces are connected to other pieces. Namely, a Lego piece is connected through interlocking pegs, not by merely touching sides. And secondly, two Lego pieces overlap when they share the same y-coordinate and any part of their length has the same x-coordinate."}, {"title": "3 Results", "content": "As previously stated, Tic-Tac-Toe as a benchmark has been tackled before [20, 34]. Since it is quite popular, we decided to replicate it before creating new games. But this time using an ASCII encoding instead of a list of moves such that we can gauge spatial reasoning through symbolic reasoning. For comparison with the model's performance, Fig. 4 presents the Tic-Tac-Toe match results of the minimax algorithm against the same random player the models played against. This outcome creates a baseline for optimal play against a random player.\nTic-tac-toe, Connect-four, and Battleship To check for a win, we determine if the player has successfully connected the winning number of pieces in a row on the board, which could be horizontally, vertically, or diagonally. To detect missed and blocking moves, we simulate all potential moves for the player by checking if placing a piece in any column leads to a win. If such a move is found, and the player does not execute it on their turn, it is recorded as a missed win, if such a move is found for the opponent and the player does not block it, we register it as missed blocking move. We define incorrect moves to mean a move that was illegal, such as playing a position that has already been played. This results in an immediate loss.\nFig. 5 encompasses comparative results from playing Connect-Four, Tic-Tac-Toe, and Battleship. Each subfigure, 5a, 5b, and 15, respectively, outlines the number of games won by the models.\nUnfortunately, the models were incapable of following the rules for the Battleship game, that is, regardless of temperature, the models lose the large majority of games, with GPT-4 not winning a single game due to incorrect moves (cf. Fig. 17). GPT-3.5 wins around 10% of the matches at low temperatures, but none at higher temperatures, we refer to Fig. 15 in the Appendix A.3.3 instead.\nIt is notable that both GPT-3.5 and GPT-4 exhibit their poorest performance in both Connect-Four and Tic-Tac-Toe at a temperature setting of 0, indicative of deterministic play that reflects the models' learned strategies (Appendix A.3). The Random Player's normal distribution across columns (Fig. 13) suggests a lower likelihood of countering GPT's central strategies, in both games, but particularly at Tic-Tac-Toe where GPT-3.5 commits more errors than GPT-4, significantly impacting outcomes due to incorrect moves (Fig. 5b). These errors generally increase with temperature, probably due to enhanced choice randomness (Fig. 11). This explains the lack of direct model losses from final defeating moves since losses often result from illegal moves.\nAverage game moves, missed wins, and blocks in both Tic-Tac-Toe and Connect-Four are further illustrated in Figs. 6a and 6b, highlighting a decrease in these metrics as temperature rises, suggesting that higher settings potentially broaden the explored moves within the models' strategies. Conclu-sively, neither model plays the games optimally, as evidenced by the considerable number of missed wins and blocks. Both subfigures demonstrate that, as temperature increases, the number of missed wins and blocks decreases. This might suggest that higher temperature settings potentially increase the explored moves in the models' learned strategy, in case there is any. We can conclude the same as before, namely that neither model can play Tic-Tac-Toe optimally given the number of missed wins and missed blocks.\nThe number of moves of GPT-3.5 and GPT-4 per game (see Fig. 62) can be thought of as a measurement of stability in gameplay, not just against the random player, but in general, given that a longer game entails that the model is not losing to illegal moves or to its oponnent. It increases linearly with temperature, inversely correlated with performance measured by the decrease in missed wins and blocks. Tic-Tac-Toe shows a linear improvement, whereas Connect-Four experiences an exponential boost in performance from temperature 0 to 0.5, followed by a linear decline. The random player consistently performs better against GPT-3.5 in Tic-Tac-Toe but loses more frequently\nShapes In the game of Shapes, a correct detection happens when the player's selected shape corresponds with the shape shown on the board. Players have four choices: \"circle,\" \"triangle,\" \"square,\" and \"cross\". Notably, a circle is never actually displayed to the model, and the positions of these choices are not randomized to test if the model displays any inherent bias for the question order. This does not affect the outcome, since the game does not change across different sessions as it is designed to operate within a single question-response framework.\nIn the shape detection test results in Fig 7 we see that GPT-3.5's performance was approximately equivalent to that of random chance when identifiying triangles and crosses, yet it completely failed to recognize squares. In contrast, GPT-4 performed remarkably well, successfully identifying shapes with an accuracy of \u2248 80%, demonstrating particularly prociency at recognizing triangles .\nIn the game of LCL, both models systematically failed to respect the two rules, namely that Lego pieces must be connected through interlocking pegs, not by merely touching sides, and secondly, that no Lego pieces may overlap, which occurs when they share the same y-coordinate and any part of their length has the same x-coordinate. For example, Figs. 8a\u00b9, and 8b2 show valid LCL assemblies, while Fig. 8c\u00b3 shows an invalid LCL structure. While subfigs. 8d4 and 8f show invalid output from GPT-3.5, and Fig.8e6 shows a valid output from GPT-4 and Fig. 8h7 shows an example of an invalid output by GPT-4."}, {"title": "4 Discussion", "content": "In Tic-Tac-Toe, both models underperform relative to the minimax algorithm baseline, while showing mixed performance at Connect-Four. GPT-4 performs unexpectedly well at the Shapes game, but GPT-3.5 does very poorly. Also unexpectedly, both models fail to assemble or detect valid Lego structures in the LCL game. In Battleship, the models' failure to follow game rules, especially at higher temperature settings, indicates a significant limitation in their ability to understand and apply structured game rules. The linear increase in the number of moves with temperature suggests that higher temperatures lead to greater exploration of possible moves, but do not improve strategic performance. The increase in missed wins and blocks with temperature further supports this, as greater randomness in decision-making does not enhance the models' strategic play.\nOverall, these results show that while GPT-3.5 and GPT-4 can play simple games to some extent, they struggle with more complex tasks and do not consistently apply optimal strategies. The performance gap between the models and the minimax algorithm highlights the limitations of current language models in tasks requiring precise strategic reasoning and the failure to play Battleship and LCL demonstrates a failure in rule adherence.\nThe primary aim of contemporary benchmarks for LLMs has been to assess these models through adaptations of Turing's test [36], evaluating their capability to process and respond to language inputs comparably to humans. However, defining the language problem solely in these terms may overlook deeper complexities. While the transformer architecture in deep neural networks has enabled models smaller than GPT-4 to exhibit what Wilhelm von Humboldt described as the \"infinite use of finite means\" [23] or their ability to generate a potentially unlimited number of contextually relevant sentences [32] (an idea popularised by Chomsky [11]), this does not necessarily imply that these models have mastered a form of reasoning. Rather, they may simply be engaging in an advanced form of pattern imitation."}, {"title": "4.1 Limitations and Future Work", "content": "Our proposed benchmark, ChildPlay, primarily uses binary (win/loss) outcomes for games, which can be considered discontinuous metrics. This formulation may exaggerate perceived capabilities by registering a full loss even if the model's failure was marginal. We try to avoid this simplistic classification by registering, for example, the choice of moves on the board games (see Appendix A.3) as well as the count of missed blocks and missed wins (cf. Fig. 6). In contrast, tasks involving shape recognition or LCL could utilize more continuous metrics, providing a smoother performance gradient and potentially more accurate reflections of a model's reasoning abilities.\nUsing discontinuous metrics in strategic games could manifest as sharp transitions in model evaluation, accentuating a sudden jump in perceived ability when the model first succeeds. Nonlinear metrics in the shape game or LCL tasks may not exhibit such abrupt transitions but could still misrepresent gradual improvements.\nBased on Schaeffer et al.'s perspective, one could argue that the games proposed in ChildPlay may not entirely reflect true generalization or emergent abilities [27]. If these benchmarks are akin to nonlinear or discontinuous metrics, they might exaggerate the weaknesses or strengths of LLMs in strategic games. For instance, a sharp failure in a game like Tic-Tac-Toe might not mean the model lacks strategic reasoning universally but that it fails under the specific discontinuous conditions of the game setup, or of temperature. Such an assessment could lead to the erroneous conclusion that LLMs are generally poor at strategic decision-making when, in fact, they might only be unsuited to the specific scenarios or metrics used in ChildPlay.\nConversely, unlike continuous metrics that might smooth over deficiencies and give a misleading picture of gradual improvement, the use of games as benchmarks could prove a better test of an LLM's cognitive and strategic abilities regardless of metric continuity (given that the model has not been overfitted on the game)."}, {"title": "5 Conclusions", "content": "Non-language-based tasks are important as they challenge models to demonstrate generalization across different information encodings or forms of input, and, most importantly, to delve into out-of-training-distribution topologies. Testing LLMs like GPT-4 (according to OpenAI, the current contender to AGI [8]) beyond the text they were primarily trained on via our \"show, don't tell\" strategy, we demonstrate that it is still mediocre at best at even very simple reasoning tasks that are outside of its training data. The models fail to play optimally at very simple games, such as tic-tac-toe, battleship, and connect-four. We also experimented with LEGO assembly, finding the LLMs still performing poorly. Mixed results were found at the task of interpreting geometric shapes from binary grids. These tasks are then designed to test reasoning without relying on language skills, such that the model cannot get by through parroting - it must be capable of playing the game. Currently, the \"non-language\" category of the BigBench benchmark shows 16 active tasks, including explicit ASCII recognition tasks, chess, and Sudoku, but, to the best of our knowledge, no task like ours [3]. Hence, we believe that ChildPlay is a useful addition to the suite of current established LLM benchmarks.\nIn general, this work shows that developing games allows us to critically examine claims regarding a models' ability to reason and solve problems regardless of the persistent issue of data contamination. In other words, we explore what the model knows by making it play games instead of asking it how to play them. Our results suggest that current LLMs show disappointing performance in terms of problem solving capabilities and reveal important aspects to be considered for future improvements."}]}