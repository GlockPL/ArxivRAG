{"title": "StateAct: State Tracking and Reasoning for Acting and Planning with Large Language Models", "authors": ["Nikolai Rozanov", "Marek Rei"], "abstract": "Planning and acting to solve 'real' tasks using large language models (LLMs) in interactive environments has become a new frontier for AI methods. While recent advances allowed LLMs to interact with online tools, solve robotics tasks and many more, long range reasoning tasks remain a problem for LLMs. Existing methods to address this issue are very resource intensive and require additional data or human crafted rules, instead, we propose a simple method based on few-shot in-context learning alone to enhance \u2018chain-of-thought' with state-tracking for planning and acting with LLMs. We show that our method establishes the new state-of-the-art on Alfworld for in-context learning methods (+14% over the previous best few-shot in-context learning method) and performs on par with methods that use additional training data and additional tools such as code-execution. We also demonstrate that our enhanced 'chain-of-states' allows the agent to both solve longer horizon problems and to be more efficient in number of steps required to solve a task. We show that our method works across a variety of LLMs for both API-based and open source ones. Finally, we also conduct ablation studies and show that 'chain-of-thoughts' helps state-tracking accuracy, while a json-structure harms overall performance. We open-source our code and annotations at https://github.com/ai-nikolai/StateAct.", "sections": [{"title": "1 Introduction", "content": "Using the in-build world and commonsense knowledge\u00b9 of large language models (LLMs), such as GPT-3, Mixtral, Gemini (Brown et al., 2020; Jiang et al., 2024; Anil et al., 2023) to perform interactive reasoning tasks has become a frontier in AI research, with \"AI Agents\" now able to solve a range of multi-modal complex tasks (Durante"}, {"title": "2 Background", "content": "AI agents have historically used reinforcement learning (RL) to solve tasks (Sutton and Barto, 2018). With the dawn of LLMs, works such as Li et al. (2022); Nottingham et al. (2023) combined LLMs and RL and trained additional policies or value functions to make predictions."}, {"title": "2.1 In-context learning approaches", "content": "Huang et al. (2022a,b); Singh et al. (2022) were among the first to use LLMs directly to act in an interactive environment, their method produces agent actions as output after receiving environment observations as input.\nReAct (Yao et al., 2023b) took this work further by combining 'acting' (Huang et al., 2022a) and 'chain-of-thought' (Wei et al., 2023). ReAct establishes state-of-the-art for in-context learning only based approaches and while it is a very scalable method, the performance (i.e. success rate) is still limited.\nExpeL (Zhao et al., 2023) uses additional training data to generate 'insights' and 'success trajectories' during training. At inference time they look-up the closest 'success trajectories' as few-shot examples to the agent (as opposed to fixed few-shot examples that we use) and augment them with these 'insights' to perform the final inference. They achieve 59% on Alfworld using retrieved \u2018success trajectories' (as few shot examples) + 'insights' and 50% using the same few-shot examples as ReAct + 'insights'."}, {"title": "2.1.1 In-context learning and additional tools", "content": "The current state-of-the-art for in-context learning based approaches in combination with additional tools is AdaPlanner (Sun et al., 2023). They introduce a code-based prompt (Li et al., 2023a) and use code-execution as an additional tool to execute the LLM generations to feed them back into the next prompt. The shortcoming of AdaPlanner is that it requires very complex human crafted prompts that are hard to scale to new environments as well as the additional step of requiring code-execution."}, {"title": "2.2 State tracking in LLM-based agents", "content": "Concurrent work to ours, AutoGuide (Fu et al., 2024), uses ReAct as the base agent and additional training data to create 'state-aware' text-based guidelines for the LLM-agent, they then use a type of retrieval augmented generation (RAG) process to guide the decision making process. They embed the current observation as a summary (this is what they call 'state-aware') and use an LLM to 'look up' the relevant 'state-aware' guideline, which is then fed into a final LLM to generate an action. Using training data and LLM-based RAG they achieve 79.1% on top of a ReAct agent on Alfworld. Their training and RAG approach could be used complementary to our StateAct LLM agent\u00b2.\nChen et al. (2024) propose state-tracking as a way to help the agent solve the task, without training data. Their method differs to ours two-fold. Firstly, they employ a complex sequence of components working together (an LLM-based attention over the observations, an LLM-based compilation of a complex state and finally a prediction of a program). Secondly, their system involves execution of actual programs. Our method on the hand requires a straight-forward extension of 'chain-of-thought' and uses a single LLM call to produce the state, thought and action and we do not require program execution."}, {"title": "2.2.1 Fine-tuning approaches", "content": "Previous fine-tuning approaches did not significantly enhance performance (Zhou et al., 2024; Yao et al., 2023b; Shridhar et al., 2021). A concurrent work, however, ActRe (Yang et al., 2024b) achieves 83% by fine-tuning gpt-3.5-1106 on additional training data."}, {"title": "2.3 \"Multi-Agent Conversation\u201d approaches", "content": "A new trend is to use multiple LLMs concurrently to 'chat' to one another to produce a result. A recent work in this direction by (Wu et al., 2023) achieves 67% on Alfworld."}, {"title": "2.4 Joined rule and LLM based agents", "content": "StateFlow (Wu et al., 2024) uses Finite State Machines (FSMs) combined with LLMs to solve Alfworld. These FSMs are human-expert crafted states, transitions and rule-based heuristics, where the LLM is asked to perform limited tasks in each of the given states. While their performance is 82%"}, {"title": "3 Method", "content": "StateAct is an LLM-based AI agent that works on top of pre-trained large language models. StateAct takes the textual 'observation' from the environment and after a single call to the pre-trained LLM returns the 'action' back to the environment (without the use of additional tools or resources), see Figure 1a.\nStateAct utilises in-context learning (Brown"}, {"title": "3.1 Goal-reminders and state- tracking using LLMS", "content": "In order to make StateAct more precise, we introduce simple notation. By denoting \u03c0 as the policy of an AI agent, in the standard case at time step t the policy predicts action $a_t$, given the history of observations and actions $[o_t, a_{t-1}, ..., o_0, a_0]$.\n\n$\\pi(a_t | o_t, a_{t-1}, ..., o_0, a_0)$ (1)\n\nWhere $a_t$ is the action produced by the agent at step t and $o_t$ is the observation produced by the environment at step t after receiving action $a_t$ as input. Usually, the first observation $o_0$ also contains the 'goal' description for the given environment.\nFor our case we need to enhance the policy to incorporate the 'state'. Similar to previous work (Yao et al., 2023b) we introduce the context vector, $C_t$. The context vector contains the action, as well as the other additional predictions of the agent, i.e. $C_t = (g_0, S_t, r_t, a_t)$. Where $g_0$ is the goal and always remains the same (for a given environment) and uses the goal extracted from $o_0$, $S_t$ represents the predicted state at time step t, $r_t$ represents 'chain-of-thought' style \u2018reasoning' at time step t, and $a_t$ represents the action at time step t, as before. The new policy \u03c0 then becomes:\n\n$\\pi_{contextual}(C_t | o_t, C_{t-1}, ..., C_0, o_0)$ (2)\n\nIn our case the LLM acts as contextual and produces the context vector at every time step."}, {"title": "4 Experimental setup", "content": "Our aim is to study long-range acting, planning and reasoning capabilities of LLM-based agents. To achieve this, in line with previous work, we turn to simulated environments as an evaluation framework and to API-based state-of-the-art large language models. Specifically, we use Alfworld (Shridhar et al., 2021), a household robotics environment, and Webshop (Yao et al., 2023a), an online shopping environment, as simulated environments. In line with previous work we use OpenAI's3 gpt-3.5-turbo-1106. Additionally, we evaluate on additional LLMs and validate our results."}, {"title": "4.1 Alfworld", "content": "Alfworld (Shridhar et al., 2021) is based on a 3D, visual, household robotics environment called Alfred (Shridhar et al., 2020), which was translated"}, {"title": "4.1.1 Alfworld correction", "content": "In our research we identified that Alfworld has a specific syntactic feature for the put command, namely put <object> in/on <place>, where \"in/on\" needs to be written exactly this way and using only \"in\" or only \u201con\u201d produces a failed command. We observed this issue with LLMs on this environment and we propose a simple fix for it. We map: 1. \"put <object> in <place>\" and 2. \"put <object> on <place>\" to the command accepted by Alfworld, namely \u201cput <object> in/on <place>\u201d.\nMethods such as AdaPlanner (Sun et al., 2023) have avoided this issue because they use code-based prompts and regex parsers. However, methods such as ReAct (Yao et al., 2023b) and ExpeL"}, {"title": "4.2 Webshop", "content": "Webshop (Yao et al., 2023a) is a simulation of an online shopping experience. Given a task, e.g. \u201cI want a blue water-proof winter jacket,\nless than $100\", the agent needs to search a\nproduct catalogue, browse through the search re-sults and select the most fitting product, select the\nattributes, e.g. colour, size, and then buy the\nproduct. In line with previous work we use the text-based version of Webshop, where all descriptions of the website are given in text form, see Figure\n4. Webshop features a realistic large-scale product catalogue, a search engine and very varied product attributes depending on the category of product.\nSee Appendix B for more details. In total the test\nset consists of 500 examples and each one is of the\ntype \"search and buy a product\u201d. Overall, Webshop\nhas a maximum of 15 steps and two commands: 1.\nsearch[<query>], 2. click[<button>].\""}, {"title": "4.3 In context learning", "content": "Since ReAct (Yao et al., 2023b) forms the underlying agent for many current (Zhao et al., 2023)\nand state-of-the-art approaches (Fu et al., 2024), we use the same few-shot 'interaction traces' as Re-Act. The main reason is to have a fair comparison and isolate additional effect - such as performance change from different in-context examples.\nIn total, Alfworld has six types of tasks and Re-Act uses two in-context examples per task type to prompt the language models. On average each Re-Act example ranges from 352 words to 591 words"}, {"title": "4.4 Models", "content": "In line with previous work we focus our main experiments are based on the API based LLMs to compare performance fairly. Concretely, 'gpt-3.5' level models were used in previous work including current state-of-the-art ActRe (Yang et al., 2024b) and state-of-the-art for in-context learning ReAct (Yao et al., 2023b). Many OpenAI models have become deprecated. Notably, all models from ReAct and AdaPlanner(Sun et al., 2023) davinci-002, gpt-3.5-turbo-0301 and gpt-3.5-turbo-0613 are deprecated now. Therefore, we re-implemented ReAct and ran the experiments to determine the most suitable model, see Table 1. We establish that gpt-3.5-turbo-1106 is the best performing (from the ones that remain available) on ReAct and we therefore chose this one. In our work, we additionally compare our method using newer state-of-the-art models to show that our method generalises to new model. Specificially, we use the gpt-40-mini and Mixtral(Jiang et al., 2024) model.. We note that full GPT-4 level models are prohibitively expensive. We use temperature 0 for all experiments and sample only the top 1 response, see Appendix C the exact settings."}, {"title": "4.5 Metrics", "content": "In terms of metrics we use the pre-defined metrics of Alfworld and Webshop, namely success rate (SR). Success is a binary metric per each environment in the respective test sets (135 and 500 respectively). Success in Alfworld means the agent has successfully complete the whole task. In Webshop it means the agent has bought an item that has a hundred percent match with the desired item based on a partially hidden list of attributes of the shopping item (e.g. the colour, size, price, etc.)."}, {"title": "5 Results", "content": "For Alfworld we present the results for ReAct, Ada-Planner with and without code execution and State-Act (ours), which consists of goal + state + thought + action. We also show StateAct without each of the components (i.e. without goal, state and thought). Interestingly we find, contrary to previous findings, that 'thought' or 'reasoning' actually sometimes harms the performance.\nIn Table 2, we can see that StateAct with all goal+state+thought and the correction performs the best. It outperforms ReAct with correction by around 13 points (using the same GPT model for ReAct) and by around 9 points (using the better model for ReAct). StateAct also outperforms Re-Act by 22 points when corrections are not used. Furthermore, StateAct even outperforms AdaPlanner by 2.48 points, an approach that uses regex for command mapping (similar to our correction) and code-execution.\nPerhaps the most surprising finding is that the simple correction described in Section 4.1.1 leads to a 16 and 23 point jump for ReAct and a 27 point jump for StateAct. This indicates that the model generally performs very well, however, struggles with minute differences in domain specific syntax."}, {"title": "5.2 Webshop", "content": "For Webshop we present results for ReAct and StateAct (ours). Similarly, to Alfworld we also present the results of StateAct without each of goal, state and thought. See Table 3. Interestingly, we see that removing thought produces the highest results and outperforms ReAct by 10 points. Our hypothesis is that domain specific syntax, which is more prevalent in Webshop than Alfworld, conflicts with using verbose thoughts."}, {"title": "5.3 Results using additional models", "content": "We validate our results further by comparing on additional LLMs. Again we see a significant boost in performance, see Table 4. We ran Gpt-40-mini and Mixtral (Jiang et al., 2024) on Alfworld. Re-Act achieves 68.15 and 72.59 respectively, while StateAct achieves 71.85(+3.7) and 83.70(+11.2)."}, {"title": "5.4 Summary of results", "content": "In conclusion we found that our simple goal-reminding and state tracking approach that purely relies on in-context learning outperforms previous in-context learning approaches by more almost 10 points and even outperforms leading approaches that rely on code-execution. Interestingly, we found that the approaches are quite sensitive to domain specific syntax and that when this is the case 'thoughts' that are verbose can harm performance."}, {"title": "6 Analysis and Ablations", "content": "In the results section we discovered that our methods perform better than previous state-of-the-art. This answers the question that we can perform better with in-context learning without resorting to additional tools, data or bigger models. In this section we want to analyse our results further and particularly also answer if our second hypothesis that goal 'reminding' and state tracking help with long-range reasoning actually holds. For all ablation studies we focus on Alfworld as it has two favourable properties over Webshop. Firstly, Alfworld has a longer time horizon (50 steps vs. 15 in Webshop), with tasks taking an average of less than 10 steps in Webshop and around 20 to 30 steps in Alfworld. Secondly, Alfworld has much less domain specific syntax and is purely text based, while Webshop has a more specific syntax to follow."}, {"title": "6.1 Do goal reminders help with long range tasks?", "content": "For this purpose we compare the original ReAct (thought+action) with just adding the goal in, i.e."}, {"title": "6.2 What effects does state-tracking have?", "content": "We also analyse whether state tracking helps with long-range reasoning and efficiency. We compared the full StateAct against StateAct without state-tracking as well as ReAct (thought + action) against StateAct with state-tracking added (state + thought + action). In Figure 6 we see that state tracking also helps with long-range reasoning. In fact, we can see that reasoning alone is unable to solve tasks longer than 40 steps, while with state tracking even longer-range tasks can be solved than with goal-tracking alone. Also, looking at Table 5 we see that state-tracking makes the model the most efficient."}, {"title": "6.3 Does the model perform actual state tracking?", "content": "We ask ourselves the question if the model is actually performing state-tracking. For that purpose we look at Alfworld and construct a self-verification algorithm that is able to track the state heuristically based on the actions the agent takes. For example if the agent produces the action go to fridge 1 and the environment accepts this action we update the state with current location: fridge 1. We compare the 'gold' state against the predicted state. Figure 7 shows that StateAct in fact does correct state-tracking 88% of the time. We also observe that thoughts and goals help the state tracking."}, {"title": "6.4 Does json structure help with performance?", "content": "Since we found that domain specific syntax harms performance, we wondered whether adding a structured format like json would help. For this purpose, we re-ran StateAct on Alfworld, but translated the state into a json format, see Appendix D for more details. Surprisingly, we found that the json format harms performance significantly, see Table 6. However, we also see that corrections help the the json format less, indicating that json helps with syntax, but harms performance."}, {"title": "7 Conclusion and future work", "content": "We propose a novel method StateAct, using our 'chain-of-states', based on in-context learning alone and establish a new state-of-the-art for agents that do not perform training, even against methods that use code-execution. The method outperforms the previous state-of-the-art, that uses in-context learning alone, between 9% and 20% given different models and tasks and outperform in-context learning with tools (code-execution) by 3%. We also show that explicit state-tracking and goal reminders make the model more efficient as well as significantly help with longer range tasks.\nWe found that 'thoughts' or explicit reasoning do not always help performance. It would be very interesting to systematise 'thought' and 'states' and"}, {"title": "8 Ethical Considerations", "content": "Running many of the experiments presented in this paper can have a significant computational footprint. We should consider the environment and financial resources for reproduciblity of our work. We aimed to address this concern by using gpt-3.5-turbo level models, reporting costs and minimising the cost of our method."}, {"title": "8.2 Hallucinations in LLMs", "content": "As LLM-based agents become more powerful and therefore more pervasive in our daily lives 'hallucinations' of LLMs can be very harmful (Wei et al., 2024). We hope that explicit state-tracking presented in this work can also lead to future work that can reduce 'hallucinations.'"}, {"title": "9 Limitations", "content": "We evaluated our method only in the English language and on two evaluation benchmarks. While we do not expect major changes in other languages, this is something that should be investigated. Furthermore, performance on other benchmarks should be evaluated as well."}, {"title": "9.2 Reasoning traces rely on human judgement", "content": "Our prompts require human annotations, as such there is a natural bias present. This can have both task-performance implications as well as ethical implications."}, {"title": "A Alfworld", "content": "Alfworld has six different environment types: 1.\nclean, 2. heat, 3. cool, 4. examine, 5. put, 6.\nputtwo.\nThe 'clean' task, e.g. Task: Put a clean\napple on table, requires the agent to first find\nthe apple, then clean it (in the sinkbasin) and then\nput it on a table.\nThe 'heat' task, e.g. Task: Put a hot pie\non table, requires the agent to first find the pie,\nthen heat it (on the stoveburner) and then put it on\na table.\nThe 'cool' task, e.g. Task: Put a cool tomato\non table, requires the agent to first find the tomato,\nthen cool it (with the fridge) and then put it on a\ntable.\nThe 'examine' task, e.g. Task: Examine the\nmug with the desklamp, requires the agent to\nfirst find the mug, then find the desklamp, and then\nuse the desklamp.\nThe 'put' task, e.g. Task: Find some apple\nand put it in sidetable, requires the agent to\nfirst find an apple, and then put it on the sidetable.\nThe 'puttwo' task, e.g. Task: Put two\ncellphone in sofa, requires the agent to first\nfind one cellphone, and then put it on the sofa, and\nthen to find the second one and put it on the sofa."}, {"title": "A.2 Action Types", "content": "Alfworld has the following valid actions: 1. go to,\n2. open, 3. close, 4. put, 5. take, 6. cool, 7. heat, 8.\nuse.\ngo to <place>\nExample: go to table 1\nopen <object>\nExample: open door 1\nclose <object>\nExample: close door 1\nput <object> in/on <place>\nExample: put apple 1 in/on table 1\ntake <object> from <place>\nExample: take apple 1 from table 1\ncool <object> with <place>\nExample: cool apple 1 with fridge 1"}, {"title": "A.3 License", "content": "Alfworld has the permissible MIT license, we used it in line with the license."}, {"title": "B Webshop", "content": "Webshop has one environment type: \u2018search &\nbuy', as well as two commands: 1. search, 2. click.\nclick[<button>]\nExample: click[< Back to Search]\nsearch[<query>]\nExample: search[interesting book]"}, {"title": "B.2 Prodcuts and attributes", "content": "Webshop has over 1 million real-world products across 5 main categories (fashion, makeup, electronics, furniture, and food) and 113 sub-categories."}, {"title": "B.3 License", "content": "Webshop has the permissible Princeton license, we used it in line with the license."}, {"title": "C Code snippet to call OpenAI / GPT-3.5", "content": "A prompt is given in Appendix E."}, {"title": "D StateAct Json Format", "content": "We translate the text based StateAct prompt:\n>goal: put a hot apple in fridge\ncurrent location: starting location\ncurrent inventory: None\nthought: To solve the task, I need to\nfind and take an apple, then heat it\nwith microwave, then put it in\nfridge. First I need to find an\napple. An apple is more likely to\nappear in fridge (1), diningtable\n(1), coffeetable (1), drawer (1),\ncabinet (1-13), garbagecan (1). I\ncan check one by one, starting with\nfridge 1.\naction: go to fridge 1\nInto the following json format:\n>{\"goal\": \"put a hot apple in fridge\",\n\"current_location\": \"starting location\",\n\"current_inventory\u201d: None,\n\"thought\": \"To solve the task, I need to\nfind and take an apple, then heat\nit with microwave, then put it in\nfridge. First I need to find an\napple. An apple is more likely to\nappear in fridge (1), diningtable\n(1), coffeetable (1), drawer (1),\ncabinet (1-13), garbagecan (1). \u0406\ncan check one by one, starting with\nfridge 1.\"\n\"action\": \"go to fridge 1\"\n}"}, {"title": "E StateAct Prompts", "content": "Interact with a household to solve a\ntask.\nHere are 2 examples:\nYou are in the middle of a room.\nyou see a\ncabinet 13, a cabinet 12, a cabinet\n11, a cabinet 10, a cabinet 9, a\ncabinet 8, a cabinet 7, a cabinet 6,\na cabinet 5, a cabinet 4, a cabinet\n3, a cabinet 2, a cabinet 1, a\ncoffeemachine 1, a countertop 1, a\ndiningtable 1, a drawer 1, a fridge\n1, a garbagecan 1, a microwave 1, a\nshelf 3, a shelf 2, a shelf 1, a\nsinkbasin 1, a stoveburner 4, a\nstoveburner 3, a stoveburner 2, a\nstoveburner 1, and a toaster 1.\nYour task is to: put a hot apple in\nfridge.\n>goal: put a hot apple in fridge\ncurrent location: starting location\ncurrent inventory: None\nthought: To solve the task, I need to\nfind and take an apple, then heat it\nwith microwave, then put it in\nfridge. First I need to find an\napple. An apple is more likely to"}]}