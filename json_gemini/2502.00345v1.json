{"title": "The Composite Task Challenge for Cooperative Multi-Agent Reinforcement Learning", "authors": ["Yurui Li", "Yuxuan Chen", "Li Zhang", "Shijian Li", "Gang Pan"], "abstract": "The significant role of division of labor (DOL) in promoting cooperation is widely recognized in real-world applications. Many cooperative multi-agent reinforcement learning (MARL) methods have incorporated the concept of DOL to improve cooperation among agents. However, the tasks used in existing testbeds typically correspond to tasks where DOL is often not a necessary feature for achieving optimal policies. Additionally, the full utilize of DOL concept in MARL methods remains unrealized due to the absence of appropriate tasks. To enhance the generality and applicability of MARL methods in real-world scenarios, there is a necessary to develop tasks that demand multi-agent DOL and cooperation. In this paper, we propose a series of tasks designed to meet these requirements, drawing on real-world rules as the guidance for their design. We guarantee that DOL and cooperation are necessary condition for completing tasks and introduce three factors to expand the diversity of proposed tasks to cover more realistic situations. We evaluate 10 cooperative MARL methods on the proposed tasks. The results indicate that all baselines perform poorly on these tasks. To further validate the solvability of these tasks, we also propose simplified variants of proposed tasks. Experimental results show that baselines are able to handle these simplified variants, providing evidence of the solvability of the proposed tasks. The source files is available at https://github.com/Yurui-Li/CTC.", "sections": [{"title": "1. Introduction", "content": "Specialization has long been recognized as a cornerstone of efficiency enhancement, a foundational concept dating back to Adam Smith's seminal analysis in The Wealth of Nations (Smith, 2002). In practical applications, this principle manifests through division of labor (DOL) and cooperative mechanisms, with DOL serving as a critical catalyst for improving both the efficiency and effectiveness of cooperative systems. Within the domain of Multi-Agent Reinforcement Learning (MARL), DOL presents a natural framework for advancing cooperative learning, as task specialization among agents can significantly improve collective performance. However, prevailing MARL testebds-including the StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019a), SMACv2 (Ellis et al., 2022), and Google Research Football (GRF) (Kurach et al., 2020)\u2014prioritize evaluations of low-level task execution while neglecting systematic assessment of agents' capacity to establish meaningful DOL structures. This limitation undermines their utility in guiding research on sophisticated cooperative policies. For instance, in SMAC tasks, optimal performance is frequently achieved through simplistic tactics such as concentrated fire on shared targets (Li et al., 2023; Liu et al., 2024; 2023; Mahajan et al., 2019; Yang et al., 2022; Yu et al., 2023; Hu et al., 2023; Shao et al., 2022; Zang et al., 2024), a policy that does not inherently necessitate role differentiation or specialization. Similarly, in GRF's academy scenarios, the most readily learnable policies involves passing the ball to agents positioned far from defenders, after which a single agent can independently score while others engage in non-coordinated movement (Fu et al., 2024b; Li et al., 2021; Xu et al., 2023). Although some MARL methods have incidentally developed DOL policies (Zang et al., 2024; Fu et al., 2022; 2024b; Li et al., 2021; Xu et al., 2023), such specialization remains non-essential for task success in these tasks. Consequently, existing testbeds fail to adequately assess or incentivize the development of advanced DOL capabilities, thereby limiting their ability to drive progress in cooperative MARL research. This gap highlights the need for testbeds that explicitly require DOL and cooperation to advance the study of cooperative multi-agent systems.\nLots of cooperative MARL researches have increasingly in-"}, {"title": "2. CTC Tasks", "content": "In this section, we first introduce how CTC ensures that DOL and cooperation are necessary condition for completing tasks. Second, we introduce three factors to expand the diversity of CTC tasks to cover more realistic situations. Finally, we introduce the SMAC-based CTC tasks implementation."}, {"title": "2.1. DOL and Cooperation Guarantee", "content": "We design the CTC tasks by combining atomic subtasks in a way that ensures DOL and cooperation are essential for task completion.\nWe first introduce two atomic subtasks: the defense subtask and the pursuit subtask. In the defense subtask, a base building is set, and a group of enemies aims to attack this building. The defense subtask fails if any enemy occupies the base building. The success condition for this subtask is the complete elimination of all enemies. In the pursuit"}, {"title": "2.2. Diversity of CTC tasks", "content": "Real-world applications are inherently complex and diverse, and merely combining two atomic subtasks does not adequately capture the full scope of these scenarios. By incorporating three key factors (information interference, subtasks dissimilarity, subtasks quantity) we can design a wider range of tasks with varying levels of difficulty. This design offers a step-by-step progression for advancing cooperative MARL methods and brings CTC tasks closer to real-world applications, providing a more robust evaluation framework for multi-agent cooperation and decision-making."}, {"title": "Information Interference", "content": "The composition of CTC tasks inherently involves multiple atomic subtasks, where the information from other subtasks is irrelevant to the completion of the current subtask. If such information is observed, it can interfere with the agent's abil-"}, {"title": "Subtasks Dissimilarity", "content": "In practical applications, subtasks are often not homogeneous in nature, and the similarity between subtasks significantly influences the difficulty of completing the overall task. Tasks composed of similar subtasks tend to be less challenging than those involving dissimilar subtasks. Furthermore, the proportion of dissimilar subtasks within a task also affects its overall difficulty. By incorporating these two factors, we ensure that the tasks we design better reflect the complexities encountered in real-world scenarios. Figures 1(b) and (c) illustrate two complex tasks we developed. Task Mixed_2_Subtask combines a pursuit subtask with a defense subtask, where the proportion of dissimilar subtasks are balanced; Task Mixed_3_Subtask integrates a pursuit subtask with two defense subtasks, where the proportion are not balanced."}, {"title": "Subtasks Quantity", "content": "The number of subtasks within a task is directly influence its complexity and difficulty. In general, tasks involving a greater number of subtasks are more challenging to solve. A method capable of managing a wider type and bigger number of subtasks demonstrates greater adaptability across diverse tasks. Additionally, the ability to handle a larger number of subtasks often indicates that the method can control more agents, thereby showcasing better scalability in"}, {"title": "2.3. SMAC-Based Implementation", "content": "We implement the CTC tasks within the SMAC environment, a widely adopted testbed for cooperative MARL. Detailed descriptions of each task are provided in Table 5. To maintain consistency across tasks, both the enemy and agent forces are composed of identical types and numbers of units. This design ensures that the difficulty of the tasks is not influenced by imbalanced military strength, allowing MARL methods to focus on addressing the unique challenges posed by the CTC tasks. For the force composition, we select three types of Terran units from StarCraft II: Marine, Marauder, and Medivac. The Medivac unit, in particular, possesses two distinctive features, which we leverage to facilitate the pursuit subtask and priority within subtask. Feature 1: When no other units are present in its group, the Medivac will continue moving toward the target point, even when under attack. Feature 2: When other units are present in its group, the Medivac will halt to heal its allies upon being attacked; If all allied units are defeated, the Medivac will resume its movement toward the target point, ignoring the attacker. Feature 1 is crucial for the pursuit subtask, as it ensures that"}, {"title": "2.4. Simplified Variants", "content": "We validate the solvability of the CTC tasks through simplified variants, providing evidence that the tasks are tractable while still remaining challenging for current methods. The simplification is achieved by adjusting two key factors: (1) the heterogeneity of the agents and (2) the symmetry of the enemies across subtasks. For factor (1), we classify tasks based on agent types as either homogeneous or heterogeneous. A task is considered homogeneous when all agents are of the same type, whereas it is heterogeneous when agents differ in type. For factor (2), we classify tasks as symmetrical or asymmetrical based on the number and type of enemies assigned to each subtask. A task is symmetrical when each subtask has identical numbers and types of enemies, while it is asymmetrical when these characteristics differ across subtasks. Using this setting, we categorize the CTC tasks as heterogeneous asymmetrical (HeA) tasks, which are the most challenging configurations compared to homogeneous or symmetrical tasks. Formally, we introduce simplified variants of CTC tasks: heterogeneous symmetrical (HeS) tasks, homogeneous asymmetrical (HoA) tasks, and homogeneous symmetrical (HoS) tasks to demonstrate the solvability of the CTC tasks. It is important to note that when agents are homogeneous, the pursuit task effectively degenerates into a defense task. Therefore, we provide homogeneous or symmetrical versions only for the tasks illustrated in Fig. 1(d), (e), and (f). The details of each task configuration are provided in Table 6, Table 7, and Table 8."}, {"title": "3. Experiments", "content": "In this section, we carry out our experiments to evaluate the performance of three types cooperative MARL methods on our proposed tasks."}, {"title": "3.1. Experimental Setup", "content": "Baselines To demonstrate the performance of the current state-of-the-art methods in our CTC tasks, we select 10 MARL methods from three categories (policy diversity, agent grouping, and hierarchical MARL) as baselines. The selected methods are as follows: QMIX\u00b9, EOI2, MAVEN3,\nhttps://github.com/hijkzzz/pymarl2\nhttps://github.com/jiechuanjiang/eoi_pymarl\nhttps://github.com/chaobiubiu/DCC"}, {"title": "3.2. Main Results", "content": "We present the maximum test winning rates of baseline methods on all CTC tasks across three seeds in Table 2. Given the prevalence of zero values, we focus on displaying only the non-zero results in Fig. 2.\nFirst, we examine the three tasks related to information interference. For the HeA_P2G-D2 task, the maximum test winning rate of all baseline methods is zero. In the HeA_P2G-D1 task, only one method achieves a non-zero maximum test winning rate, and in the HeA_P2G-D3 task, two methods show non-zero maximum test winning rates. Notably, GoMARL achieves a maximum test winning rate of 100% on the HeA_P2G-D3 task, but this drops to 18.75% on the HeA_P2G-D1 task and to zero on the HeA_P2G-D2 task. As illustrated in Fig. 2(c), the mean test winning rate of GOMARL remains non-zero only in the early stages of training, but it becomes zero for the remainder of the training. In contrast, Fig. 2(d) shows that the mean test winning rate of GOMARL steadily increases throughout the training process and eventually converges. This indicates that GOMARL performs well in the absence of information in-\nhttps://github.com/lich14/CDS\nhttps://github.com/TonghanWang/RODE\nhttps://github.com/TonghanWang/ROMA\nhttps://github.com/zyfsjycc/GoMARL\nhttps://github.com/linkjoker1006/pymarl3\nterference and is capable of utilizing information from other subtasks when available. For ROMA, the maximum test winning rate on the HeA_P2G-D3 task reaches 50%, but drops to zero on both the HeA_P2G-D1 and HeA_P2G-D2 tasks. From Fig. 2(d), we observe that the mean test winning rate of ROMA increases steadily throughout training, but it fails to converge by the end of the training process. ROMA's slower convergence and lower final performance compared to GoMARL highlight its reduced robustness to information interference. In summary, the HeA_P2G tasks present a significant challenge for all baseline methods, with information interference exacerbating the task difficulty.\nNext, we turn to tasks involving subtask dissimilarity, specifically HeA_M2G and HeA_M3G. In these tasks, the maximum test winning rate for all baselines is zero. The difficulty of these tasks, which involve dissimilar subtasks, exceeds the capabilities of the listed baseline methods.\nFinally, we analyze the performance of each baseline on tasks related to subtasks quantity. In the HeA_D2G task, the maximum test winning rate of six baseline methods is non-zero. QMIX, RODE, and GoMARL achieve high maximum test winning rates, with QMIX even reaching a 100% win rate. In contrast, ROMA, HSA, and DCC show relatively low maximum test winning rates, with DCC's performance reaching a minimum of 18.75%. As shown in Fig. 2(a) and (b), QMIX, GOMARL, and RODE maintain a continuous upward trend in performance throughout training and converge by the end of the process. In contrast, ROMA, HSA, and DCC exhibit increasing performance but fail to converge. It is worth noting that QMIX and GoMARL perform well on the HeA_D2G task, while ROMA, RODE, HSA, and DCC struggle either with convergence speed or final performance. On the more complex tasks, HeA_D3G and HeA_D4G, the maximum test winning rate of all baselines is zero, indicating that as the number of subtasks increases, the task difficulty surpasses the capabilities of the baseline methods.\nIn conclusion, all baseline methods perform poorly across the eight CTC tasks, highlighting the need for improvements in the ability of existing algorithms to solve composite tasks effectively."}, {"title": "3.3. Solvability", "content": "The maximum test winning rates of baseline methods on the HeS, HoA, and HoS tasks across three seeds are presented in Table 3. On the six tasks with lower difficulty (HoS and HoA), most baselines performed well, with QMIX, Go-MARL, RODE, and ROMA achieving a 100% winning rate on all six tasks. However, for the three HeS tasks, which are more difficult than HoA and HoS, the number of subtasks plays a significant role in performance. As the number of subtasks increases, the number of baselines that"}, {"title": "3.4. Stability", "content": "The stability coefficients of various baselines across each task are presented in Table 4, calculated using Eq. 1. The largest stability coefficients are highlighted in bold. From Fig. 3, we observe the corresponding test winning rate curves. Notably, the difference between the upper and lower bounds of these curves is substantial, indicating significant variation in performance across different training seeds and revealing poor stability in the methods. Additionally, we observed that higher stability coefficients were more commonly associated with more difficult tasks than with less difficult ones. Notably, there were no instances of large stability coefficients in the HoS tasks. This suggests that the CTC tasks impose higher stability requirements on the methods, and as task difficulty increases, these stability demands also rise."}, {"title": "4. Conclusion and Discussion", "content": "In this study, we propose the CTC tasks, designed to enhance the generality and applicability of MARL methods in real-world scenarios, there is a need for more complex tasks that demand multi-agent cooperation and the application of DOL. The CTC tasks guarantee that DOL and cooperation are necessary conditions for completing the tasks and incorporate three factors to expand the diversity to cover more realistic situations. We evaluate 10 cooperative MARL methods on CTC tasks. The results indicate that most of these baselines struggle to handle the challenges posed by CTC tasks. We also conduct experiments on simplified variants to validate the solvability of the CTC tasks. The results show that these simplified tasks are more tractable for the baseline methods, confirming the solvability of the CTC tasks. Additionally, we demonstrate the poor stability of these baselines through stability coefficients and fluctuations in test winning rates curves. In summary, the CTC provides a set of solvable, practical tasks that require the DOL and cooperation, with significant potential to advance the development of cooperative MARL methods.\nHowever, the design of CTC tasks has some limitations. Specifically, CTC tasks involve static DOL requirements and do not involve dynamic DOL conditions, which are generally more complex and have a wide range of real-world applications. While CTC tasks do not directly capture dynamic DOL, they can serve as a foundation for extending to dynamic DOL scenarios. Furthermore, the design principles of CTC tasks can offer valuable insights for constructing tasks that incorporate dynamic DOL requirements."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Appendix", "content": null}, {"title": "A.1. Related Work", "content": "In this section, we first demonstrate that the concept of division of labor (DOL) has been widely integrated into cooperative MARL methods. We then discuss the limitations of the testbeds commonly used in current cooperative MARL research.\nCooperative MARL Methods Specifically, we categorize the existing works that incorporate DOL into three main approaches: policy diversity, agent grouping, and hierarchical MARL.\nPolicy diversity refers to the formation of a cooperative paradigm in which multiple agents, sharing a common goal, adopt diverse policies. By enabling agents to utilize distinct policies, policy diversity implicitly introduces the concept of DOL, where each agent specializes in a specific aspect of the overall task. For instance, EOI (Jiang & Lu, 2021) suggests that emerging personalities and cooperative behaviors can naturally induce agents to adopt distinct roles and behaviors. In contrast, CDS (Li et al., 2021) incorporates diversity into shared policy networks and employs regularization based on information theory to maximize the mutual information between an agent's identity and its trajectory. This approach promotes policy diversity while preserving the agents' unique roles within the cooperative framework. DERE (Jiang et al., 2022a) explores the diverse relationships between agents, which can facilitate cooperative policy learning. By introducing prior knowledge to represent these relationships, the method encourages the emergence of specialization, aligning with the concept of multi-agent DOL. Finally, SPD (Jiang et al., 2022b) proposes an unsupervised MARL method that learns diverse coordination policies for agents without the need for extrinsic rewards. It characterizes agent cooperation using a relational graph, where the varying roles and interactions among agents are reflected, effectively manifesting DOL between agents. In summary, these approaches demonstrate how policy diversity can foster specialization and DOL in cooperative MARL, enabling more efficient and coordinated agent behavior.\nGrouping agents involves partitioning them into distinct subgroups based on certain similarities, and this structure can be viewed as a form of DOL, where each subgroup specializes in a specific function or task within the broader system. Several cooperative MARL methods adopt this paradigm, employing different concepts to group agents effectively. For example, SEPS (Christianos et al., 2021) groups agents based on their abilities and goals. This is achieved by encoding each agent into an embedding space based on their observed trajectories and then applying an unsupervised clustering method to these embeddings. GACG (Duan et al., 2024) calculates cooperation needs between agent pairs based on their current observations and captures group dependencies from behavior patterns observed across trajectories. To reinforce group differentiation, it introduces a group distance loss, which increases behavioral differences between groups while minimizing them within groups. QTypeMix (Fu et al., 2024a) utilizes prior knowledge of agent types to group agents. Similarly, THGC (Jiang et al., 2021) groups agents based on similarities in factors such as location, functionality, and health, while maintaining cognitive consistency within groups through knowledge sharing. VAST (Phan et al., 2021) explores value factorization for sub-teams based on the similarity of spatial information or trajectories. ROMA (Wang et al., 2020a) implicitly introduces the concept of roles within MARL, facilitating the sharing of learning among agents with similar responsibilities. By ensuring that agents with similar roles share both similar policies and responsibilities, it enables effective coordination. GoMARL (Zang et al., 2024) further enhances agent cooperation by empowering subgroups as bridges to model connections between smaller sets of agents, thereby improving overall learning efficiency. It also introduces an automatic grouping mechanism-selecting and removing agents-to dynamically create groups and group action values. In summary, these approaches illustrate how grouping agents based on various similarities fosters specialization, enhances coordination, and facilitates the DOL within cooperative MARL systems.\nHierarchical MARL decomposes a complex task into two layers: the top layer assigns subtasks to individual agents, while the bottom layer focuses on the execution of these subtasks by the agents. Each subtask can be viewed as an agent performing a distinct part of the overall task, thereby embodying the concept of DOL. For example, RODE (Wang et al., 2020b) decomposes a multi-agent collaborative task into a set of subtasks, each with a smaller action-observation space. Each subtask is associated with a specific role, and agents within the same role jointly learn a strategy to solve the subtask through shared learning. LDSA (Yang et al., 2022) learns distinct vectors to represent multiple subtasks and assigns subtask-specific policies to agents based on these vectors, enabling local coordination among agents within each subtask. HSD (Yang et al., 2019) focuses on learning distinguishable skills for agents and employs a bi-level policy structure. While it shares a similar objective of fostering local cooperation, it optimizes this objective using a different approach. DCC (Li et al., 2024) treats subtask decomposition as a fixed number of classification tasks, allowing the direct learning of a subtask selection network to guide agent behavior. In summary, hierarchical MARL approaches employ different techniques to decompose complex tasks into subtasks, facilitating specialization and local coordination among agents. This decomposition mirrors the division"}, {"title": "Cooperative MARL Testbeds", "content": "SMAC (Samvelyan et al., 2019b) is a benchmark suite specifically designed for evaluating Multi-Agent Reinforcement Learning (MARL) methods. Based on the real-time strategy game StarCraft II, SMAC provides a variety of tasks and scenarios in which researchers can test and refine their multi-agent systems. In SMAC tasks, each allied unit is controlled by an RL agent, which can observe the distances, relative positions, unit types, and health of all allied and enemy units within its field of view at each time step. The behavior of enemy units is governed by the built-in rules of the environment. To address certain limitations in SMAC, SMACv2 (Ellis et al., 2022) introduces three key modifications: random team compositions, random starting positions, and more realistic field of view and attack range dynamics. These changes encourage agents to focus on understanding their observation space more thoroughly and help prevent the learning of successful open-loop strategies\u2014those that rely solely on the time step for decision-making. Despite these enhancements, SMACv2 and SMAC remain nearly identical in all other aspects. The primary objective for the allied agents in both SMAC and SMACv2 is to eliminate all enemy units within a specified timeframe, with rewards being awarded only when enemy units are eliminated and victory is achieved. Several effective policies have been observed in these environments. For example, a commonly learned strategy is to focus fire (Li et al., 2023; Liu et al., 2024; 2023; Mahajan et al., 2019; Yang et al., 2022; Yu et al., 2023; Hu et al., 2023; Shao et al., 2022; Zang et al., 2024) on a single enemy, thereby quickly reducing the number of adversaries and minimizing the damage taken by the agents. Another effective policy involves retreating when an agent's health is low, causing the enemy to switch targets, followed by a counterattack once the agent is no longer a target for any enemy unit. However, neither of these strategies involves DOL between agents, as they focus on individual actions rather than collaborative, role-specific tasks.\nGRF (Kurach et al., 2020) is a highly dynamic and complex simulation environment that lacks clearly defined behavior abstractions, making it an ideal testbed for studying multi-agent decision-making and cooperation. The environment adheres to standard football (soccer) rules, including corner kicks, fouls, cards, kick-offs, and offside penalties. Additionally, the physical representation of players is highly realistic, allowing for a diverse range of learning behaviors to be explored, with the option to adjust the difficulty level. In GRF, the model must control a team of agents to compete against an opposing team, with the objective of scoring more goals than the opponent by the end of the match. The environment provides 19 possible actions for the agents, including movement, kicking, and other specialized actions such as dribbling, sliding, and sprinting. GRF also offers several predefined reward signals, such as scoring rewards and a penalty box proximity reward, which encourages attackers to move toward specific locations on the field. In GRF tasks, agents must coordinate their movements, timing, and positioning to organize offensive strategies and seize fleeting opportunities, as rewards are only given for scoring. However, in this environment, all agents are homogeneous and capable of performing all tasks. This means that division of labor is not a necessary condition for task completion. For instance, in the academy tasks of GRF, as long as the ball is passed to the agent far away from the defender at the start, the agent dribbling the ball can score alone, and the other agents walking around will not affect the result (Fu et al., 2024b; Li et al., 2021; Xu et al., 2023), where DOL is often not a necessary feature for optimal policies."}, {"title": "A.2. Details of CTC tasks", "content": "It is worth noting that, for the three tasks for information interference, we augment Medivac with additional units, making this task functionally equivalent to a defense task. This modification is necessary because the pursuit task is inherently challenging to train, with early training often resulting in quick failures. These repeated failures lead to the environment instances being restarted frequently, which significantly reduces the overall training efficiency."}, {"title": "3.  Stability Analysis", "content": "We introduce a stability coefficient to quantitatively assess the stability of MARL methods across multiple seeds, revealing their instability across both CTC and simplified tasks."}, {"title": "2.  Comprehensive Evaluation", "content": "We evaluate 10 cooperative MARL methods on the CTC tasks, demonstrating their inability to effectively handle tasks requiring sophisticated DOL and cooperation."}, {"title": "3.  Solvability Validation", "content": "We validate the solvability of the CTC tasks through simplified variants, providing evidence that the tasks are tractable while remaining challenging for current methods."}, {"title": "Eq. 1", "content": "\n        v =  1/M * \u2211 var ([w1, ..., wM])"}]}