{"title": "MedCodER: A Generative AI Assistant for Medical Coding", "authors": ["Krishanu Das Baksi", "Elijah Soba", "John J. Higgins", "Ravi Saini", "Jaden Wood", "Jane Cook", "Jack Scott", "Nirmala Pudota", "Tim Weninger", "Edward Bowen", "Sanmitra Bhattacharya"], "abstract": "Medical coding is essential for standardizing clinical data\nand communication but is often time-consuming and prone\nto errors. Traditional Natural Language Processing (NLP)\nmethods struggle with automating coding due to the large\nlabel space, lengthy text inputs, and the absence of sup-\nporting evidence annotations that justify code selection.\nRecent advancements in Generative Artificial Intelligence\n(AI) offer promising solutions to these challenges. In this\nwork, we introduce MedCodER, a Generative AI framework\nfor automatic medical coding that leverages extraction, re-\ntrieval, and re-ranking techniques as core components. Med-\nCodER achieves a micro-F1 score of 0.60 on International\nClassification of Diseases (ICD) code prediction, signifi-\ncantly outperforming state-of-the-art methods. Additionally,\nwe present a new dataset containing medical records anno-\ntated with disease diagnoses, ICD codes, and supporting ev-\nidence texts (https://doi.org/10.5281/zenodo.13308316). Ab-\nlation tests confirm that MedCodER's performance depends\non the integration of each of its aforementioned components,\nas performance declines when these components are evalu-\nated in isolation.", "sections": [{"title": "Introduction", "content": "The International Classification of Diseases (ICD)\u00b9, devel-\noped by the World Health Organization (WHO)2, is a glob-\nally recognized standard for recording, reporting, and mon-\nitoring diseases. In the United States, the use of ICD codes\nis mandated by the U.S. Department of Health and Human\nServices (HHS) for entities covered by the Health Insurance\nPortability and Accountability Act for insurance purposes.\nICD codes have undergone various revisions over time to\nreflect advancements in medical science\u00b3. The 10th revision,\nknown as ICD-10-CM (referred to as ICD-10 hereafter) in\nthe U.S, is the standard for modern clinical coding and com-\nprises over 70,000 distinct codes. These codes follow a spe-\ncific alphanumeric structure (Hirsch et al. 2016) and are or-\nganized into a hierarchical ontology based on the medical\nconcepts they represent. For example, the code \"150.1\" cor-\nresponds to \"Left ventricular failure, unspecified,\u201d where I50\nrepresents \"Heart Failure,\" and the \".1\" indicates its speci-\nficity. ICD-10 differs significantly from previous versions,\nmaking translation between versions challenging.\nAccurate ICD coding is essential for medical billing,\nhealth resource allocation, and medical research (Campbell\nand Giadresco 2020). This task is performed by specialized\nprofessionals known as medical or clinical coders, who use\na combination of manual techniques and semi-automated\ntools to process large volumes of medical records. Their pri-\nmary responsibility is to accurately assign ICD-10 codes to\nmedical records based on documented diagnoses and pro-\ncedures. The coding process is often time-consuming and\ncostly, with its difficulty varying depending on the complex-\nity of the cases and the level of detail in the documentation.\nErrors in ICD coding can have significant financial and legal\nimplications for patients, healthcare providers, and insurers.\nDespite the critical importance of accurate coding, few reli-\nable solutions exist to supplement or automate this process.\nAutomation of ICD coding is an active research area\nwithin the NLP community. While various approaches have\nbeen proposed, recent methods typically frame this task as\na multi-label classification problem: given a raw medical\nrecord text, the goal is to predict each of the relevant ICD\ncodes (Yan et al. 2022). Although the objective is straight-\nforward, several challenges make automatic ICD coding dif-\nficult. These include the extremely large label space, the di-\nversity and lack of standardization in medical record data,\nand the severely imbalanced distribution of labels. Advanced\ndeep learning techniques have significantly improved the\nperformance of automated ICD coding, but they still fall\nshort of fully automating the process. Furthermore, these\nmethods often lack interpretability, making it difficult to pro-\nvide justifications for the selected ICD codes.\nIn the realm of Generative AI, Large Language Models\n(LLMs) have shown remarkable capabilities in text genera-\ntion and reasoning, particularly in zero-shot scenarios. How-\never, early efforts to apply LLMs for automatic ICD cod-\ning have produced unsatisfactory results (Boyle et al. 2023;\nSoroush et al. 2024). We hypothesize that augmenting the\nintrinsic (parametric) knowledge of LLMs with complemen-\ntary techniques, such as retrieval (Lewis et al. 2020) and re-\nranking (Sun et al. 2023), can significantly improve their ac-\ncuracy in this domain.\nEvaluation, benchmarking, and reproducibility of results"}, {"title": "Related Research", "content": "Automatic ICD Coding\nAutomated ICD coding is a challenging NLP problem, ap-\nproached through rule-based (Kang et al. 2013; Farkas and\nSzarvas 2008), traditional machine learning (Scheurwegs\net al. 2016, 2017), and deep learning methods (Ji et al. 2024).\nRecent methods often treat it as a multi-label classification\ntask, utilizing architectures like convolutional (Mullenbach\net al. 2018; Cao et al. 2020), recurrent (Yu et al. 2019; Guo\net al. 2020), graph neural networks (Wang et al. 2020), and\ntransformers (Huang, Tsai, and Chen 2022). Although gen-\nerative AI and LLMs have been explored for ICD coding\n(Boyle et al. 2023; Soroush et al. 2024), results have been\nmixed.\nAn analysis by Edin et al. 2023 compared SOTA ICD cod-\ning models on MIMIC datasets and found that PLM-ICD\n(Huang, Tsai, and Chen 2022) excelled on MIMIC IV, but\ncommon ICD coding challenges persisted, with more than\nhalf of ICD-10 codes misclassified. This suggests the po-\ntential of zero-shot models like LLMs for more reliable so-\nlutions.\nLLM-based ICD coding research has yielded mixed out-\ncomes. One study achieved only a 34% match rate using a\ndataset from Mount Sinai (Soroush et al. 2024), while an\nLLM-guided tree search method achieved competitive re-\nsults (Boyle et al. 2023), though it lacked transparency in\ncode selection and was resource-intensive.\nDisease Extraction\nDisease extraction, a key component of both traditional\nmedical coding and the MedCodER framework, involves\nidentifying disease entities from medical records and is\na form of Named Entity Recognition (NER) in biomedi-\ncal NLP (Durango, Torres-Silva, and Orozco-Duque 2023).\nWhile often overlooked in ICD coding methods, disease\nNER is crucial for accurate retrieval and re-ranking of ICD\ncodes."}, {"title": "Retrieval and Re-ranking", "content": "Domain-specific models like Bidirectional Encoder Rep-\nresentations from Transformers for Biomedical Text Min-\ning (BioBERT) (Lee et al. 2019), pre-trained on biomedi-\ncal literature, achieve high F1 scores (86-89%) on bench-\nmark datasets but are more effective with data similar to\ntheir training sets. Recent advancements, such as Universal\nNamed Entity Recognition (UniNER), which uses knowl-\nedge distillation from LLMs, and Generalist Model for\nNamed Entity Recognition (GLiNER), a transformer-based\nmodel with fewer than 1 billion parameters, have shown\ncompetitive zero-shot performance on the National Cen-\nter for Biotechnology Information (NCBI) Disease corpus\n(Zhou et al. 2024; Zaratiana et al. 2023).\nUnlike general NER, which may identify a broad range of\ndisease mentions, ICD-10 extraction focuses on diagnosing\ndiseases relevant for coding, reducing noise and minimizing\nerrors in billing and documentation. Our approach targets\nprecise disease extraction aligned with ICD-10 codes.\nRetrieval and Re-ranking\nWhile traditional NLP methods often frame automatic ICD\ncoding as a multi-label classification task, it can also be ap-\nproached as a retrieval and re-ranking problem. In this per-\nspective, the goal is to retrieve the most relevant ICD codes\nfor a given medical record and then re-rank them into a pri-\noritized list. This approach addresses the challenge of deal-\ning with large label spaces by filtering out irrelevant codes,\nresulting in a more manageable set of candidates. The re-\ntrieval and re-ranking strategy is similar to the Retrieval-\nAugmented Generation (RAG) paradigm used in generative\nAI workflows, which further supports the use of LLMs in\nautomatic ICD coding.\nPrior work has explored the retrieval and re-ranking\nparadigm using pre-trained ICD coding models (Tsai,\nHuang, and Chen 2021). In this approach, the top k most\nprobable codes are selected from the pre-trained model and\nre-ranked based on label correlation. However, its effective-\nness is limited by the retriever's ability to produce rele-\nvant codes within the top k. Embedding models have also\nbeen utilized to retrieve relevant codes for a given medical\nrecord (Niu et al. 2023). While promising, this approach is\nlimited by the challenges of long input texts and lacks a clear\nrationale for ICD-10 code selections. In contrast, the Med-\nCodER framework addresses these limitations by extracting\ndisease-related text segments to enhance the retrieval of rel-\nevant ICD-10 codes."}, {"title": "MedCodER Framework", "content": "Here we introduce the MedCodER framework, which is il-\nlustrated in Fig. 1. MedCodER is an interpretable and ex-\nplainable ICD coding framework comprised three compo-\nnents: (1) extraction, (2) retrieval, and (3) re-ranking. In this\nsection, we describe each component and its relevance to\nICD-10 coding."}, {"title": "Step 1: Disease Diagnoses, Supporting Evidence &\nICD-10 Code Extraction", "content": "The first step of MedCodER uses a LLM, specifically GPT-\n44, to extract disease diagnoses, supporting evidence text,\nand associated ICD-10 codes. Disease diagnoses refer to\nthe clinical terms describing a patient's ailment, while sup-\nporting evidence includes additional information about the\ndisease, such as test results and medications. We asked the\nLLM to generate structured JSON output of these entities\nfrom the medical record.\nDrawing inspiration from Chain-of-Thought (CoT)\nprompting (Wei et al. 2022), we asked the LLM to first rea-\nson about relevant text from the medical record before gen-\nerating ICD-10 codes, mimicking the workflow of medical\ncoders. The extracted diagnoses are used in the retrieval step,\nwhile the supporting text and generated ICD-10 codes are\nused in the re-ranking step.\nTo address potential hallucinations, we implemented ad-\nditional verification steps. For diagnoses, we used fuzzy\nmatching to replace extracted terms with their exact or clos-\nest match from the medical record. For supporting evidence,\nwhich tend to be longer phrases compared to diagnoses, we\nsubstituted them with sentences from the medical record that\nhave the highest BM25 similarity score. Error mitigation for\nthe generated ICD-10 codes occurs in the re-ranking step\n(Step 3)."}, {"title": "Step 2: ICD-10 Retrieval Augmentation", "content": "Instead of relying solely on traditional deep learning mod-\nels or the parametric knowledge of LLMs for ICD coding,\nwe augmented the LLM's outputs from the previous step by\ngenerating a candidate set of ICD-10 codes through seman-\ntic search between extracted diagnoses and the descriptions\nof valid ICD-10 codes. This approach mitigates the large la-\nbel space issue by reducing the number of potential codes to\na more manageable set.\nFor the semantic search, we compiled textual descrip-\ntions of valid codes from the ICD-10 ontology and equiv-\nalent descriptions from the Unified Medical Language Sys-\ntem (UMLS) Metathesaurus, providing accurate handling\nof medical synonyms. We then embedded these descriptions\nand tagged each code with metadata related to the ontol-\nogy, such as chapter, block, and category (Boyle et al. 2023).\nDuring inference, disease diagnoses are embedded, and the\ntop k most similar ICD-10 codes based on cosine distance\nare retrieved for each diagnosis. This results in a ranked list\nof ICD-10 codes directly mapped to specific diagnoses, en-\nhancing interpretability."}, {"title": "Step 3: Code-to-Record Re-ranking", "content": "In the final step, the retrieved codes from the Step 2 and\nthose generated by the LLM are re-ranked to produce the"}, {"title": "Experimental Methodology", "content": "Dataset\nBecause current ICD coding benchmark datasets, like\nMIMIC III and IV, have restrictions on use with off-the-\nshelf, externally-hosted LLMs, and because they lack anno-\ntations of supporting evidence text, they cannot be used in\ntypical Generative AI solutions. To address these challenges,\nwe created a new dataset that extends the Ambient Clin-\nical Intelligence Benchmark (ACI-BENCH) dataset (Yim\net al. 2023). ACI-BENCH is a synthetic dataset containing\n207 transcribed conversations that simulate doctor-patient\ninteractions. Clinical notes were generated using an auto-\nmated note-generation system that summarized dialogues,\ntranscribed using human transcription and automatic speech\nrecognition tools. These notes were then reviewed and re-\nvised, as necessary, by medical domain experts to ensure\ntheir accuracy and realism, closely mimicking real-world\nclinical notes. The ACI-BENCH dataset does not incorpo-\nrate linkages to structured data, including vital signs, order\ncodes, and diagnosis codes.\nWe extended the ACI-BENCH dataset by manually an-\nnotating each clinical note with ICD-10 codes, disease di-\nagnoses, and supporting evidence texts, with the help of\nan expert medical coder. Of the 207 clinical notes, three\nwere deemed unworthy of coding. The remaining notes were\ncoded in two batches: the first batch included 184 notes,\n360 ICD-10 codes with diagnoses, and 737 supporting ev-\nidence texts, and is used to evaluate the results of various\nMedCodER components. The second batch, consisting of 20\nnotes, is intended for use in A/B testing the coding applica-\ntion with and without the AI assistant.\nMethodology\nWe evaluate the efficacy of MedCodER's components, fo-\ncusing on disease diagnoses extraction and ICD-10 cod-\ning, using the extended ACI-BENCH dataset and comparing\nthem with SOTA approaches. For disease NER, we compare\nour approach against BioBERT, SciSpacy (Neumann et al.\n2019), UniNER and GLiNER. For ICD-10 code generation,\nwe compare MedCodER against PLM-ICD, and Clinical\nCoder prompt and LLM Tree-Search (Boyle et al. 2023). Be-\ncause most automatic ICD coding baselines produce a single\nICD-10 code per diagnosis, we compare our k@1 results\nagainst these. We also demonstrate performance trade-offs\nwith increasing values of k. For non-LLM baselines, we use\npublicly available pre-trained weights, and for LLM-based\nexperiments, we use GPT-4. Supporting evidence extraction"}, {"title": "Results", "content": "In this section, we present the results of both the baselines\nand our proposed framework.\nDisease Diagnoses and Supporting Evidence\nExtraction\nThe results of disease diagnoses extraction are shown in Ta-\nble 1. We find that MedCodER's disease diagnoses extrac-\ntion for ICD-10 coding outperforms other disease NER sys-\ntems, validating our hypothesis that prompting for specific\nICD-10 diagnoses is better for this task. Because disease ex-\ntraction is central to the ICD coding framework, these results\nset an upper bound on ICD coding performance. Notably,\ngeneralist models such as GLINER outperformed domain-\nspecific models, likely due to differences between general\ndisease NER and ICD-10 diagnosis extraction objectives."}, {"title": "ICD-10 Coding", "content": "Table 2 presents MedCodER results when filtering for only\nthe top ranked ICD-10 code per diagnosis. For baselines,\nwe used the pre-trained weights of PLM-ICD on MIMIC IV\nfrom Edin et al. (2023) and a 50-call limit for the LLM Tree-\nSearch. These methods represent the SOTA deep learning\n(Edin et al. 2023) and generative AI based solutions (Boyle\net al. 2023) for automatic ICD-10 coding. MedCodER out-\nperforms these baselines, significantly enhancing ICD-10\ncoding performance while remaining interpretable."}, {"title": "Ablation Results", "content": "To evaluate the efficacy of retrieval and re-ranking on ICD\ncoding performance, we conducted an ablation study. The\nresults are shown in Fig. 2. The variations of MedCodER\nused in the study are:\n\u2022 MedCodER-Prompt: Uses only the ICD-10 codes from\nprompting. Note that a single ICD code is generated per\ndiagnosis by prompting and hence the performance of\nthis variant does not change with value of k.\n\u2022 MedCodER-Retrieve: Uses only the retrieved ICD-\n10 codes, without re-ranking."}, {"title": "Error Analysis", "content": "We conducted an error analysis to highlight MedCodER's\nlimitations and suggest future research directions.\nTable 3 presents failure cases for each component of our\nframework (k=1). We only show cases where the extracted\ndisease diagnosis matched the ground truth to highlight er-\nrors in prompting and retrieval approaches for ICD-10 cod-\ning. We observed that that even when the codes are incor-\nrect, they are often very close. Additionally, MedCodER can\novercome prompting and retrieval shortcomings due to its\nre-ranking capability."}, {"title": "UI Design and Deployment", "content": "Unlike fully automated ICD coding solutions, MedCodER is\nan AI-assisted coding tool to enhance medical coding work-\nflows. To illustrate this, we designed a preliminary but func-\ntional UI, which is illustrated in Fig. 3. For each predicted\ndiagnosis, a button in the UI is available to highlight the\ncorresponding text spans containing disease mentions and\nsupporting evidence texts (captured through the Extraction\ncomponent of our framework). Additionally, a dropdown\nmenu displays MedCodER's top five most relevant ICD-10\ncodes per diagnosis. Coders can review the highlighted texts\nand select a code from the dropdown or input a different\ncode or a list of codes as comma-separated values. The ap-\nplication is currently hosted on internal servers and is in the\nbeta-phase. We plan to train coders on usage of the tool and\nfurther stress test the application before deploying it to the\nproduction environment. For future research, we aim to use\nthe second batch of our annotated dataset (comprising 20\nrecords) along with real-world medical records to conduct\nA/B testing of the coding application with and without the\nAI assistant, assessing gains in efficacy and efficiency."}, {"title": "Discussion", "content": "The primary objective of MedCodER is to leverage SOTA\nLLMs and generative AI techniques for automated medical\ncoding. We focused on GPT-4 in our experiments, however,\nour framework is not limited to GPT-4 and can work with\nLLMs such as such as Anthropic's Claude models or Mistral\nAI's Mistral models. Future work will explore other LLMs,\nincluding those specific to the biomedical domain, as Med-\nCodER relies on the LLM's knowledge of diseases, support-\ning evidence, and ICD-10 codes. We anticipate that LLMs\ntrained in the biomedical domain will enhance performance.\nMedCodER assumes medical records are text, but they are\noften PDFs requiring conversion, which can be challenging\nwith handwritten sections, tables, and other structured data.\nAdditionally, the fixed context length of LLMs may neces-\nsitate extra pre-processing steps for longer records.\nPrivacy is crucial when using closed-source LLM APIs\nas medical records contain confidential information. Users\nmust review the terms and conditions of such LLMs. The\nuse of LLMs in healthcare is rapidly evolving, requiring at-\ntention to scientific and legal developments."}, {"title": "Conclusions", "content": "In this paper, we introduce MedCodER, a framework that\nsurpasses existing SOTA medical coding frameworks while\nmaintaining interpretability and a dataset to aid the develop-\nment of interpretable ICD coding methodologies. We ana-\nlyzed each component individually and demonstrated that\nSOTA performance is achieved by combining all compo-\nnents. Additionally, our error analysis highlighted areas for\nfurther improvement. Finally, we show how the MedCodER\nframework can be integrated into an AI-based assistant for\nmedical coders, enhancing their efficiency and accuracy."}]}