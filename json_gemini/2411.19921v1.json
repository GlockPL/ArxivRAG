{"title": "SIMS: Simulating Human-Scene Interactions with Real World Script Planning", "authors": ["Wenjia Wang", "Liang Pan", "Zhiyang Dou", "Zhouyingcheng Liao", "Yuke Lou", "Lei Yang", "Jingbo Wang", "Taku Komura"], "abstract": "Simulating long-term human-scene interaction is a challenging yet fascinating task. Previous works have not effectively addressed the generation of long-term human scene interactions with detailed narratives for physics-based animation. This paper introduces a novel framework for the planning and controlling of long-horizon physical plausible human-scene interaction. On the one hand, films and shows with stylish human locomotions or interactions with scenes are abundantly available on the internet, providing a rich source of data for script planning. On the other hand, Large Language Models (LLMs) can understand and generate logical storylines. This motivates us to marry the two by using an LLM-based pipeline to extract scripts from videos, and then employ LLMs to imitate and create new scripts, capturing complex, time-series human behaviors and interactions with environments. By leveraging this, we utilize a dual-aware policy that achieves both language comprehension and scene understanding to guide character motions within contextual and spatial constraints. To facilitate training and evaluation, we contribute a comprehensive planning dataset containing diverse motion sequences extracted from real-world videos and expand them with large language models. We also collect and re-annotate motion clips from existing kinematic datasets to enable our policy learn diverse skills. Extensive experiments demonstrate the effectiveness of our framework in versatile task execution and its generalization ability to various scenarios, showing remarkably enhanced performance compared with existing methods. Our code and data will be publicly available soon.", "sections": [{"title": "1. Introduction", "content": "Creating virtual characters with diverse motor skills such as walking, sitting, reaching, and enabling rich interactions with their environments in daily living scenarios, is crucial for robotics and VR/AR applications. Especially, depicting humans in daily indoor actions is central to animation and 3D games, requiring long-term, physically plausible, and controllable interactions with diverse styles and details to bring characters and stories to life.\nPrevious works [12, 27, 33, 36, 44] have thoroughly explored long-term motion generation for human-scene interactions in kinematics representation. However, these models still suffer from physical artifacts such as surface penetration and foot skating. To address these issues, recent studies [13, 19, 39] have started incorporating physics simulators [17] to produce more physically plausible motions. Despite these advancements, the frameworks are still re-"}, {"title": "2. Related Works", "content": null}, {"title": "2.1. Kinematic-based Human Scene Interaction", "content": "Synthesizing realistic human behavior has been a long-standing challenge. While most methods enhance the quality and diversity of humanoid movements [14, 30\u201332, 42, 45], they often overlook scene interactions. Recently, there's been growing interest in integrating human-scene interactions, crucial for applications like embodied AI and virtual reality. Many previous approaches [12, 15, 27, 33, 36, 37, 41, 43, 44] rely on data-driven kinematic models [7, 10, 29, 34, 35] for static or dynamic interactions. However, these often lack physical plausibility, resulting in artifacts like penetration, floating, and sliding, and require additional post-processing, limiting real-time use."}, {"title": "2.2. Physics-based Human-Scene Interaction", "content": "While previous physics-based animation approaches mainly focused on human motion alone [5, 6, 21, 22, 24]. InterPhys [13] presents a framework extending AMP to include character and object dynamics, using a scene-conditioned discriminator for superior performance compared to previous methods. Additionally, InterScene [19] effectively synthesizes physically plausible long-term human motions in complex 3D scenes by decomposing interactions into Interacting and Navigating processes. This method uses reusable controllers trained in simple environments to generalize across diverse scenarios. With the development of LLMs, UniHSI [39] introduces a unified framework for human-object interaction via language commands, featuring an LLM Planner and Unified Controller, which reduces training labor with LLM-generated plans. The effectiveness of this approach is evaluated using the ScenePlan dataset. In the realm of humanoid robotics, HumanVLA [40] advances the field with a vision-language-action model that leverages egocentric vision and natural language. Furthermore, PlaMo [11] combines scene-aware path planning and motion control in physical simulations, efficiently planning long-horizon paths for humanoids and ensuring navigation across uneven terrains."}, {"title": "2.3. Language Conditioned Humanoid Control", "content": "The HumanVLA[40] model enables physical humanoids to interpret visual and language inputs to perform tasks. It employs behavior cloning to replicate advanced learning techniques and is enhanced by a novel active rendering method for improved perception. Trained in the IsaacGym [17] with the Human-in-the-Room (HITR) dataset, this model demonstrates effective object rearrangement capabilities in diverse environments. UniHSI[39] is a novel unified Human-System Interaction framework that translates language commands into interactive tasks using a large language model planner and a unified controller. This system effectively executes detailed task plans without extensive interaction annotations, demonstrating robust control and adaptability in various scenarios as tested on the ScenePlan dataset. However, their chain-of-contact design could not model the diverse motions with emotional details. [28] introduces a method for controlling humanoid robots using a large language model (LLM) refined with a CLIP text encoder and codebook-based vector quantization. The approach ensures precise, actionable instructions are delivered to robots, simplifying control with a single policy network that uses specialized rewards for accurate movement. This innovation enhances the system's ability to handle diverse and unseen instructions, streamlining robotic task execution. PADL[16] uses natural language commands to control characters, mapping high-level tasks to low-level actions through adversarial imitation learning, effectively guiding simulated humanoids in complex skills."}, {"title": "3. Method", "content": "We propose a hierarchical framework utilizing LLMs as high-level script planners, and deep reinforcement learning-based control policies as low-level character controllers. In Sec. 3.1, we introduce the fundamentals of reinforcement learning. Sec. 3.2 details our method for extracting short scripts from online video clips and generating new ones. We then use LLMs to retrieve and integrate these into longer scripts. In Sec. 3.3, we explain in each skill, how humanoids are controlled using scene-aware observations and language-aware embeddings. Finally, Sec. 3.4 describes our dual-aware finite state machine, which drives humanoids by multiple skill policies, conditioned on time-series contact goals, environment states, and text conditions."}, {"title": "3.1. Preliminary", "content": "Our characters are trained using a goal-conditioned reinforcement learning framework, where an agent interacts with an environment according to a control policy to achieve a specified goal $g \u2208 G$, drawn from a goal distribution $g \\sim p(g)$. At each time step $t$, the agent observes the environment's state $s_t \u2208 S$ and responds by taking an action $a_t \u2208 A$, sampled from the policy $a_t \\sim \\pi(a_t|s_t, g)$. After executing the action $a_t$, the environment transitions to a new state $s_{t+1}$, and the agent receives a scalar reward $r_t = r(s_t, a_t, s_{t+1}, g)$ that reflects the desirability of the state transition for the given goal $g$. The agent's objective is to learn a policy \u03c0 that maximizes its expected discounted return $J(\u03c0)$,\n$J(\\pi) = E_{p(g)} E_{p(\\tau|\\pi, g)} [ \\sum_{t=0}^{T-1} \\gamma^t r_t ]$ (1)\n$p(\\tau|\\pi, g) = p(s_0) \\prod_{t=0}^{T-1} P(s_{t+1}|s_t, a_t) \\pi(a_t|s_t, g)$. (2)\nwhere $p(\\tau|\\pi,g)$ denotes the likelihood of a trajectory $\\tau = (s_0, a_0, s_1,..., s_T)$ under a policy \u03c0 given a goal g, $p(s_0)$ is the initial state distribution, and $p(s_{t + 1}| s_t, a_t)$ represents the transition dynamics of the environment. $T$ is the time horizon of a trajectory, and $\\gamma \u2208 [0, 1]$ is a discount factor."}, {"title": "3.2. Script Planning", "content": "Creating long-term scripts with LLMs is challenging due to potential redundancy and lack of guidance. Previous works, like [39], focus on generating limited keyframes with no diverse styles. We discovered that real-world films and TV shows contain fascinating scripts featuring characters navigating daily life stories in various moods (See Fig. 3). To leverage these rich resources, we use an LLM-based framework that extracts character emotions, object types, and contact details from online videos, capturing diverse interactions and summarizing story outlines. Using available human motions and furniture types, we prompt the LLM to generate short scripts that imitate human-made masterpieces. Each script comprises keyframes defined by specific emotions, interactions, and summaries, forming a diverse database. The LLM retrieves from this database based on user-provided themes, considering the summaries, and arranges them chronologically. This approach crafts a long story from multiple short scripts, each with distinct interactions or styles. The keyframes from retrieved short scripts formulate as the long-horizontal story. Each keyframe produces a set of outputs ${c, g, o}$, representing the caption, goal, and object, which are subsequently used in the policy. The policy drives the humanoid using ${z, g, h_t}$, where $z$ denotes the text embedding derived from $c$, $g$ represents the object contact goal, and $h_t$ is the heightmap computed based on the surrounding objects, please see Sec. 3.3 for detail. We then retrieve a room layout from the database that could satisfy the interaction types. Finally, the LLM synthesizes these summaries with the user's prompt to create a cohesive narrative."}, {"title": "3.3. Dual-Aware Control Policy", "content": "Once we have well-planned time-series keyframes and a 3D scene, we need to use them to guide the humanoid to perform the long-term interactions. We next need to train control policies that enable a physically simulated character to perform various high-level tasks in 3D scenes specified by language commands. At each timestep $t$, the policy $\\pi(a_t| s_t, h_t, g, z)$ receives as input the state of the character $s_t$, a egocentric heightmap $h_t$, a task-specific goal $g$, and a language embedding $z$. The goal $g$ specifies high-level task objectives that the character should achieve, such as moving to a target location or contacting a certain object. The $h_t$ is the egocentric heightmap near the character, representing the surrounding environment. The language embedding $z$ specifies the style that the character should use to achieve the desired goal, such as walking excitedly or sitting with legs crossed. In order to train a policy to perform a given task using a desired skill, we utilize a reward function consisting of two components: $r_t = r_t^{skill} + A_{task} r_t^{task}$, where $r_t^{skill}$ is a skill-reward, and $r_t^{task}$ is a task-reward with coefficient $A_{task}$.\nLanguage-Aware Embedding To control the policy language constraints, we aim to construct an embedding space fed into the policy network, where the embedding aligns motion representation with their corresponding natural language descriptions. To do this, we follow [16, 30], where a transformer auto-encoder is trained to encode motion sequences into a latent representation that aligns with the language embedding from a pre-trained CLIP text encoder [25]. Given a motion clip $m = (q_1, ..., q_n)$, a motion encoder $z = Enc_m(m)$ maps the motion to an embedding $z$. The embedding is normalized to lie on a unit sphere $||z|| = 1$. We set the embedding size $z$ to 64 to save the computation cost. For the text embedding, we first extract the feature with CLIP Encoder [25] $Enc_l$ from caption $c$, then use a multilayer perception $MLP_d$ to downsize the 512 dim CLIP feature to 64 dim and use an extra one $MLP_u$ to upsample it to 512 dim to maintain the semantic feature. The embedding $z$ should be aligned with the downsized CLIP feature. Following [30], $Enc_m (m)$ is modeled by a bidirectional transformer [4]. The motion decoder is jointly trained with the encoder to produce a reconstruction sequence $m = (\\hat{q_1},..., \\hat{q_n})$ to recover $m$ from $z$. The motion representation $q$ we use is a set of character motion features, following the discriminator observation used in AMP [22]. The auto-encoder is trained with the loss:\n$L_{AE} = L_{recon}^m + L_{align}^{cm,t} + L_{recon}^t$ (3)\nThe reconstruction loss $L_{recon}^m$ measures the MSE error between the reconstructed sequence and original motion. The alignment loss $L_{align}^{cm,t}$ measures the cosine distance between"}, {"title": "3.4. Dual-Aware Finite State Machine", "content": "As illustrated in Fig 2, our framework integrates several reusable policies, serving as low-level controllers. We have trained five policies: the Walk policy $\\pi_w$, Sit policy $\\pi_s$, Lie policy $\\pi_l$, Reach policy $\\pi_r$, and GetUp policy $\\pi_g$. Based on these control policies, our FSM provides users with five reusable skills: Sit $k_s$, Lie $k_l$, GetUp $k_g$, Reach $k_r$, and Walk $k_w$. To synthesize human motions described by the script of: \"A man walks excitedly with arms opened($z_w$) to stool, then he touches the stool to get a rest. He then walks to the sofa tiptoeing carefully and lies on the sofa with legs crossed($z_l$). He stands up from the sofa and walks to the dining table with large footsteps and reaches for the beer on the dining table. Then he walks to the corner table drukenly and sits on it with head bowed and hands on thighs($z_s$). He then gets up and walks drunkenly($z_w$) towards the TV stand.\" Our finite state machine will parse this task into the sequence of skills, goals and conditions:\n$I = {(k_w, h, g_w, z_w), (k_r, h, g_r), \\newline (k_w, h, g_w, z_w), (k_l, h, g_l, z_l), (k_g, h, g_g), \\newline (k_w, h, g_w, z_w), (k_r, h, g_r), \\newline (k_w, h, g_w, z_w), (k_s, h, g_s, z_s), (k_g, h, g_g) \\newline (k_w, h, g_w, z_w)},$ (6)\nwhere $(k_s, h, g_s, z_s)$ denotes that the character performs the sitting skill $k_s$ conditioned on the given egocentric heightmap $h$, contact goal $g_s$, and text embedding $z$. The FSM translates these instructions into a sequence of explicit control signals and schedules control policies to execute the instructions without additional training. Following [19], the"}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Dataset", "content": "Following [13, 19, 39], we use the SAMP [12] dataset to train our interaction policies. We annotate all the motion clips with text descriptions, containing detailed motions that relate to style and emotions. We also provide each motion clip with suitable interact objects, e.g., some of the sitting motions require an armrest or backrest, which means the stools are not suitable. We use 100style[18] to train our stylish locomotions. We select 30 representative categories and annotate each motion with style descriptions. We also provide each caption with 5 synonymous sentences with the help of LLM [1]. Following [39], we use the CIRCLE [2] dataset to train the reaching skill. Besides neutral, we categorize the emotion or style of the rest motions into 8 categories: happy, angry, hurried, tired, sad, stressed, drunk, and relaxed. We left-right flip all the motions so we get double the amount. For training the motion encoder, we use all of the above motions with our annotated captions.\nFor 3d scenes, we use the 80 objects from PartNet, half for training and half for testing following [39]. We use Scannet [3] and 3DFront [8] for testing and demonstration. We choose the largest 10 scenes in each room category (living room, bedroom, dinning room, library) from [8]. Since 3DFront do not provide segmentation information, we voxelize the object meshes and segment the point clouds to get the affordance surface."}, {"title": "4.2. Policy Training", "content": "We train the following 5 skills (1) Walk. (2) Sit. (3) Lie. (4) Reach. (5) GetUp. We only provide Walk, Sit, and Lie skills with text conditions since they contain diverse interaction styles that represent vivid emotions. For Reach and GetUp, we do not use text conditions.\n\u2022 Initialization. Following UniHSI [39], we create the environment by randomly sampling objects from the training set of PartNet used in [39]. For the character, we initialize it using reference state initialization [20] and default pose initialization with a random global rotation and location[19, 39] nearby the object. For the Walk skill, we randomly sampled on the whole ground plane while calculating the collision with the objects.\n\u2022 Reward. For Walk skill, we use the reward follow [13, 19, 23]. For contact skills as Reach, Sit, Lie, and GetUp, we follow [39]. See the detailed reward function in Supp.Mat.\n\u2022 Reset and early termination conditions. Following [22], we use a fixed episode length and fall detection as early termination triggers. We also use early termination when the task is accomplished for a certain time [19] or the contact forces are extremely large [39].\n\u2022 Training the get-up policy. We do not use text conditions for the GetUp skill since it is a short transition between interactions and walking and rarely contains diverse styles. Here we follow the multi-step task training setting following [39], where we train Sit and GetUp policy together, and change the contact goal and reward when the last task has been accomplished."}, {"title": "4.3. Metrics", "content": "We follow [12, 39] that uses Success Rate and Contact Error as the main metrics to measure the quality of interactions quantitatively. Success Rate records the percentage of trials that humanoids successfully complete the contact within a certain threshold. We follow [13, 19, 39] to set the threshold of Sit as 20cm, Reach as 20cm, and Lie as 30cm.\nTo evaluate motion diversity, we use two metrics from the previous papers: Fr\u00e9chet Inception Distance (FID) [5, 31] and Average Pairwise Distance (APD) [5, 33]. FID measures the similarity between the distributions of generated and real data in a feature space, reflecting the realism and quality of the generated motions. Lower FID values indicate closer alignment with real data. APD, on the other hand, quantifies the diversity within the generated motions by calculating the average pairwise distance between samples. Higher APD values indicate greater diversity in the generated motions. We calculate FID and APD on joint rotations and positions."}, {"title": "4.4. Comparison with Baselines", "content": "Our method achieves comparable results across various metrics in Sec. 4.4. Unlike previous physics-based methods [13, 19, 39] which only care about contact but not styles, our result is achieved on 4096 random text conditions sampled from the datasets. The previous methods could be viewed as just a specific situation of our model. Under this background, we can see from Sec. 4.4 that our results are only slightly lower than the best methods InterScene [19] and UniHSI [39], comparable to InterPhys [13]."}, {"title": "4.5. Different Planning results", "content": "We evaluate the performance of our policy on a set of generated short scripts. Following [39], we create scripts with varying levels of difficulty. Specifically, for simple scripts, we prompt the LLM to generate scenarios containing 2 in-"}, {"title": "4.6. Motion Diversity for Different Skills", "content": "We compared motion diversity in the Sit and Lie skills with UniHSI [39]. All experiments were conducted on a single RTX 4090 GPU, running 1024 sequences and aggregating the results over 10 trials. For each sequence, the text condition is randomly sampled from the dataset. We measure the FID between the generated motions and that of reference motions from SAMP [12]. The APD measures the diversity among the generated motion sequences. As shown in Tab. 3, our results significantly outperform UniHSI in both FID and APD metrics. Our method achieves lower FID, indicating motions produced from ours are closer to the distribution of reference motions. Notably, the APD results highlight that the motions generated by UniHSI are nearly identical, demonstrating a lack of diversity."}, {"title": "4.7. User Study results on ScanNet", "content": "To further evaluate the control capabilities of the long-term scripts, we conducted a user study on the ScanNet dataset."}, {"title": "5. Conclusion", "content": "In this paper, we present a framework that synthesizes long-term human-scene interactions by using Large Language Models as a planner and a dual-aware control policy as a controller. By integrating real-world video data and leveraging Vision-Language Models, our approach enables diverse and expressive long-term script generation. We also employ graph edit distance to retrieve scene lay-"}, {"title": "6. Limitations and Future Work", "content": "Our framework's main limitation is the insufficient amount of stylish interaction motion data, which limits our model's generalization ability. In the future, it is necessary to collect more human motion data that expresses realistic emotions and styles. It is also needed to develop better ways to structure short scripts. This would provide more temporal coherence and align with real-life logic."}]}