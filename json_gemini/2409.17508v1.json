{"title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE", "authors": ["Xun Zhu", "Ying Hu", "Fanbin Mo", "Miao Li", "Ji Wu"], "abstract": "Multi-modal large language models (MLLMs) have shown impressive capabilities\nas a general-purpose interface for various visual and linguistic tasks. However,\nbuilding a unified MLLM for multi-task learning in the medical field remains a\nthorny challenge. To mitigate the tug-of-war problem of multi-modal multi-task\noptimization, recent advances primarily focus on improving the LLM components,\nwhile neglecting the connector that bridges the gap between modalities. In this\npaper, we introduce Uni-Med, a novel medical generalist foundation model which\nconsists of a universal visual feature extraction module, a connector mixture-of-\nexperts (CMOE) module, and an LLM. Benefiting from the proposed CMoE that\nleverages a well-designed router with a mixture of projection experts at the connec-\ntor, Uni-Med achieves efficient solution to the tug-of-war problem and can perform\nsix different medical tasks including question answering, visual question answer-\ning, report generation, referring expression comprehension, referring expression\ngeneration and image classification. To the best of our knowledge, Uni-Med is the\nfirst effort to tackle multi-task interference at the connector. Extensive ablation ex-\nperiments validate the effectiveness of introducing CMoE under any configuration,\nwith up to an average 8% performance gains. We further provide interpretation\nanalysis of the tug-of-war problem from the perspective of gradient optimization\nand parameter statistics. Compared to previous state-of-the-art medical MLLMs,\nUni-Med achieves competitive or superior evaluation metrics on diverse tasks.\nCode, data and model will be soon available at GitHub.", "sections": [{"title": "1 Introduction", "content": "Driven by the growth of datasets, the increase in model size, and advances in generative language\nfoundation models [Achiam et al., 2023; Touvron et al., 2023], multi-modal large language models\n(MLLMs) now offer unprecedented abilities as general-purpose interfaces. These advancements are\nspurring innovation across various visual and linguistic tasks [Chen et al., 2023a; Lyu et al., 2023;\nSu et al., 2023]. While significant strides have been made in building a unified foundation model\nfor natural scenery [Chen et al., 2022; Lu et al., 2022, 2023], the development of generalist medical\nartificial intelligence is still in its early stages [Moor et al., 2023a].\n\nThe goal of a unified and generalist medical foundation model is to enable joint training on massive\nmedical datasets. This model aims to handle multiple tasks and modalities within a single architecture\nwith shared parameters [Zhang et al., 2023; Li et al., 2024]. It seeks to eliminate the need for\ntask-specific modules and further fine-tuning, thereby revolutionizing the traditional task-specific\napproach to model development [Wu et al., 2023; Tu et al., 2024]. However, existing open-source\nefforts have not yet fully achieved these ambitious goals."}, {"title": "2 Related work", "content": "The increasing availability of medical data, as well as advances in\nmulti-modal LLM technologies, have paved the way for the emergence of medical foundational\nmodels. Med-Flamingo [Moor et al., 2023b] continues pre-training on paired and interleaved medical\nimage-text data based on OpenFlamingo [Awadalla et al., 2023]. LLaVA-Med [Li et al., 2024]\ncurates a medical multi-modal instruction following dataset and fine-tunes LLaVA [Liu et al., 2024]\nwith it. XrayGPT [Thawkar et al., 2023] can analyze and answer open-ended questions about chest\nX-rays. BiomedGPT [Zhang et al., 2023] is a multi-task foundation model pretrained on a diverse\nsource of medical images, literature, and clinical notes. However, most of these efforts require further\nfine-tuning on task-specific data to support downstream applications. One step further, the generalist\nfoundation model uses the same weight to excel at various tasks without fine-tuning. RadFM [Wu et\nal., 2023] is dedicated to build a generalist foundation model for radiology. Med-PaLM M [Tu et al.,\n2024] is directly trained in a unified framework to jointly handle many tasks, which is perhaps most\nsimilar to our effort. Nevertheless, it does not provide access for usage.\n\nMoE is originally considered to increase the model capacity [Riquelme\net al., 2021; Fedus et al., 2022] and gains popularity in mitigating multi-task interference [Chen et\nal., 2023e, 2024]. It achieves this by utilizing a router to determine the token set handled by each\nexpert, thus reducing interference between different types of samples. Recent studies have focused\non combining MoE with LLM, such as MoE-LLaVA [Lin et al., 2024] and Mixtral 8x7B [Jiang et\nal., 2024], or combining MoE with one of the representative parameter-efficient tuning techniques,\ni.e., LoRA [Hu et al., 2021], such as Octavius [Chen et al., 2023d], MoCLE [Gou et al., 2023], and\nMTLORA [Agiza et al., 2024]. However, neither of them introduces MoE into the connector and\nexplicitly provides interpretable analysis of how the multi-task interference is mitigated.\n\nThe connector between the multi-modal encoder and the\nLLM is critical in aligning multi-modal features. One of the most popular paradigms is to map\nmulti-modal features into a feature space that aligns with language, such as linear projection [Liu\net al., 2024] and MLP projection [Liu et al., 2023a; Chen et al., 2023c]. Another paradigm is\nto transform multi-modal features into multi-modal tokens that are consistent with the embedded\nrepresentation space of LLM, such as cross-attention [Li et al., 2022; Ye et al., 2023b], perceiver\nresampler [Alayrac et al., 2022; Peng et al., 2023] and Q-Former [Li et al., 2023; Zhu et al., 2023].\nHowever, existing paradigms use the same connector when processing the same modal data for\ndifferent tasks, ignoring the necessity to learn different alignment patterns for different tasks."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Preliminaries", "content": ""}, {"title": "3.1.1 Multi-task interference", "content": "To quantify the intricate tug-of-war problem in a unified foundation model, we provide interpretability\nfrom the perspective of gradient optimization and parameter statistics."}, {"title": "Perspective of gradient optimization", "content": "When optimizing the shared parameters @ according to task\nj, the change in the update direction of loss Li for task i can be defined as [Zhu et al., 2022]:\n\n$\\AjLi (Xi) = Exj (Li (Xi; 0) - Li Xi; 0 \u2013 \u03bb\u00b7\\frac{\\nablaoLj (xj)}{||VoLj (xj) ||^2}) $\n\n$\\approx \u0395x_j (Li (Xi) \\frac{\\nabla_\\theta L_i (X_i) \\cdot \\nabla_\\theta L_j (X_j)}{||\\nabla_\\theta L_j (X_j)||^2})$\n\n(1)\nwhere xi and xj are the sampled training batches of task i and j, respectively. The interference of\ntask j on task i in the update direction can be quantified as:\n\n$GD_{ij} = Ex(\\frac{\\Delta_j L_i (X_i)}{\\Delta_i L_i (X_i)})$                                                                                                   \n(2)\nThe gradient magnitude similarity between task i and task j can be defined as:\n\n$GM_{i,j} = GM_{j,i} = \\frac{2Ex_i(||\\nabla_\\theta L_i (X_i)||^2) Ex_j(||\\nabla_\\theta L_j (x_j) ||^2)}{(Ex_i (||\\nabla_\\theta L_i (X_i)||^2))^2 + (Ex_j (||\\nabla_\\theta L_j (x_j) ||^2))^2}$          \n(3)"}, {"title": "3.1.2 Mixture-of-Experts", "content": "A Mixture-of-Experts (MoE) contains a set of expert networks E1, E2, ..., EN along with a routing\nnetwork R. For each token xi in the input sequence X = {x}_1, the output of MoE is the weighted\nsum of outputs from each expert, where the weight is calculated by the router:\n\n$Yi = \\sum_{k=1}^N R(x_i)_k \\cdot E_k(x_i)$ \n\n(6)\nThe types of R can mainly be divided into: 1) Constant router, which assigns equal weight to each\nexpert. 2) Hard router, which enforces one-to-one mapping between tasks and experts. 3) Sparse\nrouter, which selects Top-K experts with the maximum routing weight. 4) Soft router, which calculates\nthe routing weights for each expert. For more details on the routing networks, see Appendix A.1."}, {"title": "3.2 Model Architecture", "content": "With the primary goal of achieving a unified medical generalist foundation model and mitigating the\ntug-of-war problem of multi-task learning in mind, we design the overall architecture of Uni-Med as\nillustrated in Figure 3, which contains three components: a universal vision feature extraction module,\na connector-MoE module and an LLM. Detailed descriptions are presented in the following sections."}, {"title": "3.2.1 Visual feature extraction module", "content": "Taking one of the multi-modal medical images I \u2208 RH\u00d7W\u00d7C as input, the visual encoder Ven\nextracts the image tokens fr \u2208 RNv\u00d7Dv for image perception, where \u00d1 = HW/P2 is the number\nof image patches and D is the hidden size of visual embeddings.\n\nTo alleviate the efficiency issues caused by prolonged visual input tokens during the training and\ninference, we scheme a resampler with a compression rate a for visual feature aggregation. Concretely,\na adjacent visual tokens are concatenated and projected into one single embedding. Thus we obtain\naggregated image tokens fag \u2208 RNv/a\u00d7Dva as follows:\n\n$f_{ag} = resampler (V_{en} (I), a)$                                                                                                      \n(7)"}, {"title": "3.2.2 Connector-MoE module", "content": "Aligning the visual space with the language embedding space of the large language model is a critical\nprocess, especially in the complex and diverse input of multi-task multi-modal medical image text\npairs. Based on the conflict-synergy coexist hypothesis, we propose the Connector-MoE (CMOE)\nmodule, which aims to adaptively minimize task conflict and maximize task synergy at the connector.\nCMOE module has N projection experts E1, E2, ..., EN, where each expert is a two-layer MLP, and\na soft router Rsoft to control the contribution of each expert.\n\nAccording to Figure 2, we find that: (1) Gradient optimization conflict is common and consistent at\nthe task level. (2) Even for the same task, there are significant differences in conflict and synergy\nat dataset-level. To alleviate the above problems, we randomly initialize vision-level special task\ntokens {ftp}t\u2208T, where ftp \u2208 RDva and T is the set of tasks. Rsoft is a lightweight MLP designed\nto receive the concatenated inputs of fag (token level) and for (task level), and calculate the routing\nweights wsoft \u2208 RDv/a\u00d7N of each expert for each image token, which can be formulated as:\n\n$W_{soft} (f_{ag}) = \u03c3 \\cdot R_{soft} ([f_{ag}, Repeat (f_{tp})])$                                                                                                                  \n(8)\nwhere [,] denotes concatenation operation, o is SoftMax function. Then we can obtain aligned visual\ntokens falign \u2208 RNv/a\u00d7Dt through a weighted sum of all experts' output as follows:\n\n$f_{align} = \\sum_{k=1}^N w_{soft, k} \\cdot E_k(f_{ag})$                                                                                                                                               \n(9)\nwhere Dt is the hidden size of the language embedding space of the large language model and wsoft,k\ndenotes the routing weight of the k-th projection expert. We discuss and analyze the effects of router\ntype, router strategy, and number of experts in Section 4.2.1."}, {"title": "3.2.3 Large language model", "content": "Similar to the vision-level special task tokens, we assign the text-level special task identifiers for\nquestion answering (QA), visual question answering (VQA), report generation (RG), referring\nexpression comprehension (REC), referring expression generation (REG) and image classification\n(CLS) as shown in Table 1, which can help reduce multi-task ambiguity [Chen et al., 2023b]. The\ntext prompt is designed as \"<Img> <ImageFeature> </Img> [Task Identifier] Instruction\", which\nmerges the converted image features with the textual instructions. See details about our multi-task\ninstruction template in Appendix C.\n\nAfter word embedding, we can obtain textual tokens ft \u2208 RNt\u00d7Dt, where Nt denotes the number of\ntextual tokens. LLM generates the response O = {0}=1 conditioned on the aligned visual tokens\nfalign and textual tokens ft inputs in an autoregressive manner, which can be formulated as:\n\n$P (Ot | falign, ft) = \\prod_{i=1}^L P (Oi | falign, ft, O<i)$                                                                                                                                           \n(10)\nwhere L is the length of output tokens. We use low-rank adaption (LoRA) [Hu et al., 2021] for\nefficient LLM fine-tuning, which is applied to all the linear layers."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment settings", "content": "Text-only data is collected from MedQA [Jin et al., 2021] and PubMedQA\n[Jin et al., 2019] for the task of QA. Image-text pairs are collected from Path-VQA [He et al., 2020]\nand Slake-VQA [Liu et al., 2021] for the task of VQA, MIMIC-CXR [Johnson et al., 2019] and\nMPx-Single [Wu et al., 2023] for the task of RG, MedMNIST v2 [Yang et al., 2023] for the task of\nCLS. For tasks such as REG and REC that require representation of spatial locations, we use the\nbounding boxes of the format \"\", which denotes the coordinates of\nobjects. Then, we respectively process datasets Slake-VQA [Liu et al., 2021] and SA-Med2D-20M\n[Ye et al., 2023a] to get datasets Slake-REC, Slake-REG, SA-Med2D-REC, and SA-Med2D-REG.\nFor a detailed description, processing and splitting of all datasets, see Appendix B.\n\nWe adapt the open-sourced ViT-G/14 from EVA-CLIP [Fang et al., 2023]\nand LLaMA2-Chat (7B) [Touvron et al., 2023] as our visual backbone and LLM, respectively.\nDuring the training process, each task is assigned a sample rate that is calculated in proportion to the\nrespective task's data volume. The visual backbone remains frozen with an input image resolution\nof 224*224 and the LLM is fine-tuned through LoRA [Hu et al., 2021] with the rank of 8. The\ncompression rate a=4 and the number of projection experts N=5. Uni-Med only requires one-stage\ntraining on a NVIDIA A800-SXM4-80GB GPU, with the first 10k iterations to warm-up and a total\nof 100k iterations with a batch size of 4, which lasts roughly 10 hours. The peak learning rate is set\nto le-6 and it decays to le-7 following the cosine strategy. We use AdamW [Loshchilov and Hutter,\n2017] optimizer with \u03b2\u2081=0.9, \u03b22=0.95 and weight decay of 0.05."}, {"title": "4.2 Ablation study", "content": ""}, {"title": "4.2.1 Ablation on module design", "content": "Taking the connector of a two-layer MLP as baseline setup, we first discuss the\nperformance of different multi-task learning hypothesis. Specifically, connectors based on conflict-\nsynergy coexist hypothesis (CMoE with sparse / soft router) significantly outperform connectors based\non the conflict hypothesis (CMoE with hard router) and synergy hypothesis (linear, MLP, CMoE with\nconstant router). In Table 2 (a), soft router achieves the best in overall multi-task performance, while\nhard router has a obvious lead on CLS, implying that CLS is better suited to a separate connector\nto avoid conflicts with other tasks. We then discuss three types of router strategy. The strategy of\ncombining token-level with task-level information is superior to using each information separately,\nindicating the effectiveness for considering the tug-of-war problem from both token and task level.\n\nWe explore whether aggregating visual features through resampler has unfavor-\nable effects in Table 2 (b). Despite an increase in compression rate a from 1 to 4, the performance\nof models utilizing projection aggregation is improved. While the performance of average pooling\nand max pooling approaches is not satisfactory, which may be attributed to the excessive loss of\nfeature information. This phenomenon shows that appropriate visual feature compression can bring\nefficiency to the training process without losing or even improving performance."}, {"title": "4.2.2 Ablation on module generalization", "content": "We demonstrate the generalization capability of the CMoE module in any configuration, especially\nwhen the key hyperparameters and strategies for LLM fine-tuning change. We first focus on the rank\nof LoRA, which directly determines the LLM capacity, i.e., trainable parameters. Our observations\nin Table 2 (d) reveal that CMoE with soft router can steadily improve multi-task performance when\nLORA rank increases from 4 to 64. In Table 2 (e), we introduce MoE to LoRA, namely LoRA-MOE,\nwhich is considered a favorable parameter-efficient tuning solution for multi-task applications [Liu et\nal., 2023b; Chen et al., 2024]. We find that separate LoRA-MoE results in significant performance\nimprovement in 3 tasks while degradation in 2 tasks, indicating that it does not achieve an optimal\nsolution to the tug-of-war problem. After combining CMoE with the soft router at connector, we\nachieve a balance of performance gains and the efficient solution to the tug-of-war problem. See\ndetails of LORA-MoE at Appendix A.2."}, {"title": "4.3 Interpretation", "content": "We conduct interpretation analysis of the tug-of-war problem based on methods mentioned in\nSection 3.1.1. Specifically, we focus on the changes in the connector using CMoE compared to\nMLP and show how the tug-of-war problem is optimized: (1) From the perspective of gradient\noptimization, we use maximum normalization to make the tug-of-war indexes comparable under\ndifferent architectures. CMoE results in a more consistent tug-of-war indexes among different tasks\nor datasets, implying each individual gets a more balanced optimization, as shown in Figure 4 (a).\n(2) From the perspective of parameter statistics, we discrete the statistics scores into ten intervals\nand count the ratio of all parameters at connector by interval. CMoE results in an increase in the\nproportion of high-value intervals in Figure 4 (b). We show the routing weights of projection experts"}, {"title": "4.4 Overall comparison", "content": "To demonstrate the capabilities of our Uni-Med on multi-task learning, four open source and state-of-\nthe-art medical MLLMs including Med-Flamingo [Moor et al., 2023b], RadFM [Wu et al., 2023],\nLLaVA-Med [Li et al., 2024], and XrayGPT [Thawkar et al., 2023] are used for performance\ncomparison in Table 3. Despite facing individual dataset-level fine-tuned models and data leakage,\nwhich are common problems in medical MLLMs' comparison, we use readily available model\ncheckpoints for testing, following the prompt template requirements of different models. The results\nshow that our Uni-Med, a unified model without fine-tuning on any individual dataset, achieves\nleading and competitive evaluation metrics across all tasks. Although inferior to RadFM on the\nMPx-Single dataset, we have identified the cause to be data leakage, see Appendix D.2. Since the\nabove MLLMs do not support input and output in coordinate form, we report the performance of\nUni-Med on REC and REG tasks at Appendix D.3."}, {"title": "5 Conclusion", "content": "In this paper, we present a novel open-source medical generalist foundation model Uni-Med, which\ncan handle six different medical tasks. Benefiting from the proposed CMoE, which combines MoE\nwith the connector, Uni-Med achieves efficient solution to the tug-of-war problem in multi-task\nlearning. Uni-Med not only achieves competitive or superior performance compared to the open-\nsource state-of-the-art medical MLLMs, but also provides interpretability analysis from multiple\nperspectives on how the tug-of-war problem is optimized. We hope Uni-Med can greatly promote the\ndevelopment of medical generalist foundation models and inspire more research toward generalist\nmedical artificial intelligence. We will release corresponding data, codes, and models soon."}, {"title": "A Component design", "content": ""}, {"title": "A.1 Type of the routing network", "content": "The simplest routing network is to assign equal weights to the output of each\nexpert, which can be expressed as:\n\n$R_{constant}(Xi) = {1/N}_{i=1}^N$                                                                                                                                                 \n(11)\nEach token is assigned to a specific expert based on its type (task / modal), with the\nnumber of experts being equal to the number of token types. It can be formulated as:\n\n$R_{hard} (xi) = { I_{sType} (xi, k)}_{k=1}^N$                                                                                                                                       \nIsType (xi, k) =\n\n$(1, if xi belongs to type k,$\n$0, otherwise)$\n\n(12)\nUsing a small network g, the sparse router computes a score vector for each token,\nwith a length equal to the number of experts N. Subsequently, the Top-K function retains the top-K\nvalues in the vector, while setting all other values to zero. Finally, the Softmax function is applied to\nobtain the final routing vector. The whole process is shown as follows:\n\n$R_{sparse} (xi) = Softmax (Top-K (g (xi), K))$\n\nTop-\u039a (\u03c5, \u039a) =\n\n$(0, if u is in the top K,$\n$v, otherwise)$\n\n(13)\nSimilar to the sparse router, the soft router computes a score vector for each token\nthrough a small network g. Subsequently, it applies the Sigmoid function to the score vector and\nnormalizes it, yielding the final routing vector. It can be formulated as:\n\n$R_{soft}(xi) = \\frac{Sigmoid(g(x))}{Sum(Sigmoid(g(x))}$                                                                                                                           \n(14)"}, {"title": "A.2 LORA-MOE", "content": "LORA-MoE freezes the original parameters of the model to preserve world knowledge and introduces\nLORA experts to learn new knowledge, thereby improving performance across multiple downstream\ntasks with few parameters.\n\nSpecifically, given a frozen linear layer with a weight matrix Wo \u2208 Rdin\u00d7dout, LORA-MoE creates\nN low-rank trainable matrix pairs Ak and Bk, where Ak \u2208 Rdin\u00d7r, Bk \u2208 Rr\u00d7dout, and the rank\nr < min(din, dout). As in the case of LoRA, Ak is initialized with a random Gaussian distribution,\nand Bk is initialized to zero. During training, the parameters of Wo are frozen, and the parameters of\nAk and Bk are updated. The forward process of a LoRA-MoE layer can be represented as:\n\n$h= \\sum_{k=1}^N Woxi + \\Delta Wxi = Woxi + \\sum_{k=1}^N R(xi) Ak BkXi$\n\n(15)\nwhere xi is the input token, R is the router in the LoRA-MoE layer, a is the learning rate scaling\nfactor, and h is the output token. In ablation experiments, we transform each linear layer in the LLM\ninto a LoRA-MoE layer with a sparse router. The rank r = 4, the learning rate scaling factor a = 8,\nthe number of LoRA experts N = 5, and select the top 2 experts."}, {"title": "B Dataset", "content": ""}, {"title": "B.1 Data source", "content": "MedQA [Jin et al., 2021] is a open-domain multiple-choice question answering dataset\nfor solving medical problems. These questions are sourced from professional medical board exams,\nwhich feature diverse content and typically demand a comprehensive understanding of related medical\nconcepts learned from medical textbooks in order to provide accurate answers. This dataset covers\nthree languages: English, simplified Chinese, among which there are 12,723 QA pairs for English."}, {"title": "B.2 Well-crafted datasets for REC and REG tasks", "content": "As a semantically-labeled knowledge-enhanced dataset for medical\nvisual question answering, Slake-VQA provides bounding boxes for each object in the image. As\nshown in Figure 6 (a), the original format of each bounding box is [X, Y, W, H]. First, we convert it\nto the [Xmin, Ymin, Xmax, Ymax] format. Assuming the relative size of each image is 100\u00d7100, we\nthen normalize each coordinate value in the bounding box to fall within the range of 0 to 100.\n\nAs shown in Figure 6 (c), in the REC task, an image and object name are given to find the object's\nbounding box. In the REG task, an image and object bounding box are provided to identify the\nobject's name. The Slake-REC and Slake-REG datasets are thus created.\n\nEach image in the SA-Med2D-20M dataset has one or more\nmasks, with each mask corresponding to an object. As shown in Figure 6 (b), we calculate the\nbounding box for each mask and normalize it to a range of 0 to 100, resulting in a bounding box for\neach object in the [Xmin, Ymin, Xmax, Ymax] format.\n\nThe SA-Med2D-REC and SA-Med2D-REG datasets are organized as depicted in Figure 6 (c). 10,000\nsamples each are selected from the CT and MR subsets as the training set, and 2,000 samples each\nare selected as the test set."}, {"title": "C Multi-task instruction template", "content": "We have designed different instruction templates for different datasets. During the training process,\nwhen a sample from a dataset is selected, an instruction template is also sampled from the corre-\nsponding dataset's template pool and used to format the sample. Examples of instruction templates\nfor each dataset are shown below."}, {"title": "D Experiments", "content": ""}, {"title": "D.1 Evaluation metrics", "content": "Assuming m is the number of common words in the candidate C and the reference R with\nthe number of words of c and r, the precision and recall for a candidate sentence can be calculated\nas:\n\n$precision = \\frac{m}{C}$                                                              \n(16)\n\n$recall = \\frac{m}{r}$                                                              \n(17)\n\nConsidering class imbalance, F1 score is used to evaluate the performance of the model on both the\nVQA and REG tasks, which means the harmonic mean of precision and recall. A higher average F1\nscore for the dataset indicates a higher performance of the model.\n\n$F1 = \\frac{2 \\times precision \\times recall}{precision + recall}$                                                              \n(18)\nWe use BLEU-1 to assess the model's performance on both the VQA and REG tasks,\nwhile employing both BLEU-1 and BLEU-4 to evaluate its performance in the report generation task.\nGiven the candidate C and reference R, BLEU-N is defined as:\n\n$BLEU-N = \\frac{\\sum_{gram \\in C} Count_{clip} (gram)}{\\sum_{gram \\in C} Count (gram)}$                                                                                                     \n(19)\nWhen N=1, the above formula calculates BLEU-1; when N=4, it calculates BLEU-4.\n\nWe use ROUGE-1 and ROUGE-2 to evaluate the performance of the model on the RG\ntask. Given the candidate C and reference R, ROUGE-N is defined as:\n\n$ROUGE-N = \\frac{\\sum_{gram \\in R} Count_{match} (gram)}{\\sum_{gram \\in R} Count (gram)}$                                                                                                 \n(20)\nWhen N=1, the above formula calculates ROUGE-1; when N=2, it calculates ROUGE-2.\n\nROUGE-L is also used to evlaute the quality of the generated text on the task of\nreport generation, which stands for recall-oriented understudy for gisting evaluation with the longest\ncommon subsequence. Given the candidate C and reference R, let LCS(C, R) be the length of the\nlongest common subsequence, which is determined by using dynamic programming, it can be an\ndefined as:\n\n$ROUGE-L = \\frac{(1 + \\beta^2) R_{LCS} P_{LCS}}{R_{LCS} + \\beta^2 P_{LCS}}$                                                             \n(21)\nwhere RLCS = \\frac{LCS(C, R)}{LC}, PLCS = \\frac{LCS(C, R)}{LR}, \u03b2 = \\frac{PLCS}{RLCS}. LC and LR represent the length of the\ncandidate and reference. A higher ROUGE-L score means that the generated text shares more of the\nsame sequences of words as the reference text, which typically indicates better quality in terms of\ncapturing the salient points of the reference."}, {"title": "D.2 Data leakage issue in RadFM", "content": "We evaluated RadFM on the test set of MPx-Single and found that the model outputs for many\nsamples were completely consistent with ground truth. This appears to be unreasonable, raising\nsuspicions of potential data leakage. Here are some examples:"}, {"title": "D.3 Additional results", "content": "We have designed vision-level special task tokens and\ntext-level special task identifiers for visual features and text prompt, respectively. Through ablation\nexperiment, we verify whether they have a positive effect on model performance. As shown in\nTable 5, we observe that text-level special task identifiers bring limited improvement. In contrast,\nvision-level special task tokens significantly improve the model's overall performance on all datasets,\nfurther illustrating the effectiveness of mitigating the tug-of-war problem at the connector."}, {"title": "E Limitations", "content": "While Uni-Med has demonstrated strong potential as a unified and generalist medical foundation\nmodel, Uni-Med still exhibits several limitations:\n\n(1) Imbalance in domain diversity. Although Uni-Med includes medical images of 10 modes in\nthe training data, it is dominated by radiology images, while other modalities are relatively under\nrepresented. (2) The potential for performance enhancement through increasing multi-modal, multi-\ntask data has not yet been explored. In all experiments in this work, we use 12 datasets of 6 medical\ntasks during the training process, with a total data volume of 140k. (3) The potential for performance\nenhancement through LLM scaling has not yet been explored. In this work, we use LLaMA2-7B as\nour LLM backbone. (4) The tug-of-war of multi-task learning is an extremely complicated problem.\nWe attempt to combine the existing methods to analyze it from the perspective gradient optimization\nand parameter statistics. How to get the theoretical proof of the optimal solution remains to be\nexplored. (5) Data leakage and unfair comparison issues. Because of the differences in training data,\ntest data, and evaluation metrics, we find it difficult to compare all models fairly. (6) Negative societal\nimpacts. With the vision to promote the development of medical generalist foundation models, we\nwill release our data, code and models soon. But we cannot prevent potential malicious or unintended\nuses, such as generating fake profiles or wrong medical diagnoses and provide necessary safeguards."}]}