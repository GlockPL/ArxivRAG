{"title": "FuseFL: One-Shot Federated Learning through the Lens of Causality with Progressive Model Fusion", "authors": ["Zhenheng Tang", "Yonggang Zhang", "Peijie Dong", "Yiu-ming Cheung", "Amelie Chi Zhou", "Bo Hant", "Xiaowen Chu"], "abstract": "One-shot Federated Learning (OFL) significantly reduces communication costs\nin FL by aggregating trained models only once. However, the performance of\nadvanced OFL methods is far behind the normal FL. In this work, we provide\na causal view to find that this performance drop of OFL methods comes from\nthe isolation problem, which means that locally isolatedly trained models in OFL\nmay easily fit to spurious correlations due to data heterogeneity. From the causal\nperspective, we observe that the spurious fitting can be alleviated by augmenting\nintermediate features from other clients. Built upon our observation, we pro-\npose a novel learning approach to endow OFL with superb performance and low\ncommunication and storage costs, termed as FuseFL. Specifically, FuseFL decom-\nposes neural networks into several blocks and progressively trains and fuses each\nblock following a bottom-up manner for feature augmentation, introducing no\nadditional communication costs. Comprehensive experiments demonstrate that\nFuseFL outperforms existing OFL and ensemble FL by a significant margin. We\nconduct comprehensive experiments to show that FuseFL supports high scalability\nof clients, heterogeneous model training, and low memory costs. Our work is the\nfirst attempt using causality to analyze and alleviate data heterogeneity of OFL2.", "sections": [{"title": "1 Introduction", "content": "Federated learning (FL) [95; 67] has become a popular paradigm that enables collaborative model\ntraining without sharing private datasets from clients. Two typical characteristics of FL limit its\nperformance: (1) FL normally has non-IID (Independently and Identically Distributed) data, also\ncalled data heterogeneity [67], which causes unstable slow convergence [68; 145; 123; 134] and\npoor model performance [156; 93; 133; 161; 149]; (2) The extremely low bandwidth, e.g. 1 ~ 10\nMB/s of FL in Internet environments [67; 132; 128; 127; 131; 128], leads to high communication\ntime of a large neural network. For example, communicating once ResNet-50 [51] with 25.56M\nparameters (102.24MB) or GPT-3 [14] with 175B parameters (700GB) will consume around 102.24\nseconds or 194 hours, respectively. Current FL methods alleviate this problem by skipping the\ngradient synchronization of traditional distributed training to save communication costs [95; 67]. But\nthe required hundreds or thousands of communication rounds still make the communication time\nunacceptable.\nTo reduce the communication costs at extreme, one-shot FL (OFL) [158; 38; 81; 163; 25; 23] only\ncommunicates the local trained model once. Thus, the communication cost is the model size S\nfor each client, less than FedAvg-style algorithms for hundreds or thousands of times. However,\naveraging for only once cannot guarantee the convergence of FedAvg. Thus, the direct idea is to\naggregate client models on the server and conduct inference as ensemble learning does. Some\nadvanced works also consider better model averaging [63; 111; 89; 6], neurons matching [5; 140],\nselective ensemble learning [26; 52; 142], model distillation [81; 163; 25; 23]. These methods may be\nimpractical due to the requirements of additional datasets with privacy concerns, and the extra large\nstorage or computation costs. Most importantly, there still exists a large performance gap between\nOFL and the normal FL or the ensemble learning. This motivates the following question:\nHow to improve FL performance under extremely low communication costs with\nalmost no extra computational and storage costs?\nIn this work, we provide a causal view [108; 109; 3; 118] to analyze the performance drop of OFL.\nWe firstly construct a causal graph to model the data generation process in FL, where the spurious\nfeatures build up the data heterogeneity between clients, and invariant features of the same class\nremain constant in each client (domain) [3; 118; 22; 150; 160]. Then, we show the performance drop\ncomes from the isolation problem, which means that locally isolatedly trained models in OFL may\neasily fit to spurious correlations like adversarial shortcuts [40; 35; 53], instead of learning invariant\nfeatures [3; 118], causing a performance drop of OFL on the test dataset. Consider a real-world\nexample, Alice takes photos of birds in the forests, while Bob near the sea. Now, the isolated models\nwill mistakenly identify birds according to the forests or the sea [53; 35]. Based on the causal\ngraph, we intuitively and empirically show that such spurious fitting can be alleviated by augmenting\nintermediate features from other clients (Section 3).\nBuilt upon this observation, we propose a simple yet effective learning approach to realizing OFL\nwith superb performance and extremely low communication and storage costs, termed as FuseFL,\nwhich builds up the global model through bottom-up training and fusion to improve OFL performance\n(Section 4). Specifically, we split the whole model into multiple blocks 3 (The \u201cblock\" means a single\nor some continuous layers in a DNN.). For each block, clients first train and share their local blocks\nwith others; then, these trained local blocks are assembled together, and the features outputted from\nthese blocks are fused together and fed into the next local blocks. This process is repeated until the\nwhole model is trained. Through this bottom-up training-and-fusion method, local models can learn\nbetter feature extractors that learn more invariant features from other clients to avoid the isolation\nproblem. To avoid the large storage cost, given the number of clients M, we assign each local client\nwith a small model with reduced hidden dimension with ratio \\sqrt{M}, to ensure the final learned global\nmodel has the same size S as the original model.\nOur main contributions can be summarized as follows:\n\u2022 We provide a causal view to understand the gap between multi-round FL and OFL, showing that\naugmenting intermediate features from other clients contributes helps improve OFL. As far as we\nknow, this is the first work using causality to analyze the data heterogeneity of OFL.\n\u2022 To leverage causality to improve OFL, we design FuseFL, which decomposes models into several\nmodules and transmits one module for feature augmentation at each communication round.\n\u2022 We conduct comprehensive experiments to show how FuseFL significantly promotes the perfor-\nmance of OFL without no additional communication and computation cost.\""}, {"title": "2 Preliminary", "content": "2.1 Federated Learning\nIn FL, a set of clients M = {m|m \u2208 1, 2, ..., M} have their own dataset Dm. Given C classes\nindexed by [C], a sample in Dm is denoted by (x, y) \u2208 X \u00d7 [C], where x is the input in the space X\nand y is its corresponding label. These clients cooperatively learn a model F(\u03b8, x) : X \u2192 \\mathbb{R}^C that is"}, {"title": "3 Federated Learning: A Causal View", "content": "3.1 The Sequential Structure of Neural Networks\nA neural network can be decomposed into the\nsequential module-wise structure as shown in\nFigure 1. Formally", "as": "nF = \\Lambda \\circ H^K \\circ H^{K-1} \\circ \\dots \\circ H^1, \\text{ for } 1 \\le k \\le K,  (1)\nwhere \\Lambda is the final classifier, and Hk is the\nmodule that may consist of single or multiple\nblocks. The \\Lambda and each Hk are parameterized by\n\u03b8_\\Lambda \u2208 \\mathbb{R}^{d_\\Lambda} and \u03b8^k \u2208 \\mathbb{R}^{d_k}. The H^i \\circ H^j (\\cdot) means\nHj(Hi(\\cdot)). Thus, the parameter \u03b8 of F are con-\ncatenated by the \u03b8_\\Lambda and {\u03b8^k|k \u2208 1,2,... K},\nand d_\\Lambda + \\sum_{k=1}^K d_k = d.\nAs Figure 1 illustrates, each module Hk receives\nthe output of module Hk\u22121, and the final classi-\nfier receives the output of the final hidden mod-\nule HK and makes predictions \u0177 = f(x) on\nthe input x. We call the output from each mod-\nule, hk = Hk(hk\u22121) and h1 = H1(x), as the\nfeature for simplicity.\n3.2 Structure Equation Model of FL\nInspired from the analysis of out-of-distribution\nmodel (SEM) in causality [109; 3; 118; 160", "xleftarrow[": {"xrightarrow[": {"150": "."}}}]}