{"title": "Gradient-free variational learning with conditional mixture networks", "authors": ["Conor Heins", "Hao Wu", "Dimitrije Markovic", "Alexander Tschantz", "Jeff Beck", "Christopher Buckley"], "abstract": "Balancing computational efficiency with robust predictive performance is crucial in supervised learning, especially for critical applications. Standard deep learning models, while accurate and scalable, often lack probabilistic features like calibrated predictions and uncertainty quantification. Bayesian methods address these issues but can be computationally expensive as model and data complexity increase. Previous work shows that fast variational methods can reduce the compute requirements of Bayesian methods by eliminating the need for gradient computation or sampling, but are often limited to simple models. We demonstrate that conditional mixture networks (CMNs), a probabilistic variant of the mixture-of-experts (MoE) model, are suitable for fast, gradient-free inference and can solve complex classification tasks. CMNs employ linear experts and a softmax gating network. By exploiting conditional conjugacy and P\u00f3lya-Gamma augmentation, we furnish Gaussian likelihoods for the weights of both the linear experts and the gating network. This enables efficient variational updates using coordinate ascent variational inference (CAVI), avoiding traditional gradient-based optimization. We validate this approach by training two-layer CMNs on standard bench-", "sections": [{"title": "1 Introduction", "content": "Modern machine learning methods attempt to learn functions of complex data (e.g., images, audio, text) to predict information associated with that data, such as discrete labels in the case of classification [Bernardo et al., 2007]. Deep neural networks (DNNs) have demonstrated success in this domain, owing to their universal function approximation properties [Park and Sandberg, 1991] and the soft regularization inherited from stochastic gradient descent learning via back propagation[Amari, 1993]. However, despite its computational efficiency, accuracy, and scalability to increasingly large datasets and models, DNNs trained this way do not provide well calibrated predictions and uncertainty estimates, and practitioners typically utilize post-hoc calibration methods on validation datasets [Wang et al., 2021, Shao et al., 2020]. This limits the applicability and reliability of using DNNs in safety-critical applications like autonomous driving, medicine, and disaster response [Papamarkou et al., 2024], where uncertainty-sensitive decision-making is required.\nBayesian machine learning addresses the issues of poor calibration and uncertainty quantification by offering a probabilistic framework that casts learning model parameters 0 as a process of inference namely, calculating a posterior distribution over model parameters, given observed data (D = ((x1,y1),..., (xn, yn))), using Bayes' rule:\n$$p (\\theta | D) = \\frac{p (\\theta,D)}{p (0)}$$\n(1)\nThe resulting posterior distribution captures both expectations about model parameters and their uncertainty. The uncertainty is then incorporated in predictions that are, in principle, well-calibrated to novel datapoints coming from the same set. This probabilistic treatment allows methods like Bayesian neural networks (BNNs) [Hern\u00e1ndez-Lobato and Adams, 2015] to maintain the expressiveness of deep neural networks while also encoding uncertainty over network weights and thus the network's predictions. However, these methods are known to come with a significant increase in computational cost and thus scale poorly when applied to large datasets and high-dimensional models [Izmailov et al., 2021].\nIn this paper we introduce a gradient-free variational learning algorithm for a probabilistic variant of a two-layer, feedforward neural network the conditional mixture network or CMN and measure its performance on supervised learning benchmarks. This method rests on coordinate ascent variational inference (CAVI) [Wainwright et al., 2008, Hoffman et al., 2013] and hence we name it CAVI-CMN. We compare CAVI-CMN to maximum likelihood estimation and two other Bayesian estimation techniques: the No U-Turn Sampler (NUTS) variant of Hamiltonian Monte Carlo [Hoffman et al., 2014] and black-box variational inference [Ranganath et al., 2014]. We demonstrate that CAVI-CMN maintains the predictive accuracy and scalability of an architecture-matched feedforward neural network fit with maximum likelihood estimation (i.e., gradient descent via backpropagation), while maintaining full distributions over network parameters and generating calibrated predictions, as measured in relationship to state-of-the-art Bayesian methods like NUTS and BBVI.\nWe summarize the contributions of this work below:"}, {"title": "2 Related work", "content": "The Mixture-of-Experts (MoE) architecture is a close relative of the CMN model we introduce here. Jacobs et al. [1991] originally introduced MoEs as a way to improve the performance of neural networks by combining the strengths of multiple specialized models [Gormley and Fr\u00fchwirth-Schnatter, 2019]. MoE models process inputs by averaging the predictions of individual learners or experts, where each expert's output is weighted by a different mixing coefficient before the averaging. The fundamental idea behind MoE is that the input space can be partitioned in such a way that different experts (models) can be trained to excel in different regions of this space, with a gating network determining the appropriate expert (or combination of experts) for each input. This leads to composable (and sometimes interpretable) latent descriptions of arbitrary input-output relationships [Eigen et al., 2013], further bolstered by the MoE's capacity for universal function approximation [Nguyen et al., 2016, Nguyen and Chamroukhi, 2018]. Indeed, the powerful self-attention mechanism employed by transformers has has demonstrated the power and flexibility of MoE models [Movellan and Gabbur, 2020]. Non-Bayesian approaches to MoE typically rely on maximum likelihood estimation (MLE) [Jacobs et al., 1991, Jordan and Jacobs, 1994], which can suffer from overfitting and poor generalization due to the lack of regularization mechanisms [Bishop and Svenskn, 2003], especially in low data size regimes.\nTo address these issues, Bayesian approaches to MoE have been developed, which incorporate prior information and yield posterior distributions over model parameters [Bishop and Svenskn, 2003, Mossavat and Amft, 2011]. This Bayesian treatment enables the estimation of model evidence (log marginal-likelihood) and provides a natural framework for model comparison and selection [Svens\u00e9n, 2003, Zens, 2019]. Bayesian MoE models offer significant advantages, such as improved robustness against overfitting and a better understanding of uncertainty in predictions. However, they also introduce computational challenges, particularly when dealing with high-dimensional data and complex model structures.\nThe introduction of the P\u00f3lya-Gamma (PG) augmentation technique in Polson et al. [2013] enabled a range of novel and more computationally efficient algorithms for Bayesian treatment of MoE models [Linderman et al., 2015, He et al., 2019, Sharma et al., 2019, Viroli and McLachlan, 2019, Zens et al., 2023]. Here we complement these past works, which mostly rest on improving sampling methods with PG augmentation, by introducing a closed-form update rules for MoE's with linear experts in the form of coordinate ascent variational inference (CAVI)."}, {"title": "3 Methods", "content": "In this section we first motivate the use of conditional mixture models for supervised learning, and then introduce the conditional mixture network (CMN), the probabilistic model whose properties and capabilities we demonstrate in the remainder of the paper."}, {"title": "3.1 Conditional mixtures for function approximation", "content": "Feedforward neural networks are highly expressive, approximating nonlinear functions through sequences of nonlinear transformations, but the posterior distributions over their weights are intractable, requiring expensive techniques like MCMC or variational inference [MacKay, 1992, Blundell et al., 2015, Daxberger et al., 2021].\nWe circumvent these problems by focusing on the Mixture-of-Experts (MoE) models [Jacobs et al., 1991], and particularly a variant of MoE that is amenable to gradient free CAVI parameter updates. MoEs can be made tractable to gradient-free CAVI when the expert likelihoods are constrained to be members of the exponential family (see Section 2 for more details on the MoE architecture), and when the gating network is formulated in such a way to allow exact Bayesian inference (through lower bounds on the log-sigmoid likelihood [Jaakkola and Jordan, 1997, Bishop and Svenskn, 2003] or P\u00f3lya-Gamma augmentation [Polson et al., 2013]).\nThe MoE can be reformulated probabilistically as a mixture model by introducing a latent assignment variable, zn, leading to a joint probability distribution of the form\n$$p(\u03a5, \u0396, \u0398) = p(\\theta_{1:K})p(\\pi) \\prod_{i=1}^{n}p(y_{n}|z^n, \\theta_{1:K})p(z^n|\\pi),$$\nwhere yn is an observation, \u0398 = {\u03b81:K,\u03c0}, Pk(yn|\u03b8k) is the kth-component's likelihood and zn is a discrete latent variable that assigns the nth datapoint to one of the K mixture components, i.e. Pk(yn|\u03b8k) = p(yn|zn = k, \u03b81:K). For instance, if each \u2018expert' likelihood pk(yn|\u03b8k) is a Gaussian distribution, then the MoE becomes a Gaussian Mixture Model, where \u03b8k = (\u03bc\u03ba, \u03a3\u03ba).\nThe problem of learning the model's parameters, then becomes one of doing inference over the latent variables Z and parameters of the mixture model. However, mixture models are generally not tractable for exact Bayesian inference, so some form of approximation or sampling-based scheme is required to obtain full posteriors over their parameters. However, if each expert (i.e., likelihood distribution) in the MoE belongs to the exponential family, the model becomes conditionally conjugate. This allows for derivation of exact fixed-point updates to an approximate posterior over each expert's parameters. The approach we propose, CAVI-CMN, does exactly this we take advantage of the conditional conjugacy of mixture models, along with an augmentation trick for the the gating network, to make all parameters amenable to an approximate Bayesian treatment. The conditionally-conjugate form of the model allows us to use coordinate ascent variational inference to obtain posteriors over the weights of both the individual linear experts and the gating network [Wainwright et al., 2008, Hoffman et al., 2013, Blei et al., 2017], without resorting to costly gradient or sampling computations.\nGoing forward we use the term conditional mixture networks (CMN) to emphasize (1) the discriminative nature of proposed application of this approach, where the model is designed to predict an output y given an input x and (2) the fact that individual MoE layers can be stacked hierarchically into a feedforward architecture. This makes CMNs particularly suitable for tasks such as supervised classification and regression, where the goal is effectively that of function approximation; predict some output variable y given input regressors x."}, {"title": "3.2 Conditional mixture network overview", "content": "The conditional mixture network maps from a continuous input vector xo \u2208 Rd to its label y \u2208 {1, . . ., L}. This is achieved with two layers: a conditional mixture of linear experts, which outputs a joint continuous-discrete latent (x1 \u2208 Rh, z\u2081 \u2208 {1, . . ., K}) and a multinomial logistic regression, which maps from the continuous latent x\u2081 to the corresponding label y. The probabilistic mapping can be described in terms of the following operations:\n$$z_1 \\sim Mult (z_1; X_0, \\beta_0)$$\n$$x_1 = A_{z_1} [x_0; 1] + u_{z_1}, \\qquad u_{z_1} \\sim N(0, \\Sigma_{z_1})$$\n$$y \\sim Mult (y; x_1, \\beta_1)$$\nwhere we pad the input variable xo with a constant value set to 1, to absorb the bias term within the mapping matrix $A_{z_1} \\in [R^{h \\times d+1}]$, and where $Mult (z;x, \\beta)$ denotes a multinomial distribution parameterized with a regressor x and logistic regression coefficients \u03b2. Note that for every pair of"}, {"title": "3.3 Generative model for the conditional mixture network", "content": "Given a set of labels Y = {y\u00b9,y2, ..., yN}, and regressors X = {x, x,...,xN}, that define i.i.d input-output pairs x, yn, we write the joint distribution over labels Y, latents X1, Z1, and parameters as:\n$$p(Y, X_1, Z_1, \\Theta|X_0) = p (\\Theta) \\prod_{n=1}^{N} P_{\\beta_1} (y |X_1) P_{A_1} (x_1|x_0, z) P_{\\beta_0} (z|x_0)$$\n$$p (\\Theta) = p (\\beta_1) p (\\beta_0) p (\\Lambda_1)$$\n$$= \\prod_{l=1}^{L-1}P(\\beta_{l,1}) \\prod_{k=1}^{K-1}p(\\beta_{k,0}) \\prod_{j=1}^{K}P (A_{j}, \\Sigma_{j}^{-1})$$\n(3)\nNote that this model structure, with input and target variables, is often referred to as a discriminative model, as opposed to a generative model [Bernardo et al., 2007]. However, we use the term generative model to emphasize the fact that the model contains priors over latent variables (X1, Z1), and parameters $(\\Theta = (\\beta_{1:L-1,1}, \\beta_{1:K-1,0}, A_{1:K}, \\Sigma^{-1}_{1:K}))$ , and that we are estimating posteriors"}, {"title": "3.4 Coordinate ascent variational inference with conjugate priors", "content": "In this section we detail a variational approach for inverting the probabilistic model described in Equation (3) and computing an approximate posterior over latents and parameters specified as\n$$p (X_1, Z_1, \\Theta|Y,X) = \\frac{p (\u03a5, X_1, Z_1, \\Theta, X)}{p (YX)} \\approx q (\\Theta) \\prod_{n=1}^{N} [q (z^n) q (x|z)]$$\n(5)\nwhere $q (x|z)$ corresponds to a component specific multivariate normal distribution, and $q (z^n)$ to a multinomial distribution. Importantly, the approximate posterior over parameters $q (\\Theta)$ further factorizes [Svens\u00e9n, 2003] as\n$$q(\\Theta) = \\prod_{l=1}^{L-1}q (\\beta_{l,1}) \\prod_{k=1}^{K-1}q (\\beta_{k,0}) \\prod_{j=1}^{K}q (A_j, \\Sigma_{j}^{-1})$$\n$$=q(\\Lambda_1)$$\n$$q (\\beta_{l,1}) = N (\\beta_{l,1}; \\mu_{l,1}, \\Sigma_{l,1})$$\n$$q (\\beta_{k,0}) = N (\\beta_{k,0}; \\mu_{k,0}, \\Sigma_{k,0})$$\n$$q (A_{j},\\Sigma_{j}^{-1}) = \u039c\u039d (A_{j}; M_j, \u03a3j, Vj)$$\n$$q (\u03a3;) = \\prod_{i=1}^{h} \u0393 (\u03c3; aj, bi,j)$$\n(6)\nThe above form of the approximate posterior allows us to define tractable conditionally conjugate updates for each factor. This becomes evident from the following expression for the evidence lower-bound (ELBO) on the marginal log likelihood\n$$L(q) = E_{q(X_1,Z_1)q(\\Theta)} \\sum_{n=1}^{N} \\Bigg[ \\ln \\frac{P_{\\Theta} (y^n, x^n, z^n)}{q (z^n) q (x|z)} \\Bigg] + E_{q(\\Theta)} \\Bigg[\\ln \\frac{p (\\beta_1) p (\\beta_0) p (\\Lambda_1)}{q (\\beta_1) q (\\beta_0) q (\\Lambda_1)} \\Bigg]$$\n(7)\nWe maximize the ELBO using an iterative update scheme for the parameters of the approximate posterior, often referred to as variational Bayesian expectation maximisation (VBEM) [Beal, 2003] or coordinate ascent variational inference (CAVI) [Bishop and Nasrabadi, 2006, Blei et al., 2017].\nThe procedure consists of two parts:"}, {"title": "4 Results", "content": "To evaluate the effectiveness of the CAVI-based approach, we compared it to other approximate inference algorithms, using several real and synthetic datasets. We compared CMNs fit with CAVI to the following three approaches:\nMLE We obtained point estimates for the parameters A1:\u039a, \u03a31:\u03ba,\u03b2\u03bf, \u03b2\u2081 of the CMN using maximum-likelihood estimation. For gradient-based optimization of the loss function (the negative log likelihood), we used the AdaBelief optimizer with parameters set to its default values as introduced in Zhuang et al. [2020] (a = 1e-3, \u03b2\u2081 = 0.9, \u03b22 = 0.999), and run the optimization for 20, 000 steps. This implements deterministic gradient descent, not stochastic gradient descent, because we fit the model in 'full-batch' mode, i.e., without splitting the data into mini-batches and updating model parameters using noisy gradient estimates.\nNUTS-HMC We use the No-U-Turn Sampler (NUTS), an extension to the Hamiltonian Monte Carlo (HMC) samplers, that incorporates adaptive step sizes [Hoffman et al., 2014]. Markov Chain Monte Carlo converges in distribution to samples from a target distribution, so for this method we obtain samples from a joint distribution $p(A_{1:\u039a}, \u03a3^{-1}_{1:\u03ba}, \\beta_0, \\beta_1|Y, X_0)$ that approximate the true posterior. We used 800 warm-up steps, 16 independent chains, and 64 samples for each chain.\nBBVI - Black-Box Variational Inference (BBVI) method [Ranganath et al., 2014]. In constrast, to CAVI, the BBVI maximizes evidence lower bound (ELBO) using stochastic estimation of the gradients of variational parameters. While BBVI does not require conjugate relationships in the generative model, we use the same CMN model and variational distributions as we use for CAVI-CMN, in order to ensure fair comparison. For stochastic optimization,"}, {"title": "4.1 Comparison on synthetic datasets", "content": "For synthetic datasets we selected the Pinwheels and the Waveform Domains [Breiman and Stone, 1988] datasets. The pinwheels dataset is a synthetic dataset designed to test the model's ability to handle nonlinear decision boundaries and data with non-Gaussian densities. The dataset consists of multiple clusters arranged in a pinwheel pattern, posing a challenging task for mixture models [Johnson et al., 2016] due to the curved and elongated spatial distributions of the data. See Appendix C for the parameters we used to simulate the pinwheels dataset. Similarly, the Waveform Domains dataset consists of synthetic data generated to classify three different waveform patterns, where each class is described by 21 continuous attributes [Breiman and Stone, 1988].\nWe fit all inference methods using different training set sizes, where each next training set was twice as large as the previous (for pinwheels: we trained using train sizes 50 to 1600; for waveform domains: train sizes 60 to 3840). This was done in order to study the robustness of performance in the low data regime. For each training size, we used the same test-set to evaluate performance"}, {"title": "4.2 Comparison on real-world datasets", "content": "To further validate the performance of CAVI-CMN, we conducted experiments using 6 real-world classification datasets from the UCI Machine Learning Repository [Kelly et al., 2024]. Table 1 summarizes the performance of the different algorithms on all 7 different UCI datasets (the Waveform domains dataset and the 6 real datasets), using the widely-applicable information criterion (WAIC) as a measure of performance. WAIC is an approximate estimate of leave-one-out cross-validation [Vehtari et al., 2017].\nThe CAVI-CMN approach consistently provided higher WAIC scores in comparison to the MLE algorithm, and WAIC scores that were on par with BBVI and NUTS. The results confirm that using fully conjugate priors within the CAVI framework, does not diminishes the inference and the predictive performance of the algorithm, when compared to the state-of-the-art Bayesian methods like NUTS and BBVI. Importantly, CAVI-CMN offers substantial advantages in terms of computational efficiency as explored in the next section."}, {"title": "4.3 Runtime comparison", "content": "The NUTS algorithm, although considered state-of-the-art in terms of inference robustness and accuracy (for well calibrated models [Gelman et al., 2020]), is notoriously difficult to apply to large-scale problems [Cobb and Jalaian, 2021]. Hence, the preferred algorithm of choice for probabilistic machine learning applications have been methods grounded in variational inference, such as black-box variational inference (BBVI) [Ranganath et al., 2014] and stochastic variational inference (SVI) [Hoffman et al., 2013].\nIn this subsection, we analyze the runtime efficiency of the MLE and BBVI algorithms for CMN models, in comparison to a CAVI-based approach. The focus is on comparing the computation time as the number of parameters increases along different components of the model.\nTo ensure comprehensive comparison, we varied the complexity of the models by adjusting the number of components, the dimensionality of the input space, and the number of datapoints. These modifications effectively increase the number of parameters allowing us to observe how each algorithm scales with model complexity."}, {"title": "5 Conclusion", "content": "We demonstrate that the CAVI-based approach for conditional mixture networks (CMN) significantly outperforms the traditional maximum likelihood estimation (MLE) based approach, in terms of predictive performance and calibration. The improvement in probabilistic performance over the MLE based approaches can be attributed to implicit regularisation via prior information, and proper handling of posterior uncertainty over latent states and parameters, leading to a better representation of the underlying data, reflected in improved calibration error and log predictive density, even in low data regimes.\nOne of the key advantages of the CAVI-based approach is its computational efficiency compared to the other Bayesian inference methods such as Black-Box variational inference and the No-U-turn sampler (NUTS). While NUTS can sample from the full joint posterior distribution, which maximizes performance in terms of inference quality, this comes at the expense of substantial computational resources, especially for high dimensional and complex models [Hoffman et al., 2013]. The variational methods offer a scalable alternative to this answer, in the form of methods like black-box variational inference (BBVI). Although BBVI is highly efficient in comparison to NUTS, it takes longer to converge than CAVI when applied to CMN. Hence, we expect CAVI to be a more practical choice for large-scale application, especially when further combined with data mini-batching methods [Hoffman et al., 2013].\nThe benchmark results show that CAVI-CMN algorithm achieves comparable performance to BBVI and NUTS in terms of predictive accuracy, log-predictive density and expected calibration error, while being significantly faster. This balance between predictive likelihood and calibration (jointly viewed as indicators of sample efficiency) is particularly important in real-world applications where robust prediction, reflective of underlying uncertainty, are crucial.\nFurthermore, a straightforward mixture of linear components present in CMN, offers additional interoperability benefits. By using the conditionally conjugate priors, and a corresponding mean-field approximation over latent variables and model parameters, we facilitate easier interpretation of the model parameters and their uncertainties. This is particularly valuable in domains where"}, {"title": "A Variational Bayesian Multinomial Logistic Regression", "content": "In this section, we focus on a single multinomial logistic regression model (not in the context of the CMN), but the ensuing variational update scheme derived in Appendix A.4 is applied in practice to both the gating network's parameters Bo as well as those of the final output likelihood for the class label \u03b21."}, {"title": "A.1 Stick-breaking reparameterization of a multinomial distribution", "content": "Multinomial logistic regression considers the probability that an outcome variable y belongs to one of K mutually-exclusive classes or categories. The probability of y belonging to the kth class is given by the categorical likelihood:\n$$p(y = k|x, \\beta) = Pk$$\n(11)\nThe problem of multinomial logistic regression is to identify or estimate the values of regression coefficients \u03b2 that explain the relationship between some dataset of given continuous input regressors X = (x1,x2,...,xN) and corresponding categorical labels Y = (y1, y2, ...,yN), yn \u2208 1,2,..., \u039a.\nWe can use a stick-breaking construction to parameterize the likelihood over y using a set of K - 1 stick-breaking coefficients: \u03c0 = (\u03c01,..., \u03c0\u039a\u22121). Each coefficient is parameterized with an input regressor x, and a corresponding set of regression weights Bj. Stick-breaking coefficient \u03c0j is then given by a sigmoid transform of the product of the regression weights and the input regressors:\n$$\\pi_j = \u03c3(\\beta_j \\cdot [x; 1]),$$\nwhere $\u03c3 (\\beta_j \\cdot [x; 1]) = \\frac{1}{1 + exp {-\\beta_j \\cdot [x; 1]}}$,\nand $$\\beta_j \\cdot [x; 1] = \\sum_{i=1}^{d} w_{j,i} x_i + a_j.$$\n(12)\nThe outcome likelihood is then obtained via stick breaking transform\u00b9 as follows\n$$p_k = \\pi_k \\prod_{j=1}^{K-1} (1 - \\pi_j) = \u03c3 (\\beta_k \\cdot [x; 1]) \\prod_{j=1}^{K-1} (1 - \u03c3 (\\beta_j \\cdot [x; 1])) = \\prod_{j=1}^{K-1} \\frac{exp {\\beta_j \\cdot [x; 1]}}{1 + exp {\\beta_j \\cdot [x; 1]}}$$\n(13)\nwhere \u03c0\u03ba = 1, and \u1e9eK = 0.\nFinally, we can express the likelihood in the form of a Categorical distribution as\n$$Cat (y; x, \\beta) = \\prod_{k=1}^{K-1} \\Bigg(\\frac{exp {\\beta_k \\cdot [x; 1]}}{1 + exp {\\beta_k \\cdot [x; 1]}}\\Bigg)^{N_{k,y}}$$\n(14)\nwhere Nk,y = 1 for k \u2264 y, and Nk,y = 0 otherwise (or $N_{k,y} = 1 - \\sum_{j=1}^{k-1} \u03b4_{j,y}$), and \u03b4k,y = 1 for k = y and is zero otherwise."}, {"title": "A.2 P\u00f3lya-Gamma augmentation", "content": "The P\u00f3lya-Gamma augmentation scheme [Polson et al., 2013, Linderman et al., 2015, Durante and Rigon, 2019] is defined as"}, {"title": "A.3 Evidence lower-bound", "content": "Given a set of observations D = (y\u00b9, . . ., yN) the augmented joint distribution can be expressed as\n$$p (D,\\Omega, X, \\beta) = p (\\beta) \\prod_{n=1}^{N} p (x^n) p (w|y^n) e^{l(y^n,\\psi^n,\\omega^n)}$$\nWe can express the evidence lower-bound (ELBO) as"}, {"title": "A.4 Coordinate ascent variational inference", "content": "The mean-field assumption in Equation (22) allows the implementation of a simple CAVI algorithm [Wainwright et al., 2008, Beal, 2003, Hoffman et al., 2013, Blei et al., 2017] which sequentially maximizes the evidence lower bound in Equation (21) with respect to each factor in $q (\\Omega|\\hat{Y}) q (X) q (\\beta)$, via the following updates:\nUpdate to latents ('E-step')\n$$q^{(t,l)} (x^n) \\propto p (x^n) exp {E_{q^{(t-1)} (\\beta)q^{(t,l-1)} (\\omega^n)} [l (y^n, \\psi^n,\\omega^n)]}$$\n$$q^{(t,l)} (\\omega|y^n) \\propto p (\\omega|y^n) exp {E_{q^{(t-1)} (\\beta)q^{(t,l)} (\\omega^n)} [l_k (y^n, \\omega_k, \\omega_k)]}$$\n\u2200n \u2208 {1,..., N}, and for $q^{(t,0)} (\\omega^n|y^n) = q^{(t-1,L)} (\\omega^n|y^n)$ (23)\nUpdate to parameters ('M-step')\n$$q^{(t)} (B_k) \\propto exp{\\sum_{n=1}^{N}E_{q^{(t)} (x^{n})q^{(t)} (\\omega^n|y^n)} [1 (y^{n}, \\psi^{n}, \\omega^{n})]}$$\nat each iteration t, and multiple local iteration l during the variational expectation step-until the convergence of the ELBO.\nSpecifically, the update equations for the parameters of the latents (the 'E-step') are:\n$$q^{(t,l)} (x^n) \\propto N (x^n; 0, -2\\Lambda_{2,0}) exp{\\Bigg[ \\sum_{k=1}^{K-1} K_{k,y_n} \\mu_k^{(t-1)} \\cdot [x^n; 1]^T - \\frac{1}{2}\\sum_{k=1}^{K-1} K_{k,y_n} Tr (\\mu_k \\Lambda_k \\mu^T [x^n; 1] [x^n; 1]^T)\\Bigg]}$$\n$$\\Lambda_{n,t,l}^{(t-1)} =  \\sum_{k=1}^{K-1} (\\nu_{w_k,y}^{(t,l-1)} - \\frac{1}{2} K_{k,y_n}) [M_k^{(t-1)}]_{1:D, 1:D} \\Rightarrow \u0393$$,  -1,D\n$$M^{(t,l-1)} = (M_k^{-1} +  E_{k=1}^{K-1} [\\Lambda_i;  [M^{(t-1)}]^2$$"}, {"title": "and", "content": "$$q^{(t,l)} (\\omega|y^n) \\propto e^{-\\omega_k^2 (\\psi)/2}PG (\\omega; b_{k,y_n}, 0)$$\n$$\\Xi = \\sqrt{E_{q^{(t-1)} (\\beta)q^{(t,l)} (x^n)} [V_Z]}$$\n$$\\zeta_k = \\sqrt{Tr { (M^{(l-1)}_M^{ \\mu^{(n,t,l)} [\\mu^{(n,t,l)}]^T})}}$$\nwhere $M = \\frac{M^{M}}{M^M}$ and M = $\\sum_{n=1}^{N} \\eta_{k,y}^{n} + \\mu [^2.5}$$\nSimilarly, for the parameter updates ('M-step') we get\n$$q^{(t)} (\\beta_k) \\propto N (\\beta_k; 0, -2\u03c3^2_{2,0}) exp{\\sum_{n=1}^{N} \u03ba_{\u03ba,y_n}^2 [\\omega_k^2]_{Tr (\\mu^{(n,t)} \\beta^{T} - (\\frac{[\\omega \\beta]}{2})) }}$$\n\n(26)"}]}