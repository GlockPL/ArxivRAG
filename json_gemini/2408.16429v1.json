{"title": "Gradient-free variational learning\nwith conditional mixture networks", "authors": ["Conor Heins", "Hao Wu", "Dimitrije Markovic", "Alexander Tschantz", "Jeff Beck", "Christopher Buckley"], "abstract": "Balancing computational efficiency with robust predictive performance is crucial\nin supervised learning, especially for critical applications. Standard deep learn-\ning models, while accurate and scalable, often lack probabilistic features like cali-\nbrated predictions and uncertainty quantification. Bayesian methods address these\nissues but can be computationally expensive as model and data complexity in-\ncrease. Previous work shows that fast variational methods can reduce the compute\nrequirements of Bayesian methods by eliminating the need for gradient compu-\ntation or sampling, but are often limited to simple models. We demonstrate that\nconditional mixture networks (CMNs), a probabilistic variant of the mixture-of-\nexperts (MoE) model, are suitable for fast, gradient-free inference and can solve\ncomplex classification tasks. CMNs employ linear experts and a softmax gating\nnetwork. By exploiting conditional conjugacy and P\u00f3lya-Gamma augmentation,\nwe furnish Gaussian likelihoods for the weights of both the linear experts and\nthe gating network. This enables efficient variational updates using coordinate\nascent variational inference (CAVI), avoiding traditional gradient-based optimiza-\ntion. We validate this approach by training two-layer CMNs on standard bench-", "sections": [{"title": "1 Introduction", "content": "Modern machine learning methods attempt to learn functions of complex data (e.g., images, audio,\ntext) to predict information associated with that data, such as discrete labels in the case of classifi-\ncation [Bernardo et al., 2007]. Deep neural networks (DNNs) have demonstrated success in this do-\nmain, owing to their universal function approximation properties [Park and Sandberg, 1991] and the\nsoft regularization inherited from stochastic gradient descent learning via back propagation[Amari,\n1993]. However, despite its computational efficiency, accuracy, and scalability to increasingly large\ndatasets and models, DNNs trained this way do not provide well calibrated predictions and un-\ncertainty estimates, and practitioners typically utilize post-hoc calibration methods on validation\ndatasets [Wang et al., 2021, Shao et al., 2020]. This limits the applicability and reliability of us-\ning DNNs in safety-critical applications like autonomous driving, medicine, and disaster response\n[Papamarkou et al., 2024], where uncertainty-sensitive decision-making is required.\nBayesian machine learning addresses the issues of poor calibration and uncertainty quantification by\noffering a probabilistic framework that casts learning model parameters 0 as a process of inference\nnamely, calculating a posterior distribution over model parameters, given observed data (D =\n((X1,Y1),..., (xn, yn))), using Bayes' rule:\n$$p (\\theta | D) = \\frac{p (\\theta,D)}{p (D)}$$\n(1)\nThe resulting posterior distribution captures both expectations about model parameters and their\nuncertainty. The uncertainty is then incorporated in predictions that are, in principle, well-calibrated\nto novel datapoints coming from the same set. This probabilistic treatment allows methods like\nBayesian neural networks (BNNs) [Hern\u00e1ndez-Lobato and Adams, 2015] to maintain the expres-\nsiveness of deep neural networks while also encoding uncertainty over network weights and thus\nthe network's predictions. However, these methods are known to come with a significant increase\nin computational cost and thus scale poorly when applied to large datasets and high-dimensional\nmodels [Izmailov et al., 2021].\nIn this paper we introduce a gradient-free variational learning algorithm for a probabilistic variant\nof a two-layer, feedforward neural network the conditional mixture network or CMN and\nmeasure its performance on supervised learning benchmarks. This method rests on coordinate as-\ncent variational inference (CAVI) [Wainwright et al., 2008, Hoffman et al., 2013] and hence we\nname it CAVI-CMN. We compare CAVI-CMN to maximum likelihood estimation and two other\nBayesian estimation techniques: the No U-Turn Sampler (NUTS) variant of Hamiltonian Monte\nCarlo [Hoffman et al., 2014] and black-box variational inference [Ranganath et al., 2014]. We\ndemonstrate that CAVI-CMN maintains the predictive accuracy and scalability of an architecture-\nmatched feedforward neural network fit with maximum likelihood estimation (i.e., gradient descent\nvia backpropagation), while maintaining full distributions over network parameters and generating\ncalibrated predictions, as measured in relationship to state-of-the-art Bayesian methods like NUTS\nand BBVI.\nWe summarize the contributions of this work below:\n\u2022 Introduce and derive a variational inference scheme for the conditional mixture network,\nwhich we term CAVI-CMN. This relies on the use of conjugate priors for the linear experts\nand P\u00f3lya-Gamma augmentation [Polson et al., 2013] for the gating network and the final\nsoftmax layer."}, {"title": "2 Related work", "content": "The Mixture-of-Experts (MoE) architecture is a close relative of the CMN model we introduce\nhere. Jacobs et al. [1991] originally introduced MoEs as a way to improve the performance of neu-\nral networks by combining the strengths of multiple specialized models [Gormley and Fr\u00fchwirth-\nSchnatter, 2019]. MoE models process inputs by averaging the predictions of individual learners\nor experts, where each expert's output is weighted by a different mixing coefficient before the av-\neraging. The fundamental idea behind MoE is that the input space can be partitioned in such a\nway that different experts (models) can be trained to excel in different regions of this space, with\na gating network determining the appropriate expert (or combination of experts) for each input.\nThis leads to composable (and sometimes interpretable) latent descriptions of arbitrary input-output\nrelationships [Eigen et al., 2013], further bolstered by the MoE's capacity for universal function\napproximation [Nguyen et al., 2016, Nguyen and Chamroukhi, 2018]. Indeed, the powerful self-\nattention mechanism employed by transformers has has demonstrated the power and flexibility of\nMoE models [Movellan and Gabbur, 2020]. Non-Bayesian approaches to MoE typically rely on\nmaximum likelihood estimation (MLE) [Jacobs et al., 1991, Jordan and Jacobs, 1994], which can\nsuffer from overfitting and poor generalization due to the lack of regularization mechanisms [Bishop\nand Svenskn, 2003], especially in low data size regimes.\nTo address these issues, Bayesian approaches to MoE have been developed, which incorporate prior\ninformation and yield posterior distributions over model parameters [Bishop and Svenskn, 2003,\nMossavat and Amft, 2011]. This Bayesian treatment enables the estimation of model evidence\n(log marginal-likelihood) and provides a natural framework for model comparison and selection\n[Svens\u00e9n, 2003, Zens, 2019]. Bayesian MoE models offer significant advantages, such as improved\nrobustness against overfitting and a better understanding of uncertainty in predictions. However,\nthey also introduce computational challenges, particularly when dealing with high-dimensional data\nand complex model structures.\nThe introduction of the P\u00f3lya-Gamma (PG) augmentation technique in Polson et al. [2013] enabled\na range of novel and more computationally efficient algorithms for Bayesian treatment of MoE\nmodels [Linderman et al., 2015, He et al., 2019, Sharma et al., 2019, Viroli and McLachlan, 2019,\nZens et al., 2023]. Here we complement these past works, which mostly rest on improving sampling\nmethods with PG augmentation, by introducing a closed-form update rules for MoE's with linear\nexperts in the form of coordinate ascent variational inference (CAVI)."}, {"title": "3 Methods", "content": "In this section we first motivate the use of conditional mixture models for supervised learning, and\nthen introduce the conditional mixture network (CMN), the probabilistic model whose properties\nand capabilities we demonstrate in the remainder of the paper."}, {"title": "3.1 Conditional mixtures for function approximation", "content": "Feedforward neural networks are highly expressive, approximating nonlinear functions through\nsequences of nonlinear transformations, but the posterior distributions over their weights are in-\ntractable, requiring expensive techniques like MCMC or variational inference [MacKay, 1992, Blun-\ndell et al., 2015, Daxberger et al., 2021].\nWe circumvent these problems by focusing on the Mixture-of-Experts (MoE) models [Jacobs et al.,\n1991], and particularly a variant of MoE that is amenable to gradient free CAVI parameter updates.\nMoEs can be made tractable to gradient-free CAVI when the expert likelihoods are constrained\nto be members of the exponential family (see Section 2 for more details on the MoE architecture),\nand when the gating network is formulated in such a way to allow exact Bayesian inference (through\nlower bounds on the log-sigmoid likelihood [Jaakkola and Jordan, 1997, Bishop and Svenskn, 2003]\nor P\u00f3lya-Gamma augmentation [Polson et al., 2013]).\nThe MoE can be reformulated probabilistically as a mixture model by introducing a latent assign-\nment variable, zn, leading to a joint probability distribution of the form\n$$p(Y, Z, \\Theta) = p(\\theta_{1:K})p(\\pi) \\prod_{i=1}^n p(y_n|z^n, \\theta_{1:K})p(z^n|\\pi),$$\nwhere yn is an observation, \u0398 = {\u03b81:\u03ba,\u03c0}, Pk(yn|\u03b8k) is the kth-component's likelihood and zn\nis a discrete latent variable that assigns the nth datapoint to one of the K mixture components, i.e.\nPk(yn|\u03b8k) = p(yn|zn = k, \u03b81:K). For instance, if each \u2018expert' likelihood pk(yn|\u03b8k) is a Gaussian\ndistribution, then the MoE becomes a Gaussian Mixture Model, where \u03b8k = (\u03bc\u03ba, \u03a3\u03ba).\nThe problem of learning the model's parameters, then becomes one of doing inference over the la-\ntent variables Z and parameters of the mixture model. However, mixture models are generally not\ntractable for exact Bayesian inference, so some form of approximation or sampling-based scheme\nis required to obtain full posteriors over their parameters. However, if each expert (i.e., likelihood\ndistribution) in the MoE belongs to the exponential family, the model becomes conditionally con-\njugate. This allows for derivation of exact fixed-point updates to an approximate posterior over\neach expert's parameters. The approach we propose, CAVI-CMN, does exactly this we take ad-\nvantage of the conditional conjugacy of mixture models, along with an augmentation trick for the\nthe gating network, to make all parameters amenable to an approximate Bayesian treatment. The\nconditionally-conjugate form of the model allows us to use coordinate ascent variational inference to\nobtain posteriors over the weights of both the individual linear experts and the gating network [Wain-\nwright et al., 2008, Hoffman et al., 2013, Blei et al., 2017], without resorting to costly gradient or\nsampling computations.\nGoing forward we use the term conditional mixture networks (CMN) to emphasize (1) the discrim-\ninative nature of proposed application of this approach, where the model is designed to predict an\noutput y given an input x and (2) the fact that individual MoE layers can be stacked hierarchically\ninto a feedforward architecture. This makes CMNs particularly suitable for tasks such as supervised\nclassification and regression, where the goal is effectively that of function approximation; predict\nsome output variable y given input regressors x."}, {"title": "3.2 Conditional mixture network overview", "content": "The conditional mixture network maps from a continuous input vector xo \u2208 Rd to its label y \u2208\n{1, . . ., L}. This is achieved with two layers: a conditional mixture of linear experts, which outputs\na joint continuous-discrete latent (x1 \u2208 Rh, z\u2081 \u2208 {1, . . ., K}) and a multinomial logistic regression,\nwhich maps from the continuous latent x\u2081 to the corresponding label y. The probabilistic mapping\ncan be described in terms of the following operations:\n$$z_1 \\sim \\text{Mult} (z_1; X_0, \\beta_0)$$\n$$x_1 = A_{z_1} [x_0; 1] + u_{z_1}, \\qquad u_{z_1} \\sim N(0, \\Sigma_{z_1})$$\n$$y \\sim \\text{Mult} (y; x_1, \\beta_1)$$\nwhere we pad the input variable xo with a constant value set to 1, to absorb the bias term within\nthe mapping matrix $A_{z_1} \\in \\mathbb{R}^{h \\times d+1}$, and where Mult (z;x, \u03b2) denotes a multinomial distribution\nparameterized with a regressor \u00e6 and logistic regression coefficients \u03b2. Note that for every pair of"}, {"title": "3.3 Generative model for the conditional mixture network", "content": "Given a set of labels Y = {y\u00b9,y2, ..., yN}, and regressors X = {x, x,...,xN}, that define\ni.i.d input-output pairs x, yn, we write the joint distribution over labels Y, latents X1, Z1, and\nparameters as:\n$$p(Y, X_1, Z_1, \\Theta|X_0) = \\prod_{n=1}^N P_{\\beta_1} (y_n |X_1) P_{A_1} (x_1|x_0, z) P_{\\beta_0} (z|x_0)$$\n$$p (\\Theta) = p (\\beta_1) p (\\beta_0) p (\\Lambda_1)$$\n$$= \\prod_{l=1}^{L-1}p(\\beta_{l,1}) \\prod_{k=1}^{K-1}p(\\beta_{k,0}) \\prod_{j=1}^{K}p (A_j, \\Sigma^{-1}_j)$$\n(3)\nNote that this model structure, with input and target variables, is often referred to as a discrimina-\ntive model, as opposed to a generative model [Bernardo et al., 2007]. However, we use the term\ngenerative model to emphasize the fact that the model contains priors over latent variables (X1, Z1),\nand parameters $ (\\Theta = (\\beta_{1:L-1,1}, \\beta_{1:K-1,0}, A_{1:K}, \\Sigma_{1:K}))$ and that we are estimating posteriors"}, {"title": "3.4 Coordinate ascent variational inference with conjugate priors", "content": "In this section we detail a variational approach for inverting the probabilistic model described in\nEquation (3) and computing an approximate posterior over latents and parameters specified as\n$$p (X_1, Z_1, \\Theta|Y,X) = \\frac{p (Y, X_1, Z_1, \\Theta, X)}{p (Y|X)} \\approx q (\\Theta) \\prod_{n=1}^N q (z) q (x|z)$$\n(5)\nwhere $q (x|z)$ corresponds to a component specific multivariate normal distribution, and q (zn)\nto a multinomial distribution. Importantly, the approximate posterior over parameters q (\u0398) further\nfactorizes [Svens\u00e9n, 2003] as\n$$q(\\Theta) = \\prod_{l=1}^{L-1}q (\\beta_{l,1}) \\prod_{k=1}^{K-1}q (\\beta_{k,0}) \\prod_{j=1}^{K}q (A_j, \\Sigma^{-1}_j)$$\n$$=q(\\Lambda_1)$$\n$$q (\\beta_{l,1}) = N (\\beta_{l,1}; \\mu_{l,1}, \\Sigma_{l,1})$$\n$$q (\\beta_{k,0}) = N (\\beta_{k,0}; \\mu_{k,0}, \\Sigma_{k,0})$$\n$$q (A_j,\\Sigma^{-1}_j) = MN (A_j; M_j, V_j)$$\n$$q (\\Sigma^{-1}_j) = \\prod_{i=1}^h\\Gamma(\\sigma^2_i ; a_i, b_i)$$\n(6)\nThe above form of the approximate posterior allows us to define tractable conditionally conjugate\nupdates for each factor. This becomes evident from the following expression for the evidence lower-\nbound (ELBO) on the marginal log likelihood\n$$\\mathcal{L}(q) = E_{q(X_1,Z_1)q(\\Theta)} \\left[\\sum_{n=1}^N \\ln \\frac{P_{\\Theta} (y_n, x_1^n, z_n|x_0^n)}{q (z_n) q (x_1^n|z_n)} + E_{q(\\Theta)} \\ln \\frac{p (\\beta_1) p (\\beta_0) p (\\Lambda_1)}{q (\\beta_1) q (\\beta_0) q (\\Lambda_1)}\\right]$$\n(7)\nWe maximize the ELBO using an iterative update scheme for the parameters of the approximate\nposterior, often referred to as variational Bayesian expectation maximisation (VBEM) [Beal, 2003]\nor coordinate ascent variational inference (CAVI) [Bishop and Nasrabadi, 2006, Blei et al., 2017].\nThe procedure consists of two parts:"}, {"title": "4 Results", "content": "To evaluate the effectiveness of the CAVI-based approach, we compared it to other approximate\ninference algorithms, using several real and synthetic datasets. We compared CMNs fit with CAVI\nto the following three approaches:\nMLE We obtained point estimates for the parameters A1:\u039a, \u03a31:\u03ba,\u03b2\u03bf, \u03b2\u2081 of the CMN using\nmaximum-likelihood estimation. For gradient-based optimization of the loss function (the\nnegative log likelihood), we used the AdaBelief optimizer with parameters set to its default\nvalues as introduced in Zhuang et al. [2020] (\u03b1 = 1e - 3, \u03b2\u2081 = 0.9, \u03b22 = 0.999), and\nrun the optimization for 20, 000 steps. This implements deterministic gradient descent, not\nstochastic gradient descent, because we fit the model in 'full-batch' mode, i.e., without\nsplitting the data into mini-batches and updating model parameters using noisy gradient\nestimates.\nNUTS-HMC We use the No-U-Turn Sampler (NUTS), an extension to the Hamiltonian\nMonte Carlo (HMC) samplers, that incorporates adaptive step sizes [Hoffman et al., 2014].\nMarkov Chain Monte Carlo converges in distribution to samples from a target distribution,\nso for this method we obtain samples from a joint distribution p(A1:\u039a, \u03a31:\u03ba, \u03b2\u03bf, \u03b21|Y, X0)\nthat approximate the true posterior. We used 800 warm-up steps, 16 independent chains,\nand 64 samples for each chain.\nBBVI - Black-Box Variational Inference (BBVI) method [Ranganath et al., 2014]. In constrast\nto CAVI, the BBVI maximizes evidence lower bound (ELBO) using stochastic estimation\nof the gradients of variational parameters. While BBVI does not require conjugate relation-\nships in the generative model, we use the same CMN model and variational distributions\nas we use for CAVI-CMN, in order to ensure fair comparison. For stochastic optimization,"}, {"title": "4.1 Comparison on synthetic datasets", "content": "For synthetic datasets we selected the Pinwheels and the Waveform Domains [Breiman and Stone,\n1988] datasets. The pinwheels dataset is a synthetic dataset designed to test the model's ability to\nhandle nonlinear decision boundaries and data with non-Gaussian densities. The dataset consists\nof multiple clusters arranged in a pinwheel pattern, posing a challenging task for mixture models\n[Johnson et al., 2016] due to the curved and elongated spatial distributions of the data. See Ap-\npendix C for the parameters we used to simulate the pinwheels dataset. Similarly, the Waveform\nDomains dataset consists of synthetic data generated to classify three different waveform patterns,\nwhere each class is described by 21 continuous attributes [Breiman and Stone, 1988].\nWe fit all inference methods using different training set sizes, where each next training set was\ntwice as large as the previous (for pinwheels: we trained using train sizes 50 to 1600; for waveform\ndomains: train sizes 60 to 3840). This was done in order to study the robustness of performance\nin the low data regime. For each training size, we used the same test-set to evaluate performance"}, {"title": "4.2 Comparison on real-world datasets", "content": "To further validate the performance of CAVI-CMN, we conducted experiments using 6 real-world\nclassification datasets from the UCI Machine Learning Repository [Kelly et al., 2024]. Table 1 sum-\nmarizes the performance of the different algorithms on all 7 different UCI datasets (the Waveform\ndomains dataset and the 6 real datasets), using the widely-applicable information criterion (WAIC)\nas a measure of performance. WAIC is an approximate estimate of leave-one-out cross-validation\n[Vehtari et al., 2017].\nThe CAVI-CMN approach consistently provided higher WAIC scores in comparison to the MLE\nalgorithm, and WAIC scores that were on par with BBVI and NUTS. The results confirm that using\nfully conjugate priors within the CAVI framework, does not diminishes the inference and the pre-\ndictive performance of the algorithm, when compared to the state-of-the-art Bayesian methods like\nNUTS and BBVI. Importantly, CAVI-CMN offers substantial advantages in terms of computational\nefficiency as explored in the next section."}, {"title": "4.3 Runtime comparison", "content": "The NUTS algorithm, although considered state-of-the-art in terms of inference robustness and ac-\ncuracy (for well calibrated models [Gelman et al., 2020]), is notoriously difficult to apply to large-\nscale problems [Cobb and Jalaian, 2021]. Hence, the preferred algorithm of choice for probabilistic\nmachine learning applications have been methods grounded in variational inference, such as black-\nbox variational inference (BBVI) [Ranganath et al., 2014] and stochastic variational inference (SVI)\n[Hoffman et al., 2013].\nIn this subsection, we analyze the runtime efficiency of the MLE and BBVI algorithms for CMN\nmodels, in comparison to a CAVI-based approach. The focus is on comparing the computation time\nas the number of parameters increases along different components of the model.\nTo ensure comprehensive comparison, we varied the complexity of the models by adjusting the\nnumber of components, the dimensionality of the input space, and the number of datapoints. These\nmodifications effectively increase the number of parameters allowing us to observe how each algo-\nrithm scales with model complexity."}, {"title": "5 Conclusion", "content": "We demonstrate that the CAVI-based approach for conditional mixture networks (CMN) signifi-\ncantly outperforms the traditional maximum likelihood estimation (MLE) based approach, in terms\nof predictive performance and calibration. The improvement in probabilistic performance over the\nMLE based approaches can be attributed to implicit regularisation via prior information, and proper\nhandling of posterior uncertainty over latent states and parameters, leading to a better representation\nof the underlying data, reflected in improved calibration error and log predictive density, even in low\ndata regimes.\nOne of the key advantages of the CAVI-based approach is its computational efficiency compared to\nthe other Bayesian inference methods such as Black-Box variational inference and the No-U-turn\nsampler (NUTS). While NUTS can sample from the full joint posterior distribution, which maxi-\nmizes performance in terms of inference quality, this comes at the expense of substantial computa-\ntional resources, especially for high dimensional and complex models [Hoffman et al., 2013]. The\nvariational methods offer a scalable alternative to this answer, in the form of methods like black-box\nvariational inference (BBVI). Although BBVI is highly efficient in comparison to NUTS, it takes\nlonger to converge than CAVI when applied to CMN. Hence, we expect CAVI to be a more prac-\ntical choice for large-scale application, especially when further combined with data mini-batching\nmethods [Hoffman et al., 2013].\nThe benchmark results show that CAVI-CMN algorithm achieves comparable performance to BBVI\nand NUTS in terms of predictive accuracy, log-predictive density and expected calibration error,\nwhile being significantly faster. This balance between predictive likelihood and calibration (jointly\nviewed as indicators of sample efficiency) is particularly important in real-world applications where\nrobust prediction, reflective of underlying uncertainty, are crucial.\nFurthermore, a straightforward mixture of linear components present in CMN, offers additional\ninteroperability benefits. By using the conditionally conjugate priors, and a corresponding mean-\nfield approximation over latent variables and model parameters, we facilitate easier interpretation\nof the model parameters and their uncertainties. This is particularly valuable in domains where"}, {"title": "A Variational Bayesian Multinomial Logistic Regression", "content": "In this section, we focus on a single multinomial logistic regression model (not in the context of the\nCMN), but the ensuing variational update scheme derived in Appendix A.4 is applied in practice to\nboth the gating network's parameters Bo as well as those of the final output likelihood for the class\nlabel \u03b21.\nA.1 Stick-breaking reparameterization of a multinomial distribution\nMultinomial logistic regression considers the probability that an outcome variable y belongs to one\nof K mutually-exclusive classes or categories. The probability of y belonging to the kth class is\ngiven by the categorical likelihood:\n$$p(y = k|x, \\beta) = p_k$$\n(11)\nThe problem of multinomial logistic regression is to identify or estimate the values of regression\ncoefficients \u1e9e that explain the relationship between some dataset of given continuous input re-\ngressors X = (x1,x2,...,xN) and corresponding categorical labels Y = (y1, y2, ...,yN), yn \u2208\n1,2,..., \u039a.\nWe can use a stick-breaking construction to parameterize the likelihood over y using a set of K - 1\nstick-breaking coefficients: \u03c0 = (\u03c01,..., \u03c0\u039a\u22121). Each coefficient is parameterized with an input\nregressor x, and a corresponding set of regression weights Bj. Stick-breaking coefficient \u03c0j is then\ngiven by a sigmoid transform of the product of the regression weights and the input regressors:\n$$\\pi_j = \\sigma(\\beta_j \\cdot [x; 1]),$$\nwhere $$\\sigma (\\beta_j \\cdot [x; 1]) = \\frac{1}{1 + \\exp {-\\beta_j \\cdot [x; 1]}} ,$$\nand $$\\beta_j \\cdot [x; 1] = \\sum_{i=1}^d w_{j,i} x_i + a_j.$$\n(12)\nThe outcome likelihood is then obtained via stick breaking transform\u00b9 as follows\n$$p_k = \\pi_K \\prod_{j=1}^{K-1} (1 - \\pi_j) = \\sigma (\\beta_K \\cdot [x; 1]) \\prod_{j=1}^{K-1} (1 - \\sigma (\\beta_j \\cdot [x; 1])) = \\prod_{j=1}^{K-1} \\frac{\\exp {\\beta_j \\cdot [x; 1]}}{1 + \\exp {\\beta_j \\cdot [x; 1]}}$$\n(13)\nwhere \u03c0\u03ba = 1, and \u1e9eK = 0.\nFinally, we can express the likelihood in the form of a Categorical distribution as\n$$Cat (y; x, \\beta) = \\prod_{k=1}^{K-1} \\left(\\frac{\\exp {\\beta_k \\cdot [x; 1]}}{1 + \\exp {\\beta_k \\cdot [x; 1]}}\\right)^{N_{k,y}}$$\n(14)\nwhere Nk,y = 1 for k \u2264 y, and Nk,y = 0 otherwise (or $N_{k,y} = 1 - \\sum_{j=1}^{k-1} \\delta_{j,y}$), and \u03b4k,y = 1 for\nk = y and is zero otherwise.\nA.2 P\u00f3lya-Gamma augmentation"}, {"title": "A.3 Evidence lower-bound", "content": "Given a set of observations D = (y\u00b9, . . ., yN) the augmented joint distribution can be expressed as\n$$p (D,\\Omega, X, \\beta) = p (\\beta) \\prod_{n=1}^N p (x_n) p (w|y_n) e^{l(y^n,\\psi_n,w^n)}$$\nWe can express the evidence lower-bound (ELBO) as"}, {"title": "A.4 Coordinate ascent variational inference", "content": "The mean-field assumption in Equation (22) allows the implementation of a simple CAVI algorithm\n[Wainwright et al., 2008, Beal, 2003, Hoffman et al., 2013, Blei et al., 2017] which sequentially max-\nimizes the evidence lower bound in Equation (21) with respect to each factor in q (\u03a9|\u0176) q (X) q (\u03b2),\nvia the following updates:\nUpdate to latents ('E-step')\n$$q^{(t,l)} (x^n) \\propto p (x^n) \\exp \\left{E_{q^{(t-1)} (\\beta)q^{(t,l-1)} (w^n)} [l (y^n, \\psi^n,w^n)]\\right}$$\n$$q^{(t,l)} (w^n|y^n) \\propto p (w|y^n) \\exp \\left{E_{q^{(t-1)} (\\beta)q^{(t,l)} (\\psi^n)} [l_k (y^n, w_k, \\psi_k)]\\right}$$\n\u2200n \u2208 {1,..., N}, and for $q^{(t,0)} (w^n|y^n) = q^{(t-1,L)} (w^n|y^n)$\n(23)\nUpdate to parameters ('M-step')\n$$q^{(t)} (\\beta_k) \\propto \\exp \\left{\\sum_{n=1}^N E_{q^{(t)} (x^n)q^{(t)} (w^n|y^n)} [l (y^n, \\psi^n, w^n)]\\right}$$\nat each iteration t, and multiple local iteration l during the variational expectation step-until the\nconvergence of the ELBO.\nSpecifically, the update equations for the parameters of the latents (the 'E-step') are:\n$$q^{(t,l)} (x^n) \\propto \\mathcal{N} (x^n; 0, -2\\Lambda^{(n,t,l)}_{2,0}) \\exp \\left{-\\left[\\sum_{k=1}^{K-1} K_{k,y_n} \\mu_k^{(t-1)} [x^n; 1]^T \\right] - \\frac{1}{2} \\sum_{k=1}^{K-1} K_{k,y_n} Tr \\left(M_k^{(t-1)} [x^n; 1] [x^n; 1]^T \\right) \\right}$$\n$$ \\Lambda_{2,0}^{(n,t,l)} = \\Lambda_{2,0} - \\sum_{k=1}^{K-1} (W_{K_k,y_n}^{(t,l-1)} [M_k^{(t-1)}]_{1:D,1:D} \\qquad \\forall \\ k \\in [1,...,D]$$\n$$M_k^{(t-1)} = \\frac{\\tilde{\\Sigma}_k}{\\tilde{\\mu}_k^T \\tilde{\\mu}_k}\n(24)"}, {"title": "B Variational Bayesian Mixture of Linear Transforms", "content": "The variational 'M-step' in Equation (9) to update the parameters of the linear experts reduces to\na straightforward form when each expert parameterizes a multivariate Gaussian likelihood over the\nlatent variables X1 with Matrix Normal Gamma priors over the parameters of the linear function\nthat maps Xo to a distribution over X1.\nRecall the form of the update to the posterior parameters of the linear experts:\n$$q (A_{1:K}, \\Sigma^{-1}_{1:K}) \\propto \\exp \\left{-\\sum_{n=1}^N \\frac{q_{\\tilde{y}_n} z^n}{2} [w_{z_n}^T x^n - \\Sigma^{-1}_{z_n} A_{z_n} x_n]^T \\right}$$\n(27)\nThe approximate posteriors q (\u03911:\u039a, \u03a31:\u039a) and q(X1, Z1) have the following form:\n$$q (A_{1:K}, \\Sigma^{-1}_{1:K}) = \\prod_{k=1}^K \\left(MN(A_k; M_k, \\sigma_{k}^2, V_k)\\right) \\prod_{i=1}^I \\Gamma(\\sigma_i^2; a_i, b_i)$$\n$$q (X_1|Z) = \\prod_{n=1}^N \\prod_{k=1}^K \\mathcal{N}(x^n; \\mu_{k,1}, \\Sigma_{k,1})$$\n$$q (Z_1) = \\prod_{n=1}^N Cat(z_1^n; \\gamma^n)$$\nThe parameters of the kth expert $q(A_k, \\Sigma^{-1})$ can written in terms of weighted updates to the Matrix\nNormal Gamma's canonical parameters Mk, Vk, ak and bk:"}, {"title": "C Dataset Descriptions", "content": "We fit all inference methods using different training set sizes, where each next training set was twice\nas large as the previous. For each training size"}]}