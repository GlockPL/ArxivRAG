{"title": "FedClust: Tackling Data Heterogeneity in Federated Learning through Weight-Driven Client Clustering", "authors": ["Md Sirajul Islam", "Simin Javaherian", "Fei Xu", "Xu Yuan", "Li Chen", "Nian-Feng Tzeng"], "abstract": "Federated learning (FL) is an emerging distributed machine learning paradigm that enables collaborative training of machine learning models over decentralized devices without exposing their local data. One of the major challenges in FL is the presence of uneven data distributions across client devices, violating the well-known as- sumption of independent-and-identically-distributed (IID) training samples in conventional machine learning. To address the per- formance degradation issue incurred by such data heterogeneity, clustered federated learning (CFL) shows its promise by grouping clients into separate learning clusters based on the similarity of their local data distributions. However, state-of-the-art CFL approaches require a large number of communication rounds to learn the dis- tribution similarities during training until the formation of clusters is stabilized. Moreover, some of these algorithms heavily rely on a predefined number of clusters, thus limiting their flexibility and adaptability. In this paper, we propose FedClust, a novel approach for CFL that leverages the correlation between local model weights and the data distribution of clients. FedClust groups clients into clusters in a one-shot manner by measuring the similarity degrees among clients based on the strategically selected partial weights of locally trained models. We conduct extensive experiments on four benchmark datasets with different non-IID data settings. Experi- mental results demonstrate that FedClust achieves higher model accuracy up to ~45% as well as faster convergence with a signif- icantly reduced communication cost up to 2.7\u00d7 compared to its state-of-the-art counterparts.", "sections": [{"title": "1 INTRODUCTION", "content": "With the proliferation of Internet-of-Things (IoT) and the wide- spread adoption of artificial intelligence across various application domains, machine learning has been increasingly shifted toward the network edge, where computations are performed on edge devices rather than in centralized data centers [14, 28]. Such a computing paradigm shift is enabled by the rapid development of the compu- tation and storage capacity on edge devices, able to handle more complex and data-intensive tasks. To analyze and process massive data generated by various edge devices (e.g., mobile phones, wear- able devices, and autonomous vehicles), the traditional machine learning approach falls short. It requires transmitting large volumes of user data to centralized cloud servers, incurring prohibitive com- munication costs and raising privacy concerns as well.\nFederated learning (FL) has become a promising solution, al- lowing for participating devices to collaboratively train a globally shared model under the coordination of a central server, without exposing their local data. Due to its superior privacy-preservation implications, FL has been widely adopted by numerous companies (such as Google [9]) in a variety of applications, including computer vision [24], natural language processing [23], and human activity recognition [31, 38]. Essentially, FL is a distributed machine learn- ing framework, with training data resident on decentralized client devices.\nIn classical FL training [15, 21, 28], the server broadcasts the current global model to the participating clients. Each client trains the model using its local data and sends its local updates to the server. The server then aggregates model updates from partici- pating clients to update the global model, to be trained in the next round. These steps repeat until achieving a certain level of model accuracy or a pre-specified number of communication rounds.\nHowever, deploying FL often involves a number of devices that generate heterogeneous data due to their varying use styles. For instance, different users may watch videos on diverse types of content (e.g., news, sports, and entertainment) and run their smart- phones in varying frequencies. The presence of heterogeneous data across client devices breaks the conventional assumption of independent-and-identically-distributed (IID) training data, raising the new challenge of non-IID data distribution in the FL paradigm. Such data heterogeneity not only increases the overall communi- cation cost but also degrades global model performance [22, 44], increasingly drawing research attention to mitigate the adverse impact of non-IID data on FL [2, 4, 6, 14, 31, 34].\nInstead of learning a single global model, an alternative approach focuses on attaining personalized models for individual users to arrive at personalized FL, motivated by the observation that the globally learned model may exhibit lower accuracy on a participat- ing device than a local model independently trained on its own data [8]. More specifically, in addition to obtaining a collaboratively- trained global model, each client learns its personalized local model with various techniques such as regularization, local fine-tuning, model interpolation, multi-task learning, and knowledge distilla- tion [5, 13, 20, 22, 26, 27, 36, 37]. Nevertheless, apart from a lack of generalization by its nature, personalized FL suffers from lim- ited scalability, due to the extra computation overhead for learning a personalized model on each participant device. Additionally, it often fails to learn effectively on limited participant-specific data, unable to accurately capture local data distributions.\nRecently, clustered federated learning (CFL) [6, 11, 31, 34] has gained significant attention as a promising solution for tackling data heterogeneity. CFL frameworks group clients into multiple clusters based on the similarity of their data distributions and train a separate model for each cluster to alleviate the adverse effect of non-IID data. Most of the existing CFL approaches indirectly measure the data distribution similarity among clients by utilizing their local model updates or the gradients (e.g., [34]). Despite the promise of CFL, it remains an open challenge for clustering clients optimally. Existing efforts [6, 31, 38] rely on a predefined cluster count, which is difficult to determine optimally without any prior knowledge about data distributions or learning tasks among the clients. Under the assumption that the server holds a portion of globally shared data, Morafah et al. [29] proposed to cluster clients based on the similarity of the inferences on the shared data using updated local models from clients. Such a data availability assump- tion on the server may not be practical in reality. Sattler et al. [34] proposed to iteratively bi-partition clients into clusters based on the cosine similarity among their local model updates. This ap- proach is communication-inefficient as it requires a large number of communication rounds to form stabilized clusters. Moreover, state-of-the-art CFL methods hardly allow the flexibility of balanc- ing generalization and personalization.\nTo address the aforementioned limitations, we propose a novel clustered federated learning method, named FedClust, which ef- ficiently groups clients with non-IID data into suitable clusters. The design of FedClust leverages our insight into the implicit re- lationship between the local model weights and the underlying data distribution on a client device. In particular, FedClust utilizes locally-trained model weights on a client, obtained by performing a few local training iterations on its own data. To measure the simi- larity among clients, FedClust requires each client to send only the strategically selected partial weights to the server, further reducing the amount of data transmission. After receiving weights from all the clients, FedClust constructs a proximity matrix based on the Euclidean distance to efficiently identify distribution similarities among clients. An agglomerative hierarchical clustering (bottom- up approach) [3] on the proximity matrix is employed to classify similar clients into an optimal number of clusters. Remarkably, Fed- Clust operates without the need for any proxy data on the server, as opposed to all the existing solutions [7, 29]. Furthermore, in contrast to its state-of-the-art counterparts [2, 6, 34] which fail to accommodate client dynamics, FedClust provides an elegant mech- anism for effectively incorporating new clients into appropriate clusters on-the-fly.\nFinally, we have conducted extensive experiments to evaluate the performance of FedClust on four benchmark image classifica- tion datasets under different non-IID data settings, with our state- of-the-art counterparts following the LeNet-5 [18] and ResNet-9 [10] architectures. The experimental results demonstrate that the proposed approach significantly improves overall model accuracy, surpassing both global and personalized baselines by up to ~45% and ~18%, respectively. Moreover, FedClust enables faster conver- gence due to great reduction in communication costs by 1.2 - 2.7\u00d7 when compared to state-of-the-art FL methods.\nOur key contributions are summarized as follows:\n\u2022 We analyze the relationship of model weights with the un- derlying data distribution of clients. Moreover, we observe the implicit connection of different layer weights with the local data distributions.\n\u2022 We propose a novel clustered federated learning framework named FedClust to alleviate the adverse impact of non-IID data on FL. Our framework utilizes the similarity among strategically selected partial weights of locally-trained mod- els from participant devices, to optimally form clusters for efficient and effective learning. In addition, we present an elegant strategy to accommodate client dynamics, incorpo- rating newcomers into appropriate clusters in real time.\n\u2022 We conduct extensive experiments under diverse represen- tative settings to evaluate FedClust with a variety of perfor- mance metrics. Experimental results demonstrate the advan- tages of FedClust over the state-of-the-art baselines, espe- cially in improving overall model accuracy and communica- tion efficiency.\nThe remainder of this paper is organized as follows. Section 2 describes related work. The background and motivation of this work are introduced in Section 3. In Section 4, we present a brief explanation on the design of our proposed FedClust framework. In Section 5, we evaluate FedClust and compare its performance"}, {"title": "2 RELATED WORK", "content": "In this section, we review pertinent work on addressing the non-IID data issue in federated learning.\n2.1 Federated Learning with Non-IID Data\nIn federated learning (FL), data stored in each user device greatly varies due to different usage patterns and habits. Specifically, differ- ent users may prefer to browse news on different topics e.g., sports, politics, and technology, leading to non-IID data for different users. In reality, it is natural that the data used for FL training are usually non-IID. The most widely used FL algorithm FedAvg [28] fails to achieve optimal performance in the presence of non-IID data across clients. Several studies [21, 44] have shown that non-IID data not only decreases the accuracy of the trained model but also slows down training convergence with larger communication costs. To mitigate the client drift issue caused by non-IID data, FedProx [21] introduces a proximal term to the local training objective to keep local models close to the global model. FedDyn [1] introduces a dynamic regularizer for each client in every round to align the global and the local models.\nIn SCAFFOLD [15], data heterogeneity is modeled as a source of variance among clients following a variance reduction technique. It estimates the direction of updates for the global model and that of each client. The drift of local training is then calculated by compar- ing two update directions. Finally, it modifies the local updates by incorporating the drift in local training. FedNova [41] considers the number of local training epochs performed by each client during every round of FL to produce an unbiased global model. It normal- izes and scales local updates based on the number of local training epochs before updating the global model. While effective under certain scenarios, these global FL methods cannot systematically ad- dress the data heterogeneity issue. Several studies [12, 17] proposed guided participant selection strategies, with a subset of clients is selected to participate in FL training according to some predefined criteria. These approaches provide faster model convergence and better time-to-accuracy performance.\nTo mitigate the impact of non-IID data, prior approaches [5, 13, 27] focused on personalizing the global model with each client's local data via fine-tuning. They let the global model act as an initial point for learning personalized models at clients based on their local data. Nevertheless, the global model may not be a good initiator, if the local data distributions of clients highly differ among one another. Similarly, Smith et al. [36] extended multitask learning in FL training to aim at learning personalized models for multiple related tasks with the coordination of a central server. Recent work, PGFed [26], formulated client's local objectives as personalized global objectives to explicitly transfer collaborative knowledge across them.\n2.2 Clustered Federated Learning\nAlternatively, clustered federated learning (CFL) approaches [2, 6, 31, 34] have been proposed to efficiently alleviate the challenge due to non-IID data among clients. They divide clients into clusters"}, {"title": "3 BACKGROUND AND MOTIVATION", "content": "3.1 Federated Learning\nFederated learning (FL) is a privacy-preserving framework that allows distributed clients to collaboratively train machine learning or deep learning models without sharing their local data [28]. FL usually involves a set of clients and a central server. Each clients receives its initial global model from the server and then train it for a few local iterations using its local data. The server is responsible for aggregating all local model updates to update the globally shared model. The communication between the server and every client follows a predetermined communication protocol. The server has no prior knowledge about the data distribution across devices as it cannot access the raw data stored in clients.\nMcMahan et al. [28] first introduced a federated averaging al- gorithm (FedAvg) that implements the idea of federated learning. In order to optimize the communication efficiency of FL over real- world data, the FedAvg algorithm trains a globally shared model across clients by a weighted averaging of the local model param- eters of clients. In particular, the goal of FedAvg is typically to minimize the following objective function:\n$\\mathop{\text{min}}\\limits_{\\theta} F(\\theta) \\triangleq \\sum_{i=1}^{m} \\frac{n_i}{N} F_i(\\theta) \\triangleq \\sum_{i=1}^{m} \\frac{n_i}{N} \\frac{1}{n_i} \\sum_{j=1}^{n_i} f(\\theta; x_{ji}, y_{ji})$\nHere, m is the set of participating clients and client i has local dataset $D_i$, where $n_i = |D_i|$ and $N = \\sum_{i=1}^m n_i$. The local objec- tive functions of clients can be defined as the empirical loss over their local data $D_i$, i.e., $F_i(\\theta) = \\frac{1}{n_i} \\sum_{j=1}^{n_i} f(\\theta; x_{ji}, y_{ji})$, where $n_i$ is the number of client i's local samples. It is empirically shown that FedAvg provides better performance when the data distribu- tion across clients is IID [28]. In reality, data produced by different clients are usually non-IID in nature, thus negatively impacting the convergence and performance of federated learning in practi- cal applications. Recently, clustered federated learning (CFL) has attracted research efforts to address the data heterogeneity issue with promising performance improvement.\n3.2 Motivation\nAlthough CFL-based approaches [2, 6, 31, 34] have shown lofty improvement over FedAvg when dealing with non-IID data, they still lack efficiency due to their limitations of clustering strategies. We thus identify key limitations as well as opportunities in what follows:\n\u2022 Difficult to determine the cluster count in advance. Most of the existing CFL approaches [6, 31, 38] require a given number of clusters apriori, usually very hard to deter- mine without knowing the actual data distributions across clients, despite that model accuracy is highly dependent on the optimal number of clusters.\n\u2022 Require larger communication rounds to form stable clusters. Some existing CFL approaches [34] can group clients into an appropriate number of clusters. Specifically, FMTL [34] iteratively partitions clients into clusters accord- ing to the cosine similarity of their local model updates. While yielding an optimal number of clusters, it is not com- munication efficient as a large number of communication rounds are needed to form stabilized clusters.\n\u2022 Is it necessary to utilize all model weights? A majority of current CFL approaches use all model weights or model updates to calculate model similarity which reflects the un- derlying data distribution of clients. It imposes a huge pres- sure on the server when calculating the similarity over a large number of models simultaneously. In addition, existing literature [25, 33, 43] demonstrates that there are distinc- tions between the different layers in the same model, and higher layers weights are more task-related compared to lower layers weights. So, is it possible to effectively compare model similarity using just partial weights?\nTo address the above limitations, we propose a new clustered feder- ated learning approach, FedClust, that divides clients into a suitable number of clusters in a one-shot manner by calculating the similar- ity using selected partial weights of clients' locally trained models.\n3.3 Observation\nDespite previous observations [25, 33, 43] on model layers as dis- cussed before, there is a lack of understanding in how these varia- tions can potentially impact federated learning. In this section, we conduct an experimental study to investigate the implications of model weights from different layers on the underlying data distri- bution.\nWe conduct a simple experiment for a multi-class image classifi- cation task on the CIFAR-10 [16] dataset with VGG16 [35]. Specif- ically, the VGG16 model contains thirteen convolutional layers and three fully connected layers. To simulate non-IID data among clients, we assume 10 different clients and group them into two clusters based on their local label categories, e.g., $G_1 = \\{1, 2, ..., 5\\}$ and $G_2 = \\{6, 7, ..., 10\\}$. \nFrom Fig. 1, we observe that the final layer weights implicitly represent the underlying data distribution of clients. Specifically, Figs. 1(a) and 1(b) depict the distance matrices based on the two convolutional layer weights respectively. However, we cannot ob- tain the cluster structures of the clients from them. The clustering structures of the clients are clearly observed in Figs. 1(d). Moreover,"}, {"title": "3.4 Overview", "content": "To advance existing CFL pursuits, we propose FedClust which can identify cluster patterns among clients based on the distance of weights from the final layers of their local models. The proposed method, as described in Algorithm 1, is able to alleviate the ef- fects of non-IID data on practical FL applications. We first focus on clustering clients based on their local data distributions in a feder- ated network. The proposed approach is one-shot clustering that classifies m clients into n clusters, i.e., $G = \\{g_1,..., g_n\\}$, based on the similarity of their underlying data distributions before starting federation. FedClust trains one model individually for each cluster $g_k$, instead of training a common global model for all clients. The objective function for the clients of each cluster $g_k$ is defined as follows:\n$\\mathop{\\text{min}}\\limits_{\\theta} F(\\theta_{g_k}) = \\sum_{c_i \\in g_k} \\frac{n_{c_i}}{N_{g_k}} F_{ik}(\\theta_{g_k})$\nHere, $n_{c_i}$ and $N_{g_k}$ denotes the number of data samples for client $c_i$ and cluster $g_k$, respectively, and $F_{ik} (\\theta_{g_k})$ determines the empirical loss on $c_i$'s local dataset $D_i$.\nAn overview of the proposed framework is depicted in Fig. 2. To minimize computation overhead, our clustering strategy employs a static approach that avoids rescheduling clients in each round. First, the server broadcasts the initial global model to all available clients. Then, each client trains the model on its local data for a few local iterations and sends back the updated final layer weights to the server as the representation of their underlying data distri- bution. The server then computes the proximity matrix between models based on the final layer weights uploaded by each client. Finally, the server employs agglomerative hierarchical clustering (HC) [3] on the proximity matrix M to group clients with simi- lar data distribution into the same cluster. The basic idea is that, initially considering each client as a separate cluster, it repeatedly performs the following operations in each iteration: (1) identify the two clusters that exhibit the closest similarity, and (2) combine the two most similar clusters. To determine which clusters should be merged, a linkage criterion (e.g. single, average, complete, etc.) is defined [3]. For instance, the smallest distance between two points in each cluster in \"single linkage\" is defined as the pairwise Eu- clidean distance between two clusters. In this paper, we denote \u03bb as the clustering threshold, representing the distance between two clusters. The iterative process continues until a suitable number of clusters have been formed. The above clustering process is done in one communication round.\nFrom the next round, the workflow of FedClust is similar to FedAvg [28]. The server initializes all cluster models with \u03b8. A subset of available clients is selected randomly by the server and the server broadcasts $\\theta^g$ to selected clients. Each client trains the model on its local data and performs a few steps of stochastic gradient descent (SGD) updates. The clients send back their updated model parameters along with their cluster IDs to the server. The server conducts model averaging for each cluster after receiving model updates from all clients."}, {"title": "4 METHODOLOGY", "content": "4.1 Selection of Model Weights\nFedClust utilizes only the final layer weights instead of the full model weights to group clients into clusters. We have observed empirically that there exists an implicit relationship between the local data distribution of clients with the model weights trained on their dataset, consistent with the findings in [40]. Therefore,\nFedClust leverages clients model weights to infer the relative char- acteristics of the underlying data distribution. The difference in data distribution between clients can be approximated based on the difference in their model weights, referred to as the model distance, after the completion of their local model training. The model dis- tance between the model weights of any two clients $c_p$ and $c_q$ can be calculated using $l_2$ distance as follows:\n$dist (c_p,c_q) = || \\theta_{c_p} - \\theta_{c_q}||$\nGenerally, if two clients contain similar data distributions, they tend to train models in a similar fashion compared to clients with dissimilar data [31, 34, 40]. As a result, the distance between their model weights will be smaller. Therefore, the model distance can be used as a useful metric to cluster clients. The server forms a distance matrix M of size m \u00d7 m after receiving the final layer weights of all client's models. Each entity of the matrix $M_{pq}$ represents the computed model distance $dist(c_p, c_q)$ between clients $c_p$ and $c_q$. Federated learning training typically involves a large number of client devices and the target machine learning model could be complex with huge parameters, e.g., the VGG16 model has a total of 138M weights [35]. Consequently, clustering methods based on model weights would require higher computation costs, affecting the clustering efficiency.\nTo enhance clustering accuracy and reduce additional compu- tational overheads, we only select the final layer weights of the clients local model instead of full model weights to determine the similarity. In Fig. 1, we have empirically demonstrated that the final layer weight of the model reflects the model difference caused by non-IID data. Therefore, calculating the distance matrix using all weights can lead to a bad similarity matrix thus reducing the clus- tering accuracy. Moreover, the lower layers of the model contain the majority portion of the weights. Specifically, in deep learning models, especially those used for image classification tasks, e.g., CNN models, the purpose of convolutional layers is to identify and extract features from the input, while the fully connected layers focus on the final classification task. Therefore, fully connected layer weights are more task-related. In FedClust, we thus choose a subset of the model's parameters, specifically the weights and bias of the final layer, to serve as a representation of the entire model. We utilize these weights to calculate distance matrix M. It significantly reduces the computation cost as the size of the final layer weights $\\theta_{c_p}$ is much smaller than the full model weights $\\Theta_{c_p}$.\n4.2 Incorporating Newcomers\nIn reality, client devices may join in or drop out of the federated learning process due to unreliable client communications or other resource limitations. Clients who quit the training have no impact on the model training of their respective clusters. However, it is important to carefully incorporate newcomer clients into appro- priate clusters in order to maintain the scalability of our method. FedClust offers an elegant approach to accommodate newcomers who join after the federation procedure to learn their personalized model. The baseline methods except PACFL [39] did not clarify the process of incorporating newcomer clients during federation. We outlined the process of how FedClust integrates new participants who joined after the end of the federation in Algorithm 2.\nClients who are not in the existing client set are referred to as newcomers. In order to assign each new client $c_{new}$ into a suitable cluster, $c_{new}$ is required to train the initial server model on its local data, and then sends partially selected weights to the server. FedClust stores a copy of each cluster's partial model weights. After receiving the partial model weights from new client $c_{new}$, the server computes the model distances between the new client $c_{new}$'s model and the models of existing clusters. The cluster with the minimum model distance will be selected as the cluster for $c_{new}$, represented as follows:\n$g^* = arg min_{g_m} dist (\\theta_{c_{new}}, \\theta_{g_m}), g_m \\in G$"}, {"title": "5 EXPERIMENTS", "content": "5.1 Experimental Setup\nDatasets and Models. We evaluate the performance of FedClust on different image classification tasks using four popular bench- mark datasets, i.e., CIFAR-10 [16], CIFAR-100 [16], Fashion MNIST (FMNIST) [42], and SVHN [30]. To imitate non-IID scenarios, we consider three different data heterogeneity settings for each dataset as in [19], i.e., Non-IID label skew (20%), Non-IID label skew (30%), and Non-IID Dir (0.1). In our experiments, we consider LeNet-5 [18] architecture for CIFAR-10, FMNIST, and SVHN datasets and ResNet-9 [10] architecture for CIFAR-100 dataset.\nBaselines Methods. To demonstrate the performance of the proposed method, we compare the results of FedClust against the following state-of-the-art (SOTA) FL baselines. We consider FedAvg [28], FedNova [41], and FedProx [21] for baselines that train a single global model across all clients. Baselines for SOTA CFL methods include IFCA [6], PACFL [39] and Clustered-FL (CFL) [34]. SOTA personalized FL methods include Per-FedAvg [5] and LG-FedAvg [23]. In addition, we compare our results with another baseline named Local, where each client independently trains a model on its local data without any communication with others.\nImplementation. We have implemented FedClust and the base- line methods in PyTorch [32]. We assume 100 clients are available for all experiments and 10% of them are sampled randomly in each communication round. We ran each experiment 3 times for 200 communication rounds. We execute all experiments on a server, which is equipped with NVIDIA GeForce RTX 3080Ti GPU, Intel(R) Core(TM) i9-10900X CPU, and 64G RAM. We emulate both the server and clients on the same machine, substantiated by the fact that the performance metrics we consider remain unaffected by the physical location of the server and clients. The wall-clock training time may be affected but this metric is beyond our scope of focus, similar to our counterparts.\nHyperparameters Settings. In all of our experiments, we use SGD as the local optimizer with the local epoch of 10, and the local batch size of 10. We initialize the models randomly in LG-FedAvg for a fair comparison instead of using the model produced after many rounds of FedAvg. For IFCA and CFL, we used the same number of clusters as mentioned in the original papers. For PACFL, we used p = 3 in all of our experiments. The learning rate for FedAvg, FedProx, FedNova, and CFL was set to (0.1, 0.01, 0.001), while for other baselines, it was 0.01. Momentum was 0.9 for FedAvg, FedProx, and FedNova, whereas for other methods, it was 0.5. In LG, the number of local layers and global layers were set to 3 and 2. In Per-FedAvg, we used a = 1e - 2 and \u1e9e = 1e - 3. For CFL, the values of \u20ac1 and \u20ac2 were 0.4 and 0.6, respectively.\nEvaluation Metrics. We use the average of the final local test accuracy over all clients and the number of communication rounds required to reach a certain level of model accuracy as the perfor- mance metrics. In general, it is desirable to achieve higher model accuracy with fewer communication rounds. We also consider the required communication costs to reach a target accuracy.\n5.2 Results and Analysis\nPerformance comparisons. We compare FedClust with other SOTA baseline methods under two different widely used Non-IID settings, i.e. Non-IID label skew, and Non-IID Dirichlet label skew [19]. We consider two different Non-IID label skew settings 20% and 30%. In both settings, we start by randomly assigning 8% of the total available labels of a dataset to each client and then randomly distributing the samples of each label among clients who own these labels. In Non-IID Dirichlet label skew settings, we assign training data to clients according to the Dirichlet distribution similar to [19]. We run each experiment three times for 200 communication rounds with a local epoch of 10 and report the mean and standard deviation of the average of final local test accuracy.\nImpact of newcomers. In order to evaluate the performance of the newcomer clients personalized model, we conduct an experi- ment with Non-IID label skew (20%) in which only 80 out of 100 clients are involved in a federation with 50 rounds. The remaining 20 clients are incorporated into the network after the completion of the federation and receive their corresponding cluster model from the server. The newcomer clients personalize their cluster model for only 5 epochs. The average local test accuracy of the newcomer clients is reported. Table 6 demonstrates that FedClust has the capability to incorporate new participants to learn their personalized model with higher test accuracy.\nTrade-off between generalization and personalization. To address the data heterogeneity, prior works introduced a proxi- mal term in local optimization or modified the model aggregation method on the server side to benefit from some degree of personal- ization [21, 22]. Despite being effective, they lack the flexibility to balance between globalization and personalization. Our proposed FedClust framework can naturally navigate this trade-off. The per- formance of FedClust in terms of accuracy is illustrated in Fig. 4 for different values of \u03bb, which is the clustering threshold that controls the number of clusters. The blue curve and the red bars illustrate the accuracy and number of clusters respectively for each \u03bb. By changing the value of \u03bb, which is determined based on the dataset, FedClust can switch from training a fully global model (1 cluster) to training fully personalized models for each client.\nFig. 4 demonstrates that increasing values of a lead to a decrease in the number of clusters, indicating a higher degree of globalization. FedClust groups all clients into 1 cluster when A is large enough and the scenario becomes similar to the FedAvg baseline (pure globalization). On the other hand, as a decreases, the number of clusters increases, resulting in a greater level of personalization. Each client forms individual clusters when A is small enough and the scenario degenerates to the Local baseline (pure personalization). The result of our experiments across all datasets demonstrates that all clients benefit from some level of globalization. For Non-IID label skew (20%), Fig. 2 illustrates that the highest accuracy results on CIFAR-10, CIFAR-100, SVHN, and FMNIST datasets are achieved when the number of clusters are 2, 2, 2, and 4, respectively. IFCA [3] lacks this trade-off flexibility as it requires a predefined number of clusters.\nComputation overhead and privacy. The computational over- head of FedClust is minimal compared to the FedAvg baseline algo- rithm. It requires performing one-shot HC after the first round. The"}, {"title": "6 CONCLUSION AND DISCUSSION", "content": "In this paper, we propose a simple and effective clustered federated learning framework, FedClust, to address the data heterogeneity issue. The proposed framework aims to identify data distribution similarities among clients by exploiting the implicit relationship between the underlying data distribution and model weights. Fed- Clust efficiently groups clients with non-IID data into an appropriate number of clusters according to the similarity among the subset of chosen weights of their locally trained models. The effectiveness of FedClust has been demonstrated through experimental evaluations over four popular datasets with a broad range of data heterogeneity scenarios.\nThis article includes a statistical analysis of FedClust. The con- vergence analysis of FedClust is left for future work. In addition, A is a user-defined hyperparameter which plays a crucial role in determining the number of clusters for our clustering approach. We intend to pursue a data-driven method for dynamically identifying the optimal value of a for each dataset in the future."}]}