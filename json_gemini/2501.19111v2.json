{"title": "A Benchmark for Incremental Micro-expression Recognition", "authors": ["Zhengqin Lai", "Xiaopeng Hong", "Yabin Wang", "Xiaobai Li"], "abstract": "Micro-expression recognition plays a pivotal role in understanding hidden emotions and has applications across various fields. Traditional recognition methods assume access to all training data at once, but real-world scenarios involve continuously evolving data streams. To respond to the requirement of adapting to new data while retaining previously learned knowledge, we introduce the first benchmark specifically designed for incremental micro-expression recognition. Our contributions include: Firstly, we formulate the incremental learning setting tailored for micro-expression recognition. Secondly, we organize sequential datasets with carefully curated learning orders to reflect real-world scenarios. Thirdly, we define two cross-evaluation-based testing protocols, each targeting distinct evaluation objectives. Finally, we provide six baseline methods and their corresponding evaluation results. This benchmark lays the groundwork for advancing incremental micro-expression recognition research. All source code used in this study will be publicly available at https://github.com/ZhengQinLai/IMER-benchmark.", "sections": [{"title": "1 Introduction", "content": "In recent years, we have witnessed a growing interest in Micro-Expression Recognition (MER) due to its critical role in understanding concealed emotions [Ekman, 2003] and its applications in domains such as security and healthcare. Traditional approaches to micro-expression recognition have predominantly operated assuming that the entire dataset is available at once for a single training phase [Karnati et al., 2023]. However, real-world scenarios often present a dynamic environment where data arrive continuously, frequently introducing new categories. This evolving nature of data renders the retraining of models from scratch with each update impractical [Masana et al., 2022], as it entails significant computational overhead and risks catastrophic forgetting [Pf\u00fclb and Gepperth, 2019]. Thus, incremental learning, which allows models to adapt progressively to new data while retaining previously learned information [Parisi et al., 2019], is essential for the MER model to address these challenges.\nDespite the necessity of this learning paradigm, it is not trivial to introduce incremental learning to micro-expression recognition directly. The main challenges are manifold.\nA key challenge lies in establishing an appropriate learning setup. Typically, incremental learning settings include class-incremental learning [Yan et al., 2021; Wang et al., 2022a; Zhou et al., 2022] and domain-incremental learning [Wang et al., 2022b; Zhu et al., 2025]. In class-incremental learning, models learn new samples from previously unseen classes, whereas in domain-incremental learning, models adapt to new samples from the same classes but originating from different domains. In MER, however, the scenarios could be mixed. It is common to first encounter data from a dataset containing certain micro-expression (ME) classes, followed by another dataset that shares only a subset of those classes. For samples of already seen classes, the task aligns with domain-incremental learning where distinct domain differences arise. As shown in Figure 1, datasets like CASME II and SAMM differ significantly in sensor types (grayscale or RGB), image resolutions, imaging conditions, and subject demographics. Furthermore, datasets often use different protocols for inducing micro-expressions. For samples of unseen classes, the task transitions to class-incremental learning. For example, when sequentially learning from the CASME II, SAMM, and MMEW datasets, new emotion categories emerge such as Angry in SAMM and Fear in MMEW emerge. Consequently, each learning phase involves a combination of samples from new categories and new domains. This unique challenge means that existing incremental learning paradigms cannot directly address the practical scenarios encountered in incremental micro-expression recognition.\nHow to organize data poses another significant challenge. Traditional incremental learning studies typically segment a single dataset into sequential tasks, as exemplified by the Split CIFAR-100 benchmark which divides the dataset into ten tasks of ten distinct classes each (commonly referred to as the '10-10' setting). However, such single-dataset-split protocols are infeasible for MER. Micro-expression datasets typically consist of only a few categories and several hundred samples. These limited data sizes are insufficient for further splitting as the traditional \u201810-10' setting.\nThe third challenge lies in how to evaluate cross-subject generality in an incremental setting. Typical incremental learning studies usually do not involve subject-level evaluation. However, cross-subject evaluation is an important factor of interest in (micro-)expression recognition. One straightforward approach is to perform cross-subject evaluation in each learning session and then perform combinational evaluation across sessions. However, such a direct combination leads to a combinatorial explosion in the number of evaluation trials. For example, if there are 10, 12, and 15 subjects in three consecutive sessions, a direct combination would result in 10 x 12 x 15 = 1800 trials to obtain a single performance metric value. This makes the protocol impractical for developing incremental micro-expression recognition approaches.\nTo address these challenges, we construct the first benchmark for incremental micro-expression recognition (IMER). Firstly, to tackle the challenge of class-domain composite incremental scenarios, we formulate the IMER problem, reflecting both class evolution and domain shifts. Secondly, to solve the data organization issue, we carefully curate multiple micro-expression datasets organized as their publication dates. Thirdly, to avoid combinatorial explosion while still covering cross-subject test, we propose the fold binding regulation for cross-session evaluation to bound the total number of trials in cross-validation, based on which we introduce two within-session protocols. Additionally, we propose a remappable classification head technique to adapt mainstream incremental learning methods for the composite class-domain incremental task and build the quantitative baselines for IMER. By defining the problem, organizing datasets, designing evaluation protocols, introducing baseline methods, and providing results, our work establishes a solid foundation for the study of incremental micro-expression recognition."}, {"title": "2 Related Work", "content": "The evolution of micro-expression recognition has witnessed a shift from traditional approaches [Pfister et al., 2011; Li et al., 2013a] to neural network-based methods, particularly advancing from convolutional architectures [Patel et al., 2016; Kim et al., 2016; Gan et al., 2019] to transformer-based networks [Xia and Wang, 2021; Zhang et al., 2022; Li et al., 2022a; Nguyen et al., 2023].\nWhile these methods have demonstrated impressive performance, they predominantly operate under batch learning paradigms and have been validated primarily on single datasets. Such approaches, though common in the field, may not fully address real-world scenarios where continuous learning from new data streams is essential."}, {"title": "2.2 Datasets for Micro-expression Recognition", "content": "The continuous introduction of new MER datasets has significantly advanced the study of micro-expression recognition (MER). Well-known datasets such as SMIC [Li et al., 2013b], SAMM [Davison et al., 2018], the CASME series (CASME [Yan et al., 2013], CASME II [Yan et al., 2014], CAS(ME)3 [Li et al., 2022b]), MMEW [Ben et al., 2022], and 4DME [Li et al., 2022c] offer detailed annotations including onset, apex, and offset frames, along with Action Unit (AU) [Ekman and Friesen, 1978] labels. One of the latest ME datasets, CAS(ME)\u00b3, for example, offers extensive multimodal data of a larger sample size, increasing the ecological validity for researchers. However, most previous ME datasets contain less than 300 samples. Given the limited size of individual datasets, splitting a single dataset for incremental learning experiments is impractical. Instead, a cross-dataset approach following temporal order would be more suitable for micro-expression recognition tasks."}, {"title": "2.3 Incremental Learning", "content": "Incremental learning enables models to learn continuously from new data while retaining previously acquired knowledge. This paradigm addresses real-world scenarios where data becomes available sequentially, and models need to adapt without complete retraining to avoid catastrophic forgetting. The settings and protocols for incremental learning have evolved from task-incremental learning to more challenging scenarios such as class-incremental and domain-incremental learning.\nClass-Incremental Learning (CIL): This setting focuses on continuously adding new classes while maintaining performance on previous ones. Early approaches like Learning without Forgetting [Li and Hoiem, 2017] and iCaRL [Rebuffi et al., 2017] utilized knowledge distillation and exemplar memory to preserve prior knowledge. Recent methods have explored various strategies: DER [Yan et al., 2021] dynamically expands model capacity, while FOSTER [Wang et al., 2022a] employs a two-stage learning paradigm with dynamic expansion and compression. Notably, prompt-based methods like L2P [Wang et al., 2022d] and DualPrompt [Wang et al., 2022c] leverage pre-trained models by introducing learnable prompts to guide incremental learning. RanPAC [McDonnell et al., 2024] enhances feature separation through random projections between pre-trained features and the output layer.\nDomain-Incremental Learning (DIL): This setting addresses model adaptation across different data distributions while preventing the forgetting of previous domains. The primary challenge is handling concept drift [Kirkpatrick et al., 2017; Wang et al., 2021], where the underlying data distribution shifts over time. Methods like S-Prompt [Wang et al., 2022b] tackle this by learning domain-specific prompts on pre-trained models without requiring previous data rehearsal. \nWhile these settings have been extensively studied, our micro-expression recognition scenario presents unique challenges by inherently combining both class and domain incremental aspects, as MEs can vary across different domains while new ME categories may emerge. Conventional incremental learning settings and their corresponding methods, for either class or domain incremental learning separately, are not directly applicable and require substantial adaptation for this hybrid scenario."}, {"title": "3 Benchmark Construction", "content": "This section presents our key contributions in establishing IMER, as illustrated in Figure 2. We first formulate the problem definition of IMER. Subsequently, we introduce our data organization strategy that enables the systematic study of partially observed expressions, along with two evaluation protocols for effective cross-subject evaluation. We then describe the implementation and adaptation of various baseline methods to this new setting."}, {"title": "3.1 Problem Definition", "content": "We define the IMER problem as follows. Consider a stream of labeled datasets D(1), D(2), ..., D(n), where each dataset D(t) = {(x, y, id)}\u2116\u2081 consists of micro-expression samples, their corresponding emotion labels, and subject identifiers, where Nt denotes the number of samples in the t-th dataset. Let l(t) represent the set of emotion classes in the t-th dataset, and the cumulative emotion label space up to dataset t is defined as Lt = U=11(k). Our goal is to learn a mapping function from the samples space X to their emotion label space Y, while the subject identifiers id are used for data splitting in the evaluation protocol.\nThe model is incrementally trained on the data sequence D(1), D(2), ..., D(n), where D(t) is only available during the t-th training session. After training on D(t), \u04e8\u0165 is responsible for recognizing all encountered emotion classes in Lt.\nThe training process follows Algorithm 1, where the model is updated sequentially while expanding its recognition capability from Lt-1 to Lt at each session.\nUnlike conventional CIL and DIL that address isolated challenges of new classes or domain shifts, IMER presents a compound scenario where each incoming dataset D(t) may contain both new domain samples of existing classes (e.g., \"happiness\" expressions captured under different recording conditions in CASME II vs. SAMM) and novel classes (e.g., the \"fear\" category introduced in MMEW). We term this complex setting by composite class-domain incremental learning, which imposes three critical requirements: cross-domain generalization to handle domain shifts within known classes, class expansion capability for incorporating new emotion categories, and interference resistance to mitigate conflicts between cross-domain and cross-class knowledge."}, {"title": "3.2 Data and Evaluation Protocol", "content": "Establishing appropriate data organization and evaluation protocols is crucial yet challenging for incremental micro-expression recognition. Unlike conventional single-dataset-split protocols, which suffer from limited data size in IMER, we organize the learning stages according to the chronological order of dataset publication dates.This organization introduces a chronological session order for learning that aligns with two fundamental assumptions about real-world data streams [Roth et al., 2024]: first, new datasets naturally emerge after existing ones, reflecting the temporal progression of data collection; second, later datasets typically encompass richer categories and more advanced acquisition techniques. Moreover, this chronological session order presents significant challenges due to the pronounced domain gaps between datasets. An ideal incremental approach must effectively adapt to these gaps. Consequently, conducting IMER under this session order better simulates the iterative versioning process of practical systems.\nSpecifically, we construct a sequence of four learning sessions S = {S1, S2, S3, S4}, where each session introduces a new micro-expression dataset in chronological order of their publication, namely CASME II, SAMM, MMEW, and CAS(ME)\u00b3 respectively. Table 1 shows the time of different datasets and emotion categories l' of each session.\nEvaluation Protocol\nTraditional MER typically employs n-fold cross-validation protocols like LOSO (Leave-One-Subject-Out) to assess algorithm dependency on subjects. Nonetheless, in IMER, cross-dataset permutations are required. Direct enumeration of validation combinations across different sessions while maintaining n-fold validation within each session would lead to a combinatorial explosion, as mentioned in the introduction. To address this, we introduce fold binding in cross-session validation to limit the number of test trials to n for each session's internal cross-validation. More specifically, for each fold (trial) \u03c4\u2208 {1, ..., k}, the test and training sets at session t are defined as D(tr) and D(tr) respectively. During trial 7, the model is trained on D(tr) and evaluated on the test sets from all encountered sessions U-1 De), maintaining consistent fold assignments across sessions. Fold binding regulates cross-session evaluation, which maintains computational feasibility by controlling the total number of tests within O(n) complexity. As shown in Figure 3 (a), these protocols maintain consistent fold assignments across sessions through fold binding, where each fold represents a complete incremental learning process using the same test partition index across all sessions.\nBased on this cross-session evaluation regulation, we further use two Cross Validation protocols to perform experiments within a session: Subject Level Cross Validation"}, {"title": "3.3 Evaluation Metrics", "content": "In incremental learning, model evaluation requires assessing the performance in both the current session and all previous sessions. The accuracy A\u00bf of a single session i is defined as Ai = \\frac{C_i}{T_i}, where C\u2081 denotes the number of correctly classified samples and Ti represents the total number of test samples for all encountered classes in Li. On this basis, cross-session incremental learning performance is evaluated for one trial of experiments. We use both the final accuracy and average accuracy [Zhou et al., 2023] across all sessions. The final accuracy A reflects the model's overall performance after the last session:\n$$A = A_n.$$\nThe average accuracy A reflects the progressive performance by averaging the accuracies at each session:\n$$A = \\frac{\\sum_{i=1}^n A_i}{n}.$$\nAs we perform n-fold cross-evaluation, these two accuracies of each trial are further averaged over the n-fold evaluation for reliable evaluation."}, {"title": "3.4 Benchmark Methods", "content": "To provide the first baseline IMER methods, we upgrade typical MER methods to their incremental learning counterparts. The main difficulty lies in that traditional incremental learning methods cannot be directly applied to IMER as its composite class-domain incremental learning nature. A straightforward approach to handle incremental learning in micro-expression recognition would be to simply expand the classification head whenever a new emotion category appears in session i that was not present in session i \u2013 1. However, this naive solution becomes a double-edged sword: while accommodating new categories, it unwittingly amplifies concept drift across domains. The cross-domain variations in how emotions are expressed can lead to inconsistent feature representations, degrading classification performance. To address this problem, we propose a remappable classification head technique to incorporate base models into the incremental learning pipeline.\nRemappable Classification Head\nThe Remappable Classification Head (RCH) consists of two key components: redundant classification heads and dynamic remapping strategy.\nRCH maintains redundant classification heads for each session, allowing the model to preserve session-specific knowledge and prevent catastrophic forgetting. Specifically, for each session t, RCH introduces an independent classification head group Ht \u2208 R|lt|\u00d7d, where |lt| represents the number of emotion classes in the current session and d is the feature dimension.\nSubsequently, RCH employs a dynamic remapping mechanism to consolidate redundant heads representing the same emotion category across different sessions:\n$$H_{\\text{final}}^c = \\sum_{t \\in T_c} H_t^c$$\nwhere Hfinal represents the final remapped classification weight for emotion class c, To denotes the set of sessions where class c appears, and Hc represents the classification weight for class c in session t.\nFor a given feature vector x, the final prediction probability distribution is computed as:\n$$p(c|x) = \\text{softmax}(x \\cdot H_{\\text{final}}^c)$$\nRCH enables the model to maintain session-specific representations while still provides a unified classification framework. Compared to directly using all redundant heads, this summation approach eliminates redundancy while maintaining model accuracy. As illustrated in Figure 4, RCH dynamically expands its classification capacity as new sessions are introduced, while maintaining the ability to differentiate between domain-specific manifestations of the same emotion. Notably, RCH exhibits remarkable versatility as a general-purpose solution that can be seamlessly integrated with virtually any existing incremental learning framework. The modular nature of RCH allows it to serve as a plug-and-play component, replacing traditional classification heads while preserving the core mechanisms of the host incremental learning method. This universal compatibility, combined with its ability to address the challenges of overlapping and evolving class distributions, positions RCH as a powerful and flexible solution for IMER tasks.\nBase models and incremental learning methods\nWhile micro-expression recognition is essentially a video classification task, previous research has demonstrated that utilizing only key frames can achieve competitive performance [Liong et al., 2018; Li et al., 2022a; Nguyen et al., 2023; Zhai et al., 2023]. Additionally, optical flow-based methods [Zhang et al., 2022] have also proven effective in capturing subtle facial movements characteristic of micro-expressions. Following this established direction in micro-expression recognition pipeline, we build our benchmark upon three widely-used backbones: ResNet [He et al., 2016], Vision Transformer (ViT) [Dosovitskiy, 2020] and Swin Transformer (SwinT) [Liu et al., 2021] with specific frame selection strategy [Liong et al., 2018].\nThen we combine these base models with several representative incremental learning methods using the RCH technique. We conduct a comprehensive evaluation of several representative incremental learning methods, categorizing them"}, {"title": "4 Results", "content": "All experiments were implemented on a single NVIDIA L20 GPU. Training was performed with a batch size of 16 and an initial learning rate of 2e-5. The initial session was trained for 60 epochs, followed by 10 epochs of training for each subsequent session. To address the issue of limited data and ensure a fair comparison, we initialized all models with ImageNet [Deng et al., 2009] pre-trained weights.\nPerformance Analysis\nExperimental results across different backbones and testing protocols are presented in Table 2 and Figure 5. Several noteworthy observations are made.\nFirst and foremost, Ranpac consistently demonstrates superior performance across all backbone architectures and testing protocols (as shown in Table 3). Under the ILCV protocol, Ranpac achieves the highest average accuracies of 37.79%, 48.44%, and 45.77% for ResNet, ViT, and SwinT respectively. Similarly, under the more challenging SLCV protocol, it maintains its leading position with accuracies of 34.26%, 41.64%, and 37.61%. This advantage can be attributed to the design of pre-trained model-based methods that perform full fine-tuning of the backbone during the initial training stage and freeze the backbone in subsequent incremental stages, updating only the classifier. Such an architecture is particularly well-suited to scenarios where the incremental process involves a high degree of feature sharing among classes. In micro-expression recognition, where inter-class features exhibit significant overlap, pre-trained model-based approaches show a pronounced advantage.\nWhen comparing backbone architectures, we observe a general performance hierarchy where SwinT and ViT consistently outperform ResNet. This aligns with previous experimental results under the LOSO protocol, where transformer-based architectures have demonstrated superior performance in micro-expression recognition tasks. Specifically, SwinT and ViT achieve notably higher accuracies across all testing scenarios, with improvements ranging from 7-10% over ResNet-based implementations.\nFurthermore, the results reveal a consistent pattern across the two testing protocols: a method using the same backbone consistently achieves higher accuracy under the ILCV protocol compared to the SLCV protocol. For instance, Ranpac with ViT backbone achieves an average accuracy of 48.44% under ILCV compared to 41.64% under SLCV. This performance gap can be attributed to the inherent nature of SLCV that cross-subject evaluation is more challenging, which is important to evaluate the model's generalization ability across different individuals."}, {"title": "5 Conclusion", "content": "Our research establishes the first benchmark for Incremental Micro-Expression Recognition (IMER), covering key aspects such as problem definition, data organization, evaluation protocols, and baseline methods. It addresses the critical question of how to develop a micro-expression recognition model when data arrives incrementally in stages.\nOur research also advances the field of incremental learning in two key aspects. First, we identify IMER as a composite class-domain incremental learning task, which is both more practical and challenging than conventional incremental learning tasks, including class-, domain-, or cross-domain class incremental learning. Furthermore, we introduce the remappable classification head mechanism to construct a classifier tailored for composite class-domain incremental learning. It can potentially be extended to build incremental learning model for other similar tasks."}, {"title": "A Experiment results under LOSO", "content": "Experiments were conducted on CASME II dataset for 5-class classification, evaluated using accuracy, F1-score, and UAR metrics. As shown in Table 3, our SwinT-based approach achieved 81.5% accuracy and 0.805 F1-score, demonstrating competitive performance despite using general-purpose architecture. These results establish a solid foundation for incremental learning experiments."}]}