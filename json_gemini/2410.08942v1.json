{"title": "MAXIMIZING THE POTENTIAL OF SYNTHETIC DATA: INSIGHTS FROM RANDOM MATRIX THEORY", "authors": ["Aymane El Firdoussi", "Mohamed El Amine Seddik", "Soufiane Hayou", "Reda Alami", "Ahmed Alzubaidi", "Hakim Hacid"], "abstract": "Synthetic data has gained attention for training large language models, but poor-quality data can harm performance (see, e.g., Shumailov et al. (2023); Seddik et al. (2024)). A potential solution is data pruning, which retains only high-quality data based on a score function (human or machine feedback). Previous work Feng et al. (2024) analyzed models trained on synthetic data as sample size increases. Using random matrix theory, we generalize this analysis and derive the performance of a binary classifier trained on a mix of real and pruned synthetic data in a high dimensional setting. Our findings identify conditions where synthetic data could improve performance, focusing on the quality of the generative model and verification strategy. We also show a smooth phase transition in synthetic label noise, contrasting with prior works on sharp transition in infinite sample limits. Our extensive experimental setup validates our theoretical results.", "sections": [{"title": "1 INTRODUCTION", "content": "The landscape of large language models (LLMs) is evolving rapidly, with a growing trend towards training models on a combination of real and synthetic data. This synthetic data is often generated by previously trained models (Allal et al., 2024; Ben Allal et al., 2024; Abdin et al., 2024). However, the quality of these generators can significantly impact the performance of newly trained models, potentially leading to model collapse (Shumailov et al., 2023), a phenomenon in which the model drastically degrades in performance.\nModel collapse has been extensively studied, both empirically (Guo et al., 2023) and theoretically (Seddik et al., 2024), highlighting the potential risks associated with training on synthetic data. To mitigate these risks, researchers have proposed various strategies, including the verification of AI-synthesized data (Feng et al., 2024). This approach aligns with the widely adopted Reinforcement Learning from Human Feedback (RLHF) technique (Kaufmann et al., 2023). Feng et al. (2024) provided theoretical support for this strategy by analyzing synthetic data as Gaussian mixtures with noisy labels, using linear binary classifiers and scalar parameters to control verifier quality. Their findings reveal a sharp performance transition: under infinite synthetic sample size conditions, model accuracy shifts from zero accuracy (due to errors in synthetic data and verification) to optimal performance as these errors decrease.\nWhile current theoretical studies primarily focus on label noise in synthetic data (Dohmatob et al., 2024; Gerstgrasser et al., 2024; Feng et al., 2024), they often overlook potential distribution shifts in the feature space between real and synthetic data. This gap is particularly relevant in practical scenarios where generative models are trained on finite real data sets, potentially leading to imperfect learning of the underlying distribution.\nOur paper addresses this gap by proposing a statistical model that accounts for both distribution shifts in the feature space and label noise. In our model, we induce distribution shifts in the feature space by supposing that the statistics of synthetic data are empirical estimates of the underlying real data statistics. In a finite sample size regime, these estimates may be biased, resulting in distribution"}, {"title": "Summary of contributions", "content": "Our contributions are four fold:\n\u2022 We introduce a statistical model for studying synthetic data that accounts for label and feature noise, extending beyond previous models that only consider label noise.\n\u2022 By leveraging random matrix theory, we characterize the performance of a binary classifier trained on a mixture of real and synthetic data in a high-dimensional setting.\n\u2022 When training only on synthetic data, we find a smooth phase transition in classifier performance, generalizing the work of Feng et al. (2024) on sharp transitions in infinite sample size limit.\n\u2022 We validate our results with extensive experiments (toy example and realistic LLM setups)."}, {"title": "2 RELATED WORK", "content": "The use of synthesized data for model training has gained significant traction in recent years, partic-ularly with the widespread adoption of large language models (LLMs) that rely on large amounts of data in their training stages. Several studies have explored the impact of synthesized data on model performance, revealing both its advantages and limitations. A primary concern is the phenomenon of model collapse Shumailov et al. (2023), where the iterative use of generated data for model training results in a degradation of model quality. This issue has been explored theoretically and empirically across multiple studies (e.g. Alemohammad et al. (2023); LeBrun et al. (2021); Bohacek & Farid (2023); Seddik et al. (2024); Dohmatob et al. (2024)).\nSeddik et al. (2024) investigated model collapse in recursive training settings, where new models are trained on data generated by previous models. They demonstrate that recursive training on purely synthetic data inevitably leads to performance degradation. However, they show that mixing real and synthetic data can attenuate model collapse, though the proportion of real data must remain high to maintain model performance. Their findings support the idea that synthesized data alone cannot sustain model quality across iterations without a significant quantity of real data.\nGerstgrasser et al. (2024) argue that model collapse can be avoided entirely if data is accumulated rather than replaced across iterations. Their empirical studies on language models, diffusion models, and variational autoencoders indicate that accumulating both real and synthetic data helps maintain model performance over time, breaking the recursive degradation loop that leads to collapse.\nThe most relevant work to our study is Feng et al. (2024) where the authors examined the effects of synthesized data on model performance in a non-recursive setting, using the concept of rein-forcement through feedback to select high quality synthetic data. Their theoretical results, based on Gaussian mixture models, showed that adding feedback significantly improves the robustness of models trained on synthesized data. However, their setup assumes that only labels, not features, are noisy. Additionally, their focus is primarily on scenarios where only the number of data points, n, grows to infinity. Other (practical) scenarios where for instance the feature dimension, p, grows at a fixed ratio with n are not covered.\nOur work extends the Gaussian mixture model setup to include both noisy features and labels, which is a more realistic scenario when training on synthesized data. Additionally, we consider a high-dimensional regime where both p and n grow to infinity with a fixed ratio, a setup often used in Random Matrix Theory (RMT). This allows us to study the interaction between feature dimension, pruner error, and data size in a more comprehensive manner. Our approach also accounts for the presence of mixed data-original and synthetic-providing a more realistic framework for studying the effect of synthetic data in practical applications."}, {"title": "3 THEORETICAL SETUP", "content": "Real data. We suppose that real data consists of n p-dimensional i.i.d. vectors $x_1,...,x_n \\in \\mathbb{R}^p$ sampled from a Gaussian mixture of two distinct isotropic clusters $C_1$ and $C_2$ of means $\\pm \\mu$ with $\\mu \\in \\mathbb{R}^p$. Essentially, for $a \\in \\{1,2\\}$, each data vector $x \\in C_a$ has a corresponding label $y_a = (-1)^a$ and is sampled as\n$x_i = y_i \\mu + z_i, z_i \\sim N(0,I_p)$.\nGenerative model. To generate synthetic data, we consider the generative model corresponding to maximum likelihood which consists of estimating the underlying first and second-order statistics of the real data with their empirical estimates. In particular, we suppose that we are given a subset $\\tilde{n} \\leq n$ of the real dataset $\\{(x_i, y_i)\\}_{i=1}^{\\tilde{n}}$ on which we can estimate the statistics. This setup allows us to model a situation where new real data samples might be available to train next-generation models and the parameter $\\tilde{n}$ offers control over the generative model quality. The statistics for generating synthetic data are therefore computed using the following estimates\n$\\hat{\\mu} = \\frac{1}{\\tilde{n}} \\sum_{i=1}^{\\tilde{n}} y_i x_i$\n$\\hat{C} = \\frac{1}{\\tilde{n}} \\sum_{i=1}^{\\tilde{n}} (y_i x_i - \\hat{\\mu})(y_i x_i - \\hat{\\mu})^T$.\nSynthetic data. We consider that synthetic data is generated as m i.i.d. vectors $\\tilde{x}_1,..., \\tilde{x}_m \\in \\mathbb{R}^p$ with corresponding (noisy) labels $\\tilde{y}_1,........,\\tilde{y}_m = \\pm 1$ such that $x \\in \\tilde{C}_a$ with true label $y_i = (-1)^a$ for $a \\in \\{1,2\\}$ is sampled as ($\\tilde{C}_1$ and $\\tilde{C}_2$ denote the synthetic clusters)\n$\\tilde{x}_i = y_i \\hat{\\mu} + \\hat{C}^{\\frac{1}{2}} z_i, z_i \\sim N(0,I_p)$,\nand the labels $\\tilde{y}_i$ are generated such that $P\\{\\tilde{y}_i = y_i\\} = 1 - \\epsilon$ where $\\epsilon \\geq 0$ controls label noise. Essentially, the quality of synthetic data depends on the sample size $\\tilde{n}$ and the label noise rate $\\epsilon$.\nIn the asymptotic regime where $\\tilde{n} \\to \\infty$ with $\\frac{p}{\\tilde{n}} \\to 0$, we can generate synthetic samples that follow asymptotically the exact same distribution as of the real ones, and therefore only la-bel noise is relevant to the quality of the syn-thetic data. However, in the regime when both $\\tilde{n}, p \\to \\infty$ with $\\frac{p}{\\tilde{n}} \\to \\tilde{\\eta} > 0$, while the es-timation of $\\mu$ with $\\hat{\\mu}$ remains consistent, the estimation of the covariance is not. In fact, in this regime $||\\hat{C} - I_p|| \\to 0$ and the eigenval-ues of $\\hat{C}$ spread in the vicinity of 1 which is described in the limit by the Marchenko-Pastur law (Marchenko & Pastur, 1967) Eventually, such inconsistency in esti-mating the second moment in high dimensions yields a distribution shift between synthetic and real data, which might cause a drop in perfor-mance when training a new model on synthetic data generated with $\\hat{\\mu}$ and $\\hat{C}$. In the remainder, we describe precisely how the performance of a simple classifier is affected in this regime.\nObjective. Our goal throughout the paper is to study the effect of synthetic data when train-ing on a mixture of the n real and m synthetic data described above, i.e., with the following objective function\n$\\mathcal{L}(w) := \\frac{1}{n+m} \\sum_{i=1}^{n} l(x_i, y_i; w) + \\frac{1}{n+m} \\sum_{i=1}^{m} q_i l(\\tilde{x}_i, \\tilde{y}_i; w)$"}, {"title": "4 MAIN RESULTS", "content": "In this section, we present and discuss the main results obtained through the analysis of the classifier model defined in equation 6. We start by specifying the supposed growth rate assumptions.\nAssumption 4.1 (Growth Rate). We consider a high-dimensional regime where $p, n, \\tilde{n}, m \\to \\infty$ and we recall $N = n + m$ such that:\n$\\frac{p}{n} \\to \\eta \\in [0, \\infty)$,\n$\\frac{p}{\\tilde{n}} \\to \\tilde{\\eta} \\in [0, \\infty)$,\n$\\frac{m}{n+m} \\to \\pi \\in [0, 1]$,\n$||\\mu|| = O(1)$.\nRole of the assumptions. The above assumptions are central to understanding the nuances be-tween real and synthetic data (as constructed above) in a high-dimensional regime. Essentially,\n\u2022 Assumptions 1), 2), and 3) define the scaling of data dimension p and the different sam-ple sizes (n real data, m synthetic data, and $\\tilde{n}$ real samples used to train the generative model). In particular, we suppose that all these dimensions scale linearly relative to each other, which corresponds to the classical RMT regime. This setting is more general than the infinite sample size regime in the sense that the former can be recovered by taking $\\eta, \\tilde{\\eta} \\to 0$. Specifically, the parameter $\\tilde{\\eta}$ controls the generative model quality, where lower values indicate better generative model quality. Plus, the parameter $\\pi$ corresponds to the proportion of the real samples in the data mixture. For instance, $\\pi = 0$ models a setting where the training is done only on synthetic samples, and $0 < \\pi < 1$ highlights the fact that the number n of real and m of synthetic samples are of the same order, therefore, making our results scalable to any possible proportion $\\pi$.\n\u2022 The fourth condition about the magnitude of the mean vector $\\mu$ reflects the fact that the classification problem should neither be trivial ($||\\mu|| \\gg 1$) nor impossible ($||\\mu|| \\to 0$) as the dimension of data grows large. For instance, assuming $||\\mu||$ of order $O(\\sqrt{p})$ would not be relevant as $p \\to \\infty$ since the classification problem becomes trivial in this regime. We refer the reader to (Couillet & Benaych-Georges, 2016) for a more general formulation and justifications of this assumption under an extended k-class Gaussian mixture model.\nHaving stated the main assumptions, we are now in place to present our main technical findings on the performance of the classifier model trained on a mixture of real and synthetic data. As a corollary, we also cover the case where the model is trained solely on synthetic data and showcase a generalization of the result obtained by Feng et al. (2024)."}, {"title": "4.1 PARTIALLY SYNTHETIC: MIXTURE OF REAL AND SYNTHETIC DATA", "content": "We start by analyzing the general case of training on a mix of real and synthetic data. As we described in the previous section, the statistics of synthetic data are empirical estimates of the ones of real data. Under Assumption 4.1, the estimation of $\\mu$ with $\\hat{\\mu}$ remains consistent, while the estimation of the underlying real data covariance (i.e., $I$, in our setting) with $\\hat{C}$ is inconsistent as we previously discussed. As a result, studying the theoretical performance of the classifier in equation 6 demands deploying tools from random matrix theory that refines the estimation of scalar quantities depending on large random matrices. In our case, the scalar quantity of interest corresponds to the model's accuracy which depends on the random matrices $\\hat{C}$ and $XX^T$ as per equation 6.\nIn our analysis of the classifier's theoretical performance, we found that the effect of high-dimension (and that of distribution shift between real and synthetic samples) is described by three scalar quan-tities $(\\delta_*, \\delta_\\diamond, \\delta_{\\ddagger})$ which are defined as the unique solution of the following fixed point system which is derived from Lemma D.1 in the Appendix.\n$\\delta_{\\ddagger} = \\frac{\\alpha (1-\\pi) \\tilde{\\eta}}{1+\\delta_{\\ddagger}}$\n$\\delta_\\diamond = \\frac{\\delta_{\\ddagger}}{\\tilde{\\eta}} - \\eta$\n$\\delta_{*} = \\frac{\\alpha \\delta_{\\diamond}}{1 + \\delta_{\\ddagger}} \\left( 1 + \\frac{\\alpha (1-\\pi) \\eta}{\\tilde{\\eta} + \\tilde{\\eta} + (1+\\delta_{\\ddagger})(1+\\delta_{\\ddagger}) \\eta} \\right)$,\nwhere $\\alpha = \\phi (1 - \\epsilon) + \\rho \\epsilon$. These quantities will be used subsequently in our results. Intuitively, $\\delta_{*}$ captures the contribution of real data, $\\delta_\\diamond$ corresponds to the contribution of synthetic data, and $\\delta_{\\ddagger}$ corresponds to the influence of the generative model. In an infinite sample size regime where $\\tilde{n}, n, m, \\tilde{n} \\to \\infty$ while the dimension p is kept fixed, $(\\delta_*, \\delta_\\diamond, \\delta_{\\ddagger}) = (0, 0, 0)$, while under Assumption 4.1 these quantities are non zero yielding a counterintuitive behavior in high-dimension. For convenience, we further define a set of scalar quantities that will prove usefull in the next result.\n$\\bar{\\alpha} = E[q_i] = \\phi (1 - \\epsilon) + \\rho \\epsilon, \\\\ \\bar{\\lambda} = E[q_i y_i] = \\phi \\varphi(1 - \\epsilon) - \\rho \\epsilon,$\n$a = \\frac{\\pi}{1 + \\delta_{*}} + \\frac{\\alpha (1-\\pi)}{1 + \\delta_{\\ddagger}}, \\\\ b = \\gamma + \\frac{\\pi}{1 + \\delta_{*}} + \\frac{\\alpha (1-\\pi)}{1 + \\delta_{\\ddagger}}, \\\\ c = \\frac{\\pi \\lambda(1 - \\pi)}{1 + \\delta_{*}} + \\frac{\\alpha(1 + \\delta_{\\ddagger})}{(1 + \\delta_{\\ddagger})^4 h_2^2},$\n$a_1 = \\frac{\\pi \\eta}{(1 + \\delta_{*})^2 h_1^2},\\\\ b_1 = y + \\frac{\\alpha (1-\\pi) \\eta}{(1 + \\delta_{\\ddagger})^2 h_2^2}, \\\\ b_2 = \\frac{\\alpha (1-\\pi) \\eta}{(1 + \\delta_{\\ddagger})(1 + \\delta_{\\diamond})},\\\\ h_1 = 1 - \\frac{a_1}{b_1}, h_2 = 1 - \\frac{a_1}{b_2},$\nThe first set of parameters $(\\alpha, \\lambda, a, b, c)$ pop out from the expectation of the classifier's decision function while the remaining quantities are related to second-order statistics. Essentially, the main relevant quantities to our analysis are $\\tilde{\\eta}$ and $\\epsilon$ which characterize the quality of synthetic data, with $\\phi$ and $\\rho$ characterizing the verification process. In an idealized scenario, we would have $\\tilde{\\eta} = \\epsilon = 0$ which reflects the fact that there is no distribution shift nor label noise, while $\\phi = 1 - \\rho = 1$ corresponds to a perfect (oracle) verification process. Our main goal is to study how these param-eters influence the classifier's performance hence providing the conditions that make synthetic data relevant for performance boost. The main result brought by this paper is therefore stated as follows."}, {"title": "4.2 FULLY SYNTHETIC: TRAINING ON SYNTHETIC DATA", "content": "In this section, we study the fully synthetic setting which corresponds to training solely on synthetic data (i.e. n = 0 in equation 6). For simplicity, we consider only label noise and ignore feature noise in the synthetic data. Essentially, this allows us to exhibit the smooth phase transition of the classifier's accuracy in terms of label noise, which extends the result of Feng et al. (2024). Specifically, we obtain the following corollary of theorem 4.2.\nCorollary 4.3 (Performance when training only on synthetic data). Let w be the Ridge classifier described in equation 6 trained only on synthetic data with only label noise (i.e., $\\hat{C} = I_p$). Under Assumption 4.1, the decision function $w^T x$, on a test sample $x \\in C_a$ with corresponding label $y = (-1)^a$ and independent of X, satisfies\n$\\overline{w^T x} \\sim N (y \\cdot \\mu_s, \\nu_s - \\mu_s^2)$,\nwhere\n$\\mu_s = \\frac{\\varphi (1 - \\epsilon) - \\rho \\epsilon}{\\alpha ||\\mu||^2 + \\alpha + \\gamma (1 + \\delta_{\\ddagger})} ||\\mu||^2,$\n$\\nu_s = \\gamma + \\frac{\\lambda_s}{h}$ with $h = 1 - \\frac{\\alpha \\eta_s}{(\\alpha + \\gamma (1 + \\delta_{\\ddagger}))^2},$\n$\\delta_{\\ddagger} = \\frac{\\eta_s \\alpha - \\alpha - \\gamma + \\sqrt{(\\alpha + \\gamma - \\eta_s \\alpha)^2 + 4 \\eta_s \\alpha \\gamma}}{2\\gamma},$\n$\\eta_s = \\lim_{p \\to \\infty} \\frac{p}{m}, h = 1 - \\frac{\\alpha \\eta_s}{(\\alpha + \\gamma (1 + \\delta_{\\ddagger}))^2},$\nCorollary 4.3 provides an explicit formulation of Theorem 4.2 with synthetic data only and ignoring distribution shift (yielding an explicit expression of $\\delta_s$). This setting provides a clearer interpretation of the effect of label noise since the classifier's performance is directly re-lated to the quantity $\\lambda = \\varphi (1 - \\epsilon) - \\rho \\epsilon$. The breaking point of the classifier's performance occurs at $\\lambda = 0$, which corresponds to the ac-curacy of random guessing, yielding to the crit-ical value of label noise $\\epsilon^* = (1 + \\frac{\\phi}{\\rho})^{-1}$. This critical value is equivalent to the one obtained by Feng et al. (2024), however, we extend their result to the high-dimensional setting which ex-hibits a smoother phase transition Essentially, the sharp phase transition of Feng et al. (2024) is covered by our result by taking $\\eta_s \\to 0$. In this sense, the predicted smooth transition better mirrors real-world sce-narios where finite sample sizes introduce grad-ual changes in performance rather than abrupt shifts. This makes our theoretical findings more applicable and reliable for practical scenarios."}, {"title": "5 EXPERIMENTS", "content": "In this section, we present our experiments conducted on different real-world tasks and datasets in order to illustrate our theoretical findings presented in the previous section."}, {"title": "5.1 EXPERIMENTAL SETTINGS", "content": "Amazon Reviews. We use the Amazon Reviews datasets (Blitzer et al. (2007)) which include several binary classification tasks corresponding to positive versus negative reviews of books, electronics and kitchen. We apply the standard scaler from sklearn (Pedregosa et al., 2011) and estimate $||\\mu||$ with the normalized data. The synthetic data is generated following the described generative scheme (see equation 2). We use the Ridge classifier in equation 6 for this data.\nMNIST. We also conducted experiments on the MNIST (LeCun & Cortes (2010)) dataset to illus-trate our theoretical insights, by training a simple neural network with one-hidden layer and ReLU activation function. Concerning the synthetic data, we used different values of $\\tilde{n}$ to generate new samples in order to highlight the importance of the generation quality, and introduced a label noise $\\epsilon$ to emphasize on the importance of the pruning. Figure 5 shows some examples of MNIST-like synthetic data that has been generated and used in our experiments.\nLLM Safety Alignment. We also investigated the impact of synthetic text data for the task of alignment of LLMs with direct preference optimization on safety datasets, using the same approach as in (Alami et al., 2024). We finetune the Falcon 2-11B Instruct model Malartic et al. (2024) on $\\tilde{n} = 5000$ human data from the Anthropic's HH-RLHF dataset, which correspond to real data, while synthetic data are extracted from the PKU safe RLHF dataset which are generated using Alpaca3-70B. We increase the amount of synthetic data by injecting gradually five batches of 7000 samples per batch, to study the performance of the fine-tuned model as we add more synthetic data. In this experiment, we focus only on label noise by randomly perturbing the synthetic dataset. Each entry from the synthetic dataset includes a prompt $x^{(i)}$, a safe response $y_s^{(i)}$ (safety-accepted response), and a less safe response $y_{sl}^{(i)}$ (safety-rejected response). We, therefore, perturbed this dataset by swapping safe and less safe responses with a probability $\\epsilon$ (label noise), and selecting the prompts according to a verifier of parameters $\\rho$ and $\\phi$ described earlier in this paper.\nFor the evaluation, we use the ALERT dataset (Tedeschi et al. (2024)) to test the safety of responses of the finetuned model after being judged by LLama-Guard-3-8B (Dubey et al., 2024). As in (Alami et al., 2024), we compute the safety score as the percentage of safe answers labeled by Llama-Guard-3-8B. We report the results in figure 8 for strong supervision $(\\rho, \\phi) = (0.2, 0.9)$ and weak supervision $(\\rho, \\phi) = (0.5, 0.5) for both $\\epsilon = 0.1$ and $\\epsilon = 0.5$.\nLLM Q&A Safety Generation. This experiment aims to evaluate the impact of synthetically generated prompts (i.e. feature noise). To construct the generative model for this experiment, we fine-tune an LLM (M) with supervised fine-tuning (SFT) on pairs of question-answer (Q&A) sen-tences extracted from a safety dataset. Initially, we fine-tune M on 12k human annotated Q&A as safe or unsafe (Ji et al., 2024), yielding a fine-tuned model on human data denoted as Mh. Then, Mh is considered as the generative model to generate a large dataset of synthetic Q&A prompts"}, {"title": "5.2 EFFECT OF LABEL NOISE", "content": "Figures 6, 7 (left plot) 8 reflect the effect of label noise. Essentially, as theoretically anticipated, the trained models do not benefit from synthetic data unless it is accurately verified. Specifically, in the case of weak supervision, model performance drops significantly, and the improvement from using synthetic data is only visible with very high synthetic sample sizes. On the contrary, with strong supervision, we observe a monotonous performance boost as the proportion of synthetic data increases."}, {"title": "5.3 EFFECT OF FEATURE NOISE", "content": "In this section, we discuss the experiments related to feature noise. In Fig. 7 (right), we depict the performance of a one-hidden layer MLP trained on a mix of real and synthetic MNIST data following our theoretical framework. As we can observe from the figure, the performance boost from synthetic data heavily depends on the generative model quality as predicted by our theoretical results. We further observe the same trend using LLMs as depicted in Fig. 9, where we observe that the synthetic data generated by Llama3.1-8B-Instruct yields a better performance boost compared to Gemma-2-2B-it as we increase the amount of synthetic samples."}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "In this work, we conducted a comprehensive theoretical and empirical analysis of models trained on a mixture of real and synthetic data with verification. By leveraging random matrix theory, we identified critical factors such as distribution shifts and label noise that significantly impact model performance. Our findings demonstrate that synthetic data can enhance model accuracy under spe-cific conditions, particularly when the generative model is of high quality and the verification process is accurate. Additionally, we extended previous research by showing that performance transitions are smooth rather than sharp when synthetic data is incorporated in high-dimensional settings.\nDespite these advancements, our current setting is limited to label verification of synthetic data. Incorporating feature verification represents a promising extension for future research, which could provide further insights into the reliability and effectiveness of synthetic data in model training. Another possible extension of our work is to study distributions beyond the Gaussian model and analyze how higher-order statistics can be incorporated into our current framework.\nIn conclusion, this work provides a foundational understanding of the conditions under which syn-thetic data can be beneficial for model training in high-dimensional settings. By integrating both theoretical insights and empirical validations, this study provides new insights into the effective utilization of synthetic data, paving the way for more resilient and performant AI models."}, {"title": "APPENDIX", "content": null}, {"title": "A USEFUL LEMMAS", "content": "Notation: For $a \\in \\{1,2\\}$, we denote by $I_a = \\{i | x_i \\in C_a\\}$, i.e, the set of indices of vectors belonging to class $C_a$. Furthermore, we denote $\\Sigma = \\mu \\mu^T + I_p = E[x x^T]$ for $x \\in C_a$, and $\\Sigma_{\\beta} = \\mu_{\\beta} \\mu_{\\beta}^T + C$\nHere we will list the most useful lemmas and results used in our analysis."}, {"title": "A.1 GENERAL LEMMAS", "content": "Lemma A.1 (Inverse identity). For invertible matrices A and B, we have that:\n$A^{-1} - B^{-1} = A^{-1}(B - A)B^{-1}$\nLemma A.2 (Woodbury). For $A \\in \\mathbb{R}^{p \\times p}, U, V \\in \\mathbb{R}^{p \\times k}$, such that both A and $A + UV^T$ are invertible, we have:\n$(A + UV^T)^{-1} = A^{-1} - A^{-1}U (I_k + V^T A^{-1}U)^{-1} V^T A^{-1}$\nA particular case of this lemma A.2, in the case of k = 1, is called Sherman-Morisson's identity.\nLemma A.3 (Sherman-Morisson). For $A \\in \\mathbb{R}^{p \\times p}$ invertible and $u, v \\in \\mathbb{R}^p$, $A + uv^T$ is invertible if and only if : $1 + v^T A^{-1}u \\neq 0$, and:\n$(A + uv^T)^{-1} = A^{-1} - \\frac{A^{-1}u v^T A^{-1}}{1 + v^T A^{-1} u}$\nBesides,\n$(A + uv^T)^{-1} u = \\frac{A^{-1} u}{1 + v^T A^{-1} u}$"}, {"title": "A.2 DETERMINISTIC EQUIVALENTS", "content": "Let us state here the deterministic equivalent of the resolvent matrix Q defined in the general model's equation (6) for any general covariance matrix C and mean $\\mu_{\\beta} = \\beta \\mu + \\mu^{\\perp}$ that define the statistic of the synthetic data, as in equation 13.\nLemma A.4 (Deterministic equivalent of Q). Under the 4.1 assumptions listed above in the main paper, a deterministic equivalent for $Q = Q(\\gamma)$, denoted $\\overline{Q}$, is given by:\n$\\overline{Q} = \\left( \\frac{\\pi (\\mu \\mu^T + I_p)}{1 + \\delta} + \\frac{\\alpha (1 - \\pi) (\\mu_{\\beta} \\mu_{\\beta}^T + C)}{1 + \\delta_s} + \\gamma I_p \\right)^{-1}$\nwhere:\n$\\pi = \\frac{n}{n+m}, \\quad \\alpha = \\varphi(1 - \\epsilon) + \\rho \\epsilon, \\quad \\delta = \\frac{\\pi}{N} Tr(\\overline{Q}), \\quad \\delta_s = \\frac{\\alpha}{N} Tr(C\\overline{Q})$\nProof. We want to find $\\overline{Q}$ such that for all bounded $a, b \\in \\mathbb{R}^p$:\n$a^T (E[Q] - \\overline{Q})b \\to 0$\nLet $\\overline{Q} = (S + \\gamma I_p)^{-1}$. We want to determine an S that satisfies the above property. We have that:\n$E[Q] - \\overline{Q} = E[Q(S - \\frac{1}{N} VV^T)\\overline{Q}]$ (lemma.A.1)\n$= E[Q(S - \\frac{1}{N} VV^T)\\overline{Q}] = E[Q(S - \\frac{1}{N} \\sum_{i=1}^N v_i v_i^T )\\overline{Q}]$\n$= \\frac{1}{N} \\sum_{i=1}^N E[Q(S - v_i v_i^T )\\overline{Q}] = \\frac{1}{N} \\sum_{i=1}^n E[Q(S - x_i x_i^T )\\overline{Q}] + \\frac{1}{N} \\sum_{i=1}^m E[Q(S - q_i \\tilde{x}_i \\tilde{x}_i^T )\\overline{Q}]$\n$= \\frac{1}{N} \\sum_{i=1}^n \\frac{\\eta}{1 + \\delta} E[Q(\\frac{1}{N} - x_i x_i^T )\\overline{Q}] + \\frac{1}{N} \\sum_{i=1}^m \\frac{q_i \\overline{q_i}}{1 + \\delta_s} E[Q(\\frac{1}{N} - \\tilde{x}_i \\tilde{x}_i^T )\\overline{Q}]$\n$= \\frac{\\pi}{1 + \\delta} E[Q(S - \\sum \\frac{\\pi}{1 + \\delta})\\overline{Q}] + \\frac{1 - \\pi}{1 + \\delta_s} E[Q(\\frac{1}{N} - \\overline{q_i \\tilde{x}_i \\tilde{x}_i^T })\\overline{Q}] + O(N^{-1})$\n$\\overline{q_i}= \\frac{1- \\pi}{\\pi}$"}, {"title": "4.2 FULLY SYNTHETIC: TRAINING ON SYNTHETIC DATA", "content": "We start by analyzing the general case of training on a mix of real and synthetic data. As we described in the previous section, the statistics of synthetic data are empirical estimates of the ones of real data. Under Assumption 4.1, the estimation of $\\mu$ with $\\hat{\\mu}$ remains consistent, while the estimation of the underlying real data covariance (i.e., $I$, in our setting) with $\\hat{C}$ is inconsistent as we previously discussed. As a result, studying the theoretical performance of the classifier in equation 6"}]}