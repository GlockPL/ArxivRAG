{"title": "Communication is All You Need: Persuasion Dataset Construction via Multi-LLM Communication", "authors": ["Weicheng Ma", "Hefan Zhang", "Ivory Yang", "Shiyu Ji", "Joice Chen", "Farnoosh Hashemi", "Shubham Mohole", "Ethan Gearey", "Michael Macy", "Saeed Hassanpour", "Soroush Vosoughi"], "abstract": "Large Language Models (LLMs) have shown proficiency in generating persuasive dialogue, yet concerns about the fluency and sophistication of their outputs persist. This paper presents a multi-LLM communication framework designed to enhance the generation of persuasive data automatically. This framework facilitates the efficient production of high-quality, diverse linguistic content with minimal human oversight. Through extensive evaluations, we demonstrate that the generated data excels in naturalness, linguistic diversity, and the strategic use of persuasion, even in complex scenarios involving social taboos. The framework also proves adept at generalizing across novel contexts. Our results highlight the framework's potential to significantly advance research in both computational and social science domains concerning persuasive communication.", "sections": [{"title": "Introduction", "content": "Persuasion techniques play a critical role in shaping societal behaviors and public opinion (Fogg, 2009; Braca and Dondio, 2023), which has led to sustained interest across a range of disciplines. Social science research has established detailed taxonomies of persuasion strategies (Shrum et al., 2012; Lukin et al., 2017), while datasets have been developed to cover various domains, including charitable donations (Wang et al., 2019), argument ranking in debates (Toledo et al., 2019), detecting mental manipulation (Wang et al., 2024; Yang et al., 2024), and understanding advertising strategies (Kumar et al., 2023). Despite these advances, ambiguities persist in defining persuasion (Pauli et al., 2022), and applying persuasion strategies across different contexts remains complex (Bai et al., 2021; Schaefer et al., 2023; Piskorski et al., 2023). Additionally, the high cost of manually annotating quality data poses a significant challenge (Lai et al., 2022).\nThe advent of large language models (LLMs) has unlocked new possibilities for enhancing various forms of communication, including online political discourse (Argyle et al., 2023; Bai et al., 2023), personalized advertising (Matz et al., 2024; Meguellati et al., 2024), public health messaging (Lim and Schm\u00e4lzle, 2023; Espinosa and Salath\u00e9, 2024), and opinion shaping on social media (Meier, 2024). Recent research, such as that by Jin et al. (2024), has begun exploring LLM-generated persuasive dialogues. However, their approach is limited to simple, two-party dialogues where a persuader seeks to change the persuadee's viewpoint. These dialogues often lack depth, presenting brief exchanges with simplistic logic and unnatural flow, restricting their usefulness for studying persuasion in more complex settings.\nIn response to these limitations, we propose a multi-agent framework for generating persuasion data. In this framework, multiple agents are assigned distinct roles, ensuring that each aspect of the dialogue generation process is handled efficiently. This structure minimizes the risk of an agent missing important details due to task abstraction or prompt complexity, a common issue in LLM prompting (Brown et al., 2020; Huang et al., 2023). Additionally, auxiliary agents manage dialogue flow to ensure that the resulting exchanges are coherent, logically consistent, and incorporate diverse persuasive strategies, simulating natural human conversation. Our approach imposes no preconditions regarding speakers, language styles, domains, or persuasion strategies, allowing it to generate a wide range of dialogues. For instance, our framework can support adversarial dialogues, where both participants attempt to persuade one another while maintaining their original positions. Moreover, we employ a continuous labeling scheme to measure the degree of perspective change throughout the dialogue, avoiding the limitations of binary utterance labels. This framework also integrates ethical"}, {"title": "Multi-Agent Data Generation &\nAnnotation Framework", "content": "Our framework incorporates 6 groups of language agents as shown in Figure 1. In our experiments, all agents utilize a GPT-3.5 backbone, except for the utterance quality monitor and global regulation agents which are based on GPT-4 due to their need for advanced reasoning capabilities and enhanced memory retention. Note that this choice of LLMs aims to balance data generation costs with quality, and using more powerful models could further improve the effectiveness of our approach. Our preliminary experiments on model selection are outlined in Appendix A."}, {"title": "Dialogue Generation Agents", "content": "We adopt a methodology for generating multi-round conversations by cyclically using the output from one language agent as the input for another (Park et al., 2023). This technique has been validated to produce extended, logically consistent dialogues that fulfill our project requirements.\nOur framework initializes the generative agents with a description of the task settings, the predefined tasks for each language agent, and guidelines governing the models' generations, as illustrated in Figure B1. The task choices for each agent are not constrained, for instance, drawing on a cultural taboo that \"one should not pick flowers in a cemetery\" from NormBank, we could challenge the persuader to convince the persuadee to pick flowers in a cemetery, while the persuadee is instructed to resist and, if possible, persuade the persuader to abandon such thoughts.\nThe dialogues commence when we prompt a persuader agent with \u201cStart the conversation.\u201d This setup initiates a structured yet dynamic interaction between the speakers, allowing us to closely observe and analyze their persuasive strategies."}, {"title": "Utterance Quality Monitor Agent", "content": "Due to the inherent limitations of LLMs, dialogue generation agents may occasionally produce incomplete, repetitive, or off-topic content. To address these issues, we introduce a specialized LLM agent responsible for tracking the persuasion topic and generation history to evaluate new generations.\nThe initialization prompt of the utterance quality monitor agent is shown in Figure B2. During dialogue generation, this agent inspects every new utterance to check if they ends unexpectedly, repeats a previous utterance, or goes off the topic of the dialogue in a sequence. If an utterance is red-flagged for any issue, the author agent is requested to revise the utterance based on the diagnoses. Otherwise, before proceeding to the next utterance, the utterance quality monitor agent is prompted to update its memory, storing the reviewed utterance for future judgments."}, {"title": "Language Refinement Agent", "content": "Raw text produced by dialogue generation agents often adopts a conclusive rather than conversational tone, primarily because the agents are prompted in a question-answering format. This could lead to stylistic conflicts with surrounding utterances. Additionally, the generations frequently include tone-softening phrases like \u201cI understand your concerns,\" or unnecessary affirmations such as agreeing with the other speaker's views, which dilute the strength of arguments. Over the course of the conversation, these issues can compound, leading to dialogues dominated by language softeners and lacking in persuasive content.\nTo address this issue, we adopt a language refinement agent tasked with stripping out polite but superfluous phrases, thereby sharpening the dialogue's focus on substantive content. System message to this agent is shown in Figure B3. 2 examples are also provided to the agent to further regulate its behaviors. Subsequent operations, including continued dialogue generation and persuasiveness labeling, are predicated on the output from the language refinement agent, ensuring that the conversation maintains its relevance and effectiveness in conveying persuasive arguments."}, {"title": "Persuasiveness Annotation Agent", "content": "After generating each round of conversation, we employ a persuasiveness annotation agent to assess the extent of perspective shifts in each speaker, assigning a score ranging from 0 to 1. Figure B4 illustrates the system message fed to the persuasiveness annotation agent before the generation starts. In practice, we provide the annotation agent with two scoring examples to guide its behavior and minimize scoring errors, such as incorrectly assigning a score of 1 to a conversation round with no perspective shifts (Figure 11). Note that these scores reflect the cumulative viewpoint shifts across all prior rounds of communication, facilitating the analysis of gradual persuasion rather than focusing solely on the impact of a single utterance."}, {"title": "Global Regulation Agent", "content": "We employ a global regulation agent to ensure smooth logical flow in the generated conversations and to determine the appropriate time to conclude the dialogue. The system message to the global regulation agent is depicted in Figure B5.\nAfter each round of utterances is generated and annotated, we prompt this agent to verify whether any changes in each speaker's perspectives are logically influenced by the preceding utterance and whether the newly generated utterances avoid repeating previously used strategies within the same conversation. If the logical connections are insufficient or no new persuasive attempts are made, the dialogue generation agents are asked to revise their responses based on feedback from the global regulation agent. Once the revised generation passes these checks, the agent's internal memory is updated accordingly. Then the agent is prompted to assess whether the speakers have reached a mutual agreement or if no new information is likely to be introduced next, indicating that the dialogue should be concluded. Although the ideal conclusion involves the persuader and persuadee agreeing on the preset task, conversations can often devolve into repetitive and unproductive arguments (Figure I2) (Xu et al., 2022). To prevent such stagnation, we allow the dialogue to conclude even if complete agreement is not reached. The global regulation agent is responsible for determining when to end the dialogue, at which point the conversation is terminated and the agent's memory is reset."}, {"title": "Postprocessing Agent", "content": "After generating and annotating a full dialogue, we use a postprocessing agent to enhance content smoothness and naturalness. As shown in Figure B6, the agent removes redundant language, improves logical flow, and enhances language diversity. It also merges labels and reassigns them to modified utterances if the number of dialogue rounds changes."}, {"title": "Data Quality Assessment", "content": "To evaluate our data generation framework, we constructed a small dataset of 200 dialogues using randomly selected norms from NormBank for human validation. These norms consist of 98 taboos, 76 normal behaviors, and 26 expected behaviors. We intentionally placed greater emphasis on taboos because these behaviors often conflict with widely accepted moral standards, causing LLMs to refuse to generate persuasive dialogues (Figure C1). As such, they present a unique challenge in persuasion scenarios for both humans and LLMs.\nOur data assessment plan focuses on three key aspects, progressing from more specific to broader levels of analysis: (1) the language fluency of individual utterances, (2) the the topic, semantic, and logical coherence of entire conversations, and (3) the language and strategy diversity of conversations generated under the same topic and context."}, {"title": "Utterance-Level Quality Assessment", "content": "A critical goal for our framework is that each generated utterance should closely resemble a human-written sentence. To validate this, we conduct (a) a quantitative annotation task to differentiate between model-generated sentences and human-rewritten sentences, followed by (b) a qualitative error analysis that combines annotator feedback with insights from an LLM on sentences that multiple annotators agreed were distinguishable."}, {"title": "Quantitative Differentiation Task", "content": "The differentiation task aims to assess how accurately human annotators could tell model-generated sentences apart from those rewritten by humans. Similar tasks have been discussed in Gehrmann et al. (2019), Ippolito et al. (2020) and Ma et al. (2023). For our evaluation, we obtained a stratified sample of 400 utterances from 150 random sample dialogues to ensure equal representation of utterances from both the persuader and persuadee agents, covering different rounds of persuasion to reflect the dataset distribution."}, {"title": "Dialogue Smoothness and Naturalness", "content": "We further conduct dialogue-level analyses on our sample data to ensure that each generated dialogue is logically coherent and effective in persuasion."}, {"title": "Dialogue Quality Annotation", "content": "We first developed a systematic rubric for evaluating the overall quality of persuasive dialogues. Our evaluation is conducted on (a) the local level, which examines each argument-response pair between the speakers, and (b) the global level, which considers the conversation as a whole. Evaluations are based on existing human evaluation dimensions for open dialogue systems and emphasize three key aspects: the interaction between persuader and persuadee, the consistency of individual participants across multiple rounds, and the alignment of utterances with the topic. Detailed criteria and their references are outlined in Table 2.\nThe local (round-level) evaluation focuses on 2 conventional dimensions in dialogue systems: Coherence and Informativeness. Coherence refers to round-level logical consistency, i.e., speakers respond to each other in a manner appropriate to commonsense and the given context (Li and Sun, 2018; Young et al., 2018; Wu et al., 2019; Liang and Li, 2021). Informativeness measures the quality and progression of information, ensuring responses"}, {"title": "Qualitative Error Analysis", "content": "Despite high overall performance, the dialogues received lower scores regarding introducing new information (2.337 out of 3) and maintaining naturalness (2.561 out of 3). Based on annotator feedback, we identified the following common issues that explain these lower scores:\nArgument repetition. A most common error is argument repetition, where speakers restate the same points over multiple rounds of conversation with only slight variations in phrasing. \nFormalized Language. Another common issue"}, {"title": "Strategy Diversity", "content": "One advantage of our framework is its ability to generate diverse persuasion dialogues across various topics and contexts by adapting its persuasion strategies to suit each context. Ideally, the model should also be able to vary its strategies within the same context across different replicates.\nTo evaluate diversity across and within the same context, we identified 9 persuasive strategies based on existing literature (see Table 3 for a full list of techniques and references) and designed a detailed human annotation task. The persuasion strategies are categorized into 5 groups, as outlined by Anand et al. (2011). External validity involves appeals to external authority or expertise, or using"}, {"title": "Discussion", "content": "This section presents generations of our framework in strategy-controlled and multi-party dialogues to show its flexibility and generalizability."}, {"title": "Strategy-Controlled Data Generation", "content": "While our framework does not require designating persuasion strategies before utterance generation, incorporating a specific strategy as an optional input is shown to enhance the diversity of strategy selection without disrupting the framework's performance. This underscores its flexibility and customizability to meet user requirements."}, {"title": "Multi-Party Persuasion Data Generation", "content": "Our framework is not constrained to generating dialogues between 2 parties either. Enabling our framework to generate multi-party dialogues requires only minor adjustments including initializing 3 dialogue generation agents and instructing the global regulation agent to prevent repetition or conflict among agents on the same side. This further demonstrates the flexibility and generalizability of our framework, making it a powerful tool not only for model interpretation and training but also for broader persuasion-related studies involving human interactions."}, {"title": "Conclusions", "content": "This paper introduces a fully automated framework for generating persuasive dialogues, designed to address the lack of data in persuasion-related research. Leveraging this framework, we generated 200 sample dialogues based on scenarios from NormBank and validated them for language fluency, logical coherence, and the diversity of persuasion strategies. The results highlight our framework's ability to produce high-quality dialogues that follow human instructions. Additionally, we demonstrated its flexibility in handling controlled persuasion strategies and its adaptability to more complex, multi-party conversations. This framework offers significant potential for advancing persuasion research in both computer science and social sciences domains."}, {"title": "Limitations", "content": "This paper introduces a pioneering approach that employs multiple LLM agents within the same environment to generate synthetic data for analyzing persuasion tactics. Although our LLMs did not fully replicate all previously studied persuasion techniques, leaving some gaps in our dataset's coverage, the strengths of this method are significant. Our dataset provides extensive scalability and versatility in scenario and target action settings, offering a more robust foundation for persuasion-related research than currently available datasets.\nDespite these limitations, our approach's inherent flexibility and expandability underscore its significant potential. As LLM technology advances, our method's ability to encompass a broader range of persuasion techniques will likely improve. This evolution is expected to further enhance the value of our approach in the field of persuasion research, emphasizing its long-term relevance and adaptability.\nAdditionally, while our dataset was generated only in English, the proposed framework can be easily adapted to other languages supported by LLM agents with minimal modifications to the prompts."}, {"title": "Ethics Statement", "content": "Our dataset construction approach is designed to deepen the understanding of persuasion techniques and aid in identifying and mitigating malicious uses of persuasion. However, we recognize the potential risk that our approach could be misused to refine online misinformation or propaganda. Specifically, the information-based persuasion techniques demonstrated in our dataset could be exploited by malicious entities to present or distort information selectively. This manipulation could mislead individuals about specific actions' true risks or benefits, potentially leading to more deceptive advertisements. Additionally, there is a risk that our framework could be used to pre-test the effectiveness of misinformation or propaganda strategies before they are broadly released (French, 2024).\nDespite these risks, it is important to highlight that recent advancements in large language models include robust moderation mechanisms (Kumar et al., 2024). These mechanisms are designed to prevent the models' use for harmful purposes, thus protecting our approach from being exploited to deceive individuals or spread misinformation. Our experiments' queries with immoral or unethical intentions predominantly resulted in unsuccessful persuasion attempts. This demonstrates the relative safety of our proposed framework and provides valuable insights into the limitations of these techniques.\nMoreover, a deeper understanding of persuasion techniques can offer essential tools for countering malicious uses of these strategies. This underscores the importance of our research, especially in an era of misinformation and propaganda. Our work contributes significantly to the field by improving the ability to discern and mitigate the impact of persuasive strategies used in harmful ways.\nRegarding human annotators, our data quality validations are expertly managed by NLP and social science specialists due to the complexity of the task. As discussed in Section 3, all annotators undergo thorough training to ensure they fully understand the task. For clarity, the complete set of instructions provided to the annotators and auxiliary validation LLMs is available in Appendix D. All the annotators who are not co-authors of this paper are compensated at a rate of $15 per hour, which is above the minimum hourly wage in the U.S.\nFinally, we have submitted a sample of 10 randomly generated dialogues as supplementary material. The full code for our data generation framework, along with all dialogues generated for validation, will be made publicly available to support further research in this area."}, {"title": "Model Selection for Agents", "content": "In selecting the backbone models for each agent in our framework, we conducted extensive evaluations across several major LLMs, including GPT-3.5 (GPT-3.5-Turbo), GPT-4 (GPT-4-0613), GPT-40 (GPT-40-2024-08-06), and Claude 3 (Claude-3-Sonnet). As shown in Figure A1, using GPT-3.5 for all agents tends to produce overly brief, question-answer-style responses, while GPT-40 (Figure A3) often goes off-topic and generates irrelevant utterances, making it unsuitable for our needs.\nIn contrast, GPT-4 (Figure A2) performs well, generating dialogues that are fluent in language, coherent in logic, and adept at employing persuasion strategies. Claude 3 also shows promise, particularly in generating multi-round conversations based on expected behaviors from NormBank. However, it adheres to stricter ethical rules and consistently refuses to generate persuasive text for taboo norms. \nBased on these preliminary experimental results, we opted for a combination of GPT-3.5 and GPT-4 in our framework to balance performance and cost. However, using GPT-4 exclusively, or other more advanced LLMs in the future, could potentially yield even better results."}]}