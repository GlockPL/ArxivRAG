{"title": "Using Language Models to Disambiguate Lexical Choices in Translation", "authors": ["Josh Barua", "Sanjay Subramanian", "Kayo Yin", "Alane Suhr"], "abstract": "In translation, a concept represented by a single word in a source language can have multiple variations in a target language. The task of lexical selection requires using context to identify which variation is most appropriate for a source text. We work with native speakers of nine languages to create DTAILS, a dataset of 1,377 sentence pairs that exhibit cross-lingual concept variation when translating from English. We evaluate recent LLMs and neural machine translation systems on DTAILS, with the best-performing model, GPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use language models to generate English rules describing target-language concept variations. Providing weaker models with high-quality lexical rules improves accuracy substantially, in some cases reaching or outperforming GPT-4.", "sections": [{"title": "1 Introduction", "content": "Resolving ambiguity in translation is a fundamental challenge (Weaver, 1952) that remains unsolved (Campolungo et al., 2022). This paper focuses on lexical selection, a key aspect of translation that requires using context in a source sentence to determine the best translation for an ambiguous source word from several target-language options. Our work has two main goals. First, we investigate the capabilities of language models in disambiguating lexical choices in translation by comparing instruction-tuned language models with high-performing neural machine translation systems. Second, we test whether language models can be used to extract useful natural language rules that accurately describe how to translate ambiguous words based on source-side context.\nWe work with native speakers to introduce the Dataset of Translations with Ambiguity in Lexical Selection (DTAILS), a test set of 1,377 sentence"}, {"title": "2 Task and Data", "content": "To evaluate a model's ability to understand concept variations, we study lexical selection in translation. For example, the noun date has multiple lexical variations in Farsi, which distinguishes variations by fruit ripeness and dryness. We collect a dataset of translation pairs that require understanding and appropriately applying concept variation, where sufficient context is provided to distinguish between variations.\nLexical Selection In translation, lexical selection is the task of selecting the most appropriate lex-eme in the target language that maps from a single lexeme in the source language, in the context of a source sentence (Apidianaki, 2009). Formally, let (x, y) be a sentence pair where x = (x1,...,x|x|) is a sequence of words in the source language and Y = (y1,...,Y|\u1ef9|\u3009 is its translation in the target language. For a source word xi, we define the set of possible translations of xi as \u0113 = (U1,..., \uc774\ud2f0) wherej such that vj \u2208 \u1ef9. The task of lexical selection is to identify the most appropriate translation vj conditioned on the source sentence .\nSource Data Despite the existence of large-scale datasets for low-resource languages through bitext mining techniques (Schwenk et al., 2021), we focus on datasets curated by human translators to mitigate the potential for incorrect translations due to misalignment. We use OpenSubtitles (Lison and Tiedemann, 2016; Lison et al., 2018), TED2020 (Reimers and Gurevych, 2020), PMIndia (Had-dow and Kirefu, 2020), and TEP (Pilehvar et al., 2011) to acquire parallel data for English paired with 7 low-resource and 2 high-resource languages (Japanese and Farsi).\nExpert Recruitment We work with bilingual speakers to ensure our methods and data faithfully represent the processes associated with translation under concept variation. For each language, we recruit from Prolific three annotators who are fluent English speakers and native speakers of the target language."}, {"title": "2.1 Identifying Concepts with Variations", "content": "We first identify concepts that are represented as a single word in our source language (English) but have several variations in a target language. We build upon Chaudhary et al. (2021)'s approach to identify fine-grained lexical distinctions that arise due to concept variation. Given a parallel corpus, we lemmatize all words using Stanza (Qi et al., 2020) and compute word alignments for each sentence pair with the AWESOME aligner (Dou and Neubig, 2021). Using these alignments, we create a one-to-many mapping from source-language lexemes to target-language lexemes. Lastly, we remove source lexemes that do not map to at least two target lexemes, exhibit low entropy, or correspond to target lexemes that arise due to polysemy.\nWe also perform comprehensive analysis of this approach's precision and recall in identifying target-language variations of concepts. All three expert annotators for each language provide feedback on the extracted variations, including whether each variation matches the meaning of the English lex-eme (for computing precision) and whether any key variations are missing from the set (for computing recall). Precision is measured as the proportion of accurate variations; recall is measured as the proportion of concepts with all key variations re-covered. In general, the precision of identified variations is very high, even for low-resource lan-"}, {"title": "2.2 Dataset Construction", "content": "Our goal is to collect a dataset of sentence pairs that require understanding target-language concept variation for accurate translation. Expert annotators help us curate this dataset by performing the lexical selection task, provided only source-language sentences and target lexemes.\nAll annotators for a given language are presented with the same set of concepts and source-language sentences. We shuffle the order in which concepts, sentences, and target lexemes are shown to each annotator. Our resulting dataset, DTAiLS, includes sentences for which the majority of annotators se-lected the variation used in the original sentence pair, which indicates that there is sufficient con-text for consistent lexical selection. Although there could be cases of context-dependent translation where there isn't a single optimal lexical variation, for our dataset we rely on majority agreement to select examples that are clearly evaluable."}, {"title": "3 Rules for Lexical Selection", "content": "We experiment with generating human-readable English rules that describe all extracted concepts and their variations, and analyze how these rules influence model performance on lexical selection when provided as input.\nFor each target-language variation, we find the 50 longest source-language sentences with less"}, {"title": "4 Experiments", "content": "We evaluate three instruction-tuned models, GPT-4, LLaMA-3-8B-Instruct, and Gemma-1.1-7B-IT; and two high-quality NMT models MADLAD-400-10B-MT (Kudugunta et al., 2023) and NLLB-200-3.3B (NLLB Team et al., 2022), sampling with temperature of 0.\nWe hypothesize that models performing lexical selection will benefit from access to rules describing concept variations (Section 3). Thus, we eval-uate instruction-tuned LLMs in 3 settings: (1) no rules provided, (2) with self-generated rules, and (3) with rules generated by GPT-4. For each setting, we instruct the model to explain its reasoning prior to selecting a lexical choice (Wei et al., 2022).\nAccuracy is measured as the proportion of sen-tences for which the model selects the correct lexi-"}, {"title": "5 Related Work", "content": "The most closely related work is by Chaudhary et al. (2021), who study lexical selection for En-glish to Spanish and Greek. They present a pipeline for collecting lexical selection data, train models to perform lexical selection, use a linear SVM model to extract features as interpretable rules, and evaluate the efficacy of these rules in a second-language acquisition setting. In contrast, we use modern LMs, generate natural language rules, and evaluate on several low-resource languages. We also curate a test set for lexical selection validated by native speakers of our target languages."}, {"title": "6 Conclusion", "content": "We introduce DTAILS, a dataset of 1,377 sentence pairs with 9 language pairs that exhibit ambiguity in translation due to concept variation. Using this dataset, we evaluate 3 popular instruction-tuned LLMs and 2 high-performing NMT systems on the task of lexical selection. Out of nine languages tested, the strongest LLM outperforms the NMT systems on three languages, has comparable per-formance on four languages, and fall short of these systems on two languages. No model is able to disambiguate the full set of sentences that native speakers can.\nWe also present a simple approach to extract high-quality rules from language models, demon-strating improvements on lexical selection when LMs are given access to rules. We find that pro-viding weaker open-weight models with rules from a stronger LLM can effectively bridge the gap to or even outperform the stronger model for several languages. Future research could investigate ad-ditional applications of lexical rules in NMT and assess how these human-readable rules can assist L2 learners in vocabulary acquisition."}, {"title": "Limitations", "content": "Because our focus is on low-resource languages, the parallel corpora we use are small; thus, we are only able to extract roughly 20 concepts for six out of nine languages. Further, due to the time and effort required to collect human judgements on lexical selection, our test sets curated by experts are just 120-180 examples per language and 1,377 examples overall. Developing automated methods for example selection is an interesting direction for future work that will enable larger-scale evalu-ation. We also note that the recall of the pipeline for identifying concepts with variations might be inaccurate due to the challenges annotators face brainstorming all possible variations in the seman-tic space. Lastly, due to a lack of available models for WSD, dependency parsing, and POS tagging for low-resource languages, we are only able to evaluate on language pairs where English is the source language. In theory, the methods we present can work for any arbitrary language pair."}, {"title": "A Additional Annotation Details", "content": "A.1 Expert Annotation\nThe filters applied for participants before joining include (1) highest level of degree earned as tech-nical/community college or above and (2) fluency in English and native proficiency in one of the nine languages we study. First, a pilot study was con-ducted to vet annotators for fluency and compre-hension of the task. We then published two studies to the group of annotators who qualified from the pilot study. The first study required annotation of the rules generated by GPT-4 and feedback on concepts and lexical variations extracted by the pipeline (Section 2.1) for computing precision and recall. The interface for the first study can be found in Figure 3. The second study required annotators to complete the lexical selection task. The interface for the second study can be found in Figure 4. We ensure that the same annotator doesn't participate in both studies. We remove all personal identifying information from all data collected. Prior to taking part in any study, annotators were informed of the purpose of the study and how their data would be used.\nA.2 Example Selection\nCollecting human judgments of lexical selection for all parallel sentences is infeasible; for example, fully annotating Japanese would require labeling 99,741 examples (Table 2) and roughly 5 thousand total hours of work. Due to limited resources, we sample up to 20 concepts for each target language and gather 10 sentence pairs per concept, ensur-ing that each variation is represented at least once. This results in a test set of up to 200 sentences per language. For languages with more than 20 extracted concepts, we first filter for concepts that have a roughly uniform distribution over variations. Specifically, for each concept we compute the rela-tive frequency of each lexical variation. Concepts are discarded if any individual variation deviates by more than 20% from a uniform distribution. After filtering, we uniformly sample 20 concepts to be included in the lexical selection task.\nA.3 Inter-Annotator Agreement\nIn this section we report statistics on inter-annotator agreement for all studies conducted with native speakers. We include 3 metrics: total agreement is the proportion of questions for which all 3 an-notators were in agreement, Fleiss' kappa (Fleiss,"}, {"title": "B Additional Experimental Details", "content": "B.1 Pipeline for Identifying Concepts with Variations\nIn this section, we formally describe the pipeline for identifying concepts with variations, which we adopt from Chaudhary et al. (2021). Let D = {(X1,Y1), ..., (X|D|, Y|D|)} be a parallel cor-pus where (Xi, Yi) is a source- and target-language sentence pair. For each sentence pair, we compute word alignments and lemmatize all words in Xi and yi using the AWESOME aligner and Stanza respectively. Furthermore, for source sentences only, we perform automatic part-of-speech (POS) tagging and dependency parsing using Stanza and word-sense disambiguation (WSD) using EWISER (Bevilacqua and Navigli, 2020). Source words are characterized by tuples of their lemmatized form and POS tag (lx, tx) to avoid conflating different meanings across POS tags. First, we enumerate all word alignments across the corpus and create a one-to-many mapping from each source word to the lexical variations it is aligned with. Second, we remove all source words that do not map to at least two lexical variations at least 50 times. We require 50 occurrences for each variation to prevent incorrect translations being extracted due to noisy alignments. Next, we describe a process for filter-ing out source words based on entropy. For a given source word tuple (lx, tx) with lexical variations \u016a = (v1,..., U|U|), let Ni be the number of occur-rences of variations vi. We compute the conditional"}, {"title": "B.2 Lexical Selection with NMT Systems", "content": "To perform lexical selection with NMT systems, we pass the source sentence as input and parse the translated text for the predicted lexical variation. First, we check for an exact substring match in the translated text with all lexical variations. If an ex-act match is found, we take that to be the predicted variation. If not, we tokenize the translated text with Stanza and computing the Levenshtein ratio"}, {"title": "B.3 Prompts", "content": "Since the Gemma family of models do not take a system prompt as input, we prepend the system prompt to the user prompt with the role user for all experiments involving Gemma-1.1. Figures 7, 8, and 9 show the prompts that we use to evaluate LMs on lexical selection and generate rules. For lexical selection with LLMs, we apply the same fuzzy matching scheme as Section B.2 to match the generated answer to a target-language varia-tion. Qualitatively, the instruction-following capa-bilities of GPT-4 were greater than that of Llama-3 and Gemma-1.1. If any LM failed to generate an answer according to the provided template, we ap-pend \"Please enclose your selected translation from <Translations> with 3 back ticks.\" to the prompt and resample once. If the LM fails to follow the template a second time, the prediction is labeled as incorrect."}, {"title": "B.4 Position Bias in Lexical Selection", "content": "We acknowledge that with our prompt, the lexi-cal selection task is similar to a multiple choice question (MCQ) setup. While humans tend to be order-invariant when answering MCQs, several prior works have examined position bias in LLMs when solving MCQs (Robinson and Wingate, 2023; Zheng et al., 2024). To ensure our evaluation is not affected by position bias we take three steps. First, we shuffle the order of translations in the prompt for every example during lexical selection to re-duce bias. Second, we report how often the LMs select an answer choice at each position across the Afrikaans, Latvian, and Japanese subsets of DTAILS. Lastly, we plot the mean and standard deviation of each LM experiment across 3 runs in Figure 2. We find that the test accuracy is consistent for all models despite each run being initialized with a unique seed for shuffling the order of translations. Based on these findings, we conclude that LMs are approximately order-invariant when doing lexical selection with our prompting setup."}, {"title": "B.5 OpenAI Model Used", "content": "In our call to the OpenAI API, we use the model name GPT-4-Turbo which at the time of writing is a pointer to gpt-4-turbo-2024-04-09."}, {"title": "B.6 Computational Requirements", "content": "All experiments in this paper that do not involve models from OpenAI require approximately 50 GPU hours on an NVIDIA RTX A6000 GPU."}, {"title": "C Software and Licenses", "content": "The TED2020 dataset uses the CC-BY-NC-4.0 License. All models are utilized from Hugging Face; LLAMA-3-8B-Instruct uses the Llama3 Li-cense, Gemma-1.1-7B-IT uses the Gemma License, MADLAD-400-10B-MT uses the Apache License 2.0, and NLLB-200-3.3B uses the CC-BY-NC-4.0 License. Our use of datasets and models is consis-tent with their intended use."}, {"title": "probability of each variation $v_i$ as", "content": "$P(v_i|l_x, t_x) = \\frac{N_i}{\\sum_{j=1}^{\\mid\\overline{U}\\mid} N_j}$"}, {"title": "and the entropy of a source word tuple as", "content": "$H(l_x, t_x) = \\sum_{j=1}^{\\mid\\overline{U}\\mid}-p(v_j | l_x, t_x)log_2(p(v_j | l_x, t_x))$"}]}