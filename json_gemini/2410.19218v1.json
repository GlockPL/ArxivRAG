{"title": "Taxonomy-guided Semantic Indexing for Academic Paper Search", "authors": ["SeongKu Kang", "Yunyi Zhang", "Pengcheng Jiang", "Dongha Lee", "Jiawei Han", "Hwanjo Yu"], "abstract": "Academic paper search is an essential task for efficient literature discovery and scientific advancement. While dense retrieval has advanced various ad-hoc searches, it often struggles to match the underlying academic concepts between queries and documents, which is critical for paper search. To enable effective academic concept matching for paper search, we propose Taxonomy-guided semantic Indexing (TaxoIndex) framework. TaxoIndex extracts key concepts from papers and organizes them as a semantic index guided by an academic taxonomy, and then leverages this index as foundational knowledge to identify academic concepts and link queries and documents. As a plug-and-play framework, TaxoIndex can be flexibly employed to enhance existing dense retrievers. Extensive experiments show that TaxoIndex brings significant improvements, even with highly limited training data, and greatly enhances interpretability.", "sections": [{"title": "1 Introduction", "content": "Academic paper search is essential for efficient literature discovery and access to technical solutions. Recently, dense retrieval has advanced in various ad-hoc searches (Karpukhin et al., 2020; Izacard et al., 2021). It encodes queries and documents as dense embeddings, measuring relevance by embedding similarity. These embeddings effectively capture textual meanings via pre-trained language models trained on massive corpora. While effective in general domains like web search, it often shows limitations in paper search (Wang et al., 2023).\nIn paper search, it is crucial to match the underlying academic concepts between queries and documents, rather than relying on surface text and its meanings. Academic concepts refer to fundamental ideas, theories, and methodologies that constitute the contents of papers. Users often seek information on specific concepts when searching for papers. For example, consider the query \"learning to win by reading manuals in a Monte-Carlo framework\". This query encompasses various concepts: optimizing decision-making (learning to win), acquiring knowledge from text (reading manuals), and reinforcement learning using probabilistic sampling (Monte-Carlo). Accordingly, retrievers should find papers that comprehensively cover these concepts.\nOne critical limitation of existing dense retrievers is that such academic concepts are often not effectively captured, making them insufficiently considered in relevance prediction. Identifying underlying concepts from surface text requires an inherent understanding of domain-specific contents, which is not sufficiently obtained from general corpora. This challenge is even greater for queries. As shown in the previous example, user queries often encompass various academic concepts in highly limited contexts. Moreover, queries usually have different expression styles (e.g., terminology choice, language style) from documents, making it difficult to match common concepts.\nTo address this limitation, we introduce a new approach that extracts key concepts from papers in advance, and leverages this knowledge to incorporate academic concepts into relevance predictions. We construct a semantic index that stores semantic components best describing each paper. The pro-"}, {"title": "2 Related Work", "content": "Dense retrieval. The advancement of pre-trained language models (PLMs) has led to significant progress in dense retrieval. Recent studies have enhanced retrieval quality through retrieval-oriented pre-training (Izacard et al., 2021; Gao and Callan, 2022), advanced hard negative mining (Zhan et al., 2021; Qu et al., 2021), and distillation from cross-encoder (Zhang et al., 2022). Synthetic query generation has also been explored to supplement training data (Thakur et al., 2021; Dai et al., 2023).\nOn the other hand, many studies have focused on pre-training methods specialized for the academic domain. In addition to pre-training on academic corpora (Beltagy et al., 2019), researchers have exploited metadata associated with papers. Cohan et al. (2020); Ostendorff et al. (2022) use citations, Liu et al. (2022) further utilizes venues, authors, and affiliations. Mysore et al. (2022) uses co-citation contexts, and Singh et al. (2023); Zhang et al. (2023b) employs multi-task learning of tasks such as citation prediction and paper classification. Complementary to the approach of leveraging such paper metadata, we focus on organizing and exploiting knowledge in the textual corpus. TaxoIndex can be flexibly integrated to enhance the aforementioned models.\nIndexing for dense retrieval. Indexing refers to the process of collecting, parsing, and storing data to enhance retrieval (Moura and Cristo, 2009). Statistical and sparse retrieval often uses inverted indexes for term matching signals (Bruch et al., 2024). Dense retrieval relies on approximate nearest neighbor (ANN) indexes to avoid costly brute-force searches. Document embeddings are pre-computed offline, and ANN indexes are constructed by techniques such as hashing (Pham and Liu, 2022), quantization (Baranchuk et al., 2018), and clustering (Zhang et al., 2023a; Li et al., 2023). Our index is designed to extract academic concepts and leverage them to improve the accuracy of dense retrievers. As TaxoIndex encodes each text as an embedding, existing ANN indexes can still be applied to accelerate search speed.\nEnhancing retrieval with additional contexts. Several studies have enhanced retrieval by providing supplementary contexts. Our work falls into this direction. Pseudo-relevance feedback (Zheng et al., 2020; Wang et al., 2021; Yu et al., 2021) utilizes the top-ranked results from an initial retrieval. Recent generative approaches (Mao et al., 2021; Mackie et al., 2023) generate relevant contexts using PLMs. Kang et al. (2024) utilizes topic distributions of queries and documents. However, they are often limited in paper search due to the difficulty of generating proper domain-specific contexts. Moreover, these contexts are obtained and added on-the-fly during inference, making it difficult to provide information tailored to backbone retriever."}, {"title": "3 Problem Formulation", "content": "Academic taxonomy. An academic taxonomy T refers to a hierarchical tree structure outlining academic topics (Figure 2). Each node represents an academic topic, with child nodes corresponding to its sub-topics. Widely used for study categorization in various institutions, academic taxonomies can be readily obtained from the web and automatically expanded by identifying new topics from a growing corpus (Lee et al., 2022; Xu et al., 2023). We utilize the fields of study taxonomy from Microsoft Academic (Shen et al., 2018), which covers 19 disciplines (e.g., computer science, biology).\nProblem definition. To perform retrieval on a new corpus D, a PLM-based dense retriever is typically fine-tuned using a training set of relevant query-document pairs. Our goal is to develop a plug-and-play framework, which facilitates academic concept matching with the guidance of a given taxonomy T, to improve the backbone retriever."}, {"title": "4 TaxoIndex Framework", "content": "We present taxonomy-guided index construction in \u00a74.1, index-grounded fine-tuning in \u00a74.2, and retrieval process with TaxoIndex in \u00a74.3.\n4.1 Taxonomy-guided Index Construction\nWe construct a semantic index that stores semantic components that best describe each paper (Figure 2). To guide this process, we propose using the academic taxonomy. This ensures that the index organizes knowledge in alignment with the researchers' consensus and greatly improves interpretability."}, {"title": "4.1.1 Core Topic Identification", "content": "The given taxonomy may contain many topics not included in the corpus. To effectively identify core topics from the vast topic hierarchy, we introduce a two-step strategy that first finds candidate topics and then pinpoints the most relevant ones.\nCandidate topics identification. Utilizing the hierarchy, we employ a top-down traversal approach that recursively visits the child nodes with the highest similarities at each level. For each document, we start from the root node and compute its similarity to each child node. We then visit child nodes with the highest similarities. This process recurs until every path reaches leaf nodes, and all visited nodes are regarded as candidates for the document.\nThe document-topic similarity $s(d,c)$ can be defined in various ways. As a topic includes its subtopics, we incorporate the information from all subtopics for each topic node. Let $N_c$ denote the set of nodes in the sub-tree having c as a root node. We compute the similarity as: $s(d, c) = \\frac{1}{|N_c|} \\sum_{j \\in N_c} cos(e_d, e_j)$, where $e_d$ and $e_j$ denote representations from PLM for a document d and the topic name of node j, respectively.\nCore topic selection. We select core topics by filtering out less relevant ones from the candidates. We consider two strategies: (1) score-based filtering, which retains topics with similarities above a certain threshold, and (2) LLM-based filtering, which uses large language models (LLMs) to select core topics. Our preliminary analysis shows that both filtering strategies are effective and lead to comparable retrieval accuracy. In this work, we opt for LLM-based filtering, as it often handles ambiguous cases better, further enhancing retrieval interpretability. Further analysis is provided in \u00a75.3.\nWe prompt the LLM to select core topics from the candidates by excluding those that are too broad or less relevant. After identifying core topics for all documents, we tailor the taxonomy by only retaining the topics selected as core topics at least once, along with their ancestor nodes.\nIn sum, for each document d, we obtain core topics as $y \\in \\{0,1\\}^{|T_D|}$, where $y_{di} = 1$ indicates i is a core topic of d, otherwise 0. $|T_D|$ denotes the number of nodes in the tailored taxonomy."}, {"title": "4.1.2 Indicative Phrase Extraction", "content": "From each document, we extract indicative phrases used to describe its key concepts. These phrases offer fine-grained details not captured by topic level, playing a crucial role in understanding detailed content and enhancing retrieval. An indicative phrase should (1) show stronger relevance to the document than to others with similar core topics, and (2) refer to a meaningful and understandable notion.\nWe first obtain the phrase set P in the corpus using an off-the-shelf phrase mining tool (Shang et al., 2018). Then, inspired by Tao et al. (2016); Lee et al. (2022), we compute the indicativeness of phrase p in document d based on two criteria: (1) Distinctiveness $dist(p, D_d) = \\frac{exp(BM25(p, d))}{(1 + \\sum_{d' \\in D_d} exp(BM25(p, d')))}$ quantifies the relative relevance of p to the document d compared to other topically similar documents $D_d$. $D_d$ is simply retrieved using Jaccard similarity of core topic annotation y. (2) Integrity $int(p)$ measures the conceptual completeness of the phrase, typically provided by most phrase mining tools, preventing the selection of non-meaningful phrases. The final indicativeness of p is defined as: $(dist(p, D_d) \\cdot int(p))$.\nFor each document d, we select top-k indicative phrases and denote them as $y^p \\in \\{0,1\\}^{|P|}$, where $y_{dj} = 1$ indicates j is an indicative phrase of d.\nRemarks. Compared to recent clustering-based indexes for dense retrieval (Zhan et al., 2022; Li et al., 2023), which use cluster memberships from document clustering, the proposed index has several strengths: it effectively exploits domain knowledge from taxonomy, offers broad and detailed views via topics and phrases, and enhances interpretability."}, {"title": "4.2 Index-grounded Fine-tuning", "content": "We train an add-on module to enhance relevance prediction while keeping the backbone retriever frozen (Figure 3). It comprises an indexing network and a fusion network, and is applied identically to both documents and queries using shared parameters. Here, we describe it on the document side."}, {"title": "4.2.1 Indexing Network: linking text to index", "content": "A naive approach to using the index information is to append it to each text as additional input context. However, this approach has several limitations. Importantly, test queries are not accessible before the test phase. Annotating topics and phrases during inference not only incurs additional latency but also is less effective due to the limited context of queries.\nAs a solution, we propose a new strategy called index learning, which trains the indexing network to identify core topics and indicative phrases from the text. We formulate this as two-level classification tasks, i.e., topic and phrase levels.\nExtracting topic/phrase information. Given the backbone retriever embedding $h_b \\in R^l$, we extract information tailored to predict topics and phrases as $h_t$ and $h_p$, respectively. To exploit the complementarity of topics and phrases, we employ a multi-gate mixture of experts architecture (Ma et al., 2018). We use M different experts, $\\{f_m\\}_{m=1}$, each of which is a small feed-forward network $f_m : R^l \\rightarrow R^l$. Two gating networks, $g_t$ and $g_p$, with Softmax outputs control the influence of experts for topic and phrase prediction, respectively. Let $w^t = g_t(h_b)$ and $w^p = g_p(h_b)$ denote M-dimensional vectors controlling the influences. The representations for each task are computed as:\n$h_t = \\sum_{m=1}^M w_m^t f_m(h_b), h_p = \\sum_{m=1}^M w_m^p f_m(h_b)$ (1)\nThis enables the direct sharing of information beneficial for predicting both topics and phrases, mutually enhancing both tasks (Ma et al., 2018).\nGenerating class representation. We encode topics and phrases to generate class representations"}, {"title": "4.2.2 Fusion Network: fusing index knowledge", "content": "The index-based representations ($h_t$, $h_p$) encode core topics and indicative phrases comprising the academic concepts within the text. We fuse them with the backbone embedding ($h_b$), which encodes the overall textual meanings, to generate $h_d \\in R^l$. We combine the topic and phrase representations as $h_a = f_1([h_t; h_p])$ using a small network $f_1 : R^{2l} \\rightarrow R^l$. The final embedding is obtained as:\n$h_d = h_b + \\alpha \\cdot w_a \\cdot h_a$ (3)"}, {"title": "4.2.3 Fine-tuning with TaxoIndex", "content": "We train the add-on module using the standard contrastive learning $L_{CL}$ with our index learning $L_{IL}$. For each query q, the contrastive learning loss is:\n$log \\frac{e^{sim(h_q, h_{d+})}}{e^{sim(h_q, h_{d+})} + \\sum_{d^-} e^{sim(h_q, h_{d-})}}$ (4)\nwhere $d^+$ and $d^\u2212$ denote the relevant and irrelevant documents. Index learning is applied to both documents and training queries Q. The final objective is $L_{CL}(Q, D) + \\lambda_{IL}(L_{IL}(D) + L_{IL}(Q))$, where $\u03bb_{IL}$ is a hyperparameter to balance the loss. To ensure $h_a$ contains high-quality information, we initially warm up the indexing network using $L_{IL}$.\nCore topic-aware negative mining. We devise a new strategy that uses core topics to mine hard-negative documents. Core topics reveal key concepts based on taxonomy, which may not be effectively captured by the lexical overlap (e.g., BM25) widely used for negative mining (Formal et al., 2022). We utilize both topical and lexical overlaps to select negative documents. For each (q, d+) pair, we retrieve $D_{d+}$, a set of topically similar documents to $d^+$, using Jaccard similarity of core topics, as done in \u00a74.1.2. We then select documents with the highest BM25 scores for q as negative samples."}, {"title": "4.3 Retrieval with TaxoIndex", "content": "Based on the index, TaxoIndex incorporates the similarity of surface texts and the similarity of the most related concepts for relevance prediction. This approach enhances the understanding of test queries, enables more precise academic concept matching, and improves paper search.\nWe introduce advanced inference techniques to further enhance retrieval using topic/phrase predictions ($\\hat{y}^t, \\hat{y}^p$) for queries and documents.\nDocument filtering based on core topics. Before applying the retriever, we filter out irrelevant documents that have minimal core topic overlap with the query. This step enhances subsequent retrieval by reducing the search space and providing topical overlap information. We compute the topical overlap using the inner product of $\\hat{y}_q^t$ and $y_d^t$. Documents with low topical overlap are excluded, retaining only the top x% of documents from the entire corpus. In this work, we set x = 25%. We provide retrieval results with varying x in \u00a75.3.\nInterpreting search results. The topics and phrases with the highest probabilities reveal the academic concepts captured and reflected in relevance prediction. Comparing query and document predictions allows for interpreting the search results. We provide case studies in Figure 1 and Appendix C.3.\nExpanding query with indicative phrases. We can expand a query by appending top-k phrases not included in the query. The retrieval results using the expanded query are denoted as TaxoIndex ++."}, {"title": "5 Experiments", "content": "5.1 Experiment setup\nWe provide further details on setup in Appendix B.\nDataset and taxonomy. We use two datasets: CSFCube (Mysore et al., 2021) and DORIS-MAE (Wang et al., 2023), which provide test query collections along with relevance labels on the academic corpus, annotated by human experts and LLMs, respectively. We use training queries generated by Dai et al. (2023), as they are not provided in both datasets. We use the field of study taxonomy from Microsoft Academic (Shen et al., 2018) which contains 431,416 nodes. After indexing, we obtain 1,164 topics and 3,966 phrases for CSFCube, and 1,498 topics and 6,851 phrases for DORIS-MAE. For core topic selection in TaxoIndex and baselines that require LLMs, we use gpt-3.5-turbo-0125.\nMetrics. Following Mackie et al. (2023); Kang et al. (2024), we employ Recall@K (R@K) for a large retrieval size (K), and NDCG@K (N@K) and MAP@K (M@K) for a smaller K (< 10).\nBackbone retrievers. We employ two representative models: (1) SPECTER-v2 (Singh et al., 2023) is a highly competitive model trained using metadata of scientific papers. (2) Contriever-MS (Izacard et al., 2021) is a widely used retriever fine-tuned using vast labeled data from general domains.\nBaselines. We compare three types of methods for applying and improving the backbone retriever. (1) Conventional approaches: no Fine-Tuning, Full Fine-Tuning (FFT), add-on module Fine-Tuning (aFT). FFT and aFT follow standard contrastive learning with BM25 negatives. FFT updates the entire backbone retriever, while aFT only updates an add-on module identical to TaxoIndex.6\n(2) Enhancing retrieval with additional context: GRF (Mackie et al., 2023) generates relevant contexts by LLMs. We generate both topics and keywords for a fair comparison. ToTER (Kang et al., 2024) uses the similarity of topic distributions between queries and documents, with topics provided by the taxonomy. We apply both methods to FFT.\n(3) Enhancing fine-tuning using an index: JTR (Li et al., 2023) constructs a tree-based index via clustering, then jointly optimizes the index and text encoder. Though focused on efficiency, it also enhances accuracy with index-based learning. We impose no latency constraints for a fair comparison."}, {"title": "5.2 Retrieval Performance Comparison", "content": "Main results. In Table 1, TaxoIndex performs better than all baselines on both backbone models across various metrics. Notably, TaxoIndex consistently outperforms FFT despite using significantly fewer trainable parameters, and aFT despite using the same add-on module. This shows the efficacy of the proposed approach using the semantic index.\nConversely, GRF often degrades performance. The LLM-generate contexts are not tailored to target documents, potentially causing discrepancies in expressions and focused aspects.7 JTR also fails to outperform FFT. It relies on document clustering, which may be less effective in specialized domains. Among the baselines, ToTER shows competitive performance by leveraging topic information. However, it cannot consider fine-grained concepts not covered by topics, and adds topic information on-the-fly only at inference, failing to fully enhance the backbone retriever. Lastly, while TaxoIndex ++ brings improvements, they are not significantly high, possibly because the phrase information is already reflected by TaxoIndex.\nFor the subsequent analyses, we use SPECTER-v2 for CSFCube and Contriever-MS for DORIS-MAE which show the highest NDCGs in Table 1.\nImpacts of training data. Table 2 reports the improvements by FFT and TaxoIndex with limited"}, {"title": "Difficult query analysis", "content": "In Table 3, we further analyze results for difficult queries, which account for 20% of total test queries. They are identified by two factors complicating query comprehension: (a) high lexical mismatch with documents, (b) high concept diversity within the query. FFT shows limited results and even degrades performance, despite the overall improvement in Table 1. Conversely, TaxoIndex consistently improves the retrieval quality on both types of queries, effectively handling lexical mismatch and various academic concepts. These results in \u00a75.2 collectively show the effectiveness of TaxoIndex in academic paper search."}, {"title": "5.3 Study of TaxoIndex", "content": "Ablation study. Table 4 presents various ablation results. First, the best performance is achieved by indexing both topics and phrases. Notably, removing phrase information drastically degrades performance, as phrases enable fine-grained distinctions of each document. Conversely, the absence of topic-level can be partially compensated by phrases, leading to smaller performance drops. Second, both architecture choices improve the indexing network, verifying the efficacy of leveraging the complementarity of topics and phrases and the topic hierarchy. Lastly, both adaptive weight and topic-aware mining prove effective. The mining technique shows higher impacts on top-ranked documents and often leads to faster convergence in our experiments.\nDocument Filtering based on Core Topics. Figure 4 shows the retrieval performance with varying retention ratios. We observe that topic-based filtering achieves comparable results to a whole corpus search by examining about 25% of the documents. This result indicates that core topics indeed effectively capture the central theme of each document.\nWe expect that core topics can be leveraged to improve recent clustering-based ANN indexes (Zhan et al., 2022; Li et al., 2023), which conduct clustering on document embeddings and use cluster memberships to represent documents. As topics are already discrete categories, this approach can reduce the need for clustering operations and provide guidance during the clustering process. Additionally, it offers interpretability by explicitly using topic names. As this is not the focus of our work, we leave further investigation for future research.\nImpact of LLM-based topic filtering. For core topic identification, TaxoIndex utilizes LLM-based filtering (\u00a74.1.1). In Figure 5(a), we explore its impacts by replacing it with score-based filtering, which retains documents with similarity above the median similarity of all documents assigned to each topic. Both score- and LLM-based filtering consistently achieve significant improvements over FFT."}, {"title": "6 Conclusion", "content": "We propose TaxoIndex to match academic concepts in paper search effectively. TaxoIndex extracts key concepts from papers and constructs a semantic index guided by an academic taxonomy. It then trains an add-on module to identify and incorporate these concepts, enhancing dense retrievers. Extensive experiments show that TaxoIndex yields significant improvements, even with limited training data.\nWe expect that TaxoIndex will effectively improve retrieval quality in various domains where underlying search intents are not sufficiently revealed by surface text. Specifically, e-commerce (Kang et al., 2019, 2020; Lee et al., 2023) is an interesting and promising domain. In this domain, users often express their information needs in various forms rather than searching by the exact product name. They might include desired attributes, characteristics, or even specific use cases. TaxoIndex can be applied to better capture users' search intents in such scenarios. We leave further investigation as future work."}, {"title": "7 Limitations", "content": "Despite the satisfactory performance of TaxoIndex, our study has three limitations.\nFirst, we utilize an academic taxonomy obtained from the web to guide core topic identification (\u00a74.1.1). We acknowledge that the taxonomy may not reflect up-to-date information. However, we are optimistic that this issue can be addressed by leveraging automatic taxonomy construction and completion techniques, a well-established research fields with many readily available tools (Lee et al., 2022; Xu et al., 2023; Shi et al., 2024). Also, our analysis in \u00a75.3 shows that TaxoIndex has considerable robustness to taxonomy coverage by utilizing phrase information directly extracted from papers.\nSecond, for topics and phrase mining process (\u00a74.1), we employ relatively simple techniques (e.g., distinctiveness and integrity computations) that have proven effective in recent text mining work. While these choices show high effectiveness in our experiments, we acknowledge that more sophisticated techniques could be employed. Our primary contributions lie in representing each paper's concepts at two levels and incorporating them into relevance predictions, rather than in the specific details for obtaining topics and phrases.\nLastly, this work focuses on the typical dense retrieval models that represent each text as a single vector embedding. Applying TaxoIndex to multi-vector representation models (Santhanam et al., 2022) may require additional modifications, which have not been explored in this study."}, {"title": "8 Ethical Statement", "content": "We utilize widely recognized and publicly available datasets for research purposes. Our methodologies and findings do not cause harm to any individuals or groups. We do not foresee any significant ethical issues arising from our work."}, {"title": "A Prompt for Core Topic Selection", "content": "We instruct LLMs using the prompt provided below. Both datasets used in this work contain paper abstracts. In our experiments, the average number of candidate topics is 28.5, and the average number of selected topics is 9.4.\nIt is important to note that representing the vast number of nodes in the taxonomy within a single prompt is infeasible. Our two-step strategy, which first identifies candidate topics and then pinpoints core topics, facilitates effective core topic selection."}, {"title": "B Details of Experiment Setup", "content": "B.1 Dataset\nWe have surveyed the literature to find retrieval datasets in the academic domain where relevance is labeled by experts (or annotators with high capabilities). We select two recently published datasets: CSFCube (Mysore et al., 2021) and DORIS-MAE (Wang et al., 2023). They provide test query collections along with relevance labels, annotated by human experts and LLMs, respectively. They also represent two real-world search scenarios: query-by-example and human-written queries.\nFor both datasets, we conduct retrieval from the entire corpus including all candidate documents. CSFCube dataset consists of 50 test queries, with about 120 candidates per query drawn from approximately 800,000 papers in the S2ORC corpus. Annotation scores greater than '2' (nearly identical or similar) are treated as relevant. We use the title as the query and both the title and abstract for the documents. DORIS-MAE dataset consists of 100 test queries, with about 100 candidates per query drawn similarly to CSFCube dataset. For each query, average annotation scores greater than '1' (the document answers some or all key components) are treated as relevant.\nLastly, we provide results of Contriever-MS on SCIDOCS (Cohan et al., 2020; Thakur et al.,"}, {"title": "B.2 Academic Taxonomy", "content": "We use the field of study from Microsoft Academic (Shen et al., 2018), which covers 19 disciplines (e.g., computer science, biology). It contains 431, 416 nodes and 498,734 edges with a maximum depth of 4. Note that we prune the taxonomy by only retaining topics included in the target corpus, during the indexing process (\u00a74.1.1). The number of nodes after the pruning is provided in \u00a75.1."}, {"title": "B.3 Metrics", "content": "Following the previous work (Thakur et al., 2021; Mackie et al., 2023; Kang et al., 2024), we employ Recall@K (R@K) for a large retrieval size (K), and NDCG@K (N@K) and MAP@K for a smaller K. Recall@K measures the proportion of relevant documents retrieved in the top K results, without consideration of the rank of the documents. Conversely, NDCG@K and MAP@K directly consider the absolute rank of each relevant document, where a higher value indicates that relevant documents are consistently found at higher ranks."}, {"title": "B.4 Experiment Details", "content": "Backbone models. We use publicly available checkpoints: SPECTER-v29 and Contriever-MS10. SPECTER-v2 is trained via multi-task learning using paper metadata from SCIBERT (Beltagy et al., 2019), and Contriever-MS is fine-tuned via massive training queries (MS MARCO) from BERT base uncased (Devlin et al., 2018). Both backbone models have about 110 million parameters.\nComputational resources and API cost. We conduct all experiments using 4 NVIDIA RTX A5000 GPUs, 512 GB of memory, and a single Intel Xeon Gold 6226R processor. For ChatGPT API usage, we spent $11.50 on core topic selection in TaxoIndex and $39.30 on query generation.\nImplementation details. For BM25, we use Elasticsearch."}, {"title": "C Supplementary Results", "content": "C.1 SCIDOCS Results\nWe provide results of Contriever-MS on SCIDOCS. Please note that we exclude this dataset from the main experiments, as it uses citation relations for relevance labels, which are utilized for the training of SPECTER-v2. We conduct automatic evaluation using LLMs as well as conventional evaluation using relevance labels.\nConventional evaluation. Table 5 presents the retrieval results. TaxoIndex shows higher retrieval performance compared to FFT and ToTER, despite using significantly fewer trainable parameters.\nAutomatic evaluation. For a more thorough evaluation with explicit consideration of detailed contents, we leverage ranking ability of LLMs for automatic evaluation (Choi et al., 2024; Qin et al., 2023). We adopt a recent pair-wise ranking technique (Qin et al., 2023) that instructs LLMs to compare the relevance of two passages for a given query. The prompt is provided below."}, {"title": "C.2 Indexing Network Performance", "content": "Table 6 presents the classification performance of the indexing network. We report the Precision@10 results on the training set, after the warmup of the indexing network. We observe high precision for both topics and phrases, indicating they are well captured by the proposed network."}, {"title": "C.3 Additional Case Study", "content": "Setup. As discussed in \u00a74.3, topics and phrases with the highest predicted probabilities reveal academic concepts captured and reflected for retrieval. We interpret search results by comparing predictions for queries (y, y) and documents (y, y). In our case studies in Figure 1, Table 7, and Table 8, we use topics and phrases having the highest logit values. Note that we use \u0177 instead of y for documents, as unlabeled but relevant classes are naturally revealed during training.\nCase study: short query with limited context. In Table 7, we present inferred information for two example queries. For the query 'semantic parsing learning with limited labels', TaxoIndex infers concepts such as \u2018syntactic predicate' and 'semi-supervised learning'. Similarly, for the query 'domain adaptation approach for machine translation', TaxoIndex identifies related concepts like 'parallel corpus' and 'NMT (neural machine translation)'. This inferred information complements limited query context, facilitating concept matching for paper search. We highlight that our index is constructed by organizing knowledge in the target corpus, and thus these terminologies are actually used in the papers that users search.\nCase study: long and complex query. In Table 8, we explore how TaxoIndex handles long and complex queries by analyzing one that includes various concepts. For this query, we present retrieval results: (a) an easy case that is well handled by all baselines (document A), and (b) two difficult cases that are not effectively handled by baselines (documents B and C).\nThe query encompasses various academic concepts: generative approaches for creating game levels, optimization via reinforcement learning or other differentiable methods, and measuring agent performance. Document A is ranked at the top-1 by all compared methods due to its high lexical overlap, directly including terms used in the query (e.g., GAN, generated levels). In contrast, documents B and C are not retrieved near the top. Unlike document A, they express the concepts using different terms (e.g., neuroevolutionary system), making it difficult to find relevance using surface texts. Additionally, document C specifically focuses on surrogate models for a shooter game, which obscures the query concepts like level generation.\nTaxoIndex infers the most relevant topics and phrases from the query (highlighted in yellow) and incorporates them into relevance prediction. This helps to match the underlying academic concepts, improving retrieval results. However, it still shows limited effectiveness for document C, as the overlap of indexed information is relatively small. We also note that fine-grained aspects of 'surrogate model' and 'character class' are not fully included in the indexed information, potentially because there are fewer documents covering such concepts in the corpus. We expect that incorporating other knowledge sources (e.g., knowledge bases) can mitigate these problems. We leave further exploration for future work."}]}