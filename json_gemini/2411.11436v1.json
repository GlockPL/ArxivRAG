{"title": "Implicit Regularization for Multi-label Feature Selection", "authors": ["Dou El Kefel Mansouri", "Khalid Benabdeslem", "Seif-Eddine Benkabou"], "abstract": "In this paper, we address the problem of feature selection in the context of multi-label learning, by using a new estimator\nbased on implicit regularization and label embedding. Unlike the sparse feature selection methods that use a penalized estimator with\nexplicit regularization terms such as 12,1-norm, MCP or SCAD, we propose a simple alternative method via Hadamard product\nparameterization. In order to guide the feature selection process, a latent semantic of multi-label information method is adopted, as a\nlabel embedding. Experimental results on some known benchmark datasets suggest that the proposed estimator suffers much less\nfrom extra bias, and may lead to benign overfitting.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-label learning focuses on the problem that each instance\nis associated with multiple class labels simultaneously [1], [2],\n[3], which is ubiquitous in many real-world applications, such as\nimage annotation [4], [5], [6], text categorization [7], [8], and gene\nfunction classification [9], [10]. Similar to single-label learning,\nhigh-dimensional data with an enormous amount of redundant\nfeatures significantly increases the computational burden of multi-\nlabel learning, which could also lead to over-fitting and perfor-\nmance degradation of learning algorithms [11].\nTo deal with this problem, feature selection represents a very\neffective way to alleviate the curse of dimensionality by selecting\nthe most informative feature subsets from the original set. Many\nmulti-label feature selection algorithms have been proposed to\nfind a lower-dimensional representation of the original feature\nspace, which can be broadly classified into transformation based\nmethods that transform the multi-label data into either one or more\nsingle-label data subsets [12], [13], and direct methods that adapt\nthe popular learning techniques to multi-label setting, without\nrequiring any preprocessing [14], [15], [16]. In the first category,\nfeature selection approaches can be used directly as filter like\nFisher score [17], wrapper like sequential feature selection [18]\nor embedded like lasso [19]. In the second category, the feature\nselection approaches are revised in order to handle the multi-label\nparameter. We note in this category, the Robust Feature Selection\n(RFS) based on 12,1-norm regularization [20], the Multi-label\ndimensionality reduction method (MDDM) [21], the Multi-label\nInformed Feature Selection (MIFS) [22], the Multi-label feature\nselection algorithm based on ant Colony Optimization (MLACO)\n[23], the ensemble method for semi-supervised multi-label feature\nselection (3-3FS) [24] and the Global and Local Feature Selection\n(GLFS) [25], not to mention more.\nParticularly, sparse feature selection methods in the context of\nmulti-label learning have a great deal of attention in recent years.\nThanks to their estimators with explicit regularization schemes,\nlike 12,1-norm [20], 12,1/2-norm [26], MCP [27] or SCAD\n[28], the effect of the curse of dimensionality has been greatly\nmitigated and the learning process has been enormously improved.\nMotivation and our contribution. Despite the success of\nmulti-label-learning-based sparse feature selection methods, they\nsuffer from extra bias due to the regularization term introduced\nartificially to restrict the effective size of the parameter space\n[29], [30]. Somewhat more clearly, adding some kind of norm\nconstraint to an objective function of interest makes the modified\noptimization problem more complex. Hence, the overall estimator\nmay be deteriorated and may not fall below the penalty level to\naccommodate a possibly faster convergence rate.\nThe aim of this paper is to propose a new estimator for\nmulti-label feature selection, which suffers less bias than usual\nexplicitly penalized estimators, and leads to more regular solu-\ntions. The proposed estimator is mainly based on an implicit\nregularizer used to prevent the overfitting. The implicit regularizer\nleads to a change-of-variable via a simple Hadamard product\nparametrization (element-wise product) [31]. The latter plays\nthe role of explicit penalty but by transforming the penalized\nmulti-label-learning-based sparse feature selection problem into\nan unconstrained smooth problem and also provides numerical\nstability (detail in section 3). Even if the implicit regularization has\nreceived particular attention in recent years, it has been addressed\nmost often in the context of learning [32], [33], [34]. In a different\ndirection, this paper considers implicit regularization outside the\nlearning process (see section 3).\nFurthermore, we used the latent semantic analysis in con-\njunction with the implicit regularization to guide the feature\nselection process. This choice is due to the fact that estimating\nthe correlation between features and class labels is often difficult\ndue to the presence of noise and unnecessary information labels\nin the label set. Thus, motivated by Latent Semantic Indexing\n(LSI) [35], [36], [37], it is possible to decompose the multi-labeled\noutput space into a small dimension space cleaned of noise and\nunnecessary labels, and use this low-dimensional space to guide\nthe feature selection process, via an implicit regularization.\nInterestingly, our estimator gives rise to the benign overfitting\nphenomenon that typically occurs when the training error is\nsignificantly smaller than the test error. Although the explanation\nfor the cause of this phenomenon remains a mystery to researchers,\nthey are almost inclined to believe that implicit regularization is\none of its mechanisms [32], [34]. They also confirm that explicit"}, {"title": "2 RELATED WORK", "content": "In this section, we first present the explicit and the implicit\nregularization concepts; then, we present the concept of latent\nsemantics of multi-label information and a brief overview of multi-\nlabel data selection approaches."}, {"title": "2.1 Explicit/implicit regularization", "content": "The literature on sparse feature selection methods in the context\nof multi-label learning is extensive. Jian et al [22], [38]. proposed\na novel multi-label informed feature selection framework called\nMIFS, that exploits label correlations to select discriminative\nfeatures across multiple labels. He et al. [39] introduced a multi-\nlabel classification approach joint with label correlations, missing\nlabels and feature selection. Hu et al. [40] proposed a robust\nmulti-label feature selection with dual-graph regularization. Zhang\net al. [16] suggested a new method that exploits both view\nrelations and label correlations to select discriminative features\nfor further learning. Fan et al. [41] proposed a manifold learning\nwith structured subspace for multi-label feature selection. Huang\net al. [42] introduced a Multi-label feature selection via manifold\nregularization and dependence maximization.\nAll the above-mentioned works connect an explicit regular-\nization term to the gradient descent optimization in order to\nmitigate the curse of dimensionality and improve the learning\nprocess. Nevertheless, the explicit regularization is not sufficient\nfor controlling the generalization error [43], and may lead to a\nless accurate estimation due to the large bias [44]. In this context,\nsome recent work claim that regularization may also be implicit,\nand the generalization error may enormously improved using\nimplicit regularization than explicit regularization. For instance,\nYu et al. used the early stopping as an implicit regularization\nto improve prediction. Zhang et al. [43] conducted a study to\nunderstand deep learning. They showed that explicit forms of\nregularization do not adequately explain the generalization error\nand the neural networks generalize well even without explicit\nregularization. Vaskevicius et al. [45] proposed an algorithm\nbased on implicit regularization for sparse linear regression. They\nshowed that, unlike explicit regularization, algorithms based on\nimplicit regularization applied to a sparse recovery problem adapt\nto the problem difficulty and yield optimal statistical rates. Zhao et\nal. [46] considered implicit regularization for solving least square\nproblems in the context of linear regression. They illustrated\nadvantages of using implicit regularization via gradient descent\nover parametrization in sparse vector estimation. Recently, Li\net al. [32]. proved that the implicit regularization could exhibit,\nunder certain conditions, the phenomenon of benign overfitting.\nThis was confirmed later in [34], where the authors also provided\nthe situations in which benign overfitting can occur. Chatterji\net al. [47] argued that the implicit regularization is essential in\ndetermining the generalization properties of the learnt model.\nZhou et al [33]. affirmed that one of the major explanations for\nbenign overfitting is implicit regularization."}, {"title": "2.2 Latent Semantics of Multi-Label Information", "content": "Furthermore, Latent Semantics Analysis (LSA) was originally\ndeveloped, and has been most commonly applied to, for improving\ninformation retrieval [37]. This by using dimensionality reduction\ntechniques that preserves the information of inputs and meanwhile\ncaptures the correlations between the multiple outputs. Yu et al.\n[48] proposed a Multi-label informed Latent Semantic Indexing\n(MLSI) that maintains the inputs and simultaneously captures cor-\nrelations between multiple output. Changqing et al. [49] proposed\na Latent Semantic Aware Multi-view Multi-label Learning (LSA-\nMML) that simultaneously seeks a predictive common represen-\ntation of multiple views and the corresponding projection model\nbetween the common representation and labels. Zhang et al. [16]\nintroduced a technique that projects the multi-labeled information\ninto a reduced space by using the idea of latent semantic analysis."}, {"title": "2.3 Multi-label feature selection", "content": "Multi-label feature selection approaches can be roughly split into\nproblem transformation approaches and adaption approaches. The\nfirst category includes filters, wrappers and embedded approaches\nthat transform the multi-label data into either one or more single-\nlabel data subsets. Most popular filter approaches are, Fisher\nscore [17], ReliefF [50] and f-statistic [51]. Most popular wrapper\napproaches are, sequential feature selection [52], randomized fea-\nture selection [53], support vector machines and recursive feature\nelimination [53]. The commonly used embedded approaches are\nlasso [19], LARS [54], VS-CCPSO [55] and NLE-SLFS [56]. The\nsecond category includes approaches that adapt the popular learn-\ning techniques to multi-label setting, without requiring any pre-\nprocessing. Popular approaches in this category include: Robust\nFeature Selection (RFS) based on 12,1-norm regularization [20],\nthe Multi-label dimensionality reduction method (MDDM) [21],\nand the Multi-label Informed Feature Selection (MIFS) [22]."}, {"title": "3 THE MFSIR APPROACH", "content": "In this section, we first present the notations used throughout the\npaper and then introduce the formulation of our proposed method\nmFSIR."}, {"title": "3.1 Notations", "content": "We use bold-faced symbols to denote vectors and matrices.\nLet $X = [X_1,X_2,...,X_n] \\in \\mathbb{R}^{n \\times m}$ be the instance matrix and $Y =$\n$[y_1,y_2,...,y_n] \\in \\{0,1\\}^{n \\times q}$ be the label matrix. $m$ represents the\nsize of feature vectors and $q$ represents the number of class labels\n$\\left\\{c_{1}, c_{2}, \\ldots, c_{q}\\right\\}$. $y_{i}=\\left[y_{i 1}, y_{i 2}, \\ldots, y_{i q}\\right] \\in\\{0,1\\}^{q}$ is a binary vector,\nwhere $y_{i j}=1$, if $x_{i}$ is associated with the label $c_{j}$ and $y_{i j}=0$,\notherwise. We use $\\|. \\|_{p}$ for the $l_{p}$-norm. The Frobenius norm (12,2)\nis defined as:\n$\\|\\mathbf{X}\\|_{F}=\\left(\\sum_{i=1}^{n} \\sum_{j=1}^{m} \\mathbf{X}_{i j}^{2}\\right)^{1 / 2}$ (1)"}, {"title": "3.2 Problem statement", "content": "Consider the usual sparse multi-label feature selection based on\nan explicit regularization term [20]:\n$\\mathcal{E}: \\min _{\\mathbf{W}} \\|\\mathbf{X} \\mathbf{W}-\\mathbf{Y}\\|+\\gamma\\|\\mathbf{W}\\|_{2,1}$\nexplicit_term\n(2)\nwhere, $W \\in \\mathbb{R}^{m \\times q}$ is the feature coefficient matrix. $\\gamma$ is a hyper-\nparameter used to control the strength of the regularization with\nrespect to the loss function. The objective of $\\mathcal{E}$ is to make $\\mathbf{W}$ well\nsparse, and this can be done by the second term based on the $l_{2,1}$\nnorm. The bigger $\\gamma$ is, the more the coefficients of $\\mathbf{W}$ are reduced\nuntil they are exactly zero.\nIn fact, Eq. (2) is easy to be optimized and can have global\noptimal solutions. However, its explicit regularization term may\nexpose it to large bias and thus lead it to less accurate estimation\n[46]. Therefore, we assume that replacing the explicit regularizer\nwith a simple implicit regularizer can overcome the extra bias."}, {"title": "3.2.1 Implicit regularization via Hadamard product param-eterization", "content": "Here, we rely on the works of [57] and propose to replace the\nexplicit regularization in Eq. (2) by an implicit one based on the\nHadamard product parametrization (see Eq. (3)).\nAssumption 1. Instead of using a direct coefficient matrix $\\mathbf{W}$ to\nbe regularized, we use the element-wise product of two matrices\n$\\mathbf{G} \\in \\mathbb{R}^{m \\times q}$ and $\\mathbf{H} \\in \\mathbb{R}^{m \\times q}$ that should estimate $\\mathbf{W}$.\nFrom the Eq. (3), the term $\\gamma$ in $\\mathcal{E}$ is used as an explicit\nregularization term. Under the Assumption 1, the same term is\ndropped in $\\mathcal{E}$ and implicitly introduced in the form of $(\\mathbf{G} \\odot \\mathbf{H})$.\nExplicit\n$\\mathcal{E}: \\min _{\\mathbf{W}}\\|\\mathbf{X} \\mathbf{W}-\\mathbf{Y}\\|+\\gamma\\|\\mathbf{W}\\|_{2,1}$\n$\\mathbf{W}$\nterm1 term2\nreplaced by\nImplicit\n$\\mathcal{E}: \\min _{\\mathbf{G}, \\mathbf{H}}\\|\\mathbf{X}(\\mathbf{G} \\odot \\mathbf{H})-\\mathbf{Y}\\|$\nterm1\n(3)\nwhere, '$\\odot$' is the Hadamard (element-wise) product."}, {"title": "3.2.2 Latent semantic analysis", "content": "In multi-label learning, the correlation between features and class\nlabels is often difficult to be estimated due to the presence of\nnoise and unnecessary information labels in the label set. From\nthis point, using latent semantics of multi-label information could\nbe very effective to guide the feature selection process."}, {"title": "3.2.3 Objective function", "content": "To perform feature selection, we take advantage of the low-\ndimensional latent semantics matrix $\\mathbf{V}$ that encodes label corre-\nlations and greatly reduces the noise in the original multi-label\noutput space. Hence, features most related to the latent semantics\n$\\mathbf{V}$ will be chosen. Therefore, the objective function that we"}, {"title": "3.3 Optimization algorithm", "content": "Minimizing Eq. (8) jointly over G, H, V and B is a highly non-\nconvex optimization problem with many saddle points, specially if\nthe label space Y is noisy. Our objective function is differentiable\nat each variable, and its local minimizers can be found using an\nefficient alternating optimization algorithm. Thus, we can apply\nthe gradient descent with Hadamard product parameterization, by\ntaking the derivative of the objective function w.r.t. variables G,\nH, V and B, respectively:\n$\\mathbf{G}:=\\mathbf{G}-\\eta \\nabla_{\\mathbf{G}} f(\\mathbf{G}, \\mathbf{H}, \\mathbf{V}, \\mathbf{B})$\n$\\mathbf{H}:=\\mathbf{H}-\\eta \\nabla_{\\mathbf{H}} f(\\mathbf{G}, \\mathbf{H}, \\mathbf{V}, \\mathbf{B})$\n$\\mathbf{V}:=\\mathcal{P}\\left[\\mathbf{V}-\\eta \\nabla_{\\mathbf{V}} f(\\mathbf{G}, \\mathbf{H}, \\mathbf{V}, \\mathbf{B})\\right]$\n$\\mathbf{B}:=\\mathcal{P}\\left[\\mathbf{B}-\\eta \\nabla_{\\mathbf{B}} f(\\mathbf{G}, \\mathbf{H}, \\mathbf{V}, \\mathbf{B})\\right]$\n(9)\n(10)\nwhere\n$\\nabla_{\\mathbf{G}} f(\\mathbf{G}, \\mathbf{H}, \\mathbf{V}, \\mathbf{B}):=\\mathbf{H} \\odot\\left[2 \\mathbf{X}^{\\top}(\\mathbf{X}(\\mathbf{G} \\odot \\mathbf{H})-\\mathbf{V})\\right]$\n$\\nabla_{\\mathbf{H}} f(\\mathbf{G}, \\mathbf{H}, \\mathbf{V}, \\mathbf{B}):=\\mathbf{G} \\odot\\left[2 \\mathbf{X}^{\\top}(\\mathbf{X}(\\mathbf{G} \\odot \\mathbf{H})-\\mathbf{V})\\right]$\n$\\nabla_{\\mathbf{V}} f(\\mathbf{G}, \\mathbf{H}, \\mathbf{V}, \\mathbf{B}):=2\\left[(\\mathbf{V}-\\mathbf{X}(\\mathbf{G} \\odot \\mathbf{H}))\\right.$\n$\\left.\\qquad+\\alpha(\\mathbf{V} \\mathbf{B}-\\mathbf{Y}) \\mathbf{B}^{\\top}+\\beta \\mathbf{V} \\mathbf{L}\\right]$\n$\\nabla_{\\mathbf{B}} f(\\mathbf{G}, \\mathbf{H}, \\mathbf{V}, \\mathbf{B}):=2\\left[\\alpha \\mathbf{V}^{\\top}(\\mathbf{V} \\mathbf{B}-\\mathbf{Y})\\right]$\nand $\\mathcal{P}[\\mathbf{D}]$ represents a box projection operator that maps the update\n$\\mathbf{D}$ to a bounded region in order to ensure the nonnegativity:\n$\\mathcal{P}[\\mathbf{D}]_{i j}=\\left\\{\\begin{array}{ll}\\mathbf{D}_{i j} & \\text { if } \\mathbf{D}_{i j} \\geq 0 \\\\0 & \\text { otherwise, }\\end{array}\\right.$\n(11)\n$\\eta$ is a step size, used to accelerate the convergence rate and\nto reduce the running time of the algorithm. Note that, at each\niteration, one variable is updated while fixing the other three\nvariables since the objective function is convex when any three\nvariables are fixed. We illustrate the optimization of Eq. (8) in\nAlgorithm 1."}, {"title": "3.4 Time Complexity Analysis", "content": "The computational complexity of Algorithm 1 is presented by the\nfollowing lemma."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct a series of experiments on large\nscale datasets to validate the proposed framework. We compare\nour algorithm with other multi-label feature selection algorithms,\nfollowed by further analysis."}, {"title": "4.1 Experimantal Setup", "content": "To validate the performance of our proposal, we conduct exper-\niments on ten multi-label benchmark datasets. All datasets are\navailable in MULAN Project\u00b9. Table 1 describes the characteris-\ntics of each multi-label dataset S, including the number of exam-\nples (|S|), number of features (dim(S)), number of class labels\n(L(S)), label cardinality (LCard(S), and label density (LDen(S))."}, {"title": "4.2 Experimental Results", "content": "In this section, we present and discuss the obtained results. Table 2\nreports the average results (mean\u00b1std) of each comparing feature\nselection algorithms over ten aforementioned datasets in terms of\neach evaluation metric.\nIn addition, we also use the non-parametric Friedman test\nas the statistical evaluation to analyze the relative performance\namong the comparing algorithms [69]. Let y the number of\nalgorithms, 0 the number of datasets (in our case, y = 5, 0 = 10).\nAccordingly, let $R_{j}=\\frac{1}{\\theta} \\sum_{i=1}^{\\theta} r_{i}^{j}$ denotes the average rank for the\nj-th algorithm over all datasets, with $r_{i}^{j}$ the rank of the j-th of y\nalgorithms on the i-th of 0 datasets. Then, the Friedman statistic\n$F_{F}$ is calculated by Eq. (12) and is distributed according to the F-\ndistribution with y-1 numerator degrees of freedom and (y-1)(0-1)\ndenominator degrees of freedom:\n$F_{F}=\\frac{(O-1) X_{F}}{O(Y-1)-X_{F}}$\n(12)\nwhere,\n$X_{F}=\\frac{12 O}{Y(Y+1)}\\left[\\sum_{j=1}^{Y} R_{j}^{2}-\\frac{Y(Y+1)^{2}}{4}\\right]$\n(13)"}, {"title": "4.3 Further Analysis", "content": ""}, {"title": "4.3.1 Influence of Selected Features", "content": "In this section, we study the impact of changing the number of\nselected features on the performance of mFSIR. We vary the\nnumber of selected instances from 5% to 30%. Experiment is\nconducted on four datasets, including: emotions, language\nlog, tmc2007 and Yeast. Figure 2 shows the performance\ncomparison in terms of each evaluation metric across these afore-\nmentioned datasets. The major observations resulting from the\nanalysis of these results are two-fold:"}, {"title": "4.3.2 Influence of Extra bias", "content": "As explained in section 2, the connection between the explicit\nregularization term and gradient descent optimization paths can\nlead to a less accurate estimation. That is, the resulting estimator,\nby this connection, may be dominated by the bias term due to\nthe penalty, and the estimation error cannot fall below the penalty\nlevel to accommodate a possibly faster convergence rate. To show\nhow our proposed can overcome the extra bias and lead to more\naccurate estimation, we experimentally study the convergence of\nmFSIR which adopts the implicitly-regularized gradient descent\nas well as MIFS adopting the explicit term 12,1-norm. Figure 3\nprovides an illustration of convergence curves (mFSIR and MIFS)\nusing some datasets. The following stop criteria is used:\n$\\frac{\\zeta^{\\iota}-\\zeta^{\\iota-1}}{\\zeta^{\\iota-1}}<10^{-5}$\nwhere $\\zeta^{\\iota}$ represents the objective function value in the $\\iota$th\niteration. As shown in Figure 3, the estimation error of mFSIR\nis significantly lower than that of MIFS. This means that our\nproposed suffers significantly less from extra bias compared to\nMIFS and converges faster. Indeed even if MIFS ends up con-\nverging to the optimal solution, it results in a higher complexity\nthan that of mFSIR. In contrast, mFSIR may find a solution that\noptimally balances between the model complexity and goodness fit\nof the model. For the four datasets emotions, language log,\ntmc2007 and Yeast, mFSIR converges in 3 to 10 maximum\niterations."}, {"title": "4.3.3 Sparsity ensurement", "content": "In this section, we show how to empirically ensure the sparsity\nin optimal values \u011c or \u0124. As mentioned in section 3, we assume\nsparsity is ensured in these two optimal values if the initial values\nof G or H are superior or equal to zero. Thus, we compare the\nmatrix of G or \u0124 after a random initialization of G or H, and after\nan initialization with values greater than or equal to zero. Figure\n4, presents the qualitative results of \u011c using the bibtex dataset.\nFrom the figure, the initialization of G is very related to sparsity.\nWhen we initialized G with values greater than or equal to zero,\nwe got a sparse matrix with columns containing the value zero\n(blue color). In contrast, when G initialized randomly, we got a\nnon-sparse matrix with columns containing different colors."}, {"title": "4.3.4 Sensitivity Analysis", "content": "In this section, we conduct an experiment to see how the perfor-\nmance of mFSIR changes with varying parameter configurations\n$\\alpha$ and $\\beta$ used to balance each term in equation Eq. (8). We\ntune both parameters from $\\{10^{-3},10^{-2}, 10^{-1}, 1, 10, 10^{2}\\}$. We also\nvary the number of selected features from $\\{5\\%, 10\\%, 15\\%, 20\\%,\n25\\%, 30\\%\\}$ of the total number of features. Figure 5 shows the\nperformance of mFSIR across the emotions dataset in terms of\neach evaluation metric. To be specific, the first and second row\n(left subfigure) in the figure represent the performance of mFSIR\nin terms of hamming loss, ranking loss and macro-averaging AUC\nv.s. regularization parameter $\\alpha$ ($\\beta$ fixed as 1) and percentage\nof selected features. The second (right subfigure) and third row\nin the figure represent the performance of mFSIR in terms of\nthe above three metrics v.s. regularization parameter $\\beta$ ($\\alpha$ fixed\nas 1) and percentage of selected features. From the figure, the\nperformance of mFSIR is not very sensitive to the changes of\nparameters whose values change within a reasonable range. On\nthe other hand, the performance of mFSIR is more sensitive to the\nnumber of selected features. Additionally, with respect to all three\nmetrics, mFSIR performance increases as the number of features\nselected increases."}, {"title": "4.3.5 Stability Analysis", "content": "Here, we use the spider web diagram to examine the stability of\ncomparing algorithms on some multi-label datasets considering\nhamming loss metric. The spider web graph has different corners\nand lines of different colors. The corners represent the different\ndatasets and the lines represent the different algorithms. The\ncolored area surrounded by the colored line indicates the stability\nvalue of the algorithm. The rounder and larger the area, the more\nstable the algorithm. Based on [70], the prediction performance\nis normalized into [0, 0.5]. Note that a stability value close to\n0.5 is considered significant. Figure 6 shows the stability index\naccording to the hamming loss values after normalization. From\nthe figure, mFSIR is more stable compared to other algorithms, as\nits area almost covers the spider web graph."}, {"title": "4.3.6 Runtime Comparison", "content": "In this section, we illustrates the efficiency of the proposed\nmFSIR by comparing its running time (in second) with the\nother baseline approaches on some benchmark datasets. Table 4\nshows the average running time required to reach the convergence\nstate of each comparison approach. Overall, the running time\nvaries according to the size of the dataset. Furthermore, mFSIR\nis relatively comparable to the other comparing approaches and\nexhibits competitive runtime performance on various datasets."}, {"title": "4.3.7 Benign overfitting and implicit regularization", "content": "Here, we conduct empirical study to show that mFSIR can\ngeneralize thanks to the proposed implicit regularization and may\nexhibit the benign overfitting phenomenon in some datasets. Note\nthat this phenomenon occurs when the predictor perfectly fits the\ntraining data while achieving near optimal loss. We train a baseline\nclassifier on the bibtex dataset for 100 epochs and test whether\nthe baseline classifier overfits the dataset in a benign way. More\nprecisely, we train a baseline classifier on both original bibtex\ndataset (without mFSIR) and the bibtex dataset processed by\nour mFSIR approach (the number of selected features is set at\n30%). Figure 7 shows the training and validation loss over the\naforementioned datasets. The figure also shows the training and\nvalidation Macro F1-score. The major observations resulting from\nthe analysis of the figure are as follows:"}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "This paper introduces a novel framework for multi-label feature\nselection. The proposed framework is based on a new estimator\nthat overcomes the large bias and may lead to benign over-\nfitting. The proposed estimator relies on implicit regularization\nvia Hadamard product parametrization in conjunction with the\nlatent semantic analysis. Experiments validate the effectiveness\nof our proposed, which outperforms state-of-the-arts on several\nbenchmark datasets.\nOur work opens up many interesting research directions,\nincluding the adaptation of implicit regularization to multi-label\nfeature selection techniques in a semi-supervised context."}]}