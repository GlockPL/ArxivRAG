{"title": "AutoRNet: Automatically Optimizing Heuristics for Robust Network Design via Large Language Models", "authors": ["He Yu", "Jing Liu"], "abstract": "Achieving robust networks is a challenging problem due to its NP-hard nature and the vast, complex, high-dimensional solution space. Current methods, from handcrafted feature extraction to deep learning approaches, have made certain advancements but remain rigid and complex, often requiring manual design, trial and error, and large amounts of labeled data. To deal with these problems, we propose AutoRNet, a novel framework that integrates large language models (LLMs) with evolutionary algorithms to automatically generate complete heuristics for robust network design. With the intrinsic properties of robust network structure in mind, effective network optimization strategy-based variation operations are designed to provide domain specific prompts for LLMs to help them make use of domain knowledge to generate advanced complete heuristics. Moreover, to deal with the difficulty brought by the hard constraint of maintaining degree distributions, an adaptive fitness function, which can progressively strengthening constraints to balance convergence and diversity, is designed. We evaluate the robustness of networks generated by AutoRNet's heuristics on both sparse and dense initial scale-free networks. These solutions outperform those from current methods. AutoRNet reduces the need for manual design and large datasets, offering a more flexible and adaptive approach for generating robust network structures.", "sections": [{"title": "1. Introduction", "content": "Modern networked systems form the backbone of contemporary society. Understanding the robustness of these networks is crucial for ensuring stability and reliability [1, 2, 3]. Improving network robustness to prevent catastrophic disruptions is inherently challenging due to its NP-hard nature, and related research has seen significant advancements. Key contributions include approximate theoretical models [4], which simplify the complex interactions within networks to provide insights into enhancing resilience. Optimization algorithms, employing approaches such as simulated annealing (SA) [5], genetic algorithms (GAs) [6], and greedy approaches [7], offer near-optimal solutions in a reasonable time frame. In addition, machine learning techniques, particularly deep reinforcement learning[8], have been used to dynamically adapt and improve network configurations. However, these methods highly depend on manual work, expert knowledge, training data, and trial-and-error processes.\nThe emergence of coding-oriented Large Language Models (LLMs)[9] has garnered significant attention for their potential in addressing combinatorial optimization problems [10, 11]. FunSearch [12] combines a pre-trained LLM with evolutionary algorithms (EAs) to evolve initial low-scoring programs into high-scoring ones. Evolution of Heuristics (EoH) [13] co-evolves both heuristic descriptions and their corresponding code implementations. However, both FunSearch and EoH depend primarily on existing optimization algorithms, only scoring or weighting mechanisms influencing the search algorithm behavior are designed by LLMs to guide the related operations, without creating a new algorithm, hereby limiting their applicability to complex domain-specific problems, such as network robustness. There is a need to generate whole algorithms directly rather than merely modifying data through weighting and scoring.\nIn this paper, we address these issues and make a significant step by proposing AutoRNet, an integrated framework that combines the contextual intelligence of LLMs with the adaptive optimization capabilities of EAs. Specifically, we design Network Optimization Strategy(NOS)-based variation operations tailored for complex network prob- lems, which can create domain-specific prompts for LLMs, generating a variety of heuristics suitable for different network-related challenges. Moreover, we design an Adaptive Fitness Function (AFF) to evaluate these heuristics, progressively tightening constraints to balance the convergence and diversity, thereby discovering superior heuristics. AutoRNet can automatically make use of special network characteristics to design effective operations. The major contributions of this paper are summarized as follows:\n\u2022 A hybrid framework is developed where EAs and LLMs iteratively collaborate to generate and refine heuristics for network robustness.\n\u2022 NOS-based variation operations are designed, which can generate problem-specific prompts to guide LLMs in divergent thinking. These NOSs are equally applicable to other network-related challenges.\n\u2022 AFF tailored to network issues is designed, to transform hard constraints into soft ones, enhancing the diversity and resilience of heuristics.\n\u2022 Solutions generated by AutoRNet's heuristics are evaluated across eight scale-free networks with varying sizes and densities and a real-world network, demonstrating that they outperform current methods.\nThe remainder of this paper is organized as follows. Sections 2 and 3 introduce the related work and network robustness measures, respectively. Section 4 introduces AutoRNet in details. Section 5 presents the experiments. Finally, Section 6 concludes the paper with a summary of our findings."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Related Work on Improving Network Robustness", "content": "Traditional methods for improving network robustness focus on improving local redundancy and connectivity, such as maximizing clustering coefficient to form tightly knit groups of nodes. These methods also leverage high- order network structures like triangles and quadrilaterals to identify and protect critical nodes and edges to withstand targeted attacks. Techniques like greedy and local search algorithms systematically improve network robustness. Fortunato et al.[14] proposed a greedy algorithm that improves the robustness of social networks by forming tightly- knit communities. Zhang et al.[15] proposed a lazy-greedy algorithm that enhances the robustness of transportation networks by protecting the most critical nodes from failure. Hau Chan et al.[16] introduced a local search algorithm by iteratively improving the subgraph structure to improve the robustness of the network.\nMetaheuristic algorithms, including Genetic Algorithms(GAs)[17], and Simulated Annealing(SA)[18], have also been extensively employed to solve network robustness problems. Pizzuti et al. [19] proposed using GAs to enhance the robustness of complex networks by simulating the process of natural selection to evolve network configurations over generations, improving resilience against failures and attacks. Zhou et al. [6] proposed using Memetic Algo- rithms, which combine global and local search strategies, to enhance the robustness of scale-free networks by in- tegrating global and local search operators. Buesser et al. [5] proposed the use of SA to optimize the robustness of scale-free networks by rewiring the network edges, thus improving the resilience of the network to fragmentation and intentional damage. Pizzuti et al.[20] introduced RobGA, a GA designed to enhance network robustness by adding edges in a way that minimizes disruption risk.\nHowever, these methods often require manual design and multiple trial-and-error processes, making them ineffi- cient and time-consuming. The need for extensive experimentation and tuning reduces their practicality for large-scale and dynamic network environments, highlighting the need for more automated and adaptive approaches to network robustness optimization.\nDeep learning, particularly Graph Neural Networks (GNNs), has emerged as a powerful approach to address network robustness problems. Tang et al.[21] explored methods to improve the robustness of GNNs against poisoning attacks by leveraging clean graphs from similar domains. This approach helps the GNNs detect adversarial edges more effectively, enhancing their resistance to attacks. Wang et al.[22] developed certifiably robust GNNs to defend against attacks that perturb the graph structure by adding or deleting edges. Their approach ensures that the GNNs' performance remains stable even under adversarial conditions."}, {"title": "2.2. The Application of LLMs in Combinatorial Optimization", "content": "LLMs have shown a significant impact in addressing various problems. One prevalent approach involves engi- neering in-context learning prompts (EILP)[23] using techniques such as zero-shot, few-shot, and chain-of-thought (CoT) prompting[24, 25, 26]. Enhancing EILP with fine-tuning further improves response accuracy by adapting LLMs to specific datasets or tasks. Furthermore, combining EILP with ensemble learning techniques increases robustness and consistency by aggregating multiple model predictions. Integration with Retrieval-Augmented Generation (RAG) [27] uses external knowledge sources, allowing LLMs to generate more accurate and informed contextual responses.\nMany researchers have also focused on solving combinatorial optimization problems using LLMs. Shengcai Liu et al.[28] presented the first investigation into LLMs as evolutionary combination optimizers for solving the Travelling Salesman Problem (TSP). FunSearch[12] leverages LLMs to generate and optimize mathematical functions, discov- ering new constructions and improving existing solutions by iteratively refining function constructions. EoH[13] facilitates the simultaneous evolution of concept description and code implementations, emulating the process by which humans develop heuristics, to achieve efficient automatic heuristic design. These approaches have demon- strated competitive performance compared to traditional heuristics in finding high-quality solutions.\nAll of these methods are based on existing algorithms, scoring or weighting mechanisms are used to modify the original data (e.g., bin capacities, city distances), influencing the search algorithm behavior but without creating a new algorithm. There is a need to generate whole algorithms directly rather than merely modifying data through weighting and scoring. These approaches restrict their optimization improvements to relatively simple combinatorial problems and cannot extend their applicability to more complex or domain-specific optimization scenarios."}, {"title": "3. Network Robustness Measures", "content": "A network is often represented as a graph $G = (V, E)$, where V denotes the set of nodes and E represents the set of edges. To measure network robustness, several methodologies have been developed. [3] introduced a measure R to assess network robustness by evaluating the size of the largest connected component during sequential node attacks,\n$R = \\frac{1}{N} \\sum_{q=1}^{N} s(q)$ (1)\nwhere N is the number of nodes in the network and s(q) is the fraction of nodes in the largest connected cluster after removing q nodes. The normalization factor 1/N ensures that the robustness of networks with different sizes can be compared. The range of R values is between 1/N and 0.5.\nComplex networks can exhibit different topological structures, with random and scale-free networks being two of the most studied types. Random networks, characterized by a homogeneous degree distribution, are robust to tar- geted attacks on high-degree nodes. In contrast, scale-free networks, with a power-law degree distribution, exhibit exceptional robustness against random failures due to the low probability of removing a hub but are extremely sus- ceptible to targeted attacks that focus on their few high-degree hubs. These networks display distinct characteristics that significantly impact their robustness and vulnerability to failures or attacks.\nIn this paper, we adopt R to evaluate network robustness, specifically focusing on targeted attacks through the sequential removal of the highest-degree nodes. We employ scale-free networks for both training and testing graph sets to accurately reflect the structure of many real-world networks."}, {"title": "4. AutoRNet", "content": ""}, {"title": "4.1. The Framework of AutoRNet", "content": "AutoRNet uses EAs to search heuristics and maintains a population of popsize individuals, denoted as $P = \\{h_1,h_2,..., h_{popsize}\\}$. Each individual $h_i$, $i = 1,2,..., popsize$, includes a heuristic. There are T generations in to- tal, with $P_1 = \\{h_{1,1}, h_{1,2},..., h_{1,popsize}\\}$ representing the population at generation t. AutoRNet evolves the population generation by generation, and the whole framework is summarized in Algorithm 1.\nFirst, Initialize Population() initializes the initial population $P_1$ of popsize individuals using a task specification and prompt interaction with the LLM. Then, based on the training graph set G, AFF() uses the adaptive fitness function, introduced in the following text, to calculate the fitness of each individual in $P_1$. Next, the population is evolved T generations, and in each generation, NOS_Variation() first conducts the NOS-based variation operations designed in the following text on the current population, obtaining the offspring population. Then, SelectNextPopulation() uses the roulette wheel selection according to the fitness to select popsize individuals from Pt and Poffspring together to form the population for the next generation. Figure 1 schematically illustrates the framework of AutoRNet."}, {"title": "4.3. Adaptive Fitness Function", "content": "In the EA assisting LLMs for heuristic code generation, individuals are encoded as methods rather than solutions, and the search is based on LLMs to generate new or similar methods. It does not inherently provide a mechanism to measure the similarity between methods, making it difficult to navigate the method space. Maintaining a consistent degree distribution is a constraint in optimizing network robustness. Enforcing this constraint strictly from the outset can be overly restrictive, leading to a large number of invalid individuals, impeding the evolutionary process.\nTo overcome these issues, we design an adaptive fitness function (AFF), which can dynamically adjust the degree distribution constraint throughout the evolutionary process. The AFF initially relaxes this constraint, facilitating broader exploration of the method space, and then progressively tightens this constraint as the evolution proceeds. By doing so, AFF improves the ability of AutoRNet to explore various heuristics and avoid dropping into local optima."}, {"title": "4.4. Network Optimization Strategy-based Variation Operations", "content": "The heuristic method searching space in the EA assisting LLMs for heuristic code generation is vast, complex, and high-dimensional. In simpler problem domains, such as those addressed by EoH and FunSearch, this space is reduced by limiting function codes to straightforward tasks like weighting or scoring data. For complex problems like network robustness, simplifying the problem is not feasible due to the intricate and domain-specific nature of tasks. Through experiments we find, to deal with the complex problem like network robustness, by providing just general prompts without domain knowledge, current LLMs can only design simple operations on nodes or links, and lack of the ability to make deep use of domain knowledge to design advanced operations. Therefore, it is important to design mechanism which can provide LLMs domain knowledge effectively to further release LLMs' ability in design optimization methods for complex problems.\nTo cure this problem, we design Network Optimization Strategies (NOSs) with the intrinsic properties of networks in mind. Networks have features such as degree distribution, path characteristics, clustering coefficient, centrality measures, and community structure. Based on these features, strategies such as high-degree node priority, shortest path optimization, and betweenness centrality priority can be designed. Guided by these strategies, actions such as adding edges, rewiring edges, and swapping edges are then taken. This combination of features, strategies, and actions forms the NOS. Detailed information is provided in the Appendix.\nBased on NOSs, variation operations are designed to generate offspring heuristics by integrating NOSs into prompts to guide the LLMs with domain knowledge. Three types of variation operations, namely E1, M1, and M2, are designed. E1 and M1 form the prompt for LLMs by integrating randomly select 12 NOSs with the general purpose prompts, and M2 just use the general prompts.\nE1 (Exploration with NOS Integration): By providing 2 parent individuals, El prompts the LLM to create entirely new heuristics with randomly selecting 12 NOSs, ensuring that the generated offspring are diverse and inno- vative. El help AutoRNet escape local optima by introducing new strategies and methods into the population.\nM1 (Guided Modification with NOS): M1 prompts the LLM to refine existing heuristics by incorporating NOSs leading to targeted improvements and optimizations. This type of variation operation provides guided local search, leveraging domain-specific knowledge to enhance the effectiveness of the current heuristic.\nM2 (Local Adjustment): M2 prompts the LLM to make minor adjustments to existing heuristics, focusing on small-scale improvements and refinements. This type of variation operation is a pure local search, making incremental adjustments to optimize the heuristic's performance.\nIn each generation, $P_{e1}\u00d7popsize$, $p_{m1}\u00d7popsize$, $p_{m2}\u00d7popsize$ individuals are selected from the current population using the tournament selection to conduct E1, M1, M2, respectively. In this way, prompts of E1, M1, and M2 can be provided to the LLM server simultaneously. Figure 3 illustrates the prompt schematic for M1, and these for El and M2 are given in the Appendix. By utilizing these three types of variation operations, AutoRNet effectively balances the need for innovation and refinement, ensuring robust and efficient network optimization."}, {"title": "5. Experiments", "content": "AutoRNet generates heuristic methods and we analyze these methods to illustrate the design capabilities of Au- toRNet. Simultaneously, we select three existing algorithms as baselines: the Hill Climbing Algorithm (HC)[29], the Simulated Annealing Algorithm (SA)[5], and the Smart Rewiring Algorithm (SR)[30]. R is used to evaluate the ro- bustness of networks in the test graph set after optimization by all algorithms. By comparing their network robustness, we assess the effectiveness of the methods generated by AutoRNet."}, {"title": "5.1. Experimental Settings", "content": "The training graph set G consists of BA scale-free networks in two different sizes: 50 and 100 nodes. For each network size, the number of initial nodes Mo varies from 2 to 5. For each combination of network size and Mo, we create three instances, resulting in a total of $M = 2 \u00d7 4 \u00d7 3 = 24$ training graphs. The test graph set consists of three types of networks: sparse BA networks with 100, 200, 300, and 500 nodes, $N_0$ = 3, $M_0$ = 2; BA networks with 100 nodes, $N_0$ = 6, $M_0$ ranging from 2 to 5; and a real world EU power grid network[31] with 1,494 nodes and 2,066 edges."}, {"title": "5.2. Evaluation Results", "content": ""}, {"title": "5.2.1. Design Capabilities of AutoRNet", "content": "After running AutoRNet for 50 generations, we selected three highly valuable heuristics based on their fitness values, named as Heuristic-v1, Heuristic-v2 and Heuristic-v3, which significantly improved the network robustness of the test graph set. Heuristic-v1 leverages network features of critical nodes and similar nodes, Heuristic-v2 utilizes node connectivity and degree distribution, and Heuristic-v3 optimizes local topology by manipulating edges among neighbors. All these three heuristics maintain the same number of edges, with Heuristic-v1 also preserving the degree distribution. This demonstrates AutoRNet's ability to design complete algorithms that effectively utilize network features.\nHeuristic-v1, shown in Algorithm 2, employs an advanced strategy that combines edge swapping with SA while preserving the degree distribution. This heuristic identifies critical nodes(those with the highest degrees) and pairs sim- ilar nodes based on their degree deviation. It dynamically adjusts the max_diff parameter(Step 4), which controls the tolerance for degree deviation in pairing similar nodes(Steps 22-34). The algorithm iteratively swaps edges between the neighbors of these paired nodes(Steps 18-19), evaluating new configurations based on robustness improvements or probabilistic acceptance criteria derived from simulated annealing(Step 22). This sophisticated approach ensures a balance between exploration and exploitation in the search space. Without NOSs, it would be impossible to evolve such \"intelligent\u201d heuristics that smartly leverage network features for robust network design.\nHeuristic-v2 and Heuristic-v3 optimize the network through edge relocation, which, while slightly altering the degree distribution, significantly improves the robustness of the networks. Importantly, edge-relocation ac- tions incur the same real-world costs as edge-swapping. This shows that AutoRNet is not constrained by theoreti- cal conditions, but instead explores a variety of methods, making it more suitable for solving real-world problems."}, {"title": "5.2.2. Performance Comparison of Algorithms", "content": "The robustness results over all test graphs, summarized in Tables 1-3, demonstrate the effectiveness of the heuris- tics designed by AutoRNet.\nHeuristic-v1's performance is not as strong as those of Heuristic-v2 and Heuristic-v3, but it is comparable to those of the three manually designed algorithms, often surpassing their in certain scenarios. In Table 3, Heuristic-v1 performs comparably well on the EU Power Grid Network, maintaining robustness with an average of 0.205911 and a low variance, often surpassing the baseline algorithms in stability.\nSpecifically, Heuristic-v1 maintains stable performance with low variance across all network sizes and densities. For instance, in Table 1, Heuristic-v1 surpasses the baseline algorithms in the 100 nodes network scenario by achieving a lower variance 0.000069. In Table 2, for edge density $M_0$ = 2, Heuristic-v1 shows better average robustness 0.262601 with lower variance than the baseline algorithms.\nHeuristic-v2 consistently achieves the best performance across most test cases, demonstrating its superior ro- bustness enhancement capabilities. However, it does not always outperform other algorithms in every scenario. For example, in Table 2, for denser networks with $M_0$ = 4 and $M_0$ = 5, Heuristic-v2 does not achieve the highest ro- bustness, being outperformed by SA and Heuristic-v3. Nonetheless, Heuristic-v2 remains one of the top-performing algorithms overall, excelling in the majority of scenarios.\nHeuristic-v3, although not as strong as Heuristic-v2, consistently outperforms the baseline algorithms. For ex- ample, in Table 1, Heuristic-v3 shows strong performance in larger networks, achieving an average robustness of 0.307550 for 200 nodes and 0.307023 for 300 nodes, outperforming all baseline algorithms. In Table 2, for denser networks with $M_0$ = 4 and $M_0$ = 5, Heuristic-v3 achieves the highest robustness with average values of 0.412866 and 0.427933, respectively, demonstrating its effectiveness in denser networks. Similarly, in Table 3, Heuristic-v3 achieves a robustness of 0.219386 on the EU Power Grid Network, outperforming the baseline algorithms."}, {"title": "6. Conclusion", "content": "This paper proposes AutoRNet, an innovative framework designed to generate complete heuristics automatically which can improve the robustness of networks by integrating LLMs with EAs. AutoRNet uses NOS-based variation operations to create domain specific prompts for LLMs and an AFF to transfer hard constraint to soft one so that the searching space is relaxed and the searching process is more effective. The experimental results show that AutoRNet not only can design complete heuristics matching the complexity of manually ones by making use of advanced domain knowledge, but also can explore new strategies that yield better performance at the same practical cost benefiting from the AFF. Three best complete heuristics with different properties generated by AutoRNet were evaluated on both synthetic networks with varying sizes and densities and a real-world network, showing better performance over baseline algorithms. AutoRNet can significantly reduce the need for manual design and large datasets, providing a more flexible and adaptive solution. This leads to broader considerations about the potential of automated heuristic design in optimizing complex networks of the real world."}, {"title": "Appendix A. The detailed population initialization prompt", "content": "The population initialization is achieved by providing Large Language Models (LLMs) with detailed task specifi- cations, which include a description of the problem, function signature, and functionality.\n\u2022 Problem Description: An explanation of the network robustness problem, detailing the optimization goals.\n\u2022 Function Signature: Providing the function signature of the heuristic method to guide LLMs in generating the correct code.\n\u2022 Functionality: Listing the specific functions and operations that the heuristic methods can directly call or use.\nA typical task specification is shown in Figure A1:\nOptionally, heuristic seed codes can be included in the prompt to guide the LLMs. It is already executable and serves as a template. The seed code used in the experiment is designed to modify a given network graph by randomly swapping edges and utilizes the compute_robustness function to evaluate the network's robustness after each modifi- cation, ensuring that only beneficial changes are kept, which is shown in Algorithm A1."}, {"title": "Appendix B. The detailed information of NOSs", "content": "Network Optimization Strategies (NOSs) are a crucial component of AutoRNet. By integrating domain-specific knowledge into the variation operations. NOSs provide structured guidance that helps navigate the complex method space more effectively. Each NOS is composed of three main components: features, strategies, and actions.\n1. Features:\n\u2022 Degree: The number of connections each node has.\n\u2022 Path Characteristics: Shortest path, average path length, and network diameter.\n\u2022 Clustering Coefficient: Local and global measures of how nodes tend to cluster together.\n\u2022 Connectivity: Connected components and the strength of connections between them.\n\u2022 Centrality Measures: Degree centrality, betweenness centrality, closeness centrality, and eigenvector centrality.\n\u2022 Edge Attributes: Weight and direction of edges.\n\u2022 Dynamic Characteristics: Robustness to failures and ability to recover.\n\u2022 Community Structure: Tightly-knit groups within the network.\n2. Strategies:\n\u2022 High-Degree Node Priority: Focus on nodes with many connections.\n\u2022 Low-Degree Node Priority: Focus on nodes with fewer connections.\n\u2022 Betweenness Centrality Priority: Focus on nodes that frequently appear on shortest paths.\n\u2022 Closeness Centrality Priority: Focus on nodes that have short average distances to all other nodes.\n\u2022 Eigenvector Centrality Priority: Focus on nodes that have high influence over the network.\n\u2022 High-Weight Edge Priority: Focus on edges with higher weights.\n\u2022 Low-Weight Edge Priority: Focus on edges with lower weights.\n\u2022 Shortest Path Optimization: Optimize the shortest paths in the network.\n\u2022 Critical Path Optimization: Optimize paths that are crucial for network performance.\n\u2022 Similarity-Based Node Selection: Focus on nodes with similar attributes or roles.\n\u2022 Boundary Node Optimization: Focus on nodes at the boundary of communities or clusters.\n\u2022 Homophily-Based Edge Optimization: Focus on edges connecting nodes with similar attributes.\n\u2022 Heterophily-Based Edge Optimization: Focus on edges connecting nodes with different attributes.\n\u2022 Hub-Peripheral Optimization: Optimize the connectivity between hub nodes and peripheral nodes.\n\u2022 Random Node Selection: Randomly select nodes for optimization to introduce variability.\n\u2022 Central Node Optimization: Focus on nodes that are centrally located within their respective communi- ties.\n3. Actions:\n\u2022 Edge Addition: Involves the addition of new edges to a network, thereby increasing its redundancy and robustness.\n\u2022 Edge Relocation: Refers to the process of moving existing edges from one pair of nodes to another. This strategy alters the degree distribution of the nodes involved.\n\u2022 Edge Swapping: Involves exchanging the endpoints of two edges within the network. This technique preserves the original degree distribution."}, {"title": "Appendix C. C.The detailed Variation Operation prompt", "content": "We define three types of Variation Operation prompts: E1 generates offspring heuristics that are entirely different from the parent heuristics. M1 modifies the current heuristic based on NOSs to enhance its effectiveness. M2 fine- tunes the current heuristic to optimize its efficiency. Each type plays a specific role in the evolution process, balancing exploration and exploitation to enhance network robustness. M1 is given in Figure 3 of the main text, E1, M2 are given in Figures A3 and A4:"}]}