{"title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning", "authors": ["Lekai Chen", "Ashutosh Trivedi", "Alvaro Velasquez"], "abstract": "The emergence of intelligence in large language models (LLMs) has inspired investigations into their integration into automata learning. This paper introduces the probabilistic Minimally Adequate Teacher (pMAT) formulation, which leverages a probabilistic oracle that could give persistent errors randomly during answering the membership queries for deterministic finite automata (DFA) learning. Given the tendency of LLMs to produce hallucinatory content, we have developed techniques to improve answer accuracy and ensure the correctness of the learned automata. We propose the Discrimination prompt as well as the Verification prompt and explore their advantages over common prompts. Additionally, we compare DFA learning performance between the TTT algorithm and common active learning algorithms. To address the exponential number of persistent errors, we implement a dynamic query cache refinement algorithm that identifies and corrects conflicting queries by combining the active and passive learning algorithms. The empirical results demonstrate the robustness and efficiency of our approach, providing a theoretical foundation for automata learning with LLMs in the loop.", "sections": [{"title": "Introduction", "content": "Automata serve as fundamental constructs of encoding knowledge and rules in deep learning area, such as reward machine in reinforcement learning[25, 26], sequence modeling [4], and interpretable neural networks[15, 17]. With the emergence of LLMs, they are exploited as rich sources of human knowledge. LLMs are also valued for their reasoning and planning abilities[30]. However, directly using the text-based responses for logic inference and decision making are often challenging. Therefore, LLMs are trained to answer in more regular and structured ways, such as mathematical expressions[9, 16, 23], codes[7, 19], and structured data (JSON)[12].\nWhile useful in specific contexts like mathematics and programming, these approaches often fall short in capturing formal logical structures. This limitation has led to explorations into using LLMs for automata learning, where LLMs respond to answer membership queries [27] or directly output transitions[1]. However, the probabilistic nature of LLMs makes them unreliable oracles, incapable of consistently producing correct responses, thus complicating the accurate construction of DFAs. None of the methods above can promise to construct a DFA equivalent to the ground-truth model, because they fail at detecting and correcting the wrong response from LLMs. To address this, we introduce the probabilistic Minimally Adequate Teacher (pMAT), shown in figure 1, where the oracle may inaccurately answer membership queries but will always provide valid counterexamples to aid in refining hypotheses. We use this formulation for 3 reasons: 1) LLMs are adept at handling membership queries but struggle with equivalence queries[27]; 2) LLMs cannot be invariably trusted. The errors produced by LLMs are persistent and cannot be corrected by repetitive sampling; and 3) Practical implementations often validate hypotheses directly against the target model, such as in software or environmental simulations, making the valid counterexamples available.\nTo increase the accuracy of membership queries answering, we leverage prompt engineering which is a common solution to enhance the LLM's reasoning ability. One of the most effective and reliable prompt method is Chain of Thought[29], which uses a series of intermediate reasoning steps to teach LLMs how to solve the problem step by step. Inspired by CoT and specificity of our task, we develop two novel prompting strategies, Discrimination prompt and Verification prompt. The prefix and suffix prompt can divide a long word in to a prefix and a suffix such that the length of the membership query can be much lower. It also allows LLMs to utilize the results of previous queries. Concurrently, the Verification prompt, deriving from CoT principles, compels LLMs to substantiate their analyses, fostering more reliable and correct reasoning.\nAnother strategy to construct a correct automaton involves minimizing the number of membership queries by advanced automata learning algorithms. The stochastic nature of LLM outputs implies that increased querying correlates with more errors. Unfortunately, if a learner is given any wrong membership query, it cannot build a correct automaton, because the counterexample will conflict with erroneous queries. They cannot be solved by simply sampling more LLM responses, because LLMs tend to produce the same results for one single query. Therefore, we minimize query re-liance by implementing the TTT algorithm [10], which is renowned for its efficiency in analyzing counterexamples and requires fewer membership queries compared to methods like L* [2, 13].\nDespite these enhancements, the inherent inaccuracy of LLM responses means that a perfect learning outcome cannot be guaranteed solely through reduced querying and improved prompts. Angluin and Kri\u0137is proved that DFAs can be learned with finite exceptions given a reliable equivalence oracle, and they proposed the algorithm LEARNANYWAY to handle erroneous membership queries by simply replacing them with labeled counterexamples [3]. However, in a runtime verification environment, the shortest optimal counterexamples are often not available [10]. As the average length of counterexamples increases, the number of errors introduced by the active learner during hypothesis refinement grows exponentially, rendering the LEARNANYWAY algorithm infeasible due to the massive number of LLM and equivalence queries required, even though the exceptions remain finite."}, {"title": "2 Related Works", "content": "Active Learning Algorithms and the MAT Framework. Active learning algorithms [2, 13, 22, 21, 8, 5, 28, 10], such as the LStar, are pivotal in the domain of DFA learning. These algorithms keep asking an oracle, which provides answers to MQs and EQs, to iteratively refine a hypothesis automaton until it accurately constructs the target automaton. The concept of a Minimally Adequate Teacher (MAT) introduced by Angluin is central to this process [2]. A MAT is an oracle that can answer both MQs\u2014determining whether a given string is part of the target language\u2014and EQs-providing counterexamples when the current hypothesis does not match the target automaton.\nIn practice, oracles may not always provide correct answers to MQs and EQs. Moeller et al. [18] propose a new formulation called the incomplete Minimally Adequate Teacher (iMAT), which addresses scenarios where the teacher has access to only a finite number of tests or has gaps in its knowledge. The iMAT framework can still provide answers that are correct within the limited scope of its knowledge, a guarantee that LLMs cannot offer. Angluin et al. [3] delve into the issues of errors or omissions in membership query responses and the learning of finite variants of concepts in polynomial-time exact learning using membership and equivalence queries. They demonstrate that the class of regular languages, such as DFAs, is learnable in polynomial time with equivalence and malicious membership queries. However, their approach becomes impractical when dealing with the exponential increase in errors in MQs as the average length of the counterexamples grows.\nPassive Learning Algorithms. In contrast, passive learning algorithms like RPNI (Regular Positive and Negative Inference) and its variants (e.g., RPNI-EDSM) infer a DFA from a fixed set of positive and negative examples without interactively querying an oracle. Instead, they rely on state merging [20, 14]. These algorithms construct an initial hypothesis from both the positive and negative examples and refine it to accept all given examples, often achieving a model that generalizes well to unseen data. However, the learning results are often incomplete due to the limited scope of the provided samples and the potential for overfitting [6]. Yang et al. [31] propose a hybrid approach that combines the strengths of active and passive learning algorithms to improve automata learning. They improve L-star algorithm by integrating execution logs and results of passive learning. However, this approach does not address the issue of incorrect membership queries. When an oracle provides erroneous responses, the hybrid approach lacks mechanisms to detect and correct these errors.\nUsing LLMs as Oracles. Recent advancements have explored the use of Large Language Models (LLMs) as oracles in active learning for automata. Vazquez-Chanlatte et al. [27] employ the LStar algorithm, leveraging LLMs to answer membership queries. They enhance the query answer ability of LLMs by allowing the models to say unsure and subsequently use the DISS search algorithm to find answers. However, due to the probabilistic nature of LLMs, their responses may contain persistent errors, undermining the guarantee of learning an automaton equivalent to the target model. Alsadat et al. [1] utilize LLMs to translate natural language knowledge into propositions and transitions of automata. This method refines the learned DFA by utilizing counterexamples collected from the environment. However, this approach also lacks a guarantee of correctness since it does not employ existing DFA learning algorithms, and the learned DFA is merely used as a reward machine to expedite reinforcement learning optimization."}, {"title": "3 Preliminaries", "content": "Deterministic Finite Automata (DFA). The DFA is a common variant of regular languages. Here, we will introduce the basic concepts and notations of a DFA. Let \u2211 be a finite alphabet. Consider a finite set of symbols, \u2211, known as the alphabet. A DFA, denoted as A, over \u2211 is defined as a quintuple A = (QA, \u03a3, q0, \u03b4A, FA), where: QA is a finite set of states; \u2211 is a pre-defined finite alphabet; qA \u2208 QA is the initial state; \u03b4A : QA \u00d7 \u2211 \u2192 QA is the transition function; and FA \u2286 QA is the set of final (or accepting) states.\nFor a symbol a \u2208 \u03a3 and a state q \u2208 QA, the state q' = \u03b4A(q, a) is referred to as the a-successor of q. The transition function extends to words where \u03b4A(q, \u03f5) = q and \u03b4A(q, wa) = \u03b4A(\u03b4A(q, w), \u03b1) for any q\u2208 Q, \u03b1 \u2208 \u03a3, and w \u2208 \u03a3*. To evaluate a word v \u2208 \u03a3* under a DFA, we define the output function as A(q) : \u03a3* \u2192 true, false, such that A(q, v) = true iff \u03b4A(q, v) \u2208 FA. The function A[u] = \u03b4A(q0, u) maps a word u \u2208 \u03a3* to the state reached by u. A query is a tuple (w, [yes, no, unknown]) where w \u2208 \u03a3* is the word to be queried. To represent the cache that stores membership queries and equivalence queries, we use CMQ and CEQ. The access to result of a query x in the cache is denoted by C[x].\nMinimally Adequate Teacher (MAT). The MAT framework involves a Learner (L) attempting to deduce an unknown language by interacting with a Teacher, who responds to two types of queries: Membership Query (MQ) and Equivalence Query (EQ). In an MQ, the Learner proposes a word u, and the Teacher confirms if u is part of the language. In an EQ, the Learner offers a hypothetical automaton H to the Teacher, who either verifies H or provides a counterexample if incorrect.\nThe pMAT Formulation. We introduce a new formulation for DFA learning called the probabilistic Minimally Adequate Teacher (pMAT), which can err in responding to membership queries with a probability \u03f5, though counterexamples remain accurate. Similarly, the automata learner in pMAT aims to learn a target automaton based on partially correct MQ responses and correct EQ responses. The target automaton can be learned solely depending on the counterexamples because they are the only verified answers.\nThe pMAT formulation, See Figure 1, consists of three components: the automata Learner L, the probabilistic membership oracle OMQ, and the equivalence oracle OEQ. The Learner refines its hypothesis based on membership queries and requests for counterexamples. Membership queries involve checking the presence of a word w in the target language, whereas equivalence queries assess if the hypothesis H matches the target model, with a counterexample provided if it does not.\nThe probabilistic nature of the Teacher in pMAT implies that errors may occur independently and at a constant probability \u03f5 for new membership queries, simulated by a binary distribution P. Once an error occurs, it will persist and cannot be corrected by multiple samplings, as there is no guarantee that the more frequent response is more reliable. For example, consider taking N samples from P, where the probability of an erroneous response is \u03f5, but the specifics of which samples are erroneous remain unknown. In contexts like Large Language Models (LLMs), which operate on vast datasets and generally provide accurate responses, errors are rare but can occur, making every query's outcome potentially significant. This necessitates caching query results to avoid losing data from repeated queries, thereby making any errors persistent."}, {"title": "4 Methods", "content": "Verification Prompt. The Verification prompt extends the capabilities of the Chain of Thought (CoT) prompts. Initially, Large Language Models (LLMs) respond to membership queries by performing analyses similar to those demonstrated in example queries as we see in figure 5. However, due to the tendency of LLMs to generate hallucinated content, these analyses may not always be relevant to the actual query input or the language definitions, leading to incorrect membership decisions. To address this issue, we introduce a Verification step.\nUpon receiving the initial response from the LLM to a CoT query, a secondary process begins where a \"teacher\" LLM checks whether the response aligns with the expected definitions and the context of the query. This involves a comparison of the initial analysis against a set of language definitions that constitute a correct response, as well as the original query input. The LLMs are required to output a JSON object containing three components:\n\u2022 Whether the analysis matches the language definition,\n\u2022 Whether the analysis is consistent with the query input,\n\u2022 The reasons why the analysis matches the definitions, as well as why the analysis is based on the query input.\nIf the teacher finds discrepancies between the initial answer and the expected standards, the following actions are taken: a) If the input sequence and definition match but the initial response was incorrect, the answer is inverted (changed from true to false, or vice versa). b) If there is a mismatch in the input sequence or definition, the system revises its additional prompt based on the teacher's analysis and re-evaluates the query. This verification step significantly enhances the reliability of the system's outputs.\nBy ensuring that each membership query's response is not only generated but also rigorously checked, the LLMs minimize errors and align their outputs closely with accurate interpretations of the language rules.\nDiscrimination Prompt. The idea behind the Discrimination Prompt is that LLMs perform better with examples compared to direct inference. However, handling the large volume of membership queries necessary for learning an automaton is challenging, particularly when it's impractical to present all cached queries to LLMs. That is, we have to find the most similar ones to prompt LLMs. Intuitively, words that appear similar should share properties within the same language, but this isn't universally applicable. See the example in figure 2. Consider an automaton in (a), for a membership query on the word (abbbbb), among the cached queries in (c). The (bbbbbb) appears more similar due to a lower edit distance. Yet, prompting LLMs with \u3008bbbbbb\u3009 might lead them to incorrectly infer non-membership, mirroring its negative example. Conversely, using (a) as a prompt highlights that valid strings should start with 'a', though it might not instill confidence due to significant differences from the query word.\nTo address these challenges, we implement the Discrimination prompt, which maintains a discrimination tree in (b) to remember the equivalence queries and their relationships. Upon receiving a new membership query, the algorithm identifies the query's position within the tree-specifically, which leaf (state) the new word belongs to. It then proceeds to the lowest common ancestor of that leaf and selects the most similar word by edit distance from each child of that ancestor. Words on the same leaf share the same state in the hypothesized automaton, while words on adjacent leave (sub-tree) diverge in their outputs upon receiving identical inputs. In this case, both the (a) and \u3008bbbbbb\u3009 will be chosen to prompt LLMs. A more detailed process can be found in algorithm 2. This method ensures that selected queries not only exhibit similarity in terms of edit distance but also align in their properties, thereby teaching the LLM what is permissible and what is not in the language."}, {"title": "4.1 LEARNANY WAY with Passive Refinement", "content": "Angluin et al. [3] investigate the issues of errors in membership queries and the polynomial-time learnability of DFAs using MQs and EQs). They demonstrate that DFAs can be learned in polynomial time despite malicious MQs. However, their method becomes impractical when the error rate in MQs grows exponentially with the average length of counterexamples.\nFigure 3 illustrates the relationship between MQs, EQs, and errors as the average length of counterex-amples increases. Since active learning is driven by counterexamples, longer counterexamples extend the learning process. It is evident that the error count increases exponentially as the length of counter examples grows, even when the number of MQs and EQs plateaus.\nThe cost of asking tens of thousands of MQs to large language models (LLMs) for such a simple language is prohibitive. Moreover, when LEARNANYWAY cannot receive exactly same counterexam-ples to incorrect MQs, it fails to promptly correct persistent errors, causing error accumulation as new counterexamples are processed. Therefore, LEARNANYWAY struggles in environments where the shortest optimal examples are unavailable.\nAlgorithm. This algorithm, named LEARNANYWAY with Passive Refinement (LAPR), involves two distinct modes of learning: active learning and passive learning. It uses a pMAT, referred to as an oracle, which provides feedback through membership queries. We build our membership query process upon LEARNANYWAY, it adds itself to the CMQ or replace the wrong query in CMQ with its label. Initially, like other common active learning algorithms, the LAPR learner begins to hypothesize a model based on the input data it receives from the OMQ. The active learner submits its hypothesis to the OEQ, which either approves it if the hypothesis is equivalent to the target model or provides a counterexample. When a counterexample is received, it is recorded in a buffer that keeps track of MQs used during the analysis on equivalence queries. If the active learner successfully refines the hypothesis using the counterexample, the related membership queries during this refinement are stored in a dictionary for further use.\nHowever, if the refinement based on the counterexample fails or the counterexamples accumulate beyond a practical limit, the algorithm shifts to a passive learning phase. In this phase, the algorithm samples data points from the previously collected MQs and EQs to form a new training set consisting of both positive and negative examples. The passive learner then constructs its own hypothesis Hp based on this new dataset, and use Hp to correct the related MQs. It also try to estimate the error rate (\u20ac), which measures the inconsistency between the outputs of the passive hypothesis and the outputs from the membership query cache. If this error rate exceeds a pre-determined threshold, indicating significant inaccuracies, the algorithm reverts to using the passive learning mode, but now guided more intensively by the oracle to refine the hypothesis.\nTherefore, the algorithm acts more like classical active learning algoithms when \u03f5 is low. In contrast, the LAPR converge to passive learning algorithms when the \u03f5 is high. The overall goal of the LAPR"}, {"title": "5 Empirical Results", "content": "In this section, we report on a thorough analysis of the proposed prompts and algorithmic comparisons. we employed the implementations of active and passive learning algorithms available in LearnLib [11]. This allowed for a standardized and fair comparison across different approaches. Our experiments were based on a dataset of DFAs derived from the 28 exercises provided in [24], which offer a range of DFAs designed around introductory concepts in automata theory. These DFAs were chosen based on the assumption that if human learners can intuitively understand and construct DFAs, then LLMs should also possess the capability to effectively handle similar tasks.\nThe selected DFAs, while structurally simple, serve as an effective medium to assess the membership query answering capabilities of LLMs. This simplicity allows us to isolate and examine the effects of increasing the complexity of test scenarios by incrementally adjusting two main parameters in our experiments: the length of counterexamples and the error probability \u03f5 in the pMAT framework. Additionally, for each DFA, we provided one positive and one negative example, complete with detailed explanations as to why these examples are or are not part of the target language."}, {"title": "5.1 Prompt Study", "content": "The results depicted in Figure 3 demonstrate that the Chain of Thought (CoT) methodology signifi-cantly enhances the accuracy of membership queries. When compared to a baseline prompt, which asks LLMs to answer yes or no, the introduction of CoT prompts reduces the error rate \u03f5 (calculated as the ratio of errors to total unique membership queries) to 0.068. This improvement is attributed to CoT's capacity to generate more structured and contextually enriched responses from LLMs, thereby offering clearer guidance throughout the DFA learning process.\nFurther analysis reveals that the Verification prompt, which examines the alignment between the query inputs and the CoT responses and verify the valid usage of the target language definitions, considerably improves query accuracy. This prompt encourages LLMs to self-check and correct, which is crucial as it was observed that LLMs sometimes provided responses that were directly contradictory to their reasoning response. Incorporating Verification prompts reduced \u03f5 to 0.044, marking the lowest error rate among the different prompting strategies tested.\nAdditionally, the Discrimination prompt, designed to utilize information from counterexamples to determine membership status, also significantly lowers \u03f5 to 0.047. This prompt selects the most relevant counterexamples based on a discrimination tree and edit distance metrics. Notably, while the \u03f5 for the Discrimination prompt is not the lowest, it achieved the highest learning accuracy rates, achieving 0.571 for TTT and 0.536 for LStar, suggesting that this prompt effectively captures similarities between the inputs of membership queries and the counterexamples.\nAdditional finding is that the reduced number of membership queries correlates with decreased error rates in predictions made by the learned model, particularly in contexts where a probabilistic language model (LLM) serves as the oracle. The probabilistic nature of LLMs implies that fewer queries could reduce the cumulative likelihood of error propagation, thereby enhancing the overall quality of the learned automata. Our data suggest that TTT, by minimizing the number of queries, may thus be more effective at converging towards the correct automata structure."}, {"title": "5.2 Algorithm Comparison", "content": "We compared our LAPR algorithm with traditional DFA learning methods such as LStar and TTT, as well as the LEARNANYWAY algorithm, which handles incorrect membership queries through counterexamples. Our experimental setup involved restricting counterexample lengths to 5 and capping membership queries at 50 per DFA learning session, considering any excess as a timeout.\nThe comparative results, outlined in Table 2, illustrate that while LStar and TTT falter with incorrect membership queries, LEARNANYWAY adjusts queries via counterexamples but struggles with complex DFAs due to the need for additional queries to analyze lengthy counterexamples. In contrast, LAPR consistently forms equivalent automata even without optimal counterexamples, employing a dual approach of using MQs and EQs to learn from and correct the persistent errors introduced by LLMs.\nTo further validate LAPR's scalability and effectiveness, we introduced a probabilistic oracle scenario in our experiments, where the oracle might erroneously answer membership queries with a probability \u03f5, ranging from 0 to 1. Here, the counterexample length for equivalence queries was set to 10. As shown in Figure 4, LAPR required fewer MQs than LEARNANYWAY, and unlike LEARNANYWAY, which needed exponentially more counterexamples as \u03f5 increased, LAPR maintained a stable number of required counterexamples, comparable to those needed by RPNI-EDSM.\nAn interesting observation was that with \u03f5 greater than 0.18, passive algorithms like RPNI-EDSM proved more efficient than active learning methods due to their reliance solely on counterexamples. However, as shown in Table 1, proper prompting can reduce \u03f5 to below 0.1 when using LLMs as oracles, making our LAPR method more effective than both LEARNANYWAY and RPNI-EDSM in such settings."}, {"title": "5.3 Discussion, Limitations, and Challenges", "content": "Despite the advances made with our LEARNANYWAY with Passive Refinement (LAPR) algorithm, there remains scope for enhancement, particularly in terms of the algorithm's efficiency in halting execution when faced with low-quality queries. Critical to this improvement is the ability to more precisely estimate \u03f5, the error rate, and to detect conflicts with greater accuracy. These enhancements necessitate an improved design of the passive learner component.\nA potential challenge arises from the trade-off between building a proper dataset, including all queries from CMQ and CEQ, and another approach that focuses solely on counterexamples. Utilizing a complete dataset tends to introduce noise due to the erroneous data stemming from the LLMs' responses. This situation can lead to a critic that does not accurately reflect the target automaton's structure due to the high incidence of errors. On the other hand, constraining the training data to only include counterexamples ensures the correctness of the training inputs but results in a significantly smaller dataset. This limited dataset can cause the passive learner to overfit to these examples, making it less generalizable and poorly equipped to handle new, unseen queries effectively. This situation underscores the need for a balanced approach in training the passive critic, where both the integrity of training data and the volume of data are optimized to enhance the overall performance of LAPR."}, {"title": "6 Conclusion", "content": "This paper explored the integration of Large Language Models (LLMs) into the domain of automata learning, culminating in the development of the probabilistic Minimally Adequate Teacher (pMAT) framework. This approach capitalizes on the probabilistic and sometimes erroneous outputs of LLMs to answer membership queries, supplemented by accurate counterexamples for hypothesis refinement.\nOur key contributions include the introduction of innovative prompting strategies-the Discrimination and Verification prompts, which significantly enhance the reasoning accuracy of LLMs when responding to membership queries. Furthermore, we implemented and evaluated the LEARNANYWAY with Passive Refinement (LAPR) algorithm, which adeptly combines active and passive learning techniques. This hybrid approach allows for effective management of the persistent errors typical in LLM outputs, thereby facilitating the construction of accurate automata even when faced with high error rates in membership queries. The empirical results underscore the robustness and efficiency of LAPR, particularly in comparison to traditional DFA learning algorithms like LStar and TTT, and other methods such as LEARNANYWAY that rely solely on active learners.\nThrough rigorous empirical evaluation, we demonstrated that our LEARNANYWAY with Passive Refinement (LAPR) algorithm outperforms traditional DFA learning algorithms such as LStar and TTT, particularly in environments where counterexamples may be lengthy or suboptimal. LAPR's ability to dynamically switch between active and passive learning phases allows for efficient error correction and hypothesis refinement, effectively handling the exponential increase in persistent errors associated with higher counterexample lengths. This approach mitigates the inherent limitations of using LLMs as oracles and enhances the potential for developing accurate and reliable automata for a wide range of applications in computational linguistics, software engineering, and beyond."}, {"title": "A Appendix / supplemental material", "content": "A.1 Verification Prompt Example"}, {"title": "A.2 Discrimination Prompt Words Selecting Algorithm", "content": "Algorithm 2 Find Similar Counterexamples by Discrimination Tree\nprocedure HISTORYORACLE(C,q)\nif q \u2208 CEQ then\nreturn CEQ[9]\nelse\nreturn False\nend if\nend procedure\nRepresent HistoryOracle by OH\nprocedure DISCRIMINATION BASED WORD SEARCH(q)\nConstruct the hypothesis H via any active learner by OH\nBuild the discrimination tree DT based on H\nFind the lowest common ancestor d of H[q] and another child s of d.\nUse Levenshtein Distance L to estimate the similarity between two words.\nInitialize lq \u2190 inf, ls \u2190 inf.\nLet L represent Levenshtein Distance.\nfor each query qi in the C do\nif H[qi] = H[q] and L(qi, q) < lq then\nlq \u2190 L(qi, q)\nWq \u2190 qi\nelse if H[qi] = H[s] and L(qi, q) < ls then\nls \u2190 L(qi, q)\nWs \u2190 qi\nend if\nend for\nreturn Wq, Ws\nend procedure"}]}