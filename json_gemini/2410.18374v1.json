{"title": "Integrating Canonical Neural Units and Multi-Scale Training for Handwritten Text Recognition", "authors": ["Zi-Rui Wang"], "abstract": "The segmentation-free research efforts for addressing handwritten text recognition can be divided into three categories: connectionist temporal classification (CTC), hidden Markov model and encoder-decoder methods. In this paper, inspired by the above three modeling methods, we propose a new recognition network by using a novel three-dimensional (3D) attention module and global-local context information. Based on the feature maps of the last convolutional layer, a series of 3D blocks with different resolutions are split. Then, these 3D blocks are fed into the 3D attention module to generate sequential visual features. Finally, by integrating the visual features and the corresponding global-local context features, a well-designed representation can be obtained. Main canonical neural units including attention mechanisms, fully-connected layer, recurrent unit and convolutional layer are efficiently organized into a network and can be jointly trained by the CTC loss and the cross-entropy loss. Experiments on the latest Chinese handwritten text datasets (the SCUT-HCCDoc and the SCUT-EPT) and one English handwritten text dataset (the IAM) show that the proposed method can make a new milestone.", "sections": [{"title": "I. INTRODUCTION", "content": "Handwritten text recognition (HTR) is a typical sequence-to-sequence problem. It can be formulated as a Bayesian decision problem. The research efforts include segmentation-based methods [1], [2] and segmentation-free methods [3], [4], [5]. The former methods usually depend on the detected boxes of characters or extra training data. Compared with the segmentation-based methods, only text-level labels are needed during the training stage in the segmentation-free methods.\nThere are three typical segmentation-free methods, i.e., hidden Markov model (HMM) [3], [6], connectionist temporal classification (CTC)[7], [5], [8], [9], [10] and encoder-decoder (ED) framework [11], [12], [13], [14]. As shown in Fig. 1, in the HMM-based method, each character is modeled by an HMM and a text line can be represented by cascaded HMMs. A series of frames extracted from an original image by a left-to-right sliding window are assigned to the underlying states. Then, a neural network is used to estimate the posterior probabilities of the states, while the outputs of networks in CTC and ED-based approaches are character classes. In the CTC loss, a special character \"blank\u201d and a sophisticated rule are designed to split different characters, and the forward-backward algorithm can efficiently compute the probability of the underlying character sequence. In an encoder-decoder network, an image is usually fed into the encoder to generate the corresponding middle features. Based on the middle representations, the decoder is used to locate and predict the character sequence via attention mechanisms. The common cross-entropy loss can be directly used to adjust the parameters of the ED network.\nAlthough characters can be represented by a high resolution and compact HMM [6], the network used to model the state posterior probability has a large number of output nodes and can not be trained in an end-to-end way. Moreover, it is reasonable to explicitly model the 2D information of characters. However, it is very difficult to expand 1D HMM to 2D HMM [15] due to the computational complexity. Even for the CTC, ED-based approaches [16], [17], [8], [12], early networks just simply depend on the local receptive field of convolutional layers or recurrent units and gradually shrink the height of feature maps to 1 pixel via stacked pooling layers [18], which may lead to information loss.\nIn this paper, inspired by the above three modeling ways, we propose a new recognition network by using a novel 3D attention module and global-local context information. The 3D attention module is employed to explicitly extract 2D information of blocked feature maps with different resolutions. In detail, the 3D attention module is ingeniously decoupled into a 2D self- attention operation and a 1D attention-based aggregation operation. Based on the outputs (visual features) of the 3D attention module, we further extract the corresponding global-local context information via the self-attention and the recurrent unit, respectively. Finally, by integrating the visual features and the corresponding global-local context features, a well-designed representation for the recognition task can be obtained. In summary, the main contributions of this paper are"}, {"title": "II. RELATED WORK", "content": "In this section, we review related work, including recent advances of the text recognition task, different attention mechanisms and auxiliary features used in the text recognition.\nRecent Advances of the Text Recognition Task\nThe text recognition task includes the scene text recognition and the handwritten text recognition. In recent years, state-of-the-art approaches have changed from multiple procedures to end-to-end training and inference. For the scene text recognition, contrastive learning methods of different levels (character [19], [20], subword and word [21]) have been introduced. Zheng et al. [22] propose the multiplexed routing network for multilingual text recognition. Different from the early attention mechanisms [23], [24], glyph information obtained by the k-means cluster is used to generate more accurate attention. On the other side, many researchers focus on integrating visual and language information [25], [26], [27].\nFor the handwritten text recognition, Hoang et al. [9] combine the radical-level CTC loss and the character-level loss while Ngo et al. [14] construct a joint decoder to integrate the visual feature and the linguistic context feature. More recently, Peng et al. [2] built a full convolution network to simultaneously achieve the purpose of the character location, the character bounding boxes prediction and the character prediction. Lin et al. [28] propose a mobile text recognizer via searching the lightweight neural units. Furthermore, Peng et al. [29] detect and recognize characters in page-level handwritten text while Coquenet et al. [30] employ a fully convolutional network as the encoder with a transformer decoder for document recognition.\nAttention Mechanism\nSince the attention was first introduced in the neural machine translation [11], it has been widely used in different fields. The attention methods used in the text recognition task include two aspects: local attention and non-local attention. The former ones can generate a good representation for the current time step, e.g., the channel-wise attention [31], the spatial-wise attention [32], the activation-wise attention [33], the multi-aspect attention [34], [10] in convolutional layers. The non-local attention across different time steps $X = {X1, ..., Xt, ..., Xn}$can be formulated via three vectors, i.e., the query $qt$, the value $vt$, and the key $kt$ [35]. In a typical self-attention block, the queries, keys and values are transformed from the corresponding input $X, and then"}, {"title": "III. METHODOLOGY", "content": "As shown in Fig. 2, the proposed network includes three parts, i.e., the convolutional neural network (CNN), the 3D attention module and the features integration block. In the CNN, the hybrid attention module (HAM) [10] is used and each convolutional layer is equipped with the bath normalization [41]. For the outputs of the CNN, we employ a multi-scale framing strategy to extract different solutions. In this section, we elaborate on the details of the 3D attention module and the global-local context information. Moreover, the training pipeline based on the CTC loss and the CE loss is also shown.\n3D Attention\nAs shown in Fig. 3, assuming the front CNN has the output tensor $O \\in R^{C\\timesW\\timesH}$ with each feature map $O_c\\in R^{W\\timesH}$, each 3D block represented by a sliding window $f \\in R^{C\\timesS\\timesH}$ from"}, {"title": "A. 3D Attention"}, {"title": "B. Features Integration", "content": "Through the 3D attention module, a series of 3D feature maps can be transformed into the corresponding visual features $r_t(t = 0...T)$. As shown in Fig. 4-(a), for the visual feature $r_t$, the global context feature $l_t$ and the local context $s_t$ can be extracted by using the self-attention mechanism and the recurrent units, respectively. In this paper, we directly employ the computation method of the $f'$ in Fig. 3 to obtain the global features $l_t$. For the local features, the standard"}, {"title": "C. Joint Training of the CTC and the CE Losses", "content": "Given the feature sequence $V$ of a text line image, the text recognition task is to find the corresponding underlying n-character sequence $C = {C1, C2, ..., Cn}$, i.e, compute the posterior"}, {"title": "IV. EXPERIMENTS", "content": "The proposed network is validated on two Chinese handwritten text datasets(the SCUT-HCCDoc[44] and the SCUT-EPT[43]) and one English handwritten text dataset: the IAM[45].\nAll the images of SCUT-HCCDoc were obtained by Internet search. According to certain rules, only 12,253 images were preserved from the initial 100 thousand candidate images. These images were randomly split into the training and test sets with a ratio of 4:1. After splitting, the training set contains 9,801 images with 93,254 text instances and 925,200 characters. The test set contains 2,452 images with 23,484 text instances and 230,019 characters.\nThe Chinese handwritten text data set SCUT-EPT contains 4,250 categories of Chinese characters and symbols, totaling 1,267,162 characters, and it was split into 40,000 training text lines and 10,000 test text lines. These samples were collected from examination papers and were written by 2,986 students, and the training and test sets do not include the same writers. There are a total of 4,250 classes, but the number of classes in the training set is just 4,058. Therefore, some classes in the testing set do not appear in the training set and can not be correctly recognized.\nFor the English handwritten text recognition, we evaluate the performance of our method on the IAM dataset. The IAM dataset contains a total number of 9,862 text lines written by 500 writers. It provides one training set, one test set and two validation sets. The text lines of all data sets are mutually exclusive, thus, each writer has contributed to one set only.\nTable I lists the number of text lines, characters and classes in both Chinese datasets. In the SCUT-EPT dataset, we removed 681 training text images including abnormal structures, i.e. characters swapping and overlap. Similarly, in the SCUT-HCCDoc dataset, some low-quality images were removed and only 91,261 text instances were used in the training set. All test"}, {"title": "B. Experimental Results and Analysis", "content": "During the training stage, common data argument methods including text location shift, image blur, grey level and contrast changes, and linear transformation were used. The Adam optimiza- tion algorithm with default parameters was adopted, we adjusted the learning rate according to training steps until the model converged into a small range. At certain epochs, the learning rate was multiplied by 0.5. The deep learning platform Pytorch [46] and an NVIDIA RTX A6000 with 48GB memory were used.\nFirstly, we conducted ablation experiments to verify the effectiveness of the proposed network.\nAblation Experiments: In this section, we examine several important factors for recognition performance. These factors include the frame resolution, the 3D attention module, the global- local context information and the joint training."}, {"title": "V. CONCLUSION", "content": "In this paper, inspired by the segmentation-free methods, we propose a new recognition network by using a novel 3D attention module and global-local context information. Main canonical neural units including attention mechanisms, fully-connected layer, recurrent unit and convolutional layer are ingeniously organized into a network. The network weights are effectively optimized by the multi-scale training. Experiments on the latest Chinese handwritten text datasets and the English handwritten text dataset show that the proposed method can achieve comparable results with the state-of-the-art methods. Besides, the visualization analysis demonstrates the proposed"}]}