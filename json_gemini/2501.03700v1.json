{"title": "AuxDepthNet: Real-Time Monocular 3D Object Detection with Depth-Sensitive Features", "authors": ["RUOCHEN ZHANG", "HYEUNG-SIK CHOI", "DONGWOOK JUNG", "PHAN HUY NAM ANH", "SANG-KI JEONG", "ZIHAO ZHU"], "abstract": "Monocular 3D object detection is a challenging task in autonomous systems due to the lack\nof explicit depth information in single-view images. Existing methods often depend on external depth\nestimators or expensive sensors, which increase computational complexity and hinder real-time performance.\nTo overcome these limitations, we propose AuxDepthNet, an efficient framework for real-time monocular\n3D object detection that eliminates the reliance on external depth maps or pre-trained depth models.\nAuxDepthNet introduces two key components: the Auxiliary Depth Feature (ADF) module, which implicitly\nlearns depth-sensitive features to improve spatial reasoning and computational efficiency, and the Depth\nPosition Mapping (DPM) module, which embeds depth positional information directly into the detection\nprocess to enable accurate object localization and 3D bounding box regression. Leveraging the DepthFusion\nTransformer architecture, AuxDepthNet globally integrates visual and depth-sensitive features through\ndepth-guided interactions, ensuring robust and efficient detection. Extensive experiments on the KITTI\ndataset show that AuxDepthNet achieves state-of-the-art performance, with AP3D scores of 24.72% (Easy),\n18.63% (Moderate), and 15.31% (Hard), and APBEV scores of 34.11% (Easy), 25.18% (Moderate), and\n21.90% (Hard) at an IoU threshold of 0.7.", "sections": [{"title": "I. INTRODUCTION", "content": "THREE-DIMENSIONAL (3D) object detection [1]\u2013[5]\nplays a critical role in autonomous driving and robotic\nperception by enabling machines to perceive and interact with\ntheir surroundings in a spatially aware manner. Traditionally,\nthis task has relied on precise depth information from multiple\nsensors, such as LiDAR, stereo cameras, or depth sensors.\nWhile these systems provide high accuracy, they are often\nexpensive and complex to deploy. In recent years, monocular\n3D object detection has gained attention as a cost-effective\nalternative, requiring only a single RGB camera [6]\u2013[8]. De-\nspite its simplicity and accessibility, monocular 3D detection\nfaces challenges due to the absence of explicit depth cues,\nmaking effective depth reasoning essential for improving ac-\ncuracy and practicality.\nMonocular 3D object detection methods can be broadly\ncategorized into two approaches, as illustrated in Fig. 1. The\nfirst category utilizes pre-trained depth estimation models\nto generate pseudo-depth maps, which are combined with\nLiDAR-based 3D detectors for object recognition and local-\nization [9], [10]. This approach, as exemplified by methods\nlike Pseudo-LiDAR and F-Pointnet [5], [11], [12], achieves\nimproved localization but suffers from inaccuracies in depth\npriors and high computational costs due to the reliance on\nexternal depth estimators, limiting their applicability in real-\ntime scenarios.\nThe second category focuses on feature fusion, where\nfeatures from monocular images and estimated depth maps\nare extracted and combined to enhance detection. Methods\nlike D4LCN and CaDDN [13], [14], shown in Fig. 1(b), use\nspecialized convolutional architectures to integrate visual and\ndepth features. While these methods demonstrate promising\nresults, their reliance on the quality of depth maps and the\ncomplexity of their architectures can hinder efficiency and\nrobustness in dynamic environments.\nTo address these challenges, we propose AuxDepthNet,\na novel framework for real-time monocular 3D object de-\ntection that avoids the need for external depth estimators or"}, {"title": "II. RELATED WORK", "content": "Monocular 3D object detection has gained significant atten-\ntion for its cost-effectiveness compared to sensor-based ap-\nproaches, such as LiDAR and stereo cameras. Existing meth-\nods can be broadly categorized into depth-based methods and\nTransformer-based methods, which are discussed below."}, {"title": "A. MONOCULAR 3D OBJECT DETECTION METHODS", "content": "Image-based 3D object detection leverages monocular or\nstereo images to estimate depth and generate 3D proposals.\nApproaches like MONO3D [13], MLF [19], and Frustum\nPointNet [20] use depth maps or disparity estimates to com-\npute 3D coordinates, often integrating them into 2D detection\npipelines. However, poor depth representations in these meth-\nods limit the ability of convolutional networks to accurately\nlocalize objects, especially at greater distances, leading to per-\nformance gaps compared to LiDAR-based systems. Depth-\nbased methods aim to bridge this gap by explicitly or implic-\nitly leveraging depth information. One common approach is\nto estimate pseudo-depth maps from monocular images and\nutilize them for 3D object detection. For instance, Pseudo-\nLiDAR [5] and F-PointNet [12] rely on pre-trained depth\nmodels to generate point clouds, which are then processed\nwith LiDAR-style 3D detectors. While these methods im-\nprove spatial reasoning, they suffer from depth estimation\nerrors and increased computational costs due to reliance on\nexternal depth estimators.\nAnother line of work focuses on integrating monocular im-\nage features with estimated depth information in a feature fu-\nsion framework. Methods like D\u2074LCN [13] and CaDDN [21]\nincorporate depth-sensitive features into the 2D detection\npipeline to enhance accuracy. However, these approaches\nare constrained by the quality of the estimated depth maps\nand often rely on complex architectures, limiting their real-\ntime applicability. In contrast, our proposed method avoids\nreliance on external depth estimators by directly learning\ndepth-sensitive features through the Auxiliary Depth Feature\n(ADF) module. This design allows AuxDepthNet to achieve\nhigh accuracy while maintaining computational efficiency."}, {"title": "B. TRANSFORMER IN MONOCULAR 3D OBJECT\nDETECTION", "content": "The recent success of Transformers in computer vision has\ninspired their application in monocular 3D object detection.\nUnlike convolutional neural networks (CNNs), Transform-\ners capture global spatial dependencies using self-attention\nmechanisms, addressing the limitations of local feature ex-\ntraction. For example, MonoDETR [22] employs a dual-\nencoder architecture and a depth-guided decoder to im-\nprove depth representation and object localization. Simi-\nlarly, MonoPSTR [23] introduces scale-aware attention and\nposition-coded queries to enhance detection precision and\nefficiency. Recently, diffusion models [24]\u2013[27] have also\ndemonstrated their potential in depth estimation by iteratively\nrefining depth predictions, offering an alternative to tradi-\ntional depth map generation methods. These models could\ncomplement Transformer-based architectures by providing\nrobust depth priors for enhanced detection.\nDespite their advancements, Transformer-based methods\noften rely on pre-computed depth maps or handcrafted priors\nto guide detection [17], [28]. This dependence on external in-\nputs can hinder adaptability to diverse scenarios. AuxDepth-\nNet addresses this limitation by embedding depth reasoning"}, {"title": "III. PROPOSED METHOD", "content": "directly into the network via the Auxiliary Depth Feature\n(ADF) and Depth Position Mapping (DPM) modules. These\nmodules enable the model to implicitly learn depth-sensitive\nfeatures, eliminating external dependencies and delivering\nrobust, scalable performance."}, {"title": "A. OVERVIEW", "content": "As shown in Fig.2, the AuxDepthNet architecture leverages\na multi-faceted feature representation to enhance monoc-\nular 3D object detection. Specifically, the model incorpo-\nrates depth-sensitive features, context-sensitive features, and\ndepth-guided features to capture and integrate crucial infor-\nmation at different stages. The depth-sensitive features are\nextracted through the Auxiliary Depth Feature (ADF) mod-\nule, which implicitly encodes depth-related cues via auxil-\niary learning, eliminating the need for pre-computed depth\nmaps. Context-sensitive features, generated by the backbone\nand refined through the feature pyramid and DepthFusion\nTransformer (DFT), provide semantic and spatial context for\naccurate object detection. Depth-guided features, enhanced\nby the Depth Position Mapping (DPM) module and positional\nencoding, embed depth-positional cues into the feature space,\nenabling precise spatial reasoning and robust 3D localization.\nThe combined integration of these feature types ensures the\nframework captures both local and global spatial relation-\nships, providing robust 2D and 3D object detection with\nminimal computational overhead."}, {"title": "B. DEPTH-SENSITIVE FEATURE ENHANCEMENT", "content": "Current depth-assisted approaches face challenges in gener-\nalizing to varied datasets and environments, which hampers\ntheir adaptability. Moreover, their dependency on external\ndepth sensors or estimators not only increases hardware re-\nquirements but also risks propagating inaccuracies into the\ndetection pipeline [5], [13], [17], [18]. To address these chal-\nlenges, we propose the Auxiliary Depth Feature (ADF) mod-\nule, which leverages auxiliary supervision during training to\nimplicitly learn depth-sensitive features, eliminating the need\nfor external depth maps or estimators.\nThe ADF module is designed as a lightweight and effi-\ncient solution that enhances depth reasoning and spatial lo-\ncalization while maintaining low computational cost. Unlike\nprevious methods [3], [5], [8], which rely on pre-computed\ndepth maps or external estimators, the ADF module captures\ndepth-sensitive features directly from the input feature map\nusing auxiliary learning. This approach ensures scalability,\nimproves generalization across datasets, and enables real-\ntime applicability.\nAs illustrated in Fig. 3, the Auxiliary Depth Feature\n(ADF) module operates in three stages to enhance depth-\nsensitive features. First, it generates initial depth-sensitive\nfeatures through an auxiliary supervision task, which predicts\na probability distribution over discretized depth bins for each\npixel. Next, the module learns depth prototypes by leverag-\ning spatial-depth attention mechanisms, aggregating features"}, {"title": "1) Extracting Foundational depth-sensitive Features.", "content": "In the Auxiliary Depth Feature (ADF) module, foundational\ndepth-sensitive features are generated using an auxiliary\ndepth estimation task modeled as a sequential classification\nproblem. Given the input feature map $F \\in R^{C\\times H \\times W}$ from\nthe backbone, we apply two convolutional layers to predict\nthe probability distribution of discretized depth bins $D_b \\in\nIR^{D\\times H \\times W}$, where D\u266d denotes the number of depth categories.\nThe predicted probability represents the confidence of each\npixel belonging to a specific depth bin:\n$P_{i,j,d} = \\frac{exp (Z_{i,j,d})}{\\sum_{k=1}^{D_b} exp (Z_{i,j,k})}$\nwhere $Z_{i,j,d}$ is the raw score for pixel (i,j) in bin n.\nTo discretize continuous depth values into bins, we utilize\nLinear-Increasing Discretization (LID), which refines depth\ngranularity for closer objects while allocating broader inter-\nvals for farther ones. The discretization can be formulated as:\n$B_i = \\begin{cases} \\frac{(i+1)^2}{D}, & \\text{if } i < \\sqrt{D}, \\\\ \\frac{(i+1) \\sqrt{D}}{D}, & \\text{otherwise.} \\end{cases}$\nBy applying depthwise separable convolutions, the inter-\nmediate feature map $X \\in R^{C\\times H \\times W}$ efficiently captures\ndepth-sensitive features with reduced computational over-\nhead. These features, further enhanced through attention\nmechanisms, provide a robust foundation for downstream 3D\nobject detection tasks."}, {"title": "2) Depth-Sensitive Prototype Representation module.", "content": "This module aims to refine feature representations through\ndepth prototype learning and enhancement. Starting from\nthe initial depth-sensitive feature map $F_{init} \\in R^{C\\times H \\times W}$,\ngenerated by the backbone network, the module predicts the\ndepth distribution $P_{depth} \\in R^{D\\times H \\times W}$ for each pixel, where C\nis the feature dimension, H and W are spatial dimensions,\nand D represents the number of depth bins. The predicted\nprobability represents the confidence of each pixel belonging\nto a specific depth bin, which is computed using softmax\nnormalization. To enhance the feature extraction process,\ndilated convolutions(D-Conv) are employed to enlarge the\nreceptive field while maintaining spatial resolution. Using\nthe predicted depth distribution, the module estimates depth"}, {"title": "C. EXTRACTING FOUNDATIONAL DEPTH-SENSITIVE\nFEATURES", "content": "Inspired by the success of Transformer architectures in cap-\nturing long-range dependencies and modeling global relation-\nships [29], we introduce the Depth Position Mapping (DPM)\nmodule as a key component of our framework. The DPM\nmodule is designed to address the challenges of integrating\nspatial and depth cues in monocular 3D object detection.\nUnlike traditional methods that rely solely on local visual\nfeatures or predefined depth priors, DPM leverages a depth-\nguided mapping mechanism to explicitly encode positional\ndepth information into the feature space. By embedding depth\npositions into learnable queries within an encoder-decoder\nstructure, the module enables precise alignment of spatial\nand depth representations. This design enhances the under-\nstanding of scene-level depth geometry while improving the\nquality of feature fusion for downstream 3D attribute predic-\ntion. The DPM module thus provides a robust and efficient\nsolution for bridging the gap between depth estimation and\nobject localization in monocular 3D object detection."}, {"title": "1) Transformer Encoder.", "content": "The Transformer Encoder in the Depth Position Mapping\n(DPM) module plays a critical role in capturing global de-\npendencies and refining feature representations by leverag-\ning self-attention mechanisms. It enables the model to in-\ncorporate contextual information across the entire sequence,\nfacilitating a deeper understanding of spatial and depth rela-\ntionships. The encoder refines input features by employing a\nmulti-head self-attention mechanism and a feed-forward neu-\nral network (FFN) [17], [28].Given the input feature tensor\n$X \\in R^{N\\times L\\times C}$ where N is the batch size, L is the sequence"}, {"title": "3) Depth Position Mapping (DPM) module.", "content": "The Depth Position Mapping (DPM) module embeds depth-\nrelated positional information to enhance the model's under-\nstanding of spatial-depth relationships as shown in Fig 4.\nGiven the input features $X \\in R^{B\\times N \\times C}$, where B is the batch\nsize, N = H \u00d7 W is the number of spatial positions, and C is\nthe number of spatial positions, and $F \\in R^{B\\times C \\times H \\times W}$.Based\non previously predicted depth bins $D_b = [d_1,...,d_D] \\in\nIR^{D\\times C}$, where D is the number of depth bins and $d_i$ repre-\nsents the learnable embedding for the i-th depth category, the\ndepth features are locally refined using a depthwise separable\nconvolution with a 3 \u00d7 3 kernel. The operation is defined as:\n$F' = Conv_{3\\times 3}(F) + F.$\nwhere $Conv_{3\\times 3}$ integrates local depth information while\nmaintaining computational efficiency. The resulting depth-\nsensitive feature map F' is then flattened back to $R^{B\\times N \\times C}$ en-\nsuring alignment between the depth positional mappings and\nthe spatial features. By embedding depth-sensitive positional\ncues through D\u266d, the DPM module improves the model's\nability to capture 3D geometric structures, while preserving\nefficiency and scalability for downstream tasks."}, {"title": "D. LOSS FUNCTION", "content": "The proposed framework adopts a single-stage detector de-\nsign, which directly predicts object bounding boxes and class\nprobabilities using pre-defined 2D-3D anchors. This archi-\ntecture is optimized for efficient object detection and depth\nestimation in a single forward pass. To handle challenges\nsuch as class imbalance and bounding box regression, the\nframework incorporates Focal Loss [1] for classification and\nSmooth L1 Loss [2] for bounding box regression. These loss\nfunctions are combined with a custom depth loss to ensure\nrobust performance across all tasks.\nFor 3D object detection, the classification loss $L_{cls}$ and\nregression loss $L_{reg}$ are defined for positive anchors. The\nFocal Loss for classification is used to mitigate the impact\nof class imbalance by down-weighting easy samples:\n$L_{cls} = - \\frac{1}{N_{pos}} \\sum_{i=1}^{N_{pos}} FocalLoss (p_i, p_i^{gt}).$\nwhere Npos is the number of positive anchors. The Smooth\nL1 Loss for bounding box regression minimizes the deviation\nbetween predicted and ground truth box parameters:\n$C_{reg} = \\frac{1}{N_{pos}} \\sum_{i=1}^{N_{pos}} \\sum SmoothL1 (b_i, b_i^{gt}) .$\nDepth estimation is treated as a discrete classification prob-\nlem, where depth ground truth values are projected into bins\nusing Linear Increasing Discretization (LID) [21], [42]. A\nfocal-style depth loss is applied to prioritize confident pre-\ndictions:\n$L_{depth} = - \\frac{1}{N} \\sum_{i=1}^{N} w_i \\times d_i^{gt} \\times log (d_i) .$"}, {"title": "V. VISUALIZATION", "content": "Fig.6 presents qualitative examples from the KITTI validation\nset. Compared to the baseline model without depth-sensitive\nmodules, AuxDepthNet predictions align much more closely\nwith the ground truth, demonstrating the effectiveness of\nthe proposed depth-sensitive modules in enhancing object\nlocalization accuracy."}, {"title": "VI. CONCLUSION", "content": "In this study, we proposed AuxDepthNet, a framework for\nreal-time monocular 3D object detection that eliminates the\nneed for external depth maps or pre-trained depth models.\nBy introducing the Auxiliary Depth Feature Module (ADF)\nand the Depth Position Mapping Module (DPM), AuxDepth-\nNet effectively learned depth-sensitive features and integrated\ndepth positional information, enhancing spatial reasoning\nwith minimal computational cost. Built on the DepthFusion\nTransformer architecture, the framework demonstrated robust\nperformance in object localization and 3D bounding box\nregression.\nDespite its effectiveness, AuxDepthNet has limitations,\nincluding its focus on the KITTI dataset and potential chal-\nlenges in generalizing to diverse environments. Future work\nwill explore broader dataset adaptation, improve robustness\nunder extreme conditions, and optimize computational effi-\nciency for edge applications."}]}