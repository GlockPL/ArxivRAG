{"title": "GCAD: Anomaly Detection in Multivariate Time Series from the Perspective of Granger Causality", "authors": ["Zehao Liu", "Mengzhou Gao", "Pengfei Jiao"], "abstract": "Multivariate time series anomaly detection has numerous real-world applications and is being extensively studied. Modeling pairwise correlations between variables is crucial. Existing methods employ learnable graph structures and graph neural networks to explicitly model the spatial dependencies between variables. However, these methods are primarily based on prediction or reconstruction tasks, which can only learn similarity relationships between sequence embeddings and lack interpretability in how graph structures affect time series evolution. In this paper, we designed a framework that models spatial dependencies using interpretable causal relationships and detects anomalies through changes in causal patterns. Specifically, we propose a method to dynamically discover Granger causality using gradients in nonlinear deep predictors and employ a simple sparsification strategy to obtain a Granger causality graph, detecting anomalies from a causal perspective. Experiments on real-world datasets demonstrate that the proposed model achieves more accurate anomaly detection compared to baseline methods.", "sections": [{"title": "Introduction", "content": "Time series data are prevalent in the real world. Many industrial systems, such as water supply systems, aerospace, and large server systems, have extensive sensor networks that generate vast amounts of multivariate time series (MTS) data. These systems exhibit complex internal dependencies and nonlinear relationships. With the rapid development in related fields, discovering system anomalies from the large volumes of monitoring data generated by sensors has become an important issue. This is crucial for maintaining the safe operation of systems and reducing economic losses (Tuli, Casale, and Jennings 2022; Zhang et al. 2024). Capturing inter-series relationships has been proven to effectively enhance the performance of MTS anomaly detection (Zheng et al. 2023; Zamanzadeh Darban et al. 2024). Recently, Graph Neural Networks (GNNs) have shown great potential in time series anomaly detection by effectively capturing spatial dependencies between variables (Chen et al. 2021). Considering that most time series datasets do not provide readily available graphs, many existing methods adaptively learn graph structures, and then use reconstruction or prediction errors for anomaly detection (Jin et al. 2024). A key problem is that these models only learn the similarity of sequence embedding vectors without exploring the role of the graph structure in the evolution of time series.\nIn real-world scenarios, anomalies in time series are often accompanied by changes in dependency structures. For example, consider a simple pipeline in a water supply system. Under normal conditions, when water pressure increases at the inflow end, it leads to a corresponding increase at the outflow end. However, if there is a leak in the pipeline, an increase in inflow pressure may no longer result in the expected pressure increase at the outflow end. Therefore, learning interpretable dependency structures with causal relationships can effectively detect anomalies in the evolution patterns of time series. However, there are several key challenges in using causal relationships to detect anomalies:\n\u2022 Real systems have complex control logic and numerous nonlinear associations, making it difficult to learn interpretable dependency relationships between variables in a data-driven manner.\n\u2022 The dependency relationships between variables are time-varying, and dynamically capturing dependency patterns in the system and identifying anomalies is a key issue.\nTime series causality is a promising tool for modeling spatial dependencies. Granger introduced Granger causality (Granger 1969). The core idea of Granger causality is straightforward: if using the series xi does not help reduce the prediction error of another series xj, then xi does not Granger-cause xj (Shojaie and Fox 2022). The original definition of Granger causality was intended for linear systems, but recent studies (Khanna and Tan 2019; Liu et al. 2023; Cheng et al. 2024) have extended Granger causality to nonlinear relationships.\nIn this paper, we propose a Granger Causality-based multivariate time series Anomaly Detection method (GCAD). To address the first challenge, we combine deep models with Granger causality. Deep networks have powerful modeling capabilities for nonlinear relationships. We leverage this capability to uncover complex causal relationships from black-box deep networks. To tackle the second challenge, we propose using the gradients in nonlinear deep models to dynamically mine causal dependencies. During the training"}, {"title": "Related Work", "content": "Multivariate Time Series Anomaly Detection\nMultivariate time series anomaly detection is widely applied in the real world. As a classic task in the field of time series analysis, it has received extensive research attention. Early research primarily focused on statistical methods, such as ARIMA (Yu, Jibin, and Jiang 2016), (Keogh, Lin, and Fu 2005) and (Wang et al. 2018). Recently, many deep learning-based methods have been proposed, which effectively capture nonlinear information and are not constrained by the stationarity assumption. These include methods based on Convolutional Neural Networks (CNN) (Munir et al. 2018;\nZhang et al. 2019; Tayeh et al. 2022) and Long Short-Term Memory(LSTM) networks (Hundman et al. 2018), among others.\nWith the development of autoencoders and their variants, many studies have explored using Variational Autoencoders (VAE) (Park, Hoshi, and Kemp 2018; Su et al. 2019) or Generative Adversarial Networks (GAN) (Zhou et al. 2019; Li et al. 2019) for anomaly detection. However, these methods do not explicitly learn the spatial dependencies between sequences, and as pointed out in (Zheng et al. 2023), they fail to fully exploit the dependencies between variable pairs.\nRecently, graph neural networks (GNN) have been increasingly applied to the problem of sequence anomaly detection. By modeling sequences as nodes and the correlations between sequences as edges, using a graph data structure and GNNs naturally represents the spatial relationships between variables. A key challenge in applying GNNs to sequence anomaly detection is that the required knowledge of graph structure usually does not inherently exist in time series anomaly detection data (Li et al. 2021). To address this challenge, many studies have explored adaptive graph learning modules, with GDN (Deng and Hooi 2021) being a pioneering work in this area (Jin et al. 2024). GDN calculates the graph structure based on the cosine distance of sequence embeddings. Most graph-based methods (Han and Woo 2022) use GNNs for prediction or reconstruction, but they do not directly apply spatial dependency patterns to anomaly detection.\nInter-sequence Correlations Modeling\nExplicitly modeling spatial dependencies between sequences helps in discovering more complex anomalies. Most GNN-based methods construct static graph structures using randomly initialized embedding vectors to model spatial dependencies. Methods like VGCRN (Chen et al. 2022) and FuSAGNet (Han and Woo 2022) compute dot products between embeddings to generate a similarity matrix used as a graph structure. However, optimization methods based on downstream tasks may not yield stable and meaningful graph structures. Another class of methods leverages self-attention mechanisms for modeling. The reconstruction module of Grelen (Zhang, Zhang, and Tsung 2022) learns to dynamically construct graph structures that adapt to each time point based on the input time series data. However, randomly initialized attention networks may result in learned spatial relationships that lack practical significance (Zhang, Geng, and Han 2024).\nSome limited studies (Qiu et al. 2012) have explored using Granger causality to model spatial relationships between sequences, but they are based on simple linear statistical models and cannot capture complex nonlinear dependencies. Other methods (Tank et al. 2021) discover Granger causality from sparse penalized network weights. However, this approach is not suitable for anomaly detection tasks. Because data is input in a streaming fashion, and these methods have to continuously optimize parameters during testing to adapt to dynamic causal relationships, resulting in substantial computational costs."}, {"title": "Methodology", "content": "Problem Statement\nIn this paper, we investigate the task of anomaly detection in multivariate time series. Our multivariate time series data are collected from various sensors in a real system, observed at equal intervals over a period of time. The observed time series can be represented as a set of time points: {x1,x2,..., xT}, where xt \u2208 RN denotes the observations from N sensors at time t. In the anomaly detection task, the input to the model is a sliding window {Xt\u2212\u03c4,..., Xt\u22121, Xt}. The model outputs a Boolean value for each sliding window to determine whether there is an anomaly within that window.\nOverview\nOur GCAD framework aims to extract Granger causality relationships among multivariate time series and then identify anomalies from the causal patterns on the test set. It mainly consists of the following four parts:\n1. Prediction-based Gradient Generator: Utilizes predictive methods to guide training and provides channel-separated gradients during the causality discovery phase.\n2. Granger Causality Discovery: Dynamically infers Granger causal relationships from the gradients produced by the gradient generator.\n3. Causality Graph Sparsification: Applies sparsity constraints to the discovered causal relationships to obtain a causality graph matrix.\n4. Causal Deviation Scoring: Calculates the causal pattern deviation score and integrates temporal information to detect anomalies."}, {"title": "Prediction-based Gradient Generator", "content": "The widely studied Mixer predictor (Chen et al. 2023) is used as the gradient generator in our GCAD framework. The predictor consists of L stacked Mixer Predictor Layers, each containing interleaved temporal mixing and feature mixing MLPs. The temporal mixing MLPs are shared across all N features, while the feature mixing MLPs are shared across all time steps. The output of each layer is fed through skip connections into a fully connected layer to produce the predictive output.\nThe input to the predictor is the sliding window Xt\u22121 =\n{xt\u2212\u03c4, xt\u2212\u03c4+1, \u2026, xt\u22121}, where \u03c4 is the maximum time lag considered for Granger causality, and Xt \u2208 RN\u00d7\u03c4. The predictor outputs the prediction for time t: \u0177t = f(Xt\u22121). Where f is the prediction function fitted by the predictor, and yt \u2208 RN. During the training phase, the MSE (Mean Squared Error) loss is used to guide the optimization of predictor parameters: Ltrain = MSE(\u0177t, yt), where yt is the ground truth.\nDuring the testing phase, in order to explore the causal relationships between variables, the gradient generator needs to compute pairwise predictor gradients between variables. Therefore, we propose a channel-separated Error detector to generate channel loss Lt \u2208 RN:\nLt,j = (yt,j \u2013 \u0177t,j)\u00b2, (1)\nwhere Lt,j represents the prediction error of sequence j in the sliding window Xt. Next, the gradient generator performs backpropagation on each prediction error through the prediction network, obtaining the gradient Gt,j \u2208 RN\u00d7\u03c4"}, {"title": "Granger Causality Discovery", "content": "Nonlinear Granger causality is defined from the perspective of the impact of variables on each other's predictive effects. Our framework reconsiders Granger causality from the perspective of gradients in deep networks. In this paper, we adopt the widely used definition (Cheng et al. 2022) of nonlinear Granger causality:\nDefinition 1 Time series i Granger-causes j if and only if there exists X't-\u03c4:t\u22121,i \u2260 Xt-\u03c4:t\u22121,i,\nfj(xt-\u03c4:t\u22121,1, ..., xt-\u03c4:t\u22121,i, ..., xt-\u03c4:t\u22121,N) \u2260 fj(xt-\u03c4:t\u22121,1, ..., x't-\u03c4:t\u22121,i, ..., xt-\u03c4:t\u22121,N). (2)\ni.e., the past data points of time series i influence the prediction of xt,j.\nConsidering this definition from a differential perspective, let t'\u2208 (t \u2212 \u03c4 : t \u2212 1), and let x*t',i be a perturbation of xt',i \u2208 Xt:\nx*t',i = xt',i + \u0394, (3)\nwhere \u0394 is the perturbation. Based on the predictor described in the previous subsection, the following equations can be obtained:\n\u0177t,j = fj(xt-\u03c4:t\u22121,1, ..., xt-\u03c4:t\u22121,i, ..., xt-\u03c4:t\u22121,N), and\n\u0177*t,j = fj(xt-\u03c4:t\u22121,1, ..., {xt\u2212\u03c4,i, ..., x*t',i, ..., xt\u22121,i}, ..., xt-\u03c4:t\u22121,N). Where fj is the function of \u0177t,j with respect to the input in the prediction network. The change in prediction error caused by the perturbation can be transformed into the form of partial derivatives:\nlim (\\frac{L^*t,j - Lt,j}{\u0394 x_{t',i}}) = \\frac{\u2202 L_{t,j}}{\u2202 x_{t',i}} (4)\n\u0394\u21920\nwhere Lt,j = (yt,j - \u0177t,j)\u00b2. Granger causality considers the mutual influence of sequences on each other's predicted values within the maximum time lag. Therefore, we define the quantification of Granger causality as the integral of the absolute values of channel-separated gradients over the time lag:\nai,j = \\int_{t-\u03c4}^{t-1} |\\frac{\u2202 L_{t,j}}{\u2202 x_{t',i}}|d\u03b4 (5)\nwhere \u03b4 is the time index from t - \u03c4 to t - 1.The term ai,j represents the degree to which sequence i Granger causes sequence j, parameterized by a distribution of interest P. The causality matrix is defined as A = {ai,j}i,j=1.\nSince predictors composed of deep networks are capable of backpropagation, the prediction function f is inherently continuous and differentiable. For simplicity, the interest distribution P can be a uniform distribution. According to Equation 4, when ai,j \u2260 0, it can be concluded that the two predicted values are not equal, i.e., \u0177*t,j \u2260 \u0177t,j. Since the inputs corresponding to these two predicted values are x*t',i and xt',i, respectively, the differing inputs of sequence i result in differing outputs of sequence j. This is consistent with Definition 1, from which it can be inferred that sequence i Granger-causes sequence j."}, {"title": "Causality Graph Sparsification", "content": "Unlike similarity or correlation between sequences, causality must be unidirectional. The ideal Granger causality graph is a directed acyclic graph (DAG). However, in nonlinear Granger causality discovery, it is difficult to strictly guarantee the acyclic nature of the causal graph. Existing methods discover nonlinear causality through network weights constrained by sparsity. To adapt to the anomaly detection task, instead of directly constraining the weights, we employ sparsification to address the dynamic Granger causality identified from the gradients of the deep network.\nThe causal relationships discovered from the gradients of deep networks may include undirected edges, which to some extent represent the similarity between sequences. Our intuition is that this similarity should be equal in both directions. In fact, the widely used relationship matrices obtained from cosine similarity are symmetric matrices, thus assuming symmetry in both directions has convincing empirical evidence. Based on this intuition, we propose a simple and easy-to-use method for sparsifying causality graphs:\nAi,j = max(0, Ai,j \u2013 Aj,i), i \u2260 j,\nAi,i = Ai,i. (6)\nIn which A is the sparsified Granger causality graph matrix. The causality matrix subtracted from its transpose eliminates bidirectional symmetric similarities while preserving unidirectional Granger causality. Further, we set a sparsity threshold h, setting causality effect values below this threshold in the causality graph matrix to zero. This is because insignificant causal relationships may be caused by noise and are not helpful for anomaly detection."}, {"title": "Causal Deviation Scoring", "content": "By sliding a window over the test set and using the proposed framework to construct Granger causality graphs, we obtain a sequence of causality graphs. We aim to detect anomalies that deviate from the normal causal patterns within these graphs. A straightforward idea is to construct a causality graph that represents the normal causal pattern and compare each causality graph from the test set against it. To leverage the causal pattern information from the normal data, after model training, we sample the training set windows using a Bernoulli distribution and calculate the Granger causality graphs for these samples. Then, we use the mean matrix of the graph matrix sequence to represent the typical normal causal pattern:\nWtrain = Wtrain B,\nB = {b1...., bntrain},\nbi ~ Bernoulli(p),\nAnorm,i = {g(Wtrain,i)}i=1, (7)\nwhere Ntrain is the total number of sliding windows in the training set, p is the parameter of the Bernoulli distribution, n is the number of sampled windows, and g is the function used to compute the causality graph. The resulting typical"}, {"title": "Experimental Results", "content": "To evaluate our proposed GCAD framework, we conducted experiments on five widely-used real-world benchmark datasets and compared it with six popular baseline methods.\nExperimental Setup\nDatasets. Experiments are conducted on five real-world datasets, including SWaT (Mathur and Tippenhauer 2016), SMD (Su et al. 2019), MSL, SMAP (Hundman et al. 2018), and PSM (Abdulaal, Liu, and Lancewicki 2021). The statistical information of these datasets is presented in Table 1. Baseline Methods. The baseline methods include DAGMM (Zong et al. 2018), USAD (Audibert et al. 2020), GDN (Deng and Hooi 2021), AnomalyTransformer(AT) (Xu et al. 2021), GANF (Dai and Chen 2022) and MEMTO (Song et al. 2024).\nImplementation Details. Each dataset consists of two parts: unlabeled normal operation data and labeled data containing some anomalies. We use 80% of the normal data for training, and the remaining 20% is used for the validation set. Testing is conducted on the data containing anomalies. Since most baseline methods do not provide a way to set predetermined thresholds, we evaluate using two threshold-independent metrics: the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) and the Precision-Recall Curve (PRC). All experiments were conducted 10 times and the average results were reported.\nAnomaly Detection Performance\nThe results of our framework and the six baseline methods are summarized in Table 2.\nFrom the results, it can be seen that GCAD achieves state-of-the-art (SOTA) performance in most cases. DAGMM and USAD are classical anomaly detection frameworks that do not explicitly model the spatial relationships between sequences, which limits their anomaly detection performance. GDN achieves the second-best performance across four experimental metrics; however, it uses an adaptive graph structure learning strategy to learn a fixed graph structure, which is not suitable for dynamically changing systems. GANF achieves the best result on the PRC metric for the SMAP dataset. This is because the SMAP dataset has severe distribution shifts, and GANF, being a density-based method, can better handle this situation by learning the evolution of graph structures and identifying distribution shifts. It is worth noting that the MSL and SMAP datasets are relatively small and have severe class imbalance, which may lead to very low PRC values for most methods. MEMTO, by incrementally training individual items in a gated memory module, mitigates the overgeneralization problem of reconstruction-based models. Therefore, it achieves the second-best result on the MSL dataset, where the proportion of anomalous samples is extremely small.\nAblation Study\nWe investigated the effect of each component in the proposed framework. Table 3 shows the results of the ablation study on two datasets. \"-Spars\" indicates the removal of the Causality Graph Sparsification part from GCAD. \"-GC\" means not using Granger causality for anomaly detection. \"-TC\" means disregarding the temporal correlations within each sequence by excluding the time pattern deviation from the anomaly score."}, {"title": "Effect of Parameters", "content": "We further investigated the impact of key hyperparameters on the anomaly detection performance of GCAD. All experiments were conducted using the SWaT dataset, and the results are shown in Figure 2.\nThe maximum time lag \u03c4 determines how many lagged effects of Granger causality the model can discover. When \u03c4 is set to 1, the model only discovers Granger causality in adjacent time steps, and the shorter sliding window makes the model more sensitive to anomalies within the window. As \u03c4 increases, the model can discover Granger causality with higher lags, which helps extract more complex high-order spatial patterns. However, this also reduces the model's sensitivity to short-term anomalies, because GCAD focuses on the causal patterns in the input distribution over the entire window (Equation 5). Therefore, the maximum time lag parameter controls the balance between the model's sensitivity to anomalies and its ability to discover complex anomalies. A moderate \u03c4 value can achieve better performance.\nThe sparsification parameter h determines the threshold below which causal relationships are considered as noise. When h is small, there are many non-zero but insignificant causal relationships in the causality matrix. When h is too large, GCAD focuses only on the most significant causal relationships, ignoring the broader causal relationships that exist in the system, thus failing to fully utilize the anomaly information contained in the causal patterns. Experiments show that a moderate h can yield better results."}, {"title": "Analysis of Anomaly Detection Examples", "content": "To demonstrate how causal patterns reveal anomalies, we conducted a case study on anomaly event data from a real system. The experiments were performed on the SWaT dataset, as the original authors (Goh et al. 2017) provided the physical construction and sensor descriptions of this real system.\nTo facilitate observation, we visualized the causal pattern deviations provided by GCAD during the occurrence of anomaly events, as shown in Figure 3. In this figure, the deviation matrix D represents the absolute error between the causality matrix during the anomaly event and the typical causality pattern matrix, that is, D = |Atest,i \u2212 Anorm|."}, {"title": "Conclusion", "content": "In this work, we propose a Granger Causality-based multivariate time series Anomaly Detection method (GCAD). This framework models the spatial dependencies between sequences using Granger causality, constructs dynamic causal relationships based on the gradients of a deep predictor, and improves the causal graph structure using our sparsification strategy. Experiments on five real-world sensor datasets demonstrate the superiority of GCAD in anomaly detection accuracy compared to other baseline methods.\nThis study explores the integration of Granger causality with deep networks, offering a new perspective for time series anomaly detection. However, GCAD also has some limitations, as it can only capture binary causal pairs between variables and cannot model multivariable interactions. Future work may consider multivariable causal relationships to uncover more complex anomalies."}]}