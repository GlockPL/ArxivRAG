{"title": "Counting Ability of Large Language Models and Impact of Tokenization", "authors": ["Xiang Zhang", "Juntai Cao", "Chenyu You"], "abstract": "Transformers, the backbone of modern large language models (LLMs), face inherent architectural limitations that impede their reasoning capabilities. Unlike recurrent networks, Transformers lack recurrent connections, confining them to constant-depth computation. This restriction places them in the complexity class TC, making them theoretically incapable of solving tasks that demand increasingly deep reasoning as input length grows. Counting, a fundamental component of many reasoning tasks, also requires reasoning depth to grow linearly to be performed inductively. While previous studies have established the upper limits of counting ability in Transformer-based expert models (i.e., models specifically trained for counting tasks), these findings do not directly extend to general-purpose LLMs due to differences in reasoning mechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning can help alleviate some of the architectural limitations of Transformers in counting tasks. However, little attention has been paid to the role of tokenization in these models. Unlike expert models that often use character-level tokenization, LLMs typically rely on byte-level (BPE) tokenizers, which fundamentally alters the way reasoning is processed. Our work investigates the impact of tokenization on the counting abilities of LLMs, uncovering substantial performance variations based on input tokenization differences. We provide both theoretical and experimental analyses, offering insights into how tokenization choices can undermine models' theoretical computability, thereby inspiring the design of new tokenization methods to enhance reasoning in LLMs. All code, prompts, and experiment logs, API returns are released on GitHub.", "sections": [{"title": "1 Introduction", "content": "Counting, a fundamental component of most complex reasoning tasks, has been extensively stud-"}, {"title": "2 Neural Networks and Counting: A Revisit", "content": "Training neural networks for counting. Counting has been extensively studied in neural networks (NNs) as it is a fundamental skill required for more advanced tasks (Chang and Bisk, 2024). Since multi-layer perceptrons (MLPs) (Rosenblatt, 1958) can only handle fixed-length inputs, which contradicts the nature of counting, early NN training for counting tasks began with recurrent neural networks (RNNs). Rodriguez et al. (1999) trained an early RNN to recognize the regular language $a^n b^n$, which required the network to count the number of as and bs in the input string. Of the 50 networks trained, 8 successfully learned the task and generalized to longer strings, demonstrating the counting ability of RNNs. Suzgun et al. (2019) trained LSTM for counting in the context of bracket pairing, showing that LSTMs could perform dynamic counting by maintaining separate counters (represented by different types of brackets) using their memory (cell state) and gating mechanisms\u2014something RNNs were unable to achieve. More recently, Del\u00e9tang et al. (2022) systematically explored counting in mainstream NNs, including RNNs, LSTMs, and Transformers. Their experiments showed that Transformers could not"}, {"title": "3 CoT + Ideal Assumption = Complete Counting Ability", "content": "Mainstream LLMs use the Transformer architecture as their backbone (Bai et al., 2023; Touvron et al., 2023; Achiam et al., 2023), and, as a result, inherit the computability limitations of Transformers, particularly in tasks like counting. Given these constraints, it's unsurprising that Transformers struggle with counting tasks. However, Chain of Thought (CoT) reasoning has revolutionized how Transformers reason, offering new possibilities counting task. In this section, we demonstrate what counting entails and why Transformers alone cannot effectively solve counting tasks. We then show how CoT can achieve perfect counting accuracy under ideal assumptions."}, {"title": "3.1 Inductive Counting and Computation", "content": "Counting is inherently inductive in both human cognition and computability theory (Borodin et al., 1989). To count from 1 to n, one must start at 1, then proceed step by step-counting to 2 before reaching 3, and so on. Humans typically count inductively, except for very small numbers (1-3), where we rely on memorization and can immediately recognize the total at a glance. For larger quantities, like a pile of apples, we engage in iterative inductive counting from 1 to n (Figure 2). In computability, a state machine processes an input string token by token, transitioning its internal state after each token to keep track of the count. This step-by-step state transition aligns with the inductive nature of counting. Similarly, neural networks perform counting internally through their hidden state, h, which serves as the location for reasoning and intermediate information storage (e.g., counter storage) (Figure 2. In RNNs, as h is updated with each new input token $x_t$ via recurrent connections, the counter can be inductively updated at every step (Figure 2). The inductive bias of RNNs allows h to act as the smallest reasoning unit, incrementing the stored counter with each update.\nHowever, Transformers can only sequentially process h a fixed number of times (Li et al., 2024; Sanford et al., 2024; Zhang et al., 2024a), limited by their number of layers. For instance, when counting the number of as in the input string aababaa, the Transformer initializes its counter in the latent representation h. While each layer captures substantial computation through matrix operations (WX), because h is computed in parallel across positions t, it lacks the depth (sequential counting) needed for inductive counting. The counter is only updated when the hidden state h is sequentially passed from one layer to the next, limiting the model to a fixed number of counting steps. Some theoretical work (Chiang and Cholak, 2022) suggests that Transformers can perform counting with hard-coded weights, but this approach is not inductive. Instead, it treats each bit in h and W as the smallest unit of reasoning (in contrast to treating the entire h as a unit in RNNs) and performs circuit-level computations. This relies on the bit-level dependencies within matrix multiplication (e.g., $W_1X_1 + W_2X_2 +\u2026+ W_nX_n$),"}, {"title": "3.2 CoT: Sky is the Limit", "content": "As Transformer-based models, both LLMs and expert models, struggle to count effectively using only their internal reasoning state (Chang and Bisk, 2024; Del\u00e9tang et al., 2022) (also demonstrated in experiments below), Chain of Thought (CoT) (Wei et al., 2022) shifts the reasoning required for inductive counting into the text space (Li et al., 2024; Zhang et al., 2024a). Instead of simply outputting a final counting result y after processing an input sequence $x = (x_1,x_2,x_3,\u2026\u2026,x_n)$, LLMs are guided to output intermediate reasoning steps, in this case counter value, after processing each input unit $x_i$. Since internal reasoning via h can only handle a limited number of sequential counting steps, CoT allows LLMs to convert the latent counter information from h into a sequence of tokens $(o_1, o_2,\u2026\u2026, o_k)$, referred to as Thought, which represents the counter value in text (Figure 2). During subsequent computations, these thought tokens are encoded back into the latent space through the embedding layer, whose information forming the new h for the next step of reasoning (Figure 2). In essence, CoT approximates the recurrent computation in RNNs\u2014$h_{t-1} \u21d2 h_t$-by using $h_{t-1} \u21d2 (o_1,o_2,\u2026,o_k) \u21d2 h_t$, where $(o_1, o_2,\u2026, o_k)$ encodes information from $h_{t-1}$. This enables infinite reasoning depth when the CoT can be extended indefinitely (ideal assumption)."}, {"title": "4 Tokenization as a Black Box Model", "content": "Even with CoT enabled, significant failures in counting are still observed in modern LLMs like GPT-4, which consistently make errors when counting letters in words as short as 3 to 10 characters such as Strawberry (Table 1). Given the number of layers in models of this size (Touvron et al., 2023), counting within 10 digits internally should be feasible. A key factor contributing to these errors is the tokenizer used, specifically byte-level Byte Pair Encoding (BPE) (Sennrich, 2015). BPE groups a certain number of characters into tokens, both within and between words, leading to a mismatch between the unit to be counted (e.g., letters) and the unit actually being processed (BPE tokens). In this section, we explore the potential impact of tokenization on the counting abilities of LLMs and introduce our novel method for analyzing this effect, treating the model as a black box."}, {"title": "4.1 Imperfect Tokenizer + CoT < CoT Limit", "content": "We introduce the concept of Token Awareness in the context of LLMs. For any given token t (e.g., the token pre) produced after tokenization, each token has properties such as how many \"r\"s are in this token. Since LLMs are trained to predict entire tokens rather than individual letters, specific token properties may not be fully aware by the model unless they can be inferred from the training corpus. During training, a token is mapped directly into its token embedding without additional information, meaning the model may not even know if this token contain any letters.\nNow, consider a CoT-augmented LLM under ideal conditions, where it has perfect counting ability. If the counting is performed at a level that does"}, {"title": "4.2 Method", "content": "Building on the theoretical analysis, our goal is to examine the practical counting abilities of LLMs, with a particular focus on the impact of tokenization-a crucial yet often under-explored factor in a model's counting potential. We propose a novel approach to studying the role of tokenization in LLMs' counting performance. Since many LLMs, along with their tokenization algorithms, are closed-source, we designed our approach to be model-agnostic, treating LLMs as black-box systems.\nWe begin by establishing universal assumptions based on typical LLM design in real-world models: (1) Consecutive letters of length 2-4 often form a single token; (2) Inputs beside delimiters like commas or spaces, are split into separate tokens. (3) After tokenization, delimiters (e.g., spaces, commas) are often merged with the preceding or following token. However, adding consecutive delimiters can prevent this merging and separate delimiter tokens from adjacent text tokens.\nWe validated our assumptions using multiple modern open-source LLMs or models with open tokenizers (Figure 3 as well as Appendix Figure 7). Based on these assumptions, we designed input instances for counting tasks to manipulate how tokenization is applied. To highlight the impact of tokenization on counting performance, we focused on letter-level counting, where the model needs token-level letter awareness to correctly update the counter in h. It is important to note that the granularity of counting (letter vs. word level) does not change the fundamental nature of the counting task and should not affect the theoretical limits of counting ability.\nTo test how naive BPE tokenization affects counting, we designed counting instances as strings of consecutive letters, which are naturally tokenized by merging every 2-4 letters into a single token, based on assumption (1). Figure 3.(a) illustrates the tokenization of such an example using the GPT-40 tokenizer. Formally, for a string s consisting of n letters, $(l_1,l_2,\u2026\u2026\u2026,l_n)$, the task is to count the total occurrences of a target letter $l_{target}$:\n$\\sum_{i=1}^{n} a \\in \\N, a = \\mathbb{1} \\{l_i=l_{target}\\}$ (1)\nwhere a is the answer (count), and $\\mathbb{1}$ is the indicator function, which returns 1 if the current letter matches the target letter.\nAs this approach inevitably merges consecutive letters, we introduce two alternative methods of inserting between-letter delimiters to simulate cases where item-separated tokenization is applied to the same counting instance. We use two types of delimiters, $d_1 = \\text{\"} \\text{ and } d_2 = \text{ , }$, as shown in Figure 3.(b)-(c). According to assumptions (2) and (3), the resulting string $(l_1, d, l_2, d,..., d, l_n)$ manipulates the tokenizer to separate each item to be counted into its own token. However, the delimiter d and the letter $l_i$ will often be merged into a new token t. Accurately counting the instance thus requires token awareness of the merged token with the delimiter.\nLastly, by adding the delimiter \\text{\"} (quotations) along with \\text{ , }, each letter will be separated into its own individual token,"}, {"title": "5 Experiments", "content": "We adopt two mainstream foundation models for our analysis of tokenization and counting ability: GPT-40 mini API and Claude-3.5-sonnet API, considering both affordability and their represen-"}, {"title": "5.1 Setting", "content": "We adopt two mainstream foundation models for our analysis of tokenization and counting ability: GPT-40 mini API and Claude-3.5-sonnet API, considering both affordability and their represen-"}, {"title": "5.2 Main Results", "content": "We analyze the experimental results alongside our theoretical analysis of the model's upper computability, the role of CoT, and the impact of tokenization, as discussed in the previous section."}, {"title": "5.2.1 CoT Grants Counting Ability", "content": "By comparing the results for no-CoT and CoT across different length ranges and string types, we observe that CoT significantly enhances the model's counting ability in all cases, with an average performance improvement of 20% compared to no-CoT.\nAs discussed in section 2, LMs without CoT, which rely solely on Transformer's internal latent reasoning, can only count a constant number of times inductively. This is evident when performance drops sharply as the input length increases from [10, 20] to [30, 40] when CoT is not used. Without CoT, the counting accuracy for letters a and b falls from around 50% to just 8%, regardless of tokenization, which is barely above random guessing (3-4%). However, with CoT augmentation, counting accuracy remains more stable and declines much more gradually (from 96% drops all the way to 56%), especially with the best tokenization choice (choice (d)). This highlights the crucial role of CoT in unlocking a model's counting potential and the practical gap from theoretical limits, caused by factors such as training quality, long-context retrieval, and CoT length constraints."}, {"title": "5.2.2 Tokenization Greatly Affects Counting Ability", "content": "From Table 1, we observe a clear and consistent trend of improved performance as tokenization choices progress from (a) to (d), indicated by increasing color darkness, when the length range is fixed. Notably, without any special tokenization (type (a)), counting performance is even worse than not using CoT in the same setting. For string lengths greater than 20, BPE-tokenized strings perform much closer to random guessing than to CoT-less models. This highlights how letter-grouped BPE tokenization can severely degrade theoretical counting ability due to the lack of token awareness. When item-separated tokenization is applied (using delimiters such as \\text{\"} or \\text{ , }\\text{\"} in (b) and (c)), we see consistent improvements (13%-40%) over pure BPE, emphasizing the importance of per-item tokenization rather than grouped tokenization in counting tasks. However, since (b) and (c) still result in items being grouped with delimiters (e.g., spaces), even though the items are separated from one another, the performance limit is only reached when each item (letter) is clearly tokenized, separated from both the delimiters and other items. This eliminates the need for token-awareness, as each item-token (letter) is distinct and ready for comparison with the target, allowing the attention mechanism to function optimally (with higher cosine similarity between identical token embeddings in attention). We conducted the same experiments with a different pair of letters, e and z, and observed identical patterns. This further demonstrates the significant impact of tokenization on counting ability, with clearly separated targets leading to markedly better results."}, {"title": "5.3 Error Shifts Reveal Mistakes in Counting with BPE Tokenization", "content": "We analyze the error shifts, defined as the difference between the model-calculated count and the true count, when mistakes occur. As shown in Figure 4, all tokenization methods result in a bias toward negative shifts, meaning the model underestimates the count. Specifically, when using pure BPE tokenization, we observe only negative shifts. This is because BPE-tokenized tokens often contain letters the model is unaware of. For instance, the model might fail to count any \"a\"s in a single token like \"abaa,\" leading to an undercount. When"}, {"title": "5.4 Different Tokens Have Varying Sensitivity in Counting Tasks", "content": "As seen in Table 1, when counting is performed properly (with CoT enabled and tokenization not relying on pure BPE), we consistently observe higher accuracy when counting the letter b compared to the letter a, across all length and tokenization settings (Visualized in Figure 5 left). We suspect this discrepancy is due to differences in letter frequency in the natural language, which may affect token-embedding sensitivity in the model. To further investigate how token frequency impacts counting accuracy in LLMs, we conducted additional counting tasks using the most frequent letter in human language,e, and the least frequent letter, z, under the same settings. As shown in Table 2, we observe similar performance differences, with z achieving much higher counting accuracy than e across all proper counting settings (both with CoT and using letter-separated tokenization). We visualize the counting performance between each pair of settings"}, {"title": "5.5 Results on Claude and Case Study with GPT-40", "content": "We repeated the experiments with Claude and observed similar trends, with the exception that type (c) yielded the best results, outperforming types (a)-(d), as shown in Table 3. Upon investigation, we suspect this is because type (d) results in significantly longer CoT steps due to the higher number of tokens, leading to long-context reasoning failures in many cases for this model. We also provide case studies using GPT-40 mini for counting tasks, including examples where CoT led to both correct and incorrect answers, as well as cases where the model predicted incorrect answers when CoT was disabled. All case studies are detailed in the Appendix Section D.\nTo this end, we are confident that our experimental results can be generalized to other LLMs, given that the training methods and tokenization strategies (as demonstrated in Appendix Section C) are nearly identical, leading to counting being performed in a similar manner across such models."}, {"title": "6 Conclusion", "content": "This study has demonstrated that tokenization plays a significant role in the reasoning capabilities of large language models (LLMs), particularly in tasks such as counting. The way input data is tokenized influences how models process and interpret information, ultimately affecting performance outcomes. Therefore, careful consideration must be given to the selection of tokenization strategies when working with LLMs, especially in tasks where fine-grained reasoning is essential. Our findings suggest that the exploration and optimization of tokenization methods is not only relevant but crucial for improving model performance. Future research should further investigate the impact of different tokenization techniques, with a focus on refining these methods to enhance LLM reasoning abilities across a broader range of tasks."}, {"title": "Limitations", "content": "Our experiments were conducted on GPT-40 and Claude-3.5, and while both models demonstrated strong patterns and consistent evidence showing that certain types of tokenization significantly improve counting performance, we did not extend our testing to other LLMs such as LLaMA or Mistral. This was primarily due to budget and time constraints, as well as preliminary findings that these models exhibited weaker instruction-following abilities compared to GPT and Claude, making the evaluation process more challenging. However, we believe our research remains robust despite these limitations, as mainstream model training and design principles are largely universal, and the patterns observed are likely generalizable to other LLMs.\nAdditionally, our experiments did not explore extreme context lengths, such as counting instances with more than several hundred tokens. We found that such cases often led to instability due to the accumulation of long CoT steps. We aim to further investigate this aspect as LLMs improve in handling long-context retrieval and generation."}, {"title": "Appendix", "content": "In this section, we demonstrate various details, including: A. Supervised Chain of Thought: A clear definition of Supervised CoT and its benefits; B. Case studies, showing examples of correct and incorrect outputs; C. Details on tokenization for different LLMs."}, {"title": "A Supervised Chain of Thought", "content": "Naive Chain of Thought (CoT), which uses a generic \"think step by step\" prompt for all tasks, poses significant challenges for models in determining the correct steps, especially for complex, multi-step reasoning tasks. To mitigate this confounding factor, we follow previous work and employ Supervised CoT (Zhang and Ding, 2024), as the derivation of steps is not the focus of our research and should not affect performance due to incorrect CoT steps. Below, we define Supervised CoT and explain its application in counting tasks."}, {"title": "A.1 Definition", "content": "The search space for solving a task can be viewed as a combination of the prompt space and the answer space. When instructed to perform tasks step by step, language models must devise a step template which is used to determine the actions at each step. This template is crucial for solving tasks, as it specifies what information is processed and how it is computed at each CoT step. However, for a given task, there are numerous ways to perform a \"step-by-step\" approach, each computing different elements per step. Finding the optimal set of steps is challenging yet essential, as it directly influences the ability to find solutions in the answer space (Zhang and Ding, 2024).\nSupervised CoT provides human supervision in determining the step template. Rather than asking the model to develop its own plan for each step, humans identify the \"recurrent\" procedure in the computation and explicitly instruct the model to follow a specific step template. This approach allows the CoT to bypass the need to search for optimal steps, focusing instead on finding solutions within the answer space under optimal step guidance."}, {"title": "A.2 Supervised CoT and Counting", "content": "In inductive counting, which relies on CoT to compute the counter value recurrently(Figure 2), it is crucial that each step of CoT accurately extracts and outputs the counter value in text. This output is necessary for the value to be recurrently processed through \"string-vector\" conversion. Therefore, rather than simply prompting the model with \"determine the number of a in the given string\" using the generic instruction \"think step by step,\" we specifically instruct the model to print out a counter value at each step. We explicitly define the step template to ensure the model follows the optimal CoT steps, preventing deviations or the use of suboptimal steps.\nExperiments. We demonstrate the significant performance gap between Supervised and Unsupervised CoT. Specifically, we observe that supervision not only helps the model accurately extract the counter but also ensures it follows the correct steps (e.g., an incorrect step would be outputting whether the current letter is the target, rather than extracting the counter value). Even when Unsupervised CoT identifies the correct steps (i.e., extracting the counter into text), we still notice more frequent errors during the extraction process compared to"}]}