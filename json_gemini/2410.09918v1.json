{"title": "Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces", "authors": ["DiJia Su", "Sainbayar Sukhbaatar", "Michael Rabbat", "Yuandong Tian", "Qinqing Zheng"], "abstract": "In human cognition theory, human thinking is governed by two systems: the fast and intuitive System 1 and the slower but more deliberative System 2. Recent studies have shown that incorporating System 2 process into Transformers including large language models (LLMs), significantly enhances their reasoning capabilities. Nevertheless, models that purely resemble System 2 thinking require substantially higher computational costs and are much slower to respond. To address this challenge, we present Dualformer, a single Transformer model that seamlessly integrates both the fast and slow reasoning modes. Dualformer is obtained by training on data with randomized reasoning traces, where different parts of the traces are dropped during training. The dropping strategies are specifically tailored according to the trace structure, analogous to analyzing our thinking process and creating shortcuts with patterns. At inference time, our model can be configured to output only the solutions (fast mode) or both the reasoning chain and the final solution (slow mode), or automatically decide which mode to engage (auto mode). In all cases, Dualformer outperforms the corresponding baseline models in both performance and computational efficiency: (1) in slow mode, Dualformer optimally solves unseen 30 \u00d7 30 maze navigation tasks 97.6% of the time, surpassing the Searchformer (trained on data with complete reasoning traces) baseline performance of 93.3%, while only using 45.5% fewer reasoning steps; (2) in fast mode, Dualformer completes those tasks with an 80% optimal rate, significantly outperforming the Solution-Only model (trained on solution-only data), which has an optimal rate of only 30%; (3) when operating in auto mode, Dualformer achieves an optimal rate of 96.6% while utilizing 59.9% fewer reasoning steps compared to Searchformer. For math problems, our techniques have also achieved improved performance with LLM fine-tuning, showing its generalization beyond task-specific models.", "sections": [{"title": "Introduction", "content": "In psychology, the dual process theory (Wason and Evans, 1974) explains that thought can emerge through two distinct processes. Typically, one process is implicit (automatic and unconscious) and the other process is explicit (controlled and conscious). The famous book Thinking, Fast and Slow (Kahneman, 2017) explores the concept of dual process theory extensively. It describes two different systems of thinking. System 1, corresponding to the implicit process, is fast, intuitive, and emotional. System 2, corresponding to the explicit process, is slower, more deliberative, and more logical. Kahneman applies this theory to a wide range of cognitive and behavioral activities to illustrate how these two systems influence our decision making and reasoning. As outlined in the book, System 1 can lead to biases and is prone to systematic errors due to the lack of voluntary control. As a counterbalance, System 2 is more suitable for tasks that require careful analysis, consideration, and planning.\nTransformers (Vaswani et al., 2017), the sequence modeling tool that serves as the cornerstone of foundation models in various domains including Large Language models (LLMs) (Dosovitskiy, 2020; Baevski et al., 2020; Radford et al., 2021; Touvron et al., 2021; Hsu et al., 2021; Touvron et al., 2023; Dubey et al., 2024), have been widely used in many works to approach reasoning and planning problems, see e.g., Zhou et al. (2022);\nKojima et al. (2022); Pallagani et al. (2022); Valmeekam et al. (2023a); Chen et al. (2024); Gundawar et al. (2024); Wang and Zhou (2024). Interestingly, the dual process theory generalizes to artificial intelligence, where we can categorize the reasoning modes of Transformer into fast and slow. In fast mode, a Transformer will directly output a final solution without any reasoning steps, whereas the intermediate steps of thinking,"}, {"title": "Related Work", "content": "Cognitive Science & Psychology Wason and Evans (1974) first introduces the dual-process theory, which is then extensively explored and explained in Kahneman (2017). This framework has underpinned extensive research in decision-making across various fields, including psychology and behavioral economics (Thaler and Sunstein, 2008; Povey et al., 2000; Tversky and Kahneman, 1974; Kahneman, 2017). In cognitive science, the distinction between System 1 and System 2 has facilitated a deeper understanding of how humans process information and make decisions. Research has shown that System 1 operates automatically and quickly, with little or no effort and no sense of voluntary control, often relying on heuristics that lead to biases (Tversky and Kahneman, 1974). Conversely, System 2 allocates attention to effortful mental activities and is associated with the subjective experience of agency, choice, and concentration.\nLearning to Plan and Reason Tremendous efforts have been made to enhance the capability of Transformer-based models to plan and reason over a long horizon. Two main types of approaches have been developed. The"}, {"title": "Preliminaries", "content": "Our work builds upon the work of Lehnert\net al. (2024). To perform planning, we train\na Transformer to model a sequence of tokens\nthat sequentially represents the planning task,\nthe computation of A* algorithm, and the op-\ntimal solution derived from the A* search.\nThe tokenization method is illustrated in Fig-\nure 3.1. As a toy example, we consider a\nnavigation task in a 3\u00d73 Maze where the\ngoal is to find one shortest path from the start\ncell to goal cell, without hitting a wall cell. The A* algorithm has successfully determined an optimal plan.\nWe use a sequence of tokens to express both the task and the Maze structure, which also serves as the prompt\nfor Dualformer. The solution is depicted by the plan token sequence that describes the path using coordinates.\nThe A* algorithm generates a search trace sequence that records the search dynamics performed, displayed in\nFigure 4.1. Recall that the A* algorithm is a pathfinding algorithm on a weighted graph. The create clause\nadds the node (represented by the subsequent coordinates) into the search frontier, and the close clause adds the\nnode to the closed set. Each clause, either create or close, is followed by the tokens x, y, co, and c1, represent-\ning the node\u2019s coordinates, cost-since-start value, and heuristic values, respectively. For details of A* and the\ntokenization approach, we refer the readers to Russell and Norvig (2021) and Lehnert et al. (2024), respectively."}, {"title": "Structured Trace Dropping and Randomized Training", "content": "Searchformer proposed by (Lehnert et al., 2024) has proven efficacy in addressing a variety of complex decision-making tasks. However, it still suffers from two important limitations. Firstly, the model only operates in slow mode and outputs lengthy reasoning chains, which significantly increases the inference time. While this can be reduced by bootstrapping (Lehnert et al., 2024), an iterative refining technique that consists of cycles of rollouts followed by fine-tuning, such a procedure incurs significant extra demand on computational resources. Secondly, Searchformer struggles to generate diverse solutions, where identical rollouts are frequently sampled\u00b9.\nFor example, across 1000 30 \u00d7 30 maze problems we have tested, Searchformer's reasoning chain contains more than 1500 tokens on average, and can only find 7.6 unique feasible paths out of 64 responses (see\nSection 5.1.2).\nTo address these challenges, we propose a training framework that utilizes randomized reasoning traces. Our approach is inspired by two lines of work. First, we have noticed that even though Searchformer is trained on complete A* search traces, it generates shorter traces that are sketching the search process. Second, research has shown that humans often rely on shortcuts and patterns when making decisions, a concept known as System 1 thinking (Kahneman, 2017). These observations, combined with the success of dropout technique (Hinton, 2012; Srivastava et al., 2014) that randomly drop units from the neural network during training, motivated us to explore the use of randomized reasoning traces in our framework, and we aim to simplify the A* search trace by exploiting its structured elements and selectively dropping certain parts for each training example. The details of our approach are described below.\nAs shown in Figure 4.1, the A* search trace contains both the create and the close clauses, and each clause includes the node's coordinates and its (estimated) cost to reach the start and the goal locations. To derive Dualformer, we exploit the structure of the search trace and drop certain parts of it for each training example. There are three natural types of dropping:\n\u2022 D1: drop a close clause\n\u2022 D2: drop the cost tokens in a clause\n\u2022 D3: drop a create clause\nBased on them, we develop four levels of dropping strategies, each building upon the previous one. Specifically,\n\u2022 the Level 1 strategy eliminates all the close clauses from a search trace.\n\u2022 The Level 2 strategy goes further by additionally dropping all the cost tokens.\n\u2022 The Level 3 strategy is even more aggressive. It further randomly drops 30% of the create clauses.\n\u2022 Ultimately, the Level 4 strategy drops the entire trace.\nFigure 4.1 illustrates our strategies using the previously mentioned Maze task. Intuitively, the Level 1 dropping instructs Dualformer to effectively bypass the close-set computation of A* search, the Level 2"}, {"title": "Controllable Generation", "content": "One appealing property of Dualformer is that it can be easily prompted to operate in either fast or slow generation mode at inference time. The control mechanism is extremely simple: we append bos and a control token to the standard prompt (which includes environment and task description), where the control token is either plan or create. If we use plan, Dualformer will operate in fast mode and directly output the plan, bypassing the reasoning steps. On the other hand, if we inject create after bos, Dualformer will work in slow mode and generate both the reasoning trace and the final plan. See Appendix B for concrete examples. If we only use the standard prompt, Dualformer will mimic the dual process of human decision making\u2014depending on the situation, it generates either types of responses, which correspond to System 1 and System 2 reasoning."}, {"title": "Experiments", "content": "Our experiments are designed to answer the following questions:\n1. Does Dualformer outperform corresponding baselines in fast, slow and auto mode? Does it generate more diverse plans?\n2. In slow model, does Dualformer lead to faster reasoning, i.e., output a shorter trace?\n3. Does the structured trace dropping technique generalize to LLMs trained on natural language datasets?\nWe answer questions 1 and 2 in Section 5.1, where we train Transformers to solve Maze navigation tasks and the closely related Sokoban games, similar to SearchFormer (Lehnert et al., 2024). To answer questions 3, we finetune LLama-3.1-8B and Mistral-7B models to solve math problems in Section 5.2."}, {"title": "Navigation Tasks: Maze & Sokoban", "content": "Following Lehnert et al. (2024), we consider the Maze and the Sokoban tasks and use the same dataset. Both datasets contain 10\u2075 training examples with complete A* search trace. The A* implementation is non-deterministic where it breaks cost ties randomly and randomizes the order in which child nodes are expanded. The size of Maze varies from 15 x 15 to 30 \u00d7 30. For all the Maze tasks, we randomly generate 30% - 50% percentage of wall cells as obstacles, and randomly sample the goal and start locations. The"}, {"title": "Slow Mode", "content": "Table 5.2 reports the results when Dualformer operates in slow mode. The corresponding baseline is the Complete-Trace model, which uses the same architecture and is trained on data with complete A* search traces. In addition to the metrics reported before, we report the average length of reasoning traces across the 64 responses, aggregated over all the 1000 evaluation tasks. The results show that Dualformer achieves both enhanced planning power and reasoning speed. It outperforms the Complete-Trace model for all the correctness and optimality metrics: solved rates, optimal rates, and SWC. Moreover, the reasoning trace yielded by Dualformer is notably shorter than the baseline model. On average, Dualformer reduces the trace length by 49.4% across the five tasks. As before, Dualformer also generates more diverse plans compared with the baseline. We refer the readers to Appendix C for concrete examples.\nComparison with Search Dynamics Bootstrapping The Complete-Trace model is the base Searchformer model in Lehnert et al. (2024), which has also proposed a search dynamics bootstrapping method to enhance its performance on the Sokoban task, similar to those in Anthony et al. (2017); Zelikman et al. (2022). After training the Searchformer model, we fine-tune it on a newly created self-bootstrapped dataset. For each Sokoban game in the original dataset, we generate 32 responses and include the shortest optimal response into the new dataset. We can repeat this process multiple times. In this way, the Searchformer learns to generate shorter responses. Table 5.4 compares Dualformer with Searchformer models fine-tuned up to 3 steps. Dualformer is comparable or better than bootstrapped models in most of the metrics, while only using fewer than 45.1% reasoning steps. We note that each bootstrapping step requires rollouting 3.2 \u00d7 10\u2076 total responses and extra fine-tuning of 10\u2074 iterations, This means, including the 8 \u00d7 10\u2075 pretraining iterations, Searchformer step 3 requires a total of 8.3 \u00d7 10\u2075 training iterations and 9.6 \u00d7 10\u2076 rollouts, which is expensive in computation. In comparison, Dualformer only needs a single training stage that consists of 8 \u00d7 10\u2075 iterations, with no additional rollout requirements."}, {"title": "Auto Mode: Dual Process", "content": "Instead of controlling the inference mode of Dualformer by injecting a control token after bos, we can also sample from it directly, allowing it to freely determine the mode of operation, similar to the dual process of human decision making. We call this auto mode for Dualformer. Table 5.3 reported the results. The auto mode Dualformer also outperforms both the Complete-Trace and Solution-Only model for all the tasks we consider."}, {"title": "Ablation Study", "content": "As discussed in Section 4, the randomized traces for training Dualformer results from different trace dropping strategies, and there are numerous ways to combine them. We hereby ablate the design choices we made. First, to enable execution in both fast and slow modes, a na\u00efve alternative approach is to train Dualformer using a mixture of solution-only and complete-trace data, i.e. p\u2081 = p\u2082 = p\u2083 = 0 in our randomization strategy. We refer to such variants as Mix-p models, where p is the fraction of solution-only data in the training dataset. Note that the Solution-Only model is essentially a Mix-1 model, and the Complete-Trace model is a Mix-0 model. Below, we compare Dualformer to Mix-p models for both inference modes. Second, our dropping strategies are structured in a hierarchical manner. For instance, Level 2 dropping is developed based on the Level 1 strategy. We investigate how the performance changes when we halt the process at a specific level."}, {"title": "Application to LLM Training: Math Reasoning", "content": "In this section, we show the efficacy of structured trace dropping techniques for training large-scale LLMs to solve math problems. Particularly, we finetune Llama-3-8B and Mistral-7B models using a dataset that contains a variety of math questions and answers with detailed reasoning steps, where we utilize a trace dropping technique that leverages the specific structure of the reasoning trace for math problems too. We benchmark the resulting models against corresponding base models finetuned on the dataset directly."}, {"title": "Conclusion", "content": "We present a simple and easy-to-implement framework for training Transformers to solve reasoning and planning tasks. We carefully probe the structure of the reasoning traces and design corresponding dropping strategies that imitate the shortcuts in human thinking process. By randomly applying the dropping strategies to training examples, the resulting model, Dualformer, can be controlled to execute in either fast or slow reasoning mode, or in auto mode where it decides the mode to engage. Dualformer achieves enhanced performance for the maze navigation tasks and the Sokoban game, while reducing the number of reasoning steps required. Remarkably, our approach is not limited to training task-specific models from scratch. We apply techniques in the same spirit to finetune LLMs to answer math questions and obtain improved performance. Last, the proposed framework also reduces computation consumption as the input sequences are shortened after trace dropping. Future work could investigate whether our approach helps models scale, and explore methodologies such as curriculum learning and hierarchical planning to adapt Dualformer for more complex tasks."}, {"title": "Network Architecture and Hyperparameters", "content": "We use the same encoder-decoder Transformer architecture as in Lehnert et al. (2024) for Dualformer. It first converts each token into a one-hot vector, which is then transformed into a set of vectors through the embedding layer. The embedding vectors then go through the subsequent layers shown in Lehnert et al. (2024, Figure 4). We use RoPE embeddings for positional encoding, and no dropout is used in our architecture.\nFor the model size, architecture parameters, batch size, we use the same setup as in as (Lehnert et al., 2024). Specifically, for the Maze tasks, we use a 15M-parameter model which consists of 3 attention heads and 6 layers with hidden size 64. We optimize the model by the AdamW (Loshchilov and Hutter, 2019) optimizer with learning rate 2.5e-4 and batch size 16, where \\( \\beta_0 \\) and \\( \\beta_1 \\) set to 0.9 and 0.99, respectively. A linear warm-up strategy was employed for the first 2000 gradient updates. Afterward, we use the cosine learning rate scheduler (Loshchilov and Hutter, 2016).\nFor the Sokoban tasks, we use a 46M-parameter model which consists of 4 attention heads and 8 layers with hidden size 96. We use batch size 64 and learning rate 7.5e-5, while the other hyperparameters are the same as above."}, {"title": "Maze and Sokoban", "content": "We used the same dataset as in Lehnert et al. (2024), which is available at https://github.com/facebookresearch/\nsearchformer. The Maze dataset contains 10M examples and the Sokoban dataset contains 10M examples\nand we use the first 100k example sorted in accordance to their \"id\" field in mongodb (following same approach\nas (Lehnert et al., 2024)). For reader's reference, the dataset is generated as follows. For the Maze tasks,\n30-50% of the cells were first randomly designated as walls. Next, a start and a goal location were chosen\nrandomly. Then, the A* algorithm was applied to generate an optimal plan. For the Sokoban tasks, we use a\n7x7 grid map where two wall cells were randomly inserted as obstacles. Moreover, two docks, two boxes, and\ntwo worker locations were placed randomly. Once a game is generated, it is only added to the dataset if it\ncould be solved by the A* algorithm."}, {"title": "Math Reasoning", "content": "We use the implementation provided at https://github.com/meta-llama/llama-recipes for finetuning the models.\nWe train all the models for 2 epochs, using a batch size of 32. We use the AdamW optimizer with a learning rate of 5 \u00d7 10\u207b\u2076 for the Llama model and 8 \u00d7 10\u207b\u2076 for the Mistral models. The learning rate is selected as follows. We sweep over 3 values 2 \u00d7 10\u207b\u2076, 5 \u00d7 10\u207b\u2076,8 \u00d7 10\u207b\u2076 and choose the learning rate that yields the lowest validation loss. We then retrain the models using the selected learning rates on the full training dataset and report the results. We use the default values for the other hyperparameters. More specifically, we do not use linear rate warmup, weight decay, nor multistep gradient accumulation. We use betas=(0.9, 0.999), eps=1e - 8, \\(\\gamma \\) = 0.85 (multiplicative step-wise learning rate decay) for AdamW and \u201cpacking\u201d as the batching strategy."}, {"title": "Controllable Generation of Dualformer", "content": "Dualformer offers flexible output options for users to choose. In this section, we demonstrate this by an example navigation task in a 15 \u00d7 15 maze, see Figure B.1. The start location is (9, 10) and the goal location is (3, 6). The location coordinates and the maze structure is encoded in the original prompt below.\nTo configure Dualformer's operation mode, we only need to append bos and a control token to the original prompt. To output solution only (fast mode), we inject plan. On the other hand, we inject create to let Dualformer output both trace and solution."}, {"title": "Fast Mode", "content": "The modified prompt for fast mode is displayed below. The extra tokens are highlighted in purple.\nThe following box shows the generated tokens, and Figure B.1a plot the corresponding path."}, {"title": "Slow Mode", "content": "Similarly, we present the modified prompt for slow mode below. Comparing with the fast mode prompt, the only change is the control token becomes create."}, {"title": "Diversity of Generated Plans", "content": "The Dualformer outperforms baseline models in discovering unique feasible solutions. To illustrate this visually, we select one example maze task and generate 64 responses using Dualformer in fast mode. Figure C.1 plots all the unique feasible paths discovered by fast mode Dualformer alongside those found by the Solution-Only baseline (64 responses). Dualformer (fast mode) identified 42 unique feasible paths, while the Solution-Only model only found 3. Similarly, Figure C.2 compares slow mode Dualformer and the Complete-Trace (Searchformer) baseline. Dualformer (slow mode) discovered 39 unique feasible paths, whereas the Complete-Trace model only found 17."}, {"title": "Comparison with Different Randomization Strategies", "content": "In table E.1 below, we show the probabilities of different dropping strategies we use for comparing different randomization strategies."}, {"title": "Details of the Math Reasoning Experiments", "content": null}, {"title": "Solution Rewriting", "content": null}, {"title": "Solution Rewriting Prompt", "content": "I have a math problem and an initial chain of thought reasoning that needs elaboration. Please provide\na more detailed step-by-step explanation for each part of the reasoning, including intermediate steps,\ncalculations, and rationales behind each decision. Also, suggest any additional insights that might be\nrelevant. Problem: {Question}? Current Chain of Thought: {CoT} The answer is {Final-answer}.\nKeep the total response less than 2048 tokens. Please expand on the above reasoning and give detailed\nreasoning chain and explanation. Write your answer in this format: \"Here is a detailed step-by-step\nexplanation of the reasoning: Reasons: <reason step >. eg.Step 1..Step 2... Step 3...Step N... The\nanswer is: <answer >.\" Remember, always conclude with \"\\n\\nThe answer is: <answer >.\""}, {"title": "Solution Rewriting Examples", "content": "Below, we provide an example of a math question along with four generated solutions that have been rewritten by the Llama-3.1-70B-Instruct model. It is evident from this example that the original math dataset lacks sufficiently detailed solution steps required to solve the question, such as the formula for a geometric series."}, {"title": "Finetuning and Evaluation Prompts", "content": "We use the following prompt for finetuning both the Mistral and the Llama model, following (Yu et al., 2023)."}, {"title": "Fintuning Prompt", "content": "<start-header-token>Below is an instruction that describes a task. Write a response that appropriately\ncompletes the request. \\n\\n ### Instruction:\\n{Question} \\n\\n ### Response: Let's think step\nby step. {CoT steps + final solution}"}, {"title": "Slow Mode Evaluation Prompt", "content": "<start-header-token>Below is an instruction that describes a task. Write a response that appropriately\ncompletes the request. \\n\\n ### Instruction:\\n{Question} \\n\\n ### Response: Let's think step\nby step."}, {"title": "Fast Mode Evaluation Prompt", "content": "<start-header-token>Below is an instruction that describes a task. Write a response that appropriately\ncompletes the request. \\n\\n ### Instruction:\\n{Question} \\n\\n ### Response: \\n\\nThe answer\nis:"}, {"title": "Example Outputs", "content": "We provide a few example math questions and answers randomly sampled from our trained model, alongside the answers output by the baseline model. Within each answer box, on the left is the baseline model output (long and lengthy) and on the right is our method output (effective and efficient, colored in blue)."}, {"title": "LLM Generated Solutions for Maze", "content": "Readers might wonder whether modern state-of-the-art LLMs can effectively solve our maze problem. To test this, we randomly selected a 30 \u00d7 30 maze problem and ask the ol-preview model to find the shortest path. Ol-preview is the latest reasoning model by OpenAI which operates in slow mode: it spends more time thinking before they respond. It turns out that these problems are very challenging for LLMs. As illustrated in Figure G.1, the path suggested by ol-preview incorrectly traverses through the maze walls. In contrast, Dualformer correctly identifies one optimal path that follows through the maze without any errors."}]}