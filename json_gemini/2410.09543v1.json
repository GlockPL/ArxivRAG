{"title": "BOLTZMANN-ALIGNED INVERSE FOLDING MODEL AS A PREDICTOR OF MUTATIONAL EFFECTS ON PROTEIN-PROTEIN INTERACTIONS", "authors": ["Xiaoran Jiao", "Weian Mao", "Wengong Jin", "Peiyuan Yang", "Hao Chen", "Chunhua Shen"], "abstract": "Predicting the change in binding free energy ($\\Delta\\Delta G$) is crucial for understanding and modulating protein-protein interactions, which are critical in drug design. Due to the scarcity of experimental $\\Delta\\Delta G$ data, existing methods focus on pre-training, while neglecting the importance of alignment. In this work, we propose the Boltzmann Alignment technique to transfer knowledge from pre-trained inverse folding models to $\\Delta\\Delta G$ prediction. We begin by analyzing the thermodynamic definition of $\\Delta\\Delta G$ and introducing the Boltzmann distribution to connect energy with protein conformational distribution. However, the protein conformational distribution is intractable; therefore, we employ Bayes' theorem to circumvent direct estimation and instead utilize the log-likelihood provided by protein inverse folding models for $\\Delta\\Delta G$ estimation. Compared to previous inverse folding-based methods, our method explicitly accounts for the unbound state of protein complex in the $\\Delta\\Delta G$ thermodynamic cycle, introducing a physical inductive bias and achieving both supervised and unsupervised state-of-the-art (SoTA) performance. Experimental results on SKEMPI v2 indicate that our method achieves Spearman coefficients of 0.3201 (unsupervised) and 0.5134 (supervised) on SKEMPI v2, significantly surpassing the previously reported SoTA values of 0.2632 and 0.4324, respectively. Futhermore, we demonstrate the capability of our method on binding energy prediction, protein-protein docking and antibody optimization tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Protein-protein interactions (PPIs) are fundamental to the execution of diverse and essential biological functions across all organisms. High-fidelity computational modeling of protein-protein interactions is indispensable. The interaction properties of protein-protein binding can be quantitatively character-ized by binding free energy ($AG$), the difference in Gibbs free energy between the bound state and the unbound state. Predicting the change in binding free energy ($\\Delta\\Delta G$), also known as mutational effect, is crucial for modulating protein-protein interactions. Accurate $\\Delta\\Delta G$ prediction facilitates the identification of mutations that enhance or diminish binding strength informs the efficient design of proteins, thereby accelerating the development of therapeutic interventions and deepening our understanding of biological mechanisms.\nDeep learning has demonstrated significant potential in protein modeling (Lin et al., 2022; Abramson et al., 2024), inspiring a paradigm shift in computational methods for predicting $\\Delta\\Delta G$. Traditional biophysics-based and statistics-based approaches (Park et al., 2016; Alford et al., 2017; Delgado et al., 2019) are increasingly being replaced by deep learning techniques (Shan et al., 2022a; Liu et al., 2023). Despite advancements, $\\Delta\\Delta G$ prediction is still hindered by the scarcity of annotated experimental data. Consequently, pre-training on extensive unlabeled data has emerged as a prevalent strategy for $\\Delta\\Delta G$ prediction (Luo et al., 2023; Wu et al., 2024). Recent studies observe that structure prediction models and inverse folding models implicitly capture the energy landscape (Bennett et al., 2023; Widatalla et al., 2024; Lu et al., 2024). Although effective, these pre-training-based approaches simply employ supervised fine-tuning (SFT) and neglect the importance of alignment. Since SFT may result in catastrophic forgetting of the general knowledge gained during unsupervised pre-training, there remains an opportunity for better knowledge transfer.\nIn other biological tasks, recent studies have adapted alignment techniques from large language models, such as direct preference optimization (DPO) (Rafailov et al., 2023), to integrate experimental fitness information into biological generative models (Zhou et al., 2024; Widatalla et al., 2024; Chennakesavalu et al., 2024; Liu et al., 2024). However, directly adopting these alignment techniques for $\\Delta\\Delta G$ prediction is inadequate, as they lack the physical inductive bias required for energy-related biological tasks.\nIn this work, we propose a technique named Boltzmann Alignment to transfer knowledge from pre-trained inverse folding models to $\\Delta\\Delta G$ prediction. We first analyze the thermodynamic definition of $\\Delta\\Delta G$ and introduce the Boltzmann distribution to connect energy with protein conformational distri-bution, thereby highlighting the potential of pre-trained probabilistic models. However, the protein conformational distribution is intractable. To address this, we employ Bayes' theorem to circumvent direct estimation and instead leverage the log-likelihood provided by protein inverse folding models for $\\Delta\\Delta G$ estimation. Our derivation provides a rational perspective on the previously observed high correlation between binding energy and log-likelihood from inverse folding models (Bennett et al., 2023; Widatalla et al., 2024). Compared to previous inverse folding-based methods, our method explicitly considers the unbound state of the protein complex, enabling fine-tuning of inverse folding models in a manner consistent with statistical thermodynamics. Our contributions can be summarized as follows:\n\u2022 We propose Boltzmann Alignment, which introduces physical inductive bias through Boltz-mann distribution and thermodynamic cycle to transfer knowledge from pre-trained inverse folding models to $\\Delta\\Delta G$ prediction. We not only provide an alignment technique, but also offer insight into the high correlation previously observed between binding energy and log-likelihood in inverse folding models.\n\u2022 Experimental results on SKEMPI v2 indicate that our method achieves Spearman coefficients of 0.3201 (unsupervised) and 0.5134 (supervised) on SKEMPI v2, significantly exceeding the previously reported SoTA values of 0.2632 and 0.4324, respectively. Further ablations demonstrate that our approach outperforms previous inverse folding-based methods and other alignment techniques. Additionally, we showcase the broader applicability of our method in binding energy prediction, protein-protein docking, and antibody optimization."}, {"title": "2 RELATED WORK", "content": "2.1 ALIGNMENT OF GENERATIVE MODELS\nThe prevailing pre-training approach for generative models, which focuses on maximizing the likelihood of training data, often falls short in aligning with specific user preferences. Consequently, there is growing interest in integrating task-specific information while maintaining the generative capabilities, especially in large language models (LLM). A commonly used alignment method is supervised fine-tuning (SFT), where models undergo additional training on curated datasets with specific annotations, utilizing a straightforward loss function. However, SFT poses the risk of overfitting to the fine-tuning dataset, which may result in catastrophic forgetting of the general knowledge gained during unsupervised pre-training. Much of the current alignment paradigm relies on reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022). Within this framework, human preferences provided as pairwise rankings are used to train a reward model, which can then optimize a policy using proximal policy optimization (PPO) (Schulman et al., 2017). An alternative approach, known as direct preference optimization (DPO) (Rafailov et al., 2023), serves as a plug-and-play method that maximizes the likelihood of preferences. Numerous recent studies have expanded on the concepts of RLHF and DPO (An et al., 2023; Azar et al., 2023; Meng et al., 2024; Zhang et al., 2024).\nDespite these advancements in LLM, developing effective methods to integrate experimental fitness information into biological generative models remains a crucial open question. Park et al. (2023) and Chennakesavalu et al. (2024) explore alignment in the context of inverse molecular design, enhancing the desired properties of generated compounds against a drug target. Liu et al. (2024) fine-tunes retrosynthetic planning models with a preference-based loss to improve the quality of generated synthetic routes. AbDPO (Zhou et al., 2024) introduces direct energy-based preference optimization to structure-sequence antibody co-design diffusion model, producing antibodies with low energy and high binding affinity. Mistani & Mysore (2024) incorporates DPO into protein language models to generate peptides with desirable physicochemical properties that bind to a given target protein. ProteinDPO (Widatalla et al., 2024) imbues structure-informed protein language model with biophysical information through DPO, enabling it to score stability and generate stable protein sequences. These efforts collectively underscore the potential of alignment techniques in advancing the performance of biological generative models.\n2.2 PROTEIN-PROTEIN INTERACTION MODELING\nProtein-protein interaction modeling has been extensively studied for decades, primarily concentrating on two key questions: where proteins interact, known as binding site prediction (Tubiana et al., 2022; Fang et al., 2023), and how they interact, addressed by protein-protein docking (Yan et al., 2020; Ketata et al., 2023). Recently, AlphaFold3 (Abramson et al., 2024) reaches almost 80% success rate of predicting general protein complex structures, effectively tackling both the \"where\" and \"how\" of protein-protein interactions. However, structure is not everything (Lowe, 2023); biomolecular interactions cannot be fully captured by static structures alone. A comprehensive understanding requires appreciating the dynamic processes of association and dissociation between protein partners (Lu et al., 2024), which are quantified by measures such as the binding affinity (Kd) or binding free energy ($AG$). Predicting the change in binding free energy ($\\Delta\\Delta G$) is thus crucial for modulating protein-protein interactions; it enables the identification of mutations that enhance or diminish binding strength, which is essential for efficient protein design. For example, in antibody discovery, accurately predicting $\\Delta\\Delta G$ is fundamental to antibody maturation.\nA variety of methods have been developed to predict $\\Delta\\Delta G$. Traditional approaches can be categorized into two main types: biophysical and statistical methods. Biophysical methods (Park et al., 2016; Alford et al., 2017; Delgado et al., 2019) use energy calculations to model how proteins interact at the atomic level, but they often struggle with balancing speed and accuracy. Statistical methods (Geng et al., 2019; Zhang et al., 2020) depend on feature engineering, utilizing descriptors that capture geometric, physical, and evolutionary characteristics of proteins. Traditional approaches depend heavily on human expertise and struggle to accurately capture complex interactions between proteins. Recently, deep learning-based approaches have emerged. Despite advancements in end-to-end learning approaches (Shan et al., 2022a), $\\Delta\\Delta G$ prediction is still limited by the lack of annotated experimental data. As a result, pre-training on extensive unlabeled data has become a common"}, {"title": "3 METHOD", "content": "Based on the Boltzmann distribution and thermodynamic cycles, as illustrated on the right side of Figure 1, we first establish the connection between $\\Delta\\Delta G$ and protein sequence likelihoods, which we call Boltzmann Alignment (Sec. 3.1). Next, we present a method (Sec. 3.2) that integrates the inverse folding model into Boltzmann alignment. This method is named BA-Cycle and uses the inverse folding model to evaluate $\\Delta\\Delta G$ by predicting the likelihoods of protein sequences, as shown on the left side of Figure 1. Upon BA-Cycle, we introduce a method named BA-DDG (Sec. 3.3) to fine-tune the inverse folding model using $\\Delta\\Delta G$-labeled data, introducing an inductive bias from statistical thermodynamics.\n3.1 BOLTZMANN ALIGNMENT\nTo simplify the explanation, we introduce the case where the complex consists of two chains, chain A and chain B; however, the following content can be extended to cases with more chains. The binding free energy ($AG$) of the complex is the difference between the Gibbs free energy of the bound state, denoted as $G_{bnd}$, and the Gibbs free energy of the unbound state, denoted as $G_{unbnd}$. Furthermore, using the Boltzmann distribution (Atkins et al., 2023), we can express the binding free energy ($AG$) in terms of the probabilities of the two chains being in the bound state, $P_{bnd}$, and in the unbound state, $P_{unbnd}$:\n$AG = G_{bnd} - G_{unbnd} = -k_BT \\cdot (log P_{bnd} - log P_{unbnd})$\nwhere $k_B$ is the Boltzmann constant and T is the thermodynamic temperature. Specifically, the probabilities $P_{bnd}$ and $P_{unbnd}$ represent the probabilities of the protein complex structure being in the bound conformation $X_{bnd}$ and the unbound conformation $X_{unbnd}$, respectively, when the protein complex sequence $S_{AB}$ is given. Therefore, $P_{bnd}$ and $P_{unbnd}$ can be expressed as the conditional probabilities $p(X_{bnd} | S_{AB})$ and $p(X_{unbnd} | S_{AB})$, where we simplify and approximate $X_{bnd}$ and $X_{unbnd}$ as the backbone structure of the protein complex. Substituting these conditional probabilities into Eq.1 gives:\n$AG = -k_BT (logp(X_{bnd} | S_{AB}) \u2013 log p(X_{unbnd} | S_{AB}))$\nEstimating $p(X | S)$ poses several challenges. First, most current protein structure prediction models typically predict a single conformation and are not inherently probabilistic. Although these models provide uncertainty metrics such as pLDDT, PAE, and pTM (Abramson et al., 2024), and while these metrics have been observed to correlate with binding free energy (Lu et al., 2024), they do not offer a true probabilistic interpretation. Second, although recent efforts (Jing et al., 2023; 2024; Abramson et al., 2024) aim to introduce probabilistic models, specifically diffusion models, these neural networks learn to estimate the score $\\nabla_x log p(X | S)$ rather than $p(X | S)$. Moreover, due to the relatively low sample efficiency of diffusion models, directly estimating $p(X | S)$ remains intractable. Therefore, we use Bayes' theorem to circumvent direct estimation:\n$p(X | S) = \\frac{p(S | X)p(X)}{p(S)}$\nSubstituting this Bayes' theorem equation into Eq.2 and eliminating the protein complex sequence probability $P(S_{AB})$ gives:\n$AG=-k_BT \\cdot (log \\frac{p(S_{AB} | X_{bnd}) \\cdot p(X_{bnd})}{p(S_{AB})} - log \\frac{p(S_{AB} | X_{unbnd}) \\cdot p(X_{unbnd})}{p(S_{AB})})$\n$ = -k_BT \\cdot log (\\frac{p(S_{AB} | X_{bnd}) \\cdot p(X_{bnd})}{p(S_{AB} | X_{unbnd}) \\cdot p(X_{unbnd})})$\nAt this point, we have established the connection between the conditional probability of the protein sequence $p(S | X)$ and $AG$. Next, based on this, we further analyze the connection between the"}, {"title": "3.2 PROBABILITY ESTIMATION", "content": "conditional probability of the protein sequence $p(S | X)$ and $\\Delta AG$. Specifically, $\\Delta\\Delta G$ represents the difference between the $AG$ of the wild type and the $AG$ of the mutant type. The wild type and mutant type refer to the unmutated and mutated protein complexes, respectively. To distinguish between these two types, we denote them by adding \u201cmut\u201d and \u201cwt\u201d in the upper right corner of the notation; for example, $\\Delta G^{mut}$ represents the $AG$ of the mutant type. With this notation, we can express $\\Delta\\Delta G$ based on Eq.5 as follows:\n$\\Delta\\Delta G = \\Delta G^{mut} - \\Delta G^{wt}$\n$ = -k_BT \\cdot (log \\frac{p(S_{AB}^{mut} | X_{bnd}^{mut}) \\cdot p(X_{bnd}^{mut})}{p(S_{AB}^{mut} | X_{unbnd}^{mut}) \\cdot p(X_{unbnd}^{mut})} - log \\frac{p(S_{AB}^{wt} | X_{bnd}^{wt}) \\cdot p(X_{bnd}^{wt})}{p(S_{AB}^{wt} | X_{unbnd}^{wt}) \\cdot p(X_{unbnd}^{wt})})$\nThe backbone structure of the mutant type, $X^{mut}$, is actually unknown. To address this issue, we introduce the assumption used in previous methods that the protein backbone structure remains unchanged before and after mutation. Therefore, we can approximate $X_{bnd}^{wt}$ to be equal to $X_{bnd}^{mut}$ and $X_{unbnd}^{wt}$ to be equal to $X_{unbnd}^{mut}$. Based on this assumption, we can eliminate the probability terms in Eq.7:\n$\\Delta\\Delta G = -k_BT \\cdot (log \\frac{p(S_{AB}^{mut} | X_{bnd}^{mut})}{p(S_{AB}^{mut} | X_{unbnd}^{mut})} - log \\frac{p(S_{AB}^{wt} | X_{bnd}^{wt})}{p(S_{AB}^{wt} | X_{unbnd}^{wt})})$\nEq.8 transforms $\\Delta\\Delta G$ into the likelihoods of protein sequences $p(S_{AB} | X)$. Here, we demonstrate the method of estimating the sequence probabilities of the bound state, $P(S_{AB} | X_{bnd})$, and the unbound state, $p(S_{AB} | X_{unbnd})$, using the inverse folding model. Since we assume that the backbone structure remains unchanged before and after mutation, i.e., $X^{wt} = X^{mut}$, the evaluation method for both the wild type and mutant type is identical, and no additional discussion is required.\nThe bound state. In our task, the backbone structure $X_{bnd}$ of the bound state is usually known. Therefore, the inverse folding model can directly evaluate the probability $p(S_{AB} | X_{bnd})$. We denote the probability predicted by the inverse folding model as $p_\\theta (S_{AB} | X_{bnd})$, where $\\theta$ is the model parameter.\nThe unbound state. The unbound state is typically not explicitly provided in known backbone structure; therefore, we propose an alternative probability estimation method. The unbound state approximately represents the scenario where chains A and B of the complex are relatively far apart, with minimal interactions between them. Thus, a reasonable estimation approach is to independently evaluate these two chains using the inverse folding model. We denote the structure and sequence of chain A as $X_A$ and $S_A$, and those of chain B as $X_B$ and $S_B$. The estimation method for $p(S_{AB} | X_{unbnd})$ can be expressed as:\n$p(S_{AB} | X_{unbnd}) \\approx p_\\theta(S_A | X_A) p_\\theta(S_B | X_B)$\nwhere $p_\\theta(S_A | X_A)$ and $p_\\theta(S_B | X_B)$ are the probabilities predicted by the inverse folding model.\nUnsupervised estimation of $\\Delta\\Delta G$. Based on the estimation method described above, we can utilize a pretrained inverse folding model, ProteinMPNN (Dauparas et al., 2022), to achieve unsupervised evaluation of $\\Delta\\Delta G$. We name this approach BA-Cycle, which can be expressed as:\n$\\Delta\\Delta G = (log \\frac{p_\\theta (S_{AB}^{mut} | X_{bnd}^{mut})}{p_\\theta (S_A^{mut} | X_A^{mut}) p_\\theta (S_B^{mut} | X_B^{mut})} - log \\frac{p_\\theta (S_{AB}^{wt} | X_{bnd}^{wt})}{p_\\theta (S_A^{wt} | X_A^{wt}) p_\\theta (S_B^{wt} | X_B^{wt})})$\nComparison with previous work. Previous work (Bennett et al., 2023; Cagiada et al., 2024; Widatalla et al., 2024) attempts to use protein inverse folding models to predict $\\Delta\\Delta G$. However, these studies do not explicitly account for the unbound state $p_{unbnd}$ in the thermodynamic cycle. We denote these methods as $\\Delta\\Delta G_{Prev}$, summarized as:\n$\\Delta\\Delta G_{Prev} = -k_BT \\cdot (log p_\\theta(S_{AB}^{mut} | X_{bnd}^{mut}) \u2013 log p_\\theta(S_{AB}^{wt} | X_{bnd}^{wt}))$\n3.3 SUPERVISION\nWe propose BA-DDG, a method that leverages $\\Delta\\Delta G$-labeled data to fine-tune BA-Cycle through Boltzmann Alignment. BA-DDG employs a forward process identical to that of BA-Cycle. During"}, {"title": "4 EXPERIMENTS", "content": "We investigate three key questions through a series of experiments: (1) whether the physics-informed alignment technique we propose can achieve state-of-the-art (SoTA) accuracy on the SKEMPI v2 dataset under both unsupervised and supervised settings (Sec.4.2); (2) whether the introduced thermodynamic cycle is indeed effective and has advantages over traditional SFT and DPO methods; (3) whether predicted structures can effectively replace crystal structures as inputs in our method (Sec.4.3). Before addressing these questions, we first introduce the benchmarks and baselines (Sec.4.1). Additionally, we explore potential applications of our method, taking binding energy prediction, protein-protein docking and antibody optimization as examples (Sec.4.4).\n4.1 BENCHMARK\nDataset. The SKEMPI v2 dataset (Jankauskait\u0117 et al., 2019), the extensive annotated mutation dataset for 348 protein complexes, includes 7,085 amino acid mutations along with changes in thermodynamic parameters and kinetic rate constants. Despite lacking structures of the mutated complexes, it serves as the most well-established benchmark for $\\Delta\\Delta G$ prediction. To prevent data leakage, we follow Luo et al. (2023) and Wu et al. (2024), first dividing the dataset into 3 folds by structure, ensuring each fold contains unique protein complexes. This division forms the basis of our 3-fold cross-validation process, where two folds are used for training and validation, and the third fold is reserved for testing. This approach yields 3 different sets of parameters and ensures that every data point in SKEMPI v2 is tested once.\nEvaluation metrics. To thoroughly assess the performance of $\\Delta\\Delta G$ prediction, we utilize a total of 7 metrics. This includes 5 overall metrics: (1) Pearson correlation coefficient, (2) Spearman's rank correlation coefficient, (3) minimized RMSE (root mean squared error), (4) minimized MAE (mean absolute error), and (5) AUROC (area under the receiver operating characteristic). To calculate AUROC, binary classification labels are assigned to mutations based on the sign of the $\\Delta\\Delta G$ predictions. In practical settings, the correlation for individual protein complexes tends to be of greater importance. Consequently, we group mutations by their structural characteristics, calculate the Pearson and Spearman correlation coefficients for each group, and then report the average of these per-structure correlations as 2 additional metrics.\n4.2 MAIN RESULTS\nComparison with baselines. According to Table 1, our BA-DDG outperforms all the baselines across all evaluation metrics. Notably, it demonstrates a significant improvement in per-structure correlations, highlighting its greater reliability for practical applications. BA-Cycle achieves comparable performance to empirical energy functions and surpasses all unsupervised learning baselines. Detailed results for single-point, multi-point, and all-point mutations are available in Table 6.\n4.3 ABLATION STUDY\nAblation study on thermodynamic cycle. As shown in Table 2, we evaluate the impact of thermody-namic cycle using 3-fold cross-validation on the SKEMPI v2 dataset. We compare our training-free version BA-Cycle to the basic usage of protein inverse folding models, as demonstrated in previous studies (Bennett et al., 2023; Cagiada et al., 2024; Widatalla et al., 2024). As described in Eq.11, these"}, {"title": "5 DISCUSSION AND CONCLUSION", "content": "We propose Boltzmann Alignment, a method that leverages physical inductive bias through the Boltz-mann distribution and thermodynamic cycle to enhance $\\Delta\\Delta G$ prediction by transferring knowledge from pre-trained inverse folding models. Experiments on SKEMPI v2 yield Spearman coefficients of 0.3201 (unsupervised) and 0.5134 (supervised), significantly surpassing previous state-of-the-art values of 0.2632 and 0.4324. Additionally, we demonstrate the broader applicability of our approach in binding energy prediction, protein-protein docking, and antibody optimization.\nOur method has certain limitations that we aim to address in future work. Firstly, our approach requires crystal structures or reliable predicted structures as input; in cases where mutant protein structures are unavailable, we assume that the mutant backbone structure is identical to that of the wild-type protein; we take parts of the complex backbone structure as the backbone structure of single proteins. This reliance on structural data may limit the applicability and effectiveness of our method. Moreover, our current model does not consider side-chain conformations, which may be essential for $\\Delta\\Delta G$ prediction. Incorporating side-chain flexibility could enhance predictive accuracy and provide deeper insights into protein-protein interactions.\nThe positive societal impact is that this work can assist in drug design and virtual screening potentially. No negative societal impact is perceived."}, {"title": "A IMPLEMENTATION DETAILS", "content": "A.1 OTHER ALIGNMENT TECHNIQUES\nA.1.1 SFT\nThe training objective of trivial SFT is as follows:\n$L_{SFT} = ||\\Delta\\Delta G - \\Delta\\Delta G_{Prev} || - \\beta D_{KL}(P_\\theta (S | X)||P_{ref} (S | X))$\nA.1.2 DPO\nWe utilize weighted DPO algorithm proposed in Widatalla et al. (2024). The weighted DPO objective is simply minimization of the KL-divergence between the distribution parameterized by the generative model and numerical scores ($\\Delta\\Delta G$ labels).\nA.2 MODEL\nIn ProteinMPNN, part of the input protein sequence is to be designed, and the rest is fixed; decoding skips fixed regions but includes them in the context for designing the remaining positions during autoregressive decoding. When estimating $\\Delta\\Delta G$ with ProteinMPNN, we designate the mutation sites SD to be designed while fixing the rest of the sequence S \\ SD. Let $\\pi = (s_1, s_2,..., s_n)$ represent a randomly sampled decoding order, a permutation of the mutation sites SD. The joint probability $p_\\theta (S | X)$ can be expanded step by step into the product of model-predicted probabilities for each amino acid site using the conditional probability formula:\n$p_\\theta(S | X) = p_\\theta(S_D | X, S\\backslash S_D)$\n$ = p_\\theta(S_D\\backslash{s_1} | X, S \\backslash S_D \\cup {s_1})\\cdot p_\\theta({s_1}| X, S \\backslash S_D)$\nA.3 TRAINING\nA.3.1 HYPER-PARAMETERS\nWe train the model over 20,000 iterations using the Adam optimizer, with a learning rate set at 0.0001 and a batch size of 2. The small batch size is because we need to include both the wild-type and mutant complexes along with their respective monomer structures in a single batch for each iteration. The model is insensitive to the loss weight parameter $\\beta$, so it is typically set to a low value, such as 0.001, during training.\nFor the inverse folding model, we utilized the pre-trained ProteinMPNN, which includes 3 MPNN layers. It considers the 30 nearest neighbors to construct edges and has an embedding dimension of 256.\nA.3.2 HARDWARE\nAll our experiments are conducted on a computing cluster with CPUs of AMD EPYC 7763 64-Core of 3.52GHz and a single NVIDIA GeForce RTX 4090 24GB GPU."}, {"title": "B ADDITIONAL RESULTS", "content": "We compare BA-DDG with several representative methods under single-point, multi-point, and all-point mutations on SKEMPI v2. The results reported in Table 6 demonstrate that BA-DDG achieves the best overall performance for single-point mutations and the best per-structure performance for multi-point mutations. Notably, BA-DDG performs better on single-point mutations compared to multi-point mutations, which may be related to the autoregressive decoding of ProteinMPNN. In our BA-DDG, when dealing with multi-point mutations, the probability estimation for a new site depends on the estimation of the previous site. This coupling might affect performance, presenting an opportunity for future improvements."}]}