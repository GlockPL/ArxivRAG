{"title": "LLM-Human Pipeline for Cultural Context Grounding of Conversations", "authors": ["Rajkumar Pujari", "Dan Goldwasser"], "abstract": "Conversations often adhere to well-understood social norms that vary across cultures. For example, while addressing parents by name is commonplace in the West, it is rare in most Asian cultures. Adherence or violation of such norms often dictates the tenor of conversations. Humans are able to navigate social situations requiring cultural awareness quite adeptly. However, it is a hard task for NLP models.\nIn this paper, we tackle this problem by introducing a Cultural Context Schema for conversations. It comprises (1) conversational information such as emotions, dialogue acts, etc., and (2) cultural information such as social norms, violations, etc. We generate ~110k social norm and violation descriptions for ~23k conversations from Chinese culture using LLMs. We refine them using automated verification strategies which are evaluated against culturally aware human judgements. We organize these descriptions into meaningful structures we call Norm Concepts, using an interactive human-in-loop framework. We ground the norm concepts and the descriptions in conversations using symbolic annotation. Finally, we use the obtained dataset for downstream tasks such as emotion, sentiment, and dialogue act detection. We show that it significantly improves the empirical performance.", "sections": [{"title": "1 Introduction", "content": "Social norms define behavioral expectations shared across groups and societies (Sherif, 1936). They can help explain the differences in the way people from different cultural backgrounds react to the same situation (Triandis et al., 1994; Finnemore, 1996). Hymes (1972) describe norms of interaction as 'shared rules that implicate the belief system of a community', capturing the importance of representing norms when, either human or AI-systems, attempt to make sense of social interactions from different cultures.\nMotivated by Large Language Models emergent reasoning abilities (Wei et al., 2022a; Srivastava et al., 2023; Camburu et al., 2018; Suzgun et al., 2022) several recent works attempted to create repositories of cultural norms using novel prompting approaches followed by automated verification of the generated descriptions (Fung et al., 2023; Li et al., 2023; Ziems et al., 2023). However, these efforts had limitations such as using synthetic conversations or focusing on a handful of situations. LLMs also tend to suffer from several reliability issues such as hallucinations (Rawte et al., 2023) or high sensitivity to prompt structure (Kojima et al., 2023; Prystawski et al., 2023; Wei et al., 2022b).\nThese limitations motivate a more general and principled approach for Cultural Context Understanding, which, we argue, should be viewed as a pragmatic reasoning task. Norms are situated in specific settings and are associated with the social roles participants play (Hare, 2003). Different expectations can be associated with individuals based on attributes such as their status, profession, or gender. Furthermore, these expectations are situation-dependent, for example, changing when engaging in professional or leisure activities.\nTo that end, in this paper, we propose a novel Cultural Context Grounding framework for conversations. We tackle three key problems. First, norm representation capturing the multi-party situated social expectations. Second, norm induction, i.e., populating the suggested representation with norm concepts, emerging from conversational data, and associating them with relevant contextual information. Finally, grounding the norm concepts in conversational data at scale and creating a dataset of norm-schema aligned conversations.\nOur norm representation solution is inspired by the notion of scripts (Schank and Abelson, 1977), i.e., structured representations of expected activities for different roles relevant to a specific scenario, in our case mapping to social scenarios. Unlike past"}, {"title": "2 Related Work", "content": "Social Norm Datasets: Few recent works have leveraged LLMs such as GPT-3 to create useful cultural norm datasets using structure prompting approaches (Fung et al., 2023; Li et al., 2023; Ziems et al., 2023). While these works either focus on a small set or synthetically generated conversations, we generate 63k norm descriptions for 23.5k real conversations and ground them using a principled cultural grounding pipeline.\nSocial Grounding: Pacheco et al. (2023) propose an interactive concept discovery for COVID-19 tweets. Several works address the tasks of principled grounding in social domain(Smith et al., 2018; Roberts et al., 2019; Roy and Goldwasser, 2021; Demszky et al., 2019; Pujari et al., 2024). However, they mainly focus on social grounding in political settings. We focus on the cultural aspect of social context which is challenging to obtain explicit data.\nAutomated Verification using LLMs: Several previous works address automated annotation using LLMs and refining their generations.(Chiang and Lee, 2023; Li et al., 2021; Bano et al., 2023; Bang et al., 2023; Yao et al., 2022; Hendrycks et al., 2020). We build upon the frameworks proposed by Wu et al. (2023) and Arabzadeh et al. (2024) to operationalize our annotation framework."}, {"title": "3 Data", "content": ""}, {"title": "4 Cultural Context Grounding", "content": "Our high-level objective is to 'conceptualize & operationalize cultural understanding in conversational behavior.' We identify three key steps to make progress towards this objective:\n1.  Formalize relevant cultural context for better conversational understanding\n2.  Obtain high-quality cultural context data at scale, leveraging native-culture expertise and ground the conversations in this context.\n3.  Evaluate the obtained cultural context dataset for correctness and usefulness\nFirst, we propose a graph-based schema structure for culturally enriched conversational representation. Our schema consists of two complementing segments: factual components and cultural understanding components. We present an overview of the schema structure in Fig. 1 and a detailed description of the schema structure in \u00a74.1.\nThen, we introduce a robust pipeline for obtaining cultural information for real conversations and grounding the conversations in it. We efficiently leverage (1) native-culture human expertise, (2) LLMs as knowledge elicitors, (3) LLMs as symbolic annotators, and (4) LLMs as multi-agent verifiers, in this pipeline. We obtain a large-scale corpus for ~23k Chinese conversations from three existing datasets (Tab. 1). We present an overview of the proposed pipeline in Fig. 2 and a detailed pipeline description in \u00a74.2.\nFinally, we evaluate (1) correctness of the cultural information obtained using elicitor & symbolic annotator LLMs, and (2) usefulness of the"}, {"title": "4.1 Schema Structure", "content": "Context often dictates whether or not a specific behavior is considered normative. For example, while patronizing younger people might be commonplace in families in some cultures, it could be considered offensive in a professional setting. Especially if the younger person is a work superior. On the other hand, even within the same social setting, norms might vary circumstantially. While joking about an elder's forgetful nature might be okay in some situations, it could be considered insensitive if they are suffering from dementia.\nThis guides us toward two distinct genres of information that could influence conversational behavior: factual and cultural information. To account for this, we propose a schema structure with two distinct segments (Fig. 1(a)). The factual segment of our schema consists of settings, summary, conversation, & relationship components. Cultural Understanding segment consists of social norms, violations, norm concepts, & behavior components. While factual information such as age group, location, and relationships is available for many datasets, descriptions of relevant social norms, their violations, and how they affect the conversation are harder to procure. We address this challenge by efficiently using LLMs and human annotation."}, {"title": "4.2 Grounding Pipeline", "content": "Grounding in NLP usually refers to linking text or speech to real-world concepts such as entities, attitudes, etc (Chandu et al., 2021). Conversational grounding can encompass various aspects, such as mutual beliefs, shared knowledge, assumptions, and so on (Clark, 1996). In this work, we focus specifically on the cultural knowledge aspect of conversational understanding.\nObtaining exhaustive knowledge of a culture's social norms at scale is impractical. Therefore, we propose a bottom-up approach. We use real-world conversations to mine situation-dependent social norms, leveraging LLMs as knowledge elicitors. Native-culture human annotators then create structured abstractions over these descriptions,"}, {"title": "4.2.1 LLM Description Generation", "content": "Two major challenges we face in building our cultural context dataset are (1) collecting structured knowledge about social norms as they manifest in real-world conversations and (2) scaling the descriptions of culturally nuanced behaviors observed in conversations. We address these challenges by leveraging existing conversational datasets and LLMs as knowledge elicitors, respectively. We provide raw conversations to the LLM and instruct it to explain observed behavior from a cultural perspective. For each conversation, we obtain descriptions of relevant social norms, potential violations, and their effects on the conversation trajectory and the participants. We expect this step to be noisy as LLMs are susceptible to hallucinations. However, our primary focus is to obtain broad coverage of diverse cultural nuances that influence behavior in conversations. We evaluate the quality of LLM generations in \u00a75."}, {"title": "4.2.2 HiL Norm Concept Discovery", "content": "As noted in Clark (1996), humans actively draw from a 'common ground' when engaging in conversations. From a cultural standpoint, this common ground includes shared cultural beliefs. Humans often encounter social interactions where cultural awareness is practiced, which makes them adept at situating conversations in cultural common ground. Many aspects of this cultural common ground are highly general and serve us in a variety of situations. However, structured datasets that serve as a common ground for NLP models are hard to create solely using either human annotation or automated"}, {"title": "4.2.3 Symbolic Grounding", "content": "The HiL process creates norm concepts and maps them to conversation-specific descriptions. However, to fully ground the conversation in a norm concept, we must also align the conversation to the concept's symbolic structure. Hence, we leverage LLMs as symbolic annotators to identify instantiations of concept symbols in the conversation.\nWe provide the symbolic annotator LLM with a conversation, LLM-generated descriptions, and associated norm concept structure. We first ask to verify the relevance of the description to the conversation (filtering hallucinations) and the correctness of description-concept mapping (filtering incorrect HiL mappings). Then, we ask it to annotate violation status, actor roles, recipient roles, and violation details, if applicable. We evaluate violation status annotations against human annotation in \u00a75."}, {"title": "4.2.4 Automated Verification", "content": "We refine both the LLM descriptions and symbolic grounding annotations using AgentEval.\nSelf-Verification is the technique of prompting an LLM to re-consider a judgment. This is shown to be an effective technique in improving LLM response quality (Weng et al., 2023; Fung et al., 2023). We employ this technique to refine descriptions, HiL mappings, and symbolic grounding of conversations.\nMulti-Agent Verification We further improve our refinement process via a principled and highly interpretable multi-agent verification framework. AgentEval (Arabzadeh et al., 2024) is a multi-agent framework for evaluating task utility. It takes the task description and 1-2 examples of successful and unsuccessful task runs as input. It generates a categorical rubric of criteria that helps determine the success/failure of future task runs.\nThe framework uses critic, quantifier, and verifier agents. The critic agent generates several criteria to evaluate the examples, the quantifier agent rates the examples on the criteria, and the verifier agent checks the robustness of each generated criterion. We add an evaluator agent on top of the criteria judgments to decide whether a data sample should be retained or not. We use instances of GPT-4o-mini for each agent in the framework. We provide the criteria generated for each evaluation workflow in appendix G."}, {"title": "5 Qualitative Evaluation", "content": "Our cultural context dataset is prone to reliability issues from many sources, such as LLM hallucinations and scaling errors in the HiL process. Hence, we employ automated verification to rectify these errors. Now, we evaluate the quality of the obtained data and the refinement techniques via human evaluation. This allows us to quantify the reliability of the cultural context dataset as a resource. To"}, {"title": "6 Downstream Task Evaluation", "content": "Our experiments aim to evaluate the usefulness of cultural information and graph-based schema. We use empirical performance on conversational understanding tasks as a proxy for usefulness. We focus on 2 classes of models: no-context models and cultural context models. Cultural context models are further divided into 2 types: models that consume cultural context as text and as a graph. We use 3 models for our experiments: RoBERTa (Cui et al., 2021), LLama-3.1 (Dubey et al., 2024), and our ConvGraph model. We design a graph model that uses PLM as a node encoder and leverages the schema structure in Fig.1(a). We briefly discuss the datasets and tasks and then models used.\nDatasets: We perform experiments on 3 tasks across 3 existing datasets. We conduct experiments with MPDD, CPED, and LDC CCU Chinese datasets (\u00a73). We use emotion detection (all 3 datasets), sentiment detection (CPED), and dialogue act identification (LDC CCU, CPED) tasks to benchmark the models.\nTasks: For emotion detection, MPDD is annotated using 7 emotion labels, CPED with 13, and LDC CCU ZH with 9 labels. Dialogue act identification in CPED is a 19 class task, and the LDC CCU Chinese dataset uses 10 labels. CPED sentiment is a 3-way annotation. We use the original train/validation/test splits for CPED and MPDD datasets. For the LDC CCU Chinese dataset, we use the LDC2022E18 release as the train set, the LDC2023E01 release as the validation set, and the LDC2023E20 release as the test set. In this dataset, neutral label for emotion detection and other for dialogue act identification are over-represented. We also find that several samples from these classes are missed annotations. Hence, to avoid skewing the models, we down-sample these classes in all data splits to 1% of the actual labels. This makes these classes roughly the same size as the other classes."}, {"title": "6.1 Models", "content": "We experiment with 3 models: RoBERTa, LLama-3.1-8B-Instruct, & our ConvGraph model.\nROBERTa: We fine-tune the ROBERTa-Chinese-WWM-base (Cui et al., 2021) model on each task as sequence classification. We provide all the previous turns in the conversation and the current turn as inputs and predict class labels for respective tasks. We use cross-entropy loss to train the model. We use loss weighting to deal with class imbalance.\nLlaMa-3.1-8B-Instruct: We perform QLORA (Dettmers et al., 2024) fine-tuning of state-of-the-art LLM, Llama-3.1-8B-Instruct model using the same inputs as RoBERTa-Chinese-WWM model for the no-context experiments. Then, for cultural context experiments, we augment both factual component information and cultural information as text.\nConvGraph Model: For the no-context model, we create a graph from only conversational turns. The graph consists of one node with the conversation text. This node is connected to several child nodes with one turn each. We perform tasks as node classification. We use ROBERTa-Chinese-WWM-base to encode the dialogue and turn nodes. We use DGL (Wang et al., 2019) library to implement the graph model. We use two GraphSAGE (Hamilton et al., 2018) layers on top of the PLM encoders and then pass the node embedding to a final GraphSAGE layer for final task classification.\nFor the ConvGraph + cultural context model, we use the graph structure presented in Fig. 1(a). To encode cultural context nodes, which are in English, we use RoBERTa-base (Liu, 2019) as node encoder. We represent all edges as bi-directional edges. In this model, the contextual nodes such as norm concepts, relationships, settings, etc, are shared across conversations. This makes the entire data split (train/valid/test) into a single graph. We use randomly sample a neighborhood of 10 for each node during training and inference to make the computation tractable."}, {"title": "7 Results", "content": "We present the results on the test sets of each task of our experiments in Tab.4. We observe that the cultural context models outperform no-context models significantly in all the tasks. We report the"}, {"title": "8 Conclusion", "content": "We propose a novel cultural context schema grounding pipeline. We introduce Norm Concepts which abstract over cultural beliefs. Using the pipeline and LLMs, we create high-quality cultural context data and perform human evaluation. Finally, we show that the dataset improves conversational understanding empirically."}, {"title": "Limitations", "content": "We present a novel framework built upon LLMs and a human-in-the-loop approach. Both these approaches are prone to bias due to data and human components. Our evaluation is qualitative and hence relies on heuristic metrics. Our method requires expensive data collection and annotation protocol. We perform annotations for only one language. Cultural norm discovery using LLMs is limited due to the lack of depth on knowledge on some cultures for LLMs. Western bias due to training data might propagate into the dataset. This is a pioneering attempt to build large-scale datasets and hence requires careful usage of the data and technology. Our verification strategy also constitutes LLMs and hence that data still contains close to 10% noisy data event after validation."}, {"title": "Ethics Statement", "content": "Cultural norm discovery using LLMs is limited due to the lack of depth on knowledge on some cultures for LLMs. Western bias due to training data might propagate into the dataset. This is a pioneering attempt to build large-scale datasets and hence requires careful usage of the data and technology. Human annotation using cultural experts is a high-variance process as the experience of culture varies from region to region and person to person. We try to capture a high-level idea of culture in our work. It is possible that the data propagates biases in the society and hence requires careful usage. Our work is aimed to be a research artifact to help foster better models for cross-cultural interaction. This is by no means an end product to be used in large-scale applications."}]}