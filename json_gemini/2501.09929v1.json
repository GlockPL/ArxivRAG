{"title": "STEERING LARGE LANGUAGE MODELS WITH FEATURE GUIDED\nACTIVATION ADDITIONS", "authors": ["Wesley Teng", "Samuel Soo", "C. Balaganesh"], "abstract": "Effective and reliable control over large language model (LLM) behavior is a significant challenge.\nWhile activation steering methods, which add steering vectors to a model's hidden states, are\na promising approach, existing techniques often lack precision and interpretability in how they\ninfluence model outputs. We introduce Feature Guided Activation Additions (FGAA), a novel\nactivation steering method that leverages insights from Contrastive Activation Addition (CAA)\nand Sparse Autoencoder-Targeted Steering (SAE-TS). By operating in the latent space of a Sparse\nAutoencoder (SAE) and employing optimization techniques to select desired SAE features, FGAA\nconstructs precise steering vectors that provide better steering effects while maintaining coherence of\nsteered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B models across\nvarious steering tasks demonstrate that FGAA outperforms existing steering methods of CAA, SAE\ndecoder steering, and SAE-TS. Our results also highlight important trade-offs between steering scale\nand general model capabilities that are consistent across all tested steering methods.", "sections": [{"title": "1 Introduction", "content": "Concerns are growing about effective, and reliable control of the behaviour of Large Language Models (LLMs) [1], and\nthey are being increasingly recognized. Conventional methods such as prompting [2] and fine-tuning [3] provide a bit of\ncontrol, yet they unfortunately have many important limitations that users must consider. Prompting is often weak and\nopen to manipulation,f but fine-tuning needs a lot of computing power and well-organized data. A promising alternative\nis offered by activation steering, providing a stronger and more efficient approach than prompting and fine-tuning. This\nmethod adds steering vectors to the model's hidden states and this influences its behavior during the forward pass.\nHowever, existing activation steering methods often miss precision, and predictability, leading to unintended model\nchanges and poor output quality.\nRecent work on SAE-Targeted Steering (SAE-TS) [4] demonstrated the value of using Sparse Autoencoders (SAEs) to\nextract targetable features during steering. Building on this and Contrastive Activation Addition (CAA) [5], we present\nFeature Guided Activation Additions (FGAA).\nWe evaluate FGAA against multiple baselines, including traditional activation steering, SAE decoder steering, and SAE-\nTS, across various steering tasks on both Gemma-2-2B and Gemma-2-9B models [6]. Our experiments demonstrate\nthat FGAA achieves superior performance in both steering effectiveness and output coherence, particularly in complex\nsteering tasks where maintaining text coherence has traditionally been challenging.\nThis work contributes to the field of controlled text generation in several ways:\n1. We develop a novel method FGAA for constructing steering vectors, harnessing benefits from SAE insights, as\n   well as CAA and SAE-TS methods."}, {"title": "2 Related Work", "content": "Mechanistic Interpretability and SAEs Bereska and Gavves [7] outlined the central hypothesis of mechanistic\ninterpretability: models learn human-comprehensible algorithms and can be understood, despite having no incentive\nto make these algorithms legible to humans during loss minimization. A key challenge in this field was identified\nby Scherlis et al. [8], who found that individual neurons often encode multiple distinct features (polysemanticity),\nmaking direct analysis of neuron behavior difficult. This is caused by superposition, the phenomenon of models\nrepresenting more features than they have dimensions [9]. Sparse Autoencoders (SAEs) emerged as a solution to\nthis challenge, with Cunningham et al. [10] demonstrating that SAEs could extract interpretable features from these\nsuperposed representations in transformer models. Bricken et al. [11] further showed how these extracted features\ncould be manipulated during inference to affect model behavior. Our work uses SAEs to extract interpretable features\nfrom different inputs, to construct a set of desired SAE features to steer for.\nLinear Representation Hypothesis Park et al. [12] introduced the Linear Representation Hypothesis, showing\nthat neural networks encode high-level concepts linearly in their representation spaces. Several studies support this\nhypothesis: the extraction of linear features using SAEs [11], the effectiveness of linear probes in detecting features in\nthe residual stream [13], and the results from activation steering methods. We leverage this linearity assumption in both\nour feature selection process and its use of linear effect approximators to optimize steering vectors.\nActivation Steering Turner et al. [14] introduced activation steering (or activation engineering) to influence LLM\nbehavior by modifying model activations during inference. Building on this work, Panickssery [5] introduced CAA,\nwhich computes steering vectors by averaging the difference in residual stream activations between sets of positive and\nnegative examples of a particular behavior. Chalnev et al. [4] developed linear effect approximators, a linear function\nthat predicts how steering vectors affect SAE features, allowing for targeted steering vector construction with reduced\nside effects. In our work, we apply the effect approximator framework to optimize CAA-derived steering vectors which\nare represented as SAE features."}, {"title": "3 Feature Guided Activation Additions", "content": "FGAA enhances CAA by operating directly in the SAE's latent space and employing optimization techniques to create\nmore effective and coherent steering vectors. Our method consists of several key components that work together to\nidentify and utilize the most relevant activation patterns while minimizing unwanted effects. For the rest of this paper,\nin the interest of clarity, positive and negative examples of a particular behavior used in CAA are termed as desired and\nundesired examples, while features refer to SAE latents."}, {"title": "3.1 SAE-Based Contrastive Analysis", "content": "Unlike traditional CAA which operates on raw activations, FGAA computes contrastive differences in the SAE\nactivation space. Given sets of positive and negative examples $X^+$ and $X^\u2212$ which exhibit desired and undesired\nbehaviors respectively, and an SAE with encoder $f$, we compute the difference vector as:\n$V_{diff} = \\frac{1}{|X^+|} \\sum_{x \\in X^+} f(h_l(x)) - \\frac{1}{|X^-|} \\sum_{x \\in X^-} f(x)$\nwhere $h_l(x)$ represents the hidden state activations at layer $l$ for input $x$, and $f(h_l(x))$ represents the mean SAE feature\nactivations across all tokens. This produces a vector in the SAE's latent space that captures the key differences between\ndesired and undesired behavior in terms of interpretable features."}, {"title": "3.2 Feature Filtering", "content": "We apply three critical filtering steps to transform the difference vector into the target vector:\n1. Density Filtering: We zero out features with activation density above a threshold 0:\n$V_{filtered}(i) = \\begin{cases}\n0 & \\text{if } p(i) > \\theta \\\\\nV_{filtered}(i) & \\text{otherwise}\n\\end{cases}$\nwhere $p(i)$ is the activation density of feature i and 0 = 0.01 in our implementation.\n2. BOS Feature Removal: We zero out features that activate most strongly on the Beginning Of Sequence (BOS)\ntoken:\n$V_{filtered}(i) = \\begin{cases}\n0 & \\text{if isBOS(i)} \\\\\nV_{filtered}(i) & \\text{otherwise}\n\\end{cases}$\nwhere isBOS(i) identifies features that have the highest activations at the BOS token. For Gemma family\nmodels, they are represented as <bos>.\n3. Top-k Selection: Based on feature activation values, we retain the n\u2081 most positively activating and n\u2082 most\nnegatively activating features:\n$V_{target} = concat(top_{n_1}(V_{filtered}), top_{n_2}(-V_{filtered})), n_1,n_2 \\in Z^+$\nThe three filtering steps in FGAA were developed through empirical observation of feature activation patterns across\nmultiple steering tasks. Density filtering addresses a common issue where high-density features (those that activate\nfrequently across many inputs) tend to dominate the difference vector despite their limited task specificity. By filtering\nout features with activation density above 0 = 0.01, we ensure the steering vector focuses on more specialized features\nthat better characterize the target behavior. Similarly, BOS feature removal was implemented after observing a family\nof features that exclusively had the strongest activation on the BOS token (Appendix H), which often introduced\nartifacts in generation while contributing little to the desired steering effect. These features typically encode general\nlinguistic patterns rather than task-specific behaviors. Finally, the selection of top $n_1$ positive and $n_2$ negative features\nhelps eliminate noise from weakly activated features, focusing the steering vector on the most significant behavioral\nindicators."}, {"title": "3.3 Linear Approximator Optimization", "content": "We employ effect approximators [4] to solve for the optimal steering vector to produce the desired feature effects in\n$V_{target}$. The linear effect approximator can be represnted as a function $\\hat{y} = xM + b$, where x is the $d_{model}$-dimensional\nsteering vector, M is a $d_{model} \\times d_{sae}$ matrix, b has dimension $d_{sae}$, and $\\hat{y}$ is the predicted steering effects vector of\ndimension $d_{sae}$.\nThe approximator consists of a weight matrix W and bias vector b. Given our desired feature vector $v_{target}$, we compute\nthe optimized steering vector $v_{opt}$:\n$v_{opt} = \\frac{Wv_{target}}{||Wv_{target}||} - \\frac{Wb}{||Wb||}$\nFor our implementation, $V_{target}$ is L1 normalised for this calculation for consistent scaling of the relevant features, which\nhelps maintain stable steering effects regardless of the magnitude of the original target vector."}, {"title": "3.4 Final Steering Application", "content": "The final FGAA steering vector is applied to the model's hidden state at layer l during generation:\n$h_l = h_l + \\alpha v_{opt}$\nwhere \u03b1 is a scaling factor which we refer to as steering scale."}, {"title": "4 Evaluations and Discussion", "content": ""}, {"title": "4.1 Effectiveness of FGAA for Steering", "content": "For our evaluations, FGAA is implemented using a pre-trained Gemma Scope [15] SAE with 16,384 features for the\nresidual stream at layer 12 for Gemma-2-2B and Gemma-2-9B models. We selected these two models due to both\ncomputational constraints and the availability of open pre-trained SAE weights. Similarly, we apply steering to the\nresidual stream at layer 12 and utilize pretrained effect approximators from [4] for both Gemma models. We focus\non layer 12 in our evaluation, as collecting training data for effect approximators is time-intensive and must be done\nseparately for each layer. Additionally, only layer 12 approximators for the models above have been made publicly\navailable.\nWe evaluate FGAA against existing steering methods using the evaluation framework from [4], employing gpt-40-mini\nto assess both behavioral alignment and coherence on a 1-10 scale, which we then rescale to the range [0,1]. Let B\nrepresent the behavioral score which measures steering target achievement, and C represent coherence which evaluates\nsemantic correctness post-steering (exact criterion in Appendix C). We define the Behavioral-Coherence Score (BCS)\nas:\n$BCS = B \\times C, B, C\\in [0,1]$\nWe generate FGAA steering vectors using optimal n\u2081 and n\u2082 values found from a hyperparameter sweep in Appendix A1.\nEach steering vector is applied to the model by adding the steering vector to the residual stream at every token position,\nsampling 100 steered text completions, each 33 tokens long beginning with the open-ended prompt \"<bos>I think\".\nFor fair evaluation, all steering vectors are L2 normalised before applied. The following are implementation details for\nthe other steering methods.\nContrastive Activation Addition (CAA), defined as the mean difference of model activations between a set of desired\nand undesired examples, averaged over token positions and examples.\nSAE feature steering, using the decoder vector of a single relevant SAE feature.\nSAE targeted steering (SAE-TS), setting the same relevant SAE feature used for SAE feature steering as the only\nactive feature in vtarget."}, {"title": "4.2 Effects of Steering on General Model Capabilities", "content": "We evaluate the impact of steering methods on model capabilities through perplexity testing on the OpenWebText\n[18] dataset and performance on MMLU (Massive Multitask Language Understanding) [19] and MMLU-Pro [20]\nbenchmarks. MMLU is a comprehensive evaluation benchmark that tests AI models using multiple choice questions\nspanning 57 different subjects, from STEM fields to humanities and social sciences. While the original MMLU primarily\nfocuses on testing factual knowledge, MMLU-Pro builds upon this foundation by introducing more complex questions\nthat require deeper reasoning abilities and increases the number of possible answers from 4 to 10 per question.\nFor perplexity evaluation, we use a sample of 100 records from OpenWebText, evaluating using steering vectors derived\nfrom the 9 steering tasks in Table 1. For MMLU and MMLU-Pro evaluations, we use fixed subsets of questions to\nensure consistent comparison across steering methods: the first 5 questions from each subject category in MMLU, and\nthe first 10 questions from each category in MMLU-Pro. Due to computational constraints, we limit these benchmark\nevaluations to steering vectors from 3 representative tasks in Table 1: Anger, Christian Evangelist, and Conspiracy. All\nexperiments use Gemma-2-2B with steering vectors applied at layer 12 of the residual stream."}, {"title": "5 Limitations", "content": "Our current approach relies heavily on the quality of feature extraction by the underlying SAE, and performance could\npotentially improve with advances in SAE architectures that achieve more precise monosemantic feature separation.\nThe method's effectiveness may be limited by the SAE's ability to capture complex and atomic concepts in its latent\nspace, particularly for abstract or nuanced steering tasks.\nThe optimal selection of n\u2081 and n2 parameters appears to be task-dependent, making it challenging to establish universal\nguidelines for parameter selection. Also, developing metrics to evaluate the effectiveness of our feature filtering methods\nproves to be a challenging task due to the qualitative nature of interpreting features."}, {"title": "6 Future Work", "content": "Future work could proceed along several promising directions. First, investigating how SAE width and quality of SAE\nfeatures affects steering performance with FGAA could help establish optimal feature space dimensionality for general\nsteering tasks. In addition, exploring techniques to minimize capability degradation at higher steering scales while\nmaintaining steering effectiveness would address one of the key challenges identified in our experiments.\nWe believe the most promising direction to pursue would be applying FGAA to existing works in the activation steering\nspace, to see if FGAA performance improvements carry over to safety tasks such as modulating sycophancy and\nimproving honesty in RLHF models [22, 23], reducing social biases [24] and preventing jailbreaks [25]."}, {"title": "7 Conclusion", "content": "This work introduced FGAA, a novel approach that combines CAA with insights from SAE representations to\nimprove steering effectiveness in language models. Our evaluations demonstrated that FGAA achieves superior\nperformance compared to existing steering methods across multiple tasks, particularly for the Gemma-2-2B model\nwhere it outperformed baselines in 8 out of 9 steering tasks. The method's success highlights the value of operating\ndirectly in interpretable feature spaces while maintaining precision through systematic feature filtering and optimization.\nOur analysis revealed important insights about activation steering in general: effectiveness varies non-linearly with\nmodel size, performance degrades notably above certain steering scales, and there exists a fundamental tradeoff between\nsteering strength and preservation of model capabilities. The scaling behavior differences between Gemma-2 2B and 9B\nmodels indicate that steering effectiveness may be architecture-dependent and suggests the need for adaptive approaches\nas models grow in size.\nThe development of FGAA represents a significant step forward in controlled text generation, offering both theoretical\ninsights into activation patterns in LLMs and practical advances in steering methodology. While challenges remain\nin areas such as SAE quality optimization and parameter selection, the method's demonstrated effectiveness across\nmultiple tasks and architectures provides a strong foundation for future research. Particularly promising directions\ninclude investigating SAE width effects, developing techniques to minimize capability degradation at higher scales,\nand exploring applications to safety-critical steering tasks. These advances in precise model control have significant\nimplications for the development of more reliable and controllable language models, contributing to the broader goal of\ncreating AI systems that can be effectively guided while maintaining their core capabilities."}]}