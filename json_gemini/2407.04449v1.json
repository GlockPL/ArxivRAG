{"title": "Multi-modal Masked Siamese Network Improves Chest X-Ray Representation Learning", "authors": ["Saeed Shurrab", "Alejandro Guerra-Manzanares", "Farah E. Shamout"], "abstract": "Self-supervised learning methods for medical images primarily rely on the imaging modal-\nity during pretraining. While such approaches deliver promising results, they do not\nleverage associated patient or scan information collected within Electronic Health Records\n(EHR). Here, we propose to incorporate EHR data during self-supervised pretraining with\na Masked Siamese Network (MSN) to enhance the quality of chest X-ray representations.\nWe investigate three types of EHR data, including demographic, scan metadata, and inpa-\ntient stay information. We evaluate our approach on three publicly available chest X-ray\ndatasets, MIMIC-CXR, CheXpert, and NIH-14, using two vision transformer (ViT) back-\nbones, specifically ViT-Tiny and ViT-Small. In assessing the quality of the representations\nvia linear evaluation, our proposed method demonstrates significant improvement com-\npared to vanilla MSN and state-of-the-art self-supervised learning baselines. Our work\nhighlights the potential of EHR-enhanced self-supervised pre-training for medical imaging.", "sections": [{"title": "1. Introduction", "content": "Supervised training of deep neural networks requires large amounts of quality annotated\ndata (LeCun et al., 2015). This is not always straightforward in applications involving clin-\nical tasks, due to the time, cost, effort and expertise required to collect labeled data (Taleb\net al., 2020). Self-supervised learning has recently demonstrated great success in leveraging\nunlabeled data, such as in natural language processing (Lan et al., 2019) and computer vi-\nsion (Jing and Tian, 2020). Such frameworks aim to learn useful underlying representations\nduring pretraining, without any labels, which are then used in downstream prediction tasks\nvia supervised linear evaluation."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Overview of Self-Supervised Learning", "content": "Self-supervised learning methods learn task-agnostic feature representations using hand-\ncrafted pretext tasks or joint embedding architectures (Kinakh et al., 2021; Bardes et al.,\n2021). Hand-crafted pretext tasks rely on the use of pseudo-labels generated from unlabeled\ndata. Examples of such tasks include rotation prediction (Gidaris et al., 2018), jigsaw\npuzzle solving (Noroozi and Favaro, 2016), colorization (Zhang et al., 2016), and in-painting\n(Pathak et al., 2016). Joint embedding methods utilize siamese networks (Bromley et al.,\n1993) to learn useful representations by discriminating between different views of samples\nbased on a specific objective function (Bardes et al., 2021), without the need for human\nannotation or pseudo-labels.\nJoint embedding methods can be further categorized into contrastive and non-contrastive\nmethods, where the latter encompasses clustering, distillation, and information maximiza-\ntion methods (Bardes et al., 2021). Contrastive methods learn representations by maximiz-\ning the agreement between positive pairs and minimizing the agreement between negative\npairs (Van den Oord et al., 2018). Some prominent examples include SimCLR (Chen et al.,\n2020), contrastive predictive coding (Van den Oord et al., 2018), and MoCo (He et al.,\n2020). Non-contrastive methods focus on optimizing different forms of similarity metrics\nacross the learned embeddings. Examples include BYOL (Grill et al., 2020), SimSiam (Chen\nand He, 2021), and VICReg (Bardes et al., 2021). While most existing work considers con-\nvolutional networks as backbones for input encoders, recent approaches explore the role\nof vision transformers (ViT) (Dosovitskiy et al., 2020) for self-supervision, such as DINO\n(Caron et al., 2021) and MSN (Assran et al., 2022). MSN is a state-of-the-art self-supervised\nlearning architecture that operates on the principle of mask-denoising, without reconstruc-\ntion, as well as transformation invariance with transformers. MSN has limited applications\nin healthcare-related tasks, and is promising considering its computational scalability."}, {"title": "2.2. Self-Supervised Learning in Healthcare", "content": "Self-supervised learning methods have shown great promise in learning representations of\ndifferent types of medical images, such as computed tomography and magnetic resonance\nimaging (Jamaludin et al., 2017; Zhuang et al., 2019; Taleb et al., 2020), optical coherence\ntomography and fundus photography (Holmberg et al., 2020; Hervella et al., 2020; Li et al.,\n2021), and endoscopy images (Ross et al., 2018). Several studies investigated self-supervised\nlearning for applications involving CXR images. For example, Sowrirajan et al. (2021),\nChen et al. (2021), and Sriram et al. (2021) utilized MoCo (He et al., 2020) as a pretraining\nstrategy for chest disease diagnosis and prognosis tasks. Azizi et al. (2021) showed that\ninitializing SimCLR (Chen et al., 2020) during pretraining with ImageNet weights improves\ndownstream performance in CXR classification. Van der Sluijs et al. (2024) explored the\nimpact of various image augmentation techniques on siamese representation learning for\nCXR.\nSome studies investigated the use of other sources of information in designing the self-\nsupervised learning framework for CXR, mostly focusing on vision-language pretraining.\nVu et al. (2021) proposed MedAug, which considers patient metadata for generating pos-"}, {"title": "3. Methods", "content": ""}, {"title": "3.1. Problem Formulation", "content": "Consider $x_{c\u00e6r} \\in \\mathbb{R}^{h\\times w}$, where $h$ and $w$ represent the image height and width, to be a CXR\nimage collected from patient during their hospital stay, the goal is to predict a set of disease\nlabels $Y_{cxr}$. We assume that each $X_{car}$ is associated with $X_{ehr} \\in \\mathbb{R}^{n}$, a vector of static\nfeatures derived from the patient's EHR data, where $n$ is the number of variables. We use\nboth modalities to learn the CXR representation within our pretraining framework. An\noverview of the multi-modal MSN architecture for pretraining is shown in Figure 1. The\nnetwork consists of two components: (i) the visual encoders for CXR (ii) a multi-modal\nbranch that incorporates the EHR data, as described in the following sections."}, {"title": "3.2. Masked Siamese Network", "content": "We adopt the MSN (Assran et al., 2022) as the base model for our proposed framework due\nto its computational scalability and the need for pretraining transformers efficiently. For a\ngiven unlabeled input image, the goal is to align the anchor and target views, denoted by\n$x_{anchor}$ and $x_{target}$, respectively. For each image, a random transformation function $T(.)$ is\nused to generate a set of M anchor views and a single target view. The transformations\ninclude image resizing, random resized cropping, and random horizontal flipping with a\nprobability of 0.5, following Sowrirajan et al. (2021). However, we excluded color jittering\nand Gaussian blurring as the former does not apply to grayscale images, while the latter may\ndistort disease-related information (Sowrirajan et al., 2021). Since the views are patchified\nto serve as input to a ViT, the anchor views are further masked via patch dropping (either\nrandom or focal masking as shown in Figure 2), while leaving $x_{target}$ unmasked.\nTwo encoders, $f_{anchor}$ and $f_{target}$, parameterized with a ViT (Dosovitskiy et al., 2020),\nare trained to generate the image embeddings:\n$v_{cxr} = f_{anchor}(x_{anchor}) \\& v_{cxr+} = f_{target}(x_{target}).$ \n(1)\nThe target embedding $v_{cxr+}$ is further processed by a projection head $h+$ to obtain $z+$,\nwhile $v_{car}$ is used in the multi-modal mapping as described in the next section. MSN does\nnot compute the loss directly on the generated embeddings based on a certain similarity\nmetric. Instead, it utilizes a set of prototypes and seeks to map the projected embeddings\nof a given sample into the same learned prototype, where the mappings are used to compute\nthe loss. Further background information is provided in Appendix A."}, {"title": "3.3. Multi-modal Pretraining", "content": "Instead of solely relying on CXR, our proposed framework encourages MSN to leverage\nadditional information extracted from the patient's EHR. In particular, we introduce three\nadditional modules to the standard MSN architecture. First, we encode the static EHR\ndata with $f_{ehr}$ to learn a representation vector, such that:\n$v_{ehr} = f_{ehr}(X_{ehr}).$ \n(2)\nWe then concatenate (+) the EHR and CXR latent representations, $v_{ehr}$ and $v_{c\u00e6r}$. At\nthis stage, there exists dimensionality mismatch between the concatenated representations"}, {"title": "3.4. Downstream Classification", "content": "After pretraining the encoders $f_{target}$, $f_{anchor}$, and $f_{ehr}$, we use $f_{target}$ as a feature extractor.\nWe then include a classification model $f_{c}$ to predict the image labels:\n$Y_{cxr} = f_{c}(V_{cxr+})$ \n(4)\nThe main assumption behind our proposed method is that introducing additional data\nrelated to the patient or the CXR scan during pretraining would provide the model with\nvaluable contextual information. This information would contribute to enhancing the qual-\nity of the learned representations for downstream classification."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset", "content": "We conducted our experiments using the MIMIC-CXR dataset, which is a publicly available\ndataset of CXR images (Johnson et al., 2019). MIMIC-CXR includes 377,110 chest radio-\ngraphs gathered from 227,835 studies between 2011 and 2016. Each image is associated\nwith a set of ground-truth labels $y_{c\u00e6r} \\in \\mathbb{R}^{14}$ that indicates the presence of any of 14 medi-\ncal conditions extracted from corresponding radiology reports. The medical conditions are\nAtelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Cardiomediastinum, Fracture,\nLung Lesion, Lung Opacity, Pleural Effusion, Pneumonia, Pneumothorax, Pleural Other,\nSupport Devices, and No Finding. After pre-processing, the dataset consisted of 376,201\nindividual CXR images related to 65,152 patients and 82,506 stays.\nWe used MIMIC-IV (Johnson et al., 2023), an EHR database associated with MIMIC-\nCXR, to extract EHR variables related to the CXR images. Table 1 provides a summary\nof the extracted EHR features, including data types, possible values, and dimensionality.\nAfter a thorough investigation of the MIMIC-IV database, we identified three sets of relevant\nfeatures: patient demographics ($x_{D}$), scan metadata ($x_{SM}$), and inpatient stay information\n($x_{IS}$). For patient demographics, we retrieved patient age ($x_{age}$) for each specific admission\nand normalized it using MinMax normalization, as well as patient sex ($x_{sex}$) encoded as a\none-hot vector. For scan metadata, we extracted the scan view ($x_{view}$) and patient position\n($X_{pos}$) and encoded them as one-hot vectors. The possible values for view are antero-\nposterior (AP), lateral (L), postero-anterior (PA), or left-lateral (LL), while for patient\nposition the possible values are erect or recumbent. For inpatient information, we defined\n$X_{icu}$ and $X_{mort}$ as binary features indicating whether the patient required intensive care\ntreatment or experienced in-hospital mortality, respectively. We provide a summary of the\nEHR features distributions in Appendix B.\nWe used MIMIC-CXR and MIMIC-EHR for pretraining and downstream classification.\nWe followed the work of Hayat et al. (2022) to obtain the training, validation, and test\nsplits. The data splits are provided in Table 2."}, {"title": "4.2. External Validation", "content": "We also performed external validation of our pretrained encoders in downstream classifica-\ntion using the ChexPert (Irvin et al., 2019) and National Institutes of Health Chest X-Ray\nDataset (NIH-14) (Wang et al., 2017) datasets. ChexPert is a publicly available medical\nimaging dataset composed of 224,316 chest radiographs of 65,240 patients, collected between\n2002-2017, including inpatient and outpatient centers. It includes images and associated\nradiology reports, which were used to generate labels indicating the presence of 14 dis-\neases. We evaluated the pretrained encoder and classifier obtained with MIMIC-CXR on\nthe ChexPert test set since they have the same labels.\nNIH-14 is a publicly available medical imaging dataset composed of 112,120 frontal-\nview CXR images of 30,805 patients, collected between 1992-2015. Each image is assigned\n14 common disease labels. We follow the same split reported in Irvin et al. (2019) and\nperform linear evaluation using the NIH-14 training set, since it has different labels from\nMIMIC-CXR. The data splits are shown in Table 2."}, {"title": "4.3. Evaluation Protocol", "content": "To assess the quality of the learned representations using our proposed pretraining frame-\nwork, we conducted linear evaluation as the main evaluation protocol following previous\nwork in self-supervised learning (Bardes et al., 2021; Garrido et al., 2023; Huang et al.,\n2022). We froze the encoder $f_{target}$ after pretraining, and trained the classification head $f_{c}$,\nwhich is randomly initialized, with embeddings of the whole training set.\nWe report performance using the Area Under the Receiver Operating characteristic\nCurve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC) across all ex-\nperiments. We report the 95% Confidence Intervals (CI) using the bootstrapping method\n(Puth et al., 2015). We also performed statistical significance testing by comparing our\nproposed approach with the best performing baseline."}, {"title": "4.4. Uni-modal Self-supervised Pre-training Baselines", "content": "We compared the results of our proposed method with self-supervised and supervised base-\nlines that are transformer-based for a fair comparison:\n1. Vanilla MSN (Assran et al., 2022): MSN is a self-supervised learning model that\nleverages the idea of mask-denoising while avoiding pixel and token-level reconstruc-\ntion. We trained the vanilla version following the architecture in the original work.\n2. DINO (Caron et al., 2021): DINO is a transformer-based architecture that learns rep-\nresentation via knowledge distillation from a teacher backbone to a student backbone\nwithout labels.\n3. Masked AutoEncoder (MAE) (He et al., 2022): MAE is a transformer-based\nencoder-decoder architecture that learns representation by reconstructing the missing\npixels within an input image.\n4. Supervised: We train ViT-S and ViT-T in an end-to-end fashion with random and\nImageNet (Deng et al., 2009) weight initializations.\nIn our experiments, we used ViT-Small (ViT-S) as the backbone for our proposed\nmethod and the state-of-the-art self-supervised baselines. We also conducted experiments\nusing the ViT-Tiny (ViT-T) backbone. These are smaller versions than the ones defined in\nDosovitskiy et al. (2020) and we chose them due to significantly faster training time with\ncomparable performance compared to the original implementation with ViT-Base (ViT-B).\nAll implementation details and model training settings are provided in Appendix A."}, {"title": "5. Results", "content": "Here, we present the performance results of our proposed method and all self-supervised\nbaselines using the ViT-S. The performance results for ViT-T and all other baselines, in-\ncluding self-supervised and supervised learning methods, are reported in Appendix C."}, {"title": "5.1. Linear Evaluation Results", "content": "Table 3 summarizes the performance results of the linear evaluation experiments using ViT-\nS. First, we note that all of the models that incorporate EHR during pretraining achieve a\nbetter performance compared to the best performing baseline, vanilla MSN. The improve-\nment is at least 1% in terms of AUROC and AUPRC, with a maximum improvement of 2%\nwhen incorporating demographic information, 0.751 AUROC with $I_{sex}$ or $I_{D}$, compared\nto 0.731 AUROC with MSN. Additionally, in most experiments where we pretrain with a\nsingle EHR feature achieve improvements greater than 1.5%, while pretraining with groups\nof features yields slightly lower performance improvements.\nMoreover, our proposed strategies demonstrate a significant improvement compared to\nstate-of-the-art self-supervised learning baselines, specifically MAE and DINO, as well as\nthe supervised ImageNet-initialized baseline, reaching an improvement of 10% compared to\nthe worst performing model. In summary, the results of linear evaluation demonstrate that\nour approach, which incorporates EHR data during pretraining, enhances the quality of the\nlearned representations and downstream performance in a multi-label disease classification\ntask, surpassing both the vanilla MSN and other baselines. In Appendix C, we provide\nfurther results with ViT-T as the model backbone. In general, we observe similar patterns,\ndemonstrating consistent behaviour with different backbone architectures.\nFor the sake of completeness we also report in Appendix C the results of a secondary\nevaluation protocol by fine-tuning under low data regimes including 1%, 5%, and 10% of"}, {"title": "5.2. External Validation Results", "content": "Table 4 presents the external validation results for linear evaluation of our proposed methods\nusing the ViT-S backbone on the ChexPert Irvin et al. (2019) and NIH-14 Wang et al. (2017)\ndatasets. The results indicate that our multi-modal pretraining enhances the robustness of\nself-supervised models, resulting in higher validation scores and improved performance com-\npared to vanilla MSN in most scenarios. In the best scenario, the performance gain reaches\n3-4% in both evaluation datasets. However, we observe better performance improvements\nin the NIH-14 dataset, since its training set was used to train the linear evaluation classifier,\ncompared to off-the-shelf evaluation with CheXpert. The results for the VIT-T backbone\nand fine-tuning are reported in Appendix C. We observe similar patterns that are in line\nwith the findings of internal validation with MIMIC-CXR."}, {"title": "5.3. t-SNE Analysis Results", "content": "We applied t-SNE to a subset of samples with a single label in the training set (n=68,000) of\nMIMIC-CXR for the ease of interpretability, considering the challenge of visualizing multi-\nlabel samples in the latent space. Figure 3 visualizes the embeddings obtained with vanilla"}, {"title": "6. Discussion", "content": "In this paper, we propose the incorporation of EHR data during self-supervised pretraining\nfor CXR representation learning, with the aim of improving performance in downstream\nclassification. We present our proposed approach as an enhanced version of an existing self-\nsupervised learning method, MSN (Assran et al., 2022), by including clinical imaging-related\npatient EHR data during pretraining. We also comprehensively evaluated our proposed ap-\nproach on the MIMIC-CXR dataset (internal validation), and two datasets for external\nvalidation, ChexPert and NIH-14. Our results demonstrate that the integration of EHR\nmetadata during pretraining with MSN significantly improves performance in the down-\nstream linear evaluation protocol. We also observe that, in general, the inclusion of a single\nEHR feature yields better quality embeddings compared to pretraining with combinations\nof EHR features, as shown by the t-SNE embeddings in Figure 3. In addition, our proposed\nmethod shows superior performance compared with other state-of-the-art self-supervised\nbaselines, including DINO (Caron et al., 2021) and MAE, (He et al., 2022) as well as fully\nsupervised baselines pretrained with ImageNet (Deng et al., 2009). Our extensive evalua-\ntion highlights that our proposed approach generalizes well to external validation datasets,\nsignificantly outperforming vanilla MSN in various settings for CheXpert and in all cases\nfor the NIH-14 dataset.\nA relevant insight derived from our extensive experimentation using single features and\ncombinations of them is that it is not guaranteed that the addition of more EHR features\nleads to improved performance. We hypothesize that such behavior could be attributed to\nfeature interactions, which is an area of future work.\nLimitations Despite the performance improvements shown by our framework during lin-\near evaluation, our approach has some limitations. First, our approach did not show sig-\nnificant improvements during end-to-end fine-tuning. That is, it performed on-par with\nvanilla MSN, or slightly lower in the worst scenario as shown in the additional results in\nAppendix C. We hypothesize that fine-tuning the target encoder without the EHR data\ndilutes the impact of incorporating it during pretraining. Specifically, the learned repre-\nsentations during the pretraining stage (with EHR data) are updated during fine-tuning\n(without EHR data), which impacts the quality of the representations. We aim to ad-\ndress this limitation in our future work, where the EHR data is incorporated during both\nlinear evaluation and fine tuning, rather than solely during pre-training for a fair compar-\nison. Furthermore, we only tested our proposed methodology with a transformer-based\nself-supervised learning method. In future work, we will explore its applicability to other\nself-supervised learning architectures and generalization as a framework for other medical\nimaging datasets and clinical prediction tasks."}, {"title": "Appendix A. Implementation details", "content": "In this section, we present additional information pertaining to the implementation details."}, {"title": "A.1. Masked Siamese Network", "content": "The MSN network (Assran et al., 2022) is a self-supervised pretraining architecture that\nleverages mask de-noising and transformation invariance. Let B > 1 be a mini-batch of chest\nX-ray images sampled from an unlabeled training set. For each image in the dataset, a set\nof random transformations is applied to obtain M anchor views $x_{i,1}, x_{i,2}...x_{i,M}$ and a single\ntarget view denoted as $x_{i}^{+}$. As the images serve as input to a Vision Transformer (ViT),\nthe anchor and target views are initially patchified, i.e., converted into a set of patches with\ndimensions $p \\times p$ pixels, where $p$ represents the patch size. Before the patches are processed\nby the transformer-based encoders, random and focal masking are applied to the anchor\nviews. Figure 2 illustrates the masking strategies: random masking drops random patches\nat different locations across the input image, while focal masking drops a block of adjacent\npatches.\nThe anchor patches are processed by the encoder $f_{anchor}$, while the target patches are\nprocessed by the encoder $f_{target}$ (see Figure 1). The weights of $f_{target}$ are updated through\nan exponential moving average of $f_{anchor}$. Both encoders are parameterized as ViT encoders.\nThe [CLS] token of each encoder is processed by a three-layer projection head $h$ to obtain\nthe final image embeddings $z_{i,m}$ and $z_{i}^{+} \\in \\mathbb{R}^{k}$ for each encoder.\nThere also exists a set of learnable prototypes $q \\in \\mathbb{R}^{nxK}$, where K is the number of\nprototypes. The network learns to assign the learned embeddings to the prototypes based\non cosine similarity, such that $p_{i,m} := softmax(z_{i,m}^{T}q / \\tau)$ and $p_{i}^{+} := softmax(z_{i}^{+T}q / \\tau^{+})$, where $\\tau$\nand $\\tau^{+}$ are the temperature hyperparameters. As described in Assran et al. (2022), the\noverall loss function is defined using a standard cross-entropy loss $H(p_{i}^{+}, p_{i,m})$ and a mean\nentropy maximization regularizer $H(\\bar{p})$ following Equation 5:\n$\\mathcal{L} = \\frac{1}{BM} \\sum_{i=1}^{B} \\sum_{m=1}^{M} H(p_{i}^{+}, p_{i,m}) - \\lambda H(\\bar{p}),$ \n(5)\nwhere B is the batch size, M is the number of anchor views, and $\\lambda$ is the weight of\nthe mean entropy maximization regularizer. The value of $\\bar{p}$ in the regularization term is\ncalculated as presented in Equation 6:\n$\\bar{p} = \\frac{1}{MB} \\sum_{i=1}^{B} \\sum_{m=1}^{M} p_{i,m}$ \n(6)\nConcerning masking, we use a masking ratio of 0.15 for MSN and 0.6 for MAE. We use\nexponential moving average with a momentum value of 0.996 to update the target encoder\nin MSN and the teacher encoder in DINO, along with their respective projection heads.\nFor focal view generation in MSN, we crop the image to 96 \u00d7 96. We set M = 11 anchor\nviews of an image and generate 1 random mask and 10 focal masks. We also use ImageNet\nDeng et al. (2009) statistics to normalize the input CXR scans during pretraining."}, {"title": "A.2. Model Pretraining", "content": "We follow the same hyperparameter settings as in the original MSN (Assran et al., 2022),\nDINO (Caron et al., 2021), and MAE (He et al., 2022). We use the AdamW optimizer\n(Loshchilov and Hutter, 2017) for MSN and the other self-supervised baselines with a batch\nsize of 64. To select the learning rate, we conduct several experiments using different\ncombinations of learning rate and weight decay values. We empirically found that the best\nlearning rate and weight decay values for MSN are le \u2013 4 and 1e \u2013 3, respectively, while\nthe best values for DINO and MAE are le - 5 and 1e \u2013 4, respectively. For all pertaining\nexperiments, we employ the cosine annealing method as a learning rate scheduler. We\npretrain each model for 100 epochs with early stopping, using a patience of 5 epochs and a\nminimum delta of le - 5 for the training loss.\nTo incorporate EHR data in our proposal, we implement both $f_{ehr}$ and $g$ as linear layers\n(see Figure 1). We fix the output dimension of $f_{ehr}$ as 128 and match the output dimension\nof $g$ to the hidden size D according to the backbone used (see Table A1). Table A1 provides\nan overview of the size and architecture of the Vision Transformer architectures used."}, {"title": "A.3. Downstream Classification", "content": "We use the Adam optimizer (Kingma and Ba, 2014) with a batch size of 64 for all down-\nstream evaluation experiments. For each method evaluation, we perform three experiments\nwith learning rates in [1e \u2013 3,5e \u2013 4,1e \u2013 4] following (Azizi et al., 2021), each with cosine\nannealing learning rate scheduler. In this setup, we apply early stopping with a patience\nof 5 epochs if the validation AUROC does not improve by le - 4. Additionally, we con-\nduct two more experiments with learning rates in [5e \u2013 4, 1e \u2013 4] and use the Reduce on\nPlateau learning rate scheduler without early stopping. We perform this setup once for\nlinear evaluation and replicate it for fine-tuning experiments, resulting in 10 experiments\nfor each model. We select the best-performing model on the validation set for test set\nevaluation. The maximum training epochs for all downstream evaluation tasks are set to\n50 epochs. For augmentations, we apply random horizontal flip, random affine, and center\ncrop during training, and only center crop during inference. We use ImageNet (Deng et al.,\n2009) statistics to normalize the CXR images during both training and inference, following\nthe same settings as during pretraining.\nFor low-data regime fine-tuning, we conduct five experiments on five randomly selected\nsubsets of the training set, corresponding to specified percentages (1%, 5%, and 10%), and\nevaluate the model on the full test set. To run these experiments, we use the same settings"}, {"title": "Appendix B. EHR Features", "content": "Figure B1 provides the frequency distributions for the EHR features considered in our work\nduring the self-supervised pretraining stage."}, {"title": "Appendix C. Additional Results", "content": "In this section, we present additional results from our experimental setting. These results\ncomplement or extend the ones provided in the main text, including additional baselines or\narchitectures (i.e., ViT-T)."}, {"title": "C.1. Linear Evaluation", "content": "Table C1 provides performance results for linear evaluation of self-supervised methods using\nthe ViT-T architecture as backbone model."}, {"title": "C.2. Supervised learning", "content": "Table C2 reports performance results of reference architectures trained using supervised\nlearning for comparison with self-supervised methods."}, {"title": "C.3. Fine-tuning", "content": "Table C3 provides performance results for fine-tuning of self-supervised methods using the\nViT-S architecture as backbone model. Table C4 provides the results obtained using ViT-T\narchitecture as backbone."}, {"title": "C.4. External Validation", "content": "Table C5 reports external validation results on downstream classification for fine tuning of\nViT-S self-supervised models on the ChexPert and NIH-14 datasets.\nTable C6 report external validation results on downstream classification for linear eval-\nuation of ViT-S self-supervised models on the ChexPert and NIH-14 datasets. Table C7\nprovides the results for the fine-tuning of self-supervised models."}]}