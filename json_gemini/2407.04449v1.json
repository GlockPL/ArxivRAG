{"title": "Multi-modal Masked Siamese Network Improves Chest X-Ray Representation Learning", "authors": ["Saeed Shurrab", "Alejandro Guerra-Manzanares", "Farah E. Shamout"], "abstract": "Self-supervised learning methods for medical images primarily rely on the imaging modal-\nity during pretraining. While such approaches deliver promising results, they do not\nleverage associated patient or scan information collected within Electronic Health Records\n(EHR). Here, we propose to incorporate EHR data during self-supervised pretraining with\na Masked Siamese Network (MSN) to enhance the quality of chest X-ray representations.\nWe investigate three types of EHR data, including demographic, scan metadata, and inpa-\ntient stay information. We evaluate our approach on three publicly available chest X-ray\ndatasets, MIMIC-CXR, CheXpert, and NIH-14, using two vision transformer (ViT) back-\nbones, specifically ViT-Tiny and ViT-Small. In assessing the quality of the representations\nvia linear evaluation, our proposed method demonstrates significant improvement com-\npared to vanilla MSN and state-of-the-art self-supervised learning baselines. Our work\nhighlights the potential of EHR-enhanced self-supervised pre-training for medical imaging.", "sections": [{"title": "1. Introduction", "content": "Supervised training of deep neural networks requires large amounts of quality annotated\ndata (LeCun et al., 2015). This is not always straightforward in applications involving clin-\nical tasks, due to the time, cost, effort and expertise required to collect labeled data (Taleb\net al., 2020). Self-supervised learning has recently demonstrated great success in leveraging\nunlabeled data, such as in natural language processing (Lan et al., 2019) and computer vi-\nsion (Jing and Tian, 2020). Such frameworks aim to learn useful underlying representations\nduring pretraining, without any labels, which are then used in downstream prediction tasks\nvia supervised linear evaluation."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Overview of Self-Supervised Learning", "content": "Self-supervised learning methods learn task-agnostic feature representations using hand-\ncrafted pretext tasks or joint embedding architectures (Kinakh et al., 2021; Bardes et al.,\n2021). Hand-crafted pretext tasks rely on the use of pseudo-labels generated from unlabeled\ndata. Examples of such tasks include rotation prediction (Gidaris et al., 2018), jigsaw\npuzzle solving (Noroozi and Favaro, 2016), colorization (Zhang et al., 2016), and in-painting\n(Pathak et al., 2016). Joint embedding methods utilize siamese networks (Bromley et al.,\n1993) to learn useful representations by discriminating between different views of samples\nbased on a specific objective function (Bardes et al., 2021), without the need for human\nannotation or pseudo-labels.\nJoint embedding methods can be further categorized into contrastive and non-contrastive\nmethods, where the latter encompasses clustering, distillation, and information maximiza-\ntion methods (Bardes et al., 2021). Contrastive methods learn representations by maximiz-\ning the agreement between positive pairs and minimizing the agreement between negative\npairs (Van den Oord et al., 2018). Some prominent examples include SimCLR (Chen et al.,\n2020), contrastive predictive coding (Van den Oord et al., 2018), and MoCo (He et al.,\n2020). Non-contrastive methods focus on optimizing different forms of similarity metrics\nacross the learned embeddings. Examples include BYOL (Grill et al., 2020), SimSiam (Chen\nand He, 2021), and VICReg (Bardes et al., 2021). While most existing work considers con-\nvolutional networks as backbones for input encoders, recent approaches explore the role\nof vision transformers (ViT) (Dosovitskiy et al., 2020) for self-supervision, such as DINO\n(Caron et al., 2021) and MSN (Assran et al., 2022). MSN is a state-of-the-art self-supervised\nlearning architecture that operates on the principle of mask-denoising, without reconstruc-\ntion, as well as transformation invariance with transformers. MSN has limited applications\nin healthcare-related tasks, and is promising considering its computational scalability."}, {"title": "2.2. Self-Supervised Learning in Healthcare", "content": "Self-supervised learning methods have shown great promise in learning representations of\ndifferent types of medical images, such as computed tomography and magnetic resonance\nimaging (Jamaludin et al., 2017; Zhuang et al., 2019; Taleb et al., 2020), optical coherence\ntomography and fundus photography (Holmberg et al., 2020; Hervella et al., 2020; Li et al.,\n2021), and endoscopy images (Ross et al., 2018). Several studies investigated self-supervised\nlearning for applications involving CXR images. For example, Sowrirajan et al. (2021),\nChen et al. (2021), and Sriram et al. (2021) utilized MoCo (He et al., 2020) as a pretraining\nstrategy for chest disease diagnosis and prognosis tasks. Azizi et al. (2021) showed that\ninitializing SimCLR (Chen et al., 2020) during pretraining with ImageNet weights improves\ndownstream performance in CXR classification. Van der Sluijs et al. (2024) explored the\nimpact of various image augmentation techniques on siamese representation learning for\nCXR.\nSome studies investigated the use of other sources of information in designing the self-\nsupervised learning framework for CXR, mostly focusing on vision-language pretraining.\nVu et al. (2021) proposed MedAug, which considers patient metadata for generating pos-"}, {"title": "3. Methods", "content": ""}, {"title": "3.1. Problem Formulation", "content": "Consider $x_{cxr} \\in \\mathbb{R}^{h\\times w}$, where h and w represent the image height and width, to be a CXR\nimage collected from patient during their hospital stay, the goal is to predict a set of disease\nlabels $Y_{cxr}$. We assume that each $X_{cxr}$ is associated with $X_{ehr} \\in \\mathbb{R}^{n}$, a vector of static\nfeatures derived from the patient's EHR data, where n is the number of variables. We use\nboth modalities to learn the CXR representation within our pretraining framework. An\noverview of the multi-modal MSN architecture for pretraining is shown in Figure 1. The\nnetwork consists of two components: (i) the visual encoders for CXR (ii) a multi-modal\nbranch that incorporates the EHR data, as described in the following sections."}, {"title": "3.2. Masked Siamese Network", "content": "We adopt the MSN (Assran et al., 2022) as the base model for our proposed framework due\nto its computational scalability and the need for pretraining transformers efficiently. For a\ngiven unlabeled input image, the goal is to align the anchor and target views, denoted by\n$x_{anchor}$ and $x_{target}$, respectively. For each image, a random transformation function T(.) is\nused to generate a set of M anchor views and a single target view. The transformations\ninclude image resizing, random resized cropping, and random horizontal flipping with a\nprobability of 0.5, following Sowrirajan et al. (2021). However, we excluded color jittering\nand Gaussian blurring as the former does not apply to grayscale images, while the latter may\ndistort disease-related information (Sowrirajan et al., 2021). Since the views are patchified\nto serve as input to a ViT, the anchor views are further masked via patch dropping (either\nrandom or focal masking as shown in Figure 2), while leaving $x_{target}$ unmasked.\nTwo encoders, $f_{anchor}$ and $f_{target}$, parameterized with a ViT (Dosovitskiy et al., 2020),\nare trained to generate the image embeddings:\n$v_{cxr} = f_{anchor}(x_{anchor}) \\& v_{cxr+} = f_{target}(x_{target})$.\n(1)\nThe target embedding $v_{cxr+}$ is further processed by a projection head $h^+$ to obtain $z^+$,\nwhile $v_{cxr}$ is used in the multi-modal mapping as described in the next section. MSN does\nnot compute the loss directly on the generated embeddings based on a certain similarity\nmetric. Instead, it utilizes a set of prototypes and seeks to map the projected embeddings\nof a given sample into the same learned prototype, where the mappings are used to compute\nthe loss. Further background information is provided in Appendix A."}, {"title": "3.3. Multi-modal Pretraining", "content": "Instead of solely relying on CXR, our proposed framework encourages MSN to leverage\nadditional information extracted from the patient's EHR. In particular, we introduce three\nadditional modules to the standard MSN architecture. First, we encode the static EHR\ndata with $f_{ehr}$ to learn a representation vector, such that:\n$v_{ehr} = f_{ehr}(X_{ehr})$.\n(2)\nWe then concatenate (\\oplus) the EHR and CXR latent representations, $v_{ehr}$ and $v_{cxr}$. At\nthis stage, there exists dimensionality mismatch between the concatenated representations"}, {"title": "3.4. Downstream Classification", "content": "After pretraining the encoders $f_{target}$, $f_{anchor}$, and $f_{ehr}$, we use $f_{target}$ as a feature extractor.\nWe then include a classification model $f_c$ to predict the image labels:\n$Y_{cxr} = f_c(v_{cxr+})$\n(4)\nThe main assumption behind our proposed method is that introducing additional data\nrelated to the patient or the CXR scan during pretraining would provide the model with\nvaluable contextual information. This information would contribute to enhancing the qual-\nity of the learned representations for downstream classification."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset", "content": "We conducted our experiments using the MIMIC-CXR dataset, which is a publicly available\ndataset of CXR images (Johnson et al., 2019). MIMIC-CXR includes 377,110 chest radio-\ngraphs gathered from 227,835 studies between 2011 and 2016. Each image is associated\nwith a set of ground-truth labels $y_{cxr} \\in \\mathbb{R}^{14}$ that indicates the presence of any of 14 medi-\ncal conditions extracted from corresponding radiology reports. The medical conditions are\nAtelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Cardiomediastinum, Fracture,\nLung Lesion, Lung Opacity, Pleural Effusion, Pneumonia, Pneumothorax, Pleural Other,\nSupport Devices, and No Finding. After pre-processing, the dataset consisted of 376,201\nindividual CXR images related to 65,152 patients and 82,506 stays.\nWe used MIMIC-IV (Johnson et al., 2023), an EHR database associated with MIMIC-\nCXR, to extract EHR variables related to the CXR images. Table 1 provides a summary\nof the extracted EHR features, including data types, possible values, and dimensionality.\nAfter a thorough investigation of the MIMIC-IV database, we identified three sets of relevant\nfeatures: patient demographics ($x_D$), scan metadata ($x_{SM}$), and inpatient stay information\n($x_{SI}$). For patient demographics, we retrieved patient age ($x_{age}$) for each specific admission\nand normalized it using MinMax normalization, as well as patient sex ($x_{sex}$) encoded as a\none-hot vector. For scan metadata, we extracted the scan view ($x_{view}$) and patient position\n($x_{pos}$) and encoded them as one-hot vectors. The possible values for view are antero-\nposterior (AP), lateral (L), postero-anterior (PA), or left-lateral (LL), while for patient\nposition the possible values are erect or recumbent. For inpatient information, we defined\n$x_{icu}$ and $x_{mort}$ as binary features indicating whether the patient required intensive care\ntreatment or experienced in-hospital mortality, respectively. We provide a summary of the\nEHR features distributions in Appendix B.\nWe used MIMIC-CXR and MIMIC-EHR for pretraining and downstream classification.\nWe followed the work of Hayat et al. (2022) to obtain the training, validation, and test\nsplits. The data splits are provided in Table 2."}, {"title": "4.2. External Validation", "content": "We also performed external validation of our pretrained encoders in downstream classifica-\ntion using the ChexPert (Irvin et al., 2019) and National Institutes of Health Chest X-Ray\nDataset (NIH-14) (Wang et al., 2017) datasets. ChexPert is a publicly available medical\nimaging dataset composed of 224,316 chest radiographs of 65,240 patients, collected between\n2002-2017, including inpatient and outpatient centers. It includes images and associated\nradiology reports, which were used to generate labels indicating the presence of 14 dis-\neases. We evaluated the pretrained encoder and classifier obtained with MIMIC-CXR on\nthe ChexPert test set since they have the same labels.\nNIH-14 is a publicly available medical imaging dataset composed of 112,120 frontal-\nview CXR images of 30,805 patients, collected between 1992-2015. Each image is assigned\n14 common disease labels. We follow the same split reported in Irvin et al. (2019) and\nperform linear evaluation using the NIH-14 training set, since it has different labels from\nMIMIC-CXR. The data splits are shown in Table 2."}, {"title": "4.3. Evaluation Protocol", "content": "To assess the quality of the learned representations using our proposed pretraining frame-\nwork, we conducted linear evaluation as the main evaluation protocol following previous\nwork in self-supervised learning (Bardes et al., 2021; Garrido et al., 2023; Huang et al.,\n2022). We froze the encoder $f_{target}$ after pretraining, and trained the classification head $f_c$,\nwhich is randomly initialized, with embeddings of the whole training set.\nWe report performance using the Area Under the Receiver Operating characteristic\nCurve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC) across all ex-\nperiments. We report the 95% Confidence Intervals (CI) using the bootstrapping method\n(Puth et al., 2015). We also performed statistical significance testing by comparing our\nproposed approach with the best performing baseline."}, {"title": "4.4. Uni-modal Self-supervised Pre-training Baselines", "content": "We compared the results of our proposed method with self-supervised and supervised base-\nlines that are transformer-based for a fair comparison:\n1. Vanilla MSN (Assran et al., 2022): MSN is a self-supervised learning model that\nleverages the idea of mask-denoising while avoiding pixel and token-level reconstruc-\ntion. We trained the vanilla version following the architecture in the original work.\n2. DINO (Caron et al., 2021): DINO is a transformer-based architecture that learns rep-\nresentation via knowledge distillation from a teacher backbone to a student backbone\nwithout labels.\n3. Masked AutoEncoder (MAE) (He et al., 2022): MAE is a transformer-based\nencoder-decoder architecture that learns representation by reconstructing the missing\npixels within an input image.\n4. Supervised: We train ViT-S and ViT-T in an end-to-end fashion with random and\nImageNet (Deng et al., 2009) weight initializations."}, {"title": "5. Results", "content": "Here, we present the performance results of our proposed method and all self-supervised\nbaselines using the ViT-S. The performance results for ViT-T and all other baselines, in-\ncluding self-supervised and supervised learning methods, are reported in Appendix C."}, {"title": "5.1. Linear Evaluation Results", "content": "Table 3 summarizes the performance results of the linear evaluation experiments using ViT-\nS. First, we note that all of the models that incorporate EHR during pretraining achieve a\nbetter performance compared to the best performing baseline, vanilla MSN. The improve-\nment is at least 1% in terms of AUROC and AUPRC, with a maximum improvement of 2%\nwhen incorporating demographic information, 0.751 AUROC with $I_{sex}$ or $x_D$, compared\nto 0.731 AUROC with MSN. Additionally, in most experiments where we pretrain with a\nsingle EHR feature achieve improvements greater than 1.5%, while pretraining with groups\nof features yields slightly lower performance improvements.\nMoreover, our proposed strategies demonstrate a significant improvement compared to\nstate-of-the-art self-supervised learning baselines, specifically MAE and DINO, as well as\nthe supervised ImageNet-initialized baseline, reaching an improvement of 10% compared to\nthe worst performing model. In summary, the results of linear evaluation demonstrate that\nour approach, which incorporates EHR data during pretraining, enhances the quality of the\nlearned representations and downstream performance in a multi-label disease classification\ntask, surpassing both the vanilla MSN and other baselines. In Appendix C, we provide\nfurther results with ViT-T as the model backbone. In general, we observe similar patterns,\ndemonstrating consistent behaviour with different backbone architectures.\nFor the sake of completeness we also report in Appendix C the results of a secondary\nevaluation protocol by fine-tuning under low data regimes including 1%, 5%, and 10% of"}, {"title": "5.2. External Validation Results", "content": "Table 4 presents the external validation results for linear evaluation of our proposed methods\nusing the ViT-S backbone on the ChexPert Irvin et al. (2019) and NIH-14 Wang et al. (2017)\ndatasets. The results indicate that our multi-modal pretraining enhances the robustness of\nself-supervised models, resulting in higher validation scores and improved performance com-\npared to vanilla MSN in most scenarios. In the best scenario, the performance gain reaches\n3-4% in both evaluation datasets. However, we observe better performance improvements\nin the NIH-14 dataset, since its training set was used to train the linear evaluation classifier,\ncompared to off-the-shelf evaluation with CheXpert. The results for the VIT-T backbone\nand fine-tuning are reported in Appendix C. We observe similar patterns that are in line\nwith the findings of internal validation with MIMIC-CXR."}, {"title": "5.3. t-SNE Analysis Results", "content": "We applied t-SNE to a subset of samples with a single label in the training set (n=68,000) of\nMIMIC-CXR for the ease of interpretability, considering the challenge of visualizing multi-\nlabel samples in the latent space. Figure 3 visualizes the embeddings obtained with vanilla"}, {"title": "6. Discussion", "content": "In this paper, we propose the incorporation of EHR data during self-supervised pretraining\nfor CXR representation learning, with the aim of improving performance in downstream\nclassification. We present our proposed approach as an enhanced version of an existing self-\nsupervised learning method, MSN (Assran et al., 2022), by including clinical imaging-related\npatient EHR data during pretraining. We also comprehensively evaluated our proposed ap-\nproach on the MIMIC-CXR dataset (internal validation), and two datasets for external\nvalidation, ChexPert and NIH-14. Our results demonstrate that the integration of EHR\nmetadata during pretraining with MSN significantly improves performance in the down-\nstream linear evaluation protocol. We also observe that, in general, the inclusion of a single\nEHR feature yields better quality embeddings compared to pretraining with combinations\nof EHR features, as shown by the t-SNE embeddings in Figure 3. In addition, our proposed\nmethod shows superior performance compared with other state-of-the-art self-supervised\nbaselines, including DINO (Caron et al., 2021) and MAE, (He et al., 2022) as well as fully\nsupervised baselines pretrained with ImageNet (Deng et al., 2009). Our extensive evalua-\ntion highlights that our proposed approach generalizes well to external validation datasets,\nsignificantly outperforming vanilla MSN in various settings for CheXpert and in all cases\nfor the NIH-14 dataset.\nA relevant insight derived from our extensive experimentation using single features and\ncombinations of them is that it is not guaranteed that the addition of more EHR features\nleads to improved performance. We hypothesize that such behavior could be attributed to\nfeature interactions, which is an area of future work.\nLimitations Despite the performance improvements shown by our framework during lin-\near evaluation, our approach has some limitations. First, our approach did not show sig-\nnificant improvements during end-to-end fine-tuning. That is, it performed on-par with\nvanilla MSN, or slightly lower in the worst scenario as shown in the additional results in\nAppendix C. We hypothesize that fine-tuning the target encoder without the EHR data\ndilutes the impact of incorporating it during pretraining. Specifically, the learned repre-\nsentations during the pretraining stage (with EHR data) are updated during fine-tuning\n(without EHR data), which impacts the quality of the representations. We aim to ad-\ndress this limitation in our future work, where the EHR data is incorporated during both\nlinear evaluation and fine tuning, rather than solely during pre-training for a fair compar-\nison. Furthermore, we only tested our proposed methodology with a transformer-based\nself-supervised learning method. In future work, we will explore its applicability to other\nself-supervised learning architectures and generalization as a framework for other medical\nimaging datasets and clinical prediction tasks."}, {"title": "Appendix A. Implementation details", "content": "In this section, we present additional information pertaining to the implementation details."}, {"title": "A.1. Masked Siamese Network", "content": "The MSN network (Assran et al.", "strategies": "random masking drops random patches\nat different locations across the input image"}, {"content": "wheredby the encoder $f_{anchor}$ while the target patches are by the $f_{target}$ (see Figure 1). The of encoder are updated throughexponential moving average of $f_{anchor}$. Both encoders are parameterized as ViT encoders.token of each encoder is processed by a three-layer projection head h to obtainimage embeddings $z_{i,m}$ and $z_{i}^{+} \\in \\mathbb{R}^{k}$ for each encoder.also set of learnable prototypes $q \\in \\mathbb{R}^{n\\times k}$ where K is the network the based ,\\, thatwhere"}]}