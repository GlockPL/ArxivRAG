{"title": "Do Large Language Models Have an English \"Accent\u201d? Evaluating and Improving the Naturalness of Multilingual LLMs", "authors": ["Yanzhu Guo", "Simone Conia", "Zelin Zhou", "Min Li", "Saloni Potdar", "Henry Xiao"], "abstract": "Current Large Language Models (LLMs) are predominantly designed with English as the primary language, and even the few that are multilingual tend to exhibit strong English-centric biases. Much like speakers who might produce awkward expressions when learning a second language, LLMs often generate unnatural outputs in non-English languages, reflecting English-centric patterns in both vocabulary and grammar. Despite the importance of this issue, the naturalness of multilingual LLM outputs has received limited attention. In this paper, we address this gap by introducing novel automatic corpus-level metrics to assess the lexical and syntactic naturalness of LLM outputs in a multilingual context. Using our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark in French and Chinese\u00b9, revealing a tendency towards English-influenced patterns. To mitigate this issue, we also propose a simple and effective alignment method to improve the naturalness of an LLM in a target language and domain, achieving consistent improvements in naturalness without compromising the performance on general-purpose benchmarks. Our work highlights the importance of developing multilingual metrics, resources and methods for the new wave of multilingual LLMs.", "sections": [{"title": "Introduction", "content": "LLMs are becoming an integral part of society with a visible impact on the general public. However, there exists a significant disparity in how different languages are represented in LLMs, as popular models are primarily designed with English in mind. While this is a well-known issue and more multilingual LLMs are being released, most of them are still English-dominated. A prominent example is the Llama 3.1 series of models, which are claimed to be state-of-the-art multilingual LLMs: these models are trained on 15T tokens, yet only 8% of the training data is declared to be non-English.\nWe could make an analogy between these multilingual LLMs and native English speakers who are trying to acquire a new language. Their language notions are built in an English-centric system, and they inevitably bring traces of English habits into other languages when transferring their notions. Moreover, because of the lack of data in non-English languages, multilingual LLMs are often exposed - either during pre-training or post-training  to texts translated from English. Both human and machine translated language are known to suffer from translationese artifacts, which set them apart from native content. LLMs trained on such data are susceptible to suffer from the same unnaturalness problems as the second-language learners in our analogy.\nIn fact, even LLM-generated texts in English are known to exhibit distributional differences from human-written texts. Given the predominance of English data, this effect is likely more pronounced in non-English outputs. However, current evaluations of multilingual LLMs still focus on their task-solving capabilities , overlooking the aspect of naturalness. As LLMs increasingly influence various aspects of our lives, their tendency to produce less natural outputs in lower-resource languages in favor of English expressions could amplify the unfairness for the communities that speak these languages. Therefore, it is crucial to evaluate and improve the naturalness of multilingual LLMs to foster fair language representation.\nIn this paper, we take two steps towards this goal. Our first step is the introduction of a novel set of metrics to evaluate the naturalness of LLM outputs at a corpus level by comparing the lexical and syntactic distributions of LLM outputs with human written texts. We use these metrics, together with our new topically-aligned cross-lingual dataset, to benchmark and analyze the naturalness of state-of-the-art multilingual LLMs in English, French and Chinese. Our second step is the introduction of a simple and effective approach to enhancing the naturalness of multilingual LLMs in a target language. Using Direct Preference Optimization (DPO) , we leverage a new preference dataset that contrasts human-written responses with synthetically-manipulated ones. Experimental results show that our method consistently improves naturalness of an LLM in Chinese without sacrificing its capabilities on general-purpose benchmarks.\nIn summary, our contributions are threefold:\nWe develop new metrics to evaluate the lexical and syntactic naturalness of LLM outputs in a multilingual setting;\nWe create a benchmark for cross-lingual evaluation of LLM naturalness and draw insights from the benchmark results on important factors that could impact LLM naturalness;\nWe propose an alignment approach for improving the naturalness of existing LLMs with promising results across models and domains.\nWe hope our investigation will encourage further research on the limitations of multilingual LLMs beyond their scores on task-solving benchmarks."}, {"title": "Related Work", "content": "Although the evaluation of LLMs has received significant research interest in recent years, the greater part of this body of work has focused on aspects such as helpfulness , factual accuracy , safety , fairness , and task-specific performance , leaving the naturalness of LLM outputs under-investigated. Naturalness is commonly used as an evaluation criterion in machine translation, but it has mostly relied on either human ratings or trained classifiers.\nTo the best of our knowledge, our work is the first to systematically investigate linguistic naturalness in multilingual LLM generations outside of the machine translation context. Nonetheless, several adjacent research areas are highly relevant to our focus, including translationese detection, linguistic diversity evaluation, and multilingual language model analysis.\nTranslationese detection, a well-established task in machine translation, aims to determine whether a text is originally written in the target language or translated from another language. For instance,  employ a range of linguistic features - including type-token ratio, lexical density, answer lengths, dependency tree height, constituency tree height, perplexity, etc. to train a classifier to distinguish between machine-translated and naturally occurring sentences. However, these classifiers are prone to overfit on specific training data. In an effort to enhance the naturalness of machine translation outputs,  also tagged parallel training data based on target-side naturalness, contrasting models trained on natural versus translated text. Our approach builds on the concept of contrasting natural versus unnatural texts but avoids reliance on pre-trained classifiers. Instead, we use automatically manipulated unnatural texts and preference learning, extending the analysis beyond machine translation to general multilingual text generation.\nLinguistic diversity evaluation  is another area closely related to naturalness, as a key marker of unnaturalness in synthetic text is reduced linguistic diversity. Our work draws inspiration from the way these diversity features are computed. Past work compares the diversity of generated texts to human texts and consider texts to be natural if they approach the human level of diversity . However, such an approach primarily assesses the spread of the distribution while overlooking more holistic comparisons of linguistic features. To address this, we introduce metrics that directly compare vocabulary and syntactic distributions between human and machine-generated texts.\nMultilingual analysis of LLMs has recently shown that models trained on unbalanced, English-heavy corpora often rely on English as an internal pivot language. Wendler et al. (2024) demonstrate that the concept space of LLaMA-2 is more closely aligned with English than with other input languages. Similarly,  show that multilingual BERT exhibits a bias toward English-like grammatical structures. Despite these findings, there has been no systematic study on how this English-centric tendency affects the linguistic naturalness of multilingual LLM outputs, particularly in open-ended downstream tasks."}, {"title": "Evaluation Metrics for Naturalness", "content": "In this section, we present a new set of evaluation metrics aimed at measuring the naturalness of multilingual text generations at the corpus level. While our approach also requires a reference set of natively written texts, it differs from widely used reference-based metrics such as BLEU , ROUGE , and BERTScore . Indeed, instead of comparing reference and system outputs at the sample level, our new metrics allow us to investigate naturalness at the corpus (distribution) level.\nSample-level reference metrics often struggle to account for human label variability and uncertainty , particularly in open-ended tasks with multiple valid generations. Also, while a single text sample with certain choices of vocabulary or grammatical structure might seem natural, the repeated occurrence of these features across a large scale of generations would raise a red flag. That is, detecting unnaturalness in individual samples may prove challenging, but when analyzed at the corpus level, statistical features become strong indicators . Our metrics operate by comparing patterns of language use across large text collections, providing a broader, distribution-based perspective.\nHaving highlighted the advantage of a corpus-level perspective, we propose a new definition for language model naturalness. Past studies have defined the naturalness of a piece of text by asking \u201ccould it have been produced by a native speaker?\u201d . We adapt this definition to the corpus level and define the naturalness of a language model by asking \u201ccould the set of texts generated by this language model have been produced by a group of native speakers when responding to the same prompts?\u201d\nOur metrics are inspired by other divergence-based measures such as MAUVE , which quantify the information-theoretic divergence between the probability distributions of a language generator and a true natural language distribution. However, our approach differs by not relying on another language model to embed the generated texts. This reduces the risk of introducing intrinsic biases from the chosen embedding model, a crucial consideration in multilingual settings, where such the chosen embedding models are often English-dominated themselves. Our method is also more transparent and interpretable, as it clearly distinguish between two key aspects of linguistic naturalness: syntactic and lexical naturalness. However, our metrics focus exclusively on the linguistic form of the text and do not address semantic aspects. For analyzing semantics, the use of external embedding models may be inevitable.\nIn the following, we introduce the methodology for evaluating lexical and syntactic naturalness. The implementations of the metrics are described in Appendix C."}, {"title": "Lexical Naturalness", "content": "We propose to measure the lexical naturalness of an LLM by comparing the vocabulary distribution of its generated text with the that of human-written text. More specifically, we put forward a lexical naturalness metric based on computing the Jensen-Shannon Divergence (JSD) between the lexical distributions of LLM-generated and human-written text from the same prompts. The JSD provides a symmetric and bounded measure of difference between two distributions without them necessarily sharing the same support, i.e., the tokens in the vocabulary of the LLM in our case. Given the vocabulary distributions P and Q corresponding to human and model outputs, respectively, the JSD is calculated as follows:\n$JSD(P||Q) = \\frac{1}{2} (D_{KL}(P||M) + D_{KL}(Q||M))$,\nwhere $M = \\frac{1}{2}(P + Q)$ is the midpoint distribution, and $D_{KL}$ is the Kullback-Leibler divergence. By assessing the divergence, we can quantify how closely the model's vocabulary distribution aligns with human language, where lower values indicate greater similarity and higher lexical naturalness.\nWe note that our approach to directly compare the two vocabulary distributions implicitly captures the lexical statistical tendencies of past work that used type token-ratio and rank-frequency coefficient ."}, {"title": "Syntactic Naturalness", "content": "To measure the syntactic naturalness of LLMs generations we leverage the Universal Dependencies (UD) grammar framework . UD provides well-defined, theoretically-grounded linguistic structures across different languages, making it suitable for cross-lingual comparisons. Our proposed metric is based on representing each sentence as a dependency tree, where nodes correspond to words and edges specify the dependency relations between them. Additionally, each word is annotated with its corresponding Part-of-Speech (POS) tag. At a high-level, our metric computes the structural similarity of all pairs of sentences in a corpus, clustering texts whose syntax is similar and allowing us to determine if there is a distributional difference between two groups (LLM-generated text and human-written text, in our case).\nTo compute the structural similarity between pairs of dependency trees, we propose to use the Weisfeiler-Lehman (WL) graph kernel . The WL kernel iteratively refines node labels based on the labels of neighboring nodes, thereby constructing a hierarchical encoding of the graph structure. More formally, let $T_1$ and $T_2$ be two dependency trees with respective vertex, i.e., word, sets $V_1$ and $V_2$. The WL kernel, $K_{WL}(T_1, T_2)$, measures the similarity between $T_1$ and $T_2$ by comparing their subtree structures. Initially, each node in the trees is assigned a label based on its original POS tag. Then, at each iteration h, the label of each node is updated based on the multiset of labels of its neighboring nodes, and the new labels are hashed into unique labels. This process continues for H iterations, and the kernel value is computed as the number of matching node labels across all iterations:\n$K_{WL}(T_1, T_2) = \\sum_{h=0}^H \\sum_{(v_1, v_2) \\in (V_1, V_2)} \\delta(l_h(v_1), l_h(v_2))$,\nwhere $l_h(v)$ is the label of node $v$ at iteration h, and $\\delta(., .)$ is the Kronecker delta function. For our experiments, we fixed the number of iterations H to 2, as this proved to be the most effective hyperparameter across various discriminative tasks for graphs.\nGiven a set of human-generated sentences ${s_i^h}_{i=1}^{N_h}$ and model-generated sentences ${s_j^m}_{j=1}^{N_m}$, we compute the WL kernel between all pairs of dependency trees from these sets. The resulting kernel matrix $K \\in R^{N_h \\times N_m}$ has elements $K_{ij} = K_{WL}(T_i^h, T_j^m)$, where $T_i^h$ and $T_j^m$ represent the dependency trees corresponding to sentences $s_i^h$ and $s_j^m$, respectively. This kernel matrix K captures structural similarities between the dependency trees of human-written and LLM-generated sentences.\nOnce the kernel matrix is obtained, we use the Maximum Mean Discrepancy (MMD)  to compare the distributions of human-generated and model-generated sentences. In particular, the $MMD^2$ between the two sets of sentences is computed as:\n$\\sum_{i,i'} \\frac{K_{ii'}}{N_h N_h} + \\sum_{j,j'} \\frac{K_{jj'}}{N_m N_m} - 2 \\sum_{i,j} \\frac{K_{ij}}{N_h N_m}$,\nwhere $K_{ii'}$ and $K_{jj'}$ represent the similarities within the human and model-generated sentence sets, and $K_{ij}$ represents the cross-similarities between the two sets. The resulting value provides a measure of syntactic divergence between the human-generated and model-generated sentences, with a lower value indicating greater similarity.\nOur proposed syntactic naturalness metric quantifies the syntactic divergence between human and model-generated sentences by examining the distribution of their dependency tree structures. This approach considers both the dependency relationships and the hierarchical arrangement of substructures at multiple levels."}, {"title": "Cross-lingual Analysis of Naturalness", "content": "After introducing our new metrics for evaluating lexical and syntactic naturalness, we proceed with a cross-lingual analysis of the naturalness of multilingual LLMs. This process involves curating a new dataset, selecting the models, and analyzing the benchmark results."}, {"title": "Dataset and Evaluation", "content": "For our analysis, we require a corpus that is topically aligned across languages while preserving ground-truth naturalness. This means that the texts in each language must be natively written, not translated \u2013 whether by humans or models. As a result, the parallel corpora typically used in machine translation research are not suitable. Instead, we construct a new dataset that satisfies our criteria starting from Wikipedia\u00b3, which offers a wealth of texts that are frequently edited by native speakers and that we can topically-align across languages. Importantly, to minimize cultural bias in our dataset, we select the most-viewed Wikipedia articles across English, Chinese and French. Our data curation process results in 3,722 Wikipedia pages, each accompanied by their text in the three target languages. We discuss the preprocessing of this dataset in Appendix A.\nFor each entry in the dataset, we instruct the models to complete a straightforward task: generating a description for the given entry in each of the three languages. We selected this task to avoid overly constraining the content of the language model outputs, allowing them to generate in a more natural and organic manner. We stress that our focus is on the overall distribution of vocabulary and grammatical structures rather than on individual outputs, the semantic content and/or factual accuracy. The prompts and generation settings used in this task are provided in Appendix B."}, {"title": "Multilingual LLMs", "content": "We experiment with three families of open-weight LLMs: Llama , Qwen and Mistral . These models were selected for their state-of-the-art performance on diverse English and multilingual benchmarks. Additionally, they are developed by teams from regions where English, Chinese, and French are the official languages, respectively. While the exact composition of the training data for each model is not publicly available, we speculate that all are predominantly English-centric, though we expect Qwen to include more Chinese data and Mistral to include more French data than the others. We benchmark the two most recent versions from each model family to track changes in naturalness performance over time. For all models, we test their moderately scaled versions, using open-source implementations from the Transformers library . We focus on the instructed versions of the models, as these are more commonly used in real-world applications compared to the base versions."}, {"title": "Results and Analysis", "content": "The results on our naturalness dataset according to our new metrics (Section 3) are presented in Table 1. Unsurprisingly, the results demonstrate a consistent improvement in naturalness across newer versions of LLMs compared to earlier models (e.g., from Qwen-1.5 to Qwen-2). However, our metrics highlight a noticeable gap between the level of naturalness of human-written and LLM-generated text, particularly for non-English generations, validating our initial hypothesis. Note that the human reference value is calculated by randomly selecting non-overlapping subsets of human-written texts and computing the divergence between them.\nGap in lexical naturalness. In terms of lexical divergence, English outputs \u2013 especially from Mistral-v0.3 - approach human reference values, while larger gaps persist in Chinese and French, with Chinese exhibiting the most pronounced difference. This suggests that models are lexically less natural in languages other than English.\nEnglish \"accent\u201d in syntactic structures. When examining syntactic divergence, all models in all languages show significant differences from the human reference values. Since dependency trees, when parsed using Universal Dependencies gram-"}, {"title": "Improving Naturalness through Preference Tuning", "content": "We now propose an approach to improve the naturalness of language model outputs. Among the various stages of LLM development, preference-based learning is the most effective for refining stylistic features . Based on this, we focus on aligning models for better naturalness during the preference tuning stage. Among the various preference optimization techniques, we propose to use DPO due to its simplicity and efficiency."}, {"title": "Preference Dataset Construction", "content": "We construct the preference datasets starting with SFT (Supervised Fine-Tuning) datasets originally written in the target language. However, we find that non-translated, open-source SFT datasets for non-English languages are almost non-existent. Even for Chinese \u2013 despite being the second highest-resource language after English \u2013 only a limited number is available. Consequently, we conduct our DPO experiments on Chinese, using two datasets that focus on essay generation and open-domain QA tasks. These tasks are well-suited for stylistic alignment due to their open-ended, creative nature. For essay generation, we use the \u201ccomposition\" split from the Firefly dataset , and for open-domain QA, we use the OpenLabel-Chinese Conversations Dataset .\nThe original instructions from the SFT datasets are preserved, with the initial response serving as the preferred response. To generate the unnatural, i.e., rejected, response we apply synthetic manipulations through paraphrasing and back-translation via English. The prompts we use to generate the rejected responses are listed in Appendix D. We ensure semantic consistency by using BLEU to filter out pairs of chosen and rejected responses with insufficient similarity. Moreover, to ensure that the linguistic style of the rejected responses differs significantly from the chosen ones, we also filter out pairs with overly high BLEU scores. Based on an empirical analysis of BLEU score distributions, we define the final threshold as 0.15 < BLEU(Chosen, Rejected) < 0.9. We also filter out responses shorter than 10 words (often in the style of multiple-choice questions), as they cannot convey enough lexical or syntactic style. The statistics of our two resulting preference datasets are presented in Table 7.\""}, {"title": "Experimental Setup", "content": "We believe that it is beneficial to perform naturalness alignment on models that have already undergone SFT and previous rounds of preference tuning, since preference tuning is typically done iteratively. We conduct experiments with the instruction-tuned versions of Llama-3.1 and Qwen2, which demonstrated the highest and lowest performance, respectively, in naturalness for Chinese in Table 1. To increase GPU memory usage efficency and to avoid catastrophic forgetting, we apply Low-Rank Adaptation (LoRA) in conjunction with DPO. Detailed hyperparameters for both LoRA and DPO are provided in Appendix E.\nWe explore two approaches: (1) generating the preference-tuning dataset with the same model that will be aligned (e.g., using Llama-3.1 to create training data for Llama-3.1), and (2) cross-model alignment, where the dataset generated by one model is used to align another model (e.g., using Llama-3.1 to create training data for Qwen2)."}, {"title": "Results", "content": "The results in Table 3 show consistent improvements in both lexical and syntactic naturalness across models and tasks after applying our alignment method. In Table 4, we provide an example on the essay generation task, comparing responses generated by Llama-3.1 before and after naturalness alignment using the same prompts.\nAlthough we experiment with generating rejected responses with both paraphrasing and back-translation, the latter consistently outperforms the former by a small margin. We believe this is due to back-translation being more effective at introducing translationese artifacts into the rejected responses. Therefore, the final results in Table 3 are based on back-translation. However, there is no definitive conclusion on whether rejected responses should be generated using the same model undergoing alignment or a different model, and future work could explore generating response pairs with a combination of different models.\nIn addition, we evaluate our aligned models on the Chinese Massive Multitask Language Understanding (CMMLU) benchmark to ensure that their general capabilities are not compromised after naturalness alignment. Our results show that our alignment method not only improves linguistic naturalness but also slightly enhances overall language understanding performance. Interestingly, we observe that naturalness does not always positively correlate with language understanding performance. For example, Qwen2, despite achieving much higher scores than Llama-3.1 on the CMMLU benchmark, demonstrates lower naturalness, probably due to the heavy use of synthetic data. This highlights the need for future research to consider naturalness as a complementary metric alongside conventional benchmark scores."}, {"title": "Conclusion", "content": "In this paper, we address the naturalness challenges of language models, particularly in multilingual contexts, by conducting experiments in both French and Chinese. We introduce two corpus-level metrics to quantify the naturalness of model output distributions: one focused on vocabulary (lexical naturalness) and the other on grammatical structure (syntactic naturalness). These metrics are interpretable and free from biases introduced by external embeddings. Using these metrics, we benchmark state-of-the-art multilingual LLMs and analyze how factors such as training data, prompting language, and decoding strategy influence the generated language. Our analysis provides valuable insights into the current multilingual LLM landscape, complementing traditional performance benchmarks for task-solving capabilities. Finally, we propose an alignment method using DPO and a synthetically manipulated preference dataset to enhance the naturalness of model outputs. Experiments show that our aligned models consistently improve in both lexical and syntactic naturalness."}, {"title": "Limitations", "content": "Our experiments do not include any low-resource languages because our approach relies on the availability of ground-truth distributions from native, human-written data, which is often scarce or unavailable for many languages. While our naturalness evaluation metrics are designed to be language-agnostic, they depend on reliable word tokenizers and dependency parsers for the languages studied. Unfortunately, such tools are still lacking for most low-resource languages. However, we argue that naturalness evaluation should not be the priority for these languages at this stage, as it is only meaningful once models achieve a certain baseline level of overall performance.\nOur cross-lingual benchmark was limited to the Wikipedia domain, as this domain provides topically aligned, natively written content across languages. Wikipedia was the only source we could access with the necessary data. Although Wikipedia text serves as a reasonable proxy for general domain text, it may not guarantee that our findings are applicable across other domains. In the future, it would be valuable to develop more non-translated cross-lingual corpora for additional domains and extend the naturalness evaluation to those areas as well.\nOur alignment approach has so far only been tested on data from essay generation and general domain question-answering tasks. These tasks were chosen because their creative and open-ended nature allows for the expression of stylistic features. However, applying this method to more knowledge-intensive and constrained tasks may introduce unintended knowledge editing, potentially leading to increased hallucination risks. Furthermore, our experiments were limited to Chinese due to the lack of natively written SFT datasets in other non-English languages. The Aya dataset , while a valuable resource for multilingual instruction tuning with native data, provides too few samples in each language. For example, after filtering for response length, we obtained only 958 samples in French from Aya, which is insufficient for our alignment and evaluation approach.\nCollecting human annotations for naturalness evaluation is challenging. We initially attempted to gather human annotations for a meta-evaluation of our metrics, but annotators reported difficulty in distinguishing linguistic naturalness from individual generations. Our naturalness evaluation operates on a corpus level. As discussed in Section 3, while a single text with specific vocabulary choices or grammatical structures may appear natural, repeated occurrences of these features across many generations would raise concerns. Human evaluators cannot easily process a large corpus and identify these patterns. Therefore, we believe that our automatic metrics address this gap where human evaluations fall short.\nFinally, our evaluation and alignment methods focus solely on the naturalness of linguistic form, without considering social biases. However, we believe linguistic biases are significantly underexplored compared to social biases, and we aim to bridge this gap by taking an initial step in this direction."}, {"title": "Preprocessing of the Wikipedia Dataset", "content": "We truncate any summaries exceeding 1024 tokens, as determined by Llama-3.1's tokenizer. Since the Chinese version of Wikipedia often contains content in traditional Chinese, we use the zhconv-rs\u2076 library to convert all text to simplified Chinese for consistency during pre-processing."}, {"title": "Wikipedia Description Generation", "content": "We now present the employed prompts and settings for Wikipedia Description Generation."}, {"title": "Prompts", "content": ""}, {"title": "Generation Settings", "content": "We use bf16 precision for generation, with a max_new_tokens limit of 1024, matching the truncation length of the human-written summaries. For most experiments, except those analyzing the effect of decoding temperature, we use temperature sampling with a temperature of 0.6 and a repetition_penalty of 1.02."}, {"title": "Implementation of Naturalness Metrics", "content": "For lexical naturalness, we process vocabulary at the word level instead of subword token level to represent more meaningful lexical units. We use the Jieba tokenizer for Chinese and the NLTK\u00b9\u2070 tokenizer for French and English. We remove punctuation and digits, but do not discard stop words as they are an important linguistic feature (Meister and Cotterell, 2021).\nFor syntactic divergence, we parse sentences into dependency trees using the Stanza toolkit\u00b9\u00b9 (Qi et al., 2020), which generates dependency trees for each sentence according to the UD grammar ."}, {"title": "Preference Dataset Generation", "content": "We provide the prompts used for generating our preference dataset, along with statistical insights into the resulting dataset."}, {"title": "Prompts", "content": ""}, {"title": "Dataset Statistics", "content": "Statistics of the generated preference dataset is shown in Table 7. Rejected responses are shorter than the chosen ones on average, which may be due to models' tendencies to prioritize words that occur more frequently in the tokenizer's training set, resulting in fewer subword splits."}, {"title": "Hyperparameters for Preference Tuning", "content": "We conduct experiments using 8 Nvidia A100 GPUs, each with 40GB of memory. All models are trained in bf16 precision for 1 epoch. DPO training was performed with data parallelism, taking approximately 2 to 3 hours per model per dataset. We utilize the DPO implementation from the trl library\u00b9\u00b2 and the LORA implementation from the PEFT library\u00b9\u00b3. The best-performing hyperparameters are listed in Table 8."}, {"title": "Additional Results", "content": "Additional results analyzing the factors influencing model naturalness are provided in Figures 2, 3, and 4."}]}