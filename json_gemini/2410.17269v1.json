{"title": "FairFML: Fair Federated Machine Learning with a Case Study on Reducing Gender Disparities in Cardiac Arrest Outcome Prediction", "authors": ["Siqi Li, BSc", "Qiming Wu, MSc", "Xin Li, MSc", "Di Miao, MSc", "Chuan Hong, PhD", "Wenjun Gu, BSc", "Yuqing Shang, MSc", "Yohei Okada, PhD", "Michael Hao Chen, BSc", "Mengying Yan, MSc", "Yilin Ning, PhD", "Marcus Eng Hock Ong, MPH", "Nan Liu, PhD"], "abstract": "Mitigating algorithmic disparities is a critical challenge in healthcare research, where ensuring equity and fairness is paramount. While large-scale healthcare data exist across multiple institutions, cross-institutional collaborations often face privacy constraints, highlighting the need for privacy-preserving solutions that also promote fairness.\nIn this study, we present Fair Federated Machine Learning (FairFML), a model-agnostic solution designed to reduce algorithmic bias in cross-institutional healthcare collaborations while preserving patient privacy. As a proof of concept, we validated FairFML using a real-world clinical case study focused on reducing gender disparities in cardiac arrest outcome prediction.\nWe demonstrate that the proposed FairFML framework enhances fairness in federated learning (FL) models without compromising predictive performance. Our findings show that FairFML improves model fairness by up to 65% compared to the centralized model, while maintaining performance comparable to both local and centralized models, as measured by receiver operating characteristic analysis.\nFairFML offers a promising and flexible solution for FL collaborations, with its adaptability allowing seamless integration with various FL frameworks and models, from traditional statistical methods to deep learning techniques. This makes FairFML a robust approach for developing fairer FL models across diverse clinical and biomedical applications.", "sections": [{"title": "1. Introduction", "content": "Machine learning (ML) and artificial intelligence (AI) methods have been rapidly adopted in healthcare for predictive modeling, offering substantial potential to improve patient outcomes[1]. However, ensuring health equity-an essential principle in population and public health research[2]-remains a critical challenge, particularly when algorithmic findings directly impact clinical decision-making and patient care[3]. Growing concerns have been raised about the potential for ML and AI systems to underperform in historically underserved populations, including women and individuals from lower socioeconomic backgrounds[4]. In response, researchers have developed various qualitative and quantitative approaches to promote fairness in clinical model development[5].\nAlgorithmic disparity[6], commonly referred to as \u201cbiased\u201d or \u201cunfair\u201d decision-making, arises when predictive models perform unequally across subgroups[7,8] defined by sensitive attributes such as gender, race/ethnicity, and socioeconomic status[5]. For instance, studies have shown that Black patients are more frequently underdiagnosed with chronic obstructive pulmonary disease (COPD) compared to Hispanic white patients[4,9], highlighting the importance of addressing these disparities when defining COPD prevalence and improving population health[9]. This is just one of many documented inequities spanning healthcare domains such as COVID-19[3,6,7], stroke[10], emergency medicine[11-13], cardiovascular disease[14], cancer[15], and organ transplants[16].\nDespite growing efforts to develop fair models[5], most studies rely on single, centralized datasets. However, healthcare data are often distributed across multiple sites, such as electronic"}, {"title": "2. Materials and Methods", "content": "health records (EHRs) from different hospitals or mobile health data from users' devices[17,18]. Aggregating these diverse data sources could accelerate research and improve care quality[19], but privacy regulations pose significant barriers[20]. Federated learning (FL) or federated ML (FML), an ML technique, enables participants (i.e., clients) to collaboratively train models without exchanging data[18], making it an increasingly popular approach in medical research[21,22].\nWhile FL adoption is increasing, most studies focus primarily on overall predictive performance, often overlooking its potential to address algorithmic disparities[22]. Evidence suggests that standard FL algorithms struggle to reduce algorithmic biases[23,24], leading to models that retain their unfairness when transitioning from single-site analyses to FL settings. Although some studies have investigated these disparities within FL contexts, they predominantly rely on conventional ML datasets rather than real-world clinical data[24\u201326], raising concerns about the generalizability of their findings to actual healthcare settings.\nTo address these gaps, we propose Fair Federated Machine Learning (FairFML), a unified solution to promote fairness in FL. As a proof of concept, we use real-world out-of-hospital cardiac arrest (OHCA) data from the United States, focusing on gender disparities\u2014a critical concern for equity in OHCA care\u2014to demonstrate FairFML's effectiveness[12,27,28]. This case study shows that FairFML enables participating sites to develop fairer models for sensitive groups while maintaining prediction performance comparable to both local and centralized analyses."}, {"title": "2.1 Notation and problem setup", "content": "In this study, we adopt the notation introduced by Berk et al.[29]. Let $y \\in Y = [-1,1]$ represent the binary outcome, and $x \\in X = R^d$ denote the feature vectors. Each instance is categorized into one of two groups based on a sensitive variable, denoted as $X_{d+1}$. The joint distribution of $X$ and $Y$ is represented by $P$. We consider a training set $S = \\{(x_i, y_i)\\}_{i=1}^n$, consisting of $n$ independent and identically distributed (i.i.d.) samples drawn from $P$. This training set is divided into two groups, $S_1$ and $S_2$, based on the sensitive variable, with $n_1$ and $n_2$ representing the respective sizes of these groups, such that $n_1 + n_2 = n$.\nThe $\\lambda$-weighted fairness loss for a given model is defined as $L(w,S) + \\lambda f (w,S)$, where $L$ represents the standard model loss function, $w$ represents model parameters, and $\\lambda$ is a regularization parameter for the fairness penalty. Consistent with Berk et al.https://www.zotero.org/google-docs/?t7WY1i[29], we focus on a group fairness penalty, defined as\n$f(w,S) = \\frac{1}{n_1n_2}\\sum_{(x_i,y_i) \\in S_1, (x_j,y_j) \\in S_2}d(y_i, y_j) (w.x_i-w.x_j)$\nHere, $d(y_i, y_j) = 1[y_i = y_j]$ serves as the cross-group fairness weight."}, {"title": "2.2 Group fairness metrics", "content": "Demographic parity (DP), also known as statistical parity, and equalized odds (EO) are two widely used algorithmic fairness definitions for binary classifications:"}, {"title": "2.3 FairFML", "content": "We integrate the $\\lambda$-weighted fairness loss described in Section 2.1 into the FL model training, and the workflow of our proposed FairFML is illustrated in Figure 1. As shown, incorporating FairFML into any FL framework enhances the fairness of existing FL solutions by replacing the standard model loss function $L$ with the $\\lambda$ -weighted fairness loss function during FL model training. Since the fairness regularizer $f$ is convex[29], the combined objective function"}, {"title": "2.4 Dataset and Experiments", "content": "Our study population comprised OHCA patients treated by emergency medical services (EMS) providers, as recorded in the Resuscitation Outcomes Consortium (ROC) Cardiac Epidemiologic Registry (Epistry) (Version 3, covering the period from April 1, 2011, to June 30, 2015). The ROC, a North American database established in 2004, aims to advance clinical research on cardiopulmonary arrest[32]. Ethical approval was obtained from the National University of Singapore Institutional Review Board (IRB), which granted an exemption for this study (IRB Reference Number: NUS-IRB-2023-451).\nWe included patients aged 18 and older who were transported by EMS, achieved return of spontaneous circulation (ROSC) at any point prehospital, and had complete data on gender, race, etiology, initial rhythm, witness status, response time, adrenaline use, and neurological status. The primary outcome was neurological status at discharge, measured by the Modified Rankin Scale (MRS), where scores of 0, 1, or 2 were classified as a good outcome. Variables used for outcome prediction included age (in years), etiology of arrest (cardiac/non-cardiac), witness presence (yes/no), initial rhythm (shockable/non-shockable), bystander cardiopulmonary resuscitation (CPR) (yes/no), response time (in minutes), and adrenaline use (yes/no).\nWe conducted four sets of experiments to simulate real-world cross-site data by partitioning the full study cohort heterogeneously: (I) by race/ethnicity into four sites, (II) by age into four sites,"}, {"title": "3. Results", "content": "(III) by race/ethnicity into six sites, and (IV) by age into six sites. Continuous variables were standardized using the mean and standard deviation from the full cohort, and logistic regression was employed for outcome prediction. We focused on two representative FL frameworks, FedAvg and Per-FedAvg[33]. FedAvg is a foundational framework and the first proposed in the FL domain[30,34], while Per-FedAvg is a widely adopted solution for personalized FL. The latter is particularly relevant in healthcare data analysis, as it allows researchers to determine whether FL can offer localized benefits that enhance the performance of existing models for individual institutions[22].\nFor each scenario, we conducted three types of analyses: (1) a central model trained on the full cohort and local models trained independently at each site, (2) federated logistic regression using FedAvg and Per-FedAvg, and (3) fairness-enhanced federated logistic regression using the proposed FairFML method with the two FL frameworks\u2013FairFML (FedAvg) and FairFML (Per-FedAvg). We evaluated model performance using the area under the receiver operating characteristic curve (AUROC) and four fairness metrics, as described in Section 2.2, with gender as the sensitive variable, using the \u2018Fairlearn' package[35].\nFigure 2 illustrates the partitioning of 7,425 individual episodes into four or six sites following the cohort formation process, with a 7:3 split for training and testing data. eTable 1 in the Supplementary Materials summarizes the baseline characteristics of the overall cohort and each site under different experimental conditions. In cases I and III, where clients were partitioned by race/ethnicity, significant distribution differences were observed, with the proportion of White"}, {"title": "4. Discussion", "content": "individuals ranging from 88.9% to 48.8%. In cases II and IV, where clients were partitioned by age, the mean age varied considerably, ranging from approximately 80 to 60 years. Outcome prevalence varied from 7.5% to 12.6%, and other variables also exhibited heterogeneous distributions, reflecting the real-world demographic differences across regions.\nDetails of the experimental setup, including the tuning of 2 and y and other general hyperparameters for FL, are provided in eFigure 2 and eTable 2 in the Supplementary Materials. We assessed the performance of the federated model developed using FairFML by comparing it to the centralized model, local models trained independently at each site, and general FL models (FedAvg and Per-FedAvg). Figure 3 presents the average performance of each model across the testing datasets for all sites in each experimental scenario, with the percentage change in fairness metrics relative to the centralized model, displayed above each bar. Detailed results for each model and site are available in eTable 3, showing that at the client level, the fairness metrics generally improved across all clients, consistent with the average results across sites, with only a minor trade-off in prediction performance.\nKey findings from Figure 3 and eTable 3 include: 1) FairFML consistently outperformed other models in fairness, improving all four fairness metrics by up to 65% compared to the centralized model, while maintaining predictive performance nearly identical to other models, with a maximum decrease in AUC of less than 0.02 relative to the centralized model; and 2) although FedAvg and Per-FedAvg occasionally outperformed central and local models on specific fairness metrics for certain clients, their improvements were less substantial. In contrast, FairFML consistently demonstrated significant and superior performance across all fairness metrics."}, {"title": "5. Conclusion", "content": "FairFML offers a unified, model- and framework-agnostic solution[22,34] for enhancing fairness in FL collaborations. Its adaptability to various FL frameworks and ML models-ranging from traditional statistical regressions and support vector machines to deep neural networks\u2014makes it highly versatile for clinical and biomedical prediction tasks[22]. By reducing algorithmic disparities, as demonstrated in our proof-of-concept case study focusing on gender disparities in cardiac arrest outcome prediction, FairFML effectively mitigates algorithmic bias when integrated with standard FL frameworks.\nGiven that clients in cross-institutional FL collaborations often expect direct benefits for their research or clinical practice[19,22], it is essential to evaluate models against both client-level (local) and central models. Our results show that FairFML consistently outperforms traditional FL and local models in terms of fairness between the two genders, as seen in Figure 3 and eTable 3. While previous studies have highlighted Per-FedAvg's ability to enhance client-level predictions through meta-learning-aided personalization[33], our findings suggest that FairFML (FedAvg) sometimes achieves better fairness improvements than FairFML (Per-FedAvg). This suggests that fairness improvements by FairFML are not exclusively dependent on the FL framework used. Thus, FairFML has strong potential for delivering enhanced fairness, with opportunities for further refinement in balancing fairness and client-level performance.\nAlthough gender disparities in cardiac arrest are a key focus, they are not the only relevant partition for group fairness in this context[36]. Studies show that individuals from Black,"}, {"title": "Limitations", "content": "Hispanic, or lower socioeconomic status backgrounds experience pronounced disparities throughout the resuscitation pathway[37]. Our findings, presented in eTable 4 of the Supplementary Materials, highlight significant variations in gender disparities across racial and ethnic groups. This underscores the need to address multi-group fairness (i.e., multiple intersecting sensitive variables[38]) to further mitigate unfairness. Despite more than a decade of discussion on multi-group fairness[39,40], it has received limited attention in FL settings. The challenge becomes more complex when group partitions are imbalanced or, in extreme cases, when certain groups are entirely absent from some clients. In such cases, advanced approaches, such as synthetic data[41,42], may offer the promising direction for future research.\nFairness in FL is typically framed around client resource allocation and ensuring performance uniformity across clients[43,44], commonly referred to as \u201csystem fairness\u201d[45]. This is particularly relevant in scenarios involving client selection to optimize convergence speed and reduce computational costs[46], as seen in cross-device FL[18]. However, cross-institutional FL[18] which is more prevalent in healthcare settings and often involves fewer clients (typically fewer than five)[22] \u2014the focus shifts to algorithmic fairness. While various strategies have been proposed to enhance fairness in clinical models, including privacy-preserving collaborations, McCradden et al.[47] caution that relying solely on technical solutions may inadvertently harm vulnerable groups. Thus, FairFML should be viewed as a starting point, followed by further analysis of downstream patient impacts, rather than assuming that fairness can be achieved solely through ML/AI metrics[47].\nOur clinical case study uses simulated partitioned clients for FL experiments as a proof of concept, given the lack of suitable collaborators for a real-world setup. Although we simulate cross-site data heterogeneity, real-world collaborations may introduce additional complexities, particularly regarding model heterogeneity[22,34]. Further research is required to validate FairFML's robustness and applicability in real-world cross-institutional collaborations."}, {"title": "Future work", "content": "While this study focused on group fairness, our proposed method can be extended to improve individual fairness[48] by incorporating an individual fairness penalty within the convex framework[29]. A hybrid penalty combining both group and individual fairness metrics could offer a more comprehensive approach to mitigating unfairness in clinical research. Future work aims to explore these extensions and validate FairFML in real-world settings to ensure its robustness and applicability across diverse clinical environments.\nFairFML effectively mitigates bias and enhances fairness in model co-training across multiple healthcare data owners while preserving privacy. In our proof-of-concept case study using real-world emergency medicine data, FairFML consistently outperformed other models in addressing fairness disparities without compromising predictive performance. These findings highlight FairFML's robustness and practical applicability to heterogeneous clinical data, making it a promising solution for real-world healthcare settings."}, {"title": "Author Contribution", "content": "Siqi Li: Conceptualization, Project administration, Supervision, Method design, Algorithm development, Formal analysis, Data curation, Writing \u2013 original draft. Qiming Wu: Algorithm development, Formal analysis, Software, Writing \u2013 original draft. Xin Li: Data analysis, Writing \u2013 original draft. Di Miao: Formal analysis, Writing \u2013 original draft. Chuan Hong: Algorithm development, Writing \u2013 review & editing. Wenjun Gu: Data curation, Investigation, Writing \u2013 review & editing. Yohei Okada: Investigation, Validation, Writing \u2013 review & editing. Michael Hao Chen: Investigation, Validation, Writing \u2013 review & editing. Mengying Yan: Investigation, Validation, Writing \u2013 review & editing. Yuqing Shang: Algorithm development, Investigation, Writing \u2013 review & editing. Yilin Ning: Investigation, Validation, Writing \u2013 review & editing. Marcus Eng Hock Ong: Investigation, Validation, Writing \u2013 review & editing. Nan Liu: Conceptualization, Supervision, Funding acquisition, Resources, Writing \u2013 review & editing."}, {"title": "Funding", "content": "This work was supported by the Duke/Duke-NUS Collaboration grant. The funder of the study had no role in the study design, data collection, data analysis, data interpretation, or writing of the report."}, {"title": "Competing Interest", "content": "NL, SL and MEHO hold a patent related to the federated scoring system. The other authors declare no competing interests."}, {"title": "Code Availability", "content": "The Python code for FairFML is available at https://github.com/nliulab/FairFML."}]}