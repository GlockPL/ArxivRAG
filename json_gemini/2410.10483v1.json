{"title": "ADVANCING NEWBORN CARE: PRECISE BIRTH TIME DETECTION USING AI-DRIVEN\nTHERMAL IMAGING WITH ADAPTIVE NORMALIZATION", "authors": ["Jorge Garc\u00eda-Torres", "\u00d8yvind Meinich-Bache", "Anders Johannessen", "Siren Rettedal", "Vilde Kolstad", "Kjersti Engan"], "abstract": "Around 5-10% of newborns need assistance to start breath-\ning. Currently, there is a lack of evidence-based research,\nobjective data collection, and opportunities for learning from\nreal newborn resuscitation emergency events. Generating\nand evaluating automated newborn resuscitation algorithm\nactivity timelines relative to the Time of Birth (ToB) offers\na promising opportunity to enhance newborn care practices.\nGiven the importance of prompt resuscitation interventions\nwithin the \"golden minute\" after birth, having an accurate\nToB with second precision is essential for effective subse-\nquent analysis of newborn resuscitation episodes. Instead,\nToB is generally registered manually, often with minute pre-\ncision, making the process inefficient and susceptible to error\nand imprecision. In this work, we explore the fusion of\nArtificial Intelligence (AI) and thermal imaging to develop\nthe first AI-driven ToB detector. The use of temperature\ninformation offers a promising alternative to detect the new-\nborn while respecting the privacy of healthcare providers and\nmothers. However, the frequent inconsistencies in thermal\nmeasurements, especially in a multi-camera setup, make nor-\nmalization strategies critical. Our methodology involves a\nthree-step process: first, we propose an adaptive normaliza-\ntion method based on Gaussian mixture models (GMM) to\nmitigate issues related to temperature variations; second, we\nimplement and deploy an AI model to detect the presence of\nthe newborn within the thermal video frames; and third, we\nevaluate and post-process the model's predictions to estimate\nthe ToB. A precision of 88.1% and a recall of 89.3% are re-\nported in the detection of the newborn within thermal frames\nduring performance evaluation. Our approach achieves an\nabsolute median deviation of 2.7 seconds in estimating the\nToB relative to the manual annotations.", "sections": [{"title": "1. INTRODUCTION", "content": "Approximately 5-10% of newborns experience insufficient\nbreathing at birth and need some level of assistance to achieve\ncardiopulmonary stability [8]. Birth asphyxia is one of the\nprimary causes of neonatal mortality and morbidity, respon-\nsible for an estimated 900,000 deaths annually [21]. In\naccordance with neonatal resuscitation guidelines [16, 33],\nnewborn resuscitation is time-critical, and immediately pro-\nviding ventilation can significantly mitigate the risk of death\nand long-term damage related to birth asphyxia [5]. Effec-\ntive resuscitative interventions should be initiated within the\nso-called \"golden minute\". This timeframe refers to the first\nminute after the Time of Birth (ToB), defined as the moment\nwhen the head, torso, and nates (gluteus) of the newborn\nbecome distinctly visible outside the mother's perineum [3].\nThe analysis of newborn resuscitation videos has shown\npromising results for evaluation, debriefing, and training pur-\nposes [26, 12]. Nevertheless, in order to accurately exam-\nine the impact of guidelines and determine the optimal tim-\ning and duration of the different Newborn Resuscitation Al-\ngorithm Activities (NRAA), a substantial number of docu-\nmented NRAA episodes with detailed timeline information is\nessential. Manually annotating this data is time-consuming,\ninefficient, and raises privacy concerns. Besides, newborn re-\nsuscitation videos recorded at a resuscitation station lack piv-\notal information on the ToB and the duration of transferring\nnewborns to the resuscitation station. To properly evaluate\nthe treatment given, NRAA timelines must be relative to an\naccurate ToB, recorded with second precision. Currently, in\nclinical practice, ToB is recorded manually, often with minute\nprecision, which is prone to error and imprecision.\nThe potential of Artificial Intelligence (AI) has brought\nsignificant positive changes to the field of neonatology, im-\nproving neonatal applications such as imaging interpretation,\nprediction modeling using different health records, real-time\nmonitoring integration, and documentation [1, 28, 9]. AI has\nalso demonstrated the capability to automate NRAA timelines"}, {"title": "2. DATA MATERIAL", "content": "Data were collected at Stavanger University Hospital (SUS),\nNorway, using a passive thermal module per room mounted\nin eight delivery rooms and one operation theatre. Each ther-\nmal module, provided by Mobotix [20], consisted of a thermal\nsensor Mx-O-SMA-TPR079 connected to an Mx-S16B cam-\nera. The sensor was mounted to the ceiling, centered above\nthe head of the mother in the delivery rooms, as illustrated\nin Figure 2. At the operating theatre, the thermal sensors are\nmounted to the ceiling minimizing blocking of the view from\nthe operating light and other equipment. The dataset contains\na total of 137 birth videos recorded with thermal cameras,\nwith nearly equal distribution between delivery rooms and the\noperation theatre.\nThe data collection process was done semi-automatically.\nThermal cameras initiated recording once a temperature\nvalue exceeded 30 degrees Celsius within a single pixel.\nThis threshold was set at 30 degrees to streamline recording\nsystem operations, optimize data acquisition, and guaran-\ntee the detection of human presence. At delivery, ToB was\nmanually registered by midwives or nurse assistants using\na mobile application called Liveborn Observation App [4]\n(Laerdal Medical AS, Stavanger, Norway). This application\nwas specifically designed to document post-birth events in\nresearch projects and typically registers more accurate ToB"}, {"title": "3. BACKGROUND & PROBLEM FORMULATION", "content": "The skin temperature of a newborn immediately after birth is\nnotably higher than the skin temperature of other people in the\nroom. Effectively capturing and exploiting this information is\ncrucial for the detection of ToB. In Kolstad et al. [14], we\ndemonstrated the capacity of thermal imaging to facilitate the\nrecognition of ToB and cord clamping during manual visual\ninspections of birth videos captured by thermal cameras.\nDespite these promising results, automating ToB detec-\ntion is more challenging. In an ideal scenario where a sin-\ngle thermal camera is used, all thermal images would be cap-\ntured under uniform conditions, with no variations in bright-\nness or contrast. However, due to the inherent nature of ther-\nmal imaging and the fluctuating environmental conditions in a\nbirth setting, maintaining consistency with the raw output of a\nthermal camera remains almost impossible. While most ther-\nmal imaging applications are either image-based, conducted\nin controlled environments with a single camera setup, or dis-\nregard the absolute temperature values, our study is different.\nWe utilize multiple independent cameras without controlling\nthe environment (to avoid interfering with medical profes-\nsionals' work), but we focus on the sequential relationship\nbetween frames and the physical significance of relative tem-\nperature values.\nTo address the automated ToB detection, we unsuccess-\nfully attempted to apply basic image processing techniques in\na preliminary test using a few real births recorded using a ther-\nmal camera. Several environmental factors during birth had"}, {"title": "4. METHODOLOGY", "content": "In this paper, we present the first ToB detector based on AI\nand thermal imaging. As presented in Figure 1, our method-\nology involves three steps. First, we propose an adaptive nor-\nmalization method based on GMM to provide a more uni-\nform representation of the data. Second, we formulate a bi-\nnary image classification problem to detect the presence of the\nnewborn within the thermal video frames using Convolutional\nNeural Networks (CNN). Third, we evaluate and post-process\nthe model's predictions to estimate the ToB.\nGoing forward, let $V \\in \\mathbb{R}^{N \\times H \\times W}$ denote a single-\nchannel thermal video with N thermal frames and a spatial\nresolution of H \u00d7 W. Let also I(n) define the thermal frame\nat index n = 0, 1, ..., N \u2013 1 so that:\n$V = \\{I(0), I(1), ..., I(N - 1)\\}$\n(1)"}, {"title": "4.1. Step I. Gaussian Mixture Model Normalization", "content": "The motivation behind the use of Gaussian Mixture Models\n(GMM) is to address the variability problem in temperature\nvalues. As outlined in Section 3, some limitations make tra-\nditional normalization approaches unfeasible. Therefore, we\naim to find a video-specific temperature range of interest that\nencompasses the human skin temperature. This adaptive ap-\nproach enables us to normalize temperature values within the\nestimated range in each thermal video, focusing on the most"}, {"title": "4.1.1. Gaussian Mixture Model", "content": "A Gaussian mixture model [24] is a parametric proba-\nbilistic density function represented as a weighted sum of\nM Gaussian component densities. Let $z \\in \\mathbb{R}^D$ denote\na continuous-valued data vector such as measurement or\nfeatures. The Gaussian component densities, denoted as\n$g(z | \\mu_i, \\Sigma_i)$, i = 1, ..., M, collectively form the GMM prob-\nabilistic density function as follows:\n$p(z) = \\sum_{i=1}^M a_ig(z | \\mu_i, \\Sigma_i)$\n(2)\nwhere $a_i$ represents the mixture weights determining the con-\ntribution of each component, and $X = \\{a_i, \\mu_i, \\Sigma_i\\}$ is the set"}, {"title": "4.1.2. Adaptive Normalization", "content": "We applied GMM under the assumption that thermal data can\nbe effectively represented within three primary regions: low\ntemperatures (background), mid temperatures (clothes, hair,\nbed sheet, etc), and high temperatures (human skin and very\nwarm elements). Therefore, M = 3 Gaussian components\nare used to model the density distribution of the thermal data.\nFigure 5 provides an example of generating a GMM using\na thermal video from our dataset. For each thermal video\nV, intensity values are extracted from frames spaced at 30-\nsecond intervals to capture a comprehensive representation.\nThese values are converted into temperature values and or-\nganized into a one-dimensional temperature value vector v,\nwhich is then used to fit the GMM p(v|\u5165). Among these"}, {"title": "4.2. Step II. Newborn Detection", "content": "Parameters optimization, computational efficiency, inference\nspeed, and accuracy are specific requirements when designing\nCNN architectures for lightweight model deployments, par-\nticularly on resource-constrained devices. Considering the fu-\nture implementation of our ToB detector in a real-time system\nin settings with limited computational resources, we prioritize\nthe selection of CNN backbones for feature extraction that\nachieves a careful balance between accuracy and efficiency.\nUtilizing existing architectures also allows us to use transfer\nlearning from pre-trained weights. In particular, we use Mo-\nbileNetV3 [13] and EfficientNetV2 [27], the latest iterations\nof MobileNet and EfficientNet. Since multiple variations of\nthese models exist, we focus on the smallest and largest ver-\nsions of MobileNetV3 (Small and Large) and the base ver-\nsions of EfficientNetV2 (B0, B1, B2, and B3) for comparison.\nFor each backbone, we adjust its input size to match our\nthermal video frame resolution. We also replace the original\narchitecture's classifier layer with one classifier containing\ntwo fully connected layers with 1024 and 2 units (binary\nclassification), respectively, with randomized weights and\na dropout layer of 0.5 in between during training. When\npre-trained weights are used, the single-channel input data is\nadapted to the same number of channels as RGB data. This\nis achieved by expanding the intensity (grayscale) values to\nthe three channels. Otherwise, backbone layers are adapted\nto match the single-channel input shape. In both cases, as im-\nplemented in MobileNetV3 and EfficientNetV2, input values\nare scaled to fall between -1 and 1 prior to being fed into the\nnetwork."}, {"title": "4.2.1. CNN Backbone", "content": "Parameters optimization, computational efficiency, inference\nspeed, and accuracy are specific requirements when designing\nCNN architectures for lightweight model deployments, par-\nticularly on resource-constrained devices. Considering the fu-\nture implementation of our ToB detector in a real-time system\nin settings with limited computational resources, we prioritize\nthe selection of CNN backbones for feature extraction that\nachieves a careful balance between accuracy and efficiency.\nUtilizing existing architectures also allows us to use transfer\nlearning from pre-trained weights. In particular, we use Mo-\nbileNetV3 [13] and EfficientNetV2 [27], the latest iterations\nof MobileNet and EfficientNet. Since multiple variations of\nthese models exist, we focus on the smallest and largest ver-\nsions of MobileNetV3 (Small and Large) and the base ver-\nsions of EfficientNetV2 (B0, B1, B2, and B3) for comparison.\nFor each backbone, we adjust its input size to match our\nthermal video frame resolution. We also replace the original\narchitecture's classifier layer with one classifier containing\ntwo fully connected layers with 1024 and 2 units (binary\nclassification), respectively, with randomized weights and\na dropout layer of 0.5 in between during training. When\npre-trained weights are used, the single-channel input data is\nadapted to the same number of channels as RGB data. This\nis achieved by expanding the intensity (grayscale) values to\nthe three channels. Otherwise, backbone layers are adapted\nto match the single-channel input shape. In both cases, as im-\nplemented in MobileNetV3 and EfficientNetV2, input values\nare scaled to fall between -1 and 1 prior to being fed into the\nnetwork."}, {"title": "4.2.2. Implementation Details", "content": "Binary cross-entropy [2] is employed as the loss function.\nSince it is a binary classification problem, we define the posi-\ntive prediction score for newborn detection as \u0177 for simplicity.\nTo mitigate the impact of class imbalance during the model\ntraining, we estimate the inverted class weight $w_c$ for each\nclass c \u2208 {0 (NB), 1 (ToB)}. Let S denote the total number\nof samples in the training set, C the number of classes, and $s_c$\nthe number of instances for a particular class c. The inverted\nclass weight is defined as follows:\n$w_c = \\frac{S}{Cs_c}$\n(5)\nWith balanced data, $w_c$ equals 1 for all classes as the number\nof instances per class is the same. Representing the true label\nof the data sample index q as $y_q$, the weighted binary cross-\nentropy loss function L is defined as:\n$L(y_q, \\hat{y}_q) = w_1y_q \\log(\\hat{y}_q) + w_0(1 - y_q) \\log(1 - \\hat{y}_q)$\n(6)\nWe deploy and evaluate several image-based models using\ndifferent CNN backbones as feature extractors. We train for\na maximum of 100 epochs, setting a batch size of 16 per\nGPU and early stopping. During hyperparameter searching,\nStochastic Gradient Descent (SGD) with a momentum of 0.9\nand Adaptive Moment Estimation (Adam) with $B_1$=0.9 and\n$B_2$=0.999 are utilized. Weight decay of 0.97 is applied in both\noptimizers every 1k steps, whereas the learning rate ranges\nfrom 0.1 to 1e-6 in powers of 10. We also use a moving av-\nerage with a 0.9999 decay rate. Two Teslas P100 GPUs with\n16 GB RAM are used.\nData augmentation is performed during training. We ap-\nply random left-right flipping at video frame level. Addition-\nally, brightness and contrast augmentation are applied to mod-"}, {"title": "4.2.3. Evaluation & Metrics", "content": "We assess precision and recall as evaluation metrics for binary\nexperiments [15]. We also employ the Matthew's Correlation\nCoefficient (MCC) as a more comprehensive metric, defined\nas:\n$MCC = \\frac{TN \\cdot TP - FP \\cdot FN}{\\sqrt{(TN+FN)(FP+TP)(TN+FP)(FN+TP)}}$\n(7)\nwhere TP, TN, FP, and FN are the True Positives, True Neg-\natives, False Positives, and False Negatives, respectively. TP\noccurs when the model correctly identifies positive instances,\nwhile TN denotes the correct identification of negative in-\nstances. FP arises when the model wrongly identifies negative\ninstances as positive, and FN occurs when positive instances\nare wrongly identified as negative.\nAdditionally, interpretability algorithms are crucial to\nunderstanding how deep learning models make decisions.\nTo enhance system transparency and reliability, we utilize\nthe gradient-based class activation map (GradCAM) algo-\nrithm [25]. This technique generates a coarse localization\nmap, highlighting significant regions within an image that\ncontribute the most to predicting a specific class."}, {"title": "4.3. Step III. Time of Birth Detection", "content": "In the previous step, we trained to detect frames that include\na visible newborn. However, the main goal of this study is"}, {"title": "5. EXPERIMENTS & DISCUSSION", "content": "In this section, we introduce the experimental details for im-\nplementing binary classification of newborn visibility on ther-\nmal video frames. We first define the test set by manually se-\nlecting 10 fully annotated thermal videos, ensuring the same\namount of recordings from both the delivery room and the\noperation theatre. This test set does not include scenarios in-\nvolving twins, i.e., each thermal video contains only a single\nbirth event. The remaining fully annotated thermal videos are\nallocated in the training and validation sets, performing an\n85%/15% division.\nNewborn detection is implemented as a binary classifi-\ncation task in order to identify the presence of the newborn\nwithin the thermal frames. However, identifying the newborn"}, {"title": "5.1. Experimental Details", "content": "In this section, we introduce the experimental details for im-\nplementing binary classification of newborn visibility on ther-\nmal video frames. We first define the test set by manually se-\nlecting 10 fully annotated thermal videos, ensuring the same\namount of recordings from both the delivery room and the\noperation theatre. This test set does not include scenarios in-\nvolving twins, i.e., each thermal video contains only a single\nbirth event. The remaining fully annotated thermal videos are\nallocated in the training and validation sets, performing an\n85%/15% division.\nNewborn detection is implemented as a binary classifi-\ncation task in order to identify the presence of the newborn\nwithin the thermal frames. However, identifying the newborn"}, {"title": "5.2. Newborn Detection (Step II)", "content": "We present the main results regarding newborn detection\nwithin thermal video frames in Table 2. The performance\ncomparison across various CNN backbones reveals interest-\ning insights. Firstly, the increased model complexity and\narchitecture optimization of EfficientNetV2 contributes to a\nmore effective representation of the underlying patterns in the\ndata while maintaining comparable computational efficiency\nto MobileNetV3. Secondly, the different \"Base\" versions\nof EfficientNetV2 provide similar metrics, so the selection\nentails a trade-off between model size, performance, and ef-\nficiency. In our study, EfficientNetV2B1 stands out as the\ntop-performing model, surpassing other backbones in Preci-\nsion and MCC without requiring high computational costs.\nOur newborn detector shows promising results in identi-"}, {"title": "5.3. Time of Birth Detection (Step III)", "content": "Given that our system relies on accurately identifying the\npresence of the newborn within a fixed timeframe for es-\ntimating the ToB, minimizing FP instances before birth is"}, {"title": "6. CONCLUSION", "content": "In this work, we introduce the first AI-driven ToB detector\nbased on thermal imaging. The inherent challenges of work-\ning with thermal data make reliance on absolute tempera-\nture values impractical. Moreover, difficulties arise when at-\ntempting to normalize thermal frames both within a single\nvideo and across multiple videos in a multi-camera setup.\nTo address these adversities, we propose an adaptive normal-\nization approach based on GMM, aiming to generate more\nconsistent relative temperature measurements around the hu-\nman skin temperature. Using the EfficientNetV2B1 backbone\nfine-tuned on thermal birth video frames as a feature extrac-\ntor, our system achieves a precision of 88.1% and a recall\nof 89.3% in detecting the presence of the newborn within\nthermal video. Furthermore, by postprocessing the prediction\nscores, our system accurately estimates ToB with an absolute\nmedian deviation of 2.7 seconds relative to the manual anno-\ntations.\nWhile the obtained results offer promising implications\nfor automated ToB detection, this study has some limitations.\nOur system exclusively relies on the spatial information of the\nthermal frames to estimate the ToB, omitting valuable insights\nfrom the temporal dimension. Besides, our proposed GMM\nnormalization is done after the full thermal video is gener-\nated. While this approach is suitable for debriefing, quality\nimprovement, and research purposes, it may not be adequate\nfor real-time decision support. In such scenarios, it becomes\ncrucial to locate the ToB in real time and periodically update\nthe parameters of the GMM based on the available thermal\nframes.\nFuture search efforts will prioritize addressing these lim-\nitations to enhance ToB estimates, focusing on the analysis\nof thermal video clips. Furthermore, extending our research\nto refine our proposed GMM normalization approach or ex-\nploring alternative normalization techniques will deepen our\nunderstanding of thermal data standardization. Lastly, we\nwill integrate the ToB detector with our previously developed\nNRAA classifier to culminate the creation of NewbornTime-\nline\na complete system capable of automatically generat-\ning detailed NRAA timelines, including activities and events"}, {"title": "Funding", "content": "The NewbornTime project is funded by the Norwegian Re-\nsearch Council (NRC), project number 320968. Additional\nfunding has been provided by Helse Vest, Fondation Idella,\nand Helse Campus, Universitetet i Stavanger. Study regis-\ntered in ISRCTN Registry, number ISRCTN12236970."}]}