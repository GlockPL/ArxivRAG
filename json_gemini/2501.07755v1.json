{"title": "Performance Optimization of Ratings-Based Reinforcement Learning", "authors": ["Evelyn Rose", "Devin White", "Mingkang Wu", "Vernon Lawhern", "Nicholas R. Waytowich", "Yongcan Cao"], "abstract": "This paper explores multiple optimization methods to improve the performance of rating-based reinforcement learning (RbRL). RbRL, a method based on the idea of human ratings, has been developed to infer reward functions in reward-free environments for the subsequent policy learning via standard reinforcement learning, which requires the availability of reward functions. Specifically, RbRL minimizes the cross entropy loss that quantifies the differences between human ratings and estimated ratings derived from the inferred reward. Hence, a low loss means a high degree of consistency between human ratings and estimated ratings. Despite its simple form, RbRL has various hyperparameters and can be sensitive to various factors. Therefore, it is critical to provide comprehensive experiments to understand the impact of various hyperparameters on the performance of RbRL. This paper is a work in progress, providing users some general guidelines on how to select hyperparameters in RbRL.", "sections": [{"title": "Introduction", "content": "Recent advancement in reinforcement learning (RL) has shown its potential to solve a wide range of tasks from mastering Atari games (Badia et al. 2020) to complex robotics tasks (Tang et al. 2024). One key requirement when employing the developed RL algorithms is the existence of properly defined reward functions. However, the design of proper reward functions for real world tasks is often difficult or even infeasible due to the large state and action spaces. Often, a slightly different reward function can lead to a completely different policy even if trained via the same RL algorithm. Hence, it is critical to investigate how to infer reward functions without manually assigning reward functions.\nOne typical approach to infer reward function is RL from human feedback (RLHF), which seeks to solicit human feedback that can be used to learn a reward signal. The main principle of RLHF is that the learned reward can explain the human feedback well. In other words, the estimated feedback from the learned reward should match the actual human feedback. Preference-based reinforcement learning (PbRL) (Christiano et al. 2017) is a well-know RLHF method that utilizes humans' preferences over sample pairs to learn the reward function. A well trained reward function can yield estimated preferences that match humans' preference well. In other words, if one segment is preferred over the other one in the pair by the human user, the cumulative reward of the former based on the learned reward should be larger than that of the later. Recently, rating-based reinforcement Learning (RbRL) (White et al. 2024), was developed that utilizes humans' ratings of individual samples to learn the reward function. It is shown in (White et al. 2024) that ratings can be easy and more acceptable to humans as comparing two similar samples in PbRL can be challenging. Similar to PbRL, a well trained reward function can yield estimated ratings that match humans' ratings. In other words, if one segment has a high rating, its cumulative reward should be relatively high. However, for two samples with the same ratings, their cumulative rewards can still be different. The allowable differences among samples in one rating depends on the human user's standard, which can be different from one to another. In addition to the different types of human feedback, RbRL has more hyperparameters than PbRL, which makes it more challenging yet important to optimize RbRL.\nIn this paper, we focus on addressing the gap of optimizing RbRL via leveraging some key principles in the existing machine learning literature. For example, existing studies on machine learing optimization include the use of different optimizers(Loshchilov and Hutter 2019), the inclusion of dropout (Srivastava et al. 2014), the use of different number of hidden layers (Shafi et al. 2006), and different activation functions and learning rates (Wu et al. 2019; Dubey, Singh, and Chaudhuri 2022). Moreover, we will also investigate other optimization techniques unique to RbRL, including, e.g., design of rating probability estimation functions, reward boundary selections, and confidence index k, to provide a comprehensive study on how the hyperparameters of RbRL should be selected for performance optimization.\nIn this paper, we will perform a preliminary set of experiments aimed at identifying the best optimization techniques to improve both consistency and performance of RbRL. We perform a total of 8 optimization techniques, including classic machine learning optimization techniques and optimization methods that are unique to RbRL. Our experiments show that by performing standard optimization techniques in some cases, we can drastically improve the performance by 100%. Meanwhile, we show that the optimized RbRL can retain a consistent performance across different num-"}, {"title": "Problem Statement", "content": "We frame our problem as a Markov Decision Process (MDP) with human ratings in a reward-free setting. This process can be represented by a tuple (S, A, P, \u03b3, n), where S is the state space, A is the action space, P denotes the state transition probability distribution, y is the discount factor mitigating the impact of infinite rewards in standard RL, and n is the number of rating classes. For each state s \u2208 S, the RL agent takes an action a \u2208 A sampled by the RL policy \u03c0. The goal of RL is to find an optimal policy \u03c0* that maximizes the cumulative rewards of the agent's behaviors. In standard RL, this process can be formulated as\n$$\u03c0^* = arg max_\u03c0 E_{(s_t, a_t)_{t=0}^{\\infty} \\sim p_\u03c0} [\\sum_{t=0}^{\\infty} \u03b3^t r(s_t, a_t)],$$"}, {"title": "Rating-based Reinforcement Learning (White et al. 2024)", "content": "To learn the reward from human ratings, let's define a segment (short video clip) as a set of state action pairs \u03c3 = ((\u03b4\u2080, \u03b1\u2080), (s\u2081, A\u2081), (s\u2082, A\u2082), ..., (s\u2099, a\u2099)). This segment is then mapped to an inferred reward \u00ee(s, a) via a reward predictor. To do this, the first step is to define cumulative reward for a segment R(\u03c3) = \u03a3\u1d62\u208c\u2080\u207f r(s\u1d62, a\u1d62) and is then normalized across a batch as\n$$\u0154(\u03c3) = \\frac{R(\u03c3) \u2013 min_{\u03c3'\u2208X} R(\u03c3')}{max_{\u03c3'\u2208X} R(\u03c3') \u2013 min_{\u03c3'\u2208X} R(\u03c3')},$$\nwhere X is the samples in a batch. Since the number of samples in a given class for the batch is known, we can find a lower and upper bound reward for a given class such that the total number of samples within the lower and upper bounds matches the number of samples in that class based on human ratings. By defining the lower and lower bound of a class i as Ri and Ri+1, the predicted probability for a sample in the rating class i is given by\n$$Q_\u03c3(i) = \\frac{e^{-k(\u0154(\u03c3)-R_i)(\u0154(\u03c3)-R_{i+1})}}{\\sum_{j=0}^{n-1} e^{-k(\u0154(\u03c3)-R_j)(\u0154(\u03c3)-R_{j+1})}},$$\nwhere k is the confidence index indicating the user's relative confidence. A higher k means a high confidence, vice versa. The reward predictor then infers the reward function via minimizing the cross entropy loss given by\n$$L(f) = - \\sum_{\u03c3\u2208X} \\sum_{i=0}^{n-1} \u03bc_\u03c3(i) log (Q_\u03c3(i)),$$\nwhere \u03bc\u03c3 (i) = 1 if the sample \u03c3 is rated in class i by the user and \u03bc\u03c3 (i) = 0 otherwise.\nOnce the reward function is learned, many existing RL algorithms, such as PPO, A2C, SAC, can be used to train policies based on the learned reward function."}, {"title": "Optimization", "content": "Let's now focus on the optimization method to be performed on the RbRL algorithm. To simplify our studies, we have used synthetic humans in our studies, where synthetic ratings are generated based on the true environment reward. For example, if the maximum true environment reward for a sample is 50 and the number of rating classes is 2, the threshold between rating class 0 (\"Bad\") and rating class 1 (\"Good\") can be any value between 0 and 50. A higher threshold means a higher bar for the samples in rating class 1 and vice versa. Our first optimization method is to quantify the impact of the reward boundary on the performance of RbRL.\nThe second optimization method is the design of a different function that predicts the probability of a sample in the rating class i. In the existing approach (3) proposed in (White et al. 2024), the probability is determined based on the distances between the cumulative reward of the current sample and the lower/upper bounds. Different from this method, we here propose a new form that is based on the distance between the cumulative reward of the current sample and the average of the upper and lower bounds. In other words, the new method is a direct measure of the deviation of the cumulative reward of the current sample from the middle of the rating class i. Specifically, the new predicted probability is given by\n$$Q_\u03c3(i) = \\frac{e^{-k(\u0154(\u03c3)-\\frac{R_i+R_{i+1}}{2})^2}}{\\sum_{j=0}^{n-1} e^{-k(\u0154(\u03c3)-\\frac{R_j+R_{j+1}}{2})^2}},$$"}, {"title": "Results", "content": "Our experimental set up utilizes the codebase for Rating-based Reinforcement Learning (White et al. 2024), a variation of BPref (Lee et al. 2021). We use Proximal Policy Optimization (Schulman et al. 2017) as our RL algorithm, where we replace the true environment reward with a predicted reward based on the reward predictor. For the first 32,000 timesteps, an entropy based exploration reward is used to gather initial segments which will be rated. After the initial 32,000 timesteps, only the predicted reward is used in learning. In our experiments, a total of 1,000 ratings was used for Walker. 2,000 ratings were used for both Quadruped and Cheetah."}, {"title": "Discussion, Limitations, and Future Work", "content": "In this paper, we have shown that RbRL developed in (White et al. 2024) has room to yield better performance by optimizing its hyperparameters. Currently, our investigation into optimizing performance of RbRL is still in progress. However, our preliminary results show potential of selecting optimal hyperparameters to achieve better and more consistent performance. Our work serves as an initial study towards developing optimal RL methods from human cognitive inspired learning strategies, such as RbRL discussed in this paper. Yet, the optimization of the developed approach using the classic machine learning optimization techniques has not been properly investigated. Some interesting open questions include: (1) Is there a method which can be used to mitigate performance harm done by mislabeled segments? (2) Do these optimization techniques transfer from traditional robotics tasks to language tasks? (3) Can we leverage both Preference-based Reinforcement Learning (PbRL) and RbRL in the same learning schema to improve consistency in performance? (4) Conduct human subject tests to validate these methods for users with different backgrounds."}]}