{"title": "Tabular Data Synthesis with Differential Privacy: A Survey", "authors": ["MENGMENG YANG", "CHI-HUNG CHI", "KWOK-YAN LAM", "JIE FENG", "TAOLIN GUO", "WEI NI"], "abstract": "Data sharing is a prerequisite for collaborative innovation, enabling organizations to leverage diverse datasets for deeper insights. In real-world applications like FinTech and Smart Manufacturing, transactional data, often in tabular form, are generated and analyzed for insight generation. However, such datasets typically contain sensitive personal/business information, raising privacy concerns and regulatory risks. Data synthesis tackles this by generating artificial datasets that preserve the statistical characteristics of real data, removing direct links to individuals. However, attackers can still infer sensitive information using background knowledge. Differential privacy offers a solution by providing provable and quantifiable privacy protection. Consequently, differentially private data synthesis has emerged as a promising approach to privacy-aware data sharing. This paper provides a comprehensive overview of existing differentially private tabular data synthesis methods, highlighting the unique challenges of each generation model for generating tabular data under differential privacy constraints. We classify the methods into statistical and deep learning-based approaches based on their generation models, discussing them in both centralized and distributed environments. We evaluate and compare those methods within each category, highlighting their strengths and weaknesses in terms of utility, privacy, and computational complexity. Additionally, we present and discuss various evaluation methods for assessing the quality of the synthesized data, identify research gaps in the field and directions for future research.", "sections": [{"title": "1 INTRODUCTION", "content": "Data sharing is essential as it drives innovative collaboration and enables informed decision-making across various domains. Numerous public data-sharing platforms, including Kaggle [4], Data.gov [3], and the UCI repository [45],"}, {"title": "2 BACKGROUND KNOWLEDGE", "content": ""}, {"title": "2.1 Tabular data synthesis", "content": ""}, {"title": "2.1.1 Concepts", "content": "This Section briefly presents the concept of tabular data synthesis.\nTabular data. Tabular data is data organized in a structured format with rows and columns, similar to a table. Each row represents a specific record or instance, such as a customer, transaction, or observation, and each column corresponds to a variable or attribute, such as name, age, or product price.\nData synthesis. Data synthesis is the process of creating artificial datasets that replicate the structure, statistical properties, and relationships of real-world tabular data. The primary goal is to generate synthetic data that preserves the essential characteristics of the original data while ensuring privacy and enabling safe data sharing. This process is particularly valuable for scenarios where privacy concerns or data scarcity limit the use of real datasets.\nCentralized data synthesis. Centralized data synthesis refers to the process of generating synthetic datasets where all the original data is collected, stored, and processed in a single, centralized location. A data curator or central authority typically holds the entire dataset and applies data synthesis techniques to produce a synthetic version.\nDistributed data synthesis. Distributed data synthesis involves generating synthetic datasets from data that is stored and processed across multiple locations or nodes. Each data owner retains control over their local data and collaborates with other parties to jointly create synthetic data without centralizing the original datasets."}, {"title": "2.1.2 Challenges", "content": "Tabular data synthesis poses several challenges, particularly due to the complexity and diversity of tabular datasets compared to other data types like images or text. Some of the key challenges include:\nData heterogeneity. Tabular data often includes a mix of different data types (e.g., categorical, numerical and ordinal). Modelling the relationships between these different types of features is challenging.\nData distribution complexity. First, many features in tabular data may not follow a simple distribution, and some may have multiple peaks (multi-modality), making it hard to model these distributions accurately. Additionally, real-world tabular data is often imbalanced, with skewed distributions in certain classes, it is challenging to represent the imbalance.\nFeature dependencies. The relationships between features can be highly complex (e.g., non-linear correlations). Capturing these relationships accurately in the synthetic data is challenging.\nGenerating synthetic tabular data with differential privacy introduces additional challenges beyond those involved in general tabular data synthesis. These include the high sensitivity and dimensionality of the tabular data, which require significant noise to ensure privacy, and the cumulative noise can substantially degrade the overall quality of the synthetic data. Therefore, It is challenging to balance the utility and privacy of the synthetic data."}, {"title": "2.1.3 Privacy disclosure in synthetic data", "content": "One of the primary purposes of generating synthetic data is to enhance data privacy, facilitating data sharing without compromising users' sensitive information. However, research [11, 127] have shown that sensitive information can still be disclosed if an attacker possesses some background knowledge about the victim. Several types of attacks can be performed on synthetic data, including the following:\nRe-identification attack. Re-identification [75] attacks typically exploit auxiliary information or background knowl-edge that an attacker possesses about the individuals in the dataset. By correlating this additional information with the synthetic data, the attacker can identify specific individuals and extract sensitive information.\nInference attack. An inference attack occurs when an attacker deduces sensitive information about individuals from synthetic data by exploiting statistical properties, patterns, or background knowledge. This type of attack does not necessarily re-identify individuals but can still extract confidential information. It includes membership [153] and attributes [13] inference and correlation exploitation."}, {"title": "2.2 Differential privacy", "content": ""}, {"title": "2.2.1 Definition of differential privacy", "content": "Differential privacy, introduced by Dwork et al. [47] in 2006, is a provable privacy concept. It ensures that changing one person's data does not have a big effect on the result, making it hard to tell if that person was included or not, all while still getting useful insights from the data. In differential privacy, \u0454 plays a crucial role as a privacy parameter that controls the trade-off between privacy and accuracy. It sets the upper limit on how much the algorithm's output can differ when one individual's data is changed. A smaller e value means stronger privacy protection and also tends to introduce more noise into the data, potentially reducing the accuracy of the results. In contrast, a larger e provides more accurate results but weaker privacy guarantees. (\u03b5, \u03b4)-differential privacy is also known as approximate differential privacy, which is an extension of the standard differential privacy framework. It introduces an additional parameter, d, which allows for a small probability of a stronger privacy loss than e would permit. The pure e-differential privacy is a special case of the approximate differential privacy when 8 = 0."}, {"title": "2.2.2 Differential privacy mechanisms", "content": "Differential privacy is achieved by introducing randomness into statistical computations. Commonly used mechanisms include the Laplace and Gaussian mechanisms for numerical data and the Exponential mechanism for categorical data.\nLaplace Mechanism. The Laplace Mechanism is one of the most widely used methods in differential privacy. It protects privacy by adding noise to the output of a function, and this noise is drawn from the Laplace distribution. The scale of noise is determined by the sensitivity of the function, which measures how much the function's output could change by altering a single individual's data and the privacy parameter e. The Laplace mechanism preserves e-differential privacy.\nGaussian Mechanism. The Gaussian Mechanism works similarly to the Laplace Mechanism but adds noise drawn from the Gaussian distribution instead of the Laplace distribution. The noise magnitude is determined by both the sensitivity of the function and the privacy parameters, \u0454 and 8. The Gaussian mechanism satisfies (\u20ac, \u03b4)-differential privacy.\nExponential Mechanism. Unlike the Laplace or Gaussian mechanisms, the Exponential Mechanism introduces noise to the selection process by assigning a probability to each possible output based on a utility function that measures how desirable each option is. The probability of selecting an output increases exponentially with its utility value, ensuring that the most useful outputs are more likely to be chosen, but with a privacy-preserving layer of randomness. The exponential mechanism preserves e-differential privacy."}, {"title": "2.2.3 Composition properties", "content": "Differential privacy has several composition properties that facilitate the analysis of more complex privacy-preserving algorithms.\nParallel Composition. The Parallel Composition Theorem [48] states that if a method includes m independent randomized functions, each providing its own e-differential privacy guarantee, and if each function operates on a distinct portion of the dataset, then the overall privacy guarantee for the entire method is determined by the function with the highest e. In other words, the privacy level of the combined method is as strong as the function with the weakest privacy protection (the largest \u20ac).\nSequential Composition. The Sequential Composition Theorem [48] states that when multiple differentially private mechanisms are applied sequentially to the same dataset, the overall privacy loss accumulates. Specifically, if m independent functions each provide e-differential privacy guarantees and are applied to the same data, then the combined privacy guarantee is the sum of the individual privacy guarantees. This means that the total privacy loss increases with each additional query or operation on the same dataset, as the more queries are made, the more information about the dataset could potentially be revealed.\nAdvanced Composition. The Advanced Composition Theorem [48] provides a refined privacy guarantee when applying differentially private mechanisms multiple times. In the context of m-fold adaptive composition, where multiple queries are applied adaptively to the same dataset, the theorem shows that the privacy loss grows slower than the basic composition rule suggests. Specifically, for (e, \u03b4)-differentially private mechanisms, the total privacy guarantee becomes (e', md + \u03b4')-differential privacy, where e' = $\\epsilon\\sqrt{2m\\log(1/\\delta')} + m\\epsilon(e^{\\epsilon} - 1)$. This results in a tighter bound on the cumulative privacy loss, offering stronger privacy protection compared to the simple summing of the privacy parameters, especially when the number of queries m is large.\nMoments Accountant [5]. The Moments Accountant method is a powerful technique used to track and control cumulative privacy loss when applying differential privacy mechanisms multiple times, particularly in scenarios like deep learning. It works by measuring the privacy loss at each step. The approach extends beyond considering just the expectation, using higher-order moments to bound the tail of the privacy loss variable. For each -th moment, the Moments Accountant provides a tighter bound on the cumulative privacy loss compared to traditional composition rules."}, {"title": "2.2.4 Relaxations of differential privacy", "content": "Strict differential privacy often requires adding significant noise to the data or query results, which can substantially degrade the utility of the data. Relaxations, such as (\u20ac, \u03b4)-differential privacy, allow for a controlled trade-off between privacy and utility. In addition to (\u20ac, \u03b4)-differential privacy, several relaxations of differential privacy have been proposed, offering greater flexibility in handling data complexities and providing tighter bounds on privacy loss. In this section, we present two commonly used definitions in the literature, R\u00e9nyi differential privacy and zero-concentrated differential privacy.\nDefinition 1 ((\u03b1, \u03b5)-RDP [99]). A randomized mechansim M: D \u2192 R is said to have e-Renyi differential privacy of order a, or (\u03b1, \u03b5)-RDP for short, if for any adjacent D, D' \u2208 D it holds that\n$D_{\\alpha}(M(D)||M(D')) \\le \\epsilon$,\nwhere $D_{\\alpha}(M(D)||M(D'))$ is the Renyi Divergence of order a > 1, specifically,\n$D_{\\alpha}(M(D)||M(D')) = \\frac{1}{\\alpha - 1} \\log E_{x\\sim M(D)} [(\\frac{Pr[M(D) = x]}{Pr[M(D') = x]})^{\\alpha-1}]$ (1)\nRDP provides a flexible and refined method for measuring privacy loss using R\u00e9nyi divergence. This approach offers a more accurate measure of cumulative privacy loss and tighter bounds when multiple queries are composed. By parameterizing privacy loss with R\u00e9nyi divergence, RDP enables a smoother and more controlled tradeoff between privacy and utility, making it easier to achieve an optimal balance [78]. It simplifies the accumulation of privacy loss, reducing the complexity of maintaining privacy guarantees throughout multiple stages of data analysis.\nDefinition 2 (Zero-Concentrated Differential Privacy (zCDP) [31]). A randomized mechanism M: X\" \u2192 \u0423 is p-zero-concentrated differentially private if, for all x, x' \u2208 X\" differing on a single entry and all a \u2208 (1,\u221e),\n$D_{\\alpha}(M(x)||M(x')) \\le \\rho\\alpha$\nwhere $D_{\\alpha}(M(x)||M(x'))$ is the a- R\u00e9nyi divergence between the distribution of M(x) and M(x').\nzCDP uses concentration inequalities to provide a more refined measure of privacy loss. This allows for tighter control over the distribution of privacy loss, leading to more efficient privacy guarantees. One of the major advantages of zCDP is its simpler and more efficient composition properties. zCDP allows for the straightforward addition of privacy loss terms when composing multiple queries."}, {"title": "3 CENTRALIZED DATA SYNTHESIS WITH DP", "content": "Centralized data synthesis involves generating synthetic data from a centralized server or database, where all original data is collected and stored in a single location. In this setup, the data curator is trusted and aims to release differentially private synthetic data that prevents sensitive information from being disclosed to third parties."}, {"title": "3.1 Statistical method", "content": "Statistical methods emphasize maintaining the data distributions by modelling the joint distribution of attributes and subsequently generating samples from this model. It ensures that the synthetic data maintains identical statistical characteristics as those observed in the original dataset. We categorize the methods into several groups based on the statistical models used to generate the synthetic data.\n3.1.1 Copula. A copula is a statistical concept used to model and analyze the dependence structure between random variables. Copulas works by providing a mathematical framework to separate the modelling of the individual marginal distributions of random variables from the modelling of their joint dependence structure. Sklar's Theorem [114], formulated by Henry Sklar in 1959, is a fundamental theorem in copula theory. It states that any joint cumulative distribution function (CDF) of multiple random variables can be expressed in terms of their individual marginal CDFs and a copula function. Let F(x1), F(x2), ..., F(xn) be the marginal CDFs of n random variables x1, x2, ..., xn, and C(u1, u2, ..., u2) be a copula function of n variables. Then, the joint CDF F(x1, x2,..., xn) of these random variables can be represented as:\n$F(x_1, x_2, ..., X_n) = C(F(x_1), F(x_2), ..., F(x_n))$ (2)\nIn this equation, C(u1, U2, ..., u2) represents the joint distribution of the random variables with uniform marginal distributions. The general process of using copulas is to first transform your data into uniform marginal distributions, often through the use of CDFs of the individual variables. These uniform variables are then subjected to a copula function that captures how the variables' joint probabilities are related. Choosing a suitable copula function is a critical step. Copulas come in various families, including Gaussian, Clayton, Gumbel, and Frank, each tailored to different types of dependence structures.\nSelecting the appropriate copula family can be challenging. Li et al. [82] and Asghar et al. [17] employed a Gaussian copula to model the joint distribution of the data, drawing inspiration from the common observation [103] that many real-world high-dimensional datasets often exhibit Gaussian dependence structures. The main advantage of using a Gaussian copula lies in its efficiency, with a run-time that scales quadratically with the number of attributes. However, it assumes a linear relationship between variables [133]. This can be problematic when modelling financial assets or other"}, {"title": "3.1.2 Probabilistic graphical models", "content": "Probabilistic graphical models (PGMs) are a class of statistical models that use graphical representations to express and manipulate the joint probability distributions over a set of random variables. There are two main types of PGMs: Bayesian networks and Markov networks (also known as Markov random fields). Besides, Junction Trees play a crucial role in probabilistic inference within PGMs, primarily due to their efficiency in computing marginal probabilities, making them indispensable for handling complex probability distributions. Therefore, we categorize the methods into three main groups: methods based on Bayesian networks, approaches centered around Markov networks, and techniques leveraging Junction Trees.\nBayesian Network. A Bayesian network utilizes a directed acyclic graph-based structure, where nodes serve as representations for random variables or events, and the directed edges convey probabilistic relationships among these variables [117]. The joint distribution within the dataset can be estimated through the conditional distributions of attribute-parent pairs, as demonstrated below.\n$Pr[X_1, X_2, ..., X_a] = Pr[X_1]Pr[X_2|X_1] ... Pr[X_a|X_1,...,X_{d-1}]$, (3)\nwhere Xi represents the attribute of the dataset and d is the number of attributes.\nThe procedure for constructing a Bayesian Network and estimating the joint distribution involves several key steps. Initially, the selection of attribute-parent pairs is essential. Once these pairs are identified, the subsequent step is to create a collection of conditional distributions for these selected attribute-parent pairs. To enhance privacy and confidentiality, it is crucial to incorporate differential privacy noise into all these computational steps. The synthetic data can then be sampled from the approximated distribution.\nAttribute-parent pair selection. The choice of attribute-parent pair plays a critical role in shaping the estimated joint distribution, aiming to approximate the dataset distribution closely. PrivBayes [147] is the representative work. It employs the KL-divergence to assess the distance between two distributions. The smallest distance is achieved by maximizing the mutual information between the attribute denoted as Xi and its corresponding parent set \u03a6. \u03a4\u03bf accomplish this, they employ a greedy approach to select attribute-parent pairs with maximal mutual information,\nLessons learned and discussion. Bayesian networks have the flexibility to model both discrete and continuous variables, making them suitable for a wide range of data types. However, they heavily rely on assumptions about the conditional dependencies between variables [80]. If these assumptions are incorrect or incomplete, the synthetic data generated by the network may not accurately reflect the true data distribution. In addition, when estimating the joint distribution, there is a constraint placed on the number of marginals used, typically limited to a maximum of d marginals, where d represents the number of attributes. Consequently, capturing all important dependencies among the attributes becomes a challenging task. Furthermore, constructing Bayesian networks can be quite challenging, as it involves navigating a vast search space of potential network structures. Additionally, conducting inference in large Bayesian networks can be computationally expensive, posing challenges in efficiently generating synthetic data, especially for datasets with numerous variables."}, {"title": "Markov network", "content": "Markov network. Markov networks, also named Markov random fields, are one of the most widely used graphical models. In Markov networks, nodes represent random variables, while edges capture dependencies between them. Unlike Bayesian networks, Markov networks use undirected edges, making them well-suited for scenarios where relationships between variables are not easily represented by a causal hierarchy. The key feature of the Markov network is that it uses potential functions, denoted as \u00a2c(Xc) = e0(Xc), to describe the relationships between nodes (random variables) in the cliques (a subset of nodes fully connected) of the graphical model. O is the parameter corresponding to Xc. The joint probability distribution over all variables is defined as a product of these potential functions:\n$Pr[X_1, X_2,\\cdots, X_a] =  \\frac{1}{Z}  \\prod_{c \\in C} \\Phi_c(X_c)$  (4)\nwhere C is the set of all cliques and Z is the normalization factor.\nSeveral key steps are involved in constructing a Markov network. Initially, a dependency graph is created to represent the interactions between variables. The noisy marginals are generated by adding Laplace or Gaussian noise to the marginal statistics. Parameters for the potential functions are then determined using statistical inference techniques, such as maximum likelihood estimation. The joint distribution is then estimated as described in Eq. 4. Finally, the synthetic data can be generated using the joint distribution information.\nDependency graph construction. Constructing the dependency graph plays a crucial role in capturing attribute correlations. The general idea of constructing the dependency graph is to insert the edges to the graph when two attributes exhibit a high degree of correlation as measured by certain metrics, such as mutual information. Researchers often focus on modelling low-order correlations to mitigate the challenges posed by high-dimensional data and ensure differential privacy. Chen et al. [37] assessed the mutual information between pairs of attributes and added an edge to the graph if the noisy mutual information is larger than a threshold. Besides, they utilize sampling techniques to amplify the privacy level to reduce the noise added to the mutual information calculation. A potential problem of the algorithm developed in [37] is that the constructed dependency graph may result in quite sizable cliques. When this occurs, the marginal distribution of the clique becomes high-dimensional, demanding a significant amount of noise for privacy protection. Cai et al. [32] proposed an iterative approach that greedily selects the node pairs with larger noisy scores, and inter the edges into the graph, while ensuring that the maximal clique in the triangulated graph remains below a specified threshold, which ensures there is no over-size clique in the tree. McKenna et al. [98] proposed a method, named Private-PGM, that aims to infer a data distribution by formulating an optimization problem that aligns the produced marginals closely with the observed ones. It does not offer a way to determine which marginals should be selected initially. One key consideration when working with Private-PGM is that the quality of the synthetic data it generates is heavily reliant on the accuracy of the provided marginals. To mitigate this issue, in their following work [96], they constructed a maximum spanning tree to select the marginals combined with some selection rules, then employed Private-PGM as a post-processing tool for distribution estimation given noisy marginals.\nSynthetic data generation. Once the noisy marginals are obtained and the parameters of the potential functions are estimated, the joint distribution can be determined using Eq. 4. Following this, synthetic data can be sampled based on this joint distribution. However, as the dataset's dimensionality increases, Markov networks can become complex, resulting in computationally expensive when sampling from high-dimensional joint distributions. Junction tree is commonly applied to ensure efficient and precise inference [32].\nLessons learned and discussion. Compared to Bayesian networks, Markov networks offer greater flexibility in modelling relationships between variables. However, learning the optimal structure of a Markov network from data can be"}, {"title": "Junction tree", "content": "A junction tree is a data structure and associated algorithm used to facilitate efficient probabilistic inference in Markov networks. By constructing a junction tree, one can transform the Markov network into a tree-structured graphical model, which enables the calculation of joint and marginal distributions. Constructing a junction tree from a given dependency graph involves several steps. First, the graph must be triangulated by adding edges to ensure that any new edge between non-adjacent nodes forms a triangle, creating a chordal graph. Next, maximal cliques are identified within this triangulated graph; these are subsets of nodes that are fully connected and cannot be expanded without losing this connectivity. Then, for each pair of adjacent cliques, separator sets are determined. These sets consist of nodes that are common to both cliques and serve to separate them. Finally, the junction tree is constructed by representing each maximal clique as a node and each separator set as an edge connecting these nodes. When provided with an attribute set A = {A1, A2,, Ad}, the estimation of the joint distribution Pr[A] is achieved by leveraging the marginals of a collection of cliques C\u00a1 and their corresponding separators Sij = C\u012f \u2229 Cj. The calculation [37] is shown as follows.\n$\\Pr[A] = \\frac{\\Pi_{C_i \\in T} \\Pr [C_i]}{\\Pi_{S_{ij} \\in T} \\Pr [S_{ij}]}$ (5)\nOnce the junction tree is constructed, the marginal distributions of the cliques are determined. These are then used to estimate the overall joint distribution, enabling the generation of synthetic samples.\nNoisy marginal generation. The straightforward approach to get a noisy margin is to compute the marginals of the cliques within the tree and the marginals of the separators. Then, add differential privacy noise to each of these marginals. Since the separators represent the shared attributes between adjacent cliques, we can always derive the marginal distribution of the separators from the cliques. Therefore, the focus should be on obtaining noisy marginals for the cliques, which we can then use to get the separator marginals and estimate the joint distribution as described in Eq.5. The straightforward approach comes with a potential challenge. Specifically, when dealing with a junction tree containing a substantial number of cliques, a larger privacy budget is needed to obfuscate these marginals. This, in turn, can lead to a reduction in the accuracy of the estimated joint distribution. To mitigate this issue, similar strategies to those applied when deriving the separators have been considered. In particular, Chen et al. [37] proposed to merge the cliques to minimize statistical variance and obtain the noisy merged marginal distribution first. Subsequently, they derive the marginal distribution for each individual clique.\nSynthetic dataset generation. The joint distribution of the dataset, as defined by Eq. 5, presents computational challenges when it comes to sampling. To overcome this computational hurdle and efficiently sample data points from this"}, {"title": "3.1.3 Query-based method", "content": "This stream of methods typically involves a series of queries that calculate the proportion of instances meeting certain criteria. It aims to respond to a wide range of statistical queries by creating a synthetic dataset from which the answers are derived.\nHardit et al. [65] proposed a Multiplicative Weights Exponential Mechanism (MWEM) to approximate the distribution across the dataset's domain. It continually refines this approximation to enhance accuracy concerning both the private dataset and the desired query set by employing a combination of the Multiplicative Weights (MW) method [66] and the Exponential Mechanism. Specifically, it initializes a uniform distribution. Then, it identifies the query where the answer on the real data significantly differs from its corresponding answer on the approximate data, utilizing the Exponential Mechanism. It then increases the weights of the records that contribute positively to the approximation, while simultaneously decreasing the weights of records that have a negative impact.\nMWEM offers simplicity in implementation and usage, demonstrating commendable accuracy in practical applications [65]. However, its efficacy diminishes when handling high-dimensional data due to its requirement to manipulate an object whose size scales linearly with the data universe's size. Several follow-up research tries to make improvements on top of MWEM. For example, Mckenna et al. [98] scaled MWEM by replacing the multiplicative weights update step with a call to Private-PGM, which does not need to materialize the full contingency table. Later, they proposed AIM [97], which refined MWEM-PGM [98] by implementing a superior initialization strategy, allocating a fraction of the privacy budget to measure 1-way marginals. Additionally, they devised new selection criteria, such as imposing more significant penalties on larger marginals, when selecting queries. Furthermore, they introduced adaptive rounds and budget allocation adjustments, further enhancing the statistical accuracy of the method. Liu et al. [87] improved the accuracy of MWEM by adaptively reusing past query measurements and selecting the synthetic data distribution with maximum entropy.\nGaboardi et al. [55] model the synthetic data generation process as a zero-sum game, involving two players: a data player and a query player, which was first proposed by Hsu et al. [68]. The data player's action set is the entire data universe X, and their strategy involves approximating the distribution of the true database. Conversely, the query player's action set is the query class Q. Gaboardi et al. [55] inverted the roles of the two players in the game, introducing a method named DualQuery. Specifically, it comprises a query player employing the no-regret learning algorithm and a data player that identifies optimal responses through the resolution of an optimization problem using MWEM. The proposed method is effective in managing high-dimensional data, but it doesn't lead to an improvement in accuracy. Vietri et al."}, {"title": "3.1.4 Other methods", "content": "In addition to the mainstream statistical methods we have discussed, several other approaches have been proposed in the literature.\nMaximum entropy optimization. Maximum entropy is a powerful and widely used principle for estimating joint probability distributions of multiple random variables, which has been widely adopted in the research of k-way marginal estimation [108, 151], where k can be as large as the number of attributes. Privacy is ensured by introducing noise into the selected lower-dimensional marginals. The key idea behind maximum entropy is to find the most unbiased or least informative probability distribution that is consistent with the available information (e.g., the marginals) or constraints (e.g., the consistent or non-negative constraint). It seeks to maximize the entropy of the distribution while satisfying these constraints. Existing studies [40, 108, 121] primarily focus on estimating low-dimensional marginals. Applying these methods to high-dimensional data would be computationally intensive.\nProjection-based method. Xu et al. [138] proposed the use of the Johnson-Lindenstrauss transformation to project high-dimensional datasets into a lower-dimensional space. According to Johnson-Lindenstrauss's theory, this transformation yields a reduced representation that preserves pairwise distances between points. Privacy is enhanced by adding noise to the projected dataset. While this method maintains the Euclidean distances between high-dimensional vectors, the resulting dataset often differs in shape from the original dataset, which may not be desirable.\nGradually update method. Zhang et al. [152] presented an alternative method in their work called the \"gradually update method (GUM)\" to generate the synthetic dataset utilizing the selected noisy marginal. This method begins with the initialization of a random dataset and then proceeds to iteratively update its records to ensure consistency with the provided marginals. The resulting data records generated using this method tend to generally align more closely to the noisy marginal statistics than those generated by methods like probabilistic graphical models. However, it is worth noting that the dataset updating process can encounter convergence issues that achieving convergence may not always be straightforward, making it a challenging aspect of this approach."}, {"title": "3.2 Deep learning-based method", "content": "Deep learning (DL) methods are widely utilized for image synthesis, typically handling homogeneous numerical data. However, there is an increasing interest in applying these techniques to tabular data, with adaptations being made to suit this purpose.\n3.2.1 Autoencoder (AE). The autoencoder is a highly prevalent unsupervised learning model with the primary objective of acquiring a compact data representation, often employed for dimensionality reduction [24, 41]. This neural network architecture operates by simultaneously training an encoder, responsible for converting high-dimensional data points into lower-dimensional representations, and a decoder, tasked with reconstructing high-dimensional data from the compressed representation. This process allows the model to capture essential features within the data while minimizing the overall data volume.\nChen et al. [36] trained a differentially private autoencoder and made the encoder available for generating synthetic data by inputting the user's own data. The resulting synthetic data is then employed in downstream prediction tasks. However, the generated synthetic dataset is a low-dimensional data representation that differs from the original dataset in terms of data format. Abay et al. [6] applied the expectation maximization function to optimize the output of the encoded data and generate the synthetic data by decoding the encoded data.\nLessons learned and discussion. Such group methods cannot produce arbitrary synthetic datasets. Typically, autoencoders are used in conjunction with other generative models, such as GANs, serving as a preprocessing step to prepare the input data for the generative model.\n3.2.2 Variational Autoencoder (VAE). A Variational Autoencoder is a generative model that combines the principles of autoencoders and probabilistic modelling. Different from AE, which only tries to reduce the dimension, VAEs are designed to learn and represent complex, high-dimensional data in a lower-dimensional space while simultaneously capturing the underlying probability distribution of the data [107]. An encoder network in VAE maps the input data to a probability distribution in the latent space, while a decoder network generates data samples from this distribution. VAEs use variational inference to model uncertainty in the latent space, which allows for the generation of not just deterministic reconstructions but also diverse and expressive data samples."}, {"title": "3.2.3 Generative Adversarial Network (GAN)", "content": "Generative Adversarial Networks [62] are a class of deep learning models consisting of two neural networks, a generator and a discriminator, engaged in a competitive game. The generator creates synthetic data samples from random noise or other input sources. The discriminator evaluates both the synthetic data generated by the generator and the real data samples to differentiate them. The parameters of the generator and discriminator are updated based on the computed loss to improve the performance. The training process continues until the generator achieves the ability to generate synthetic data that closely resembles real data, and the discriminator reaches a point where its accuracy in distinguishing between the two levels is off. Due to the mode complexity of GAN, the training samples are easily remembered by the model [135]. To ensure privacy, two privacy models are considered in the literature, Differentially Private Stochastic Gradient Descent (DPSGD) [5] and Private Aggregation of Teacher Ensembles (PATE) [105].\nDPSGD. The widely used framework DPSGD has been applied to the GAN training process, specifically adding noise to the gradient of the discriminator during training to provide provable privacy protection. Frigerio [54] and Fan et al. [50] optimized this process by reducing the clipping bound for each iteration, in turn, reduces the introduced noise. Fan et al. [50] further improved the performance by privately selecting the best model across all training epochs. Besides, they train an embedding model to capture the relationships between features. However, to save the privacy budget, the embedding mode is required to train on a public dataset.\nPATE. PATE employs an ensemble of teachers trained on different subsets of data, ensuring that no single model has access to the entire sensitive dataset [88]. In a typical GAN framework, there is a single discriminator trained in direct opposition to the generator. PATE-GAN [77], however, introduces k teacher discriminators alongside a student discriminator. To ensure differential privacy, the student discriminator is only trained on records generated by the generator and labelled by the teacher discriminators. This framework limits the influence of individual samples on the model, providing strong differential privacy guarantees. However, the approach assumes that the generator can cover the entire real data space during training. If most synthetic records are labelled as fake, the student discriminator could be biased and fail to learn the true data distribution. Different from PATE-GAN, Long et al. [89] proposed an ensemble of teacher discriminators to replace the GAN's single discriminator. A differentially private gradient aggregator is incorporated to collect information from these teacher discriminators, which guides the student generator to improve synthetic sample quality. Instead of ensuring differential privacy for the discriminator, noise is added to the flow of information from the teacher discriminators to the student generator.\nIn addition, GANs can suffer from mode collapse [150], where they generate limited varieties of samples, especially in complex and high-dimensional data spaces like tabular datasets. This can result in a lack of diversity in generated samples, failing to represent the full complexity of the original data distribution [120]. Addressing these challenges often involves modifications to the GAN architecture and exploring novel training methods tailored for the tabula data generation task. Some efforts have been made in the literature, for example, Fan et al. [50] learned an embedding during the training process to capture the relationships between attributes using a public dataset. Conditional GAN [100] was applied to deal with the imbalanced label distribution. Specifically, it encodes the label as a condition vector to guide the generator to generate samples with the label. Long et al. [89] proposed to utilize a small privacy budget to estimate the"}, {"title": "3.2.4 AE+GAN", "content": "One challenge with GANs is that they are primarily suited for continuous data types, whereas tabular data often includes a mix of both categorical and numerical data types. Autoencoders can effectively address this issue as they are capable of encoding categorical data into a numerical format using techniques such as one-hot encoding and label encoding. Additionally, embedding layers in autoencoders can transform sparse categorical variables into dense, lower-dimensional representations, making them more amenable to processing by neural networks. Therefore, the AE is used in conjunction with GANs to handle mixed data types in tabular datasets [123"}]}