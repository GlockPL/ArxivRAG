{"title": "Neuropsychology of AI: Relationship Between Activation Proximity and Categorical Proximity Within Neural Categories of Synthetic Cognition", "authors": ["Michael Pichat", "Enola Campoli", "William Pogrund", "Jourdan Wilson", "Michael Veillet-Guillem", "Anton Melkozerov", "Paloma Pichat", "Armanouche Gasparian", "Samuel Demarchi", "Judicael Poumay"], "abstract": "Neuropsychology of artificial intelligence focuses on synthetic neural cognition as a new type of study object within cognitive psychology. With the goal of making artificial neural networks of language models more explainable, this approach involves transposing concepts from cognitive psychology to the interpretive construction of artificial neural cognition. The human cognitive concept involved here is categorization, serving as a heuristic for thinking about the process of segmentation and construction of reality carried out by the neural vectors of synthetic cognition.", "sections": [{"title": "Introduction", "content": "Explainability aims to make the activity of an artificial neural network understandable to humans (Du et al., 2019; Pichat, 2023, 2024a). This involves translating the observable behavior of a neural network into an interpretative framework, allowing for the assignment of relevant meaning to this behavior according to the observer's goals. In our context, this framework is that of cognitive psychology. Therefore, it involves using the categories of human cognitive thought as conceptual referents to establish analogies between human and artificial cognitive behaviors. More specifically, we will focus on the notion of categorization, as this concept from cognitive psychology appears particularly relevant for analyzing the synthetic cognition of language models, which largely involves a dynamic extraction of linguistic categorical invariants (Jawahar et al., 2019; Clark et al., 2019; Bills et al., 2023; Clark et al., 2023).\nIn this work, we focus on an epistemological explainability with fine cognitive granularity (Pichat, 2024b). In other words, we examine a microscopic explainability where the unit of observation is the formal neuron. This low-granularity explanatory approach aims to directly penetrate the \"black box\" system that an artificial neural network represents by creating elements of understanding about how thought categories and concepts are encoded and structured locally within a language model (Dalvi et al., 2019, 2022). The objective is, therefore, to interpret how categorical knowledge is constructed and utilized by the fundamental elements of the networks, namely the formal neurons themselves (Fan et al., 2023)."}, {"title": "Human Categorization", "content": "Categorization plays a central role in a variety of human cognitive activities of different scales (Sternberg, 2007; Roads et al., 2024): classification and sequencing, identification and denotation of objects, comprehension, reasoning, problem-solving, memorization, inference and prediction, property transfer, conceptualization, etc.\nFrom a formal perspective, a category is defined by two types of elements: its comprehension and its extension (Nadeau, 1999). The comprehension of a category, also called intension, is the set of properties that necessarily and sufficiently define this category, whether these properties are physical, structural, functional, procedural, or goal-oriented (i.e., related to the purpose of the involved task) (Tijus, 2004). Its extension is the set of members belonging to this category.\nHistorically, the classical notion of a category was shaped by Plato and then Aristotle, who positioned a category as being defined by a series of necessary and sufficient properties. This perspective gave rise, for example, to trait-based categories, conceived as the result of breaking down a category into a series of characteristics, all of which are necessary and collectively sufficient to define that category (Katz, 1972). However, cognitive studies of the actual human processes of categorization quickly revealed the rigidity of this foundational conception of the rules or theoretical defining elements of categorization."}, {"title": "Human Categorization by Similarity Judgment", "content": "Similarity-based categorization approaches assume that an object is assimilated to a class based on its estimated proximity to what represents the class (Thibault, 1997; Jacob et al., 2021; Kaniuth et al., 2022; Roads et al., 2021, 2024). This is based on (i) a space of traits or dimensions deemed relevant for comparison and (ii) a mode of calculating the distance between compared instances.\nThe involvement of similarity judgment as a basis for categorization seems broad-spectrum (Thibault, 1997), especially for classes without explicit definitions and with hierarchical organization, making it possible for some items to clearly belong to a category (Hampton, 1997).\nThe theories of categorization by prototype (Posner and Keele, 1968; Reed, 1972; Rosch & Mervis, 1975; Medin and Schaffer, 1978) mentioned previously, in fact, highlight the role of similarity in the categorization process (Sanborn et al., 2021): an item is assigned to a category if it is judged to be close to the central representation, which is the prototype. The same applies to the exemplar-based approach to categorization (Medin and Schaffer, 1978; Brooks, 1987; Nosofsky, 1992): an item is assigned to a categorical class if it is estimated to be closest to the significant elements that make up that class. In both cases, categorization results from the estimated distance between the item in question and what represents the category (Ayeldeen et al., 2015; Roads et al., 2024)."}, {"title": "Arguments Against Human Categorization by Similarity", "content": "Arguments against the effective or possible foundation of categorization on similarity reasoning are diverse (Love, 2002):\n\u2022\nAn element is assigned to the category that best explains it (Murphy and Medin, 1985), beyond possible initial classification by similarity (Keil, 1989).\n\u2022\nCategories with explicit definitions cannot be directly based on similarity judgment (Kalyan et al., 2012).\nHowever, the main arguments are based on the idea that the singular choice of similarity judgment criteria, which are just one possibility among others in the space of traits or dimensions, does not necessarily align with what constitutes or should constitute categorical assignment (Reppa et al., 2013; Poth, 2023):\n\u2022\nCategorization is influenced by information external to the objects being classified: general theories about the world or elements specifically related to the category involved (Rips, 1989).\n\u2022\nThe assignment of an object to a category is also a function of the relationships between the other objects that make up that category (Medin et al., 1993).\n\u2022\nIn cases where categorization is determined by the purpose of tasks, particularly in the case of ad hoc categories (Barsalou, 1991), similarity will invoke comparison criteria that are not suited to this finalized activity."}, {"title": "Counterarguments in Favor of Human Categorization Based on Similarity", "content": "In response to arguments against the involvement or relevance of similarity in the categorization process, several responses have been provided (Bobadilla et al., 2020; Hebart et al., 2020).\nGoldstone (1994) offers the following fourfold counter-argument: (i) The argument that similarity is too unstable to serve as a basis for categorization does not hold because it presupposes that categorization itself would not also be flexible; (ii) Even if superficial in some cases, similarity is functional in that it can genetically facilitate the discovery of \"deeper\" indicators of categorization and thereby the creation of new, more \"fundamental\" categories; (iii) Experiments show that similarity is not as unstable as is often argued; (iv) Categories that are not based on similarity are resistant to generalization.\nThibault (1997), in response to criticisms about the subjective relativity of similarity, posits that categorization is actually a sub-type of similarity. The author acknowledges that while similarity is indeed contingent upon the choice of comparison criteria, categorization operates similarly, except that it selects its own criteria from a set of traits defining the relevant category. Criticizing psychological essentialism, Thibault (idem) also asserts that the argument regarding the weakness of similarity (in the face of contextual elements, for example) does not hold, as this position assumes that the criteria for categorical segmentation have, could have, or should have an intrinsic, ontological per se value, independent of the individual.\nFinally, Hampton (1997) argues that categorization itself can be affected by irrelevant similarity elements (often perceptual and notably visual), or at least those deemed irrelevant by a priori logical analysis that dogmatically asserts that thought should operate in accordance with the standards of what is instantiated as logic. Moreover, the author shows, based on a fuzzy logic approach, that while subjects sometimes find it difficult to define categories, they nevertheless have no difficulty indicating the extent to which two items from these categories differ or identifying typical members, i.e., engaging in cognitive activities of similarity regarding them. In both cases, once again, Hampton emphasizes that the cognitive weaknesses attributed to similarity are based on an erroneous"}, {"title": "Problem Statement", "content": null}, {"title": "Context", "content": "Numerous studies reveal or infer a diversity of categories (linguistic, logical, positional, etc.) encoded in neurons and attention heads. In the classic experiment by Clark et al. (2019) on BERT, the authors highlight the converging linguistic functions of attention heads from the same layers. In their fascinating study on GPT-2XL, Bills et al. (2023) identify a series of specific neurons, noting that some are highly sensitive to context. Research also shows a geographic distribution of the type of categorial neural activity according to layer depth. Thus, the early layers respond more to morphological categories at the word level, while the deeper layers are more sensitive to the syntactic categorical features of sentences (such as passive/active voice, tense) and semantic categories (Jawahar et al., 2019).\nIn the context of our present work, we build on certain aspects of the study conducted by Bills et al. (2023), redirecting them toward other, more cognitive and epistemological issues (Pichat, 2024), as we will specify in the following section. Before that, let us briefly explain the approach of Bills et al. (idem). Based on the hypothesis that a neuron activates specifically for a given property, the researchers undertook a comprehensive analysis of the categorical semantics of all the neurons in GPT-2XL. Methodologically, they subjected GPT-2XL to an extensive series of token sequences, randomly selected from the internet data used to train the model. For each token, the activation values of all neurons across all layers were recorded. GPT-4 was then used to automatically identify the elements to which each neuron reacts (i.e., to generate the \"categorical explanation\") based on an instructional and example prompt applied only to the five sequences with the highest activations."}, {"title": "From Human to Synthetic Categorization", "content": "In line with the question we have presented in the field of human cognition regarding the relationship between categorization and similarity, our transposition of this heuristic question into the field of synthetic cognition is as follows: Is the level of categorical membership of tokens (arriving at a given neuron in the form of embeddings) to the category associated with that neuron related to their level of similarity? In other words, are the intensity of categorical membership and the intensity of similarity of tokens, as analyzed by a given neuron, two related aspects of the same phenomenon? Put differently, is the neural space of categorical membership segmented according to the segmentation of the similarity space? This is a largely unexplored issue in the field of synthetic explainability (Fan et al., 2023; Luo et al., 2024; Zhao et al., 2024), and it seems particularly pertinent to investigate.\nFrom an epistemological perspective, we have transposed the notion of categorical membership (measured on a dichotomous nominal scale, yes/no) in the realm of human cognition to the notion of the level of categorical membership (thus measured on an ordinal scale) in the realm of synthetic cognition; this is because, in the artificial neural field, categorical membership (i.e., activation, as we will elaborate later) is a numerical value rather than a boolean one."}, {"title": "Methodology", "content": null}, {"title": "Choice of GPT-2XL", "content": "In our study, we focused on OpenAI's GPT models because this suite of models, which inaugurated a significant part of contemporary generative AI, presents the paradox of being both the most popular in terms of media coverage and user numbers, while at the same time being the least studied directly internally, that is, in terms of fine-granularity explainability. The model chosen is GPT-2XL. This model is of particular interest because it is sophisticated enough to study the high-level synthetic cognitive phenomena that interest us without reaching the complexity of GPT-4, and even more so the multimodal GPT-40, whose complexity does not seem appropriate for the initial cognitive inquiry we are pursuing; in other words, GPT-2XL appears to offer a good level of compromise. Beyond this epistemic reason, a pragmatic reason also guided our choice of GPT-2XL: for the first time, in 2023, OpenAI broke its \"black box\" tradition (which is certainly logical from a commercial standpoint) regarding its products by providing, in the context of the article by Bills et al. (2023), information on the parameters and activation values of the neurons constituting GPT-2XL. These parameters and activation values will therefore serve as our starting data for this study.\nFor all practical purposes, let us clarify that GPT-2XL is the broad-spectrum variant of GPT (Generative Pre-trained Transformer) 2, developed by OpenAI and released in 2019. As its name suggests, GPT is a transformer, combining layers of attention heads and feed-forward perceptron-type layers. Its activation function is GeLU. Resulting from unsupervised training (at least directly) on a dataset of 8 million web pages, the model has approximately 1.5 billion parameters distributed across 48 layers. Each of these layers consists of 6.400 neurons and operates on 1.600-dimensional embeddings; each layer (or transformer block) comprises an attention sub-layer with 25 attention heads and two feed-forward"}, {"title": "Our Specific Data Choices", "content": "For the sake of simplification, in this exploratory study, we limited ourselves to the first two layers of GPT-2XL (layer 0 and layer 1) and the 6.400 neurons in each of these layers.\nRegarding the tokens and their activation values within these 2 x 6.400 = 12.800 formal neurons, we chose, for each of these neurons, to consider as relevant data its top 100 most activated tokens on average, along with their respective activation values. Indeed, the selection of only hyperactivated tokens, as conducted by Bills et al. (2023), seems too restrictive to us because it is not representative of the variability of tokens for which a neuron activates, potentially giving us a too limited view of the category of tokens to which a given neuron responds. In other words, we believe that Bills et al. (idem) may not interpret neurons extensively but instead identify a very limited subcategory of the category encoded by each of these neurons. Another argument that informed our choice is that the tokens with high average activation values selected are, de facto, less sensitive to contextual effects, which, although crucial (and this constitutes, in a sense, a limitation of our approach), can themselves also limit the extension of tokens belonging to a given neural category and thus the categorical semantics of the neurons involved."}, {"title": "The Interpretative Construction of Our Observables", "content": "As we just said before, the average activation level of a token within a neuron appears to us as a good operationalization of the equivalent in synthetic cognition of the level of categorical membership in human cognition. Indeed, the average activation of the 100 most activated tokens seems to us to be effectively representative of the extent to which these tokens are part of the extension of a category. This is, in the field of synthetic cognition, in line with the hypothesis of Bills et al. (2023) that a neuron activates specifically for a property, and the foundations of low-granularity explainability studies. Let us illustrate this idea with a case highlighted by Bills et al. (idem): concerning neurons that activate after a repeated occurrence of tokens, the stronger the repetition (i.e., the more the involved token sequence meets the defining condition of this category, namely, token repetition), the stronger the activation. This transposition between the level of categorical membership and the level of activation also seems justified, this time in the neurobiological field of human cognition, by the fact that the activation function is the analog corollary of the transfer function (Savioz et al., 2010), whose purpose is precisely to clarify the inputs belonging to the category to which a biological neuron should react, by increasing the signal-to-noise contrast, figure/ground (Servan-Schreiber, 1990), that is, the contrast between what belongs to the category of elements for which the neuron should activate versus residual elements."}, {"title": "Statistical Details", "content": "Our descriptive and inferential statistical calculations were conducted using Python libraries from the SciPy suite, based on the guidelines of Howell (2008) and Beaufils (1996).\nThe preliminary study of data normality, conducted for exploratory purposes as well as to verify the conditions for conducting certain parametric tests (ANOVA, regression, Grubbs' test), was twofold. Firstly, it was done using various inferential tests, each with its respective advantages: Shapiro-Wilk test (valid for small samples), Lilliefors test (valid for small samples and cases where the parameters of the normal distribution are unknown and estimated from the data), Kolmogorov-Smirnov test (better suited for large samples), and Jarque-Bera test (focused on skewness and kurtosis, valid for large samples). It should be noted that, concerning the cosine similarity measurements, these tests were systematically performed on all four mentioned embeddings to verify the convergence of the results. Secondly, a numerical descriptive approach was employed (skewness and kurtosis indices, mean-median deviation) as well as a graphical approach (QQ-plot comparing the actual distribution with the theoretical normal distribution). This variety of approaches provides us with a broad-spectrum view.\nTwo types of statistical units were identified. For our \"micro\" investigations, neuron by neuron, the instantiated statistical units are tokens, specifically the 100 most activated tokens on average for each neuron, which we call \"core"}, {"title": "The Question Investigated and Its Operationalization", "content": "By choosing, as previously mentioned, to operationalize categorical membership through activation and similarity through the cosine similarity measure, our operationalized initial question becomes: Is the activation level of tokens related to their cosine similarity level? Expressed in functional terms (in the mathematical sense), this question is formulated as follows: Is there a relationship between activation (in the activation space) and cosine similarity distance (in the cosine similarity space)?\nTo further operationalize this question, we choose to study it from the perspective of the proximity of activation intensity between tokens. This is based on the inference that if the level of similarity (i.e., categorical proximity) were related to the level of categorical membership, it would likely imply a relationship between the proximity of categorical similarity intensity and the proximity of categorical membership intensity. The question then becomes operationalized as follows: Is there a relationship between activation proximity and cosine similarity (proximity) between tokens?\nIn terms of statistical units, as already indicated, we choose to focus, for each neuron, on the 100 tokens that are subject to its 100 highest average activations. This choice is also made because it does not seem relevant to us to focus on tokens that do not belong, or belong only weakly, to the category associated with each neuron (i.e., those that are not highly activated); indeed, it seems unlikely, except by statistical chance, to find such tokens that would be systematically linked in terms of similarity. Thus, our question, as instantiated concerning activation proximity, guides us methodologically towards the following final"}, {"title": "Results", "content": null}, {"title": "Preliminary Statistical Explorations for Methodological Purposes", "content": "For both layers (see Tables No. 1 & 2 and corresponding Graphs No. 1 & 2), the comparison of minimum, mean, range, Q1, and CV (coefficient of variation) values for the cosine similarity between successive core-token pairs (as per their activation level) appears to highlight a relative deficiency in the three embedding models-Alibaba, Mixedbred, and WhereIsAI-compared to GPT-2XL: the latter exhibits a greater discriminative power. This phenomenon could potentially be partially explained by the following methodological bias, as intentionally noted earlier in our methodological section: the core tokens involved are de facto compliant with the GPT-2XL tokenization system and not necessarily aligned with the segmentation modalities that governed the other three embedding databases. As a result, in all cases, the cosine values from the GPT-2XL embeddings will be considered more reliable in the continuation of this study. However, this does not imply outright rejection of insights from the other embedding models for the purpose of (i) verifying the inter-embedding convergence of our results, and (ii) especially since the current results are highly convergent among these three embedding systems, which would still argue for a certain reliability concerning them."}, {"title": "Activation Proximity and Cosine Proximity", "content": "In our investigation of the relationship between activation proximity and cosine (similarity) proximity among successive core-tokens of each neuron, Tables No. 1 (neurons from layer 0) and No. 2 (neurons from layer 1) seem enlightening. We indeed observe low averages, relative to a theoretical positive span from 0 to 1 of cosine similarity values, with respective values of .38 and .42 for measurements made based on the embeddings from GPT-2XL. An inferential x2 goodness-of-fit measurement conducted on the percentage of neurons having average cosine similarity values below .5 (i.e., relatively low), assuming a theoretical equidistribution hypothesis, is fully compatible with this initial descriptive observation (p(x2) < .05 for both layers, N=6400). This result is also consistent with the similarly low average Q3 values (respectively .48 and .51), and the very low average minimum cosine values (respectively .06 and .1). This exploratory view supports the notion, at the level of the overall distribution of cosines taken as a whole, that activation proximity (i.e., categorical level closeness between two tokens) does not coincide with cosine proximity (i.e., categorical proximity between these two tokens).\nGraphs No. 3 (control neuron 0 from layer 0) and No. 4 (control neuron 0 from layer 1) show representative examples of the distribution of cosine similarity as a function of the activation value of the first token in each pair (for the 100 core-tokens selected) (see appendices for other control neurons). From these specific examples, we can globally observe: (i) again relatively low values of cosine similarity (notably in the case of measurements with GPT-2XL embeddings), (ii) a qualitatively significant variability of cosine similarity; this being consistent across the four embedding models (even though the variability seems more pronounced when the cosine is calculated from GPT-2XL embeddings, which is expected given the more discriminant semantic power of this embedding system for our specific data). This qualitative view, as it only pertains to examples, illustrates the potential fact that, at the level of the overall distribution of cosines, activation proximity does not correlate with cosine proximity; indeed, we do not obtain here stable graphs (i.e., linear of the type y=a) with a relatively high and constant value of cosine similarity for successive core-tokens regarding their activation level."}, {"title": "Categorical Discontinuity of Successive Core-Tokens", "content": "To investigate the previously mentioned trend, namely that, at the level of the overall distribution of cosines, activation proximity (i.e., the level of categorical belongingness between two tokens) does not equate to cosine proximity (i.e., categorical proximity between these two tokens), we test the following initial hypothesis, which represents a first perspective on this trend, extremized as already indicated to demonstrate the potential intensity it might occasionally exhibit: there is a categorical discontinuity in successive core-tokens regarding their level of activation. In other words, there are categorical breakpoints (i.e., semantic breaks) between successive core-tokens. Put another way, there are particularly low cosine similarities between successive core-tokens relative to their level of activation.\nTo test this, we first operationalize this hypothesis in terms of outliers in the distribution of the cosine similarity variable, more precisely lower outliers, with the notion of a lower outlier perfectly embodying the spirit of our hypothesis. Tables No. 5 (layer 0) and No. 6 (layer 1) show the average numbers of significant (p<.05) lower outliers per neuron obtained inferentially with the Grubbs test. These average numbers (respectively .007 and .005 with GPT-2XL embeddings) are very low and do not support our hypothesis. However, the reliability of this test is questionable here since the condition for its application, the normality of the distribution of the cosine similarity variable, is not well verified as previously indicated. A non-parametric approach, here the interquartile range, is therefore more secure; and it shows more lower outliers: on average, respectively .151 and .149 lower outliers per neuron (again with GPT-2XL), with extremely low average cosine similarity means (respectively .057 and .082), clearly demonstrating the strong intensity that the categorical discontinuity of successive core-tokens can sometimes take, even if this phenomenon remains here marginal (but not nonexistent) and relatively statistically normal when operationalized with an outliers approach. Table No. 7 qualitatively illustrates this categorical discontinuity by showing core-tokens that are semantically quite distant from each other."}, {"title": "Categorical Inhomogeneity of Successive Core-Tokens at the Same Activation Level", "content": "Continuing to explore the trend initially mentioned at the level of the overall distribution of cosines that activation proximity is not equivalent to cosine proximity-we now test the following second hypothesis, which represents another perspective on this trend, again extremized to demonstrate the potential intensity it might have: there exists a categorical inhomogeneity of successive core-tokens at the same activation level. In other words, core-tokens having the same levels of activation are not categorically closest. This new extremized viewpoint this time focuses not immediately on the lowest cosine similarities, as was done before, but on cases where activations are close to being identical and should then be strongly associated with high cosine similarities if activation proximity and cosine proximity were phenomena that went hand in hand.\nTo operationalize the testing of this hypothesis, we define successive core-tokens with (almost) identical activations as those whose activation levels are equal to two decimal places. We define, for each neuron, a \"d\" distance indicator that equals the gap between the maximum cosine similarity of this neuron and the cosine similarity of successive core-tokens at the same activation; a distance whose superiority to the first quartile Q1 of the cosine distribution for this neuron is then verified, to show that statistically core-tokens at the same activation are not categorically closest. Tables No. 10 (layer 0) and No. 11 (layer 1) indicate first that tokens with (almost) identical activations are very numerous (respectively 47.25 and 35.31 tokens for 100 tokens per neuron), allowing us to consistently study the phenomenon of interest here. We see that the average distances are very large (.44 and 46, when measured with GPT-2XL embeddings, but also quite significant with other embeddings more inclined to over-represent strong cosine similarities). The percentages of neurons showing a distance d greater than Q1 are extremely high (respectively 80.72% and 78.94%); and significant (p(x2) < .05) when evaluated inferentially with a univariate x2 goodness-of-fit test based on a theoretical 25%/75% distribution consistent with our use of Q1. These elements seem compatible with our hypothesis of mono-activational categorical inhomogeneity of successive core-tokens, at the"}, {"title": "Synthesis", "content": "Let us now summarize our statistical treatments aimed at studying, at the level of successive core-tokens of each neuron, the characteristics of a possible existence of a relationship between activation proximity and cosine similarity (proximity). We obtained statistical results that are coherent with the two hypotheses formulated regarding phenomena of synthetic cognition:\n\u2022 A hypothesis of categorical discontinuity of successive core-tokens regarding their level of activation, positing that there are particularly low cosine similarities between successive core-tokens.\n\u2022 A hypothesis of mono-activational categorical inhomogeneity of successive core-tokens, stating that core-tokens with the same levels of activation are not categorically closest."}, {"title": "Discussion of Our Results", "content": "Our series of studies questioned, concerning successive core-tokens, the possible existence of a relationship between proximity of categorical membership level and proximity of categorical similarity; proximities operationalized in terms of level of activation for the former and level of cosine similarity for the latter. Our results tend toward the idea of an independence, at the global level, between activation proximity and cosine proximity; i.e., towards a non-equivalence, for given tokens, between their proximity of belonging to a neuronal category and the intensity of their similarity. In other words, just because two tokens are close in terms of activation does not mean they are close at the categorical level. In the context of our current work, this phenomenology is manifested through two observables of synthetic cognition that we have attempted to elucidate: the"}, {"title": "Current Trends in Artificial Neuronal Explainability", "content": "Several current interpretative trends in the field of synthetic neuronal explainability seem to us to be elementary keys, to be combined, for understanding the phenomenology of synthetic divergence between activation and similarity. We briefly present these trends below before attempting to elaborate on their interrelationship.\nSynthetic neuronal polysemy is a primary concept. It refers to the idea that synthetic neurons are conceptually distributive (Fan et al., 2023), meaning they can correspond simultaneously to multiple semantic concepts (Bills et al., 2023). Thus, these authors suggest that neurons (i) may not have simple explanations but only long and disjunctive interpretations, (ii) should perhaps not be thought of as semantically homogeneous computational units. Bricken et al. (2023) further denote that formal neurons respond to unrelated traits.\nAuthors tend to link synthetic polysemy to the notion of superposition, which expresses the idea that cognitive properties and semantic traits can be ventilated within many polysemic neurons (Olah et al., 2020). And that a single concept is thus distributed across different neurons (Bills et al., 2023).\nAnother notion introduced by Bills et al. (2023), which seems relevant here and refers to a potential illusion of interpretability (Pichat, 2024b), is that of an alien concept. This means that formal neuronal concepts might be concepts for which humans have no word (no signifier in the sense of Saussure) or might even correspond to \"natural abstractions\" not yet discovered by humans (lacking human signified). This is insofar as, the authors indicate, language models deal with things different from us, for instance, statistical constructs useful for the task for which they have been trained, like predicting the next token.\nFinally, Bricken et al. (2023) argue that the neuron does not constitute a good unit of semantic interpretation by introducing the idea of the existence of intermediate synthetic semantic vector spaces. A neural network would create a virtual intermediate vector space, each base vector of which would be an a priori independent, fundamental, unique, and mono-semantic feature. Each of these features is obtained by linear combination of neurons, i.e., each feature is a vector on these neurons. Each feature thus constitutes an interpretable linear direction, an elementary semantic direction. Hence, the activation vector at the output of a neuronal layer could be decomposed in this intermediate space whose unit vectors are the elementary features. Each of these features would, by definition, be invisible at the level of a single neuron, which is why the neuron might not necessarily be the right unit of analysis according to the authors; they indicate, for example, in their study, that only 512 neurons can represent tens of thousands of features. From these fundamental semantic directions, more complex directions would be created, those constituted by the neurons, which then appear de facto polysemic insofar as they are conceptually a compressed projection, i.e., low-dimensional, of these much vaster intermediate vector spaces."}, {"title": "The Dissociation of Categorical Proximity and Categorical Similarity as a Sign of the Categorical Singularity of Synthetic Cognition", "content": "Following our presentation of current trends in neuronal explainability, let's attempt to address our central empirical observation by reshaping and transposing them within a suitable explanatory framework. Why isn't activation proximity a corollary of similarity? Because a neuron codes for a synthetic category, which it creates in the context of its targeted activity, and this category is not unified; that is, it is polysemous. This polysemy makes this category appear as an alien concept (which it effectively is for our human cognition) insofar as it results from a superposition of sub-categorical dimensions generated by its intermediate categorical vector base (a base that we do not conceptualize in the same terms as Bricken et al. (2023) but rather, for a given neuron in layer n, in terms of categorical dimensions output from its precursor neurons in layer n-1). Two close activations (cf. the notion of categorical discontinuity) and even identical ones (cf. the notion of mono-activational categorical inhomogeneity) can thus correspond to crystallizations, materializations, local instantiations (by quantum analogy, we could speak of wave function collapses) of different sub-categorical dimensions. In other words, close activations can thus involve the actualizations of distinct sub-dimensions; which will then mechanically translate into cosine similarity measures demonstrating categorical discontinuity or inhomogeneity, concepts specific to synthetic cognition; at least concepts that appear to us when we study this synthetic cognition from our own human reference framework postulating an a priori semantic logical coherence between activation and similarity.\nNeuronal polysemy does not seem to be categorically segmented into activation segments: categorical segments and activation segments appear to be two dissociated registers in synthetic neuronal cognition. Because this synthetic neuronal categorical cognition, unlike our human thought, is not categorically unified, at least not unified within concepts analogous to ours. Therefore, seeking a convergence between categorical proximity and cosine proximity is partly ipso facto an anthropocentric approach that could only lead to the empirical correlational divergence that the synthetic notions of discontinuity and categorical inhomogeneity illustrate. This is especially true within the framework of our methodological approach of using cosine similarity as an instrument to measure categorical proximity: cosine similarity, being based on the vector categorical space of the initial embeddings of GPT-2XL (which is by construction more in tune with human semantics) is not the same as the recombinant vector spaces (by the values of their respective aggregation functions) of the neurons of the layers investigated.\nSpeaking of neuronal polysemy might epistemologically be a cognitive anthropocentrism (Pichat, 2024). Indeed, we use the term polysemy because synthetic categories appear semantically inhomogeneous given that we have no human thought categories to pair them with. But isn't it the very nature of categorical abstraction to bring together initially separate categorical segments? The invoked polysemy is, in fact, the result of a categorical grouping to which"}, {"title": "Conclusion", "content": "Our hypothesis of categorical discontinuity of successive core-tokens regarding their activation level (postulating that there are particularly low cosine similarities between successive core-tokens) and our hypothesis of mono-activational categorical inhomogeneity of successive core-tokens (stating that core-tokens with the same activation levels are not categorically closest) are complementary; this is because they pertain to the question of the global existence of a relationship between activation proximity and cosine proximity (similarity). They should be complemented by a third hypothesis, focusing on the possible evolution of the distributional dynamics of this relationship depending on the level of value of the activation segments; this hypothesis aims to investigate a possible categorical convergence of pairs of successive core-tokens based on activation, proposing that as the activation levels of successive core-tokens (i.e., close at the activation level) increase, the categorical variability of these core-tokens decreases (i.e., categorical proximity increases). We will soon publish our results in this area (Pichat et al., in press a)"}]}