{"title": "AEGIS: An Agent-based Framework for General Bug Reproduction from Issue Descriptions", "authors": ["XINCHEN WANG", "PENGFEI GAO", "XIANGXIN MENG", "CHAO PENG", "RUIDA HU", "YUN LIN", "CUIYUN GAO"], "abstract": "In software maintenance, bug reproduction is essential for effective fault localization and repair. Manually writing reproduction scripts is a time-consuming task with high requirements for developers. Hence, automation of bug reproduction has increasingly attracted attention from researchers and practitioners. However, the existing studies on bug reproduction are generally limited to specific bug types such as program crashes, and hard to be applied to general bug reproduction. In this paper, considering the superior performance of agent-based methods in code intelligence tasks, we focus on designing an agent-based framework for the task. Directly employing agents would lead to limited bug reproduction performance, due to entangled subtasks, lengthy retrieved context, and unregulated actions. To mitigate the challenges, we propose an Automated gEneral bug reproductIon Scripts generation framework, named AEGIS, which is the first agent-based framework for the task. AEGIS mainly contains two modules: (1) A concise context construction module, which aims to guide the code agent in extracting structured information from issue descriptions, identifying issue-related code with detailed explanations, and integrating these elements to construct the concise context; (2) A FSM-based multi-feedback optimization module to further regulate the behavior of the code agent within the finite state machine (FSM), ensuring a controlled and efficient script generation process based on multi-dimensional feedback. Extensive experiments on the public benchmark dataset show that AEGIS outperforms the state-of-the-art baseline by 23.0% in F \u2192 P metric. In addition, the bug reproduction scripts generated by AEGIS can improve the relative resolved rate of Agentless by 12.5%.", "sections": [{"title": "1 INTRODUCTION", "content": "Software testing [33, 46, 62] plays a crucial part in the software development lifecycle, which involves executing tests to identify and verify functionalities bugs. Existing automated test generation techniques [9, 15, 30, 36, 47, 59] are capable of producing high-quality unit tests for specified functionalities, thereby reducing the repairing effort and enhancing development efficiency.\nReproducing general bugs from issue descriptions is very essential. Numerous tests in project repositories originate from issue reports [22], aiming to prevent their recurrence, which has become a standard practice in workflows. Multiple studies have concluded that generating bug reproduction scripts from issues closely aligns with developers' requirements [10, 23] and enhances the performance of automated debugging tools [32, 48]. The dynamic execution information from reproduction scripts [24] aids in more precise bug localization and targeted solutions. However, manually constructing these scripts is time-consuming and demands strong multilingual understanding and programming skills. Meanwhile, existing reproduction techniques [32, 48] primarily focus on addressing crashes and are not applicable to general bugs described in issue descriptions. As a result, the task of reproducing general bugs from issue descriptions has gained increasing attention.\nProgram repair-based code agents demonstrate the best performance in general bug reproduction; however, optimized agent-based methods for this task are lacking. Considering the powerful context understanding of large language models (LLMs), Libro [22] leverages LLMs and prompt engineering [44] to achieve effective outcomes in the bug reproduction task. However, Libro cannot dynamically execute and modify reproduction scripts, limiting its performance. Code agents [5, 7, 27], as intelligent systems that integrate code retrieval, generation, and editing, can invoke various external tools and APIs, process execution feedback, and take subsequent actions [8, 40, 41]. Currently, code agents have demonstrated great promise in program repair [57, 61]. Niels et al. [31] make simple prompt adjustments to existing program repair-based code agents, achieving state-of-the-art results in the general bug reproduction task. However, they do not design modules or optimize methods specifically for this task.\nCode agents face numerous challenges in generating reproduction scripts. (1) Entangled Sub-tasks: During the process of bug reproduction, code agents perform multiple subtasks, including context retrieval, reproduction script execution, and multi-file editing. The frequent switching between these subtasks reduces the efficiency. (2) Lengthy Retrieved Context: While retrieving context relevant to the issue description, code agents also collect a large amount of irrelevant intermediate results. An overly large context window increases the burden on the code agents [25]. (3) Unregulated Actions: Code agents obtain various feedbacks by invoking external tools. However, they struggle to effectively utilize these feedbacks and even take incorrect actions [26].\nTo address the above limitations and challenges, in this paper, we propose an Automated gEneral buG reproductIon Scripts generation framework, named AEGIS. AEGIS mainly contains two modules: (1) A concise context construction module, which aims to extract structured information from issue descriptions, identify issue-related code with detailed explanations, and integrate these elements to construct the comprehensive context; (2) A FSM-based multi-feedback optimization module to further regulate the behavior of the agent within the finite state machine (FSM) [51], ensuring a controlled and efficient script generation process based on multi-dimensional feedback."}, {"title": "2 MOTIVATION", "content": "In this section, we explore and summarize the challenges faced by the code agent in the general bug reproduction task.\nSince existing code agents [5, 7, 57] are designed for program repair tasks, we design a simple AgentBaseline for bug reproduction tasks based on the existing code agent framework. AgentBaseline possesses the capability to search and view code, as well as to edit and execute bug reproduction scripts. It simultaneously performs context retrieval, script execution, and file editing, submitting the refined reproduction script. Figure 1 illustrates the issue description and part of the reproduction process for instance 'django-16408'. Figure 1(a) indicates that the issue arises due to \u2018Multi-level FilteredRelation with select_related() may set wrong related object', suggesting adding a test case in the ExistingRelatedInstancesTests class and providing the current output.\n(1) Entangled Subtasks: Decouple the subtasks of context retrieval, script execution, and multi-file editing. As shown in Figure 1(b), the code agent alternates between retrieving context (Turn 1, 4), editing files (Turn 2, 5), and executing the scripts (Turn 3, 6). The code agent rushes into reproduction script editing without adequately gathering context, resulting in unsuccessful attempts and necessitating further information retrieval. Additionally, the code agent has to modify tests.py to add the test case described in the issue and create reproduce.py as the test entry point. However, if the reproduction fails, both files need to be modified simultaneously (Turn 2, 5, 7, 8). The entanglement of these subtasks results in high costs with low efficiency.\n(2) Lengthy Retrieved Context: Extract and integrate issue-related context. As shown in Figure 1(c), the code agent invokes various APIs and system commands for context retrieval. In Turn 1, 'grep \u2013 r' is called to search for the identifier \u2018pool' mentioned in the issue description, returning multiple results. In Turn 2, the \u2018ls' command is used to list all files under the project directory. In Turn 3, \u2018cat' is invoked to browse through \u2018tests.py', which spans 166 lines. Obviously, not all retrieved context is pertinent to the issue. For instance, the output of commands like 'ls' and 'cat' provides minimal assistance in understanding the issue. The issue-related information is sparse and spread across a large context window, making comprehension difficult.\n(3) Unregulated Actions: Effectively utilize feedback and adopt more regulated actions. On one hand, the code agent can leverage various external tools to obtain feedback. As shown in Figure 1(d), in Turn 3, after applying the patch, the built-in syntax checking tool detects and reports syntax errors. In Turn 5, the code agent executes the reproduction script and receives error messages. On the other hand, the code agent exhibits high flexibility, with its thinking and actions not being constrained. In Turn 4, after multiple unsuccessful attempts to reproduce the bug, the code agent resorts to utilizing a 'raise AssertionError' statement to explicitly trigger an error. In Turn 5, the code agent mistakenly interprets the error message \"Test Failed\" as a successful reproduction of the bug, failing to consider the current output described in the issue and lacking external constraints on its judgment.\nBased on the findings and analysis above, we propose an Automated bug rEproduction scripts Generation framework from Issue deScriptions, named AEGIS in this paper. AEGIS decouples the subtasks of context retrieval, script execution, and multi-file editing. AEGIS extracts essential information from the code context and issue description, reducing the code agent's context window. Additionally, AEGIS takes actions based on feedback within a finite state machine framework, ensuring a controlled and effective process."}, {"title": "3 PROPOSED FRAMEWORK", "content": "In this section, we introduce the overall framework of AEGIS. As shown in Figure 2, AEGIS consists of two main modules: (1) a concise context construction module for integrating essential issue-related information into a structured context, (2) a FSM-based multi-feedback optimization module for iterative refinement of reproduction scripts within the finite state machine process. The input to the AEGIS includes the issue description, project codebase, and the buggy code along with unit tests retrieved by the Searcher Agent."}, {"title": "3.1 Concise Context Construction Module", "content": "The concise context construction module aims to construct structured information from unstructured issue descriptions and extract issue-related code snippets from extensive contexts."}, {"title": "3.1.1 Pilot Experience", "content": "Based on the following observations and considerations, we find that directly integrating the issue descriptions and the context retrieved by the Searcher Agent as input is not appropriate: (1) Different users pose issues with distinct personal styles and inconsistent formatting. Additionally, issue descriptions contain varied information, including current results, expected results, reproduction methods, runtime environments, tool versions, and user comments. Extraneous and unstructured content may hinder the Reproducer Agent's comprehension of the issue and distract it from bug reproduction. (2) The Searcher Agent retrieves extensive code contexts based on the issue description. However, many contexts are intermediate results, such as outputs from commands 'ls' and 'find' as shown in Figure 1(c). These contexts may not be relevant to the issue but can greatly expand the Reproducer Agent's context window. Additionally, the absence of explanations linking the code contexts to the issue may confuse the Reproducer Agent in utilizing these contexts for bug reproduction."}, {"title": "3.1.2 Concise Context Construction", "content": "Based on the above findings, we extract three types of information: structured issue information, issue-related code, and relevance explanation. Figure 3 illustrates an example of the concise context."}, {"title": "3.2 FSM-based Multi-Feedback Optimization Module", "content": "The FSM-based multi-feedback optimization module leverages multi-dimensional feedback to regulate the Reproducer Agent through iterative optimization. This process results in the generation of a ranked reproduction script."}, {"title": "3.2.1 Initialization of Finite State Machine", "content": "A finite state machine (FSM) [51] is a computational model used to represent a system with a finite number of states and the transitions between these states. FSMs are fundamental in various fields, including compiler design, network protocols, and automation systems, where they are used to model and manage state-based behavior efficiently. Code agents struggle with complex constraints and often deviate from planned execution in bug reproduction tasks as illustrated in Figure 1(d), leading to ineffective scripts and reduced performance. In the FSM process, code agents make decisions based on the current state and execute predefined state transitions according to feedback, ensuring a more controlled reproduction process.\nIn this paper, the FSM-based multi-feedback optimization module is a 5-tuple (Q, \u03a3, \u03b4, qo, F), comprising the following components:\n\u2022 State Set Q: A finite set of states defines the transitions within the FSM. For example, the state set is Q = {qo, q1, . . ., qn }. Each state qi \u2208 Q possesses distinct attributes and is capable of executing different actions.\n\u2022 Input Alphabet \u03a3: A finite set of symbols represents all possible inputs to the FSM. For instance, the input alphabet can be \u2211 = {a, b, c}. Input a \u2208 \u03a3 can include not only characters but also events, such as input signals or timer expirations, as well as conditions like \u2018temperature > 100' or \u2018if (flag == true)'.\n\u2022 Transition function 8: This defines the transition of the FSM from one state to another based on an input a \u2208 \u2211, which is typically represented as a mapping \u03b4 : Q \u00d7 \u2211 \u2192 Q.\n\u2022 Initial State q\u2030 \u2208 Q: The state from which the FSM starts.\n\u2022 Accepting State Set F \u2286 Q: It is a set of states where the FSM accepts the input."}, {"title": "3.2.2 FSM-based Multi-Feedback Optimization", "content": "As illustrated in Figure 4, we propose a FSM-based process to direct the Reproducer Agent in generating bug reproduction scripts.\n\u2022 State Set Q = {qo, q1, q2, q3, q4, q5, q6} denotes the various states of the Reproducer Agent, including Create, Execute, Self-Verify, External-Verify, Report, Modify and Restart, respectively.\n\u2022 Input Alphabet \u2211 = {a, b, c, d, e, f, g, h, i, j, k, l} represents the different events and conditions for the Reproducer Agent, including Command Check, Error Message, Syntax Check, Referee Verification, Patch Check, Failure Experience, and others, as illustrated in Figure 2.\n\u2022 Initial State qo (Create) indicates that the Reproducer Agent starts by creating reproduction scripts based on the constructed concise context.\n\u2022 Accepting State q4(Report) indicates that the Reproducer Agent has finalized the reproduction scripts and reported success.\n\u2022 Transition function 8 describes how the Reproducer Agent transitions between states in response to various events and conditions.\n\u03b4(qo, a) = q\u2081: In (Create) state qo, the Reproducer Agent generates the initial reproduction script(a) based on the constructed concise context. Then, the FSM transitions to state q1.\n\u03b4(q1,b) = q2: In (Execute) state q1, the Reproducer Agent executes the reproduction script and obtains error messages(b). The dynamic execution information is essential for analyzing bug reproduction. Then, the FSM transitions to state q2.\n\u03b4(q1, e) = q\u2081: In (Execute) state q1, the Reproducer Agent is restricted to using the script execution command, called command check. File editing and context retrieval commands are not permitted to ensure process simplicity. If the command check fails(e), a system prompt will be added to guide proper command usage, and the state remains q1.\n\u03b4(q2, c) = q3: In (Self-Verify) state q2, the Reproducer Agent analyzes the error messages based on the issue description and determines whether the error message reproduces the issue. If verify pass(c) occurs, the FSM transitions to state q3.\n\u03b4(q2, f) = q5: Conversely, if verify fail (f) occurs, the FSM transitions to state q5 with the detailed failed reasons. These reasons include explanations for the reproduction failure and suggestions for optimization.\n\u03b4(q3, d) = q4: In (External-Verify) state q3, an independent external referee determines whether the error message reproduces the issue, called referee verification. If verify pass(d) occurs, the FSM transitions to the final (Report) state q4 and exits. Additional external verification helps to improve the quality of the reproduction scripts.\n\u03b4(q3, g) = q5: Conversely, if verify fail(g) occurs, the FSM transitions to state q5 with the detailed failed reasons provided by the referee.\n\u03b4(q5, h) = q5: In (Modify) state q5, the Reproducer Agent reflects on the reasons for reproduction failure and edits the current reproduction script. After editing, the patch is first checked, called patch check. Besides the format checking, the patch check ensures that only the reproduction scripts can be modified to keep the process streamlined. If the patch check fail(h) occurs, the complete reproduction script for reference will be provided and the state remains q5.\n\u03b4(q5, i) = q5: The Reproducer Agent leverages external syntax tools to verify the reproduction script, called syntax check. If the syntax check fail(i), the state remains 95.\n\u03b4(q5, j) = q1: If the script passes all the checks, the FSM transitions back to state q1 with the modified reproduction script(j).\n\u03b4(q5, k) = q6: If the edits number reaches the limit, called over edit times(k) in state q5, the process transitions to state q6. A straightforward idea is to generate more diverse reproduction scripts from scratch, rather than struggle in refinement.\n\u03b4(96, l) = qo: In (Restart) state q6, the Reproducer Agent summarizes the reasons for failure based on the modification history called failure experience(l). Then the FSM transitions to state qo and restart. The accumulated failure experiences can serve as references to avoid repeating mistakes.\nOverall, we propose the FSM framework to instruct the behavior of the Reproduce Agent through multiple feedback. Specifically, static and dynamic information from Error Message and Syntax Check guide AEGIS in analyzing reproduction situations and take targeted modifications. Patch Check and Command Check direct AEGIS to focus on optimizing reproduction scripts, thereby avoiding the entanglement of subtasks as shown in Figure 1(b). Referee Verification introduces external checks to prevent unreasonable verifications as illustrated in Figure 1(d). Additionally, failure experience and the restart enable AEGIS to draw from historical experiences, explore various reproduction ways, and avoid repetitive modifications as noted in Figure 1(d)."}, {"title": "3.2.3 Ranking and Selection", "content": "During the FSM process, we extract reproduction scripts generated at each restart and prioritize them to select the only reproduction script: (1) Double-verified Script: Select the script if it passes both self-verify and external-verify. (2) Single-verified Script: If no scripts pass double verification, select the script that passes self-verify alone. (3) Last-modified Script: If no scripts meet the above criteria, select the most recently modified script."}, {"title": "4 EXPERIMENTAL SETUP", "content": "In this section, we evaluate the proposed AEGIS and aim to answer the following research questions (RQs):\nRQ1: How does AEGIS perform in the bug reproduction script generation from issue descriptions compared with different methods?\nRQ2: What is the influence of different modules on the performance of AEGIS?\nRQ3: How do the reproduction scripts generated by AEGIS improve Agentless's resolved rate?\nRQ4: How do the different hyper-parameters impact the performance of AEGIS?"}, {"title": "4.1 Datasets", "content": "To answer the questions above, we choose the popular SWE-bench Lite [20] dataset to evaluate AEGIS. SWE-bench Lite consists of 300 real-life GitHub issues from 12 repositories, consisting of issue descriptions and the corresponding open-source repositories."}, {"title": "4.2 Comparative Methods", "content": "To evaluate the performance of our framework, we compare two types of bug reproduction script generation methods: LLM-based and Agent-based."}, {"title": "4.2.1 LLM-based methods", "content": "\u2022 ZEROSHOTLITE prompts the LLM with the issue description and instructions to generate a new script.\n\u2022 ZEROSHOTAGENT is similar to ZEROSHOTLITE but includes the code context retrieved by the Searcher Agent.\n\u2022 ZEROSHOT [31] prompts the LLM with the issue description, related code context retrieved using BM25 [45], and instructions to generate patch files in unified diff format.\n\u2022 ZEROSHOTPLUS [31] is similar to ZEROSHOT but leverages adjusted diff format, which allows entire functions or classes to be inserted, replaced, or deleted.\n\u2022 LIBRO [22] first generates multiple proposal tests based on the bug report, then it runs all generated tests and selects the test whose error message is most relevant to the issue."}, {"title": "4.2.2 Agent-based methods", "content": "\u2022 SWE-AGENT [57] comprises several principal components, including search, file viewer, file editor, and context management. It is also allowed to employ system commands and utilities.\n\u2022 SWE-AGENT+ [31] is a variant based on the SWE-AGENT, which is instructed to execute the generated reproduction scripts explicitly.\n\u2022 AUTOCODEROVER [61] consists of two stages: the context retrieval stage and the patch generation stage. In the first stage, AUTOCODEROVER iteratively searches for issue-related code snippets. In the second stage, it generates patches based on the issue description and code context, retrying until the patch is successfully applied.\n\u2022 AgentBaseline is our designed bug reproduction-based agent based on the existing code agent framework as mentioned in Section 2."}, {"title": "4.3 Implementation Details", "content": "Searcher Agent: The Searcher Agent possesses fundamental capabilities for code retrieval and file viewing. It incorporates essential interfaces, such as \u2018search_class' and \u2018review_file', as well as basic system commands, including 'ls' and 'grep'. The Searcher Agent iteratively retrieves buggy code and unit tests until it reports \"I have retrieved all relevant contexts\" or the maximum number of iterations is reached.\nHyber-Parameters of Inference: The Searcher Agent and the Reproducer Agent are provided access to GPT4o-2024-0513 [35]. The sampling temperature for the Searcher Agent is set to 0, with a maximum of 40 retrieval iterations. The sampling temperature for the Reproducer Agent is set to 0.7, with a maximum of 5 restarts and up to 5 edits per restart.\nBackbone Large Language Models:\n\u2022 GPT3.5-turbo [6] is a commercial LLM from OpenAI.\n\u2022 GPT4 [34] is a follow-up version of GPT35.\n\u2022 GPT40 [35] is a variant of GPT4 optimized for specific applications.\nOther Details: We construct environment-completed docker images for each instance based on the open-source project SWE-bench-docker [1]. For each instance, AEGIS launches a new docker container to automatically generate bug reproduction scripts. To speed up script generation, we turn on 4 processes for parallelized inference."}, {"title": "4.4 Evaluation Metrics", "content": "We identify four different execution states when running the reproduction scripts in the corresponding repository both before and after resolving issues: Fail-to-Pass (F2P), Fail-to-Fail (F2F), Pass-to-Pass (P2P), Pass-to-Fail (P2F). We propose three main metrics to assess the performance of any method in generating bug reproduction scripts:\nThe Fail-to-Pass: The Fail-to-Pass rate ($F \\rightarrow P = \\frac{F2P}{F2P+F2F+P2P+P2F}$) measures the portion of bug reproduction scripts where the scripts fail in the current repository but pass when the issue has been resolved (i.e. the successful reproduction scripts). The higher the Fail-to-Pass rate, the higher the quality of the reproduction script generated by the method.\nFail-to-Pass of Fail-to-Any Rate: This rate ($\\frac{F \\rightarrow P}{F \\rightarrow X} = \\frac{F2P}{F2P+F2F}$) measures the proportion of reproduction scripts that successfully execute after the issue is resolved, among those that fail to execute before the issue is resolved.\nFail-to-Pass of Any-to-Pass Rate: This rate ($\\frac{X \\rightarrow P}{F \\rightarrow P} = \\frac{F2P}{F2P+P2P}$) measures the proportion of reproduction scripts that fail to execute before the issue is resolved, among those that successfully execute after the issue is resolved."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "5.1 RQ1: Effectiveness of AEGIS in General Bug Reproduction\nTo answer RQ1, we conduct a comprehensive comparative analysis against five LLM-based methods and four Agent-based methods across three metrics. The experimental results are shown in Table 1.\nThe proposed AEGIS consistently exhibits superior performance compared with the baseline methods. As shown in Table 1, AEGIS outperforms all the baseline methods across three metrics. Specifically, AEGIS improves by 23.0%, 7.4%, and 48.5% over the best baseline method across the three metrics, respectively. Compared to the average performance of current methods, AEGIS demonstrates great improvements of 28.2%, 21.0%, and 64% in three metrics. Regarding the F \u2192 P metric, AEGIS also surpasses the best baseline by 11.0%.\nThe feedback information aids in reproducing the bug. We discover that agent-based methods, such as SWE-AGENT+, AgentBaseline, and AEGIS, consistently outperform LLM-based methods. Specifically, agent-based methods achieve top-3 results in 8/9 metrics. When considering average performance, agent-based methods show improvements of 8.8%, 13.9%, and 17.5% over LLM-based methods across the three metrics. These results highlight the advantages of agent-based methods, which can leverage various external tools and system commands to self-optimize and adapt based on feedback.\nGenerating a new script is more effective than modifying an existing one. For LLM-based methods, ZEROSHOTLITE and ZEROSHOTAGENT are tasked with generating a new reproduction script named 'reproduce.py', while ZEROSHOT and ZEROSHOTPLUS are required to modify\n\u00b9We follow the experimental results of ZEROSHOT, ZEROSHOTPLUS, LIBRO, SWE-AGENT, SWE-AGENT+, and AU-TOCODEROVER by Niels et al [22]. The evaluation of these methods is based on a subset of SWE-Bench lite. Since the filtered instances are not publicly available and the difficulty of issue reproduction is generally averaged, the overall reproduction rate shows minimal fluctuation. Additionally, in the F\u2192 P column, we assume that these methods can successfully reproduce all the filtered instances."}, {"title": "5.2 RQ2: Effectiveness of Different Modules in AEGIS", "content": "To answer RQ2, we explore the effectiveness of different modules on the performance of AEGIS. Specifically, we study the two involved modules including the concise context construction module (i.e. CCC) and FSM-based multi-feedback optimization module (i.e. FMO)."}, {"title": "5.2.1 Comprehensive Context Construction Module", "content": "To understand the impact of this module, we deploy a variant of AEGIS without the concise context construction module (i.e. w/o CCC). It directly employs the context retrieved by the Searcher Agent along with the original issue description.\nTable 2 shows the performance of the variant on the PASS@1 and PASS@5 setting. The addition of the concise context construction module yields enhancements of 4.3%, 5.1%, and 0.2% across three metrics in the PASS@1 setting. Similarly, in the PASS@5 setting, three metrics are improved by 3.7%, 3.7%, and 4.0%, respectively. Overall, the results indicate that the concise context construction module enables the Reproducer Agent to focus on a streamlined context window, thereby better analyzing the issue description and referencing issue-related code and its explanations to construct reproduction scripts, ultimately enhancing its ability to reproduce bugs."}, {"title": "5.2.2 FSM-based Multi-Feedback Optimization Module", "content": "To explore the contribution of the FSM-based multi-feedback optimization module, we also construct a variant of AEGIS without the FSM-based multi-feedback optimization module (i.e. w/o FMO). It creates, executes, and refines the reproduction script freely based on the initial system prompt, without following a standardized process.\nAs illustrated in Table 2, the addition of the FSM-based multi-feedback optimization module leads to improvements of 23.7%, 18.2%, and 48.6% across three metrics in the PASS@1 setting. Likewise, in the PASS@5 setting, three metrics show enhancements of 12.7%, 10.1%, and 32.2%, respectively. The results demonstrate that FSM-based multi-feedback optimization module greatly boosts performance. This enhancement can be attributed to the module's capability in guiding the Reproducer Agent to make appropriate decisions based on muti-dimension feedback, avoiding unregulated actions, and ensuring a controlled and efficient script generation process.\nTo further investigate the influence of different components within FSM-based multi-feedback optimization module, we design four variants: including those without Modify-in-FMO, Restart-in-FMO, Rank-in-FMO, and Referee-in-FMO. These variants represent generating scripts in a single attempt without execution, modifying scripts continuously until reporting success or exceeding the maximum iterations, only selecting scripts that have passed dual verification, and verifying scripts without external referees, respectively.\nTable2 shows the performances of the four variants. Under the PASS@1 setting, the components Modify, Restart, Rank and Referee contribute to an improvement in the F \u2192 P metric by 10.0%, 2.7%, 19.3%, and 1.0% respectively. In the PASS@5 setting, the performance changes are 6.0%, -5.0%, 24.7%, and 2.0% respectively. The results reveal that within FSM-based multi-feedback optimization module, the Rank component has the most noticeable impact on AEGIS, indicating that our ranking strategy effectively selects the suitable reproduction script according to predefined rules. The Modify component also exhibits a great influence, suggesting that AEGIS can iteratively optimize the reproduction scripts based on multiple feedbacks, thereby progressively approaching accurate issue reproduction. The Referee component also boosts AEGIS performance by providing oversight and recommendations from external referees, which help mitigate the Reproducer Agent's overconfidence. The Restart component improves AEGIS performance in the PASS@1 setting, indicating that accumulating failure experiences aids in exploring diverse reproduction ways. Although the metric improves with the w/o Restart-in-FMO variant in the PASS@5 setting, it consumes approximately twice the time as AEGIS. It is noteworthy that the w/o Rank-in-FMO variant shows notable improvements in the $F\\rightarrow P$ and $\\frac{F\\rightarrow P}{F \\rightarrow X}$ metrics. Selecting only the double-verified reproduction scripts is indeed more precise. However, this variant discards too many other potential reproduction scripts, resulting in a much low F \u2192 P metric."}, {"title": "5.3 RQ3: Agentless's Resolved Rate Improvement via AEGIS", "content": "To answer RQ3, we explore the impact of reproduction scripts generated by AEGIS on the resolved rate of Agentless [55]. Agentless is a straightforward method for automatically resolving software issues, consisting of two stages: localization and repair. Agentless leverages LLM to generate multiple potential patches, applying regression testing [54] to filter out those failing the existing"}, {"title": "5.4 RQ4: Influence of Hyper-parameters on AEGIS", "content": "To answer RQ4, we explore the impact of different hyper-parameters, including the number of restarts and edits per restart in the FSM-based multi-feedback optimization module."}, {"title": "5.4.1 Number of restarts", "content": "We experiment on how the number of restarts impacts the performance of AEGIS. Figure 5 illustrates the performance of AEGIS on the F \u2192 P metric with different numbers of restarts. As the number of restarts increases(from 1 to 5), the F \u2192 P metric rises from 30.7% to 33.7%. This indicates that as the accumulation of failure experience, AEGIS can incorporate more suggestions and make divergent attempts. Each restart offers AEGIS new opportunities to explore different script generation ways while avoiding previous failures. However, as the number of restarts continues to rise(from 7 to 9), the F \u2192 P metric stabilizes. This implies that after a certain number of restarts, further increases have a limited impact on performance, likely because AEGIS has already utilized all available failure experiences and cannot derive additional insights."}, {"title": "5.4.2 Number of edits per restart", "content": "Similarly, as the number of edits increases, the F \u2192 P metric ascends from 30.0% to 32.0%(from 1 to 5). This demonstrates that AEGIS can effectively leverage feedback from error messages, syntax checks, patch checks, and referee verification to iteratively optimize the reproduction script. However, as the number of edits further increases(from 5 to 9), the F \u2192 P metric tends to stabilize, indicating that further modifications based on the feedback become increasingly challenging. This stabilization may be attributed to AEGIS reaching an optimization bottleneck, where further improvements require more complex modifications or higher quality feedback."}, {"title": "5.5 RQ5: Distribution of Successfully Reproduced Bugs by AEGIS", "content": "To answer RQ5, we present the statistics of successfully reproduced bugs by AEGIS from different aspects, including issue length, time, and projects.\nIssue Length: Figure 6(a) depicts the distribution of successfully reproduced bugs and total bugs in terms of issue length. The rates of successfully reproduced bugs are 34.6%, 39.7%, and 29.5% for issues with 0-50 words, 50-200 words, and longer than 200 words, respectively. The result indicates that when the issue descriptions are too brief, the insufficient information hinders the reproduction, while overly long issues make it difficult for agents to understand the issue well.\nTime: Figure 6(b) shows the distribution of successfully reproduced bugs and total bugs throughout the past decade. It reveals a consistent upward trend in the number of GitHub issues in SWE-Bench lite, ascending from 1 in 2012 to 57 in 2022. Concurrently, the number of successfully reproduced bugs has also risen, from 1 in 2014 to 24 in 2022. However, the reproduction rate has been fluctuating. While the growth in repository size increases the availability of test cases for reference, it also introduces additional complexity in locating and analyzing bugs.\nProjects: Figure 6(c) presents the projects ranked by the number of successfully reproduced bugs. The proportion of successfully reproduced bugs is highest in django [12] and sympy [49] projects, with 38 and 36 issues, respectively. This trend can be attributed to the more complete testing frameworks and well-structured issue descriptions. In contrast, projects like pytest [39], requests [38], and flask [37] show fewer reproductions. The lower number of requests and flask may be due to their reliance on well-configured network environments. Additionally, many pytest reproduction scripts utilize its proprietary test framework, where successful execution does not necessarily equate to passing tests."}, {"title": "6 DISCUSSION", "content": "6.1 AEGIS Performance Against Current Challenges\nIn Section 2, we conclude the challenges faced by the code agent in the general bug reproduction task, including entangled subtasks, lengthy retrieved context, and unregulated actions. As shown in Figure 4(b) We select four cases with different reproduction challenges from those successfully reproduced by AEGIS to better demonstrate its intelligent capabilities.\nThe first case('django-13757') suggests adding unit tests to the existing test files. AgentBaseline only retrieves three rounds of context before attempting to reproduce the bug, frequently returning to context retrieval until the rounds are exhausted. Additionally, AgentBaseline is hindered by modifications across multiple files. In contrast, AEGIS is restricted to modifying only the bug reproduction script and separates the context retrieval task from the script optimization task. By decoupling these subtasks, AEGIS can focus more effectively on the current objective, thereby improving performance.\nIn the second case("}]}