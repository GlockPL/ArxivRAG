{"title": "USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction\nand Gaussian Splatting", "authors": ["Kang Chen", "Jiyuan Zhang", "Zecheng Hao", "Yajing Zheng", "Tiejun Huang", "Zhaofei Yu"], "abstract": "Spike cameras, as an innovative neuromorphic camera that\ncaptures scenes with the 0-1 bit stream at 40 kHz, are\nincreasingly employed for the 3D reconstruction task via\nNeural Radiance Fields (NeRF) or 3D Gaussian Splat-\nting (3DGS). Previous spike-based 3D reconstruction ap-\nproaches often employ a casecased pipeline: starting with\nhigh-quality image reconstruction from spike streams based\non established spike-to-image reconstruction algorithms,\nthen progressing to camera pose estimation and 3D recon-\nstruction. However, this cascaded approach suffers from\nsubstantial cumulative errors, where quality limitations of\ninitial image reconstructions negatively impact pose esti-\nmation, ultimately degrading the fidelity of the 3D recon-\nstruction. To address these issues, we propose a syner-\ngistic optimization framework, USP-Gaussian, that uni-\nfies spike-based image reconstruction, pose correction, and\nGaussian splatting into an end-to-end framework. Lever-aging the multi-view consistency afforded by 3DGS and the\nmotion capture capability of the spike camera, our frame-\nwork enables a joint iterative optimization that seamlessly\nintegrates information between the spike-to-image network\nand 3DGS. Experiments on synthetic datasets with accurate\nposes demonstrate that our method surpasses previous ap-\nproaches by effectively eliminating cascading errors. More-\nover, we integrate pose optimization to achieve robust 3D\nreconstruction in real-world scenarios with inaccurate ini-\ntial poses, outperforming alternative methods by effectively\nreducing noise and preserving fine texture details.", "sections": [{"title": "1. Introduction", "content": "Spike cameras [10], inspired by the human retina [44], in-\ntroduce an innovative paradigm for encoding high-speed\ndynamic scenes. In contrast to conventional imaging sys-\ntems [18] that rely on fixed exposure intervals to integrate\nvisual information, spike cameras continuously sense dy-"}, {"title": "2. Related Work", "content": "While neuromorphic cameras generate high frame rate bit-\nstream information, it is imperative to transform them into\nmulti-bit grayscale images that are perceptible to human ob-\nservers. To achieve this Spike-to-Image target, a plethora of\nmethodologies [4, 5, 31, 36, 40, 41, 44] have been proposed,\nwhich can be broadly classified into model-based, super-\nvised, and self-supervised learning paradigms. Model-\nbased methods, rooted in the sampling principle of the spike\ncamera, aim to infer high-fidelity images drawing from the\nphysical spike-image relationship [43], biological retinal\nmodels [44], and short-term synaptic plasticity [41]. Con-\nfronted with pervasive dark current noise that is challenging"}, {"title": "2.2. Novel View Synthesis based on the Neuromor-phic Camera", "content": "Recent research has extensively employed neuromorphic\ncameras, including event cameras [8], which encode inten-\nsity difference in dynamic scenarios with low-latency and\nspike cameras [10], for 3D scene reconstruction.\nEvent Camera. Klenk et al. [12] proposed the E-NeRF\nbased on the event generation mechanism, which can re-\ncover sharp 3D scenes under high-speed camera motion.\nRudnev et al. [20] further harnessed a color event camera to\nlearn the color representation of scenes. Qi et al. [19] intro-\nduced E2NeRF with a multi-modality input, which synthe-\nsizes high-quality images of new viewpoints by exploiting\nthe complementary information between the blurry input\nand the event stream. To fully utilize the motion percep-\ntion capabilities of event cameras, further research [1, 16]\nfocused on recovering dynamic scenes with rigid deforma-\ntions from event streams. In addition to NeRF, EvGGS [25]\nintroduces a generalizable Gaussian splatting framework for\nevent cameras designed to reconstruct 3D scenes without\nretraining. EF-3DGS [14] develops a free-trajectory frame-\nwork by aligning the RGB input, event streams, and pose.\nEvent3DGS [27] presents an effective solution for rapid\nego-motion robotics scenarios.\nSpike Camera. Zhu et al. [45] design a differentiable\nSNN and integrate it into the NeRF training pipeline to sim-\nulate the generation of spike streams, where the loss func-\ntion is formulated based on the rendered spike output and\nthe input spike. Dai et al. [7] further introduce SpikeNVS by\nextending the spike cameras to RGB-Spike input. Addition-\nally, Guo et al. [9] introduced a color 3D scene reconstruc-\ntion framework by integrating Bayer-pattern spike streams\ninto the 3DGS training pipeline. SpikeGS [32] initially"}, {"title": "3. Preliminary", "content": "The spike camera consists of the photodetector, voltage in-\ntegrafor, comparator, and synchronous readout as shown in\nFig. 2. Within the designated time interval $T = [t_s, t_e]$, the\nphotodetector continuously captures the light intensity $I(t)$\nand converts it into the voltage signal $V(t) = \\mu I(t)$, where\n$\\mu$ is the photoelectric conversion coefficient. Concurrently,\nthe comparator assesses the integrator voltage $A(t)$ against\nthe predefined threshold $\\Theta$. When the threshold is met, the\nspike is emitted and the integrator voltage is reset, formu-\nlated as:\n$$A(t) = \\int_{t_s}^{t_e} I(t) dt \\mod C,$$\nwhere $C = \\Theta /\\mu$ is a constant proportional to the spike\nfiring threshold $\\Theta$. While the integrator continuously ag-\ngregates voltage, the readout circuit synchronously retrieves\nspike signals from the registers at up to 40 kHz. Spikes fired\nduring the specified interval $T$ are represented as a three-\ndimensional bit stream $S \\in {0,1}^{K\\times H\\times W}$, where $H$ and\n$W$ indicate the image height and width, with $K$ denoting\nthe spike sequence length."}, {"title": "3.2. 3D Gaussian Splatting", "content": "3D Gaussian Splatting leverages a set of three-dimensional\nGaussian primitives, symbolized as ${G}_{n=1}^{N_{gs}}$, to effectively"}, {"title": "4. Methods", "content": "Given spike streams ${S}_{v=1}^{V}$\nfrom $V$ different viewpoints with initial inaccurate poses\n${P}_{v=1}^{V}$, the USP-Gaussian is designed to simultaneously\noptimize the Recon-Net, poses, and Gaussian primitives as\nshown in Fig. 1, mathematically formulated as:\n$$(S, P, G, \\theta) \\xrightarrow{\\text{USP-Gaussian}} (P^*, G^*, \\theta^*),$$\nwhere $S$ and $P$ represent the sets of spike streams and poses\nfrom $M$ viewpoints, $G$ denotes the ensemble of Gaussian\nprimitives, and $\\theta$ encapsulates the Recon-Net parameters.\nThe notation $(*)$ indicates the optimized parameters."}, {"title": "4.1. Spike-based Image Reconstruction Network", "content": "The Recon-Net $F$ is designed to facilitate the mapping from\nthe spike input to the sharp image as shown in Fig. 2(b), for-\nmulated as $I(t) = F(S_T,S_t, t; \\theta)$ with $t$ signifying the central\nmoment of the interval $T$. Prior research [4, 31, 36] em-\nploys a fixed and brief window (41 frames in their config-\nuration) of the spike stream as the network input. How-\never, this representation is insufficient for scenarios with\nlow spike firing rates due to the limited information em-\nbedded within.\nWhile extending the window size allows the network to\nencompass more data from the spike stream, it also compli-"}, {"title": "4.2. Spike-based Gaussian Splatting", "content": "Suppose that retrieving the pose for each timestamp within\nthe interval is straightforward. Following Sec. 4.1, we uni-\nformly obtain $M$ poses from the interval $T$, yielding a pose\nsequence ${T(t_m)}_{m=1}^{M}$ with the corresponding image se-\nquence ${I_{gs}(t_m)}_{m=1}^{M}$ rendered via the 3DGS rasterization\npipeline as shown in Fig. 3.\nWhile reconstructing a sequence of images from the\nspike stream by shortening the exposure interval in Eq. (6)\nfor the training of 3DGS sounds feasible, these short-\nexposure images are noisy and lack texture details [3]. To\nthis end, we utilize the long-exposure image $E(T)$ as the su-\npervision and design the motion-reblur loss for 3DGS sim-\nilar to Eq. (7), expressed as:\n$$L_{gs} = L(\\frac{1}{M}\\sum_{m=1}^{M} I_{gs}(t_m), E(T))$$"}, {"title": "4.3. Spike Camera Trajectory Modeling", "content": "Building on the learnable pose representation during cam-\nera exposure in previous research [26, 37], we employ a\nsimilar approach to optimize the camera pose throughout\nthe interval $T$ as shown in Fig. 3. For each spike stream\n$S_T$, we optimize both the start pose $T_{start}$ and end pose $T_{end}$\nover the interval, while intermediate poses are represented\nthrough interpolation between these two.\nFor the pose $T(t_m)$ at time $t_m$, we utilize linear inter-\npolation in the Lie algebra of SE(3) to estimate the pose at\ntime t, formulated as:\n$$T(t_m) = T_{start} \\cdot exp\\left(\\frac{t_m}{T}log(T_{start}^{-1} \\cdot T_{end})\\right).$$", "formula": "T(t_m) = T_{start} \\cdot exp\\left(\\frac{t_m}{T}log(T_{start}^{-1} \\cdot T_{end})\\right)."}, {"title": "4.4. Joint Optimization", "content": "Given the spike stream $S_T$ along with the start pose $T_{start}$\nand the end pose $T_{end}$ to be optimized, we perform evenly\nspaced sampling interpolation as in Eq. (10) during the in-\nterval, obtaining the pose sequence ${T(t_m)}_{m=1}^{M}$.\nFor each pose in the sequence, we select a short spike\nstream centered around the timestamp $t_m$, obtaining the\nimage sequence ${I_{rec}(t_m)}_{m=1}^{M}$ based on the Recon-Net in\nSec. 4.1 and the image sequence ${I_{gs}(t_m)}_{m=1}^{M}$ rendered\nfrom the 3DGS in Sec. 4.2 as shown in Fig. 3. While both"}, {"title": "5. Experiment", "content": "We construct the synthetic dataset\nbased on scenes from Deblur-NeRF [15] for quantitative\nevaluation. Following the motion trajectory specified in\nDeblur-NeRF, we start with rendering sharp images at a\nresolution of 400 \u00d7 600 for each camera viewpoint, with\n12 additional intermediate frames inserted between consec-\nutive views. Subsequently, we employ the XVFI [22] to\ninsert 7 images between each frame pair. We employ the\nspike simulator on the enhanced video sequence to emulate\nthe real-world spike stream as in prior research [3], thus"}, {"title": "5.2. Experimental Results", "content": "We compare our USP-Gaussian with two-stage cascading\npipelines and approaches specially designed for the spike\ncamera, i.e., SpikeNeRF [45] and SpikeGS [32]. For the\ntwo-stage approach, we initially apply TFP [43], TFI [43],\nand Spk2ImgNet [36] to reconstruct sharp images from the\ninput spike, followed by the common 3DGS processing\npipeline. All Gaussian-based methods are conducted based\non the same 3DGS backbone, and identical camera poses\ninput for a fair comparison. While SpikeGS requires sev-\neral poses per spike stream for the exposure loss, which\npresents a notable challenge for COLMAP in real-world\ndata, we omit the exposure loss and retain the rest of the\nframework. All comparative metrics are calculated based\non the 3D reconstruction task, with further comparison on\nthe Spike-to-Image reconstruction task and implementation\ndetails available in the supplementary materials."}, {"title": "5.3. Ablation Study", "content": "In this subsection, we demonstrate the effectiveness of our\nproposed joint optimization, complementary spike input,\nand multi-reblur loss strategies.\nJoint Learning. The highlight of USP-Gaussain lies\nin the design of the joint loss, which aligns outputs from\nthe 3DGS and Recon-Net thus achieving complementary\nperformance improvement. We conduct the ablation ex-\nperiment on loss functions of $L_{gs}$, $L_{recon}$ and $L_{joint}$ with\nquantitative results listed in Tab. 2. As illustrated in the\ntable, the incorporation of the joint loss (ID-V) yields\nnotable improvements in PSNR (18.24%/11.89%), SSIM\n(14.93%/20.76%), and LPIPS (15.38%/50.28%) for both\n3DGS and Recon-Net compared to independent optimiza-"}, {"title": "6. Conclusion", "content": "In this paper, we propose a unifying framework USP-\nGaussian for the collaborative optimization of spike-based\nimage reconstruction, pose correction, and Gaussian Splat-"}]}