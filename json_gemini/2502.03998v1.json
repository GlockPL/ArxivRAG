{"title": "Online Learning of Counter Categories and Ratings in PvP Games", "authors": ["Chiu-Chou Lin", "I-Chen Wu"], "abstract": "In competitive games, strength ratings like Elo are widely used to quantify player skill and support matchmaking by accounting for skill disparities better than simple win rate statistics. However, scalar ratings cannot handle complex intransitive relationships, such as counter strategies seen in Rock-Paper-Scissors. To address this, recent work introduced Neural Rating Table and Neural Counter Table, which combine scalar ratings with discrete counter categories to model intransitivity. While effective, these methods rely on neural network training and cannot perform real-time updates. In this paper, we propose an online update algorithm that extends Elo principles to incorporate real-time learning of counter categories. Our method dynamically adjusts both ratings and counter relationships after each match, preserving the explainability of scalar ratings while addressing intransitivity. Experiments on zero-sum competitive games demonstrate its practicality, particularly in scenarios without complex team compositions.", "sections": [{"title": "1. Introduction", "content": "An effective strength measurement is a cornerstone of both games and multi-agent systems, providing quantitative skill information to players and supporting matchmaking mechanisms that influence engagement, fairness, and overall user experience (Schell, 2008). Beyond games, these mechanisms also play a vital role in training artificial agents, where balanced matches promote robust learning and strategy development (Vinyals et al., 2019).\nTo quantify player strength, widely-used rating systems such as Elo, Glicko, Whole-History Rating (WHR), TrueSkill, and Matchmaking Rating (MMR) offer scalar evaluations of individual skill levels (Elo, 1966; Glickman, 1999; Coulom, 2008; Herbrich et al., 2006; Pramono et al., 2018; Ebtekar & Liu, 2021). These systems have demonstrated their effectiveness in competitive games, from traditional board games to modern online multiplayer titles. Scalar ratings outperform simple win-rate statistics by accounting for disparities in player skill, thereby creating more balanced and meaningful matches. However, they are fundamentally limited in handling complex win rate intransitivity, such as counter relationships found in games like Rock-Paper-Scissors or Pok\u00e9mon, where strategies cyclically dominate one another (Lin et al., 2024b; Pendurkar et al., 2023).\nAs illustrated in Figure 1, scalar rating systems often fail to capture intransitive strategy dynamics, which are common in games featuring cyclic dominance among strategies. Such dynamics enrich the strategic diversity of gameplay by encouraging players to explore different approaches. Players naturally favor specific strategies that align with their preferences or expertise, further contributing to the variety of interactions (Lin et al., 2021, 2024a). However, this diversity presents a significant challenge for scalar rating systems, which are built on the assumption of transitive relationships."}, {"title": "2. Background", "content": "To propose a new rating system, it is essential to first revisit the foundations of existing rating systems, particularly the Bradley-Terry model and the widely used Elo rating system. These models form the basis for quantifying player strength and provide a framework for understanding how strengths translate into win probabilities. Building upon these, we introduce the Neural Rating Table (NRT) and Neural Counter Table (NCT), which serve as the foundation of our proposed system."}, {"title": "2.1 Bradley-Terry Model and Elo Rating", "content": "A common approach to model a player's strength is to describe it as a single numeric value. However, an important question is how this number relates to the player's actual performance in a competitive match. Bradley & Terry (1952) addressed this by proposing a simple yet powerful probabilistic model for two-player zero-sum games. The win probability of player i against player j is given by:\n$P(i > j) = \\frac{\\gamma_i}{\\gamma_i + \\gamma_j}$ (1)\nwhere $\\gamma$ denotes the positive real-valued strength of player X. This formulation intuitively represents how relative strengths affect win probabilities.\nTo ensure numerical stability and ease of computation, $\\gamma_x$ is often reparameterized using an exponential function, leading to:\n$P(i > j) = \\frac{e^{\\lambda_i}}{e^{\\lambda_i} + e^{\\lambda_j}}$ (2)\nWith further algebraic transformations, this can be expressed in a form that directly aligns with the commonly used Elo rating system:\n$P(i > j) = \\frac{1}{1+10^{(R_j-R_i)/400}}$ (3)\nwhere $R_i$ and $R_j$ represent the Elo ratings of players i and j, respectively (Elo, 1966).\nWhile the Bradley-Terry model provides a static representation of strength, the Elo rating system extends this by introducing an incremental update mechanism. After each game, player ratings are updated using a simple formula:\n$R'_A = R_A + K (S_A \u2013 E_A),$ (4)\nwhere $R'_A$ is the updated rating, $S_A$ is the actual result (1 for a win, 0 for a loss), $E_A$ is the expected probability of winning, and K is a constant controlling the update magnitude.\nElo's incremental update mechanism makes it well-suited for real-time applications in competitive games. Its simplicity, transparency, and ease of interpretation have contributed to its widespread adoption. Our proposed rating system builds upon these principles to maintain player-friendliness while incorporating enhanced modeling capabilities."}, {"title": "2.2 Neural Rating/Counter Table", "content": "Building on the Bradley-Terry model, Lin et al. (2024b) proposed the Neural Rating Table (NRT) and Neural Counter Table (NCT) to reduce the complexity of game balance analysis and address the intransitivity problem that the Bradley-Terry model cannot handle. For example, scenarios such as A > B, B > C, and C > A illustrate the intransitivity of strength relationships, which scalar ratings struggle to model. The NRT and NCT introduce neural networks to capture these relationships and provide a structured approach for counter category learning.\nThe Neural Rating Table (NRT) is a rating predictor that extends the Bradley-Terry model. While the original model is limited to two-player settings, NRT generalizes to team-based matches by aggregating the players in a team into a single entity. A Siamese neural network is employed to approximate the strength of unseen team combinations based on observed data, significantly reducing the need for exhaustive lookup tables while capturing synergy effects among team members.\nBuilding upon NRT, the Neural Counter Table (NCT) addresses the intransitivity of strength relationships that scalar ratings cannot model. Unlike multi-dimensional rating systems (Chen & Joachims, 2016; Balduzzi et al., 2018; Vadori & Savani, 2024), NCT simplifies the representation of counter relationships by clustering residual win values $W_{res}$ into discrete counter categories. The residual win value quantifies the deviation between the actual match outcome W and the Bradley-Terry model's prediction, defined as:\n$W_{res} (A, B | R_\\Theta) = W - \\frac{R_\\Theta(A)}{R_\\Theta(A) + R_\\Theta(B)},$ (5)\nwhere $R_\\Theta(A)$ and $R_\\Theta(B)$ denote the predicted strengths of teams A and B, respectively.\nA neural network maps team compositions into counter categories, leveraging a static dataset and a pretrained NRT model. By employing vector quantization techniques (van den Oord et al., 2017), NCT clusters residual win values into a finite set of interpretable categories. This process effectively translates complex intransitive relationships into discrete counter categories, offering a domain-agnostic framework for representing strategic interactions.\nThe final win probability prediction combines both the scalar rating from NRT and the residual adjustment from NCT, expressed as:\n$P(A > B) = \\frac{R_\\Theta(A)}{R_\\Theta(A) + R_\\Theta(B)} + W_{res} (A, B | C_\\Theta),$ (6)\nwhere $W_{res} (A, B | C_\\Theta)$ is retrieved from the learned counter table $C_\\Theta$ based on the predicted counter categories of A and B. This formulation allows the model to capture intransitive relationships while preserving the interpretability and efficiency of scalar ratings.\nWhile NRT and NCT demonstrate promising results in modeling synergy and counter relationships, they rely on neural networks and static datasets, limiting their suitability for incremental rating systems like Elo. To address this limitation, we propose an online learning algorithm that integrates counter category learning into the Elo framework, allowing for real-time updates and maintaining the simplicity of scalar ratings."}, {"title": "3. Elo Residual Counter Category", "content": "To design an online update algorithm for NRT and NCT, we clarify that this work does not address the complexity of general team compositions. Although NRT is capable of handling such scenarios to some extent, we simplify the model by focusing solely on individual ratings. Specifically, we adopt the standard Elo rating system to serve as the foundation for our approach, avoiding additional formulations required for team-based settings. While methods like the generalized Bradley-Terry model (Hunter, 2004) could provide solutions for more complex scenarios, our aim is to propose a straightforward method applicable to individual player settings.\nThe primary challenge, therefore, lies in implementing an online version of NCT. In particular, we must determine how to assign and update counter categories dynamically for each individual. Below, we introduce our approach and detail the proposed rating system algorithm."}, {"title": "3.1 Key Ideas", "content": "In the original NCT paper (Lin et al., 2024b), the authors first trained an NRT model to establish a stable rating foundation before clustering the residual win values ($W_{res}$). However, in an online setting, obtaining a stable $W_{res}$ prediction is challenging due to the absence of a pre-trained model for reference and the lack of batch outcomes to calculate the average residual values.\nTo address this limitation, we model a categorical distribution for each individual, initialized uniformly. After each match, player ratings are updated using the Elo formula. Simultaneously, counter categories for both individuals are sampled from their respective distributions. Although these sampled categories may not always be accurate, the sampling mechanism enables exploration to determine the best-fitting category for each individual.\nUsing the sampled categories and the residual win value derived from the match outcome and Elo ratings, we update the MX M counter table through tabular regression. Additionally, we maintain an expected residual win value for each individual across all possible categories. While an individual's true category remains uncertain, observing the sampled opponent's category allows us to iteratively refine these expected residuals, enabling the inference of the most suitable category.\nThis two-step process-learning individual residual values and refining category assignments-can be interpreted as an expectation-maximization (EM) algorithm (Dempster et al., 1977). Based on the inferred best category, we refine the categorical distribution for future matches, as illustrated in Figure 4.\nTo ensure stability in win value prediction, the category with the highest probability is used during inference, rather than sampled categories."}, {"title": "3.2 Algorithm", "content": "The proposed algorithm follows these steps:\n1. Elo Rating Update: Update player ratings using the standard Elo formula.\n2. Counter Table Update: Sample counter categories for both players from their categorical distributions and update the MXM counter table using tabular regression.\n3. Expected Residual Update: Refine the expected residual win values for each individual based on the sampled opponent's category.\n4. Category Refinement: Identify the best-fitting categories by minimizing the discrepancy between the counter table and expected residuals. Update categorical distributions accordingly.\nThe complete algorithm is presented in Algorithm 1, leveraging softmax for modeling categorical distributions.\nAlgorithm 1 Online Update Algorithm for Elo Residual Counter Category\nRequire: Initial ratings R, category distributions C, counter table T, expected residual table E, learning rates \u03b7\u03c1, \u03b7\u03c4, \u03b7\u03c2, number of categories M.\nEnsure: Updated ratings R and category distributions C.\n1: for each match (i, j) with result Oi (1 for win, 0 for loss, 0.5 for tie) do\n\u25b7 Step 1: Elo Rating Update\n2:  $P_i = \\frac{1}{1+10^{(R_j-R_i)/400}}$\n3:  $R_i \u2190 R_i + \u03b7_R(O_i - P_i)$\n4:  $R_j \u2190 R_j + \u03b7_R((1 \u2013 O_i) \u2013 (1 \u2013 P_i))$\n\u25b7 Step 2: Counter Table Update\n5:  Sample $c_i\u223c C_i, c_j \u223c C_j$\n6:  $W_{res} \u2190 O_i - P_i$\n7:  $T[c_i, c_j] \u2190 T[c_i, c_j] + \u03b7_\u03c4(W_{res} \u2013 T[c_i, c_j])$\n8:  $T[c_j, c_i] \u2190 -T[c_i, c_j]$\n\u25b7 Step 3: Update Expected Residuals\n9:  $E_i [c_j] \u2190 E_i [c_j] + \u03b7_\u03c4 (W_{res} - E_i [c_j])$\n10:  $E_j [c_i] \u2190 E_j [c_i] + \u03b7_\u03c4(-W_{res} - E_j [c_i])$\n\u25b7 Step 4: Category Refinement\n11:  $D_i[c] \u2190 |\u03a3T[c, c'] - E_i[c']|$\n12:  $D_j[c] \u2190 |\u03a3T[c, c'] \u2013 E_j [c']|$\n13:  $c \u2190 arg min_c D_i[c]$\n14:  $c \u2190 arg min_c D_j [c]$\n\u25b7 $\u03b4_{c',c}$ is a one-hot encoded vector:\n15:  $\u03b4_{c',c} =\\begin{cases}   1, & \\text{if } c=c', \\\\   0, & \\text{otherwise.}  \\end{cases}$\n16:  $C_i[c] \u2190 C_i[c] + \u03b7_c (\u03b4_{c, -} C_i[c])$\n17:  $C_j[c] \u2190 C_j[c] + \u03b7_c (\u03b4_{c, -} C_j[c])$\n18: end for\nThis algorithm provides an efficient and interpretable framework for the online learning of ratings and counter categories. Experimental results confirm its ability to achieve comparable or superior performance to NCT in scenarios without complex team combinations."}, {"title": "4. Experiment", "content": "To validate the effectiveness of our proposed algorithm, we adopt the evaluation process from Lin et al. (2024b), utilizing their publicly available datasets and evaluation codes. This ensures a direct comparison with the Neural Rating Table (NRT) and Neural Counter Table (NCT), which serve as benchmarks for our method.\nAlthough these datasets are originally designed for analyzing the strength relationships between card decks or civilization choices to study game balance, they are well-suited for assessing rating systems. Unlike human player ratings, which may involve subjective factors, these datasets provide clear and objective strength relationships with well-documented evidence, allowing for rigorous evaluation of strength prediction accuracy."}, {"title": "4.1 Datasets and Evaluation Method", "content": "The datasets used in our experiments are collected from two-player zero-sum player-versus-player (PvP) games, including both synthetic toy games and real-world popular online games. Below, we describe each dataset in detail:"}, {"title": "4.1.1 Rock-Paper-Scissors", "content": "The classical Rock-Paper-Scissors dataset consists of 3 individuals (rock, paper, and scissors), with win outcomes represented as 1, 0, or 0.5 for win, lose, or tie, respectively. Matches are generated by uniformly sampling strategies for both players, resulting in a dataset of 100,000 matches."}, {"title": "4.1.2 Advanced Combination Game", "content": "This synthetic game features an element pool of size 20. Each player selects three unique elements from the pool to form a team. The score $s_c$ of a team c is the sum of its elements. The win probability between two teams is determined by the distribution:$P(c_1 > c_2) = \\frac{(s_{c_1})^2}{(s_{c_1})^2+(s_{c_2})^2}$. There are $C^{3}_{20}$ = 1,140 unique teams in this game. To introduce strategic elements beyond numerical summation, each team is assigned a category $T = s_c mod 3$, representing Rock (0), Paper (1), or Scissors (2). Teams categorized as Rock, Paper, or Scissors receive a +60 score bonus against their respective weaker category during win-lose sampling. This modification introduces strategy dynamics while preserving numerical selection as a fundamental aspect. The dataset contains 100,000 matches, uniformly sampled across possible team combinations."}, {"title": "4.1.3 Age of Empires II (AoE2)", "content": "Age of Empires II is a classic and widely popular real-time strategy (RTS) game where players control civilizations to gather resources, build armies, and engage in tactical battles (Ensemble Studios, 1999; Forgotten Empires et al., 2019a,b). The dataset for this game is derived from match records of 1-on-1 ranked matches, collected and provided by the aoestats website. These records are from patch 99311 and include games played on random maps across all Elo brackets. The dataset features 45 civilizations as testing individuals, with a total of 1,261,288 matches."}, {"title": "4.1.4 Hearthstone", "content": "Hearthstone is a widely popular collectible card game (CCG) where players build decks and compete in turn-based matches, leveraging strategic card synergies and counterplays (Blizzard Entertainment, 2014). The dataset for this game focuses on named decks rather than all possible deck configurations. These named decks are collected from the HSReplay platform. Specifically, the dataset includes 91 predefined decks as testing individuals, with match records sourced from standard-ranked games at the Gold level in January 2024. In total, the dataset comprises 10,154,929 matches."}, {"title": "4.1.5 Evaluation Method", "content": "We follow the evaluation protocol outlined in Section 4.2 of the work by Lin et al. (2024b). Pairwise win rates between individuals are computed, and thresholds are used to categorize the strength relations as stronger, weaker, or equal. Specifically:\n\u2022 A win rate within the range [0.499, 0.501] indicates equal strength.\n\u2022 A win rate outside this range classifies one individual as stronger or weaker than the other, depending on the direction.\nThe ground truth for strength relations is derived from these thresholds. The accuracy of a rating method is determined by its ability to predict the same strength relations as the ground truth. All models are trained for 100 epochs (each epoch utilizes the full training set once) using 5-fold cross-validation. The datasets include predefined static splits for cross-validation to ensure consistency."}, {"title": "4.2 Strength Relation Accuracy", "content": "In this section, we evaluate the performance of our proposed Elo-RCC algorithm against several baseline methods. To assess the impact of different counter category sizes, we test M = 3,9, 27, 81, aligning with the configurations used in NCT. The methods compared include:\n\u2022 NRT: The Neural Rating Table, a neural network-based extension of the Bradley-Terry model. While it effectively models synergies between players or teams, it cannot address win value intransitivity."}, {"title": "5. Conclusion and Future Works", "content": "In this paper, we proposed Elo Residual Counter Category (Elo-RCC), a novel rating system that extends the Neural Counter Table (NCT) to support online updates with tabular approximations. The core innovation lies in modeling counter relationships through categorical distributions and leveraging an EM algorithm to iteratively refine pseudo-category labels. Experimental results demonstrate the effectiveness of our algorithm, particularly in popular online games, where it achieves comparable performance to NCT while offering a lightweight, real-time solution suitable for game balance analysis.\nThe applications of rating systems extend far beyond player ranking and matchmaking in games. They can be broadly applied to any competitive scenario, including image or video preference ranking, sports analysis, movie recommendation, peer grading, elections, and even the evaluation of large language models (Li et al., 2021; Chen & Joachims, 2016; Zheng et al., 2023). By providing a simpler and more accessible implementation of NCT, Elo-RCC has the potential to facilitate a wider range of applications in these and other domains.\nFuture work could explore further enhancements to Elo-RCC, such as adapting it for more complex team compositions, incorporating dynamic counter category sizes, or integrating it with neural network models for hybrid approaches. Additionally, investigating its performance in asymmetric or non-zero-sum games and extending its use cases to multi-agent learning frameworks could unlock new avenues for research and practical applications."}]}