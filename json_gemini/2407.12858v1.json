{"title": "Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)", "authors": ["Krishnaram Kenthapadi", "Mehrnoosh Sameki", "Ankur Taly"], "abstract": "With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems in high-stakes domains, ensuring the trustworthiness, safety, and observability of these systems has become crucial. It is essential to evaluate and monitor Al systems not only for accuracy and quality-related metrics but also for robustness, bias, security, interpretability, and other responsible AI dimensions. We focus on large language models (LLMs) and other generative Al models, which present additional challenges such as hallucinations, harmful and manipulative content, and copyright infringement. In this survey article accompanying our tutorial, we highlight a wide range of harms associated with generative Al systems, and survey state of the art approaches (along with open challenges) to address these harms.", "sections": [{"title": "1 Introduction", "content": "Considering the increasing adoption of Artificial Intelligence (AI) technologies in our daily lives, it is crucial to develop and deploy the underlying AI models and systems in a responsible manner and ensure their trustworthiness, safety, and observability. Our focus is on large language models (LLMs) and other generative AI models and applications. Such models and applications need to be evaluated and monitored not only for accuracy and quality-related metrics but also for robustness against adversarial attacks, robustness under distribution shifts, bias and discrimination against under-represented groups, security and privacy protection, interpretabil-ity, hallucinations (and other ungrounded or low-quality outputs), harmful content (such as sexual, racist, and hateful responses), jail-breaks of safety and alignment mechanisms, prompt injection at-tacks, misinformation and disinformation, fake, misleading, and manipulative content, copyright infringement, and other respon-sible Al dimensions.\nIn this tutorial, we first highlight key harms associated with generative Al systems, focusing on ungrounded answers (halluci-nations), jailbreaks and prompt injection attacks, harmful content, and copyright infringement. We then discuss how to effectively ad-dress potential risks and challenges, following the framework of identification, measurement, mitigation (with four mitigation lay-ers at the model, safety system, application, and positioning lev-els), and operationalization. We present real-world LLM use cases, practical challenges, best practices, lessons learned from deploy-ing solution approaches in the industry, and key open problems. Our goal is to stimulate further research on grounding and evalu-ating LLMs and enable researchers and practitioners to build more robust and trustworthy LLM applications.\nWe first present a brief tutorial outline in \u00a71.1, followed by an elaborate discussion of different responsible Al dimensions in \u00a72. We devote \u00a73 to the problem of grounding for LLM applications, and \u00a74 to the emerging area of \"LLM operations\". For each dimen-sion (discussed in \u00a72 to \u00a74), we present key business problems, tech-nical solution approaches, and open challenges."}, {"title": "1.1 Tutorial Overview", "content": "Our tutorial consists of the following parts:\nIntroduction and Overview of LLM Applications. We give an overview of the generative AI landscape in industry and motivate the topic of the tutorial with the following questions. What con-stitutes generative AI? Why is generative AI an important topic? What are key applications of generative AI that are being deployed across different industry verticals? Why is it crucial to develop and deploy generative AI models and applications in a responsible man-ner?\nHolistic Evaluation of LLMs. We highlight key challenges that arise when developing and deploying LLMs and other generative AI models in enterprise settings, and present an overview of solution approaches and open problems. We discuss evaluation dimen-sions such as truthfulness, safety and alignment, bias and fairness, robustness and security, privacy, model disgorgement and unlearn-ing, copyright infringement, calibration and confidence, and trans-parency and causal interventions."}, {"title": "2 Holistic Evaluation of LLMs", "content": "The overarching goal of evaluation is to determine whether a trained LLM is fit for deployment in an enterprise setting. A commonly quoted maxim is that LLMs must ensure helpful, truthful, and harmless responses [6]. While this seems straightforward, each of these di-mensions has several nuances. For instance, lack of truthfulness can range from subtle misrepresentations to making blatant false statements (colloquially known as \"hallucinations\") [48]. Similarly, harmful responses can vary from racially biased responses, to vio-lent, hateful, and other inappropriate responses, to responses caus-ing social harm (e.g., instruction on how to cheat in an examination without getting caught). Further, in the context of evaluating LLMs, it is important to be aware of shortcomings that have been high-lighted with human and automatic model evaluations and with commonly used datasets for natural language generation [37].\nBesides evaluations of response quality, practitioners also have to worry about training data privacy, model stealing, copyright vi-olations, and security risks such as jailbreaking [137] and prompt injection [121]. In some settings, one may also seek calibrated con-fidence scores for responses, interpretability, and robustness to ad-versarial prompts.\nIn the rest of this section, we outline several evaluation dimen-sions that arise in enterprise deployments. Evaluation of LLMs is an important topic and there have been a number of dedicated frameworks [33, 66, 83] describing evaluation datasets, metrics, and benchmarks for various dimensions. A growing collection of tools and resources have been proposed across different phases of LLM development [71]. Here, we focus on the key business con-cerns, leading solution approaches, and open challenges for each evaluation dimension."}, {"title": "2.1 Truthfulness", "content": "Business problems: How do we ensure that LLM responses are informed, relevant, and trustworthy? How do we detect and re-cover from hallucinations?\nSolution approaches: There is extensive work on hallucinations in LLMs [48, 54], including, the causes and sources of hallucina-tions [77], and measures for evaluating LLMs based on their vul-nerability to producing hallucinations [94]. A variety of methods have been proposed to detect hallucinations, ranging from sam-pling based approaches [75] to approaches leveraging internal states of the LLM [104]. There is also early work on detecting and pre-venting hallucinations in large vision language models [41] and other multimodal foundation models [128].\nA number of methods have been proposed to fundamentally re-duce hallucinations by tuning models. One line of work involves training or fine-tuning LLMs on highly curated textbook-like datasets [40, 134]. Another approach involves fine-tuning LLMs on pref-erence data for factuality, i.e., response pairs ranked by factual-ity [109]. A fundamental hypothesis here is that LLMs have sys-tematic markers for when they are being untruthful [59, 110]. The fine-tuning process aims to train LLMs to tap into these markers and upweight factual responses. Related to this, it has been conjec-tured that LLMs internalize different \"personas\" during pretrain-ing, and by training on truthful question-answer pairs, one can upweight the \"truthful\u201d persona (even on unseen domains) [58]. Reducing hallucination on a synthetic task has been explored as a way to reduce hallucination on real-world downstream tasks [57]. Finally, a recent work shows that fine-tuning LLMs on new infor-mation that was not acquired during pretraining can encourage the model to hallucinate [38]. Curating fine-tuning sets to avoid this issue paves another path to reducing hallucinations.\nWhile truthful responses are table stakes for enterprise deploy-ments, we may want to go one step further and ensure that all responses are aligned with a specific knowledge base (e.g., a set of enterprise documents). This is known as grounding. This is a vast topic in itself, and therefore we dedicate \u00a73 entirely to it.\nFinally we emphasize that not all hallucinations are equally bad. For instance, hallucinations in response to nonsensical prompts or prompts with false premises (see [115] for examples of questions whose premises are factually incorrect and hence ideally need to be rebutted) are relatively less concerning than hallucinations in re-sponse to well-meaning prompts. Furthermore, hallucinations in high stakes verticals like healthcare and life sciences may be far more concerning than hallucinations in other verticals.\nOpen challenges: A key open challenge is detecting hallucina-tions in video, speech, and multimodal settings. Another open chal-lenge is getting LLMs to generate citations when they answer from parametric knowledge. More specifically, can the LLM be made aware of document identifiers during pre-training, similar to the work on differential search indexes [108], so that it can generate the appropriate markers as citations for various claims in its re-sponse? A broader challenge is to leverage ideas and lessons from search and information retrieval literature [80, 136] to improve rel-evance, trustworthiness, and truthfulness of LLM responses. For example, how can we incorporate valuable information such as document authors, document quality, authoritativeness of the do-main, timestamp, and other relevant metadata during pre-training and subsequent stages of LLM development?"}, {"title": "2.2 Safety and Alignment", "content": "Business problems: How do we prevent an LLM from generating toxic, violent, offensive, or otherwise unsafe output? How do we detect such content in cases where prevention fails to work? How do we ensure that the responses from an LLM are aligned with hu-man intent even in settings where it is hard for human experts to verify such alignment?\nSolution approaches: The problem can be addressed during dif-ferent stages of the LLM lifecycle. During data collection and cura-tion, we can apply mechanisms to detect unsafe content and take remedial steps, such as excluding or modifying such content. Dur-ing pretraining and fine-tuning, we can incorporate constraints or penalties to discourage the learning of unsafe sequences. In the reinforcement learning from human feedback (RLHF) stage, we can include response pairs with preference labels on which one is more appropriate, and tune the model to \"align\" its responses with the preferences [18]. As part of prompt engineering, we can include instructions to discourage the LLM from generating unde-sirable outputs. Finally, when prevention fails, we can apply toxic-ity classifiers to detect undesirable outputs (as well as undesirable inputs) and flag such instances for appropriate treatment by the user-facing Al applications.\nAnother direction in alignment research is leveraging more pow-erful LLMs to detect safety and alignment issues with a weaker LLM in a cost-effective and latency-sensitive fashion. The prob-lem can be framed as a constrained optimization problem: given cost or latency constraints, determine the subset of prompts and responses to be evaluated using a more powerful LLM (e.g., GPT-4). In certain settings, the task to be evaluated could be too hard for even human experts (e.g., comparing two different summaries of a very large collection of documents or judging the quality of hy-potheses generated based on a large volume of medical literature), necessitating the use of powerful LLMs in a manner that aligns with human intent. The converse problem of leveraging less pow-erful LLMs to align more powerful LLMs with human intent has also been explored in alignment research. A related challenge is to ensure that Al systems with superhuman performance (which could possibly be smarter than humans) are designed to follow hu-man intent. While current approaches for Al alignment rely on human ability to supervise AI (using approaches such as reinforce-ment learning from human feedback), these approaches would not be feasible when Al systems become smarter than humans [13].\nOverall, alignment is an active area of research, with approaches ranging from data-efficient alignment [55] to alternatives to RLHF [25] to aligning cross-modal representations [84].\nOpen challenges: There has a been a bunch of recent work on generating adversarial prompts to bypass existing mechanisms for mitigating toxic content generation [119, 137]. A key open chal-lenge is mitigating toxic content generation even under such adver-sarial prompts. Recent research has shown that LLM based guardrail models could themselves be attacked. For instance, a two-step prefix-based attack procedure - that operates by (a) constructing a uni-versal adversarial prefix for the guardrail model, and (b) propagat-ing this prefix to the response - has been shown to be effective across multiple threat models, including ones in which the adver-sary has no access to the guardrail model at all [76]. How do we develop effective LLM based guardrails that are robust to such at-tacks (and even better, have provable robustness/security guaran-tees)? Another challenge lies in balancing reduction of undesirable outputs with preservation of the model's ability towards creative generation. Finally, as LLMs are increasingly deployed as part of open-ended applications, an important socio-technical challenge is to investigate the opinions reflected by the LLMs, determine whether such opinions are aligned with the needs of different appli-cation settings, and design mechanisms to incorporate preferences and opinions of relevant stakeholders (including those impacted by the deployment of LLM based applications) [101]."}, {"title": "2.3 Bias and Fairness", "content": "Business problems: How do we detect and mitigate bias in foun-dation models? How can we apply bias detection and mitigation throughout the foundation model lifecycle?\nSolution approaches: There is extensive work on detecting and mitigating bias in NLP models [12, 14, 15, 22, 36, 98]. In addition to known categories of bias observed in predictive ML models, new types of bias arise in LLMs and other generative AI models, e.g., gender stereotypes, exclusionary norms, undesirable biases to-wards mentions of disability, religious stereotypes, and sexual ob-jectification [10, 30, 106, 122]. Additionally, due to the sheer size of datasets used, it is difficult to audit and update the training data or even anticipate different kinds of biases that may be present. Mitigation approaches include counterfactual data augmentation (or other types of data improvements), finetuning, incorporating fairness regularizers, in-context learning, and natural language in-structions. For a longer discussion, we direct the readers to the survey by Gallegos et al. [30]. More broadly, we can view bias mea-surement and mitigation as an important component of building a reliable and robust application that works well across different subgroups of interest (including but not necessarily limited to pro-tected groups). By performing fine-grained evaluation and robust-ness testing across such groups, we can identify underperforming groups, improve the performance for such groups, and thereby po-tentially boost even the overall performance.\nOpen challenges: Bias and fairness mitigation is a relatively nascent space, and a key open question is identifying and designing prac-tical, scalable processes from the large class of bias measurement and mitigation techniques proposed for LLMs. A related challenge is ensuring that the bias mitigation approach does not cause the model to inadvertently demonstrate disparate treatment, which could be considered unlawful in a wide range of scenarios under US law [70]. Further, how do we audit LLMs and other genera-tive Al models for different types of implicit or subtle biases and design mechanisms to mitigate or recover from such biases, al-though the models may not show explicit bias on standard bench-marks [8, 45, 46]? It has recently been argued that harmful biases are an inevitable consequence arising from the design of LLMs as they are currently formulated, and that the connection between bias and fundamental properties of language models needs to be"}, {"title": "2.4 Robustness and Security", "content": "Business problems: How do we measure and improve the ro-bustness of LLMs and other generative AI models and applications against minor prompt perturbations, natural distribution shifts, and other unseen or challenging scenarios? How do we safeguard LLMs against manipulative efforts by bad actors to (jail-)break alignment, reveal system prompts, and inject malicious instructions into prompts (also called prompt injection attacks [121])?\nSolution approaches: Many techniques proposed for measuring and improving robustness in NLP models can be adopted or ex-tended for LLMs. In particular, the following ideas and notions could be relevant for LLMs: definitions, metrics, and assumptions regarding robustness (such as label-preserving vs. semantic-preserving); connections between robustness against adversarial attacks and ro-bustness under distribution shifts; similarities and differences in robustness approaches between vision and text domains; model-based vs. human-in-the-loop identification of robustness failures. Mitigation approaches involve learning invariant representations, and ensuring models do not rely on spurious patterns using tech-niques like data augmentation, reweighting, ensembling, inductive-prior design, and causal intervention [117]. Open-source evalua-tion frameworks and benchmarks such as Stanford HELM [66], Eleuther Harness [33], LangTest [83], and Fiddler Auditor [51] can be utilized for benchmarking different LLMs and evaluating robust-ness in application-specific settings.\nLLMs have been shown to be vulnerable to adversarial pertur-bations in prompts [135], prompt injection attacks [121], data poi-soning attacks [116], and universal and transferable adversarial at-tacks on alignment [137]. Several benchmarks have been proposed for red-teaming / testing LLMs against adversarial attacks and re-lated issues [31, 87, 135]. Metrics for quantifying LLM cybersecu-rity risks, tools to evaluate the frequency of insecure code sugges-tions, and tools to evaluate LLMs to make it harder to generate malicious code or aid in carrying out cyberattacks have also been proposed [11]. Additional discussion and approaches can be found in survey articles by Barrett et al. [9] and Yao et al. [127].\nOpen challenges: A key challenge is to ensure that robustness and security mechanisms are not intentionally or unintentionally removed in the process of finetuning an LLM [90]. Another chal-lenge lies in ensuring that the mechanisms work not just during evaluation but also during deployment (e.g., not subject to decep-tive attacks [49]). A broader challenge is to investigate robustness, security, and safety of systems that could be composed of multiple LLMs. For example, it has been shown that adversaries can mis-use combinations of models by decomposing a malicious task into subtasks, leveraging aligned frontier models to solve hard but be-nign subtasks, and leveraging weaker non-aligned models to solve easy but malicious subtasks [56]. As such attacks do not require the aligned frontier models to generate malicious outputs and hence can go undetected, there is a need to extend red-teaming efforts beyond single models in isolation."}, {"title": "2.5 Privacy, Unlearning, and Copyright Implications", "content": "Business problems: How do we ensure that LLMs, diffusion mod-els, and other generative Al models do not memorize training data instances (including personally identifiable information (PII)) and reproduce such data in their responses? How do we detect PII in LLM prompts / responses? How do prevent copyright infringe-ment by LLMs? How can we make an LLM / generative AI model forget specific parts, facts, or other aspects associated with the training data?\nSolution approaches: Recent studies have shown that training data can be extracted from LLMs [17] and from diffusion models [16] (which could have copyright implications in case the model is perceived as a database from which the original images or other copyrighted data can be approximately retrieved). Several approaches for watermarking [28, 39, 62] (or otherwise identifying / detecting [81]) AI generated content have been proposed. Detecting PII in LLM prompts / responses can be done using off-the-shelf packages, but may require domain-specific modifications since what is con-sidered as PII could vary based on the application. Unlearning in LLMs [68], and more broadly, model disgorgement [2] (\u201cthe elim-ination of not just the improperly used data, but also the effects of improperly used data on any component of an ML model\") are likely to become important for copyright and privacy safeguards, ensuring responsible usage of intellectual property, compliance, and related requirements as well for reducing bias or toxicity and increasing fidelity.\nOpen challenges: A key challenge would be designing practical and scalable techniques. For example, how can we develop differen-tially private model training approaches (e.g., DPSGD [1], PATE [85]) that are applicable for billions or trillions of parameters in gener-ative Al models? How can we ensure privacy of end users when leveraging inputs from end users as part of retraining of LLMs (us-ing, say, PATE-like approaches)? Considering the importance of high quality datasets for evaluating LLMs for truthfulness, bias, ro-bustness, safety, and related dimensions, and the challenges with obtaining such datasets in highly sensitive domains such as health-care, how do we develop practical and feasible approaches for dif-ferentially private synthetic data generation [7, 69, 107], poten-tially leveraging a combination of sensitive datasets (e.g., patient health records and clinical notes) and publicly available datasets along with the ability to generate data by querying powerful LLMs?"}, {"title": "2.6 Calibration and Confidence", "content": "Business problems: How can we deploy LLMs in a human-Al hy-brid setting to quantify the uncertainty (confidence score) associ-ated with an LLM response and defer to humans when confidence is low? Specifically, how can we achieve this in high-stakes and latency-sensitive domains such as Al models used in healthcare settings?\nSolution approaches: Learning to defer in human-AI settings is an active area of research [61], necessitating uncertainty quantifi-cation and confidence estimation for the underlying AI models. It also involves understanding the conditions under which humans can effectively complement AI models [24]. In the context of LLMs, recent approaches such as selective prediction, self-evaluation and calibration, semantic uncertainty, and self-evaluation-based selec-tive prediction have been proposed [20] (see references there-in).\nOpen challenges: A key challenge is to ensure that self-evaluation, calibration, selective prediction, and other confidence modeling ap-proaches for LLMs are effective in out-of-distribution settings. This is particularly important for adoption in high-stakes settings like healthcare. Another challenge is ensuring robustness of confidence modeling approaches against adversarial prompts."}, {"title": "2.7 Transparency and Causal Interventions", "content": "Business problems: How do we explain the inner workings and responses of LLMs and other generative Al models, especially in scenarios requiring the development of end-user trust and meeting regulatory requirements? How can we modify factual associations linked to an LLM without retraining it?\nSolution approaches: Explainability methods for LLMs have been well studied [131], including techniques such as Chain-of-Thought Prompting [120] and variants. However, there is work on unfaith-ful explanations in chain-of-thought prompting [113], with con-nections to language model alignment through externalized rea-soning (getting models to do as much processing/reasoning through natural language as possible). Mechanistic interpretability [93] is another active area of research, which has the potential to be fur-ther accelerated by the availability of small language models like phi-2. Causal tracing approaches have been proposed to locate and edit factual associations in LLMs. This involves first identifying neuron activations that are decisive in the model's factual predic-tions, and then modifying these neuron activations to update spe-cific factual associations [78].\nOpen challenges: Analogous to the use of simpler approximate models for explaining complex predictive ML models (e.g., LIME), can we employ simpler approximate models to explain LLMs and other generative Al models (e.g., using approaches such as model distillation) in a faithful manner? Additionally, can we develop more efficient and practical causal intervention approaches?"}, {"title": "3 Grounding for LLMs", "content": "Business problem: How do we ensure that responses generated by an LLM are grounded in a user-specified knowledge base? Here, \"grounding\" means that every claim in the response can be attrib-uted [92] to a document in the knowledge base. We distinguish between the terms \"grounding\" and \"factuality\". While \"ground-ing\" seeks attribution to a user-specific knowledge base, \"factual-ity\" seeks attribution to commonly agreed world knowledge.\nIn the context of \"grounding\", the knowledge base may be a set of public and/or private documents, one or more Web domains, or the entire Web. For instance, a healthcare company may want its chatbot to always produce responses that are grounded in a set of healthcare articles it consider authoritative. In addition to ground-ing to the knowledge base, one may also want responses to contain citations into the relevant documents in the knowledge base. This enables transparency and allows the end-user to corroborate all claims in the response."}, {"title": "3.1 Solution Approaches", "content": "In \u00a72.1, we laid out some key directions for detecting and prevent-ing hallucinations in LLM responses. As mentioned earlier, the re-quirement of grounding goes a step further from merely prevent-ing hallucinations. We seek responses that are fully aligned with a given knowledge base. For instance, there may be a well-supported, non-hallucinated claim that disagrees with the provided knowl-edge base. Such a claim would still be considered ungrounded. There is a vast and growing literature on grounding for LLMs. Below, we sketch out the key directions in this space.\nRetrieval Augmented Generation. Grounding failures often oc-cur because not all information in the knowledge base is stored in the LLM's parametric memory. One popular approach to circum-venting this challenge is Retrieval Augmented Generation (RAG) [52, 65], which leverages in-context learning to expose the model to rel-evant information from the knowledge base. Specifically, given a prompt (user question), we retrieve relevant snippets (called con-text) from the knowledge base, augment the prompt with this con-text, and then generate a response with the augmented prompt. The success of a RAG system relies on the success of the retrieval step and the generation step. Consequently, RAG systems are eval-uated based on dimensions such as context relevance (that is, whether the retrieved context is relevant to the given prompt), answer faith-fulness (that is, whether the response generated by the LLM is prop-erly grounded in the retrieved context), and answer relevance (that is, whether the response is relevant to the user question) [27, 100].\nThe retrieval step seeks to efficiently retrieval all relevant infor-mation for a given prompt. This typically involves chunking and indexing the knowledge base into a vector database, and query-ing it based on the prompt. The tremendous commercial interest in RAG systems has led to a proliferation of enterprise-grade vec-tor databases (e.g., Pinecone [88], FAISS [26]) that enable retrieval from arbitrary knowledge bases.\nTo sharpen the retrieval step, several recent works have been ex-ploring various aspects of it, including, the appropriate granularity of retrieval (such as a paragraph or a sentence) [21, 125], strate-gies for decomposing complex prompts into one or more retrieval queries [19, 53, 89], supervising retrieval based on quality of down-stream generation systems [118], and leveraging LLMs as retrieval indexes [108, 129]."}, {"title": "3.2 Open Challenges", "content": "Grounding for LLMs is a rapidly evolving area with several open challenges. A key practical challenge for RAG frameworks is grap-pling with imperfect retrieval. For instance, how should the model respond when the retrieval includes multiple opinions that contra-dict with each other, when the retrieval is missing crucial infor-mation sought by the prompt, or when the retrieval is completely irrelevant? In some cases, even when the retrieval is missing infor-mation, the model may still have the necessary information in its parametric memory. How should models balance amongst answer-ing from the context versus answering from parametric memory versus not answering at all (punting)?\nA key challenge in tuning LLMs towards generating grounded responses is that the models may optimize for grounding at the ex-pense of losing creativity and helpfulness. For instance, they may quote verbatim from the provided context, which was recently ob-served for a number of commercial generative AI search engines [67].\nFinally, a large open area is extending RAG frameworks to multi-modal settings \u2013 for instance, settings where the underlying knowl-edge base may consist of text, images, audio, and video, or the query may be a combination of text and audio. This is an emerging area, and we refer interested readers to a recent survey by Zhao et al. [133]."}, {"title": "4 LLM Operations and Observability", "content": "Business problems: What processes and mechanisms are impor-tant for addressing grounding and evaluation related challenges in real-world LLM application settings in a holistic manner? How can we monitor LLMs and other generative AI applications deployed in production for metrics related to quality, safety, and other responsi-ble Al dimensions? How can we anticipate and manage risks from frontier Al systems?"}, {"title": "4.1 Solution Approaches", "content": "The emerging area of \"LLM operations\" deals with processes and tools for designing, developing, and deploying LLMs, as well as monitoring LLM applications once they are deployed in produc-tion. Frameworks such as the following have been proposed to address potential harms and challenges pertaining to grounding, robustness, and evaluation in real-world LLM applications [10, 30, 64].\n\u2022 Identification [31, 49, 87]: Recognizing and prioritizing po-tential harms through iterative red-teaming, stress-testing, and thorough analysis of the Al system.\n\u2022 Measurement [42, 74, 100, 135]: Establishing clear metrics, creating measurement test sets, and conducting iterative, systematic testing-both manual and automated-to quan-tify the frequency and severity of identified harms.\n\u2022 Mitigation [50, 80, 95, 106]: Implementing tools and strate-gies, such as prompt engineering and content filters, to re-duce or eliminate potential harms. Repeated measurements need to be conducted to assess the effectiveness of the imple-mented mitigations. We could consider four layers of miti-gation at model, safety system, application, and positioning levels.\n\u2022 Operationalization [43, 111]: Defining and executing a de-ployment and operational readiness plan to ensure the re-sponsible and ethical use of Al systems.\nDepending on the domain requirements, an \"AI safety layer\" for detecting toxicity and other undesirable outputs in realtime can be included between the model and the application. Measuring shifts in the distribution of LLM prompts or responses could be helpful to identify potential degradation of the model quality over time, and further this information can be combined with any user feedback signals to determine regions where the model may be underper-forming [43].\nFurther, we need to differentiate undesirable outcomes or fail-ures in LLM applications caused by adversarial attacks from fail-ures due to the LLM's behavior in an unexpected manner in certain contexts. To address the latter class of \"unknown unknown\" fail-ures, we should not only perform extensive testing and red team-ing to preemptively identify and mitigate as many potential harms as possible but also incorporate processes and mechanisms to react quickly to any unanticipated harms during deployment. As an ex-ample, Microsoft introduced a new category of harms called \"Dis-paraging, Existential, and Argumentative\u201d harms as part of the re-sponsible Al evaluation for conversational AI applications in re-sponse to the unexpected behavior of the Bing AI chatbot as re-ported by a New York Times journalist [99].\nMore broadly, the risk profile associated with frontier AI sys-tems is expected to expand in light of extensions of existing LLMs, e.g., multimodality, tool use, deeper reasoning and planning, larger and more capable memory, and increased interaction between Al systems [102, 111]. Of these, tool use is considered to create several new risks and vulnerabilities."}, {"title": "4.2 Open Challenges", "content": "A key challenge is to classify potential risks associated with tool use, Al agents, interaction between Al systems, etc., in terms of the level of attention and action needed now and at different points in the future. This involves prioritizing investments to address such risks, especially in the following two areas: (1) Identifying failure modes and tendencies of LLM-based applications: We need to pin-point how these applications can be led astray, and (2) Develop-ing new safety and monitoring practices: This involves leveraging metrics like weight updates, activations, and robustness statistics, which are not currently available as part of LLM APIs."}, {"title": "5 Conclusion", "content": "Given the increasing prevalence of AI technologies in our daily lives, it is crucial to integrate responsible AI methodologies into the development and deployment of Large Language Models (LLMs) and other Generative AI applications. We must understand the potential harms these models may introduce, and leverage state-of-the-art techniques for enhancing overall quality, fairness, ro-bustness, and explainability. Addressing the responsible AI related harms and challenges not only reduces legal, regulatory, and rep-utational risks, but also safeguards individuals, businesses, and so-ciety as a whole. Moreover, there is a pressing need to establish ways to quantitatively assess the performance, quality, and safety of such models. Without comprehensive evaluations, establishing trust in LLM-based applications becomes exceedingly difficult. The goal of this tutorial is to establish a foundation for the development of safer and more reliable generative AI applications in the future."}]}