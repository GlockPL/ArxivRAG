{"title": "Varco Arena: A Tournament Approach to Reference-Free Benchmarking Large Language Models", "authors": ["Seonil Son", "Ju-Min Oh", "Heegon Jin", "Cheolhun Jang", "Jeongbeom Jeong", "Kuntae Kim"], "abstract": "The rapid advancement of Large Language Models (LLMs) necessitates robust evaluation methodologies. Current benchmarking approaches often rely on comparing model outputs against predefined prompts and reference outputs. Relying on predefined reference outputs hinders flexible adaptation of benchmarks to the rapidly evolving capabilities of LLMs. This limitation necessitates periodic efforts to prepare new benchmarks. To keep pace with rapidly evolving LLM capabilities, we propose a more flexible benchmarking approach. Our method, Varco Arena, provides reference-free benchmarking of LLMs in tournament style. Varco Arena directly compares LLM outputs across a diverse set of prompts, determining model rankings through a single-elimination tournament structure. This direct pairwise comparison offers two key advantages: (1) Direct comparison, unmediated by reference text, more effectively orders competing LLMs, resulting in more reliable rankings, and (2) reference-free approach to benchmarking adds flexibility in updating benchmark prompts by eliminating the need for quality references. Our empirical results, supported by simulation experiments, demonstrate that the Varco Arena tournament approach aligns better with the current Elo model for benchmarking LLMs. The alignment is measured in terms of Spearman correlation, showing improvement over current practice of benchmarking that use reference outputs as comparison anchors.", "sections": [{"title": "1 Introduction", "content": "The versatility of Large Language Models (LLMs) stems from their generative capacity to address a wide array of tasks. The multi-faceted capability of LLMs enables flexible applications across numerous user scenarios, exemplified by generalist assistants (Ouyang et al. 2022; K\u00f6pf et al. 2024; Roziere et al. 2023). As versatility emerges as a core attribute of LLMs, the challenge of accurately gauging their skill becomes increasingly significant. In response to this challenge, numerous benchmarks evaluating LLM capability have emerged (Hendrycks et al. 2020; Srivastava et al. 2023; Zhong et al. 2024). Many LLM benchmarks employ formats amenable to automated scoring. Examples include benchmarks for arithmetic problems (Gao et al. 2022; Patel, Bhattamishra, and Goyal 2021), multiple-choice questions (Lin, Hilton, and Evans 2022), and code execution (Austin et al. 2021; Chen et al. 2021). While these benchmarks are valuable, they primarily assess problem-solving abilities. The need for LLM benchmark extends beyond this, as the primary value of LLMs lies in their generative capabilities. Researchers have primarily relied on pariwise comparison to evaluate and rank the generative capabilities of LLMs. A representative example of this approach is Chatbot Arena (Chiang et al. 2024), which computes Elo ratings based on a massive number of human-annotated pairwise comparisons. While Chatbot Arena benefited from the reliability of open-ended prompts from users, acquiring such a large number of annotations for benchmarking LLMs is time-consuming and resource-intensive. To address these challenges, several benchmark datasets for generation tasks have been developed. These leverage reference responses for ranking LLMs using automated LLM judges (Zheng et al. 2024) includes AlpacaEval (Li et al. 2023), Arena-Hard-Auto (Li et al. 2024), and MTBench (Zheng et al. 2023). Relying on reference outputs offers two key advantages: (1) The number of comparisons required for ranking multiple LLMs reduces linear to the number of LLMs. (2) Reference texts establish a quality bar for evaluating responses.\nHowever, we propose that directly comparing the responses of LLMs, rather than relying on reference outputs, is a more effective way to achieve accurate LLM rankings. We suggest that combined with tournament-styled match making, direct comparison of outputs can achieve a better correlation to human preference within the same number of comparisons. Additionally, reference-free benchmarking mitigates potential biases in open-ended generation tasks and provides more flexibility compared to static benchmark datasets.\nWe devised Varco Arena, which directly compares LLM outputs without the need for reference text as a comparison baseline. We leveraged the efficiency of single-elimination tournaments, which always require a linear number of matches relative to the number of participants. For brevity, we will refer to single-elimination tournaments simply as tournaments throughout the paper. Varco Arena conducts a tournament for each prompt across all participating LLMs and then computes Elo ratings (Elo and Sloan 1978) based on the results of all matches occurred. By performing tournament matches for multiple LLMs on each prompt, we"}, {"title": "2 Preliminaries: Quantifying Generation Ability", "content": "Quantifying an LLM's generation ability is crucial for ranking models, but it presents several challenges. The outcomes of comparisons between two LLMs are often probabilistic, influenced by various factors such as test prompts and the subjective nature of human preferences. A straightforward approach is to evaluate an LLM's performance across diverse test prompts, which approximates its capability in real-world scenarios. Two popular measures for LLM generation ability are: win rate against reference text and elo rating."}, {"title": "2.1 Win rate Against the Reference Text", "content": "Several benchmarks quantify response generation quality, with AlpacaEval (Li et al. 2023) and Arena-Hard (Li et al. 2024) being representative examples. These benchmarks employ LLMs as judges for automated evaluation of generation capability. Given a prompt and two responses, the judge LLM is tasked with determining the superior response. These benchmarks include predefined sets of test prompts and reference outputs that serve as quality standards. The evaluation process involves comparing the LLM's output against the reference to assess response quality. The LLM's win rate across test prompts is used as a measure of its generation proficiency."}, {"title": "2.2 Elo Rating", "content": "The Elo rating system, introduced by Elo and Sloan, has since become a popular method for quantifying performance levels in competitive sports and, more recently, for evaluating the generative abilities of LLMs. The primary purpose of the Elo rating system is to represent a participant's skill level as a single scalar value, enabling the prediction of relative win rates between participants who have never directly competed. Assessing the superiority of one LLM over another in terms of generative capability shares several similarities with determining winners in competitive sports. Varying test prompts may yield different results, similar to how weather or other factors can influence the outcome of a sports match. While a superior team or LLM is not guaranteed to outperform its inferior counterpart in every instance, it tends to succeed more frequently. With Elo ratings, one can expect the relative chance of winning between al pair of players. The expected win rate between a pair of players i and j is computed as follows:\n$P(i > j) = \\frac{1}{1+10^{(R_j-R_i)/400}} = \\frac{1}{1 + 10^{A_{ij}/400}}$ (1)\nComputing accurate Elo ratings requires a sufficient number of matches to estimate relative win rates between pairs of participants. The resulting matrix of relative win rates is then used to estimate Elo ratings through logistic regression, similar to multivariate logistic regression with a sigmoid function. In Equation 1, $P(i > j)$ represents the expected win rate of participant i over j, $R_i$ is the Elo rating of participant i, and $A_{ij}$ is the Elo rating difference between participants j and i ($R_j - R_i$)."}, {"title": "3 Varco Arena", "content": "Varco Arena represents our novel approach to reference-free benchmarking of LLMs. Rather than comparing LLM responses to references, we propose iterated tournaments over benchmark dataset to build leaderboards. This addresses concerns about effectiveness and scalability of using direct comparison over traditional benchmarking practice of relying on references. Section 3.1 and Algorithm 1 detail how Varco Arena runs iterated tournaments as we suggest. We then explain our rationale for using direct comparison of LLM responses over bridged comparing to references (Section 3.2), and argue why iterated tournaments can produce effective LLM leaderboards (Section 3.3)."}, {"title": "3.1 Iterating Tournaments over Benchmark Dataset to Elo ratings", "content": "Figure 1 and Algorithm 1 illustrate the methodology of Varco Arena for benchmarking Large Language Models (LLMs) using a tournament approach. In this context, |X|\ndenotes the cardinality of the prompt set in the benchmark dataset (i.e. number of prompts in the dataset). As described running Varco Arena invokes tournament of participant LLMs for each prompt in benchmark dataset, which we will refer to as iterated tournaments.\nThe application of tournament structures to LLM benchmarking presents both opportunities and challenges. While a single tournament effectively identifies a champion, it does not provide the granularity required for a comprehensive ranking system; it only declares a champion, leaving the precise ordering of other participants ambiguous. To retain the efficient nature of tournaments but to accomplish a fine-grained ranking of LLMs, what modifications should be applied to? To mitigate the aforementioned ranking resolution issue, we propose conducting independent tournaments for each benchmark prompt, with randomized initial match-ups. This approach, which we term iterated tournaments, offers several advantages:\n1. It facilitates the resolution of ties among non-champion participants from previous tournaments.\n2. It reduces the impact of unfavorable match-ups in any single tournament.\n3. While not providing exhaustive pairwise comparisons, the accumulated matches between model pairs enable more precise estimation of relative win rates and Elo ratings, culminating in a well-aligned overall ranking.\nIn terms of total number of matches, our iterated tournaments approach requires |X| * (nmodels \u2013 1) which is always numbers of comparisons saved compared to anchored comparison to references. A favorable trait of conducting tournaments for each prompt is that every participating LLM engages in at least one competition per prompt. The number of matches an LLM faces per test prompt varies from 1 to log2(nmodels)], based on its performance. Allocating more matches for better LLMs will help precise separation between higher rated models.\nWe further elaborate and argue why aggregating multiple tournaments should reach a good sorting of participant LLMs in Section 3.3. Additionally, we provide prospect match numbers LLMs would confront within, offering a comprehensive view of the method's efficiency and effectiveness."}, {"title": "3.2 Comparing to Reference is not Always Helpful", "content": "Here, we inspect into failure modes of relying on references for ranking LLMs, which further elaborates the previously mentioned concerns on current practices for LLM benchmarking. Consider a highly accurate judge capable of definitively comparing two outputs of different quality without errors or ties. In this scenario, all matches involving participant LLMs contribute to their relative win rates and, ultimately, to their final Elo ratings (Section 3.3 and Equation 2). Consequently, all matches involving the LLM of interest contribute to Elo rating and ranking of the LLM.\nWhen evaluating LLM performance relative to reference texts, even with the same precise judge, the comparison doesn't always provide a reliable measure of relative win rates. Suppose we have only two participants of interest M1"}, {"title": "3.3 Iterated Tournaments as a Rough Approximation of Merge Sort", "content": "In this section, we argue that iterated tournaments will reach to a good sorting of participants LLMs comparing the similarities in merge sort algorithm and iterated tournament regarding the presumptions of Elo model for LLM capability. Our approach is predicated on a key assumption: although the prompts in the benchmark dataset are diverse, they can be considered approximately equivalent in their capacity to differentiate LLM performance. This assumption of 'indistinguishable prompts' aligns with the Elo model of LLM capability, which posits that prompt-specific variations should not significantly affect the expected outcome likelihood of a single match.\nBuilding on this assumption, we propose that conducting tournaments across the benchmark prompts can lead to an accurate ranking of LLMs. We suggest that single-elimination tournaments function analogously to merge sort, with a crucial distinction: they perform only the initial comparison at each step of a merge operation and discard the losing side (i.e. only the winner proceeds to the next rounds). This makes a single tournament perform n 1 comparisons in total. This number of comparisons is less than\n\u00b9Given the simplicity of the Elo model's assumptions regarding LLM capability, we posit that ranking LLMs with Elo ratings for a mixture of disparate capabilities may be inappropriate, as a higher Elo rate does not guarantee uniform mastery across multiple capabilities.\nO(nlog2n) which most of the efficient sorting algorithms requires. However, regarding the assumption of 'indistinguishable prompt' we mentioned above, number of tournament matches from the other dataset prompt would help filling the lack of comparion matches. By iterating over the benchmark dataset with randomized initial match-ups, these tournament matches can eventually approximate the complete set of comparisons required for merge sort, given a sufficient number of benchmark prompts (|X|).\nOur approach provides O(|X|/log2nmodel) times more comparisons than the minimum required for merge sorting nmodel LLMS. In practice, number of matches to figure which model is better from a pair is much larger since the judge model will not be ideal and also performance of LLMs will be stochastic regarding the prompt it was asked to answer. Nevertheless, the large number of prompts (|X|), typically ranging from hundreds to thousands, lead us to the worth of empirical validation of the idea, that iterated tournaments will accurately sort LLMs with confidence. This foundational argument underpins our unbiased judge simulation experiment to test the potential of tournament approach (Varco Arena)."}, {"title": "4 Experiments", "content": "We run two experiments with different settings for comparing tournament and anchored comparison approaches. The first experiment (Section 4.2) simulates an unbiased judge to control various factors that affect LLM benchmarking. This simulation tests our foundational propositions for Varco Arena design (mentioned in Section 3.2 and 3.3) under a more controlled, simplified environment. It eliminates noisy factors such as potential biases of LLM judges (Park et al. 2024).\nIn subsequent empirical experiments (Section 4.3), we use gpt-40[-mini] as a judge, to validate the effectiveness of our tournament approach against current LLM benchmarking practices. By presenting both simulation and empirical results, we aim to demonstrate the effectiveness of our tournament approach. We first describe the common experimental settings before getting into specific details in the following subsections."}, {"title": "4.1 Chatbot Arena Leaderboard Ratings as Ground-Truth LLM Rankings", "content": "We compare the results of each benchmarking approach against the rankings from the Chatbot Arena leaderboard. Chatbot Arena is currently considered one of the most reliable leaderboards due to its large number of human preference annotations. In this platform, users are presented with responses from a pair of LLMs to the same query and asked to choose which response they prefer. Given the substantial number of votes and the wide range of submitted prompts used for model comparison, the resulting ranks from Chatbot Arena are likely accurate enough to serve as ground-truth rankings.\nWhere O(|X|/log2nmodel) = X|(model-1)\nO(nmodel log2 nmodel) representing the ratio between matches in iterated tournaments and matches needed for sorting LLMs with an ideal judge."}, {"title": "4.2 Experiment 1: Simulating an Unbiased Judge", "content": "We designed a simple stochastic modeling experiment to simulate LLM matches following the Elo preference model. Our model assumes that the primary factor governing the outcome of each match is the Elo rating gap (\u2206ij), as shown in Equation 3. We also introduced a secondary factor, judge precision (Pjudge), which represents the likelihood that a judge correctly declares the higher-rated LLM as the winner. The observed outcome of a single match is sampled according to the likelihood of model i winning against model j (Ppredict(i > j)). This probability is the product of the judge precision and the estimated relative win rate expected by ground-truth Elo ratings (Pgt(i > j)). This simplified simulation model aligns with the fundamental principle of the Elo system: the outcome of a match between a pair of models is primarily determined by their skill gap (\u2206ij). This gap is derived from past wins and losses, effectively condensing all sophisticated factors into a single mean value.\n$P_{predict}(i > j) = P_{judge} * P_{gt}(i > j) = P_{judge} * \\frac{1}{1 + 10^{A_{ij}/400}}$ (3)\nThe details of our simulation settings are as follows:\nGround-truth Elo ratings: We extracted Elo ratings from the English category of Chatbot Arena as of June 23, which includes one million user-submitted prompts and judgments (approximately 60% of the total submissions to the platform). While any set of Elo ratings could be valid for this simulation, we chose to use real values computed from human preferences.\nJudge Precision: In real-world, judge precision would be an adaptive value that varies based on the prompt-response pair being judged or the method of prompting the LLM-as-a-judge. In our experiments, we controlled judge precision, ranging from 0.6 to 0.9.\nNumber of participant LLMS (nmodel) and benchmark dataset size (|X|): We varied these factors to assess the robustness of tournament and anchored comparison approaches under diverse conditions. A larger number of participants may require more matches to achieve precise results. Similarly, the benchmarking environment may be data-poor or data-rich. We emulated the varying dataset size by sampling different numbers of matches from Ppredict. The simulation process follows these steps:\n1. Select participants from the top of the Chatbot Arena leaderboard, avoiding ties (e.g., selecting only one model from a tied rank).\n2. Compute expected relative win rates (Pgt, Equation 3) from the participants' Elo ratings.\n3. Sample match outcomes across LLM pairs for tournaments or anchored comparisons. An unbiased judge with precision Pjudge (Equation 3) determines the winner according to Equation 3, considering only the Elo gap \u2206ij.\n4. Repeat step 3 for the designated benchmark dataset size |X|.\n5. Calculate scores for ranking:\n(a) For anchored comparison, use the win rate against the reference LLM (we choose"}, {"title": "4.3 Experiment 2: Running Varco Arena using gpt-40[-mini] as Judge", "content": "We conducted an empricial experiemnt of Varco Arena to validate our proposed method. We conducted tests for both tournament approach and anchored comparison approach using 20 top-ranked models from the Chatbot Arena leaderboard. This experiment differs from our modeling simulation in several key aspects. We elaborate on the details of the experiments in order of how we prompted the LLM judges and our practice to managing API costs while conducting a large number of experiments.\nDataset for Testing Benchmarking Approaches In order to test the benchmarking approaches, tournament and anchored comparison, we need two sorts of data. One is a prompt to be asked for LLMs and the other is a reponse by each LLM given the prompt. For the benchmark dataset, we choose Arena-Hard-Auto (Li et al. 2024). Arena-Hard-Auto benchmark prompts are carefully assorted from Chatbot Arena which are user-typed queries to LLMs. The total number of prompts in Arena-Hard-Auto is 500, each 2 instances for 250 subtopics. Choosing Arena-Hard-Auto as our benchmark dataset is natural since it minimizes the disparity between our benchmarking environment and the ground truth environment where human annotators actually judged LLMs. As suggested in Arena-Hard-Auto, we use the responses of gpt-4-0314 as reference outputs. For responses of the LLMs to be ranked, we use the reserved outputs of the top 21 models from Huggingface space of the Arena-Hard-Auto Browser.\nParticipant LLMS As a list of models to be ranked, we choose 20 LLMs from the top of ChatBot Arena leaderboard in Hard prompts category which is considered to synchronize the most to Arena-Hard-Auto. To avoid egocentric bias, we use gpt-40-2024-05-13 and 40-mini as our judge, gpt-4-0314 as a reference (anchor) model to perform anchored comparisons on. You can see the evaluation prompt on Listing 1 at Appendix.\nLLM Judges We deploy gpt-40 and gpt-40-mini as judges. To prompt the pairwise comparison of the responses from the participant LLMs, we refer to (Zeng et al. 2024), equally for both tournament and anchored comparison approaches. To avoid position bias (Wu and Aji 2023), we alternate the position of the model responses"}, {"title": "5 Results and Discussion", "content": "We report on the reliability of Varco Arena for simultaneously ranking multiple LLMs. Both simulated and empirical results demonstrate that the tournament approach of Varco Arena produces ranks more closely aligned with ground-truth Elo ranks."}, {"title": "5.1 Experiment 1: Unbiased Judge Simulation", "content": "Figure 2 reveals distinct gaps in Spearman correlation, indicating that the tournament approach demonstrates greater reliability compared to the anchored comparison approach. These significant gaps persist across various controlled parameters, including the number of participating LLMs (nmodels), prompts in the benchmark set (|X|), and judge precision (Pjudge). While the simulation necessarily abstracts real-world complexity, the consistent performance patterns observed under systematically varied conditions align with the empirical findings from Experiment 2, suggesting potential robustness of the tournament approach beyond the specific conditions tested empirically."}, {"title": "5.2 Experiment 2: Running Varco Arena with gpt-40[-mini] Judge", "content": "Our empirical experiments corroborate the trends observed in the simulation. As illustrated in Figure 3 and Table 1, while the gaps are less pronounced compared to the simulation results (Figure 2), the Varco Arena results obtained through the tournament approach consistently demonstrate higher correlation across varying benchmark sizes (|X|, denoted as nprompts in Figure 3). This empirical evidence further strengthens our argument for the effectiveness of the tournament approach."}, {"title": "5.3 Incorporating a New LLM into an Existing Leaderboard", "content": "While our main focus has been on ranking multiple LLMs simultaneously, it's worth considering how our approach might apply to the common scenario of adding a single new model to an existing leaderboard. We explored a binary search-like placement method for this purpose (detailed in Appendix 2), but found it yielded less stable results than the reference output approach (Table 2). We attribute this instability to two factors: (1) The reduced number of comparisons (only |X| comparisons per model, the minimum for a tournament run). (2) Incomplete coverage of the benchmark dataset's prompts.\nHowever, the concept of a reference model as an anchor could still be valuable in this context. One could estimate the win rate against a anchor of one's own choice for all models in the leaderboard, using this as a ranking score. For leaderboards built with Varco Arena, any missing matches could be imputed using expected relative win rates derived from Elo ratings or directly from existing match records. This approach offers a potential compromise between the stability of reference-based methods and the comparative advantages of our tournament approach."}, {"title": "6 Related Works", "content": null}, {"title": "6.1 Elo Modeling for LLM Benchmarking", "content": "Utilizing Elo modeling of LLM capability based on human preference is a recently recognized means for benchmarking (Boubdir et al. 2023). Chatbot Arena (Chiang et al. 2024) is introduced as a benchmark platform that targets live benchmarking of LLMs toward open-ended generation tasks with the proper use of Elo ratings. Except that the platform requires massive human judge annotation, Chatbot Arena is considered a successful example of dynamic benchmarking for open-ended tasks as suggested in (Kiela et al. 2021). RAGElo (Rackauckas, C\u00e2mara, and Zavrel 2024) deploy Elo model with LLM as a judge (Zheng et al. 2023) to improve retrieval augmented generation (RAG) pipeline which indicates that modeling LLM capability with Elo rating is practically effective."}, {"title": "6.2 Reference-free Evaluation", "content": "Alleviating the need for reference text has long been considered an attractive goal in Natural Language Generation (NLG) evaluation. Recent advances in LLMs have shown promising results in their ability to assess open-ended responses (Jauhiainen and Guerra 2024). Also, findings from the WMT23 metrics task in neural machine translation (Freitag et al. 2023) have demonstrated that reference-free quality estimation metrics outperform reference-based metrics in terms of correlation to human evaluation. Specifically, metrics such as XComet (Guerreiro et al. 2023) and MetricX (Juraska et al. 2023) have shown superior performance, particularly when dealing with scenarios involving poor-quality references."}, {"title": "7 Conclusion", "content": "We proposed Varco Arena, a reference-free LLM benchmarking approach for measuring generative capabilities of LLMs. Varco Arena deploys a tournament-style approach with direct pairwise comparisons, eliminating the need for reference outputs. Our simulation experiments demonstrate that this tournament approach achieves aligned LLM rankings better than current practice of reference use, under unbiased judging conditions. We also report the empirical ef-"}, {"title": "A Appendix", "content": null}, {"title": "A.1 Machine Requirements for Experiments", "content": "All the experiments we performed are conducted by querying API and post-processing those, which does not require GPU machine locally. Experiments could be run on personal desktops. The lowest specification of the machine we deployed had i5-8400 CPU, 16 GiB RAM."}, {"title": "A.2 Elo ratings from Varco Arena compared to Human Annotations", "content": "Figure 4 shows the Elo ratings computed out of Varco Arena. For judge, we used gpt-40. As mentioned in the caption, the Elo ratings are bootstrapped median value from 500 trials. 95% confidence intervals also plotted as an error bar, which look negligible in scale compared to observed values. Matches are performed over Arena-Hard-Auto benchmark dataset (500 prompts)."}, {"title": "A.3 Binary search vs. Win rate over reference", "content": "Binary Search We tried binary search placement of a newly added LLM to the leaderboard without reference text in Table 3. Details of how we implemented binary search are attached in Appendix 2. It turns out that binary search based on leaderboard ranks is not as reliable as the current approach of scoring the newcomer to the reference outputs. The number of judge operations performed is equivalent to the matches allocated to the least-performant model in a tournament, which is |X| (i.e. maximum possible matches"}, {"title": "Decoding Parameters", "content": "We did not configure decoding parameters of judge LLMs (gpt-40[-mini]), which its temperature defaults to 1. The only parameter we have adjusted is maximum number of tokens to be generated, which for our prompt is less than 6 (i.e. The output of our prompt is (a) or (b)). To avoid position bias, we alternated the position of the responses from a certain model across the benchmark prompt."}, {"title": "B Reproducibility Checklist", "content": "Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes)\nClearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes)\nProvides well marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper (yes)\nDoes this paper make theoretical contributions? (yes)\nAll assumptions and restrictions are stated clearly and formally. (yes)\nAll novel claims are stated formally (e.g., in theorem statements). (partial)\nProofs of all novel claims are included. (partial)\nProof sketches or intuitions are given for complex and/or novel results. (yes)\nAppropriate citations to theoretical tools used are given. (yes)\nAll theoretical claims are demonstrated empirically to hold. (yes)\nAll experimental code used to eliminate or disprove claims is included. (no)\nDoes this paper rely on one or more datasets? (yes)\nA motivation is given for why the experiments are conducted on the selected datasets (yes)\nAll novel datasets introduced in this paper are included in a data appendix. (NA)\nAll novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes)\nAll datasets drawn from the existing literature (potentially including authors' own previously published work) are accompanied by appropriate citations. (yes)\nAll datasets drawn from the existing literature (potentially including authors' own previously published work) are publicly available. (yes)\nAll datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. (NA)\nDoes this paper include computational experiments? (yes)\nAny code required for pre-processing data is included in the appendix. (no)\nAll source code required for conducting and analyzing the experiments is included in a code appendix. (no)\nAll source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes)\nAll source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (partial)\nIf an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (yes)\nThis paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. (partial)\nThis paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. (yes)\nThis paper states the number of algorithm runs used to compute each reported result. (yes)\nAnalysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. (yes)\nThe significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (yes)\nThis paper lists all final (hyper-)parameters used for each model/algorithm in the paper's experiments. (yes)\nThis paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting. (yes)"}]}