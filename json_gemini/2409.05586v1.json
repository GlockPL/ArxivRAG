{"title": "Interpretable Responsibility Sharing as a Heuristic for Task and Motion Planning", "authors": ["Arda Sarp Yenicesu", "Sepehr Nourmohammadi", "Berk Cicek", "Ozgur S. Oguz"], "abstract": "This article introduces a novel heuristic for Task and Motion Planning (TAMP) named Interpretable Responsibility Sharing (IRS), which enhances planning efficiency in domestic robots by leveraging human-constructed environments and inherent biases. Utilizing auxiliary objects (e.g., trays and pitchers), which are commonly found in household settings, IRS systematically incorporates these elements to simplify and optimize task execution. The heuristic is rooted in the novel concept of Responsibility Sharing (RS), where auxiliary objects share the task's responsibility with the embodied agent, dividing complex tasks into manageable sub-problems. This division not only reflects human usage patterns but also aids robots in navigating and manipulating within human spaces more effectively. By integrating Optimized Rule Synthesis (ORS) for decision-making, IRS ensures that the use of auxiliary objects is both strategic and context-aware, thereby improving the interpretability and effectiveness of robotic planning. Experiments conducted across various household tasks demonstrate that IRS significantly outperforms traditional methods by reducing the effort required in task execution and enhancing the overall decision-making process. This approach not only aligns with human intuitive methods but also offers a scalable solution adaptable to diverse domestic environments. Code is available at https://github.com/asyncs/IRS.", "sections": [{"title": "1. Introduction", "content": "Domestic robots are expected to handle household tasks defined by humans to improve their standards of living. These tasks include serving, cleaning, and caring in environments also created by humans. Due to this common human factor in sequential decision-making and manipulation problems, incorporating human bias into robots' decision-making processes can enhance their overall performance [1, 2] (Figure 1). Task and Motion Planning (TAMP) can effectively address these household tasks. Current TAMP research [3, 4, 5] often focuses on formulating these high-level tasks at a logical level, generating task plans using either uninformed or informed search, and finding feasible motions to execute these plans in a sequential or interleaved fashion. However, these studies often overlook human bias.\nOur perspective on human bias in this study is that when humans construct their environment, they also create auxiliary objects that might be beneficial for certain human-defined tasks. For example, a tray often exists in a kitchen environment and is frequently used as the initial object for a household serving task. Therefore, we propose utilizing these objects when necessary, as they reflect human bias in these situations. We are not concerned with any other potential influences or interventions possibly made by humans. Our goal is to develop a systematic method to leverage this bias, achieving a more efficient and effective TAMP formulation.\nSeveral methods have been developed to tackle sequential decision-making and manipulation problems using TAMP [6, 7, 8, 9, 10, 11, 12]. While recent approaches using heuristics perform well [13, 14, 15, 16], the heuristic functions are often constrained to specific task domains, limiting their generalizability to other robotic tasks and their application in real-world household scenarios. Uninformed task planners [17, 18], which generate plans by performing a tree search through a knowledge base, are more generalizable due to their domain-agnostic nature. However, they struggle to find plans in a reasonable time frame because of the combinatorial nature of the search problem, where the search space increases exponentially, reducing their applicability [16]. Additionally, they aim to find feasible plans with the minimum number of high-level actions, which may lead to sub-optimal plans.\nMoreover, both approaches fail to leverage the human factor. While this is not a concern in uninformed search, heuristic-defined, informed search-based TAMP formulations often adopt an oracle approach. This oracle informs the planner about the cost of an action, typically based on displacement, which may not accurately reflect human preferences. Recognizing the importance of the human factor, researchers have recently explored using annotated data or structured reward mechanisms to train deep policies with sequential planning objectives [19, 20, 21, 22]. These methods show promising results in both performance and adaptability to unseen environments, making them a viable alternative for real-world robotic applications. Furthermore, they implicitly incorporate human bias through curated datasets or tailored reward mechanisms. However, the application of deep policy methods in task and motion planning faces interpretability issues, making embodied agents without a transparent decision-making process potentially unsafe for use in households [23, 24].\nTherefore, effective and interpretable task and motion planning require meeting two main criteria: leveraging the human bias present in sequential decision-making and manipulation problems and enhancing interpretability in the decision-making process.\nTo meet these criteria, we propose a novel planning heuristic, Interpretable Responsibility Sharing (IRS), which leverages human bias while providing an interpretable perspective on how an embodied agent uses this heuristic in its decision-making process (Figure 2). To address this inductive bias\u2014referring to the assumptions or patterns derived from human-designed environments and tasks that influence the agent's behavior or decision-making process\u2014we introduce a new concept called Responsibility Sharing (RS).\nIn household tasks, auxiliary objects such as trays often exist in the environment for serving items. The robot can manipulate the target objects either directly with its end-effector or indirectly using these auxiliary objects. We refer to this indirect relationship as Responsibility Sharing. By delegating some responsibilities to auxiliary objects (e.g., placing the objects on a tray, which the agent then carries), the original problem is divided into sub-problems (move the objects to the tray, then move the tray to the final destination). This approach simplifies and enhances the effectiveness of the combined planning problem by leveraging inductive bias.\nIt should be noted that this concept differs from tool usage, as using these objects is not mandatory to complete the task. For example, one may choose to use a tray or not while serving. This non-mandatory nature of these objects presents challenges for existing TAMP formulations. In uninformed approaches, using these objects increases the depth of the search, while in existing informed approaches, the task-dependent nature limits the range of objects that can be effectively utilized.\nTo address this non-mandatory nature, we need an adaptive yet interpretable decision-making mechanism to determine when to use these auxiliary objects across multiple tasks. We propose a novel approach called Optimized Rule Synthesis (ORS), which generates rules for agents to decide whether to utilize non-mandatory objects in the environment, based on state information defined under first-order logic.\nORS operates through two fundamental components: Representative Rule-Based Learner (RRL) [25] and Correlation and Order-Aware Rule Learning (CARL) [26]. These components are vital for the functionality of ORS: \u2460 RRL is crucial for creating and learning rules based on system data, identifying patterns and relationships between actions and object states, and establishing the foundational rules that guide initial decision-making processes. \u2461 Enhancing ORS, CARL introduces sensitivity to the sequence of actions and the relationships between different object states and actions, ensuring that the rules maintain logical coherence while adapting effectively to dynamic environments.\nBy integrating these critical components, ORS constructs a robust system capable of making informed predictions and justifying the use of auxiliary objects in various scenarios, thereby enhancing the adaptive capabilities of interpretable domestic robots.\nORS is trained on a counterfactual dataset created through Counterfactual Plan Generation (CPG), ensuring that auxiliary objects are responsible for performance changes. This training enables ORS to identify when and how to use these objects in an interpretable manner, enhancing transparency and decision-making in real-world applications. By combining ORS and RS, IRS guides the TAMP formulation, improving effectiveness and efficiency in an interpretable manner, particularly in domains where data is scarce (e.g., robotics). Overall, this work introduces the following contributions:\n\u2022 Interpretable Responsibility Sharing (IRS), a heuristic for task and motion planning, aiming to improve the effectiveness and interpretability"}, {"title": "2. Related Work", "content": "Combined TAMP involves jointly solving high-level symbolic action planning and low-level motion planning [3, 9, 27]. TAMP settings are defined by a combination of symbolic actions, states, and physical constraints, with tasks represented by either symbolic or physical goals. To identify feasible high-level action sequences for these goals, various TAMP planners have been developed. These planners have evolved from linear, flow-like approaches [28, 29] to more complex, interleaved planners [4, 5, 30, 31] that integrate motion planning to varying extents, ensuring the feasibility of symbolic plans. However, TAMP inherently faces scalability challenges due to large action and state spaces, limiting the horizon of proposed plans and their adaptability to complex scenarios [32, 33]. Common strategies to address this include heuristic-guided search [16, 34, 35] and breaking down the original problem into sub-problems [36, 37, 38]. Our approach aims to divide the original problem into sub-problems using the proposed concept of responsibility sharing, if it is inferred to improve the agent's performance."}, {"title": "Rule-Based and Advanced Knowledge Graph Reasoning Methods", "content": "Traditional rule-based models such as decision trees, rule lists, and rule sets, while being transparent, often face limitations in terms of scalability and generalization [39, 40, 41]. These models rely heavily on heuristic methods or prolonged itemset mining, which may not always yield the most effective solutions and are time-intensive on larger datasets. To enhance these, ensemble models like Random Forests and Gradient Boosted Decision Trees have been employed, which typically offer better performance by integrating multiple sub-models [42]. However, the complexity of these ensemble methods often detracts from their interpretability, which is crucial for certain applications such as robotics [43].\nAddressing the challenges of training rule-based models, gradient-based methods for discrete model training like the Straight-Through Estimator (STE) and Gradient Grafting have emerged [44, 45]. These methods attempt to optimize binary or quantized neural networks, enhancing both network compression and acceleration. The integration of gradient data from both discrete and continuous models aims to improve the efficacy of training processes without sacrificing the benefits of rule-based logic [46].\nInductive Logic Programming (ILP) extends rule-based modeling by utilizing induction to derive logical rules from observed instances and background knowledge, aiming to generate understandable and verifiable rules [47]. However, similar to traditional rule-based approaches, ILP struggles with scalability and data noise. Path-based reasoning, another facet of symbolic logic-based methods, deduces general laws from specific observations but often lacks generalization capabilities, despite producing precise and interpretable results [48, 49].\nThe embedding of knowledge graphs represents a significant shift towards computational efficiency in reasoning methodologies. Techniques like TransE, DistMult, and RotateE encode entities and relations into vectors within a defined space, facilitating fast and effective reasoning processes [50, 51, 52]. However, these embedding methods generally suffer from a lack of interpretability, presenting challenges for applications that require transparent decision-making processes. In contrast, neural-symbolic methods blend the computational prowess of neural networks with the logical rigor of symbolic reasoning [53]. This hybrid approach not only enhances model performance and efficiency but also retains a degree of interpretability through the use of symbolic constraints on embeddings.\nThe evolution from traditional rule-based models to advanced neural-symbolic and embedding-based techniques illustrates a broader trend in knowledge graph reasoning towards integrating computational efficiency with logical robustness. Each methodology compensates for the limitations of its predecessors, striving to balance performance with interpretability. For instance, while ILP and traditional rule-based models provide a solid foundation in logical reasoning, their scalability issues are addressed by ensemble methods. Further, the computational limitations observed in ensemble and traditional methods are mitigated by embedding and neural-symbolic techniques, which also attempt to reintroduce interpretability lost in purely computational approaches. Our approach with IRS aims to leverage the strengths of both symbolic reasoning and machine learning methods, focusing on creating models that are not only effective and efficient but also transparent and adaptable to a wide range of tasks."}, {"title": "Interpretability in Robotics", "content": "Interpretability in robotics is crucial as it ensures transparent and understandable decision-making processes, particularly in domestic and collaborative environments. Interpretable robotic systems are pivotal for enhancing trust, safety, and user acceptance, as they provide clear explanations for their actions and decisions [54, 55, 56]. Various methods have been explored to foster interpretability in robotics, including rule-based approaches.\nAmong these, fuzzy rule-based methods utilize fuzzy logic to manage uncertainty and vagueness in robotic decision-making [57, 58, 59]. These methods generate fuzzy rules either from expert knowledge or through data-driven learning. However, the interpretability of fuzzy rules may diminish as the number of rules and the complexity of their membership functions increase [60]. Furthermore, although fuzzy logic adeptly handles partial truths, its nuanced nature can obscure the decision-making process, especially when compared to more structured, binary approaches [61].\nOur method, termed Interpretable Responsibility Sharing (IRS), overcomes these challenges by focusing on a human-centric design that capitalizes on inherent biases and common practices in human environments. IRS employs a rule-based framework that is computationally more efficient than the graph-based calculations required by GNNs, facilitating its real-time application in robotic systems. By decomposing complex tasks into simpler sub-tasks and employing Optimized Rule Synthesis (ORS) for rule generation, IRS ensures that each decision is both transparent and effective. This structured simplicity and focus on human-centric design not only enhance interpretability but also make IRS particularly well-suited for task and motion planning in domestic settings."}, {"title": "3. Background", "content": "Interpretable Responsibility Sharing (IRS) serves as a heuristic for Task and Motion Planning. In this study, we used Logic Geometric Programming (LGP) and Multi-Bound Tree Search (MBTS) to solve TAMP problems [4, 65]. During our dataset construction, we created counterfactual scenarios and used Individual Treatment Effect to determine the better scenario [66]. Representative Rule-Based Learner (RRL) [25] and Correlation and Order-Aware Rule Learning (CARL) [26] are the main components of ORS, providing interpretable responsibility sharing. This section will provide a brief background on these concepts"}, {"title": "Logic-Geometric Programming", "content": "Logic Geometric Programming (LGP) serves as the foundational framework for solving TAMP problems. We will now outline the optimization strategy of LGP, as formulated in Equation 1 [4]. Let the configuration space $X \\subset \\mathbb{R}^n \\times SE(3)^m$ represent m rigid objects and n articulated joints, starting from the initial condition $x_0$. The objective of LGP is to optimize a sequence of symbolic actions $a_{1:K}$, states $s_{1:K}$, and the associated continuous trajectory $x(t)$, where $t \\in \\mathbb{R}$ maps to $X$, in order to achieve a symbolic goal g. Positions, velocities, and accelerations are denoted by $\\hat{x} = (x, \\dot{x}, \\ddot{x})$. The domain of symbolic states $s \\in S$ and actions $a \\in A(s)$ is discrete and finite, with state transitions $s_{k-1} \\rightarrow s_k$ defined by a first-order logic language, similar to PDDL.\n$\\min\\limits_{X,s_{1:K},a_{1:K},K} \\int_{0}^{KT} c(\\hat{x}(t), s_k(t)) dt$\ns.t. $\\hat{x}(0) = x_0$,\n$\\forall t \\in [0, KT]: h_{path}(\\hat{x}(t), s_k(t)) = 0$,\n$g_{path}(\\hat{x}(t), s_k(t)) \\leq 0$,\n$\\forall k \\in \\{1, ..., K\\}: h_{switch}(\\hat{x}(t_k), a_k) = 0$,\n$g_{switch}(\\hat{x}(t_k), a_k) \\leq 0$,\n$a_k \\in A(s_{k-1})$,\n$s_k \\in succ(s_{k-1}, a_k)$,\n$s_K \\in S_{goal}(g)$.\nFor any given sequence of actions $a_{1:k}$ and states $s_{1:K}$, the functions $h(\\cdot)$, $g(\\cdot)$, and $c(\\cdot)$ are continuous and have segments that are piecewise differentiable. Optimizing the trajectory is an undertaking of a nonlinear program (NLP). At the heart of LGP lies the integration of discrete symbolic search (i.e., the pursuit of a sequence of symbolic states that culminate in the goal) with simultaneous nonlinear optimization. This combined strategy is tasked with computing trajectories that adhere to the defined constraints and determining the feasibility of actualizing a sequence of symbolic actions in a continuous manner."}, {"title": "Multi-Bound Tree Search", "content": "The discrete elements of the LGP formulation, detailed in Equation 1, give rise to a decision tree populated with sequences of symbolic states originating from $s_0$. The leaf nodes where $s \\in S_{goal}(g)$ are identified as potential solutions, alternatively goal states $s_g$. The feasibility of each pathway is assessed by solving the NLP produced by the state sequence from the root to the evaluated node. This process, however, can be computationally demanding as the number of NLPs to consider is typically large. To address this challenge, Multi-Bound Tree Search (MBTS) initially resolves more relaxed versions of Equation 1 [65]. The feasibility of these relaxed instances is a prerequisite for the feasibility of the corresponding original NLP, thus acting as computational lower bounds. For this work, we utilize two bounds: $\\text{P}_{seq}$ (joint optimization of the mode-switches of the sequence) and $\\text{P}_{path}$ (the full motion planning problem)."}, {"title": "Counterfactual Scenarios & Individual Treatment Effect (ITE)", "content": "A counterfactual scenario is the scenario that would have occurred if the sample had received a different treatment, which refers to the action applied to a sample [66, 67]. In binary treatment scenarios, $Y'$ represents the counterfactual outcome, simplified as $Y' = Y(W = 1 - w)$, where w is the actual treatment received by the sample. Starting from the same initial conditions and using a fixed planner, only one plan can be generated if transition dynamics are deterministic. This is because deterministic dynamics ensure a single, predictable outcome for a given set of initial conditions and planner. In this study, to understand the effects of incorporating auxiliary objects, we generate counterfactual plans, $P'$. These counterfactual plans allow us to observe how the inclusion of different auxiliary objects might alter the outcomes, providing insights into the impact and utility of these auxiliary elements in the planning process.\nThe treatment effect quantifies the impact of a treatment or intervention on an outcome of interest. In our work, we investigate whether incorporating auxiliary objects will increase the performance of the agent [66, 67]. As each scenario is unique, we employ Individual Treatment Effect (ITE) to measure the difference in outcomes for a specific sample if it were to receive the treatment versus not receiving it (i.e., shared responsibility or not).\n$ITE_i = Y_i(W = 1) - Y_i(W = 0)$"}, {"title": "Rule-Based Representation Learner (RRL)", "content": "RRL [25] is designed to merge scalability with interpretability by employing non-fuzzy rules for data representation and classification. This model introduces a novel method called Gradient Grafting to train discrete models effectively. RRL's hierarchical model structure consists of layers for feature discretization, rule-based representation, and rule importance evaluation. The improved logical activation functions in RRL convert multiplications into additions using logarithms, thereby avoiding the vanishing gradient problem often encountered in neural networks.\nThe binarization layer in RRL divides continuous features into bins, allowing for end-to-end feature discretization. Gradient Grafting, the key training method, utilizes gradient information at both discrete and continuous parametric points, optimizing the training process for discrete models. The Gradient Grafting equation as provided in [25] captures how the gradient updates are adjusted during the training process to accommodate both continuous and discrete parameter states effectively:\n$\\hat{g} = g_{continuous} \\times I(\\sigma_{logical}(x) \\geq 0.5) + g_{discrete} \\times (1 - I(\\sigma_{logical}(x) \\geq 0.5))$"}, {"title": "Correlation and Order-Aware Rule Learning (CARL)", "content": "CARL [26] captures deeper semantic information in rules by ensuring relations are aware of each other and paying attention to logical sequence sensitivity. The model leverages the semantic consistency between the rule body and rule head as its learning objective. The Correlation Module within CARL explores internal correlations between domain relations using a multi-head attention mechanism to select \u201cactive relations\u201d, which reduces computational complexity.\nPath sampling in CARL involves a random walk-based sampler that gathers closed paths from the knowledge graph, forming probable rules. Positional encoding generates unique positional vectors for each relation using trigonometric functions, distinguishing rules based on their positions within the rule body. The Semantic Learner component iteratively merges relations to reduce the length of the rule body while continuously introducing more semantic information.\nThe positional encoding equations in CARL are designed to assign a unique encoding to each relation in a rule based on its position, which is crucial for maintaining the order sensitivity in logical sequences.\n$PE_{pos, 2i} = sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$\n$PE_{pos, 2i+1} = cos(\\frac{pos}{10000^{\\frac{i}{d_{model}}}})$\nThese functions, sine and cosine, are utilized to generate positional vectors, where pos represents the position of a relation in the rule, and $d_{model}$ is the dimensionality of the model. These vectors aid in distinguishing the roles of the same relation appearing at different positions within the rule sequence, ensuring that the model captures the order-sensitive information effectively.\nThe Multi-Head Attention mechanism in the Semantic Learner component is used to merge relations and enhance the semantic understanding of the rule body by focusing on important relations.\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) V$\nHere, Q, K, and V represent queries, keys, and values, respectively, in the attention mechanism, and $d_k$ is the dimensionality of the keys. The softmax function is applied to the scaled dot-product of Q and K (scaled by $\\sqrt{d_k}$), ensuring that the attention weights sum to one. This mechanism allows the model to focus on the most relevant parts of the rule body, facilitating better integration of relational semantics.\nThe objective function is used to train the model by optimizing the semantic consistency between the rule body and the rule head.\n$\\mathcal{L} = - \\sum_{(r_b, r_h) \\in Z}  U_{rk} \\sum_{k=0}^{\\vert \\mathcal{R} \\vert} log\\theta_{zk}$"}, {"title": "4. Methodology", "content": "We aim to leverage human bias in sequential decision-making and manipulation problems without compromising interpretability. Human-constructed environments and tasks introduce this bias, represented in our formulation as Responsibility Sharing (RS). Household environments often contain auxiliary objects that, while not mandatory, can be used to indirectly manipulate target objects. RS allows for the division of the original problem into sub-problems through shared responsibilities. For example, an agent can hold an object directly or use a bag to hold the object, with the bag sharing the responsibility of holding the object. By using RS, the agent can simplify complex tasks by breaking them down into more manageable sub-tasks involving auxiliary objects, thereby improving the agent's effectiveness by leveraging inductive bias in human-constructed problem settings.\nTo decide when to utilize these auxiliary objects, we train an interpretable model, Optimized Rule Synthesis (ORS), which generates rules for the appropriate conditions under which the agent should delegate responsibilities to these objects. To ensure robustness, we use Counterfactual Plan Generation (CPG) to create a dataset demonstrating how auxiliary objects impact performance changes. This approach ensures that the agent uses auxiliary objects only when it leads to performance improvements. Together, these components enhance the effectiveness and efficiency of TAMP formulation in an interpretable manner. The following subsections elaborate on these components and the problem setup adopted in this study."}, {"title": "Problem Setup", "content": "We consider fully observable domains with a symbolic state space defined under first-order logic S, configuration space X, action space A, and deterministic transition function \u03b4 : S \u00d7 A \u2192 S. We assume a finite set of goals G. Each g\u2208G is a binary condition function g : S \u2192 {0,1} indicating if a symbolic state is a goal state Sgoal(g) or sg. The goal is to reach a goal state sg starting from an initial state so following a task plan P = {a1, a2, ..., \u0430\u0442}, where T is the required number of actions to reach the goal."}, {"title": "Generating a Feasible Plan", "content": "Sequential decision-making and manipulation problems are defined by logical states, actions, goals, and physical constraints. Following the TAMP formulation, the agent employs a search in the state space following a transition function, s' = d(s,a). Starting from the initial state so, the search algorithm expands from this state by taking actions at, transitioning to the next state s', and gradually reaching a goal state sg. This generates a series of actions following the formulation detailed in Equation 1, resulting in a task plan, P = {a1, a2, ..., \u0430\u0442}, where T represents the number of actions required to reach the goal state from the initial state. CPG utilizes Multi-Bound Tree Search [65], as detailed in Section 3.2, as the search algorithm for finding a task plan due to its domain-agnostic and geometrically aware nature, making it well-suited for these types of problems."}, {"title": "Generating a Counterfactual Plan", "content": "Next, we generate a counterfactual plan using the real plan that was generated in the previous step. The aim of this counterfactual generation is to determine how the plan would change if we utilized the auxiliary objects, i.e., if the agent shared the responsibility. Since the real plan is generated by the uninformed search, it does not utilize these objects as they increase the depth of the solution. Using the same initial state so, transition function \u03b4, and search method (which is uniformed), we introduce an additional subgoal si, that satisfies the responsibility sharing conditions (e.g., if a tray will be used to carry the objects, they are first placed on the tray). The search expands from the initial state so until it reaches the goal state sg while satisfying the intermediate state si. This generates a counterfactual series of actions, forming a counterfactual task plan, P' = {a1, a2, ..., \u03b1\u03c4'}, where T' represents the number of actions required to reach the goal state from the initial state while employing Responsibility Sharing."}, {"title": "Dataset Construction", "content": "By repeating the first two steps, we generate multiple real and counter-factual plan pairs, P and P', based on the initial and goal states, so and Sg. To decide which plan yields better performance, we employed Individual Treatment Effect (ITE) formulated as Equation 2. We selected the L2 norm as the measure for the first two tasks, serving and pouring, and Manhattan distance for the last task, handover, due to obstacles in the environment. After selecting the effective plan P* among the pairs, we use it as a binary label I for the initial and goal state pair: positive if utilizing auxiliary objects yields better performance and negative otherwise. We compile them into a dataset, D = {(s1, s, I1), ..., (s\", sn, In)}, to train ORS, where n represents the number of problems generated.\""}, {"title": "Optimized Rule Synthesis (ORS)", "content": "Optimized Rule Synthesis (ORS) aims to generate rules for an embodied agent, defining the appropriate conditions for responsibility sharing with auxiliary objects present in the environment (Figure 4). To enhance overall effectiveness and interpretability, ORS integrates Rule-based Representative Learner (RRL) [25] and Correlation and Order-Aware Rule Learning (CARL) [26] (Section 3.4 and 3.5). The integration logic involves parsing rules from various sources into a structured format, generating all possible combinations of these rules, and evaluating their applicability to data samples. This process includes calculating the confidence levels for each rule combination and selecting the best-performing rules.\nIn the integrated approach, the rules are evaluated against the initial and goal states, so and sg, to determine the appropriate conditions for responsibility sharing, taking into account all tasks and environments. The system iteratively checks each combination against the data to find the best match, balancing accuracy and interpretability. The selected set of rules is consistent across different tasks, increasing the generalizability of ORS compared to informed search approaches.\nORS checks whether so and sg meet all specified conditions of a rule using logical operations like AND or OR. Based on whether the conditions are met, a predicted label (indicating the usage of auxiliary objects, as described in Section 4.2.3) is assigned to each data sample, accompanied by a confidence score representing the proportion of conditions satisfied:\n$Confidence = \\frac{Number \\ of \\ conditions \\ satisfied}{Total \\ number \\ of \\ conditions}$\nTwo key factors in this integration are accuracy and interpretability. Accuracy refers to the correctness of the rule-based decisions, while interpretability relates to how easily these rules can be understood and applied by humans. There is often a trade-off between these two factors: highly accurate models may be complex and difficult to interpret, while simpler, more interpretable models might not achieve the same level of accuracy [69]. Accuracy can be measured using the formula:\n$Accuracy = \\frac{True \\ Positives + True \\ Negatives}{Total \\ number \\ of \\ samples}$\nInterpretability is often inversely related to the complexity of the rules and can be simplified as the number of conditions in the rules.\nThe integration logic ensures a balance between accuracy and interpretability. Using a validation set, the model optimizes both the accuracy and interpretability of the rules. This balance is achieved through an iterative process of testing and refining the rules, ultimately leading to a set of high-quality, interpretable rules that can effectively handle the usage of auxiliary objects in complex household tasks. ORS generates all possible combinations by combining rule components based on logical operations (AND and OR), starting from a minimum threshold (minimum number of conditions) up to the total available conditions. To optimize the rule-based system, ORS iteratively tests increasing numbers of minimum thresholds for combining conditions, seeking the best Balance Score between prediction accuracy and rule interpretability, as defined in Equation 10.\n$Balance \\ Score = a \\times Accuracy + (1 - a) \\times Interpretability$\nwhere a = 0.5. This optimization process uses validation data to adjust the complexity of the rule set for improved performance. Results from each trial are saved in a structured format allowing for detailed analysis of the rule system's effectiveness over multiple iterations. The main loop reshuffles and re-evaluates the data multiple times, refining the approach to achieve optimal results based on the performance metrics and saved outcomes. This iterative, data-driven method enhances the adaptability and effectiveness of the rule-based decision-making process by investigating how different rules interact and influence predictions.\nIn the evaluation phase, these combinations are applied across a dataset, and their outcomes are recorded and sorted by the confidence level of their predictions. If ORS infers the use of auxiliary objects, it also returns predefined usages of these objects, defining sub-problems \u03a8 for the IRS framework. To elaborate on these predefined usages, if the environment contains a tray that is beneficial to use in a certain scenario, the objects available in the environment first need to be placed on the tray, and then the robot will carry the tray to the final position to complete the task. In this scenario, \u03a8 is defined in terms of first-order logic as follows: if so = 4 objects, Sg = objects on table, then \u03a8 = {objects on tray, tray on table} using the carry action of the tray. The complete ORS algorithm is shown in Algorithm 1."}, {"title": "Interpretable Responsibility Sharing as a Heuristic", "content": "In this work", "4": "and used Multi-Bound Tree Search (MBTS) [65", "16": ".", "formulation.\n$\\min\\limits_{X,S_{1": "K"}, "a_{1:K},K_{\\psi}} \\sum_{\\psi=1}^{\\Psi} \\sum_{k=1}^{K_{\\psi}} c(\\hat{x}(t), s_k(t)) dt$\ns.t. $\\hat{x}(0) = \\begin{cases} x_0 & \\text{if } \\psi = 1\\\\ \\hat{x}_{T_{K_{\\psi-1}}} & \\text{otherwise} \\end{cases}$\n$\\forall k \\in \\{1, ..., K_{\\psi}\\} : h_{switch}(\\hat{x}(t_k), a_k) = 0$,\n$g_{switch}(\\hat{x}(t_k), a_k) \\leq 0$,\n$a_k \\in A_{\\psi}(s_{k-1})$,\n$s_k \\in succ(s_{k-1}, a_k)$,\n$s_{K_{\\psi}} \\in S_{goal}(g_{\\psi})$.\nHere, we provide the details of our strategy called"]}