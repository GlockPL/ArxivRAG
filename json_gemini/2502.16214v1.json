{"title": "SalM2: An Extremely Lightweight Saliency Mamba Model for Real-Time Cognitive Awareness of Driver Attention", "authors": ["Chunyu Zhao", "Wentao Mu", "Xian Zhou", "Wenbo Liu", "Fei Yan", "Tao Deng"], "abstract": "Driver attention recognition in driving scenarios is a popular direction in traffic scene perception technology. It aims to understand human driver attention to focus on specific targets/objects in the driving scene. However, traffic scenes contain not only a large amount of visual information but also semantic information related to driving tasks. Existing methods lack attention to the actual semantic information present in driving scenes. Additionally, the traffic scene is a complex and dynamic process that requires constant attention to objects related to the current driving task. Existing models, influenced by their foundational frameworks, tend to have large parameter counts and complex structures. Therefore, this paper proposes a real-time saliency Mamba network based on the latest Mamba framework. As shown in Figure 1, our model uses very few parameters (0.08M, only 0.09~11.16% of other models), while maintaining SOTA performance or achieving over 98% of the SOTA model's performance.", "sections": [{"title": "Introduction", "content": "With the rapid advancement of autonomous driving technology, understanding and predicting driver behavior has become increasingly important. Among the many factors affecting driving safety, the driver's attention state is crucial (Balasubramani et al. 2024). Distraction or insufficient attention can lead to reduced perception of the surrounding environment, increasing the risk of traffic accidents (Fang et al. 2024). Therefore, research on methods for recognizing driver attention is of great significance for enhancing road safety and advancing the development of intelligent driving assistance systems.\nGiven that the traffic driving scenario is dynamic and complex (Alkaabi 2023), it is unwise to analyze driver attention solely based on features from the driving scene. The driving environment contains not only rich \u201cBottom-up\" image features (such as the sky, buildings, vehicles, etc.) but also \"Top-down\" information related to the current driving task (such as vehicles ahead, pedestrians crossing the road, traffic lights, etc.). Relying only on features within the driving scene to identify driver attention may cause the model to overfit to the labeled driver attention data, leading to a lack of understanding of the driving context and reducing the model's generalization ability. However, within the visual attention system, relevant stimuli and cues can be selected while filtering out less relevant ones from the rich visual information around us. Attention resources are then allocated to these stimuli and cues, typically by directing gaze toward objects of interest in the visual environment (Hertz-Palmor et al. 2023; Itti and Koch 2001). This process is known as selective visual attention or selective attention allocation (Evans et al. 2011). Similarly, in specific traffic driving scenarios, a driver's visual attention, driven by the driving task, tends to focus on one or more regions or targets that are related to the task at hand. When performing a driving task, a driver's attention needs to remain highly focused, consistently allocating attention to objects or areas relevant to the current driving task. If the driving scenario or task changes, the driver's attention will also shift accordingly. Therefore, the semantic information of the current driving scene is crucial for understanding the subsequent shifts in driver attention.\nNumerous researchers have studied driver attention prediction, but most of these studies focus on the overall features of the driving scene, often neglecting the understanding of semantic information present within the scene (Xia et al. 2019; Baee et al. 2021; Shen et al. 2022). Additionally, existing models tend to have large parameter counts and high computational complexity (Fang et al. 2022; Chen, Nan, and Xiang 2023). Therefore, this paper proposes a novel driver attention recognition model based on driving events, which not only has a small parameter count but also utilizes the semantic information from the driving scene to guide the recognition of current driver attention. Specifically, we employ a dual-branch structure: one \u201cBottom-up\" branch extracts image features from the driving scene, while the other \"Top-down\" branch captures the semantic information of the scene. At the deepest layer of image feature extraction, we integrate the semantic information of the driving scene to guide the image features. This semantic-guided feature is then passed through a decoder module to obtain the final driver attention map. Since our driver gaze prediction is driven by the contextual information of the driving scene, the resulting attention maps align more closely with the actual distribution of driver attention. To summarize, the contributions of this work are the following:\n\u2022 We propose a driver attention prediction architecture inspired by human visual attention mechanism. Our architecture include a \"Bottom-up\" branch that extracts stimulus-driven image features from the driving scene, and a \"Top-down\" branch that extracts task-driven semantic features. The \"Top-down\" branch includes a novel Selective Channel Parallel Mamba (SCPM) layer, which not only addresses the parameter explosion issues caused by high-dimensional data in Mamba, but also corrects the information loss from fixed channel partition.\n\u2022 We design a novel Cross-Modal Attention (CMA) fusion module. This module integrates the semantic information from the cognitive process and the image features to guide the allocation of driver attention in driving scenarios. Our module adopts the idea of channel attention to address the challenge of feature dimension mismatch between semantic and image information, allowing for effective fusion. The entire module introduces only a single additional parameter.\n\u2022 We develop an extremely lightweight effective driver attention prediction network. To the best of our knowledge, the SalM\u00b2 network is the most lightweight model for driver saliency prediction, with only 0.08M trainable parameters. The model is trained on three popular datasets, and SalM\u00b2 achieves SOTA performance using only a fraction (approximately 0.09% to 11.16%) of the parameters compared to other models, or reaches 98+% of the performance of the SOTA models."}, {"title": "Related Work", "content": "Previous works have made notable advances in driver saliency prediction, primarily focusing on inherent visual features or semantic information from image segmentation and optical flow. Tawari et al. pioneered first-person gaze estimation using Google Glass (Tawari et al. 2014), while Deng et al. proposed a bottom-up saliency model combining low and high-level features (Deng, Yan, and Li 2018), though limited by traditional machine learning's feature extraction capabilities.\nAs research progressed, numerous datasets and deep learning algorithms emerged to address conventional limitations. The DR(eye)VE dataset by Alletto et al. covers various driving conditions but lacks scene diversity and semantic richness (Alletto et al. 2016). The BDDA dataset introduced by Xia et al. enriches the field with urban driving scenarios and emergency events (Xia et al. 2019). Fang et al. presented DADA-2000 dataset covering normal and accident scenarios with a semantic-guided attention fusion network, though limited to collision scenes and segmentation-based semantics (Fang et al. 2019). The TrafficGaze dataset by Deng et al. offers comprehensive clear-weather data with a lightweight CNN framework (Deng et al. 2019), while the DrFixD-rainy/night dataset specifically addresses adverse weather conditions (Tian, Deng, and Yan 2022; Deng et al. 2024). Nevertheless, these approaches rely on scene features or segmentation-based semantics, lacking scene understanding. Brishtel et al. demonstrated correlations in gaze patterns across driving modes (Brishtel et al. 2022). Vozniak et al. successfully incorporated semantic danger cues in attention prediction (Vozniak et al. 2023), though obtaining annotated semantic data remains challenging.\nIn summary, while substantial research has developed numerous models for predicting driver attention, there remains a gap in utilizing basic semantic guidance of driving scenes for driver saliency prediction. Therefore, this paper employs the CLIP (Radford et al. 2021) model to extract semantic information from driving scenes. To validate the effectiveness of our proposed method in diverse and dynamic driving scenarios, we conduct experiments across datasets with different weather conditions (TrafficGaze, DrFixD-rainy) and complex semantic information (BDDA)."}, {"title": "Downstream Tasks", "content": "This is an important task in computer vision aimed at detecting the most prominent object regions in an image. In the context of traffic driving scenes, drivers often automatically filter out objects unrelated to the current driving task. Therefore, some existing works use driver attention allocation as prior knowledge to detect prominent or key objects (Qin et al. 2022; Shi et al. 2024). This approach does not detect all objects in the driving scene but focuses on those most relevant to the current driving task, thereby reducing redundant information.\nIn the field of intelligent transportation systems, detecting driving events by analyzing"}, {"title": "Proposed Method", "content": "Since the traffic scene is a complex and dynamically changing setup, the driver requires continuous focus on objects/areas that are significantly relevant to the current driving scene. However, most of the previous work only considered spatio-temporal information in driving scenarios. In fact, solely relying on visual information from the driving scene is not sufficient to accurately identify the locations or areas that need the driver's attention (Rasouli and Tsotsos 2018) that is the \"Bottom-up\" image information. This is because the driver's attention is closely related to the current driving task, and it is also necessary to consider the \u201cTop-down\" semantic information related to the current driving task that is embedded in the driving scene.\nConsidering these elements, we propose a saliency mamba model, named SalM2 that uses \"Top-down\" driving scene semantic information to guide \"Bottom-up\" driving scene image information to simulate human drivers' attention allocation. The overall framework is illustrated in Figure 2, our model first utilizes an efficient network framework Mamba structure to build an extremely lightweight backbone network for extracting \"Bottom-up\" features from image and decoding the features. Simultaneously, a visual encoder is used to understand the \"Top-down\" semantic information driven by the driving task in the current traffic scene. Then, when the deepest features representing image information of the driving scene is extracted from the backbone network, we utilize the \"Top-down\" semantic information to guide the \"Bottom-up\" image information. Finally, we decode the perceived information in the decoder of the backbone network to obtain the driver attention map.\nSince we aim for intelligent vehicles to have the capability to focus on objects related to driving tasks in the scene, just like a driver, we need to develop a network with low computational cost and deployability with lower hardware requirements. To achieve this goal, we construct an extremely lightweight backbone network as the image encoding and decoding network, as shown in Figure 3. This design allows us to fully utilize the Mamba structure's efficient feature representation capability, accelerating the learning of the representation space at the channel level. Inspired by the work of Wu et al. (Wu et al. 2024), we modified it to perform high-dimensional feature representation in our selective channel parallel Mamba (SCPM) layers. This framework contains only 0.0759M parameters, with a model size of just 1.6MB.\nIn this architecture, the overall backbone network is a hierarchical structure based on Mamba (Gu and Dao 2023). We first use convolutional network layers to extract low-level features from images, and then employ SCPM layers to further extract high-level features. To leverage the efficiency of the Mamba framework and address the parameter disaster caused by high-dimensional data, a typical solution is to split the input $X \\in \\mathbb{R}^{B\\times C\\times H\\times W}$ into four parts $x_i \\in \\mathbb{R}^{B\\times C/4\\times H\\times W}, i = 1,2,3,4$ and then process them in parallel through the Mamba layers. To correct the information loss caused by fixed channel splitting, we design the SCPM layer. This layer embeds the input features into convolutional blocks in parallel and then feeds them into the Mamba layer in parallel. The output feature channels of the parallel branches differ, but the feature dimensions remain consistent. The operation of each parallel channel is as shown in Equation 1.\n$\\Phi_i(X) = Conv_i(X) + \\alpha \\times Mamba(Conv_i(X)),$  (1)\nwhere $\\Phi_i(X)$ represents the output of the $i^{th}$ parallel layer, $\\alpha$ is a skip scale, $Y \\in \\mathbb{R}^{B\\times C'\\times H\\times W}$ represents the final output of the SCPM layer, and $\\Sigma$ represents the summation operation, which in this context is the summation along the channel dimension.\nFor most models, the backbone network is used to extract \"Bottom-up\" image information to predict driver attention (Min and Corso 2019; Deng, Yan, and Yan 2021; Tian, Deng, and Yan 2022; Chen, Nan, and Xiang 2023). However, the traffic environment is a complex and dynamic process where different driving scenarios involve specific driving tasks (such as stopping at traffic lights, yielding to pedestrians crossing the road, etc.). Therefore, we need to understand the specific driving tasks in the current driving scenario. Using this \"Top-down\" scene information to guide the \u201cBottom-up\" image information can better identify the driver's attention allocation. However, text data contains strong semantic and logical properties, making it difficult to match and align with image data in the feature space. To further explore how to use semantic information to guide driving scene information, our proposes using a cross-modal attention mechanism (CMA).\nAs shown in Figure 4, CMA uses the semantic information $Info_{text}$ extracted by CLIP (Radford et al. 2021) and the image information $Info_{image}$ extracted by the backbone network as inputs. However, directly fusing these inputs leads to a dimension mismatch problem as $Dim(Info_{text}) \\neq Dim(Info_{image})$. Therefore, we project the semantic information into the same feature channel"}, {"title": "Cross-Modal Attention Mechanism", "content": "$\\sum_{i=1}^{i=n} Y , i \\in \\{1,2,3,4\\}, n = 4,$\t(2)\nspace as the image information. Using the simple structure of the channel attention module (Fu et al. 2019), we achieve cross-modal information fusion, thereby avoiding the issue of scale mismatch between different types of information.\nDue to CLIP being a large model trained through contrastive learning, the extracted semantic information and image features have already been aligned in a similar feature space through contrastive training, enabling us to readily obtain the original semantic information $Info_{forat} \\in \\mathbb{R}^{B\\times Token}$ from images. Nevertheless, we still need to perform a easy dimensional projection on the semantic information to ensure identical channel dimensions between the two modalities while preserving all information. we map the original semantic information $Info_{tent}$ to the image feature space, obtaining new semantic information $Info_{text}$.\n$Info_{text} = projector(Inforat)$.\t\t(3)\nWithin the CMA module: we first reshape the semantic information $Info_{text} \\in \\mathbb{R}^{B\\times C\\times H_1\\times W_1}$ to $Q \\in \\mathbb{R}^{B\\times C\\times N_1}$ as Query and transpose $Info_{image}$ to $K \\in \\mathbb{R}^{BXN_1XC}$ as Key. Then, we perform a matrix multiplication between the $Q$ and the $K$. Finally, we apply a softmax operation to obtain the saliency semantic representation $S \\in \\mathbb{R}^{B\\times C\\times C}$.\n$S_{ji} = \\frac{exp(Q_i, K_j)}{\\sum_1^{C} exp(Q_i, K_j)}$.\t(4)\nwhere $s_{ji}$ measures the influence of the $i^{th}$ channel on the $j^{th}$ channel, C is the total number of channels, and $exp(\\cdot,\\cdot)$ is used to measure the similarity between two channels.\nIn addition, we reshape the deepest image information $Info_{image} \\in \\mathbb{R}^{BXC\\times H_2\\times W_2}$ to $V \\in \\mathbb{R}^{B\\times C\\times N_2}$ as the value and perform a matrix multiplication between S and V, then reshape the result to $Info_{fusion} \\in \\mathbb{R}^{B\\times C\\times H_2\\times W_2}$. Next, we multiply the attention $S'$ by a scaling parameter $\\gamma$ and add the image feature information $Info_{image}$ to obtain the final cross-modal fused information $S' \\in \\mathbb{R}^{B\\times C\\times H_2\\times W_2}$.\n$S'\\_i = \\sum\\_{i=1}^{C}(\\gamma\\cdot(S_{ji} - V\\_i)) + Info\\_{image}$             (5)\nwhere $\\gamma$ is a learnable weight and is initialized to 0, $N_1 = H_1 \\times W_1, N_2 = H_2 \\times W_2, H_2 = 2H_1$, and $W_2 = 2W_1$.\nIn this work, we train our proposed SalM2 model on 3 representative driver attention datasets: Traf-"}, {"title": "Conclusion", "content": "In driving, a driver's attention is predominantly focused on the area ahead, such as the road's endpoint or the vehicle in front. However, in complex environments, this attention can shift due to sudden events in the field of view, making the semantic information of the current driving scene crucial for understanding driver attention. We have investigated driver attention through the lens of dual-process visual cognition and developed a driver attention prediction model based on driving scene information. This model integrates \"Top-down\" semantic information (high-level) with \"Bottom-up\" image information (low-level) for effective attention prediction.\nExtensive experiments demonstrate that guiding model understanding of driver attention through driving scene information can be an effective approach. Our model achieves SOTA or near-SOTA performance with minimal parameters. In future work, we aim to explore more effective methods for using driving scene semantic information to guide driver attention prediction, further improving prediction accuracy."}]}