{"title": "Dependency-Aware CAV Task Scheduling via Diffusion-Based Reinforcement Learning", "authors": ["Xiang Cheng", "Zhi Mao", "Ying Wang", "Wen Wu"], "abstract": "In this paper, we propose a novel dependency-aware task scheduling strategy for dynamic unmanned aerial vehicle-assisted connected autonomous vehicles (CAVs). Specifically, different computation tasks of CAVs consisting of multiple dependency subtasks are judiciously assigned to nearby CAVs or the base station for promptly completing tasks. Therefore, we formulate a joint scheduling priority and subtask assignment optimization problem with the objective of minimizing the average task completion time. The problem aims at improving the long-term system performance, which is reformulated as a Markov decision process. To solve the problem, we further propose a diffusion-based reinforcement learning algorithm, named Synthetic DDQN based Subtasks Scheduling, which can make adaptive task scheduling decision in real time. A diffusion model-based synthetic experience replay is integrated into the reinforcement learning framework, which can generate sufficient synthetic data in experience replay buffer, thereby significantly accelerating convergence and improving sample efficiency. Simulation results demonstrate the effectiveness of the proposed algorithm on reducing task completion time, comparing to benchmark schemes.", "sections": [{"title": "I. INTRODUCTION", "content": "With advancements in communication and autonomous driving technologies, connected autonomous vehicles (CAVS) have become increasingly prevalent, catering to people's traffic demands [1]. They have to execute various computation-intensive and delay-sensitive tasks, including perception fusion, real-time navigation based on video or Augmented Reality (AR), and multimedia entertainment, etc [2]. These tasks necessitate joint processing to guarantee safe driving while satisfying the quality of service [3].\nDue the limited computing resources for CAVs, it will inevitably prolong tasks completion time when multiple tasks need to be processed simultaneously. To minimize the task completion time, some researches [4] [5] used mobile edge computing (MEC) to directly offload entire task to the base station (BS) for fast processing through the vehicle-to-infrastructure (V2I) link, which may cause extended completion time due to the additional task transmission. Thus, some literature [6] [7] had proposed the partial offloading scheme, one part of the task is assigned to the local vehicle, while the remaining is offloaded to edge servers. Although the scheme reduces transmission delay, it remains challenging to guarantee efficient tasks scheduling due to the high dynamic.\nConsequently, vehicle edge computing (VEC) based tasks scheduling is gradually emerging, other vehicles with available computing resources are acted as extensions of the edge and termed service vehicles (SVs) [8], the tasks generated by task vehicles (TVs) can be offloaded to nearby service vehicles.\nFurthermore, the fine-grained vehicular tasks partition and scheduling accelerates the completion of the task [9] [10], since tasks can be divided into several dependent subtasks, which are modelled as the directed acyclic graph (DAG) [11] describing subtasks interdependency, and offloaded to other SVs or BS. The existing researches illuminate the advantages of the internet of vehicle (IoV) combined with VEC task offloading. However, due to geographical limitations, placing many BSs on highway may not be feasible on economic benefits [12]. Thus, for tasks with diverse computation requirements, optimizing the tasks scheduling with high-mobility SVs and the limited BS servers is a challenging problem.\nIn this paper, we investigate the task scheduling problem for highway CAVs. TVs offload subtasks to SVs with available computing resources, but time-varying task computation requirements and computing resources make it challenging to make adaptive task scheduling decisions in real-time, which will prolong the completion time when the required computing resources of a subtask exceeds the computing capacity of SVs. Due to the advantages of flexibility and easy deployment, the unmanned aerial vehicle (UAV) can be deployed for relaying subtasks to surrounding BS server, aiming at compensating for the offloading needs of TVs when the number of SVs is insufficient or subtasks have a higher workload. Firstly, with the goal of on-demand tasks scheduling, we construct a task scheduling model, i.e., two-side priority adjustment, for determining the scheduling priority of subtasks while considering mobility and resources for selecting optimal offloading targets. Second, we formulate the long-term subtasks scheduling problem that minimizes the average completion time of overall tasks, and the deep reinforcement learning (DRL)-driven synthetic DDQN based subtasks scheduling algorithm (SDSS) is proposed for solving the problem in a dynamic environment. Thirdly, simulation results demonstrate the effectiveness of the proposed algorithm on reducing task completion time. The main contributions of this paper are summarized as follows:\n\u2022 We design a dependency-aware task scheduling strategy to reduce task completion time for CAV networks;\n\u2022 We formulate a long-term optimization problem for mini- mizing the overall tasks average completion time and then"}, {"title": "II. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "We consider a UAV-assisted highway CAV task scheduling scenario, as depicted in Fig. 1, where a set of TVs can offload the generated computation-intensive tasks that composed of several dependent subtasks to SVs directly via vehickle-to-vehicle (V2V) links, or offload the task to the BS server with the help of the relay UAV, for satisfying TVs tasks offloading demands as needed. In addition, the set of TVs and SVs is denoted as N and S, respectively.\nA. Communication Model\nBy allocating the communication resources to different transmitting vehicles, the orthogonal frequency-division multiple access technique [13] is considered to avoid interference among multiple vehicles. There are two types of links considered in this paper: V2V links and UAV-assisted V2I links. The TVs can offload tasks using no more than one link type. In V2V links, the data transmission rate for tasks offloading from TV n to SV s can be calculated as\n$T_{n,s} = B_{n,s} \\log_2 \\bigg(1+\\frac{P_n h_{n,s}}{\\sigma^2}\\bigg),$ (1)\nwhere $B_{n,s}, P_n, h_{n,s}$, and $\\sigma^2$ represent the channel bandwidth, transmitting power at TV n, channel gain for the link between TV n and SV s, and the noise power, respectively.\nIn V2I links, the UAV can relay the tasks from TV n to the BS server j. The data rate from TV n to the UAV, and from UAV to the BS j can be calculated as follows,\n$T_{n,u} = B_{n,u} \\log_2 \\bigg(1+\\frac{P_n h_{n,u}}{\\sigma^2}\\bigg),$ (2)\n$T_{u,j} = B_{u,j} \\log_2 \\bigg(1+\\frac{P_u h_{u,j}}{\\sigma^2}\\bigg),$ (3)\nwhere $B_{n,u}$ and $B_{u,j}$ represent the channel bandwidth of the links from TV n to UAV u, and from UAV u to BS server j, respectively. $P_u$ is the transmitting power at UAV n, $h_{n,u}$ and $h_{u,j}$ are the channel gain from n to u and from u to j.\nB. Task Dependency Model\nThe TV generates M consecutive and computation-intensive tasks simultaneously. Let $I_m = (W_m, T_m, f_m)$ represents the characteristic of m-th task, $1 < m < M$, where $W_m, T_m$, and $f_m$ represent the computation workload, the maximal tolerant delay, and the computing resource demand of the m-th task,"}, {"title": "C. Task Scheduling Model", "content": "1) Scheduling Time: A series of subtasks can be scheduled from TV n to nearby SVs, relayed to BS by UAV, or executed on the local TV. The completion time of each subtask is composed of the transmission delay and computing delay. The transmission delay is ignored when the subtask is executed on a local TV. When subtasks are executed in TV n locally, the computing time is calculated as\n$T_w^n = \\frac{A_w^m}{f_n},$ (4)\nwhere $A_w^m$ and $f_n$ are computing resources required to complete w and available computing capacity of the TV n.\nWhen one subtask w is offloaded to SVs, the completion time of the subtask is given by\n$T_{s}^{w, total} = T_{n,s}^{w} + T_{s}^{w},$ (5)\nwhere $T_{n,s}^{w}$ and $T_{s}^{w}$ represent the data transmission and computing delay, i.e., $T_{n,s}^{w} = w_m/r_{n,s}$ and $T_{s}^{w} = A_w^m/f_s$, and the $f_s$ indicates available computing capacity of the SV s. When the computing demands exceed the computing capacity of the SV, the subtasks could be relayed to BS. The completion time of subtask executed in the BS server j is given as\n$T_{i,j}^{w, total} = T_{n,u,j}^{w} + T_{j}^{w},$ (6)\nwhere $T_{n,u,i}^{w}$ and $T_{j}^{w}$ represent the transmission delay from the SV to BS and computing delay. Then we have $T_{n,u,j}^{w} =$"}, {"title": "D. Problem Formulation", "content": "In this part, we formulate the optimization objective that minimizes the average completion time of overall tasks. Firstly, the completion time $T_m$ of a task m composed of multiple dependency subtasks is denoted as follows,\n$T_m = \\frac{1}{M}\\sum_{m=1}^{M}\\sum_{w=1}^{W} (x_{w,s}T_{s}^{w, total} + y_{w,j}T_{j}^{w, total} + z_{w,n}T_w^n),$ (8)\nwhere the V2V scheduling of subtask w, the V2I scheduling, and the local execution are denoted by the binary variable $X_{w,s}, Y_{w,j},$ and $Z_{w,n}$, which are equal to 1 if the V2V scheduling, UAV-assisted V2I scheduling, and local execution are used, respectively, otherwise, 0. It is assumed that a subtask w can be executed at only one target: local TV n, one SV s, or BS server j. The Eq. (9) must be met to guarantee that each subtask can only be offloaded to one target,\n$X_{w,s_i} + Y_{w,j} + Z_{w,n} = 1.$ (9)\nTherefore, considering the scheduling and offloading of subtasks, the problem of minimizing the overall average completion time of all subtasks can be formulated as\nP:\n$\\min\\limits_{A_w, x_{w,s}, y_{w,j}, z_{w,n}} \\frac{1}{M}\\sum_{m=1}^{M}\\sum_{w=1}^{W}$ (10a)\ns.t. $A_w^m \\le f_s$, $\\forall w \\in W$, $\\forall s \\in S$, (10b)\n$\\sum\\limits_{w=1}^{W} T_w \\le T_m, \\forall m$, (10c)\n$\\tau_{w+1} \\ge \\tau_{w}, \\forall w$, (10d)\n$X_{w,s} + Y_{w,j} + z_{w,n} = 1$, (10e)\n$X_{w,s}, Y_{w,j}, Z_{w,n} \\in {0, 1}$, (10f)\n$\\alpha \\in N,$ (10g)\nwhere $\\alpha_w, X_{w,s_i}, Y_{w,j}, Z_{w,n}$ represent optimization variables related to two-side priority adjustment, $\\alpha_w$ is the scheduling decision of subtasks, and $X_{w,s_i}, Y_{w,j}, Z_{w,n}$ represent the offloading decision. Constraint (10b) states the computing resources of the selected SV cannot be smaller than the required computing resource of subtask w. (10c) guarantees the completion time of the task does not exceeds the maximal tolerant delay. (10d) means that the successor subtasks can only be executed after completing the predecessor subtasks. (10e) ensures that a subtask can only be assigned to one target. (10f) and (10g) defines the range of variables.\nSince the above optimization problem P is nonlinear and has multiple integer variables, it is difficult to find the optimal solution within polynomial time under the dynamic subtasks workload, network environment, and computing resources of SVs. Moreover, the selection of offloading targets also needs to consider the status of each SV and subtasks workload. The above factors make it difficult to find the optimal scheduling strategy using traditional optimization methods for the optimization problem. Therefore, we propose the data-driven DRL algorithm [14] to solve the above problem. The key idea is to model the long-term scheduling process of dependent subtasks in the dynamic environment as an MDP."}, {"title": "III. ALGORITHM DESIGN", "content": "In this section, we design the SDSS algorithm based on synthetic experience replay (SER) [15]-double deep Q network (DDQN) [16], where DDQN is the representative DRL algorithm in discrete decisions, the novel combination aims to adaptively control the discrete scheduling decisions while guaranteeing the efficient strategies exploration by additionally generating high-reward transitions from the diffusion-based SER module. We describe the long-term subtasks scheduling and offloading problem as an MDP and then propose the SDSS algorithm to solve it.\nA. MDP Formulation\nThe optimization problem P is described as an MDP four-tuple {S, A, P, R}, including state S, action A, transition probability P, and reward R. The main components are as follows.\n1) State: The state is denoted as the combination of sub-tasks information, scheduling decisions, and offloading targets information. The state for subtask w is denoted as\n$S = {I_w, \\alpha_w, O_r},$ (11)\nwhere $I_w$ denotes information of subtasks, including workload w and interdependency, i.e., the indicator of predecessor and successor subtask Prew and Sucw. $\\alpha_w$ denotes the scheduling decision of subtasks. $O_r$ is the offloading targets information, including the available computing resources of SVs and BS server $f_s$ and $f_j$, and the distance between TVs and SV.\n2) Action: Each subtask w of the vehicle task m can be computed in local TV n or offloaded to SV s and BS server j. For a whole vehicle task, the scheduling action of all subtasks can be represented as\n$\\alpha_w = {\\alpha_w, B_w},$ (12)\nwhere $\\alpha_w$ is the scheduling priority of subtasks and $B_w = {X_{w,s},Y_{w,j}, Z_{w,n}}$ as an array denotes the different selection of offloading targets.\n3) Reward Function: The reward function is designed as the negative increment of delay after making a scheduling action, it is denoted as\n$R(S_w, \\alpha_w) = -\\Delta \\tau_w,$ (13)\nwhere $\\Delta \\tau_w$ is represented as the difference of completion delay between two adjacent subtasks scheduling decisions, i.e., $\\Delta \\tau_w = \\tau_{w+1} - \\tau_w$. The goal is to find the optimal decision to obtain the maximum cumulative reward.\nB. SDSS Algorithm\n$\\max\\limits_{a}Q(s, a|\\theta) = \\arg\\max\\limits_{a}Q_{estim}(s, a; \\theta),$ (14)\nand then the target Q value network $\\theta'$ uses the action with the maximum Q value to calculate the expected value of the next state $S_{w+1}$, the calculation of the target Q value is denoted as\n$Y_{target} = r + \\gamma Q(s', \\arg\\max_{a'} Q_{estim}(s', a', \\theta), \\theta'),$ (15)\nwhere $\\gamma$ represents the discount factor, which can be used to adjust the long-term reward, and its range is set is $\\gamma \\in$ (0.1, 0.99). Subsequently, it can obtain the loss function by comparing the target Q value, i.e., $Y_{target}$ with the output $Q_{estim}(s, a, \\theta)$ of Q network,\n$L(\\theta) = E_{s,a;s'} {\\frac{1}{2}(r + \\gamma Q(s', \\arg\\max_{a'} Q_{estim}(s', a, \\theta); \\theta') - Q_{estim}(s, a; \\theta))^2},$ (16)\nthe above operation of decoupling action selection and value estimation can effectively avoid the overestimation problem of the target Q value, which is helpful to improve the reliability of the high dimension discrete strategies.\nOn the other hand, the main goal placing the experience replay buffer between the networks and the environment interaction is to store the transitions obtained from the agent's interaction with the environment. These stored transitions (s, a, r, s'), as the exploration experience, can provide training data for these networks, which also makes the training samples of the agent have a certain diversity.\nIn practice, there exist a large number of ineffective transitions in the initial strategy exploration stage. It will inevitably cause the difficulty in convergence by sampling and utilizing these data for agent training. Therefore, to improve the algorithm convergence ability and sample efficiency, we consider using the emerging generative diffusion model [17] to provide real and synthetic transitions for agent training through diffusion-based transitions generation, i.e., the SER module. Two steps of SER module are as follows.\n1) The forward noising: For a original data distribution p(x) with standard deviation $\\sigma$, x represents the real transitions data, considering the noised distribution p(x; $\\sigma$) that obtained by adding independent and identically distributed Gaussian noise of deviation $\\sigma$ to the p(x), i.e., p ($x_t|x_0$; $\\sigma_t$), which will make the final data distribution with unknown noise become the indistinguishable random noise."}, {"title": "IV. PERFORMANCE EVALUATION", "content": "In this section, simulation results are provided to demonstrate the effectiveness of the proposed SDSS algorithm. The traffic simulator Simulation of Urban MObility (SUMO) [19] is used to generate vehicle mobility. The DAG generator [20] is used to simulate task DAG with different dependencies. We consider the simulation experiment of CAV tasks scheduling in a section of one km highway lacking BS coverage, where the single relaying UAV can hover to cover the whole section. The simulation parameters and the algorithm hyperparameter configurations are listed in Table I. Next, we will first compare and analyze the convergence performance of the proposed SDSS algorithm with the original DDQN algorithm [16] in part of validation of analytical results, and compare the algorithms performance with the random subtasks scheduling scheme under diverse workload and computing capacity conditions in performance comparisons.\nA. Validation of Analytical Results\nFigure 4 shows the reward convergence curve of the SDSS algorithm compared with DDQN, where the curve is the mean value with five random seed simulations, and the shadow part represents their positive and negative standard deviation. It is obvious that the proposed SDSS algorithm can achieve rapid convergence and higher reward, and the stability of the algorithm is better than the original DDQN. The main reason is that SDSS can train the agent with generated experience data under the original strategy exploration stage. When the agent begins to learn the discrete decision of subtasks offloading, the generated transitions from the real interaction experiences can help it avoid the inefficient trial and error process, which not only can reduce the train time but also improve the performance of discrete decisions.\nB. Performance Comparisons\nIn this part, the average completion time of the proposed SDSS algorithm is compared with that of the DDQN and random offloading schemes. We consider evaluating algorithm performance under different subtasks workload size and SV computing capacity, i.e., 1) subtasks workload size (Unit: KB): four subtasks and three combinations with low, high, and mix workload size, i.e., W1 (500, 600, 700, 800), W2 (4000, 4300, 4600, 4900), and W3 (800, 2500, 1200, 4500). 2) SVs computing capacity (Unit: GHz): five SVs and three combinations with low, middle, and high available computing resources, i.e., C1 (2, 3, 4, 3, 2), C2 (4, 5, 5, 8, 3), and C3 (6, 7, 5, 8, 8).\nFigure 5 indicates the performance comparison with two schemes under different subtasks workload size and SVs computing capacity. In Fig. 5(a), we compare the average completion time of different algorithms. It can be seen that the proposed SDSS algorithm can achieve a lower task completion time under different computing requirements, which demonstrates the higher robustness of scheduling decisions. The main reason is that SDSS can adaptively adjust the offloading target selection, while determining the subtasks scheduling priority. It can achieve the efficient subtasks offloading by estimating the target states, subtask completion time, and overall task completion time.\nFigure 5(b) illustrates that subtasks with high workload can be relayed to the BS server through UAV, but the data transmission overhead cannot be ignored. The computing resources of SVs need to be effectively utilized to complete all subtasks within the tolerant delay. Regarding combination C1, due to the low computing resources of SVs, SDSS can choose to offload some subtasks with high workload to BS by UAV, which will lead to a higher completion delay. When existing the SV with high computing resources, SDSS can adaptively make subtasks offloading decisions to reduce overall task average completion delay."}, {"title": "V. CONCLUSION", "content": "In this paper, we have studied the task scheduling problem for CAV dependency subtasks offloading in highway scenario. Firstly, the task dependency model and scheduling model have been established to formulate the optimization problem of minimizing the average completion time of overall tasks. Then, the long-term optimization problem is reformulated as an MDP to find the optimal subtask scheduling strategies. Moreover, to achieve the objective, we have designed the SDSS algorithm based on synthetic-DDQN. Finally, we have shown that SDSS can yield faster scheduling decision exploration and lower task completion time in a dynamic environment than other schemes under different scales of subtasks workload size and SVs computing capacity."}]}