{"title": "VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces", "authors": ["Somnath Sendhil Kumar", "Yuvaraj Govindarajulu", "Pavan Kulkarni", "Manojkumar Parmar"], "abstract": "In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of our method under various scenarios, including the availability of top-k prediction probabilities, top-k prediction labels, and top-1 labels.", "sections": [{"title": "1 Introduction", "content": "With the rise in MLaaS (Machine Learning as a Service), which performs tasks from minute levels [34], [2]&[13] to multitasking across domains [40], [10]; There has been a significant increase in model performance, correlating with their size and the ability to accommodate large input spaces. Previous model extraction attacks [51],[35], [44] & [52] have predominantly targeted small datasets such as MNIST and CIFAR, and at the best case scenario have achieved acceptable extraction accuracy on CIFAR-100, which are easily outperformed by current datasets and more robust models. Although there are studies scaling to large real-world models like [6], these are specifically crafted for a target architecture or task, making a generalized approach challenging.\nOn the contrary, some methods employ surrogate datasets [51], [44], [17], [57], [54] to train a substitute models, providing a prior about the target dataset. However, studies finding a balance [17] between surrogate and target datasets are limited in terms of scalability. With the affordable cost of hardware and increased services offering model fine-tuning for user data[19],\nPreprint. Under review."}, {"title": "2 Related Work", "content": "We have outlined the motivation for this work in the introduction; this section will review the seminal literature related to each component or domain critical to our study."}, {"title": "2.1 Model Extraction Attacks", "content": "Previous efforts have attempted to propose algorithms for model extraction in Softlabel settings and compute the approximate gradients for backpropagation of objectives [51], [35], [5], [21]. Works such as [44] extensively evaluate pipelines for Hardlabel settings, establishing a precedent for real-world model extraction. Although these approaches utilize similar frameworks with varied mechanisms for training the Generator, they share a common goal: optimizing the divergence between the Victim and Substitute as a discriminator for the Generator. However, these works are also limited in terms of performance due to the query costs required to approximate gradients for a single sample using the Zeroth Order gradient approximation. Efforts have been made to train an efficient Generator using an Evolutionary Algorithm [41], [4], [42], [26] have demonstrated significantly lower extraction accuracy compared to the methods previously discussed [39]. Miscellaneous works like [53] focus on generating class-specific samples using minimum decision boundaries, which is superior to other approaches based on sample efficiency to train the Substitute model. However, computing these samples requires a high number of Victim Model queries, making it impractical in real-world scenarios due to the extensive querying required. Inferring from previous work, we understand the influence of samples significantly determines the extraction accuracy and efficiency of the approach. We attempt to address this trade-off by developing an auxiliary objective based on SHAP for the generator that is query-efficient and also improves the fidelity of the generated samples, enabling richer extraction of the Victim model."}, {"title": "2.2 Interpretable AI for GAN, Model Extraction", "content": "Research on utilizing Interpretable AI algorithms for training GANs remains sparse [36], largely because while explanations facilitate human interpretation, they are inferior in information density compared to gradients through the target network and discriminator. On the contrary, these methods have the potential for applications involving black box models, particularly in model extraction [55], [52], [37], [35]. While [55] and [37] simply train victim model to have the same explanation as that of the victim model, this does not direct the model to have better extraction accuracy and [35] assumes direct gradients from victim model defeating the purpose. In [52] it employs GradCam [45] to enhance sample augmentation by focusing on saliency maps from the substitute model to refine the loss function. The approach is constrained because GradCam depends on gradients from the model; Hence, using the substitute model for such operations leads to a noisy and unstable training process. Although the authors demonstrate stability within a confined study using predefined images from a surrogate dataset, the scalability of this approach to diverse real-world objectives remains dubious. While We Iterate on this by computing SHAP[46] values, which don't require gradient and can therefore be computed directly on the Victim model within a constrained max_evals budget for each sample. While acquiring SHAP values from the Victim model for max_evals for each sample is costly, we mitigate this by learning to estimate SHAP values within an Energy GAN framework [56], as deriving SHAP values [20] is more feasible than predicting gradients for the victim model."}, {"title": "2.3 Surrogate Dataset and Settings", "content": "In this subsection, we discuss the different settings and the utilization of surrogate datasets in prior research. Numerous studies have employed surrogate datasets [51], [44], [52], [26] each with different assessment on how samples should be selected from the surrogate or proxy dataset. Although these approaches significantly accelerate the extraction process, they require a prior understanding of the data distribution of the victim model, which complicates scalability as [50] summarizes the adverse effect of poor surrogate datasets. With the expansion of MLaaS platforms and the increasing number of classes for tasks, several aggregators [14], [1], [32], [33] now allow entities to deploy their models with the following settings: 1) Top-1 class labels 2) Top-k class labels 3) Top-1 prediction probability 4) Top-k prediction probability. We adopt these settings for our evaluations and also adopt to prediction probability of all classes to ensure comparative analysis with previous research. To offer a framework comparable to other algorithms for utilizing a surrogate dataset, we label the results"}, {"title": "3 Preliminary", "content": "In this section, we briefly introduce SHAP, an Additive explanation utilizing shapely values, particularly focusing on its application in defining objectives. We employ Partition Explainer[46], which recursively computes shapley value through a hierarchy of features; this hierarchy defines feature coalitions and results in the Owen values[29] from game theory. A detailed view on which is given in Appendix.A.1. Adhering to the fundamental principles of any SHAP explainer, we begin with the additive property presented in Eq. 1. In this equation f represents the target black-box model, M the size of input space, E[f(.)] the expected value of f over a uniform random distribution and \u03c6i is the shapley value calculated over for the sample x, expressed as \u03c6(f, x). Here xi represents the ith feature in x. The relationship between x' and x is given by the mapping function x = h(x') as defined in [30, Section 2], with x' \u2208 [0, 1]M standardised for the algorithms.\nFor generalizing to the different scenarios outlined in sec.2.3, we define our black-box victim model using Eq.2. This approach ensures consistent outputs across any top-k prediction setting. Here, topk_probs represents the probability values returned for topk predictions, and topk_indicies are the indices corresponding to these predictions. The output is a column vector of dimension [0, 1]num_classes, depicting a softmax output for a single class prediction scenario, which aligns closely with the intended application within the SHAP framework.\nFor hard labels, we utilize the definition specified in Eq. 3. which assigns a straightforward binary output from the target black box model. We further explore how this method, although it conveys less information compared to the soft label approach, is adequately informative for calculating shapley values.\nWith the help of the definition Eq.1 which is a approximation under the local accuracy property given in [30, Section 3] and with choosing either function from Eq.2 or Eq.3, we derive Eq. 4a. We then represent the variables in the equation as vectors, reformulating \u03c6 = (\u03c61,..., \u03c6i, ...) as a column vector and x' = (x\u00b41, . . ., x\u00b4i, . . . ) as a column vector, regardless of their original shape to obtain Eq. 4b. By applying this framework and forcing on specific class id c, we refine the formula to Eq. 4c."}, {"title": "4 Approach", "content": "The overall attack setup is well outlined by previous works [51], [44], with V the Victim black box model, S a substitute model and A generator G which is responsible for crafting input samples. While our objective is to learn S that closely mimics the V. We employ KL divergence[51] for soft label setting given in Eq.7a, and employ CrossEntropy Loss[44] for hard label setting given in Eq.7b to optimize S. To optimize G, we use an adversarial loss to increase the divergence between Student and victim model[51, 44] which is given by Eq. 8 As we use Conditional Generator instead we also specify cT Target class index to generate samples for a particular class.\nAlongside this setup, we additionally employ the ClassWise Objective defined in Eq.6, while \u03c6 value obtained from the explainer is not differentiable we introduce a estimator P(sx, cT) which estimates the SHAP value of the input given the input sample and the targetted class index. The architecture of P is a conditional UNet such that the shape of the predicted shap values is sample as the original input. The model P predicts a normal distribution of the SHAP value, this format is also helpful to"}, {"title": "5 Experiments", "content": "This section evaluates our Vidmodex approach under diverse and challenging settings, detailed in sec. 2.3, using both image and video models across various datasets such as MNIST[12], CIFAR10, CIFAR100[24], Caltech101[25], Caltech256[16] and ImageNet1K[11] for images, and UCF11[27], UCF101[49], Kinetics 400[22], Kinetics 600[7] and Something-Something v2[15] for videos. These tests evaluate over increasing number of classes and complexities; we ensure the evaluation of image model extraction on high-resolution datasets to demonstrate efficiency in large search space. We compare the benchmark primarily across DFME[51], DFMS-HL[44] which we reproduce with our best efforts. We additional utilizes results reported from ZSDB3KD[53], MAZE[21], KnockoffNets[41] and BlackBox Dissector[52]. We chose not to replicate results from other studies since our selected methods have already outperformed them in prior works. Further, we assess the impact of max_evals on the extraction process and learning within the discriminator in Sec.5.2.1, identifying what we consider the best configuration. We conduct an ablation study to explore performance variations across different top_k settings. Our research primarily focuses on black box model extraction, but we also examine the implications of employing a surrogate dataset (grey box access), discussing"}, {"title": "5.1 Results", "content": "We present our results for blackbox extraction results in Sec.5.1.1, While further we analyze the influence of top-k on softlabel and hardlabel setting in Sec.5.1.2. We additionally share the results of Greybox extraction in Sec.5.1.3."}, {"title": "5.1.1 BlackBox Extraction", "content": "For the blackbox extraction, our initial investigation centers on SoftLabel Setting with probabilities of all classes from the victim model, in line with previous studies like [53], [21], [41]. As illustrated in Table. 1, we present the accuracies for these methods as reported in their work and reproduced numbers from [44] and [51] alongside our work. To ensure reproducible comparative study, we provide the training epochs required to replicate the victim models, as prior studies often do not offer standardized or pre-trained weights. We train the Target victim architecture from a random initialized state on the target dataset with all configuration details including seeds are available in our code repository. We employ the same architecture for both the clone and the victim model, thereby eliminating any potential bias that might arise from architectural differences.\nWe also detail the Query Budget, for reported work we either present the reported value or calculate based on the algorithms described, particulary for [53]. Our approach demonstrates higher extraction across most tested configurations except in MNIST, where it performs comparably to [44] and slightly behind [53]. Notably, our method is 25\u00d7 efficient than [53] based on Query Budget. We employ uniform Query Budget across the methods we reproduce, where we outperform [51] and [44] with equivalent budgets. We see a pattern of reducing extraction accuracy with increase in difficulty in the dataset which is correlated to the increase in resolution and increase in number of classes. The increase in difficulty can also be seen with reducing victim accuracy for a higher training epochs. Our approach outperforms [51] on a average of 16.45%, with a maximum improvement of 35.31%. Comparatively, Vidmodex and DFMS-SL Shows a mean improvement of 11.67% and a maximum"}, {"title": "5.1.2 Impact of TopK Setting on Soft and Hardlabel extraction.", "content": "We explore the scenario where top-k labels are available for model extraction, illustrating the real-world applicability of our pipeline. This analysis adheres to the definition provided in Eq.2 for softlabel and Eq.3 for hardlabel, ensuring consistency across all scenarios. Our approach does not incorporate any specific methodology for handling top-k labels beyond these definitions. This analysis aims to demonstrate that the computation of the SHAP values and the introduction of the New Shap-based objective do not negatively impact performance when fewer labels are returned. As shown in Fig.6, we plot the mean clone accuracy for each value of K, with regions defined by the standard deviations of these points for each K. For Softlabel extraction, we report results for image and video models across K \u2208 1,3,5, 10, ALL, While for Hardlabel extraction, we report K \u2208 1, 3, 5, 10; the 'All' category is omitted as it would equate to no information for hardlabel; hence the focus is limited to these K values. Notably, for datasets like MNIST, CIFAR10 and UCF11, we do not present numbers for K=10 in hardlabel as the total number of classes is 10 or 11, making hardlabels redundant in such scenarios. From Fig.6a for image models, there is an observed upward trend in"}, {"title": "5.1.3 Grey Box extraction", "content": "We also evaluate the efficacy of our approach using a surrogate dataset. Although enhancing grey box accuracy is not our primary focus, these experiments serve to confirm that our SHAP-based objective does not adversely affect the generator's learning process when paired with a proxy or surrogate dataset. Instead of delving into the methodology for selecting an appropriate surrogate dataset, we utilize portions of established datasets. Specifically, we incorporate ImageNet-22KK[43] for image models and Kinetics-700[8] and CHARADES[47] for video model extractions. These datasets are shuffled and employed without targeting any specific subclasses.\nThe experimental details and configurations are outlined in Table. 7 for image models and Table.8 for video models, with results visualized in Fig. 7. Our analysis includes only three methods: [51], [44], and our own approach, examining both SoftLabel and HardLabel settings. As expected, extraction accuracy declines with increased difficulty, utilizing all labels for SoftLabel settings and only top-1 labels for HardLabel settings.\nDespite this complexity, our method demonstrates notable robustness and effectiveness, particularly in the SoftLabel and image model contexts, where it shows a mean improvement of approximately 15.23% over DFME and 9.24% over DFMS-SL, with peaks of 32.99% and 21.98%, respectively. For the HardLabel setting in image models, our approach yields an average improvement of 15.15% over DFME and 9.24% over DFMS-HL, reaching up to 29.14% and 14.16%, respectively. In video model extractions under SoftLabel settings, we observe average enhancements of 19.04% over DFME and 12.65% over DFMS-SL, with maximum gains of 28.29% and 18.05%, respectively. The HardLabel setting shows our method outperforming DFME by 15.34% and DFMS-HL by 9.80% on average, with maximum improvements of 24.26% and 19.67%, respectively. These findings underscore the robustness and efficacy of our extraction pipeline."}, {"title": "5.2 Ablation study", "content": ""}, {"title": "5.2.1 Discriminator Learning", "content": "In this section, we examine the impact of the max_eval parameter on SHAP value computations, crucial for training the discriminator P with the objective defined in Eq. 10. A higher max_eval value results in more fine-grained SHAP values for each feature, thereby improving local accuracy as outlined in Eq.1 and discussed in [30]. To enhance the approximation quality of P, we initially set max_eval to a high value. However, a high max_eval also increases the number of victim model queries per sample. To manage this, we progressively reduce max_eval throughout the training process, similar to learning rate decay techniques [48]. This reduction strategy halves max_eval progressively until reaching a minimal threshold, beyond which the discriminator is no longer trained."}, {"title": "6 Conclusion", "content": "In this study, we aimed to enhance the DataFree model extraction framework by integrating Explainable AI algorithms as an auxiliary objective alongside existing methodologies. We rigorously tested our approach in real-world scenarios, encompassing both hard and soft label settings across various top-k outputs, aligning with the constraints typical of contemporary MLaaS offerings. Our research extends the scope of model extraction attacks to video classification models, where we observed significant improvements over previous methods. Both quantitative and qualitative analyses were conducted to assess the impact of SHAP values, underscoring notable enhancements in model extraction. We also detailed the implementation of our pipeline and explored the influence of additional hyperparameters to facilitate reproducibility and further development. Although our approach is broadly applicable to any target model task including audio, text, and tabular data-this paper focuses on a constrained study to substantiate our claims. Future work could explore the development of generalized extraction techniques for even larger models with billions of parameters, aiming to achieve this at a reasonable cost. While our work primarily details the attack on such models, our overarching goal is to enrich the community's awareness of the substantial MLaaS industry. We believe it is of utmost importance to understand the potential risks involved."}, {"title": "A Appendix / supplemental material", "content": ""}, {"title": "A.1 Derivation of Hierarchical Shapley Values from Owen Values", "content": "In this section we focus on the Derivation of the Hierarchical Shapley Values, This is implemented as the Partition Explainer. And is defined using the principals of the Owen Value decomposition [29]. To do so we start with the following Definition 1 and the Theorem 1 from [30]."}, {"title": "B Additional Comparative Results", "content": ""}, {"title": "B.1 Soft Label", "content": "For a more detailed analysis, we present experimental results on the variance of top-k values for SoftLabel Black Box extraction. The results are summarized in Figures 6a and 6b, with further details provided in Section. 5.1.2. We have comprehensively analyzed a sufficient range of top-k values, specifically 1, 3, 5, 10, and All. These results are detailed in Table. 3 for image models, which includes information about the target victim model, the clone models, victim and extraction accuracy, and the query budgets for each experiment. The dataset on which the victim model was trained, including the number of training epochs, is also listed. Similarly, results for the video model are presented in Table. 4, including all the aforementioned metrics."}, {"title": "B.2 HardLabel", "content": "Building on the previous analysis, this section presents a detailed exploration of the hard label scenario, including top-k labels for 1, 3, 5, and 10. Unlike in the soft label scenario, the category 'All' is excluded as it offers no information about the predicted class. This section also details additional metrics such as the architecture of the target victim and clone models, the target dataset, and the number of epochs over which the target is trained. Additionally, the query budget for each experiment and the extraction accuracy are documented. It should be noted that for target datasets comprising only 10 or 11 classes, the top 10 labels effectively represent all classes, rendering this category redundant. The results for the image models are detailed in Table 5, and those for the video model are shown in Table 6.\nContrary to the trend observed in the top-k settings for soft labels, there is a decrease in extraction accuracy for hard labels, which can be attributed to a significant increase in the entropy of these model outputs as defined by Eq. 3, where probability is equally distributed. This substantial reduction in information from the victim model adversely impacts extraction accuracy, a phenomenon consistent not only with our approach but also observed in studies by DFME [51] and DFMS-HL [44]. Additionally, the positive correlation between extraction accuracy and victim model accuracy is evident in the hard label setting, for reasons discussed earlier."}, {"title": "B.3 Grey Box extraction", "content": ""}, {"title": "C Qualitative Analysize", "content": ""}, {"title": "C.1 Activation Atlases to visualize efficacy of objectives", "content": "The Activation Atlas [9] serves to visualize samples [38], optimizing them to align with specific activations using Eq. 20. This alignment corresponds to a UMAP [31] based projection of the activation space. For a given sample x, activation maximization targets a specific objective J(\u03b8f, x,T), achieved through the optimization process outlined in Eq. 19. Here, f represents the target model, \u03b8f its trained parameters, and T encompasses random transformations (e.g., Flip, Rotate, RandomCrop) applied to x to enhance generalization and prevent exploitation of local features.\nTo demonstrate the quality of information provided by the loss function, we optimize the sample to minimize the loss and visualize the samples in an Activation Atlas, modifying our objective to the negative of the losses in each method. Eq. 21 and Eq. 22 detail the objectives employed for the target class c. We utilize a target model trained on ImageNet1K for further analysis. Unlike black box model extraction, where we have only black box access to the model with no data, here we utilize the parameters and compute gradients while having access to the datasets and their respective activations. To test, we replace the generator in our pipeline with the activation maximization algorithm to generate samples that represent the optimal input space that can be learned with respective loss functions. Complete implementation details are provided in the codebase."}, {"title": "C.2 Visualization of Training Samples", "content": "In this section, we showcase samples learned from the DFME approach [51] and our method across various iteration steps. Our objective is to explore the relationship between SHAP values and image quality within the resulting pipeline. Figure 9 displays samples generated by different algorithms at various timestamps, accompanied by SHAP plots for the top four classes. We have selected CIFAR10 for this analysis to facilitate clearer differentiation among the top four classes."}]}