{"title": "VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces", "authors": ["Somnath Sendhil Kumar", "Yuvaraj Govindarajulu", "Pavan Kulkarni", "Manojkumar Parmar"], "abstract": "In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of our method under various scenarios, including the availability of top-k prediction probabilities, top-k prediction labels, and top-1 labels.", "sections": [{"title": "1 Introduction", "content": "With the rise in MLaaS (Machine Learning as a Service), which performs tasks from minute levels [34], [2]&[13] to multitasking across domains [40], [10]; There has been a significant increase in model performance, correlating with their size and the ability to accommodate large input spaces. Previous model extraction attacks [51],[35], [44] & [52] have predominantly targeted small datasets such as MNIST and CIFAR, and at the best case scenario have achieved acceptable extraction accuracy on CIFAR-100, which are easily outperformed by current datasets and more robust models. Although there are studies scaling to large real-world models like [6], these are specifically crafted for a target architecture or task, making a generalized approach challenging.\nOn the contrary, some methods employ surrogate datasets [51], [44], [17], [57], [54] to train a substitute models, providing a prior about the target dataset. However, studies finding a balance [17] between surrogate and target datasets are limited in terms of scalability. With the affordable cost of hardware and increased services offering model fine-tuning for user data[19],"}, {"title": null, "content": "f(x) = E[f(.)] + \\Sigma_{i=1}^{M} \\phi_i * x_i"}, {"title": "2 Related Work", "content": "We have outlined the motivation for this work in the introduction; this section will review the seminal literature related to each component or domain critical to our study."}, {"title": "2.1 Model Extraction Attacks", "content": "Previous efforts have attempted to propose algorithms for model extraction in Softlabel settings and compute the approximate gradients for backpropagation of objectives [51], [35], [5], [21]. Works such as [44] extensively evaluate pipelines for Hardlabel settings, establishing a precedent for real-world model extraction. Although these approaches utilize similar frameworks with varied mechanisms for training the Generator, they share a common goal: optimizing the divergence between the Victim and Substitute as a discriminator for the Generator. However, these works are also limited in terms of performance due to the query costs required to approximate gradients for a single sample using the Zeroth Order gradient approximation. Efforts have been made to train an efficient Generator using an Evolutionary Algorithm [41], [4], [42], [26] have demonstrated significantly lower extraction accuracy compared to the methods previously discussed [39]. Miscellaneous works like [53] focus on generating class-specific samples using minimum decision boundaries, which is superior to other approaches based on sample efficiency to train the Substitute model. However, computing these samples requires a high number of Victim Model queries, making it impractical in real-world scenarios due to the extensive querying required. Inferring from previous work, we understand the influence of samples significantly determines the extraction accuracy and efficiency of the approach. We attempt to address this trade-off by developing an auxiliary objective based on SHAP for the generator that is query-efficient and also improves the fidelity of the generated samples, enabling richer extraction of the Victim model."}, {"title": "2.2 Interpretable AI for GAN, Model Extraction", "content": "Research on utilizing Interpretable AI algorithms for training GANs remains sparse [36], largely because while explanations facilitate human interpretation, they are inferior in information density compared to gradients through the target network and discriminator. On the contrary, these methods have the potential for applications involving black box models, particularly in model extraction [55], [52], [37], [35]. While [55] and [37] simply train victim model to have the same explanation as that of the victim model, this does not direct the model to have better extraction accuracy and [35] assumes direct gradients from victim model defeating the purpose. In [52] it employs GradCam [45] to enhance sample augmentation by focusing on saliency maps from the substitute model to refine the loss function. The approach is constrained because GradCam depends on gradients from the model; Hence, using the substitute model for such operations leads to a noisy and unstable training process. Although the authors demonstrate stability within a confined study using predefined images from a surrogate dataset, the scalability of this approach to diverse real-world objectives remains dubious. While We Iterate on this by computing SHAP[46] values, which don't require gradient and can therefore be computed directly on the Victim model within a constrained max_evals budget for each sample. While acquiring SHAP values from the Victim model for max_evals for each sample is costly, we mitigate this by learning to estimate SHAP values within an Energy GAN framework [56], as deriving SHAP values [20] is more feasible than predicting gradients for the victim model."}, {"title": "2.3 Surrogate Dataset and Settings", "content": "In this subsection, we discuss the different settings and the utilization of surrogate datasets in prior research. Numerous studies have employed surrogate datasets [51], [44], [52], [26] each with different assessment on how samples should be selected from the surrogate or proxy dataset. Although these approaches significantly accelerate the extraction process, they require a prior understanding of the data distribution of the victim model, which complicates scalability as [50] summarizes the adverse effect of poor surrogate datasets. With the expansion of MLaaS platforms and the increasing number of classes for tasks, several aggregators [14], [1], [32], [33] now allow entities to deploy their models with the following settings: 1) Top-1 class labels 2) Top-k class labels 3) Top-1 prediction probability 4) Top-k prediction probability. We adopt these settings for our evaluations and also adopt to prediction probability of all classes to ensure comparative analysis with previous research. To offer a framework comparable to other algorithms for utilizing a surrogate dataset, we label the results"}, {"title": "3 Preliminary", "content": "In this section, we briefly introduce SHAP, an Additive explanation utilizing shapely values, particularly focusing on its application in defining objectives. We employ Partition Explainer[46], which recursively computes shapley value through a hierarchy of features; this hierarchy defines feature coalitions and results in the Owen values[29] from game theory. A detailed view on which is given in Appendix.A.1. Adhering to the fundamental principles of any SHAP explainer, we begin with the additive property presented in Eq. 1. In this equation f represents the target black-box model, M the size of input space, $E[f(.)]$ the expected value of f over a uniform random distribution and $\\phi$ is the shapley value calculated over for the sample x, expressed as $\\phi(f, x)$. Here $x_i$ represents the ith feature in x. The relationship between x' and x is given by the mapping function $x = h(x')$ as defined in [30, Section 2], with $x' \\in [0, 1]^M$ standardised for the algorithms.\nFor generalizing to the different scenarios outlined in sec.2.3, we define our black-box victim model using Eq.2. This approach ensures consistent outputs across any top-k prediction setting. Here, topk_probs represents the probability values returned for topk predictions, and topk_indicies are the indices corresponding to these predictions. The output is a column vector of dimension [0, 1]num_classes, depicting a softmax output for a single class prediction scenario, which aligns closely with the intended application within the SHAP framework."}, {"title": null, "content": "topk_probs[i]\nfst =\n1 - sum(topk_probs)\nnum_classes \u2013 k\nif i \u2208 topk_indices,\notherwise."}, {"title": null, "content": "\\begin{cases}\n1/k & \\text{if } i \\in \\text{ topk_labels,}\\\\\n0 & \\text{otherwise}\n\\end{cases}"}, {"title": null, "content": "f(x) = E[f(.)] + \\Sigma_{i=0}^{M} \\phi_i * x_i"}, {"title": null, "content": "f(x) = E[f(.)] + \\phi(f,x)^T * x'"}, {"title": null, "content": "f(x|c) = E[f(.|c)] + \\phi(f(.|c), x)^T * x'"}, {"title": null, "content": "\\begin{aligned}\n&\\underset{x}{\\text{argmax}} f(x|c) \\\\\n&= \\underset{x}{\\text{argmax}} E[f(.|c)] + \\phi(f(.|c), x)\\\\\n&= \\underset{x}{\\text{argmax}}\\ \\phi(f(.|c), x)^T*x'\n\\end{aligned}"}, {"title": "4 Approach", "content": "The overall attack setup is well outlined by previous works [51], [44], with V the Victim black box model, S a substitute model and A generator G which is responsible for crafting input samples. While our objective is to learn S that closely mimics the V. We employ KL divergence[51] for soft label setting given in Eq.7a, and employ CrossEntropy Loss[44] for hard label setting given in Eq.7b to optimize S. To optimize G, we use an adversarial loss to increase the divergence between Student and victim model[51, 44] which is given by Eq. 8 As we use Conditional Generator instead we also specify $c_T$ Target class index to generate samples for a particular class."}, {"title": null, "content": "\\mathcal{L}_{sl}(x) = \\sum_{i \\in topk\\_indices} V(x_i) \\log \\frac{V(x_i)}{S(x_i)}\\qquad (7a)"}, {"title": null, "content": "\\mathcal{L}_{hi}(x) = \\sum_{i \\in topk\\_indices} V(x_i) *log(S(x_i))\\qquad (7b)"}, {"title": null, "content": "z \\sim N(0,1)\\qquad (8)\nx = G(z, c_T)\n\\Rightarrow \\underset{\\theta_G}{argmax} \\underset{\\theta_s}{argmin} \\mathcal{L}(x)"}, {"title": null, "content": "\\begin{aligned}\n&\\underset{x}{\\text{ClassObj}} = \\underset{x}{\\text{argmax}}\\ \\sum E[P(s|x, c)]\\\\ &= \\underset{x}{\\text{argmax}}\\ \\sum E[P(s|x, c)] \\odot P(Sgt|x, c)\n\\end{aligned}"}, {"title": null, "content": "L_p = \\sum S_{gt} - s, \\text{where } s \\sim P(x,c)\\qquad (10)"}, {"title": "5 Experiments", "content": "This section evaluates our Vidmodex approach under diverse and challenging settings, detailed in sec. 2.3, using both image and video models across various datasets such as MNIST[12], CIFAR10, CIFAR100[24], Caltech101[25], Caltech256[16] and ImageNet1K[11] for images, and UCF11[27], UCF101[49], Kinetics 400[22], Kinetics 600[7] and Something-Something v2[15] for videos. These tests evaluate over increasing number of classes and complexities; we ensure the evaluation of image model extraction on high-resolution datasets to demonstrate efficiency in large search space. We compare the benchmark primarily across DFME[51], DFMS-HL[44] which we reproduce with our best efforts. We additional utilizes results reported from ZSDB3KD[53], MAZE[21], KnockoffNets[41] and BlackBox Dissector[52]. We chose not to replicate results from other studies since our selected methods have already outperformed them in prior works. Further, we assess the impact of max_evals on the extraction process and learning within the discriminator in Sec.5.2.1, identifying what we consider the best configuration. We conduct an ablation study to explore performance variations across different top_k settings. Our research primarily focuses on black box model extraction, but we also examine the implications of employing a surrogate dataset (grey box access), discussing"}, {"title": "5.1 Results", "content": "We present our results for blackbox extraction results in Sec.5.1.1, While further we analyze the influence of top-k on softlabel and hardlabel setting in Sec.5.1.2. We additionally share the results of Greybox extraction in Sec.5.1.3."}, {"title": "5.1.1 BlackBox Extraction", "content": "For the blackbox extraction, our initial investigation centers on SoftLabel Setting with probabilities of all classes from the victim model, in line with previous studies like [53], [21], [41]. As illustrated in Table. 1, we present the accuracies for these methods as reported in their work and reproduced numbers from [44] and [51] alongside our work. To ensure reproducible comparative study, we provide the training epochs required to replicate the victim models, as prior studies often do not offer standardized or pre-trained weights. We train the Target victim architecture from a random initialized state on the target dataset with all configuration details including seeds are available in our code repository. We employ the same architecture for both the clone and the victim model, thereby eliminating any potential bias that might arise from architectural differences."}, {"title": "5.1.2 Impact of TopK Setting on Soft and Hardlabel extraction.", "content": "We explore the scenario where top-k labels are available for model extraction, illustrating the real-world applicability of our pipeline. This analysis adheres to the definition provided in Eq.2 for softlabel and Eq.3 for hardlabel, ensuring consistency across all scenarios. Our approach does not incorporate any specific methodology for handling top-k labels beyond these definitions. This analysis aims to demonstrate that the computation of the SHAP values and the introduction of the New Shap-based objective do not negatively impact performance when fewer labels are returned. As shown in Fig.6, we plot the mean clone accuracy for each value of K, with regions defined by the standard deviations of these points for each K. For Softlabel extraction, we report results for image and video models across $K \\in$ {1,3,5, 10, ALL}, While for Hardlabel extraction, we report $K \\in$ {1, 3, 5, 10}; the 'All' category is omitted as it would equate to no information for hardlabel; hence the focus is limited to these K values. Notably, for datasets like MNIST, CIFAR10 and UCF11, we do not present numbers for K=10 in hardlabel as the total number of classes is 10 or 11, making hardlabels redundant in such scenarios. From Fig.6a for image models, there is an observed upward trend in"}, {"title": "5.1.3 Grey Box extraction", "content": "We also evaluate the efficacy of our approach using a surrogate dataset. Although enhancing grey box accuracy is not our primary focus, these experiments serve to confirm that our SHAP-based objective does not adversely affect the generator's learning process when paired with a proxy or surrogate dataset. Instead of delving into the methodology for selecting an appropriate surrogate dataset, we utilize portions of established datasets. Specifically, we incorporate ImageNet-22KK[43] for image models and Kinetics-700[8] and CHARADES[47] for video model extractions. These datasets are shuffled and employed without targeting any specific subclasses.\nThe experimental details and configurations are outlined in Table. 7 for image models and Table.8 for video models, with results visualized in Fig. 7. Our analysis includes only three methods: [51], [44], and our own approach, examining both SoftLabel and HardLabel settings. As expected, extraction accuracy declines with increased difficulty, utilizing all labels for SoftLabel settings and only top-1 labels for HardLabel settings.\nDespite this complexity, our method demonstrates notable robustness and effectiveness, particularly in the SoftLabel and image model contexts, where it shows a mean improvement of approximately 15.23% over DFME and 9.24% over DFMS-SL, with peaks of 32.99% and 21.98%, respectively. For the HardLabel setting in image models, our approach yields an average improvement of 15.15% over DFME and 9.24% over DFMS-HL, reaching up to 29.14% and 14.16%, respectively. In video model extractions under SoftLabel settings, we observe average enhancements of 19.04% over DFME and 12.65% over DFMS-SL, with maximum gains of 28.29% and 18.05%, respectively. The HardLabel setting shows our method outperforming DFME by 15.34% and DFMS-HL by 9.80% on average, with maximum improvements of 24.26% and 19.67%, respectively. These findings underscore the robustness and efficacy of our extraction pipeline."}, {"title": "5.2 Ablation study", "content": ""}, {"title": "5.2.1 Discriminator Learning", "content": "In this section, we examine the impact of the max_eval parameter on SHAP value computations, crucial for training the discriminator P with the objective defined in Eq. 10. A higher max_eval value results in more fine-grained SHAP values for each feature, thereby improving local accuracy as outlined in Eq.1 and discussed in [30]. To enhance the approximation quality of P, we initially set max_eval to a high value. However, a high max_eval also increases the number of victim model queries per sample. To manage this, we progressively reduce max_eval throughout the training process, similar to learning rate decay techniques [48]. This reduction strategy halves max_eval progressively until reaching a minimal threshold, beyond which the discriminator is no longer trained."}, {"title": "6 Conclusion", "content": "In this study, we aimed to enhance the DataFree model extraction framework by integrating Explainable AI algorithms as an auxiliary objective alongside existing methodologies. We rigorously tested our approach in real-world scenarios, encompassing both hard and soft label settings across various top-k outputs, aligning with the constraints typical of contemporary MLaaS offerings. Our research extends the scope of model extraction attacks to video classification models, where we observed significant improvements over previous methods. Both quantitative and qualitative analyses were conducted to assess the impact of SHAP values, underscoring notable enhancements in model extraction. We also detailed the implementation of our pipeline and explored the influence of additional hyperparameters to facilitate reproducibility and further development. Although our approach is broadly applicable to any target model task including audio, text, and tabular data-this paper focuses on a constrained study to substantiate our claims. Future work could explore the development of generalized extraction techniques for even larger models with billions of parameters, aiming to achieve this at a reasonable cost. While our work primarily details the attack on such models, our overarching goal is to enrich the community's awareness of the substantial MLaaS industry. We believe it is of utmost importance to understand the potential risks involved."}, {"title": "A Appendix / supplemental material", "content": ""}, {"title": "A.1 Derivation of Hierarchical Shapley Values from Owen Values", "content": "In this section we focus on the Derivation of the Hierarchical Shapley Values, This is implemented as the Partition Explainer. And is defined using the principals of the Owen Value decomposition [29]. To do so we start with the following Definition 1 and the Theorem 1 from [30]."}, {"title": null, "content": "Definition 1 (Additive feature attribution methods) have an explanation model that is a linear function of binary variables. where z' \u2208 {0,1}M, M is the number of simplified input features, and \u03a6\u03af \u0395 R"}, {"title": null, "content": "f(x) = g(x') = \\phi_o +  \\sum_i \\phi_i x'_i \\qquad (11)"}, {"title": null, "content": "Property 3 (Consistency) Let fx(z') = f(hx(z')) and z' \\ i denote setting z' 0. For any two models f and f', if\nf(z') \u2212 f(z' \\ i) \u2265 fx(z') \u2013 fx(z' \\ i)\nfor all inputus z' \u2208 {0,1}M, then \u03c6i(f', x) \u2265 \u03c6i(f, x). \\qquad (12)"}, {"title": null, "content": "\\phi_i(f, x) = \\sum_{z'\\subset x'}  \\frac{|z'|!(M - |z'|-1)!}{M!} [f_x(z') \u2013 f_x(z' \\setminus i)] \\qquad (13)"}, {"title": null, "content": "Property 4 (Group Homogeneity) Let Gk \u2264 M and i, j\u2208 Gk are in the same group.\nE[|\u03a6i(f, x) \u2013 \u03a6j(f,x)|] \u2264 \u03b5(|Gk|) \\qquad (14)\nwhere \u025b(|Gk|) is a small positive constant that decreases as |Gk| decreases."}, {"title": null, "content": "\\phi_i(f,x) \u2248  \\frac{1}{|G_k|} \\sum_t \\phi_t(f,x) \\qquad (15)"}, {"title": null, "content": "(f) =  \\sum_{t:G_k\\subset T \\subset M\\setminus{i}}  \\frac{|T \\cap G_k|! (|G_k|-|T \\cap G_k|-1)!}{|G_k|!}   \\frac{|T|!(|M|-|T|-1)!}{|M|!} \u00b7 [f(T\u222a{i}) \u2212 f(T)]  \\qquad (16)"}, {"title": null, "content": "\\phi_i(f,x) = \\sum_{T_k:G_k\\subset T \\subset M\\setminus{i}} \\sum  \\frac{|T_k \\cap G_k|! (|G_k|-|T_k\\cap G_k|-1)!}{|G_k!|}   \\frac{|T|!(|M|-|T|-1)!}{|M|!}   \u00b7 [f(T\u222a {i}) \u2212 f(T)]  \\qquad (17)"}, {"title": null, "content": "E[]\\phi_i(f, x) \u2013 \\phi_j(f,x)|] \u2264 \u03b5(|G_k|)"}, {"title": null, "content": "\\phi_i(f, x) \u2248   \\frac{1}{|G_k|} \\sum_{i\\in G_k} \\phi_i(f,x)"}, {"title": null, "content": "(\\phi_t(f, x) =   \\frac{1}{|G_k|} \\sum_{i\\in G_k} \\phi_t(f) \\qquad (18)"}, {"title": "B Additional Comparative Results", "content": ""}, {"title": "B.1 Soft Label", "content": "For a more detailed analysis, we present experimental results on the variance of top-k values for SoftLabel Black Box extraction. The results are summarized in Figures 6a and 6b, with further details provided in Section. 5.1.2. We have comprehensively analyzed a sufficient range of top-k values, specifically 1, 3, 5, 10, and All. These results are detailed in Table. 3 for image models, which includes information about the target victim model, the clone models, victim and extraction accuracy, and the query budgets for each experiment. The dataset on which the victim model was trained, including the number of training epochs, is also listed. Similarly, results for the video model are presented in Table. 4, including all the aforementioned metrics."}, {"title": "B.2 HardLabel", "content": "Building on the previous analysis, this section presents a detailed exploration of the hard label scenario, including top-k labels for 1, 3, 5, and 10. Unlike in the soft label scenario, the category 'All' is excluded as it offers no information about the predicted class. This section also details additional metrics such as the architecture of the target victim and clone models, the target dataset, and the number of epochs over which the target is trained. Additionally, the query budget for each experiment and the extraction accuracy are documented. It should be noted that for target datasets comprising only 10 or 11 classes, the top 10 labels effectively represent all classes, rendering this category redundant. The results for the image models are detailed in Table 5, and those for the video model are shown in Table 6."}, {"title": "B.3 Grey Box extraction", "content": "Expanding our analysis, we assess the our framework through grey-box model extraction. Although not our primary focus, this experiment is crucial to demonstrate that the SHAP-based objective does not introduce adverse effects during training. This is particularly important as the generator in the black box setting only employs the divergence between the teacher and student models along with the SHAP objective. But in the grey-box scenario, the generator is trained using GAN-based methods with a subset of the target dataset to capture the distribution of samples. Introducing an additional SHAP-based optimization could potentially cause a significant shift in the distribution of these synthetic samples or the generator might encounter a saddle point in this process, impacting the generation of optimal samples for extraction.\nIn our grey box extraction analysis, we concentrate on top-all predictions for the softlabel setting and top-1 labels for the hardlabel setting, as these configurations are commonly used in most grey box approaches and have demonstrated superior performance in our prior assessments. We maintain consistency by presenting the same metrics as in previous experiments, with additional details about the surrogate dataset and the proportion of the subset used to train the Generator. Furthermore, we document the number of generator iterations over this dataset to enhance the value of future comparative studies. The results for Image models are detailed in Table 7, and for Video models in Table 8.\nAll previously observed trends are maintained in the grey box extraction, exhibiting no significant performance degradation when compared to other algorithms tested, including those documented in [51] and [44]. For image models, we also include performance metrics from [52]. Our algorithm consistently matches or exceeds the performance of these referenced experiments in both settings."}, {"title": "C Qualitative Analysize", "content": ""}, {"title": "C.1 Activation Atlases to visualize efficacy of objectives", "content": "The Activation Atlas [9] serves to visualize samples [38], optimizing them to align with specific activations using Eq. 20. This alignment corresponds to a UMAP [31] based projection of the activation space. For a given sample x, activation maximization targets a specific objective $J(\\theta_f, x,T)$, achieved through the optimization process outlined in Eq. 19. Here, f represents the target model, $\\theta_f$ its trained parameters, and T encompasses random transformations (e.g., Flip, Rotate, RandomCrop) applied to x to enhance generalization and prevent exploitation of local features."}, {"title": null, "content": "(\\frac{(h_{x,y})n+1}{\n(||h_{x,y}||\u00b7||v||)n'}\\qquad where: \\qquad (20)"}, {"title": null, "content": "x* = argmax J(\u03b8f,x,T)  \\qquad (19)"}, {"title": null, "content": "IDME(\u03b8\u03bd,\u03b8\u03c2,c,x,T) = V(T(x)\\c) + S(T(x)\\c) + \u03a3V(T(x)\\c) log  \\frac{V(T(x)|c)}{S(T(x)\\c)} \\qquad (21)"}, {"title": null, "content": "TSHAP(\u03b8\u03bd,\u03b8\u03c2,0p,c,x,T) = IDFME(0v,0s,c,x, T) + E[P(s|T(x), c)]\\qquad (22)"}, {"title": "C.2 Visualization of Training Samples", "content": "In this section, we showcase samples learned from the DFME approach [51] and our method across various iteration steps. Our objective is to explore the relationship between SHAP values and image quality within the resulting pipeline. Figure 9 displays samples generated by different algorithms at various timestamps, accompanied by SHAP plots for the top four classes. We have selected CIFAR10 for this analysis to facilitate clearer differentiation among the top four classes."}, {"title": null, "content": "We present samples for DFME at steps [120, 4800, 12000, 60000, 180000, and 243120]. For our pipeline, we provide three samples from step 2500. Unlike DFME, our generator is conditional, producing all three samples specifically for the cat class. Although SHAP or SHAPley values are not primarily used for learning or improving generator performance, they play a critical role in analyzing sample quality and understanding feature importance within an input. As evident from Figure 9, samples from the DFME approach display very low SHAP values and lack distinctive features correlating to any particular class. In contrast, samples from Vidmodex, even at intermediate training stages, show many features that are not only optimized for the target class but also exhibit features that negatively correlate with other classes. We restrict our qualitative analysis since all generated samples are non-sensical and cannot be meaningfully interpreted. Therefore, using SHAP values as a sole metric to evaluate these samples is not recommended."}]}