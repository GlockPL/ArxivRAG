{"title": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation", "authors": ["Haichuan Hu", "Yuhan Sun", "Quanjun Zhang"], "abstract": "Retrieval-Augmented Generation (RAG) has become a primary technique for mitigating hallucinations in large language models (LLMs). However, incomplete knowledge extraction and insufficient understanding can still mislead LLMs to produce irrelevant or even contradictory responses, which means hallucinations persist in RAG. In this paper, we propose LRP4RAG, a method based on the Layer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations in RAG. Specifically, we first utilize LRP to compute the relevance between the input and output of the RAG generator. We then apply further extraction and resampling to the relevance matrix. The processed relevance data are input into multiple classifiers to determine whether the output contains hallucinations. To the best of our knowledge, this is the first time that LRP has been used for detecting RAG hallucinations, and extensive experiments demonstrate that LRP4RAG outperforms existing baselines.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP). However, they often generate plausible yet incorrect answers, a phenomenon known as \"hallucination\". Hallucinations have been observed across various generation tasks (Xu et al., 2024; Guerreiro et al., 2023; Wang et al., 2023b; Ji et al., 2023; Adlakha et al., 2024) in NLP. The issue of hallucination makes LLMs untrustworthy and presents risks when deploying LLMs in practical scenarios. To mitigate LLM hallucination, Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) has been proposed. RAG typically consists of a retriever and a generator. The retriever extracts relevant knowledge based on queries, subsequently guiding the generator to produce more accurate responses. Nevertheless, the study \"Lost in the Middle\" (Liu et al., 2024) highlights the challenge that LLMs struggle to identify the most relevant within long contexts. Even when the retriever supplies pertinent contextual information, the generator may still produce hallucinations during the response generation process. The conditions under which hallucinations occur in RAG systems, however, are not yet systematically understood.\nExisting methods for detecting hallucinations in LLMs include: 1) Uncertainty-based methods (Manakul et al., 2023a; Chen et al., 2024; Fadeeva et al., 2023; Zhang et al., 2023; Fadeeva et al., 2024) that utilize model inner states to evaluate the confidence of the outputs. 2) Perturbation-based methods (Wang et al., 2023a) apply perturbations to the original inputs and then evaluate the self-consistency of the outputs. Inspired by these approaches, we aim to explain and detect RAG hallucinations by analyzing the relevance between the prompt and the LLM response.\nIn this paper, we propose LRP4RAG, a method based on back-propagation to detect hallucinations in RAG. LRP4RAG leverages the back-propagated from the generated tokens to the input tokens as correlation clues to detect whether hallucinations occur during the generation process.\nSpecifically, we employ a classic interpretable algorithm called LRP (Binder et al., 2016) for relevance analysis. We perform LRP backward after the generator outputs logits to obtain a relevance matrix. We then analyze and preprocess this relevance matrix before feeding it into the classifiers. Finally, we train the classification models to predict whether a sample is hallucinated. Prior to evaluating LRP4RAG, we conduct an analysis on the connection between relevance and RAG hallucinations and conduct a simple hallucination detection test based on relevance threshold. The result of the threshold-based test gives a preliminary verification to the efficacy of relevance in detecting RAG hallucinations.\nWe evaluate our approach using the RAGTruth dataset (Wu et al., 2023), and our results demonstrate that our method outperforms existing baselines.\nOur contributions can be summarized as follows:\n\u2022 We highlight the challenge of RAG hallucination and identify the need of hallucination detection in RAG.\n\u2022 We analyze the correlation between RAG hallucination and LLM internal states, demonstrating a significant association between the relevance of inputs and outputs of LLMs and their hallucination.\n\u2022 We propose LRP4RAG, a back-propagation based hallucination detection method, which outperforms other mainstream baselines in hallucination detection task on RAG hallucination benchmark."}, {"title": "Related Work", "content": "2.1 LLM Hallucination\nHallucination in LLMs is typically classified into three categories. Input-conflicting hallucination arises when LLM response deviates from user input. Context-conflicting hallucination arises when LLMs lose track of the context or fail to maintain consistency during conversation. Fact-conflicting hallucination arises when LLMs generate information that contradicts established world knowledge.\n2.2 Hallucination Detecting\nTask-specific Previous research has primarily concentrated on detecting hallucination in specific natural language generation tasks, such as machine translation (Dale et al., 2023; Guerreiro et al., 2023), dialogue generation (Dziri et al., 2022) and question answering (Durmus et al., 2020). In our work, we focus on detecting LLM hallucination in RAG task.\nPerturbation-based Perturbation-based methods (Wang et al., 2023a) apply slight perturbations to LLM input and then evaluate the self-consistency of the outputs. If there exists significant divergence among answers, it suggests hallucination happens. Similar methods (Shi et al., 2022) prompt LLMs to generate multiple responses to the same question and evaluating the self-consistency of those responses. In our work, we use the representative SelfCheckGPT (Manakul et al., 2023b) as our baseline.\nUncertainty-based Token-level uncertainty estimation (e.g. entropy) has proven efficient in hallucination detecting on NLP tasks (Malinin and Gales, 2021; Huang et al., 2023). Recent studies (Duan et al., 2024; Yin et al., 2023) further work on sentence-level and language-level uncertainty to make extensive progress. However, as INSIDE (Chen et al., 2024) points out, such methods only apply post-hoc semantic measurement on decoded sentences, which is inferior to utilizing model inner states directly. Hence, we concentrate on examining the internal states of LLMs during RAG generation process for RAG hallucination detecting.\nLLM-based LLM-based methods design prompts or directly fine-tune LLMs to detect hallucination. We follow RAGTruth (Wu et al., 2023) and use LLM-based methods as our baselines.\n2.3 Retrieval-Augmented Generation\nTraditional NLP tasks have been benefiting from RAG since ChatGPT appears. RAG enhances LLMs by retrieving relevant document chunks from outside knowledege base. By referencing external knowledge, RAG effectively reduces the problem of LLM hallucination.\nHowever, recent studies show RAG still faces problems which make hallucination inevitable. Barnett (Barnett et al., 2024) lists 7 main problems identified from case studies, including insufficient context, missing top ranked documents, etc. These problems lead to a degradation in RAG output quality, ultimately resulting in hallucination. The work \"Lost in the Middle\" (Liu et al., 2024) also emphasizes the difficulty that LLMs encounter when attempting to pinpoint the most pertinent information within extensive contextual spans. This deficiency in handling long contextual information can lead to hallucination when RAG generates text.\nRecognizing the numerous limitations of RAG, this paper delves into an exhaustive analysis of the distinct internal states of LLMs that accompany the occurrence of hallucination. Capitalizing on these insights, we propose a novel methodology, LRP4RAG, designed to detect hallucinations during the generation process with RAG."}, {"title": "Methodology", "content": "3.1 Task Definition\nIn a RAG task, given retrieved context sequence \\(C = (c_1, ..., c_n)\\), question sequence \\(Q = (q_1, ..., q_m)\\), prompt template sequence \\(T = (t_1, ..., \\{C\\}, ..., \\{Q\\}, ..., t_k)\\), generator will output answer sequence \\(A = (a_1, ..., a_t)\\) based on prompt sequence P composed of C, Q, T. Relevance between P and A will be a weight matrix which has t rows and n + m + k columns. The object is to infer whether A contains hallucination based on \\{P, A, R\\}. The main process of our method is shown in Figure 1.\n3.2 LRP-based Relevance\nLayer-wise Relevance Propgation LRP calculates layer-level relevance between input and output from language model, which offering insights on the importance of source tokens to the response tokens. In our work, we use LRP to get token-to-token level relevance between prompt and response during RAG generation process. First, the maximum logit is selected as the relevance value for the last layer. Secondly, through back propagation functions, relevances are calculated sequentially for each layer until encountering the embedding layer. Finally, we accumulate the relevance along the embedding dimension to obtain the final relevance. Main functions used in LRP process are outlined below.\nMatMul We design the MatMul function for matrix multiplication, which is used in QKV calculating of attention layers. Relevance is calculated for both matrix multiplication terms A and B. Matrix C is the product of multiplying matrix A by matrix B, and matrix \\(R_C\\) represents the relevance that is back propagated to matrix C.\n\\(R_A = R_C \\times B^T * A\\)  \\(R_B = R_C \\times A^T * B\\) (1)\nLinear We treat Linear function as a special case of MatMul function, where relevance is only calculated for input I.\n\\(R_{t,i-1} = R_{t,i} \\times W^{-1}_{i, *} * I_i\\) (2)\nNon-Parameter For non-parameter layers like normalization layers (Softmax, LayerNorm), activation layers (Sigmoid, Relu), we compute the Jacobian matrix of the layer's output with respect to its input, and replace the W in equation 2 with it.\n\\(Jy(x) = \\begin{pmatrix}\n  \\frac{\\partial y_1}{\\partial x_1} & ... & \\frac{\\partial y_1}{\\partial x_n} \\\\\n  ... &  & ...\\\\\n  \\frac{\\partial y_m}{\\partial x_1} & ... & \\frac{\\partial y_m}{\\partial x_n}\n\\end{pmatrix}\\) (3)\n\\(R_{t,i-1} = R_{t,i} \\times JO_i(I_i) * I_i\\) (4)\n3.3 Relevance Statistics\nRelevance for Prompt and Response When LRP ends, the gradient of embedding layer will output \\(R^*\\), the relevance matrix between the prompt and response. We further compute relevance both for prompt tokens (\\(R^{prompt}\\)) and response tokens (\\(R^{response}\\)).\nWe obtain the i-th element of \\(R^{prompt}\\) by calculating the mean of the i-th column of \\(R^*\\).\n\\(R^{prompt}_i = \\frac{1}{t} \\sum_{j=1}^t R^*_{ij}\\) (5)\nSimilarly, we obtain the j-th element of \\(R^{response}\\) by calculating the mean of the j-th row of \\(R^*\\).\n\\(R^{response}_j = \\frac{1}{m+n+k} \\sum_{i=1}^{m+n+k} R^*_{ij}\\) (6)"}, {"title": "Hallucination Classifier", "content": "3.4 Hallucination Classifier\nWhen classifying the processed relevance data, we employ two methods. First, we treat the relevance as feature vectors and use simple classifiers (SVM, MLP Classifier, Random Forest) to directly classify \\(R^{prompt}\\) and \\(R^{response}\\).\n\\(\\hat{c}_t = tanh(W_c[x_t, h_{t-1}] + b_c)\\) (10)\n\\(C_t = f_t C_{t-1} + i_t \\tilde{C}_t\\) (11)\n\\(h_t = o_t o tanh(c_t)\\) (12)\nAdditionally, we consider \\(R^*\\) as time series data related to the input or output time step, and classify it using the RNN model LSTM combined with a Imhead. The computations at each time step in an LSTM are as illustrated by formulas 10 - 12, where for conciseness we have omitted ft, it and Ot.\nWhen inputting \\(R^*\\) into LSTM, we first regard \\(R^*\\) as a token sequence with the same length as the prompt and an embedding dimension equivalent to the length of the response. After undergoing LSTM computations for n+m+k steps, LSTM outputs \\(h_{m+n+k}\\). We then use the Imhead to do binary-classification based on \\(h_{m+n+k}\\). Likewise, we pick the dimension of \\(R^*\\) representing prompt as embedding and employ LSTM to process it over t time steps, where t corresponds to the length of the response. In the experimental results, we retain only the latter, as it proves to be more effective compared to the former.\nWe adopt five-fold cross-validation to evaluate the hallucination detection performance of the classification model on the entire dataset."}, {"title": "Experiments", "content": "4.1 Datasets\nWe conduct experiments on a public RAG dataset called RAGTruth (Wu et al., 2023). Different from most popular benchmarks like HotpotQA (Yang et al., 2018) and Squad (Rajpurkar et al., 2018), RAGTruth consists of human-annotated RAG samples. Compared to applying similarity metrics (Rouge, BLEU) based on ground truth, we calculate accuracy, precision, recall and f1 score based on true/false labels, which make the result more intuitive and easier to measure. Also, RAGTruth provides hallucinated samples from multiple LLMs, enabling us to analyze hallucination across models. In particular, we select the QA part of the dataset which has 989 open questions. We then test these questions with Llama-2-7b-chat and Llama-2-13b-chat. Responses from Llama-2-7b-chat includes 510 hallucinated samples and 479 normal samples, while responses from Llama-2-7b-chat includes 399 hallucinated samples and 590 normal samples.\n4.2 Experimental Settings\nDuring the model generation phase, we keep the model's hyperparameters consistent with the dataset settings. In the LRP phase, to ensure numerical stability, we set epsilon to le-6 when normalizing the relevance scores. We use the radial basis function (RBF) kernel in SVM classifier, and set up a two-layered LSTM classifier, with hidden_size set to 256. The learning rate for the LSTM classifier is set to 5e-4.\n4.3 Baselines\nWe compare our hallucination detection method with three baselines (Hallucination Detection Prompt, SelfCheckGPT (Manakul et al., 2023b), LLMs Fine-tuning).\nHallucination Detection Prompt We manually craft hallucination detection prompt to instruct LLMs (Llama-2-7b-chat and gpt-3.5-turbo) to identify hallucination and corresponding span.\nSelfCheckGPT We utilize SelfCheckGPT to check consistency between sampled responses. If inconsistency exceeds the threshold, we suppose hallucinations happen.\nLLM Fine-tuning We fine-tune Llama-2-7b-chat and Qwen-2-7b-instruct with QA pairs and corresponding labels. We adopt five-fold cross-validation to obtain classification result on full dataset."}, {"title": "Results", "content": "4.4 Results\nWe adopt the methods mentioned in Section 3.3 to compare the difference of relevance distribution between hallucinated samples and normal samples. To make the results more intuitive, we uniformly re-sample both \\(R^{prompt}\\) and \\(R^{response}\\) with the same \\(L^{new}\\)=100 and calculate the mean of all samples. We then perform noise reduction on the extreme values of \\(R^{prompt}\\) and \\(R^{response}\\), and normalize them to the interval (0, 1).\n4.4.1 Hallucination vs Normal\nIn the first experiment, we aim to analyze the differences in the distribution of relevance between hallucinated samples and normal samples. We illustrate the differences from 3 perspectives (box plot, line graph, heatmap).\nWe use box plots in figure 2 to visualize the cumulative relevance at the sample level. Excluding few outliers, it can be observed that the median of the relevance for normal samples is above that of the hallucinated samples. It indicates that compared to hallucinated samples, normal samples exhibit a stronger correlation between the prompt and response.\nThis can also be verified in figure 3. Although the overall trend of relevance distribution is consistent, the relevance curve for normal samples consistently remains above that of hallucinated samples. The pattern holds true in the statistical results of \\(R^{prompt}\\) and \\(R^{response}\\), both across models. Another noteworthy point is that compared with \\(R^{prompt}\\) of Llama-2-7b-chat and \\(R^{response}\\) of Llama-2-13b-chat separately, \\(R^{response}\\) of Llama-2-7b-chat shows less noise in its overall distribution, with more pronounced separation between the curves. It indicates two things: 1) \\(R^{response}\\) is more suitable for hallucination detection compared with \\(R^{prompt}\\) in Llama-2-7b-chat, 2) hallucinations in larger models are more subtle and harder to detect.\nHotmaps in figure 4 further show the token-to-token level relevance distribution. We apply a 2D resampling method similar to the 1D one as mentioned at the end of Section 3.3. The result is consistent with figure 2 and figure 3. The hotmaps on the right contain more dark regions than the left ones, indicating that stronger relevance between the prompt and response exists in normal samples. Besides, the pair of hotmaps above show more pronounced differences compared to the pair below, indicating that hallucinations in Llama-7b are easier to discern than those in Llama-13b.\nThrough the above analysis of relevance from three different perspectives, we have obtained the following findings:\nFinding 1:\n1. RAG Hallucinations lead to low relevance.\n2. \\(R^{response}\\) is more suitable than \\(R^{prompt}\\) for RAG hallucination detection.\n3. RAG Hallucinations are harder to detect for larger models.\nWe further employ the Mann-Whitney U test to quantitatively analyze whether there exist significant differences in \\(R^{prompt}\\) and \\(R^{response}\\) between hallucinated samples and normal samples across different models. Due to unequal numbers of hallucinated and normal samples, we adopt a random sampling strategy where 200 samples are randomly chosen for each iteration, then paired and subjected to a U test. The median of the 200 test results is obtained to represent the overall difference as shown in figure 5. It can be observed that there is a significant difference in \\(R^{response}\\) between hallucinated samples and normal samples with pvalue > 0.05.\n4.4.2 Threshold-based Result\nTo validate the conclusions drawn in Section 4.4.1 and demonstrate the effectiveness of relevance in detecting RAG hallucinations, we devise a simple threshold-based classification approach.\nFor the sake of brevity, we maintain consistency with Section 4.4.1 in parameter configuration by resampling \\(R^{prompt}\\) and \\(R^{response}\\) to a length of 100 before computing their means. With a step size of 0.1, we search across the threshold space (0,1). Table 1 enumerates part of the hallucination detection results under different threshold values. For Llama-2-7b-chat, the threshold values for \\(R^{prompt}\\) and \\(R^{response}\\) are found to be optimally effective at around t = 0.3 and t = 0.5, respectively, for detecting hallucinations. Furthermore, \\(R^{response}\\) demonstrates significantly better performance compared to \\(R^{prompt}\\), surpassing the 60% mark across all metrics. It excels in both the number of hallucination samples detected and the overall classification accuracy, holding an advantage in every aspect.\nOn the Llama-2-13b-chat dataset, it is challenging to attain effective classification outcomes, with difficulty in striking a balance between accuracy and recall, and precision levels generally remaining low. Consequently, we contend that introducing \\(R^{prompt}\\) or \\(R^{response}\\) alone is not significantly effective in detecting hallucination in large-scale parameter LLMs.\n4.4.3 Comparison with Baselines\nAfter validating the effectiveness of the threshold-based approach for RAG hallucination detection, we further proceed with experiments on machine learning-based RAG hallucination detection.\nThe main result of our approach is shown in Table 2. Compared to existing methods, our approach has strong performance across all metrics. When detecting hallucinations of Llama-2-7b-chat, the LRP-based methods achieve the highest accuracy (69.16%), precision (69.35%) and f1 score (70.64%), which makes an improvement of 7.59%, 6.85% and 3.02% over non-LRP-based methods. When detecting hallucinations of Llama-2-13b-chat, the LRP-based methods also have the best performance, with the highest accuracy (69.87%), precision (68.26%) and the second highest f1 score (55.56%).\nFinding 2: The LRP-based methods we propose outperform all three baselines when detecting RAG hallucination in Llama-2-7b-chat and Llama-2-13b-chat.\nWhen comparing the results of the two models, we find that both the baseline methods and the LRP-based methods experience a decrease in recall when detecting hallucinations in Llama-2-13b-chat. This suggests that these methods become more conservative in predicting hallucinations when facing models with larger parameters, leading to an increase in False Negative samples.\nDespite SelfCheckGPT achieving high recall scores on Llama-2-13b-chat dataset, ranking first with 89.47% and second with 75.94%, both methods exhibit low accuracy and precision, falling below 50%. This suggests that a substantial number of normal samples are incorrectly classified as hallucinated cases. Such outcomes undermine the purpose of hallucination detection, as even if hallucinated samples are identified, a considerable amount of manual review is still required, thereby negating the efficiency gains sought from automated detection. In contrast, the LRP-based method maintain relatively good stability across overall metrics. Its accuracy reaches up to 69.87%, and the precision peaks at 68.26%, showing virtually no change compared to its performance on the Llama-2-7b-chat dataset in terms of both accuracy and precision. The recall has experienced a certain degree of decline, but it still maintains an approximately 20% advantage compared to the results obtained by directly fine-tuning the LLMs. Therefore, we believe that LRP-based methods still exhibit remarkable applicability on the Llama-2-13b-chat dataset.\nFinding 3: Despite the increased difficulty in detecting hallucinations in larger LLMs, LRP-based methods still maintain good performance.\nAnother noteworthy point is that when using \\(R^*\\) instead of \\(R^{prompt}\\) or \\(R^{response}\\), the \\(LRP_{mean\\_resampling}\\) + 1stm classification results show a decline compared to the \\(LRP_{mean\\_resampling}\\) + svm results with only \\(R^{response}\\) introduced on Llama-2-7b-chat dataset. However, on Llama-2-13b-chat dataset, \\(LRP_{mean\\_resampling}\\) + 1stm outperforms \\(LRP_{mean\\_resampling}\\) + svm.\nThrough the visualization analysis of the relevance distribution in Section 4.4.1, we believe this is due to the following reasons: 1) On Llama-2-7b-chat dataset, \\(R^{prompt}\\) contains more noise compared to \\(R^{response}\\), thus introducing \\(R^{prompt}\\) alone leads to better performance. 2) On Llama-2-13b-chat dataset, increased model complexity leads to changes in the relevance distribution. Therefore, introducing \\(R^{response}\\) only is not sufficient, whereas \\(LRP_{mean\\_resampling}\\) + 1stm take \\(R^*\\) into consideration and demonstrates improved performance.\n4.4.4 Impact of Resampling Length\nIn search for the most effective resampling length \\(L^{new}\\), we test with different value in the interval (0,500). In Table 3, we present the effectiveness of \\(LRP_{mean\\_resampling}\\) + svm at different resampling lengths. We find the classifier performs best with \\(L^{new}\\) setting to 220. With such setup, resampling retains the details while denoising, preventing distortion which may harm classifier efficiency."}, {"title": "Conclusion", "content": "In this work, we propose a LRP-based approach LRP4RAG to detect RAG hallucination. Different from existing works that utilize RAG to mitigate LLM hallucination, we aim to discuss and explore vulnerability in RAG. We look into LLMs inner states and consider the relevance between the prompt and response. We find the difference of relevance distribution between normal and hallucinated samples, and use it as evidence for classification. Extensive experiments show LRP4RAG achieves performance gains over strong baselines."}, {"title": "Limitations", "content": "LRP4RAG has only been evaluated on two datasets, and the cross-model hallucination analysis encompasses just two models. A broader range of models, including those that are more diverse and larger in scale, need to be included in the comparison to substantiate the universal efficacy of our approach in detecting RAG hallucination."}]}