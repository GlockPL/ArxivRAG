{"title": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation", "authors": ["Haichuan Hu", "Yuhan Sun", "Quanjun Zhang"], "abstract": "Retrieval-Augmented Generation (RAG) has become a primary technique for mitigating hallucinations in large language models (LLMs). However, incomplete knowledge extraction and insufficient understanding can still mislead LLMs to produce irrelevant or even contradictory responses, which means hallucinations persist in RAG. In this paper, we propose LRP4RAG, a method based on the Layer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations in RAG. Specifically, we first utilize LRP to compute the relevance between the input and output of the RAG generator. We then apply further extraction and resampling to the relevance matrix. The processed relevance data are input into multiple classifiers to determine whether the output contains hallucinations. To the best of our knowledge, this is the first time that LRP has been used for detecting RAG hallucinations, and extensive experiments demonstrate that LRP4RAG outperforms existing baselines.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP). However, they often generate plausible yet incorrect answers, a phenomenon known as \"hallucination\". Hallucinations have been observed across various generation tasks (Xu et al., 2024; Guerreiro et al., 2023; Wang et al., 2023b; Ji et al., 2023; Adlakha et al., 2024) in NLP. The issue of hallucination makes LLMs untrustworthy and presents risks when deploying LLMs in practical scenarios. To mitigate LLM hallucination, Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) has been proposed. RAG typically consists of a retriever and a generator. The retriever extracts relevant knowledge based on queries, subsequently guiding the generator to produce more accurate responses. Nevertheless, the study \"Lost in the Middle\" (Liu et al., 2024) highlights the challenge that LLMs struggle to identify the most relevant within long contexts. Even when the retriever supplies pertinent contextual information, the generator may still produce hallucinations during the response generation process. The conditions under which hallucinations occur in RAG systems, however, are not yet systematically understood.\nExisting methods for detecting hallucinations in LLMs include: 1) Uncertainty-based methods (Manakul et al., 2023a; Chen et al., 2024; Fadeeva et al., 2023; Zhang et al., 2023; Fadeeva et al., 2024) that utilize model inner states to evaluate the confidence of the outputs. 2) Perturbation-based methods (Wang et al., 2023a) apply perturbations to the original inputs and then evaluate the self-consistency of the outputs. Inspired by these approaches, we aim to explain and detect RAG hallucinations by analyzing the relevance between the prompt and the LLM response.\nIn this paper, we propose LRP4RAG, a method based on back-propagation to detect hallucinations in RAG. LRP4RAG leverages the back-propagated from the generated tokens to the input tokens as correlation clues to detect whether hallucinations occur during the generation process.\nSpecifically, we employ a classic interpretable algorithm called LRP (Binder et al., 2016) for relevance analysis. We perform LRP backward after the generator outputs logits to obtain a relevance matrix. We then analyze and preprocess this relevance matrix before feeding it into the classifiers. Finally, we train the classification models to predict whether a sample is hallucinated. Prior to evaluating LRP4RAG, we conduct an analysis on the connection between relevance and RAG hallucinations and conduct a simple hallucination detection test based on relevance threshold."}, {"title": "Related Work", "content": "LLM Hallucination\nHallucination in LLMs is typically classified into three categories. Input-conflicting hallucination arises when LLM response deviates from user input. Context-conflicting hallucination arises when LLMs lose track of the context or fail to maintain consistency during conversation. Fact-conflicting hallucination arises when LLMs generate information that contradicts established world knowledge.\nHallucination Detecting\nTask-specific Previous research has primarily concentrated on detecting hallucination in specific natural language generation tasks, such as machine translation (Dale et al., 2023; Guerreiro et al., 2023), dialogue generation (Dziri et al., 2022) and question answering (Durmus et al., 2020). In our work, we focus on detecting LLM hallucination in RAG task.\nPerturbation-based Perturbation-based methods (Wang et al., 2023a) apply slight perturbations to LLM input and then evaluate the self-consistency of the outputs. If there exists significant divergence among answers, it suggests hallucination happens. Similar methods (Shi et al., 2022) prompt LLMs to generate multiple responses to the same question and evaluating the self-consistency of those responses. In our work, we use the representative SelfCheckGPT (Manakul et al., 2023b) as our baseline.\nUncertainty-based Token-level uncertainty estimation (e.g. entropy) has proven efficient in hallucination detecting on NLP tasks (Malinin and Gales, 2021; Huang et al., 2023). Recent studies (Duan et al., 2024; Yin et al., 2023) further work on sentence-level and language-level uncertainty to make extensive progress. However, as INSIDE (Chen et al., 2024) points out, such methods only apply post-hoc semantic measurement on decoded sentences, which is inferior to utilizing model inner states directly. Hence, we concentrate on examining the internal states of LLMs during RAG generation process for RAG hallucination detecting.\nLLM-based LLM-based methods design prompts or directly fine-tune LLMs to detect hallucination. We follow RAGTruth (Wu et al., 2023) and use LLM-based methods as our baselines.\nRetrieval-Augmented Generation\nTraditional NLP tasks have been benefiting from RAG since ChatGPT appears. RAG enhances LLMs by retrieving relevant document chunks from outside knowledege base. By referencing external knowledge, RAG effectively reduces the problem of LLM hallucination.\nHowever, recent studies show RAG still faces problems which make hallucination inevitable. Barnett (Barnett et al., 2024) lists 7 main problems identified from case studies, including insufficient context, missing top ranked documents, etc. These problems lead to a degradation in RAG output quality, ultimately resulting in hallucination. The work \"Lost in the Middle\" (Liu et al., 2024) also emphasizes the difficulty that LLMs encounter when attempting to pinpoint the most pertinent information within extensive contextual spans. This deficiency in handling long contextual information can lead to hallucination when RAG generates text.\nRecognizing the numerous limitations of RAG, this paper delves into an exhaustive analysis of the distinct internal states of LLMs that accompany the occurrence of hallucination. Capitalizing on these insights, we propose a novel methodology, LRP4RAG, designed to detect hallucinations during the generation process with RAG."}, {"title": "Methodology", "content": "Task Definition\nIn a RAG task, given retrieved context sequence $C = (c_1,..., c_n)$, question sequence $Q = (q_1,..., q_m)$, prompt template sequence $T = (t_1, ..., \\{C\\}, ..., \\{Q\\}, ..., t_k)$, generator will output answer sequence $A = (a_1, ..., a_t)$ based on prompt sequence P composed of C, Q, T. Relevance between P and A will be a weight matrix which has t rows and n + m + k columns. The object is to infer whether A contains hallucination based on {P, A, R}.\nLRP-based Relevance\nLayer-wise Relevance Propgation LRP calculates layer-level relevance between input and output from language model, which offering insights on the importance of source tokens to the response tokens. In our work, we use LRP to get token-to-token level relevance between prompt and response during RAG generation process. First, the maximum logit is selected as the relevance value for the last layer. Secondly, through back propagation functions, relevances are calculated sequentially for each layer until encountering the embedding layer. Finally, we accumulate the relevance along the embedding dimension to obtain the final relevance. Main functions used in LRP process are outlined below.\nMatMul We design the MatMul function for matrix multiplication, which is used in QKV calculating of attention layers. Relevance is calculated for both matrix multiplication terms A and B. Matrix C is the product of multiplying matrix A by matrix B, and matrix $R_c$ represents the relevance that is back propagated to matrix C.\n$R_A = R_C \\times B^T * A$\n$R_B = R_C \\times A^T * B$                                                                                                                                                          (1)\nLinear We treat Linear function as a special case of MatMul function, where relevance is only calculated for input I.\n$R_{t,i-1} = R_{t,i} \\times W^{-1}_{i, *} * I_i$                                                                                                                                                             (2)\nNon-Parameter For non-parameter layers like normalization layers (Softmax, LayerNorm), activation layers (Sigmoid, Relu), we compute the Jacobian matrix of the layer's output with respect to its input, and replace the W in equation 2 with it.\n$J_y(x) =  \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} & ... & \\frac{\\partial y_1}{\\partial x_n} \\\\  ... & ... & ...  \\\\  \\frac{\\partial y_m}{\\partial x_1} & ... & \\frac{\\partial y_m}{\\partial x_n}  \\end{bmatrix}$                                                                                                                                           (3)\n$R_{t,i-1} = R_{t,i} \\times J_{O_i}(I_i) * I_i$                                                                                                                                                           (4)\nRelevance Statistics\nRelevance for Prompt and Response When LRP ends, the gradient of embedding layer will output $R^*$, the relevance matrix between the prompt and response. We further compute relevance both for prompt tokens ($R^{prompt}$) and response tokens ($R^{response}$). We obtain the i-th element of $R^{prompt}$ by calculating the mean of the i-th column of $R^*$.\n$R^{prompt}_i = \\frac{1}{t} \\sum_{j=1}^{t} R^*_{ij}$                                                                                                                                                                           (5)\nSimilarly, we obtain the j-th element of $R^{response}$ by calculating the mean of the j-th row of $R^*$.\n$R^{response}_j = \\frac{1}{m+n+k} \\sum_{i=1}^{m+n+k} R^*_{ij}$                                                                                                                                                                       (6)\nRelevance Resampling Due to the differences in the lengths of the RAG generator's input and output, $R^{prompt}$ and $R^{response}$ have different shapes and cannot be directly fed into the classifier for hallucination detecting. To keep $R^{prompt}$ and $R^{response}$ at a fixed length while minimizing the loss of original information, we adopt a mean resampling method to normalize the relevance vectors.\n$\\rho = \\frac{L_{old}}{L_{new}}$                                                                                                                                                                                              (7)\nFirst, we define the scaling factor $\\rho$, which determines the resampling range.\n$R^{new}_{start_i} = [i \\cdot \\rho]$\n$R^{new}_i = \\frac{\\sum_{j=start_i+1}^{endi} R^{old}_j}{end_i - start_i}$                                                                                                                                                              (8)\n$end_i = [(i+1) \\cdot \\rho]$                                                                                                                                                                                                       (9)\nFor each index i in the new sample, we perform mean resampling in the corresponding region (starti, endi) of the original sample. For cases where the sampling range exceeds the length of the original sample, we pad the end with the last value of the original sample. For a 2D relevance matrix $R^*$, we similarly perform mean resampling along both dimensions to obtain a matrix of a specific shape.\nHallucination Classifier\nWhen classifying the processed relevance data, we employ two methods. First, we treat the relevance as feature vectors and use simple classifiers (SVM, MLP Classifier, Random Forest) to directly classify $R^{prompt}$ and $R^{response}$.\n$\\hat{c}_t = tanh(W_c[x_t, h_{t-1}] + b_c)$                                                                                                                                                                               (10)\n$c_t = f_t c_{t-1} + i_t \\tilde{c}_t$                                                                                                                                                                                             (11)\n$h_t = o_t \\sigma tanh(c_t)$                                                                                                                                                                                                   (12)\nAdditionally, we consider $R^*$ as time series data related to the input or output time step, and classify it using the RNN model LSTM combined with a lmhead. The computations at each time step in an LSTM are as illustrated by formulas 10 - 12, where for conciseness we have omitted $f_t$, $i_t$ and $o_t$. When inputting $R^*$ into LSTM, we first regard $R^*$ as a token sequence with the same length as the prompt and an embedding dimension equivalent to the length of the response. After undergoing LSTM computations for n+m+k steps, LSTM outputs $h_{m+n+k}$. We then use the lmhead to do binary-classification based on $h_{m+n+k}$. Likewise, we pick the dimension of $R^*$ representing prompt as embedding and employ LSTM to process it over t time steps, where t corresponds to the length of the response. In the experimental results, we retain only the latter, as it proves to be more effective compared to the former.\nWe adopt five-fold cross-validation to evaluate the hallucination detection performance of the classification model on the entire dataset."}, {"title": "Experiments", "content": "Datasets\nWe conduct experiments on a public RAG dataset called RAGTruth (Wu et al., 2023). Different from most popular benchmarks like HotpotQA (Yang et al., 2018) and Squad (Rajpurkar et al., 2018), RAGTruth consists of human-annotated RAG samples. Compared to applying similarity metrics (Rouge, BLEU) based on ground truth, we calculate accuracy, precision, recall and f1 score based on true/false labels, which make the result more intuitive and easier to measure. Also, RAGTruth provides hallucinated samples from multiple LLMs, enabling us to analyze hallucination across models. In particular, we select the QA part of the dataset which has 989 open questions. We then test these questions with Llama-2-7b-chat and Llama-2-13b-chat. Responses from Llama-2-7b-chat includes 510 hallucinated samples and 479 normal samples, while responses from Llama-2-7b-chat includes 399 hallucinated samples and 590 normal samples.\nExperimental Settings\nDuring the model generation phase, we keep the model's hyperparameters consistent with the dataset settings. In the LRP phase, to ensure numerical stability, we set epsilon to le-6 when normalizing the relevance scores. We use the radial basis function (RBF) kernel in SVM classifier, and set up a two-layered LSTM classifier, with hidden_size set to 256. The learning rate for the LSTM classifier is set to 5e-4.\nBaselines\nWe compare our hallucination detection method with three baselines (Hallucination Detection Prompt, SelfCheckGPT (Manakul et al., 2023b), LLMs Fine-tuning).\nHallucination Detection Prompt We manually craft hallucination detection prompt to instruct LLMs (Llama-2-7b-chat and gpt-3.5-turbo) to identify hallucination and corresponding span.\nSelfCheckGPT We utilize SelfCheckGPT to check consistency between sampled responses. If inconsistency exceeds the threshold, we suppose hallucinations happen.\nLLM Fine-tuning We fine-tune Llama-2-7b-chat and Qwen-2-7b-instruct with QA pairs and corresponding labels. We adopt five-fold cross-validation to obtain classification result on full dataset.\nResults\nWe adopt the methods mentioned in Section 3.3 to compare the difference of relevance distribution between hallucinated samples and normal samples. To make the results more intuitive, we uniformly resample both $R^{prompt}$ and $R^{response}$ with the same $L^{new}$=100 and calculate the mean of all samples. We then perform noise reduction on the extreme values of $R^{prompt}$ and $R^{response}$, and normalize them to the interval (0, 1).\nHallucination vs Normal\nIn the first experiment, we aim to analyze the differences in the distribution of relevance between hallucinated samples and normal samples. We illustrate the differences from 3 perspectives (box plot, line graph, heatmap)."}, {"title": "Impact of Resampling Length", "content": "In search for the most effective resampling length $L_{new}$, we test with different value in the interval (0,500). In Table 3, we present the effectiveness of LRPmean_resampling + svm at different resampling lengths. We find the classifier performs best with $L_{new}$ setting to 220. With such setup, resampling retains the details while denoising, preventing distortion which may harm classifier efficiency."}, {"title": "Conclusion", "content": "In this work, we propose a LRP-based approach LRP4RAG to detect RAG hallucination. Different from existing works that utilize RAG to mitigate LLM hallucination, we aim to discuss and explore vulnerability in RAG. We look into LLMs inner states and consider the relevance between the prompt and response. We find the difference of relevance distribution between normal and hallucinated samples, and use it as evidence for classification. Extensive experiments show LRP4RAG achieves performance gains over strong baselines."}, {"title": "Limitations", "content": "LRP4RAG has only been evaluated on two datasets, and the cross-model hallucination analysis encompasses just two models. A broader range of models, including those that are more diverse and larger in scale, need to be included in the comparison to substantiate the universal efficacy of our approach in detecting RAG hallucination."}]}