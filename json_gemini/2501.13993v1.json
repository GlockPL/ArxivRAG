{"title": "CAPRAG: A Large Language Model Solution for Customer Service and Automatic Reporting using Vector and Graph Retrieval-Augmented Generation", "authors": ["Hamza Landolsi", "Kais Letaief", "Nizar Taghouti", "Ines Abdeljaoued-Tej"], "abstract": "The introduction of new features and services in the banking sector often overwhelms customers, creating an opportunity for banks to enhance user experience through financial chatbots powered by large language models (LLMs). We initiated an AI agent designed to provide customers with relevant information about banking services and insights from annual reports. We proposed a hybrid Customer Analysis Pipeline Retrieval-Augmented Generation (CAPRAG) that effectively addresses both relationship-based and contextual queries, thereby improving customer engagement in the digital banking landscape. To implement this, we developed a processing pipeline to refine text data, which we utilized in two main frameworks: Vector RAG and Graph RAG. This dual approach enables us to populate both vector and graph databases with processed data for efficient retrieval. The Cypher query component is employed to effectively query the graph database. When a user submits a query, it is first expanded by a query expansion module before being routed to construct a final query from the hybrid Knowledge Base (KB). This final query is then sent to an open-source LLM for response generation. Overall, our innovative, designed to international banks, serves bank's customers in an increasingly complex digital environment, enhancing clarity and accessibility of information.", "sections": [{"title": "Introduction", "content": "Banks all over the world are constantly introducing new features, delivering new financial services and providing their customer with clear booklets to guide them on how to use their mobile applications, how to benefit from all their available services, how to navigate their websites, etc. However when banks grow, even with the most straightforward user experience and with their efforts to make all informations accessible via promotions and accessible website, the task of naviagting and being informed with updates made the user lost in a maze (Ravalimanana, 2020; Shumba, 2023). As the banking industry continues to embrace digital transformation, the potential applications of financial chatbots with Large Language Models (LLMs) are limitless presenting an exciting opportunity for banks to innovate and differentiate themselves in a rapidly changing market (Eisfeldt et al., 2023; Ooi et al., 2023; Abbott and Smith, 2023).\nIn fact, LLMs and GenAI have revolutionized the field of AI, allowing machines to create diverse content such as text (Pandey et al., 2024), images (Elasri et al., 2022), music (Dong, 2024) and videos (Zhou et al., 2024). Unlike discriminative models that classify, GenAI models generate new content by learning patterns and relationships from human-created datasets (Decardi-Nelson et al., 2024).\nRAG framework is a solution to feed concise relevant context similar to the question and then passed to LLM to answer to a given query (Lewis et al., 2020). This come as a solution to overcome the context window of LLMs limitations. Recently a graph RAG-based approach (Procko and Ochoa, 2024) led to major improvements. This method seeks to replicate the way our minds think and reason, similar to the functioning of a graph. When faced with a question that demands deep thought and reasoning, we begin to model the relationships among key factors, entities, and agents that may impact our area of interest. Many experts from the industry claim that agentic RAG is the future (Nguyen et al., 2024): It aims to deliver a standalone solution that can be independent relying on multiple agents, specializing each in a defined workflow. With agentic style we can combine process and do multiple actions over a lot of sources and interact seamlessly with multiple agent or \"expert\" in their task. It leads to more context aware and more adaptability (Immorlica et al., 2024). In this work, we designed an AI agent tasked to provide the customers with information related to the bank financial services, features, and key insights about the company through its annual reports. We obtained a hybrid approach using Vector and Graph RAG, to handle efficiently the nuances of both relationship based and contextual questions. Instead of thinking in a linear way that isolates each element, we connected various elements and asked questions that relate hidden factors. After iteratively refining our experiments and testing each component we conclude to our solution Customer Analysis Pipeline RAG (CAPRAG). We discussed in next sections each workflow separately and the challenges and limitations of our solution. In Figure 5 we gave the big picture of the workflow and we mentioned a Vector and Graph RAG pipelines."}, {"title": "Data source and description of bank A", "content": "The data used in this study is derived from publicly available documents and corporate publications pertaining to an international bank, that we anonymized as A. Data sources include SEC filings, brochures and booklets.\u00b9\nSEC filings contain comprehensive financial statements, management discussions, and risk factors, providing a detailed overview of the bank's financial performance and strategic outlook. From the SEC filings, we find annual reports containing qualitative data, such as bank A's strategic initiatives, corporate social responsibility (CSR) activities, and customer engagement programs. These qualitative insights offer a broader understanding of the bank's positioning and operational strategies beyond the financial statements. We extracted financial metrics such as total assets, net income, return on equity (ROE), and risk-weighted assets over a period of five years"}, {"title": "Evaluation pipeline: LLM as a judge", "content": "There are several ways to monitor and track the evolution of our work, for this project we used LLMs to help us rate and assess our metrics of interest. This technique is called \"LLM as a judge\".\nWe setup an evaluation pipeline in an automatic way. First of all, we choose the metrics that align the best with our challenges. To ensure the effectiveness and reliability of the RAG model, it is essential to evaluate its outputs using specific metrics. The three metrics we focus on are answer relevance, context relevance and groundedness Es et al. (2024); Roychowdhury et al. (2024); Yu et al. (2024).\nDefinition 3.1 (Answer Relevance) Answer relevance measures how accurately the generated response addresses the given query. A high answer relevance score indicates that the response is directly related to the question and provides useful information.\nDefinition 3.2 (Context Relevance) Context relevance evaluates the extent to which the generated response aligns with the context provided. This metric ensures that the response is coherent within the given context and maintains the flow of the conversation or narrative.\nDefinition 3.3 (Groundedness) Groundedness assesses whether the generated response is based on factual information retrieved from reliable sources. A grounded response is one that can be traced back to authoritative references, ensuring the accuracy and trustworthiness of the information."}, {"title": "Vector RAG: Methodology and Results", "content": "As our main focus is improving customer service efficiency in the banking industry, we leveraged advanced chunking optimization techniques, query expansion, and retrieval enhancement strategies Finardi et al. (2024);\nModi et al. (2024). The goal is to improve the accuracy and speed of responses to customer inquiries by optimizing how Knowledge Base (KB) articles are retrieved and processed. In this Section we detailed further the Vector RAG component (See Figure 2).\nWe first explored various chunking optimization techniques to segment large KB into smaller, more relevant pieces for retrieval (we have chosen the semantic chunking for our solution). Second, we utilized query expansion strategy to refine customer queries, making them more robust and comprehensive for retrieval systems. These techniques allow for a better match between the customer query and relevant chunks. For the retrieval system, we compare different retrieval enhancement techniques tailored to the banking domain. We choose finally the FlashRank re-ranker (Damodaran, 2017) as a post retrieval enhancement strategy."}, {"title": "Graph RAG", "content": "The first step in creating a powerful graph knowledge base is to model the data in the most appropriate way (to capture the complex relationships between chunks). The graph should be created with nodes (representing the entity) and edges (the relationship between two nodes).\n5.1 Schema Construction\nWe first created nodes of label Document, where in each case we could store the document name and other important metadata as properties. The second type of node is Section, where we store sections of each document. Since documents can have sections and many hierarchical subsections, we can handle this by adding two relationships:\n1. Section to Section relationship: UNDER-SECTION that link the subsection to its parent section;\n2. Section to Document relationship: HAS-DOCUMENT which can link sections to the documents they are located in.\nThis gives a comprehensive overview for our hierarchy of the corpus layout to our knowledge base. The third type of node is Chunk. Here we made special node for Table chunks, so we preserve only table related information and separate them from other information (without loosing the scope and the context around the table). We can do so by establishing new edges: HAS_PARENT, CONSECUTIVE, PERSON_LINK, PRODUCT_LINK, etc. For example, the relationship between Table and Section is given by HAS_PARENT; idem for Chunk to Section relationship. The Chunk to Chunk relationship is given by the edge LINKED (that link each chunk with the next chunk in corpus). This can be a solution for the problem of lengthy chunks that lead to loss of information. When we divide them to two chunks carrying the information of the same product or service (information loss or chunk corruption). The Table to Chunk relationship is given by CONSECUTIVE, which links Tables to their next and previous Chunks or other Tables.\nLet focus on some of the powerful entities deemed insightful for our use case. Figure 4 visualizes our expanded graph at this stage. Person is often a very important entity in our corpus that we should focus on. With the length of our chunks, this crucial information may be diluted in the embedding process hence we lose this information. For the Location entity, we focused on the geospatial coordinate (Longitude and Latitude) rather than only working with its semantic vector representation. We made the LLM responsible for text-to-cypher mission the ability to construct Cypher queries capable of solving spatial metric related questions.\n5.2 Methodology\nAlthough semantic similarity is very powerful, it might fail also to identify other types of similarity such as geospatial similarity. For example, the user query might be: \"I am in Ariana and I am wondering what's the nearest bank A.3 ATM branch to me?\u201d.\nThe complexity of the graph database come with more burden and challenges. Having vector representation of embedded user question doesn't help us to query our knowledge graph. Thus, we apaplied LLMs for generating Cypher queries from the user query.4 Achieving this task can be through choosing a powerful LLM, expanding its context window with tailored prompting. The option to go for this use case is often one of the top performing LLMs in the leaderboard."}, {"title": "Customer Analysis Pipeline RAG (CAPRAG)", "content": "We aim to setup a processing pipeline that refine the text data. Than we simultaneously used this processed data in two main pipelines Vector RAG and Graph RAG, see Figure 5. Finally we populated respectively the vector database and graph database with the final ready data to use for retrieval.\nThe router is a LLM prompted to divide the query or reformulate it to each of the next components the graph RAG and the Vector RAG.\nThe Cypher query component is responsible for querying graph database, and we tried several techniques to handle text-to-cypher problem (Francis et al., 2018).5 So when a user enter his question, we pass the query to a Query Expansion module that will enrich our user query and send it to a router where he constructs a query from the hybrid knowledge base and finally send the final prompt to the open source LLM for generation.\nWe used a decoder only based LLM architecture (Brown et al., 2020). We employed HuggingFace H4/zephyr-7b-beta model checkpoint, configured to generate text (Jain, 2022), via HuggingFaceEndpoint. It is configured with a maximum length of 2000 tokens and a temperature of 0.01, which means we are encouraging the LLM to be more deterministic and not allowing it to be creative. In our use case, the LLM sticks to the provided context and reduces the hallucination as much as possible, see Table 3."}, {"title": "Discussion", "content": "In this study, we highlighted the limitations of using embedded user queries for retrieving chunks based solely on cosine similarity with their vector representations. This approach is inadequate for capturing complex relationships and geo-spatial information. Consequently, alternative methods may be necessary to enhance the effectiveness of information retrieval in these contexts. Our findings indicated that, although several automatic expansion techniques are available, manual expansion can be optimal and preferable. This is particularly true for specific industries.\nWhile many researchers have utilized graph RAG for ontology-based graph construction in the banking sector, there are notable variations in their approaches. For instance, Kang and Cheung (2023) explored novel methods for extracting accurate and insightful entities. Additionally, some studies have focused on establishing the relationships between sections and the structure of the document hierarchy. These diverse methodologies contribute to a deeper understanding of how to effectively leverage graph RAG in this domain. Other research efforts have concentrated on geospatial data, achieving significant advancements in enhancing embeddings that capture both semantic and spatial meanings Hu et al. (2024). Some studies have focused on customizing agentic workflows to facilitate interactions with geographic data Ning et al. (2024). These contributions highlight the importance of integrating semantic and spatial dimensions in geospatial analysis.\nOur findings complement the work of Sarmah et al. (2024), which aims to enhance questions related to spatial terms. This enhancement requires metrics of similarity that extend beyond semantic-oriented measures, such as cosine similarity. For that, we are letting a router LLM to think what metric to choose and which Cypher query to run. We pave the way for innovative approaches that rely on alternative similarity metrics. Our method begins by identifying issues in similarity that cannot be captured semantically. We then extract the relevant entities associated with these problems. Finally let the LLM decide how to query these nodes of interest. The extraction of product entities can give banks and financial institutions the opportunity of either up-selling or cross-selling their products based on the customer profile. As ontology based graph knowledge is gaining a huge success proven by recent research works, there is a growing need for identifying relevant entities for each sector. Relying on curated factors and entities from domain experts can be more effective than allowing LLMs to construct the knowledge graph independently. An ideal approach may involve combining both methods. We have developed a pipeline that intelligently selects the appropriate path and components for answering specific questions. This pipeline includes a router that understands the requirements of each question. This ensures a more accurate and relevant response.\nAs we don't have access to closed source models, and the hardware required to run open source LLMs, we focused on using open source only models. We were limited by the Hugging Face maximum quota of requests. So we couldn't evaluate our pipelines on open data benchmarks in this field or conclude to comparisons. We could potentially get better results in extracting entities of our focus by experimenting with specialized LLMs or agents for converting human-like queries to Cypher given our proposed scheme. It may be possible to use a structured output format more easily, using features that are only supported in closed source models."}, {"title": "Conclusion", "content": "Our research highlights the integration of Vector RAG and Graph RAG in an advanced pipeline, for an application into the banking sector. By refining text data and utilizing a hybrid approach, we have developed a robust Customer Analysis Pipeline RAG (CAPRAG) that effectively addresses the complexities of customer inquiries. This innovative solution not only enhances the retrieval of relevant information but also fosters deeper connections between key factors and entities within the banking ecosystem. Our AI agent serves as a valuable tool for providing timely insights into financial services, thereby empowering customers to make informed decisions."}]}