{"title": "BrainDecoder: Style-Based Visual Decoding of EEG Signals", "authors": ["Minsuk Choi", "Hiroshi Ishikawa"], "abstract": "Decoding neural representations of visual stimuli from electroencephalography (EEG) offers valuable insights into brain activity and cognition. Recent advancements in deep learning have significantly enhanced the field of visual decoding of EEG, primarily focusing on reconstructing the semantic content of visual stimuli. In this paper, we present a novel visual decoding pipeline that, in addition to recovering the content, emphasizes the reconstruction of the style, such as color and texture, of images viewed by the subject. Unlike previous methods, this \"style-based\" approach learns in the CLIP spaces of image and text separately, facilitating a more nuanced extraction of information from EEG signals. We also use captions for text alignment simpler than previously employed, which we find work better. Both quantitative and qualitative evaluations show that our method better preserves the style of visual stimuli and extracts more fine-grained semantic information from neural signals. Notably, it achieves significant improvements in quantitative results and sets a new state-of-the-art on the popular Brain2Image dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "Understanding neural representations in the brain and the information they encode is crucial for enhancing our knowledge of cognitive processes and developing brain-computer interfaces (BCIs) [1]. In particular, decoding and simulating the human visual system has emerged as a significant challenge. Recent advancements have led to substantial progress in visual decoding, allowing for the reconstruction of visual stimuli perceived by a subject during brain activity measurement. [2] [3] [4] [5] [6] [7] [8] [9]\nElectroencephalography (EEG) is a technique for recording brain signals, widely used due to its non-invasive nature, cost-effectiveness, and high temporal resolution. Although it has notable limitations [10] such as relatively lower spatial resolution as well as susceptibility to physiological artifacts and individual differences, conducting research based on EEG remains crucial for practical applications. The technique's accessibility and ability to capture real-time brain activity make it invaluable."}, {"title": "II. METHODOLOGY", "content": "We introduce a novel framework for reconstructing images viewed by an EEG subject, as illustrated in Fig. 1. It consists of three main components: A) Aligning EEG signals with CLIP image space, B) Aligning EEG signals with CLIP text space, and C) Combining the CLIP-aligned EEG representations for visual stimuli reconstruction."}, {"title": "A. EEG Alignment in Image Space", "content": "Prior work [17] [19] [20] has demonstrated the ability of CLIP image embeddings to facilitate both semantic and stylistic transfer when the generator model is conditioned accordingly. Building on these findings, our approach aims to extract image-related information from EEG signals by aligning them with CLIP image embeddings. To achieve this, we process the EEG signals and their corresponding ground truth images (i.e., the ones that the EEG subject was watching when the signal was taken) using an EEG encoder and a CLIP image encoder, respectively, and aim to correlate the outputs. Let $F_{image}$ be an encoder that processes the input EEG signal x, and $E_{image}$ the CLIP image encoder applied to the input image I. We call $F_{image}$ the EEG-image encoder because it is trained to align with the image as encoded by $E_{image}$. We employ Mean Squared Error (MSE) as the loss function to measure the similarity between the EEG and image representations:\n$\\mathcal{L}_{image} = MSE(F_{image}(x) - E_{image}(I)).$ (1)\nTo effectively encode the EEG data, we extend upon previous approaches [2] [21] [6] by utilizing an LSTM-based encoder architecture followed by fully connected layers."}, {"title": "B. EEG Alignment with Text Space", "content": "Recent approaches for visual brain signal decoding [11] [22] have sought to align brain signals with CLIP [15] text embeddings obtained from captions generated by pretrained image caption generators. However, since CLIP was trained on image-text pairs publicly available on the Internet with often short captions, those methods using longer generated captions, particularly with Stable Diffusion [16], have been less effective. Although Stable Diffusion allows up to 77 tokens as input, empirical evidence suggests that the effective token length of CLIP is considerably shorter [23]. Accordingly, we adopt a simpler labeling approach: we make the caption by appending the class label of the image to the text \"an image of\". We show empirically that this method improves performance over previous approaches and that more fine-grained information can be captured by the EEG-image encoder instead. We use the CLIP text encoder $E_{text}$ that embeds the caption C to train the EEG-text encoder $F_{text}$ that encodes the corresponding EEG signal x. As in the image alignment step, we use MSE as the loss function to quantify the similarity between the EEG and the text representations:\n$\\mathcal{L}_{text} = MSE(F_{text}(x) - E_{text}(C)).$ (2)\nSimilar to the image processing pipeline, an LSTM-based encoder is used for EEG signal encoding."}, {"title": "C. Visual Stimuli Reconstruction", "content": "After training the EEG-image and EEG-text encoders, we leverage the resulting EEG embeddings to generate images. Our method uses a pretrained latent diffusion model (e.g., Stable Diffusion [16]), with the EEG embeddings from both encoders serving as distinct conditioning inputs. This is achieved through a decoupled cross-attention mechanism [17]. We hypothesize that by aligning the EEG signals in CLIP image space, the EEG encoder can capture detailed semantics and style that may not be easily conveyed through text alone. This approach is analogous to the way latent diffusion models incorporate both text and image prompts as conditioning factors. The reconstructed visual stimuli are defined as:\n$\\hat{y} = SD(F_{i}(x), F_{t}(x))$ (3)\nHere, \u0177 represents the reconstructed image, and SD denotes the pretrained Stable Diffusion, conditioned on the outputs of both the EEG-image encoder $F_{image}$ and the EEG-text encoder $F_{text}$."}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "This section is divided into two main parts. We begin by detailing our experimental setup for training the EEG encoders. Following this, we present our findings and discuss various ablation studies."}, {"title": "A. Dataset", "content": "We utilize the Brain2Image [21] [2], an EEG-image pair dataset with 11,466 EEG recordings from six participants, for our experiments. These recordings were captured using a 128-channel EEG system as the participants were exposed to visual stimuli for 500 ms. The stimuli consisted of 2,000 images with labels spanning 40 categories, derived from the ImageNet dataset [24]. Each category included 50 easily recognizable images to ensure clarity in the participants' neural responses."}, {"title": "B. Implementation", "content": "For the EEG encoders, we extend from previous approaches [21] [2] [6] and use a 3-layered LSTM network with a hidden dimension of 512. The output of the network is then passed through a fully connected linear network with a BatchNorm [25] and LeakyReLU [26] activation function in between. Only the EEG encoders are trained in our framework, keeping the framework computationally efficient. We use the Adam [27] optimizer with a weight decay of 0.0001. The initial learning rate is set to 0.0003 and a lambda learning rate scheduler is used with a lambda factor of 0.999.\nIn order to align with CLIP image space, we follow the approach outlined in the IP-Adapter [17] framework, utilizing the CLIP-Huge model to process the images. For aligning EEG with CLIP text space, we process the captions using the CLIP-Large model which is used by Stable Diffusion 1.5. The captions are generated by concatenating \u201can image of\u201d with the class label. For ablation studies, we employ the LLaVA-1.5-7b model [28] for layout-oriented caption generation and BLIP [29] for general caption generation. For visual reconstruction, we employ Stable Diffusion version 1.5, aligning our method with recent results for fair comparison and we employ a PNDM scheduler [30] with 25 inference steps."}, {"title": "C. Evaluation Metrics", "content": "We employ the following metrics to objectively assess the performance of our framework. ACC: The N-way Top-K Classification Accuracy [8] [31] evaluates the semantic accuracy of the reconstructed images. We set N = 50 and K = 1. IS: The Inception Score [32] assesses the diversity and quality of the generated images. FID: Fr\u00e9chet inception distance [33] measures the distance from the ground truth images. SSIM: The Structural Similarity Index Measure [34] evaluates the quality of images. CS: CLIP Similarity [35] [11] reflects how well the generated images capture the semantic and stylistic content of the ground truth images."}, {"title": "D. Results", "content": "Fig. 2 presents sample outputs of BrainDecoder. Beyond capturing the high-level semantics, our method demonstrates the ability to retain fine-grained visual features, including color and texture. Notably, there is also a resemblance in the color composition of the background in addition to the main object's color. This capability is further illustrated in the example of the electric locomotive class. The object's color is depicted as light blue\u2014matching the visual stimuli\u2014despite the range of potential color variations. This demonstrates the model's ability to recover nuanced visual attributes with a high fidelity.\nThis is further demonstrated in Fig. 3, where we compare our results with prior studies. Notably, in the second image, our method is able to reconstruct not only the wooden texture of the chair, but the grass in the background as well, which was absent in the results by other methods.\nTable. I shows the quantitative results of BrainDecoder compared to baselines [2] [12] [11] [7]. We evaluate our methodology on 5 evaluation metrics in \u00a7III-C. BrainDecoder outperforms the state-of-the-art in both reconstruction fidelity and generation quality. Notably, BrainDecoder achieves a surprising 95.2% on the 50-way top-1 classification accuracy metrics, showing that the trained EEG encoders are able to extract rich information from the brain signals very well."}, {"title": "E. Ablation", "content": "We conduct an ablation study to understand the contributions of each component. Rows 3-5 of Table II show visual decoding with the EEG-image encoder yields a higher SSIM (0.2239) than with only the EEG-text encoder (0.1845). This supports our premise that aligning in CLIP's image space facilitates style transfer. Furthermore, the framework achieves the best performance when both encoders are used, indicating the complementary nature of the two encoders.\nAdditionally, we empirically show that captions generated by Vision Language Models (VLMs) are suboptimal for EEG-based visual decoding. We compare our label caption method with two VLMs: BLIP [29] and layout-oriented LLaVA [28]. A key challenge in image reconstruction from brain signals is preserving the visual layout. We hypothesize that associating EEG signals with detailed layout-oriented CLIP text embeddings might help. Using the LLaVA model, we generate layout-oriented captions following the instruction: \u201cWrite a description of the image layout. EXAMPLE OUTPUT: [object] is in the top left of the image, facing right.\" Fig. 4 shows example layout-oriented captions. Notably, rows 1-3 of Table II indicate that the simple label caption (\"an image of [class name]\") performs best, while layout-oriented captions (row 1) perform the worst. This further supports our premise that simple label captions are more effective for EEG encoders and complex prompts are harder for CLIP to fully interpret."}, {"title": "IV. CONCLUSION", "content": "Our research introduces BrainDecoder, a novel approach to image reconstruction from EEG signals that preserves both stylistic and semantic features of visual stimuli. By aligning EEG signals with CLIP image and text embeddings separately, we bridge the gap between neural representations and visual content. Our analysis demonstrates significant improvements over existing models, offering a richer interpretation of neural signals through the dual-alignment strategy."}]}