{"title": "UGMATHBENCH: A DIVERSE AND DYNAMIC BENCHMARK FOR UNDERGRADUATE-LEVEL MATHEMATICAL REASONING WITH LARGE LANGUAGE MODELS", "authors": ["Xin Xu", "Jiaxin Zhang", "Tianhao Chen", "Zitong Chao", "Jishan Hu", "Can Yang"], "abstract": "Large Language Models (LLMs) have made significant strides in mathematical reasoning, underscoring the need for a comprehensive and fair evaluation of their capabilities. However, existing benchmarks often fall short, either lacking extensive coverage of undergraduate-level mathematical problems or probably suffering from test-set contamination. To address these issues, we introduce UGMathBench, a diverse and dynamic benchmark specifically designed for evaluating undergraduate-level mathematical reasoning with LLMs. UGMathBench comprises 5,062 problems across 16 subjects and 111 topics, featuring 10 distinct answer types. Each problem includes three randomized versions, with additional versions planned for release as leading open-source LLMs become saturated in UGMathBench. Furthermore, we propose two key metrics: effective accuracy (EAcc), which measures the percentage of correctly solved problems across all three versions, and reasoning gap (A), which assesses reasoning robustness by calculating the difference between the average accuracy across all versions and EAcc. Our extensive evaluation of 23 leading LLMs reveals that the highest EAcc achieved is 56.3% by OpenAI-01-mini, with large A values observed across different models. This highlights the need for future research aimed at developing \"large reasoning models\" with high EAcc and A = 0. We anticipate that the release of UGMathBench, along with its detailed evaluation codes, will serve as a valuable resource to advance the development of LLMs in solving mathematical problems.", "sections": [{"title": "1 INTRODUCTION", "content": "Mathematical reasoning and problem-solving are critical components of human intelligence, and the ability of machines to understand and address mathematical challenges is crucial for their deployment (Ahn et al., 2024; Liu et al., 2024; He et al., 2024a). Solving mathematical problems with machines has been a significant research topic in natural language processing since the 1960s (Bobrow et al., 1964), initially focusing on elementary math word problems (Patel et al., 2021; Wang et al., 2017; Ling et al., 2017; Welbl et al., 2017; Cobbe et al., 2021). With the advent of Large Language Models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Team et al., 2023; Anthropic, 2024), interest in using these advanced technologies to solve math problems has continued to grow. Researchers are exploring various approaches to improve the mathematical reasoning capabilities of LLMs, including prompting (Wei et al., 2022; Wang et al., 2022; Kojima et al., 2022; Zhang et al., 2023; Zheng et al., 2023), supervised fine-tuning (Yue et al., 2023; Yu et al., 2023; Gou et al., 2023; Li et al., 2024a; Tong et al., 2024; Yan et al., 2024), and continued pretraining (Lewkowycz et al., 2022; Shao et al., 2024; Azerbayev et al., 2023). Consequently, LLMs have become increasingly capable of solving complex mathematical problems (Hendrycks et al., 2021; Ahn et al., 2024)."}, {"title": "2 RELATED WORK", "content": "Mathematical Benchmarks. Mathematical reasoning is increasingly vital for assessing the fundamental reasoning capabilities of LLMs (Ahn et al., 2024). Several math-related datasets have been proposed in this area (Koncel-Kedziorski et al., 2016; Amini et al., 2019; Hendrycks et al., 2020; Cobbe et al., 2021; Hendrycks et al., 2021; Chen et al., 2022). Among these, GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) are the most representative datasets for elementary and high school-level math reasoning, respectively. However, as modern LLMs become increasingly powerful, these benchmarks lack sufficient challenge forlatest LLMs. Notably, 01 (OpenAI, 2024b) achieves 94.8% accuracy on MATH, which was previously considered highly complex. To better"}, {"title": "Dynamic Benchmarks for Mathematical Reasoning.", "content": "Test set contamination, wherein benchmark test data appear in a newer model's training set, significantly challenges fair LLM evaluation by artificially inflating performance (Deng et al., 2023; Dong et al., 2024; Golchin & Surdeanu, 2023; Roberts et al., 2023). Since pretraining data often involve large corpora scraped from the Internet, any static benchmark risks data contamination (Zhang et al., 2024; Qian et al., 2024). To mitigate this, recent benchmarks maintain private test sets (Zhang et al., 2024; Huang et al., 2024), requiring anyone who wishes to evaluate their models to submit predictions for centralized processing before publishing results on their leaderboards. However, this process can be inefficient and lacks transparency for error analysis (Qian et al., 2024). An alternative is releasing dynamic benchmarks that are periodically updated (Srivastava et al., 2024; Qian et al., 2024; White et al., 2024). For example, Srivastava et al. (2024) have functionalized a subset of the MATH dataset to regenerate new versions of the test set by reassigning variable values. In this vein, our UGMathBench is a dynamic benchmark featuring different sampled values for variables by setting distinct random seeds. Currently, we release three snapshots for each question in UGMathBench and plan to release new versions if leading open-source LLMs reach accuracy saturation."}, {"title": "3 THE UGMATHBENCH BENCHMARK", "content": null}, {"title": "3.1 UGMATHBENCH OVERVIEW", "content": "We introduce the UGMathBench, a dynamic undergraduate-level mathematical reasoning benchmark designed to thoroughly and robustly assess the mathematical reasoning ability of LLMs. UG-MathBench enables fair evaluation through randomized versions of single problems. Unlike GSM1K (Zhang et al., 2024), our test set labels are publicly available, facilitating efficient evaluation and effective error analysis. UGMathBench covers fifteen core subject areas in undergraduate-level mathematics, including single-variable calculus, multivariable calculus, differential equations, probability, and more, encompassing a total of 111 specific topics (details in Appendix A.2). UGMathBench comprises a set of 5,062 problems in 3 different randomized snapshots with 10 different answer types (see Appendix A.3). These answer types range from atomic types (e.g., numerical value, expression) to compound types (e.g., multiple answers in ordered or unordered lists), setting UGMathBench apart from many other math-related benchmarks that focus primarily on a single answer with an atomic type. We randomly select 100 problems to examine student performance using our grading system' records, with each problem being completed by varying numbers of students ranging from 99 to 1,537. The average accuracy on the first attempt is 56.5%, while the average accuracy on the final attempt increased to 96.1%."}, {"title": "3.2 UGMATHBENCH CREATION", "content": "Our UGMathBench creation process has three distinct phases: data collection, data cleaning & deduplication, and answer type annotation."}, {"title": "Data Collection.", "content": "The dataset for UGMathBench is carefully compiled from the online grading system of our institute's undergraduate courses (see Appendix B.1). All problems in our system are generated by programs that specify particular variable values to ensure correctness and maintain the same solution (see Appendix A.1). We gather all mathematics-related problems, resulting in 16 subjects and 111 topics in total. To prevent student cheating, our grading system offers randomized versions of most problems (see Figure 1), similar to the variable disturbance approach in Qian et al."}, {"title": "3.3 EVALUATION METRICS", "content": "We denote the set of test examples in UGMathBench by $D$ with a specific test example denoted as $e_i$, where $i$ represents the index of the example. Each example $e_i$ consists of questions presented in different randomized versions: $q_1^i, q_2^i, ..., q_V^i$, where $V$ is the total number of versions. The corresponding ground-truth answers for these versions $q_1^i, q_2^i, ..., q_V^i$ are denoted by $a_1^i, a_2^i, ..., a_V^i$. The answer generated by an LLM $M$ for a specific version of the question in the $i$-th test example is denoted by $M(q_v^i)$. Inspired by Srivastava et al. (2024), we define the following metrics to evaluate the true mathematical reasoning ability of LLM $M$ in UGMathBench.\nAccuracy of Version v $Acc_v$, is defined as the average accuracy of model $M$ on the set of questions with version $v$ in $D$:\n$Acc_v = \\frac{\\sum_{e_i \\in D} I[M(q_v^i) = a_v^i]}{|D|}$\nwhere $I$ is an indicator function and $|D|$ denotes the number of examples in UGMathBench. It assesses the performance of an LLM on the specific version $v$ from UGMathBench.\nAverage Accuracy AAcc is defined as the mean of all Acc:\nAAcc = $\\frac{\\sum_{v=1}^{V} Acc_v}{V}$\nThis metric evaluates the performance across all versions of the questions.\nEffective Accuracy EAcc is defined as the accuracy in solving a test example $e_i$ across all its $V$ versions:\nEAcc = $\\frac{\\sum_{e_i \\in D} I[M(q_v^i) = a_v^i, \\forall v \\in \\{1, 2, ..., V\\}]}{|D|}$\nIf a model is able to solve a test case using proper reasoning, it should correctly solve this problem for all randomized versions. Thus, effective accuracy measures the fraction of test cases correctly solved across all versions $V$. It measures true reasoning of test cases in UGMathBench."}, {"title": "Reasoning Gap", "content": "$A$ is defined as the percentage decrease between $AAcc$ and $EAcc$. It provides a measure of the robustness of reasoning, with \u2206 = 0 being true reasoning with high robustness.\nRobustness Efficiency (RE) is defined as the ratio of the Reasoning Gap (A) to the EAcc, expressed as RE = $\u2206$/EAcc. This metric evaluates the extent of the reasoning gap relative to the model's effective reasoning ability (i.e., EAcc). RE captures robustness by taking the effectiveness of mathematical reasoning into account, with lower values indicating superior performance in adapting to variations across different versions of problems in UGMathBench. Achieving a higher EAcc and a lower A results in a more favorable (lower) RE, reflecting improved robustness relative to \"true\" reasoning ablility of LLMs."}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Evaluated LLMs. Our evaluation covers 23 leading LLMs, including closed-source commercial LLMs and open-source LLMs. Based on our UGMathBench, we provide a thorough evaluation of the mathematical reasoning capabilities of current LLMs. The evaluated LLMs are listed below:\nFor proprietary LLMs, we select OpenAI-01-mini (OpenAI, 2024b), GPT40 (OpenAI, 2024a), GPT40-mini (OpenAI, 2024a), and Claude-3-Opus (Anthropic, 2024).\nFor open-source general-purpose LLMs, we evaluated the LLaMA-3-Instruct series (8B, 70B) (AI@Meta, 2024), Qwen2-Instruct (7B, 72B)(Yang et al., 2024a), Yi-1.5-Chat (6B, 9B, 34B) (AI et al., 2024), Mistral-7B-Instruct (Jiang et al., 2023), Mistral-Nemo-Instruct-2407 (Mistral, 2024c), Mistral-Small-Instruct-2409 (Mistral, 2024d), Mistral-Large-Instruct-2407 (Mistral, 2024b), DeepSeek-MOE-16B-Chat (Dai et al., 2024), and DeepSeek-V2-Lite-Chat (DeepSeek-AI, 2024).\nWe also include some specialized math LLMs: DeepSeekMath-7B (-RL, -Instruct) (Shao et al., 2024), Qwen2-Math (7B, 72B) (Yang et al., 2024b), Mathstral-7B (Mistral, 2023), and NuminaMath-7B-CoT (Beeching et al., 2024)."}, {"title": "Evaluation Settings.", "content": "We employ Acc to evaluate the average performance of version v, AAcc to measure the average performance across all versions, EAcc to quantify true reasoning, and reasoning gap A to assess the robustness of reasoning (see Section 3.3). To remove the effect of sensitivity of few-shot prompts (Lu et al., 2021; Ma et al., 2023), all our experiments use zero-shot prompts, tailored to different answer types for better answer extraction and rule-based matching. Detailed prompts are given in Appendix C.2. We use vLLM\u2074\u00b9 to speed up the evaluation process. To maintain consistency in evaluations and facilitate reproduction, we set the maximum output length to 2,048 tokens and employ a greedy decoding strategy with temperature 0."}, {"title": "4.2 MAIN RESULTS", "content": "The overall experiment results are shown in table 4. We have the following key observations:\nUGMathBench is a challenging benchmark for evaluating the mathematical reasoning ca-pabilities of LLMs. Even LLMs with the most advanced reasoning abilities, OpenAI-01 (mini version), achieve only 56.3% EAcc on UGMathBench, while most open-source LLMs, including most specialized mathematical models, struggle to reach a 30% EAcc. Compared to commonly used mathematics benchmarks like MATH (Hendrycks et al., 2021), UGMathBench proves to be more challenging. For instance, OpenAI-01-mini achieves 90% on MATH (v.s. 56.3% on UGMathBench).\nEven leading LLMs still have inconsistencies when solving problems with multiple versions. LLMs with an AAcc greater than 20% display a reasoning gap A exceeding (or near) 10%. All LLMs demonstrate extremely high RE on UGMathBench, with values ranging from 20.78% to 196.6%. Among the five models with the lowest RE, three of them are from OpenAI (OpenAI-01-mini: 20.78%; GPT-40: 20.89%; Mistral-Large-Instruct: 24.36%; Qwen2-Math-72B-Instruct: 24.39%;"}, {"title": "5 ANALYSIS", "content": "In this section, we conduct an in-depth analysis of the performance of the 23 LLMs evaluated on UGMathBench by investigating the following research questions:\nWhat is the relationship between EAcc and Acc? (Section 5.1)\nHow do model size and model series influence performance on UGMathBench? (Section 5.2)\nHow do LLMs perform across different subjects, difficulty levels, and, different topics on UGMathBench? (Section 5.3)\nWhat are the typical response errors made by the best-performing LLM (OpenAI-01-mini), and how are they distributed? (Section 5.4)"}, {"title": "5.1 RELATIONSHIP BETWEEN EACC AND ACC", "content": "To investigate the relationship between EAcc and Acc, scatter plots of EAcc against each Accv are shown in Figure 2. From Figure 2, we have the following conclusions:\nAll LLMs fall below the diagonal lines. Each LLM evaluated is represented as a point in the subfigures of Figure 2, plotted on the axes of (Accv, EAcc). Although LLMs exhibit small variations in accuracy across different versions, they consistently demonstrate a lower EAcc than Accv, which suggests that the accuracy of individual versions is insufficient for assessing the reasoning capabilities of LLMs. By considering EAcc alongside accuracy, we can gain a better understanding of how LLMs perform when solving problems that have different randomized versions. The discrepancy between EAcc and Acc highlights a new inconsistency mode (Ahn et al., 2024) of current LLMs: they may become inconsistent in their answers when the problem is slightly altered.\nThere is an apparent trend that a high EAcc consistently leads to a high Acc. A model with high EAcc is more effective in handling variable disturbances, resulting in high accuracy for each version. However, as EAcc becomes increasingly large, the difference between Accv and EAcc tends to increase until it stabilizes around 10%."}, {"title": "5.2 THE EFFECT OF MODEL SIZE AND MODEL SERIES", "content": "Figure 3 has shown how EAcc and 2-RE changes with the parameter size for the 23 LLMs evaluated. We can observe that:\nLLMs within the same series have shown steady improvement as the parameter size increases. When the model size increases from 7B to around 100B, EAcc substantially improve and RE steadily decrease for Qwen-Chat, Qwen-Math, Mistral, Deepseek-Chat, LLaMA-3-Instruct, and Yi-Chat series, indicating a steady improvement in performance in effectiveness and robustness of mathematical reasoning.\nSpecialized mathematical LLMs typically outperform their general-purpose counterparts. For example, the Qwen2-Math series achieves significantly higher EAcc and lower RE than its general-purpose chat LLMs with the same model size. Among all 7B specialized mathematical LLMs, Qwen2-Math-7B-Instruct ranks first surpassing DeepSeek-Math-RL (second best) by a large margin."}, {"title": "5.3 PERFORMANCE ACROSS DIFFERENT SUBJECTS AND DIFFICULTY LEVELS", "content": "Figure 4a shows the average EAcc of different models in each subject. Figure 4b shows the averaged EAcc of all subjects with respect to different levels of difficulty. The detailed performances across different subjects and topics for each model can be found in Appendix D and F."}, {"title": "5.4 ERROR ANALYSIS", "content": "We perform a comprehensive error analysis on OpenAI-01-mini by randomly selecting 100 problems, each having at least one incorrectly solved version (yielding a total of 300 versions). As shown in Figure 5a, there are 231 incorrect versions, and OpenAI-01-mini failed to solve 56% of the problems across all versions. We then categorize these errors into six types, as illustrated in Figure 5b. Calculation errors, including both numerical and expression errors, represent the largest category, with several examples provided in Appendix E. We find that OpenAI-01-mini tends to streamline its outputs to avoid generating too long responses, sometimes leading to erroneous results. Additionally, we encounter some \"bad questions\" that primarily arise due to overly complex structures (e.g. containing long tables) or inadequately described (e.g. undefined variables in previous problems). This is because our homework grading system (see Appendix B.1) is designed for students with a user-friendly interface, and some problems may not be suitable for LLM to solve. In our sample of 300 versions, 19 were identified as \"bad problems,\" giving us an estimated occurrence of approximately 2.7% in our UGMathBench. These problems do not impact our main claims, as no LLMs are able to solve these \"bad questions.\" We will refine these types of problems to make them more suitable for LLM evaluation in the future. No answer cleaning process is perfect, and improved evaluation codes continue to be released in MATH (Hendrycks et al., 2021). We will also actively update our evaluation repository to improve its quality.\nNotably, we have found that even when OpenAI-01-mini solves a problem incorrectly among all its randomized versions, the error types can be different. As shown in Figure 5a, there are around 16.1% such inconsistent errors among the 100 problems sampled."}, {"title": "5.5 FURTHER ANALYSIS", "content": "About Self-Improvement. To examine how LLMs perform with refinement on UGMathBench, we conducted experiments using Progressive-Hint Prompting (PHP) (Zheng et al., 2023) (see Appendix G). Detailed results can be found in Table 32 in Appendix G. Although PHP improves EAcc and AAcc for most LLMs, the enhancements are not significant, indicating considerable room for future development. The fine-grained results in Table 33 suggest that the impact of refinement for GPT-40 varies across different subjects. For instance, PHP improves GPT-40's performance in abstract algebra by 7.14%, yet reduces its performance in probability by 2.08% in terms of EAcc. Our UGMathBench serves as an excellent testing ground for future research into refinement methods for solving undergraduate-level mathematics with LLMs."}, {"title": "Reasoning Gap and Test Set Contamination.", "content": "To explore how models specifically overfitting to a particular variation affect the reasoning gap, we mixed a portion of the test set from one version with MetaMathQA (Yu et al., 2023) and then conducted supervised fine-tuning (SFT) Llama-3-8B on this data. Details of the SFT process are provided in Appendix H, and the results are presented in Table 5. As the proportion of the test set included in the training data increases, the reasoning gap (A) also becomes more pronounced. This study serves as an initial investigation into test set"}, {"title": "6 CONCLUSION", "content": "Current mathematical benchmarks are often inadequate, lacking comprehensive coverage of undergraduate-level math problems or being susceptible to test-set contamination. To fill these gaps, we propose UGMathBench, a diverse and dynamic benchmark for undergraduate-level mathematical reasoning. Our fine-grained analysis has pointed out the potential inconsistencies when LLMs encounter problems with slightly different versions. We hope that our UGMathBench can contribute to future development of \"true\" reasoning LLMs."}, {"title": "LIMITATIONS", "content": "This work has several limitations. First, UGMathBench focuses on text-only reasoning, whereas some undergraduate-level math problems require images for their solutions. Developing a multimodal benchmark for undergraduate-level mathematics will be future work. Second, UGMathBench is designed as an English-language benchmark. Extending UGMathBench to support multiple languages could be an interesting avenue for future research. Third, the number of problems in certain subjects is limited. Expanding these subjects would be valuable."}, {"title": "A DETAILED STATISTICS OF UGMATHBENCH", "content": null}, {"title": "A.1 INTRINSIC MECHANISM OF DYNAMIC PROBLEMS", "content": "A key feature of our UGMathBench is its dynamic nature. In this appendix, we detail how this is achieved. All problems in our homework grading system (see Appendix B.1) are stored as programs written in Program Generation, an established programming language for mathematics. These programs strictly specify conditions to ensure that the generated variations of each problem do not fundamentally alter their nature, required solution approach, difficulty level, or the underlying knowledge points. One such program is shown in Listing 1, and two different versions of this problem it generates is given in Figure 7. The relationship between the first and second variables is defined by\n$expnt = -1+ 2 * a;$, which maintains the consistency of the concepts, techniques, and solutions involved in different versions of each problem."}, {"title": "A.2 DISTRIBUTION OF PROBLEMS", "content": "Our UGMathBench covers various subjects in undergraduate-level mathematics. The detailed topics and the number of subtopics of each topic across different subjects are listed in Table 6 and 7. There are 111 topics and 583 subtopics across 16 subjects in total. Furthermore, the distribution information of our benchmark on different subjects and difficulty level is presented in Table 8. Note that there are problems with missing difficulty level in our online homework grading system (see Appendix B.1) and we remain as is for consistency. The keywords of our UGMathBench are shown in Figure 6. The detailed results across different subjects and topics are discussed in Appendix D and F."}, {"title": "A.3 ANSWER TYPES", "content": "By carefully reviewing a large collection of problems and referring to various past benchmarks (He et al., 2024a; Huang et al., 2024), we classify all answers to be two main categories: atomic and compound. There are 8 atomic types and 2 compound types. Each compound type is composed of a list of atomic ones. These types are designed to encompass a wide range of problems. Detailed definitions for each answer type can be found in Table 9."}, {"title": "BUGMATHBENCH CREATION", "content": null}, {"title": "B.1 DATA SOURCE", "content": "UGMathBench originates from questions in the online homework grading system of our institute, utilizing WebWork, an open-source online platform licensed under GNU. Widely employed for assigning mathematics and science homework in educational settings, WebWork benefits from collaborative contributions by educators across various institutions.\nEach question in WebWork is tagged with keywords related to concepts and difficulty level based on Bloom's taxonomy, which helps simplify statistical analysis and cognitive assessment. To prevent cheating from each other, WebWork is able to generate tailored problem sets with different random seeds, making it popular among educational institutions."}, {"title": "B.2 DEDUPLICATION", "content": "After converting and cleaning all the problems, we perform deduplication within each subject. More specifically, we adhere to the following steps: First, we transform each question into a vector with dimension 1536 using the embedding model text-embedding-ada-002, which is the most capable 2nd generation embedding model of OpenAI7. We then calculate pairwise cosine similarities using the embeddings in the previous step. Finally, a threshold is selected based on manual inspection within each subject, and problems that have a cosine similarity higher than that threshold with existing problems are excluded. The thresholds and the number of questions filtered out for different subjects are presented in Table 10. We use subject-agnostic thresholds and filter out 9,382 questions in total."}, {"title": "C DETAILED EXPERIMENTAL SETUP", "content": null}, {"title": "C.1 EVALUATED LLMS", "content": "A variety of LLMs are covered in our evaluation, including closed-source commercial models and open-source models, general-purpose models and models dedicated for math problem solving. Closed-source LLMs are as follows:\n01-preview (OpenAI, 2024b): An early preview of OpenAI's ol model, designed to reason about hard problems using broad general knowledge about the world. We used ol-preview-2024-09-12 for our evaluation.\nGPT-40 (OpenAI, 2024a): GPT-40 is multimodal, and has the same high intelligence as GPT-4 Turbo but is much more efficient.\nClaude-3-Opus (Anthropic, 2024): Anthropic's most intelligent model, claimed to outperform its peers on most of the common evaluation benchmarks for AI systems."}, {"title": "We evaluated the following open-source general-purpose LLMs on our benchmark:", "content": "Llama-3-Instruct (AI@Meta, 2024): LLaMA 3 Community License."}, {"title": "C.2 EVALUATION PROMPTS", "content": "The evaluation prompts in our experiments are given in Table 11, where detailed answer type descriptions are given in Table 12. Following He et al. (2024a); Huang et al. (2024), these prompts are specially designed for different subjects and answer types for better evaluation. Note that, for chat models, we will apply chat template for better evaluation."}, {"title": "D RESULTS ACROSS DIFFERENT SUBJECTS", "content": "The detailed results across different subjects are given in Table 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, and 28. From the results, we have the following observations:\nOpenAI-01-mini achieves the best results across nearly all subjects, although GPT-40 sometimes excels in terms of RE.\nFor open-source LLMs, Qwen-2-Math-72B-instruct achieves the best results in almost all subjects. However, Mistral-Large-instruct-2407 outperforms Qwen-2-Math-72B-instruct in Algebra.\nSome LLMs even achieve zero EAcc in certain subjects. For instance, Mistral-7B-Instruct get zero EAcc in Set Theory and Logic, and seven LLMs exhibit zero EAcc in Abstract Algebra.\nThe variation in the reasoning gap differs significantly across subjects, providing more fine-grained information on how different LLMs perform across various domains."}, {"title": "E ERROR ANALYSIS", "content": "We perform error analysis in Section 5.4, and here we showcase several examples of various error types in Table 29, 30, and 31.\nThe distribution of the relative error for OpenAI-01-mini numerical values is illustrated in Figure 9. We excluded numerical answers identical to the ground truth, as their logarithmic relative error would be negative infinity."}, {"title": "F RESULTS ACROSS DIFFERENT TOPICS", "content": "The performances of differerent LLMs on 20 topics are shown in Figure 10, 11, 12 and 13. We observe that different LLMs exhibit varying performance patterns across these topics, and even models within the same family show differences in their rankings."}, {"title": "G REFINEMENT RESULTS", "content": "Progressive-Hint Prompting (PHP) (Zheng et al., 2023) is a technique designed to enhance automatic, iterative interactions with LLMs. PHP uses previously generated answers as hints to progressively guide users toward the correct solutions. In our experiments, we employ the zero-shot manner to ensure a fair comparison with the primary experiments in Table 4, which helps to assess the impact of refinement on the performance of LLMs in solving undergraduate-level mathematical problems. We have set the maximum number of interaction rounds to five. To save cost, we only experiment with GPT-40 for closed-source LLMs. The results are presented in Table 32. While PHP can improve AAcc and EAcc in most cases, the improvements are not substantial. There remains considerable potential for enhancing the mathematical reasoning abilities of LLMs in solving undergraduate-level"}, {"title": "H REASONING GAP AND TEST SET CONTAMINATION", "content": "Given the limited number of examples in the test set for one version (in terms of SFT), it is necessary to mix the test set with general mathematical SFT data. For our experiments, we adopt MetaMathQA (Yu et al., 2023), a high-quality SFT dataset for math word problems, which includes 395,000 training examples. We use Llama-3-8B as our base model and set the maximum output token length to 4096. Following Tong et al. (2024), we set the learning rate to 5e-5, use a warmup ratio of 0.03, adopt cosine decay, and train the model for one epoch. Training one model takes approximately three hours on four A100 GPUs. The results are presented in Table 5. We set the proportion of the test set (from one version) for SFT to 5%, 10%, 15% and, 20% and see how the reasoning gap varies. As the proportion of the test set included in the training data increases, the reasoning gap (\u25b3))also becomes more pronounced. This study serves as an initial investigation into test set contamination during the SFT stage. It is important to note that contamination at the pre-training stage is also a significant area of interest (Razeghi et al., 2022; Jiang et al., 2024)."}]}