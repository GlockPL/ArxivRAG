{"title": "DateLogicQA: Benchmarking Temporal Biases in Large Language Models", "authors": ["Gagan Bhatia", "MingZe Tang", "Cristina Mahanta", "Madiha Kazi"], "abstract": "This paper introduces DateLogicQA, a benchmark with 190 questions covering diverse date formats, temporal contexts, and reasoning types. We propose the Semantic Integrity Metric to assess tokenization quality and analyse two biases: Representation-Level Bias, affecting embeddings, and Logical-Level Bias, influencing reasoning outputs. Our findings provide a comprehensive evaluation of LLMs' capabilities and limitations in temporal reasoning, highlighting key challenges in handling temporal data accurately. The GitHub repository for our work is available at https://github.com/gagan3012/EAIS-Temporal-Bias", "sections": [{"title": "1 Introduction", "content": "Accurate temporal reasoning is essential for real-world applications like event planning and historical questions. However, biases in Large Language Models (LLMs) can lead to misinterpretations or errors in date-related tasks. Understanding these biases is essential for precisely handling numerical structures and contextual meanings, making temporal reasoning ideal for identifying and analysing biases in tokenization, representation, and logical reasoning.\nA significant source of these biases originates from the tokenization process. While tokenizers divide the text into subword units, inconsistencies in tokenizing dates can disrupt reasoning tasks. This can lead to two types of biases: Representation-Level Bias, caused by inconsistencies in embeddings affecting semantic structures of dates, and Logical-Level Bias, where correct tokens do not yield accurate outputs due to misaligned internal processing. Together, these biases highlight the challenges LLMs face in preserving the integrity and interpretability of temporal data across diverse formats and contexts."}, {"title": "2 Related Works", "content": "Impact of Tokenization on Language Models\nTokenization significantly affects the efficiency and reasoning abilities of large language models (LLMs). Research by Gu et al. (2024) and Goldman et al. (2024) highlights that tokenizers with higher compression rates enhance representation efficiency, particularly in smaller models. However, Schmidt et al. (2024) argue that effective tokenization also depends on pre-tokenization and vocabulary design. Studies like Ahia et al. (2023) show that poorly tokenized languages face performance and fairness issues. Furthermore, choices in tokenization impact reasoning; Zhang et al. (2024) and Singh and Strouse (2024) indicate that numerical tokenization can lead to errors in arithmetic and counting tasks. Rajaraman et al. (2024), Alberts et al. (2024), Minixhofer et al. (2024), and Gastaldi et al. (2024) show how well-designed tokenizers improve sequence pattern modelling and numerical reasoning through advanced embedding methods. Our study extends this work by examining tokenization's role in handling diverse date formats for temporal reasoning.\nTemporal Reasoning in LLMS Temporal reasoning poses challenges for LLMs due to inherent biases. Zhu et al. (2024) discussed \"nostalgia bias\" (favouring outdated knowledge) and \"neophilia bias\" (speculative future predictions), while Tan et al. (2023b) observed inconsistent generalisation across different time periods. Structured approaches like temporal graphs (Xiong et al., 2024a) and synthetic datasets (Fatemi et al., 2024) enhance performance by explicitly encoding temporal relationships. Additionally, tokenization critically affects temporal reasoning; Zhao et al. (2024) found that temporal misalignment hampers accuracy, and Kishore and He (2024) identified inductive biases in models like GPT-3.5 and GPT-4. Su et al. (2024a) propose task-agnostic approaches to enhance temporal reasoning, while Gastaldi et al. (2024) and Rajaraman et al. (2024) link tokenization to reasoning performance. By analysing how tokenization strategies affect temporal reasoning, especially for date formats, our work fills a gap in understanding the interplay between tokenization and temporal task performance."}, {"title": "3 DateLogicQA", "content": "We introduce DateLogicQA, a dataset designed to explore how LLMs handle dates in various formats and contexts to tokenize, interpret, and reason with them. It consists of 190 questions divided into four categories: commonsense, factual, conceptual, and numerical. Each category features one of seven date formats across three temporal contexts: past, present, and future. This systematic variation allows for an in-depth analysis of LLMs\u2019 performance with temporal information.\nObjective and Purpose The dataset aims to assess LLMs' tokenization and understanding of dates, as errors can lead to interpretative biases. By embedding dates within questions, we evaluate context-rich date interpretation, simulate real-world scenarios where dates carry contextual significance, and test LLMs' ability to extract and interpret date information accurately."}, {"title": "4 Methodology", "content": "The study proposes three interests to investigate temporal bias in models: tokenization process, temporal task capability, and internal computation across different LLMs."}, {"title": "4.1 Semantic Integrity", "content": "This experiment targets the tokenization process in different LLMs to identify how it influences the semantic interpretation of dates when presented in different formats. We specifically focus on the Semantic Integrity Metric, which measures the extent to which the original semantic meaning of a date is preserved after tokenization. Several key highlights are observed, such as how a single date input is presented after being tokenized, the extent of semantic preservation, and the ability to generalise across different date representations. These findings provide valuable insights into the tokenization process and its impact on temporal reasoning in LLMs.\nSemantic integrity evaluates how well the tokenized date output maintains its original meaning and structure. The semantic integrity score ranges from 0 to 1. A higher score - closer to 1 - indicates that the date segmentation is nearly accurate, better preserving the intended structure and information. In contrast, a score closer to 0 indicates an inadequately tokenized date structure. The formula for calculating semantic integrity is as follows:\n$SI = max (0, min (1,1 \u2013 P \u2013 S \u2013 T \u2013 R))$"}, {"title": "4.2 Human-Led Temporal Bias Assessment", "content": "Understanding temporal contexts is crucial for analysing events over time. This includes grasping temporal references like \"How many years has it been since...\" (Past) and \"What will the contract's last day be...\" (Future), along with the maintenance of logical chronological order and adaptation to changes in context. For large language models, this capability is vital for tasks such as historical inquiries, time-sensitive query handling and predictions about future events. Assessing biases in temporal reasoning is essential for accuracy across various applications. We utilized the dataset referenced in Section 3.\nWe conduct a human evaluation to assess the temporal bias of LLMs as automated methods may exhibit inherent biases that affect results, ultimately undermining the evaluation's purpose. This methodology provides a more reliable analysis, identifying outliers that respond accurately without fully comprehending temporal aspects. Instead, it relies on contextual clues or learned patterns acquired during training or through retrieval-augmented generation.\nModel responses are categorised based on colours in Figure 2, representing levels of temporal understanding. Dark Orange () denotes incorrect answers or temporal hallucinations from failure to tokenize dates or grasp context. Light Orange () reflects Representation-Level Temporal Bias, where the model tokenizes dates inaccurately but reaches the correct answer through logical reasoning. This suggests that some internal reasoning within the model compensates for misunderstanding the date format. Light Teal () signifies Logical-Level Temporal Bias, where the model tokenizes correctly but misapplies logic due to misattributing events or calculation errors. Finally, Dark Teal () denotes correct answers, indicating successful tokenization and logical reasoning. This illustrates a complete understanding of the question."}, {"title": "4.3 Understanding Temporal Bias", "content": "We investigate potential biases in the internal embedding space and softmax computations of large language models (LLMs) when processing texts with different temporal references, such as past, present, and future contexts. Temporal biases in LLMs fall into two main types: Representation-Level Temporal Bias indicates significant differences in internal embeddings across time references, revealing inconsistencies in encoding semantic information. In contrast, Logical-Level Temporal Bias occurs when output probabilities vary for semantically identical inputs due to changes in temporal references.\nWe established a controlled experimental framework to quantify these biases, analysing embeddings and softmax outputs across three temporal categories: past, present, and future. We measured representation-level biases using cosine similarity between averaged embeddings, with lower similarity indicating greater divergence. We assessed logical-level biases using KL divergence for softmax distributions, where higher divergence reflects substantial probability differences. Additionally, we examined sensitivity to seven date formats from"}, {"title": "5 Results", "content": "5.1 Impact of tokenizers"}, {"title": "5.2 Temporal Reasoning Analysis", "content": "Temporal reasoning, including processing and drawing inferences from historical and future dates, is one of the most challenging tasks for large language models. The current study investigates whether there are any differences in LLM performance when reasoning with historical dates, such as \"July 20, 1969\", and future dates, such as \"January 1, 2050\". To this end, we present the testing of 12 state-of-the-art LLMs using a question-answer dataset encompassing different date formats and various temporal contexts. This paper examines their skills in tokenization, comprehension, and inference on dates. We classify the answers into four categories based on their accuracy and treatment of the dates and logical structure involved, thereby providing a systematic evaluation framework.\nIn order to ensure that the assessment is robust, four human annotators, each with at least four years of experience in computer science, evaluated the responses across the four categories. The labelling achieved a high inter-annotator agreement with a Cohen's kappa (K) score of 0.80, confirming the reliability of the evaluation framework. These results evidence two critical areas where LLMs shine and their struggles, giving further information about their strengths and limitations concerning temporal reasoning.\nPerformance of Selected LLMs The evaluation of 12 language models, accessed through Hugging Face and OpenAI APIs, provided a comprehensive overview of their performance on temporal reasoning tasks. Small models like Llama-3.2-3B (Dubey et al., 2024) and Phi-3.5-mini (Abdin et al., 2024) gave bad performances, with 58% and 66% incorrect answers, respectively. Due to their restricted processing and resources, these models performed poorly in tokenization and reasoning. Mid-sized models, including Mistral-7B (Jiang et al., 2023), Llama-3-8B (Dubey et al., 2024), and Llama-2-7B (Touvron et al., 2023b), demonstrated a more moderate improvement. They had trouble with complex reasoning problems, although they were able to improve their tokenization accuracy. Larger models, including Llama-3-70B (Dubey et al., 2024), Qwen2.5-72B (Yang et al., 2024), and Command R+ (Cohere, 2024), were more robust in their performance, especially in date interpretation and logical reasoning. However, there were inconsistencies in specific formats. Proprietary models, including GPT-3.5 (Brown et al., 2020), GPT-4-turbo (OpenAI et al., 2023), GPT-4o, and GPT-4o-mini (OpenAI et al., 2024) outperformed all the rest, with GPT-4-turbo leading on correct responses with 63% and the lowest rate of incorrect answers at 16%. These results emphasise that model size, architecture, and diversity of pretraining data all bear on performance related to temporal reasoning tasks.\nPerformance Based on Date Formats The format of the date had a significant impact on model performance. Models performed best for formats that included clear separators and natural language cues, such as \"YYYY, Mon DD\" with 57% correct and \"DDMonYYYY\" with 54% correct. The poorest performance was from formats like \"YYYY/DD (Julian)\" and \"DD/YYYY (Julian)\", with only 31% and 34% correct, respectively, since the representation is less common and more complex in tokenization. This trend indicates format standardisation's apparent relevance in improving date processing efficiency in LLMs.\nPerformance Across Temporal Contexts Temporal context also mattered a lot. Models were better with future dates, 50% correct, compared to historical dates, 44%, and present dates, 35%. This runs contrary to the expectations and may point to the fact that future-oriented reasoning tasks tap into the generative and predictive capabilities of the models. Historical and present contexts, which often require exact recall or conformity to training data, proved more difficult due to inconsistencies in the coverage of pretraining corpora.\nPerformance by Question Type Question type further modified results, with commonsense reasoning questions reaching the highest percentage of correctness: 51%. These questions depended less on explicit tokenization and more on logical inference, which LLMs did comparatively well. Factual questions were at 45%, while conceptual questions reached slightly lower performances of 40%. Numerical reasoning questions were the hardest; only"}, {"title": "5.3 Temporal Sensitivity Analysis", "content": "We analyse temporal biases and format sensitivity by examining the embeddings and softmax outputs of the model for prompts across three temporal categories - past, present, and future and multiple date formats, as shown in Table 2. We organise the findings into representation-level bias, logical-level bias, and format sensitivity, and we show the results in Figure 6 and Figure 7.\nRepresentation-Level Bias We evaluated representation-level bias by calculating the cosine similarity between the averaged embeddings for prompts across the three temporal references. The leftmost heatmap in Figure 6 illustrates these similarities.\nThe embeddings for past and present references exhibit no measurable similarity (0.00), emphasising that the model encodes historical and contemporary contexts with distinct semantic structures. However, the moderate similarity between future and present suggests some shared contextual features between these categories while maintaining semantic differentiation. The moderate similarity between past and future indicates that these categories share overlapping contextual features while remaining semantically distinct. This further implies that the model is somewhat confused regarding futuristic references, which may frequently misattribute to a different temporal category, likely reflecting the training data distribution.\nLogical-Level Bias We assessed logical-level bias by measuring the Kullback-Leibler (KL) divergence between softmax outputs for prompts across temporal references. The leftmost heatmap in Figure 7 illustrates these divergences\u2014prompts referencing the present exhibit the lowest divergence, indicating stable and consistent output probabilities. However, significant divergence is observed between past-present and future-present comparisons, highlighting the model's reliance on different priors when predicting tokens for noncontemporary contexts.\nThe moderate divergence between past and future outputs suggests that the model differentiates between these temporal categories while leveraging some shared contextual grounding. The distinct KL divergences for non-present prompts indicate a logical-level bias, where the model's probabilistic outputs are sensitive to the temporal context, even when the semantic content of the prompts remains equivalent."}, {"title": "5.4 Format Sensitivity Analysis", "content": "Figures 6 and 7 (second to fourth columns) show the model's sensitivity to variations in date formats for each temporal reference. Both embeddings and softmax outputs reveal notable patterns of variability across formats.\nRepresentation-Level Bias The cosine similarity heatmaps in Figure 6 indicate that date formats with standard separators (e.g., %Y-%m-%d) yield higher consistency, particularly for present references. Non-standard formats (e.g., %Y%m%d, %d%m%Y) result in lower similarity, especially for past and future prompts. The future category exhibits the highest variability in embeddings across formats, suggesting that futuristic contexts rely more on consistent input structures. In contrast, embeddings for present references remain robust across formats, likely due to the dominance of contemporary contexts in the training data.\nLogical-Level Bias The KL divergence heatmaps in Figure 7 reflect similar trends. Standardised data formats (e.g., %Y-%m-%d) produce more stable predictions, while non-standard formats (e.g., %d%m%Y) introduce higher variability. This sensitivity manifests most prominently in future references, where the KL divergence values are consistently higher, indicating that the model's predictions have increased uncertainty. In contrast, present references remain relatively stable, reinforcing the model's preference for standardised inputs and contemporary contexts.\nThe results highlight two fundamental temporal biases in the model. First, representation-level biases reveal that the model encodes temporal contexts with distinct semantic structures, likely shaped by training data distribution. Second, logical-level biases indicate inconsistencies in output probabilities across temporal references, underscoring the challenges of achieving temporal generalisation. Furthermore, the heightened sensitivity to non-standard date formats underscores the importance of input standardisation for ensuring consistent model behaviour in temporal reasoning tasks."}, {"title": "6 Discussion", "content": "This study highlights the need for targeted strategies to address temporal biases in large language models (LLMs). A key step is to enhance pre-training datasets to ensure temporal diversity, incorporating historical, contemporary, and futuristic contexts. While resources like Redpajama (Weber et al., 2024) and Dolma (Soldaini et al., 2024) are open source, researchers should develop data focused on temporal reasoning with varied formats and cultural contexts.\nPost-training methods, such as Direct Preference Optimization (DPO) (Rafailov et al., 2024), offer a promising avenue for fine-tuning models using curated datasets specifically designed to improve their logical temporal reasoning capabilities (Su et al., 2024b; Tan et al., 2023a). These approaches can help align the models' outputs with human-preferred logical reasoning patterns, addressing specific shortcomings in temporal tasks. Additionally, Retrieval-Augmented Generation (RAG) (Liu et al., 2024) enhances LLMs by integrating external knowledge dynamically during inference, allowing the models to access up-to-date or context-specific temporal information beyond their static training data. Moreover, prompting techniques such as Chain of Thought (CoT) prompting (Wei et al., 2023) enable models to break down complex temporal reasoning tasks into incremental steps, improving interpretability and logical coherence (Liu et al., 2024; Xiong et al., 2024b).\nHowever, while these post-training methods significantly mitigate biases in temporal reasoning and improve model performance, they are not sufficient to completely eliminate inherent biases. Factors such as the limitations of pre-trained embeddings, the static nature of foundational knowledge, and the variability in task-specific datasets mean that biases are likely to persist at some level. Thus, post-training approaches should be viewed as an important step toward reducing biases."}, {"title": "7 Conclusion", "content": "Our paper addresses the challenges of temporal biases in large language models (LLMs) and proposes a structured approach to analyse their performance with temporal data. We introduced the DateLogicQA dataset and the Semantic Integrity Metric to evaluate the impact of diverse date formats and contexts on tokenization and reasoning. Our findings highlighted representation-level biases, where temporal contexts are inconsistently encoded, and logical-level biases, evident in varying outputs for similar prompts. We suggest mitigation strategies, such as temporally balanced pretraining datasets, post training and prompting methods."}, {"title": "Limitations", "content": "Future Scalability. The manual human evaluation approach for temporal reasoning performance analysis was time-consuming and challenging for future scalability. Furthermore, the evaluation technique requires high consensus among evaluators, especially when team size expands. Maintaining the evaluation quality in a larger team is also particularly difficult, and it might require more effort to cross-validate the results."}, {"title": "Ethical Considerations", "content": "AI usage. It's pertinent to acknowledge the role of AI tools such as ChatGPT in our project. Specifically, Grammarly was utilized minimally and primarily for grammar corrections in our documents. This use was strictly confined to enhancing linguistic accuracy and improving the readability of our written materials. It's important to clarify that the core research, analysis, and development were conducted independently by our team.\nHuman Annotation. The human annotators involved in this project are professionals with expertise in computer science. No sensitive or personally identifiable data was used in the annotation process, adhering to ethical guidelines and data privacy standards. The human annotators are co authors on this paper."}]}