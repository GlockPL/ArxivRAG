{"title": "CRScore: Grounding Automated Evaluation of Code Review Comments in Code Claims and Smells", "authors": ["Atharva Naik", "Marcus Alenius", "Daniel Fried", "Carolyn Ros\u00e9"], "abstract": "The task of automated code review has recently gained a lot of attention from the machine learning community. However, current review comment evaluation metrics rely on comparisons with a human-written reference for a given code change (also called a diff), even though code review is a one-to-many problem like generation and summarization with many \"valid reviews\" for a diff. To tackle these issues we develop a CRScore a reference-free metric to measure dimensions of review quality like conciseness, comprehensiveness, and relevance. We design CRScore to evaluate reviews in a way that is grounded in claims and potential issues detected in the code by LLMs and static analyzers. We demonstrate that CRScore can produce valid, fine-grained scores of review quality that have the greatest alignment with human judgment (0.54 Spearman correlation) and are more sensitive than reference-based metrics. We also release a corpus of 2.6k human-annotated review quality scores for machine-generated and GitHub review comments to support the development of automated metrics.", "sections": [{"title": "1 Introduction", "content": "Code Review is an essential peer analysis quality control tool for software engineers to ensure that source code is free of bugs and upholds standards (McIntosh et al., 2014; Bavota and Russo, 2015). Software engineers prefer lightweight, async reviews like GitHub's review comment feature over formal in-person reviews (Beller et al., 2014; Badampudi et al., 2023) leading to the creation of benchmarks for automated generation of natural language (NL) review comments (Li et al., 2022; Tufano et al., 2022). However, these benchmarks use reference-based evaluation metrics like BLEU (Papineni et al., 2002), which have been shown to have low validity (Reiter, 2018; Evtikhiev et al., 2023), especially when paired with limited and low-quality references.\nCode review fundamentally is a one-to-many problem, where a given diff can have multiple possible issues that can be tackled by a review. Having a limited number of reference reviews (e.g. one per diff in CodeReviewer (Li et al., 2022)) leads to unfairly low scores with reference-based metrics. For example for the diff shown in Figure 1, the ground truth review focuses on whether the ToHexString() function could be a performance issue. However, the model-generated review focuses on the ToHexString().Equals(\"0000000000000000\") being an odd condition with a scenario where it's triggered being unlikely, which is also a valid review for the diff. However, the BLEU score value for the model-generated review is very low at 0.0458 due to poor n-gram overlap. Additionally, the references can also be low-quality with missing context, as shown in Table 4. Such low-quality references paired with reference-based metrics can harshly and unfairly penalize models.\nMotivated by these drawbacks we propose CRScore an automated but reference-free evaluation metric that uses dimensions of review quality from prior work (Piorkowski et al., 2020; Turzo and Bosu, 2024). Namely, Comprehensiveness \u2013 does the review convey all the necessary information? Conciseness \u2013 does the review only convey the necessary information in an efficient way and Relevance \u2013 is all the information on topic. We operationalize our metric through a two-step process: 1 generate a list of pseudo-references spanning information like possible claims, issues, and implications of a code change, (2) use semantic textual similarity (STS) to align parts of the review to the pseudo-references. To generate the pseudo-references we use a neuro-symbolic approach that combines Large Language Models (LLMs) and Code Analysis Tools (CATs) that can detect formatting errors, faulty design patterns (code smells Rasheed et al. 2024), etc. We combine these meth-"}, {"title": "2 Related Work", "content": "In this section, we summarize the limitations of reference-based evaluation, the need for better code review evaluation metrics, inspiration from reference-free evaluation for other tasks, and how code smells can be leveraged to evaluate reviews."}, {"title": "2.1 Reference Based Evaluation Metrics:", "content": "Reference-based metrics like BLEU (Papineni et al., 2002), ROUGE (Ganesan, 2018), and BERTScore (Zhang et al., 2020) have seen widespread adoption for several text generation tasks like machine translation and summarization due to their convenience. While BLEU and ROUGE and similar metrics like character F-score (Popovi\u0107, 2015, 2017) use n-gram overlap between the reference and candidate text, metrics like BERTScore (Zhang et al., 2020) try to capture the semantic similarity. However prior studies have shown metrics like BLEU to have low validity (overlap with human judgment) and reliability (Reiter, 2018; Evtikhiev et al., 2023). Meanwhile, BERTScore can fall for candidates with errors that are lexically and stylistically similar to references (Hanna and Bojar, 2021)."}, {"title": "2.2 Code Review Evaluation", "content": "Due to the high time and resource demands of code review automated approaches have gained popularity (Yang et al., 2024). Tufano et al. (2021, 2022); Li et al. (2022) proposed the most popular datasets for code review tasks like detecting code changes that require review, generating review comments, and refactoring code. However, both of these datasets used reference-based metrics and thus suffer from the issues highlighted in the previous section. While many studies have focused on modeling methods for code review tasks (Pornprasit and Tantithamthavorn, 2024; Lu et al., 2023; Dong-Kyu, 2024; Fan et al., 2024; Yu et al., 2024; Lin et al., 2024), these studies either retain the same reference-based automated metrics like BLEU score (Papineni et al., 2002) for evaluating review comments or use human evaluation. In this work, we show that the use of reference-based metrics combined with noisy references fails to capture human notions of review quality. Hence we propose CRScore the first reference-free metric for code review comment generation to overcome the limitations of these metrics."}, {"title": "2.3 Reference Free Evaluation", "content": "Reference-free evaluation metrics have been proposed for various generative tasks like dialog to deal with the issue of multiple valid outputs. These metrics broadly target the evaluation of various \"quality dimensions\u201d like relevance, informativeness, etc. VIFIDEL (Madhyastha et al., 2019) InfoMetIC (Hu et al., 2023), ClipScore (Hessel et al., 2021) evaluate dimensions like faithfulness, informativeness, and relevance, respectively for image captioning. FED (Mehri and Eskenazi, 2020a), and USR (Mehri and Eskenazi, 2020b) have been proposed for evaluating dialog across dimensions like informativeness, relevance, and overall quality. Finally, studies on the helpfulness of software documentation like (Piorkowski et al., 2020) report the usefulness of quality dimensions like conciseness, completeness, relevance, and supporting evidence while studies on code review like (Turzo and Bosu, 2024) identify concise and understandable reviews to be helpful. The common trend across these studies is some notion of conciseness, informativeness (comprehensiveness in our work), and relevance being useful, prompting us to focus on them. Additionally, our metric is also similar to InfoMetIC which has text precision, visual recall, and overall quality. However, none of these studies target code reviews, making our work the first automated reference-free metric for code reviews."}, {"title": "2.4 Code Smell Detection", "content": "\"Code smells\u201d (Fowler, 1997) are symptoms of design flaws and bad practices (also called anti-patterns) that can snowball into maintainability issues. Detecting code smells automatically has been traditionally done by analysis-based heuristic approaches (Tsantalis et al., 2008; Paiva et al., 2017; Liu and Zhang, 2017), with machine learning (Sandouka and Aljamaan, 2023) and transfer learning (Sharma et al., 2021) based approaches being proposed for learning more complex heuristics. Recent approaches have leveraged LLMs via prompting (Liu et al., 2024) and agents Rasheed et al. (2024) to achieve further improvement or tackle repository-level code smell detection. However, these approaches are limited in the types of smells they target (Liu et al., 2024; Sandouka and Aljamaan, 2023), training data requirements (Zhang et al., 2024), or lack comprehensive evaluation (Rasheed et al., 2024). Also, code smells differ across programming languages (Abidi et al., 2019), and transfer learning approaches can only be leveraged for similar languages (Sharma et al., 2021). Due to these limitations of learning-based methods and to mitigate the self-selection bias of LLMs (section A.1) we use code analysis tools."}, {"title": "3 Operationalizing CRScore", "content": "Motivated by the one-to-many nature of code reviews, noisy references, and the pitfalls of reference-based automated metrics we develop CRScore a reference-free, quality dimension-based automated metric. As shown in Figure 2, instead of relying on explicit references, our metric generates \"pseudo-references\" from the code change spanning claims, implications, and issues or smells that could hurt maintainability down the line \u2013 in other words some topics that a review should address (Rasheed et al., 2024). Then we use semantic textual similarity (STS) measures to quantify how much these topics are addressed by a code review as shown in Figure 2 through the lens of three quality dimensions: conciseness, comprehensiveness, and relevance. They capture review quality in a way similar to precision, recall, and f-score for classification and retrieval. We describe the three main components of our framework \u2013 the quality dimensions, pseudo-reference generation, and similarity measurement \u2013 below."}, {"title": "3.1 Quality Dimensions", "content": "We pick a subset of the quality dimensions proposed by Piorkowski et al. (2020) for software documentation evaluation and adapt them to code review. We pick dimensions of \u201ccompleteness\u201d and \"conciseness\" Piorkowski et al. (2020) to capture and strike a balance between comprehensive reviews with a lot of detail and concise, minimalist reviews. To ensure that reviews are comprehensive while minimizing irrelevant information we propose a precision-like measure called conciseness, a recall-like measure comprehensiveness, and a combined measure of the overall quality or relevance similar to the f-measure (and \u201crelevance\" dimension from Piorkowski et al. 2020). Additionally, we also do a human evaluation of the validity of the pseudo-references focusing on aspects like supporting evidence (Piorkowski et al., 2020)."}, {"title": "3.2 Pseudo Reference Generation", "content": "To generate pseudo references we develop an LLM-based pipeline for generating claims about the code changes on two levels of abstraction: 1) Low-level changes and 2) High-level \u201cimplications\" of the change. To allow for reproducibility and ease of deployment we use a 6.7B parameter open source"}, {"title": "3.3 Computing Similarity with Pseudo References:", "content": "We use a Sentence Transformer (Reimers, 2019) model (mxbai-embed-large-v1) to compute Semantic Textual Similarity (STS) between pseudo-references and review sentences. The pairwise similarities are then used to compute the conciseness, comprehensiveness, and relevance as shown by\n$Con = \\frac{\\sum_{r \\in R} I[max_{p \\in P} s(c, r) > \\tau]}{|R|}$ (1)\nrepresents the fraction of r-sents from the model-generated review with greater similarity to any p-ref above a threshold \u03c4. Here, I is an indicator variable such that:\n$I[x]=\\begin{cases} 1, & \\text{if } x \\text{ is true} \\\\ 0, & \\text{otherwise} \\end{cases}$\nCon resembles precision as it captures the fraction of r-sents (candidate set) that are \"on topic\" concerning the p-refs (reference/gold set). The Comprehensiveness (Comp) computed as:\n$Comp = \\frac{\\sum_{p \\in P} I[max_{r \\in R} s(c, r) > \\tau]}{|P|}$ (2)\nrepresents the fraction of p-refs that have greater similarity to any of the r-sents than the threshold \u03c4."}, {"title": "4 Validating CRScore", "content": "To be a valid metric, CRScore needs to satisfy a few properties. Firstly the generated pseudo-references should have few errors and unverifiable claims and be as exhaustive as possible. Additionally, we want our metric dimension scores (Con, Comp, and Rel), especially Rel to correlate with human judgment for the corresponding dimension. However, arguably most importantly we want our metric to rank a diverse set of review generation systems similar to how humans would rank them based on overall review relevance. In the subsequent sections, we describe how we design the experiments to collect these annotations (section 4.1, 4.2), choose a set of systems to be rated by humans (section 4.3), and set up reference-based metrics (section 4.4) for comparison against CRScore."}, {"title": "4.1 Rating Quality of Pseudo-References", "content": "To show that LLM-generated pseudo-references used by the CRScore evaluation pipeline are high-quality, we gather annotations for their quality, capturing incorrect, unverifiable, and missing claims. To rate the quality of the pseudo-references, we had two trained human annotators judge their quality for 100 randomly sampled code changes each in Python, Java, and Javascript (300 total). The annotators were asked to code the pseudo-references as 1 (correct based on evidence), 0 (incorrect based on evidence), and -1 (unverifiable due to lack of evidence). They were also asked to add any pseudo-references about issues/claims not covered by the pseudo-references. We report the fraction of correct claims (accuracy), incorrect claims (error rate), unverifiable claims (unverifiable rate), and missing claims (missing rate). If Nc, Nu, Ni, and Na represent the number of correct, unverifiable, incorrect, and added claims then each of these rates can be calculated as:\n$\\begin{aligned}\n\\text{Accuracy} &= \\frac{N_c}{N_c + N_u + N_i} \\\\\n\\text{Error Rate} &= \\frac{N_i}{N_c+ N_u + N_i} \\\\\n\\text{Unverifiable Rate} &= \\frac{N_u}{N_c+ N_u + N_i} \\\\\n\\text{Missing Rate} &= \\frac{N_a}{N_c + N_u + N_i}\n\\end{aligned}$\nBased on these expressions: Accuracy + Unverifiable Rate + Error Rate = 1. The results for each language are shown in table 1. The annotators follow guidelines laid out in a codebook (section D.1) which includes examples for each category. We measured the coding reliability of our approach by collecting annotations from both annotators on a common set of 100 pseudo-references. These annotations yielded a Cohen Kappa of 0.804 which indicates great inter-annotator reliability (Landis and Koch, 1977). Also, we only evaluate the LLM-generated claims here as we know the static code analysis tools are rule-based and reliably correct."}, {"title": "4.2 Rating Review Quality Dimensions", "content": "To show that CRScore aligns with the human judgment of review quality along the proposed dimensions: comprehensiveness, conciseness, and relevance we gather annotations from the same annotators on reviews generated by 9 systems (section 4.3)"}, {"title": "4.3 Review Generation Systems", "content": "To test the ability of CRScore to rank code review systems of varying capabilities, we choose a diverse set of review generation systems. They span various parameter sizes, pre-training, and domain-specific fine-tuning strategies for code review:\nSimple baselines: We create two simple baselines a BM-25 retriever and an LSTM as described in section D.3. We choose these models with the expectation that they will likely perform the worst, to see if our metric assigns them a low score.\nCodeReviewer: We pick the CodeReviewer model from (Li et al., 2022) as it is a transformer-based model trained on code review-specific data and objectives.\nOpen source LLMs: We prompt several open-source LLMs in a few-shot manner with a fixed set of three example code changes and review pairs from the validation set. We use LLMs in the 3-13B parameter range: Stable-Code-3B (Pinnaparaju et al.), DeepSeekCoder-6.7B (Guo et al., 2024), Magicoder-6.7B (Wei et al., 2023), CodeLLaMA-7B and 13B (Roziere et al., 2023) and LLaMA-3-8B (AI@Meta, 2024).\nClosed source LLMs: We prompt closed-source LLMs like GPT-3.5 in a manner similar to the open-source LLMs."}, {"title": "4.4 Reference-based Metrics", "content": "We pick commonly used reference-based metrics for code review and other text generation tasks to compare with CRScore:\nBLEU (Papineni et al., 2002) measures the n-gram precision between the generated text and references with an additional brevity penalty to discourage short outputs. It is used for evaluation in both (Tufano et al., 2021) and CodeReviewer (Li et al., 2022). We report the results with and without stop word removal.\nNormalized Edit Distance is a normalized Levenshtein distance used in prior work (Tufano et al., 2021; Bairi et al., 2024) to measure the number of edits required to match candidate and target reviews or code.\nROUGE-L F-measure is a popular recall-oriented metric originally proposed for summarization and machine translation. We use the longest common subsequence-based sentence level f-measure implementation.\nchrF: (Popovi\u0107, 2015) Is a machine translation metric which is essentially a character level F score computed using character level n-grams.\nchrF++: (Popovi\u0107, 2017) Is a variant of chrF that incorporates word n-grams as well and was found to correlate more with human assessment.\nBERTScore: (Zhang et al., 2020) We use the BERTScore F1 measure to capture the semantic similarity between the reference review and the generated review."}, {"title": "5 Results", "content": "We show the rates of correct, incorrect, unverifiable, and missing claims as explained in section 4.1 in Table 1. The pseudo-references produced by our pipeline were relatively accurate according to the annotators with roughly 82.6% accuracy across the languages. The best performance is for"}, {"title": "5.1 Validity of Pseudo-References", "content": "Javascript and the worst performance is for Java. The most frequent issues in the code claims were incorrect claims (for Java and Python). Most of the errors were in code comprehension (\u201cmisreading the code\") and over-generalization (incorrect assumptions/generalizations made from the limited context, contradicting file level context). Some examples are shown in Table 11.\nFor Javascript, we observed unverifiable claims to be the biggest issue. There were a variety of claims about code efficiency, functionality, or performance made without evidence. E.g.: \"However, this change could also potentially enable less strict or less strict-like behavior, depending on the context in which the function is used. This could make the code less efficient or less performant\"."}, {"title": "5.2 Validity of Review Quality Dimension Scores (Con, Comp and Rel)", "content": "Correlation with human Likert score annotations: We compute the Spearman and Kendall rank correlations between the human-annotated Likert scores and the metric values. These values were gathered for the 300 CodeReviewer test instances mentioned in section 4.1 The results are shown in Table 2. Notably, we exclude human annotations done on the CodeReviewer \"ground truth\" references (\"Ground Truth\" row in Table 9) because the reference-based metric value for these would be 1 by default unfairly lowering the correlation values for reference-based metrics that we compare against. We observe that while all the reference-based metrics have weak to no correlations with human judgment our metric Rel is the most correlated, achieving a moderate correlation with human annotations. We also show correlations between the human Likert scale annotations for all the dimensions and the reference-based metrics as well as the rest of our proposed metrics\nCon and Comp in Table 8.\nComparing system rankings: Arguably the most important quality a metric should have is the ability to rank systems similar to human evaluators. To see if our metric Rel is capable of doing that we compare the system rankings produced by the human annotations (mean relevance Likert scores or \u201cHuman Annotations - Rel\" column in Table 9) and by Rel (\"Our Metric - Rel\" column in Table 9), for the"}, {"title": "5.3 CodeReviewer Dataset Reference Quality", "content": "We compare the quality of the CodeReviewer reference reviews with the reviews generated by the 9 system evaluated by the human annotators. The average scores for conciseness, comprehensiveness, and relevance attained by the CodeReviewer references are 3.05, 1.88, and 2.13, while the average scores obtained by all 9 systems are 2.57, 1.84, and 1.99. This suggests that the average reference review is barely better than the average evaluated system according to the human annotators for relevance, but they are more concise. Additionally the best system according to human annotators (as evident from Table 9), GPT-3.5 achieves average scores of 3.63, 2.65, and 2.9 for each dimension much better than the reference reviews. This provides further motivation for the development of reference-free evaluation metrics like CRScore for code review."}, {"title": "5.4 Failure Cases", "content": "We analyze the cases where our metric greatly overestimates or underestimates the quality of a review with respect to human annotations. We find such cases using the procedure described in Appendix E.1.\nFor underestimation cases we observe our pseudo-reference generation pipeline generates fewer references on average (2.44) compared to the whole data (4.76). This suggests that having fewer references makes it harder to evaluate the relevance of reviews. Additionally, we observe reviews like \"Why do we need these imports\" which are brief, contain stopwords, and have fewer relevant tokens, making it hard for STS to recognize their relevance to pseudo-reference.\nFor overestimation cases, we observe the presence of inline code snippets at a higher rate (45%) compared to all reviews (28%) and underestimation cases (12%). Some example reviews for both cases are shown in Table 15 and 16."}, {"title": "6 Discussion", "content": "In this work we identify issues with current code review evaluation benchmarks like CodeReviewer (Li et al., 2022) which fail to capture the one-to-many nature of code review and contain noisy references. To enable auditing current evaluation metrics and aid the development of a better metric we propose three review quality dimensions conciseness, comprehensiveness, and relevance based on current literature on reference-free evaluation (Mehri and Eskenazi, 2020b; Piorkowski et al., 2020; Turzo and Bosu, 2024).\nTo ground these dimensions into topics that reviews should address (Rasheed et al., 2024) we propose an automated pseudo-reference generation pipeline that leverages Large Language Models (LLMs) and code smell detectors to generate claims and detect issues in code changes to be covered by reviews. We validate the quality of these pseudo-references via human evaluation. Based on these dimensions and pseudo-references we develop reliable guidelines for coding the quality of reviews and collect annotations for 9 review generation systems and the \"ground truth\" reviews for the CodeReviewer dataset spanning Python, Java and Javacript.\nThe collected annotations show that current reference based metrics indeed fail to capture human preferences which is further compounded by humans preferring some models over the references. We propose CRScore as a metric to capture the three dimensions using the pseudo-references through STS models like sentence transformers (Reimers, 2019) and show that our approach has the greatest alignment with human preferences in terms of review level scores (0.4577 \u0442 and 0.5425 rs) and system ranking (0.95 rs and 0.8889 \u03c4) and greatest sensitivity (Figure 8). Despite this we"}, {"title": "7 Conclusion", "content": "Our work takes the first steps towards addressing the variety of challenges involved in evaluating the quality of code reviews. We propose useful dimensions for capturing review quality, propose a neuro-symbolic method for operationalizing them in a grounded way using automatically generated pseudo-references, collect a dataset of human judgment of review quality, and propose a reference-free automated metric called CRScore to measure review quality along the proposed dimensions using pseudo-references. We benchmark our metric against 7 reference-based metrics, for evaluating 9 diverse review generation systems over the human annotations and show that our metric achieves the best alignment and is the most sensitive, but there is still scope for improvement by developing better pseudo-reference generation and STS matching methods."}, {"title": "8 Future Work", "content": "As discussed before, although our metric takes a great first step towards better reference free evaluation of code reviews, it still suffers from systematic under and over estimation errors in certain cases. According to us these limitations ultimately stem from the lack of exhaustiveness of the pseudo-reference generation pipeline and the limitations of STS methods when it comes to matching data containing both code and text. We think research is required to add components that can detect issues of code security, efficiency, etc. to the pseudo-reference generation pipeline for greater coverage of possible issues and better embedding models should be developed to capture functional properties of code to improve STS methods for matching code and text.\nAdditional limitations of our study include coverage of only Python, Java, and Javascript code for annotations and code smell detection even though the CodeReviewer dataset contains more languages like C, C++, Go, and Ruby. Future work could explore ways to extend the code smell detection"}, {"title": "Limitations", "content": "\u2022 While annotating reviews for the review quality dimensions of conciseness, comprehensiveness and relevance, the human annotators are encouraged to use claims and issues that start of as the same list of psuedo-references used by CRScore. However we argue that this isn't a source of anchoring bias because the human annotators are allowed to add and remove claims from the pseudo-references while they annotate them from review quality. This means the final list of claims used for review quality annotations is different from the one used by CRScore. Additionally having a common list of claims and issues for the two annotators helped us improve the reliability of our coding framework as talked about in section 4.2. It also allowed the annotators to ground the comprehensiveness of reviews into a concrete set of things that a review should cover.\n\u2022 Semantic textual similarity (STS) models are imperfect at matching relevant pseudo-references to the review sentences, especially when there are very few claims and the reviews contain inline code snippets, where the latter can inflate the STS scores.\n\u2022 Our pseudo-reference generation pipeline is not comprehensive enough in some cases which can lead to underestimation of review quality as shown by our failure case analysis. Additionally, it could be extended by adding more modules like code smell detectors for aspects like code security, code efficiency, etc. similar to Rasheed et al. (2024). Also, code smell detector tools can be added for languages other than Python, Java, and Javascript, like Go, C/C++, and Ruby present in the CodeReviewer dataset.\n\u2022 Our metric only achieves a moderate correlation with human annotations of review quality, and while it is much better than the reference-based metrics in terms of alignment with human judgment and sensitivity to review quality as judged by humans, it is only a first step towards developing better metrics for code review. Future work should try to address the"}, {"title": "Ethics Statement", "content": "We believe our work doesn't violate any ethical guidelines and is compliant with copyright rules and regulations as we use an existing publicly available dataset and augment it with annotations of review quality using reviews generated by 9 systems and the references in the dataset. While there is a slight risk of harmful or toxic text being a part of the pseudo-references generated by the LLM component in our pseudo-reference generation pipeline we don't believe it to be a major risk based on the annotations done for the pseudo-reference quality."}, {"title": "A.1 Benefits of Neuro Symbolic Pseudo-Reference Generation", "content": "While LLMs have recently shown promise for evaluating natural language generation (NLG) (Li et al., 2024) they suffer from biases like favoring their own generations (\u201cself-selection bias\") (Panickssery et al., 2024) or in other words if we were to have Magicoder or GPT-3.5 as the evaluator LLM it would assign higher scores to text generated by Magicoder and GPT-3.5 respectively. Code analysis tools (CATs) on the other hand are limited in scope compared to LLMs in detecting issues like best practice violations (Vijayvergiya et al., 2024) but don't have any self-selection bias. However, combining these methods can reduce the self-selection bias of LLMs, while supplementing the narrow coverage of code analysis tools. Indeed the results show that despite using Magicoder as the evaluation LLM, our metric CRScore doesn't preferentially rank Magicoder above any models other than LLaMA-3 when compared to the human ranking."}, {"title": "B More Related Work", "content": "Due to the popularity and convenience of automated reference-based metrics like BLEU (Papineni et al., 2002), ROUGE (Ganesan, 2018), and BERTScore (Zhang et al., 2020) the research community has developed several code-specific versions like CodeBLEU (Ren et al., 2020), RUBY (Tran et al., 2019), CrystalBLEU (Eghbali and Pradel, 2023) and CodeBERTScore (Zhou et al., 2023). CodeBLEU extends BLEU by incorporating code structure through dataflow and syntax match between generated code and references, while CrystalBLEU filters out trivially shared n-grams. RUBY incorporates the distance between the syntax tree and program dependency graph of references and generated code. CodeBERTScore extends the embedding-based BERTScore by replacing BERT with a pre-trained CodeBERT (Feng et al., 2020) model. However prior studies have shown metrics like BLEU to have low validity (overlap with human judgment) and reliability for text generation (Reiter, 2018), code generation (Evtikhiev et al., 2023), and code migration (Tran et al., 2019). However, metrics like ROUGE, BERTScore, and CodeBERTScore all have a notion of precision, recall, and f-score which is captured by conciseness, comprehensiveness, and relevance respectively."}, {"title": "B.2 Code Review Automation", "content": "Due to the high time and resource demands of code review automated approaches have gained popularity (Yang et al., 2024). Pornprasit and Tantithamthavorn (2024); Lu et al. (2023); Dong-Kyu (2024); Fan et al. (2024); Yu et al. (2024) propose fine-tuning and prompt engineering approaches to leverage LLMs for code review and code-change related tasks. Fr\u00f6mmgen et al. (2024); Rahman et al. (2024) propose methods for code refactoring based on review comments. Vijayvergiya et al. (2024) propose the detection of \u201cbest practice violations\u201d . Lin et al. (2024) propose oversampling reviews from experienced reviewers as a proxy of review quality improving informativeness and correctness of generated reviews. Rasheed et al. (2024) propose an LLM agent for code review and code smell detection."}, {"title": "B.3 Code Smell Detection", "content": "The problem of detecting \"code smells\u201d or symptoms of design flaws and bad practices (also called anti-patterns) has been traditionally tackled by analysis-based approaches (Tsantalis et al., 2008; Paiva et al., 2017; Liu and Zhang, 2017). Recent work has explored learning-based methods for potentially more nuanced detection of code smells. Sandouka and Aljamaan (2023) create a dataset of 1k Python code smells like \"Long Method\" and \"Large Class\" to train traditional ML models like random forests. Sharma et al. (2021) leverage"}, {"title": "C Method Details", "content": "This appendix contains additional details on the implementation of our CRScore metric."}, {"title": "C.1 Distribution of Sentence Similarity Scores", "content": "We plot the histogram of values of the sentence similarity scores in Figure 5 showing a roughly normal distribution. We also plot the quantile-quantile (Q-Q) plot in Figure 6 that compares the quantile of a normal distribution with the empirically observed distribution of sentence similarity scores. Ideally, the Q-Q plot should be a straight line (shown in red) but we observe deviation to-"}, {"title": "C.2 Code Smell Detection Details", "content": "In this section, we cover some of the details of the code smell detectors used in this study."}, {"title": "C.2.1 Class Cohesion", "content": "Class cohesion captures the degree to which the elements of a class belong together (Contributors, 2019a). In other words, cohesion measures the strength of the relationship between pieces of functionality (attributes and methods) within a given class. For example, in highly cohesive classes functionality is strongly related and methods and attributes are more co-dependant and hang together as a logical whole (mschwager, 2016)."}, {"title": "C.2.2 Cyclomatic Complexity", "content": "Cyclomatic complexity is a software metric used to indicate the complexity of a program (Contributors, 2019b). It is a quantitative measure of the"}]}