{"title": "CONSTRAINED POSTERIOR SAMPLING: TIME SERIES GENERATION WITH HARD CONSTRAINTS", "authors": ["Sai Shankar Narasimhan", "Shubhankar Agarwal", "Litu Rout", "Sanjay Shakkottai", "Sandeep Chinchali"], "abstract": "Generating realistic time series samples is crucial for stress-testing models and protecting user privacy by using synthetic data. In engineering and safety-critical applications, these samples must meet certain hard constraints that are domain-specific or naturally imposed by physics or nature. Consider, for example, generating electricity demand patterns with constraints on peak demand times. This can be used to stress-test the functioning of power grids during adverse weather conditions. Existing approaches for generating constrained time series are either not scalable or degrade sample quality. To address these challenges, we introduce Constrained Posterior Sampling (CPS), a diffusion-based sampling algorithm that aims to project the posterior mean estimate into the constraint set after each denoising update. Notably, CPS scales to a large number of constraints (~ 100) without requiring additional training. We provide theoretical justifications highlighting the impact of our projection step on sampling. Empirically, CPS outperforms state-of-the-art methods in sample quality and similarity to real time series by around 10% and 42%, respectively, on real-world stocks, traffic, and air quality datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Synthesizing realistic time series samples can aid in \"what-if\" scenario analysis, stress-testing machine learning (ML) models (Rizzato et al., 2022; Gowal et al., 2021), anonymizing private user data (Yoon et al., 2020), etc. Current approaches for time series generation use state-of-the-art (SOTA) generative models, such as Generative Adversarial Networks (GANs) (Yoon et al., 2019; Donahue et al., 2018) and Diffusion Models (DMs) (Tashiro et al., 2021; Alcaraz & Strodthoff, 2023; Narasimhan et al., 2024), to generate high-fidelity time series samples.\nHowever, generating realistic and high-fidelity time series samples requires strict adherence to various domain-specific constraints. For example, consider generating the daily Open-high-low-close (OHLC) chart for the stock price of an S&P 500 company. The generated time series samples should have opening and closing stock prices bounded by the high and low values. Similarly, consider generating stock price time series with a user-specified measure of volatility to stress-test trading strategies. If the generated samples do not have the exact volatility, the stress testing results might not be accurate.\nOn a more general note, the advent of large-scale generative models for language and vision, like GPT-4 (Bubeck et al., 2023) and Stable Diffusion (Podell et al., 2023), has increased the focus on constraining the outputs from these models,"}, {"title": "2 PRELIMINARIES", "content": "Notations: We denote a time series sample by $x \\in \\mathbb{R}^{K \\times L}$. Here, $K$ and $L$ refer to the number of channels and the horizon, respectively. A dataset is defined as $D = \\{x^1,...,x^{N_D}\\}$, where the superscript $i \\in [1, . . . N_D]$ refers to the sample number, and $N_D$ is the total number of samples in the dataset. $P_{data}$ denotes the real time series data distribution. $x^i$ is the realization of the random vector $X^i$, where $X^1,... X^{N_D} \\sim P_{data}$. The Probability Density Function (PDF) associated with $P_{data}$ is represented by $p_{data} : \\mathbb{R}^{K \\times L} \\rightarrow \\mathbb{R}$, where $\\int p_{data}(x)dx = 1$. Here, $\\int$ refers to the integration operator over $\\mathbb{R}^{K \\times L}$. The notation $\\mathcal{N}(\\mu, \\Sigma)$ refers to the Gaussian distribution with mean $\\mu$ and covariance matrix $\\Sigma$. Similarly, $\\mathcal{U}(a, b)$ indicates the uniform distribution with non-zero density from $a$ to $b$. $||. ||_2$ is overloaded and indicates the $l_2$ norm in the case of a vector and the spectral"}, {"title": "2.1 BACKGROUND AND RELATED WORK", "content": "GANS (Goodfellow et al., 2014) have been the popular choice for time series generation(Yoon et al., 2019; Donahue et al., 2018; Srinivasan & Knottenbelt, 2022; Ni et al., 2021). Recently, DMs have dominated the landscape of image, video, and audio generation (Rombach et al., 2022; Ho et al., 2022; Kong et al., 2020). Denoising DMs (Ho et al., 2020; Dhariwal & Nichol, 2021) generate samples by learning to gradually denoise clean data, sampled from the data distribution $P_{data}$, corrupted with Gaussian noise. Denoising Diffusion Probabilistic Models (DDPMs) (Ho et al., 2020) define a Markovian forward noising process, where the clean data sample $x$, referred to as $z_0$, is transformed into $z_T$ with iterative Gaussian corruption for $T$ noising steps, such that $z_T \\sim \\mathcal{N}(0,I)$. With abuse of notation, 0 represents zero mean, and $I$ represents the identity covariance. The forward process introduces T conditional Gaussian distributions with fixed covariance matrices governed by the diffusion coefficients $\\bar{\\alpha}_1,..., \\bar{\\alpha}_T$, where $\\bar{\\alpha}_1 = 1, \\bar{\\alpha}_t \\in [0, 1]$, and $\\bar{\\alpha}_{t-1} > \\bar{\\alpha}_t \\forall t \\in [1, T]$. Formally, $q_t(Z_t | z_0)$ is the PDF of the conditional Gaussian distribution at the forward step $t$ with mean $\\sqrt{\\bar{\\alpha}_t}z_0$ and covariance matrix $\\sqrt{1 - \\bar{\\alpha}_t}I$. The PDF associated with the marginal distribution at $t = 0$ is given by $q_0 = P_{data}$.\nThe sample generation or the reverse process is also Markovian, where we autoregressively sample from T Gaussian distributions with fixed covariance matrices, indicated by PDFs $p_{\\theta,t}(Z_{t-1} | z_t) \\forall t \\in [1,T]$, to get from $z_T$ to $z_0$, where $z_T \\sim \\mathcal{N}(0,I)$. The means of $p_{\\theta,t}(z_{t-1} | z_t)$ are learned using neural networks. DDPMs are trained to maximize the log-likelihood of observing the clean data, i.e., $\\log p_{\\theta} (z_0)$, where $p_{\\theta}(z_0) = \\int p_{\\theta}(z_{0:T})dz_{1:T}$. The joint PDF $p_{\\theta}(z_{0:T})$ can be factorized as $p(z_T) \\prod_{t=1}^T p_{\\theta,t}(Z_{t-1} | z_t)$, due to the Markovian nature of the reverse process, with $p(z_T)$ representing the PDF of $\\mathcal{N} (0, I)$. With successive reparametrizations, the training objective can be"}, {"title": "3 CONSTRAINED POSTERIOR SAMPLING", "content": "To generate realistic samples with high likelihood, our approach assumes the availability of a pre-trained diffusion model trained on the dataset $D$. Given the diffusion denoiser model $e_\\theta$, we propose Constrained Posterior Sampling (CPS, check Fig. 4) to restrict the domain of a generated sample without sacrificing sample quality. Described in Algorithm 1, CPS effectively guides the diffusion denoising process towards the constraint set.\nWe follow the typical DDIM inference procedure. Starting with a sample from the standard normal distribution $\\mathcal{N} (0, I)$ (line 1), we perform sequential denoising (lines 2 to 10). Line 3 refers to the forward pass through the denoiser to obtain the noise estimate $\\epsilon_\\theta (z_t, t)$. After every denoising step, we obtain the posterior mean estimate $z_0(z_t; \\epsilon_\\theta)$ (line 4). We then project this estimate towards the constraint set $C$ to obtain the projected posterior mean estimate $z_{0,pr}(z_t; \\epsilon_\\theta)$ (line 5). Later, we perform a DDIM reverse sampling step with $z_{0,pr}(z_t; \\epsilon_\\theta)$ and $\\epsilon_\\theta (z_t, t)$ to obtain $z_{t-1}$ (lines 7-9).\nThe projection step in line 5 solves an optimization problem with the objective function $(\\frac{1}{2}(||z - z_0(z_t; \\epsilon_\\theta)||_2 + \\gamma(t)\\mathbb{I}(z)))$. The first term of the objective function ensures that $z_{0,pr}(z_t; \\epsilon_\\theta)$ is close to $z_0(z_t; \\epsilon_\\theta)$, thereby ensuring that $z_{t-1}$ is not heavily perturbed for the denoiser to perform poorly. We define the constraint violation function $\\mathbb{I}: \\mathbb{R}^{K \\times L} \\rightarrow \\mathbb{R}$ as $\\mathbb{I}(z) = \\sum_{c_i=1}^{N_c} max(0, f_{c_i}(z))$, such that $\\mathbb{I}(z) = 0$ if $z \\in C$ and $\\mathbb{I}(z) > 0$ otherwise. For the denoising step $t$, the constraint violation function is scaled by a time-varying penalty coefficient $\\gamma(t)$. Our key intuition is to design $\\gamma(t)$ as a strictly decreasing function of $t$ that takes small values for the initial denoising steps ($t$ close to $T$) and tends to $\\infty$\nfor the final denoising steps. This ensures that the constraint satisfaction is not heavily enforced during the initial denoising steps when the signal-to-noise ratio in $z_t$ is very low. Given the requirements for the penalty coefficient, we choose $\\gamma(t) = e^{1/(1-\\bar{\\alpha}_{t-1})}$ such that $\\gamma(t)$ is close to 0 for the initial denoising steps ($\\gamma(T) \\sim e$) and $\\gamma(t) \\rightarrow \\infty$ for $t = 1$. Note that our choice of $\\gamma(t)$ ensures that $\\gamma(t)$ is strictly decreasing with respect to $t$ since $\\bar{\\alpha}_t$ strictly decreases with t.\nObserve that CPS is the DDIM sampling process with one change. We replace the posterior mean estimate $z_0 (z_t; \\epsilon_\\theta)$ with the projected posterior mean estimate $z_{0,pr} (z_t; \\epsilon_\\theta)$. Additionally, CPS can be viewed similarly to the penalty-based methods to solve a constrained optimization problem. With each progressing denoising update, the penalty coefficient increases, thereby pushing the posterior mean estimate towards the constraint set.\nWe do not add noise after the final denoising step ($\\sigma_1 = 0$). This ensures that the efforts of the final projection step towards constraint satisfaction are not compromised by additional noise. For convex constraint sets with assumptions on the convexity of the constraint definition functions $f_{c_i}$, we note that the projection step is an unconstrained minimization of a convex function with the optimal constraint violation value being 0 if $\\gamma(1) > 0$. With a suitable choice of solvers (Diamond & Boyd, 2016), the optimal solution can be obtained for these cases, thereby ensuring constraint satisfaction ($\\mathbb{I}(z_{0,pr}(z_1; \\epsilon_\\theta)) = 0$) when $\\gamma(1)$ tends to $\\infty$."}, {"title": "3.1 THEORETICAL JUSTIFICATION", "content": "Now, we provide a detailed analysis of the effect of modifying the traditional DDIM sampling process with CPS. For ease of explanation, we consider $z \\in \\mathbb{R}^n$. We indicate the identity matrix in $\\mathbb{R}^{n \\times n}$ as $I_n$. First, we describe the exact distribution from which the samples are generated. For this, we make the following assumption.\nAssumption 1. Let the constraint set be $C = \\{z | f_c(z) = 0\\}$, where $f_c : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ and the penalty function $\\mathbb{I}(z) = ||f_c(z)||_2^2$ has $L$-Lipschitz continuous gradients, i.e., $||\\nabla \\mathbb{I}(u) - \\nabla \\mathbb{I}(v)||_2 \\leq L||u - v||_2 \\forall u, v \\in \\mathbb{R}^n$.\nTheorem 1. Suppose Assumption 1 holds. Given a denoiser $e_\\theta : \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ for a diffusion process with noise coefficients $\\bar{\\alpha}_0,..., \\bar{\\alpha}_T$, if $\\gamma(t) > 0 \\forall t \\in [1,T]$, the denoising step in Algorithm 1 is equivalent to sampling from the following conditional distribution:\n$$p_{\\theta,t}(z_{t-1} | z_t) = \\begin{cases} p_{\\theta,init}(z_0 | z_{0,pr}(z_1; \\epsilon_\\theta)) & \\text{if t = 1,} \\\\ q_{\\theta,t}(z_{t-1} | z_t, z_{0,pr} (z_t; \\epsilon_\\theta)) & \\text{otherwise.} \\end{cases}$$\nHere, $p_{\\theta,init}(z_0 | z_{0,pr}(z_1;\\epsilon_\\theta))$ indicates the PDF of $\\mathcal{N} (z_{0,pr}(z_1;\\epsilon_\\theta), \\sigma_1 I_n)$, and $q_{\\theta,t}(z_{t-1} | z_t, z_{0,pr} (z_t; \\epsilon_\\theta))$ indicates the PDF of $\\mathcal{N}(\\sqrt{\\bar{\\alpha}_{t-1}}z_{0,pr}(z_t; \\epsilon_\\theta) + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2}\\epsilon_{\\theta}(z, t), \\sigma_t^2 I_n)$. $\\sigma_1,..., \\sigma_T$ denote the DDIM control parameters, and $\\gamma(t)$ indicates the penalty coefficient for the denoising step $t$ in Algorithm 1.\nIntuitively, Algorithm 1 can be viewed as replacing $z_0(z_t; \\epsilon_\\theta)$ with $z_{0,pr} (z_t; e_\\theta)$ and following the DDIM sampling process. Therefore, the reverse process PDFs are obtained by replacing $z_0(z_t; \\epsilon_\\theta)$ with $z_{0,pr}(z_t; \\epsilon_\\theta)$ in Eq. 3. More formally, under Assumption 1, the projection step (line 5) can be written as a series of gradient updates that transform $z_0(z_t; \\epsilon_\\theta)$ to $z_{0,pr} (z_t; \\epsilon_\\theta)$. Having Lipschitz continuous gradients for $f_c$ allows for fixed step sizes which can guarantee a reduction in the value of the objective function $(\\frac{1}{2}(||z - z_0(z_t; \\epsilon_\\theta)||_2 + \\gamma(t)||f_c(z)||_2^2))$ with each gradient update. We refer the readers to Sec. A.1 in the Appendix for the detailed proof. Now, we investigate the convergence properties for Algorithm 1 under the following assumption.\nAssumption 2. The real data distribution is $\\mathcal{N}(\\mu, I_n)$, where $\\mu \\in \\mathbb{R}^n$, and the constraint set $C$ is defined as $C = \\{z | Az = y\\}$ with $A \\in \\mathbb{R}^{m \\times n}$ such that $rank(A) = n \\leq m$. Additionally, for the real data distribution $\\mathcal{N}(\\mu, I_n)$ and the constraint set $C = \\{z | Az = y\\}$, there exists a unique solution to Eq. 4, indicated by $x^*$.\nWe note that Assumption 2 ensures the existence of a unique solution to the linear problem $Ax = y$. While there exist many efficient methods to solve such problems under this assumption, the focus of this paper is not on solving this problem efficiently. Instead, we use this well-studied problem as a framework to analyze the convergence properties of Algorithm 1, providing valuable insights for better practical performance.\nTheorem 2. Suppose Assumption 2 holds. For a diffusion process with noise coefficients $\\bar{\\alpha}_0,..., \\bar{\\alpha}_T$, where $\\bar{\\alpha}_0 = 1, \\bar{\\alpha}_T = 0, \\bar{\\alpha}_t \\in [0,1] \\forall t \\in [0, T]$, if $\\bar{\\alpha}_t < \\bar{\\alpha}_{t-1}$ and $\\gamma(t) = \\frac{2^{k(T-t+1)}}{A_{min}(A^TA)}$ with any design parameter $k > 1$, then in the limit as $T \\rightarrow \\infty$, Algorithm 1 returns $x^{gen}$ such that:\n$$||x^{gen} - x^* ||_2 < \\frac{\\sqrt{\\alpha_1}}{k} (||x^*||_2 + ||\\mu||_2)$$\nWe refer the readers to Sec. A.2 in the Appendix for detailed proof. Briefly, the proof in Sec. A.2 indicates that the terminal error $|| x^{gen} - x^* ||_2$ reduces to 0 as $T, k \\rightarrow \\infty$, thereby ensuring that Algorithm 1 converges to the true solution. From the proof, we observe that under Assumption 2, the convergence can be guaranteed when the penalty coefficient is set to very large values for the final denoising step. This is in accordance with our choice of penalty coefficients which assumes very large values for the final denoising step."}, {"title": "4 EXPERIMENTS", "content": "This section describes the experimental procedure, including the wide range of datasets and metrics used to evaluate CPS against the state-of-the-art constrained generation approaches.\nDatasets: We use real-world datasets from different domains, such as stocks (Yoon et al., 2019), air quality (Chen, 2019), and traffic (Hogue, 2019). Specifically, we test the performance of CPS on both conditional and unconditional variants of these datasets. We also evaluate our approach on a simulated sinusoidal waveforms dataset to generate sinusoids with varying amplitudes, phases, and frequencies specified as constraints.\nOur evaluation procedure is framed to test any approach for generating the maximum likelihood sample from a constraint set, such that the real time series samples from the constraint set were never seen during training. To achieve this, from every sample in the test dataset, we first extract an exhaustive set of features such that only one test sample exists per set of features. These features are considered constraints, which we impose on the generative model.\nConstraints: We extract the following features to be used as constraints - mean, mean consecutive change, argmax, argmin, value at argmax, value at argmin, values at timestamps 1, 24, 48, 72, & 96. For the stocks dataset, we additionally impose the natural OHLC constraint, i.e., the opening and closing prices should be bounded by the highest and the lowest prices. Similarly, for the sinusoidal waveforms dataset, we extract the locations and values of the peaks and valleys and the trend from a peak to its adjacent valley. Note that these constraints can be written in the form $Ax < 0$. Projection to such constraint sets is easy and can be handled by numerous off-the-shelf solvers (Diamond & Boyd, 2016; Virtanen et al., 2020). This allows us to analyze the effect of the sampling process without worrying about the off-the-shelf solvers that influence the projection step. We provide a budget of 0.01 for constraint violation.\nBaselines: We compare against the Constrained Optimization Problem (COP) approach (Coletta et al., 2024) and its fine-tuning variant, which is referred to as COP-FT. COP projects a random sample from the training dataset to the required set of constraints, whereas COP-FT projects a generated sample. Both these variants rely on a discriminator to enforce realism after perturbation. We also compare our approach against Guided DiffTime (Coletta et al., 2024), a guidance-based diffusion sampling approach. It is unfair to compare against Loss DiffTime, a training-based approach proposed by (Coletta et al., 2024), as the other methods were not explicitly trained for adhering to a constraint set. All baselines, except COP, utilize the same TIME WEAVER-CSDI denoiser backbone (Narasimhan et al., 2024) for fair comparison.\nMetrics: We evaluate the performance of CPS on three fronts - sample quality, ability to track the test time series, and constraint violation. For sample quality, we use the Frechet Time Series Distance (FTSD) metric (Narasimhan et al., 2024; Paul et al., 2022) for the unconditional setting and"}, {"title": "5 CONCLUSION", "content": "We proposed Constrained Posterior Sampling \u2013 a novel training-free approach for constrained time series generation. CPS is designed such that it exploits off-the-shelf optimization routines to perform a projection step towards the constraint set after every denoising step. Through an array of sample quality and constraint violation metrics, we empirically show that CPS outperforms the state-of-the-art baselines in generating realistic samples that belong to a constraint set.\nFuture work. We aim to apply our approach for constrained trajectory generation in the robotics domain with dynamics constraints, typically modeled by neural networks. Additionally, constrained time series generation readily applies to style transfer applications. Hence, we plan on extending the current work to perform style transfer from one time series to another by perturbing statistical features.\nReproducibility. The pseudo-code and hyper-parameter details have been provided in the Appendix to help reproduce the results reported in the paper. The source code will be released post publication."}, {"title": "APPENDIX", "content": "In this section, we provide the detailed proof for the theorems stated in the manuscript."}, {"title": "A.1 PROOF OF THEOREM 1", "content": "We first describe the assumption on the constraint set. The constraint set is defined as $C = \\{z | f_c(z) = 0\\}$, where $f_c : \\mathbb{R}^n \\rightarrow \\mathbb{R}$, and the penalty function $\\mathbb{I}(z) = ||f_c(z)||_2^2$ has $L$-Lipschitz continuous gradients, i.e., $||\\nabla \\mathbb{I}(u) - \\nabla \\mathbb{I}(v)||_2 \\leq L||u - v||_2 \\forall u, v \\in \\mathbb{R}^n$.\nLine 7 of the Algorithm 1 modifies the traditional DDIM sampling by replacing $z_0(z_t; \\epsilon_\\theta)$ with $z_{0,pr}(z_t; \\epsilon_\\theta)$. Without this modification, the DDIM sampling denotes the following reverse process when started with $x_T \\sim \\mathcal{N}(0_n, I_n)$, where $0_n$ indicates the zero mean vector in $\\mathbb{R}^n$ and $I_n$ is the identity matrix in $\\mathbb{R}^{n \\times n}$:\n$$p_{\\theta,t}(z_{t-1}|z_t) =\\begin{cases} p_{\\theta,init}(z_0 | z_{0}(z_1; \\epsilon_\\theta)) & \\text{if t = 1,} \\\\ q_{\\theta,t}(z_{t-1} | z_t, z_{0}(z_t; \\epsilon_\\theta)) & \\text{otherwise,} \\end{cases}$$\nwhere $q_{\\theta,t}(z_{t-1} | z_t, z_{0}(z_t; \\epsilon_\\theta))$ represents the PDF of the Gaussian distribution $\\mathcal{N} (\\sqrt{\\bar{\\alpha}_{t-1}}z_{0}(z_t; \\epsilon_\\theta) + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2}\\epsilon_{\\theta}(z, t), \\sigma_t^2 I_n)$ with $\\sigma_t$ as the DDIM control parameter. Similarly, $p_{\\theta, init}(z_0 | z_{0}(z_1; \\epsilon_\\theta))$ is the PDF of the Gaussian distribution with mean $z_0(z_1; \\epsilon_\\theta)$ and covariance matrix $\\sigma_1 I_n$ (Song et al., 2022).\nNote that sampling from $q_{\\theta,t}(z_{t-1} | z_t, z_{0}(z_t; \\epsilon_\\theta))$ provides the DDIM sampling step (check Eq. 2).\nWe reiterate that the main modification with respect to the DDIM sampling approach is the projection step in line 5 of Algorithm 1. Therefore, we first analyze the projection step,\n$$z_{0,pr}(z_t; \\epsilon_\\theta) = arg \\min_z \\frac{1}{2} (||z - z_{0}(z_t; \\epsilon_\\theta)||_2^2 + \\gamma(t)||f_c(z)||_2^2) .$$ Here, $z_{0}(z_t; \\epsilon_\\theta) = \\frac{z_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_{\\theta}(z,t)}{\\sqrt{\\bar{\\alpha}_t}}$ (line 3, predicted $z_0$). We will denote the objective function $(\\frac{1}{2}(||z - z_{0} (z_t; \\epsilon_\\theta)||_2^2 + \\gamma(t)||f_c(z)||_2^2))$ as $g(z)$. Note that we replaced the constraint violation function $\\mathbb{I}(z)$ by $|| f_c(z)||_2^2$ for this case. Given that $f_c$ is a differentiable and convex with $||f_c||_2$ having $L$-Lipschitz continuous gradients, Eq. 7 can be written as a series of gradient updates with a suitable step size such that the value of the objective function decreases for each gradient update.\nFrom the statement, we observe that $\\gamma(t) > 0 \\forall t \\in [1,T]$. Under this condition and Assumption 1, note that the function $g(z)$ is convex and has $(2+\\gamma(t)L)$-Lipschitz continuous gradients, as $||z - z_{0}(z_t; \\epsilon_\\theta)||_2$ has 2-Lipschitz continuous gradients, $\\gamma(t)||f_c(z)||_2^2$ has $(\\gamma(t)L)$-Lipschitz continuous gradients, and the fraction $\\frac{1}{2}$ makes $g(z)$ to have $\\frac{(2+\\gamma(t)L)}{2}$-Lipschitz continuous gradients. Let $\\eta$ be the step size of the projection step. From Nocedal & Wright (1999), we know that $\\eta \\in (0,\\frac{2}{(2 + \\gamma(t)L)})$ ensures that the objective function in Eq. 7 reduces after each gradient update.\nWe denote the gradient update as\n$$\\eta_{z_0} (z_t; \\epsilon_\\theta) = \\eta^{-1}z_0(z_t; \\epsilon_\\theta) - \\eta \\nabla_{z_t} (g(x))\\vert_{\\eta^{-1}z_0(z_t; \\epsilon_\\theta)},$$ where $z_0(z_t; \\epsilon_\\theta) = z_0(z_t; \\epsilon_\\theta)$ and $z_{0,pr}(z_t; \\epsilon_\\theta) = \\eta_{pr}^{N_{pr}} z_0 (z_t; \\epsilon_\\theta)$. Here, $N_{pr}$ is the total number of gradient update steps.\nThe iteration in Eq. 8 always leads to $z_{0,pr}(z_t; \\epsilon_\\theta)$ deterministically. Therefore, the projection step can be considered sampling from a Dirac delta distribution centered at $z_{0,pr}(z_t; \\epsilon_\\theta)$, i.e., $\\delta(z - z_{0,pr}(z_t; \\epsilon_\\theta))$. Consequently, using the law of total probability, the reverse process corresponding to the denoising step $t\\forall t \\in [2, T]$ in Algorithm 1 is given by\n$$p_{\\theta,t}(z_{t-1} | z_t) = \\int p_{\\theta,t}(z_{t-1}, z_0 | z_t)dz_0,$$"}, {"title": "A.2 PROOF OF THEOREM 2", "content": "We note that the intermediate samples in a T-step reverse sampling process are denoted as $z_T, ..., z_0$, where $z_0 = x^{gen}$ and $z_T \\sim \\mathcal{N}(0_n, I_n)$. Once again, we reiterate the assumptions. We consider the real data distribution to be Gaussian with mean $\\mu \\in \\mathbb{R}^n$ and covariance matrix $I_n$, i.e., $\\mathcal{N}(\\mu, I_n)$. The constraint set C is defined as $C = \\{z | Az = b\\}$ with $A \\in \\mathbb{R}^{m \\times n}$ such that $rank(A) = n$, where $m \\geq n$. Additionally, for the real data distribution $\\mathcal{N} (\\mu, I_n)$ and the constraint set $C = \\{z | Az = y\\}$, there exists a unique solution to Eq. 4, indicated by $x^*$"}, {"title": "B METRICS", "content": "For the FTSD and J-FTSD metrics, we train the time series and condition encoders using the procedure given in Narasimhan et al. (2024). For FTSD, we only train the time series encoder using supervised contrastive loss to maximize the similarity of time series chunks that belong to the same sample. For J-FTSD, we perform contrastive learning training in a CLIP-like manner to maximize the similarity between time series and corresponding paired metadata, as explained in Narasimhan et al. (2024).\nWe use Informer models as the encoders. Additionally, just as in the case of (Paul et al., 2022; Narasimhan et al., 2024), we observe that the approaches corresponding to the lowest values of FD metrics have the lowest TSTR and DTW scores, and the highest SSIM scores. This further validates the correctness of the FTSD and J-FTSD metrics used for evaluation.\nWe sourced the implementations of DTW and SSIM from the public domain. For SSIM, we used 1D uniform filters from SciPY Virtanen et al. (2020). We set the values of $C_1$ and $C_2$ to $10^{-4}$ and $9 \\times 10^{-4}$.\nFor the constraint violation magnitude, we computed the violation for each constraint, excluding the allowable constraint violation budget."}, {"title": "C DATASETS", "content": "We compared CPS against the existing baselines for six settings - air quality (Chen, 2019), air quality conditional (Chen, 2019), traffic (Hogue, 2019), traffic conditional (Hogue, 2019), stocks Yoon et al. (2019), and waveforms. The training and testing splits for the air quality and traffic datasets are taken from Narasimhan et al. (2024). We additionally evaluated the constrained generation approaches on the stocks and the waveforms datasets. We used the preprocessing scripts provided by Yoon et al. (2019) for the stocks dataset. The waveforms dataset was synthetically generated. We generated 64, 000 sinusoidal waveforms of varying amplitudes, phases, and frequencies. The amplitude varies from 0.1 to 1.0. The phase varies from 0 to 2\u03c0. The frequency limits were chosen based on the Nyquist criterion. The generators and the GAN models were trained on this dataset. However, for the TSTR metrics, we created a subset of this dataset with 16,000 samples. All the datasets except the waveforms dataset were standard normalized.\nThe Air Quality dataset is a multivariate dataset with six channels. The total number of train, val, and test samples are 12166, 1537, and 1525, respectively.\nThe Traffic dataset is univariate. The total train, val, and test samples are 1604, 200, and 201, respectively.\nThe Stocks dataset is a multivariate dataset with six channels. The total train, val, and test samples are 2871, 358, and 360, respectively.\nThe truncated form of the waveforms dataset used for evaluation consists of 13320, 1665, and 1665 train, val, and test samples, respectively.\nThe horizon length used for all datasets is 96."}, {"title": "D IMPLEMENTATION", "content": "In this section, we will describe the implementation details of our approach, each baseline, trained models, metrics, etc."}, {"title": "D.1 DIFFUSION MODEL ARCHITECTURE", "content": "We utilize the TIME WEAVER-CSDI denoiser for all the diffusion models used in this work. The training hyperparameters and the model parameters are precisely the same as indicated in (Narasimhan et al., 2024). The total number of residual layers is 10 for all the experiments. Further, we used 200 denoising steps with a linear noise schedule for the diffusion process. All the baselines and CPS use the same base diffusion model with the TIME WEAVER-CSDI denoiser backbone.\nWe use 256 channels in each residual layer, with 16-dimensional vectors representing each time series channel. The diffusion time step input embedding is a 256-dimensional vector. Further, the metadata encoder has an embedding size of 256 for the"}]}