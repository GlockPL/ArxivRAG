{"title": "Do great minds think alike? Investigating Human-AI Complementarity\nin Question Answering with CAIMIRA", "authors": ["Maharshi Gor", "Hal Daum\u00e9 III", "Tianyi Zhou", "Jordan Boyd-Graber"], "abstract": "Recent advancements of large language models (LLMS) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT) that enables quantitative assessment and comparison of problem-solving abilities of question-answering (QA) agents: humans and AI systems. Through analysis of over 300,000 responses from ~ 70 AI systems and 155 humans across thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in knowledge domains and reasoning skills. Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning, while state-of-the-art LLMS like GPT-4-TURBO and LLAMA-3-70B show superior performance on targeted information retrieval and fact-based reasoning, particularly when information gaps are well-defined and addressable through pattern matching or data retrieval. These findings highlight the need for future QA tasks to focus on questions that challenge not only higher-order reasoning and scientific thinking, but also demand nuanced linguistic interpretation and cross-contextual knowledge application, helping advance AI developments that better emulate or complement human cognitive abilities in real-world problem-solving.", "sections": [{"title": "Introduction", "content": "The NLP community has focused on human behavior emulation, treating human performance as ceiling for models. However, the latest wave of LLMS has turned the discussion to supremacy: models are purportedly acing tests (Liu et al., 2023; Hendrycks et al., 2020) that many humans find challenging."}, {"title": "Background and Preliminaries", "content": "This section describes the source of the Quizbowl QA data (\u00a7 2.1) and preliminaries of IRT and MIRT (\u00a7 2.2), the foundation of CAIMIRA (\u00a7 3)."}, {"title": "QUIZBOWL: Where Trivia Nerds Practice", "content": "Our overarching goal is to identify similarities and differences between how systems and humans respond to questions. These questions must be diverse, less prone to false presuppositions, and designed to be challenging for humans, enabling us to draw conclusions about the strengths and weaknesses of agents without needing to \"question the question\" (Min et al., 2020; Yu et al., 2022). Following the categorization by Rogers et al. (2023), we focus on depth-testing \u201cprobing\u201d questions over \"information seeking\u201d ones. This approach aligns with the Manchester paradigm outlined by Rodriguez and Boyd-Graber (2021), which highlights the significance of research agendas in the development of human-like, intelligent QA systems. More importantly, we need questions with many examples of diverse human answers. While humans may not answer Google queries (Kwiatkowski et al., 2019) for fun, they do answer trivia questions as a hobby or to prepare for trivia competitions. Hence, we use the \"Protobowl\" (He et al., 2016), a dataset of trivia questions based on the Quizbowl (QB) QA setting (Boyd-Graber et al., 2012). Quizbowl, the source of questions for ProtoBowl, is a trivia game consisting of questions with sentence-clues decreasing in difficulty and culminating with a \u201cgiveaway\u201d hint at the end of the question. It is the only open source QA dataset that contains records of many human players of varying levels of expertise answering questions across different categories like history, science and literature"}, {"title": "A review of Item Response Theory (IRT)", "content": "We compare humans and AI systems by capturing their skills using Item Response Theory (IRT), a framework used to understand question quality and participant strengths, by analyzing responses (ruled as correct or incorrect) to a set of questions (or, \"items\"). It is widely adopted in psychometrics (Morizot et al., 2009), medical education (Downing, 2003), and other fields for developing standardized tests for human subjects.\nIn the context of this work, IRT assumes (1) a set of question-answer pairs, (2) subjects spanning humans and QA systems, and (3) binary correctness rulings of their responses. The IRT objective is to predict the response correctness (Ui,j) based on the subject's skill si and the question's difficulty dj, where i and j are the indices of the subject and question, respectively. The probability of response correctness, p(Ui,j = 1), is modeled as \u03c3(si \u2013 dj), where \u03c3 is the sigmoid function.", "equations": ["p(U_{i,j} = 1|s_i, d_j) = \\sigma(s_i - d_j). \\qquad (1)"]}, {"title": "Multidimensional Latent IRT (MIRT)", "content": "To relax the monotonicity assumption and model multi-factor characteristics, MIRT was developed (Reckase, 2006; Chalmers, 2012). It models two question characteristics: a scalar difficulty dj, and an m-dimensional discriminability aj that interacts with the m-dimensional skill vector si. The skill value si,k corresponds to the agent's expertise on the kth latent aspect. The objective then becomes:", "equations": ["p(U_{i,j} = 1 | S_i, d_j, a_j) = \\sigma(s_i^T a_j \u2013 d_j). \\qquad (2)"]}, {"title": "Bootstrapping IRT with CAIMIRA", "content": "We propose CAIMIRA\u2014Content-aware, Identifiable, and Multidimensional Item Response Analysis, an IRT framework that addresses the limitations of MIRT (\u00a7 2.2) by introducing three key modifications: (i) a novel concept of relevance (rj) for each item j, (ii) zero-centered difficulty (dj), and (iii) learnable content-aware transformations (fR and fD) that produce rj and dj from the raw questions. These enable CAIMIRA to provide interpretable and identifiable results, and handle new questions without prior response data. The response prediction model, the probability of agent i correctly answering question j, for an m-dimensional CAIMIRA, is given by Equation 3.", "equations": ["p(U_{i,j} = 1 | S_i, r_j, d_j) = \\sigma ((s_i \u2013 d_j)^T r_j). \\qquad (3)"]}, {"title": "Introducing question relevance rj", "content": "An interpretable item response analysis should include an item characteristic for each question that has the single responsibility of capturing how relevant each latent aspect is for estimating the likelihood of an agent correctly answering a particular question, p(Ui,j). We call this relevance.\nRelevance rj measures how differences between and agent skills and question difficulty (si \u2013 dj), or latent scores, align across the m-dimensions (Eq 3), assigning each dimension (or, latent aspect) a proportion (rj,k) to show its importance. To ensure clarity and prevent overlap with difficulty, rj is defined as a probability distribution across the m dimensions. For instance, for a Thermodynamics question, CAIMIRA assigns greater"}, {"title": "Zero Centering of difficulty dj", "content": "Aggregating differences between agent skills and question difficulty (si \u2013 dj) across dimensions (Eq 3), leads to non-unique skill and difficulty values for same likelihood estimate p(Ui,j = 1). We alleviate this non-identifiability issue by normalizing each question's raw difficulty d to have a zero mean for each dimension (Equation 7). This normalization constrains skill and difficulty ranges and enables comparisons across dimensions."}, {"title": "Content-Aware Transformations", "content": "CAIMIRA improves upon MIRT by incorporating question content, enabling CAIMIRA to compute characteristics for new questions without requiring prior response data, making it \u201ccold-start friendly\".\nAt its core, CAIMIRA maps question text into relevance and difficulty values using learnable functions, fR, fD: Q \u2192 Rm, transforming a question qj from the space of question texts Q into raw relevance (r) and raw difficulty (d) vectors. These are modeled as linear transformations over a pre-trained embedder fE: Q \u2192 Rn (e.g., BERT), which represents qj \u2208 Q in an n-dimensional space as an embedding ej:", "equations": ["e_j := f_E(q_j) = \\text{BERT}(q_j), \\qquad (4)", "r_j := f_r(q_j) = W_R e_j + b_R, \\qquad (5)", "d_j := f_D(q_j) = W_D e_j \\qquad (6)"]}, {"title": "Zero Centering of difficulty dj", "content": "where WR, WD \u2208 Rm\u00d7n and bR\u2208 Rm are the parameters of the linear transformations. The raw values are then normalized to obtain final relevance (rj) and difficulty (dj) values:", "equations": ["r_j := \\text{softmax}(r_j),  d_j := d_j - \\frac{1}{n_q} \\sum_{j=1}^{n_q} d_j, \\qquad (7)"]}, {"title": "Agent Skills.", "content": "CAIMIRA learns an agent skill embedding matrix Ea \u2208 Rna\u00d7m, where na is the number of agents, and the skill vector for agent i is the ith row of this matrix:", "equations": ["S_i = E_i \\qquad (8)"]}, {"title": "Learning Objective.", "content": "To optimize CAIMIRA's parameters (\u0398), which include the agent skill embedding matrix Ea and the linear transformation"}, {"title": "Experimental Setup", "content": "This section describes how we collect responses from humans and QA systems, assess their answers, and analyze the latent traits learned by CAIMIRA."}, {"title": "Protobowl Logs.", "content": "We collect player logs from the \"Protobowl\" platform over QB questions spanning various categories. Player logs record question metadata, including category (e.g. History), time taken to answer the question, answer string, and the correctness ruling by the platform. The best players have deep knowledge and excellent lateral thinking skills (Jennings, 2006)."}, {"title": "Constructing QA Dataset.", "content": "QB questions are inherently multi-sentence (typically five) with each sentence serving as a distinct clue for the answer. In our dataset, each item is formed by cumulatively adding clues from a QB question, with the first item containing the initial clue and subsequent items incorporating an additional clue each; i.e., the first item consists of only the first clue, the second item comprises the first two clues together, and so on. This cumulative clue addition provides insight into how progressively revealing information affects agents' response accuracy."}, {"title": "Mapping Player Responses to Cumulative Clues.", "content": "Player responses are mapped to these cumulative clue items to analyze the effectiveness of each clue set in eliciting correct answers. Responses to q31 after only the first clue are recorded under q31_1, and responses after the second clue (which include the information from both clues) are recorded under q31_2, and so on. This mapping is further refined through a backfilling process. Because clues are meant to be progressively easier, we assume that a player who correctly answers a question at clue t, would also correctly answer the question at clue t' > t. So, we mark those as correct as well. An analogous argument holds for t' < t when humans answer incorrectly. Consequently, we collect a total of 3042 entries in our refined dataset."}, {"title": "Human Agents", "content": "In exploring the complementary QA abilities of human and AI, a key challenge is the sparsity of individual human data: most players only engage with a set of few dozen questions. To address this, we form synthetic human agents by grouping individual human players. This approach serves two primary purposes: it helps in accumulating a dataset where agents have attempted a substantial portion of the questions, and it mitigates the issue of non-representativeness of data from a few power users."}, {"title": "Group Formation and Decision Mechanism", "content": "Our dataset comprises only five human players who have answered over 1500 questions each. While these \"power users\" are invaluable, relying solely on their data could skew the understanding of human-AI interaction, as they might not be representative of the broader player base. Therefore, we introduce \"grouped human agents\". Each grouped agent is a synthetic construct, amalgamating responses from multiple human players with similar skill levels. We group human players such that the overall coverage of questions attempted by the group is maximized. In cases where multiple players in a group answer the same question, we use a majority rule to determine the group's response. If no majority is reached, a response is sampled based on the votes.\nWe consider group sizes of 1 (individual), 5, 10, and 15, creating five groups for each size, totaling 20 human agents spanning 155 distinct players. Our human participants, all fluent in US English, are experienced Quiz Bowl players. While this sample may not encompass the full diversity of the broader population, their expertise in trivia games, particularly in Quiz Bowl, allows us to contrast the"}, {"title": "AI Agents", "content": "To capture skill differentials across AI models and humans and to learn the effects of various training and modeling techniques, we select a broad range of QA systems, grouped as below:"}, {"title": "Retrievers.", "content": "These agents, indexing Wikipedia, use sparse (e.g., BM25), and dense-GRIT-LM (Muennighoff et al., 2024) and CONTRIEVER (Izacard et al., 2021)\u2014methods to fetch the k most relevant context documents to a query (where k = 1, 3, 5, 10). We call these context-retrievers. We also test a title-retriever, where only the title(s) associated with the retrieved document(s) are answer predictions. Retrievers are evaluated on recall, with a point scored if the answer appears within retrieved documents for context-retrievers, or in the title for the title-retrievers."}, {"title": "Large Language Models (LLMs).", "content": "We assess LLMS zero-shot in-context learning (Brown et al., 2020), providing a task instruction followed by a single QA pair demonstration. These LLMs include base models (OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021) and Pythia (Biderman et al., 2023)), instruction-tuned models (OPT-IML (Iyer et al., 2022), T0, T0pp (Sanh et al., 2021), Flan-T5 (Chung et al., 2022) and Flan-UL2 (Tay et al., 2022)), very large-scaled models like LLAMA-3-70B (Touvron et al., 2023), Falcon40B (Almazrouei et al., 2023), Cohere's CMD-R+9 and Mixtral 8x7b (Jiang et al., 2024), and closed-sourced APIs such as GPT-4o, GPT-4-TURBO (OpenAI, 2023) and Gemini-family (Team et al., 2024)."}, {"title": "Retriever-augmented Generative Models (RAG).", "content": "We combine above defined retrievers with generative models for answer production, primarily using FlanT5-XL (Chung et al., 2022) with top 3 documents and exploring Flan-UL2 (Tay et al., 2022), and CMD-R+ to accommodate all ten."}, {"title": "Answer Match Equivalence.", "content": "Traditional exact-match (Rajpurkar et al., 2016) often misses alternative answer that have different wordings or forms but the same semantic sense as the correct answer (Bulian et al., 2022). To better handle this, we adopt a fuzzy match evaluation using answer aliases (Si et al., 2021): if the character level matching rate between the predicted answer and the gold answer exceeds a certain threshold, the prediction is considered as correct. We tuned the threshold against human judgments on a small dev set."}, {"title": "CAIMIRA Setup", "content": "We ablate the number of latent dimensions, m. Validation loss plateaus beyond m = 5 (Fig 4). We thus train a 5-dimensional CAIMIRA model using all-mpnet-base-v2, an SBERT variant (Reimers and Gurevych, 2019) as the question embedder fE. To capture information gaps between questions and answers, we supplement SBERT's text input with both the answer and it's Wikipedia page summary. We minimize LCAIMIRA (Equation 11) using Adam optimizer (Kingma and Ba, 2014), with learning rate 0.005, batch size 512, and \u03bbd\u2081 = \u03bbs = 1e - 5."}, {"title": "Interpreting Latent Aspects.", "content": "To study the latent dimensions of CAIMIRA, we use Logistic Regression as a supplemental interpretative tool. We build upon Benedetto et al. (2020), which uses Linear Regression to post-hoc explain the latent item difficulty parameters, and follow Gor et al. (2021) to interpret the latent relevance dimensions using logistic regression. For each latent dimension (k), Logistic Regression predicts if the relevance rjk is greater than 0.6 as a function of interpretable features extracted from the questions. These features span topical question subcategories, clue counts, temporal expression mentions, question similarity with corresponding Wikipedia pages (Wiki-MatchScore), and linguistic features from Lee et al. (2021)."}, {"title": "Question and Agent Analysis", "content": "This section interprets the latent aspects of CAIMIRA, emphasizing their role in differentiating agent skills. It also examines the patterns of question difficulty and agent performance."}, {"title": "Latent aspects and Agent skills", "content": "CAIMIRA uncovers five latent aspects, each capturing distinct question styles and content, determined by specific linguistic and topical features. These aspects highlight varying agent skills across the latent dimensions. In naming and interpreting these aspects, we draw on educational assessment frameworks, particularly Bloom's Taxonomy (Anderson and Krathwohl, 2001), which emphasizes the stages of knowledge recall, comprehension, and application\u2014skills central to the Quizbowl dataset."}, {"title": "Abductive Recall.", "content": "The first aspect captures a cognitive process that combines elements of inferential reasoning with targeted knowledge retrieval. It requires bridging indirect clues and vague references to formulate the information gap, and recalling specific entities to fill the gap. This distinguishes it from purely creative and commonsense-based abductive reasoning tasks in linguistics literature (Bhagavatula et al., 2019; Shi et al., 2024). We term this aspect \"abductive recall\" to highlight the interplay between hypothesis generation and gap resolution through targeted fact retrieval. Questions often narrate events and describe characters from a fictional realm while deliberately avoiding direct references to named entities or key phrases A low WikiMatchScore\u2014semantic"}, {"title": "History and Events.", "content": "In contrast, the second dimension involves historically grounded questions, where the information gap is clearer, though the queries are complex. These questions challenge participants to synthesize multiple pieces of information and infer connections between events. For e.g, \"This man was killed by a crossbow bolt while besieging the castle Charlus-Chabrol\", requires identifying both the event and the historical figure. While these questions still feature lower WikiMatchScores, the gap is more structured, centering around entity relations like events, people, and places. Bigger LLMS excel in this category, often outperforming humans and retrievers, suggesting effective recall and application of historical information through their parametric memory."}, {"title": "Scientific Facts.", "content": "This aspect focuses on domain-specific conceptual knowledge, often featuring questions from scientific domains. Retrieval-based systems fare well when allowed to retrieve sufficient documents Notably, these questions, along with history-related ones, best differentiate instruction-tuned LLMS from base models, with the former outperforming the latter. Humans and large-scale LLMS excel in this category, as do closed-source systems like GPT-4-TURBO."}, {"title": "Cultural Records.", "content": "This aspect represents questions focusing on prominent figures such as authors, composers, artists, and leaders, asked in the style of \"who did what\", testing direct knowledge recall"}, {"title": "Complex Semantics.", "content": "The final aspect pertains to questions about popular events, featuring complex semantic relationships and detailed sentences with less common, domain-specific keywords. Despite their intricacy, they are particularly retriever-friendly due to high WikiMatchScores, indicating a significant overlap with relevant source documents. The most prominent fact about the answer is directly mentioned in both the question and the document, enabling retrievers to locate correct documents. However, agents without retrieval abilities, or large parametric memories, struggle."}, {"title": "Which Questions are most difficult?", "content": "To identify groups of questions that present different challenges, we analyze each question's effective difficulty, denoted as de. This metric represents the contribution of the k-th latent aspect to the difficulty of question j, calculated as rj,kdj,k according to Equation 3. We cluster questions into twelve groups using KMeans on their 5-dimensional effective difficulty de, then analyze mean relevance and mean effective difficulty per cluster across dimensions (Fig 10, full set in Appendix E). The mean effective difficulty d(e) on the dimension k for a question set D is calculated as a weighted mean of the effective difficulty scores over the questions in D, normalized by the total relevance.", "equations": ["d^{(e)}_{D,\\mu_k} = \\frac{\\sum_{j \\in D} r_{j,k} d_{j,k}}{\\sum_{j \\in D} r_{j,k}} \\qquad (13)"]}, {"title": "Related Work", "content": "Adoption of IRT in NLP. Current evaluation paradigms for machine and human QA inadequately segment datasets, treating questions as independent single transaction without assessing relative differences between the test set items. To remedy this, Lalor et al. (2019) propose adopting the IRT ranking method from educational testing as a novel evaluation framework for NLP. Rodriguez et al. (2021) argue for the adoption of IRT as the de facto standard for QA benchmarks, demonstrating its utility in guiding annotation effort, detecting annotator error, and revealing natural partitions in evaluation datasets. Byrd and Srivastava (2022) further uses IRT to estimate question difficulty and model skills, and use question features to post-hoc predict question difficulty. Yet, existing studies are confined to a one-dimensional IRT models. Our research advances this domain by enhancing the learning method and capturing question traits that effectively differentiate human and AI QA abilities.\nIdeal Point Models (IDP) IRT and IPM are two prominent statistical models used in different fields for distinct purposes. Both models deal with"}, {"title": "Human-AI Complementarity.", "content": "Research in NLP has increasingly focused on augmenting human skills with language models, particularly in the areas like creative writing and question-answering. Studies have explored collaborative writing with LLMS, such as having human writers use GPT-3 for suggestions (Lee et al., 2022) or modifying user-selected text spans for enhanced descriptiveness (Padmakumar and He, 2021). For trivia, experts and novices have teamed up with AI (Feng and Boyd-Graber, 2018), and for information retrieval, humans used AI-generated queries to find answers Our approach diverges by focusing modeling latent factors that best accentuate the distinct capabilities of trivia nerds and AI in QA. This strategy aims to identify the benchmarking methods for assessing and enhancing AI systems in subsequent work."}, {"title": "Conclusions", "content": "CAIMIRA enables discovery and interpretation of latent aspects in QA datasets that highlight the skills of various QA agents. On contrasting AI systems with humans, we find notable disparities: systems like GPT-4-TURBO and Gemini Pro excel at direct, context-rich queries that require connecting events and figures, but struggle with indirectly phrased questions lacking explicit entity references-domains where human acumen shines.\nAlthough GPT-4-TURBO matches individual human performance on complex knowledge-intensive abductive reasoning tasks, we caution against interpreting this as indicative of superhuman abilities. Given that the quiz questions that Protobowl is based off have been publicly available since 2011, and that these models' training data is not fully known, accurately assessing the reason for their near-perfect performance is challenging. Future research should aim to develop stronger and innovative evaluations that better gauge AI systems' ability to understand implicit contexts, and systematically contrast their skills with those of humans.\nLastly, this work opens up new avenues for research on estimating agent skills that can be combined to assess multi-agent systems and collaborations, which becomes crucial as NLP evolves toward conversational agents and real-world problem-solving."}, {"title": "Limitations", "content": "Dataset and Task Limitations Our study faces constraints related to dataset and task setup: (1) Limited language diversity: Our English-only dataset restricts generalizability to other languages. (2) Lack of diverse task types: We rely solely on trivia-based questions, lacking non-trivia datasets with human responses in competitive settings. (3) Absence of multilingual trivia benchmarks: We lack multilingual trivia datasets with human responses and performance benchmarks. Future work should address these by creating datasets that include non-trivia tasks, multiple languages, and human responses, offering a more comprehensive understanding of human and AI performance across diverse linguistic and task environments.\nChallenges in interpreting near-perfect scores While models like GPT-4-TURBO match or exceed individual humans on complex tasks, caution is needed when interpreting these results as superhuman. Quiz questions in our Protobowl-based dataset have been public since 2011, and the models' full training data is unknown. This makes it difficult to determine if their near-perfect performance stems from genuine reasoning or exposure to specific questions during pre-training. genuine reasoning or exposure to specific questions during pre-training. This limitation highlights the need for more robust evaluation methods to accurately assess Al systems' understanding and reasoning abilities compared to humans.\nLack of information on specific human players Because of the nature of the Protobowl platform that we used to collect the human response data, we do not have access to information about the specific human players to incorporate that into our analysis. Future work can focus on collecting such information whilst hiding the user identity."}, {"title": "Ethical Considerations", "content": "In conducting this study, we adhered to strict ethical guidelines to ensure respect for privacy, obtaining informed consent from human participants and annonimization of their data. Our work complies with all relevant ethical standards, underscoring our commitment to ethical research practices in advancing NLP technologies. We utilized GitHub Copilot for low level coding and writing assistance-reimplementing plotting codes, as well as editing the prose in this document to improve readability and conciseness.\nRegarding ethical considerations about running computationally expensive models, we acknowledge the carbon footprint of training and running large-scale language models. In our study we only train a very small of order 25000 parameters, for 20 minutes of single A4000 GPU time. We also use a pre-trained SBERT model for encoding the question text."}]}