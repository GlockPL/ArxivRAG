{"title": "WEB ARCHIVES METADATA GENERATION WITH GPT-40: CHALLENGES AND INSIGHTS", "authors": ["Abigail Yongping Huang", "Ashwin Nair", "Goh Zhen Rong", "Liu Tianrui"], "abstract": "Current metadata creation for web archives is time consuming and costly due to reliance on human effort. This paper explores the use of gpt-40 for metadata generation within the Web Archive Singapore, focusing on scalability, efficiency, and cost effectiveness. We processed 112 Web ARChive (WARC) files using data reduction techniques, achieving a notable 99.9% reduction in metadata generation costs. By prompt engineering, we generated titles and abstracts, which were evaluated both intrinsically using Levenshtein Distance and BERTScore, and extrinsically with human cataloguers using McNemar's test. Results indicate that while our method offers significant cost savings and efficiency gains, human curated metadata maintains an edge in quality. The study identifies key challenges including content inaccuracies, hallucinations, and translation issues, suggesting that Large Language Models (LLMs) should serve as complements rather than replacements for human cataloguers. Future work will focus on refining prompts, improving content filtering, and addressing privacy concerns through experimentation with smaller models. This research advances the integration of LLMs in web archiving, offering valuable insights into their current capabilities and outlining directions for future enhancements. The code is available at https://github.com/masamune-prog/warc2summaryfor further development and use by institutions facing similar challenges.", "sections": [{"title": "1 Introduction", "content": "The digital landscape is constantly evolving and the need to preserve our online heritage has become increasingly urgent. The Resource Discovery (RD) department of the National Library Board Singapore (NLB) [1] provides cataloguing services for the collections of the National Library (NL) and public libraries in Singapore. NL, like many other archives and libraries worldwide, collects and archives websites in its effort to preserve the mercurial history of the web. This collection is known as \"Web Archive Singapore\" [2]. Each website is manually reviewed and catalogued according to the local application profile based on Dublin Core [3]. NL had been crawling websites on a curated basis since 2006, with each website requiring individual website owner consent. In 2019, the National Library Board Act [4] was amended to impart NL with the authority to harvest all websites in the .sg domain without explicit consent from each individual website owner. The legislative change resulted in the explosive growth of the web collection [5]. As of June"}, {"title": "1.1 Background and Motivation", "content": "This paper addresses the critical need for an efficient and accurate method of generating metadata for web archive collections. We focus on the challenge of managing large-scale web archives, where manual metadata curation is no longer practical due to resource constraints and the sheer volume of data."}, {"title": "1.2 Problem Statement", "content": "This paper addresses two primary challenges:\n1. Efficiency: How can we develop an automated system that can process and generate metadata for a large number of websites that significantly reduces the time and resources required?\n2. Accuracy: How can we ensure that the automatically generated metadata maintains a decent level of quality, accurately representing the content of the archived web pages?\nThese challenges are non-trivial for collections such as WAS, which aims to preserve the digital heritage of Singaporean life, culture, and history in the 21st century [2]. With an estimated 20,000 Web ARChive (WARC) files created annually that comply with quality control standards, there is a critical need for a scalable, reliable, and cost-effective solution for metadata generation."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Web Archive Files", "content": "WARC files are a standardized format crucial for web crawling, archiving, and digital preservation [6]. They play a significant role in historical web content preservation and the development of Large Language Models (LLMs) [7]. For instance, Common Crawl, a major source of training data for LLMs, provides monthly crawls of billions of web pages in WARC format, significantly contributing to models such as GPT-3 [8]. Additionally, WARC files ensure that historical web data remains accessible and support research into web trends and digital culture by offering detailed snapshots of web pages [6]. This capability is essential for tracking changes and innovations, while the format's design ensures effective accessibility and interoperability for both research and archiving [9, 10]."}, {"title": "2.2 Metadata Strategies for Web Archives", "content": "This work focuses on the provision of descriptive metadata to support search and access of individual sites in a web archive. There can be varying metadata approaches [11] such as :\n\u2022 Harvesting the metadata as-is\n\u2022 Reviewing only basic metadata fields such as Title and Language\n\u2022 Providing fuller metadata for specially curated web collections, including assigning subject headings\nIn many cases, a combination of approaches is used. For institutions conducting large scale automating crawling (e.g. by domain), this would be:\n\u2022 for websites that have been curated into themes, metadata is individually reviewed and enhanced\n\u2022 for the remainder of full crawl, metadata is used as-is\n\u2022 for both, full-text search is available."}, {"title": "2.3 Large Language Models", "content": "The advent of LLMs has revolutionized the field of Natural Language Processing (NLP). These sophisticated models, capable of processing and generating human-like text, have surpassed traditional methods in numerous NLP tasks. What was once considered a complex challenge, such as text summarization, is now routinely managed with impressive accuracy and efficiency by LLMs. Leading the charge in the commercialization of large language models are models such as gpt-40 by OpenAI [19], Claude by Anthropic [20], and Gemini by Google [21]. Built upon the groundbreaking transformer architecture introduced in \"Attention Is All You Need\" [22], these models leverage the attention mechanism to outperform earlier methods for NLP tasks that relied heavily on linguistic techniques like stemming and lemmatization or other neural networks like Recurrent Neural Networks (RNN) and Gated Recurrent Units (GRU). The integration of Reinforcement Learning from Human Feedback [23] and Direct Preference Optimization [24] further enhances the quality of LLM-generated output, making them more human-like thus suitable for generation of high quality content. Fine-tuning LLMs with datasets from a particular domain not only boosts their contextual understanding and expertise in domain specific tasks but also enhances the accuracy of its output [25, 26]."}, {"title": "2.4 In-Context Learning", "content": "However, given the limitations of time and computational resources, the process of fine-tuning Language Models for specific downstream tasks often incurs significant costs. As a result of this challenge, in-context learning (ICL) has gained significant popularity. The ICL technique enables LLMs to acquire new tasks without modifying the model parameters, by relying on task-specific examples provided in the input context. ICL has demonstrated strong performance in few-shot and zero-shot learning. Zero-shot learning leverages the model's existing knowledge to deduce task requirements. As illustrated by Brown et al., GPT-3 was utilized to showcase the system's versatility in the absence of task-specific fine-tuning. With few-shot learning, the model is provided with a restricted number of examples to showcase the task, resulting in enhanced comprehension and performance [8]. Radford et al have demonstrated that few-shot prompting can significantly enhance the model's accuracy across a diverse range of tasks [27]."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Data Collection and Preparation", "content": "To obtain web data for our inquiry, we collected a total of 112 WARC (Web ARChive) files from the Web Archives of Singapore. The HTML content from these files was extracted using the WARCIO [32] and fastwarc [33] python libraries. BeautifulSoup facilitated the extraction of relevant metadata, such as titles and primary text content, from the HTML. We removed unnecessary tags and scripts during this procedure to ensure that the main content is highlighted. We standardized the URLs and conducted a quality assurance assessment to eliminate any substandard or irrelevant data, ensuring uniformity. This involved identifying common indicators of non-functional material, such as \"404\" errors or placeholder text like \"lorem ipsum.\" In addition, we implemented a deduplication technique to consolidate individual records obtained from duplicate URLs. This ensured the preservation of the information's originality and relevance.\nWe employed multi-threaded processing using a Thread PoolExecutor to efficiently handle the vast amount of data, resulting in optimized resource utilization and significantly reduced processing time. The final step was combining the processed data into DataFrames, which were then ready for further analysis and model development. An indispensable requirement for the reliability and robustness of our investigation was a high-caliber dataset ensured by this approach."}, {"title": "3.2 Heuristics for Data Reduction", "content": "To reduce the number of tokens (and consequently cost) of the input to be used, our methodology involved developing and evaluating various heuristic methods for efficient content extraction and summarization. The three heuristics were:\n1. About Page Priority: We prioritized extracting content from the 'About' page of the website. In cases where an 'About' page was unavailable, we defaulted to using the content from the shortest URL\n2. Shortest URL: This method involved extracting content exclusively from the web page with the shortest URL\n3. Shortest URL with Regex Filtering: We extracted content from the shortest URL and applied regular expression (regex) filters to reduce the token count, thereby optimizing the input for our model.\nThese heuristics were inspired by observations of professional cataloguers, who typically require only a few pages to make accurate judgments about content categorization. This step reduces the token count across all heuristic methods. By adopting this methodology, we systematically compared various strategies for content extraction and summarization, striking a balance between computational efficiency and accuracy in content representation."}, {"title": "3.3 Prompt Engineering for Title and Abstract Generation", "content": "After some initial testing with in-context learning in LLMs, we settled on two meticulously designed prompts with contrasting characteristics for generating titles and abstracts to enhance metadata accuracy and website classification. These COT prompts guide the generation process by providing clear instructions for cataloguing and summarizing website content. The output was then subjected to both automated and manual evaluation processes. This prompt, which builds upon the previous one, provides specific rules for the summarization of a variety of websites, such as corporate websites, personal blogs, and property listings. It guarantees that the summary is appropriate for the website's type, thereby enhancing the relevance and precision of the abstract that is generated.\nPrompt 1:\nYou are a diligent cataloguer working to create metadata for websites. Let's think step by step to ensure accurate and comprehensive metadata creation:"}, {"title": "3.4 Evaluation Methods", "content": "Automated Evaluation:\nWe employed an aggregation of two metrics to assess the quality of approaches used.\n\u2022 Levenshtein Distance [34] for title comparison\n\u2022 BERTScore [35](using bert-based-cased as the embedding) for title and content similarity.\nOur ranking algorithm evaluates heuristic variants based on their ability to generate abstracts and titles through an aggregated scoring system. This method integrates three primary criteria: the minimal median of Levenshtein Distance, the maximum median of BERTScore, and the minimum standard deviation. The semantic similarity between reference and produced texts is assessed by BERTScore through contextual embeddings from BERT; a larger median indicates increased quality. The exact match accuracy is measured by the Levenshtein Distance; a lower median indicates a closer alignment with the reference. The standard deviation of BERTScores is a metric that indicates consistency; values that are lower indicate more consistent performance. A total score is generated by combining the rankings of heuristics for each criterion. This comprehensive method ensures a fair evaluation of accuracy and consistency, thereby influencing our selection of heuristics for text production."}, {"title": "3.5 Software and Hardware", "content": "To verify if heuristics are able to effectively reduce the number of tokens needed for metadata generation, we wrote a python package warc2summary which provides a pipeline for metadata creation and evaluation.In short, this package uses WARCIO [32] and FastWARC [33] as the WARC file processor and for title and abstract generation we use OpenAI API (gpt-40) as the main model and Instructor [38] to constrain the output.\nWe ran the software on a Lenovo Thinkpad T14 with a 11th Gen Intel(R) Core(TM) i5-1145G7 CPU and 16GB of DDR4 RAM. The pipeline took close to 2 hours to process the WARC files and another 2 hours to generate the synthetic data from API calls. We expect faster performance in WARC file processing with a better CPU. In addition, the API call pipelines were called sequentially in order to not breach OpenAI API rate limits. With higher rate limits, it is possible to parallelise the calls for better performance.The WARC files are warc.gz files and are taken from Web Archive Singapore. This provided us with a Dataframe containing synthetic titles and abstracts.\nWe identified 6 different combinations of prompts and heuristics as stated above. We then used a ranked aggregation scoring method, incorporating Levenshtein Distance for the title and BERTScore (using bert-based-cased embeddings), with criteria of maximum BERTScore median, minimum Levenshtein median, and minimum standard deviation for both, to shortlist two specific combinations of prompts and heuristics. Finally, a team of 8 trained cataloguers performed manual grading."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Data Reduction", "content": "From the DataFrame, each heuristic finished within 15 seconds on 112 files. The heuristics were consolidated. Based on our heuristics, the final DataFrame should have an identical number of rows as the number of WARC files. Based on the collection of 112 WARC files we have, we obtain a 99.9% reduction in total token count, and consequently costs compared to letting OpenAI parse the entirety of the WARC files."}, {"title": "4.2 Statistical Test", "content": "We performed Cochran's Q test by having trained cataloguers rate the titles and abstracts, without knowledge of their provenance. Based on a 5% confidence level, we found that the synthetic titles and abstracts are statistically distinguishable from human-generated titles and abstracts. Since the p-value (0.02) is less than our significance level ($\\alpha = 0.05$), we reject the null hypothesis, indicating a significant difference between the heuristic-generated and human-generated metadata."}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Implications of Results", "content": "Our study demonstrates both the potential and limitations of using Large Language Models like gpt-40 for automated metadata generation in web archives:\n1. Scalability and Efficiency: Our approach efficiently processes large volumes of web archive data, addressing a critical challenge in digital preservation. The method achieves a 99.9% reduction in associated costs compared to full WARC processing.\n2. Cost-effectiveness: By reducing the need for manual cataloguing, our approach significantly lowers the costs associated with metadata creation, allowing institutions to reallocate resources to other critical tasks.\n3. Quality Comparison: Our adaptation of the Turing Test, involving 8 cataloguers evaluating metadata from different sources, provided valuable insights into the current capabilities of LLMs, specifically gpt-40 in this domain. The synthetic titles and abstracts were statistically distinguishable from human curated metadata, with a p-value of 0.02, suggesting that human generated metadata still maintains an edge in quality. Furthermore, there is no significant difference between LLM based approaches, with rules and without rules."}, {"title": "5.2 Limitations and Challenges", "content": "Our study revealed several important limitations and challenges, both from our experimental results and broader considerations in the field.\n1. Content Accuracy and Hallucinations: 19.6% of LLM generated titles and abstracts had content issues such as inaccuracies, incompleteness, or hallucinations, higher than the 6.3% observed in human-curated metadata. This highlights the known challenge of LLM hallucinations, where generated content can be factually incorrect or not grounded in the input data. Mitigating these hallucinations remains an active area of research.\n2. Language Translation: Both LLM and human generated metadata faced challenges with language translation, highlighting the complexity of handling multilingual content in web archives.\n3. Legal Considerations: The generation and use of metadata from copyrighted web content raises complex legal questions. Navigating fair use and copyright in the context of web archiving and LLM generated metadata requires careful consideration to avoid potential legal complications [40].\n4. Privacy and Dependency Concerns: The automated nature of LLMs for metadata generation poses challenges in honoring individual requests for content removal or anonymization, which are crucial aspects of privacy rights. Additionally, our reliance on proprietary, closed-source LLMs like gpt-40 introduces dependencies that may limit flexibility and control. LLMs can potentially leak training data, raising concerns about privacy and data protection [41]. The potential for these models to use web content for further training underscores the need for transparency and alternatives."}, {"title": "6 Conclusion", "content": "This study introduces a novel approach to generating metadata for web archives using gpt-40, combining data reduction heuristics with evaluation methods inspired by the Turing Test. Our findings highlight the efficiency and scalability of LLM generated metadata, though it still falls short in quality and accuracy compared to human-curated metadata. gpt-40 produced more inaccuracies and hallucinations but is cost effective and scalable for WARC file metadata creation. This suggests LLMs are best as assistive tools for human cataloguers rather than replacements. Future efforts could focus on changing the prompts to fit the cataloguers' standards, more aggressive heuristics to filter out promotional website content, developing strategies to reduce and identify hallucinations and using smaller language models to circumvent privacy concerns.\nOur study marks a significant step towards leveraging AI in web archiving, offering valuable insights into its current capabilities and limitations. Addressing the identified challenges will help us work towards a future where AI enhances the preservation and accessibility of digital heritage, maintaining high standards crucial for the utility of web archives. This research sets a roadmap for future improvements, aiming to bridge the quality gap between AI and human curated metadata."}]}