{"title": "COMBINING TEXT-BASED AND DRAG-BASED EDITING FOR PRECISE AND FLEXIBLE IMAGE EDITING", "authors": ["Ziqi Jiang", "Zhen Wang", "Long Chen"], "abstract": "Precise and flexible image editing remains a fundamental challenge in computer vision. Based on the modified areas, most editing methods can be divided into two main types: global editing and local editing. In this paper, we discussed two representative approaches of each type (i.e., text-based editing and drag-based editing. Specifically, we argue that both two directions have their inherent draw-backs: Text-based methods often fail to describe the desired modifications pre-cisely, while drag-based methods suffer from ambiguity. To address these issues, we proposed CLIPDrag, a novel image editing method that is the first try to combine text and drag signals for precise and ambiguity-free manipulations on diffusion models. To fully leverage these two signals, we treat text signals as global guidance and drag points as local information. Then we introduce a novel global-local motion supervision method to integrate text signals into existing drag-based methods (Shi et al., 2024) by adapting a pre-trained language-vision model like CLIP (Radford et al., 2021). Furthermore, we also address the problem of slow convergence in CLIPDrag by presenting a fast point-tracking method that enforces drag points moving toward correct directions. Extensive experiments demonstrate that CLIPDrag outperforms existing single drag-based methods or text-based methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, notable breakthroughs in diffusion models (Ho et al., 2020; Song et al., 2020a;b), have led to many impressive applications (Meng et al., 2021; Dong et al., 2023; Kumari et al., 2023). Among them, image editing is recognized as a significant area of innovation and has gained enormous at-tention (Kim et al., 2022; Nichol et al., 2021; Sheynin et al., 2024; Valevski et al., 2023). Generally, the goal of this task is to edit realistic images based on various editing instructions. Mainstream methods for image editing can be coarsely categorized into two groups: 1) Global Editing, edits given images by a text prompt (Li et al., 2023; Kawar et al., 2023) or an extra image (Zhang et al., 2023a; Epstein et al., 2023) containing global information of the desired modification. Most of them involve finetuning a pre-trained diffusion model. 2) Local Editing, mainly consists of drag-based methods (Pan et al., 2023). This framework requires users to click several handle points (handles) and target points (targets) on an image, then perform the semantic edit to move the content of han-dles to corresponding targets. Typical methods (Mou et al., 2023; Shi et al., 2024) usually contain a motion supervision phase that progressively transfers the features of handles to targets by updating the DDIM inversion latent, and a point tracking phase to track the position of handles by performing a nearest search on neighbor candidate points.\nAlthough the aforementioned methods have gained significant prominence, two drawbacks of them cannot be overlooked. 1) Imprecise Description in Global Editing. Global editing such as text-based image methods (Kawar et al., 2023) is unambiguous but is hard to provide detailed edit in-structions. For example, in Figure 1(b), the prompt (i.e. \"The sculpture is smiling\") tells the model how to edit the image, but it is difficult to provide fine-grained editing information, like the ex-tent of the smile. 2) Ambiguity Issues in Local Editing. Although local editing like drag-based editing methods can perform precise pixel-level spatial control, they suffer from ambiguity because the same handles and targets can correspond to multiple potential edited results. For example, in Figure 1(a), there exist two edited results meeting the drag requirements: one enlarging the face, the other making the woman smile. A natural approach to solving the ambiguity problem is to add more point pairs, but it does not work in real practice. This is because the diffusion network is a Markov chain and related errors accumulate as the update continues. Thus when adding more points, it usually means more update iterations, which will result in the degradation in image fidelity. In conclusion, local editing is precise but ambiguous while global editing is exactly the opposite.\nSince these two kinds of edit are complementary, it is natural to ask: can we combine these two control signals to guide the image editing process? In this way, text signals can serve as global information to reduce ambiguity. Meanwhile, drag signals can act as local control signals, pro-viding more detailed control. However, combining these two signals presents two challenges: 1) How to integrate two different kinds of signals efficiently? This is difficult because text-based and drag-based methods have completely different training strategies. Specifically, most text-based methods (Kim et al., 2022) require finetuning a pre-trained diffusion model to gradually inject the prompt information. But drag-based methods (Shi et al., 2024) typically involve freezing the diffu-sion model and only optimizing the DDIM inversion latent of the given image. Besides, the update in text-based editing often involves the whole denoising timesteps while drag-based approaches only focus on a specific timestep. 2) How to solve the optimization problem and maintain the image quality? Previous drag-based methods are very slow in some situations. Sometimes the handles will stuck at one position for many iterations. In worse situations, they even move in the oppo-site direction of targets. This phenomenon becomes more serious when adding text signals because combining two different signals usually requires more optimization steps. Thus we need a better approach to optimize the update process while maintaining fidelity as much as possible.\nTo address these problems, we propose CLIPDrag, the first method to combine text-based and drag-based approaches to achieve precise and flexible image editing (For brevity, we denote this kind of editing as text-drag edit). This approach was built based on the general drag-based diffusion editing framework, which optimizes the DDIM inversion latent of the original image at one specific timestep. Specifically, we propose two modules to solve the aforementioned problems after a typical identity finetuning process. 1) Global-Local Motion Supervision (GLMS). The key of GLMS is to utilize the gradient from text and drag signals together for ambiguity-elimination. Specifically, for text information, we obtain the global gradient by backward of the global CLIP loss for the latent. Similarly, for drag points, the local gradient is calculated using a similar paradigm in DragDif-fuion (Shi et al., 2024). Because CLIP guidance methods cannot operate at a single fixed step, simply adding these two gradients is ineffective. Specifically, in GLMS, We disentangle the global gradient into two components: Identity Component perpendicular to the local gradient to maintain the image's global structure information, and Edit Component parallel to the local gradient, which will be combined with the local gradient from drag signals to edit the image. By comparing their"}, {"title": "2 RELATED WORK", "content": "Text-based Image Editing. Unlike text-to-image generation which involves creating an image from scratch (Ho et al., 2022; Dhariwal & Nichol, 2021; Saharia et al., 2022; Gu et al., 2022), text-based image editing is about altering certain areas of a given image. Typically, training-based methods can achieve reliable performance in various editing tasks through training techniques like weak supervision (Kim et al., 2022; Zhu et al., 2017), self-supervision (Zhang et al., 2023b; Xie et al., 2023) or full supervision (Sheynin et al., 2024; Xie et al., 2023). Test-time finetuning editing approaches edit images on a significant step by different strategies, ranging from model fine-tuning (Valevski et al., 2023) to embedding finetuning (Yang et al., 2023). Another line is training and fine-tuning free approaches. Instead of optimizing the model of intermediate embeddings, these methods perform the edit by refining the text prompt (Lin et al., 2023) or modifying the attention map (Hertz et al., 2022; Cao et al., 2023).\nDrag-based Image Editing. This kind of method achieves precise spatial control over specific re-gions of the image based on user-provided drag instructions, first proposed by DragGAN (Pan et al., 2023). DragDiffusion (Shi et al., 2024) transfers this idea to the realm of diffusion model. Building on this foundation, GoodDrag (Zhang et al., 2024), StableDrag (Cui et al., 2024), Drag Noise (Liu et al., 2024), and FreeDrag (Ling et al., 2023) have made significant improvements to this method. Besides, by utilizing feature correspondences, DragonDiffusion (Mou et al., 2023) and DiffEdi-tor (Mou et al., 2024) transform the image editing task into a gradient-based process by formulating an energy function that aligns with the desired edit results."}, {"title": "3 APPROACH", "content": "Problem Formulation. Given an image I and two text prompts Po and Pe, where Po describes the original image and Pe depicts the edited image. After choosing n handles {h1,h2,..., hn} and corresponding targets {91, 92,..., gn}, a qualified text-drag edit method has to satisfy the following two requirements: 1) Move the feature of point hi to gi, while preserving the irrelevant content of the original image. 2) The edited image must align with the edit prompt Pe.\nGeneral Framework: As shown in Figure 2, our method CLIPDrag, consists of three steps:\n1) Identity-Preserving Finetuning (Sec 3.1): Given the input image I and its description Po, We first finetune a pre-trained latent diffusion model es using the technique of low-rank adpation (LoRA (Hu et al., 2021)). After the finetuning, we encode the image into the latent space and obtain the latent zt at a specific timestep t through DDIM inversion. zt will be optimized for many iterations to achieve the desired edit, and each iteration consists of two following phases."}, {"title": "3.1 IDENTITY-PRESERVING FINETUING", "content": "As analyzed in previous work (Shi et al., 2024), directly optimizing the latent z\u0142 in diffusion-based methods will cause the problem of image fidelity. So finetuning on a pre-trained diffusion model is necessary to encode the features of the image into the U-Net. Specifically, the image and its description prompt Po are used to finetune the diffusion model 60 through the LoRA method:\n$\\mathcal{L}_{f t}(z, \\Delta \\theta)=\\mathbb{E}_{\\epsilon, t}\\left[\\left|\\epsilon-\\epsilon_{\\theta+\\Delta \\theta}\\left(a_{t} z+\\sigma_{t} \\epsilon\\right)\\right|\\right]$\nwhere z is the latent space feature map concerning image I. \u03b8 and \u2206\u03b8 represent the U-Net (Ron-neberger et al., 2015) and LoRA parameters, at and ot are constants pre-defined in the diffusion schedule, e is random noise sampled from distribution N(0, I). After the finetuning, we choose a specific timestep t and obtain the DDIM inversion latent (Song et al., 2020a) as follows:\n$z_{t+1}=\\sqrt{a_{t+1}}\\left(\\frac{z_{t}-\\sqrt{1-a_{t}} \\cdot \\epsilon_{\\theta}\\left(z_{t}\\right)}{\\sqrt{a_{t}}}\\right)+\\sqrt{1-a_{t+1}} \\cdot \\epsilon_{\\theta}\\left(z_{t}\\right)$\nthe latent will be optimized in the subsequent process while keeping all other parameters frozen."}, {"title": "3.2 GLOBAL-LOCAL MOTION SUPERVISION", "content": "This step aims to combine text and drag signals, as shown in Figure 3. We will introduce how to process each control information, and then show how to combine them."}, {"title": "CLIP-guidance Gradient.", "content": "We extract knowl-edge from text signal using a local direction CLIP loss (Kim et al., 2022), which aligns the direction between the source image I and gen-erates image \u00ce with the direction between orig-inal prompt P. and edit prompt Pe:\n$\\mathcal{L}_{direction}(I, \\hat{I}, P_{o}, P_{e}) := 1-\\frac{<\\Delta I, \\Delta T>}{\\|\\Delta I\\|\\|\\Delta T\\|}$\nwhere \u0394I = E1(\u00ce(z)) \u2013 E1(\u0399), \u0394\u03a4 = \u0415\u0442 (\u0420\u0435) - \u0415\u0442 (\u0420\u043e). Here \u0415\u0442, E\u2081 represents text and image encoder from a pre-trained CLIP model. However, the identity component of Pe is canceled out during the calculation of A\u03a4, making it difficult to maintain the image iden-tity. Besides, sometimes it is impossible to cal-culate the direction loss because Po is not pro-vided (Zhang et al., 2024). Consequently, we choose the global target loss to extract more informa-tion from edit prompt Pe as follows:\n$\\mathcal{L}_{global}(\\hat{I}, P_{e}) := 1-\\frac{<E_{I}(\\hat{I}(z)), E_{T}(P_{e})>}{\\|E_{I}(\\hat{I}(z)\\|\\|E_{T}(P_{e})\\|}$\nThen we can obtain the global gradient (G9) from the text signal: Gg = dLglobal/dz. Later we will explain how to use Gg to maintain the image identity and guide the edit."}, {"title": "Drag-guidance Gradient.", "content": "We denote the U-Net output feature maps obtained by k-th updated latent z\u0142 as F(z).And the gradient from drag signals is obtained by the motion supervision loss Lms, which is the difference between the features of corresponding targets and handles:\n$\\mathcal{L}_{m s}(z)=\\sum_{i=1}^{n} \\sum_{q \\in \\Omega\\left(p_{i}, r_{1}\\right)}\\left|\\mathbf{F}_{q+d_{i}}\\left(z\\right)-s g\\left(\\mathbf{F}_{q}\\left(z\\right)\\right)\\right|$\nWhere (pi, r1) = {(x,y) : ||x - xi|| \u2264 r1, ||y - yi|| \u2264 r1}, and r1 represents the patch radius. sg(.) represents the stop gradient operation (van, 2017), di = (gi-h)/||gi \u2013 h ||, is an unit vector from ho to gi. Thus the local gradient (G\u2081) can be calculated by: G\u2081 = dLms/dz."}, {"title": "Global-Local Gradient Fusion.", "content": "Now we explain how to incorporate the two gradients in detail. The key motivation of this method is to decompose the process of text-based editing process into two parts: the edit component and the identity component. Specifically, as shown in Figure 3, when the direction of the edit component from Gg is consistent with G\u03b9 (Figure 3(a)), it means both signals agree with how to update the latent. Thus we use G\u2081 to guide the edit while preserving the structure of the image by the identity component of Gg. When the two edit directions are contradictory (Figure 3(b)), we choose to correct the drag direction using the editing component, inspired by (Zhu et al., 2023). This approach can be formalized as follows:\n$G_{\\text {final }}=\\left\\{\\begin{array}{ll}G_{l}+\\sin \\left(G_{g}, G_{l}\\right) \\cdot G_{g}, & \\cos \\left(G_{g}, G_{l}\\right)>0, \\\\G_{l}-\\lambda \\cos \\left(G_{g}, G_{l}\\right) \\cdot G_{g}, & \\cos \\left(G_{g}, G_{l}\\right)<0,\\end{array}\\right.$\nwhere Gfinal means the final gradient to update the latent code and A is a hyper-parameter."}, {"title": "3.3 FAST POINT TRACKING", "content": "Although CLIP guidance can relieve the ambiguity problem, it makes the optimization of GLMS more difficult. In drag-based methods, a similar optimization issue arises when more point pairs are added. In the previous point tracking strategy, handles sometimes get stuck in one position or move far away from their corresponding targets. This significantly slows down the editing process. To remedy this issue, we add a simple constraint on the point tracking process: when updating the handles through the nearest neighbor search algorithm, only consider the candidate points that are"}, {"title": "4 EXPERIMENTS", "content": "4.1 IMPLEMENTATION DETAILS\nWe used Stable Diffusion 1.5 (Rombach et al., 2022) and CLIP-ViT-B/16 (Dosovitskiy et al., 2020) as the base model. For the LoRA finetuning stage, we set the training steps as 80, and the rank as 16 with a small learning rate of 0.0005. In the DDIM inversion, we set the inversion strength to 0.7 and the total denoising steps to 50. In the Motion supervision, we had a large maximum optimization step of 2000, ensuring handles could reach the targets. The features were extracted from the last layer of the U-Net. The radius for motion supervision (r1) and point tracking (r2) were set to 4 and 12, respectively. The weight A in the Global-Local Gradient Fusion process was 0.7."}, {"title": "4.2 TEXT-DRAG EDITING RESULTS", "content": "Settings. To show the performance of CLIPDrag we compared both drag-based methods (DragDif-fusion, FreeDrag), and text-based method (DiffCLIP) on text-drag image editing tasks. Specifically, drag-based methods require drag points as edit instructions while text-based methods need an edit prompt to perform the modification. For CLIPDrag, both editing prompts and drag points are re-quired to perform the edit. All input images are from the DRAGBENCH datasets (Shi et al., 2024).\nResults. As illustrated in Figure 4, our method shows better performance over the two different editing frameworks. Compared with text-based methods (DiffCLIP), CLIPDrag can perform more precise editing control on the pixel level. Compared with drag-based methods (DragDiffusion, Free-Drag), CLIPDrag successfully alleviates the ambiguity problem, as shown in Figure 4(b)(d)(e). This is because former drag-based methods intend to perform structural editing like moving or reshaping, instead of semantic editing such as emotional expression modification. It is reasonable because mov-ing an object is much easier than changing its morphological characteristics. Consequently, these models prefer to choose the shortcut to realize feature alignment in motion supervision, resulting in the ambiguity problem. Our proposed method effectively solves the issue, by introducing CLIP guidance as the global information to point out a correct optimization path.\nBesides, former drag-based approaches cannot guarantee edited image quality when multiple drag point pairs exist. So we also give some examples with multiple point pairs to compare the stability of these methods. As shown in Figure 4(a)(c), these results validate the effectiveness of the two techniques in our method: identity component's guidance to preserve the image quality, and fast point tracking to achieve better drag performance.\nMore results of CLIPDrag are shown in Figure 5. The leftmost three examples verify the effective-ness of our method: by combining the information of drag points and edit prompts (Pe), CLIPDrag achieves an edit with high image fidelity and no ambiguity. The middle three examples show the situations when users do not want to consider the ambiguity or find it difficult to describe the desired edit. And we found CLIP also works well when the edit prompt is replaced with the original prompt (Po). The rightmost three examples show the results when adding a mask."}, {"title": "4.3 DRAG-BASED EDITING RESULTS", "content": "Settings. Since our method is based on general drag-based frameworks, we also explored the perfor-mance of CLIPDrag in pure drag-based editing tasks. Specifically, we replaced the editing prompt (Pe) with the corresponding original prompt (Po). This ensures that no extra text information will be introduced. We compared our CLIPDrag with DragDiffusion on their DRAGBENCH benchmark on five different max iteration step settings. To evaluate the image fidelity, we reported the average 1-LIPIS score (IF). Besides, Mean distance (MD) was calculated to show the distance between the final handles and targets (Lower MD represents better drag performance)."}, {"title": "4.4 ABLATION STUDY", "content": "Ablation on Text Signals. We performed ablation studies to clarify the effect of text signals in CLIPDrag by using different edit prompts and keeping the drag points unchanged."}, {"title": "Ablation on Different Point Tracking Strategies.", "content": "Finally, We showed the effect of different point tracking strategies. We based this experiment on our CLIPDrag method while replacing the fast point tracking with the normal one in DragDiff (Shi et al., 2024). Besides, to make the result more convincing, we tried to make edited images from these two methods similar by adjusting the random seed and learning rate, while keeping other parameters like patch radius unchanged.\nResults. As shown in Figure 8, when achieving similar editing results, FPT effectively reduces the optimization iterations consumed. From the moving trajectory in Figure 8(d), we found that handles could get stuck at one point or move in circles in previous methods. Instead, handles move closer and closer to the targets in our FPT strategy, thus speeding up the editing process. Figure 8(c) shows that the FPT method does not harm the image quality. To further show the effect of FPT, we give another example and their intermediate results, shown in Figure 9(c). At 20-th iteration, FPT strategy began to perform semantic editing, while PT method was still in the stage of identity-preserving."}, {"title": "5 CONCLUSION", "content": "In this work, we tackled the ongoing challenge of achieving precise and flexible image editing within the field of computer vision. We observed that existing text-based methods often lack the precision for specific modifications, while drag-based techniques are prone to ambiguity. To overcome these challenges, we introduced CLIPDrag, a pioneering method that uniquely integrates text and drag sig-nals to enable accurate and unambiguous manipulations. We enhanced existing drag-based methods by treating text features as global guidance and drag points as local cues. Our novel global-local gra-dient fusion method further optimized the editing process during motion supervision. Additionally, to address the issue of slow convergence in CLIPDrag, we developed a FPT method that efficiently guides handle points toward their target positions. Eextensive experiments clearly demonstrated that CLIPDrag significantly outperforms existing methods that rely solely on drag or text-based inputs."}]}