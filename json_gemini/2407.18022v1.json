{"title": "Learning mental states estimation through self-observation: a developmental synergy between intentions and beliefs representations in a deep-learning model of Theory of Mind", "authors": ["Francesca Bianco", "Silvia Rigato", "Maria Laura Filippetti", "Dimitri Ognibene"], "abstract": "Theory of Mind (ToM), the ability to attribute beliefs, intentions, or mental states to others, is a crucial feature of human social interaction. In complex environments, where the human sensory system reaches its limits, behaviour is strongly driven by our beliefs about the state of the world around us. Accessing others' mental states, e.g., beliefs and intentions, allows for more effective social interactions in natural contexts. Yet, these variables are not directly observable, making understanding ToM a challenging quest of interest for different fields, including psychology, machine learning and robotics. In this paper, we contribute to this topic by showing a developmental synergy between learning to predict low-level mental states (e.g., intentions, goals) and attributing high-level ones (i.e., beliefs). Specifically, we assume that learning beliefs attribution can occur by observing one's own decision processes involving beliefs, e.g., in a partially observable environment. Using a simple feed-forward deep learning model, we show that, when learning to predict others' intentions and actions, more accurate predictions can be acquired earlier if beliefs attribution is learnt simultaneously. Furthermore, we show that the learning performance improves even when observed actors have a different embodiment than the observer and the gain is higher when observing beliefs-driven chunks of behaviour. We propose that our computational approach can inform the understanding of human social cognitive development and be relevant for the design of future adaptive social robots able to autonomously understand, assist, and learn from human interaction partners in novel natural environments and tasks.", "sections": [{"title": "1. Introduction", "content": "Humans' ability to represent and predict what others think or want from short interactions is crucial to our social lives (Baker et al., 2017). This ability to attribute mental states, such as intentions, desires and beliefs, to others is referred to as Theory of Mind (ToM), or mentalising (Frith & Frith, 2005). The importance of having a ToM for humans to successfully navigate the social world is widely recognised (e.g., for collaboration, communication, imitation, teaching, deceiving, and persuading (Devaine et al., 2014, Frith & Frith, 1999; Kov\u00e1cs et al., 2010; Rakoczy, 2017; Tomasello et al., 2005)). Furthermore, its critical role in social cognition has also been evidenced by studies assessing individuals with neurodevelopmental disorders characterised by social deficits (e.g., autism spectrum disorder (ASD)) who tend to display atypical ToM ability (e.g., Baron-Cohen et al., 1985; Murray et al., 2017; Schneider et al., 2013). This capacity is not limited to the ability to process others' behaviours (Frith & Frith, 1999). Knowing that other individuals have mental states which may differ or contrast with our own, that may be inaccurate with respect to the state of the world, and that drive their behaviour, is fundamental for efficient and fluid human social interaction (Frith & Frith, 1999; Rakoczy, 2017). Hamlin et al. (2013) have further distinguished ToM ability into \u201cfull ToM\" (or high-level ToM), which assumes second-order beliefs (i.e., the realisation that it is possible to hold a false belief about someone else's belief) vs \u201cmid ToM\u201d (or low-level ToM), which assumes second-order goals (i.e., the realisation that the goal of one agent depends on the goal of another agent) but not second-order beliefs.\nGiven the implications of ToM in social cognition and human-human interactions, it has been previously suggested that equipping robots with a ToM would also improve human-robot interactions (HRI). Indeed, with robots increasingly becoming part of the society, achieving more natural and successful HRI is important, and providing robotic architectures with a ToM is considered one of the \"Grand Challenges of Science Robotics\" (Yang et al., 2018, p. 9). In the last few decades, the fields of artificial intelligence and robotics have greatly advanced, resulting in the development of increasingly sophisticated virtual and physical intelligent agents with complex abilities and behaviours (e.g., Bhat et al., 2016; G\u00f6r\u00fcr et al., 2017; Hoffmann et al., 2017; Milliez et al., 2014). Al and robotic agents' increasingly humanoid features and human-inspired complex behaviours have enhanced humans' positive attitude towards them (Hegel et al., 2008; Rossi et al., 2023; Thellman et al., 2022). Nonetheless, the integration of Al and robots among humans is still far from optimal. This may be due to two main reasons. On the one hand, Al's still limited social capabilities continue to hinder humans' acceptance of Al in their daily lives and of robots as social companions (Abubshait & Wiese, 2017). Considerable steps were accomplished in this direction with the recent advances of Al in the linguistic domain, where an improvement in ToM-related tasks was seen with Large Language Models like chatGPT (Kosinski, 2023; Soubki et al., 2024). However, their performance is still far below human level and it further decreases when physical aspects are taken into account (e.g., lack of embodiment within an action-oriented environment), limiting their application in robots and embodied settings (Ma et al., 2023; Sap et al., 2022; Strachan et al., 2023). On the other hand, humans have been often seen as a source of complexity, disturbance, and unpredictability that could affect autonomous agents' performance (e.g., Hiatt et al., 2011; Koay et al., 2007; Sisbot et al., 2007), thus limiting Al and robotic agents' application. Equipping robots with a ToM would aid both these issues. Indeed, an artificial ToM would endow robots and intelligent agents with increased social capabilities, by being able to learn, represent, and reason about mental states and appropriately react to them. In addition, it would allow them to understand mental states-driven human behaviour, thus decreasing humans' unpredictability and disturbance, resulting in more fluid and efficient HRI.\nWhile humans provide the best example of effective ToM (Tomasello et al., 2005), the literature suggests that we still know relatively little about ToM. Although decades of research in various disciplines, from psychology and neuroscience to artificial intelligence and robotics"}, {"title": "1.1 Related Work", "content": "1.1.1 Early vs late ToM emergence. While it is now widely accepted that children older than 4 years of age show evidence of ToM ability (e.g., Wellman et al., 2001), whether this cognitive ability can be observed at younger ages is yet to be confirmed (Apperly & Butterfill, 2009; Ruffman & Perner, 2005; Sodian & Kristen, 2016). It has been previously indicated that implicit (or spontaneous) ToM competencies may exist from early infancy, and that only later during development, once children overcome verbal and inhibitory demands, this ability becomes explicit (e.g., Sodian et al., 2020). However, this remains debated (Sodian et al., 2020; van Overwalle & Vandekerckhove, 2013). Findings in support of early ToM ability in infants come from behavioural, neuroimaging, and computational studies (see Supplemental Table 1 for further details on these studies and measures). For example, a study by Southgate and Vernetti (2014) investigated early ToM ability in 6-month-old infants by assessing their brain activity through electroencephalography (EEG) in response to an agent's varying belief states and related actions. The results from this study suggest that 6-month-old infants are able to both create and update representations of another person's beliefs, even when these are incongruent to their own beliefs, and that such representations guide their predictions of that person's future behaviour. From a computational perspective, Hamlin et al. (2013) evaluated infants' understanding of others' beliefs by testing which of the following strategies would drive 10-month-old infants' judgements in a social evaluation task: (1) the analysis of the mental states at play (i.e., the protagonist's preference for an object and the helper's knowledge of such preference), (2) the assignment of the same knowledge about the protagonist's mental states to both the protagonist and the helper, or (3) the reliance on low-level cues in the scene (e.g., valence-matching hypothesis (Scarf et al., 2012)). To do so, the authors compared a Bayesian computational ToM model with simpler models that rely on cue-based and goal-attribution accounts (instead of full ToM), which were used in combination with the more classical behavioural measure of ToM (looking behaviour). The authors concluded that the Bayesian computational ToM model best described infant looking behaviour in their task, providing evidence of ToM processing from an early age rather than non-ToM computations. By investigating the developmental trajectory and learning of mental state attribution in a computational model, this paper contributes to the debate surrounding ToM emergence by providing a stance supported by additional computational findings.\n1.1.2 Mechanisms underlying ToM. An important point of discussion concerns the non-ToM-specific computational processes that may underlie infants' observed behaviour in ToM studies. Specifically, three main psychological intention recognition theories, i.e., action-effect association (Csibra & Gergely, 2007), simulation (Rizzolatti et al., 2001) and teleological"}, {"title": "1.2 Our approach", "content": "In the present paper, using an adaptation of the deep-learning model of ToM presented in Rabinowitz et al. (2018) and exploiting its multi-task learning, we aim to contribute with a computational modelling approach to the open debates surrounding ToM introduced above, i.e., early vs late ToM emergence, the mechanisms and factors underlying ToM, and inform computational models of (machine) ToM.\n1.2.1 Early vs late ToM emergence. We explore the developmental trajectory of explicitly representing others' beliefs by simulating different levels of experience (training data size). Through ablation, we further delve into the nature of the impact that acquiring representations of others' beliefs has on predicting intentions, thereby emphasising the ecological significance of early acquisition of full ToM for predicting behaviour during social interactions. Rabinowitz's (2018) approach is particularly fit to provide a baseline on the combined impact of experience and belief estimation as their system learns to infer others' beliefs and intentions from scratch, making few assumptions about the processes driving observed behaviours.\n1.2.2 Mechanisms underlying ToM. We introduce the \u201clike-them\u201d assumption based on the association mechanism for ToM (see Figure 1). In more detail, we borrow from the \"like me\" theory of social cognition (Meltzoff, 2007a, 2007b) the assumption of a shared abstract structural framework that facilitates knowledge transfer between self and others' representations. However, to understand observed others' behaviours (and underlying mental states), our \"like-them\u201d variation does not rely on physically shared representations as those that would be embodied in the observer's control and planning systems (simulation mechanism). In contrast, this process is inverted. Instead of generating trajectories on the fly using the observer's control and planning system, the \u201clike them\u201d assumption crucially posits that, through a mechanism of self-observation, one's own mental states during task execution"}, {"title": "1.2.3 Machine ToM.", "content": "Using a simple feed-forward deep learning model, we conducted a series of experiments to explore the suitability of employing this computational architecture for attaining machine ToM. Starting from human-inspired computational modelling, we interpreted the results from a robotics perspective to inform the development of artificial architectures and social agents equipped with adaptive ToM."}, {"title": "2. Methods", "content": "2.1 Architecture\nThe architecture in this study consists of a Social Perception System (SPS) and an Agent Control System (ACS) (see Figure 2). The SPS interprets and allows for predictions of the observed actor's behaviour (i.e., next actions, goals and beliefs). The ACS defines the actor's behaviour based on tasks interpreted in the SPS. The quantities emerging from the control system, i.e., actions, intentions and beliefs, are also used to fully train the SPS; in contrast, others' behaviours are only used for partial training, as others' beliefs cannot be observed."}, {"title": "2.1.1 Social Perception System (SPS):", "content": "The SPS's goal is to make predictions about the observed actor's future behaviour, with a specific interest on the actor's target position. Two types of SPS were trained, the Beliefs SPS (the ToM agent) and the NoBeliefs SPS (the classical agent), i.e., an ablation of the Beliefs SPS deprived of the explicit belief prediction. They both predict an actor's target position, next action, and next resulting state; however, only the latter has the additional ability to predict beliefs as well.\n2.1.1.1 Input sensing and routing: The SPS can observe themselves or others. The input vector for the system can then be provided by a common reference frame to represent either the observer's self-localisation state (during self-observation learning), or the physical state of the other actor (both for learning and prediction of other actor's behaviour). Several architectures (e.g., Demiris & Johnson, 2003; Wolpert et al., 2003) have studied the problem of how to switch between the processing of one's own and others' state and how to perceive or infer others' physical states. Differently from motor-based simulation models (which also rely on switching the input for their social perception system), in our case, perceiving others' state will not interfere with behaviour execution. Conversely, training the SPS through self-observation during execution of a task may affect prediction of others' mental states.\n2.1.1.2 Input encoding and pre-processing: The input consists of a number (max 5 in the reported experiment) of past steps of an actor's trajectory on a single grid map and the map itself. In particular, observed actions-states pairs are combined through a spatialisation concatenation operation, whereby actions are tiled over space into a tensor and concatenated to form a single tensor of shape (11 x 11 x 20). While 11 x 11 represents the size of the gridworld environments, 20 vectors are provided as inputs consisting of information regarding (a) actions (9 possible actions in experiments, thus 9 vectors); (b) objects coordinates,"}, {"title": "2.1.1.3 Prediction Net:", "content": "Following spatialisation concatenation operation, tensors are passed through a deep learning architecture. The architectures utilised to conduct the reported experiments are formed of a shared prediction net trunk (Crawshaw, 2020) and subsequently of separate prediction heads.\n\u2022 The trunk is implemented as a 2-layer ResNet with 32 channels, leaky ReLU nonlinearities, and batch-norm. The heads have all a similar architecture but differ in terms of output size and format.\n\u2022 Target Location Prediction Head. The output from the trunk is inputted into a 1-layer Convnet with 32 channels and leaky RELU, another 1-layer Convnet with 16 channels and leaky RELU, followed separately by (a) a fully connected layer to 121-dim logits (11 x 11 gridworld) and (b) another 1-layer Convnet with 4 channels to 1. These are then summed, thus following a residual network approach.\n\u2022 Next Action Prediction Head. The output from the trunk is inputted into a 1-layer Convnet with 32 channels and leaky RELU, followed by average pooling, and 2 fully connected layers to 9-dimensions (9 possible actions).\n\u2022 Next State Prediction Head. The output from the trunk is inputted into a 1-layer Convnet with 32 channels and leaky RELU, another 1-layer Convnet with 16 channels and leaky RELU, followed separately by (a) a fully connected layer to 121-dim logits (11 x 11 gridworld) and (b) another 1-layer Convnet with 4 channels to 1. These are then summed.\n\u2022 Beliefs Prediction Head. The output from the trunk is inputted into a 1-layer Convnet with 32 channels and leaky RELU, another 1-layer Convnet with 16 channels and leaky RELU, followed separately by (a) a fully connected layer to 121-dim logits (11 x 11 gridworld) and (b) another 1-layer Convnet with 4 channels to 1. These are then summed."}, {"title": "2.1.2 Agent Control System (ACS):", "content": "The ACS defines the agent's behaviour. Under the \"like-them\u201d assumption, own (mental) states and actions generated in ACS together with sensory observation provide a pair of supervisory signals and observed samples to train a SPS, predictor of others' mental states. The actor has full information about its own position in the environment and partial observability over the target position in the environment, i.e., they could see it only when it was in their 5x5 field of view. To account for the resulting uncertainty and related information-gathering behaviours (Friston et al., 2015), the actors' trajectories were generated using the POMDP planner based on Monte Carlo tree search proposed by (Ognibene et al., 2019b) which extends (Silver & Veness, 2010) and integrates a Bayesian filter that explicitly represents the actor's beliefs about the state of the task, i.e., the probability distribution on the target position. Silver & Veness (2010) shows that the actor's belief and related uncertainty reduction allows efficient sampling. In the current paper, the actor's belief in the starting particle can be used as a training objective for the belief prediction head. Note that while the beliefs design of the observed actor are necessarily determined by its task (i.e., represent the target position estimation), this does not affect the generality of the observer's performance because it is blind to the task and belief design assumptions."}, {"title": "2.2 Environments", "content": "For all experiments in this study, the environments consisted of 11x11 gridworld maps, which varied in the location of walls, columns, and free cells to move around the map (see Figure 3 for a visualisation of example random maps and connected behaviours). \u03a4\u03bf assess the impact of dataset size on the proposed approach, we created multiple training datasets comprising 5, 10, 15, 20, 25, 30, 60, 120, and 300 maps, together with a 10 maps testing set. The environments enabled a common action space (north, east, south, west, northeast, northwest, southeast, stay) with deterministic results. During each trial, both in training and testing, the target (in yellow in Figure 3 below), the distractors (green), and the actor (blue) are randomly positioned on free cells. The actor's previous positions (if present) are also provided (pink) in the gridworld map."}, {"title": "2.3 Trajectory Dataset Generation", "content": "A total of 30 trajectories were generated per gridworld map by randomly selecting for each trajectory the initial locations of both the actor and target. For example, when considering the dataset with 60 gridworld maps, a total of 1800 actors' behaviours were created, while 9000 total behaviours were created for the dataset with 300 maps. When considering all generated actors' behaviours from all gridworld maps datasets, the actor took, on average, approximately 3 steps (\u03c3\u00b2 = ~11) to reach its target and complete the task. Furthermore, on average, the target was not visible to the actor for approximately 3 steps (\u03c3\u00b2 = ~12), while the actor took, on average, approximately 3 steps (\u03c3\u00b2 = ~ 10) to reach its target once it became visible. At training and test time, 3 distractor objects, identical to the target, were positioned at different empty locations on the map. The positioning of the distractor objects was random, unless otherwise specified. This enabled a wider environment and contextualised behavioural diversity with no additional computational cost."}, {"title": "2.4 Training", "content": "All architectures (both Beliefs and NoBeliefs) were trained with the Adam optimiser, with varying learning rates (we tested 6 levels from 0.00015 to 0.001), using batches of size 32. A learning rate scheduler with the following parameters was used in all experiments: milestones = [30, 60, 80, 160], gamma = 0.5. The Cross Entropy loss function was utilised for all heads training, except for the Belief head, for which the Kullback-Leibler divergence loss function was used instead. This choice was driven by the fact that the Action, State and Target prediction heads all required one-hot encoding to identify one single position in the map, whereas a distribution of probabilities over each map location was needed for the Belief prediction head. Losses calculated for all prediction heads were linearly combined with linear factors extracted during a preliminary tuning phase. A preliminary tuning of L1 and L2 regularisation was performed using the L1 and L2 factor search; the final values of the L1 and L2 parameters used in all experiments were 0.005 and 0.001, respectively. In addition, early stopping was also integrated during training as a means to prevent overfitting."}, {"title": "3. Results and Discussion", "content": "3.1 Developmental synergy between the prediction of others' intentional behaviour and the explicit representation of others' beliefs\nOverall, our results show a possible synergy between the prediction of others' intentional behaviour and the explicit representation of others' beliefs from an early age, likely resulting from a regularisation effect of multitask learning (Crawshaw, 2020; Ruder, 2017). We show this effect without forcing an explicit interaction between the two processes, which may have resulted in an even higher improvement in behaviour prediction performance. Furthermore, our results show that self-observation (of own behaviours and processes) can be effectively used as a training signal (\u201clike-them\u201d assumption based on association mechanisms for ToM). The positive impact of learning beliefs attribution towards intention prediction is stronger in conditions of partial observability, e.g., when the observed actor does not perceive or even know the location of its target. This last condition closely resembles the false-belief scenario typically employed in the study of ToM. Finally, our results indicate that this positive impact can be generalised to actors with different cognitive and physical abilities, suggesting that self-other similarity is not a strict requirement for successful ToM. Although a slight decrease in behaviour prediction performance is seen when the observer and actor are increasingly different. From a computational modelling perspective, our work indicates that developing the capability of learning to explicitly represent beliefs through a shared abstract representational framework (a) poses architectural demands that are simpler than previously believed, (b) contributes in speeding-up the acquisition of socio-cognitive prediction skills, (c) strongly improves the interpretation of beliefs-driven behaviours, and (d) increases the generalisation ability to predict behaviours of others acting in different environments. We thus highlight a usable approach for adaptive and social robots with increasing ToM skills."}, {"title": "3.2 Maximal synergy with hidden target", "content": "Beliefs representations determine behavioural chunks when the target object is not yet visible to the actor and its position is not deterministically known. In such conditions, as shown in Figure 5, the performance difference between the Beliefs and NoBeliefs architectures peaks. A maximum difference in performance of ~7% (p value < 0.001) is provided by the Beliefs (58.17%) over the NoBeliefs (51.11%) architecture when trained with 15 maps (or 450 task executions), thus showing a relative gain of 14%. We must note that this environmental setting is more challenging for the observing agents, as we see the worst performance at 37.44% and the best at 63.11%. We indeed reasoned that unknown target position would produce wandering behaviours in the observed actor, which are not informative with respect to the actual target position. This being said, as performance was substantially above chance (25%), we next explored the dynamics by which the SPS could extract information from the presence of non-target objects, i.e., distractors, whose configuration could significatively affect intention prediction performance.\nFrom a ToM perspective, these results suggest that being able to process beliefs may be mostly informative towards behaviour prediction when observing individuals engaging in exploratory behaviours. In exploratory conditions, individuals indeed rely more strongly on their beliefs (Knox et al., 2012); thus, if an observer is aware of them (i.e., is endowed with a ToM), a substantial gain in performance may be obtained. False-belief tasks can be considered the golden example of this condition, as they require beliefs processing for behaviour prediction of agents with partial knowledge about the environment. Indeed, observers need to predict and interpret the behaviour of observed agents that have false beliefs about the state of the world due to partial knowledge about the environment (Wimmer and Perner, 1983), e.g., the observed agent (falsely) believes an object to be in a box as they did not see that the object was previously removed from the box by another agent. Our results show a developmental trend with beliefs processing being impactful towards prediction of exploratory behaviours with minimal experience, reaching highest impact at a medium level of experience, and showing decreased impact with high levels of experience. Therefore, these results seemingly support previous literature - and in particular those studies that showed infants' ability to successfully complete false belief tasks (e.g., Southgate and Vernetti, 2014; Hamlin et al., 2013; see also Supplemental Table 1) - in favour of an existing ToM capability in infancy. Furthermore, our findings again support ToM further development with increasing experience during adolescence (e.g., Meinhardt-Injac et al., 2020) and middle adulthood (e.g., Spenser et al., 2020) until reaching a plateau and / or a decrease in ToM ability in late adulthood (e.g., Pearlman-Avnion et al., 2018)."}, {"title": "3.3 Extracting information from avoided distractors", "content": "We reasoned that the most informative distractors for predicting an actors' intentional behaviour would be those ignored by the actor during its previous steps. Figure 6 shows the corresponding performance by both the Beliefs and NoBeliefs architectures in settings with a varying number of ignored objects by the actor (examples provided for 25 and 120 maps used for training). In addition, target visibility (whether the target was in or out of the actor's field of view) was also changed. Independently of the considered architecture or number of training maps, performance was much higher (up to 99.00%) when 3 ignored objects were present in the environment (Figure 6, A and C). The worst performance was still high at 79.67% when only 1 object was ignored and the target was not visible to the actor (Figure 6, D). The maximum gain of the Beliefs over NoBeliefs architecture was 6.56% (p value < 0.001) when 1 object was ignored and a high number of training maps (120 vs 25 maps) was used (see Figure 7). We assume that this reflects a strong contribution of belief learning in discarding objects that are not yet ignored, when these are not being approached in an efficient way by the actor (i.e., the actor is not moving in a goal-directed behaviour as the actor is still searching for the target). We assessed this specific condition in the next experiment. In addition, our results suggest that learning to predict beliefs is useful even when 3 objects have been ignored, especially with few training maps. This implies that the gradient for belief prediction helps learning to discard the ignored objects.\nFrom a ToM perspective, these results are coherent with the developmental trend observed in previous experiments with respect to the impact of beliefs on target prediction. Indeed, when considering the condition in which beliefs were most impactful (i.e., object ignored and target not visible), the gain in performance driven by beliefs processing was highest at a higher level of experience (120 vs 25 maps) (see Figure 7). Overall, this suggests that the developmental trend extends to visual crowding when the target is not visible by the actor, as well as when the target is visible. A developmental study by Richardson et al., (2023) investigating ToM in sighted and congenitally blind children (4-17 year old) indeed concluded that vision facilitates, but is not necessary for, ToM development. As an extension, our results support engagement in ToM and its development in scenarios where visual information is not available. Recognizing that the impact of the different environmental conditions is largely significant, it remains challenging for an observer to accurately assess the probability of these environmental conditions. Hence, consistent improvements across all scenarios serve as compelling evidence underscoring the importance of predicting others' beliefs."}, {"title": "3.4 Extracting information from target-aligned distractors", "content": "We assumed that the least informative distractors would be those aligned with the future trajectory of the actor when moving towards the target. Target visibility (whether the target was in or out of the actor's field of view) was also changed. As Figure 8 shows, the presence of such distractors strongly impacted the system performance (examples provided for 25 and 120 maps used for training). The worst performance (11.56%) was reached by the NoBeliefs architecture when 3 objects were aligned with the target (25 training maps, Figure 8, B), thus the target was visible to the actor. In contrast, the Beliefs architecture showed an even stronger gain in this challenging condition, with a top gain of 12.83% over the NoBeliefs architecture (56.67 vs 43.83%, respectively, p value < 0.001) (120 training maps, Figure 9), thus a relative gain of about 30% during target-visible conditions. We interpreted this result as evidence that beliefs learning provides a more efficient way of learning to discard distractors that have become visible to the actor before the target itself, even if they are aligned. The Beliefs architecture is likely to better exploit the agent's suboptimal approach behaviour toward visible distractors, even if they are finally traversed. When the target is not visible by the actor, the performance gain of the Belief architecture also reaches a top gain of 11.22% over the NoBelief architecture (46.28 vs 35.06%, respectively, p value < 0.001) (120 training maps, Figure 9). This shows that learning to predict beliefs helps learning to interpret beliefs-driven searching behaviours. This is important information for a predicting agent, considering that beliefs-driven searching behaviours would be suboptimal in terms of object approaching, and thus challenging to predict with a teleological approach.\nFrom a ToM perspective, results from sections C and D advanced the previous findings by highlighting that, rather than relying on randomness, the multi-task-induced regularisation between beliefs and target processing is mostly beneficial for recognising others' beliefs-driven behaviours. This in turn allows better disambiguation of objects in the environment, ultimately resulting in improved prediction of others' intentions. While supporting early ToM emergence, this study also highlights the advantages of having a ToM (intended here as beliefs processing) for improved human behaviour prediction in several, and challenging, situations (e.g., when an actor approaches multiple aligned objects). This is in concordance with previous developmental literature suggesting infants' ability to infer an observed agent's preference for one of two objects when the actor approaches them, i.e., condition where target and a distractor object are aligned. For example, Hamlin et al. (2013) investigated 10-month-old infants' judgements in a social evaluation task and indicated infants' ToM ability to attribute to an observed agent preference for an object, as supported by their computational modelling."}, {"title": "3.5 Generalisation to different actors", "content": "Lastly, we explored whether the advantageous effect of learning to explicitly represent beliefs under the self-observation hypothesis would help interpreting also the behaviour of observed actors with a decision system different to that of the observer. Specifically, whether this synergy between beliefs and intention learning would be useful to interpret behaviours with different modes and effectiveness compared to those used for training. This was tested by changing actors' (1) beliefs representation precision (which could be related to an actor with different cognitive abilities), and (2) action speed (which could be related to an actor with different physical abilities).\nDiffering beliefs representation precision (1) was achieved by varying the number of samples used by the POMCP algorithm (Silver & Veness, 2010) driving the observed actor's behaviour. More specifically, actors provided at testing differed in the amount of available cognitive resources determining the depth of navigation in their internal model of action-state relationships and associated rewards. Therefore, limited cognitive resources result in incorrect representation and action-sequence assessments, leading to suboptimal deliberation, planning and choices (Ognibene et al., 2019a). While actors provided during training presented high cognitive capabilities (250 max samples), those at testing had lower cognitive capabilities, i.e., 150, 50, 25 and 5 max samples. Figure 10 shows good target prediction accuracies for actors with varying cognitive capabilities comparable to our original results, regardless of the architecture; although accuracies were lower for actors who were increasingly different from the observer (e.g., 5 max samples condition). Similarly, the role of beliefs for target prediction remains evident, although varying significance was observed. Specifically, a lower gain in performance driven by beliefs processing can be globally reported when observing actors with lower cognitive capabilities."}, {"title": "4. Conclusions", "content": "Studying ToM while taking advantage of the crosstalk between disciplines allows a more global and complete understanding of this human cognition (see also Cangelosi & Schlesinger, 2018; Hassabis et al., 2017; Lake et al., 2017; Sandini et al., 2021). By capitalising on psychology and computational modelling techniques, we provide new insights into human and machine ToM.\nIn a series of studies, we investigated the \u201clike them\u201d approach to learning explicit beliefs representation for predicting others' intentions and its developmental trajectory. This was done in an attempt to show the advantages and computational sustainability of beliefs processing throughout development towards understanding others' mental states, to inform both human and machine ToM development. Overall, our results show the presence and importance of a developmental synergy of learning beliefs and intention attribution simultaneously, especially when limited samples are available and acquiring new ones is expensive or risky. We show that this architectural solution is particularly helpful in complex but ecologically common situations, such as when multiple distracting objects are present and close in the environment or when interpreting suboptimal exploratory behaviours performed with limited sensory input. We also demonstrate that the acquired ToM model, trained upon self-observation, is able to generalise (in terms of intentions predictions) to different actors, while preserving the intention-beliefs advantageous synergy. Altogether, these results outlined that taking a \u201clike them\" approach for beliefs is beneficial towards improved performance for predicting others' intentions. In line with previous computational studies in the field (Baker et a;., 2017; Rabinowitz et al., 2018; Ramirez & Geffner, 2011), we adopted a simplified environment model which can have an impact on models' performance (Ognibene et al., 2019a). Learning to explicitly represent beliefs does indeed increase, although only linearly, the computational demands and complexity of the task. However, it ultimately helps predict others' intentions. This is the case even in a simple feedforward architecture where the impact of beliefs prediction on intention prediction is not hardwired through some specific mechanism but indirectly occurs"}]}