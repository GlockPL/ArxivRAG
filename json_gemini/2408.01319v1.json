{"title": "A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks", "authors": ["Jiaqi Wang", "Hanqi Jiang", "Yiheng Liu", "Chong Ma", "Xu Zhang", "Yi Pan", "Mengyuan Liu", "Peiran Gu", "Sichen Xia", "Wenjun Li", "Yutong Zhang", "Zihao Wu", "Zhengliang Liu", "Tianyang Zhong", "Bao Ge", "Tuo Zhang", "Ning Qiang", "Xintao Hu", "Xi Jiang", "Xin Zhang", "Wei Zhang", "Dinggang Shen", "Tianming Liu", "Shu Zhang"], "abstract": "In an era defined by the explosive growth of data and rapid technological advancements, Multimodal Large Language Models (MLLMs) stand at the forefront of artificial intelligence (AI) systems. Designed to seamlessly integrate diverse data types-including text, images, videos, audio, and physiological sequences-MLLMs address the complexities of real-world applications far beyond the capabilities of single-modality systems. In this paper, we systematically sort out the applications of MLLM in multimodal tasks such as natural language, vision, and audio. We also provide a comparative analysis of the focus of different MLLMs in the tasks, and provide insights into the shortcomings of current MLLMs, and suggest potential directions for future research. Through these discussions, this paper hopes to provide valuable insights for the further development and application of MLLM.", "sections": [{"title": "I. INTRODUCTION", "content": "MLLMs are sophisticated artificial intelligence (AI) systems designed to process and integrate various types of data, including text, images, videos, audio, and physiological sequential data [1], [2], [3]. As we navigate the era of multimodal data fusion, marked by rapid advancements in information technology and an explosive increase in data volume, the capabilities of single-modality systems no longer suffice for complex real-world tasks [4], [5], [6]. Thus, the development of MLLMs is not only an inevitable trend in technological evolution but also a critical enhancement for improving the effectiveness of AI applications. By synergizing information from multiple data sources, MLLMs cultivate a more comprehensive and accurate representation of information, this capability not only unlocks significant potential but also demonstrates substantial practical application value across a diverse range of fields. The integration of diverse datasets tailors MLLMs to perform more effectively, establishing them as indispensable next-generation techniques in the ongoing quest to harness the full potential of AI technologies [7], [8], [9]. Notably, MLLMs have shown remarkable performance across diverse multimodal tasks, including language, image, video, and audio processing. These models excel in integrating multimodal information to enhance the effectiveness of multimodal tasks.\nIn Natural Language Processing (NLP) tasks such as text generation and machine translation, MLLMs leverage images, video, and audio to provide contextual support, enhancing the accuracy and expressiveness of the generated text [10], [11], [12]. These models also excel in sentiment analysis and dialog systems by integrating multimodal information to improve understanding and generation capabilities. In particular, MLLMs transform NLP by incorporating visual and auditory data, thus enriching text generation and machine translation [13], [14], [15]. These models enhance the accuracy and expressiveness of the generated text, providing nuanced contextual support that traditional models cannot. In sentiment analysis and dialogue systems, MLLMs are capable of the integration of multimodal information, which further deepens the understanding of system and response capabilities, presenting a leap forward in human-computer interaction [16], [17].\nIn addition, in the vision tasks, MLLMs significantly enhance improves task comprehension, analysis, and generation. Integrating textual descriptions and image instructions commands allows for more accurate tasks such as image classification, target detection, and image annotation. For instance, MLLMs such as GPT-4V [13], and Gemini [18] combine image content with natural language descriptions to produce more vivid and precise annotation results. These models also show progress in image generation, creating images from textual descriptions or enabling cross-modal image style migration, thus broadening the possibilities within this field. Meanwhile, video processing poses unique challenges due to its complexity. Nevertheless, the advent of MLLMS has propelled forward the capabilities of language models in this domain. Models like NEXT-GPT [19] and Sora [20] are pioneering multimodal video generation, producing richer and more realistic video content by learning from multimodal data. Furthermore, advancements in intelligent video understanding technologies, such as VideoChat [21] and Video-LLaVA [22], have significantly enhanced the ability to analyze and process video content. These developments promise enhanced user experiences in virtual reality, video games, and educational applications.\nFurthermore, In audio tasks, MLLMs bring a new technological change to audio processing tasks. Traditional audio processing usually relies on unimodal signal processing methods, such as speech recognition [23] or audio classification [24], which have limitations in processing complex multimodal data.MLLMs are able to understand better and generate audio-related content by combining audio signals with textual and visual information through the combination of Large Language Models (LLMs). For example, in speech generation tasks, MLLMs can utilize textual and visual information to generate more natural and contextually relevant speech output [25], [26]. In audio understanding tasks, these models can perform sentiment recognition, audio classification, or audio event detection more accurately by combining visual cues and textual descriptions. In addition, MLLMs show strong potential in tasks such as cross-modal audio-text translation, audio soundtrack generation, and multimodal sentiment analysis [27], [18]. These technological advances not only improve the effectiveness of audio processing, but also expand its scenarios in real-world applications such as smart homes, virtual assistants, movie and television production, etc.\nAs shown in Fig I, we summarize MLLMs from recent years. This paper reviews the current state of the art of MLLM applications, introduces the basic concepts and main architectures of MLLMs in Section 2, describes their performance in different domains to identify their strengths and weaknesses in Section 3, highlights the transformative impact of MLLMs through a comparative analysis in Section 4, and provides a roadmap for future research in Section 5. Our discussion aims to incentivize continuous innovation and ensure that MLLMs remain at the forefront of AI technology development. Through a comprehensive review of current implementations and progress, this paper aims to summarize research results, provide valuable references, and offer guidance for future research in the field of MLLMs. Our goal is to inspire new ideas and directions for the continued development of MLLMs, ensuring that they remain at the forefront of AI technology development."}, {"title": "II. OVERVIEW OF MULTIMODAL LARGE LANGUAGE MODELS", "content": "Overall, MLLMs represent a significant advancement in the field of artificial intelligence and machine learning, embodying the capability to process and interpret a diverse data type including text, images, audio, and video [28], [29], [30]. By integrating and synthesizing data from these different modalities, MLLMs achieve a more comprehensive and precise understanding and generation of information [3].\nIn particular, MLLMs are sophisticated and comprehensive systems engineered to handle and decode multimodal data concurrently. The core principle of MLLMs lies in the integration and interplay of varied modalities, which significantly amplifies the effectiveness of the models. This multimodal approach not only enhances the understanding of individual data types but also fosters a more nuanced interaction between them, thereby expanding the scope and accuracy of AI applications. For instance, in tasks such as image captioning, MLLMs leverage both text and visual data to produce accurate and contextually relevant descriptions of images. This synergy enables the models to surpass the limitations of single-modality systems, offering richer and more detailed outputs. Additionally, the combination of audio and visual data can greatly improve the performance of tasks like video understanding and annotation, making MLLMs invaluable for applications requiring detailed multimedia analysis.\nBy harnessing the collective strengths of various data types, MLLMs not only enhance the capability of AI to interpret and interact with the world but also pave the way for groundbreaking developments in how machines understand complex, multifaceted information."}, {"title": "B. Main Components of Multimodal Large Language Models", "content": "A MLLM harnesses several vital components to efficiently process and integrate data from diverse modalities. These components are designed to transform raw input from various sources into actionable insights, tailoring these models incredibly versatile and effective. The architecture of these models can be broadly categorized into three main components: the multimodal input encoder, the feature fusion mechanism, and the multimodal output decoder.\n1) Mutimodal Input Encoder: The multimodal input encoder is a vital component in MLLMs, designed to transform raw input data from various modalities into a structured format that the model can process effectively. This crucial module is specialized in handling different types of data, ensuring that each data form is optimally encoded to contribute effectively to the model's overall function. Here's how the encoder works for each data type:\nText: For textual data, the encoder utilizes technologies like embedding layers, which map words into vectors of continuous numbers, and Multi-Layer Perceptrons (MLP), or more advanced transformers that manage long-range dependencies and context within text.\nImages: Visual data is processed using state-of-the-art architectures such as Vision Transformers (ViT) [31], which treat parts of an image as sequences to better capture relationships, or Residual Networks (ResNet) [32], which help in learning deeper features without losing context through layers.\nAudio: Audio data is analyzed using models like C-Former [33], HuBERT [34], BEATs [35], or Whisper [36]. Each of these models is tailored to capture the unique properties of sound, from basic tones to complex spoken language, enhancing the model's ability to interpret auditory information accurately.\nSequential Data: For sequential data such as EEG and heartbeats, the encoder employs a combination of 1D-Convolutional Neural Networks (1D-CNN) and Long Short-Term Memory (LSTM) units. This setup is particularly effective in picking up temporal and spatial patterns in data, which is crucial for early diagnostics in medical applications.\nUniversal Encoder: A more recent innovation is the universal encoder, which aims to standardize the encoding process across highly diverse data types, including audio, video, and functional Magnetic Resonance Imaging (fMRI). This encoder leverages a generalized approach to handle and integrate multiple forms of data, promoting consistency and efficiency in data processing. Each of these encoders converts raw inputs into feature vectors, which are then transformed into a fixed-length feature sequence. This standardization is critical as it prepares the data for further processing, ensuring that the subsequent layers of the model can perform feature fusion and decoding effectively.\nBy accommodating and optimizing the initial processing of varied data types, the multimodal input encoder not only enhances the model's performance but also expands its applicability across different fields. Whether it's improving the accuracy of image captioning, enriching the context of machine translation, or advancing the precision of diagnostic tools in healthcare, this encoder plays a foundational role in enabling AI models to perform complex tasks that require a nuanced understanding of diverse inputs.\n2) Feature Fusion Mechanism: The heart of a multimodal model is its ability to integrate features from different modalities. This integration can occur at various stages [37], [38]:\nEarly Fusion: Combines input data at the initial stage, leveraging the raw interconnectedness of different modalities.\nIntermediate Fusion: Merges features during the feature extraction phase, allowing each modality to contribute its unique properties to a unified representation.\nLate Fusion: Integrates the final outputs from individual modal pathways at the decision stage, often used in tasks requiring consolidated judgments from multiple data types.\nJoint Fusion: A hybrid approach that merges early, intermediate, and late fusions to maximize data utilization across all stages. These fusion processes often employ pre-trained LLMs, which, while initially designed for textual data, are adapted to handle and synthesize multimodal inputs through advanced feature projection and serialization techniques.\nMultimodal Output Decoder: Lastly, the multimodal output decoder reconverts the fused, integrated multimodal information back into a usable form tailored to specific tasks, such as Image captioning, the decoder might generate descriptive text based on visual inputs. Video understanding tasks, it could produce annotations or summaries combining both visual and auditory data. Each decoder is meticulously designed to optimize accuracy and quality, ensuring that the output precisely reflects the combined insights gained from the integrated modalities.\nTo summarize, the sophisticated architecture of multimodal large language models empowers them to tackle complex tasks by harnessing and synthesizing data across text, images, and audio. This capability not only enhances the performance of AI applications but also opens up new avenues for innovation in how we understand and interact with technology."}, {"title": "C. Overview of Multimodal Feature in LLMs", "content": "When fusing multimodal features, it is common practice not to train new models from scratch. Instead, existing pre-trained large models, such as LLMs, are utilized. Although pre-trained LLMs are primarily designed to handle text input, various techniques can be employed to adapt these models for processing multimodal data. We will introduce a specific example in this section to illustrate the fusion process and understand it in detail.\nFirst, data from each modality needs to be encoded and projected into a unified feature space. For instance, pre-trained models like ResNet or Vision Transformer can be used to convert image data into feature vectors $V_{image}$. Text data can be converted into feature vectors $V_{text}$ using pre-trained text encoders like BERT [39], and audio data can be transformed into feature vectors $V_{audio}$ using pre-trained audio encoders like wav2vec [40]. Then, the feature vectors from different modalities are mapped into a shared feature space through linear transformations or other projection methods. To input these multimodal features into a pre-trained LLM, the features from different modalities need to be organized into a sequence. A multimodal feature sequence can be formed by simply concatenating the features from different modalities, such as $[V_{image}, V_{text}, ..., V_{audio}, V_{text}]$.\nNext, the constructed multimodal feature sequence is fed into the pre-trained LLM for processing. The Transformer model processes the input feature sequence through multiple layers of self-attention mechanisms and feedforward neural networks. Each layer, comprising self-attention and feedforward network modules, updates and integrates the feature representations, progressively extracting higher-level features.\nAfter passing through multiple Transformer layers, the model generates a feature representation sequence that encapsulates comprehensive information. Depending on the task requirements, a specific output layer can generate the final result. For instance, if the task is to generate a textual description, the integrated feature representation can be fed into a text generator to produce descriptive text.\nBy following these steps, multimodal features can be effectively processed by an LLM. Although pre-trained language models like GPT, LLAMA are primarily designed for text input, their capabilities can be extended to handle and integrate multimodal data through feature projection and serialization methods, enabling them to perform complex multimodal tasks."}, {"title": "III. TASK CLASSIFICATION OF MULTIMODAL LARGE LANGUAGE MODELS", "content": "In this section, we will detail the contribution of MLLMs in visual and audio tasks in terms of its application to specific tasks. At the same time, we will explore how to construct MLLMs suitable for these task types."}, {"title": "A. Image Tasks", "content": "MLLMs represent a significant advancement in AI industry by integrating visual and textual data to perform complex tasks. These models are designed to understand and generate images, leveraging their ability to process and combine different types of information. This section explores the two primary tasks of MLLMs: image understanding and image generation, illustrating their capabilities and applications.\nIn the realm of image understanding, MLLMs excel at interpreting and extracting meaningful information from images. They can identify objects, understand scenes, and even infer relationships between elements within an image. By combining visual data with textual descriptions, these models can provide comprehensive analyses that surpass traditional image processing methods. For instance, an MLLM can analyze a photograph to identify the objects present, describe their interactions, and correlate this with relevant textual information, offering a nuanced understanding of the visual content.\nOn the other hand, the image generation capabilities of MLLMs are equally impressive. These models can create realistic images from textual descriptions, generate variations of existing images, and even produce entirely new visual content that aligns with given textual inputs. This generative ability is crucial for applications such as creative industries, where generating novel visual content is essential. By training on vast datasets of images and texts, MLLMs can synthesize new images that are coherent with the specified descriptions, pushing the boundaries of what automated image creation can achieve.\nThe integration of image understanding and generation tasks in MLLMs highlights their versatility and potential. By effectively bridging the gap between visual and textual modalities, these models open up new possibilities for advanced AI applications. They enable more intuitive human-computer interactions, where users can receive detailed visual analyses or generate desired images through simple textual commands. This dual capability not only enhances the user experience but also expands the potential use cases across various fields, from automated content creation to enhanced data interpretation.\nIn the following contents, the image tasks of MLLMs encompass two main areas: image understanding and image generation. These capabilities showcase the models' ability to analyze and create visual content by integrating and leveraging multimodal data. The advancements in these tasks underscore the transformative impact of MLLMs in the realm of AI, promising continued innovation and expanded applications."}, {"title": "1) Image Understanding:", "content": "The development of image understanding techniques in MLLMs has gone through several stages, from the earliest reliance on traditional feature extraction methods to the current widespread application of deep learning technologies, resulting in a diverse range of research achievements and innovations. The following are the developmental stages of image understanding techniques in MLLMs: 1. Image understanding based on traditional feature extraction methods; 2. Application of deep learning technologies in image understanding; 3. Multimodal image understanding and cross-modal learning; 4. Application of reinforcement learning in image understanding; 5. Integration of image generation and understanding.\na) Image Understanding Based on Traditional Feature Extraction Methods: In the early days, image understanding primarily relied on traditional feature extraction methods such as HOG and SIFT. These methods involved manually designing features to describe image content, followed by utilizing traditional machine learning algorithms for tasks like classification and detection. While these methods performed well in some simple image tasks, they exhibited limitations in descriptive capabilities and generalization when faced with complex image data.\nb) Application of Deep Learning Technologies in Image Understanding: With the rapid advancement of deep learning technologies, image understanding entered a new phase. Through techniques like deep convolutional neural networks (CNN), it became possible to learn high-level feature representations directly from raw pixel data, enabling more precise image classification, object detection, image segmentation, and other tasks. CNN networks achieved breakthroughs in image understanding, as evidenced by their victory in the ImageNet image classification competition, marking the widespread application of deep learning technologies in the field of image understanding.\nc) Multimodal Image Understanding and Cross-Modal Learning: As NLP and computer vision intersect, multimodal image understanding has become a research hotspot. By integrating multimodal data such as images and text, tasks like image description and visual question answering can be achieved. Additionally, the application of cross-modal learning technologies has opened up new possibilities for image understanding by fusing different modal information to enhance the comprehension and representation of image content.\nd) Application of Reinforcement Learning in Image Understanding: In recent years, the application of reinforcement learning in image understanding has been increasing. Reinforcement learning can help models make decisions in complex environments, thereby enhancing their ability in image understanding. When combined with deep learning technologies, reinforcement learning can achieve more intelligent and adaptive image understanding processes in tasks such as image classification and object detection.\ne) Integration of Image Generation and Understanding: Recent research indicates that combining image generation and understanding can yield more comprehensive and in-depth information comprehension. Through generative models like Generative Adversarial Networks (GAN), images with semantic flexibility can be generated to assist models in better understanding image content. This integration brings more inspiration and innovation to image understanding tasks, while also providing additional support and application scenarios for image generation technologies.\nIn conclusion, the image understanding techniques in s have undergone several developmental stages, including traditional feature extraction, deep learning, multimodal learning, reinforcement learning, and the integration of image generation and understanding. The continuous innovation and development of these techniques have propelled the progress of image understanding, bringing forth more possibilities and opportunities for research and application in the field of computer vision. As artificial intelligence technology continues to evolve, the future development of image understanding techniques will become more diverse and intelligent, providing stronger support for achieving more intelligent image understanding and applications."}, {"title": "Model Introduction:", "content": "Multimodal models play a crucial role in image understanding tasks by integrating text and image information to achieve more intelligent and comprehensive understanding and reasoning. In this field, MiniGPT-4 [41], [42],InstructBLIP [43] ,and Wiki-LLaVA [44] are three highly regarded models, along with other related models such as 3DMIT [45], GroundingGPT [46], ModaVerse [47], Vary-toy [48], LLaVA-MOLE [49], and CogCom [50].\nMiniGPT-4 is a multimodal model based on GPT-4 that combines text and image information, exhibiting excellent image understanding capabilities. MiniGPT-4 adopts a modular architecture design, which allows for rapid adaptation to various LLMs, enabling flexible understanding and reasoning for different types of images. The model utilizes advanced image encoders and text generators to accurately describe and infer information from images, providing strong support for image understanding tasks.\nAnother prominent model is InstructBLIP, which demonstrates exceptional image understanding capabilities. With its modular architecture design, InstructBLIP can quickly adapt to different LLMs and perform instruction tuning by freezing the image encoder and the LLM. This approach enables efficient understanding and reasoning of image information. InstructBLIP excels in vision-language instruction tuning, generating rich responses while effectively considering the dialogue history, thus providing impressive performance for image understanding tasks.\nBesides MiniGPT-4 and InstructBLIP, there are other noteworthy multimodal models that also play significant roles in image understanding tasks. 3DMIT employs advanced three-dimensional information processing technology, enabling deep understanding and analysis of 3D images. GroundingGPT focuses on effectively correlating natural language and visual information to achieve cross-modal understanding and reasoning. Models like ModaVerse, Vary-toy, LLaVA-MOLE, and CogCom also exhibit outstanding image understanding capabilities to varying degrees, each with unique features and advantages, contributing significantly to image understanding tasks.\nIn summary, multimodal models play an essential role in image understanding tasks. MiniGPT-4, InstructBLIP, and other related models have shown excellent performance and potential, providing strong support for the development of image understanding tasks. With continuous technological advancements and improvements in model performance, multimodal models are expected to play an increasingly important role in image understanding tasks, bringing more innovation and breakthroughs to the field of artificial intelligence."}, {"title": "MiniGPT-4", "content": "Architecture: MiniGPT-4 is designed to align visual information from a pre-trained vision encoder with an advanced LLM. Specifically, it utilizes Vicuna as the language decoder, which is built upon LLaMA and can perform a wide range of complex linguistic tasks. For visual perception, the model employs the same visual encoder used in BLIP-2, consisting of a ViT backbone coupled with a pre-trained Q-Former. Both the language and vision models are open-sourced. A linear projection layer bridges the gap between the visual encoder and the LLM. The architecture allows MiniGPT-4 to handle diverse vision-language tasks without relying on external vision models.\nDatasets: During the first pre-training stage, MiniGPT-4 is trained using a large collection of aligned image-text pairs. This includes datasets like Conceptual Caption, SBU, and LAION, amounting to approximately 5 million image-text pairs. In the second fine-tuning stage, a smaller yet high-quality dataset is curated, which includes detailed image descriptions tailored for vision-language alignment. This stage involves only 3,500 high-quality image-text pairs designed in a conversational template to enhance the model's generation reliability and usability.\nTraining & Evaluation: MiniGPT-4 employs a two-stage training approach. The initial pre-training stage involves training the model on a large collection of aligned image-text pairs, keeping the pre-trained vision encoder and the LLM frozen while only training the linear projection layer. This stage spans 20,000 training steps with a batch size of 256, taking approximately 10 hours on 4 A100 GPUs. The second fine-tuning stage addresses the limitations observed after the first stage, such as incoherent linguistic outputs. It fine-tunes the model using a smaller but high-quality dataset, significantly improving its generation reliability. This fine-tuning is computationally efficient, taking only around 7 minutes with a single A100 GPU."}, {"title": "InstructBLIP", "content": "Architecture: InstructBLIP builds upon the BLIP-2 architecture by incorporating an instruction-aware Q-Former module. This module takes in instruction text tokens as additional input, interacting with the query embeddings through self-attention layers. This process encourages the extraction of task-relevant image features, allowing the LLM to receive visual information conducive to following instructions. InstructBLIP uses the same image encoder as BLIP-2, a ViT backbone, and adapts the model to different frozen LLMs, including FlanT5-XL, FlanT5-XXL, Vicuna-7B, and Vicuna-13B. The architecture facilitates instruction-aware visual feature extraction, substantially improving performance in both held-in and held-out evaluations.\nDatasets: InstructBLIP is trained using a comprehensive set of 26 publicly available datasets, transformed into an instruction tuning format. These datasets cover a wide range of tasks such as image captioning, visual question answering (VQA), and visual instruction following. Notable datasets include OK-VQA, ScienceQA, HatefulMemes, Visual Dialog, MSVD, and MSRVTT. The training process balances the datasets to prevent overfitting smaller datasets and underfitting larger ones, ensuring effective learning across varied tasks.\nTraining & Evaluation: InstructBLIP employs a two-stage training process. The first stage involves vision-language pre-training, initializing the model from pre-trained BLIP-2 checkpoints. Only the parameters of the Q-Former are fine-tuned while keeping the image encoder and the LLM frozen. The second stage, instruction tuning, involves training the model with a maximum of 60,000 steps, validating performance every 3,000 steps. The AdamW optimizer is used with specific hyperparameters, and training is conducted on 16 Nvidia A100 GPUs over 1.5 days. Inference involves generating responses for image captioning and open-ended VQA tasks, and using vocabulary ranking for classification and multi-choice VQA tasks."}, {"title": "2) Image Generation:", "content": "The application of multimodal models in image generation tasks is one of the hot research directions in the field of computer vision. Traditional image generation tasks mainly rely on single-modal data input, such as generating images from random noise or generating images based on context. In contrast, multimodal models simultaneously consider multiple data modalities, such as text descriptions, speech information, etc., to achieve more accurate and diversified image generation.\nIn image generation tasks, the application of multimodal models is mainly reflected in the following aspects: First, by integrating information from different modalities, multimodal models can achieve conditional image generation tasks. For example, given a text description, multimodal models can learn the correspondence between text and images, thus generating images that match the description. This approach makes image generation more targeted and semantically coherent, improving the quality and accuracy of generated images.\nSecondly, the application of multimodal models in image generation also manifests in cross-modal generation tasks. For example, applying artistic styles from one image to another to achieve image style transfer; or generating corresponding speech descriptions based on image content. These tasks require the model to effectively learn the correlated information between different modal data, thereby realizing the transformation and generation of cross-modal information, bringing more creativity and imagination to image generation tasks.\nThe technological development of image data generation in s can be roughly divided into the following stages: 1. Image generation based on Generative Adversarial Networks (GAN), 2. Improvement and optimization of image generation models, 3. Multimodal generation combining images and text, 4. Application of transfer learning and self-supervised learning in image generation.\na) Early Stage of Image Data Generation in s based on GAN: GAN as a deep learning architecture were widely used for generating realistic images in the early stages of image data generation in s. GAN consists of a generator and a discriminator, and it improves the capability of the generator to produce realistic images through adversarial training. Despite the significant success of GAN in image generation, its training process faced challenges such as mode collapse and training instability. Researchers continuously optimized GAN models and introduced new techniques and methods during the training process to address these issues.\nb) Improvement and Optimization of Image Generation Models: To address the challenges of GAN, researchers proposed various improvement and optimization techniques. For example, the introduction of conditional Generative Adversarial Networks (cGAN) achieved image generation under specific conditions; Wasserstein GAN (WGAN) improved training stability by enhancing the loss function; upgraded versions of Generative Adversarial Networks (GAN) such as ProGAN, StyleGAN, etc., could generate higher resolution and more realistic images. These improvement and optimization measures significantly enhanced the quality and stability of image generation models.\nc) Multimodal Generation combining Images and Text: With the fusion development of NLP and computer vision, researchers began studying multimodal generation combining images and text. This phase focused on integrating text descriptions with image generation, achieving tasks such as image description generation, visual question answering, and more. Multimodal generation technology brought wider applications and innovations to image generation tasks, expanding the diversity and scope of image generation.\nd) Application of Transfer Learning and Self-Supervised Learning in Image Generation: In recent years, techniques such as transfer learning and self-supervised learning have been widely applied in image generation tasks. Transfer learning effectively transfers learned knowledge to new tasks, improving the effectiveness of image generation, while self-supervised learning utilizes intrinsic label information from data to learn effective representations, enhancing the performance of generative models. The application of these technologies brings more possibilities and efficiency improvements to image generation tasks, driving the continuous advancement of image data generation in s.\nIn conclusion, the task of image generation in MLLMs represents a dynamic and rapidly evolving area of computer vision. By leveraging the integration of multiple data modalities, these models have significantly enhanced the precision, diversity, and creativity of image generation. The progression from traditional GAN-based methods to more sophisticated techniques incorporating transfer learning and self-supervised learning underscores the continuous innovation driving this field. Multimodal models enable conditional and cross-modal image generation, providing semantically coherent and targeted outputs that open new avenues for applications in various domains. As these technologies advance, they promise to further refine and expand the capabilities of image generation, solidifying the role of MLLMs at the forefront of artificial intelligence research."}, {"title": "Model Introduction:", "content": "In current multimodal deep learning research, image generation tasks have garnered significant attention, leading to the emergence of several innovative models. This article introduces several representative models, showcasing their applications and innovations in image generation tasks.\nFirstly, let's consider ProGAN [51]. ProGAN builds upon the concept of progressive growing for generative adversarial networks. ProGAN incrementally increases the resolution of generated images during the training process, starting from low-resolution images and gradually adding layers to enhance details and complexity. This approach significantly stabilizes the training dynamics and improves the quality of the generated images, reducing artifacts that were common in earlier GAN models. ProGAN's ability to produce high-definition images with resolutions up to 1024x1024 pixels has demonstrated remarkable performance across various benchmarks, making it a pivotal advancement in the field of image generation.\nAnother innovative model is MM-Interleaved [52], which builds upon the Multimodal Feature Synchronizer (MMFS). MMFS improves efficiency in extracting visual details for MLLMs by reducing the required number of visual annotations. MM-Interleaved optimizes the interaction between text and images through end-to-end training, particularly suitable for tasks requiring dynamic and efficient image attention mechanisms. It demonstrates remarkable performance across various multimodal benchmarks, highlighting its potential and advantages in image generation tasks."}, {"title": "ProGAN", "content": "Architecture: ProGAN, or Progressive Growing of GANs, employs a unique architecture where both the generator and discriminator grow progressively during training. This means starting from a low resolution, new layers are added incrementally to model increasingly fine details as training progresses. The generator and discriminator both start with a 4x4 pixel resolution and grow to a maximum resolution, such as 1024x1024 pixels, by doubling the resolution at each step. This incremental approach stabilizes training and improves image quality by allowing the model to learn high-level structures before focusing on fine details. Key innovations include pixel-wise feature vector normalization in the generator and minibatch standard deviation in the discriminator, which help maintain the stability and variation of the generated images throughout the training process.\nDatasets: ProGAN is primarily trained on high-quality datasets such as CelebA-HQ, which consists of high-resolution images of celebrities. This dataset is essential for the model to learn to generate photorealistic human faces. Additionally, ProGAN has been tested on other large-scale datasets like LSUN, which includes categories such as churches, bedrooms, and outdoor scenes, allowing the model to generate a diverse range of photorealistic images beyond just human faces.\nTraining & Evaluation: The training of ProGAN involves a novel technique where the generator and discriminator are progressively grown. This method starts with a simple 4x4 resolution and progressively adds layers to increase the resolution of the generated images. The training uses the Wasserstein GAN loss with a gradient penalty (WGAN-GP) to improve stability and convergence. Training is performed on multiple GPUs, such as 8 Tesla V100 GPUs, over several days to ensure high-quality results. Evaluation of ProGAN shows its capability to generate highly realistic images with minimal artifacts, as demonstrated on benchmark datasets like CelebA-HQ and LSUN."}, {"title": "MM-Interleaved:", "content": "Architectures: MM-Interleaved is an end-to-end generative model designed for interleaved image-text data. It integrates a Visual Foundation Model (VFM), a LLM, and a Diffusion Model (DM) to handle both text and image generation tasks effectively. The architecture comprises three key components: a VFM-based Image Tokenizer that uses a pre-trained vision model, such as CLIP-ViT, to extract image features, which are then processed by a Perceiver Resampler to map each image to a fixed number of visual tokens; an LLM-based Multi-modal Model that utilizes a pre-trained LLM, such as Vicuna, to extract context features from interleaved image-text sequences; and a Diffusion Model used for generating images from text inputs by leveraging the strengths of denoising-diffusion processes. The architecture is optimized end-to-end to preserve fine-grained image details and handle multiple images efficiently, reducing the computational and memory demands typically associated with multi-image scenarios. Datasets: The training of MM-Interleaved involves a two-stage approach. First, the model is pre-trained using a mixture of large-scale image-text datasets, including LAION-COCO and LAION-En, which provide extensive image-text pairs. The use of webdataset ensures efficient data loading and handling during the pre-training phase. For fine-tuning, MM-Interleaved employs datasets designed for complex multi-modal instructions, such as MMC4, OK-VQA, and VQA. These datasets help the model learn to follow complex multi-modal instructions and generate coherent text and image outputs based on interleaved sequences. Training & Evaluation: MM-Interleaved's training process is structured in two stages. The first stage, single-modal pre-training, involves pre-training the modality-specific components (VFM and LLM) separately on large-scale image-text paired data to align their outputs with the LLM's word embedding space. The second stage, multi-modal instruction tuning, involves fine-tuning the integrated model using a high-quality multi-modal instruction-following dataset. This process includes both positive and negative examples to enhance the model's ability to handle arbitrary combinations of input modalities. The training utilizes DeepSpeed ZeRO-1 for efficient distributed training, ensuring scalability and performance during large-scale training sessions. Evaluation is conducted using zero-shot benchmarks across various datasets, ensuring the model's versatility and effectiveness in recognizing visual details and generating consistent outputs based on both textual and visual conditions."}, {"title": "B. Video Tasks", "content": "With the rapid development of information technology, video has become the primary means for people to access information and enjoy entertainment, playing an increasingly important role in today's social media and internet culture. However, manually processing such a vast amount of video content has proven to be a time-consuming and labor-intensive task. Effectively understanding and processing video content, especially when the volume of video data is large, the information is dynamic, and it is multimodal, has long been a major challenge in the technology field. In recent years, with the rise of MLLMs, the field of MLLMs has experienced rapid and flourishing development in videos domains. This section first introduces detailed explanations of two mainstream tasks in video fields: video generation and video understanding. Then, we introduce the development of multimodal large language models in these tasks. Finally, we detail the architecture, datasets, training methods, and evaluations of several representative models."}, {"title": "1) Video Understanding Task:"}]}