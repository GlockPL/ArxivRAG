{"title": "HIERARCHICAL OBJECT-ORIENTED POMDP PLANNING FOR OBJECT REARRANGEMENT", "authors": ["Rajesh Mangannavar", "Alan Fern", "Prasad Tadepalli"], "abstract": "We present an online planning framework for solving multi-object rearrangement problems in partially observable, multi-room environments. Current object rearrangement solutions, primarily based on Reinforcement Learning or hand-coded planning methods, often lack adaptability to diverse challenges. To address this limitation, we introduce a novel Hierarchical Object-Oriented Partially Observed Markov Decision Process (HOO-POMDP) planning approach. This approach comprises of (a) an object-oriented POMDP planner generating sub-goals, (b) a set of low-level policies for sub-goal achievement, and (c) an abstraction system converting the continuous low-level world into a representation suitable for abstract planning. We evaluate our system on varying numbers of objects, rooms, and problem types in AI2-THOR simulated environments with promising results.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-object rearrangement with egocentric vision in realistic home environments is a fundamental challenge in embodied AI, encompassing complex tasks that require perception, planning, navigation, and manipulation. This problem becomes particularly demanding in multi-room settings with partial observability, where large parts of the environment are not visible at any given time. Such scenarios are ubiquitous in everyday life, from tidying up households to organizing groceries, making them critical for the development of next-generation home assistant robots.\nExisting approaches to multi-object rearrangement typically fall into two categories: Reinforcement Learning (RL) methods and hand-coded planning systems. RL methods often struggle as the problem becomes increasingly complex and lengthy, making it difficult to scale to more challenging scenarios. To address this limitation, many researchers have adopted a modular approach, decomposing the task into a series of subtasks (Gu et al., 2022) such as manipulation skills, navigation skills, or exploration skills. These subtasks are then sequenced together in different ways to accomplish the overall goal. However, current modular approaches have their own limitations. Some pre-determine the sequence in which to apply skills, while others use greedy planners (Trabucco et al., 2022). This constrains their potential for full optimization in terms of determining the optimal order to interact with objects and handling new problems, such as when another object blocks the path to an object or location or if the goal location itself is obstructed. To overcome these challenges, a more general approach that incorporates high-level planning based on the current state would be beneficial. Such an approach would enable the system to handle novel problems without requiring extensive re-learning from scratch, thus increasing its adaptability and efficiency in diverse environments. Solving these new problems is particularly important in the context of household robots, where obstacles, blocked paths, and obstructed goals are common occurrences that a robust system must be able to handle effectively.\nWhile significant progress has been made in rearrangement, the majority of current research focuses on single-room settings or assumes that a large number of objects are visible at the beginning of the task, either through a third-person bird's eye view (Ghosh et al., 2022) or a first-person view where most of the room is visible. However, as we move towards the more practical version of the problems, such as cleaning a house, the majority of the space and objects to be manipulated are not initially visible, and existing solutions begin to falter. This scenario of partial observability introduces several major challenges: 1) uncertainty over object locations, as the starting positions of objects are unknown; 2) execution efficiency of searching for objects while simultaneously moving them to the correct goal locations; 3) scalability of planning over increasing numbers of objects and rooms; 4) extensibility to scenarios involving blocked goals or obstructed paths; and 5) graceful handling of object detection failures.\nWe employ a hierarchical approach to decompose the complex multi-object rearrangement problem into two distinct levels. This structure allows us to address object combinatorics at the high level while managing regional interactions at the low level. By separating these aspects, we effectively mitigate the challenges inherent to each. We use an object-oriented representation rather than low-level perceptual representations such as point clouds, which reduces the complexity of planning and is more natural in our setting. Our hierarchical Object-Oriented Partially Observed Markov Decision Process (HOO-POMDP) planning framework has the following components: 1) A perception module that detects objects and outputs object-oriented observations, 2) A belief update and state abstraction system, 3) A high-level planning module that plans the sequence of locations to be visited or objects to be transported, 4) A low-level navigation and manipulation module that plans a path to its next destination or control actions to pick and place objects.\nIn more detail, the agent maintains a factored belief state over object locations based on its prior beliefs and the output of its object detection module. An abstract object-oriented POMDP planner produces a high-level policy for the given goal. The first high-level action in the plan is treated as a (navigation or manipulation) sub-goal for the low-level planner, which produces a lower-level sequence of actions. The agent takes the first action in the lower-level plan, updates its belief state based on its new perception, and the cycle repeats.\nThe main contributions of our research are as follows:\n\u2022 A modular planning system, which includes an object-oriented planner and a state abstraction module for object rearrangement in multi-room environments.\n\u2022 A new dataset featuring blocked path problems and expanded room configurations alongside existing rearrangement challenges.\n\u2022 An empirical evaluation of the system in the new dataset in AI2Thor under different conditions."}, {"title": "2 RELATED WORK", "content": "Rearrangement: Rearrangement is the problem of manipulating the placement of objects by picking, moving, and placing them according to a goal configuration. In this work, we are mainly concerned with the rearrangement of objects by mobile agents in simulated environments such as AI2Thor and Habitat (Kolve et al., 2017), (Szot et al., 2021) and (Gan et al., 2020). We hypothesize that successful rearrangement planning in simulators with noisy sensors and effectors will go a long way towards successful planning and execution in robotics.\nRearrangement has also been studied from the Task and Motion Planning (TAMP) perspective (Garrett et al., 2020a; 2021; 2020b). perspective. Garrett et al. (2020b) is limited to a single-room kitchen problem and assumes perfect detection of objects. Unlike most previous work, our proposed solution optimizes the traversal cost and addresses multi-room settings and imperfect object detection in an integrated POMDP framework. Tekin et al. (2023) and Mirakhor et al. (2024b) address the multi-room rearrangement problem. However, the decision-making process of Tekin et al. (2023) about when to explore and when to move an object is fixed and assumes perfect object detection. Our framework optimizes, naturally handles exploration and manipulation, and addresses object detector failures in a unified framework. Mirakhor et al. (2024b) make the assumption that objects are always on top of or inside containers. This limits its extendability to handling new problems, such as blocked paths where the objects could be in the path of another object without any containers. Our framework naturally allows for these new possibilities.\nObject Oriented POMDP (OOPOMDP) Planning: Our work is partly inspired by Wandzel et al. (2019b), who OOPOMDP and perform a multi-object search in a 2D environment. Zheng et al. (2023) and Zheng et al. (2022) extend this formulation to object search in 3D environments. However, they are limited to the task of object search. In our work, we build on their formulation of OOPOMDP and extend it to include rearrangement actions and their corresponding belief updates. We further extend this rearrangement OO-POMDP to HOO-POMDP through action abstraction."}, {"title": "3 PROBLEM FORMULATION", "content": "Environment and Agent: Our agent is developed for the AI2Thor simulator environment (Kolve et al., 2017). It consists of a simulated house with a set of objects located in one or more rooms. The agent can take the following low-level actions: $A_{S} = {MoveAhead, MoveBack, MoveRight, MoveLeft, RotateLeft, RotateRight, LookUp, LookDown, PickObject_{i}, PlaceObject, Startloc, Done}$. The Move actions move the agent by a distance of 0.25m in the environment. The Rotate actions rotate the agent pitch by 90 degrees. The Look actions rotate the agent yaw by 30 degrees. Start action starts the simulator and places the agent at the given location, and the Done action ends the simulation. After executing any of the above actions, the simulator outputs the following information: a) RGB and Depth images, 2) the agent's position (x, y, pitch, yaw), and 3) whether the action was successful. There are two types of objects in the world - interactable objects and receptacle objects. The interactables are the ones that can be picked and placed. The receptacles are objects that are not movable but can hold other objects.\nTask Setup: Rearrangement is done in 2 phases. Walkthrough phase and rearrange phase. The walkthrough phase is meant to get information about stationary objects. The 2D occupancy map is generated in this phase, as well as the corresponding 3D Map. We get the size of the house(width and length) from the environment and uniformly sample points in the environment. We then take steps to reach these locations (if possible - some might be blocked). This simple algorithm ensures we explore the full house. At each of the steps, we receive the RGB and Depth. Using this, we create a 3D point cloud at each step and combine them all to get the overall 3D point cloud of the house with stationary objects. We then discretize this point cloud into 3D map voxels of size 0.25m, we further flatten this 3D map into a 2D map of grid cells (location in the 2D map is occupied if there exists a point at that 2D location at any height in the 3D map). While doing this traversal, we also get information about the receptacles by detector on the RGB images we receive during this traversal. This ends the walkthrough phase. (this walkthrough process needs to be done only once for any house configuration of stationary objects - walls, doors, tables, etc.). Then, objects are placed at random locations (done using AI2Thor environment reinitialization). This is when the rearrangement phase begins, with the planner taking the following as input - the map generated in the walkthrough phase, the set of object classes to move, and their goal locations."}, {"title": "4 HIERARCHICAL OBJECT ORIENTED POMDP (HOO-POMDP) PLANNER", "content": "This section presents our online planning framework designed to solve multi-object rearrangement problems in partially observable, multi-room simulated home environments. Our approach enables the system to tackle complex rearrangement tasks that involve challenging sub-problems, such as clearing blocked paths.\nOverview: Once the initial list of receptacles and 3D map have been generated, they, along with the goal information, are sent to the HOO-POMDP planner. The system operates in a cyclic fashion, integrating perception, belief update, state abstraction, abstract planning, and action execution (see Figure 2 and Algorithm 1). First, the perception subsystem detects objects in the RGB and depth image and outputs the observation z. This z is used by the belief update subsystem to update the object-oriented belief state, which consists of the probability of each object being at a certain location in $M^{2D}$. The abstraction system uses this information to update its abstract state. The updated abstract state is sent to the abstract POMDP planner, which outputs a sub-goal that corresponds to a low-level policy. The low-level policy executor executes the low-level policy corresponding to the sub-goal. This might involve navigating to a specific location, grasping an object, or placing an object in a new position. After each action is executed, the environment state changes. The agent receives new output from the environment, and the cycle repeats until the overall rearrangement task is completed. In the rest of this section, we will discuss each of the subsystems and their interaction.\nAbstract OO-POMDP Planning :"}, {"title": "Background: POMDP:", "content": "A POMDP is a 7-tuple (S, A, T, R, \u03b3, \u039f, Omodel) (Kaelbling et al., 1998). The state space S is the set of states in which the agent and the objects in the environment can be. Action space A is the set of actions that can be taken in the environment. The transition function $T(s,a, s') = p(s'|s, a)$ is the probability of reaching the state s' when the action a is taken in the current state s. The agent receives an observation z \u2208 O when an action is taken. The probability of receiving an observation when being in a given state s after having taken action a is defined by the observation model $Omodel(s, a, z) = p(z|s, a)$. The reward function $R(s, a)$ defines the reward received when taking action a in state s, and y is the discount factor. In a partially observed world, the agent does not know its exact state and maintains a distribution over possible states, i.e., a belief state b. The belief is updated when an action is taken, and observation is received with the following equation, where \u03b7 is the normalizing constant:\n$b' (s') = \u03b7O(s', a, z) \\sum_{s \\in S} T(s, a, s')b(s)$   (1)\nObject Oriented POMDP: Object-oriented POMDP factors the state and observations over the objects. Each state s is represented as a tuple of its n objects $s = (s_{1},...,s_{n})$, each observation $z = (z_{1},..., z_{n})$ (Wandzel et al., 2019a) and the belief state b is factorized as $b = \\prod_{i} b_{i}$.\nAbstract Object Oriented POMDP:\nWe now instantiate the rearrangement problem as an abstract POMDP. In our definition of the abstract OOPOMDP, we make an object independence assumption - that at any given time, the observation and belief of any object do not depend on any other object.More formally, $(P(z_{i}|s_{j}, z_{j}, s_{i}) = P(z_{i}|s_{i})$, observation $z_{i}$ is independent of the states and observations of other objects, conditioned on its own state $s_{i}$ (observations are conditionally independent). Similarly, we also assume $P(s'|s_{i}, s_{j},a) = P(s'|s_{i}, a)$ when $j != i$, i.e., the next state of object i only depends on its own previous state and the action. This allows us to represent the state and observation as entities factored on objects, which in turn helps make independent belief updates for each object(Algorithm 2)."}, {"title": "\u2022 State Space:", "content": "We use a factored state space that includes the robot state $s_{r}$, and the target object states $s_{targets}$. The complete state is represented as $s = (s_{r}, s_{targets})$. $s_{targets}=(s_{target1},...,s_{targetn})$ where n is the number of objects to be moved. $s_{targeti} = (loc_{i}, pick_{i}, place_{locs}, is\\_held, at\\_goal, g_{i})$: $loc_{i}$ is the current location of the object, $pick_{i}$ corresponds to the location from where this object can be picked, $place_{locs}$ corresponds to the set of locations (absolute 2D coordinates) from where this object can be placed from. $g_{i}$ is the goal location of the object. All locations are discretized grid coordinates in the 2D map ($M^{2D}$).\n\u2022 Action Space: The action space consists of abstract navigation and interaction actions.\n$A = {Move_{AB}, Rotate_{angle}, PickPlace_{Object_{i}-goal_{loc}}, Done}$"}, {"title": "\u2022 Transition Model :", "content": "$Move_{AB}$ - The move action moves the agent from location A to location B.\n$Rotate_{angle}$ - The rotate action rotates the agent to a given angle.\n$PickPlace_{Object_{i}-goal_{loc}}$ - The PickPlace action picks $Object_{i}$ from the current position of the robot and places it at the given $goal_{loc}$."}, {"title": "\u2022 Observation Space:", "content": "We use a factored observation space similar to state space factorization. Each observation can be divided into the robot observation and object observation $z = (z_{robot}, z_{objects})$, where $z_{objects} = (z_{target1}, z_{targetn})$. Each observation $z_{targeti} \\in L\\cup Null$ - is a detection of the object i's location or Null based on the detector's output for object i.\n\u2022 Observation model: By definition of z above, $Pr(z|s) = Pr(z_{r}|s_{r}) Pr(z_{objects}|s_{targets})$ and $Pr(z_{r}|s_{r}) = 1$ since the robot pose changes deterministically. Under the conditional independence assumption, $Pr(z_{objects}|s)$ can be compactly factored as follows:"}, {"title": "\u2022 Reward Function:", "content": "$Move_{AB}$: The cost of moving from location A to B $[Cost = \u22121 * N_{a} (where N_{a}$ number of required actions)].\n$Rotate_{angle}$: The cost of rotating the agent from the current rotation to the final given angle.\n$PickPlace_{Object_{i}-goal_{loc}}$: Cost of moving from current location to goal location + cost of pick + cost of place. It gets an additional reward of 50 if the object is being placed at its goal location $g_{i}$ and this $g_{i}$ is free in the current state.\nDone - This action receives a reward of 50 if all objects have been placed at their goal location and -50 otherwise."}, {"title": "Abstract OOPOMDP Planner:", "content": "Given a task defined as an abstract OOPOMDP and an initial abstract state, the OOPOMDP planner uses partially observable UCT (POUCT) Silver & Veness (2010) to search through the space of abstract actions to find the best sub-goal. POUCT is an extension of the UCT algorithm Kocsis & Szepesv\u00e1ri (2006) to partially observable settings. The search tree in POUCT is over the histories instead of states. A history is a sequence of actions and observations $h_{t} = (a_{1}, z_{1},..., a_{t}, z_{t})$. Similar to the UCT algorithm, the POUCT maintains a tree, where each node corresponds to a history $T(h)$, and for each node, a count variable $(N(h))$ and a value variable $(v(h))$ are stored, representing the number of times this history has been visited and the expected value of history h, which is estimated by the expected return of all simulations starting at h. The algorithm samples a state from the belief space b for the current history, and if the tree already contains all the children for the current node, then the action with the best value is selected using the equation $V (ha) = \\frac{V(ha) + c\\sqrt{logN(h)}}{N(ha)}$ to compute the value for all actions. If the tree does not contain all children, then a rollout policy is used to select actions for simulations, and the tree is updated with information from the simulations. Once that is done, the best action is selected and returned. The full algorithm is in Algorithm 3 in the Appendix. The rollout policy is a random policy that picks randomly amongst available actions. The Move and the Rotate actions are initialized by the values in the abstract state for each object. Recall that each object has the following information in its abstract state $s_{i} = (loc_{i}, pick_{i}, place_{locs}, is\\_held, at\\_goal)$. A separate $Move_{AB}$ is initialized with A = agent_pos and B = all pick locations defined for all objects. $Rotate_{angle}$ for all objects, less than 2m from the agent, the angle is computed based on the agent's required orientation to view the object from its current position. $PickPlace$ - is defined for each object where the agent is less than 2m away from that particular object, for all locations in the $place_{locs}$ as $goal_{loc}$ initializing a set of $PickPlace$ action for each object.\nPolicy Executor: Each sub-goal/action in the abstract OO-POMDP corresponds to a policy. When the planner outputs a sub-goal, the information in the given sub-goal is used to initialize the low-level policy. The output from the low-level policy is a sequence of low-level actions.\nThe Move sub-goal corresponds to the Move policy, which uses the A* algorithm to move from location A to B. The Rotate policy also uses the A* algorithm. The PickPlace policy consists of 2 RL agents and A* that picks the object from the current location and places it at the goal location."}, {"title": "Algorithm 2: HOO-POMDP Belief Update", "content": "1 Function UpdateBelief (beliefState, action, observation):\n2 b\u2190 beliefState ; z \u2190 observation;\n3 for each object i in b do\n4 | for each possible state $s_{ij}$ of object i do\n5 | | if action is navigation then\n6 | | | $b(s_{ij}) \u2190 np(z_{i}|s_{ij})b_{i}(s_{ij})$\n7 | | else if action is place then\n8 | | | $b(s_{ij}) \u2190 1 if s_{ij}$ = action.goalLocation, 0 otherwise\n9 | | else if action is pick then\n10 | | | $b(s_{ij}) \u2190 1 if s_{i} = action.agentLocation, 0 otherwise\n11 return b';\n\u2022 Sub-goal $Rotate_{angle}$ gives the Rotate policy the final angle to be at, which is used as the final state the A* system must reach and outputs a sequence of rotate actions.\n\u2022 Sub-goal $PickPlace_{Object_{i}-goal_{loc}}$ provides the object to interact with and which location to place it at. The policy takes this information as input and outputs a sequence of actions consisting of Pick, Place, and navigation actions that enable it to pick the object, move to the given destination, and place the object at that location. The PickPlace consists of 3 separate components a) An RL model trained to pick an object, b) the A* navigation model to go its destination c) An RL model trained to place the object when the agent is near the goal. All 3 of these run sequentially and make up the PickPlace Policy. It is designed this way to improve modularity and reduce the complexity of each part. Both the pick model and place model are trained using the PPO algorithm. The action space is {$A_{S}$ \u2013 Place} for pick model and {$A_{S}$ \u2013 Pick} for the place model. The training process for the pick model involves randomly positioning the target object within a specified proximity to the agent. The goal is to pick a selected object successfully. For the place model, the training methodology follows a similar approach, with the key distinction being the absence of object detection requirements, as the agent begins each scenario already holding the object. In all training instances for the place model, the initial state consists of the agent holding an object, and the task involves depositing the object at a predetermined location.\nPerception: Once the agent executes the low-level action, it receives an RGB and Depth image. An object detector is used to detect objects in the RGB image, and the depth map is used to get their 3D location in the world. This is used to generate the object-oriented observation $z = (z_{1},..., z_{n})$.\nBelief Update: Algorithm 2 presents the belief update function for our HOO-POMDP. The UpdateBelief function takes as input the current belief state b, the performed action a, and the received observation z. For each object i in the belief state and each possible state $s_{ij}$ (j = 1, ..., L, where L is the set of all its possible locations in the 2D Map) of that object, the algorithm updates the belief based on the action type. For navigation actions, it applies a probabilistic update using the observation model $p(z_{i}|s_{ij})$ (Line 6). For \u2018place\u2019 actions, it sets the belief to 1 if the object's state matches its desired goal location $g_{i}$, and 0 otherwise (Line 8). For 'pick' actions, it assigns a belief of 1 if the object's state corresponds to the agent's location and 0 otherwise (Line 10).\nGenerating Abstract State: We now have a belief state over the set of all possible locations for each object. We need to generate the abstract object-wise state consisting of object location information and their corresponding pick-and-place information. The information that needs to be computed for each object is as follows: $pick_{i}, place_{locs}, is\\_held, at\\_goal$.\nThe value for $is\\_held$ comes from the previous low-level action and previous state. If the previous state had $is\\_held$ as false and low-level action was to pick the object of interest, $is\\_held$ is set to true. If the previous action was not a pick or a pick action for a different object, then the variable remains unchanged. If the previous action was place and $is\\_held$ is true, then it is set to False.\nThe value for $at\\_goal$ is copied from the previous state if the last low-level action was not the place action. If it was, and if $is\\_held$ was true in the previous state, then at-goal is set to true."}, {"title": "\u2022 Success Rate (SR):", "content": "1 if all objects have been moved to the correct goal locations, 0 otherwise.\n\u2022 Object Success Rate (OSR): (Total Objects successfully moved)/(Total objects to move) - this metric captures the proportion of objects moved to the correct goal location.\n\u2022 Total Actions taken (TA): A measure of the efficiency of the system in terms of the total number of actions taken. We present the average number(rounded up) of actions taken during successful runs where the scene was fully rearranged."}, {"title": "\u2022 Perfect Knowledge (PK):", "content": "In this baseline, we will start with all the information about the world. That is, we know the initial locations of all the objects. This is the upper limit of the system's performance as there is no uncertainty to manage.\n\u2022 Perfect Detector with partial observability (PD): In this baseline, we will be solving our multi-object rearrangement problem with a perfect detector (objects in the visual field are detected with 100% probability). The main challenge is to find all objects and move them around efficiently.\n\u2022 OURS (Imperfect Detector with partial observability): In this setting, we will solve the multi-object rearrangement challenge where the agent is expected to handle perception uncertainty (the detector fails to detect objects in the visual field) along with object's starting position uncertainty."}, {"title": "\u2022 OURS-HP (ablation):", "content": "In this setting, we remove the hierarchical planning and use the POMDP planner to output low-level actions directly."}, {"title": "Experimental setup:", "content": "Each experimental setting was evaluated across 100 distinct rearrangement configurations. For the RoomR dataset, we utilized 25 different room setups, with four rearrangement configurations per setup. For the other datasets, we employed 100 unique room configurations, each with one rearrangement configuration. In the blocked goal and swap settings, at least one object's goal location was obstructed by another object. The swap case required the exchange of positions for at least one pair of objects to complete the rearrangement task. In the blocked path scenario, all scenes contained a minimum of one object that needed to be moved to its goal position from its initial location to enable the rearrangement of other objects."}, {"title": "6 RESULTS AND DISCUSSION", "content": "Methods comparison: The perfect knowledge system is the best performance our agent can have - there is no uncertainty in the world, and hence, it boils down to a path-traversal length optimization problem. We can see from Table 1 that the results of the perfect detector setting and the perfect knowledge setting are similar in the overall scene success rate as well as the object success rate. This shows that the PPOMDP planner is able to explore and find all the objects perfectly. The failures in both cases are due to the low-level policies failing to pick/place objects. The difference between the methods can be seen in the steps taken. The perfect detector system has to explore and rearrange and hence ends up taking more steps than the perfect knowledge system. The third set of results are for our system with an imperfect detector and partial observability. We can see that the detector failure causes our success rate to fall by a fair amount - this is because if an object is not detected more than once at its location, the planner's belief update prevents it from going to that region to pick that object up. Since our detector only has a 50-60% success rate (different for different classes of objects), it misses a fair subset of the objects, and hence the drop. As a result of needing to explore more due to detector failure, the number of steps taken is greater than that of the other methods. The difference between OURS and OURS-HP clearly shows the importance of hierarchy and abstraction. The performance drop is significant across all settings for OURS-HP.\nComparison Across Datasets: We can see that the results of the RoomR dataset of a single room and the Proc dataset with multiple rooms are similar (in scene success rate and object success rate). This indicates that our method effectively manages the exploration of multiple rooms as well as a reduction in the number of objects visible initially. Initial object visibility decreases when scaling from 2 to 4 rooms in MultiRoomR with ten objects, yet success rates remain relatively stable. The performance drops in our custom dataset with a larger number of objects. In particular, the object success rate drops slightly, but the scene success rate drops significantly. This is because, for a scene to succeed, we need all objects to be rearranged correctly, so even if 9/10 objects are rearranged correctly, it is still considered a failure. Hence, there is a bigger reduction in scene success rate.\nAcross Different Challenges: From the different rows for each dataset (the difference being the existence of blocked paths), we can see that the results for the blocked path version of the problem are lower. This is because the low-level policy is not as good at picking the objects on the floor as it is at picking the objects from the top of other objects. Also, the number of PickPlace actions increases as we tackle more complex problems such as swaps.\nError Analysis: The majority of failures of our system are due to low-level policy failures - the pick or the place action fails due to the imperfections of the low-level RL policy. The other type of failure in our system is due to belief estimation errors caused by detector failures - if a detector is looking at a certain location and it fails to detect an object multiple times (due to partial occlusion or high distance), then our belief about the object being in that region reduces to a large extent. In that case, we are unlikely to come back to that part of the house. If a false positive happens, an agent might pick up the wrong object and place it and believe it has placed the correct object - leading to object and scene failure. These are the main causes of our failures in imperfect detector settings.\nComparison to existing baselines: It is important to note that our system addresses a variant of the multi-object rearrangement problem that differs in key aspects from those tackled by existing baselines. Unfortunately, this means that direct comparisons are not meaningful or informative. The primary distinction lies in the prior knowledge available to our system: we are given information about the classes of objects to be moved, whereas other systems Mirakhor et al. (2024a) (Mirakhor et al., 2024b), (Gadre et al., 2022) operate without this advantage. The motivation for it is that when this information is not provided, agents must perform a walkthrough phase for each new goal configuration to identify movable objects. In contrast, our formulation requires only a single initial walkthrough to map stationary objects in the environment. Subsequently, our system can efficiently handle multiple goal configurations without additional walkthroughs.\nHowever, it is worth highlighting that our problem formulation introduces its own set of challenges. In particular, while existing systems report initial visibility of approximately 60% ((Mirakhor et al., 2024b), table 1) of target objects at the outset of their tasks, only about 20% of the objects are initially visible in our problem settings, necessitating more extensive and strategic exploration. This reduced initial visibility significantly increases the complexity of our task in terms of efficient exploration and belief management. It underscores the importance and effectiveness of our hierarchical planning approach in handling partial observability and perception uncertainty. The other key difference is we use low-level policies for navigation and manipulation, whereas other works Mirakhor et al. (2024b) and Mirakhor et al. (2024a) assume perfect navigation and manipulation capabilities.\nWhile the difficulties faced by our system and existing baselines are not directly comparable, we believe our results demonstrate the efficacy of our approach in solving a challenging and practically relevant variant of the multi-object rearrangement problem. The performance across various metrics and scenarios, as detailed in the preceding sections, showcases the robustness and scalability of our HOO-POMDP framework in environments with high uncertainty and limited initial information.\nLimitations: Our system makes an object independence assumption - that the state and observation of one object are not dependent on the state of other objects. However, this assumption doesn't always hold in cluttered environments. In such cases, observing, picking up, or placing an object might be affected by the positions of nearby objects. As a result, our object-oriented belief update system would need to be modified to handle these object interactions in more complex scenarios. HOO-POMDP cannot handle an unknown class of objects. It could potentially be handled by categorizing all of the known object types into a single \u2018unknown' class. The difficult part, however, is to plan to find an empty space to move the unknown object to. In the worst case, this could lead to complicated packing problems that are NP-hard."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "In this paper, we presented a novel Hierarchical Object-Oriented POMDP Planner (HOO-POMDP) for solving multi-object rearrangement problems in partially observable, multi-room environments. Our approach decomposes the complex task into a high-level abstract POMDP planner for generating sub-goals and low-level policies for execution. Key components include an object-oriented state representation, belief updating to handle perception uncertainty, and an abstraction system to bridge the gap between continuous and discrete planning. Experimental results across multiple datasets demonstrate the effectiveness of our approach in handling challenging scenarios such as blocked paths and goals. The HOO-POMDP framework showed robust performance in terms of success rate and efficiency comparable to Oracle baselines with perfect knowledge or perfect detection. Notably, our method scaled well to environments with more objects and rooms. One of the ways to expand the scope is to relax the assumption of object independence partially. We can allow objects to be dependent on a small number of objects (e.g., objects in their close vicinity). Belief updates can now consider a small set of objects at any time. This relaxation helps maintain the efficient belief update while accounting for more real-world situations such as object-object interaction. Another potential future work is to handle stacking of objects and more cramped spaces, where more careful reasoning about object interactions is needed to plan the actions and order them appropriately."}, {"title": "A.1 OBJECT DETECTION", "content": "A.1.1 DETECTION MODEL: YOLOV 10\nWe collect data from the AI2Thor simulator. We do this by placing the agent in random locations in 500 scenes and extracting the RGB images along with the ground truth object bounding box annotations from the simulator. We have 50 pickupable object classes in all our scenes combined. We train the YoloV10 detector"}]}