{"title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting", "authors": ["Zilong Wang", "Zifeng Wang", "Long Le", "Huaixiu Steven Zheng", "Swaroop Mishra", "Vincent Perot", "Yuwei Zhang", "Anush Mattapalli", "Ankur Taly", "Jingbo Shang", "Chen-Yu Lee", "Tomas Pfister"], "abstract": "Retrieval augmented generation (RAG) combines the generative abilities of large language models (LLMs) with external knowledge sources to provide more accurate and up-to-date responses. Recent RAG advancements focus on improving retrieval outcomes through iterative LLM refinement or self-critique capabilities acquired through additional instruction tuning of LLMs. In this work, we introduce SPECULATIVE RAG a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM. Each draft is generated from a distinct subset of retrieved documents, offering diverse perspectives on the evidence while reducing input token counts per draft. This approach enhances comprehension of each subset and mitigates potential position bias over long context. Our method accelerates RAG by delegating drafting to the smaller specialist LM, with the larger generalist LM performing a single verification pass over the drafts. Extensive experiments demonstrate that SPECULATIVE RAG achieves state-of-the-art performance with reduced latency on TriviaQA, MuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy by up to 12.97% while reducing latency by 51% compared to conventional RAG systems on PubHealth.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable success in question answering tasks (Brown et al., 2020; Achiam et al., 2023; Team et al., 2023). Trained on massive datasets, LLMs leverage their extensive parametric memory to generate seemingly plausible responses to user queries (Kojima et al., 2022; Kamalloo et al., 2023). However, when faced with knowledge-intensive questions demanding up-to-date information or obscure facts (Petroni et al., 2021), LLMs can struggle with factual inaccuracies and produce hallucinated content (Huang et al., 2023; Xu et al., 2024).\nRetrieval Augmented Generation (RAG) has emerged as a promising solution to mitigate these issues. By incorporating information retrieved from an external database into the context (Gao et al., 2023b), RAG effectively reduces factual errors in knowledge-intensive tasks. This approach not only enables easy and efficient access to vast databases but also facilitates timely and accurate knowledge integration Due to the inherent limitations in the precision of current dense retrievers and the vastness of knowledge required to answer complex questions (Chen et al., 2022), RAG systems typically retrieve multiple documents to ensure the inclusion of all necessary information in the context (Petroni et al., 2021). This practice inevitably increases the length of the input to the LLMs,"}, {"title": "2 Related Works", "content": "Retrieval Augmented Generation Retrieval Augmented Generation (RAG) enhances LLMs by retrieving relevant documents from external databases and incorporating them into the generation process (Gao et al., 2023b; Lewis et al., 2020; Khandelwal et al., 2020; Izacard & Grave, 2021; Luo et al., 2023a). Recent work has primarily focused on enabling LLMs to understand when and what to retrieve (Ma et al., 2023; Chen et al., 2023b; Jiang et al., 2023b; Schick et al., 2024), or designing approaches to better utilize contexts (Yu et al., 2023; Yoran et al., 2023; Wang et al., 2023b; Sarthi et al., 2024; Baek et al., 2023; Xu et al., 2023; Kim et al., 2024). Among them, SAIL (Luo et al., 2023a) fine-tunes a pre-trained LLM on web search data to filter irrelevant contents. Self-Reflective RAG (Asai et al., 2023) introduces reflection tokens to guide retrieval and annotation in instruction-tuning datasets. However, both approaches require additional instruction-tuning of generic LLMs, which is resource-intensive and may lead to forgetting or over-fitting (Luo et al., 2023b). Furthermore, long context with retrieved documents can suffer from computational inefficiency and position bias (Liu et al., 2024). Corrective RAG (Yan et al., 2024) on the other hand proposes a lightweight retrieval evaluator, but it lacks the capability for high-level reasoning. In contrast, our proposed SPECULATIVE RAG addresses these limitations by leveraging a smaller RAG drafter model to efficiently understand diverse perspectives in retrieval results and generate drafts for the generalist LMs to verify and integrate.\nSpeculative Decoding Speculative decoding (Stern et al., 2018; Xia et al., 2023; Chen et al., 2023a; Leviathan et al., 2023; Xia et al., 2024) aims to reduce auto-regressive decoding latency through a draft-then-verify paradigm. This involves drafting multiple future tokens with a small model and verifying them in parallel with the target model (Xia et al., 2024). The draft model is typically either an independent model from the same series (Leviathan et al., 2023; Chen et al., 2023a) or the target model itself (Zhang et al., 2023a; Cai et al., 2024). Our approach extends this concept from token-level drafting to answer-level drafting. In contrast to traditional verification criteria (Stern et al., 2018; Xia et al., 2023; Leviathan et al., 2023; Chen et al., 2023a; Miao et al., 2024), which accept or reject tokens based on their generation probabilities, we leverage language modeling objectives to directly assess the confidence of entire answer drafts."}, {"title": "3 Speculative Retrieval Augmented Generation through Drafting", "content": "Problem Formulation In knowledge intensive tasks, each entry can be represented as $(Q, D, A)$, where Q is a question or statement that requires additional knowledge; $D = \\{d_1, ..., d_n\\}$ is a set of n documents retrieved from the database; A is the expected answer. Particularly, in question answering tasks, Q and A are the question and the expected answer in natural language form; in the statement verification tasks, Q is a statement and $A \\in \\{\\text{True}, \\text{False}\\}$ is a Boolean value indicating the statement's correctness; in the multiple choice tasks, Q is a question with a few options and \u0410 \u2208 {A, B, C, ...} is the index of the correct answer. The objective of a RAG system is to generate a fluent response containing the expected answer or select the expected answer from the provided options based on the context provided by the retrieved supporting documents."}, {"title": "3.1 Overview", "content": "We introduce Speculative Retrieval Augmented Generation (SPECULATIVE RAG), as illustrated in Figure 1. We aim at enhancing the reasoning ability of LLMs over retrieved documents without compromising processing speed. Instead of relying on brute-force parameter scaling or instruction-tuning an entire LM to handle knowledge-intensive tasks, we propose a divide-and-conquer approach. We utilize a smaller specialist LM, the RAG drafter, to rapidly generate multiple answer drafts based on retrieved results. Then, a larger generalist LM, the RAG verifier, assesses these drafts, selects the best one based on its rationale, and integrates it into the generation results."}, {"title": "Algorithm 1: SPECULATIVE RAG", "content": "Data: (Q, D = {di}) is the question and n retrieved documents; m subsets, each containing k\ndocuments, are sampled from D; k also corresponds to the number of clusters during clustering.\nResult: A is the predicted answer to the question.\n1 Function Speculative RAG (Q, D, m, k):\n2\n$\\qquad \\{\\text{C}_1, \\text{C}_2, ..., \\text{C}_k\\} \\xleftarrow{K-Means} C(\\text{d}_1, ..., \\text{d}_n|\\text{Q}) $ \n3$\\qquad \\triangle \\leftarrow \\{\\}$\n4$\\qquad$ repeat\n5$\\qquad$ $\\qquad \\delta_j \\leftarrow \\{\\}$\n6$\\qquad$ $\\qquad$ for $c_i \\in \\{\\text{C}_1, ..., \\text{C}_k\\}$ do\n7$\\qquad$ $\\qquad$ $\\qquad \\delta_j = \\delta_j \\cup \\{\\text{random.sample}(c_i)\\}$\n8$\\qquad$ $\\qquad$ end\n9$\\qquad$ $\\qquad \\triangle = \\triangle \\cup \\delta_j$\n10$\\qquad$ until $|\\triangle| = m $\n11$\\qquad$ for $\\delta_j \\in \\triangle$ do in parallel\n12$\\qquad$ $\\qquad a_j, \\beta_j \\leftarrow \\text{M}_{\\text{Drafter}}.\\text{generate}(Q, \\delta_j)$\n13$\\qquad$ $\\qquad P_j \\leftarrow \\text{M}_{\\text{Verifier}}.\\text{score}(a_j|Q, \\beta_j)$\n14$\\qquad$ end\n15$\\qquad$ $\\hat{A} \\leftarrow \\text{arg max}_{a_j} P_j$\n16 return $\\hat{A}$\nSpecifically, as shown in Algorithm 1, we first cluster the retrieved documents with regard to their relation to the posed question, where each cluster represents one perspective in the retrieval results (Line 2). Then we sample one document from each cluster into a subset so the documents in this subset covers the multiple perspectives in the retrieval results. We aim at minimizing redundancy and increase the diversity of the documents (Line 5 to 8). We denote one subset as $\\delta \\subset D$ that contains retrieved documents with diverse contents and multiple perspectives in the retrieval results. Then, we distribute each subset d to a RAG drafter endpoint $\\text{M}_{\\text{Drafter}}$ with the posed question Q to generate the answer draft a and the rationale \u1e9e in parallel (Line 12). The RAG drafter is instruction-tuned to be a specialist in understanding the retrieved documents and produce rationales that are faithful to the input documents. It is smaller than generalist LMs, and its parallel processing further ensures high efficiency. For each draft-rationale pair (\u03b1, \u03b2) from $\\text{M}_{\\text{Drafter}}$, we compute a confidence score with the generalist LM $\\text{M}_{\\text{Verifier}}$ based on the question Q and corresponding rationale \u1e9e (Line 13). It is worth mentioning that $\\text{M}_{\\text{Verifier}}$ does not need to be instruction-tuned since we leverage its language modeling ability already learned during pre-training. Meanwhile, $\\text{M}_{\\text{Verifier}}$ can verify the drafts based on the informative rationale provided by $\\text{M}_{\\text{Drafter}}$ instead of processing tedious or possibly redundant retrieved documents. Finally, we select the answer draft with the highest confidence score as the final answer and integrate it into the generation results of the generalist LM (Line 15)."}, {"title": "3.2 Specialist RAG Drafter", "content": "Instead of tuning a large generalist LM for the RAG scenario, we leverage a smaller specialist LM, $\\text{M}_{\\text{Drafter}}$, to understand retrieved documents. $\\text{M}_{\\text{Drafter}}$ is specialized in answering the given question based on the supporting documents and not expected to cope with general problems. It serves as a RAG module for the generalist LMs when solving knowledge-intensive tasks. We train $\\text{M}_{\\text{Drafter}}$ to generate both the answer draft and the rationale to better understand the contextual documents.\nInstruction Tuning Given a triplet (Q, A, D), where Q is a general query, A is the response, and D is a retrieved supporting document, we augment it with the rationale of the response A based on the document D. We denote the rationale as E which extracts essential information from the document and explains why the response is reasonable to the query concisely (Hsieh et al., 2023) so it is of"}, {"title": "Multi-Perspective Sampling", "content": "For each knowledge-intensive question, we retrieve a set of documents from the database using the posed question as the retrieval query. These documents may contain diverse content due to the ambiguity inherent in the query. To minimize redundancy and enhance diversity of the document subsets used for generating answer drafts, we employ a multi-perspective sampling strategy. We first cluster the documents into a few topics using an instruction-aware embedding model (Peng et al., 2024) and the K-Means clustering algorithm (Jin & Han, 2011).\n$\\text{emb}(\\text{d}_1), ..., \\text{emb}(\\text{d}_n) = E(\\text{d}_1, ..., \\text{d}_n|\\text{Q})$\n$\\text{C}_1, ..., \\text{C}_k = \\text{K-Means}(\\text{emb}(\\text{d}_1), ..., \\text{emb}(\\text{d}_n))$\n$d = \\{\\text{random.sample}(\\text{c}) \\text{ for } c\\in \\{\\text{c}_i\\}\\}\\$\nwhere $\\mathcal{E}$ is an instruction-aware embedding model which embeds a string with regard to a provided instruction (the posed question Q); emb(d\u2081) is the embedding for the retrieved document di; cj is a cluster of retrieved documents with similar topics and contents; k is a hyper-parameter that controls the number of clusters. We sample one document from each cluster into a document subset & so each subset contains k documents of diverse contents. In total, we construct m subsets for parallel inference with the RAG drafter."}, {"title": "RAG Drafting", "content": "We run $\\text{M}_{\\text{Drafter}}$ over the m document subsets and produce corresponding answer drafts. Refer to Appendix B for detailed prompt. We incorporate each document subset into the prompt and query $\\text{M}_{\\text{Drafter}}$ for responses. We obtain m drafts as the answer candidates and each draft is grounded based on the multiple perspectives in the retrieval results. Specifically, given a document subset $d_j = \\{d_{j1}, .., d_{jk}\\}$, we query $\\text{M}_{\\text{Drafter}}$ in parallel with the following prompt for the answer draft and rationale: $Q, d_{j1}, ..., d_{jk} \\rightarrow a_j, \\beta_j$, where the prompt contains the posed question Q along with the document subset; the generation result contains the answer draft a and the rationale B. We denote the conditional generation probability as $p_{\\text{Draft},j} = P(\\beta_j|Q, d_{j1}, ..., d_{jk}) + P(a_j|Q, d_{j1}, ..., d_{jk}, \\beta_j)$, which measures the reliability of generating rationales and the confidence in producing answer drafts."}, {"title": "3.3 Generalist RAG Verifier", "content": "After generating drafts and the rationale from the RAG drafter $\\text{M}_{\\text{Drafter}}$, we evaluate them by a generalist LM $\\text{M}_{\\text{Verifier}}$ to filter out the less reliable drafts and select the best answer. The generalist LM can be any off-the-shelf pre-trained LM. We only consider the draft-rationale pair (\u03b1, \u03b2) and skip the tedious and redundant retrieval results. We resort to the language modeling ability of the generalist LM to rank and select the draft-rationale pairs.\nEvaluation Scores First, we calculate the self-consistency score by determining the conditional probability of generating a draft-rationale pair given the question, $P_{\\text{Self-contain}} = P(a, \\beta|Q)$. This score helps assess whether the draft and rationale are self-consistent in the context of the question. Given the characteristics of language modeling, a self-consistent draft-rationale pair is expected to yield a higher probability. Furthermore, we incorporate a self-reflection statement R that prompts $\\text{M}_{\\text{Verifier}}$ to assess the reliability of an answer draft (e.g. \u201cDo you think the rationale supports the answer, yes or no?\u201d). We define the self-reflection score as $p_{\\text{Self-reflect}} = P(\\text{``Yes''}|Q, a, \\beta, R)$ where we compute the conditional probability of the positive answer (\u201cYes\u201d) to the self-reflection statement.\nComputation Method We can efficiently compute the self-consistency and self-reflection scores within one forward pass of $\\text{M}_{\\text{Verifier}}$. Given a question Q and a draft-rationale pair (\u03b1, \u03b2), we construct a prompt [Q, \u03b1, \u03b2, R, \"Yes\"], where R is the self-reflection statement. We encode the prompt with $\\text{M}_{\\text{Verifier}}$, and acquire the probability of each token conditioned on the previous tokens $P(t_i|t_{<i})$. We leverage this auto-regressive feature and aggregate the probability of the relevant"}, {"title": "4 Experiments", "content": "We evaluate our proposed SPECULATIVE RAG on four public retrieval augmented generation benchmarks: TriviaQA (unfiltered) (Joshi et al., 2017), MuSiQue (Trivedi et al., 2022), PubHealth (Zhang et al., 2023b), and ARC-Challenge (Clark et al., 2018). TriviaQA and MuSiQue are challenging open-domain question answering datasets where RAG systems are required to answer questions on factual knowledge. TriviaQA typically requires one accurate piece of evidence from the documents, whereas MuSiQue demands multiple documents to construct a multi-hop reasoning chain. Following previous works (Guu et al., 2020; Asai et al., 2023; Yan et al., 2024), we evaluate performance of the free-form generation based on whether gold answers are contained within the generated response or not. PubHealth and ARC-Challenge are closed-set generation datasets. PubHealth is a dataset of medical claims spanning a variety of biomedical subjects and it requires the RAG system to verify a given claim based on the retrieved documents. ARC-Challenge introduces a multi-choice question answering dataset, composed of science exam questions from grade 3 to grade 9. For closed-set generation tasks, we use accuracy metrics to evaluate whether the generated answers match the ground truth."}, {"title": "4.1 Baselines", "content": "Standard RAG For standard RAG, we incorporate all the retrieved documents into the prompt as contextual information. Refer to Appendix C for detailed prompts. We run standard RAG experiments on off-the-shelf LLMs including Mistral7B, Mistral-Instruct7B (Jiang et al., 2023a), Mixtral8x7B, Mixtral-Instruct8x7B (Jiang et al., 2024), and Alpaca7\u0432 (Dubois et al., 2024). We also include the performance of Toolformer (Schick et al., 2024) and SAIL (Luo et al., 2023a) which are originally reported from Asai et al. (2023). Toolformer7\u00df is an LM instruction-tuned to use tools including a search engine, and SAIL7B is an LM instruction-tuned on the Alpaca instruction tuning set augmented with search results from different sources such as DuckDuckGo and Wikipedia.\nSelf-Reflective RAG and Corrective RAG Self-Reflective RAG (Self-RAG) (Asai et al., 2023) and Corrective RAG (CRAG) (Yan et al., 2024) are more advanced RAG systems that enhances the quality of contextual information in the retrieval results. CRAG introduces an external evaluator to assess the quality of retrieved documents, and to refine them before the response generation. Self-RAG instruction-tunes an LM to generate special self-refection tags. These tags guides the LM to dynamically retrieve documents when necessary, critique the retrieved documents relevance before generating responses. Self-CRAG is to apply the Self-RAG approach on the refined documents of CRAG. We adopt the same backbone LLMs across all methods as our proposed SPECULATIVE RAG for fair comparisons."}, {"title": "4.2 Experiment Settings", "content": "In our experiments, we utilize Mistral7B (v0.1) as our base LM for the RAG drafter. For RAG verifier, we employ either Mistral7B (v0.1) or Mixtral8x7B (v0.1) without any fine-tuning, denoted as MVerifier-7B or Mverifier-8x7B. We pre-compute embeddings of retrieved documents using a lightweight instruction-aware embedding model InBedderRoberta (Peng et al., 2024) as part of the retrieval process. Inference is conducted using the vLLM framework (Kwon et al., 2023) with greedy decoding (temperature = 0). We adopt the same experiment settings from Asai et al. (2023) and include a more challenging benchmark, MuSiQue (Trivedi et al., 2022). Our focus is on RAG reasoning rather than evidence citation, so we omit the other two long-form generation benchmarks, Biography (Min et al., 2023) and ALCE-ASQA (Gao et al., 2023a). On TriviaQA, PubHealth, and ARC-Challenge, we retrieve top 10 documents and generate 5 drafts per query (m = 5), with each draft based on a subset of 2 documents (k = 2). For the MuSiQue dataset, we retrieve top 15 documents and generate 10"}, {"title": "4.3 Main Results", "content": "We compare SPECULATIVE RAG with standard RAG approaches, as well as the more advanced Self-Reflective RAG and Corrective RAG on four datasets: TriviaQA, MuSiQue, PubHealth, and ARC-Challenge. We report the performance of $\\text{M}_{\\text{Drafter-7B}}$ when used alone or paired with the RAG verifier (e.g. $\\text{M}_{\\text{Verifier-7B}}$, $\\text{M}_{\\text{Verifier-8x7B}}$). Following prior work (Asai et al., 2023; Yan et al., 2024), we report accuracy as the performance metric.\nSuperior Performance over Baselines Table 1 demonstrates that SPECULATIVE RAG consis-tently outperforms all baselines across all four benchmarks. Particularly, $\\text{M}_{\\text{Verifier-8x7B}} + \\text{M}_{\\text{Drafter-7B}}$ surpasses the most competitive standard RAG model, Mixtral-Instruct8x7B, by 0.33% on TriviaQA, 2.15% on MuSiQue, 12.97% on PubHealth, and 2.14% on ARC-Challenge. With a comparable number of instruction-tuned parameters, $\\text{M}_{\\text{Verifier-7B}} + \\text{M}_{\\text{Drafter-7B}}$ outperforms all Self-Reflective and Corrective RAG methods, and $\\text{M}_{\\text{Drafter}}$ alone can surpass these baselines in most settings.\nEffective Instruction Tuning for RAG Drafter Our instruction tuning is effective in enhancing the reasoning ability of the drafter model (Hsieh et al., 2023), as we observe a remarkable perfor-mance improvement comparing Mistral7B and $\\text{M}_{\\text{Drafter-7B}}$. Moreover, the performance of Mixtral8x7B significantly improves when paired with the instruction-tuned RAG drafter $\\text{M}_{\\text{Drafter-7B}}$, showing gains of 14.39% on TriviaQA, 12.41% on MuSiQue, 39.52% on PubHealth, and 31.83% on ARC-Challenge. Similar improvements are observed with Mistral7\u00df as well. For Mistral7\u00df, we observed improvements of 19.76% on TriviaQA, 14.32% on MuSiQue, 40.94% on PubHealth, and 33.44% on ARC-Challenge. We attribute these improvements to the superior reasoning capabilities of the RAG drafter over the retrieved documents in SPECULATIVE RAG. By minimizing the redundancy in the sampled documents, the RAG drafter generates higher quality answer drafts based on diverse perspectives from the retrieval results.\nReliable Scoring by RAG Verifier The reliable draft verification by the generalist LM also con-tributes to the enhanced performance. The performance improves remarkably comparing $\\text{M}_{\\text{Drafter-7B}}$ and $\\text{M}_{\\text{Verifier-7B}} + \\text{M}_{\\text{Drafter-7B}}$. The instruction-tuned RAG drafter is specialized in generating answer drafts based on the retrieved documents while the language modeling capabilities of generic LMs are leveraged to validate each draft in light of its rationale. This method is both effective and easy to implement, showcasing the effectiveness of this verification approach."}, {"title": "4.4 Latency Analysis", "content": "We analyze the latency of Standard RAG and our SPECULATIVE RAG on TriviaQA, MuSiQue, PubHealth, and ARC-Challenge. We randomly sample 100 cases from each dataset and report the average time cost for each case, as shown in Figure 2. To simulate real-world application scenarios, we process cases individually without batching. As representative example, we run $\\text{M}_{\\text{Verifier-8x7B}} + \\text{M}_{\\text{Drafter-7B}}$ for SPECULATIVE RAG and Mixtral-Instruct8x7B for Standard RAG, as these demonstrate the highest performance among competitive baselines (see Table 1). We launch 5 endpoints of $\\text{M}_{\\text{Drafter-7B}}$ for parallel drafting on TriviaQA, PubHealth, and ARC-Challenge. We launch 10 endpoints for MuSiQue due to more drafts. We use tensor parallelism to fit Mixtral-Instruct8x7B into the GPU memory. We report the latency of Mixtral-Instruct8x7B under tensor parallelism sizes of 4, 8, 16. Increasing tensor parallelism does not improve efficiency due to overheads in tensor aggregation and communication. In contrast, SPECULATIVE RAG, with its smaller RAG drafter and parallel draft generation, consistently achieves the lowest latency across all datasets. Particularly, it reduces latency by up to 23.41% on TriviaQA, 17.28% on MuSiQue, 51.25% on PubHealth, and 26.73% on ARC-Challenge. This highlights the advantage of our approach in reducing processing time while maintaining high performance."}, {"title": "4.5 Ablation Studies", "content": "We conduct ablation studies on the key components of SPECULATIVE RAG during both drafting and verification stages on TriviaQA and PubHealth in Table 2. We use $\\text{M}_{\\text{Verifier-8x7B}} + \\text{M}_{\\text{Drafter-7B}}$ as a running configuration. Same as the main results, we report the accuracy as performance metrics.\nDiversity and reduced redundancy in retrieval improves draft quality significantly. In the first set of experiments, we evaluate the impact of multi-perspective sampling during the drafting. Recall that SPECULATIVE RAG clusters retrieved documents into distinct perspectives and sample one document from each cluster to reduce redundancy for the draft generation. We compare this against two alternative sampling strategies: (1) Random sampling without clustering, where we randomly select a document subset as context, and (2) Sampling from the same cluster, where we select all documents from a single cluster. Our results indicate that our proposed sampling method yields the best performance thanks to its ability to leverage diverse context. Particularly, it improves the accuracy up to 1.88% on TriviaQA and 2.23% on PubHealth. While random sampling without clustering introduces diversity, it is prone to including redundant documents, degrading draft quality. Sampling from the same cluster significantly underperforms due to a lack of diverse perspectives.\nScoring method on self-consistency and self-reflection refines draft quality effectively. In the second set of experiments, we examine the scoring method during verification. We remove each of the specific confidence scores, $P_{\\text{Draft}}$, $P_{\\text{Self-contain}}$, or $P_{\\text{Self-reflect}}$ in turn. Performance drops are observed when any score is removed. Particularly, removing $P_{\\text{Draft}}$ leads to a minimal decline, 0.19% on TriviaQA and 1.12% on PubHealth, likely due to the limited verification capability of the smaller RAG drafter. Removing either $p_{\\text{Self-contain}}$ or $p_{\\text{Self-reflect}}$ results in similar performance decreases, around 2.0% on TriviaQA and around 0.8% on PubHealth, indicating that both self-containment and self-reflection capture different key aspects of reasoning and are crucial during verification. Random"}, {"title": "4.6 Effects of Generated Rationale for Verification", "content": "In SPECULATIVE RAG, we utilize the generated rationale \u1e9e from the RAG drafter as an indicator of the trustworthiness of answer drafts a. The rationales highlight relevant points, omit redundant information, and bridge logical gaps between drafts and their supporting documents. To evaluate the effectiveness of the rationales, we create two alternative scoring methods: (a) replacing rationale with retrieved documents ($p = \\text{Score}(a|Q, \\delta)$), or (b) adding retrieved documents to rationale ($p = \\text{Score}(a|Q, \\beta, \\delta)$). We compare these alternatives to the scoring method used in SPECULATIVE RAG ($p = \\text{Score}(a|Q, \\beta)$) in Table 3. The results show that incorporating longer retrieved documents does not consistently improve performance and tends to increase latency. This suggest that the generated rationale is already of high quality and serves as an effective bridge between the supporting documents and the generated answer drafts. By leveraging this rationale, we can efficiently verify drafts using a generic LM, leading to accurate final results."}, {"title": "4.7 Effects of Draft Number and Document Subset Size", "content": "We investigate the performance of SPECULATIVE RAG under varying numbers of drafts. Using $\\text{M}_{\\text{Verifier-7B}} + \\text{M}_{\\text{Drafter-7B}}$ with 5, 10, 15, 20 drafts on TriviaQA and PubHealth. We sample two documents as context per draft. The results are illustrated in Figure 3(a). Since we retrieve top 10 documents in total, we sample up to 20 drafts in these experiments. The results indicate that incorporating more drafts can further improve performance, likely thanks to higher coverage of diverse perspective of documents. Importantly, in SPECULATIVE RAG, we can launch multiple RAG drafter instances to generate drafts in parallel without additional latency.\nWe also examine the effect of document subset size. By varying the number of documents (1, 2, 4, or 6) sampled for draft generation on TriviaQA and PubHealth (Figure 3(b)), we find that including more documents in the context does not always lead to consistent performance improvement. While TriviaQA queries may benefit from more supporting documents due to their complexity, $\\text{M}_{\\text{Verifier-7B}} + \\text{M}_{\\text{Drafter-7B}}$ can surpass Mistral-Instruct7\u00df even with a single supporting document per draft. Furthermore, with two or more documents per draft, $\\text{M}_{\\text{Verifier-7B}} + \\text{M}_{\\text{Drafter-7B}}$ can even surpass Mixtral-Instruct8x7B. This further demonstrates the effectiveness of our drafting design."}, {"title": "5 Conclusion", "content": "Our proposed SPECULATIVE RAG decomposes RAG tasks into two separate steps of drafting followed by verification. SPECULATIVE RAG delegates the heavy lifting of drafting to a small specialized RAG drafter, while verification is done using a large generalist LM. The parallel generation of multiple drafts from diverse document subsets provides high quality answer candidates while reducing input token counts and the potential risk of position-bias-over-long-context, resulting in substantial improvements in both the quality and speed of the final output generation. We demonstrate the effectiveness of SPECULATIVE RAG with accuracy gains up to 12.97% while reducing latency by 51% compared to conventional RAG systems. SPECULATIVE RAG sheds new light on the potential of collaborative architectures for enhancing RAG performance through task decomposition."}, {"title": "Limitations", "content": "In this paper, we demonstrate that a smaller, specialized RAG drafter can effectively augment a larger, general-purpose LM for knowledge-intensive tasks. While SPECULATIVE RAG enhances both accuracy and efficiency, it does require training an additional drafter model. Although this step is computationally inexpensive, it adds a layer of complexity compared to using a vanilla instruction-tuned RAG model."}, {"title": "E Instruction-Tuning Settings", "content": "We construct our training dataset for the RAG drafter from diverse instruction-following pairs. We sample instances from Open-Instruct processed data (Wang et al., 2023a) and knowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). We augment the instruction-following pairs with retrieved documents and generated rationale. We use the off-the-shelf dense retriever Contriever-MS MARCO (Izacard et al., 2021) to retrieve up to 10 documents for each pair and use Gemini-Ultra (Team et al., 2023) to generate rationale. In total, we acquire a dataset of 40k instances. We use Mistral7B (v0.1) as our base LM for the RAG drafter. We reproduce the performance of Self-RAG (Asai et al., 2023) and CRAG (Yan et al., 2024) with Mistral7\u00df (v0.1) for a fair comparison. We implement the training scripts using the Transformers library from Hugging Face (Wolf et al., 2019). We employ DeepSpeed (Rasley et al., 2020) to accelerate the training process. All experiments are conducted on a Linux server equipped with 16 Nvidia A100-SXM4-40GB GPUs."}, {"title": "F Effects of Self-Reflection Statement", "content": "We use \"Do you think the explanation supports the answers? (Yes or No)\u201d as the self-reflection statement in our main results. In this study, we replace it with other alternatives to see how the self-reflection statement affects the accuracy. The results are reported in Table 4. We observe that the performance does not change a lot given"}]}