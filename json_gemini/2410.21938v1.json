{"title": "ReMix: Training Generalized Person Re-identification on a Mixture of Data", "authors": ["Timur Mamedov", "Anton Konushin", "Vadim Konushin"], "abstract": "Modern person re-identification (Re-ID) methods have a weak generalization ability and experience a major accuracy drop when capturing environments change. This is because existing multi-camera Re-ID datasets are limited in size and diversity, since such data is difficult to obtain. At the same time, enormous volumes of unlabeled single-camera records are available. Such data can be easily collected, and therefore, it is more diverse. Currently, single-camera data is used only for self-supervised pre-training of Re-ID methods. However, the diversity of single-camera data is suppressed by fine-tuning on limited multi-camera data after pre-training. In this paper, we propose ReMix, a generalized Re-ID method jointly trained on a mixture of limited labeled multi-camera and large unlabeled single-camera data. Effective training of our method is achieved through a novel data sampling strategy and new loss functions that are adapted for joint use with both types of data. Experiments show that ReMix has a high generalization ability and outperforms state-of-the-art methods in generalizable person Re-ID. To the best of our knowledge, this is the first work that explores joint training on a mixture of multi-camera and single-camera data in person Re-ID.", "sections": [{"title": "1. Introduction", "content": "Person re-identification (Re-ID) is the task of recognizing the same person in images taken by different cameras at different times. This task naturally arises in video surveillance and security systems, where it is necessary to track people across multiple cameras. The urgent need for robust and accurate Re-ID has stimulated scientific research over the years. However, modern Re-ID methods still have a weak generalization ability and experience a significant performance drop when capturing environments change, which limits their applicability in real-world scenarios.\nThe main reasons for the weak generalization ability of modern methods are the small amount of training data and the low diversity of capturing environments in this data. In person Re-ID, the same person may appear across multiple cameras from different angles (multi-camera data), and such data is difficult to collect and label. Due to these difficulties, each of the existing Re-ID datasets is captured from a single location. In contrast, collecting images of people from one camera (single-camera data) is much easier; for example, these images can be automatically extracted from YouTube videos featuring numerous diverse identities in distinct locations and a high diversity of capturing environments (Tab. 1).\nHowever, single-camera data is much simpler than multi-camera data in terms of the person Re-ID task: in single-camera data, the same person can appear on only one camera and from only one angle (Fig. 1). Directly adding such simple data to the training process degrades the quality of Re-ID. Therefore, single-camera data is currently used only for self-supervised pre-training. However, we hypothesize that this approach has a limited effect on improving the generalization ability of Re-ID methods because subsequent fine-tuning for the final task is performed on relatively small and non-diverse multi-camera data.\nIn this paper, we propose ReMix, a generalized Re-ID method jointly trained on a mixture of limited labeled multi-camera and large unlabeled single-camera data. ReMix achieves better generalization by training on diverse single-camera data, as confirmed by our experiments. We also experimentally validate our hypothesis regarding the limitations of self-supervised pre-training and show that our joint training on two types of data overcomes them. In our ReMix method, we propose:\n\u2022 A novel data sampling strategy that allows for efficiently obtaining pseudo labels for large unlabeled single-camera data and for composing mini-batches from a mixture of images from labeled multi-camera and unlabeled single-camera datasets.\n\u2022 A new Instance, Augmentation, and Centroids loss functions adapted for joint use with two types of data, making it possible to train ReMix. For example, the Instance and Centroids losses consider the different complexities of multi-camera and single-camera data, allowing for more efficient training of our method.\n\u2022 Using self-supervised pre-training in combination with the proposed joint training procedure to improve pseudo labeling and the generalization ability of the algorithm.\nOur experiments show that ReMix outperforms state-of-the-art methods in the cross-dataset and multi-source cross-dataset scenarios (when trained and tested on different datasets). To the best of our knowledge, this is the first work that explores joint training on a mixture of multi-camera and single-camera data in the person Re-ID task."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Person Re-identification", "content": "Rapid progress in solving the person re-identification task over the past few years has been associated with the emergence of CNNs. Some Re-ID approaches used the entire image to extract features . Other methods divided the image of a person into parts, extracted features for each part, and aggregated them to obtain full-image features. Recently, transformer-based Re-ID methods have emerged, which also improve the quality of solving the problem.\nRecent Re-ID methods perform well in the standard scenario, but their quality is significantly reduced when applied to datasets that differ from those used during training (when capturing environments change). In this paper, we explore the problem of weak generalization ability of existing Re-ID methods and show that it can be improved by properly using a mixture of two types of training data multi-camera and single-camera."}, {"title": "2.2. Generalizable Person Re-identification", "content": "Generalizable person re-identification aims to learn a robust model that performs well across various datasets. To achieve this goal, improved normalizations adapted to generalizable person Re-ID were proposed in . A new residual block, consisting of multiple convolutional streams, each detecting features at a specific scale, was proposed in to create a specialized neural network architecture adapted to the person Re-ID task. In , the ideas from  were continued, and an updated architecture with normalization layers was proposed to improve the generalization ability of the algorithm. Transformer-based models were also used to solve the problem under consideration: in  it was shown that local parts of images are less susceptible to domain gap, making it more effective to compare two images by their local parts in addition to global visual information during training. In , a new effective method for composing mini-batches during training was suggested, which improved the generalization ability of the algorithm.\nAs we can see, in most existing approaches, improving the generalization ability of the Re-ID algorithm has been achieved through the use of complex architectures. In contrast, in this paper, we prove that generalization can be achieved by properly training an efficient model using a variety of data, which is important in practice."}, {"title": "2.3. Self-supervised Pre-training", "content": "Self-supervised pre-training is an approach for training neural networks using unlabeled data to learn high-quality primary features. Such pre-training is usually performed by defining relatively simple tasks that allow training data to be generated on the fly, for example: context prediction , solving a puzzle , predicting an image rotation angle . In  self-supervised approaches based on contrastive learning were proposed: there, the neural network was trained to bring images of the same class closer in space and push away negative instances.\nSelf-supervised pre-training was also used in person re-identification ."}, {"title": "3. Proposed Method", "content": null}, {"title": "3.1. Overview", "content": "The scheme of ReMix is presented in Fig. 2. The proposed method consists of two neural networks with identical architectures the encoder and the momentum encoder. The main idea of ReMix is to jointly train the Re-ID algorithm on a mixture of labeled multi-camera data for this task, and diverse unlabeled single-camera images of people. Therefore, during training, mini-batches consisting of these two types of data are used. The novel data sampling strategy is described in Sec. 3.2.\nThe encoder is trained using new loss functions that are adapted for joint use with two types of data: the Instance Loss $L_{ins}$ (Sec. 3.3.1), the Augmentation Loss $L_{aug}$ (Sec. 3.3.2), and the Centroids Loss $L_{cen}$ (Sec. 3.3.3) are calculated for both types of data, whereas the Camera Centroids Loss $L_{cc}$ (Sec. 3.3.4) is calculated only for multi-camera data. The general loss function in ReMix has the following form:\n$L = L_{ins} + L_{aug} + L_{cen} + \\gamma L_{cc}$.\n(1)\nThe encoder is updated by backpropagation, and for the momentum encoder, the weights are updated using exponential moving averaging:\n$\\theta_m^t = \\lambda \\theta_m^{t-1} + (1 - \\lambda) \\theta_e^t$,\n(2)\nwhere $\\theta_e^t$ and $\\theta_m^t$ are the weights of the encoder and the momentum encoder at iteration $t$, respectively; and $\\lambda$ is the momentum coefficient.\nThe use of the encoder and the momentum encoder allows for more robust and noise-resistant training, which is important when using unlabeled single-camera data. During inference, only the momentum encoder is used to obtain embeddings. To train ReMix, loss functions involving centroids are applied. Therefore, to achieve training stability and frequent updating of centroids, only a portion of the images passes through the encoder in one epoch. Additionally, this approach reduces computational costs by generating pseudo labels only for a subset of single-camera data in one epoch, rather than for an entire large dataset (Sec. 3.2). ReMix is described in more detail in the supplementary material (see Algorithm 1)."}, {"title": "3.2. Data Sampling", "content": "Let us formally describe the training datasets. Labeled multi-camera data (Re-ID datasets) consist of image-label-camera triples $D_m = \\{(x_i, y_i, c_i)\\}_{i=1}^{i_m}$, where $x_i \\in X$ is the image, $y_i \\in Y_m = \\{1, 2,..., M_m\\}$ is the image's identity label, and $c_i \\in C_m = \\{1, 2, ..., K_m\\}$ is the camera ID. As for unlabeled single-camera data $D_s$, it is a set of videos $\\{V_i\\}_{i=1}^{i_s}$, where each video $V_i$ is a set of unlabeled images $\\{x_j\\}_{j=1}^{N_i}$ of people. In single-camera data, each person appears on only one video.\nSingle-camera data pseudo labeling. Since the proposed method uses unlabeled single-camera data, pseudo labels are obtained at the beginning of each epoch. This is done according to the following algorithm: a video $V_i$ is randomly sampled from the set $D_s$, and images from the selected video are clustered by DBSCAN using embeddings from the momentum encoder and pseudo labeled. This procedure continues until pseudo labels are assigned to all images necessary for training in one epoch. As mentioned in Sec. 3.1, not all images are used for training in one epoch, so we know in advance how many images from unlabeled single-camera data should receive pseudo labels. Thus, our method iteratively obtains pseudo labels for almost all images from the large single-camera dataset. Additionally, it is worth noting that the pseudo labeling procedure uses embeddings from the momentum encoder with weights updated in the previous epoch, which leads to iterative improvements in the quality of pseudo labels. The proposed single-camera data pseudo labeling procedure is described in more detail in the supplementary material (see Algorithm 2).\nMini-batch composition. In our ReMix method, we compose a mini-batch from a mixture of images from multi-camera and single-camera datasets as follows:\n\u2022 For multi-camera data, $N_m^p$ labels are randomly sampled, and for each label, $N_m^i$ corresponding images obtained from different cameras are selected.\n\u2022 For single-camera data, $N_s^p$ pseudo labels are randomly sampled, and for each pseudo label, $N_s^i$ corresponding images are selected.\nThus, the mini-batch has a size of $N_m^p \\times N_m^i + N_s^p \\times N_s^i$ images."}, {"title": "3.3. Loss Functions", "content": null}, {"title": "3.3.1 The Instance Loss", "content": "The main idea of the proposed Instance Loss is to bring the anchor closer to all positive instances and push it away from all negative instances in a mini-batch. Thus, the Instance Loss forces the neural network to learn a more general solution.\nLet us define $Y_{m+s} = Y_m \\cup Y_s$ as the set of all labels for multi-camera data and pseudo labels for single-camera data in a mini-batch. $\\hat{y}_i \\in Y_{m+s}$ is either a label or pseudo label corresponding to the i-th image in a mini-batch. $B_m = N_m^p N_m^i$ is the number of images from multi-camera data in a mini-batch. And $B_s = N_s^p \\times N_s^i$ is the number of images from single-camera data in a mini-batch. Then the Instance Loss is defined as follows:\n$L_{ins} = \\frac{B_m}{B_m+B_s}L_{ins}^{multi-camera} + \\frac{B_s}{B_m+B_s} L_{ins}^{single-camera}$,\n(3)\n$L_{ins}^{multi-camera} = -\\frac{1}{B_m} \\sum_{i=1}^{B_m} log \\frac{exp((f_i\\cdot m_{\\hat{y_i}})/T_{insm})}{\\sum_{k=1}^{N_m+1} exp((f_i \\cdot m_k)/T_{insm})}$,\n(4)\n$L_{ins}^{single-camera} = - \\sum_{i=B_m+1}^{B_m+B_s} log \\frac{exp((f_i\\cdot m_{\\hat{y_i}})/T_{inss})}{\\sum_{k=1}^{N_s+1} exp((f_i \\cdot m_k)/T_{inss})}$,\n(5)\nwhere $f_i, m_i$ are embeddings from the encoder and the momentum encoder for the anchor i-th image in a mini-batch, respectively; $N_m$ and $N_s$ are the numbers of negative instances for the anchor (for multi-camera and single-camera data, respectively); and $(\\cdot)$ denotes cosine similarity. Since multi-camera and single-camera data have different complexities in terms of person Re-ID, we balance them by using temperature parameters in the Instance Loss: $T_{insm}$ for multi-camera data and $T_{inss}$ for single-camera data."}, {"title": "3.3.2 The Augmentation Loss", "content": "The distribution of inter-instance similarities produced by the algorithm can change under the influence of augmentations. After augmentations, an anchor image from the perspective of the neural network may become less similar to its positive pair, but at the same time, similarity to negative instances increases. Thus, current methods may be unstable to image changes and noise that may occur in practice.\nTo address this problem, we propose the new Augmentation Loss, which brings the augmented version of the image closer to its original and pushes it away from instances belonging to other identities in a mini-batch:\n$L_{aug} = -log \\frac{exp((f_{aug_i}\\cdot m_i^o)/T_{aug})}{\\sum_{j=1}^{N+1} exp((f_{aug_i} \\cdot m_j)/T_{aug})}$,\n(6)\nwhere $f_{aug_i}$ is the embedding from the encoder for the augmented i-th image in a mini-batch; $m_i^o$ is the embedding from the momentum encoder for the original i-th image in a mini-batch; and $N$ is the number of negative instances. It is important to note that in the Augmentation Loss, embeddings for the original images are obtained from the momentum encoder, as the momentum encoder is more stable."}, {"title": "3.3.3 The Centroids Loss", "content": "Let us define the concept of a centroid for a label or pseudo label $y \\in Y_{m+s}$ as follows:\n$\\mu_{y_i} = \\frac{1}{P_{y_i}} \\sum_{m \\in M_{\\hat{y_i}}} m,$\n(7)\nwhere $M_{\\hat{y_i}}$ is the set of embeddings from the momentum encoder corresponding to the label or pseudo label $\\hat{y}_i$, and $m$ is an embedding from this set.\nThen the new Centroids Loss can be defined as:\n$L_{cen} = \\frac{1}{B_m + B_s} (\\sum_{i=1}^{B_m} L_{cen}^i (T_{cenm}) + \\sum_{i=B_m+1}^{B_m+B_s} L_{cen}^i (T_{cens}))$,\n(8)\n$L_{cen}^i (T) = -log \\frac{exp(f_{\\hat{y_i}}\\cdot \\mu_{\\hat{y_i}}/T)}{\\sum_{j=1}^{N +1} exp(f_{\\hat{y_i}} \\cdot \\mu_{y_i}/T)}$,\n(9)\nwhere $f_{\\hat{y_i}}$ is the embedding from the encoder for the image with the label or pseudo label $\\hat{y}_i$. Thus, this loss function brings instances closer to their corresponding centroids and pushes them away from other centroids. Like the Instance Loss, this loss function uses different temperature parameters for multi-camera and single-camera data."}, {"title": "3.3.4 The Camera Centroids Loss", "content": "Since the same person could be captured by different cameras in multi-camera data, it is useful to apply information about cameras for better feature generation. In our ReMix method, we use the Camera Centroids Loss. This loss function brings instances closer to the centroids of instances with the same label, but captured by different cameras. Thus, the intra-class variance caused by stylistic differences between cameras is reduced."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Datasets and Evaluation Metrics", "content": "Multi-camera datasets. We employ well-known datasets CUHK03-NP, Market-1501, DukeMTMC-reID, and MSMT17 as multi-camera data for evaluating our proposed method. The CUHK03-NP dataset consists of 14,096 images of 1,467 identities captured by two cameras. Market-1501 was gathered from six cameras and consists of 12,936 images of 751 identities for training and 19,732 images of 750 identities for testing. DukeMTMC-reID contains 16,522 training images of 702 identities and 19,889 images of 702 identities for testing, all of them collected from eight cameras. MSMT17, a large-scale Re-ID dataset, consists of 32,621 training images of 1,041 identities and 93,820 testing images of 3,060 identities captured by fifteen cameras. Additionally, we use MSMT17-merged, which combines training and test parts. We also employ a subset of the synthetic RandPerson dataset, which contains 132,145 training images of 8,000 identities, for additional experiments. It is worth noting that DukeMTMC-reID was withdrawn by its creators due to ethical concerns, but this dataset is still used to evaluate other modern Re-ID methods. Therefore, we include it in our tests for fair and objective comparison.\nSingle-camera dataset. We use the LUPerson dataset as unlabeled single-camera data. This dataset consists of over 4 million images of more than 200,000 people from 46,260 distinct locations. To collect it, YouTube videos were automatically processed. As we can see, this dataset is much larger than multi-camera datasets for person Re-ID and covers a much more diverse range of capturing environments (Tab. 1). Therefore, this kind of data is also useful for training Re-ID algorithms.\nMetrics. In our experiments, we use Cumulative Matching Characteristics (CMC) Rank1, as well as mean Average Precision (mAP) to evaluate our method."}, {"title": "4.2. Implementation Details", "content": "In this paper, we use ResNet50 with IBN-a layers as the encoder and the momentum encoder. These encoders are self-supervised pre-trained on single-camera data from LUPerson using MoCo v2. Adam is used as an optimizer with a learning rate of 0.00035, a weight decay rate of 0.0005, and with a warm-up scheme in the first 10 epochs. As for the momentum coefficient $\\lambda$ in Eq. (2), we set $\\lambda$ = 0.999. ReMix is trained for 100 epochs. In our experiments, we set $N_p$ = $N_s$ = 8 and $N_m^i$ = $N_s^i$ = 4, so the size of each mini-batch is 64. According to , we choose $\\gamma$ = 0.5 in Eq. (1). In ReMix, all images are resized to 256 \u00d7 128, random crops, horizontal flipping, Gaussian blurring, and random grayscale are also applied to them."}, {"title": "4.3. Parameter Analysis", "content": null}, {"title": "4.3.1 Temperature Parameters", "content": "Multi-camera parameters analysis. First, we analyze the quality of our ReMix method for different values of parameters $T_{insm}$ and $T_{aug}$ in the Instance Loss (Sec. 3.3.1) and the Augmentation Loss (Sec. 3.3.2), respectively. Single-camera data is not used in these experiments. As can be seen from Tab. 2a, the best quality of cross-dataset Re-ID can be achieved with $T_{insm}$ = $T_{aug}$ = 0.1. According to , we choose $T_{cenm}$ = 0.5 in Eq. (8).\nSingle-camera parameters analysis. Multi-camera and single-camera data have different complexities in terms of person Re-ID. So, in the Instance Loss (Sec. 3.3.1) and the Centroids Loss (Sec. 3.3.3) we propose to use special temperature parameters for single-camera data ($T_{inss}$ and $T_{cens}$, respectively). According to Tab. 2b and Tab. 2c, the best results achieved when $T_{ins} = 0.2$ and $T_{cens} = 0.6$.\nConclusions from the analysis. The temperature parameters $T_{insm}$ = 0.1 and $T_{cenm}$ = 0.5 are selected for multi-camera data, $T_{ins} = 0.2$ and $T_{cens} = 0.6$ are selected for single-camera data. Higher temperature values make the probabilities closer together, which complicates training on simpler single-camera data. Accordingly, we confirm our hypothesis about the different complexities of multi-camera and single-camera data."}, {"title": "4.3.2 Epoch Duration", "content": "To achieve training stability and frequent updating of centroids, only a portion of the images is used during one epoch (Sec. 3.1). Also, this approach reduces computational costs by generating pseudo labels only for a subset of single-camera data in one epoch, rather than for an entire large dataset (Sec. 3.2). In this paper, one epoch consists of 400 iterations. As can be seen from the experimental results presented in Tab. 3, this number of iterations is a trade-off between the accuracy of our method and its training time."}, {"title": "4.4. Ablation Study", "content": "Proof-of-concept. We conduct a series of experiments to demonstrate the effectiveness of the proposed idea of joint training on multi-camera and single-camera data. The results of these experiments are presented in Tab. 4. As we can see, using single-camera data in addition to multi-camera data significantly improves the generalization ability of the algorithm and the quality of cross-dataset Re-ID. It is worth noting that the use of single-camera data most significantly affects the mAP metric. That is, our method produces higher similarity values for images of the same person and lower values for different ones. This is achieved due to a more diverse training data, which is primarily obtained from large amounts of single-camera data.\nMoreover, the effectiveness of our approach is demonstrated in comparison with self-supervised pre-training: the model trained using the proposed joint training procedure achieves better accuracy than the self-supervised pre-trained model. In Sec. 1 we hypothesized that self-supervised pre-training has a limited effect, since subsequent fine-tuning for the final task is performed on relatively small multi-camera data. The results of our experiments validate this hypothesis. Indeed, by using our joint training procedure together with self-supervised pre-training, we can achieve the best quality. Thus, we experimentally confirm the importance of data volume at the fine-tuning stage. ReMix uses unlabeled single-camera data, and this result can also verify that self-supervised pre-training improves the quality of clustering and pseudo labeling.\nUsing single-camera data in loss functions. In addition to experiments showing the validity of our joint training procedure, we conduct an ablation study to demonstrate the effectiveness of adapting the proposed loss functions for joint use with two types of data. In this study, we gradually add single-camera data in losses and measure the final accuracy. As we can see from Tab. 5, each loss function added to a combination improves the performance, and using all losses with single-camera data jointly provides the highest quality. Thus, the proposed loss functions are successfully adapted for joint use with two types of training data multi-camera and single-camera."}, {"title": "4.5. Comparison with State-of-the-Art Methods", "content": "We compare our ReMix method with other state-of-the-art Re-ID approaches using two test protocols: the cross-dataset and multi-source cross-dataset scenarios. According to the first protocol, we train the algorithm on one multi-camera dataset and test it on another multi-camera dataset. In the multi-source cross-dataset scenario, we train the algorithm on several multi-camera datasets and test it on another multi-camera dataset. Thus, we evaluate the generalization ability of our method in comparison to other existing state-of-the-art Re-ID approaches. Also, we illustrate several complex examples in Fig. 3, where ReMix manages to notice important visual cues.\nThe cross-dataset scenario. As can be seen from Tab. 6, the proposed method demonstrates a high generalization ability and outperforms others in the cross-dataset scenario. In our ReMix method, the momentum encoder is trained to obtain embeddings for each query and gallery image, after which they are compared using cosine similarity. QAConv, TransMatcher, and QAConv-GS , which are among the most accurate methods in cross-dataset person Re-ID, use more complex architectures: in addition to the encoder, a separate neural network is used. This network compares features between the query and gallery images and predicts the probability that they belong to the same person. PAT uses a transformer-based model, which is more computationally complex compared to ResNet50 with IBN-a layers in ReMix. Thus, most existing state-of-the-art approaches improve generalization ability by using complex architectures. In contrast, the high performance of our method is achieved through the training strategy that does not affect the computational complexity, so our method can seamlessly replace other methods used in real-world applications. It is also worth noting that in the comparison in Tab. 6, some methods use larger input images. In the supplementary material, we show that the accuracy of ReMix increases with the size of the input image (see Sec. 6.3).\nThe multi-source cross-dataset scenario. The comparison presented in Tab. 7 shows the effectiveness of our joint training procedure, even when using several multi-camera datasets and one single-camera dataset during training. This further proves the consistency and flexibility of ReMix."}, {"title": "5. Conclusion", "content": "In this paper, we proposed ReMix, a novel person Re-ID method that achieves generalization by jointly using limited labeled multi-camera and large unlabeled single-camera data for training. To the best of our knowledge, this is the first work that explores joint training on a mixture of multi-camera and single-camera data in person Re-ID. To provide effective training, we developed a novel data sampling strategy and new loss functions adapted for joint use with these two types of data. Through experiments, we showed that our method has a high generalization ability and outperforms state-of-the-art methods in the cross-dataset and multi-source cross-dataset scenarios. We believe our work will serve as a basis for future research dedicated to generalized, accurate, and reliable person Re-ID."}]}