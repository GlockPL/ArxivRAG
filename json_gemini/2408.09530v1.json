{"title": "PA-LLaVA: A Large Language-Vision Assistant for Human Pathology Image Understanding", "authors": ["Dawei Dai", "Yuanhui Zhang", "Long Xu", "Qianlan Yang", "Xiaojing Shen", "Shuyin Xia", "Guoyin Wang"], "abstract": "The previous advancements in pathology image understanding primarily involved developing models tailored to specific tasks. Recent studies has demonstrated that the large vision-language model can enhance the performance of various downstream tasks in medical image understanding. In this study, we developed a domain-specific large language-vision assistant (PA-LLaVA) for pathology image understanding. Specifically, (1) we first construct a human pathology image-text dataset by cleaning the public medical image-text data for domain-specific alignment; (2) Using the proposed image-text data, we first train a pathology language-image pretraining (PLIP) model as the specialized visual encoder for pathology image, and then we developed scale-invariant connector to avoid the information loss caused by image scaling; (3) We adopt two-stage learning to train PA-LLaVA, first stage for domain alignment, and second stage for end to end visual question & answering (VQA) task. In experiments, we evaluate our PA-LLaVA on both supervised and zero-shot VQA datasets, our model achieved the best overall performance among multimodal models of similar scale. The ablation experiments also confirmed the effective-ness of our design. We posit that our PA-LLaVA model and the datasets presented in this work can promote research in field of computational pathology.", "sections": [{"title": "I. INTRODUCTION", "content": "The gold standard for diagnosing numerous diseases is tissue examination by a pathologist. This process involves meticulous analysis of tissue samples, typically using a micro-scope, to identify cellular abnormalities. Pathologists rely on extensive training and experience to interpret nuanced patterns and features that indicate various disease states, underscor-ing the critical role of pathology in accurate diagnosis and patient care. The application of artificial intelligence (AI) in pathology research stems from the time-consuming nature and subjective limitations of traditional pathology diagnostic methods. AI-based methods have demonstrated considerable advances in numerous tasks such as metastasis detection [1], cancer subtyping [2], survival prediction [3]\u2013[5], unknown primary origin site prediction [6], [7], image search [8], [9], and prediction of molecular alterations [10], [11]. However, recent advancements in this field primarily involve developing models tailored to specific tasks."}, {"title": "II. RELATED WORK", "content": "The previous advancements in pathology image understand-ing were achieved mainly through the framework of develop-ing models tailored to the specific tasks. Liu et al. [12] pio-neered the fully automated generation of multimodal language-image instruction-following data using large language models, and introduced a large language-vision assistant (LLaVA) trained end-to-end on such instruction-following data, aimed at general purpose of visual and language understanding. LLaVA sets new standards for efficiency and effectiveness in multi-modal learning, and has been rapidly adopted in the medical image field. Unlike traditional methods, this new framework requires instruction-following image-text data. We summarize the research from the perspectives of medical image-text data and LLaVA-based medical model."}, {"title": "A. Medical Image-Text Datasets.", "content": "The difficulty in collecting medical datasets stems primar-ily from critical patient privacy concerns. Researchers have created multimodal medical datasets from public resources by using meticulous extraction methods. For instance, Lin et al. developed a biomedical dataset named PMC-OA [20] from PubMed Central's OpenAccess subset, which includes 1.6 million image-text pairs. Similarly, the PubMedVision [21] dataset was constructed from image-text pairs in PubMed and reformatted using GPT-4V. Ikezogwo et al. transformed un-structured videos from YouTube into vision-based instruction data and included other open-source data, such as Twitter and research papers, totaling one million image-text pairs to construct the Quilt-1M [22] dataset. Furthermore, PMC-15M [23], created by mining PubMed Central articles, utilizes figure caption pairs from these articles to compile the largest biomedical image-text dataset to date. Huang et al. constructed an OpenPath [24] dataset containing 208,414 pathology image-text pairs; however, only publicly provided model-processed feature vectors and not the original images. For medical VQA tasks, PMC-VQA [25] contains 227k question-answer pairs that cover various modalities and diseases based on images. For VQA datasets specifically targeting the field of pathology, PathVQA [26] is a commonly used evaluation dataset, with each image associated with multiple questions categorized as either open-ended or closed-ended based on the nature of the answers."}, {"title": "B. LLaVA for Medical Field.", "content": "Li et al. [17] first introduced the concept of LLaVA in the biomedical field, using GPT-4 to create a biomedi-cal multimodal instruction-following dataset from PMC-15M. Subsequently, they trained a medical multimodal conversa-tional assistant named LLaVA-Med. Lu et al. [18] adjusted the foundational visual encoder for pathology based on the LLaVA architecture to create PathChat, fine-tuning it on a self-created dataset containing 450,000 instruction pairs. On multiple-choice diagnostic questions and open-ended ques-tions related to pathology, PathChat outperformed several multimodal visual-language AI assistants, including GPT-4V."}, {"title": "III. METHODOLOGY", "content": "An overview of our PA-LLaVA model involving a three-stage learning process is as illustrated in Fig 2 (c). Stage 1 trains a pathology language-image pre-training (PLIP) model to perform the role of the visual encoder. Stage 2 achieves the pathology domain alignment, while stage 3 involves VQA instruction fine-tuning. The data used in the three stages can be classified into two categories (some illustrations are presented in Fig 4): (1) Image-Caption Data: consists of pathology images paired with natural language descriptions accurately describing the content of pathology images. (2) VQA Data: This involved questions and answers regarding the content of pathology images. All the training data were obtained from publicly available medical image-text datasets. To obtain the human pathology image-text data, we used publicly available models for data cleaning. To further verify the capability of our PA-LLaVA on the zero-shot task, we transformed several pathology image classification datasets into VQA data to evaluate our PA-LLaVA."}, {"title": "A. Data Construction", "content": "1) Pathology Image-Caption Data: The raw data were ob-tained from three datasets: Quilt-1M, PMC-OA, and PubMed-Vision Alignment, totaling 1,409,058 image-text pairs (named \u201cPCaption-C\u201d). (1) Quilt-1M: This contains one million image-text pairs from YouTube, Twitter, research papers, and the general Internet; it addresses the scarcity of similar data in the medical domain, particularly in pathology. (2) PMC-OA: It includes 1.6 million image-caption pairs collected from the open-access subset of PubMed Central. (3) PubMedVision-Alignment: We selected a sub-dataset of microscopic images; this dataset commonly used in pathology-related research, totaling 132,973 image-description pairs.\nHowever, these datasets contain substantial amounts of data unrelated to human pathology. To obtain the human pathology image-text data, we performed two cleaning processes on the raw data, as illustrated in Fig 2 (a): (1) Removing non-pathological images We used the GLM-4V-9B [30] multi-modal language model, which has strong visual understand-ing capabilities, to process the original image-text pairs. By inputting appropriate prompts into the model to determine whether an image is non-pathological; if the model returns yes, we remove the image-text pair. (2) Removing non-human pathology data: We employed the Qwen2-7B-Instruct [31] model and input appropriate prompts to it to detect text descriptions. This step determines whether text descriptions in-volve non-human organisms. We deleted image-text pairs that included such references. After processing, the Quilt-1M and PMC-OA datasets were reduced to 584,195 and 110,233 pairs, respectively. Combined with the PubMedVision-Alignment subset, this results in approximately 827,401 refined pathology image-description pairs (named \u201cPCaption-0.8M\u201d for stage 1). Additionally, we excluded image-text pairs with textual descriptions of fewer than 20 words. Ultimately, we obtained 518,413 image-text pairs (named \u201cPCaption-0.5M\u201d for stage 2) for the aligned training dataset.\n2) VAQ Data for Stage 3: This type of training data was obtained from PathVQA [26] and PMC-VQA [25]. PathVQA: This is a high-quality dataset specifically designed for visual question-answering (VQA) tasks in pathology; It aggregates 4,998 pathology images, generating 19,755 question-answer pairs. PMC-VQA: This was a multimodal dataset containing 227k VQA questions and encompassing 149k medical images covering various medical modalities or diseases; After the cleaning, we obtained 15,788 question-answer pairs related to human pathology. Lastly, we combined PathVQA and Human pathology data obtained from PMC-VQA, thereby constructing a training dataset of 35543 question-answer pairs as the training data. As illustrated in Fig 3, we count the frequency of noun words in the text, which can reflect the domain distribution of the samples. Additionally, for the test dataset (named \"Test\" and \"Test-Clean\") in PMC-VQA, we also perform the cleaning operations to obtain the human pathology image-text data: \u201cHT\u201d and \u201cHT-clean\u201d."}, {"title": "3) VQA Data for Zero-Shot Test", "content": ": We selected three pathol-ogy image classification datasets (ICIAR 2018 BACH [32], OSCC [33] and ColonPath\u00b9) and transformed them into the closed-set VQA task. ICIAR 2018 BACH is a Breast Cancer Histology (BACH) dataset that classifies 400 H&E-stained breast histology microscope images evenly into four categories based on the type of breast cancer: normal, benign, in situ, and invasive carcinoma. OSCC is the initial component of the histopathological imaging database for oral cancer analysis and includes 528 images categorized into normal oral epithelium and oral squamous cells. ColonPath is a binary classification dataset for lesion cells in colonic tissues. We used its validation set, which contains 5,355 tissue images with 2,727 positive samples and 2,628 negative samples, to evaluate the model. The following questions were used to evaluate the models for the three datasets (How to design the prompt is still an open problem):\n\"What choice best describes this breast tissue? Just give your choice: A:Normal tissue B: Benign tumors C: In situ cancer D: Invasive cancer\"; \"What choice best describes this oral epithelium tissue? Just give your choice: A:Normal oral epithelium B:Oral squamous cell carcinoma\u201d; \u201cWhat choice best describes this colon tissue? Just give your choice: A:Normal tissue B:Tumor tissue\""}, {"title": "B. Architecture of PA-LLaVA", "content": "As illustrated in Fig 2 (b), our PA-LLaVA consists of a vision encoder to extract the features of the pathology images; a connector that maps the tokens of the image to a specific number and dimension; and a LLM to output the answer. For our PA-LLaVA model, we first obtained the initial representation of the input pathology image using a PLIP model. Subsequently, the visual representation is encoded using a learnable connector, combined with tokenized textual queries, and fed into a LLM to generate the expected response.\nIn the original LLaVA model, the visual encoder is typically a pre-trained encoder trained on a generic domain dataset, which requires the input images to be scaled to a fixed size. Introducing LLaVA into computational pathology presents two major challenges: (1) The general pre-trained model may have weak cross-domain generalization when transferred to the medical domain. To address this, we trained a PLIP model using our self-constructed pathology image-text dataset as the image encoder to enhance the feature representation capability of the pathology images. We adopted the same architecture as BLIP [35] for our PLIP model. (2) For pathological images, scaling can lead to changes or loss of detailed features. Con-sequently, we employed a dynamic high-resolution strategy to retain the original image size. Pathological images typically contain comprehensive details and complex structures crucial for accurate diagnosis and analysis."}, {"title": "C. Three-Stages Learning for PA-LLaVA", "content": "Pathological Image-Text Pretraining Model. We intro-duced two widely used loss functions to train the PLIP model:\nImage-Text Contrastive (ITC) loss [36] and Image-Text Match-ing (ITM) loss [36]. The ITC loss aligns the two modalities of pathological images and text in the embedding space. The ITM loss function promotes fine-grained fusion between the two modalities, essentially representing a classification task where the model predicts whether the input image and text are matching pairs. Our PLIP model was trained using the PCaption-0.8M dataset.\nPathological Image-Text Cross-Domain Alignment for LLM. Same with LLaVA, we also employed LM loss [12] to train our PA-LLaVA. This training stage aligns the patholog-ical image and semantics for the LLM. Specifically, the PA-LLaVA learns to generate descriptions of pathological images. We designed a series of questions related to image content and combined them with PCaption-0.5M, resulting in 518,413 question-answer pairs. For each pathological image, questions regarding the image content were randomly sampled from the designed question set. By guiding the model to generate descriptions, we established an initial alignment of the model with pathological images and semantic descriptions, laying the foundation for VQA fine-tuning. During the training, we froze the visual encoder and LLM, only training the connectors and LoRA that added to LLM. In the traditional LLaVA, only connectors are trained at this stage. However, we found that fine-tuning both connector and LoRA of the LLM can obtain the lower alignment errors.\nVQA Instruction Learning. We used multimodal visual question-answer (VQA) instruction-following data, including PathVQA and a sub-dataset of PMC-VQA to fine-tune the LLM and connector parts. The VQA dataset, containing 35,543 pairs, introduced greater diversity in questions and answers. This enhanced the ability of the PA-LLaVA model to respond accurately to various types of pathology-related instructions. Ultimately, our model could answer specific pathological questions and performed excellently on some public pathological test datasets."}, {"title": "IV. EXPERIMENTS", "content": "For the supervised task, we reported the accuracy (Acc.) /percentage of ground-truth tokens appearing in the generated sequences (closed-set questions); For open-ended questions, recall (Rec.) was used to measure the ratio of the ground-truth tokens that appeared in the generated sequences. For zero-shot task (classification task), we employed the metrics such as Acc., Rec., and Precision (Pre.) to further evaluate the model's ability to distinguish each category."}, {"title": "B. Implementation details", "content": "PLIP: We implemented training using the PyTorch frame-work and trained the model for 30 epochs on 4\u00d7NVIDIA A100 GPUs. We used a batch size of 4 x 48. We employed the AdamW optimizer with a weight decay of 0.01. The learning rate was initialized to le-5, warmed up to 1e-4 in 1,000 steps, and decayed at fixed intervals to 5e-5 over 30 epochs. Furthermore, we used randomly cropped images with a resolution of 224x224 as input. The text was truncated to a length of 100.\nPA-LLaVA: We implemented the PA-LLaVA model using the Xtuner toolkit and trained it on 16 \u00d7 NVIDIA A100 GPUs. Our training process is divided into two stages: alignment phase and instruction fine-tuning phase. (1) For stage 1, we set the gradient accumulation steps to 6, and the batch size was set to 16 \u00d7 6 \u00d7 6; The learning rate was linearly increased from the initial value to 1e-4 and then gradually decayed to 0 using the cosine annealing strategy. This phase of training lasted for 9 epochs. (2) For stage 2, the batch size was 16 \u00d7 6 \u00d7 4; The learning rate of the connector module was linearly increased from le-5 to le-4 and then cosine decayed to 1e-6; Meanwhile, the learning rate of the LLM's LoRA gradually increased to 2e-4 and finally also cosine decayed to le-6; This training was conducted for 12 epochs. We used the AdamW optimizer and used mixed precision training to improve computational efficiency and save memory."}, {"title": "C. Comparisons with SOTA", "content": "We compared our PA-LLaVA with popular methods, includ-ing the general domain LLaVA [12], and large language visual models of the medical domain (LLaVA-Med [17] and Quilt-LLaVA [19]), and the previous SOTA methods on pathology VQA task, with results shown in Table I. We can make the main observations as follows:\nFirst, our PA-LLaVA significantly outperformed the general domains of LLaVA. The main reasons can be noted as follows: (1) the LLM LLama3 performed better than that of Vicuna employed by LLaVA; (2) the initialization of the vision encoder from our PLIP model, pre-trained on pathological image-text data, surpasses that of general-domain CLIP [37]; (3) our PA-LLaVA has undergone cross-domain alignment of pathological images, text, and fine-tuning of visual question answering data.\nSecond, our PA-LLaVA performed better than the su-pervised SOTA on both closed-set and open-set VQA on PathVQA and PMC-VQA datasets. The main reasons are as follows: (1) Unlike other SOTA methods, where visual encoders require resizing medical images to a fixed size, the designs of our visual encoder and connector preserved the original size of pathological images, avoiding information loss caused by image scaling; and (2) the features extracted from the PLIP are more efficient than those of the general encoder used in previous SOTA models.\nThird, we further evaluated our PA-LLaVA on the zero-shot task. The comparisons are presented in Table II. We can make the main observations that our PA-LLaVA significantly outperformed both the general and medical domain models. The main because can be noted that our visual encoder and connector provide a more efficient feature representation for the pathology image than the original structure of LLaVA used in other LLaVA-based medical models. Additionally, compared to these models, the data used in our PA-LLaVA only focuses on the field of pathology. As shown in Fig 3, owing to the unbalanced distribution of pathological images used for the VQA task, our PA-LLaVA can exhibit different performance on different organs of pathological image."}, {"title": "D. Ablation Studies", "content": "To verify the effectiveness of each module\u2014including our instruction data, domain-specific visual encoder, and connec-tor. We reported the performance of the different PA-LLaVA variants in Table III. PA-LLaVA represents the proposed model; PA-LLaVA*, PA-LLaVA\u2020and PA-LLaVA\u2021represent variants using different modules.\nThe main observations are as follows: (1) Our PA-LLaVA trained on PCaption-0.5M significantly outperformed the PA-LLaVA that trained on PCaption-C data. This indicates that our data-cleaning process was effective for downstream tasks. (2) When our specialized visual encoder PLIP in PA-LLaVA was replaced with the general visual encoder CLIP, while keeping all other conditions unchanged (PA-LLaVA vs PA-LLaVA\u2021), the performance degraded, especially on zero-shot tasks. This indicates that our specialized PLIP model can pro-vides more effective feature representations for the pathology images. (3) Building on the previous step, when we replaced the connector part with an MLP structure (PA-LLaVA* vs PA-LLaVA\u2021), which reverted to the original architecture of LLaVA, the model performance was further degraded, which confirmed the effectiveness of our proposed connector."}, {"title": "E. Alignment Evaluation for LLM", "content": "This stage aligns the embeddings of pathology image with their corresponding embeddings of text. After this training stage, our PA-LLaVA model generated descriptions of pathol-ogy images based on specific prompts. To verify performance, we compared our PA-LLaVA with Quilt-LLaVA and LLaVA-Med. Specifically, we leverage GPT-4 to evaluate the semantic similarity of the responses and captions, and give an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. As listed in Table IV, the quality of description generated by our proposed PA-LLaVA is closer to LLaVA-Med, significantly better than Quilt-LLaVA. Typically, the better the model aligns during this stage, the better its understanding of pathology images, and the more likely it is to perform downstream tasks (see PA-LLaVA vs PA-LLaVA\u2020in Table III). However, this stage typically requires a large-scale dataset of images and content descriptions, which is mainly obtained on the Internet, often resulting in lower-quality text annotations. Thereby, constructing high-quality pathology image-text data are meaningful to our PA-LLaVA."}, {"title": "V. DISCUSIONS", "content": "Limitations. The quality of our pathology image-text pairs used for domain alignment still requires improvement. This is because domain alignment requires a sufficiently large sample size. Since this type of training data is mainly sourced from the internet, ensuring the quality of the images and texts is challenging. In this study, we performed the data cleaning on the public data, and constructed a dataset of human pathology image-text pairs. However, we cannot ensure their quality or correctness due to the highly specialized nature of interpreting pathology images. Therefore, improving the high-quality descriptions of the pathology images is beneficial for downstream tasks. Additionally, more balanced distribution of pathological VQA data can further improve the comprehensive performance of our PA-LLaVA.\nConclusions. (1) We constructed human pathology image-text instruction-following data by cleaning only the public data for the computational pathology field. However, we posit that the quality of these image texts can be further improved. (2) We developed an improved LLaVA-based model for understanding pathology images, where a PLIP model was used as a vision encoder and a scale-invariant connector was designed to avoid information loss caused by image scaling. To test its performance, we evaluated our PA-LLaVA on both supervised and zero-shot VQA datasets, where it achieved the best overall performance. We posit that the PA-LLaVA model and multimodal datasets presented in this study can promote research in the field of computational pathology."}]}