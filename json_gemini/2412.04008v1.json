{"title": "Deep-Unrolling Multidimensional Harmonic Retrieval Algorithms on Neuromorphic Hardware", "authors": ["Vlad C. Andrei", "Alexandru P. Dr\u0103gu\u021boiu", "Gabriel B\u00e9na", "Mahmoud Akl", "Yin Li", "Matthias Lohrmann", "Ullrich J. M\u00f6nich", "Holger Boche"], "abstract": "This paper explores the potential of conversion-based neuromorphic algorithms for highly accurate and energy-efficient single-snapshot multidimensional harmonic retrieval (MHR). By casting the MHR problem as a sparse recovery problem, we devise the currently proposed, deep-unrolling-based Structured Learned Iterative Shrinkage and Thresholding (S-LISTA) algorithm to solve it efficiently using complex-valued convolutional neural networks with complex-valued activations, which are trained using a supervised regression objective. Afterward, a novel method for converting the complex-valued convolutional layers and activations into spiking neural networks (SNNs) is developed. At the heart of this method lies the recently proposed Few Spikes (FS) conversion, which is extended by modifying the neuron model's parameters and internal dynamics to account for the inherent coupling between real and imaginary parts in complex-valued computations. Finally, the converted SNNs are mapped onto the SpiNNaker2 neuromorphic board, and a comparison in terms of estimation accuracy and power efficiency between the original CNNs deployed on an NVIDIA Jetson Xavier and the SNNs is being conducted. The measurement results show that the converted SNNs achieve almost five-fold power efficiency at moderate performance loss compared to the original CNNs.", "sections": [{"title": "I. INTRODUCTION", "content": "Multidimensional Harmonic Retrieval (MHR) is a ubiquitous problem in signal processing, finding applications in radar, and wireless joint communications and sensing [1], among others. Due to the growing bandwidths and high mobility scenarios, developing MHR methods that require one or very few samples is desirable. Compressed sensing (CS) has been an attractive solution due to its good reconstruction qualities and performance guarantees [2]. On the other hand, such approaches incur high computational complexity with increased resolution"}, {"title": "II. PRELIMINARIES", "content": "In the following, we derive a general mathematical formulation of the single-measurement MHR problem as in [15] and explain how the framework of sparse recovery can be used to solve it. Let $Ep = [\\xi_{1p}, \\xi_{2p},..., \\xi_{Mp}] \\in [-1,1]^M$ be the parameters associated to a source $p = 1,..., P$. Let $N_1, N_2,... N_M$ represent the number of sensors used to capture each dimension and $N = \\prod_{m=1}^M N_m$. With $a_m(\\xi_{mp}) \\in \\mathbb{C}^{N_m}$, where $(a_m (\\xi_{mp}))_k = e^{-j2\\pi\\xi_{mp}k}$ for $k = 0, ... N_m -1$, and the matrices $A_m(E_p) = [a_m(\\xi_{m1}),..., a_m(\\xi_{mp})] \\in \\mathbb{C}^{N_m \\times P}$, the measurement model is given by [15]\n\n$y = (A_1(E_1) * \\dots * A_M(E_M)) b + z = A(E)b + z.$\n\nHere $*$ denotes the Khatri-Rao (KR) product, $b = [b_1,...,b_p] \\in \\mathbb{C}^P$ are the complex amplitudes and $z \\sim \\mathcal{N}(0, \\sigma^2 \\mathbb{I}) \\in \\mathbb{C}^N$. The goal of SS-MHR is to estimate the harmonics $\\{\\xi_p\\}_{p=1}^P$ given the measurement $y$. Since the number of sources is in general unknown, in the sparse recovery paradigm, $L > N$ harmonics $\\Xi_l$ are chosen, then an oversampled dictionary $A(\\Xi) \\in \\mathbb{C}^{N \\times L}$ is constructed by replacing the KR with the Kronecker-product in Eq. (1). Subsequently, the parameters are estimated from the nonzero indices of the sparse solution found by solving\n\n$\\min_{b \\in \\mathbb{C}^L} ||y - A(\\Xi)b||_2^2 - \\lambda ||b||_1,$\n\nwhere $\\lambda > 0$. This procedure, also known as $l_1$-relaxation, theoretically yields near-optimal estimates [16].\n\nA popular approach to solving the problem in Eq. (2) is the Fast Iterative and Shrinkage Algorithm (FISTA) [16], which at each iteration $t = 1,...,T$ constructs a solution as\n\n$b_t = S_{\\lambda/\\eta} (W_1y + W_2b_{t-1})$\n$W_1 = \\eta^{-1}A^H(E), W_2 = I - \\eta^{-1}A^H(E)A(E)$\n\nwhere $\\eta$ is the largest eigenvalue of $A^H(E)A(E)$ and\n\n$S_a(x) = sign(u) max(|u \u2013 a|, 0)$\n\nis the elementwise soft-thresholding operator with threshold $a > 0$. This approach can be seen as a modification of gradient descent, where the solution after each gradient step is projected onto a union of subspaces of restricted support, i.e., with a finite number of non-zero elements. Even if this approach is theoretically optimal and drastically improves the convergence of traditional ISTA [16], it still needs a very high number of iterations to reach a sparse solution $b$ [14]."}, {"title": "B. Structured LISTA", "content": "In order to address the issue of a large number of iterations in FISTA, the Structured Learned Iterative Shrinkage and Thresholding Algorithm (S-LISTA) was proposed in [14]. The algorithm, depicted in Figure 1 unfolds FISTA recursion in Eqs. (3) to (4) and only considers the first $T$ iterations of the algorithm. It furthermore replaces the matrices $W_1, W_2$ with trainable CNNs and considers the threshold $a_t = \\lambda/\\eta$ also as a trainable parameter. More formally,\n\n$b_t = S_{a_t} (Conv_{1x1}^{W_1} (y) + Conv^{W_2} (b_{t-1}))$\n\nwhere $Conv_{1x1}^{W_1}$ is 1x1-convolution [17] (equivalent to matrix-vector multiplication) and $Conv^{W_2}$ is the convolution operation. The proposed architecture is motivated by the fact that the matrix $W_2$ is block-Toeplitz, which allows the matrix-vector multiplication in Eq. (4) to be implemented efficiently with M-D convolutions with separable kernels, which is a fast operation on modern GPUs. We refer the reader to [2], [14] for detailed derivations. At last, the well-known equivalence between matrix-vector multiplications and 1x1-convolutions [17] not only theoretically underpins the layer computations in Eq. (6), but also allows for fast and efficient training on GPUs.\n\nIn order to train the network, we first generate a labeled training dataset $D = \\{(y_d, b_d)\\}_{d=1}^D$ for multiple numbers of sources $P$ and noise variances $\\sigma^2$. We do this by using a normalized version of Eq. (1) to obtain a sample-wise signal-to-noise ratio (SNR) equal to $1/\\sigma^2$. We train the network by back-propagation using the ADAM optimizer [18] with the following modified objective:\n\n$\\mathcal{L}(b_d, b_d) = \\sum_{d=1}^D \\frac{||b_d - b_d||_1}{||b_d||_1},$\n\nwhere $b_d$ denotes the network output. In [14], the $l_2$-norm was used, but we empirically observed that the $l_1$-norm helps us achieve faster training convergence and better enforces sparse structure."}, {"title": "C. Spiking Neural Networks and SpiNNaker2", "content": "In order to explain the conceptual difference between artificial neural networks (ANNs) and SNNs, we recall the fact that ANNs process a real(or complex)-valued input vector $x \\in \\mathbb{R}^D$ in using the iterated composition of element-wise nonlinear functions, called activations, with affine-linear maps [6].\n\nIn contrast, SNNs do not operate on real-valued inputs but rather on a time series of spikes, which are then processed by following the dynamics of a neuron model instead of being passed to an activation function. More formally, for the well-known leaky-integrate-and-fire (LIF) neuron, the internal dynamics for $K$ timesteps are given by [5]:\n\n$v_n(t) = \\beta v_n(t \u2212 1) + W_n z_{n-1}(t) + z_n(t \u2212 1)V_{th}$\n\n$z_n(t) = \\Theta(v_n(t) - V_{th}), t = 0, . . ., K \u2212 1$\n\nwhere $z_n(t) \\in \\{0,1\\}^{D_n}, v_n(t) \\in \\mathbb{R}^{D_n}, W_n \\in \\mathbb{R}^{D_n \\times D_{n-1}}$ denote the spikes, membrane potentials and weights at time $t$ for the $n$-th layer, with the element-wise Heaviside function, i.e. $\\Theta(x) = 1, x > 0$ and 0 otherwise. Here $\\beta > 0, V_{th} \\geq 0$ is the leakage factor and the threshold potential, which determines whether the neuron fires or not.\n\nIn order to process real- or complex-valued data with SNNs, one first needs to encode the input $x$ into an initial spike train $z_0(t)$, with rate- and latency-encoding being the most popular methods [5]. Furthermore, note that the firing rate of the LIF neuron with rate-coded spikes is described by the rectified linear unit (ReLU) [9]. Thus, in terms of converting an ANN to an SNN, the representational power of the LIF neuron is restricted to real-valued SNNs employing ReLU activation functions [9], [10].\n\nSpiNNaker2 is a massively parallel, highly scalable and energy-efficient neuromorphic system that takes a hybrid approach to neuromorphic computing by allowing both spiking and classical rate-based neural networks to be executed onto the board [4]. One chip contains 152 ARM Cortex-M4 cores, also called Processing Elements (PEs), which can host at least 250 neurons with a total number of 1000 synapses per neuron. Each PE runs a small asynchronous program and communicates with other PEs via Network-on-Chip (NoC) components, which also facilitates process scheduling and dynamic voltage and frequency scaling (DVFS) in order to allocate resources only if an operation occurs."}, {"title": "III. PROPOSED METHOD", "content": ""}, {"title": "A. Complex-Valued FS Coding", "content": "In order to leverage the enhanced parallelism and energy efficiency of neuromorphic platforms, we convert the trained models to SNNs. For this, the complex values and activations need to be converted to a spiking domain following a spike coding scheme. We propose an extension of the FS coding scheme [13], which utilizes two distinct FS neurons, one for encoding the real part and another for the imaginary part. Each of these neurons transmits serialized binary codes over a $K$ timesteps, which aggregated approximate an arbitrary activation $f(s) : B \\rightarrow \\mathbb{C}$ over a bounded domain $B \\subset \\mathbb{C}$. The internal dynamics of a single neuron are governed by:\n\n$v_r(t + 1) = v_r(t) \u2013 h_r(t)z_r(t),$\n\n$v_i(t + 1) = v_i(t) \u2013 h_i(t)z_i(t),$\n\n$z_r(t) = \\Theta(v_r(t) \u2013 T_r(t)),$\n\n$z_i(t) = \\Theta(v_i(t) \u2013 T_i(t)).$\n\nIn this formulation, $d_r, h_r, T_r, d_i, h_i, T_i \\in \\mathbb{R}^K$ represent parameters associated with the real and imaginary parts of the FS neurons. Furthermore, $v_{r,i}(t)$ and $z_{r,i}(t)$ represent the membrane potentials and the generated spikes of two FS neurons. Note that the original formulation for real-valued processing in [13] is recovered by only considering Eqs. (10) and (12). The output of the neuron, which approximates the original activation, is then computed as:\n\n$f(s) = \\sum_{t=0}^{K-1} (d_r(t)z_r(t)+j \\cdot d_i(t)z_i(t)).$\n\nIn order to get a good approximation over $B, B$ points are sampled, and the following optimization problem is solved by gradient descent and a triangular approximation for the subgradient of $\\Theta(\\cdot)$ as done in [13]\n\n$\\min_{d_r, h_r, T_r, d_i,h_i, T_i \\in \\mathbb{R}^K} \\sum_{i=1}^B ||f(s_i) \u2013 f(s_i)||^2.$\n\nThe spike processing mechanism of FS neurons is characterized by two cyclic stages. Specifically, these neurons undergo a stage of spike accumulation spanning $K$ timesteps followed by an additional stage of spike transmission spanning another $K$ timesteps. This behavior effectively enables each neuron to process new input at regular intervals of $2K$ timesteps, as the neurons remain idle after each receiving-transmitting cycle. Consequently, this operational paradigm improves throughput capacity within the neural network architecture."}, {"title": "B. ANN-to-SNN Conversion Pipeline", "content": "In the following, we describe how the S-LISTA network presented in Section II-B is converted to an equivalent SNN based on complex FS coding. In principle, each layer implements the same functional form as (6) but with the activations replaced by their FS approximations. Furthermore, the spike encoding is done by passing the input data to the FS approximation of the identity. More formally,\n\n$b_t = FS_{S_{a_t}} (Conv_{1x1}^{W_1} (FS_{id} (y)) + Conv^{W_2} (b_{t-1})),$\n\nwhere $FS_{S_{a_t}}$ and $FS_{id}$ are obtained by solving (15) for the function in Eq. (5) and $f(s) = s$, respectively.\n\nIn order to understand the steps of the full ANN-to-SNN conversion in Algorithm 1, it is important to understand how FS neurons are implemented on SpiNNaker2. On the board, FS neurons within a population, i.e. internal data structure representing the spiking neurons for $y$ and $b_t$, generate and process spikes in parallel. Once the simulation begins, the populations enter a synchronized receive-send cycle. This ensures that spikes do not travel across the network continuously, as the receiving population is inactive while the transmitting population is. To address this, spike transmission in the previous layer is delayed by a stage duration $K$ to synchronize spike arrival with the receiving stage of the next layer. This approach enables pipelining, allowing neurons to process new input after"}, {"title": "IV. RESULTS", "content": "We first evaluate the proposed neuron model in terms of function approximation capabilities. Figure 2 shows the real and imaginary parts of the true and approximated function, as well as the error for different timesteps $K$. For all trials, a learning rate of $\\eta = 0,009$ and $I = 10000$ iterations were employed for the gradient descent solver. We observe that the proposed method qualitatively and quantitatively approximates the functions quite well. The real and imaginary parts look very similar, and the value of the optimization criterion in Eq. 15 stays below $8 \\cdot 10^{-3}$. Furthermore, the approximation error tends to decrease with the number of timesteps $K$. We thus observe that the required $K$ to get a good approximation is below 30, which makes the method promising regarding delay and energy efficiency, since it is much less than the hundreds of timesteps LIF neurons usually employ [9].\n\nWe now present the task performance results for the converted SNNs on the SpiNNaker2 neuromorphic board. Due to the very strict memory limitations of the 1-chip evaluation board, we were only able to map S-LISTA models for $M = 1$-D HR problems with a maximum of $T = 5$ layers and an observation size of $N = 64$ and dictionary size $L = 128$. We have trained the S-LISTA networks for 30 epochs using a learning rate of $5 \\cdot 10^{-4}$. For the ANN-to-SNN conversion pipeline, we have used a learning rate of $10^{-2}$ for the optimization in (15) and terminated the SGD solver once an error below $5 \\cdot 10^{-4}$ was reached. Figure 3 shows the support recovery error of the FISTA algorithm, the S-LISTA networks, and the converted SNN for $P = 1$ to 5 sources, which is given by\n\n$SRE = \\frac{1}{D} \\sum_{d=1}^D \\frac{|\\{l \\neq l' \\mid l \\in supp(b_d), l' \\in supp(b_d)\\}|}{|supp(b)|},$\n\nwhere $supp(b) = \\{l \\in \\mathbb{N} | |b_l| \\geq 0\\}$. We observe that the converted SNN outperforms FISTA and performs similarly to S-LISTA for a few sources. The SNN performance worsens with an increasing number of sources but still shows comparable results to the original FISTA formulation at a fraction of the resources, i.e., 5 layers with less complexity compared against the 100 iterations needed for FISTA. This is mainly explained by quantization effects and the residual errors in the FS neuron approximations. We thus observe that quantization is the bottleneck to good performance and that the employed post-training quantization is not optimal. Furthermore, the FS approximation artifacts can be partly mitigated by employing a bigger $\\beta$ and more finely sampling $B$. We now turn our attention to the power efficiency measurements on the NVIDIA Jetson Xavier NX [19], and the experimental SpiNNaker2 [4]"}, {"title": "V. CONCLUSIONS", "content": "We casted the MHR problem as a sparse recovery problem, and solved it using the recently proposed S-LISTA algorithm. We developed a novel method for converting the complex-valued convolutional layers and activations in S-LISTA into SNNs based on the recent FS conversion. At last, the converted SNNs were mapped onto the SpiNNaker2 neuromorphic board, and a comparison in terms of support recovery error and power efficiency between the original CNNs, deployed on embedded GPU, and the SNNs was conducted. The measurement results show that the converted SNNs can achieve much better power efficiency at moderate performance loss compared to the original CNNs."}]}