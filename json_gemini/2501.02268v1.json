{"title": "What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph", "authors": ["Yutao Jiang", "Qiong Wu", "Wenhao Lin", "Wei Yu", "Yiyi Zhou"], "abstract": "Recent Multimodal Large Language Models (MLLMs) often use a large number of visual tokens to compensate their visual shortcoming, leading to excessive computation and obvious visual redundancy. In this paper, we investigate what kind of visual tokens are needed for MLLMs, and reveal that both foreground and background tokens are critical for MLLMs given the varying difficulties of examples. Based on this observation, we propose a graph-based method towards training-free visual token pruning, termed G-Prune. In particular, G-Prune regards visual tokens as nodes, and construct their connections based on their semantic similarities. Afterwards, the information flow is propagated via weighted links, and the most important tokens after iterations are kept for MLLMs, which can be front or background. To validate G-Prune, we apply it to a recent MLLM called LLaVA-NeXT, and conduct extensive experiments on a set of benchmarks. The experiment results show that G-Prune can greatly reduce computation overhead while retaining high performance on both coarse- and fine-grained tasks. For instance, G-Prune can reduce 63.57% FLOPs of LLaVA-NeXT on VQA2.0 and TextVQA with only 0.95% and 2.34% accuracy drops, respectively. Our code is available at https://github.com/jytmelon/G-Prune.", "sections": [{"title": "Introduction", "content": "Recently, extending large language models (LLMs) to more modalities has become a research hotspot (Achiam et al. 2023; Bai et al. 2023a; Touvron et al. 2023b; Chen et al. 2024b; Guo et al. 2024), i.e., multimodal LLM (MLLMs). For vision-language (VL) tasks, the most common paradigm is to directly project the extracted visual features into the semantic space of LLMs (Li et al. 2023a; Team et al. 2023; Achiam et al. 2023; Liu et al. 2024b,a; Luo et al. 2024) as the input tokens. Despite effective, these MLLMs (Team et al. 2023; Achiam et al. 2023; Liu et al. 2024b,a) often suffer from more severe visual hallucinations, and perform worse on granular VL tasks like TextVQA (Singh et al. 2019).\nA straightforward solution to remedy the visual short-coming of MLLMs is to increase image resolution, i.e., using more visual tokens, thereby making vision matters\nin multimodal reasoning. For instance, LLaVA-NeXT (Liu et al. 2024a) partitions images into multiple regions and directly concatenates the visual tokens mapped from each region. InternVL (Chen et al. 2024b) presets a variety of high-resolution image inputs with different aspect ratios. Although simple, this solution does improve the visual reasoning of MLLMs to a large extent, achieving new SOTA performances on a set of benchmarks. However, a counterpart is the prohibitively expensive computation. For instance, LLaVA-NeXT uses 2880 visual tokens, and require about 18.52 TFLOPs computational for each inference.\nIn this case, a question rises, do we really need so many visual tokens for MLLMs? The large image tokens can provide more detailed visual semantics for multi-modal reasoning. However, these visual tokens are often processed by the well pre-trained image encoders, e.g., ViT (Dosovitskiy et al. 2020), and have a large receptive field, i.e., representing much more information than its actual area. In this case, heavy redundancy does exist in these image tokens (Pan et al. 2021). Thus, randomly dropping a certain number of visual tokens has almost no impact on the performance on MM-bench (Goyal et al. 2017), a widely used MLLM benchmark for common scenarios, also shown in Fig.1-a. However, on the granular VL task, e.g. TextVQA (Singh et al. 2019), randomly removing a larger number of visual tokens will lead to a drastic decline in performance, indicating the great loss of key information, as shown in Fig.1-a. Conclusively, the large number of visual tokens are apparently redundant, but what visual tokens to preserve remains an challenge given the varying difficulties of examples.\nPrevious research (Rao et al. 2021; Liang et al. 2022; Xu et al. 2022; Yang et al. 2022) often focuses on the foreground information of the given images, i.e., the main objects, but we note that both the foreground and background tokens are important for MLLMs. To explain, for a conventional image recognition task, the foreground object mainly responds to the label of this image, and only a small number of foreground tokens can be capable of recognition. In contrast, for MLLM tasks, some details are often questioned. Meanwhile, we computed the 12-Norm frequency distribution histograms for both the foreground and background, and found that their distributions have significant overlap, as shown in Fig.1-b. Nevertheless, how to recognize the important tokens for different images is still challenging in practice.\nIn this paper, we propose a graph-based method towards the training-free visual token pruning of MLLMs, termed G-Prune. In particular, we consider the visual tokens as graph nodes, and construct their connections according to their feature distances. Afterwards, information propagation is executed among nodes with an iterative algorithm to update the importance scores. Lastly, the most important tokens can be selected for MLLMs, which could be either foreground or background ones. In this way, we can select the most representative visual tokens for MLLMs, thereby greatly reducing the sequence length and computation complexity.\nTo validate G-Prune, we apply it to LLaVA-NeXT(Liu et al. 2024a), and conduct extensive experiments on eight competitive MLLM benchmarks, including VQA2.0 (Goyal et al. 2017), GQA (Hudson and Manning 2019), ChartQA (Masry et al. 2022), TextVQA (Singh et al. 2019), DocVQA (Mathew et al. 2020), POPE (Li et al. 2023b), MME (Fu et al. 2023) and MMB (Liu et al. 2023). The experimental results show that G-Prune significantly reduces computational costs while retaining high performance across various bench-marks. For instance, G-Prune can reduce the computation of LLaVA-NeXT on MME by 63.50% without performance drops. Moreover, even on granular tasks like TextVQA, G-Prune can also achieve a higher accuracy than the compared pruning methods while dropping a larger number of tokens, e.g., 59.31 v.s. 39.02 (ToMe) on TextVQA while dropping 90% tokens.\nConclusively, the contribution of this paper is three-fold:\n\u2022 We investigate the importance of different types of visual tokens for MLLMs, and show that both foreground and background ones are critical.\n\u2022 We propose a graph-based method towards the training-free visual token pruning of MLLMs, which can mine the significant tokens via iterative information propagation.\n\u2022 G-Prune can reduce the visual tokens of MLLMs to a large extent while retaining high performance on different VL benchmarks, even on the text-oriented benchmarks, such as TextVQA and DocVQA."}, {"title": "Related Work", "content": "Multimodal Large Language Models. Driven by the re-markable success of large language models (LLMs)(Touvron et al. 2023a,b; Meta 2024; Team et al. 2023; Reid et al. 2024; Achiam et al. 2023; Brown et al. 2020) in managing text-only tasks with demonstrated exceptional capabilities, the field of multimodal large language models (MLLMs)(Team et al. 2023; Achiam et al. 2023; Liu et al. 2024b,a; Luo et al. 2024; Zhang et al. 2024) is also attracting increasing scholarly interest. The core principle underlying the extension of LLMs to MLLMs entails transforming visual information into a sequence of tokens. These tokens are then amalgamated with textual tokens to create a unified input for processing by LLMs. Some approaches employ a series of learnable tokens to dynamically aggregate information from image sequences, which are then amalgamated with text tokens for processing within LLMs. For instance, BLIP-2 (Li et al. 2023a) introduces the QFormer module to dynamically schedule information aggregation from the outputs of the visual backbone. Similarly, Qwen-VL (Bai et al. 2023b) employs cross-attention mechanism to optimize the processing of information from ViT outputs. Although these methods have demonstrated success and incur only limited additional computational overhead, the inadvertent loss of visual information during the process limits the upper limit of such methods (Yao et al. 2024). To achieve this, other methods map visual information into the feature space of LLMs using a simple Multi-Layer Perceptron (MLP) layer. For example, MINI-GPT4(Zhu et al. 2023) applies a projection layer to map visual features into the semantic space of the LLM. Similarly, LLaVA (Liu et al. 2024b) follows this paradigm but further enhances it with a meticulously devised training strategy. For higher resolution, LLaVA-NeXT (Liu et al. 2024a) partitions images into multiple regions, and ViT is used to obtain visual features from upsampled images. InternVL (Chen et al. 2024b) presets a variety of high-resolution image inputs with different aspect ratios, and ViT is used to extract features from images of a specific size. These methods effectively preserve visual information by fully retaining the visual tokens. While a large amount of redundant information is fed into LLMs. To this end, it is necessary to filter out redundant tokens efficiently and effectively for the applications of MLLMs.\nToken Pruning. Token pruning is a widely applied method for Transformer-based networks (Vaswani 2017; Devlin et al. 2018; Dosovitskiy et al. 2020), which reduces the less important tokens to speed up inference. In the aspect of MLLM, existing methods mainly use the calculation process of MLLM to locate redundant visual content. For instance, FastV (Chen et al. 2024a) uses the first two layers as the key to multi-modal information exchange and directly removes most of the visual tokens with lower attention weights after the sec-"}, {"title": "Method", "content": "In this paper, we propose a graph-based method towards the training-free visual token pruning of MLLMs, termed G-Prune. The principle of G-Prune is to select the most representative tokens from the perspective of graph, which could be either foreground or background ones.\nIn practice, G-Prune considers the visual tokens as graph nodes and builds their connections based on the feature similarity. Afterwards, an iterative algorithm is executed to propagate information among nodes, based on which the most important ones emerge.\nConcretely, given a set of visual tokens \\(X \\in \\mathbb{R}^{N\\times d}\\), where N and d represent the number and dimension of visual tokens, we first construct the graph according to node-wise feature distances:\n\\[ A_{ij} = \\begin{cases} cos(X_i, X_j), & cos(X_i, X_j) \\geq s, \\\\ 0, & otherwise, \\end{cases} \\qquad(1) \\]\nwhere \\(cos(\\cdot,\\cdot)\\) represents the cosine similarity between two tokens. s is a threshold for the connection. Then we apply the softmax function to normalize each row of A. In this way, the tokens with similar semantics are connected, e.g., the background ones or the ones of the same object.\nAfterwards, we iterate information propagation to find out the most representative token for each connected component. We first initialize the amount of information of visual tokens according to their 12-Norm:\n\\[ S^{(0)} = \\sum_{j=1}^{d} X_{ij}^{2}, \\qquad(2) \\]\nwhere \\(X_{ij}\\) denotes the j-th component of the i-th visual token. And \\(S \\in \\mathbb{R}^{1\\times N}\\) denotes the initial score for each token. Then, the score of each token at the t-th step is obtained by\n\\[ S^{(t)} = S^{(0)} (A^t). \\qquad(3) \\]\nEach object has a token that best represents it, and we gather scores for all tokens belonging to that object. However, since objects contain different numbers of tokens, their representative tokens can vary significantly. This variation makes it challenging to directly identify the most representative token. To mitigate the influence of connected components size, we apply the degree of each token to normalize the scores. The degree \\(D \\in \\mathbb{R}^N\\) of tokens can be calculated by\n\\[ D_i = \\sum_{j=1}^{N} 1(A_{ij} > 0), \\qquad(4) \\]\nwhere \\(1(\\cdot)\\) indicates an indicator function, which returns 1 when the condition is met. Subsequently, the normalized score of each token can be defined as\n\\[ S'^{(t)} = S^{(t)}/D_i. \\qquad(5) \\]\nHere, \\(S'^{(t)}\\) is the normalized representativeness of each token in its object. Thanks to the normalization operation, we can use a unified standard to directly obtain the most representative token of each object. Finally, the index of the retained tokens can be selected by\n\\[ I_k = \\arg \\max_{I \\subset \\{1,2,...,n\\}\\,|I|=k} \\sum_{i \\in I} S'^{(t)}. \\qquad(6) \\]\nWe find the top k tokens by token score \\(S'^{(t)}\\). In this way, we can retain the objects from both the foreground and the background, while only the most representative token is retained to represent each object. The detailed algorithm of G-Prune is described in Alg. 1."}, {"title": "Experiments", "content": "Datasets and Metrics\nWe first evaluate G-Prune for two common general benchmarks, i.e., VQA2.0 (Goyal et al. 2017), GQA (Hudson and Manning 2019). Furthermore, for fine-grained tasks, we evaluate OncePrue for text-oriented VQA benchmarks, including TextVQA (Singh et al. 2019), DocVQA (Mathew et al. 2020) and ChartQA (Masry et al. 2022). We also evaluate G-Prune on three emerging multimodal benchmarks for MLLMs, including POPE (Li et al. 2023b), MME (Fu et al. 2023) and MMB (Liu et al. 2023). These benchmarks tend to be more challenging than traditional vision-language evaluations, aiming to assess different aspects of MLLMs, like detailed reasoning and OCR."}, {"title": "Implementation Details", "content": "We introduce G-Prune, a training-free, plug-and-play method that seamlessly integrates into existing MLLMs. We implement G-Prune on widely-used open-source MLLM, LLaVA-NeXT-8B (Liu et al. 2024a) following its default settings. We use the evaluation toolkit LMMs-Eval (Li et al. 2024) to evaluate the performance of these MLLMs across various datasets. For specific implementations, we configure 5 iterations with a similarity threshold of 0.5."}, {"title": "Quantitative Experiments", "content": "Comparison with state-of-the-art methods. In Tab. 1, we compare the effect of G-Prune with existing pruning methods on LLaVA-NeXT, including ToMe (Bolya et al. 2022), FastV (Chen et al. 2024a), and the random pruning. From the Tab. 1, we can first observe that, overall, our method achieves significant advantages in the above three types of data sets. On the general VQA tasks, the proposed G-Prune method has significant advantages at all pruning ratios, e.g., G-Prune improves performance by 0.88%-1.58% for VQA2.0 compared to ToMe. For the MLLM benchmarks, the proposed G-Prune improves performance on MME by 2.27% while using only 50% of the visual tokens. On the POPE dataset, our method consistently outperforms the baseline across all pruning ratios. As for text-oriented VQA tasks, the proposed G-Prune can better maintain the performance compared to other methods. When the pruning ratio is increased to 90%, we can observe that the performance of previous methods, e.g., ToMe, has a significant decline, especially on text-oriented VQA benchmarks, i.e., the performance drops by 40.35% on TextVQA and 26.02% on ChartVQA. In terms of ToMe, its complex indexing leads to large latency in MLLMs. In contrast, the proposed G-Prune can better maintain the performance, e.g., G-Prune improves the performance by 51.99% for TextVQA benchmark compared to ToMe when pruning 90% tokens. These results well validate the effectiveness of our G-Prune method in pruning visual tokens in the MLLM.\nFor a more fine-grained comparison, we plot the computational cost-performance curve in Fig. 3. From the curve, we can see that in the initial stage of reducing computational overhead, each method has no significant impact on the performance of the model on all benchmarks. As the more FLOPs are reduced, the performance of the model begins to decline rapidly. Specifically, on POPE benchmark, when reducing FLOPs from 1.83 \u00d7 1013 to 6.69 \u00d7 1012, there is only 0.09% performance decreased. While reducing FLOPs from 1.83 x 1013 to 3.37 \u00d7 1012, the performance significantly drops by 4.11%. This phenomenon shows that there is obvious redundancy in the visual tokens of MLLM. Therefore, in the initial stage, all methods can achieve token pruning without performance loss. However, as the number of pruning increases, tokens must be selected more carefully to achieve competitive performance. Meanwhile, we can observe that the proposed G-Prune method maintains the great advantages over other methods under different FLOPs reductions. For instance, when reducing 1.53\u00d71013 FLOPs for TextVQA benchmark, the proposed G-Prune improves performance by 52.05% compared to ToMe. As for reducing 1.41 \u00d7 1013 FLOPs, the proposed G-Prune keep 46.67% advantage than FastV for DocVQA benchmark. These results better validating the advantages of G-Prune for token pruning task in MLLM.\nComparison with Alternatives. In Fig. 4, we compare our G-Prune with the alternative using 12-Norm as the metric for LLaVA-NeXT on GQA, POPE and TextVQA benchmarks. The curve reflects the average performance of GQA, POPE and TextVQA under different pruning ratios. Specifically, baseline denotes the default MLLM without token pruning. \"12-Norm\" indicates retaining the tokens with the largest 12-Norm value according to the percentage.\nAs shown in Fig. 4, it is apparent that when the retention tokens exceed 50%, G-Prune consistently maintains stable performance compared to the baseline method, i.e., G-Prune still retains 99.84% of the original performance. At the same time, \"12-Norm\" method drops performance by 2.56%. Based on this, we can find that in the complex scenarios that MLLM needs to deal with, it is impossible to achieve token pruning by relying solely on the amount of information. Moreover, across all evaluated pruning ratios, G-Prune performs better than the method according to 12-Norm only. This advantage grows more pronounced as the pruning ratio increases. For instance, when the pruning ratio is increased to 90%, our G-Prune can improve the performance by 7.28% compared to \"12-Norm\" method. It fully shows that it is effective to measure the value of tokens by constructing a similarity graph between tokens in the visual token pruning task. These experiment results fully demonstrate that the proposed graph-based information propagation method in our G-Prune can effectively help us better perform visual token pruning in MLLMs.\nAblation Study. Further, we conduct ablation experiments for each component in Tab. 2 to explore the contributions and impacts of different components of our method. From Tab. 2, we can first observe that when randomly pruning 30% tokens, i.e., the first line in Tab. 2, there is a significant performance drop, e.g., the performance drops by 22.75% for TextVQA dataset. Then, we apply 12-Norm to measure the value of each token, and directly prune the tokens with lower 12-norm, this approach primarily preserves elements with higher information content (Lu and Zhang 2022), resulting in an average performance of 68.47. Conversely, we employ information propagation by constructing graph, and initial the score of each token to 1. With the better consideration to objects from both foreground and background, this approach improves the performance by 8.56% compared to 12-Norm only method. Finally, when applying both two methods, i.e., our G-Prune method, the average performance is improved by 8.77% compared to the random pruning method. These experiment results not only validate the effectiveness of each component in our G-Prune, but also show that every object from the foreground and background should be fully considered during the token pruning of MLLM.\nIn Tab. 3 we conduct experiment based on the different hyper-parameter settings. We first fix the number of iteration turn t in Eq.3. When the similarity threshold s in Eq.1 gradually increased from 0.3 to 0.9, we can observe that the average performance gradually increase to 2.78% when s = 0.7. With the further increment of the similarity threshold s, the performance is dropped by 4.95%. The reason for this phenomenon is that a too low similarity threshold s will cause all regions to be regarded as the same object, while a too high s will cause an object to be split into multiple objects unnecessarily. When fix the similarity threshold s to 0.5, we can observe that the best performance occurred with 5 iterations, i.e., the average performance is achieved"}, {"title": "Qualitative Experiments", "content": "To further delve into the intrinsic mechanisms of our method, we visualize the information propagation in Fig. 5, the heatmap represents the score of tokens. The tokens in the red areas have higher scores. In the initial stage, multiple tokens have high scores for the same object. This also shows that there is a lot of redundant information. As the information continues to propagate, we can find that the information gradually gathers in several tokens from both foreground and background. This shows that our G-Prune can effectively retain representative tokens for objects. These experiment results clearly prove the effectiveness of our G-Prune in visual token pruning. Then we visualize the results on the LLaVANeXT using the TextVQA dataset, pruning 50%, 70%, and 90% of the tokens to explore the pruning mechanisms of our method. We also compare our G-Prune method with ToMe and FastV. As shown in Fig. 6, we first focus on the performance of our method under different pruning ratios. When keep 50% retain tokens, the foreground occupies a large proportion, while the background area is preserved uniformly. This phenomenon shows that our G-Prune can effectively preserve information for each area. With the decreasement of retain tokens, we can observe that more tokens will be retained in areas with complex textures, whether from the background or the foreground. At the same time, in each area with high similarity, a certain number of tokens will be retained. Subsequently, we conducted a visual comparison of our method with others. For instance, in the left image, even with a pruning ratio of 50%, ToMe and FastV begin to lose fine-grained details, such as text on a wall image. This situation is even more serious at a 90% pruning ratio. Our method not only preserves more detailed information but also significantly reduces redundancy by targeting areas with higher similarity. On the other hand, from the right image, we can find out that the visual tokens from the background are completely pruned. This may cause MLLM to be unable to solve some tasks involving background information. Above all, with the decreasement of retain tokens, we can observe that more tokens will be retained in areas with complex textures, whether from the background or the foreground. At the same time, in each area with high similarity, a certain number of tokens will be retained. Under this paradigm, our G-Prune method can better preserve the information in the original image. These visualizations prove that G-Prune not only effectively maintains representative visual tokens, but also well alleviates redundancy."}, {"title": "Conclusion", "content": "In this paper, we introduced a novel, training-free visual token pruning method for multimodal large language models (MLLMs) named G-Prune. Our G-Prune method does not need to intervene in the reasoning process of MLLM, saving a lot of computational overhead. Meanwhile, the proposed G-Prune method considers objects in the foreground and background at the same time, fully meeting the complex problems that MLLM needs to deal with. Specifically, our approach constructs a graph based on token similarities, iteratively propagating information to identify and retain the most representative tokens for objects from both foreground and background. Extensive experiments across various benchmarks prove superiority of G-Prune over existing pruning methods, maintaining a great balance between computational efficiency and performance. For instance, G-Prune achieves a 63.57% FLOPs reduction on LLaVA-NeXT for TextVQA while only incurring a 2% decrease in accuracy. These findings suggest that G-Prune significantly advances the efficiency and scalability of MLLMs."}]}