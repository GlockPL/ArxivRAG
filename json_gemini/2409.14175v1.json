{"title": "QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling", "authors": ["Blessed Guda", "Gabrial Zencha Ashungafac", "Lawrence Francis", "Carlee Joe-Wong"], "abstract": "Abstract-Large Language models (LLMs) have brought about substantial advancements in the field of Question Answering (QA) systems. These models do remarkably well in addressing intricate inquiries in a variety of disciplines. However, because of domain-specific vocabulary, complex technological concepts, and the requirement for exact responses, applying LLMs to specialized sectors like telecommunications presents additional obstacles. GPT-3.5 has been used in recent work, to obtain noteworthy accuracy for telecom-related questions in a Retrieval Augmented Generation (RAG) framework. Notwithstanding these developments, the practical use of models such as GPT-3.5 is restricted by their proprietary nature and high computing demands. This paper introduces QMOS, an innovative approach which uses a Question-Masked loss and Option Shuffling trick to enhance the performance of LLMs in answering Multiple-Choice Questions in the telecommunications domain. Our focus was on using open-source, smaller language models (Phi-2 and Falcon-7B) within an enhanced RAG framework. Our multi-faceted approach involves several enhancements to the whole LLM-RAG pipeline of finetuning, retrieval, prompt engineering and inference. Our approaches significantly outperform existing results, achieving accuracy improvements from baselines of 24.70% to 49.30% with Falcon-7B and from 42.07% to 84.65% with Phi-2.", "sections": [{"title": "I. INTRODUCTION", "content": "The field of Question Answering (QA) systems has witnessed significant advancements with the advent of large language models (LLMs) [1]. These models have demonstrated remarkable capabilities in understanding and responding to complex queries across various domains. However, their application to specialized fields, such as telecommunications, presents unique challenges due to domain-specific terminology, intricate technical concepts, and the need for precise, accurate responses [2]. Recent research has shown promising results in applying LLMs to telecom-specific QA tasks. A notable study, Telco-RAG achieved accuracies around 75% using GPT-3.5 within a Retrieval Augmented Generation (RAG) framework for telecom-related questions [3]. While this demonstrates the potential of LLMs in domain-specific applications, the computational demands and resource requirements of such models often pose significant challenges for practical, widespread implementation [4]. Also, the use of GPT-3.5 deemphasizes open-source AI, as the training data details and architecture of GPT-3.5 are not open. This paper explores an innovative approach to addressing multiple-choice questions (MCQs) in the telecommunications domain using open, small language models within an enhanced RAG framework. Answering telecommunications standards questions presents a particularly challenging environment for LLMs due to its frequent use of unique abbreviations and rapidly evolving technologies and regulatory considerations [2]. To further complicate the problem, LLMs have been shown to demonstrate selection bias when answering MCQS which significantly degrades performance [5]. In addition, due to rapid changes and data sparsity, large language models will require frequent fine-tuning to keep them up to date with latest telecommunications standard. Our research aims to achieve competitive performance of larger models while maintaining efficiency and reducing computational costs. We focus on the Phi-2 [6] and Falcon-7B [7], which have shown promise in achieving competitive performance with significantly fewer parameters than their larger counterparts. Our work addresses these challenges through a multi-faceted approach called QMOS that combines several novel techniques:\n1) We leverage multiple embedding models to diversify and enrich the documents retrieved by the RAG system, potentially capturing a broader range of relevant information.\n2) Due to the high number of abbreviations used in Telecommunication standards, we enhance the dictionary of abbreviations used in [3] to boost the hit rate of successful abbreviations.\n3) We meticulously design our prompts to guide the model to reason over the documents when selecting the answer.\n4) We employ LoRA (Low-Rank Adaptation) fine-tuning on the Phi-2 model with a question-masked loss function to efficiently adapt it to the telecommunications domain.\n5) We implement an innovative optimization technique: inference and train time option batch-shuffling, which enhances the accuracy of the Phi-2 model by eliminating bias in option position of correct answers.\nBy focusing on small language models and employing these advanced techniques, our work addresses the growing need for efficient, scalable NLP solutions in specialized domains. This research has significant implications for developing cost-effective QA systems that can be deployed in resource-"}, {"title": "II. RELATED WORK", "content": "Small language models have emerged as a promising alternative to large models for domain-specific applications. Models like Phi-2, Falcon-7B and TinyLlama-1.1B [8] offer advantages such as resource efficiency, faster inference times, and easier fine-tuning for specific domains [9]. In [10] Piovesan et al. (2024) conducted a comprehensive evaluation of the small language model Phi-2 in the telecommunications domain, comparing it to larger models like GPT-3.5 and GPT-4. Their findings demonstrate that Phi-2, despite being significantly smaller, achieved an overall accuracy of 52.30% on the TeleQnA dataset [2], compared to 67.29% for GPT-3.5 and 74.91% for GPT-4. Notably, when enhanced with Retrieval-Augmented Generation (RAG), Phi-2's performance in the challenging 'Standards Specifications' category improved from 44.27% to 56.63%, nearly matching GPT-3.5's 56.97%. In another study, Ahmed et al. [11] conducted a comprehensive evaluation of several small language models, including Falcon 7B, in the telecommunications domain. Their findings demonstrate that Falcon 7B, despite having 7 billion parameters, achieved an overall accuracy of only 15.70% on the TeleQnA dataset, significantly lower than larger models like GPT-3.5 (67.29%) and GPT-4 (74.91%). While there is limited study on improving the small language model for telecom application, there has been an exhausive study in improving the performance or large language models. In [12] Soudani et al show that finetuning and RAG are both feasible methods in improving the performance of language models."}, {"title": "III. PROPOSED METHODOLOGY", "content": null}, {"title": "A. Retrieval-Augmented Generation Architecture", "content": "Retrieval-Augmented Generation is an interesting technique used to enhance the performance of large language models on tasks where the required knowledge was not present in the training data [13]. RAG is achieved by integrating external knowledge sources in the prompt from which the LLMS result is generated. In question answering tasks, questions are augmented with contextually relevant texts from external documents when creating prompts.\n1) Splitting Documents into Chunks: For RAG, we only need relevant parts of a large number of documents, and this requires a search over these documents. A common approach being employed by commercial tools for RAG such as LlamaIndex [14] and LangChain [15] splits the document such that each chunk contains a certain number of characters with an overlap between chunks. The parameters 'chunk-size' and 'chunk-overlap' are often adjusted such that the split leads to meaningful chunks ('chunk-size' specifies the number of characters in a chunk of text while 'chunk-overlap' specifies the overlap of texts between two contiguous chunks). We employed a custom document splitting strategy where we first split the documents into individual sections, excluding the table of contents. Each section is further split into chunks containing chunk-size characters (in our setting, chunk-size = 1024). Additionally we ensured that each chunk begins with the heading of the document section to which it belongs by prepending this heading to each chunk.This chunking strategy is inspired by our analysis of the structure of the 3GPP standard documents. Also, we observe that the first few pages of the documents containing sections like the title page, scope, references and table of contents are not very informative and as such we do not include those sections when creating the chunks. Having obtained the necessary chunks, we proceed to create vector embeddings of these chunks.\n2) Creating chunk embeddings: To enable similarity search, we created associated embeddings for each chunk using an embedding model. The choice of embedding model was influenced by models on the Massive Text Embedding Benchmark (MTEB) leaderboard [16]. We opted for the best performing and easily accessible models, favouring the use of the models 'stella_en_400M_v5' and 'gte-Qwen2-1.5B-instruct' [17] with 400M and 1.5B parameters respectively. We used these text embedding models from the 'Sentence Transformer' library [18] and sped up the embedding process by batching the chunks, using a batch size of 64. The chunks and their corresponding embeddings were saved to disk to be used for context retrieval for the question answering task.\n3) Chunk retrieval: For the chunk retrieval process, we employed a k-Nearest Neighbors (KNN) approach on the similarity scores between a question/query embedding and the chunk embeddings [19]. The use of a dot product similarity score implies that higher scores denote higher similarity. Hence, we retrieve the top k similar chunks for a given question/query. The number of retrieved chunks is chosen such that the context length of the language model is not exceeded when creating the prompt. We used the top 2 chunks retrieved with each embedding model. Additionally, we employed the use of the BM25 [20] algorithm which is a statistical approach for information retrieval that measures similarity based on the frequency of terms from the query that appear in the chunks. The motivation behind this is to ensure that the retrieval also includes chunks containing specific terms used in the query but are not necessarily enforced in the neural embedding models.This enabled us to create a context that consists of chunks from 2 embedding models (stella_en_400M_v5 and gte-Qwen2-1.5B-instruct [17]) and BM25 [20]."}, {"title": "B. Model Prompt", "content": "Prompt engineering is crucial in RAG-based LLM systems as it significantly enhances their performance and reliability. By carefully crafting prompts, engineers can guide LLMs to produce more accurate, relevant, and contextually appropriate responses without the need for extensive finetuning [21]. This is particularly important in RAG systems, where the integration of external knowledge sources in the prompts helps mitigate issues like hallucinations and factual inaccuracies [22]. In designing our prompt we make the following considerations:\n\u2022 Question Repitition: we draw from the observation of [3], which showed that in answering Telecommunication questions, repeating the question before and after the contexts helps make the Phi-2 model reason over the contexts for the answer.\n\u2022 Enhanced Abbreviation Expansion: As we noticed that a lot of the questions in the TeleQnA dataset are about abbreviations, we also decide to include the abbreviations in the prompt like [3]. However, we notice that the method in [3] missed a lot of abbreviations because of the insufficient dictionary of abbreviations. This is because the abbreviation dictionary used was generated by only considering the Vocabulary for 3GPP specifications document [23]. We expand this dictionary by searching for other abbreviations in the \"Definitions of terms, symbols and abbreviations\" sections of all the documents. With this enhanced abbreviation dictionary, we are able to achieve a hit rate of 95.16% in the test set which is a significant improvement over the 63.74% achieved using the approach in [3].\n\u2022 Model Pretraining format: We consider how the prompts in the training phase of the model were structured. This is crucial because presenting the model with prompts structured based on how it's trained can help the model perform better.\n1) Phi-2 model: With the above considerations, we design our prompt for Phi-2 as:\nInstruct: **Question**\nAbbreviations:\n**abbreviation: full form**\nConsidering the following retrieved contexts\ncontext 1: context...\ncontext 2: context...\n**Question**\n** option 1:\n** option 2: ....\nOutput :\n2) Falcon-7B: We make slight modifications to the prompt for the Falcon 7B based on its poor performance on the TeleQnA dataset [11]. We observed that when provided with the options, the model does not do well therefore, we do not provide the options to the model.\nYoure a Telecommunication standards expert. Please answer the question first consider the given context for the answer.\n**Question**\nAbbreviations:\n**abbreviation: full form**\nConsidering the following retrieved contexts\ncontext 1: context...\ncontext 2: context...\n**Question**\nFrom the answer sentence generated by the model, we compute the embeddings of the generated answer sentence and take the answer option that has the highest similarity. We use a simple sentence BERT model [24] for this answer extraction."}, {"title": "IV. QMOS FINETUNING APPROACH", "content": null}, {"title": "A. Phi-2 Finetuning", "content": "To finetune the Phi-2 model, we used LoRA (Low-Rank Adaptation) [25] fine-tuning approaches instead of full fine-tuning due to the size of the model and the small size of the training data. We explored both LoRa and Quantized LoRa (QLoRa) [26] fine-tuning. We did not explore fine-tuning the Falcon 7B model, due to its large size which exceeds our computational budget.\n1) QLoRa: QLoRA (Quantized Low-Rank Adaptation) is a parameter-efficient fine-tuning technique for large language models. It combines quantization and low-rank adaptation to significantly reduce memory requirements and computational costs. In QLORA, the base model's weights are quantized to 4-bit precision and frozen. Then, small trainable \"adapter\" layers are added using low-rank decomposition. We envision that these adapters would capture the Telecommunication MCQ reasoning ability during fine-tuning while keeping most of the model fixed.\nWe configure the model for QLoRA fine-tuning with the following parameters:\n\u2022 Low Rank: Set to 64 to balance between performance and computational efficiency.\n\u2022 Alpha: Set to 16 to scale the low-rank updates.\n\u2022 Dropout: Set to 0.05 to prevent overfitting during training.\n\u2022 Adapter Layers: We add adapter layers to the Query, Key, and Value and Feedforward Weights of the Transformer layers of the Phi-2 model.\n2) LoRa: The use of QLORA finetuning reduces the memory requirement during training. However, the training time is considerably increased due to the quantization and dequantization operations being performed during QLoRA finetuning. In order to investigate the influence of the finetuning objective on model's performance on MCQs, we used LoRA since it offers faster training time compared to QLORA. We compared the model's performance when the training objective only considers the answers versus when the entire prompt and answer are considered in the objective.\nThe original cross-entropy loss for next token prediction is defined as:\n$L = - \\sum_{t=1}^{T} y_t \\log(\\hat{y}_t)$\nwhere:\n\u2022 T is the total number of tokens in the sequence (including both the question/prompt and the answer).\n\u2022 $y_t$ is the actual token at position t (one-hot encoded).\n\u2022 $\\hat{y}_t$ is the predicted probability distribution over the vocabulary for token t.\nTo focus only on answer generation, we introduce a masking vector $m_t$ such that:\n$m_t = \\begin{cases} 0 & \\text{if } t \\in \\{1, 2, ..., Q\\} \\\\ 1 & \\text{if } t \\in \\{Q+1, Q+2, ..., T\\} \\end{cases}$\nThe modified cross-entropy loss, which we call question-masked loss $L_{masked}$ is then:\n$L_{masked} = \\sum_{t=1}^{T} m_t \\cdot y_t \\log(\\hat{y}_t)$\nThis loss function ensures that only the tokens corresponding to the answer part of the sequence contribute to the overall loss, effectively masking out the contributions from the question/prompt part."}, {"title": "B. Option Batch-Shuffle Trick", "content": "Recent research [2], [5], [27] has unveiled a significant bias in LLMs when answering multiple-choice questions (MCQs). These models exhibit a strong sensitivity to the order of options, often selecting specific answer positions regardless of the content. This phenomenon, termed \"selection bias,\u201d stems from the models' tendency to assign higher probabilities to certain option labels (like \"A\" or \"B\" in options [\"A\", \"B\", \"C\", \"D\" and \"E\"] ). Consequently, LLMs may prioritize these options even when logically incorrect, undermining the reliability of their performance on MCQ assessments. To avert this we employ a trick where we create multiple prompts for a question, with each prompt having a different option order. The correct answer is thus determined by chosing the most selected answer by the model after observing the answers generated for all created prompts. Given that we have to permute these options to obtain all possible option ordering, the complexity of doing so is O(n!) wheren is the number of options present in an MCQ. This complexity significantly increases the inference time for a single question. For example, when an MCQ has 4 options, we create 4! = 24 prompts instead of 1 prompt. For 5 options, we create 5! = 120 prompts. To reduce this complexity, we randomly sample k prompts from the n! prompts, create a batch of k prompts and generate answers for the batch using the Phi-2 model. Using k prompts instead of n! reduces the complexity from O(n!) to O(k), where k < n!. The model generates answers for these k prompts in a single batch, thus significantly reducing inference time while still benefiting from diverse option orderings. The selection of the most frequent answer from these k prompts can be described as:\n$a = \\arg \\max_{a \\in A} \\sum_{i=1}^{k} I(a_i = a)$                (1)\nwhere I(\u00b7) is the indicator function, ai is the answer chosen by the model for the i-th prompt, and A is the set of all possible answers.\nWe call this the 'batch-shuffle trick'. Using the batch-shuffle trick, we noticed over 6% at inference time and this increased to about 10% when we include the trick into the training phase. In the training phase, we only shuffle the options at the end of each epoch and do not use any explicit sampling. We hypothesize further improvement in performance as we increase k, the number of samples from n! prompts of an MCQ. In our case, we find k = 20 to be a good balance between efficiency and accuracy."}, {"title": "V. EVALUATION", "content": "We evaluated our approach using a subset of the TeleQnA dataset containing only two categories; Standards Specifications and Standards Overview. Matoouk et al [2] showed that GPT-3.5 and GPT-4 performed better in other question categories than in these two categories. The dataset, obtained from the 'Specializing Large Language Models for Telecom Networks by ITU AI/ML in 5G Challenge' on Zindi [28] contains a train set of 1461 questions, a public test set 366 questions, and a private test set of 2000 questions. Evaluation results, as obtained from the leaderboard, are reported for the private test set. The train set was used for fine-tuning purposes. Additionally, we used the technical documents provided by the challenge as external knowledge sources for RAG.\nWe compare the performance of the base Phi-2 model and its performance with RAG, with fine-tuning and with the batch-shuffle trick both at inference and at training time. For the Falcon7B, we compare the base model performance and its performance with RAG and with the options excluded in the prompt."}, {"title": "A. Phi-2 model", "content": "Table I shows the performance of the Phi-2 model on the private test sets as obtained from the submissions on Zindi. The accuracy score measures the percentage of correctly answered questions. The base Phi-2 model has an accuracy of 42.07%, and this accuracy was increased to 66.39% with the introduction of RAG. The result obtained with RAG was further improved by fine-tuning the model using the training set. With fine-tuning, the accuracy increased to 76.90%. We had pointed out that LLMs exhibit a strong sensitivity to options ordering in MCQs and introduced the batch-shuffle trick in Section IV. Using the batch-shuffle trick with the fine-tuned Phi-2 model, we obtained an accuracy of 81.65% which is a 6.18% increase over the fine-tuned model and 84.65% when we shuffle the options during training.\nWhile fine-tuning the Phi-2 model on the training data, we discovered that the model's performance stops improving after certain epochs of training. To investigate this we modified the objective (next token prediction) to only account for answer generation by masking the part of the cross entropy loss associated with question/prompt prediction.\nUsing a train-validation split (20% validation) while fine-tuning, it is expected that the validation accuracy increases as the validation loss decreases. However, shows no improvement in the validation accuracy even as the validation loss decreases. We suspected that this decrease in validation loss results from the model getting better at predicting the question and not the answers.  shows the result obtained when the loss associated with questions is masked out, allowing the training objective to focus only on the answers.  shows that the validation accuracy increases as the validation loss decreases. We, therefore, hypothesize that focusing solely on the answers during fine-tuning may yield better results. Validating this hypothesis is an interesting area of future work to be explored with additional experiments."}, {"title": "B. Falcon-7B", "content": "The the performance of the Falcon-7B model is summarized in Table II\nFor the Baseline 7B model, when prompted with the options we notice that in some cases the model does not output the options but just some unrelated texts.In that case, we randomly choose an option. This yielded an accuracy of 24.51%. Adding the contexts from the RAG further enhanced the score to 36.61%. Finally, by removing the options and allowing the model to generate the answer freely. We then used the embedding of the generated answer with the embedding of the options to select the right option using a cosine similarity metric. With this strategy the model was able to achieve an accuracy of 49.93%, which is significantly higher than the baseline of 24.51%. We note that the baseline Falcon-7B model is not fine-tuned considering it has more parameters (7B) than Phi-2 (2.7B). Since fine-tuning will increase the computational cost, we strictly rely on the effectiveness of prompting and RAG systems in a bid to improve its baseline performance on question answering in telecommunications domain."}, {"title": "VI. CONCLUSION", "content": "In this research, we have presented a comprehensive approach to addressing multiple-choice questions (MCQs) in the telecommunications domain using small, open-source language models within an enhanced Retrieval-Augmented Generation (RAG) framework. Our study demonstrates that small models such as Phi-2 and Falcon-7B, when combined with advanced techniques like LoRA fine-tuning, diversified embedding models for RAG, innovative prompt engineering, and batch-shuffle trick can achieve competitive performance compared to larger, proprietary models like GPT-3.5, while significantly reducing computational costs.Future work will involve fine-tuning the embedding models (used for RAG) for the telecommunication domain and also further investigate the performance of the proposed QMOS framework on the other language models and MCQs datasets."}]}