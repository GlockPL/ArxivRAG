{"title": "Training an LLM-as-a-Judge Model: Pipeline, Insights, and Practical Lessons", "authors": ["Renjun Hu", "Yi Cheng", "Libin Meng", "Jiaxin Xia", "Yi Zong", "Xing Shi", "Wei Lin"], "abstract": "The rapid advancement of large language models (LLMs) has opened new possibilities for their adoption as evaluative judges. This paper introduces Themis, a fine-tuned LLM judge that delivers sophisticated context-aware evaluations. We provide a comprehensive overview of the development pipeline for Themis, highlighting its scenario-dependent evaluation prompts and two novel methods for controlled instruction generation. These designs enable Themis to effectively distill evaluative skills from teacher models, while retaining flexibility for continuous development. We introduce two human-labeled benchmarks for meta-evaluation, demonstrating that Themis can achieve high alignment with human preferences in an economical manner. Additionally, we explore insights into the LLM-as-a-judge paradigm, revealing nuances in performance and the varied effects of reference answers. Notably, we observe that pure knowledge distillation from strong LLMs, though common, does not guarantee performance improvement through scaling. We propose a mitigation strategy based on instruction-following difficulty. Furthermore, we provide practical guidelines covering data balancing, prompt customization, multi-objective training, and metric aggregation. We aim for our method and findings, along with the fine-tuning data, benchmarks, and model checkpoints, to support future research and development in this area.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement in large language models (LLMs) has endowed today's most capable artificial intelligence systems with near-human cognitive abilities, including language understanding, mastery of world knowledge, instruction following, reasoning, and planning [29, 41, 42]. Often likened to revolutionary technologies such as electricity, LLMs are being deployed across various domains, including those with high-stakes [10, 33, 37]. Alongside rapid progress and widespread adoption comes an increasing concern on the large-scale potential risks [3]. As LLMs continue to evolve, evaluating their capacity [12, 14] as well as alignment with user intentions [8, 18, 20], ethical standards [17, 36], and human values [4, 11, 16] becomes pivotal.\nIn this study, we focus on assessing LLMs' alignment with user intentions in open-ended tasks, i.e., the ability to accurately adhere to open-ended instructions and meet user expectations [51]. This represents the most natural usage of LLMs and such alignment is fundamental to ensuring their helpfulness [2]. While manual evaluation [8] is straightforward, it is expensive and can suffer from subjective inconsistency. Established evaluation metrics such as perplexity [15], BLUE [32], and ROUGE [25] often fall short in capturing the nuanced dimensions of alignment evaluation. Additionally, the assumption of unique ground-truth reference responses is frequently invalid in many open-ended scenarios, e.g., advice seeking and writing assistance. These gaps underscore the necessity for more sophisticated and context-aware evaluation mechanisms capable of operating automatically.\nThe continually improving capabilities of LLMs has created a new paradigm for this problem: deploying LLMs as judges to assess other LLMs, known as LLM-as-a-judge [18, 20, 24, 51]. Recent studies have demonstrated that general-purpose or specifically fine-tuned LLMs are qualified judges in at least two aspects. First, they could obtain a high evaluation agreement rate with human, matching the same"}, {"title": "2 Related Work", "content": "LLMs have greatly revolutionized the field of natural language processing. Classic overlapping-based methods [25, 32] are no longer suitable for evaluating today's LLMs. Human evaluation suffers from its high cost and is time-consuming [8]. Currently, automatic LLM evaluation could be roughly divided into three categories.\nEvaluation on static benchmarks. Static benchmarks have been developed to assess the performance of LLMs across various tasks, such as language understanding [12], world knowledge [14, 52], reasoning [48], coding [7], and math [13]. These benchmarks usually consist of objective, e.g., multi-choice, questions designed to rigorously evaluate specific abilities. Metrics like accuracy could then serve as valuable references for model comparison and advancement. However, evaluation on static benchmarks has its limitations. A notable shortcoming is that the tested metrics only capture LLMs' performance on predefined tasks with close-ended outputs, resulting in a gap between user perceptions of LLMs' usefulness in real-world applications. Additionally, static benchmarks may inadvertently incentivize models to over-fit rather than developing generalizable ability [5, 53]. Recent efforts have sought to expand the variety of benchmarks to include more diverse and up-to-date knowledge [27] or reasonable perturbations [19] for assessing genuine capacity.\nHuman-inspired evaluation. Techniques in the second category treat LLMs as if they possess human-like qualities, utilizing methodologies originally developed for human assessments to evaluate these models. This types of approaches often focus on the social characteristics of LLMs, such as creativity [49], values [4, 11, 16], ethical standards [17, 36], trustworthiness [39], etc.\nLLM-as-a-Judge. The concept of utilizing LLMs as judges has emerged as a new evaluation paradigm [24, 51]. This innovative approach leverages the intrinsic capabilities of LLMs to provide fine-grained evaluations, and it has been demonstrated that LLMs can achieve high agreement rates with human evaluators [23, 51], effectively serving as substitutes for traditional evaluation metrics. Along the line, some studies have devoted to constructing instruction sets for evaluation, with those large-scale [50], in the wild [23], and challenging [22] standing out. Others have explored fine-tuning an LLM as judge, which has also been verified effective and more economical [18, 20, 43].\nOur work belongs to the third category, differing from related work in two aspects. Methodologically, Themis integrates a combination of step-by-step evaluation prompts and controlled instruction generation, featuring better flexibility to develop required evaluation ability. Empirically, we provide both scenario-centric"}, {"title": "3 Development Pipeline", "content": "In this section, we present the complete development pipeline of Themis. Strategically, Themis adopts scenario-dependent evaluation prompts, employs two methods for controlled instruction generation, and learns from GPT-4 rationales. We establish two human preference benchmarks to quantify Themis's performance."}, {"title": "3.1 Prompt Design", "content": "The effectiveness of LLM-as-a-Judge is significantly influenced by the design of evaluation prompts. Previous studies have explored three types of prompts: unified [24, 51], scenario-based [18, 20], and instruction-based [23]. We choose scenario-based prompts for Themis because they provide the necessary context-awareness for instruction-specific evaluations while imposing reasonable additional requirement, i.e., a scenario classification model, to achieve evaluation automation.\nWe employ human-AI collaboration to designed scenarios and their corresponding judge criteria. Initially, we draft a proposal of common LLM use scenarios, including their names and descriptions, and solicit suggestions from an advanced LLM, such as GPT-4. In a subsequent iteration, we request the same LLM to output its scenario design based on both the initial human proposal and its own modification suggestions. From this process, we finalize 10 scenarios from the initial 15. These include: three question-answer scenarios (close QA, open QA, and math-related QA), three writing scenarios (creative writing, informative and professional writing, and rewriting), and four professional scenarios (translation, reading comprehension and extraction, role-playing, and programming-related). For each chosen scenario, we follow the same iterative process to derive the judge criteria. This involves an initial human proposal, suggestions from the AI, a revised AI proposal incorporating these suggestions, and a final human-edited version. Detailed scenario descriptions and the chosen judge criteria (81 in total) are provided in Appendix A, as well as a scenario comparison with Llama 3 [1] which empirically justifies our scenario design through human-Al collaboration.\nWe then develop the scenario-based evaluation prompts. Similar to previous work, we support three variants of judgement: single answer grading, reference-guided grading, and pairwise comparison.  These detailed, step-by-step prompts offer several benefits. First, they provide judge LLMs with concrete instructions on how to perform evaluations, including both general grading tiers and steps, and scenario-specific criteria. Second, they encourage LLMs to elucidate the reasons for their ratings, which enhances learning efficiency during model training and improves interpretability during deployment. To perform evaluations using"}, {"title": "3.2 Data Construction", "content": "We next outline the data construction pipeline for the supervised fine-tuning [30] of Themis, including collecting user instructions, their corresponding responses, and evaluations of these instruction-response pairs. The primary challenge is gathering user instructions, as responses and evaluations can be automatically generated by LLMs. Typical methods for collecting user instructions involve utilizing existing instruction sets [18, 20, 50] or generating instructions from a small set of seed examples [40, 44]. However, these methods may not adequately balance instruction distribution across scenarios, potentially impacting the performance of judge LLMs due to unbalanced or insufficient data. To address this, we introduce the idea of controlled instruction generation, employing reference-based questioning and role-playing quizzing.\nReference-based questioning. Our first method leverage LLMs' generative ability to synthesize user instructions for specific scenarios based on reference texts. We achieve this efficiently by fine-tuning a questioning model [45] using data generated by GPT-4. We adopt a prompt (available in an extended version due to the space constraint) specifies the scenario name and description to guide question synthesis. A piece of reference text is also provided. Both of them enhance controllability for the process. It also outlines synthesis requirements, provide examples from a small set of manually crafted seed instructions, and requests GPT-4 to generate five instructions at a time. We manually validate these outputs and use the filtered data to fine-tune the questioning model.\nRole-playing quizzing. While the reference-based method excels in generating questions for seven scenarios, it struggles with instruction adherence and quality for the remaining three scenarios, particularly in scenarios like math-related QA and programming, where reference text suitability is crucial. To this end, we propose the role-playing quizzing method, which leverages LLMs' ability to act as test writers to generate instructions for these challenging scenarios. This method specifies quiz-related information such as difficulty level, audience, subject, topic, and task to improve controllability. Detailed prompts for this method will also be provided in a future extended version.\nThese two methods together ensure a balanced and comprehensive collection of user instructions across diverse scenarios. We then gather responses to these instructions from LLMs of varying capacity, including ChatGLM3-6B, Baichuan2-13B, Yi-34B, Qwen-72B, and GPT-3.5-turbo. These instruction-response pairs are evaluated with GPT-4 using the evaluation prompts developed earlier, and we use the detailed evaluation outputs to fine-tune Themis."}, {"title": "3.3 Fine-tuning", "content": "We now detail the fine-tuning process for our models. We choose the Qwen-2 series base models [42] as the foundation models. The fine-tuning data are summarized in Table 2. All training tasks are executed on Nvidia H800 GPUs, utilizing DeepSpeed ZeRO-3 [34] to optimize GPU memory usage and accelerate training.\nScenario classification LLM. We manually label 18,874 records for fine-tuning the scenario classification model. Each labeled record is converted into a prompt. This prompt enumerates the scenarios with both name and description, specifies a user instruction, and ask the LLM to classify a scenario for the instruction. We used the"}, {"title": "3.4 Performance Assessment", "content": "Benchmarks. We create two human preference benchmarks for performance assessment. (1) Alignbench [26] contains 683 manually selected real user instructions and we extend the data with a scenario label and five responses by the same set of LLMs in Sec. 3.2"}, {"title": "4 Insights from Scenario-centric Analysis", "content": "Exp-1. Detailed performance across scenarios. We first investigate the performance of Themis across different scenarios and the detailed results of single answer grading Agr(2, 2), as well as the corresponding z-value, on our benchmarks are reported in Table 4. Z-values exceeding 0.5 and falling below -0.5 are highlighted in bold and underlined, respectively. The results reveal that Themis generally excels in open-ended scenarios such as role-playing, open QA, creative writing, and informational and professional writing. On the other hand, its performance diminishes in close-ended scenarios like close QA and math-related QA, which demand higher knowledge and reasoning capabilities for accurate responses and evaluations. Additionally, we observe a positive correlation between scenario-based Agr(2, 2) and the average labeled scores (i.e., avg. y) of our responding LLMs on these scenarios: the Pearson correlation coefficient is 0.822 and 0.426 on Alignbench and SynUI, respectively (see Fig. 1). This suggests that the inherent capacity of LLMs significantly influences their effectiveness as judges.\nInsight 1: The evaluative performance of LLMs positively correlates with their inherent capacity.\nExp-2. The impacts of reference answers. The availability of reference answers would make the evaluation tasks more manageable for humans. Analogically, we next explore how reference answers affect alignment evaluation with LLMs. Alignbench includes a reference answer drafted by GPT-4 and refined by human for each instruction, while SynUI uses GPT-4's responses as reference answers. The results of reference-guided grading Agr(2, 2), as well as the resulting improvement by reference answers, are also presented in Table 4, from which we find the following. First, reference answers improve the Agr(2, 2) of Themis by 0.059 on Alignbench, but have minimal overall impacts on SynUI. This difference likely arises from Alignbench's higher answer quality and"}, {"title": "Exp-3. Fine-tuning with single scenario data", "content": "Previous research has validated that data composition significantly impacts model performance during pre-training and fine-tuning [9, 47]. As a basis for exploring these effects for LLM-as-a-Judge, we first examine the performance of fine-tuning with data from individual scenarios. We randomly sample 800 evaluation records for each of the ten scenarios, ensuring a relatively balanced distribution of grading scores and pairwise ratings. We then fine-tune ten judge models, each trained on data from a single scenario, and assess their judging performance for all scenarios on our two benchmarks. We report the combined performance metric, i.e., averaged multiple metrics on both benchmarks, where higher numbers indicate better performance.\nFrom the table we find that all fine-tuned models outperform the baseline foundation model, highlighting the general benefit of fine-tuning for LLM-as-a-Judge. However, the impact of data from different scenarios varies. For example, data from informative and professional writing enhances the evaluation performance across all scenarios; where data from close QA lead to performance deterioration in several scenarios like open QA and translation. This deterioration likely results from mismatches in evaluation criteria and the structured quality issues of LLM-generated fine-tuning data. Surprisingly, fine-tuning on data from translation and programming scenarios leads to decreased performance on their respective tasks, further evidencing the limitations of data quality. These results suggest that data from different scenarios can have both positive and negative effects on evaluation performance.\nInsight 3: Fine-tuning generally benefits LLM-as-a-Judge, but careful data engineering at the scenario level is crucial due to the varying synergistic and inhibitory effects of different scenario data."}, {"title": "Exp-4. Data composition and scaling", "content": "In the last set of analysis, we investigate the effects of data composition and scaling on model performance. Following the method in [38, 46], we use K-means algorithm to group scenarios to manage the number of required tests. We employ the columns of Fig. 2 as the clustering features, exclude the close QA scenario due to its minimal overall improvement, and choose K = 3, resulting in the clusters (A: MQA, GP), (B: IPW, RW, T, RP), and (C: OQA, CW, RCE). Intuitively, scenarios within same clusters exhibit similar influence on model's"}, {"title": "5 Practical Lessons", "content": "This section shares the practical lessons we learn during developing and optimizing the performance of Themis. Note that the numbers in this section are not evaluated on the latest benchmarks, thus may be inconsistent with those in previous sections.\nBalancing fine-tuning data. We use full parameter fine-tuning to speedup model adaption and we find that the distribution of evaluation scores and pairwise ratings of fine-tuning data severely influences the rating bias of the obtained model. For instance, we fine-tuned an earlier version of Themis with all single answer grading evaluation records, and the resulting MAE and Agr(2,2) are 1.068 and 0.455 on the Alignbench benchmark, much worse than its teacher GPT-4 with 0.868 and 0.509. During model diagnosis we observe that the model has an extreme high trend to rate response with score 4. We check the distribution of scores in the fine-tuning data and find that evaluation records with score 4 account for approximately 56%. We then down-sample these records to achieve a more balanced score distribution, leading to optimized MAE and Agr(2,2) with 0.908 and 0.467. And the predicted scores are less biased to a specific one.\nSupporting custom evaluation prompts. Recall Table 1 that our scenario-based prompts use fixed criteria, steps, and a five-tier rating system for evaluation. During the deployment of Themis, our initial users request for supporting custom prompts for criteria and rating systems. To this end, we have constructed a custom prompt generation procedure which augments required data without extra API usage for GPT-4.\n(1) Rephrasing criteria and descriptions. Referring to [31], we employ an LLM to paraphrase existing criteria, requiring the paraphrased names and descriptions to have low textual similarity to"}, {"title": "Enabling multi-objective training", "content": "In the standard SFT process, LLMs learn from predicting the exact next tokens by minimizing the cross entropy loss. However, we note that not all tokens in the evaluation output need to be perfectly predicted. Take the output in Fig 5 as an example. The content in black is scores and format-related text, and we require these words to be predicted accurately. On the other hand, the words in blue are the explanation for the specific score, for which we could tolerate more noises as long as the predicted content is semantically similar to the target.\nThis idea inspires a multi-objective training method illustrated in Fig 5. Specifically, we first label each output tokens with either SFT or Sim. For SFT tokens, we still minimize the cross entropy loss between the the predicted and ground-truth logits. For Sim tokens, we minimize the difference between the embeddings of the top-1 predicted and ground-truth tokens. We find that this training method reduces the MAE from (0.684, 0.676) to (0.673, 0.652) and improves Agr(2,2) from (0.586, 0.581) to (0.594, 0.591) on the two benchmarks, respectively.\nUnifying performance metrics. We finally share a lesson for development efficiency. During our deployment of Themis, a long-term challenge is to determine which fine-tuned checkpoint, or equivalently the corresponding optimization technique, is better. Recall that Themis supports three variants of evaluations, which is a tradition for the LLM-as-a-Judge paradigm, as well as we create two benchmarks and use two metrics for performance assessment. Putting these together, we need to compare more than 10 numbers to come to a decision, which is not easy. Indeed, we have had a lot of controversies for which one is better within our team. Later, we decide to aggregate all performance metrics into one to close controversies. The most straightforward method is to use the average score. However, we find this is unfair due to the different effective scales for metrics. For instance, it is much harder to optimize the Agr(2,2) by 0.1 than MAE. In this case, using average score will let MAE dominate the choice of optimization directions. To address this, we perform a linear transformation on the original metrics such that random guessing is mapped to 0 and the best performance metric is mapped to 1. Averaging the transformed metrics gives us a fair overall performance metric which help us choose promising optimization strategies."}, {"title": "6 Conclusion", "content": "In this paper, we developed an LLM judge model named Themis for user intent alignment evaluation. We first detailed the development pipeline of Themis. Specifically, it utilized scenario-dependent evaluation prompts, incorporated two innovative methods for controlled instruction generation, and distilled evaluative skills from GPT-4. Results on our human preference benchmarks demonstrated the effectiveness of our training pipeline: Themis could offer automatic and contextually informed evaluations with an accuracy close to GPT-4 while using much lower serving costs. We also presented key insights which could enhance the understanding of the LLM-as-a-judge paradigm. To advance further research and development, we shared our experience for performance optimization and committed to release our data, benchmarks and model checkpoints to the community. A couple of problems deserve further investigation. We are exploring multi-agent collaboration and human-in-the-loop to mitigate the data quality issues of LLM-generated SFT data. In addition, we seek to train foundation models specific for LLM-as-a-judge to boost generalization."}]}