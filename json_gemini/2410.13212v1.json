{"title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\nAsymmetric Quantization Configurations", "authors": ["Qian Tao", "Wenyuan Yu", "Jingren Zhou"], "abstract": "Large language models have shown exceptional\ncapabilities in a wide range of tasks, such as\ntext generation and video generation, among\nothers. However, due to their massive parame-\nter count, these models often require substantial\nstorage space, imposing significant constraints\non the machines deploying LLMs. To over-\ncome this limitation, one research direction\nproposes to compress the models using inte-\nger replacements for floating-point numbers, in\na process known as Quantization. Some recent\nstudies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing\nquantization techniques that treat the key and\nvalue matrices equivalently.\nThis work delves deeper into the asymmetric\nstructural roles of KV Cache, a phenomenon\nwhere the transformer's output loss is more sen-\nsitive to the quantization of key matrices. We\nconduct a systematic examination of the atten-\ntion output error resulting from key and value\nquantization. The phenomenon inspires us to\npropose an asymmetric quantization strategy.\nOur approach allows for 1-bit quantization of\nthe KV cache by implementing distinct config-\nurations for key and value matrices. We carry\nout experiments across a variety of datasets,\ndemonstrating that our proposed model allows\nfor the quantization of up to 75% decoder lay-\ners with 1 bit, while simultaneously maintain-\ning performance levels comparable to those of\nthe models with floating parameters.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have gained con-\nsiderable interest of late due to their remarkable per-\nformance in various directions (McKeown, 1992;\nWang et al., 2022; Taylor et al., 2022; Ji et al., 2021;\nGruver et al., 2024). However, to achieve a high\nlevel of expressiveness, LLMs typically require bil-\nlions of parameters, which necessitates substantial\nstorage space and poses challenges for deployment\non machines with limited resources.\nA line of research has been dedicated to enabling\nthe deployment of these models on machines with\nless available space through model compression\ntechniques. One such technique, model quanti-\nzation, aims to represent the parameter matrices\nin LLMs using fewer bits (e.g., integer, binary),\nthereby making them more suitable for deployment\non hardware with limited storage capacity (Kim\net al., 2023). More recently, the Key-Value cache\n(KV cache) in LLMs has been shown to occupy a\nlarge proportion of space (Pope et al., 2023; Mo-\nhtashami and Jaggi, 2023), especially when the\nlength of context increases, and numerous works\nhave focused on the quantization of KV cache (Liu\net al., 2024a,c; Kang et al., 2024). Nonetheless,\nthese studies typically employ the same quantiza-\ntion configuration for both key and value matrices.\nIn this paper, we cast a spotlight on the asymmet-\nric structural roles of key and value matrices. Our\nanalysis reveals that, while a quantization method\ncould yield a quantized matrix with a commen-\nsurate loss for both key and value matrices, the\nmultiplication of query and application of the acti-\nvation function to the key matrix results in a larger\nloss of key matrix in the transformer's output as\ncompared to the value matrix.\nDrawing on this observation, this paper intro-\nduces a simple yet efficacious quantization strategy,\nwhich entails the use of asymmetric and layer-wise\nquantization configurations for key and value matri-\nces. Specifically, during the next token's inference,\nwe employ a higher-bit quantization strategy (for in-\nstance, a 4-bit strategy) for the first I decoder layers,\nwhilst a lower-bit strategy (i.e., the 1-bit strategy)\nis applied for the remaining decoder layers. For\nkey and value matrices, we choose different l to\naccount for their asymmetric structural positions.\nOur extensive experiments reveal that the adoption\nof an asymmetric and layer-wise quantization strat-\negy allows us to quantize a subset of layers using a\n1-bit approach, resulting in a strategy that is both"}, {"title": "2 Preliminaries", "content": "2.1 Attention Mechanism and KV Cache\nGiven the input embeddings of an attention mech-\nanism, $X \\in \\mathbb{R}^{t\\times h}$, where $t$ represents the num-\nber of tokens already generated and h is the di-\nmension of attention head, an attention mechanism\nM (Vaswani, 2017; Ainslie et al., 2023; Shazeer,\n2019) obtains the hidden states as follows:\n$Q = XW^{Q},K = XW^{K}, V = XW^{V}$\n$A^{w} = sm(\\frac{QK^T}{\\sqrt{h}})$\n$A^{o} = A^{w}V$\nHere, W, Wk and Wu are the weight matrices for\nthe query, key, and value, respectively, and sm(\u00b7)\nsignifies the softmax function. Aw and A\u00b0 are\ntypically referred to as the attention weights and\nattention output, respectively.\nAs an LLM generates tokens, the embeddings of\nthe newly produced token are appended to the end\nof X, necessitating the generation of query, key,\nand value matrices. Consequently, we can store the\nembeddings of K and V from previous tokens and\nonly generate the corresponding segments for the\nnew token in K and V. Specifically, by partition-\ning X into the embeddings of previous tokens, i.e.,\nX1:t-1, and the embeddings of the current token,\nXt, we can leverage the key and value cache to\nenhance LLM's computational efficiency.\n$x_q = X_tW^{Q}, x_k = X_tW^{K}, x_v = X_tW^{V}$\n$K_{1:t} = cat(K_{1:t-1}, x_k), V_{1:t} = cat(V_{1:t-1}, x_v)$\n$A^{w} = \\frac{x_q K_{1:t}^T}{\\sqrt{h}}$ (1)\n$A^{w} = sm(A^{w})$ (2)\n$A^{o} = A^{w}V_{1:t}$ (3)\nHere, the key and value matrices, K1:t-1 and\nK1:t-1 are cached while generating the last token.", "subsections": [{"title": "2.2 KV Cache Quantization", "content": "Round-To-Nearest Quantization. While enhanc-\ning computational efficiency, the KV cache de-\nmands considerable memory, particularly as more\ntokens are generated. To mitigate this, previous\nstudies propose quantizing the key and value ma-\ntrices into integers to accommodate more tokens\nusing a Round-To-Nearest (RTN) methodology.\nFormally, given a key or value matrix, $M\\in\n\\mathbb{R}^{t\\times h}$, an RTN quantization breaks down M into\nthe quantized matrix Mp, the scaling matrix s, and\nthe zero-point matrix z as follows.\nQuantization Phase:\n$z = min(M), s = \\frac{max_i(M) \u2013 min_i (M)}{2^b \u2013 1}$ (4)\n$M_Q = [\\frac{M-z}{s}]$ (5)\nDequantization Phase:\n$M^* = (M_Q + z) * s$ (6)\nHere, b represents the required bit of quantization,\nand $min_i$ (respectively $max_i$) is a function that"}]}, {"title": "3 Asymmetric Attention Sensitivity of KV\nCache Quantization", "content": "As shown in Equ. 1-Equ. 3, the key matrix and\nvalue matrix perform distinct roles in transform-\ners. While existing studies have proposed intri-\ncate quantization methods to mitigate the loss from\nquantization and some studies (Dong et al., 2024)\nhave recognized the disparate roles of the key ma-\ntrix and value matrix, an important question still\nlingers: provided that the key matrix and value ma-\ntrix play different roles from various perspectives,\nfor instance, the multiplication of xq and the opera-\ntion of softmax function on key matrix, what factor\ntruly contributes to the loss of the transformer?\nObservation. For the key (respectively value) ma-\ntrix, we hold the value (respectively key) matrix in\nfloating type, and evaluate the accumulated mean\nsquared error between the output with key (respec-\ntively value) matrix in floating type and that with\n2-bit quantization at different stages of the atten-\ntion. Fig. 1 illustrates the average loss per element\nduring the inference of the Llama-2 model of size\n7b. Here, the green (respectively red) line denotes\nthe MSE between the attention output with floating\ntype and the 2-bit quantization of the key (respec-\ntively value) matrix in different stages of the atten-\ntion. The number on the lines depicted in Fig. 1\nrepresents the ratio between the MSE that arises\nfrom the key matrix quantization and the MSE that\narises from the value matrix quantization.\nInterestingly, even though the quantization strat-\negy results in a comparative loss (i.e., the MSE\nafter Equ. 6) on the key matrix and value matrix,\nthere emerges marginal gap loss for key matrices\nafter the multiplication of xq, i.e., after Equ. 1. The\ngap is further amplified after the softmax function,\ni.e., after Equ. 2. This indicates that even though\nthe quantization methods can guarantee a similar\nloss for key and value matrices, the multiplication\nof xq and the activation function makes the MSE of\nthe attention output for the key matrix significantly\nlarger than that of the value matrix.\nMSE Amplification. Next, we analyze why the\nmultiplication of xq and the softmax function ex-\nacerbates the MSE of the key matrix. Consider\na matrix M and its quantization matrices, MQ, z,\nand s. M could be either the key matrix or the value\nmatrix. Assume that the deviation of each element\nbetween M and the quantized matrix follows the\ndistribution P, i.e., $|M_{i,j} \u2013 M^*_{i,j}| \\sim P$. We aim\nto understand how the error of an element in the\nmatrix varies after being multiplied by a vector.\nProposition 1. Consider a matrix M and its esti-\nmation M*. Denote the error by E = M \u2013 MQ.\nUpon left multiplying by a matrix A, the error ma-\ntrix becomes AE. Correspondingly, a right multi-\nplication of A results in the error EA.\nProof. Consider the (s, r)-th element of AM. We\ncould obtain its error\n$A_{s,}.M_{.,r} \u2013 A_{s,}.M^*_{.,r}$\n$= \\sum_i A_{s,i} (M_{i,r} - M^*_{i,r})$\n$= \\sum_i A_{s,i}t_{i,r}$ (8)\nwhich precisely corresponds to the (s,r)-th ele-\nment of AE. Similarly, the right multiplication of"}, {"title": "4 AsymKV: Layer-wise Quantization with\nAsymmetric Configuration", "content": "From the discussion in Sec. 3, it is evident that\nthe quantization of the key matrix could potentially\nresult in a more significant loss for the attention\noutput due to the specific role of the key matrix.\nIn response to this, our study introduces AsymKV,\na simple yet efficacious quantization strategy that\nblends various degrees of quantization for the key\nand value matrix based on their respective impacts\non the loss of the attention mechanism.\nBasic Idea. AsymKV applies various degrees of\nquantization to the key and value matrix at the\nlayer level. Specifically, AsymKV introduces two\nparameters, lk and l\u012b, to control the degree of quan-\ntization for the key and value matrix, respectively.\nDuring the inference of the model, for the key (re-\nspectively value) matrix, the initial lk (respectively\nlv) attention layers utilize a quantization method\nwith a higher number of bits (e.g., 4-bit or 2-bit).\nIn contrast, the remaining attention layers employ\na quantization method with fewer bits (i.e., 1-bit).\nFig. 3 illustrates the design of AsymKV where\ngreen, blue, and red blocks symbolize the matrices\nin full-precision, higher-bit quantization, and lower-\nbit quantization, respectively. For each attention\nlayer, its key and value matrices are cached with\ndifferent quantization bits based on the layer index.\nAs demonstrated in Fig. 3, those layers with a layer\nindex i < lk (respectively i < l) will cache the\nquantized key (respectively value) matrices with\nhigher bits, while the other layers will use lower\nbits. After generating the query, key, and value\nmatrix of the current token, i.e., Kt, Qt, and Vt,\nthe LLM will produce the output of the attention\nA\u00ba, as illustrated in Equ. 1-Equ. 3. Given that\nAsymKV chooses lk and lv such that lv \u2264 lk, those\ndecoder layers with indices in range [lv, lk] will\ncontain a blend of higher bits for key matrix and\nlower bits for value matrix.\nThe design of AsymKV relies on the observa-\ntions in Sec. 3 as well as certain intuitive insights.\n(1) Asymmetric Configuration. In light of our\nobservation in Sec. 3, we decide to independently\nconfigure the degree of quantization for key and\nvalue matrices by defining the configuration pa-\nrameters lk and lv, respectively. Besides, since the\nquantization error for the key matrix results in a\nlarger error for the attention output, we generally\nchoose a larger lk than l\u2082 to achieve performance\ncomparable to the models with full precision.\n(2) Layer-wise Quantization. While generating a\ntoken, the quantization error is accumulated as the\nnumber of attention layers increases. Therefore, by\nchoosing the later attention layers to be quantized\nwith fewer bits, we can mitigate the error caused by\nthe quantization from being amplified, while con-"}, {"title": "5 Evaluation", "content": "5.1 Experimental Setup\nTested Models. We examine the performance\nof AsymKV using the widely used LLM fam-\nily Llama (Touvron et al., 2023), which includes\nLlama-2-7b and Llama-2-13b. All models are de-\nployed based on the LLM implementation from\nHuggingface\u00b9 with the default implementation of\nquantization selected from (Liu et al., 2024c).\nTasks and Baselines. In terms of model per-\nformance, we evaluate AsymKV on tasks with\na standard context length, including CoQA and\nTruthfulQA from LM-Eval (Gao et al., 2024),\nas well as tasks with long context length from\nLongBench (Bai et al., 2023), including TriviaQA,\nTREC, SAMSum, RepoBench-P, and Qasper. Re-\ngarding model efficiency, we assess the memory\nusage of AsymKV under various quantization con-\nfigurations, comparing it with previous works that\nhandle the key and value matrices uniformly, in-\ncluding the original floating implementation, and\nKIVI (Liu et al., 2024c) with 2-bit quantization.\nImplementation. Each decoder layer in AsymKV\nadheres to the quantization scheme outlined in\nKIVI (Liu et al., 2024c), that is, per-channel quan-\ntization for the key matrix and per-token quanti-\nzation for the value matrix, with a group size of\n32. AsymKV utilizes a combination of higher 2-\nbit quantization and lower 1-bit quantization. To\nvalidate our analysis concerning the diverse errors\ninstigated by the key matrix quantization and value\nmatrix quantization, we also examine AsymKV\nunder various quantization configurations."}, {"title": "5.2 Evaluation Results", "content": "5.2.1 Tasks with Normal Context Length\nTable 1 presents the experimental results for tasks\nwith normal context length, namely CoQA and\nTruthfulQA. In this case, the model AsymKV-lk/lv\nrepresents AsymKV where the key and value matri-\nces in the first lk and lv attention layers are respec-\ntively quantized with 2-bit, while those in other\nlayers are quantized with 1 bit.\nUpon examining AsymKV with various quanti-\nzation configurations, we observe that AsymKV-\n1https://huggingface.co/\n16/0 (respectively AsymKV-20/0) performs better\nthan AsymKV-0/16 (respectively AsymKV-0/20)\nfor Llama-7b (respectively Llama-13b). This find-\ning aligns with our observation and analysis in\nSec. 3, where the quantization of key matrices\nresults in a higher loss than that of value matri-\nces. Therefore, even though AsymKV-16/0 and\nAsymKV-0/16 occupy the same space in GPU\nmemory, a quantization strategy that employs\nhigher bits for the key matrix and lower bits for\nthe value matrix enhances performance.\nBesides, AsymKV yields performance compa-\nrable to Llama and KIVI while using less GPU\nmemory, achieved by implementing asymmetric 1-\nbit quantization. In particular, AsymKV-16/0 and\nAsymKV-20/0 assures a minimum performance of\n91.0% that of Llama and 92.2% that of KIVI. In\ncontrast to KIVI, which quantizes both key and\nvalue matrices with 2 bits, AsymKV allows for\n75% decoder layers quantized with the extreme 1\nbit, which is more efficient in peak memory."}, {"title": "5.2.2 Tasks with Long Context Length", "content": "Table 2 presents the experimental results for tasks\nwith long context lengths. Mirroring the tasks with\nnormal context length, AsymKV with a higher\nbit count in the key matrix (i.e.,AsymKV-32/0\nfor Llama-7b and AsymKV-40/0 for Llama-13b)\nonce more surpasses AsymKV with value matrices\nquantized with higher bits (i.e., AsymKV-0/32 and\nAsymKV-0/40). This aligns with the reasons as\nillustrated in Sec. 5.2.1.\nBesides, in the case of long context lengths,\nAsymKV necessitates more decoder layers quan-\ntized with higher bits to attain performance com-\nparable to the baselines (lk = 32/40 for long\ncontext length vs. lk = 16/20 for normal con-"}, {"title": "5.2.3 Peak Memory", "content": "Fig. 4 reports the experimental results of the peak\nmemory in GPU for AsymKV. We choose a batch\nsize of 48 for Llama-7b and 36 for Llama-13b, and\nreport the peak storage consumption by varying the\nquantization configurations lk and lv. Specifically,\nwe first set l = 0, implying all value matrices of\nthe decoder layers are quantized with 1 bit, and in-\ncrease the number of key matrices quantized with\n2 bits, i.e., lk, from 0 to the maximum number of\ndecoder layers, illustrated in the left part of Fig. 4a\nand Fig. 4b. Then, we keep all key matrices quan-\ntized with 2 bits and further increase the number\nof value matrices quantized with 2 bits, i.e., lv, as\nshown in the right part of Fig. 4a and Fig. 4b. It\nis noteworthy that when both lk and lv achieve the\nmaximum number of layers, the results correspond\nto the performance of KIVI.\nFrom Fig. 4, as more attention layers are quan-\ntized with higher bits, the consumed space in GPU\nincreases almost linearly until all attention layers\nemploy a quantization configuration with higher\nbits. The locations where AsymKV achieves com-\nparable performance to the floating-point model\non tasks with normal and long context lengths are\nhighlighted. For Llama-7b, AsymKV can ensure\nsimilar performance while saving 9.0 GB and 6.0\nGB of space for the tasks with normal and long con-\ntext lengths respectively, compared to KIVI. For\nLlama-13b, the memory saved increases to 10.4GB\nand 7.0GB space for tasks with normal and long\ncontext lengths respectively."}, {"title": "6 Related Works", "content": "Large language models have gained considerable\nattention since their inception. Despite their impres-\nsive performance, these models are constrained by\ntheir immersive quantity of parameters, which re-\nsults in hardware limitations and poor throughput.\nTo address these issues, recent research trends\nare centered on reducing the size of LLMs (Kim\net al., 2023). Among these methods, quanti-\nzation techniques target the transformation of a\nportion of the model's parameters into integers,\nwhich reduces the space of LLMs. For instance,\nllm.int8 (Dettmers et al., 2022) suggests quantiz-\ning the query, key, and value weights of LLMs\nusing the round-to-nearest method, i.e.,, mapping\neach floating-point number to its closest integer.\nAWQ (Lin et al., 2024) and SmoothQuant (Xiao\net al., 2023) further introduce an amplifying scale\nprior to quantization to prevent extremely large out-\nliers during the process. Omniquant (Shao et al.,\n2023) devises a quantization algorithm by imple-\nmenting a learnable scale and learnable clipping\nduring quantization. GPTQ (Frantar et al., 2022)\nperceives quantization as a problem of minimizing\nsquare error and designing the quantization algo-\nrithms using an approximation of the second-order\ninformation. These studies mainly focus on the\nquantization of the model weights.\nOn the other hand, to mitigate redundant com-\nputations across token generation, LLMs utilizes\nKV cache. While KV cache enhances inference\nefficiency, it consumes significant space, particu-\nlarly when generating long contexts. Consequently,\nanother line of research focuses on the compression\nof KV cache (Zhang et al., 2024; Kwon et al., 2023;\nJin et al., 2023; Liu et al., 2024b). Among these\napproaches, quantization techniques have garnered\nmuch attention and have emerged as a popular tool"}, {"title": "7 Conclusions", "content": "This paper primarily concentrates on the asymmet-\nric roles of the key and value matrices in the quan-\ntization of the KV cache. We analyze why quan-\ntizing the key matrix leads to a more significant\nperformance drop than quantizing the value matrix\nand attribute it to the multiplication of xq and the\nimplementation of the softmax function. Based on\nthis analysis, we introduce AsymKV, which applies\nasymmetric and layer-wise quantization configu-\nrations to the key and value matrices. AsymKV\nfacilitates a mixed quantization approach with 2\nbits and 1 bit, while simultaneously ensuring per-\nformance comparable to the floating-type model.\nExtensive experiments validate our analysis of the\nasymmetric roles of the key and value matrices."}, {"title": "8 Limitations", "content": "Despite AsymKV facilitating the quantization of\n1 bit for KV cache in LLMs, it still depends on\nexhaustive testing to identify the optimal config-\nurations for different LLMs, i.e., configurations\nthat yield performance close to models in floating\ntypes. This approach is relatively inefficient. A\npotential futural direction could involve efficiently\nidentifying the optimal configurations for LLMs."}, {"title": "A Supplemental Experiments", "content": "In this section, we present the complete experimental setup and the corresponding results."}, {"title": "A.1 Experimental Settings", "content": "Inference Settings. Following KIVI (Liu et al., 2024c), AsymKV employs per-channel quantization for key matrices and per-token quantization for value matrices. Consequently, both KIVI and AsymKV store the key matrices of a limited number of tokens in floating-point types, a parameter referred to as residual length. We choose a residual length of 128 for tasks with normal context length, while for tasks with long context length, we opt for a residual length of 512. For the peak memory usage experiments, we standardized the generation length of tokens to 4096."}, {"title": "A.2 Experimental Results", "content": "Implementation. AsymKV is implemented using PyTorch and is built upon the Huggingface codebase. All experiments are executed on a machine equipped with 200GB memory and an A800 GPU with 80GB memory."}, {"title": "A.2.1 Results on Tasks with Normal Context Length", "content": "Table 3 proposes the performance of AsymKV with varying $l_k$ and $l_v$ values for tasks with normal context length. For Llama-7b, we choose $l_k, l_v \\in {6,12,16, 20}$ and for Llama-13b, we consider $l_k, l_v \\in {5, 10, 20, 30}$.As the number of decoder layers quantized with higher bits increases, the performance of AsymKV improves until it reaches performance levels comparable to the floating-point model and KIVI. Besides, we observe that AsymKV with value matrices quantized using lower bits, i.e.,AsymKV-1/0, consistently outperforms AsymKV with key matrices quantized using lower bits, i.e.,AsymKV-0/l, and the difference is substantial. This observation confirms that choosing a configuration with $l_k > l_v$ can enhance the performance of AsymKV. AsymKV can achieve at least 90% of the performance of floating-point models when a quantization configuration that follows AsymKV-16/0 for Llama-7b and AsymKV-20/0 for Llama-13b is utilized."}, {"title": "A.2.2 Results on Tasks with Long Context Length", "content": "Table 4 presents the experimental results for tasks with long context length. For key and value matrices, we set aside one type of matrices quantized with higher bits (i.e., $l_k/l_v = 32/40$) and vary the number of the other type of matrices that are quantized with lower bits.Similar to the tasks with normal context lengths, the performance of AsymKV augments as more key and value matrices are quantized with higher bits. Besides, AsymKV with key matrices quantized with higher bits (AsymKV-32/$l_v$ for Llama-7b and AsymKV-40/$l_v$ for Llama-13b) outperforms AsymKV with value matrices quantized with higher bits, despite them occupying the same GPU memory."}]}