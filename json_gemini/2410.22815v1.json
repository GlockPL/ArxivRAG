{"title": "Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients", "authors": ["Jabin Koo", "Minwoo Jang", "Jungseul Ok"], "abstract": "Federated fine-tuning for Large Language Models (LLMs) has recently gained attention due to the heavy communication overhead of transmitting large model updates. Low Rank Adaptation (LoRA) has been proposed as a solution, yet its application in federated learning is complicated by discordance in aggregation. Existing methods addressing this discordance often suffer from performance degradation at low ranks in heterogeneous data settings. In response, we introduce LORA-A2 (Low Rank Adaptation with Alternating freeze and Adaptive rank selection), which demonstrates robustness in challenging settings with low ranks and high data heterogeneity. Our experimental findings reveal that LoRA-A2 maintains performance even under extreme heterogeneity and low rank conditions, achieving up to a 99.8% reduction in uploaded parameters compared to full fine-tuning without compromising performance. This adaptive mechanism boosts robustness and communication efficiency in federated fine-tuning, enabling the practical deployment of LLMs in resource-constrained environments.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), exemplified by ChatGPT (OpenAI, 2024), Llama (Dubey et al., 2024) and others, represent a hallmark of the current era. These models are being widely applied in real-world scenarios by fine-tuning them on various task-specific datasets (Dodge et al., 2020). With the expansion of edge devices, the potential to leverage rich, privacy-sensitive data for fine-tuning LLMs has shifted the focus toward federated fine-tuning. Despite its potential, this is often infeasible due to the large size of LLMs, which require extensive computational and communication resources from local devices.\nParameter-Efficient Fine-Tuning (PEFT) methods (Lester et al., 2021; Liu et al., 2022) are increasingly being explored in the context of federated fine-tuning. Among these, Low-Rank Adaptation (LORA) (Hu et al., 2021) is particularly noteworthy for its significant reduction in number of communicated parameters. However, naive application of LoRA in Federated Learning (FL) (McMahan et al., 2017) environment comes with several challenges such as aggregation discordance. Although several solutions have been proposed, they often remain vulnerable to high heterogeneity and low ranks, making it difficult to reduce rank size for communication efficiency in realistic FL scenarios.\nTo address this, we introduce LoRA-A2 (Low Rank Adaptation with Alternating freeze and Adaptive rank selection), which is robust to both high heterogeneity and low ranks. LoRA-A2 incorporates two main strategies: alternating freeze, which switches between freezing LoRA modules B and A in each round, and adaptive rank selection, which identifies and updates only important ranks in LoRA modules. We conduct experiments across various rank sizes and heterogeneity levels, comparing our algorithm with multiple baselines. Through the experiments, we reveal the vulnerabilities of existing methods and highlight the robustness of LoRA-A2 in challenging conditions, providing analyses of the reasons for its robustness. Additionally, we empirically demonstrate that our approach achieves performance comparable to or exceeding that of full fine-tuning, while uploading less than 0.2% of parameters to the server.\nOur contributions can be summarized as follows:\n\u2022 We address the vulnerabilities of previous federated LoRA methods in high heterogeneity and low-rank settings, and propose a novel algorithm, LoRA-A2, which demonstrates robustness in these challenging conditions.\n\u2022 Our algorithm effectively reduces communi-"}, {"title": "2 Related Works", "content": "LORA with adaptive rank selection LoRA (Hu et al., 2022) is a widely used PEFT method for LLMs. It tries to approximate the updated part of the pre-trained model with two smaller size of matrices. This approach is inspired by previous studies (Li et al., 2018; Aghajanyan et al., 2021), which suggest that newly learned parameters for adaptation lie within a low dimensional subspace. AdaLoRA (Zhang et al., 2023) assumes a scenario where the total parameter budget is limited. It adaptively selects the rank for each LoRA adapter under this constraint, with a criterion for rank selection based on singular values of the updated part. ALORA (Liu et al., 2024) utilizes a router for each LoRA adaptor. The router determines which part of each LoRA adaptor should be either turned on or off, enabling efficient fine-tuning via pruning. Similarly, DORA (Mao et al., 2024) re-splits LoRA into smaller groups of LoRAs. During the training session, it estimates the importance of each small LORA, allowing the parts with less contribution across the whole LoRA to be pruned. Our research extends this adaptive rank selection in centralized learning so that each client adaptively selects different ranks suitable for their own dataset.\nFederated learning with LoRA As training LLMs on mobile devices becomes feasible, fine-tuning LLMs via FL has recently gained attention. In line with this trend, using LoRA for federated fine-tuning (Babakniya et al., 2023; Kuo et al., 2024; Wang et al., 2024), is also being considered. However, simply adopting LORA for FL presents several obstacles, which are discussed in Section 3. HetLORA (Cho et al., 2023) assumes that each client may have different computational power, which is a common scenario in FL. Based on this assumption, it allows each client to use a LORA adapter of varying sizes. Zero-padding is then applied to equalize the LoRA sizes for aggregation. Sun et al. (2024) point out that aggregating the two matrices of a LoRA adapter separately cannot fully approximate the original LoRA adapter. Based on this finding, they propose FFA-LORA, which addresses this issue by freezing half of each LORA throughout the entire fine-tuning session. FlexLoRA (Bai et al., 2024) aggregates the product of two matrices that make up each LoRA adapter and then decomposes the aggregated parameters back into two smaller matrices via singular value decomposition. This approach allows FlexLORA to overcome the challenges addressed by HetLoRA and FFA-LoRA, respectively, though at the cost of increased computational cost on the server-side for the decomposition process."}, {"title": "3 Problem Formulation", "content": "Low rank adaptation Because LLMs have billions of parameters, fine-tuning them for specific domains demands significant computational power, which may be infeasible in many situations. To address this issue, PEFT techniques such as LORA (Hu et al., 2022) have recently gained attention, as they can effectively reduce the number of parameters that need to be trained. Specifically, when fine-tuning a pre-trained weight matrix $W_o \\in \\mathbb{R}^{d_1 \\times d_2}$ to obtain W, LoRA achieves this by decomposing $\\Delta W$, the update of the weight matrix, into smaller matrices $B \\in \\mathbb{R}^{d_1 \\times r}$ and $A \\in \\mathbb{R}^{r \\times d_2}$:\n$W = W_o + \\Delta W = W_o + BA$, (1)\nwhere $r < \\{d_1, d_2\\}$ denotes the rank of LoRA. With this approximation, the number of trainable parameters is reduced from $d_1 \\cdot d_2$ to $r \\cdot (d_1 + d_2)$.\nFederated LoRA and discordance problem\nConsider a global pre-trained model $W_o$ and a set of clients $\\{1, 2, \\dots , K\\}$. The objective in federated fine-tuning is to update $W_o$ to obtain a model $W$ that is suitable for local datasets $D_k$. However, fine-tuning LLMs is very expensive for local devices in terms of both computation and communication, as billions of parameters must be trained and transmitted in each round.\nLORA presents a promising approach in FL for reducing communication costs, as only B and A are trained and transmitted, allowing the number of communicated parameters to be linearly reduced by the rank r of LoRA modules. However, the straightforward application of LoRA in FL introduces a significant issue known as discordance (Sun et al., 2024), primarily due to aggregation algorithms. In methods like FedAvg (McMahan et al., 2017), where each weight is aggregated individually, discordance occurs between the actual and aggregated parameters. That is,\n$\\sum_{k=1}^K w_k \\Delta W_k = \\sum_{k=1}^K w_k B_k A_k = (\\sum_{k=1}^K w_k B_k) (\\sum_{k=1}^K w_k A_k)$, (2)\nin general, where $\\sum_{k=1}^K w_k = 1$ with $w_k > 0$ for all $k \\in [K]$. One might consider aggregating $\\Delta W_k = B_k A_k$ directly to eliminate the discordance, but this approach entails decomposing $\\Delta W = \\sum_{k=1}^K w_k \\Delta W_k$ back into B and A for the next round, which is computationally unstable and non-trivial.\nLimitation of existing methods with low rank and high data heterogeneity This discordance can be easily handled by various methods presented by previous works, However, they all have weakness to high heterogeneity and low ranks. For instance, FFA-LORA (Sun et al., 2024), which is one of major work handling discordance problem in federated LoRA, let A be frozen to be $A_o$ and shared across all the clients, so that\n$\\sum_{k=1}^K w_k B_k A_o = (\\sum_{k=1}^K w_k B_k) (\\sum_{k=1}^K w_k A_o)$, (3)\nthanks to the distributive law.\nHowever, Figure 2 indicates that the straightforward approach of freezing A leads to degraded performance, particularly in scenarios with high data heterogeneity and lower ranks. We suggest that this freezing method constrains the optimization space and reduces the number of available parameters, causing increased conflicts among them.\nFlexLORA (Bai et al., 2024), which can also address the discordance problem by directly aggregating $\\Delta W$ instead of aggregating LoRA modules, exhibits significant performance drops at lower ranks, as shown in figure 2. This is due to large approximation error from singular value decomposition at reduced ranks. This vulnerability to high heterogeneity and lower rank means that previous methods can not reduce LoRA ranks for communication efficiency without severely damaging performance in realistic FL scenarios with data heterogeneity."}, {"title": "4 Proposed Method", "content": "To tackle the identified challenges, we propose a novel framework called Low Rank Adaptation with Alternating freeze and Adaptive rank selection for federated learning, or LoRA-A2, for communication efficient FL with LoRA. LoRA-A2 adaptively selects LoRA ranks for training and communication. And it trains and transmits only the selected part of each adaptor in an alternating way.\n4.1 Alternating Freeze\nLoRA-A2 efficiently addresses the issue of discordance by employing a simple alternating freeze technique to train the LoRA modules B and A. Instead of solely training module B while keeping module A frozen permanently, as suggested by FFA-LORA (Sun et al., 2024), LoRA-A2 alternates between the two: LORA module A is frozen during even rounds, while module B is frozen during odd rounds. This method preserves the optimization space while effectively resolving discordance. Specifically, when freezing A, we have\n$\\Delta W = \\sum_{k=1}^K (w_k B_k) A = \\sum_{k=1}^K (w_k B_k A_k) = \\sum_{k=1}^K (w_k \\Delta W_k)$, (4)\nand the same applies when freezing B. In this way, LoRA-A2 trains both B and A, ensuring that A does not remain the same as its initial value.\nTo further enhance the effect of alternating optimization, we adopt different learning rates for B and A, inspired by LoRA+ (Hayou et al., 2024). Figure 6 demonstrates the effectiveness of alternating freeze and learning rate adjustment.\n4.2 Adaptive Rank Selection\nTo further improve robustness to lower ranks and data heterogeneity, we employ an adaptive rank selection strategy. This approach selects important LORA ranks to match local communication rank budget $r_i$ out of global LoRA adapter with rank $r_g$ adaptively based on the local dataset. We mainly focus on communication cost for uploading parameters to the server as it is known that upload bandwidth is generally much slower than download bandwidth and is the major part of communication cost (Kone\u010dn\u00fd et al., 2017; Suresh et al., 2017; Kairouz et al., 2021). The adaptive rank selection process provides two key benefits: it minimizes client conflicts by allowing different clients to choose different LoRA ranks in high heterogeneity, and reallocates rank resources from unimportant LoRA modules to modules that require more fine tuning which is especially effective when communication rank budget is small.\nTo quantify which ranks are more important, we introduce our novel criterion, which measures the importance of ranks in each module by calculating how much each rank contributes to the change in $\\Delta W$. We define the importance score of rank i in module m, denoted as $S_{m,i}$, as follows:\n$S^{B_k}_{m,i} = || \\Delta B_k [:,i] \\Delta A_k [i,:] ||_F$\n$S^{A_k}_{m,i} = ||B[:,i] \\Delta A_k [i,:] ||_F$ (5)\nWe have also tried simple magnitude based (Frankle and Carbin, 2019) and sensitivity based importance score (Lee et al., 2019), demonstrating that our proposed importance score outperforms these alternatives. The details of the ablation study on importance scores are provided in Section 5.5.\nAfter computing $S^{B_k}_{m,i}$ or $S^{A_k}_{m,i}$ for each module m, we select top-($r_i \\cdot N$) LoRA ranks from a total of $r_g \\cdot N$ based on the scores across the entire model, where N denotes the number of target modules across all the layers of the base model. We refer to the set of selected ranks of client k as $R_k$.\nOnce the ranks are selected, each client defines LoRA module mask $M_k^{(m)}$ for the module m to be\n$M_k^{(m)} [i,:] = \\begin{cases} 1_{d_1} & \\text{if } i \\in R_k \\\\ 0_{d_1} & \\text{otherwise} \\end{cases} ,$\n$M_k^{(m)} [:,i] = \\begin{cases} 1_{d_2} & \\text{if } i \\in R_k \\\\ 0_{d_2} & \\text{otherwise} \\end{cases} ,$ (6)\nwhich is producted element-wise to the updated part of $B_k$ (or $A_k$). That is, before each backpropagation, LORA-A2 calculates\n$\\Delta B_k^{(m)} \\leftarrow \\Delta B_k^{(m)} \\odot M_k^{(m)}$\n$\\Delta A_k^{(m)} \\leftarrow \\Delta A_k^{(m)} \\odot M_k^{(m)}$ (7)\nfor each $B_k$ (or $A_k$), where the notation $\\odot$ stands for the Hadamard product. After each local training, each client uploads $B_k \\odot M_k$ (or $A_k \\odot M_k$), resulting in sparsification and reducing the number of uploaded parameters. Then, the server aggregates the uploaded ones, which are again added to the $B_k$ (or $A_k$) saved two rounds before. Algorithm 1 and 2 provide the pseudocode of LoRA-A2."}, {"title": "5 Experiments", "content": "In this section, we evaluate the performance of our algorithm against existing FL methods combined with LoRA across various heterogeneity settings and datasets. We assess performance based on accuracy and the total number of uploaded parameters.\n5.1 Experimental Settings\nAcross all experiments, we utilize RoBERTa-base (Liu et al., 2019) pre-trained model as the base model. For fine-tuning, we choose BANKING77 (Casanueva et al., 2020) and 20 Newsgroups (Lang, 1995) datasets for fine-tuning the base model. BANKING77 is an intent classification dataset with 77 fine-grained intents related to the banking domain, comprising 10,003 training samples and 3,080 test samples. 20 Newsgroups is a widely used text classification dataset with 20 classes, each representing a unique topic. It contains 11,314 training samples and 7,532 test samples. To simulate various levels of data heterogeneity, we employ a Dirichlet distribution following methodologies from prior research, such as (Hsu et al., 2019). For our experiments, we choose Dir(0.5), Dir(0.1) and Dir(0.01). Dataset statistics for different levels of heterogeneity are reported in Appendix A.\nUnless otherwise stated, we trained 30 local clients, assuming a full participation setting, i.e., $\\mathcal{K}^{(t)} = [\\mathcal{K}]$ for all $t \\in [\\mathcal{T}]$. The total number of communication round T is set to be 50, and each client trains 5 epochs locally before each communication round. Detailed hyperparameters for experiments are specified in Appendix B.\nFor baselines, we adopt four methods that utilize LORA for federated fine-tuning: FL + LoRA, FFA-LORA (Sun et al., 2024), FlexLoRA (Bai et al., 2024), and HetLoRA (Cho et al., 2023), where FL + LORA stands for the naive implementation of LORA in FedAvg (McMahan et al., 2017).\n5.2 Main results\nWe compare our algorithm with the baseline methods under various data heterogeneity settings in BANKING77 and 20 Newsgroups datasets to demonstrate that our algorithm outperforms previous federated LoRA fine-tuning methods across different non-IID settings and LoRA ranks.\nRobustness of LoRA-A2 in low ranks and high heterogeneity Table 1 highlights the vulnerability of previous methods under conditions of high heterogeneity and low ranks. The accuracy of baseline methods declines significantly as rank decreases, whereas our algorithm maintains its performance, achieving up to a 23% accuracy advantage. This suggests that reducing LoRA ranks is chal-"}, {"title": "5.3 Analysis on Adaptive Rank Selection", "content": "In this section, we visualize the process of our adaptive rank selection, and explore how we efficiently train and send important ranks, highlighting the robustness of our algorithm in heterogeneous and low rank environments. To simulate extreme cases of both identical and different client distributions, we test our algorithm on a pathological toy dataset using the 20 Newsgroups dataset. In this setup, 20 clients each holds data from only two classes, with consecutive pairs sharing the same classes, while others do not. For instance, clients 0 and 1 have classes \"medical\" and \"space,\" whereas clients 2 and 3 have \"motorcycle\" and \"religions\". Detailed settings are shown in Appendix C.\nRobustness to low rank by Adaptive Module Selection In this experiment, our algorithm selects 2 $\\cdot N^{(m)}$ ranks from a total of 16 $\\cdot N^{(m)}$ across the whole ROBERTa model, guided by our importance criterion, and visualizes the adaptive selection of modules. Figure 3 illustrates the number of ranks selected for each module in the model during the training. The figure shows that most modules are allocated with zero ranks, indicating either no need for fine-tuning or the insignificance of updates on those modules. This suggests that our adaptive rank selection automatically prunes out modules that do not require additional fine-tuning.\nTo further justify that our adaptive rank selection adequately selects important modules, we conduct an ablation study on module selection, akin to the approach in AdaLoRA (Zhang et al., 2023) but in a federated environment. Figure 5 displays the"}, {"title": "5.4 Experiments on Resource Heterogeneity", "content": "In this section, we assume that each client has a different communication cost budget (Chen et al., 2023). For example, some clients might use smartphones with Wi-Fi, while others may use 3G networks for federation. We aim to allow each client to have its own rank for the LoRA adapter, allowing clients with lower budgets to participate in training. In Table 2, we compare our method with HetLoRA and FlexLoRA, two previous LoRA methods that can handle resource heterogeneity in FL. Detailed experimental setup can be found in Appendix B."}, {"title": "5.5 Ablation Studies", "content": "Through these ablation studies, we show empirical evidence for our engineering choices on aggregation tactics and rank selection criteria.\nEfficacy of alternating freeze To address the discordance problem in federated LoRA aggrega-"}, {"title": "6 Conclusion", "content": "In this work, we tackle the vulnerability of previous methods in high heterogeneity and low ranks by proposing a novel algorithm, LoRA-A2, which shows robustness in these challenging conditions with alternating freeze and adaptive rank selection. Our approach offers significant improvements in communication efficiency without compromising performance, as demonstrated by a reduction of 99.8% in parameter uploads compared to full fine-tuning. Through extensive experiments, we establish LoRA-A2 as a superior alternative, providing a practical pathway for efficient and effective federated fine-tuning in diverse and resource-constrained environments."}, {"title": "7 Limitations", "content": "LORA-A2 shows promising results and we plan to distribute the implementation code with detailed instructions for reproducibility. However, several areas remain open for future exploration.\nFirst, our work mainly focuses on classification tasks, primarily due to computational constraints and the use of Dirichlet distribution to simulate non-IID conditions. However, extending LoRA-A2 to more complex tasks, such as natural language generation, could offer additional perspectives. Future work with more resources could explore these broader applications.\nSecond, our experiments are primarily conducted on comparatively smaller language model, ROBERTa-base, due to limited computation resources. Applying LoRA-A2 to larger models, such as LLaMA or GPT-style architectures, could provide an opportunity to test its scalability. Investigating how well the method handles the increased parameter space of these state-of-the-art models could further demonstrate its efficiency.\nThird, integrating Differential Privacy (DP) into LORA-A2 represents a key direction for future research. Not only would this strengthen privacy protections for user data, but it would also highlight the benefits of our approach, as the discordance problem, which arises from inconsistencies in model aggregation, becomes more severe when applying DP. This challenge has been recognized by methods like FFA-LoRA, and LoRA-A2 could provide a more robust solution under these conditions.\nFinally, due to the limited access to real world datasets, our current results are mainly based on simulated settings. Extensive research on real world dataset, which typically exhibit more diverse types of noise and heterogeneity would help understand performance and robustness of LoRA-A2 in practical, dynamic environments."}, {"title": "A Dataset Statistics", "content": "We provide the statistics of two datasets, 20 newsgroups (Lang, 1995) and BANKING77 (Casanueva et al., 2020), in Table 4 and Table 5, respectively. $\\mathcal{D}_k$ and $\\mathcal{C}_k$ denotes the local dataset of k and the number of unique classes in $\\mathcal{D}_k$, respectively. Figure 7 shows the distribution of a local dataset for varying $\u03b1$ simulating the Dirichlet distribution."}, {"title": "B Reproducibility", "content": "Hyperparameters When training, we use AdamW (Loshchilov and Hutter, 2019) optimizer with a learning rate of $\u03b7 = 0.0005$. For LoRA-A2, since B and A of each LoRA module are optimized separately, we use different learning rates for them. Specifically, $\u03b7_A = \u03b7$ is used for A and $\u03b7_B = 5 \\cdot \u03b7_A$ is used for B, which is inspired by LoRA+ (Hayou et al., 2024). For HetLoRA, $\u03b3 = 0.99$ is used for the decaying factor as suggested by Cho et al. (2023). When evaluating, we merge the LoRA adapter $\u0394W$ with the pre-trained model $W_0$ using a scaling factor, so that $W_{ft} = W_0 + 16\u0394W$."}, {"title": "C Additional Experiments", "content": "Pathologic Setting Table 6 provides experiments on pathologic setting, which is also used to generate Figure 4 in Section 5.3, to show the efficacy of adaptive rank selection. In this setting, we have $\\mathcal{K} = 20$ clients. And client $(2k \u2013 1)$ and client $(2k)$ exclusively possess half of class $(2k \u2013 1)$ and $(2k)$ of 20 Newsgroups datasets, respectively, for $k = 1,2,\\dots,10$."}]}