{"title": "Never Reset Again: A Mathematical Framework for Continual Inference in Recurrent Neural Networks", "authors": ["Bojian Yin", "Federico Corradi"], "abstract": "Recurrent Neural Networks (RNNs) are widely used\nfor sequential processing but face fundamental limitations with\ncontinual inference due to state saturation, requiring disruptive\nhidden state resets. However, reset-based methods impose syn-\nchronization requirements with input boundaries and increase\ncomputational costs at inference. To address this, we propose\nan adaptive loss function that eliminates the need for resets\nduring inference while preserving high accuracy over extended\nsequences. By combining cross-entropy and Kullback-Leibler\ndivergence, the loss dynamically modulates the gradient based\non input informativeness, allowing the network to differentiate\nmeaningful data from noise and maintain stable representations\nover time. t our reset-free approach outperforms traditional reset-\nbased methods when applied to a variety of RNNs, particularly\nin continual tasks, enhancing both the theoretical and practical\ncapabilities of RNNs for streaming applications.\nIndex Terms-RNNs, state saturation, State Resetting, Loss\nFunction", "sections": [{"title": "I. INTRODUCTION", "content": "Recurrent neural networks (RNNs) constitute the foundation\nfor processing sequential data in deep learning and are the\nnatural choice for time series modelling [1], [2]. Over time,\nthese architectures have evolved into specialized variants de-\nsigned to address a variety of computational tasks. Notably,\nGated Recurrent Units (GRUs) [3] optimize gradient flow,\nLinear Recurrent Networks including State Space Models\n(SSMs) [4]-[6] enable efficient and parallel training of large\ndatasets, and Spiking Neural Networks (SNNs) implement\nbiologically plausible, energy-efficient computation for edge\ndevices [7], [8]. Currently, RNN principles are applied to\nmodern Large Language Models (LLMs), which, although\nbased on transformer architectures [9], act as RNNs during\ninference by processing tokens sequentially with cached states\nfor computational efficiency [10], [11]. Despite these architec-\ntural advancements in RNNs, fundamental challenges remain\nwhen continuously processing input data streams. In these\nscenarios, networks are required to generate stable representa-\ntions and accurate predictions while processing uninterrupted\ndata streams that substantially exceed typical training sequence\nlengths. This must be achieved without compromising infer-\nence performance.\nA major challenge in continual inference settings is the state\nsaturation problem, which significantly limits the practical\ndeployment of RNNs [12], [13]. This challenge originates\ndespite their theoretical Turing completeness, which highlights\ntheir potential to solve a diverse range of tasks [14], [15]. This\nphenomenon manifests as progressive accuracy degradation\nduring prolonged input exposure, affecting all RNN variants:\ntraditional architectures, SNNs, and even LLM models [12],\n[13]. The degradation is particularly apparent in continuous\nspeech recognition and streaming tasks [16], [17], where\nperformance declines gradually with exposure to long input\nspeech sequences.\nThe mechanism that leads to state saturation is the temporal\naccumulation of information within the hidden states. During\ncontinual operation, this accumulation generates interference\nbetween historical and present information, progressively de-\ngrading the network's capacity to process new inputs [12],\n[13], [18]. Recent theoretical advances have provided insights\ninto this limitation. Merrill [12] used language theory to\nformally analyze the behavior of saturated networks, while\nPanahi et al. [19] explained the implicit additive properties\nof RNNs by incorporating biologically-inspired saturation\nbounds. The significance is further highlighted by Paassen et\nal.'s [20] that have demonstrated that RNNs can emulate any\nfinite state machine by mapping neurons to states. Current\nmethods to address state saturation involve state collapse\nprevention via continual pre-training on longer sequences [13]\nand dynamic state resets using an extra action selection circuit\n[17]. Other methods use periodic or dynamic hidden state\nresets but struggle with the need for synchronization with input\nboundaries, often missing in real-world scenarios [21]. While\ndistinct sample boundaries are manageable in controlled set-\ntings, they're typically absent in practical streaming and edge\ndeployments [17], [22]. Despite these efforts, state saturation\nstill challenges RNNs during long inferences, impacting their\nability to process continuous data streams reliably.\nThese observations motivate our central research question:\n\"Can we develop a method for training RNNs that\neliminates the need for resetting during continual\ninference?\"\nThis study offers a mathematical approach to the state sat-"}, {"title": "II. BACKGROUND", "content": "The sequential modelling performance of recurrent architec-\ntures is fundamentally constrained by and also benefits from\ntheir hidden dynamics [12], [13], [20]. This section examines\nthe theoretical foundations and use of state reset mechanisms,\ncontextualizing their role in mitigating saturation phenomena.\nSequential data processing in deep learning is described\nby the general recurrent framework $h_t = f(h_{t-1},x_t)$, where\n$h_t$ represents the hidden state at time t, and f(.) denotes a\nlearnable state transition function operating on the previous\nstate $h_{t-1}$ and current input $x_t$. This framework is commonly\nused in RNNs and for applications in many computational\ntasks. For example, vanilla RNNs implement this frame-\nwork through $h_t = tanh(W_h x_t + U_h h_{t-1} + b_h)$, where\n${W_h, U_h,b_h}$ constitute the learnable parameters. To solve\nthe vanishing gradient problem inherent in vanilla RNNs, GRU\nunits have been introduced with adaptive gating mechanisms:\n$r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)$ and $z_t = \\sigma(W_z x_t +\nU_z h_{t-1}+b_z)$, facilitating controlled information flow through\n$h_t = (1-z_t)h_{t-1}+z_t tanh(W_h x_t +U_h(r_t \\odot h_{t-1})+b_h)$.\nAdditionally, SSMs introduce a continuous-time perspective\nbecause they formulate the time dynamics as $h(t) = Ah(t)+\nBx(t), y(t) = Ch(t)+Dx(t)$, where ${A, B, C, D}$ represent\nlearnable state-space parameters for long-range dependency. In\naddition, SNNs introduce bio-inspired principles and temporal\nprocessing by relying on the neuron's membrane potential\ndynamics $u_t = au_{t-1}+\\sum_i W_i s_{i-1}$, coupled with a threshold-\nbased spike generation mechanism $s_t = H(u_t - \\theta)$, where\nH denotes the Heaviside function and $\\theta$ represents the firing\nthreshold. Each architectural variant in this unified framework\noffers unique characteristics in modeling capacity, biological\nplausibility, and computational efficiency, allowing for tailored\nsolutions in sequential data processing."}, {"title": "A. Formalization of Reset Mechanisms", "content": "In general, Reset techniques in RNNs can be formalized by\nstate transformation operations. The reset function $\\Phi : R^d \\to$\n$R^d$ operates on hidden states $h_t \\in R^d$ according to:\n$h_t = \\Phi(h_{t-1},r_t) = r_t \\odot h_{t-1}$  (1)\nwhere $r_t \\in {0,1}^d$ denotes a binary reset mask, and $\\odot$\ndenotes the Hadamard product. This formulation requires a\npriori knowledge of sequence boundaries\u2014an assumption that\nproves problematic in continuous processing paradigms [24],\n[25].\nThis reset operation breaks the recurrent dynamic by enforc-\ning $h_t\\leftarrow h_0$, where $h_0$ typically follows either zero initializa-\ntion ($h_0 = 0$) or random initialization ($h_0 \\sim N(0, \\sigma^2 I)$). The\nDefault reset method in this paper is random initialization."}, {"title": "B. Gated Architectures and Soft Reset Functions", "content": "Some recurrent neural network architectures incorporate\nlearnable reset mechanisms through gated structures [3], [26].\nFor example, the GRU [3] introduces a reset gate formulation\nas follows:\n$r_t = \\sigma(W_r [h_{t-1};x_t] + b_r)$  (2)\nwhere $W_r\\in R^{d \\times (d+n)}$ and $b_r\\in R^{d}$ are the learnable\nparameters, and [\u00b7;\u00b7] denotes vector concatenation. The same\nidea extends to LSTMs [26] that exploit a forget gate mech-\nanism with context information, which learns when to reset\nthe network state during continual input processing, learning\nlonger temporal dependencies from the data.\nRecent methods introduce surprisal-driven feedback [27]\nand binary input gated [28] mechanisms in recurrent networks\nto enable effective resetting of predictions and selective pro-\ncessing during interference. However, these solutions address\na different problem because they focus on learning more\ncomplex temporal patterns and long-term dependencies, rather\nthan tackling the issue of state saturation during inference in\ncontinuous operations [29]."}, {"title": "C. Theoretical Framework", "content": "The theoretical foundations of reset operations can be\nexamined using dynamical systems theory. Current theories\nfocus more on discrete-time scenarios, leaving continuous\noperational dynamics an area of ongoing exploration [14],\n[15], [30]. Let H denote the hidden state manifold, where\nthe reset operation induces a transformation $T : \\mathcal{H} \\to \\mathcal{H}$.\nTo establish the mathematical necessity of hidden state resets\nin RNNs, where $T(h_t) = h_0$, we examine two within this\ndynamical systems 1:\n1) Lyapunov Stability: A fixed point $h^* \\in \\mathcal{H}$ exhibits\nLyapunov stability [30] if, for every $\\epsilon > 0$, there exists\n$\\delta > 0$ such that:\n$||h_0 - h^*|| < \\delta \\implies ||h_t - h^* || < \\epsilon, \\forall t \\ge 0$.  (3)\nThe reset operation to $h_0$ maintains the system state\nwithin a stable neighborhood of fixed point $h^*$, elim-\ninating the accumulation of computational artifacts that\ncould misleading future processing."}, {"title": "III. THEORY OF RESET-FREE LOSS", "content": "In this section, we present the detailed formulation of\nthe proposed loss function and explain how it addresses the\nstate saturation problem in RNNs during continual inference\nwithout the need for resetting the hidden states.\nConsider a sequence of input data ${x_t}_{t=1}^T$ and correspond-\ning target labels ${y_t}_{t=1}^T$, where T is the sequence length. The\nRNN processes the input sequence to update hidden states\n${h_t}_{t=1}^T$ and outputs probability distributions over C classes\nat each time step t:\n$p_t = [p_{t,1}, p_{t,2},...,p_{t,c}] = softmax(W_o h_t + b_o),$ (6)\nwhere $W_o$ and $b_o$ are the output weight matrix and bias\nvector, respectively.\nResetting hidden states in traditional RNNs is done before\nnew input sequences.\nTo address this, hidden states are reset (see Section II-C)\nto enhance relevant information and ensure $p_t$ focuses on the\ncurrent input $x_t$ with a random state $h_0$. Thus, to avoid explicit\nstate reset, we need to formulate a loss function that enables\nthe model to learn from data and adjust output probabilities\namidst noise without resetting the hidden states, ensuring\ntemporal coherence. Our objectives are:\n\u2022 Accurately predict the true class $y_t$ when $m_t = 1$.\n\u2022 Output a uniform distribution over classes when\n$M_t$\n= 0, indicating maximum uncertainty, effectively\nresetting the output probabilities without reinitialize the\nhidden states.\nWe firstly introduce a mask ${m_t}_{t=1}^T$ that indicates whether\neach time step t corresponds to informative data ($m_t = 1$) or\nnoise ($m_t = 0$). Then, we define the total loss $L_{total}$ with two\nparts:\n1) Categorical Cross-Entropy Loss for informative inputs\n($m_t = 1$):\n$L_{CE}(t) = - \\sum_{k=1}^C Y_{t,k} log p_{t,k}$  (7)\n2) Kullback-Leibler Divergence Loss towards a uniform\ndistribution for noise inputs ($m_t = 0$):\n$L_{KL}(t) = D_{KL}(p_t || u) = \\sum_{k=1}^C p_{t,k} log(\\frac{p_{t,k}}{u_k})$  (8)\nwhere u is the uniform distribution over classes, i.e.,\n$u_k = \\frac{1}{C}$ for all k.\nFinally, the total loss over the sequence is given by:\n$L_{total} = \\sum_{t=1}^T [m_t\\cdot L_{CE}(t) + (1 - m_t) \\cdot L_{KL}(t)]$ .  (9)\nOur proposed objective function adaptively initializes states\nin recurrent architectures using a dual optimization. It jointly\nminimizes the cross-entropy while constraining the hidden\nstate distribution via Kullback-Leibler divergence regulariza-\ntion. This approach lets the network adjust its internal states\nautomatically, preventing state saturation without needing ex-\nplicit resets. Moreover, this ensures the model retains hidden\nstates to smoothly transition from noisy input to new data,\navoiding saturation and ensuring continuous gradient flow."}, {"title": "IV. EXPERIMENTS OF CONTINUOUS STREAMING TASKS", "content": "We evaluate the performance of our proposed loss function\nwith sequential tasks that exceed the training length distribu-\ntion, mimicking real-world applications in which the inference\nprocess cannot assume input data with clear separations as, for\nexample, in audio applications (e.g., keyword spotting). The\nexperimental protocol involves testing the trained network on"}, {"title": "A. Dataset", "content": "We first examine our loss function on a cross-domain\nsequential learning task (Sequential Fashion-MNIST) as illus-\ntrated in Fig. 2. The experimental setup comprises 84-timestep\nsequences, where each sequence contains one Fashion-MNIST\nsample randomly interspersed with two MNIST digit se-\nquences (p=0.1 dropout). Both datasets contain 60,000 train-\ning and 10,000 test instances across 10 categorical classes.\nEach 28x28 grayscale image is transformed into a temporal\nsequence with 28 time steps. During training, the network\nimplements state reset or detach mechanisms at 28-timestep\nintervals for various loss functions. At inference, we assess\ncontinual inference on concatenated sequences without inter-\nmediate resets or detach, enabling quantitative measurement of\ncross-domain discriminative capability and temporal stability\non expended sequences.\nTo allow selective learning of Fashion-MNIST patterns,\nwe implement a binary masking mechanism across the 84-\ntimestep sequences. The mask tensor ($m_t \\in {0,1},t \\in$\n${1,...,84}$) marks Fashion-MNIST segments with ones and\nMNIST segments with zeros, enabling targeted gradient flow,\nin Figure 2. For each sequence, we generate six possible\npermutations of segment arrangements, randomly selected\nper batch. The Fashion-MNIST target sequence maintains its\ntemporal coherence while MNIST segments undergo random\npermutation and dropout (p=0.1) to prevent memorization.\nThis masking strategy ensures the network learns to iden-\ntify fashion-specific temporal patterns while treating MNIST\nsegments as structured noise, effectively creating a controlled\nenvironment for evaluating temporal pattern recognition.\nOur second benchmark utilizes the Google Speech Com-\nmands dataset\u2013GSCv2 [31]. The expanded GSCv2 comprises\n35 keyword classes plus an \"unknown\" category, with 36,923\ntraining and 11,005 test instances. The acoustic signals un-\ndergo spectral decomposition via Mel-frequency cepstral co-\nefficient (MFCC) analysis, implemented through a bank of 40\nsecond-order bandpass filters logarithmically spaced along the"}, {"title": "B. Metrics", "content": "To validate the performance, we evaluate across multi-\nple recurrent architectures with our reset-free mechanism,\nincluding vanilla RNN, GRU, SSM, and SNNs, comparing\ntheir performance against traditional periodic reset approaches.\nThe evaluation includes both discrete sample classification\nand continual inference setting. Let $m_t \\in {0,1}$ denote\nthe binary indicator of pattern presence at time step t, and\n$\\hat{y}_t$, $y_t$ represent the predicted and true labels, respectively. For"}, {"title": "V. RESULTS", "content": "Experimental results demonstrate the performance of our\nmethod in cross-domain sequence processing. We evaluate the\nproposed approach on the Sequential Fashion-MNIST task\nusing a two-layer GRU with 256 hidden neurons at each\nlayer, examining both last-frame (accp) and frame-wise (accf)\nclassification performance across sequence lengths ranging\nfrom single samples to 128 concatenated sequences (> 10k\ntimesteps). As shown in Table I, our method exhibits im-\nproved temporal stability compared to baseline approaches.\nThe experimental results indicate that hidden state detach-\nment significantly outperforms periodic reset by preserving\ntemporal context while selectively regulating gradient flow.\nWhile periodic reset exhibits minimal accuracy degradation\n(-0.06%), its last frame and frame-wise performance (74.47%,\n85.69%) remain suboptimal due to complete state elimina-\ntion in resetting. Under hidden state detachment, our newly\nproposed loss function achieves 88.43% last-frame accuracy\non single sequences and maintains 86.69% accuracy with\n128 concatenated sequences, yielding minimal performance\ndegradation (-1.74%). This contrasts with the masked cross-\nentropy (mCE), which exhibits significant decay (-3.72%).\nThe stability improvement becomes more pronounced in the\nreset configuration, where our method constrains accuracy\ndegradation to -1.57% versus -11.89% for the mCE. The\nframe-wise accuracy metrics corroborate these findings, with\nour method consistently preserving temporal stability across\nextended sequences in this cross-domain learning paradigm."}, {"title": "B. Google Speech Command", "content": "a) Single sample: The results on the GSCv2 dataset il-\nlustrate obvious performance disparities across different neural\narchitectures and training methodologies (Table II). Our reset-\nfree loss consistently demonstrates high performance across\nall architectures, with the GRU variant achieving the best"}, {"title": "VI. DISCUSSION", "content": "In this work, we demonstrated improvements in contin-\nuous sequence processing through three key contributions.\nFirstly, our reset-free methodology enables compact recurrent\narchitectures to maintain stable performance during continual\ninference without performance decline. Secondly, the proposed\ndual-objective loss function achieves this by simultaneously\noptimizing for target data learning while enforcing diver-\ngence on non-relevant inputs, yielding performance metrics\ncomparable to conventional reset-based approaches. Thirdly,\nthrough extensive empirical evaluation across diverse archi-\ntectural paradigms-from classical RNNs to modern SSMs\nand biologically-inspired SNNs-we established the broad"}, {"title": "APPENDIX A\nMATHEMATICAL ANALYSIS OF STATE SATURATION", "content": "State saturation in RNNs can be formally analyzed through\nthe lens of dynamical systems theory. Consider a general RNN\nwith state evolution:\n$h_{t+1} = f(W h_t + U x_t + b)$  (12)\nwhere $h_t \\in R^n$ is the hidden state at time t, $x_t \\in R^m$ is the\ninput, $W \\in R^{n \\times n}$ is the recurrent weight matrix, $U \\in R^{n \\times m}$\nis the input weight matrix, $b \\in R$ is the bias term, and f is\nan activation function."}, {"title": "A. Linear RNNS", "content": "For linear RNNs where f is the identity function, the state\nevolution can be expressed as:\n$h_{t+1} = W h_t + U x_t + b$  (13)\nThrough recursive expansion, the state at time T can be\nwritten as:\n$h_T = W^T h_0 + \\sum_{k=0}^{T-1} W^k (U x_{T-1-k} + b)$  (14)\nState saturation in linear RNNs occurs when:\n1) Eigenvalue Domination: If $||\\lambda_{max}(W)|| > 1$, where\n$\\lambda_{max}$ denotes the eigenvalue with largest magnitude, the\nterm $W^t h_0$ grows exponentially, causing state explo-\nsion.\n2) Fixed Point Convergence: If $||\\lambda_{max}(W)|| < 1$, the state\nconverges to a fixed point:\n$h_{\\infty} = (I \u2013 W)^{-1}(Ux + b)$  (15)\nwhere x represents the average input. This convergence\nlimits the network's ability to capture new information."}, {"title": "B. Nonlinear RNNS", "content": "For nonlinear RNNs with bounded activation functions (e.g.,\ntanh, sigmoid), state saturation manifests differently. Consider\nthe tanh activation:\n$h_{t+1} = tanh(W h_t + U x_t + b)$.  (16)\nSaturation occurs through two primary mechanisms:\n1) Activation Saturation: When $||W h_t + U x_t + b|| \\gg 1$,\nthe tanh function saturates:\n$\\lim_{z \\to \\infty} tanh(z) = 1, \\lim_{z \\to -\\infty} tanh(z) = -1$  (17)\nThe gradient in saturated regions approaches zero:\n$\\frac{d tanh(z)}{dz} = 1 - tanh^2(z) \\approx 0$  (18)\n2) Dynamic Attractor Formation: The nonlinear system\nforms attractors in state space described by the fixed-\npoint equation:"}, {"title": "C. Information Theoretic Perspective", "content": "State saturation can be quantified through the mutual infor-\nmation between inputs and states:\n$h^* = tanh(W h^* + U x + b)$  (19)\nOnce the state approaches these attractors, the network's\ncapacity to encode new information diminishes.\n$I(X_{t-\\tau:t}; h_t) = H(h_t) \u2013 H(h_t|X_{t-\\tau:t})$  (20)\nwhere H is the entropy and $X_{t-\\tau:t}$ represents the input\nsequence from time $t - \\tau$ to t. As saturation occurs:\n$\\lim_{t \\to \\infty} I(x_t; h_t| X_{t-\\tau:t-1}) \\to 0$  (21)\nindicating diminishing capacity to encode new information in\nthe saturated state."}, {"title": "D. Implications for Continuous Processing", "content": "The mathematical analysis shows that while reset mech-\nanisms successfully prevent state saturation, they introduce\nfundamental limitations in both information preservation and\ngradient propagation. These limitations become particularly\nproblematic in continuous processing scenarios, where:\n1) The optimal reset timing cannot be determined a priori.\n2) Important temporal dependencies may span across reset\npoints.\n3) The discontinuity in state evolution may introduce arti-\nfacts in the output sequence."}, {"title": "APPENDIX B\nTRAINING", "content": "We implement all experiments using the AdamW optimizer\nwith a batch size of 512 and an initial learning rate of\n3e-3 across all experimental conditions. Hidden states were\ninitialized using random sampling during reset operations.\nFor the SNNs, we employ a Gaussian-like surrogate gradient\nfunction for backpropagation through the spiking activation\nfunction."}]}