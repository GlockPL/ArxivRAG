{"title": "Cooperative and Asynchronous Transformer-based Mission Planning for Heterogeneous Teams of Mobile Robots", "authors": ["Milad Farjadnasab", "Shahin Sirouspour"], "abstract": "Coordinating heterogeneous teams of mobile robots for tasks such as search and rescue is highly challenging. This is due to the complexities of perception, decision making and planning in such environments, with agents' non-synchronous operation, constrained communication, and limited computational resources. This paper presents the Cooperative and Asynchronous Transformer-based Mission Planning (CATMiP) framework, which leverages multi-agent reinforcement learning (MARL) to effectively coordinate agents with heterogeneous sensing, motion, and actuation capabilities. The framework introduces a Class-based Macro-Action Decentralized Partially Observable Markov Decision Process (CMD-POMDP) model to handle asynchronous decision-making among different agent classes via macro-actions. It also extends the Multi-Agent Transformer (MAT) architecture to facilitate distributed, ad hoc communication among the agents. CATMiP easily adapts to mission complexities and communication constraints, and scales to varying environment sizes and team compositions. Simulations demonstrate its scalability and ability to achieve cooperative mission objectives with two classes of explorer and rescuer agents, even under severe communication constraints. The code is available at https://github.com/mylad13/CATMiP.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-robot systems (MRS) are becoming increasingly prevalent in search and rescue operations [1], monitoring and inspection [2], and industrial automation [3], where coordinated efforts improve efficiency and adaptability in complex tasks. Particularly, heterogeneous systems composed of agents with complimentary capabilities can be more effective than homogeneous teams in carrying out complex missions [1], [4], [5]. However, leveraging the full potential of heterogeneous MRS requires scalable and robust distributed control strategies that can enable seamless coordination among robots with varying roles and abilities, while maintaining resilience to failures and minimizing communication overhead.\nMulti-agent reinforcement learning (MARL) allows robots to autonomously learn coordination strategies through interaction with their environment [6]. However, MARL is known to suffer from expensive computations and the reality gap when transferring from simulation to real-world deployment [7], [8]. Additionally, MARL typically assumes synchronous agent actions, whereas in many real-world scenarios robotic agents operate asynchronously [9].\nThis paper proposes Cooperative and Asynchronous Transformer-based Mission Planning (CATMiP), an MARL framework designed to train heterogeneous teams of mobile robots to effectively cooperate with sporadic communication and asynchronous decision-making. CATMiP introduces a Class-based Macro-Action Decentralized Partially Observable Markov Decision Process (CMD-POMDP) to model asynchronous operation of heterogeneous teams through macro-actions. It extends the Multi-Agent Transformer (MAT) architecture [10] to support asynchronous and distributed decision-making via ad hoc and sporadic communication. CATMiP is scalable and robust with respect to the map size, team size, and team composition throughout the mission. It also uses domain and action delay randomization to help bridge the reality gap.\nThe rest of the paper is organized as follows. Section II reviews prior relevant work. Section III states and formulates the problem of mission planning for heterogeneous robotic teams along with the proposition of CMD-POMDP. Section IV describes the CATMiP framework in detail. Section V describes the simulation environment set-up, including the observation and action spaces and the reward structure. The results of the simulations are presented and discussed in Section VI. The paper is concluded in Section VII, with a brief discussion of potential directions for future work."}, {"title": "II. BACKGROUND LITERATURE", "content": "Deep reinforcement learning (DRL) has been increasingly used in mobile robotics for exploration and navigation, handling complex tasks in both single-agent and multi-agent settings [6], [11], [12]. Such control strategies are typically divided into end-to-end and two-stage approaches. The end-to-end methods derive control actions directly from sensor data, whereas the two-stage approaches first select target locations using RL and then employ a separate method for control actions, improving sample efficiency and generalization. In single-agent scenarios, DRL-based two-stage planning combines high-level goal selection with classical path-planning algorithms [13]\u2013[17].\nDeep multi-agent reinforcement learning (DMARL) methods have been developed to enhance exploration efficiency and handle communication dropouts in multi-robot settings [18]\u2013[20]. DMARL solutions for coordinating heterogeneous agents have also been explored in recent works, such as [21]-[24].\nAsynchronous reinforcement learning approaches such as Asynchronous Coordination Explorer and Qrainbow have been used to make real-time decisions in the presence of communication delay [9], [25]. They utilize decentralized execution and attention-based relations encoding to reduce communication traffic and improve generalization in dynamic environments.\nThere are several challenges in the way of applying deep multi-agent reinforcement learning for complex cooperative tasks including large computations, nonstationarity, partial observability, and credit assignment [7]. The Centralized Training and Decentralized Execution (CTDE) scheme [26] attempts to overcome some of these hurdles. CTDE balances computational cost and coordination by allowing agents to share information during training, while requiring them to act on their local observations during execution. Some methods share the policy network parameters among the agents to improve sample efficiency [27], [28]. However, this leads to homogeneous behaviour and sub-optimal performance in complex tasks that require behavioral diversity among the agents. Additionally, intrinsic rewards [29] and value function factorization [30] have proven effective in improving sample efficiency and solving the credit assignment problem.\nThe Multi-Agent Advantage Decomposition Theorem [31] shows that MARL problems can be decomposed into multiple sequential RL problems. This theorem suggests that agents can achieve a positive joint advantage by sequentially selecting their local actions, rather than searching through the entire joint action space simultaneously. By framing MARL as a sequence modeling problem, its complexity is reduced from multiplicative to additive [10], enabling faster training and more effective handling of heterogeneous teams.\nTransformers [32], known for their success in natural language processing and other domains [33], are well-suited for sequence modeling due to their ability to capture relationships within sequences using scaled dot-product attention. This mechanism calculates attention weights by taking the dot product between a matrix of queries ($Q \\in \\mathbb{R}^{n \\times d_k}$) and keys ($K \\in \\mathbb{R}^{n \\times d_k}$), scaling the result, and normalizing it by a softmax function,\n$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ (1)\nwhere n is the length of the input sequence and dk is the dimension of the queries and keys.\nThe weighting mechanism allows the model to focus on the most relevant parts of the input sequence by applying the calculated attention weights to the values ($v_i \\in \\mathbb{R}^{d_v}$). Transformers and the attention mechanism have been recently applied in RL [34]. Particularly, the Multi-Agent Transformer (MAT) architecture [10] has achieved state-of-the-art performance in cooperative MARL tasks, by enabling the agents to learn unique behaviors and focus on critical parts of each other's observations, enhancing their coordination. The present work extends MAT to accommodate macro-actions and asynchronous updates, leveraging the transformer's ability to handle variable sequence lengths."}, {"title": "III. PROBLEM STATEMENT AND FORMULATION", "content": "This paper is concerned with the design of distributed controllers for a heterogeneous team of mobile robots executing a cooperative mission in an unknown environment. In particular, it focuses on an indoor search and target acquisition scenario with two explorer and rescuer agent types. The mission requires a rescuer agent to reach a target, whose location is initially unknown, as quickly as possible. The control policies must leverage the diverse capabilities of the team, encouraging specialized behaviors for each agent class. For example, the explorer robots are faster and better suited to rapidly mapping the environment and finding the target, whereas a rescuer is slower but is capable of engaging with the target.\nThe robots use a combination of global and local planning for navigation; the global planner selects a goal within an area centered on the robot and computes the shortest path to it, which serves as the robot's macro-action (MA), and the local planner generates motion commands to handle obstacle avoidance and smooth movement along the path.\nThis work introduces a novel formulation for the two-level asynchronous control problem in heterogeneous teams of agents carrying out a cooperative mission. This is accomplished by defining a Class-based Macro Action Decentralized Partially Observable Markov Decision Process (CMD-POMDP). It is an extension of the MacDec-POMDP model [35] that supports varying properties across different agent classes. Assuming a team of N agents with $C \\leq N$ different classes, the problem is formalized as the tuple $(I, C, S, \\{M^c\\}, \\{A^c\\}, T, \\{R^c\\}, \\{\\Omega^c\\}, \\{Z^c\\}, \\{\\zeta^c\\}, \\{O^c\\},\\gamma,h)$, where\n$I=\\{1,...,N\\}$ is a finite set of agents;\n$C = \\{1,...,C\\}$ is a finite set of agent classes, with $C(i)$ indicating the class of agent $i$;\n$S$ is the global state space;\n$M^c$ is a finite set of MAs for agents of class $c$. The set of joint MAs is then $M = \\times_{i \\in I} M^{C(i)}$;\n$A^c$ is a finite set of (primitive) actions for agents of class $c$. The set of joint actions is then $A=\\times_{i \\in I} A^{C(i)}$;\n$T:S\\times A\\times S \\rightarrow [0,1]$ is a state transition probability function, indicating the probability of transitioning from state $s\\in S$ to state $s'\\in S$ when the agents are taking the joint action $\\bar{a}\\in A$. In other words, $T(s,\\bar{a},s')=Pr(s'|\\bar{a},s)$;\n$R^c: S \\times A \\rightarrow \\mathbb{R}$ is the agent-class-specific reward function, with $R^{C(i)}(s, \\bar{a})$ being the reward an agent $i$ of class $c$ receives when the joint action $\\bar{a}$ is executed in state $s$;\n$\\Omega^c$ is a finite set of macro observations (MOs) for agents of class $c$. The set of MOs is then $\\Omega = \\times_{i \\in I} \\Omega^{C(i)}$;\n$Z^c: \\Omega^c \\times M^c \\times S \\rightarrow [0,1]$ is the MO probability function for agents of class $c$, indicating the probability of the agent receiving the MO $z^i \\in \\Omega^{C(i)}$ given MA $m^i \\in M^{C(i)}$ is in progress or has completed and the current state is $s' \\in S$."}, {"title": "IV. PROPOSED FRAMEWORK", "content": "The proposed CATMiP framework for asynchronous and distributed mission planning in cooperative heterogeneous robotic teams follows the CTDE principle. It consists of several key modules: the Asynchronous Multi-Agent Transformer (AMAT), Collaborative Simultaneous Localization and Mapping (CSLAM), the High-Level Processor, and the Low-Level Navigation Controller. These modules are implemented and utilized differently during the training and execution phases, as described later in this section.\nFig. 1 (a) depicts the CATMiP architecture and data flow at time stept of a simulated training episode with full communication, where the active subset $I_{1:g}$ of the agents are obtaining new MAs. During mission execution, each agent generates MAs individually or together with other active agents in its communication neighborhood, as shown in Fig. 1 (b).\nAt each time step of the mission, agents are considered active, inactive, or on standby. Inactive agents continue executing primitive actions generated by their current MA but enter standby mode when their MA termination condition is met. Agents on standby remain at their current location and delay selecting their next MA by a random number of time steps. This delay not only allows them to find other agents on standby within their communication neighborhood so they can activate simultaneously and obtain their new MAs with one pass through the network, but it also helps simulate the variable time it might take to execute an MA in real-world scenarios. If an agent's maximum standby time is reached and no other on-standby agents are found in the neighborhood, it activates alone. Agents that update their MAs become inactive again.\nCentralized Training\nThe pseudocode of the centralized training process is presented in Algorithm 1. At every time step t of a training episode, the agents perform the primitive actions generated by the navigation controller module based on each agent's current MA and collect the state-action-dependent reward. At the $T^{th}$ instance of there being a subset $I_{1:g(\\tau)}$ of agents ready to update their MAs (flagged as active agents), an ordered sequence of macro-observations (MOs) $\\tilde{z}_{1} = \\{z_{i1},...,z_{ig(\\tau)},z_{ig(\\tau)+1},...,z_{iN}\\}$ are formed and fed as the input to the AMAT module. Subsequently, a sequence of latent representation of MOs of active agents, $\\hat{z}_{1:g(\\tau)}$, are calculated and used to obtain new MAs $\\hat{m}_{1:g(\\tau)}$ and value function estimates $V(\\hat{z}^{\\hat{m}}_{1:g(\\tau)})$. The rewards accumulated by the active agents since their previous activation $R^{\\hat{m}}_{1:g(\\tau)}$, along with $\\tilde{z}_{1}$, $\\hat{z}_{1:g(\\tau)}$, and $V(\\hat{z}^{\\hat{m}}_{1:g(\\tau)})$ are stored in a buffer to be used for updating the network parameters at the end of each episode. The fully trained network is then sent to each agent to be used for inference during the distributed execution phase."}, {"title": "B. Distributed Execution", "content": "During the execution phase, the robots establish time-varying communication neighborhoods through a dynamic mobile ad hoc network. This setup is modeled by a communication graph $G = (I,\\mathcal{E})$, where the vertices $I$ represent the set of agents and the edges $\\mathcal{E}$ represent the communication links between them. The probability of a communication link $E_{ij}$ existing between agents $i$ and $j$ is modeled as $Pr(E_{ij}) = e^{-d_{ij}/\\sigma^2}$ where $d_{ij}$ denotes the distance between agents, and $\\sigma$ controls the rate of probability decay. This model is adapted from [36]. An agent's neighborhood, $N(i)$, consists of all agents (including itself) that can exchange information either directly or indirectly with agent $i$.\nAgents within a neighborhood that have been activated at the same time step elect a broker to handle the generation and transmission of new MAs. The broker forms an ordered sequence of MO embeddings $(\\tilde{z}_{e1},..., \\tilde{z}_{em})$, starting with its own MO embedding $\\tilde{z}_{1}$ and followed by $\\tilde{z}_{e2:g}$ received from active and $\\tilde{z}_{eg+1:m}$ received from inactive agents in the neighborhood. This sequence is fed as input to the broker's local AMAT network, generating a sequence of new MAS $(\\hat{m}^i_1,..., \\hat{m}^i_g)$ to be sent to their respective agent.\nThe following subsections detail each module's role in the training and execution phases."}, {"title": "C. Asynchronous Multi-Agent Transformer", "content": "The AMAT module, illustrated in Fig. 2, consists of four components: the macro-observations embedder, the encoder, the macro-actions embedder, and the decoder. In summary, AMAT processes a sequence of macro-observations from agents $I_{1:m}$, accompanied by a vector of active flags indicating the subset $I_{1:g}$ (g\u2264m) of these agents requiring new macro-actions. It then outputs the corresponding sequence of MAs. The input and output sequence lengths are flexible, allowing AMAT to model policies for a variable number of agents within a single network, and to support asynchronous and distributed inference.\nDuring centralized training, $I_{1:m}$ represents the complete agent set $I = \\{1,...,N\\}$, while in the distributed execution phase, it denotes the set of agents in the same neighborhood $N(i)=\\{i_1,...,i_m\\}$. In both cases, the sets are ordered to put the active agents $I_{1:g}$ first.\nMacro-Observations Embedder: During the centralized training phase, this module transforms a sequence of macro-observations $(\\tilde{z}_{e1},...,\\tilde{z}_{em})$ into a sequence of macro-observation embeddings $(\\tilde{z}'_{e1},...,\\tilde{z}'_{em})$ to be used as input tokens for the encoder. In the distributed execution phase, each agent $i \\in N(i)$ inputs its macro-observation $z$ into the trained macro-observations embedder on its own hardware, then transmits the resulting embedding $\\tilde{z}'^i \\in\\mathbb{R}^d$ to its neighborhood's broker. Transmitting MO embeddings, vectors of length d, instead of actual MOs, which in our case are local and global multi-channeled maps of dimensions $L\\times L\\times 4$ and $S\\times S\\times 8$, significantly reduces network traffic. The broker subsequently passes the full sequence of embeddings $(\\tilde{z}'_{1},...,\\tilde{z}'_{em})$ to the encoder.\nThe architecture of the macro-observations embedder is shown in Fig. 3, where two separate convolutional neural networks (CNNs) extract features from the multi-channeled global and local maps, concatenate and feed them into a multi-layer perceptron (MLP). An adaptive max pooling layer is used after the first CNN to scale the global feature maps of any $S\\times S$ dimensions to $G\\times G$ zones, allowing the model to work in any environment size. To enable the learning of unique behaviors for different classes of agents, the output of the MLP is combined via element-wise addition with a learnable class encoding obtained by passing a one-hot class identifier of size $C$ through a fully connected (FC) layer with output size d.\nEncoder: The encoder is made up of several encoding blocks each consisting of a self-attention mechanism, an MLP, and residual connections. The sequence of MO embeddings $\\tilde{z}'_{1:m}$ are fed as the input to these computational blocks, where they are encoded into a sequence of MO representations $(\\tilde{z}_{i1},..., \\tilde{z}_{im})$. These latent representations carry information both about each agent's current view of the environment, as well as the high-level interrelationships among the agents. An additional MLP is also used during the training phase to approximate the value of each agent's observation. Values associated with the active subset of agents, $V(\\tilde{z}^{i1}),..., V(\\tilde{z}^{ig}$, are used to minimize the empirical Bellman error\n$L_{Encoder}(\\phi) = \\frac{1}{T} \\sum_{\\tau=0}^{T-1} \\frac{1}{g(\\tau)} \\sum_{l=1}^{g(\\tau)} [R^\\tau +\\gamma V(s_{\\tau+1}) -V(\\tilde{z}^l)]$ (3)\nwhere T is the total number of MA updates, g(T) is the number of active agents at the $T^{th}$ MA update instance, and $\\phi$ is the non-differentiable target network's parameter.\nMacro-Actions Embedder: This module consists of an MLP and an fully connected layer. The MLP converts a one-hot encoded representation of MAs $m^i_{0:l-1,l=\\{1,...,m\\}}$ into MA embeddings $\\tilde{m}^i_{0:l-1}$, which form a sequence of vectors with dimension d. Similar to the MO embedder, the FC layer generates learnable class encodings, which are combined with the MA embeddings to associate each macro-action with the agent class responsible for executing it.\nDecoder: The decoder processes the joint MA embeddings $\\tilde{m}_{0:l-1}, l=\\{1,...,m\\}$ through a series of decoding blocks, with $\\tilde{m}_0$ acting as an arbitrary token designating the start of decoding. Each decoding block is made up of a masked self-attention mechanism, a masked attention mechanism, and an MLP followed by residual connections. The masking ensures that each agent is only attending to itself and the agents preceding it, preserving the sequential updating scheme and the monotonic performance improvement guaranty during training [10]. The final decoder block outputs a sequence of joint MA representations $\\{\\hat{m}_{0:h-1}\\}_{h=1}$, which is then fed to an MLP to obtain the probability distribution of agent $i$'s MA, which is the high-level policy $\\mu^i(\\hat{m}^i|z_{i1:m},m^i_{0:l-1})$, where $\\theta$ represents the decoder parameters. The decoder is trained by minimizing the following clipped PPO objective, which only uses the action probabilities and advantage estimates of the active subset of agents $I_{1:g(\\tau)}$ at the $T^{th}$ instance of MA updates.\n$L_{Decoder}(\\theta) = \\frac{1}{T} \\sum_{\\tau=0}^{T-1} \\frac{1}{g(\\tau)} \\sum_{l=1}^{g(\\tau)} min(r(\\theta),\\text{clip}(r(\\theta),1\\pm \\epsilon) \\hat{A})$, (4)\nr(\\theta) =  \\frac{\\mu^i_{\\theta}(\\hat{m}^i|z_{i1:g}, m^i_{0:l-1})}{(\\mu^i_{old}(\\hat{m}^i|z_{i1:g}, m^i_{0:l-1})}$ (5)"}, {"title": "D. High-Level Processor", "content": "During centralized training, the high-level processor module manages agent activation, forms macro-observations, and updates the navigation controller with the current global goal. In the execution phase, this module handles communication and the exchange of MO embeddings and MAs with the neighbors as well."}, {"title": "E. Low-Level Navigation Controller", "content": "This module consists of a global path planner and a motion controller that generates the robot's primitive action, a, at each time step t. A path planning algorithm, such as D* Lite [38], could be employed to determine the shortest path from the robot's current location to the global goal specified by its MA $m^i$. This path is represented as a sequence of coordinates on the global occupancy grid map. The motion controller may use a local planner [39] to optimize a local trajectory based on the global path and the robot's sensory data, ensuring the trajectory meets the robot's motion constraints, avoids obstacles, and minimizes execution time. The optimized trajectory is then converted into low-level motion control commands, forming the primitive action a."}, {"title": "F. Collaborative Simultaneous Localization and Mapping", "content": "This module enables robots to collaboratively build a shared global occupancy grid map. A state-of-the-art algorithm like Swarm-SLAM [40] can be utilized during execution, where agents may experience intermittent communication. A designated robot temporarily processes data from connected robots to generate the most accurate map and pose estimates, ensuring convergence to a unified global reference frame after multiple rendezvous among robot subsets. The discretized occupancy grid map is then employed by the high-level processor to construct macro-observations and by the path planner to set navigation goals."}, {"title": "V. SIMULATION SETUP", "content": "CATMiP is evaluated in a customized 2D grid-world environment designed in Minigrid [41], with randomly generated environments for each episode, featuring varied numbers of cluttered rooms with different shapes, and a randomly positioned static target. The environment provides clues to the target's whereabouts in the form of detectable traces in nearby cells."}, {"title": "B. Macro-Action and Macro-Observation Spaces", "content": "Global goal candidates, or macro-actions, are defined as cells within a square of side length L centered on the agent, giving rise to a discrete macro-action space of size $L^2$. Once a macro-action is selected, it is mapped to its corresponding coordinate (x,y) on the grid map, establishing the agent's global goal. Invalid actions, such as those targeting occupied or out-of-bounds cells, are handled using invalid action masking [42]. This has proven effective in large action spaces such as those in StarCraft II [43] and Dota 2 [44].\nEach agent's macro-observation is composed of three elements: a global information map of size $S\\times S\\times 8$, a local information map of size $L \\times L \\times 4$, and a one-hot encoded agent class identifier. The channels in the local map provide information about cells in the agent's immediate vicinity, including whether they have been explored, their occupancy, the presence of the target or any clues leading to it, and the agent's last or current navigation goal. Similarly, the global map's first four channels represent this information for the entire map, while the remaining four channels encode the agent's current location, its decaying trajectory, the locations of other rescuer robots, and the locations of other explorer robots. In Minigrid, the primitive action space includes four actions: moving forward, turning right, turning left, and stopping. An agent's primitive actions are selected to follow the shortest path to the global goal."}, {"title": "VI. SIMULATION RESULTS", "content": "We evaluated the effectiveness of the proposed CATMiP framework using the Minigrid simulation environment under varying environmental conditions and communication constraints. Simulations were performed on three tasks, each featuring different map sizes and team compositions. Task 1 involved one rescuer and one explorer agent in a 16\u00d716 grid. Task 2 increased the map size to 20 \u00d7 20, with an additional randomly selected agent. Task 3 scaled up to a 32\u00d732 environment with 8 agents, consisting of 2 rescuers, 4 explorers, and 2 randomly assigned agents.\nTwo different models with the same network architecture and training hyperparameters were trained on Tasks 1 and 2, utilizing 128 parallel environments. Training was conducted on an NVIDIA GeForce RTXTM 3090 GPU over 250 million total steps, equivalent to 9765 episodes with a horizon of 200 steps. Model 1 required approximately 52 hours of training, while Model 2 took 96 hours. shows the progression of the average mission success rate throughout training, where an exponential moving average with a span of 200 is applied to the data. Key hyperparameters were set as follows: L=7, G=4, $y$=1, with 10 training epochs and a learning rate of 10\u20134.\nScalability: To assess the scalability of CATMiP with respect to environment size and team composition, we tested the two trained models across all three tasks, conducting 500 simulated episodes per task with identical random seeds and no communication loss. shows the probability of success as a function of elapsed time since the start of the mission. Both models demonstrated the ability to scale effectively, handling larger teams and environments. Notably, Model 2 outperformed Model 1 across all tasks, particularly in Task 3, which required higher levels of team coordination. This improvement can be attributed to the additional training experiences per episode and the development of more refined coordination strategies through training with multiple agents of each class.\nCommunication: Model 2 was tested across 500 simulated episodes under four communication conditions: no communication loss, $\\sigma$= 6, $\\sigma$=4, and $\\sigma$= 2. Note that a lower $\\sigma$ value corresponds to a steeper decline in communication probability as a function of the distance between agents. As shown in Fig. 7, even under severe communication constraints, where agents lose connection beyond 5 unit distances, they are still able to complete the mission within a reasonable time frame."}, {"title": "VII. CONCLUSIONS & FUTURE WORK", "content": "This paper introduced CATMiP, a novel framework for coordinating heterogeneous multi-robot teams in environments with communication constraints. The proposed CMD-POMDP model provides the mathematical foundations of asynchronous and decentralized decision-making of heterogeneous agents by incorporating class-based distinctions across its components. Leveraging the transformer's ability to handle variable input sequence lengths, the Multi-Agent Transformer architecture was extended to develop scalable coordination strategies that can be executed in a distributed manner by any number of agents. CATMiP demonstrated robustness and scalability across different team sizes, compositions, and environmental complexities in simulations of search and target acquisition tasks. The results highlighted the framework's adaptability to sporadic communication, asynchronous operations, and varied map conditions, achieving high mission success rates even under strict communication constraints. By addressing key challenges such as communication dropout, asynchronous operations, and agent heterogeneity, CATMiP shows significant potential for real-world applications where mobile robots must operate under resource limitations and unpredictable conditions.\nFuture research will investigate integrating temporal memory into the network, expanding the framework to scenarios involving dynamic targets. The system's performance will also be evaluated in more realistic 3D simulation environments as well as real-world experiments."}]}