{"title": "A novel approach\nto\nthe relationships\nbetween data features\nbased on\ncomprehensive examination\nof\nmathematical,\ntechnological,\nand\ncausal methodology", "authors": ["JaeHong Kim"], "abstract": "The expansion of artificial intelligence (AI) has raised concerns about transparency, accountability, and\ninterpretability, with counterfactual reasoning emerging as a key approach to addressing these issues.\nHowever, current mathematical, technological, and causal methodologies rely on externalization\ntechniques that normalize feature relationships within a single coordinate space, often distorting\nintrinsic interactions. This study proposes the Convergent Fusion Paradigm (CFP) theory, a framework\nintegrating mathematical, technological, and causal perspectives to provide a more precise and\ncomprehensive analysis of feature relationships. CFP theory introduces Hilbert space and backward\ncausation to reinterpret the feature relationships as emergent structures, offering a potential solution to\nthe common cause problem a fundamental challenge in causal modeling. From a mathematical-\ntechnical perspective, it utilizes a Riemannian manifold-based framework, thereby improving the\nstructural representation of high- and low-dimensional data interactions. From a causal inference\nperspective, CFP theory adopts abduction as a methodological foundation, employing Hilbert space for\na dynamic causal reasoning approach, where causal relationships are inferred abductively, and feature\nrelationships evolve as emergent properties. Ultimately, CFP theory introduces a novel Al modeling\nmethodology that integrates Hilbert space, backward causation, and Riemannian geometry,\nstrengthening AI governance and transparency in counterfactual reasoning.\nKeywords: Convergent Fusion Paradigm (CFP) theory, Counterfactual explanations, Feature\nrelationships, Riemannian manifolds, Hilbert space and backward causation, Abduction", "sections": [{"title": "I. Introduction", "content": "The issues of transparency and accountability in artificial intelligence (AI) represent some of the most\ncritical legal and ethical challenges arising from the pervasive integration of AI into modern society as\na leading core technology of the 21st century. [To address these challenges,] there has been much\ndiscussion of the existence of a \"right to explanation\" in the EU General Data Protection Regulation\n(GDPR), and its merits and disadvantages (Edwards and Veale 2017; Malgieri and Comand\u00e9 2017;\nMendoza and Bygrave 2017; Selbst and Powles 2018; Wachter, Mittelstadt, and Floridi 2017; Wachter,\nMittelstadt, and Russell 2017, p842)\nHowever, explanations provided to data subjects do not necessarily require them to fully understand\nthe operational mechanisms of algorithmic systems. Instead, revealing the system's inner workings\ncould infringe on the rights of counterparties, such as trade secrets and privacy, or increase the\nlikelihood of data subjects manipulating or gaming the decision-making system. Therefore,\nexplanations should aim solely to assist data subjects, as this approach can resolve the issues of\nalgorithmic transparency and accountability without undermining trust between data subjects and data\ncontrollers. In principle, such explanations should be provided without disclosing the \"black box\" of"}, {"title": null, "content": "the algorithm. (Wachter, Mittelstadt, and Russell 2017) argue that the concept of \"unconditional\ncounterfactual explanation(counterfactuals)\u201d meets these requirements and overcomes many challenges\nrelated to algorithmic interpretability and accountability. Counterfactuals have been discussed as a key\ntechnique within the theme of algorithmic explainability, which is central to explainable Artificial\nIntelligence (xAI) (Molnar 2020).\nThe process of generating counterfactuals involves two steps: first, identifying a decision that\ncontradicts the data subject's intent; second, determining the minimal changes required for the decision\nmodel to produce a favorable outcome for the data subject. Notably, these changes are defined at the\nfeature level of the data. For instance, in a \"loan approval/denial decision model,\" if a loan application\nis denied based on the submitted data, the features requiring modification to secure approval may\ninclude employment period, job change, salary, and bank account balance. Understanding the\nrelationships among features like employment period, salary, and bank account balance is essential to\nproviding meaningful responses to data subjects through counterfactuals.\nTo better understand the relationships among data features, the following questions may arise:\nIs there a direct or inverse relationship between employment period and job change?\nTo increase salary, should employment period be extended, or is a job change more appropriate?\nCould salary be considered a common cause of both employment period and job change?\nHow does a change in salary affect the bank account balance? Could salary be viewed as a\nhigher-level feature of bank account balance?\nAs these questions illustrate, the crucial challenge in generating counterfactuals lies in accurately\ncapturing the relationships among data features. However, properly identifying and structuring these\nrelationships is a highly complex and challenging task, involving intertwined technical, mathematical,\nlogical, and philosophical issues. Nevertheless, if AI, as a data science, fails to go beyond the\nmathematical and technical constraints of data processing and represent the real-world relationships\namong features accurately, a series of legal (right to explanation) and theoretical (xAI) efforts to ensure\nAl transparency and accountability will inevitably remain incomplete. Moreover, these feature\nrelationships align closely with the limitations of current mathematical, technical, and causal\nmethodologies. Thus, efforts to rigorously elucidate these relationships are one of the key prerequisites\nfor addressing the broader social and legal issues associated with AI.\nThis study aims to clarify the relationships among data features by identifying their inherent limitations\nand proposing a series of strategies to overcome them. Also, it approaches multidisciplinary by\nintegrating the mathematical, technological, and causal perspectives. To this end, the study reviews the\nnoteworthy literature on feature relationships within the past decade of AI and computer science,\nparticularly those related to counterfactuals. It highlights the methodological limitations underlying\nthese studies and proposes the Convergent Fusion Paradigm (CFP) theory (Kim and Shim 2025) as a"}, {"title": null, "content": "philosophical and theoretical foundation for overcoming these challenges.\nThe CFP theory, from the mathematical and technological perspective, particularly focusing on its\nintegrative potential to unify these domains for a cohesive framework, provides new insights into the\ngeometric structure of data related to Riemannian manifolds, offering a structural framework for\nconverging high- and low-dimensional data, which facilitates a clearer understanding of the data\ngeneration process. Moreover, this study argues that the \"common cause\" problem, often regarded as a\nlogical conundrum in applying causal methodologies, could be naturally understood as emergent\nfeatures by extending the limited spatial or temporal coordinates (Buckner 2020) such as Euclidean\nspace into Hilbert space and equipping it with backward causation. In this context, the CFP theory offers\na methodological insight into establishing a new causal model employing emergent feature. Lastly, in\nthis context, this study further posits that abduction could serve as a basis for a new causal model in\nHilbert space.\nTo achieve the above objectives, this study proceeds as follows.\nIn Sect. 2, this study reviews the noteworthy AI literature related to counterfactuals, focusing on how\nthe relationships among features are addressed. The literature is categorized based on whether it is dealt\nin monotonic geometry or multiple geometry, with representative works from each approach being\nsummarized. By doing so, this study identifies the characteristics and limitations of the methodologies\nemployed in these works. Additionally, this literature is classified according to externalization and\ninternalization methods, as well as mathematical-technological and causal methodologies. Particular\nattention is given to the limitations posed by the \u201ccommon cause\" problem when addressing feature\nrelationships from a causal perspective. To address this issue as a problem of emergent features, this\nstudy emphasizes the need for new conditions and causal reasoning. Subsequently, it examines Hilbert\nspace and backward causation as a framework for space and causal logic, arguing for the necessity of a\ntheoretical foundation (CFP theory) to implement a new causal model aligned with the properties of\nHilbert space.\nSect. 3 begins from a more fundamental perspective to establish the CFP theory. Since AI research is\nclosely linked to data statistics, this study briefly examines the two cultures of statistical modeling-\ndata modeling culture and algorithmic modeling culture-proposed by (Breiman 2001). Following this,\nthis study defines the concept of \u201cdata-algorithm relationship modeling\" as the unique statistical\nmodeling approach of Deep Neural Networks (DNNs). The analysis highlights the differences between\ndata statistics and AI, particularly DNNs, and conceptualizes the developmental phase created by the\nrelationship modeling of data and algorithms as an \u201cexpansion of dimensions accompanied by\nqualitative transformation.\" To illustrate this developmental phase, two conceptual hypotheses are\nproposed. These are informed by achievements in physics and biology, as well as Hofstadter's\nreformulation of G\u00f6del's proof into a new formula, (TNT + G\u03c9 PROOF - PAIR{a, a'}) (Hofstadter 1999),\nculminating in the proposals of \u201ccreating relative Space-Time in Relationship (crSTR)\" and \"Duplex\nContradictory Paradoxical Stratified structure of Thorough Closure-Eternal Opening (DCPSs of TC-\nEO)."}, {"title": null, "content": "Sect. 4 applies the CFP theory established in Sect. 3 to both mathematical-technological and causal\nperspectives. First, from the mathematical-technological perspective, it establishes the philosophical\nand theoretical insights gained when reviewing the works of (Joshi et al. 2019) and (Arvanitidis et al.\n2018, 2020) through the lens of the CFP theory. In sequence, from the causal perspective, this study\napplies the framework of the CFP theory to the logic step of abduction, providing a detailed outline of\na methodological foundation for a new causal model addressing emergent features.\nFinally, the conclusion explores the potential of Hilbert space in data generation and examines why\nphilosophical reflection is essential to support this endeavor."}, {"title": "II. Theoretical and Philosophical Challenges Related to Data Feature\nRelationships", "content": "As mentioned earlier, it is essential to review how the relationships among data features are addressed\nin the existing AI literature, particularly those related to counterfactuals. Such a review allows for a\ncomprehensive understanding of the current mathematical, technological, and logical approaches to\ndata feature relationships. Moreover, it provides insights into the theoretical and philosophical\nunderpinnings of these approaches while clarifying their limitations."}, {"title": "1. Overview of Existing AI Literature on Data Feature Relationships", "content": null}, {"title": "1) Methods of Understanding Relationships Among Features", "content": null}, {"title": "- Introduction of Distance Concepts", "content": "Most existing AI literature interprets the relationships among data features in terms of the concept of\ndistance between various observed feature values and external reference points, such as decision\nboundaries (Ustun et al. 2019) or population mean values (Gupta et al. 2019). To use distance as a basis,\nfeatures must be normalized into certain reference feature values. These normalization processes ensure\nthat features are on a common scale, enabling meaningful comparisons. For instance, the increases in\nemployment period and salary do not naturally correspond to each other. In such cases, normalization\nattempts to capture the fact that salaries may vary on the order of thousands or tens of thousands of\ndollars, but length of employment (in years, say) varies at a numerically much smaller scale (Barocas\net al. 2020, p83). In other words, normalization assigns appropriate weights to features, allowing them"}, {"title": null, "content": "to be compared mathematically within a unified coordinate plane (non-dimensionalize). Typically, this\nnormalization process is calculated using the distance metric of the features.\nAdditionally, AI literature addressing the relationships among data features can be classified based on\nwhether it employs monotonic geometry (simple geometric space) or multiple geometry (complex\ngeometric space). This study categorizes the former as an \u201cExternalization Methodology (EM)\" for\nunderstanding feature relationships and the latter as an \u201cInternalization Methodology (IM)\"."}, {"title": "2) Monotonic Geometry (MG) \u2013 Externalization Methodology (EM)", "content": "Representative works employing externalization methodology include (Ustun et al. 2019) and\n(Wachter, Mittelstadt, and Russell 2017).\nA. (Wachter, Mittelstadt, and Russell 2017)\nWachter's study was the first to apply counterfactuals to concrete cases, namely the LSAT Dataset and\nPima Diabetes Database, in order to derive solutions. Nevertheless, this paper did not establish a\ndecision boundary and normalize the distances between features to compute the counterfactuals. Instead,\nit formulated the relationships between features through one-hot encoding (Verma et al. 2020) and then\ncomputed counterfactuals by normalizing both the original condition world and the counterfactual\ncondition world with respect to the original data by means of a distance metric."}, {"title": null, "content": "$\\\\arg \\\\min[l(f(x_i), y) + \\\\rho(\\\\omega)],$   (1)\n$\\\\arg \\\\min_x\\\\min_\\\\lambda (f(x') \u2013 y')^2 + d(x_i, x'),$  (2)\nwhere the label is $y_i$, data point $x_i$ and $p(w)$ is a regularizer over the weights. To find counterfactual\ndata point $x'$ as close to the original data $x_i$ as possible such that $f_w(x')$ is equal to new target $y'$.\nTo elaborate, in the LSAT Database, data features such as Race (Black or White), GPA, LSAT, and\nScore inherently differ in scale and dimensionality. However, these features possess characteristics of\ndiscrete variables, allowing them to be encoded as binary or categorical values through one-hot\nencoding (Verma et al. 2020). This encoding process ensures that each feature can be compared within\na single coordinate plane. After implementing this encoding, Wachter established a decision boundary\nusing Equation (1) with a weight w based on the original data. Then, Equation (2) was employed to\nderive counterfactuals based on the original data world, applying a normalization process using both\nL1 and L2 norms, when it comes to the LSAT database. This process facilitated the relative adjustment"}, {"title": null, "content": "of feature interactions based on quantitative magnitude (Eq. (3)). Consequently, the counterfactual\noutcomes derived through this methodology became comparable to the original data within a single\ncoordinate plane, allowing the paper to reveal racial discrimination between Black and White applicants\nin law school admissions.\nThis stripped-down version of the LSAT dataset is used in the fairness literature, as classifiers trained\non this data naturally exhibit bias against 'black' people (Kusner et al. 2017; Russell et al. 2017). As a\nresult, we will find evidence of this bias in our neural network in some of the counterfactuals we\ngenerate (Wachter, Mittelstadt, and Russell 2017, p856).\n$d(x_i, x') = \\\\Sigma_{k\\\\in P} \\\\frac{|x_{ik}-k'|}{MA_k}$   (3)\nwhere $d(x_i, x')$ is distance between original data point $x_i$ and counterfactual data point $x_k'$, and $MAD_k$\nstands for the Manhattan distance metric (L1 norm) which is $MAD_k = median_{(j\\\\in P)}(|X_{(j,k)} \u2013\nmedian_{(l\\\\in P)} (X_{(l,k)})|)$ for the median absolute deviation of feature k, over the set of points P.\n$d(x_i, x') = \\\\sum_{k\\\\in P}(X_{i,k} - X_k')^2,$ (4)\nwhere $d(x_i, x')$ is set of distance of unweighted squared Euclidean distance (L2 norm).\nHowever, this series of processes deviates from the fundamental intent of counterfactual reasoning.\nCounterfactuals are originally intended to determine the minimum change in a cause necessary to alter\nthe outcome within a causal framework. However, in the LSAT database as analyzed by (Wachter,\nMittelstadt, and Russell 2017), the decision boundary, which determines the outcome change, is not\nexplicitly defined. In this situation, (Wachter, Mittelstadt, and Russell 2017) set a single causal variable\n(the score) to '0' and used the corresponding changes in other causal variables as counterfactuals. Yet,\nas (Wachter, Mittelstadt, and Russell 2017) themselves acknowledge in their paper, there is no guarantee\nthat the \u201csynthetic counterfactual x'\u201d corresponds to a valid data point.\nTo demonstrate the importance of the choice of distance function, we illustrate below the impact of\nvarying d(,) on the LSAT dataset. A further challenge lies in ensuring that the synthetic counterfactual\nx' corresponds to a valid data point (Wachter, Mittelstadt, and Russell 2017, p855).\nThis is a misinterpretation arising from the limitations of mathematical properties-first, there is no\nguarantee that a sufficiently close solution aligns with the decision boundary. If the decision boundary\nis determined using pre-change conditions while the premise itself is altered, an inconsistency between\nthe already-applied and altered assumption arises. In other words, it is impossible to represent both the\noriginal data condition world and the counterfactual data condition world on a single, unified scale."}, {"title": null, "content": "Put differently, counterfactual reasoning should determine the necessary changes in features (causes)\nto alter an outcome-for example, what changes in features would enable Tom, who was rejected from\nlaw school, to be admitted? Instead, (Wachter, Mittelstadt, and Russell 2017)'s method fixes one causal\nvariable and examines the relative changes in other causal variables, representing them within a single\ncoordinate plane based on the original data. This approach is inherently impossible, because although\nboth the original data world and the counterfactual world may share seemingly identical attributes (such\nas race, LSAT, and score), they fundamentally belong to distinct worlds that cannot be represented\nwithin the same coordinate system. In summary, the counterfactual approach of (Wachter, Mittelstadt,\nand Russell 2017) merely fixes one causal factor while comparing the relationships among others,\nallowing for the sociological interpretation such as racial bias. However, this methodology deviates\nsignificantly from the original intent of the counterfactual method, which seeks to infer causal changes\nnecessary for an outcome to be altered.\nB. (Ustun et al. 2019)\n(Ustun et al. 2019) established a decision boundary and resolved the relationships among features by\ncomputing distances between them. In their paper, they introduced an approach that calculates the\noptimal combination of features that allow an individual to cross the decision boundary, generating a\nflipset as a recommendation. The decision subject then selects the most appropriate combination from\nthis flipset. In other words, (Ustun et al. 2019) devised a mathematical formulation to generate flipset\nand implemented an algorithm capable of computing them."}, {"title": "Methodological Overview", "content": "Notably, (Ustun et al. 2019) introduced the concept of recourse, which differs from the counterfactual\napproach proposed by (Wachter, Mittelstadt, and Russell 2017), and designed a methodology that\nidentifies actionable features that can be modified. For example, if an individual is denied a credit loan,"}, {"title": null, "content": "the actionable features that could be adjusted include reducing debt or increasing savings. The set of\nthese recommended features is referred to as the flipset, wherein immutable features, such as age and\ngender, are excluded.\nTo elaborate, the series of calculations to derive the optimal combination mentioned above is\nconducted within a linear integer programming framework. The entire process revolves around\nidentifying an action set that minimizes a cost function\u00b9. Here, minimizing the cost function refers to\nachieving minimal changes in feature values that result in minimal changes in function values. During\nthis minimization process, constraints are imposed to ensure that only actionable features are considered,\nwhile immutable features or those mutable in an infeasible way are excluded. In summary, (Ustun et al.\n2019) formulated an approach that identifies the feature values minimizing the cost function, iteratively\nadjusting them to discover a group of flipset capable of crossing the decision boundary, which the\ndecision subject then selects.\nHowever, this methodology presents fundamental limitations in establishing relationships among\nfeatures."}, {"title": "Fundamental Limitations of This Approach", "content": "a. Arbitrary Nature of Normalization in Decision-Making\n[From the perspective of the decision-maker, the] normalization [process] based simply on the\ndistribution of data is somewhat arbitrary. [For instance,] one decision maker might scale the axes such\nthat increasing income by $5,000 annually is equivalent to an additional year on the job. A competing\nlender, using different training data, could conclude that $10,000 of income corresponds to one year of\nwork. These lenders might therefore produce different explanations depending on the scaling of\nattributes (Barocas et al. 2020, p84). While differences in training datasets used by the two cases may\nexplain these discrepancies, they do not justify such arbitrariness.\nNormalization techniques typically rely entirely on data distribution and do not incorporate an external\npoint of reference. However, without an external point of reference to ground these scales, the meaning\nof the relative difference in feature values is unclear (Barocas et al. 2020). In practice, decision-\nmakers consider an external decision boundary to determine the optimal combination that crosses it,\nmaking normalization indispensable. As a result, the relationships among features are no longer\ndetermined by their intrinsic interactions but rather dictated by external objectives, such as external\npoints of reference or an external decision boundary. This process distorts the genuine nature of feature"}, {"title": null, "content": "$\\\\min cost(a; x) s.t f(x + a) = 1 and a \\\\in A(x)$. Where $A(x) \\\\rightarrow R_+$ is a cost function that encodes preferences\nbetween actions, or measures quantities of interests for an audit. Users can specify any cost function that satisfies\ntwo properties: (i) cost(0; x) = 0 (no action \u2194 no cost); (ii) cost(a; x) \u2264 cost(a + e1;; x) (larger actions \u2194\nhigher cost) (Ustun et al. 2019, p12)."}, {"title": null, "content": "relationships, as they become subject to external constraints rather than reflecting inherent\ndependencies. In our study, the normalization process, which evaluates feature relationships solely\nthrough quantitative weighting within a single coordinate plane, is referred to as externalization. This\nexternalization process is akin to sketching an object labeled \"relationship\" on a canvas a mere\napproximation rather than a faithful representation of its true structure.\nb. Overly Simplistic and Coarse Representation of Feature Relationships\nAnother significant limitation of the externalization process is that it oversimplifies and coarsely\nrepresents the intricate relationships among features. To better understand this, we need to first\nrecognize an implicit assumption in (Ustun et al. 2019). An implicit assumption in formulations of\nrecourse is that if attribute X is actionable and attribute Y is actionable, then X and Y are jointly\nactionable. We contend that this is not necessarily so (Venkatasubramanian and Alfano 2020, p290).\nConsider a case where an individual seeks a loan and can modify employment period, income, or job\nchange as actionable features. Suppose the decision subject can either increase their salary by $3,000\nor extend their employment period by eight months to improve their creditworthiness.\nIf the individual switches jobs, their income increases by $2,000 immediately. In this case, switching\njobs directly results in a $2,000 salary increase.\nHowever, job changes also reset employment period, meaning the individual's employment history\nis negatively impacted.\nThis demonstrates that changing a single feature (job change) can positively affect one feature (income)\nwhile negatively impacting another (employment period). Furthermore, the magnitude of these effects\ndiffers across features. Yet, flipset methodology (Ustun et al. 2019)\u2014relying solely on quantitative\nweighting via normalization-fails to capture these complex interdependencies among features."}, {"title": "3) Multiple Geometry (MG) \u2013 Internalization Methodology (IM)", "content": "Representative works addressing the mathematical, technological, and logical challenges mentioned\nabove include (Arvanitidis et al. 2018, 2019, 2020, 2021; Dombrowski et al. 2024; Dominguez-Olmedo\net al. 2023; Joshi et al. 2019; Karimi, von K\u00fcgelgen, et al. 2020; Karimi et al. 2021, 2023; K\u00fcgelgen et\nal. 2023; Louizos et al. 2017; Pegios et al. 2024). These works avoid reducing the relationships among\nfeatures to external criteria (externalization process) and instead incorporate the relationship between\ntopological phase of features. This approach is referred to as the Internalization Methodology (IM). This\nstudy focuses particularly on the works of (Arvanitidis et al. 2018, 2020, 2021; Dombrowski et al. 2024;\nDominguez-Olmedo et al. 2023; Joshi et al. 2019; Pegios et al. 2024)."}, {"title": "A. (Joshi et al. 2019)", "content": "(Joshi et al. 2019) [modifies, as a purely mathematical approach to addressing this,] the underlying\ngeometry in which the distance associated with recourse is being calculated. In particular, [the]\ndependency among attributes correspond[s] to moving along a submanifold of the underlying vector\nspace - informally, [it is] a curved surface in the space, rather than the entirety of the space itself\n(Venkatasubramanian and Alfano 2020, p292). By adopting this method, the distance to the boundary\nof the manifold becomes the key to understanding feature relationships. Unlike (Ustun et al. 2019),\nwhich measure distances non-differentially in a two-dimensional coordinate plane based on external\nconditions (e.g., decision boundaries), (Joshi et al. 2019) calculate distances along submanifolds\nimplied in the coordinate plane, thus revealing the topology of feature relationships.\n(Joshi et al. 2019) addresses this issue as an optimization problem through the following equation.\n$x' = \\\\arg \\\\min_z \\\\min_\\\\lambda l ((log y_{do(t)}, 1) + \\\\lambda c(x*, G_o (z)))$  (1)\nHowever, it is crucial to note that even though the distance calculation method of (Joshi et al. 2019) is\nconducted under the assumption of the manifold hypothesis, it must also be examined through the lens\nof the practical implementation of dimensional expansions in Euclidean space. From this perspective,\nthe submanifold induced by optimization method (Joshi et al. 2019) creates a virtual surface that differs\nfrom the actual representation of dimensional expansion. This is because the optimization process (Joshi\net al. 2019) method is performed along external boundaries, rather than in the intrinsic space of the\nexpanding dimensions (manifold) - [a point also highlighted as] informally, a curved surface in the\nspace, rather than the entirety of the space itself (Venkatasubramanian and Alfano 2020,\np292). Therefore, the optimization performed along external boundaries may be associated with the\nreality of dimensional expansion but ultimately creates a virtual submanifold that is somewhat aloof\nfrom the actual dimensional expansion itself.\nIn consideration of the above aspect, describing the dependence of features on a virtual surface\nfollowing the external boundaries of all dimensions (Venkatasubramanian and Alfano 2020) essentially\nseparates the understanding of dimensional expansion (represented as a reduction in the coordinate\nplane) from feature relationships. While dimensional expansion is modeled through the submanifold,\nfeature relationships are instead processed as properties of a virtual surface. Such a separation, even\nwhen premised on Euclidean space, contradicts the fundamental requirement that features must exist,\nat least, within the dimensional space itself. Consequently, while the primary advantage of (Joshi et al.\n2019)-revealing the topology of feature relationships through distance calculations based on a\nsubmanifold is critical, it is, to a degree, diluted due to the separation between dimensional expansion\nand feature relationships.\nEqually important as addressing the mathematical challenges is tackling the logical and technical tasks\nof encoding these manifolds into Variational Autoencoders (VAEs). Classifier-based decision systems"}, {"title": null, "content": "utilizing optimization methods may implicitly represent feature relationships to capture information\nsuch as shortest distances. However, these systems often operate based on correlation rather than\nprediction accuracy, potentially learning spurious correlations and yielding erroneous outcomes\n(Caruana et al. 2015; Joshi et al. 2019). [This issue arises due to the failure to encode causal relationships\nbetween variables, which has exposed] the importance of learning and deploying causal models in\npractical decision-making systems. In a causal decision-making system, one of the primary goals of\ncausal decision systems is to evaluate outcomes under different \"treatments\" and use interventions\ncorresponding to the treatment that improve the outcome (Joshi et al. 2019, p5).\nAs a concrete implementation method, (Joshi et al. 2019) explicitly utilizes a causal model employing\nconfounders rather than relying on counterfactual-based Structural Causal Models (SCM) (Karimi,\nBarthe, et al. 2020; Karimi et al. 2021, 2023; K\u00fcgelgen et al. 2023) to learn manifolds. In other words,\n[the causal model] in developing such decision-making systems attempts to (approximately) learn in\nthe presence of hidden confounders (Louizos et al. 2017; Madras et al. 2019) by estimating these\nconfounders (Joshi et al. 2019, p5). During the learning process, the back-door adjustment formula is\nemployed (Pearl et al. 2016). The main assumption made [here] is that hidden confounders can be\nreasonably estimated via latent variable models leveraging (approximate) learning algorithms (Joshi et\nal. 2019, p5).\nMeanwhile, (Joshi et al. 2019) highlights the limitations of methods that approximately estimate\nhidden confounders as follows:\n\"While methods that approximately estimate hidden confounders are an empirical improvement\nover classification systems (Louizos et al. 2017), a myriad of issues ranging from mis-\nspecification of the underlying causal model, approximations used for tractability of the latent\nvariable estimation, and selection bias in the data can cause causal models to be less than perfect.\nAlso, while more accurate, outcomes can still be undesirable for many individuals scrutinized\nunder such systems (providing treatment still does not improve outcome even though on average,\ntreatments are effective)\" (Joshi et al. 2019, p6).\nB. (Tosi et al. 2014) and (Arvanitidis et al. 2018)\nOn the other hand, the issues inherent in the optimization-based distance calculation method of (Joshi\net al. 2019)\u2014namely, dimensional expansion being performed through submanifold settings while\nfeature relationships are separated and processed as a virtual surface along the external boundaries of\nthe submanifold\u2014can be linked to the identifiability problem in latent variable models, where different\nrepresentations can give rise to identical densities (Arvanitidis et al. 2018; Bishop 2006). In a\nmathematical and technical sense, such virtual surface can be understood as straight lines in the latent\nspace Z of VAEs or Generative Adversarial Networks (GANs). [However, because the] straight lines in\nZ are not shortest paths in any meaningful sense and therefore do not constitute natural interpolants\n(Arvanitidis et al. 2020, p1)"}, {"title": null, "content": "To address this, (Joshi et al. 2019) reformulated the aforementioned mathematical and technical\nchallenges of optimization into a logical and technical problem of causality. [Specifically, they noted\nthat] classification based decision-making systems are limited in that they do not encode causal\nrelationships between variables while potentially learning spurious correlations (Caruana et al. 2015,\np5). Through latent variable models capable of utilizing approximate learning algorithms, they\nattempted to resolve the issue of causality by estimating hidden confounders. Nevertheless, this\napproach addresses mathematical and technical challenges indirectly, by reframing them as logical and\ntechnical problems. Therefore, there remains a need to solve these challenges directly as mathematical\nand technical issues.\nIn this regard, it has been proposed to endow the latent space with a Riemannian metric such that curve\nlengths are measured in the ambient observation space X (Arvanitidis et al. 2018, 2020; Tosi et al. 2014).\nThis approach immediately solves the identifiability problem. In other words, this ensures that any\nsmooth invertible transformation of Z does not change the distance between a pair of points, as long as\nthe ambient path in x remains the same (Arvanitidis et al. 2020, p1).\nThis can be expressed in mathematical terms as follows,\n$Length[f(rt)] = \\\\int_0^T||f'(yt)||dt = \\\\int_0^T||J_{yt}yt||dt, J_{yt} = \\\\frac{\\\\partial f}{\\\\partial z}|_{z=yt},$ (2)\n$||J_{yt}yt|| = ||\\\\frac{\\\\partial f}{\\\\partial y}(yt)||=\\\\sqrt{(I_{yt})^T (J_{yt})}= \\\\sqrt{(IJ_y)y} = \\\\sqrt{y^TM_yy},$   (3)\nHere, $J_y y$ is the Jacobian matrix of $f(yt))$ Furthermore, in Eq. (2), M\u2081 = $JJ_y$ is a symmetric positive"}, {"title": null, "content": "definite matrix that defines the Riemannian metric.\nFurthermore, in the context of Riemannian geometry, the Jacobian matrix $J_y$ is always positioned on\nthe left and right of the Riemannian metric, in the form of a transposed matrix and an original matrix,\nrespectively. Additionally, the Riemannian metric placed between them represents a level that is one\nhigher than the"}]}