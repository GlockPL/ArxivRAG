{"title": "FAS: Fast ANN-SNN Conversion for Spiking Large Language Models", "authors": ["Long Chen", "Xiaotian Song", "Andy Song", "BaDong Chen", "Jiancheng Lv", "Yanan Sun"], "abstract": "Spiking Large Language Models have been shown as a good alternative to LLMs in various scenarios. Existing methods for creating Spiking LLMs, i.e., direct training and ANN-SNN conversion, often suffer from performance degradation and relatively high computational costs. To address these issues, we propose a novel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking LLMs in two stages. The first stage employs a full-parameter fine-tuning of pre-trained models, so it does not need any direct training from scratch. The second stage introduces a coarse-to-fine calibration method to reduce conversion errors and improve accuracy. Our experiments on both language and vision-language tasks across four different scales of LLMs demonstrate that FAS can achieve state-of-the-art performance yet with significantly reduced inference latency and computational costs. For example, FAS only takes 8 timesteps to achieve an accuracy of 3% higher than that of the OPT-7B model, while reducing energy consumption by 96.63%.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs), with recent success in various applications, e.g., GPT-3 (Brown et al., 2020), LLaVA (Liu et al., 2024) and LLaMA 3 (Dubey et al., 2024), have become strong candidates in various tasks. However, all these models suffer from high energy consumption, mainly due to Floating-Point Multiplication and Addition (MAC) operations. For example, training GPT-3 consumes ~1,287 MWh of energy (de Vries, 2023), which is equivalent to the annual energy consumption of 120 households. In recent years, a low-power alternative to vanilla LLMs, Spiking LLMs, have appeared, which are based on Spiking Neural Networks (SNNs), inspired by the spiking sig-"}, {"title": "2. Related Works", "content": "Existing ANN-SNN conversion methods can be categorized into two groups: one-stage and two-stage conversion."}, {"title": "2.1. One-stage ANN-SNN Conversion", "content": "One-stage ANN-SNN conversion aims to directly convert ANN models to SNN models without any further optimization on the converted SNN models, e.g., fine-tuning. This type of method focuses on reducing the conversion as much as possible. For instance, Cao et al. (Cao et al., 2015) initially introduced the one-stage method by training ANNs with ReLU activation functions and then replacing these activations with spiking neurons. Based on this, Diehl et al. (Diehl et al., 2015) proposed model-based and data-based normalization to narrow the gap between ANN and SNN. Furthermore, Sengupta et al. (Sengupta et al., 2018) introduced scaling methods to normalize weights and thresholds of SNNs, improving the conversion performance. To further mitigate conversion loss, Rueckauer et al. (Rueckauer et al., 2016) and Han et al. (Han & Roy, 2020) introduced a \"reset-by-subtraction\u201d mechanism, which preserves temporal information and reduces information loss, enhancing precision during conversion. More recently, Bu et al. (Bu et al., 2022) analyzed conversion error and proposed the Quantization Clip-Floor-Shift activation function to replace ReLU in ANNs. This method can effectively approximate the SNN activation function and reduce conversion loss. One-stage ANN-SNN conversion has shown promising performance, however, it typically requires a large number of time steps to achieve SOTA performance. Note that 'time step' is the number of cycles used to analog the dynamic behavior of neurons, and plenty of time steps can cause the lengthy inference latency and huge energy consumption of the SNN model. Therefore, it is impractical to apply the one-stage methods to complex datasets and models with larger parameter scales, i.e., LLMs."}, {"title": "2.2. Two-stage ANN-SNN Conversion", "content": "Two-stage ANN-SNN conversion involves additionally optimizing the SNN converted by the one-stage methods to further to improve its performance. For example, SPR (Hao et al., 2023a) proposed an optimization strategy that uses residual membrane potential to reduce unevenness errors for converted SNN models. Similarly, COS (Hao et al., 2023b) optimized the converted SNN models by shifting the"}, {"title": "3. Preliminary", "content": ""}, {"title": "3.1. Analog Neuron Model for LLMs", "content": "LLMs are typically composed of Transformer architectures, which are structured layer by layer. The output \\(a^l\\) of the neurons in the l-th layer is achieved through a linear weighted combination followed by a nonlinear mapping:\n\\[ a^l = f(W^l a^{l-1}), \\]\nwhere \\(W^l\\) is the weight matrix of the l-th layer, and \\(f(\\cdot)\\) is the nonlinear activation function, e.g., ReLU or GELU."}, {"title": "3.2. Spiking Neuron Model", "content": "For SNN, we follow the conversions (Diehl et al., 2015; Han et al., 2020; Deng & Gu, 2021) and consider the Integrate-and-Fire (IF) neuron model (Cao et al., 2015). Its kinetic behavior can be represented by Eq. (2):\n\\[ v^l(t) = v^l(t - 1) + W^lS^{l-1}(t)\\theta^{l-1} - S^l(t)\\theta^l, \\]\nwhere \\(v^l(t)\\) represent the membrane potential at time steps t in the i-th layer. \\(W^l\\) and \\(\\theta^l\\) are the weight matrix and firing threshold of the IF neuron, respectively. \\(S^l(t)\\) denotes the transmission of discrete spikes at the l-th layer at time steps t. Note that when \\(v^l(t - 1) + W^lS^{l-1}(t)\\)\\theta^{l-1}\\) exceeds the threshold \\(\\theta^l\\), the IF neuron is activated, so \\(S^l(t)\\) equals 1. Otherwise, the IF neuron is inhibited and \\(S^l(t)\\) equals 0."}, {"title": "3.3. Conversion Error of ANN-to-SNN", "content": "ANN-SNN conversion aims to establish a consistent relationship between the analog neurons and the spike rates of IF neurons. The spike rate \\(r^l(T)\\) can be represented as Eq. (3) (more details are in Appendix A):\n\\[ r^l(T) = clip(\\frac{0}{T} | \\frac{TW^lr^{l-1}(T) + v^l(0)}{\\theta^l} , 2, 0, 0'). \\]\nSpecifically, the conversion can be achieved by mapping the activation value \\(a^l\\) of ANNs (see Eq. (1)) to the spike rate \\(r^l\\) of SNNs (see Eq. (3)). However, the conversion process still has three types of conversion errors: \u2460 Quantization error: The spike rate is a discrete distribution, with values occurring at regular intervals of \\(\\theta^l/T\\). When \\(a \\in [k\\theta^l/T, (k + 1)\\theta^l/T]\\), it is mapped to \\(k\\theta^l/T\\). The discrepancy \\(a^l - k\\theta^l/T\\) is a source of errors. \u2461 Clipping error: This is caused by the different value ranges of ANN and SNN. Specifically, when \\(a^l \\in [0, a_{max}]\\) and \\(r^l \\in [0, \\theta^l]\\), where \\(a_{max}\\) denote the max value in \\(a^l\\), the value \\(a^l \\in [\\theta^l, a_{max}]\\) will be all mapped to \\(\\theta^l\\), also generating errors. \u2462 Temporal error: It refers to the inconsistency between \\(a^l\\) and \\(r^l\\) due to the fluctuation in the temporal sequences of spike arrivals in activation layers. This variation can result in a higher or lower number of spikes than expected, resulting in poor performance."}, {"title": "4. Methodology", "content": ""}, {"title": "4.1. Overall Framework", "content": "As discussed in Section 3.3, the quantization errors and clipping errors come from the process of discretizing the continuous ANN activation function, and the temporal errors are caused by using the disordered temporal sequences to generate spikes. The proposed FAS method is tailored for eliminating these errors in LLMs, which can secure the high-performance and low-energy spiking LLMs. Specifically, the overall framework of FAS is shown in Figure 2, consisting of two stages. The arrow between the gray circle on the left and the middle blue circle represents two types of errors, Quantization error and Clipping error, respectively, totally denoted as QC errors. The arrow between the middle and right circles represents Temporal errors. These errors are addressed in the two stages of FAS. More specifically, Stage 1 addresses QC errors through full- parameter fine-tuning. Stage 2 employs a layer-wise and neuron-wise coarse-to-fine calibration optimization strategy to minimize Temporal errors. The details of the algorithm are presented in Appendix B, and the two stages of FAS will be discussed in the next sections."}, {"title": "4.2. Stage 1: Eliminating QC Errors", "content": "Inspired by Eq. (3), we select a continuous step function to replace the activation function in ANN to approximate the activation function of SNNs, thereby eliminating QC errors. In this paper, we consider Quantization Clip-Floor-Shift (QCFS) function (Bu et al., 2022), which is described as:\n\\[ a^l = f(W^l a^{l-1}) = clip(\\frac{\\theta^l}{L} , f(a^{l-1}), \\frac{\\lambda^l}{2}). \\]"}, {"title": "4.3. Observations from Post-Stage-1 Analysis", "content": "To better understand the source of the remaining error, i.e., temporal error, we conducted an in-depth analysis.\nDefinition. We define Theoretical Maximum Spike Count \\(\\psi^l\\) of neuron i in the l-th layer as max of Theoretical Spike Count \\(T_{theor}\\) during the interval [0, T] for all data, that is:\n\\[ \\psi^l = Max(T_{theor}) = Max(\\frac{a^l}{\\lambda^l}). \\]\nwhere \\(a^l/\\lambda^l\\) represents the normalized output in the ANN. \\(T_{theor}\\) denotes the number of spikes needed by an SNN neuron to accurately represent \\(a^l\\). \\(\\psi^l\\) indicates that the \\(T_{theor}\\) in the i-th neuron will not exceed \\(\\psi^l\\). If the spike count in the SNN matches \\(T_{theor}\\), the conversion error would be zero. However, due to the temporal error, the actual spike count typically does not equal \\(T_{theor}\\). Based on the definitions, we have made the following three observations:\nObservation 1. By lowering the thresholds when \\(T_{theor} < T\\), the temporal error can be reduced.\nThe maximum activation value \\(\\theta\\) in the SNN is aligned with the upper activation bound \\(\\lambda^l\\) of the ANN, thereby eliminating clipping error. However, when \\(a^l\\) is lower than \\(\\theta\\), \\(\\theta^l\\) can be set within the range \\([a^l, \\theta^l]\\) without affecting the firing rate mapping to \\(a^l\\), provided that other data is not considered. Furthermore, temporal error, caused by firing more or fewer spikes than expected, can be expressed as:\n\\[ Error_T = |\\frac{T_{real} - T_{theor}}{T}|, \\]"}, {"title": "4.4. Stage 2: Eliminating Temporal Error", "content": "To minimize temporal error, we first reduce the overall temporal error through layer-wise calibration of the thresholds and initial membrane potentials. Then, we perform neuron-wise calibration to optimize each neuron.\nLayer-wise calibration (LWC): As shown in Observation 2, adjusting the threshold and initial membrane potentials in each layer can reduce temporal error. Thus, we optimize the thresholds and initial membrane potentials as:\n\\[ \\theta^l = \\alpha^l * \\theta^l, \\tilde{v(0)}^l = \\beta^l * \\theta, \\]\nwhere \\(\\theta^l\\) is the threshold of the l-th layer, \\(v(0)^l\\) is the initial membrane potential of the l-th layer; \\(\\alpha^l\\) and \\(\\beta^l\\) are their optimization weights; \\(\\tilde{\\theta}\\) and \\(\\tilde{v(0)}^l\\) represent the optimized thresholds and initial membrane potentials, respectively. The optimal value of \\(\\alpha^l\\) can be determined by analyzing the distribution of theoretical spike counts in the training set.\nNeuron-wise calibration (NWC): For each neuron, we set a trainable threshold \\(\\theta\\) and initial membrane potential \\(v(0)\\), with initial values set after layer-wise calibration. Using the ANN as a guide, we input the same data into both the ANN and SNN for forward propagation, minimizing the distance between the firing rate of each SNN neuron and the output of the corresponding ANN neuron. We freeze all model parameters except the thresholds and initial membrane potentials of the IF neurons, then update these parameters for each neuron to achieve neuron-wise optimization. Due to the discrete and non-differentiable nature of spikes, standard backpropagation cannot be used here, so we employ BPTT (Lee et al., 2016; Wu et al., 2017) for calibration.\nNext, the proposed loss functions used for backpropagation are discussed, i.e., activation align loss and logits loss.\nActivation align loss: To minimize the temporal error of SNN, the firing rate of IF neurons should align with the activation values of the ANN. The loss function can be described as:\n\\[ L_a = mse(a^l, r^l) = mse(f(W^l a^{l-1}), \\frac{1}{\\rho} \\sum_{t=1}^{\\rho} S_t^l), \\]\nwhere \\(\\rho\\) denoted the time steps used for calibration. \\(a_i^l\\) is the output of the i-th neuron in the l-th layer, while \\(r\\) signifies the fire rate of the i-th neurons in the l-th layer.\nLogits loss: Following (Hinton et al., 2015), we use logits loss, which lets the SNN learn the prediction distribution of the ANN. To measure the distance between two distributions, we choose KL-divergence:\n\\[ L_{logits} = -\\sum_i Softmax(\\frac{a_i}{\\tau})log(Softmax(\\frac{r_i}{\\tau})), \\]\nwhere \\(\\tau\\) is the temperature parameter, \\(a\\) represents the output of the ANN, \\(r\\) represents the spike fire of the SNN, and c represents the number of classes. Therefore, the total loss contains two terms:\n\\[ L_{all} = \\lambda_1 \\sum L_a + \\lambda_2L_{logits}, \\]\nwhere \\(\\lambda_1\\) and \\(\\lambda_2\\) are the hyper-parameters that control the weight of activation align loss and logits loss, respectively."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Datasets & Baselines & Settings", "content": "To evaluate the proposed FAS method, we selected various language and vision-language tasks. More details are presented in Appendix C. In addition, we chose various SOTA models, including LLM and multimodal LLM with 7B parameters, as the peer competitors, and their details are provided in Appendix D. Note that the experiment settings are also presented in Appendix E."}, {"title": "5.2. Experiments on NLU Tasks", "content": "Performance Analysis on Bert: To the best of our knowledge, this study is the first to report and analyze the performance of an ANN-SNN conversion-based spiking LLM on multiple tasks. Table 1 compares FAS with other SOTA models in NLU tasks using BERT, demonstrating FAS achieves new SOTA performance across seven text classification datasets. Specifically, on the QNLI and MRPC tasks, FAS surpasses other directly trained SNN models (the second block of Table 1) by at least 5%. On the QQP and STS-B tasks, SNN-TextCNN fails to converge, while FAS performs significantly well. Compared to other ANN-SNN methods (the third block of Table 1), FAS outperforms the QCFS by 10% on the RTE task, the SRP by 5.21% on the MNLI-m task, and the COS by 6.36% on the MRPC task. Notably, FAS achieves superior performances with time steps of only 4, indicating the low latency of the model."}, {"title": "5.3. Experiments on NLG Tasks", "content": "Table 5 presents the results of FAS using the GPT-2 architecture on the Enwik8 and WikiText-103 datasets. It outperforms all other methods with low latency inference (T=16). For Enwik8, FAS achieves 0.968 BPB, whereas the QCFS and SRP methods reach 1.016 and 1.014 BPB at 32 time steps. COS reaches 1.01 BPB at 16 time steps, but re-"}, {"title": "5.4. Experiments on Vision-Language Tasks", "content": "To validate the generalization of FAS, we conducted experiments on the multimodal Pailgemma-3B and LLaVa-1.5-7B. As shown in Table 4, FAS outperformed the baseline Pailgemma-3B model on the HallusionBench and BLINK benchmarks with 8 time steps. Moreover, despite having only 3B parameters, FAS exceeded the performance of sev-"}, {"title": "5.5. Ablation Study and Impact of Hyper-Parameters", "content": ""}, {"title": "5.5.1. PARAMETER p :", "content": "We investigate the impact of the hyperparameter calibration steps p in Stage 2 of FAS. Figure 4 illustrates the performance of BERT and GPT-2 across different p values, revealing that p significantly affects model performance. Table 6 provides detailed performance for GPT-2 with various p values, showing that the SNN accuracy tends to converge as p gradually approaches T. When p = T, the best results are achieved. Furthermore, when T is very small T < 2, setting p = T leads to significant improvements. This indicates"}, {"title": "5.5.2. LWC AND NWC OF STAGE 2:", "content": "As described in Stage 2, the proposed FAS method involves Layer-wise calibration (LWC) and Neuron-wise calibration (NWC). As shown in Table 7, compared to the first row, where none is present, NWC and NWC can both bring improvement. The performance is the best for every time step when both are present. This indicates LWC and NWC have a positive impact on the performance of FAS."}, {"title": "5.5.3. PARAMETER L:", "content": "To minimize the time steps T, it is intuitive to set L as small as possible. However, setting L too low will reduce the model's capacity, leading to lower accuracy in the converted SNN. As shown in Table 8, increasing L initially can improve the performance of SNN, but causes a performance drop when L reaches 16. Choosing L is a trade-off between achieving high accuracy and maintaining low latency SNN."}, {"title": "5.5.4. PARAMETER A1, A2 :", "content": "The impact of the weights \\(\\lambda_1\\), for activation alignment loss, and \\(\\lambda_2\\), for logits loss (see Eq.10) are shown in Table 9. They do show an impact on accuracy, especially with small time steps. When T>8, their impact becomes minor."}, {"title": "5.6. FAS' Effectiveness on Temporal Error", "content": "The distributions of thresholds and initial membrane potentials before and after applying Stage 2 are shown in Figure 5a. Most optimized thresholds are lower than their original, with a few outliers. Figure 5b depicts the optimized initial membrane potentials, primarily clustered above 0, with some values exceeding \u00b110. We further analyzed the MSE between \\(r^l\\) and \\(a^l\\), as shown in Figure 5c. The MSE reductions in the last four layers are dramatic, 83.95%, 84.42%, 84.8%, and 86.66%, confirming that the proposed adjustments can effectively minimize temporal error."}, {"title": "6. Conclusion", "content": "FAS, a fast ANN-SNN conversion method tailored for LLMs, is presented. It aims to leverage the low-cost computing benefits of Spiking LLMs while maintaining high performance. The conversion process is optimized through a two-stage strategy: firstly, full-parameter fine-tuning is applied so training from scratch is not needed. Secondly, a coarse-to-fine calibration strategy is proposed to further minimize conversion errors, particularly temporal errors. Experiments demonstrate that FAS can achieve both high performance and low latency. The evaluation using both"}, {"title": "A. The Spike Rate of SNNS", "content": "This section present the details to get the functional representation of spike rate. First, as indicated in Subsection 3.2, the kinetic behavior of IF neuron can be represented by Eq. (11):\n\\[ v^l(t) = v^l(t - 1) + W^lS^{l-1}(t)\\theta^{l-1} - S^l(t)\\theta^l, \\]\nwhere \\(v^l(t)\\) represent the membrane potential at time steps t in the i-th layers. \\(W^l\\) and \\(\\theta^l\\) are the weight matrix and firing threshold of the IF neuron, respectively. \\(S^l(t)\\) denotes the transmission of discrete spikes at the l-th layer at time steps t. Note that when \\(v^l(t - 1) + W^l S^{l-1}(t)\\theta^{l-1}\\) exceeds the threshold \\(\\theta^l\\), the IF neuron is fired, and \\(S^l(t)\\) equals to 1. Otherwise, the IF neuron is muted and \\(S^l(t)\\) equals to 0.\nBy accumulating Eq. (11) over time steps 1 to T, the spike rate \\(r^l(T)\\) of layer l can be obtained by Eq. (12):\n\\[ r^l(T) = W^l r^{l-1}(T) + (-\\frac{v^l(T) - v^l(0)}{T}). \\]\nIt can be seen from the formula that \\(r^l\\) and \\(r^{l-1}\\) have a linear relationship, similar to the activation function in ANNs. Therefore, we can map the activation value \\(a^l\\) of analog neurons in ANNs to \\(r^l\\) of IF neurons in SNNs. When \\(0 < v^l(T) < \\theta^l\\) and \\(W^l r^{l-1}(T) \\in (0, \\theta^l)\\), Eq. (12) can be approximated as below in Eq. (13):\n\\[ \\tilde{S}^l(T) = | \\frac{0}{T}|.\nFinally, combining the situation \\(W^l r^{l-1}(T) \\notin (0, \\theta^l)\\), the spike rate \\(r^l\\) of IF neurons at layer l can be represented as a continuous step function, as shown in Eq. (14):\n\\[ r^l(T) = clip(\\frac{0}{T} | \\frac{T W^l r^{l-1}(T) + v^l(0)}{\\theta^l} , 2, 0, 0'). \\]"}, {"title": "B. Algorithm Details of FAS", "content": "The detailed steps of FAS are in Algorithm 1. Lines 2-4 are the processes of Stage 1, addressing QC errors through full-parameter fine-tuning. More specifically, it starts by replacing the activation function with QCFS (Line 2) and fine-tuning the model on the datasets D and \\(\\hat{D}\\) (Line 3). Subsequently, the weights are transferred from the fine-tuned ANN model to the SNN model (Line 4). Lines 6-11 describe Stage 2, which employs a layer-wise and neuron-wise coarse-to-fine calibration optimization strategy. Each layer undergoes layer-wise calibration (Line 7) followed by neuron-wise calibration on mini-batches sampled from \\(\\hat{D}\\) (Line 10)."}, {"title": "C. Datasets", "content": "This is the supplementary for Section Datasets & Baselines. For NLU tasks, we chose seven different types of tasks, i.e., six classification and one regression tasks, from the GLUE benchmark. We selected Quora Question Pair (QQP) and Microsoft Research Paraphrase Corpus (MRPC) for classification tasks, and Semantic Textual Similarity Benchmark (STSB) for regression task to evaluate our FAS on similarity and paraphrase tasks. For inference tasks, we opted for MultiGenre Natural Language Inference (MNLI), Question Answering NLI (QNLI), and Recognizing Textual Entailment (RTE) datasets. For single-sentence-based sentiment analysis tasks, we chose Stanford Sentiment Treebank (SST-2).\nFor NLG task, we chose the following two classic text classification datasets, i.e., Enwik8 and WikiText-103, to evaluate the text generation performance of FAS. Specifically, the Enwik8 dataset is a large-scale text dataset consisting of the first 100 million characters from Wikipedia. It is widely used for character-level language modeling and text generation tasks, providing a challenging benchmark for models due to its extensive and varied content. The Bit-Per-Byte (BPB) metric is commonly employed to assess its performance. In addition, the WikiText-103 dataset is another comprehensive text dataset derived from Wikipedia articles. It contains over 100 million words and is known for its high-quality, naturally occurring text. WikiText-103 is commonly used for training and evaluating language models, particularly in tasks involving text generation, language modeling, and machine translation. Perplexity (PPL) is the metric of choice for evaluating the performance.\nFor vision-language tasks, several key benchmarks are widely used. BLINK is a benchmark for multimodal language models, consisting of 14 classic computer vision tasks reformatted into 3,807 multiple-choice questions. It is designed to evaluate visual perception abilities such as relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning. HallusionBench, on the other hand, focuses on image-context reasoning in large visual-language models. It contains 346 images paired with 1,129 expert-crafted questions, assessing logical consistency, response tendencies, and failure modes like language hallucination and visual illusion. MMMU is another crucial benchmark for evaluating multimodal models on advanced, college-level tasks. With 11.5K questions across six core disciplines, 30 subjects, and 183 subfields, it tests perception and reasoning with domain-specific knowledge across 32 heterogeneous image types, including charts, diagrams, maps, and chemical structures."}, {"title": "D. Baselines", "content": "Following Section Datasets & Baselines, we selected various SOTA baseline models to verify the effectiveness of our FAS on NLU and NLG tasks.\nNLU tasks - The baselines are as follows:\n\u2022 CBOW (Wang et al., 2018): CBoW is a simple sentence representation technique that averages the GloVe embeddings of individual words, ignoring syntactic structure and contextual dependencies.\n\u2022 BILSTM (Wang et al., 2018): BiLSTM combines LSTM networks with a bidirectional structure to capture both past and future context in sequences\n\u2022 BiLSTM+Attn (Wang et al., 2018): BiLSTM+Attn combines BiLSTM's sequence understanding with Attention's focus on relevant sentence parts.\n\u2022 GenSen (Subramanian et al., 2018): GenSen is a multi-task learning framework that combines diverse objectives to learn general-purpose sentence representations, leading to improved performance on various NLP tasks.\n\u2022 SNN-TextCNN (Lv et al., 2023): It is a variant of the TextCNN that combines spiking neural networks.\n\u2022 BERT (Devlin et al., 2019): BERT is a bidirectional language model based on the Transformer Encoder-only architecture and an auto-encoding training paradigm.\n\u2022 spikeBERT (Lv et al., 2024): It transfers knowledge from the transformer-based BERT model to the spiking neuron-based architectures with knowledge distillation.\n\u2022 SpikeLM (Xing et al., 2024): SpikeLM is a novel language model based on SNN that addresses the performance limitations of traditional SNNs in language tasks. By employing an elastic bi-spiking mechanism, SpikeLM achieves"}, {"title": "E. Experiment Settings", "content": "This section supplements the section of Datasets & Baselines. To conserve GPU memory, we employed DeepSpeed's ZeRO-2 optimization, utilizing mixed-precision computation on two Nvidia RTX 3090 GPUs, each with 24GB of memory. For stability, gradient clipping was applied with a threshold of 1. The AdamW optimizer was used throughout.BERT was fully fine-tuned on the WikiText-103 dataset, whereas GPT-2 was trained on 0.3 billion tokens from the FineWeb-Edu dataset (Lozhkov et al., 2024).\nFor downstream tasks, we utilized the respective task's training dataset. In the absence of a standard split, we followed the convention (Lv et al., 2023), randomly selecting 10% of the samples as the test set. The hyperparameters were set as follows: \u03b1\u00b2 = 0.6 and \u03b2\u00b2 = 0.1 for both BERT and GPT-2, \u03bb\u2081 = 1 and X2 = 0.0012 for GPT-2, and \u5165\u2081 = 1 with A2 ranging from 0.2 to 1 in increments of 0.1, selecting the optimal result.\nMore detailed settings of the learning rate and epochs for each task are presented in Table 10. More specifically, in stage 1, BERT is initially trained on the WikiText-103 dataset used a threshold learning rate of 0.01 and other parameters set at 5 \u00d7 10-5, over 3 epochs. Fine-tuning on downstream tasks adjusted the threshold learning rate between 0.01 and 0.08, with other parameters ranging from 2 \u00d7 10-5 to 6 \u00d7 10\u22125. In addition, GPT-2 is trained on the FineWeb-Edu dataset and used a threshold learning rate of 0.001. Other parameters in training GPR-2 are set at 9 \u00d7 10\u22125, covering 0.3 billion tokens in 1 epoch. Fine-tuning on WikiText-103 set the threshold learning rate to 0.005 and other parameters to 1.2 \u00d7 10\u22124. In stage 2, the learning rate of BERT is ranged from 0.01 to 0.06, depending on the convention of different downstream tasks. Furthermore, the learning rate of GPT-2 is set to 0.005 on the WikiText-103 dataset."}, {"title": "F. Analysis of Power & Energy Efficiency", "content": "Since the converted model still involves MAC operations, we focused our comparison solely on the energy consumption of the spiking parts and their corresponding parts in the ANN. Specifically, synaptic operations in SNNs vary depending on spike sparsity with sparse accumulation (AC). In contrast, synaptic operations involving multiplication and accumulation (MAC) in ANNs remain constant within a defined network structure. We measure floating-point AC and MAC operations, using 0.9 pJ per AC and 4.6 pJ per MAC, as reported in (Li et al., 2021).\nTo quantitatively assessrgy savings, we compare our converted GPT-2 with their ANN counterparts (Merolla et al., 2014) in terms of performance and energy consumption. Table 11 lists the results relative to those of ANN in percentiles for FAS and SOTA ANN-SNN conversation methods QCFS, SRP and COS. On the GPT-2 model, FAS outperforms other ANN-SNN conversion methods in terms of accuracy. In the case of GPT-2, FAS can achieve similar ANN performance with time steps 2, 4 & 8. Other methods could not go above the category of \"2-3\" of performance drop, e.g., the increase in perplexity.\nThen, we visualize the sparsity of our optimized SNN as shown in Fig. 6, which illustrates the spike rate of all layers of the GPT-2 model using the WikiText-103 dataset with T = 4. A spike rate of 1 means that the numbers of operations in the ANN and SNN are identical. Fig. 6 reveals that the maximum spike rate observed is below 0.64, while the minimum is around 0.25. This suggests that our FAS-generated SNN can significantly reduce the required operations compared to the ANN counterpart.\nThe analysis of power and energy efficiency demonstrates the following:\n1. FAS achieves SOTA performance across all time steps, surpassing the LLM. In particular, the accuracy of FAS with 16 time steps using the BERT model exceeds that of its ANN counterpart and other SOTA methods.\n2. FAS runs fast, especially under similar energy consumption. For example, on GPT-2 models, the energy consump-tion of FAS and QCFC is 7.04% and 8.99% under 4 and 8 time steps, respectively. As a result, FAS achieves lower perplexity, and the reduced number of time steps results in faster inference times."}, {"title": "G. The Impacts of Parameter p on BERT", "content": "We examine the effect of the hyperparameter calibration steps p in Stage 2 of FAS. Table 12 shows performance of BERT with different values of p. It is evident that p significantly influences performance. When p = 2, BERT consistently achieves better results across"}]}