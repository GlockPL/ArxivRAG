{"title": "Robust AI-Generated Text Detection by Restricted Embeddings", "authors": ["Kristian Kuznetsov", "Eduard Tulchinskii", "Laida Kushnareva", "German Magai", "Serguei Barannikov", "Sergey Nikolenko", "Irina Piontkovskaya"], "abstract": "Growing amount and quality of AI-generated texts makes detecting such content more difficult. In most real-world scenarios, the domain (style and topic) of generated data and the generator model are not known in advance. In this work, we focus on the robustness of classifier-based detectors of AI-generated text, namely their ability to transfer to unseen generators or semantic domains. We investigate the geometry of the embedding space of Transformer-based text encoders and show that clearing out harmful linear subspaces helps to train a robust classifier, ignoring domain-specific spurious features. We investigate several subspace decomposition and feature selection strategies and achieve significant improvements over state of the art methods in cross-domain and cross-generator transfer. Our best approaches for head-wise and coordinate-based subspace removal increase the mean out-of-distribution (OOD) classification score by up to 9% and 14% in particular setups for RoBERTa and BERT embeddings respectively. We release our code and data.", "sections": [{"title": "1 Introduction", "content": "The proliferation of generative AI leads to an explosion in AI-generated content. Large language models (LLMs) can produce text that is very similar to human-written. However, AI-generated text can be used for malicious purposes, which leads to the artificial text detection (ATD) problem: has a given text or image been created by an AI model or a human? Existing approaches for artificial text detection can be divided into score-based and classifier-based. The former aim to identify and measure features that distinguish artificial text from real; e.g., generated text may exhibit statistical artifacts due to the specific generation process used by a language model (Gehrmann et al., 2019), the difference may lie in perplexities measured by another language model (Solaiman et al., 2019), curvature of the probability function (Mitchell et al., 2023), or intrinsic dimensionality of contextualized representations (Tulchinskii et al., 2023). However, score-based methods often rely on prior knowledge about a specific generator and/or semantic domain, and known traces may be easy to remove, e.g., by paraphrasing the text (Krishna et al., 2023). One notable exception is the intrinsic dimension feature for text content, shown to be robust to domain transfer and paraphrasing (Tulchinskii et al., 2023), but its overall detection quality is relatively modest.\nSupervised classification methods show almost perfect in-domain detection quality, but fail to generalize to unseen text topics and writing styles (Wang et al., 2024b; Tulchinskii et al., 2023). The choice of training data, both artificial and generated, is crucial for successful out-of-distribution (OOD) transfer. In general, while usually there exist features that can distinguish between natural and artificial subsets of the training set, the classifier may lock into dataset-specific spurious differences and hence generalize poorly. It is hard to say in advance if a classifier trained on a given dataset will generalize well to new unseen generators and data sources. Previous approaches to OOD detection for ATD include UID-based detectors (Venkatraman et al., 2023) and domain adversarial training (Bhattacharjee et al., 2024), but most of these methods are very data-intensive (Wang et al., 2024a).\nIn this work, we aim to improve supervised classification by ignoring spurious features to enhance cross-distribution robustness, training on small number of domains or generator models. Namely, we focus on methods of extracting residual subspaces and deleting information from embeddings. In many applications, retaining only important dimensions of high-dimensional data while treating projections onto less loaded subspaces as residual noise can benefit downstream tasks. However, for tasks such as OOD detection the principal components of a dataset may be the least useful. Kamoi and Kobayashi (2020) found that nullifying the first (least important) principal components in the embedding space fine-tuned on in-distribution (ID) data enhances OOD detection quality; this is known as the partial Mahalanobis distance. Podolskiy et al. (2021) conducted similar analysis for Transformer-based text classifiers and found that ID data has orthogonal classes and lies on a unit sphere in a low-dimensional space. The main difference between ID and OOD data lies in the residual subspace, hence the partial Mahalanobis distance performs well in OOD detection.\nIt is important to note that not all neural networks learn useful residual subspaces for a given dataset; e.g., Podolskiy et al. (2021) and Ren et al. (2021) find that on text, CNN classifiers learn representations where components with low singular values contain too much information about ID data, making it difficult to distinguish OOD examples.\nIn this work, we apply similar techniques to artificial text detection (ATD). Distribution shift, with variations in text styles, topics, and new generation models, is a major challenge for ATD. Supervised classifiers, even performing well on validation datasets, struggle in realistic settings, where the domain and model of the AI-written text are unknown. To address this, we first show that training a classifier on some residual subspace obtained by coordinate removal or layer pruning may significantly enhance ATD robustness under domain and model shift. Next, we show that controllable subspace removal can improve robustness, while also providing us with interpretable information about AI-written texts and domains. In particular, we use recent advances in concept erasure (Belrose et al., 2023), experimenting with erasing semantic and syntactic concepts based on probing tasks by Conneau et al. (2018); we show that some concepts are harmful for cross-domain and cross-model transfer.\nOur primary contributions are: (i) a first application of the residual subspace approach for robust ATD; we show that restricting the detector to a residual subspace increases cross-topic and cross-model robustness with especially significant improvements on the most difficult samples; (ii) analysis of different residual decomposition techniques, such as nullifying head-wise subspaces in intermediate data representations and concept erasure; (iii) analysis of applicability of our methods with different encoder- or decoder-based backbone models. Besides, we create and release an extension for one of the datasets with recent generating model GPT-4-0 on three domains. Below, Section 2 surveys related work, Section 3 describes the proposed methods, Section 4 introduces the datasets, Section 5 presents a comprehensive experimental evaluation, and Section 6 concludes the paper."}, {"title": "2 Related Work", "content": "Linear subspaces in Transformer-based models are known to represent concepts. Hernandez and Andreas (2021) studied low-dimensional subspaces that encode linguistic features in BERT; linear structure is known for such concepts as truthfulness (Marks and Tegmark, 2023) and sentiment (Tigges et al., 2023). This direction has been extended to the linear representation hypothesis that posits that language models operate with one-dimensional representations of concepts in the activation space (Bricken et al., 2023; Park et al., 2023). However, Engels et al. (2024) showed that some concepts are multi-dimensional.\nComponents of Transformer-based embeddings can provide useful features via the geometry of their inner representations or parameter spaces. For instance, outlier dimensions in the embedding spaces of models such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), characterized by unusually high variance and/or mean values, have been studied in detail, including their emergence during training and effects of disabling them post-training (Kovaleva et al., 2021), their relationship with positional embeddings and impact on word-in-context tasks (Luo et al., 2021), influence on the quality of representations (Timkey and van Schijndel, 2021), and relations to the shapes of attention maps and token frequencies (Puccetti et al., 2022). Activations of Transformer-based LMs have been investigated for language structure information (Jawahar et al., 2019), semantic and syntactic features (Conneau et al., 2018); the latter work also introduces a comprehensive selection of probing tasks. However, only outlier dimensions have been studied in full detail; we aim to address this gap by studying how removing specific dimensions from RoBERTa embeddings can improve detection of artificially generated text.\nSemantics of attention heads in Transformers have been studied for a long time: Kovaleva et al. (2019) provided empirical research on BERT attention heads, demonstrating overparameterization by pruning some of them, Michel et al. (2019) showed"}, {"title": "3 Methods", "content": "Removing unnecessary features is often an effective method to improve the robustness of a machine learning model. The embedding space has linear substructures responsible for linguistic features such as token frequencies, word-in-context information etc. (Luo et al., 2021; Puccetti et al., 2022). We aim to detect and erase such substructures, which are harmful for ATD generalization."}, {"title": "3.1 Linear decompositions of embeddings", "content": "PCA and the standard basis. Let x be some text input, $z \\in R^d$, its embeddings obtained by some model, $z = M(x)$, $C = \\{c_1,...,c_d\\}$, a basis of $R^d$, and let $a_i$ be coefficients of z in C, $z = \\sum_{i=1}^{d} a_i c_i$. We want to split C into good and bad parts, $C = C_g \\cup C_\\flat$, so that components in $C_g$ contain most of the information general for all domains, while $C_\\flat$ is responsible for spurious domain-specific features. Then, we construct a classifier on restricted embeddings $z'$ where the \u201cbad\u201d part is nullified, $z' = \\sum_{i \\in C_g} a_i c_i$. Intuitively, information about the style, topic, and other semantic properties is harmful for ATD, and we want to focus on residual features that are less important for other NLP tasks. Podolskiy et al. (2021) show that PCA can serve as such a decomposition for a Transformer-based model: removing top components computed for an in-domain dataset improves OOD detection. Indeed, for a dataset of natural texts D, subspace $\\langle C_\\flat \\rangle$ should \u201cexplain\u201d the data variability, while the variance of D projected on $\\langle C_g \\rangle$ is expected to be low. PCA is a theoretically optimal way to find such subspaces (see Appendix A.1).\nDespite PCA's solid theoretical background, in practice it does not always perform well; in ATD, we usually deal with a small dataset that cannot fully capture the real distribution, which is bad for PCA. To access data properties beyond those represented in our train set, we propose to utilize the internal structure of the pretrained embedding model. Indeed, Transformer-based models tend to disentangle some data properties during training, and semantic interpretation has been discovered for some neurons and embedding dimensions (Luo et al., 2021; Timkey and van Schijndel, 2021; Puccetti et al., 2022). We hypothesize that such \"built-in\" disentanglement could lead to meaningful subspaces spanned by a subset of the standard basis, i.e., vectors $\\{e_1 = [1, 0, . . ., 0], e_2 = [0, 1, . . ., 0], . . ., e_d = [0, 0, . . ., 1]\\}$. Projection to a subspace $\\langle e_{i_j} | i_j \\in S \\rangle$ for some subset of indices S can be done by simply nullifying all embedding dimensions except S. Our experiments support this intuition: PCA-based decomposition does not lead to any significant changes in detector's quality (see"}, {"title": "Attention heads as linear substructures", "content": "Both decompositions discover global linear structure, i.e., universal directions in the embedding space independent of input data. But it is much more natural to rely on local linearity of the data and try to discover substructures in a data manifold that does not necessarily form global linear subspaces. For text embeddings, the neural network represents a function from $R^{d \\times T}$ to a data manifold M. We can decompose this function into a sum of input-dependent components of the same functional form. Cammarata et al. (2020) proposed linear circuits, showing that the data flow in a Transformer can be represented as the main residual stream with linear addition of flows from other elements of the model (attention heads and feed-forward blocks). We are mostly interested in attention flows because it is well known that attention heads in Transformers have highly specialized functions (Kovaleva et al., 2019; Pande et al., 2021), so we hypothesize that head-wise decomposition should reflect the \"built-in\" disentanglement of the pretrained model. We can represent a Transformer-based embedding as\n$z = \\Pi [\\alpha(x)x_0 + \\sum_{l=1}^{L} \\beta_l(x)MLP^l + \\sum_{l=1}^{L} \\sum_{h=1}^{H} \\gamma_{l,h}(x)A^{l,h}(x)],$"}, {"title": "Concept erasure", "content": "Finally, we consider an embedding space decomposition based on extracted linear directions or low-rank subspaces responsible for some harmful semantic feature $Z_F$. If such a direction is found, we can remove it by subtracting the component corresponding to this direction from the embedding. Namely, we erase the feature as\n$z' = z - P_F(z),$"}, {"title": "3.2 Subspace removing methods", "content": "Greedy search. Our basic approach chooses the best features using a small subset of domains. Given a multi-domain dataset $D = D_1 \\cup ... \\cup D_k$, where $D_i$ are domain subsets, we choose two domains $D_{search} = \\{D_1, D_2\\}$ to perform feature selection. On each step, we train a classifier on $D_1$, removing one component, and look how its performance changes on $D_2$, getting a feature ranking on $D_1 \\to D_2$ transfer. Then, we do the opposite, getting a ranking for $D_2 \\to D_1$. The final set of residual features is obtained as the union of top-score lists in both rankings (see Appendix B.4).\nHead pruning removes some components in decomposition (1) by replacing the output of a given head with zeros on inference. Importantly, this approach is approximate because, besides its direct impact as a component in the decomposition, each head also has an indirect influence on all computations on subsequent layers. But Gandelsman et al. (2023) showed that this indirect impact is small and can be ignored (see also Appendix A.2). To choose the set of heads for pruning, we note that different layers contain different kinds of information (e.g., semantic information is mostly in bottom and middle layers), and the linguistic complexity of tasks solved by attention heads grows from bottom to top (Kovaleva et al., 2019; Tenney et al., 2019). Therefore, we simply prune every layer separately.\nConcept erasure by probing tasks. To remove a linear subspace responsible for some data properties, we apply a concept erasure technique called LEACE (Belrose et al., 2023). Suppose we have a k-class classification task defined by a dataset Z with one-hot labels Y, and we want to erase all the knowledge required for linear separation of the classes. LEACE is a projection-based method of the form (2), with theoretical guarantees that any linear classifier on top of z' cannot solve the classification task better then a constant predictor. Erasing a concept from an embedding z is defined as\n$\\tilde{z} = z - W^+(W \\Sigma_{ZY}) (W \\Sigma_{ZZ})^+\\widetilde{W} z,$"}, {"title": "4 Data", "content": "ATD datasets. There are few high-quality datasets with both human and artificial text. One such dataset was presented by Wang et al. (2024b) and used in the SemEval-2024 competition; it covers five domains: Wikipedia, Reddit, WikiHow, PeerRead, and arXiv. We have used five text generation models: GPT-3.5 (Schulman et al., 2022), Davinci003, Cohere, Dolly-v2 (Conover et al., 2023), and BLOOMz (Muennighoff et al., 2023). Since the amount of human-written text in each domain is larger than generated by each model, we crop human data so that there are 3000 samples of parallel data for each domain and model/human combination.\nOur second dataset, used by Tulchinskii et al. (2023), has three domains\u2014Wikipedia, Reddit, StackExchange\u2014with davinci003 generations. Compared to SemEval, it has a larger distribution shift in basic text features (e.g., length), which makes it harder for cross-domain transfer. We extend it by adding similar text generated by GPT-4o: continuing text from Wikipedia articles, long-form question answering on Reddit Q&A and StackExchange. Thus, we obtain a dataset, called below GPT-3D, with six domain-model pairs.\nExperimental setup. Similar to Wang et al. (2024b), we create two tasks for SemEval dataset: (1) in the cross-domain task, we concatenate data across generating models, getting five binary ATD tasks in different domains; (2) in the cross-model task, concatenation across domains yields five binary ATD tasks for each generator model. Thus, results are presented as 5 \u00d7 5 heatmaps and its aggregations.\nFor GPT-3D we report average OOD scores, i.e. the accuracy of classifiers trained on one domain-model subset and evaluated on the rest; average accuracy values do not include training sets.\nFor more technical details, see Appendix B.\nProbing datasets. For probing and concept erasure experiments, we use the dataset used by Conneau et al. (2018) with several supervised classification tasks: SentLen, predicting the length of the sentence, TreeDepth, finding the depth of a syntactic tree, TopConst, classifying the high-level syntactic structure (top two nodes in the syntax tree), classifying Tense, SubjNum (subject number), and ObjNum (object number) in the main clause, detecting errors with BShift (bigram shift, word order inversion in a bigram), SOMO (Semantic Odd Man Out, where a word is replaced with a random grammatically fitting word), and CoordInv (Coordination Inversion, whether the coordination of two clauses in the sentence is inverted), and predicting exact words from a 1000-word vocabulary in WC (Word Content)."}, {"title": "5 Results and Analysis", "content": "Here we present results on baseline, heads pruning, concept erasure and selecting coordinates. PCA-based results are reported in Appendix H.\nBaseline RoBERTa. As a baseline we use logistic regression (LR) trained on mean-pooled RoBERTa embeddings. Results are shown in Fig. 1 for SemEval and Fig. 3a for GPT-3D; the cross-domain and cross-model settings are challenging in both tasks. Fig. 1 shows that in-domain classification is almost perfect for baseline LR on RoBERTa embeddings, but the cross-domain part is very inconsistent: e.g., transfer from Reddit to PeerRead works well across all models (91% avg accuracy) but transfer from arXiv to WikiHow is uniformly bad (54%). In SemEval, Wikihow is the hardest domain to transfer to, while Arxiv is the hardest domain to transfer from (Table 1); both domains contain syntactic anomalies (very few or many \"!\" and \"?\" marks, unusual average sentence lengths etc.). Bloomz is the hardest model to transfer both to and from (Table 1), and it also generates unusual texts (very short sentences replete with \"!\" and \"?\"). But generally, it is not easy to predict which transfer direction is easier in ATD or explain the reasons for it; e.g., Wikipedia, often used for NLP model evaluation (Merity et al., 2016), is far from the best basis for transfer, especially in the cross-model setting (Fig. 3a). We also compare (Fig. 3f) our proposed methods with the approach based on the intrinsic dimensionality (PHD) of real and artificial texts tokens embedding point clouds, according to (Tulchinskii et al., 2023).\nAverage transfer results. Table 3 and Fig. 3 show that our methods provide a stable improvement of OOD scores for classifiers trained on separate domain-model subsets, for both SemEval and GPT-3D datasets. TopConst concept erasure yields the highest increase among methods that do not have access to OOD data (+3%), and improvement increases for the most difficult domain pairs (e.g., +6% for Wikipedia\u2013Reddit). Interestingly, the PHD method by Tulchinskii et al. (2023), while providing very stable cross-domain results for GPT-3-based generations, completely fails to deal with GPT-4o (Fig. 3 (f)), while our methods increase cross-model scores up to 10%. Still, results for the most difficult pairs are unsatisfactory, falling below the random baseline; the only method that can achieve at least random level for any OOD subset is head pruning, where the heads are selected on validation set combined of all models and domains examples (+9.1% \u201ccross-all\" compared to full RoBERTa, Fig. 3 (e)). Further we describe results for each method in details and in the Appendix D we describe the combination of methods.\nHead pruning for transfer tasks. We adapt head pruning (Voita et al., 2019) to remove a whole layer of attention heads. Since layers of a model have rough linguistic meanings (Jawahar et al., 2019), thus we analyse the impact of structural-level information on ATD. Fig. 2 and Table 2 show detailed results for each layer pruned on SemEval. Removing the first layer improves average cross-domain accuracy by 3%, but the improvement is unstable (from -7.1% to +18.9% in different domains). Pruning layers 3 and 4 is more stable and beneficial in both settings. Cross-domain ATD is more challenging; Fig. 2 (top) shows that some domains (Wikipedia and WikiHow) exhibit similar patterns but others are unrelated. The best scores are in transfer from Reddit, achieving 81% mean balanced accuracy with 0-th layer pruned (+5% to full RoBERTa). The cross-model setting is easier and not greatly affected by pruning layers, with the exception of BLOOMz. Here the best source model is GPT-3.5-davinci, with 92% cross-model accuracy after removing layer 4.\nConcept erasure. Generally, results on SemEval show that the best concepts to erase are TopConst and TreeDepth, improving up to 2.1% on cross-domain transfer and not hurting the cross-model transfer. Erasing WC also performs well but is less stable. Figs. 4 give more detailed information. Although changes compared to Table 7 are marginal on average, they range from -8.5% to +13% across domains and models. Grammatical properties, (Tense, SubjNum, ObjNum) have no significant impact, while erasing global syntax information (TopConst, TreeDepth) improves cross-domain transfer up to +13%, especially from wikipedia and arxiv. This means that LLMs in general are not good in mimicking complicated syntactic structures, but have no problem with local grammatical categories. Erasing WC erasure leads to the largest cross-domain improvement, which means that word semantics produce domain-specific spurious features that harm generalization. There is one outlier: wikihow\u2192arxiv; we hypothesize that these domains have common word-level features due to many bullet points, numbered lists, and sequential structures in both. For cross-model transfer, erasing all three tasks related to error detection in sentence structure (BShift, CoordInv, SOMO) are harmful for ATD performance and robustness; erasing global syntax (TopConst, TreeDepth) improves performance, while word content (WC) leads to contradictory results.\nWe conclude that the ability to detect grammatically correct sentences is crucial for robust AI-generated text detection; the difference in global syntax between natural and generated texts is significant, but varies between models and domains, so erasing this information helps generalization, and individual word semantics is a source of spurious features. On the other hand, world-level grammatical categories are captured well by all generators and do not influence ATD performance.\nSelecting embedding components and heads. To evaluate component removal, we use Reddit and Wikipedia domains from GPT-3D as Dsearch, as they have the lowest cross-domain ATD accuracy. For head selection, we used a lay-off evaluation set with samples of all generators and domains from GPT-3D. We evaluate on GPT-3D and SemEval, using the same set of removed heads or components. Fig. 3 and Table 3 show the results; transfer to and from Wikipedia and Reddit subsets has improved. Head selection greatly improves performance on validation domains, achieving the best scores among all the methods. In cross-task transfer (from GPT-3D to SemEval, Appendix C), component and head removal works better if components are chosen on the same data distribution where the classifier is trained; still, cross-dataset transfer here is generally on par with the baseline.\nInfluence of the embedding model. RoBERTa is commonly used as the encoder for ATD (Krishna et al., 2023; Solaiman et al., 2019; Tulchinskii et al."}, {"title": "6 Conclusion", "content": "In this work, we aim to improve the robustness of artificial text detectors via linear feature removal from text embeddings. We propose three ideas that are extremely easy to implement and achieve stable improvement in robustness averaged across domains and models, up to 14% depending on the text encoder. More importantly, we conclude with the following novel insights from our work.\nFirst, new generation models can completely break detectors; e.g., on the GPT-4 family previous detectors' perform below random, while on the same model classifiers demonstrate very high performance in the cross-domain setting. The reason could be the presence of watermarks in GPT-4 generations; if so, watermarks unknown for ATD developers are dangerous, leading to unpredictable black-box behaviour.\nSecond, performance with respect to the training subset is often counterintuitive; e.g., a classifier trained on Wikipedia may perform worse than on Reddit, although Wikipedia is considered a cleaner domain, better suited for general-purpose models.\nThird, Transformer encoders learn disentangled intrinsic features in coordinates and attention heads, and simple decompositions perform better for ATD than more complex approaches. But this effect is less pronounced for decoder models. We plan to study differences in the geometry of encoder and decoder-based text representations in future work.\nFinally, global syntax and sentence complexity is a key point for ATD, but the exact differentiating features are domain- and model-specific, so this information should be ignored. Local grammatical categories do not provide an important signal for ATD. Instead, the classifier should rely on features for detecting various types of inconsistencies."}, {"title": "7 Limitations", "content": "In this work we show how state of the art ATD methods may fail, for instance, to transfer to new generative models. Our method increases OOD performance on some generators, but there is no guarantee that this property will be preserved for all future models. Novel pretraining techniques, data collection and processing paradigms, and model architectures can change the picture entirely. Since our method is based on supervised classification, it is not clear which features are actually important for it. It can also lead to unexpected results, especially in the presence of watermarks, small changes in data distribution inside each generated sample deliberately injected by generative model developers. We believe that for truly reliable ATD detection, all conclusions should be interpretable, so that a human analyst could inspect the decision. By proposing the concept erasure approach, we have made a step towards interpretable ATD.\nWe have tested our approaches using relatively small subsets of uni-model or uni-domain data and demonstrated promising quality improvements. Nevertheless, it is still not identical to real-world scenarios, where at least several domains and generators are available in training time, and even more have to be considered during the model's application. One of our objectives in this work has been to propose a novel direction that can significantly improve ATD methods in the future and make them more reliable, but currently it is not yet a fully practical production-ready solution.\nFinally, we do not address the real-word case of post-processed and paraphrased generations, and also texts partially written by humans. For example, if some sentences of this section have been generated by GPT-4o but then partially corrected by the authors, most probably the methods considered in this work would not be able to detect it. We leave this direction for further study."}, {"title": "A Residual subspaces for ATD", "content": "A.1 Formal definitions and theory\nIn this subsection, we introduce formal definitions and recap some statements from linear algebra that are useful for a better understanding of the geometry and properties of residual subspaces. First, we define the notion of explained variance and relative explained variance to be able to quantify the properties of residual subspaces.\nDefinition 1 (Subspace explained variance (Shen and Huang, 2008; Gandelsman et al., 2023)). Let $D \\subset R^d$, $D = \\{x_1,...,x_x\\}$ be a dataset, and $S \\subset R^d$ is an arbitrary subspace, with $P_S(x) : R^d \\to S$ being the projection function onto S. We call the variance of the projections $P_S(D)$ the explained variance of subspace S with respect to D:\n$EV_D(S) = E_D[||P_S(x - E[x])||^2] = \\frac{1}{X} \\sum_{x \\in D} ||P_S(x) - P_S(\\mu)||^2,$"}, {"title": "A.2 Head-wise decomposition", "content": "In our derivation of the form of head-wise flows, we follow the ideas proposed by Gandelsman et al. (2023). In the following, we consider Transformer blocks with post-layer-normalization, such as in BERT and RoBERTa models. The transformation inside each layer can be written as\n$z_1 = LN(z_{l-1} + MHA(z_{l-1})),$\n$z = LN(z_1 + MLP(\\tilde{z}_1)), where$\n$LN(x) = \\frac{x - \\bar{x}}{||x - \\bar{x}||^2},$"}, {"title": "B Technical details of the experiments", "content": "B.1 Preprocessing and models\nFor text preprocessing, we only replaced consecutive spaces, trailing spaces, and a newline character with one space, as was done by Tulchinskii et al. (2023).\nFor embeddings extraction, we used standard pretrained models from the Hugging-Faces library: roberta-base (125M parameters), microsoft/phi-2 (2.7B parameters), bert-base-uncased (110M parameters). We use each text sample as an input for chosen model and obtain the resulting embedding from the last layer of this model. We take the mean pooling of that embedding to decrease the dimensionality and get a vector of dimension 768; this is our text feature vector.\nFor all further experiments with embeddings, we use the logistic regression model from the scikit-learn package on the training subset with default"}, {"title": "B.4 Greedy search for embedding components", "content": "Recall that for these experiments, we chose two domains $\\{D_1, D_2\\}$, and train the classifier on subset $D_1$, using corresponding feature vectors of size d. To find the \"harmful\" subspace, we start to remove the components of these feature vectors one-by-one. First, we train the classifier with 0-th component of the feature vector removed, than with 1-st component removed and so on, up to the last d-th component, remembering, which component removal increases out-of-domain accuracy on $D_2$ the most (or decreases it the least). After finding that most \u201charmful\" component, we remove it for good and repeat the process again for the vector of size d \u2013 1 to see, which one from the remaining components is the most harmful (or least useful) now. We repeat this process until only one component remains in the feature vector.\nAfter this, we get a list of the removed components and correseponding accuracy scores. We"}, {"title": "H PCA", "content": "We investigated the PCA decomposition of the embedding spaces of RoBERTa, BERT and Phi-2. We tried to remove components with highest and lowest variance to check how it affects the overall accuracy and generalization abilities of the models. The results are shown in Figures 13 and 14.\nFigure 13 shows that while we remove PCA components of the RoBERTa embedding space with the largest variance, the transferability between the different domains and models drops significantly. At first, the transferability from GPT-4o to GPT-3.5-davinci goes down to random; next, transferability between different domains of texts generated with GPT-3.5-davinci goes down to random; and finally, transferability between GPT-3.5-davinci and GPT-40 drops down. Interestingly, transferability"}]}