{"title": "A Multi-model Approach for Video Data Retrieval in Autonomous Vehicle Development", "authors": ["Jesper Knapp", "Klas Moberg", "Yuchuan Jin", "Simin Sun", "Miroslaw Staron"], "abstract": "Autonomous driving software generates enormous amounts of data every second, which software development organizations save for future analysis and testing in the form of logs. However, given the vast size of this data, locating specific scenarios within a collection of vehicle logs can be challenging. Writing the correct SQL queries to find these scenarios requires engineers to have a strong background in SQL and the specific databases in question, further complicating the search process. This paper presents and evaluates a pipeline that allows searching for specific scenarios in log collections using natural language descriptions instead of SQL. The generated descriptions were evaluated by engineers working with vehicle logs at the Zenseact on a scale from 1 to 5. Our approach achieved a mean score of 3.3, demonstrating the potential of using a multi-model architecture to improve the software development work- flow. We also present an interface that can visualize the query process and visualize the results.", "sections": [{"title": "1 Introduction", "content": "The amount of software in modern cars is continually growing and is measured in hundreds of millions of lines of code that process petabytes of data. One of the major drivers of this development in the automotive industry is the autonomous driving functionality. As software advances in autonomous capability, moving from features such as adaptive cruise control and assisted parking toward com- plete autonomy, the need for increasingly sophisticated software increases [14]. The many systems required to make autonomous vehicles a reality, generate a large amount of data that needs to be processed and stored at a high rate [12]. As autonomous vehicles evolve, this data generation will continue to accelerate. Modern high-end vehicles are equipped with multiple imaging sensors for tasks like object recognition. However, autonomous vehicles usually require diverse"}, {"title": "2 Related Work", "content": "The number of studies on the use of LLMs in code generation, summarization and testing is constantly growing (over 1,000 studies published in only 2023 [5]). Many different implementations of table-to-text generation have been explored previously. One example of table-to-text generation is the generation of biogra- phies using Wikipedia information tables, where the authors trained a language model to output a biography from an input table of information for the person [7]. The technique used to extract information from each field of the table and"}, {"title": "3 Research Design", "content": "We address the proposed research questions by conducting an design science research project with our industrial partner Zenseact [13]. We combined em- pirical evaluation with the stakeholders with artefact design and computational experiments to evaluate it."}, {"title": "3.1 Dataset", "content": "The data used in this paper are collected from test drives conducted by Zenseact. Signals: During test drives, all sensors and cameras generate high-frequency data, which is processed and combined into a comprehensive set of signals. These signals provide crucial information to several key systems within the car. Each drive's data is saved as a log file and can be accessed as a table in parquet format. These tables can contain up to 6,000 columns, each representing a processed signal. Examples of these signals include detecting snow in the lane ahead or determining if the vehicle is currently inside a tunnel. Each log corresponds to a single test drive, with durations ranging from less than an hour to a full eight-hour day. The data table captures the entire dura- tion of the drive, resulting in tables with thousands of signals and hundreds of thousands of data points per signal, often totaling on the order of $10^8$ entries. To manage this vast amount of data, logs are divided into scenarios, each rep- resenting a 30-second segment of the drive. This segmentation reduces the size of each scenario's table to the order of $10^6$ entries, though this is still too large for traditional methods of feeding all the data to a LLM. Videos: The signal data is complemented with video footage captured from a forward-facing camera mounted on top of the vehicle. To utilize both data sources and match the corresponding signals scenarios, these videos are divided into 30-second segments and saved in mp4 format."}, {"title": "3.2 Architecture", "content": "The architecture we use is shown in Figure 4, it is a data retrieval pipeline with the following stages: Preprocessing (yellow box): converts signal logs and video logs into textual descriptions. Combining Descriptions and Embedding (grey box): combines the textual descriptions via text generation model(Gemma 7B), extracts embeddings (via BGE-large) and stores them in the form of vector through ChromaDB. Querying Specific Scenarios (pink box): queries specific scenarios using nat- ural language. It compares the similarity between query embeddings and saved embeddings via ChromaDB, then displays the top N closest scenarios based on the settings. The results are returned in a JSON file containing metadata and a link to the Zenseact's internal database, which can be used to access the matched scenarios."}, {"title": "3.3 Models", "content": "Each of the three parts of the architecture use distinct models due to different modalities of the data in each stage. LLaVA-1.5-7b: The Large Language-and-Vision Assistant (LLaVA) is a multimodal language model that excels across various benchmarks by utilizing GPT-4 generated language-image instructions during training. We opted for the smallest 7-billion-parameter model due to its capacity to handle prompts up to 4,096 tokens and process images at a resolution of 336x336 pixels [9]. Gemma-7b: Gemma, an open-source model family from Google [15], offers two sizes: 2 billion and 7 billion parameters. After assessing both, Gemma 7b emerged as the preferred choice. While the 7b model utilizes 4-bit quantization for its weights, the 2b model employs 8-bit quantization. Despite the higher resolution of weights in the 2b model, the advantages of the larger model size outweighed this consideration. With a token limit of 8,192, ample space is avail- able for few-shot prompting and generating lengthy descriptions; however, the quantization might potentially hinder the model's ability to comprehend exten- sive inputs and outputs. BGE-large: To facilitate semantic comparison between generated scenario descriptions and user queries, we employ a text embedding model. After evaluat- ing various options using the Hugging Face Massive Text Embedding Benchmark (MTEB) Leaderboard [10], we prioritize the \"retrieval\" performance, assessing how effectively a model can compare a query with a collection of documents and retrieve the most relevant ones. A standout performer in this metric is BGE-large, boasting high scores while maintaining a relatively small size of 335 million parameters. BGE is a part of C-pack, a comprehensive package com- prising training data, benchmarks, and embedding models. Available in different sizes, we opt for the largest model due to its superior performance, as indicated in C-pack: Packaged resources to advance general chinese embedding. [16]."}, {"title": "4 Evaluation", "content": "To evaluating the generated descriptions and embeddings, we aim to provide an interface that enables users to query our dataset and visualize the results. The interface for 'Genius' streamlines the querying and debugging processes for users. Illustrated in Figure 5, the interface is divided into distinct sections."}, {"title": "4.2 Model Selection", "content": "To quantitatively evaluate the impact of the chosen text generation model on the final extraction, we designed a test process. In this process, we query Genius for a scenario known to exist within a predefined set of scenarios. We then group the distances of each response based on the originating scenario and plot these distances. This approach illustrates the variation within the same scenario and highlights the differences between correct and incorrect answers. Our test set includes descriptions of each scenario, with 10 iterations totaling 80 embedded scenarios. Each iteration yields different results, enabling us to test the variability and stability of the process. This procedure is repeated for each model to ensure consistent evaluation."}, {"title": "4.3 Retrieval Evaluation", "content": "To quantitatively evaluate the retrieved embedding for the 100 scenarios, a few varied queries are analyzed using the following metrics: Largest Gap (LG): Measures the largest gap in distance between adjacent scenarios Min Distance (Min D): Measures the distance to the closest scenario Max Distance (Max D): Measures the distance to the furthest scenario Range: Measures the range of distances, e.g. Max Distance - Min Distance Standard Deviation (SD): Measures the standard deviation of the scenario distances Relative Largest Gap (Rel. LG): Measures the Largest Gap relative to the Range In order to gain an metric to evaluate correct from incorrect answers an Average Relative Largest Gap (ARLG) metric is calculated. By querying a set of queries with known correct answers and taking the ARLG, a standard ARLG for correct answers can be calculated. By comparing the ARLG measured for the test set, 14.5% against the baseline ARLG (26.6% + 11.1%)/2 = 18.8%, it is observable to see that the average characteristics of the test set answers are closer to the characteristics of a query without answers rather than one with"}, {"title": "4.4 Survey", "content": "To evaluate the quality of the scenario descriptions generated, a form was sent out to engineers working with the log data at the Zenseact. The 8 scenarios were split into 4 forms with 2 scenarios in each. The forms included links to an internal visualization tool for the specific scenarios along with the correspond- ing descriptions generated by Genius. The developers were asked to grade the"}, {"title": "4.5 Evaluation of RQs", "content": "RQ1: Current LLM models can not fully utilize the vast tabular data that was used in this paper, at the very least not with the smaller size of models that we deployed, instead, some sacrifice will have to be made in what signals the model should utilize. Based on our findings from evaluating the descriptions generated when using a subset of signals, the model can generate descriptions that are acceptable and capture some of the most important aspects of a vehicle scenario. RQ2: During embedding and model evaluations, we found that using both signals and video information as input resource does indeed enhance the de- scriptions of the scenarios. The model effectively captures key aspects such as weather, road, and traffic conditions, and type of environment. Combining these visual data with signal data, which capture more complex car experiences, re- sults in a more detailed and complete scenario description. However, it sometimes misses details like other cars and confuses urban with rural settings, likely due to heavy quantization or the model's 336x336 pixel resolution. Additionally, the multimodal LLM processes images one at a time; while it is feasible to input images sequentially to denote a video, we opted for a single frame per scenario to expedite processing, albeit at the expense of potentially missing subsequent scenario details. RQ3: Our finding shows that using an embedding model to embed the se- mantic meaning of a vehicle scenario works well to group similar scenarios close together. Some limitations exist in our solution, the text embeddings model"}, {"title": "5 Conclusion and Future Research", "content": "The current implementation of the artifact may not replace SQL-based scenario searches but could serve as a complementary tool for finding specific scenarios challenging to detect through signal data alone, such as \"snowy scenario in a rural area\" or \"intersection in an urban area with traffic lights,\" accessible only from video data. The artifact's search functionality has proven stable, yielding satisfactory results. When querying for existing signals and scenarios, similar scenarios are typically returned first, albeit not always in the most fitting order. Searching for scenarios functions well at a larger scale, although finding homogeneous scenarios like \"lane change\" remains challenging due to its frequent occurrence across various scenarios. While determining answer correctness without manual verification was chal- lenging, the keyword search method effectively flagged incorrect answers by iden- tifying semantic similarities. However, it struggled with distinguishing between correct and incorrect answers when common keywords were present in all sce- narios. The biggest limitation with the current approach is that it can not process all the signals in the data, this is due to the fact that there are several thousand signals and LLMs have a limited context size. Due to hardware limitations we also choose to process just one frame of the camera feed for each scenario, this results in limited knowledge of information that is only present in the camera feed. The set of scenarios that were generated for the evaluation survey were specif- ically chosen to contain events that are captured in these signals, therefore the results might not have been as clear with a different selection of scenario cate- gories. However, information from video footage of the scenario was also utilized, therefore information that was present in a signal, for example if the car was in a tunnel or not, could have been captured anyway even that signal was missing. This fact might have impacted the results of the evaluation form that we sent out to developers at Zenseact in our favor, since we are using scenarios where the utilized signals are relevant. To improve results, future work should focus on three main areas: first, filter and preprocess signal data by incorporating more relevant signals and exclud- ing those that remain unchanged during scenarios; second, increase the number of frames used from video footage to generate more detailed descriptions, or shorten scenario lengths to include more frames; and third, train a customized LLM, either by developing separate models to process signal data and generate descriptions or by creating a multi-modal model capable of handling both inputs."}]}