{"title": "ScarNet: A Novel Foundation Model for Automated Myocardial Scar Quantification from LGE in Cardiac MRI", "authors": ["Neda Tavakoli", "Amir Ali Rahsepar", "Brandon C. Benefield", "Daming Shen", "Santiago L\u00f3pez-Tapia", "Florian Schiffers", "Jeffrey J. Goldberger", "Christine M. Albert", "Edwin Wu", "Aggelos K. Katsaggelos", "Daniel C. Lee", "Daniel Kim"], "abstract": "Background: Late Gadolinium Enhancement (LGE) imaging remains the gold standard for assessing myocardial fibrosis and scarring, with left ventricular (LV) LGE presence and extent serving as a predictor of major adverse cardiac events (MACE). Despite its clinical significance, LGE-based LV scar quantification is not used routinely due to the labor-intensive manual segmentation and substantial inter-observer variability.\nMethods: We introduce ScarNet that synergistically combines a transformer-based encoder in Medical Segment Anything Model (MedSAM) and a convolution-based decoder in U-Net with tailored attention blocks to automatically segment myocardial scar boundaries while maintaining anatomical context. This network was trained an existing database of 552 ischemic cardiomyopathy patients with expert segmentation of myocardial and scar boundaries in LGE images, and it was tested on a separate set of 184 patients.\nResults: In 184 testing (not seen during training) patients, ScarNet achieved accurate segmentation of the scar boundary (median DICE= 0.912 [IQR: 0.863-0.944]), significantly outperforming both MedSAM (median DICE= 0.046 [IQR: 0.043-0.047]) and nnU-Net (median DICE= 0.638 [IQR: 0.604-0.661]). The bias and the coefficient of variation (CoV) were considerably lower for ScarNet (difference = -0.63% [4.82% of mean]; CoV = 4.3%) than MedSAM (difference = -13.31% [198.18% of mean]; CoV = 130.3%) and nnU-Net (difference = -2.46% [20.31% of mean]; CoV = 20.3%). In the Monte Carlo simulation with noise perturbation, ScarNet (0.892 \u00b1 0.053; CoV = 5.9%) produced significantly higher scar DICE than MedSAM (0.048 \u00b1 0.112; CoV = 233.3%) and nnU-Net (0.615 \u00b1 0.537; CoV = 28.7%).\nConclusion: ScarNet outperformed MedSAM and nnU-Net for predicting myocardial and scar boundaries in LGE images of patients with ischemic cardiomyopathy. Using Monte Carlo simulation, the model demonstrated robust performance across varying image qualities and scar patterns.", "sections": [{"title": "INTRODUCTION", "content": "Cardiovascular disease remains a leading cause of mortality worldwide, accounting for approximately 32% of all global deaths [1]. Myocardial injury frequently results in myocardial scarring in both ischemic and non- ischemic etiologies [2]. Late Gadolinium Enhancement (LGE) cardiovascular magnetic Resonance (CMR) [3] is the gold standard non-invasive test for myocardial fibrosis and scarring, providing essential information for risk stratification, treatment planning, and prognosis evaluation. Among LGE metrics, left ventricular (LV) scar volume is especially significant, as it has proven to be a predictor of major adverse cardiac events (MACE) and arrhythmic complications [4].\nDespite its clinical importance, LGE-based LV scar quantification remains challenging to implement in practice due to several factors [5]. Traditional manual segmentation requires around 15-20 minutes per case and is highly user-dependent, with considerable inter-observer variability [6-8]. This variability stems from the complex and heterogeneous appearance of scar tissue in LGE images, which often present with diffuse boundaries, irregular shapes, and varying intensity patterns. Additionally, imaging artifacts such as motion blur, intensity inhomogeneity, ghosting artifacts arising from arrhythmia and noise further complicate accurate scar delineation [9], making reliable manual segmentation both labor-intensive and inconsistent.\nRecent advancements in deep learning (DL) have inspired efforts to automate LV scar segmentation in LGE [10]. Traditional convolutional neural networks (CNNs), particularly UNet-based architectures [11, 12], have shown promise in medical image segmentation tasks. However, these methods have encountered"}, {"title": null, "content": "limitations in cardiac scar segmentation. A 2021 meta-analysis of 35 Al-based left ventricular scar quantification studies demonstrated mean DICE scores of only 0.616 and 0.633 for supervised and unsupervised methods, respectively [13]. These limited performances are attributed to the inherent challenges of capturing long-range dependencies in cardiac anatomy, handling multi-scale variations in scar appearance [14], and accurately delineating poorly defined scar boundaries.\nThe recent development of transformer-based models [15], including the Segment Anything Model (SAM) [15, 16] and its medical variant, MedSAM [17], has introduced new possibilities for medical image segmentation. These models excel at capturing global context and handling diverse object appearances, presenting a unique opportunity to address the challenges of cardiac scar segmentation. However, their generalist nature and lack of domain-specific optimization limit their direct applicability to LGE analysis [18, 19].\nTo address these challenges, we propose ScarNet, a novel foundation model designed specifically for LV scar segmentation in LGE as illustrated in Fig. 1. ScarNet integrates the global context-awareness of MedSAM's transformer-based encoder with the precise localization capabilities of a UNet decoder through an adaptive fusion mechanism. This architecture is further strengthened by specialized attention mechanisms, including multi-scale feature extraction pathways enhanced by Squeeze-and-Excitation (SE) [20] modules and novel ScarAttention blocks that dynamically focus on subtle scar regions while preserving anatomical context.\nTo address the class imbalance inherent in cardiac scar segmentation, ScarNet incorporates a comprehensive multi-component loss function. This loss function combines DICE, Focal [21], and Cross- Entropy losses with class-specific optimization terms for scar and myocardium segmentation, supported by an adaptive weighting scheme to balance the contributions of each component throughout training.\nOur novel ScarNet architecture addresses key challenges in cardiac scar segmentation through several innovative design elements. The hybrid structure combines global context understanding with precise local feature detection, while specialized attention mechanisms enhance the capture of heterogenous scar patterns. Furthermore, an efficient inference pipeline supports seamless integration into clinical workflows, and the adaptive fusion design enables comprehensive multi-scale feature extraction. The main contributions of our work are as follows:\n1. We propose ScarNet, a unique hybrid architecture that fine-tunes and extends MedSAM's transformer- based encoder by combining it with a UNet decoder. This adaptive fusion mechanism leverages both the global context awareness of transformers and the precise localization capabilities of CNNs, enabling comprehensive feature extraction at multiple scales.\n2. ScarNet incorporates multi-scale feature extraction pathways enhanced by SE modules for refined feature representation. The model includes ScarAttention blocks that dynamically focus on subtle scar features while preserving anatomical context and applies an adaptive fusion strategy to optimally combine features from both MedSAM and UNet streams.\n3. We propose a multi-component loss function that includes a weighted combination of DICE, Focal, and Cross-Entropy losses. This loss function integrates class-specific optimization terms for scar and myocardium segmentation, as well as an adaptive weighting scheme to balance different loss components during training.\n4. We demonstrate ScarNet's superior performance, achieving minimal volume quantification error (-4.82%) compared to MedSAM and nnU-Net. The model shows robust generalization across varied scar patterns and imaging conditions, along with efficient inference times that support seamless integration into clinical workflows.\n5. This work establishes a novel framework that combines transformer-based and CNN architectures for medical image segmentation, with potential applications extending beyond cardiac imaging."}, {"title": null, "content": "The purpose of this study was to implement ScarNet and compare its performance against MedSAM and nnU-Net on an existing database of LGE images of ischemic cardiomyopathy patients with expert segmentation of myocardial and scar boundaries as reference."}, {"title": "METHODS", "content": ""}, {"title": "Patient Cohort", "content": "This is a retrospective study using de-identified LGE images derived from 736 patients (mean age = 57.82 \u00b1 18.58 years; 60% males, mean left ventricular ejection fraction = 40.3\u00b111.0%) with coronary artery disease and left ventricular dysfunction enrolled in the previous DETERMINE registry (ClinicalTrials.gov ID NCT00487279) and PRE-DETERMINE study (ClinicalTrials.gov ID NCT01114269) [22] (see Table 1 for participant demographics and characteristics). Additional clinical profiles were not available to the study team at the time of this study. LGE images were manually segmented using the full width at half maximum technique [23] in QMass software (version 7.6, Medis Medical Imaging Systems, Leiden), by two cardiology attendings with more than 10 years of clinical experience in CMR. For additional details of the dataset and data preparation, please see Appendix in the Supplementary Materials."}, {"title": "Model Architecture", "content": "We include a literature review of related works to highlight their relative strengths and weaknesses, ultimately justifying the need for our study. Please see Appendix in Supplementary Materials for this literature review.\nThe ScarNet architecture, shown in Fig. 1 and Fig. 2, integrates global and local features by combining the pretrained MedSAM with a UNet pathway via an adaptive fusion mechanism, effectively capturing both anatomical context and spatial details necessary for precise LV scar segmentation in LGE. The MedSAM pathway uses a Vision Transformer (ViT) backbone, pretrained on large-scale general and medical datasets, to leverage global contextual information and subtle structural relationships in cardiac imaging. The complete segmentation pipeline is detailed in Algorithm 1, which outlines the key steps of our approach from input processing to final mask generation. The feature fusion process between MedSAM and UNet pathways is elaborated in Algorithm 4. Additional algorithmic details can be found in the Appendix.\nBy dividing the input image into non-overlapping patches, each mapped into an embedding space, MedSAM processes these features through a multi-scale decoder that progressively reduces channel dimensions from 256 to 32, preserving essential spatial relationships. This pathway is further enhanced with Squeeze- and-Excitation (SE) blocks that recalibrate channels, highlighting scar-relevant regions while suppressing background features, thus maximizing the representational power of the pretrained MedSAM (Eq. 3\u20134). In parallel, the U-Net pathway captures localized spatial details through a symmetric encoder-decoder architecture, progressively reducing spatial resolution while increasing feature depth with each encoding block (Eq. 5). Skip connections preserve high-resolution features by linking corresponding encoder and decoder blocks, which enables accurate boundary delineation critical for scar segmentation (Eq. 6). The adaptive fusion mechanism combines outputs from both pathways, balancing global and local context by aligning feature maps and applying an adaptive weighting mechanism. This fusion consolidates ScarNet's learned representations into a single four-class segmentation map-distinguishing background, myocardium, blood pool, and scar regions-through a final 1\u00d71 convolutional layer."}, {"title": "A. Attention Mechanisms", "content": "ScarNet's attention mechanisms focus on scar-relevant features, enhancing the model's ability to generalize across diverse imaging conditions. SE modules, embedded within the MedSAM pathway, recalibrate channel-wise features by modeling interdependencies to boost sensitivity to scar tissue regions, while"}, {"title": null, "content": "suppressing irrelevant background areas. Complementing this, the ScarAttention blocks, detailed in Algorithm 2, dynamically emphasize scar-specific features by reweighting spatial attention. These blocks use positional relationships and intensity variations unique to scars, improving the model's accuracy in highlighting subtle scar regions while preserving surrounding anatomical structures."}, {"title": "B. Training Pipeline", "content": "ScarNet's training pipeline, described in Algorithm 1, is structured to ensure optimal segmentation capabilities. Initially, input LGE images are normalized and fed into both MedSAM and UNet pathways. The parallel processing of these pathways generates both global and local feature representations, which are then enhanced by SE and ScarAttention modules before being fused to yield a four-class segmentation map. Loss calculation follows, with Lscarnet computed based on the adaptive weighting of DICE, Focal, and Cross-Entropy losses, guiding the model to optimize for both global and local accuracy. Backpropagation is conducted using an Adam optimizer, with gradients calculated to minimize LscarNet (Algorithm 1, Steps 13- 15). Regularization techniques, such as dropout and data augmentation, are applied to improve generalization.\nDuring inference, ScarNet processes unseen LGE images and generates segmentation maps for background, myocardium, blood pool, and scar regions, with the inference algorithm detailed in Algorithm 4 dynamically adjusting feature weighting for efficient predictions.\nTo assess ScarNet's robustness and reliability, we conducted a Monte Carlo simulation using 200 iterations with a Gaussian noise level of 0.05. This noise level was chosen to simulate realistic image quality variations encountered in clinical settings while providing a test of model stability. Each iteration introduced random perturbations to the input images while maintaining consistent noise patterns through controlled random seed initialization, allowing for reproducible yet comprehensive stress testing of the model's performance."}, {"title": "C. Mathematical model", "content": "The ScarNet model, denoted by H, combines a fine-tuned and enhanced MedSAM and UNet architecture to achieve accurate segmentation of cardiac scar regions. The model's core function can be expressed as:\n\nH(x) = F(MedSAM(x),UNet(x))                                                                            (1),\n\nwhere x \u2208 RH\u00d7W\u00d71 represents a grayscale medical image, and F is a fusion function that combines the outputs of the MedSAM and UNet branches. The fusion function F takes two feature maps, f\u2081 and f2, produced by MedSAM and UNet, respectively, and combines them through concatenation and a 1x1 convolution to produce the final segmentation output:\n\nF(f1, f2) = Conv(Concat[f1, f2])                                                                              (2),\n\nwhere Conv is a 1x1 convolution that reduces the concatenated feature map to the desired number of classes, and Concat represents channel-wise concatenation.\n1) MedSAM Pathway\nThe MedSAM component of ScarNet leverages a ViT backbone to capture global dependencies within the image, which is crucial for identifying subtle structures like scar tissue. MedSAM consists of patch embedding and position encoding stages, where the input image x\u2208 RH\u00d7W\u00d71 is divided into p\u00d7p non- overlapping patches. Each patch is flattened and mapped to an embedding space, generating patch embeddings E:\n\nE = Wembed P(x)                                                                                                       (3),"}, {"title": null, "content": "where P(x) denotes the patches, and Wembed is a learnable embedding matrix. The transformer operates on these patch embeddings, applying self-attention to learn relationships across the entire spatial dimension of the image. For query Q, key K, and value V matrices derived from E, self-attention is computed as:\n\nAttention(Q, K,V) = softmax( QKT/\u221ad )V                                                                                  (4),\n\nwhere d is the embedding dimension, enabling global context learning. The encoded features are passed through multiple Attention Blocks and SE layers in a multi-scale decoder. This process enhances the features relevant to scar regions. The detailed processing steps of the MedSAM branch are presented in Algorithm 2, highlighting the transformer-based processing and attention mechanisms.\n2) U-Net Pathway\nThe U-Net pathway captures local spatial details via a hierarchical encoder-decoder structure. The encoder consists of convolutional layers followed by downsampling steps, progressively reducing spatial resolution and increasing feature channels:\n\nFenc = Downsample(fend)                                                                                           (5).\n\nEach level l extracts features at a smaller spatial scale, enabling the capture of localized features. The decoder restores spatial resolution by upsampling, integrating high-resolution features from the encoder via skip connections:\n\nFl dec = Upsample(fd+1 dec) + fenc                                                                                                 (6).\n\nThe final layer produces a feature map MuNetin RH\u00d7W\u00d7C, where C is the number of classes. Algorithm 3 provides a detailed breakdown of the U-Net branch operations, including the encoder-decoder pathway and skip connection enhancement steps.\n1) Loss function\nIn ScarNet, the overall loss function LscarNet is designed to address class imbalance and to focus on achieving high segmentation accuracy, especially for challenging regions like cardiac scars. The ScarNet model aims to minimize the overall loss function with respect to the model parameters \u03b8, ensuring accurate segmentation across classes, with a particular emphasis on the scar class. This can be formulated as:\n\n0* = arg min LscarNet (0)                                                                                                           (7),\n\u03b8\nwhere 0* represents the optimal parameters of the model, and Lscarnet is the combined loss function defined as:\n\nLscarNet = \u03bb1. FTL + \u03bb2. DL + \u03bb3. CE                                                                                      (8),\n\nwhere, FTL is the Focal Tversky Loss, which mitigates class imbalance by focusing more on the scar region DL is the DICE Loss, which measures spatial overlap between the predicted and ground truth masks, CE is the Cross-Entropy Loss, weighted by class-specific importance \u03bb1, \u03bb2, and \u03bb3 are coefficients that control the contribution of each term, tuned to prioritize scar segmentation. The optimization is subject to constraints that guide the model's focus:\n1. Class Imbalance Constraint: Emphasis is placed on the scar class by weighting the Focal Tversky Loss term FTL"}, {"title": null, "content": "2. more heavily, enabling the model to manage class imbalances effectively. This constraint prioritizes reducing false negatives and false positives in scar segmentation, thus emphasizing the critical region of interest.\n3. Spatial Accuracy Constraint: The DICE Loss DL component ensures that the segmentation output maintains spatial alignment with the ground truth by maximizing overlap. This constraint encourages accurate boundary delineation, especially for small or complex structures.\n4. The Cross-Entropy Loss CE, with class-specific weights, further reinforces focus on critical regions, allowing flexibility in segmentation where non-scar regions may have less impact on the overall segmentation goal.\nFocal Tversky Loss (FTL) is adapted from the Tversky Index, emphasizing false positives and false negatives to handle class imbalance effectively. FTL is defined as:\n\nFTL = (1 \u2212 \u03a3(p. g) + \u03f5/\u03a3(p\u00b7 g) + \u03b1\u03a3(p\u00b7 (1 \u2212 g)) + \u03b2\u2211((1 \u2212 p). g) + \u03f5                                                        (9).\n\nDICE Loss helps maximize the overlap between predicted p and ground truth g, masks, improving spatial accuracy. The DL is calculated as:\n\nDL = 1 \u2212 2.|png| + \u03f5/|p| + |g| + \u03f5                                                                                                        (10).\n\nCross-Entropy Loss is calculated for each pixel, weighted by class to improve representation of the scar class over background regions:\n\nCE = \u2212\u2211cwcgc log(pc)                                                                                                       (11),\n\nwhere wc is the weight for class c and pc and gc are the predicted and true probabilities for class c, respectively.\nThis loss formulation provides a balanced objective that aligns ScarNet to achieve high accuracy in cardiac scar segmentation by reducing errors across both global context (captured by MedSAM) and local detail (captured by U-Net). The training process aims to minimize Lscarnet by updating the model weights to reduce misclassifications, particularly for the scar class. This is achieved by Gradient Descent Optimization: Using optimizers like Adam, gradients of LscarNet with respect to model parameters are computed. These gradients indicate the direction and magnitude of updates needed to minimize the loss. Regularization methods like dropout (if applicable) and data augmentation help the model generalize, while tuning the weights \u03bb\u2081, \u03bb\u2082, and 13 in the combined loss function focuses the model on scar segmentation without overfitting.\nScarNet demonstrates superior performance in cardiac scar segmentation by effectively delineating scar boundaries, reducing false positives, and providing consistent segmentation quality across varying scar sizes, shapes, and image qualities. By combining MedSAM's ability to capture global context with U-Net's precision in local feature extraction, ScarNet creates a robust foundation for accurate cardiac scar segmentation. The use of attention mechanisms at multiple scales further enhances the model's ability to focus on relevant features, resulting in reliable and precise segmentation outcomes.\nAdditional implementation details of our network are available in Supplementary Materials."}, {"title": "Two Secondary Experiments to Demonstrate Robustness", "content": "First, to compare the performance of our model against other models as a function of training data size, we trained and tested the models with training-testing (maintaining a 0.25 ratio) size ranging from 15 to 552 patients. Second, to compare the performance of different models against noise perturbation, we conducted a Monte Carlo simulation using identical testing conditions (200 iterations, noise level 5%). This noise level was empirically determined to be an upper limit in clinical practice by a cardiothoracic Radiologist attending with 7 years of clinical experience in CMR."}, {"title": "Statistical Analyses", "content": "We tested for variable normality using the Kolmogorov-Smirnov, Anderson-Darling, and Shapiro-Wilk tests. A variable was deemed normality distributed if it passes all three tests. Bland-Altman analysis was performed on DICE scores to assess the level of agreement between measurements, and the coefficient of variation (CoV) was defined as the standard deviation of the difference divided by the mean. One-way analysis of variation (Kruskal-Wallis if not normally distributed) with Bonferroni correction was conducted to detect any significant differences among groups. A p-value < 0.05 was considered statistically significant for all tests performed."}, {"title": "Results", "content": "Figure 3 shows a comparison of segmentation performance between MedSAM and ScarNet and their corresponding feature logits and probability maps. This figure does not include nnU-Net, because it does not provide feature maps. As shown, ScarNet focused on the correct anatomic features better than MedSAM for predicting the region of interests.\nFigure 4 compares the performance of MedSAM, nnU-Net, and ScarNet as a function of training data size. For both myocardium and scar segmentations, ScarNet achieved higher accuracy throughout and hit the plateau faster. As shown in the violin plots combing all training data sizes in Fig. 4, ScarNet not only produced higher median scar DICE (0.912) but also tighter score distributions compared to MedSAM (0.128) and nnU-Net (0.375) with lower median DICE scores and considerably wider distributions. Likewise, the same trends were observed for the myocardial DICE scores.\nThe remaining results are from the training size of 552 patients and testing size of 184 patients. Figure 5 compares segmentation performance between MedSAM, nnU-Net, and ScarNet in four representative patients, where ScarNet consistently outperformed the other networks. The DICE scores and scar volumes in 184 testing patients were not normally distributed (p < 0.05), necessitating non-parametric analyses. Because ScarNet produced the highest DICE scores, we compared differences with ScarNet as the reference. Figure 6 shows statistical results: for myocardial segmentation, compared against ScarNet (median DICE=0.961 [IQR: 0.920-0.999]), nnU-Net (median DICE= 0.878 [IQR: 0.838-0.915]) was significantly (p<0.001) different, whereas MedSAM (median DICE = 0.242 [IQR: 0.116-0.342]) was also significantly (p<0.001) different. For the scar segmentation, compared with ScarNet (median DICE= 0.912 [IQR: 0.863-0.944]), MedSAM (median DICE= 0.046 [IQR: 0.043-0.047]) and nnU-Net (median DICE= 0.638 [IQR: 0.604-0.661]) were significantly (p<0.001) different. For scar volume quantification, compared against manual as the reference (median DICE= 0.114 [IQR: 0.094-0.180]), MedSAM (median DICE= 0.000 [IQR: 0.000-0.001]) was significantly (p<0.001) different, whereas ScarNet (median DICE = 0.109 [IQR: 0.087- 0.173]) and nnU-Net (median DICE= 0.095 [IQR: 0.068-0.150]) were not significantly (p>0.192 and p>0.0009, respectively) different. According to the Bland-Altman analysis with manual scar volume as the"}, {"title": null, "content": "reference (Figure 7), the bias and the CoV were considerably lower for ScarNet (difference = -0.63% [4.82% of mean]; CoV = 4.3%) than MedSAM (difference = -13.31% [198.8% of mean]; CoV = 130.3%) and nnU- Net (difference = -2.46% [20.31% of mean]; CoV = 20.31%). Correlation analysis of scar volume measurements further supported these findings, with ScarNet showing near-perfect correlation with manual measurements (R\u00b2 = 1.0, y = 0.96x - 0.11), while nnU-Net demonstrated good correlation (R\u00b2 = 0.94, y = 0.8x + 0.21), and MedSAM showed poor correlation (R\u00b2 = 0.65, y = 0x + 0.01).\nFigure 8 shows representative examples with noise added in the Monte Carlo simulation. The mean of average myocardial DICE for ScarNet (0.912 \u00b1 0.063) was significantly (p<0.001) higher than MedSAM (0.185 \u00b1 0.142) and nnU-Net (0.823 \u00b1 0.072), and the mean of average scar DICE for ScarNet (0.892 \u00b1 0.053) was significantly (p<0.001) higher than MedSAM (0.048 \u00b1 0.112) and nnU-Net (0.615 \u00b1 0.537). Likewise, the mean CoV of myocardial DICE for ScarNet (6.9%) was significantly (p<0.001) higher than MedSAM (76.8%) and nnU-Net (8.7%), and the mean CoV of scar DICE for ScarNet (5.9%) was significantly (p<0.001) higher than MedSAM (233.3%) and nnU-Net (28.7%)."}, {"title": "DISCUSSION", "content": "In this study, we developed ScarNet, a novel deep learning model that combines a complete MedSAM pathway with a parallel UNet pathway through an adaptive fusion mechanism for automated myocardial scar segmentation in LGE images. After fine-tuning the model using a dataset of 552 patients and evaluating on 184 test patients, ScarNet achieved superior segmentation accuracy for both myocardium (median DICE=0.961) and scar tissue (median DICE=0.912), significantly outperforming both MedSAM and nnU- Net. Using Monte Carlo simulations with 5% Gaussian noise perturbation, the model demonstrated exceptional robustness with minimal bias (-0.63%) and coefficient of variation (4.3%) in scar volume quantification, while maintaining consistently high DICE scores for both myocardium (0.912 \u00b1 0.063; CoV = 6.9%) and scar tissue (0.892 \u00b1 0.053; CoV = 5.9%).\nOur approach differs from previous deep learning methods by addressing several key limitations in automated scar segmentation. Traditional CNN-based approaches [10-12], while effective for general medical segmentation, struggle with the inherent challenges of LGE imaging, achieving limited DICE scores of only 0.616 and 0.633 for supervised and unsupervised methods, respectively [13]. Earlier efforts by Zabihollahy et al. [24] and Bai et al. [25] demonstrated the potential of CNNs for scar segmentation, but faced challenges in capturing complex scar patterns. A significant advancement came from Fahmy et al., who introduced a deep learning-based image fusion approach that improved scar quantification accuracy by combining multiple image features [12]. While their method showed promise in handling varying contrast patterns, it still faced challenges with complex scar morphologies and required careful parameter tuning. Subsequent developments by Xiong et al. [26] with dual fully convolutional networks and Zhuang et al. [27] with multi-scale patch-based approaches improved performance but still struggled with consistent accuracy."}, {"title": null, "content": "Recent work on unsupervised domain adaptation [28] demonstrated promising results in handling multi- center variability by adapting the network to different scanner characteristics without requiring additional annotations, though the method still showed limitations in capturing fine scar details. Similarly, comparative studies of dark- and bright-blood LGE imaging techniques [29] showed improved visualization of subendocardial scars across different myocardial pathologies, particularly in cases where traditional bright- blood LGE faced challenges in distinguishing scar tissue from adjacent blood pool. While these approaches advanced our understanding of scar imaging and quantification, they still relied heavily on manual intervention or faced challenges with consistent automated analysis."}, {"title": null, "content": "Recent transformer-based models like SAM [15, 16] and MedSAM [17], despite leveraging pretraining on a vast dataset of over 50,000 medical images and SAM's foundation of 1 billion masked image segments lack the domain-specific optimization necessary for accurate scar delineation [18, 19]. ScarNet's hybrid architecture overcomes these limitations by combining the global context awareness of transformers with the precise localization capabilities of CNNs, enhanced by specialized attention mechanisms that specifically target scar features while preserving anatomical context.\nSeveral findings from our study have important implications for clinical practice. First, ScarNet's ability to achieve near-manual-level accuracy (R\u00b2 = 1.0, y = 0.96x - 0.11) in scar volume quantification suggests its potential for reliable automated analysis in clinical workflows. The incorporation of transformer architecture and specialized attention mechanisms proved particularly effective in identifying irregular scar patterns and heterogeneous enhancement, challenges that have traditionally limited automated approaches. The model's robust performance across varying training data sizes indicates its effectiveness even with limited datasets, a crucial advantage for clinical implementation. Furthermore, the stability demonstrated in noise perturbation experiments (5% Gaussian noise level, empirically determined as an upper limit in clinical practice) resulted in consistently high performance (scar DICE = 0.892 \u00b1 0.053, CoV = 5.9%), suggesting reliable performance across different imaging conditions and scanner types. This robustness to noise perturbation, combined with the model's attention-driven ability to identify complex scar patterns, addresses major challenges in clinical deployment of Al tools.\nOur study has several limitations that should be acknowledged. First, while our dataset included 736 patients, it was derived from specific clinical trials (DETERMINE and PRE-DETERMINE), potentially limiting generalizability to broader patient populations. Second, our ground truth annotations were created using the full width at half maximum technique, which, although widely accepted, may not capture all patterns of enhancement. Third, as a foundation model, ScarNet requires a GPU for optimal performance, which may impact deployment in some clinical settings. While recent advances in GPU computing [30-33] and memory management, such as improved collective MPI libraries for Intel GPUs [34], offer promising solutions for optimizing foundation model deployment, GPU requirements remain a consideration for clinical implementation. Fourth, while the model shows robust performance on varying scar patterns, its effectiveness on rare or atypical presentations requires further validation. Fifth, although we demonstrated stability under simulated noise (5% Gaussian), real-world testing on low-quality or artifact-laden images remains necessary. Sixth, the current validation focused on 2D slice-based analysis; extension to true 3D volume segmentation could potentially improve spatial consistency. Future studies should validate ScarNet's performance against multiple expert annotations using various quantification techniques and explore its performance in non-ischemic cardiomyopathies, where scar patterns can be more diffuse and challenging to quantify.\nIn conclusion, we present ScarNet, a novel foundation model specifically designed for automated left ventricular scar quantification in LGE, distinguishing itself from existing cardiac foundation models that focus on general cardiac structure analysis rather than specific scar assessment. While other foundation models have advanced cardiac imaging analysis broadly, ScarNet's focused approach to scar quantification fills a crucial gap in automated cardiac tissue analysis. Its successful implementation could facilitate routine quantitative assessment of myocardial scar, potentially improving risk stratification and treatment planning in patients with ischemic cardiomyopathy. Future work should focus on validation across different cardiac pathologies, integration with clinical decision support systems, and exploration of applications beyond cardiac imaging."}, {"title": "Data Availability Statement", "content": "Due to data-sharing agreement policies, the dataset used in this study cannot be made publicly available. For further inquiries regarding data usage, please contact the PRE-DETERMINE and DETERMINE study steering committee.\nSoftware code: The ScarNet implementation and associated code are available at:\nhttps://github.com/NedaTavakoli/ScarNet."}, {"title": "II. APPENDIX", "content": "This section summarizes a literature review of related works using deep learning to automate LV scar quantification."}, {"title": "A. Deep Learning Approaches for LV Scar Segmentation", "content": "The emergence of deep learning marked a significant shift in LV scar quantification approaches. Zabihollahy et al. [24] developed one of the first CNN-based methods for LV scar segmentation, achieving enhanced automation but with limited robustness. Fahmy et al. advanced scar quantification by combining deep learning with image fusion techniques for automated LGE analysis [12], though challenges persisted in handling complex scar patterns. Bai et al. [25] demonstrated improved results using fully convolutional networks for automated cardiac MRI analysis. Xiong et al. [35] demonstrated the effectiveness of a dual fully convolutional network for cardiac chamber segmentation in LGE-MRI. Building upon these advances, Karim et al. [36] conducted a comprehensive evaluation of various segmentation algorithms for scar tissue quantification, establishing benchmarks for the field.\nThe complexity of scar tissue appearance led to more sophisticated approaches. Xiong et al. [26] developed a global benchmark of segmentation algorithms specifically for LGE-MRI, while Zhuang et al. [27] introduced multi-scale patch-based approaches for cardiac image analysis. These works underscored the advancements made in DL-based scar quantification while also highlighting ongoing challenges, such as handling class imbalance and capturing nuanced scar characteristics."}, {"title": "B. Foundation Models and Transformer-based Approaches", "content": "The rise of foundation models, particularly in image analysis, has introduced new directions for cardiac imaging tasks. The Segment Anything Model (SAM) [37", "38": "adapted SAM's broad capabilities to medical imaging through domain-specific training, achieving remarkable generalizability across medical segmentation tasks. Recent advances in cardiac imaging have seen the emergence of specialized foundation models, such as the Vision-Language Foundation Model for echocardiogram interpretation [39", "40": ".", "Segment Anything": "odels (Med-SAM1, Med-SAM2) for left atrial segmentation in 3D LGE MRI [18"}]}