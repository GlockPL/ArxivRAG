{"title": "CONTEXT MATTERS: LEVERAGING CONTEXTUAL FEATURES FOR TIME SERIES FORECASTING", "authors": ["Sameep Chattopadhyay", "Pulkit Paliwal", "Sai Shankar Narasimhan", "Shubhankar Agarwal", "Sandeep Chinchali"], "abstract": "Time series forecasts are often influenced by exogenous contextual features in addition to their corresponding history. For example, in financial settings, it is hard to accurately predict a stock price without considering public sentiments and policy decisions in the form of news articles, tweets, etc. Though this is common knowledge, the current state-of-the-art (SOTA) forecasting models fail to incorporate such contextual information, owing to its heterogeneity and multimodal nature. To address this, we introduce ContextFormer, a novel plug-and-play method to surgically integrate multimodal contextual information into existing pre-trained forecasting models. ContextFormer effectively distills forecast-specific information from rich multimodal contexts, including categorical, continuous, time-varying, and even textual information, to significantly enhance the performance of existing base forecasters. ContextFormer outperforms SOTA forecasting models by up to 30% on a range of real-world datasets spanning energy, traffic, environmental, and financial domains.", "sections": [{"title": "INTRODUCTION", "content": "Numerous state-of-the-art (SOTA) solutions to time series forecasting (Lin et al., 2021) have predominantly depended only on the time series history. However, in many real-world forecasting applications, such as predicting stock prices, air quality, or household energy consumption, future values are frequently influenced by external contextual factors like geographical and economic indicators. Industrial solutions for forecasting, such as predicting the demand for online food delivery (Chad Akkoyun, 2022), have shown the potential to improve forecasting accuracy by incorporating macroeconomic factors like tax refunds. However, the current SOTA forecasting models (Liu et al., 2024; Nie et al., 2023) still are unable to handle these contextual factors and solely rely on the historical time series to predict the future. We attribute this to the inherent diversity and multimodality of these contextual factors. For example, consider the task of predicting the price of a stock. The contextual factors can vary from categorical indicators like stock category (e.g., energy, technology, or healthcare), continuous and time-varying indicators like market cap and interest rates, or even textual information in the form of news articles. We refer to this multimodal contextual information as metadata and use both these terms interchangeably. Incorporating metadata into forecasting models is hard for the following reasons:\n1. Lack of multimodal metadata encoders. We note that the time series domain lacks the availability of foundation models trained on multimodal datasets to extract aligned representations (e.g., CLIP Radford et al. (2021)). These are key to mapping the time series history and multimodal metadata into the same representation space from which the forecast can be decoded.\n2. Non-uniformity in metadata across datasets. The metadata associated with stock price prediction, such as new articles, tweets and opinions, interest rates, etc., are completely different from the metadata associated with weather prediction, such as rainfall levels, pollution levels, wind direction, and speed, etc. This prevents us from pooling datasets together to train a context-aware foundation model for forecasting.\n3. Diversity of metadata within datasets. For a given dataset, the metadata could be categorical (e.g., national holidays), continuous (e.g., interest rates), or even time-varying (e.g., oil prices). Current approaches often end up modeling such diverse metadata through simple linear regressors (Das et al., 2024), which may be insufficient to capture the complex correlations.\nConsequently, for these exact reasons, we note that the recent wave of foundation models for forecasting relies only on history. To this end, we propose a plug-and-play approach to build context-aware forecasting models on top of context-agnostic SOTA forecasting models. Our approach includes novel architectural additions to handle categorical, continuous, time-varying, and even textual metadata. Additionally, we introduce novel training modifications to ensure that the context-aware forecast is at least as good as the context-agnostic forecast with respect to the traditional forecasting metrics. Our architectural and training modifications are inspired by the-oretical insights on improving any regression model with new, correlated features. Our primary contributions are as follows:\n1. We propose ContextFormer, a novel framework for incorporating diverse multimodal metadata into any context-agnostic forecasting model. ContextFormer surgically inserts aligned representation of metadata using cross-attention blocks (Vaswani et al., 2017) into the existing forecasting model architectures.\n2. We introduce a plug-and-play fine-tuning approach to effectively incorporate metadata and ensure that the resulting forecasts are at least as good as that of the context-agnostic base model.\n3. We show definitive improvements in the forecasting performance of state-of-the-art context-agnostic forecasting models, such as PatchTST (Nie et al., 2023) and iTransformer (Liu et al., 2024) across a wide range of real-world forecasting tasks spanning retail, finance, energy, and environmental domains."}, {"title": "RELATED WORKS", "content": "Classical methods. These methods predict future values for time series using statistical techniques. Established approaches include ARMA (AutoRegressive Moving Average), which captures temporal dependencies through autoregression and moving averages, and exponential smoothing methods like Holt-Winters and STL (Seasonal-trend decomposition using LOESS), which account for trends and seasonality. The Box-Jenkins methodology is also used for building models to handle non-stationary data (Shumway & Stoffer, 2017; Hyndman & Athanasopoulos, 2018). These techniques have long been the foundation of time series forecasting, providing reliable ways to analyze past trends and make accurate predictions. Prophet (Taylor & Letham, 2017), developed by Facebook, builds upon the traditional techniques by incorporating additional features such as holiday effects and non-linear trends, thus providing greater flexibility and accuracy. Prophet is capable of managing complex seasonal patterns and irregularities and is known for its robustness in handling missing data and outliers. Classical time series forecasters struggle against deep learning models because they lack the adaptability to fully utilize large datasets and learn intricate temporal dependencies.\nDeep-learning methods. These methods have been the go-to method to learn the time series features and perform forecasting, utilizing the recent advancements in neural network architectures, such as RNNs (Sherstinsky, 2020) and transformers (Vaswani et al., 2017). Notable RNN-based approaches include DeepAR (Salinas et al., 2019) and LSTNet (Lai et al., 2018). SOTA transformer-based approaches include iTransformer (Liu et al., 2024), which applies the attention and feed-forward network on the inverted dimensions and PatchTST (Nie et al., 2023), a channel-independent transformer which takes time series segmented into subseries-level patches as input tokens. Other prominent approaches include Autoformer (Wu et al., 2021), Informer (Zhou et al., 2021b), FEDformer (Zhou et al., 2022) and TimesNet (Wu et al., 2022a). Recent works have focused on foundation models, which are deep models pre-trained on large amounts of data, enabling them to learn extensive information and a variety of patterns. They can be fine-tuned or adapted to specific tasks with relatively small amounts of task-specific data, showcasing remarkable flexibility and efficiency. Popular foundation models include Time-LLM (Jin et al., 2024), CHRONOS (Ansari et al., 2024), Lag-Llama (Rasul et al., 2024), and TimesFM (Das et al., 2024). Existing deep learning forecasters fail against context-aware models because they lack the ability to incorporate external factors and dynamic contextual information into predictions.\nForecasting with covariates. The current literature in this field predominantly focuses on domain-specific forecasting (Zhou et al., 2021a; Wu et al., 2022b;a) to enhance downstream task performance (Li et al., 2023), or on the development of foundation models for universal forecasting (Das et al., 2024; Ansari et al., 2024), often overlooking correlated time-varying conditions that may be available as categorical or continuous metadata. One of the earliest works in conditional forecasting, a CNN-based approach by Borovykh et al. (2018), utilizes stacks of dilated convolutions to capture a broad range of historical data for forecasting. More recent models, such as TiDE (Das et al., 2023) and TimesFM (Das et al., 2024), incorporate covariates into their forecasting processes. TiDE encodes the past of a time series along with covariates using dense MLPs and then decodes the series with future covariates, again using dense MLPs. TimesFM treats covariates as exogenous regressors and fits linear models outside its decoder at the time of inference (check Appendix B for further details). Another recent development is TimeWeaver (Narasimhan et al., 2024), a diffusion-based architecture designed for conditional synthesis, which integrates contextual metadata. Our approach builds on these advancements by integrating valuable contextual information from commonly used datasets into the training process of existing SOTA forecasters. We aim to improve forecasting accuracy by utilizing the rich contextual metadata, which is typically neglected in existing methods."}, {"title": "PROBLEM FORMULATION", "content": "In this section, we formally define the context-aware time series forecasting problem. We are given a multivariate time series $X_{hist} = (x_1, x_2,...,x_L)$, with $x_t \\in \\mathbb{R}^F$. Here, $L$ denotes the history time series length, while $F$ represents the number of channels. Each sample $x_t$ is associated with contextual metadata $c_t$, comprising both categorical features $c^{cat}_t \\in \\mathbb{N}^{K_{cat}}$ and continuous features $c^{cont}_t \\in \\mathbb{R}^{K_{cont}}$. The symbols $K_{cat}$ and $K_{cont}$ represent the number of categorical and continuous metadata features, respectively. Together, these features form a vector $c_t = c^{cat}_t \\oplus c^{cont}_t$, where $\\oplus$ denotes vector concatenation. Note that $c_t$ can include both time-varying and time-invariant metadata features. Now, we can define a metadata sequence as $C_{hist} = (c_1, c_2, ..., c_L)$, having the same number of timesteps as $X_{hist}$.\nTo understand this better, let us take the example of the Beijing AQ dataset, which contains the time series data of six air pollutant concentrations: $CO, NO_2, SO_2, O_3, PM2.5,$ and $PM10$ concentration ($F = 6$), sampled on an hourly basis for four days ($L = 96$). The metadata here includes information on the location, air pressure, amount of rainfall, temperature, dew point, wind speed, and wind direction for each time series sample. In this dataset, location (12 unique labels) and wind direction (17 unique labels) are categorical values ($K_{cat} = 2$), while the other features"}, {"title": "THEORETICAL MOTIVATION", "content": "This section provides theoretical justifications for enhancing forecasting accuracy by incorporating context. From an information-theoretic perspective, we show that including context reduces forecasting uncertainty, thereby improving model accuracy. We then examine the integration of contextual information into a simple linear regression model, illustrating how this approach improves the performance of a simple autoregressive forecaster."}, {"title": "A PERSPECTIVE FROM INFORMATION THEORY", "content": "Taking inspiration from a recent work on retrieval-based forecasting (Jing et al. (2022)), we illustrate the relationship between the variables $X_{hist}, C_{hist}, X_{future}$, and $\\hat{X}_{future}$ for context-aware and context-agnostic forecasters in the form of the graphical models given in Fig. 3.\nOn analyzing these models from an information theoretic per-spective, for the context-aware forecaster (Fig. 3a), we can show the following.\n$I (X_{future}; \\hat{X}_{future}) \\geq I (X_{future}; X_{hist}, C_{hist}) > I (X_{future}; X_{hist}),$ (4)\nwhere the quantity $I (A; B)$ represents the mutual information between the variables A and B. The first inequality in Eq. 4 is a direct result of the very well-known data processing inequality, while the second inequality comes from\n$I (X_{future}; X_{hist}, C_{hist}) = I (X_{future}; X_{hist}) + I (X_{future}; C_{hist} | X_{hist}),$ (5)\n$I (X_{future}; C_{hist} | X_{hist}) \\geq 0 \\forall X_{future}, X_{hist}, C_{hist}.$ (6)\nThe increase in the mutual information between the input variables and the forecast on the inclusion of context to the input data would imply a lower uncertainty for the context-aware forecast over"}, {"title": "ADDING CONTEXT TO AN AUTOREGRESSIVE FORECASTER", "content": "Let $y_t$ be a time-varying quantity influenced by past values and additional contextual information. Here, the underlying assumption is that the true forecast $y_t$ is a linear combination of $p$ lag terms and $q$ context terms. First, we model $y_t$ only as a $p$-order autoregressive (AR) process $y_t = x_t^T\\beta + \\epsilon$. Here, $x_t = [y_{t-1} \\space y_{t-2} \\space ... \\space y_{t-p}]$ is a vector of the previous $p$ lagged values, $\\beta$ is a vector of AR coefficients with $\\beta \\in \\mathbb{R}^p$, and $\\epsilon$ is the error term. Note that even though $y_t$ depends on additional contextual information, the assumed linear model only depends on the lag parameters, reflecting the context-agnostic case.\nGiven $n + p$ observations, we construct an $n \\times p$ matrix $X$ of lagged values, allowing the model to be written as $Y = X\\beta + \\epsilon$, where $Y$ is the vector of observed values and $Y \\in \\mathbb{R}^n$. This formulation enables the least squares estimation of $\\beta$ and the corresponding error in an autoregressive framework. The least squares estimate of $\\beta$ and the associated error are defined as\n$\\hat{\\beta}_{opt} = (X^TX)^{-1} X^TY, \\space \\space E_{orig} = ||Y - X\\hat{\\beta}_{opt} ||^2.$ (8)\nNow, we shift to the context-aware case. Let $C$ be an $n \\times q$ matrix representing the contextual metadata corresponding to $X$. Our key intuition is that a straightforward way to integrate this contextual information into an AR model without altering its structure is by performing an exogenous regression on the residuals. This approach preserves the original au-toregressive framework, allowing the contextual information to account for the variance unexplained by the AR model. In this method, the AR model first captures the time dependencies, and the metadata refines the forecast by re-ducing the residual error, leading to improved accuracy. The new context-aware autoregressive model can be expressed as $Y' = C\\gamma + \\epsilon'$, where $Y' = Y - X\\hat{\\beta}_{opt}$ represents the residuals from the AR model, $\\gamma \\in \\mathbb{R}^q$ is the vector of co-efficients for the contextual metadata, and $\\epsilon'$ is the new error term. For this model, the least squares estimate of $\\gamma$ and the regression error is represented by\n$\\hat{\\gamma}_{opt} = (C^TC)^{-1} C^TY', \\space \\space E_{new} = ||Y' - C\\hat{\\gamma}_{opt} ||^2.$ (9)\nWe can demonstrate that the error $E_{new}$ for the context-aware model is less than or equal to the error $E_{orig}$ of the original model. The error for this model can be expressed as\n$E_{new} = min_{\\gamma} ||Y - X\\hat{\\beta}_{opt} - C\\gamma||^2.$ (10)\nSince $0_q$, the zero vector in $\\mathbb{R}^q$, is a feasible solution for $\\gamma$, we have\n$E_{new} = ||Y - X\\hat{\\beta}_{opt} - C\\hat{\\gamma}_{opt} ||^2 \\leq ||Y - X\\hat{\\beta}_{opt} ||^2 = E_{orig}.$ (11)"}, {"title": "METHODOLOGY", "content": "In this section, we propose our method, ContextFormer, to incorporate multimodal information into deep-learning forecasting models to enhance forecasting accuracy. Additionally, we propose a plug-and-play fine-tuning approach for this architecture to optimize its performance further."}, {"title": "MODEL STRUCTURE", "content": "The ContextFormer architecture, illustrated in Fig. 5, consists of a metadata embedding module, a temporal embedding module, and multiple cross-attention blocks. These components complement the base model architecture. The working of these components and the required pre-processing steps for time series forecasting are described herein:\nBase Model. The base model for the ContextFormer can be any forecasting model capable of processing input time series and generating a hidden state representation. Neural network-based forecasters usually have an input layer, some hidden layers, and a final projection layer. The input layer processes the time series to generate embeddings, which are passed through the hidden layers. The projection layer maps the final hidden state to the dimensionality of the output $X_{future}$.\nMetadata Embedding. The metadata embedding block embeds the paired metadata for a given time series sample $x_t \\in \\mathbb{R}^F$. As outlined in Sec. 3, the paired metadata $c_t$ includes both cate-gorical $c_t^{cat}$ and continuous $c_t^{cont}$ features, which are combined as $c_t = c_t^{cat} \\oplus c_t^{cont}$. Categorical features are represented as one-hot encodings and concatenated with the continuous features. This combined input is passed through a transformer network (Vaswani et al., 2017), allowing the model to simultaneously learn and capture correlations within the categorical and continuous domains.\nTemporal Embedding. Similar to the metadata embedding block, this module generates temporal embeddings from the timestamps of a given sample. Timestamps are first decomposed into com-ponents such as year, month, day, hour, and minute, depending on the dataset's granularity. These"}, {"title": "TRAINING", "content": "The ContextFormer architecture can either be fully trained from scratch along with the base model using paired contextual metadata or can be used to fine-tune a pre-trained base model. One potential fine-tuning strategy involves a plug-and-play approach, where the context-aware model builds on a pre-trained forecaster that has already been trained on historical time-series data. In this approach, the pre-trained base model (except for the final layer) is frozen, and the ContextFormer components are added with their weights initialized to zero. The zero-initialization approach is motivated by the AR example in Sec. 4.2, where the model with zero-initialized parameters performs identically to the context-agnostic model. Subsequently, the temporal and metadata embedding modules, along with the cross-attention blocks and the final layer, are trained for a specified number of epochs.\nWhy do we opt for fine-tuning rather than training the context-aware model from scratch?\nThe advantages of using the plug-and-play fine-tuning setup with a pre-trained forecaster, compared to training a context-aware model from scratch, are as follows:\n1. The fine-tuned model is guaranteed to perform at least as well as the context-agnostic base model, provided the test distribution matches the training distribution. However, this guarantee does not apply to a context-aware model trained from scratch.\n2. For datasets with irrelevant metadata, training a context-aware model from scratch can be unstable, potentially hindering the model from effectively learning the time series' trend and seasonality. In contrast, fine-tuning can utilize a pre-trained model, which has already captured time series dependencies, allowing it to focus solely on learning the contextual information.\n3. If a time series dataset includes metadata for only some data points, training a context-aware model from scratch would either require ignoring data points without metadata or augmenting new metadata, both of which are undesirable. With our fine-tuning approach, the model can be pre-trained on the entire dataset and then fine-tuned using only the data points that have metadata.\n4. The plug-and-play design of our model allows the creation of multiple context-aware models from a single forecaster. This flexibility motivates the development of dataset-agnostic universal forecasting models, which can be fine-tuned to generate dataset-specific models."}, {"title": "EXPERIMENTS", "content": "We have extensively evaluated the Con-textFormer framework using two state-of-the-art transformer-based forecasters, PatchTST (Nie et al., 2023) and iTrans-former (Liu et al., 2024), across vari-ous forecasting applications and time hori-zons. These evaluations showcase the impact of incorporating contextual meta-data to enhance forecast accuracy. Al-though transformer-based forecasters were utilized as the base models in our study, the ContextFormer method is highly ver-satile and not limited to any particular model architecture. Its flexible design allows it to be inte-grated with any pre-existing forecasting model, irrespective of its internal implementation.\nPreliminary Experiment. Before experimenting with real-world data, we validated our architec-tural implementation using a synthetic dataset. In the first experiment with the ContextFormer ar-chitecture, we generated a dataset from samples of ARMA(2,2) processes with randomly chosen coefficients and added Gaussian noise to perturb the sequences (see Appendix C.1 for more details)."}, {"title": "CONCLUSION", "content": "In this paper, we introduced ContextFormer, a novel technique for integrating contextual informa-tion into existing forecasting models. Through comprehensive evaluations on diverse real-world datasets, we demonstrated ContextFormer's ability to effectively handle complex, multimodal metadata while consistently outperforming baseline models and even forecasting foundation models. In addition, we proposed a resource-efficient plug-and-play fine-tuning framework that offers signifi-cant improvements to forecasting accuracy over training context-aware models from scratch.\nIn future work, we aim to test our approach on other contextual modalities such as images, videos, etc. An interesting future work is to analyze the effects of forecasting metadata first; thereby, we propose a two-step forecasting pipeline where we first forecast metadata. We hypothesize that the forecasted metadata can be used to obtain better forecasts. Our key intuition is that metadata is more human-interpretable, and therefore forecasting metadata could be an easier task to solve."}, {"title": "MSE LOSS AND MUTUAL INFORMATION", "content": "Following the approach in Jing et al. (2022), we can demonstrate that minimizing the MSE loss between $X_{future}$ and $\\hat{X}_{future}$ is equivalent to maximizing the mutual information $I (X_{future}; \\hat{X}_{future})$, assuming a Gaussian noise model between the two variables. Specifically, minimizing the MSE is shown to be equivalent to maximizing the log-likelihood, which, in turn, maximizes the mutual information.\nSuppose that the relationship between $X_{future}$ and $\\hat{X}_{future}$ is modeled as\n$\\hat{X}_{future} = X_{future} + Z,$\nwhere $Z$ is Gaussian noise with zero mean and variance $\\sigma^2$, i.e., $Z \\sim N(0, \\sigma^2)$. The log-likelihood of obtaining $X_{future}$ given $\\hat{X}_{future}$ can be derived from the probability density function of the Gaus-sian distribution. The log-likelihood function is given by\n$logp(X_{future} | \\hat{X}_{future}) = -\\frac{1}{2} log(2\\pi\\sigma^2) - \\frac{||X_{future} - \\hat{X}_{future}||^2}{2\\sigma^2}.$\nSince $-\\frac{1}{2} log(2\\pi\\sigma^2)$ is a constant with respect to $\\hat{X}_{future}$, the log-likelihood is maximized when the term $|| X_{future} - \\hat{X}_{future}||^2$ is minimized, which is the same as minimizing the MSE. Therefore, minimizing the MSE is equivalent to maximizing the log-likelihood.\nHaving shown the equivalence of MSE and the log-likelihood, now the mutual information $I(X_{future}; \\hat{X}_{future})$ between $X_{future}$ and $\\hat{X}_{future}$ can be expressed as\n$I(X_{future}; \\hat{X}_{future}) = H(X_{future}) - H(X_{future} | \\hat{X}_{future})$\nwhere $H(X_{future})$ is the entropy of $X_{future}$ and $H(X_{future} | \\hat{X}_{future})$ is the conditional entropy of $X_{future}$ given $\\hat{X}_{future}$. For Gaussian noise, $H(X_{future} | \\hat{X}_{future})$ is related to the conditional variance of $X_{future}$ given $\\hat{X}_{future}$,\n$H(X_{future} | \\hat{X}_{future}) = -E log [logp(X_{future} | \\hat{X}_{future})].$\nMaximizing the likelihood of the observed data $X_{future}$ given the model (in this case, $\\hat{X}_{future}$) reduces the uncertainty $H(X_{future} | \\hat{X}_{future})$, effectively increasing the mutual information. Now, we know that minimizing the MSE maximizes the log-likelihood. This corresponds to making the estimate $\\hat{X}_{future}$ as close as possible to the true value $X_{future}$, which reduces the variance of the noise $Z$ (or the uncertainty in $\\hat{X}_{future}$).\nSince mutual information $I(X_{future}; \\hat{X}_{future})$ is a measure of the reduction in uncertainty about $X_{future}$ given $\\hat{X}_{future}$, minimizing the conditional variance (or equivalently, maximizing the log-likelihood) increases $I(X_{future}; \\hat{X}_{future})$. Thus, minimizing MSE maximizes the log-likelihood, which in turn maximizes the mutual information $I(X_{future}; \\hat{X}_{future})$,\n$min E_{p_{data}} || X_{future} - \\hat{X}_{future}||^2 \\Leftrightarrow max I (X_{future}; \\hat{X}_{future}).$"}, {"title": "TIMESFM WITH COVARIATES", "content": "TimesFM (Das et al., 2024), developed by Google Research, is one of the latest foundational models for time-series forecasting. It boasts superior zero-shot performance compared to other foundational models across most of the commonly used benchmark datasets. A key feature of TimesFM is its ability to incorporate static and dynamic covariates during inference, making it a context-aware forecaster by our definition. As a multipurpose model, it is not trained on any dataset-specific co-variates but at the time of dataset-specific inference, it treats them as exogenous regression variables and fits linear models onto them."}, {"title": "SYNTHETIC DATA", "content": "We generated a dataset comprising 10000 time-series sequences, each containing 192 samples, based on ARMA(2,2) processes with randomly initialized coefficients, sampled from a uniform distribu-tion, for the first preliminary experiment. The stability of each ARMA process was ensured by verifying that all the roots of the corresponding characteristic equations were within the unit cir-cle. To cause some perturbation, Gaussian noise with 0.1 variance was added to all the sequences. The metadata for the dataset, consisting of the four ARMA coefficients, was continuous and time-invariant. These coefficients differed significantly from the ARMA coefficients obtained from the noisy data for most of the sequences. The dataset was split in a 7:1:2 ratio among the train, valida-tion, and test splits."}, {"title": "BEIJING AQ (AIR QUALITY)", "content": "The dataset obtained from Chen (2019) contains hourly air pollutant concentration data and cor-responding meteorological data from 12 locations in Beijing. The task is to forecast a 6-channel multivariate time series using historical data and weather forecast metadata. Missing values in the dataset are handled by imputing continuous metadata and time series values with their mean, while missing categorical metadata (such as wind direction) are assigned an \"unknown\" label. The data, spanning from 2013 to 2017, is split into training, validation, and test sets in a 7:1:2 ratio. For each set, we first apply a sliding window of length 144 with a stride of 24, resulting in 9828 training, 1332 validation, and 2796 test time series samples. Then, we use a sliding window of length 192 with a stride of 24, yielding 9012 training, 1188 validation, and 2556 test time series."}, {"title": "STORE SALES (RETAIL)", "content": "The dataset, sourced from a popular Kaggle competition (Kaggle, 2022), contains daily sales data from 2013 to 2017 for 34 product families sold across 55 Favorita stores in Ecuador. The dataset includes features such as store number, product family, promotional status, and sales figures. Ad-ditionally, supplementary information like store metadata and oil prices is provided, offering time-"}, {"title": "PEMS-SF (TRAFFIC)", "content": "The dataset, sourced from Wu et al. (2021), contains 15 months of daily data from the California Department of Transportation PEMS website. It captures the occupancy rate (ranging from 0 to 1) of various freeway car lanes in the San Francisco Bay area. The data spans from 2008 to 2009, with measurements sampled every 10 minutes. The forecasting task involves predicting the electricity demand pattern for a single sensor (selected from a total of 861 sensors), framed as a univariate time series forecasting problem. The data is split temporally into training, validation, and test sets in a 7:1:2 ratio. We then apply sliding windows of lengths 144 and 192, each with a stride of 24. The number of samples for all the sets is given in Table 7."}, {"title": "ECL (ELECTRICITY)", "content": "The dataset taken from Trindade (2015) consists of power consumption data for 370 users in Portugal over a period of 4 years from 2011 to 2015. It is a commonly used dataset for time series forecasting (Wu et al., 2021; Liu et al., 2024; Ansari et al., 2024). The forecasting task with respect to this dataset is to predict the electricity demand pattern for a single user, which is framed as a univariate time series forecasting problem. In the absence of any innate metadata features, we consider the 370 user IDs to be the only metadata. The data is sampled every 15 minutes, resulting in a time series with 96 daily timesteps. We process the data to remove days with significant 0 values. The data has been split, and sliding windows with stride 24 are used to get approximately a 7:1:2 ratio for the number of samples in the training, validation, and test sets, with the exact numbers given in table 7."}, {"title": "MONASH (BITCOIN)", "content": "The dataset, sourced from the Monash Time Series Forecasting Repository (Godahewa et al., 2021), contains daily Bitcoin closing prices from 2010 to 2021, along with 18 potential influencing factors. These include metrics like hash rate, block size, mining difficulty, and social indicators such as the number of tweets and Google searches related to the keyword \u201cBitcoin.\" During preprocessing, we excluded the number of tweets due to its limited availability, leaving us with 17-dimensional continuous time-varying metadata for the univariate forecasting task. The dataset is divided into training, validation, and test sets in a 7:1:2 ratio. We apply sliding windows of lengths 144 and 192 with a stride of 24 to the data."}, {"title": "ADDITIONAL EXPERIMENT (BITCOIN-NEWS)", "content": "In the absence of a dataset containing both Bitcoin prices and corresponding news articles, we cre-ated a new dataset with hourly Bitcoin prices and daily financial news articles scraped from online"}, {"title": "CONTEXT-AWARE AUTOREGRESSION", "content": "To provide theoretical justification for the ideas in Subsection 4.2, we generated a dataset consisting of 500-length sequences formed by linear combinations of an autoregressive process with five latent variables, followed by added perturbations. For this scenario, we choose the latent variables as the contextual information. The resulting dataset, denoted as D, is structured as $\\{(X^n,C^n, Y^n)\\}_{n=1}^N$, For our experiments, we set N = 1000.\nWe initially start by modeling these sequences through vanilla AR(10) models of the form Y = $X\\beta$ + $\\epsilon$, where X is a 490 \u00d7 10 matrix and Y is a 490-length vector. The corresponding paired metadata for such a sequence can be represented as C, which is a 490 \u00d7 5 matrix.\nNext, we incorporate contextual metadata into the existing AR models by fitting them as exogenous regressors for the residuals. To demonstrate the impact of increasing context on forecasting accuracy, we gradually increase the dimensionality of the regression model from q = 1 to 5. The results for this context-aware autoregressive forecaster are visualized in Fig. 4."}, {"title": "CONTEXT-AWARE TRANSFORMERS", "content": "The core architecture for both the ContextFormer-enhanced forecasters contains an input history em-bedding layer, six hidden layer blocks, and a final projection layer. Each of the hidden layer blocks consists of two parallel cross-attention layers and a self-attention layer. Each attention layer operates in a 256-dimensional representational space while employing an 8-head attention mechanism.\nPatchTST (Nie et al., 2023) is a popular transformer-based architecture for time series forecasting. In contrast to the traditional models that treat each time step as an individual token, PatchTST divides the data into patches, similar to what Vision Transformers (Dosovitskiy et al., 2021) do for images. Each patch in this setup represents a sequence of time steps, which enables the model to focus on long-term temporal patterns. By applying self-attention to these patches, PatchTST captures long-range dependencies more efficiently, reducing the computational cost associated with traditional transformers. This patch-based approach enables the model to handle longer sequences and large-scale forecasting tasks more effectively. PatchTST outperforms standard transformers on various benchmarks and can handle both univariate and multivariate time series data, making it a versatile choice for various forecasting tasks."}, {"title": "CONTEXTFORMER ADDITIONS", "content": "Metadata Embedding. The metadata embedding mod-ule consists of two fully connected (FC) networks fol-lowed by a transformer encoder. Discrete metadata isone-hot encoded and processed through one FC network,while continuous metadata is passed through a separateFC network. The input size of each network correspondsto the number of discrete or continuous features in thedataset, with the output dimensions consistently set to 256 for all experiments. If both metadatatypes are present, then their outputs are concatenated and processed through a linear layer to reducethe dimensionality from"}]}