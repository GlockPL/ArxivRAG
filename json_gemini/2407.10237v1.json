{"title": "Towards Green AI: Current Status and Future Research", "authors": ["Christian Clemm", "Kishan Wimalawarne", "Lutz Stobbe", "Jan Druschke"], "abstract": "The immense technological progress in artificial intelligence research and applications is increasingly drawing attention to the environmental sustainability of such systems, a field that has been termed 'Green AI'. With this contribution we aim to broaden the discourse on Green AI by investigating the current status of approaches to both environmental assessment and ecodesign of AI systems. We propose a life-cycle-based system thinking approach that accounts for the four key elements of these software-hardware-systems: model, data, server, and cloud. We conduct an exemplary estimation of the carbon footprint of relevant compute hardware and highlight the need to further investigate methods for Green AI and ways to facilitate wide-spread adoption of its principles. We envision that AI could be leveraged to mitigate its own environmental challenges, which we denote as 'AI4greenAI'.", "sections": [{"title": "I. INTRODUCTION", "content": "We are in the midst of an explosive growth of the development and integration of artificial intelligence (AI)- based systems into all aspects of human activities that has been speculated to be 'as transformative as the industrial revolution' and could incur profound social and economic changes [1]. The release of 'generative Al' applications, notably the text generator ChatGPT, text-to-image generators like Midjourney, and text-to-video models like Sora have recently brought public attention to the rapidly progressing technological capabilities. Al has been poised to be capable of accelerating breakthroughs in science [2], medicine [3], and other fields, due to increasingly sophisticated algorithms and advancing availability of the required computational power.\nHowever, besides the recognition of the immense potential of Al systems, the rapid progress has raised concerns about their potential impacts on societies, shared by researchers, policy-makers, and the public [4]. Besides social impacts, such as the expected transformation of the job market, the environmental consequences of the broad deployment of resource- and energy-intensive hardware-software systems raise concerns. The trend towards rapidly growing model complexity and size, trained on expansive volumes of data, has been suggested to be a prerequisite for increasing the performance of Al models. Reported \u2018neural scaling laws' [5] are based on the empirical observation of power-laws that govern the scaling of neural network performance with model and dataset size [6]. Accordingly, concurrent increases in model parameters, training tokens, and computational power are required to uphold the trend towards increasing model performance, in some ways reminiscent of Moore's law, which has been driving the miniaturization and performance progression of semiconductor technologies for decades [7].\nIndeed, assessing the growth in model complexity and consumed energy for the training of OpenAI's generative pre- trained transformer (GPT) models, the large language model (LLM) behind the popular ChatGPT, reveals that the model size has increased from 1.5 billion to an estimated 1.7 trillion parameters between GPT-2 and GPT-4, an increase by a factor of 1,000 [8][9]. Based on approximations, the energy consumed for the training of GPT models has grown by a factor of 2,000 between the release of GPT-2 and GPT-4 in a time span of just 3.5 years [8][10]. To provide context, the energy used to train and run ChatGPT has been estimated to be equal to the annual carbon emissions of 175,000 Danish citizens [11].\nThe rapidly growing computational requirements of Al models necessitate increasingly powerful hardware to provide the computational infrastructure required for the training and inference of Al models. Graphics processing units (GPU) provide the parallel processing capabilities and are employed in server systems operated in globally distributed data centers ('the cloud'). The energy needs of the compute hardware and required heating, ventilation, and air conditioning (HVAC) in data centers are ever-increasing. The IEA projects the electricity consumption of data centers could reach 1,000 TWh in 2026, roughly equivalent to the electricity consumption of Japan [12]. Some have voiced the idea to power Al-centric data centers via built-in nuclear reactors [13]. Besides energy, the resources needed to fabricate the integrated circuits and periphery, including critical raw materials (CRM), and to operate data centers, also require attention.\nThese concerns have led to increasing interest in 'Green Al', a term denoting \u201cAI research that yields novel results while taking into account the computational cost\" [14]. The goal of this paper is to broaden the scientific discourse on Green AI. We discuss the status and future research needed to support the development of Green AI through the lens of system- and life-cycle-thinking accounting for both 'assessment' and 'ecodesign' of the underlying software and hardware systems. We aim to identify limitations of the current status and discuss a vision of future research directions. In doing so, we do not set our focus primarily on 'green by Al', the notion of leveraging Al to solve sustainability challenges, such as training models to steer systems towards higher efficiency.\nIn this work, we use the terms 'green' and 'sustainable' synonymously with the environmental dimension of sustainability without implying the social and economic dimensions.\nThe remainder of this paper is organized as follows: Section II provides background information on the basics of AI, Green AI, and environmental assessment of such systems. Section III introduces an analysis framework that supports system- and life-cycle-thinking for environmental assessments and optimization of Al systems. Section IV provides an overview on the current status of the application of life cycle assessment methods to Al systems and provides illustrative examples. Section V discusses approaches and techniques that have been proposed to enhance the environmental compatibility of Al systems. Section VI discusses future research that is needed to support the further development and broad uptake of Green AI paradigms."}, {"title": "II. BACKGROUND", "content": "This section summarizes basic concepts of Al systems, Green AI, and of environmental impact pathways and assessment methods applied to Al systems.\nAI is the overarching term for the science of artificial intelligence, in which computers are used to simulate intelligent behavior, such as learning, judgment, and decision- making. Machine learning (ML) is a subfield of AI focused algorithms that are capable of learning from data [15]. ML models are trained on data to automatically identify patterns, which can then be used to make predictions or decisions on previously unseen data [16], known as 'inference'. Deep learning (DL) is a subset of ML and refers to an approach that utilizes artificial neural networks that are composed of layers of stacked artificial neurons employed to approximate complex, non-linear functions [17]. DL is at the core of the dramatic improvements in the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains [18]. DL is used for a wide number of tasks such as classification, regression, and generative modelling. A common example is image recognition, where the input is an image file and the output is a probability distribution among numerous potential objects. Other popular applications include so- called 'generative AI', including large language models (LLM) for chatbots, image, voice, music, and video generators.\nModel size is often identified via its number of parameters, which are coefficients in the model that are adjusted during the training process. The size of the training dataset is often identified via the number of features (e.g., tokens of a language model, pixels of an image, nodes of a graph) and number of entries. The performance of models is, inter alia, governed by hyperparameters that are set before training, such as the number of nodes and layers of a neural network. To find the best set of hyperparameters, models are typically trained repeatedly with different sets of hyperparameters, multiplying training time, cost, and energy consumption.\nAn Al model, at its core, is software code that is executed to perform calculations on specialized hardware that is run on servers, commonly located in data centers ('the cloud'). Most of the environmental impacts of AI systems derive from the manufacturing, installation, operation, and end-of-life treatment of the server systems and supporting data center and communication network infrastructure. The hardware for Al is typically based on an x86-type CPU (central processing unit), GPU (graphics processing unit) or TPU (tensor processing unit) that are operated as part of a server. CPU systems are based on software and memory (DRAM) which enables relatively slow sequential computing. CPUs are typically utilized for small AI models which require relatively short training cycles. GPUs are based on arithmetic logic units (ALU) and allow parallel computing to a much higher degree in comparison to CPUs. GPUs are therefore faster and allow calculations of medium to large Al models with considerable batch sizes. GPUs require considerable memory capacity which is typically provided through high bandwidth memory (HBM). TPUs are application-specific processors consisting of matrix multiplication units (MUX) in a systolic array computer architecture which work in conjunction with HBM. Memory access during matrix multiplication is not necessary which enables exceedingly high speeds. Both GPUs and TPUs are used for matrix calculations and large Al models with considerable complex training phases.\nData-driven processes (data mining), algorithms (ML), or artificial neural networks (DL) as well as big data analytics characterize Al systems. They require big data systems with specific properties to be capable of learning, modeling, and model application. These systems are necessarily distributed, as the data has become too large to fit on a single machine, be it hard drives or random access memory (RAM). Therefore, any algorithm used to analyze big data systems must be parallelizable. The data is available in different and unstructured formats (e.g., audio, text, or video) and cannot be handled according to a standardized scheme. In addition, the data is updated very quickly or delivered at a speed that does not allow the entire stream to be stored. The handling of such data streams imposes restrictions in terms of algorithms and fault tolerance. Fault tolerance in big data systems means that the systems continue to work even if individual nodes or network connections fail. This is also an important prerequisite for horizontal scaling, as a system can grow by adding nodes/connections. More compute nodes provide more storage and processing capacity for big data. The system's application programming interface (API) ensures that the application is not affected by the expansion of the system (no reprogramming). The data must be divided and distributed to different nodes, improving the overall application performance. In comparison to traditional data systems, the functionality moves from system level to application level. The reasons are the limitations of relational database systems, which are not good in data distribution and partitioning."}, {"title": "B. Background on Green AI", "content": "The negative environmental impact of ML is usually directly related to the required high computational power, i.e., the amount of energy required to train and run ML models [19]. While AI research and development has traditionally aimed to maximize model performance, such as minimizing loss functions, Green AI takes account of the environmental footprint and focuses on minimizing computation while still producing accurate results [20][14]. Factors beyond software- related computational power like hardware manufacturing, transportation, or e-waste are harder to quantify and often not considered in literature to date.\nA systematic review of literature on Green Al identified 98 studies, with 76% having been published since 2020, indicating the recently growing interest in this field [21]. Among others, the review concluded that most existing literature has addressed Green Al at the level of energy efficiency only [21]. Energy efficiency is related to the computational power required for training and operation of AI systems. Further, most studies in the field thus far have focused on the model training phase, which is frequently highlighted for its immense compute requirements and energy consumption for the training of increasingly complex models [21]. The most frequently addressed topic in Green AI papers was reported to be monitoring approaches to study the energy and/or carbon footprint of AI models, followed by research on hyperparameter tuning, model benchmarking, deployment, and model comparison [21].\nIn our view, Green AI should be considered to encompass any method and measure that aims to minimize or avoid any category of environmental impact that may arise during the AI/ML system's life cycle, including embodied and operational carbon emissions, material efficiency and circularity, and potentially other relevant categories such as the water footprint."}, {"title": "III. PROPOSED ANALYSIS FRAMEWORK", "content": "Al is a complex system of algorithms, model architecture, data sourcing, training, and hardware, among other components, and achieving Green AI requires effort that targets all stages of an Al system's life cycle [22]. We agree with this understanding and propose a framework consisting of the key elements of Al systems on both software and hardware level."}, {"title": "IV. ENVIRONMENTAL ASSESSMENT OF AI SYSTEMS", "content": "Discourse on the sustainability of Al systems and the development and application of effective measures to mitigate environmental impacts requires suitable metrics and indicators capable of capturing the full scale of system-wide effects. This section explores methodical approaches and related aspects for environmental assessments of AI/ML systems, following the parallel life cycle thinking approach outlined in section III.\nLife cycle assessment (LCA) is an ISO-standardized method used to quantify environmental impacts of products and services (ISO 14040/44). All material and energy inputs and outputs as well as products, waste, and emissions are accounted for along all processes associated with the entire life cycle of a product or service. Environmental impacts are quantified along various categories, including global warming potential (GWP), abiotic resource depletion potential (ADP), ozone depletion potential (ODP), or ecotoxicity and human toxicity potential. Carbon footprinting is a subset of LCA in which the carbon-equivalent emissions of greenhouse gasses are considered as the only impact category. The product carbon footprint (PFC) is a suitable indicator for the environmental impact of the two most important life cycle phases; the manufacturing phase, including raw material extraction, and the use phase, which relates to the power consumption of hardware systems during usage.\nLCA and PFC consider the entire life cycle of products. For physical products, the main life cycle stages are typically raw material sourcing, processing, manufacturing, distribution, usage, and end-of-life treatment (EoLT). In the life cycle of software, such as AI/ML models, the equivalent stages consist of model development and data sourcing, model training, deployment, and decommissioning. In hardware-software systems, the parallel life cycles of hardware and software intersect at the use stage.\nA key decision when applying LCA or PCF is on the functional unit (FU), which is a measure of the function or utility of a product or service to which all environmental effects are scaled. For instance, for a car, a typical FU is a vehicle kilometer traveled. All environmental impacts occurring throughout the car's life cycle, including manufacturing and EoLT, are proportionately allocated to the functional unit."}, {"title": "A. Software Life Cycle", "content": "Unlike physical products, software or code is a virtual good that exists and functions only in conjunction with computer hardware. Environmental assessments of Al systems are therefore largely assessments of the hardware on which the software operates. An environmental assessment should consider the entire hardware system required to create and operate an AI service, including the computer hardware and network components that connect the service to end-users.\nThe decisions taken during the development of AI/ML models directly determine the computational power required for training and inference, and therefore also the scale of the environmental impacts from hardware manufacturing and usage. Computing power is typically measured in floating- point operations per second (FLOPS), processor (e.g., GPU, CPU) utilization rate, and compute hours [23]. During use, the utilization rate, lifetime, and maintenance of Al services are further variables determining the total energy consumption.\nA number of previous studies have evaluated the energy consumption and carbon emissions of AI/ML models. Prominently, reference [24] analyzed the carbon impact of training their own state-of-the-art models, estimating that training one large transformer model takes 274,120 GPU hours, consuming 656,347 kWh energy in the process, resulting in 284 metric tons CO2e emissions, based on the average U.S. electricity carbon intensity. A further 29 papers that address the carbon footprint or ecological footprint have been identified in a systematic review of Green Al literature [21]. From the previous literature, it can be concluded that the training phase of AI/ML models has thus far been the largest factor in life cycle energy consumption. The training and re- training of models for activities such as hyperparameter tuning have been identified as the current hot-spots [21].\nThe above insights emphasize that the software design is the main predictor of the environmental impacts of AI/ML systems that arise as a consequence of software compute requirements through the manufacturing and usage of hardware."}, {"title": "B. Hardware Life Cycle", "content": "In the field of information and communication technologies (ICT), the dominant life cycle phases in terms of environmental impacts are typically the manufacturing (embodied impacts) and the use phase (operational impacts).\nThe manufacturing phase includes all raw materials, component production, electronic packaging, assembly, and testing. Existing LCAs of computers show that integrated circuits (ICs) such as processor and memory chips, as well as printed circuit boards (PCBs), connectors, and certain bulk materials are very energy and resource intensive and therefore generate a significant carbon footprint in the manufacturing phase [25]. The environmental impact of the manufacturing phase is influenced by the technology generation of the component (e.g., the miniaturization of the transistors), the production process technology, the age of the production equipment, and the available energy mix at the production site. Carbon-equivalent emissions from the production stage are also considered 'embedded emissions'.\nThe use phase can also have a high environmental impact depending on the processor technology, memory capacity and network configuration of the computer. In the past few years, the power consumption of the processor unit has been increasing constantly. This trend is driven by an increasing number of compute cores and higher clock frequency. Not only CPU systems but also GPU systems demonstrate this trend. High performing processor units reach a power consumption of 500 to 700 watts [26]. The environmental impact of this increasing power consumption and resulting energy density in the server racks derives primarily from the generation of electricity.\nOn cloud-level, key factors are the energy efficiency of the operation, including server and HVAC equipment, and the carbon intensity of the energy mix, which is determined by the share of renewable and fossil-based energy sources.\nTo conduct environmental assessments of the hardware component of AI/ML systems, first, a use case needs to be selected that defines the access and distribution network between the customer's end-user device and the Al server system. The AI system might be situated in a large data center thousands of kilometers away from the user. The number and type of network hops on the way to the data center should be considered in the life cycle inventory (LCI). The type and distance of the access network e.g., over cable (optical fiber) or radio (mobile communication) contributes the most to the environmental impact and needs specification [27]. Secondly, the Al systems and the related network are a shared infrastructure. Therefore, the environmental assessment of an Al service accounts for the resources and energy consumption of the physical Al system only proportionally. Due to this condition, a use pattern needs to be defined that specifies the number of users that share the system or the number and data intensity of the AI applications (services) that are created by the Al system and distributed over networks to the users. Our own calculations indicate that the network creates only a minor environmental impact in comparison to the computer system and end-user device, since it is an exceptionally large, shared infrastructure."}, {"title": "C. Exemplary PCF of Server Hardware", "content": "This section provides an exemplary estimation of the carbon footprint of server hardware to illustrate which technical and operational aspects of current AI/ML hardware contribute to increasing power consumption and carbon footprint. Due to data availability, a CPU-based system is discussed in more detail to demonstrate the general approach, with a following discussion on notable differences between CPU- and GPU-based systems, the latter of which are much more relevant for AI/ML systems.\nThe carbon footprint of the manufacturing phase of a server can be approximated based on technical data and generic conversion factors. The primary data are the type, number, manufacturing generation, and die area of the integrated circuits (ICs) [28]. A distinction must be made between CMOS technology for processors, DRAM technology for memory, and NAND for storage ICs. Another dataset includes the chip packaging technology as well as the size and number of layers of the interposer and PCB. A modern, scalable 4th generation Intel\u00ae Xeon\u00ae processor (Sapphire Rapids, 8468), which has been manufactured in Intel 7 lithography generation since 2023, serves as an example. This CPU has 48 cores and a die area of 8.12 cm\u00b2 [29]. The power consumption (Thermal Design Power, TDP) is specified as 350 Watts. The CPU is suitable for the LGA4677 socket, can be equipped with up to 4 TB of DRAM and uses the Intel C741 chipset. The server mainboard used for the calculation example is the MBD X13SEM-T from Supermicro. This mainboard in Micro-ATX format has an area of 595 cm\u00b2 and can be equipped with 2 TB DRAM on 8 DIMM sockets (citation needed). Regarding the 2 TB DRAM, it is assumed that a total of 1.024 modern 2 GB DRAM ICS are installed, each 0.6 cm\u00b2 in die area. The remaining IC (die) area is estimated at 6 cm\u00b2 based on the IC packages visible on the mainboard. The number of layers of the mainboard is not known. At least 16 layers are assumed for the example.\nThe carbon footprint of the production is estimated based on these data and assumptions. The manufacturing carbon footprint of the large and complex XEON CPU is estimated at 3.0 kg CO2e per cm\u00b2 die, the remaining, less complex ICs at 1.8 kg CO2e per cm\u00b2 die, and the assembled 16-layer mainboard at 0.06 kg CO2e per cm\u00b2 PCB. All other components and materials including connectors, direct water- cooling system, fans, power supply unit, and housing are not included in the exemplary calculation. The manufacturing of these components is less complex, and the environmental impact is mainly due to the number and volume of materials used. The carbon footprint calculation based on the above assumptions shows that the production of the CPU system amounts to almost 1,200 kg CO2e. The production of the DRAM has a significant impact due to the large overall die area. If the CPU system is used for 5 years, the proportional carbon footprint per year is 240 kg CO2e.\nDuring the use phase, the power consumption of the \u03a7\u0395\u039f\u039d 8468 CPU is 350 W. The DRAM can be estimated at 0.2 watts per die. The DRAM therefore has an additional electrical output of 205 W. At higher clock frequency, this value can almost double. The computer system has an estimated power requirement of 555 watts in active mode. Under the assumption that this system is utilized 75% of the time with 555 W and 25% of the time in idle mode with around 200 W, the resulting annual electricity consumption is 4084 kWh. With an average carbon intensity of the energy mix of 0.4 kg CO2e per kWh (typical for Germany in 2022), this results in annual emissions of 1,634 kg CO2e. The total annual carbon footprint of the CPU system is therefore 1,874 kg CO2e. The use phase clearly predominates with 87%. A shorter service life of the server system would influence this value. If the server were only used for three years, the annual carbon footprint would increase to 2034 kg CO2e/a. If the server were operated using power from renewable energies at 0.05 kg CO2e per kWh, the carbon footprint of the usage phase would be drastically reduced from 1634 kg CO2e/a to 204 kg CO2e/a. Under this condition the annual use phase would be slightly lower than the annual production share of 240 kg CO2e/a.\nAs this example shows, not only the processor but also the memory configuration is influencing the environmental footprint of the server system in both the manufacturing and use phase. This general trend applies not only to CPU systems but to GPU and TPU systems as well."}, {"title": "D. Exemplary Comparison of a CPU and GPU System", "content": "The NVIDIA H100 with SXM5 board form factor is a GPU system specified with 80 GB HBM (High Bandwidth Memory) and a TDP of 700 W. The 80 GB memory configuration of the GPU system is only 2% of the 4 TB DRAM of the CPU system. At around 16 x 8 cm\u00b2, the SXM5 board is significantly smaller in area than the 24.4 x 24.4 cm\u00b2 Micro-ATX board of the CPU system, by over a factor four. However, the environmental assessment needs to consider the actual implementation of the H100 GPU module in an AI/ML compute system. An example is the NVIDIA DGX AI system which consists of eight H100 modules fully connected via four NVLink switches. All the modules are mounted on a mainboard.\nThe significantly smaller memory capacity and compact printed circuit board results in a much lower manufacturing carbon footprint of about 200 kg CO2e for the GPU system. This is a rough estimation since a bill-of-material (BOM) and other technical data such as the packaging technology of the bandwidth memory (HBM) and the PCB stack-up are not available. Moreover, the environmental assessments need to consider the entire system set-up including the power supply, network connectivity, direct water-cooling system, and housing. Nevertheless, the GPU system seems to have an environmental advantage with respect to the manufacturing phase due to the considerably lower memory capacity requirements. However, the memory capacity of GPU systems is increasing with each new system generation as well. Memory is therefore a good indicator for the environmental impact of a computer system.\nConcerning the use phase, the higher average power consumption of about 700 W negatively affects the environmental footprint. Let us assume a similar use profile with 75% of the time in active mode and 25% in idle as in the case of the CPU system. The idle power consumption is unknown, and we make the assumption of 250 W. Based on these assumptions the annual power consumption accumulates to 5,147 kWh. This is 26% more power consumption per year compared to the CPU system and results at 0.4 kg CO2e per kWh to 2,059 kg CO2e/a. The use phase power consumption clearly dominates the environmental impact when electricity from predominantly fossil fuels is used. If renewable energy sources are used, the carbon footprint is considerably lower with 257 kg CO2e/a at an energy mix of 0,05 kg CO2e per kWh."}, {"title": "V. METHODS FOR GREEN AI SYSTEMS", "content": "This section discusses the current status of methods for Green Al systems by discussing approaches that have been proposed in literature aiming to improve the efficiency of AI/ML systems. Following our analysis framework we first discuss approaches on software-level (model and data) and on hardware-level (server and cloud). Due to the broad scope, we do not aim to provide a comprehensive analysis, but highlight relevant and potentially impactful methods."}, {"title": "A. Software: Model and Data", "content": "Methods have been researched to develop sustainability in AI/ML for years [30]. These methods have been looked at from different perspectives of AI/ML models, data, and distribution of data and models.\nA commonly used approach to improve the efficiency of AI/ML models is by quantization [31], compression/pruning [32], knowledge distillation [33], and low-rank decomposition [34]. Quantization converts the high-bit floating-point parameters of AI/ML models into lower numerical precision low-bit floating-points, thus reducing the memory requirements and gaining faster computations. Methods such as sparse regularization allow pruning of redundant variables in AI/ML models during the training process, such that the overall models use less memory and obtain fast inferences. In some AI/ML models, parameters are arranged as large multidimensional arrays which can be decomposed as smaller arrays using low-rank decomposition methods. The low-rank decomposed smaller arrays can reconstruct approximations of the original parameters that use less memory and enable faster training of AI/ML models. Knowledge distillation is another useful approach that transfers the knowledge of a large, trained model to a smaller model. In practice, the development of AI/ML models by using the above methods can be more cost- and resource-efficient to obtain sustainability in AI/ML compared to improving hardware or network configurations. Their effectiveness is highly evident with many uses in popular AI/ML models. Large language models have been developed with low rank adaptation [35] to reduce the training parameters by approximately factor 10,000, requiring fewer compute budget and thereby GPUs for model training. Reference [36] have demonstrated a compression ratio by factor 71 with pruning of the convolutional neural networks used for the popular image dataset AlexNet.\nData used in modern AI/ML applications such as LLM, computer vision, and recommendation systems can be considered as big data due to both the substantial number of features and the data entries. Sustainability in AI/ML can be pursued by using data-centric methods as provided in the survey by [37], which includes methods such as data augmentation, data distillation, and few-shot learning. Moreover, dimension reduction is a well-understood and easily implementable method that can reduce the memory requirements and gain fast computations in ML/AI applications. Such dimension reduction methods can be as simple as multiplying the high-dimensional features of data with a simple randomly generated matrix to project to a small- dimensional features [38] or more complex as learning to reduce dimension by a learning models such as an auto- encoder [39]. Learning from reduced dimensions can greatly improve memory requirements and improve training times with limited amounts of loss of accuracy. Additionally, the use of appropriate optimization methods [40] for ML/AI problems is crucial for fast training times and efficient memory usages.\nApart from the above data and model-based approaches, we also want to emphasize that efficient parallelism of AI/ML models is fundamentally essential to achieve efficient computations. The advent of multi-core and multi-GPU systems with large memory have helped to achieve a significant parallelism for ML/AL models. Furthermore, popular Al programming frameworks such as PyTorch and TensorFlow provide support for writing code for parallel execution of AI/ML algorithms easier for programmers. Additionally, frameworks such as Colossal-AI provide support on various methods of parallelism such as data parallelism, pipeline parallelism, and novel approaches such as sequence parallelism [41], zero redundancy optimizer (ZERO) [42], and auto-parallelism [43].\nDistributed learning is also a common method to achieve computational speed for tasks with high-volumes of data and computationally intensive models. One of the most investigated recent advances in distributed learning for Green AI/ML is Federated Learning [44]. This approach has a master server and distributed child nodes; however, each node learns with its own data locally and the learned parameters are communicated with the server. The server learns from the aggregate of all locally learning parameters before redistributing the updated parameters to each child node. Despite limitations of achieving sustainability targets, distributed learning and federated learning remain as an important research direction for achieving sustainability in AI.\nRecent research provides a summary of 30 techniques for development of Green AI/ML models, including some of the above-mentioned methods [20]. They have identified 5 tactics to investigate sustainability as data-centric, algorithmic design, model optimization, model training, and deployment. We believe that their work can be considered as a foundation for Green AI/ML to further develop practical methodologies. However, their tactics may not be complete since well-known Green Al methods such as low-rank decomposition are not considered. Additionally, AWS provides support for Green Al development which can also be a valuable resource for the development of Green AI/ML."}, {"title": "B. Hardware: Server and Cloud", "content": "While the discourse on Green AI/ML is still relatively new, the discussion and implementation of strategies to minimize environmental impacts of computer hardware and data centers has been ongoing for decades. We therefore do not aim to provide a comprehensive overview of individual measures, but discuss select strategies and initiatives of higher perceived relevancy.\nIn 2021, cloud infrastructure providers and data center operators in Europe formed the \u2018Climate Neutral Data Center Pact', a self-regulatory initiative to make European data centers climate neutral by 2030 via measures including the usage of low-carbon energy, water conservation, heat recycling, and reuse and repair of hardware [47]. In the US, Google has pledged to run of their global data centers on low- carbon power by 2030. Microsoft has declared to be carbon- negative by 2030 and offset all their historical carbon footprint by 2050 [48]. Two main ways have been described to achieve carbon-neutral data centers in practice: First, by maximizing energy efficiency and the use of low-carbon energy, and second, by carbon offsetting [49].\nOptimizing the energy consumption of cloud computing has long been a topic of interest, both due to economic considerations but also environmental concern. For instance, load balancing is conducted to distribute the workload across multiple nodes in a shared pool of computing resources. It helps in optimal utilization of available resources and enhancing the performance of the system, thereby reducing energy consumption and the associated carbon emission rate. Regarding HVAC, Google reported already several years ago to have identified a method to reduce data center cooling needs by 40%, by training a deep learning model on historical data and predicting conditions in the immediate future to increase the overall efficiency [51].\nBeyond minimizing energy consumption of data centers, reducing the carbon intensity of consumed energy is key, such as by locating data centers in close proximity to renewable energy sources, such as wind parks. Such measures are of high relevance as the power consumption of AI systems dominates their overall carbon footprint with approximately 80% of their total emissions, as we have highlighted in section IV.\nWhile the above strategies and initiatives are mostly concerned with reducing the operational impacts of data centers, minimizing the embedded impacts of server and cloud hardware is also key for the realization of Green AI. In the EU, Regulation (EU) 2019/424 on ecodesign requirements for servers and data storage products under the European Ecodesign Directive sets minimum requirements to limit the environmental impact of servers and data storage products. While focusing primarily on energy efficiency, the regulation also includes material efficiency requirements, including the use of joining, fastening or sealing techniques that do not prevent the disassembly of several key components, including CPU, GPU, memory, and mainboard to enhance repair and reuse, and further mandates secure data deletion techniques to facilitate reuse.\nBeyond such measures, resource efficiency during the fabrication of server equipment (embedded impacts) need to be addressed. Naturally, maximizing the use of low-carbon energy in the fabrication facilities for semiconductors, PCBs, and other required server hardware that powers AI/ML is an effective lever to minimize embedded carbon. Further, striving to apply principles of the circular economy, including lifetime extension of the hardware including through reuse, repair, and remanufacturing, maximizing collection and material recovery rates in recycling processes can all be effective measures. This is already practiced, at least in part, with for instance Amazon Web Services having implemented refurbishment processes for data center hardware to extend its useful life [53]. Other companies like Google and Microsoft have also introduced circular and zero- waste strategies to boost reuse and recycling of hardware to maximize resource efficiency and avoid emissions from the production of new equipment [53].\nAs a last example, optimizing hardware and matching of software requirements and hardware capabilities is used to contribute to efficiency and thereby Green AI. While GPU are more proficient than CPU for typical AI/ML workloads, more specific and dedicated hardware like TPU, ASIC (application- specific integrated circuits) and FPGA (field-programmable gate array) can be more efficient for specialized tasks, increasing the achievable operations per unit of input energy."}, {"title": "VI. DISCUSSION OF FUTURE RESEARCH", "content": "This section discusses what we believe is missing from the current discourse on Green AI and/or where future research could contribute to its further development and uptake. We make proposals for future research on software-level (model and data), hardware level (server and cloud), and related to environmental assessment and metrics."}, {"title": "A. Proposals on Software-Level: Model and Data", "content": "The most effective method to achieve sustainability in AI/ML is by designing AI/ML models to minimize the resource usage and the carbon footprint. Therefore, it is important to further research on sustainable methods on AI/ML model design, data processing, optimization, deployment, and distributed learning. A major practical limitation in developing Green Al models is the technological complexity of implementing methods such as quantization, pruning, knowledge distillation, and low-rank decomposition. Incorporation of such methods to existing AI/ML systems may require a considerable amount of expertise, time, and resources, which are not always at hand for companies, institutes, and individuals. Hence, we want to propose several strategies to make Green Al models development more practical.\nAwareness and availability of knowledge and resources of Green Al methodology for model developers is one of the primary and effective ways of making wide-spread usage of Green Al models. We believe the development and practice of (Machine Learning Operations) MLOps and design patterns for Green Al is vital. Amazon Web Services (AWS"}]}