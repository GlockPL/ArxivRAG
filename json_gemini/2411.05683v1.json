{"title": "Data-Driven Distributed Common Operational Picture from Heterogenous Platforms using Multi-Agent Reinforcement Learning", "authors": ["Indranil Sur", "Aswin Raghavan", "Abrar Rahman", "James Z Hare", "Daniel Cassenti", "Carl Busart"], "abstract": "The integration of unmanned platforms equipped with advanced sensors promises to enhance situational awareness and mitigate the \"fog of war\" in military operations. However, managing the vast influx of data from these platforms poses a significant challenge for Command and Control (C2) systems. This study presents a novel multi-agent learning framework to address this challenge. Our method enables autonomous and secure communication between agents and humans, which in turn enables real-time formation of an interpretable Common Operational Picture (COP). Each agent encodes its perceptions and actions into compact vectors, which are then transmitted, received and decoded to form a COP encompassing the current state of all agents (friendly and enemy) on the battlefield. Using Deep Reinforcement Learning (DRL), we jointly train COP models and agent's action selection policies. We demonstrate resilience to degraded conditions such as denied GPS and disrupted communications. Experimental validation is performed in the Starcraft-2 simulation environment to evaluate the precision of the COPs and robustness of policies. We report less than 5% error in COPs and policies resilient to various adversarial conditions. In summary, our contributions include a method for autonomous COP formation, increased resilience through distributed prediction, and joint training of COP models and multi-agent RL policies. This research advances adaptive and resilient C2, facilitating effective control of heterogeneous unmanned platforms.", "sections": [{"title": "INTRODUCTION", "content": "The integration of unmanned platforms equipped with advanced sensors holds promise for mitigating the \"fog of war\" and elevating situational awareness. However, managing and disseminating the influx of data from such platforms poses a substantial challenge to the information processing capabilities of central Command and Control (C2) nodes, particularly given the exponential growth in data volume with increasing platform numbers. The current manual processing methods are ill-suited for future C2 scenarios involving swarms of unmanned platforms. In this study, we present a framework utilizing a multi-agent learning approach to overcome this barrier.\nWe consider a framework where agents communicate with each other (and with humans) in an autonomous fashion, and such communication functions are trained in a data-driven manner. At each time step, each agent can send/receive a real-valued message vector. The vector is a learned encoding of the agent's perception or field of view (FoV). The vectors are not easily interpretable by adversaries, allowing for secure message transfer.\nOn the receiver's side, the message must be decoded to recover the sender's perception and action. Furthermore, the information should be integrated (aggregated over time) into a Common Operational Picture (COP). Like the encoder, the decoder is also learned in a data-driven manner. In this paper, we simplify the definition of a COP as the current state (position, health, shield, weapon, etc.) of each friendly and enemy agents on the battlefield. We argue that the COP is essential to decision-making agents.\nIn recent years, AI/ML approaches that are trained end-to-end in a data-driven manner have shown great promise. In the context of a data-driven autonomous COP, one advantage is that no modelling assumptions are made about the noise in the sensors and actuators, the dynamics of the adversary, etc. With sufficient training, our data-driven method will produce highly precise COPs.\nHowever, ML models can be sensitive to deviations from the training data or training scenarios. This contrasts with the DDIL (denied, disrupted, intermittent, and limited impact) environments, which are typically assumed in army C2 scenarios. Our experiments emphasize evaluation of the resilience to increased fog, denied GPS, and disruption of communications (e.g., jamming).\nData-driven end-to-end training of our encoders and decoders is achieved using deep learning of deep neural networks (DNN). One challenge associated with the application of DNNs to COP formation is the lack of human interpretability in communications. Human interpretability is crucial for a human operator to effectively control the swarm. For example, by interpreting the communication, the operator might understand the features used by the swarm for (autonomous) decision-making. Our method is human-machine interchangeable, meaning that a human operator can decode the incoming messages and encode their perceptions to communicate with the swarm. The resulting COP enables human directability of the swarm.\nIn practice, the COP is heavily used in mission execution, e.g., to ensure coordinated movements. We hypothesize that incorporating the COP into autonomous decision-making agents will produce resilient multi-agent policies (e.g., resilience to changes in the enemy). Our experiments compare multi-agent policy learning with and without the COP against multiple state-of-the-art methods and validate the hypothesis.\nNext, we summarize our methodology. We first describe our deep learning formulation in which each agent encodes its perceptions and actions into compact vectors and transmits them. The underlying embedding vector space is shared across agents to enable a shared situational understanding. An encoder-decoder is trained per agent to produce local COPs. The local COP should be consistent with agent perceptions and should predict all units' state (incl. position) over the area of operation.\nEnd-to-end training of the COP is performed jointly with agent policies using Deep Reinforcement Learning (DRL) on a diverse set of simulated scenarios, initial force configurations, and adversary actions. The output of training is an encoder-decoder neural network (NN) and a policy NN shared across agents. The training can be configured in several ways: to minimize bandwidth, maximize resilience to disruption e.g., channel noise, packet loss, jamming of GPS, etc. The method can be applied to coordinated information-gathering missions.\nExperiments are performed in the Starcraft-2 (SC2) multi-agent environment [1]. The effectiveness of our method is empirically observed in multiple blue-vs.-red scenarios that are modelled in SC2. Specifically, we test and evaluate our method on the challenging and realistic TigerClaw scenario that was developed by the DEVCOM Army Research Lab (ARL) and Army subject-matter experts (SMEs) at the Captain's Career Course, Fort Moore, Georgia, US [2]."}, {"title": "PROBLEM FORMULATION", "content": "Consider a multi-agent system consisting of N agents. Each agent is defined as the tuple (Oi, Ai, Cin, Cout, Pi), for agent i = 1 ..., N. The agents' perception of the environment is a mapping Oi: S \u00d7 Pi \u2192 Rd from the underlying state of the world (s \u2208 S) and agent-specific capabilities P (e.g., field-of-view) to a d-dimensional observation vector o\u1f31= O(s,p\u2081) \u2208 Rd (dropping the dependence on time for brevity).  Figure 1 shows an example of state and agent observations. In this paper, agent observations and actions are in the agent's frame of reference i.e., egocentric, whereas states are represented in a global frame of reference. For example, agent observations might include range, bearing, and health of observed units, whereas the state will include ground truth about all quantities of all units (blue-vs-red).\nAt each time step, each agent receives a C-dimensional communication message from each agent from the previous time step. The messages are processed using the function Cin: RCN \u2192 RC. In this paper, denied communication and out-of-range communication is represented as zero-valued vectors at the receiver Cin. We do not assume stable communication pathways to be able to send and receive transmissions; rather, messages are \"zero-ed\" out in the communication channel unbeknownst to the sender.\nAt each time step, each agent sends a C-dimensional communication message using the function Cout:0i \u00d7 Ai \u00d7 RC \u2192 RC that maps the agent's local observation, action at the current time step and the result of Cin to an output message. Information to uniquely identify the agent can also be transmitted by including a unique ID for each agent in the observation space.\nThe functions Cin and Cout are represented as neural networks (NN) with learnable weights. The NN weights are shared across agents (and human operators).\nEach agent can take an action a\u00b9 from its action set A\u00b9 that affects the evolution of the state. The action set can be fixed or a function of capabilities P\u00b9 or observation o\u012f. We train a policy shared across agents \u03c0: \u039f\u1f31 \u00d7 RCN \u2192 Ai that maps its local observation and messages from other agents to an action in its own action set.\nIn one sense, the multi-agent system forms a distributed mobile sensor network where each agent senses only a part of the world. Inference of the underlying state (including friendly and enemy positions) corresponding to the agents' local observations is a key component of situational awareness and is necessary for developing a Common Operational Picture (COP), i.e., a global understanding of the uncertainty in the underlying state as the battle evolves. In the distributed setting, each agent forms a local prediction of the COP via communication and propagation of other agent's local COP, observation and action, which allows for uncertainty reduction and improved situational awareness.\nFor example, a friendly agent could observe the range and bearing of enemy units in its local frame of reference (e.g., a tank at 1200 meters, 30 degrees north). Given this information, a different friendly agent in another part of the battlefield needs to infer the tank's relative position to itself. This is challenging, especially when global positioning is denied. This challenge is illustrated in Figure 2. Our COP enables a solution to this challenge using the shared embedding space for communications.\nFigure 3 shows the overall learning-to-communicate framework. All the components are represented with NNs, and all the weights are trained end-to-end in a data-driven manner. Since the communication modules are shared across agents and the decoder output is updated in real-time using communication, we refer to the predicted state as the Common Operational Picture (COP). Future work can extend our data-driven COP framework to incorporate future actions, such as agent intent.\nSince the communication is grounded in an interpretable state, human operators can use the learned black boxes to receive and interpret the communication between autonomous platforms. Similarly, human operators can encode their observations and transmit them using Cout. In the rest of this paper, we do not consider human operators and assume the agents to be autonomous platforms.\nThe multi-agent system models a swarm performing a joint task, e.g., autonomous platforms in a C2 operation. The joint task is captured as an overall system reward"}, {"title": "COP MODEL AND TRAINING", "content": "In this section, we describe the neural network (NN) architecture and training for the communication modules Cin and Cout, decoder D, and the policy \u03c0. We motivate the design choices that will be investigated in Section 4. Note that our COP model can be optimized with any MARL algorithm such as QMIX.\nWe use the autoencoder (AE) concept as follows: an NN encoder transforms an agent observation to a latent vector z. Paired with the encoder is an NN decoder that reconstructs the observation given the latent vector z. An AE is trained end-to-end to minimize reconstruction error, e.g., mean squared error (MSE).\nFollowing [17], instead of communicating \"raw\" observations, we can communicate the compact latent vector z and ensure that the embedding vectors correspond to the agent observations. The embeddings are not easily interpretable by the adversary allowing for secure message transfer. However, there are two significant challenges in the application of AE to COP.\nSuppose each agent encodes its own observation o\u012f at the current time step and action a\u22121 of the previous time step and transmits the latent vector. Note that the observation and action are in the agents' frame of reference. On the receiver side at time t + 1, an agent (that can receive the message) can use an NN decoder D to decode the sender's observation. However, the decoding will be in the senders' frame of reference.\nSecondly, communication of agent observations alone is insufficient. Consider agents in a linear chain where each agent can only communicate with its neighbor. For the first agent's observation to reach the last agent, each agent must not only transmit its own (encoded) observation, but also (re-)transmit the message received from its neighbor. In general, the number of messages can grow exponentially (e.g., in tree-shaped connectivity structure). Rather, the agent must integrate the received observation (after decoding) and its own observation into a local COP. Then, only the latent vector corresponding to the local COP needs to be transmitted.\nIn this distributed setting, local integration of COP leads to predictions about agents outside any given agents' visual range. The local COP, or an aggregation of local COPs, should be interpreted as a density over the entire battlefield. Based on prior information, recency of observation and communication, the COP should have high density around likely agent positions. The spread or uncertainty should decrease with the frequency of observation of a given part of the battlefield by any agent.\nWe used the agent position attribute as an example to motivate the challenges in COP prediction and our distributed solution. In practice, our COP contains all information relevant to the state of an operation, e.g. agent armor, health, weapon status etc., each with an associated uncertainty (e.g., standard deviation). Our data-driven learning method, where ground truth is used to train each agent's COP model, is general to all attributes. We do not need to assume a sensor noise model or agent motion model in order to capture the uncertainty in each attribute represented in the COP."}, {"title": "OBSERVATION ENCODING AND DECODING", "content": "We apply a standard AE to the observation and action. Each agent encodes its own observation o\u012f at the current time step and action a\u22121 of the previous time step using NN encoder Eobs, and transmits the embedding latent vector z = Eobs(o, a-1). We use a tanh activation on the encoder output so that each entry in the vector z\u012f is bounded in the range [-1, +1].\nWe consider two versions of the observation decoder:\n\u2022 An explicit decoder NN Dobs that maps z to (o, a-1). The observation AE is trained to minimize\n\u2022 An implicit decoder that is part of a COP decoder, where reconstruction (o, a-1) is extracted from the COP and the known agent capabilities (e.g., FoV)."}, {"title": "INTEGRATION OF COMMUNICATION", "content": "As described above, each agent must integrate received communications and produce an embedding vector corresponding to the COP. The embedding should be informative as an additional input to agent policies. The embedding space is shared across agents to enable a shared situational understanding among agents and human operators.\nWhat type of NN architecture should be used to integrate communication? All received communications, from all agents can be potentially informative. Thus, we choose an attention-based architecture [8], which uses all-to-all connections, over an architecture based on convolutions that capture local connections. The attention-based architecture can learn to selectively attend to certain messages as a function of inputs, which is more powerful than fully connected architectures (like perceptron).\nWe use cross-attention [8] to process two sequences. Let z be the integrated COP embedding vector. Let_c = [z\u00b9,...,zN] and c = [z},...,z] be the embeddings received by the ith agent (zi as in Eq. 2). Then,\nNext, we track the temporal evolution of the COP using a recurrent NN (e.g., GRU [9]) that updates hs = GRU(zs) using the history of zs values. It is this hs that we pass to the policy and the COP decoder.\nWe train a COP decoder NN Dcop with the ground truth egocentric state si = f(s, i), s \u2208 S, where f is centering the state on the agent. For example, centering translates the positions of all units relative to the position of agent i."}, {"title": "INCORPORATING INITIAL STATE", "content": "In some scenarios, information about the initial state is known (e.g. initial enemy positions gathered from ISR). Incorporating a known initial state can significantly increase the accuracy of predicted COPs because it provides ground truth positions especially for enemy units outside the field-of-view of all friendly units. The COP model can learn to track the changes in the state by decoding the friendly actions and projecting the effect on the state. By using different initial states, we can increase the diversity of the training set.\nWhen an initial state so is provided (it is optional), we use it to initialize the hidden state of the GRU (Figure 4). The initial state is projected to the dimensions of the GRU hidden state using one linear layer. Winit is trained end-to-end alongside the other parts of the model."}, {"title": "HALLUCINATION", "content": "Note that the COP prediction is over a generic set of attributes including position x and health attributes h, etc., over all the units. The loss function in Eq. (2) and Eq. (3) gives uniform weightage to all attributes. We add a term to the training objective Eq. (4) to explicitly penalize hallucinations of agents that are present in the COP, but, in fact, not present in the simulation due to zero health."}, {"title": "EXPERIMENTS", "content": "In this section, we evaluate the data-driven approach for COP prediction and its impact on multi-agent policy learning. While ML-driven methods such as ours can be highly precise, they can be fragile and may deviate from the training scenarios or simulations. Therefore, we evaluate the resilience of the COP and the policy on separate test scenarios [12]."}, {"title": "OBSERVATION AND ACTION SPACES", "content": "As mentioned earlier, we use the Starcraft-2 (SC2) [3] game environment to study Command-and-Control (C2) scenarios. We use the Starcraft Multi-Agent Challenge (SMAC [1] and SMACv2 [10]) that instruments the simulator and provides a multi-agent interface. We use the pyMARL software library [1] and build on the algorithm implementations therein. To study heterogenous agents, we extend pyMARL and add agent-specific capabilities:\n\u2022 Sight range: circular field-of-view in pixels.\n\u2022 GPS: 2D (X,Y). Set to zeros when GPS is jammed.\n\u2022 Shoot range: in pixels.\nEach agent can execute movement actions (in four cardinal directions N, S, E, W by a fixed number of pixels) and attack actions (one action per enemy agent within shoot range). The state vector contains the following information for each agent:\n\u2022 Absolute position (in global coordinates).\n\u2022 Health, shield, weapon status, unit type.\nAmong these, the coordinates of agents require transformation between egocentric and global frames."}, {"title": "METRICS", "content": "We evaluate all these metrics on training and test scenarios. We evaluate if our method produces a COP that matches the ground truth in terms of the Mean Squared Error (MSE). We compute the MSE for (1) the predicted COP and the ground truth state (from the simulator), and (2) the predicted field-of-view of all agents and the ground truth field-of-view. For each time step, the MSE is averaged over the agents and episode length, and the goal is to achieve a normalized MSE < 5% over the friendly and enemy features (location, health, etc.).\nWe define a key metric for COP prediction called hallucination. Hallucination refers to the prediction that an agent has non-zero health in the COP when the agent has zero health (dead agent) in the ground truth (simulation). Hallucination is calculated as the average error in the predicted health over dead agents.\nFinally, we evaluate the success of the policy w.r.t. a clearly defined win condition. We report the average win rate (over 5000 episodes) for training and test scenarios."}, {"title": "SCENARIOS", "content": "Air-Ground Recon: This scenario tests the ability of a friendly aerial agent to track the movements of an enemy ground unit and communicate its position in a manner that friendly ground forces can decode. Upon decoding, friendly forces must move to attack the enemy before the time expires. In the Starcraft-2 (SC2) simulation, we used the scenario \"1010B-vs-1R\u201d introduced in [11]). It contains one friendly aerial unit, ten friendly ground units, and one enemy unit. The enemy must be attacked by all ten friendly units to be defeated. The win condition is to kill the enemy unit before the timer expires.\nWithdrawing Attack: In this scenario, the enemy force outnumbers the friendly force five to three. However, the friendly agents have a speed advantage while in retreat. The friendly units must jointly attack and withdraw in a coordinated fashion to evenly distribute the damage from enemy attacks across the friendly units by performing a \"kiting\" micromanagement strategy. Precise coordination in this scenario requires precise COPs. In SC2, we used the \"3S-vs-5Z\" scenario [1]. The win condition is to kill all enemy agents before the timer expires.\nTigerClaw: The TigerClaw melee map [13] is a high-level recreation of the TigerClaw combat scenario developed using the StarCraft-2 map editor. The scenario (\u201cTC_5B-vs-6R\u201d) was developed by Army subject-matter experts (SMEs) at the Captain's Career Course, Fort Moore, Georgia. The Blue Force is an Armored Task Force (TF), which consists of combat armor with M1A2 Abrams, mechanized infantry with Bradley Fighting Vehicles (BFV), mortar, armored recon cavalry with BFV, and combat aviation. The Red Force is a Battalion Tactical Group (BTG) with attached artillery battery and consists of mechanized infantry with BMP, mobile artillery, armored recon cavalry, combat aviation, anti-armor with anti-tank guided missiles (ATGM), and combat infantry. As seen in Figure 7, the terrain is challenging in this scenario because there are only four viable wadi crossing points. The win condition is to kill 80% of the red agents. The red force uses fixed behavior rules in all our scenarios."}, {"title": "TEST SCENARIOS", "content": "We evaluate our trained policies on modified laydowns (change in the initial positions of friendly and enemy) in all scenarios. These Out-of-Distribution (OOD) maps helps understand the generalization capability of our method.\nOOD maps for Air-Ground Recon (\"1010B-vs-1R\") and Withdrawing Attack (\"3S-vs-5Z\") are showing in Figure 8 and Figure 9 In TigerClaw, we change the blue force spawning region from defending all crossings to either the south wadi or further north,"}, {"title": "COMPARISON METHODS AND BASELINES", "content": "Our method of prediction of COPs is general and can be learned with a fixed policy, or jointly learned with any MARL method. In this paper, we integrated COP learning into the QMIX [4] MARL method. Future work can explore integration with more recent MARL methods. Note that QMIX does not use inter-agent communication. We built two strong baselines on top of QMIX to compare against.\n\u2022 QMIX w/ so: A version of QMIX leveraging the initial state knowledge. This is a strong baseline because it alleviates the issue of partial observability. In this baseline, each agent takes an additional input, i.e., the initial state vector, in addition to the agent observation input.\n\u2022 QMIX w/ Cross Attention: A baseline that incorporates inter-agent communication into QMIX. Each agent receives an additional input, a message containing the raw observations of all the agents. The agent architecture is modified to process these observations using Cross Attention.\nWe compare to recent multi-agent RL methods that learn the inter-agent communication function: (1) MASIA [14] predicts the state from other agents' observations within a QMIX method, (2) NDQ [11] and (3) TarMAC [15] are prior work on MARL where learned communication is not grounded on state prediction."}, {"title": "HYPERPARAMETERS", "content": "Training is performed end-to-end using the training objective in Eq. (4). We perform training in simulation for 20 million steps. We use the Adam optimizer with learning rate of 1e-3. Gradients are clipped at a norm of 20. Policy updates uses \u03bbtd = 0.3 and \u03b3 = 0.99. We set C = 32, (communication dimension) and hs = 64 (hidden dimension) with four cross-attention heads. By default, hallucination penalty \u03bbh = 3. For exploration, we use \u03b5-greedy exploration, annealing \u03b5 from 1 to 0.05 over a scenario-dependent number of steps, typically 105 steps, except in TigerClaw, we use a schedule of 5x105 steps."}, {"title": "RESULTS", "content": "First, we evaluate the model without GPS capability for any agent. We track the MSE of the COP against the ground truth state. As seen in Figure 11, in the \"3s-vs-5z\" scenario, the MSE is initially high (top panel), and with further training, the COP converges to low MSE (bottom panel), showing that the COP model produces highly accurate COPs.\nFigure 12 shows the convergence of COP in all three scenarios. We show the COP MSE for health and XY prediction separately to evaluate the effect of denied GPS. Note that XY prediction requires a change in the frame of reference. In all scenarios, the COP model learns to predict XY positions (< 0.05, bottom panel) more accurately than the health attribute (~0.1, top panel).\nIn 3S-vs-5Z, while the MSE is low, there is a noticeable increase in COP error over training (blue line), but not in other scenarios where the COP error decreases monotonically. The different dynamics reflect the differences between the scenarios. To succeed in the 3S-vs-5Z scenario, the policy needs to learn to evade, withdraw, and attack. The training seems to tradeoff COP errors and focus on policy learning. In the other two scenarios (green and orange lines), accurate prediction of enemy agents' locations is required to win. Hence, we see stable and accurate COP XY throughout training."}, {"title": "Human Interpretability", "content": "A key feature of our method is that the COPs are human-interpretable. Figure 13 shows a visualization of the state and corresponding predicted COP for all three scenarios.\nIn the example from the 3S-vs-5Z scenario (first row, left panel), three enemy agents are engaging one friendly unit on the far east side of the map. We see that the friendly can successfully communicate the positions of enemies (first row, right panel) to friendly agents on the far west side of the map. In the second example (middle row) from the 1010B-vs-1R scenario, we see that the position of the enemy (unit marked \u201cEO\u201d in the top left of the map) is accurately communicated by the aerial unit and represented with low uncertainty, meaning that the model can leverage heterogeneous agent capabilities. In the third example (third row) from the TigerClaw scenario, we observe that the model can predict friendly positions accurately. Still, the enemy positions are not accurate beyond the visual range.\nTo visualize the uncertainty in the COP, we averaged the predicted XY positions across all agents and showed axis-aligned ellipses with major and minor axes equal to the standard deviation in X and Y coordinates. A small ellipse means low uncertainty and high consensus among agents."}, {"title": "COP Improves Convergence Rate of Policy", "content": "We trained the agent policies jointly with the COP models. The training results are shown in Figure 14. As expected, QMIX is unable to solve the Air-Ground Recon problem (second row) as it does not use communication. The scenario timer expires before the enemy position is discovered. The baseline QMIX w/ Cross Attention directly uses the raw observations of all agents, it does not suffer from the partial observability issue. Similarly, QMIX w/ so has access to the initial state (including enemy position) which is highly informative in the 1010B-vs-1R scenario.\nIn comparison with methods that learn to communicate, our COP-based method outperforms TarMAC [15] , NDQ [11] , and MASIA [14] , in all three scenarios. In two out of three tested scenarios, our method is more than an order of magnitude faster to converge. Faster convergence or lower sample complexity reduces the time and effort required to produce such multi-agent policies for autonomous platforms. The results show that our method provides an inductive bias based on situational awareness that facilitates learning. In the next sections, policies are evaluated for robustness and resilience to variations between training and testing."}, {"title": "Resilience to Fog", "content": "In this section, we test the trained policies in scenarios where the agents' sight range is reduced. In the TigerClaw scenario, the agents use a sight range of 30 pixels for training, i.e., they can observe enemies up to 30 pixels away.\nWhen we reduce the sight range, we notice that the performance of QMIX and QMIX w/ so (these do not use communication) drops significantly (0.88/0.91 win rate down to 0.43/0.53 win rate). In comparison, our method is significantly more robust (0.9 at 30 pixels vs 0.64 at 10 pixels), meaning the intelligent inter-agent communication mitigates the degradation in the visual range. We also observe the performance of QMIX w/ Cross Attention at visual range of 20 pixels is lower than our method (0.74 vs 0.81 for our method). This difference is significantly higher in the 3S-vs-5Z scenario (0.11 vs 0.94 for our method). We see that our method is the most robust in all three scenarios with variations in sight range."}, {"title": "Resilience to Denied GPS", "content": "In this experiment, agents have GPS enabled during training and are not denied. We compare the performance to an upper bound of retraining without GPS. In the Air-Ground Recon scenario (1010B-vs-1R), we deny GPS for all ground units, and only the aerial unit has GPS. In the Withdrawing Attack scenario (3S-vs-5Z), we deny GPS for all agents except one selected at random. In the TigerClaw scenario, we deny GPS for all agents except the helicopter and scout platoons. We report the win rate degradation In the Air-Ground Recon scenario, we see a significant degradation (0.97 vs 0.68 win rate) compared to the upper bound win rate of 0.94. In the Withdrawing Attack scenario, there is degradation in win rate (0.93 down to 0.83 and 0.7). However, the performance matches the upper bound. That is, our method achieves the same performance without retraining the policies without GPS. Overall, in all three scenarios we see that our method can mitigate the degradation even if one of the agents has GPS (e.g., win rate 0.85 vs 0.73 in TigerClaw)."}, {"title": "Resilience to Change in Enemy Laydown", "content": "A potential vulnerability of MARL trained policies is when the initial state distribution is different to the training configuration. Specifically, a change in the initial enemy positions, aka laydown, can cause degradation in win rate.\nAmong methods that learn to communicate, our method based on COP outperforms prior methods TarMAC, NDQ, and MASIA. In the TigerClaw scenario, our method retains the win rate on test laydowns as well. The average win rate over scenarios and laydowns for our method is 0.837 vs the second-best 0.645 TarMAC."}, {"title": "MODEL ABLATIONS RESULTS", "content": "As mentioned in Section 3.4, hallucination of agents is an issue in our COP model. Four red agents are reported in the COP, including the artillery unit"}, {"title": "Hallucination", "content": "In general, we observed from the rollouts that dead agents are represented in the COP for a few time steps before they are reflected as dead. Even though this issue is reduced over training, there are still hallucinations observed in the trained model. Therefore, we introduced an explicit hallucination penalty as described in Section 3.4. We found that the increased weightage for health predictions significantly reduces the hallucinations as shown in Figure 16. We observed that the hallucination penalty \u03bbh = 3 significantly reduces the hallucination without reducing the win rate."}, {"title": "Number of rounds of communication", "content": "We use multiple communication rounds per simulation step to refine the COP further and achieve a consensus among agents. compares one vs three rounds of communication. The table shows that three rounds of communication achieve a higher win rate, especially in the 1010B-vs-1R scenario."}, {"title": "DISCUSSION", "content": "The experiments showed resilience to novel scenarios: GPS denial, communication denial (fog), and unknown enemy laydown. The experimental results show that the method produces precise COPs and highly resilient policies. We identified the issue of hallucination and introduced a training regularizer to control it.\nCurrently, most COP formation is performed manually in such challenging scenarios by collating communications at a C2 node. This process is too slow to produce the COP, a key data product for decision-making processes and autonomous policies that run on the platforms. The manual process is not scalable with the number of platforms and the amount of data to be processed. The paper addresses this barrier to future C2.\nAs shown in our experiments, this work also significantly advances multi-agent reinforcement learning (MARL). Existing MARL methods do not work well in our challenging scenarios. Among MARL methods that learn to communicate, existing methods suffer from a lack of grounding: it is unknowable what the swarm is communicating, whether it accurately captures an evolving scenario, and how a human operator can be brought into the loop. This can produce unintended or undesired behaviors in new scenarios.\nIn terms of future directions, multi-task training is a straightforward extension to make the COPs and policies even more resilient. We can randomly vary the scenario, sight range, communication range, laydown, capabilities, etc., during training. Future work should explore sparse communication (sparse in time). We did not vary the bandwidth requirement of communications in this paper and future work must explore this as well.\nMulti-agent exploration of an unknown dynamic battlefield (e.g., ISR) is challenging as the number of agents grows. A promising direction for future research is to use our COPs for exploration. The COP can help identify areas of high uncertainty, and methods similar to active sensing can be derived to explore such regions of the battlefield in a coordinated manner.\nA key unsolved modelling challenge is to support a variable number of agents. Future work can consider sequence-based models to allow messages of arbitrary length without dependence on the number of agents. In fact, the success of the cross-attention mechanism used in this paper indicates the likelihood of success of such models for COP formation.\nAnother key challenge is the comprehensive evaluation of COPs and policies. So far, we have manually explored the space of enemy laydowns to come up with challenging scenarios. Future work can explore scenario co-design methods (e.g., [16]) where challenging scenarios are created within the training loop.\nIn summary, this paper presents a data-driven method for common operational picture (COP) formation in a multi-agent system. The method works with general perceptions from heterogenous platforms in a GPS-denied environment. The COP is general, including but not limited to unit positions (vs. methods that only estimate positions, such as dead reckoning). The COP formation is fully autonomous, real-time, and human-interpretable."}]}