{"title": "Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework", "authors": ["Reza Averly", "Xia Ning"], "abstract": "Clinical named entity recognition (NER) aims to retrieve important entities within clinical narratives. Recent works have demonstrated that large language models (LLMs) can achieve strong performance in this task. While previous works focus on proprietary LLMs, we investigate how open NER LLMs, trained specifically for entity recognition, perform in clinical NER. In this paper, we aim to improve them through a novel framework, entity decomposition with filtering, or EDF. Our key idea is to decompose the entity recognition task into several retrievals of sub-entity types. We also introduce a filtering mechanism to remove incorrect entities. Our experimental results demonstrate the efficacy of our framework across all metrics, models, datasets, and entity types. Our analysis reveals that entity decomposition can recognize previously missed entities with substantial improvement. We further provide a comprehensive evaluation of our framework and an in-depth error analysis to pave future works.", "sections": [{"title": "1 Introduction", "content": "Clinical narratives hold immense value for clinical experts (Tayefi et al., 2021; Mahbub et al., 2022; Raghavan et al., 2014; Rannikm\u00e4e et al., 2021), largely due to their wealth of information often inaccessible in the structured data of electronic health records (EHR) (Mahbub et al., 2023; Goodman-Meza et al., 2022; Kharrazi et al., 2018; Rannikm\u00e4e et al., 2020; Hernandez-Boussard et al., 2019; Boag et al., 2018). Their free format, however, causes significant challenges for healthcare systems to utilize. The richness of information trapped within the narratives, followed by its significance, has spurred a plethora of works in tackling the clinical information extraction problem within the clinical NLP community (Wang et al., 2018; Landolsi et al., 2023).\nOne key building block in clinical information extraction is named entity recognition (NER), focusing on identifying clinical concepts within these narratives. As surveyed (Wang et al., 2018), prior studies rely on either traditional natural language processing (NLP) techniques or supervised learning methods. Nevertheless, the former approach can be fragile, while the latter requires significant effort to annotate. In addition, supervised methods cannot simply scale for the large number of concepts available in the clinical domain (Bodenreider, 2004).\nIn light of this, Large Language Models (LLMs), with their strong capabilities for zero and few-shot learning (Chowdhery et al., 2023; Brown et al., 2020; Thoppilan et al., 2022; Touvron et al., 2023), serve as promising solutions for clinical NER. While previous works focus on LLMs trained on general tasks (Agrawal et al., 2022; Liu et al., 2023; Gero et al., 2023), here we focus on LLMs specifically trained for entity recognition, or open NER LLMs (Zhou et al., 2023; Ding et al., 2024). Inspired by their results in clinical domain (Zhou et al., 2023), outperforming even proprietary LLMs (Brown et al., 2020), we conduct deeper investigations in this study. Surprisingly, our preliminary experiment (Section 5.1) suggests a stark performance gap between retrieving different clinical entities (Figure 2). For instance, an open NER LLM called UniversalNER (Zhou et al., 2023) performs significantly better at extracting medications rather than clinical treatments (85.88% vs 53.81% Exact Match F1-scores). Upon closer inspection, we find these unidentified treatment entities can be effectively recognized by exploiting simpler, albeit specific, entity types. For example, by explicitly specifying \"medication\" rather than \"treatment\u201d as input, the model can capture a substantial portion of the previously unidentified medication-related treatment entities.\nBuilding upon this insight, we present a novel"}, {"title": "framework, entity decomposition with filtering, or EDF, aimed at tackling clinical NER. To the best of our knowledge, we are the first to explore strategies to effectively use open NER LLMs in the clinical domain without using any samples. We draw inspiration from the divide-and-conquer paradigm (Knuth, 1998), which breaks down a complex problem into simpler sub-problems. Con- cretely, we posit that a direct retrieval of entities may be too complex for the model and instead propose to disentangle it into a series of retrievals through entity decomposition. Unlike the previous approach (Xie et al., 2023), entity decomposition breaks down the task by identifying through their sub-entity types, which, ideally, are easier to re- trieve. Nonetheless, entity decomposition alone is insufficient since some sub-entity types do not form strict subsets (further discussed in Section 3.2.3). To address this, we introduce a filtering mechanism in our framework to further improve performance. We illustrate them in Figure 1", "content": null}, {"title": "2 Related Work", "content": null}, {"title": "2.1 LLMs for Clinical NER", "content": "LLMs are promising for many clinical tasks (Singhal et al., 2022; Agrawal et al., 2022; Clusmann et al., 2023). Concurrently, several works aim to improve their performance on clinical NER. For instance, Agrawal et.al. (Agrawal et al., 2022) pro- poses a guided prompt design along with a resolver to handle the structured output space required by NER, while others (Hu et al., 2024, 2023; Liu et al., 2023) use prompt engineering. Outside the clinical domain, several works tackle NER either by framing it as a sequence labeling task (Wang et al., 2023), using label decomposition and syntac- tic augmentation (Xie et al., 2023), or improving the structured label space (Li et al., 2024), similar to Agrawal et.al. (Agrawal et al., 2022). Most of these works focus on LLMs trained in handling di- verse tasks such as ChatGPT (Brown et al., 2020).\nIn contrast, we focus on open NER LLMs (Zhou et al., 2023; Ding et al., 2024), which have two key differences. First, they are trained specifically for entity recognition tasks and do not require struc- tured output space handling (Agrawal et al., 2022; Li et al., 2024). Second, their instruction-tuning mostly focused on the diversity of entities rather than the instructions (e.g., keeping the prompt con- stant), which may limit the efficacy of prompt engi- neering techniques. Furthermore, unlike previous works (Hu et al., 2024, 2023; Liu et al., 2023), our work does not fall under prompt engineer- ing. Notably, prompt engineering is limited to prompt-based models, while our work is model- agnostic and, thus, is applicable to BERT-based models (Zaratiana et al., 2023)."}, {"title": "2.2 Task Decomposition in LLMs", "content": "The idea of task decomposition, solving complex tasks through solving its constituent simpler sub- tasks, can be dated back to (Lazarou et al., 1998). Previous works propose task decompositions for LLMs to tackle complex problems (Zhou et al., 2022; Xie et al., 2023). Concurrent with our work, Xie et.al. (Xie et al., 2023) suggests decomposing NER into a multi-turn dialogue, asking the model one question for each label. However, some open NER LLMs (Zhou et al., 2023) can only extract one label at a time, thus limiting the efficacy of Xie et.al. (Xie et al., 2023). Here, we propose to decom- pose NER on entity-level rather than label-level. Concretely, we can further decompose each label into simpler labels. Our method also complements Xie et.al. (Xie et al., 2023) since these decomposi- tions can be performed sequentially. Besides, our work aims to improve open NER LLMs, which have several key differences from other LLMs as briefly discussed in Section 2.1"}, {"title": "2.3 Open NER LLMs", "content": "Clinical narratives fall under domains with a large number of concepts and scarce annotations. Thus, developing open named entity recognition LLMs (Zhou et al., 2023; Ding et al., 2024) is timely and crucial research for clinical NER. De- spite the progress, existing works focus on training the backbone models. Furthermore, these models present a unique challenge and cannot be treated similarly to other LLMs (Section 2.1). Our work paves a way to adapt them for clinical domains without finetuning."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Problem Definition", "content": "Clinical narrative holds important entities about a patient's medical history. In this work, we aim to tackle clinical NER, focusing on extracting them. We frame the problem through the lens of a text- generative task. Let x be a clinical narrative, and further let t be the target entity type we want to extract. We aim to retrieve the target entities set Y"}, {"title": "3.2 ENTITY DECOMPOSITION WITH FILTERING", "content": "As introduced in Section 1, directly retrieving the target clinical entities may be too challenging, par- ticularly for models without domain-specific train- ing, such as open NER LLMs. In this work, we propose to break the task into multiple retrievals of sub-entities instead. We define sub-entities as subsets of the target clinical entities. Concretely, let \\u0178i be a set of sub-entities corresponding to the i-th sub-entity type si and let \\u0178 be our complete set, where ideally \\u2200i \\u2286 \\u0178. The first part of our frame- work, entity decomposer, aims to iteratively collect \\u0178i to produce \\u0178, or \\u22c3Ni=1\\u0178i = \\u0178. The last part of our framework, filter, involves removing disjoint sub-entity set from \\u0178. The filtered version Vf then serves as the final output. We provide more details of our framework in the following sections. Figure 1 presents the overall architecture of our frame- work EDF."}, {"title": "3.2.1 Entity Decomposer", "content": "To identify sub-entities \\u0178i, it is essential to define their types. Let D(t) = S be an entity decomposer module, aimed to produce a set of N sub-entity types S = {s1, s2, ..., sN} using the target en- tity type t. We note that these sub-entity types can either be manually curated (which may in- volve clinical experts) or obtained using existing tools such as a medical knowledge base (Bodenreider, 2004). Namely, clinical practitioners can resort to cost-effective approaches to construct D. To illustrate how the module works, here we take \"treatment\" as an example. In a pa- tient's discharge summary, \u201ctreatment\u201d constitutes a myriad of entities, including medications, med- ical procedures, and more (see Figure 1). In this case, D aims to decompose t = \u201ctreatment\u201d into S = {\"medication\u201d, \u201cmedical procedure\", ...}.\""}, {"title": "3.2.2 Open NER LLM", "content": "After defining the sub-entity types S, the next step is to retrieve all the sub-entities \\u0179. In this work, we leverage an open NER LLM. We base its for- mal definition to (Zhou et al., 2023). Let f be an open NER LLM tasked to retrieve the sub-entities \\u0178i of si from the clinical narrative x. That is, f(x, si) = \\u0178i. To construct the complete set \\u0178, the model would iteratively collect \\u0178i from each si. This may be of concern, given that it requires multiple iterations. To address this, we also con- sider other variants (Ding et al., 2024; Zaratiana et al., 2023) of open NER LLMs, which we for- mally define as f*(x, S) = \\u0178. That is, in contrast to f, these variants are capable of simultaneously extracting multiple entity types at once, thereby reducing the whole iterative sub-entity retrievals to only one forward pass.\nNext, we provide our reasons for using an open NER LLM in our framework. First, open NER LLMs are versatile in recognizing arbitrary enti- ties, which is vital given that sub-entities are not confined to specific entities. This stands in con- trast to traditional supervised NER models, which"}, {"title": "3.2.3 Filter", "content": "We discuss the last step of our framework here. Formally, let L(\\u0178, t, k) = \\u0178f be the filter module, where k denotes a context and \\u0178f \\u2286 \\u0178. In this framework, L aims to eliminate sub-entities within \\u0178 that do not fall under the target entity type t based on its context k. Similarly, L can also be viewed as a binary classifier, assigning a positive label if the entity corresponds to t, and a negative label otherwise. The filter module then aggregates and outputs the positively classified entities.\nHere, we discuss how some sub-entity types may not adhere to \\u0178i \\u2286 \\u0178. We formally define these atypical sub-entity types as s and their correspond- ing sub-entities as \\u0178, where \\u0178i \\ \\u0178 \\u2260 0. Con- cretely, some sub-entities in \\u0178i are disjoint to \\u0178, or they fall outside \\u0178. Take \u201ctreatment\u201d and \u201cmed- ical procedure\" for example. While \u201ctreatment\" includes \u201cmedical procedure", "medical pro- cedure": "ualify as a \u201ctreatment\u201d. For instance, in a clinical narrative, some medical procedures may serve purely for diagnosis rather than treatment. Thus, they do not fall under \"treatment\" and should be removed.\nNext, we provide the rationale behind context in the filter module. Unlike general domain entities, clinical entities can largely rely on context cues. To illustrate, consider \"adverse drug event (ADE)\u201d, a clinically significant entity as evidenced by its association with emergency visits and hospitaliza- tions (Zed et al., 2008; Lazarou et al., 1998). By definition,", "injury resulting from a medical intervention\" (Henry et al., 2020). In our framework, one of its sub-entity types may be ": "n- jury", "ADE": "it needs to be aware of other medical interventions and whether the in- jury stems from any of them. In other words, the filter requires the context in which the entity occurs to provide an accurate prediction. Moreover, our experiment suggests that while some clinical enti-"}, {"title": "4 Experimental Setup", "content": "We provide the experimental setup here and leave the details in the Appendix. All of the base models are available in huggingface*.\n4.1 Open NER LLMs\nWe take SOTA open NER LLMs in our experi- ment and further improve them. Per definition in Section 3.2.2, they may be categorized based on how many entity types can be extracted simulta- neously. To this end, we use UniversalNER (Zhou et al., 2023) and GNER (Ding et al., 2024) as the representative for f and f*, respectively. Con- cretely, we use UNIVERSALNER-TYPE-7B and GNER-LLAMA-7B. Both of them are finetuned on PileNER dataset (Zhou et al., 2023), which is generated from GPT 3.5 (Brown et al., 2020). In contrast to other LLMs, open NER LLMs are trained on the diversity of entity types rather than the prompts (Zhou et al., 2023). Thus, we only experiment with their default prompts.\n4.2 Entity Decomposers\nHere, we experiment with different techniques to decompose entities. First, given that clinical narra- tives require specialized knowledge, we consider sub-entity types curated by clinical experts. Specif- ically, we take the annotation guidelines avail- able from the datasets. Second, we use Chat- GPT (Brown et al., 2020) to decompose clinical entities automatically. We draw our inspiration from the recent success of ChatGPT in the clini- cal domain (Agrawal et al., 2022; Singhal et al., 2022). Furthermore, using ChatGPT for entity de- composition is more cost-effective and scalable. Third, we utilize the Unified Medical Language System (UMLS) (Bodenreider, 2004), a medical knowledge base, for retrieving sub-entity types. We provide more details in the Appendix A.\n4.3 Filters\nWe use Asclepius (Kweon et al., 2023) and LlaMA-2 (Touvron et al., 2023) trained on clin- ical and general domains, respectively. Specif- ically, we use ASCLEPIUS-LLAMA2-7B and LLAMA-2-CHAT-7B versions. Given the inher- ent generative nature of LLMs, we restrict their"}, {"title": "outputs to \"Yes/No\" responses (when applicable) using grammar-constrained decoding (Geng et al., 2023). This strategic constraint reduces the num- ber of generated tokens, resulting in increased in- ference speed. By default, we prompt the model by asking \"Can {entity} be considered as a/an {entity_type}?\". We try different prompts in Sec- tion 5.3.4. For entities that require context, we use a simple preprocessing so that the context provides sufficient information to extract the clinical entities.", "content": null}, {"title": "4.4 Datasets", "content": "We focus our experiment on extracting con- cepts from publicly available clinical notes. We use ClinicalIE (Agrawal et al., 2022), i2b2 2010 (Uzuner et al., 2011), i2b2 2012 (Sun et al., 2013), i2b2 2018 Task 2 (Henry et al., 2020) and CLEF 2014 (Mowery et al., 2014) datasets in this paper. They are available in Harvard DBMI for i2b2 datasets, PhysioNet\u2020 (Goldberger et al., 2000) for CLEF 2014 and huggingface for Clini- calIE. By default, all clinical entities in our experi- ment datasets do not require context except for the i2b2 2018 dataset. We provide further details in the Appendix \u0412."}, {"title": "4.5 Baselines and Metrics", "content": "Given the scarce methods to compare with, we use Xie et.al. (Xie et al., 2023) as our baseline. Concretely, we extract each entity type one at a time using UniversalNER and GNER. We also compare with UNIVERSALNER-ALL, trained with both PileNER and over 40 supervised datasets, in- cluding our experiment sets. We use Precision (P), Recall (R), and Exact Match F1-Score (F1) as eval- uation metrics, similar to previous works."}, {"title": "5 Experimental Results", "content": "Throughout this section, we abbreviate the entity types in the tables as follows to save space: Tr for treatment, Pr for problem, Te for test, CD for clinical department, DD for disease/disorder, AD for adverse drug, and ADE for adverse drug event.\n5.1 Preliminary Experiment\nWe conduct a preliminary experiment to confirm that open NER LLMs perform better at recognizing sub-entities rather than the target entity types. For target entities, we use the i2b2 2012 dataset, which"}, {"title": "contains decomposable entity types (i.e., entities can further be divided into sub-entities). For sub- entities, we use ClinicalIE, a medication extraction dataset.\nFigure 2 illustrates the result and confirms our hypothesis. That is, it is harder to recognize the target entity types (that are decomposable) com- pared to the sub-entity. For instance, we observe a stark difference between \"medication\" extraction and \"treatment\u201d extraction, where the former is a sub-entity of the other.", "content": null}, {"title": "5.2 Overall Performance", "content": "We present our results in Table 1 and Figure 3. For detailed numbers on precision and recalls, we leave them in the Appendix C.\nOn average, EDF outperforms baseline by 2.54% and 5.82% F1-score on UniversalNER and GNER, respectively. Interestingly, for some entity types, GNER performs similarly or even out- performs UniversalNER. This suggests that mod- els that can recognize multiple entities simultane- ously can benefit more from using our framework.\nEntity decomposition (ED) improves recall but decreases precision. As illustrated in Fig- ure 3, we observe a consistent improvement in re- call across diverse datasets and entity types for both models, suggesting that entity decomposition facilitates the identification of previously missing entities while being robust to the backbone mod- els. For precision, however, we notice a drop in performance. As discussed in Section 3.2.3, some sub-entities may not form a subset of the target entities, causing performance degradation on preci- sion. Further examination of the F1-score reveals a decline in overall performance, which justifies the necessity of incorporating a filtering mechanism.\nConversely, filtering (F) benefits precision but degrades recall. This shows contrasting results compared to entity decomposition across datasets, entity types, and models. The overall F1-score"}, {"title": "improvement on EDF compared to using each com- ponent individually suggests that entity decompo- sition and filtering complement each other. This emphasizes the necessity to incorporate both of them.\nEDF is robust to out-of-distribution entities compared to supervised training. We want to emphasize that our method does not require any training as opposed to the supervised approach. We use UniversalNER+EDF and UniversalNER-all as comparison. Despite the performance gap, we ob- serve that on entities not covered in the training label set (e.g., adverse drug or AD), EDF outper- forms by more than 10% on the F1-score. This shows the robustness of our method.", "content": null}, {"title": "5.3 Ablation Study", "content": "We perform additional experiments to test the ef- ficacy of our framework using different entity de- composers or filters. We focus on the overall per- formance or Fl-score. To reduce the cost of our experiments, we only experiment with the i2b2 2012 dataset, given their diversity in entity types. Unless otherwise specified, we use UniNER as the open NER LLM, annotation guideline for the entity decomposer module, and Asclepius for the filter.\n5.3.1 Entity Decomposers\nFirst, we conduct ablation study with different en- tity decomposers as described in Section 4.2 and present our results in Table 2. We do not experi- ment with ChatGPT and UMLS for the \"clinical department\" entity type since (1) ChatGPT is un- able to produce reasonable sub-entity types and (2) we find there are no correspondence semantic types in UMLS.\nEDF is robust to entity decomposer module. We show that a general LLM and an existing med- ical knowledge base may serve as alternatives to clinical experts, as indicated by their competitive performance (e.g. 58.25% vs 58.09% between an- notation and ChatGPT on treatment, respectively) in Table 2. This circumvents the necessity of hav- ing clinical experts to curate sub-entity types.\nWithout filters, UMLS outperforms other en- tity decomposers. Most sub-entity types in UMLS form subsets to the target entities; hence, a filter may not be necessary. Interestingly, for some enti-"}, {"title": "5.3.2 Filter Models", "content": "Here, we investigate how different filter models affect the overall performance of our framework. Specifically, we compare domain-specific and gen- eral LLMs. We present the results in Table 3\nClinical model is better at recognizing entities requiring clinical expertise. Specifically, we ob- serve that it is superior to a general domain model in \u201ctreatment\u201d, \u201cproblem\u201d, and \u201ctest\u201d entities. For \"clinical department\u201d, however, they perform simi- larly. This is unsurprising since the former entities are heavily involved with entities requiring clinical- specific knowledge, while the latter is splintered"}, {"title": "with entities that often appear in the general do- main, such as hospitals.", "content": null}, {"title": "5.3.3 Filter Context", "content": "As discussed in Section 3.2.3, some clinical en- tities necessitate a context. Here, we investigate whether context helps for entities not requiring it. We compare the performance between filtering (1) without context, (2) including the sentence the en- tity appears in, and (3) providing the whole clinical narrative or document. We show the results in Ta- ble 4.\nContext may improve or hurt performance. Overall, we observe mixed results across different entity types, with and without an entity decom- poser. For instance, we observe slight improve- ments for \"treatment\" and \"test\" entities. On the other hand, the performance of the \"problem\" en- tity consistently drops the more context we provide. We provide further analysis in Section 5.4.2."}, {"title": "5.3.4 Filter Prompts", "content": "LLMs are often brittle to prompting strategies (Zhu et al., 2023). Rather than constructing different tem- plates, we experiment with how incorporating the entity description into the filter affects our frame- work's performance. For \u201ctreatment\u201d, \u201cproblem\u201d"}, {"title": "5.4 Error Analysis", "content": null}, {"title": "5.4.1 Entity Decomposition Missing Entities", "content": "Despite the significant improvement in recall through entity decomposition, some entities remain unrecognizable. Thus, we analyze the potential sources of these errors. First, we calculate the percentage of entities fully absent from the predic- tions. To illustrate, if the label is \u201chis aspirin\u201d and the prediction is \u201caspirin\u201d, we do not deem it fully absent since the prediction partially captures the label. Figure 4 illustrates the percentage of fully absent entities for each entity type in the dataset.\nEntity decomposition significantly reduces the number of fully absent entities. For instance, only 5.5% entities are fully absent for \u201ctreatment\u201d in the i2b2 2010 dataset after entity decomposition. We observe improvements across all entity types.\nThe majority of fully absent entities are abbre- viations and homonyms. For example, open NER LLM cannot capture \"CVA\", an abbreviation for \"cerebral vascular accident\", after entity decomposi- tion. Another example is \u201cdelivery", "childbirth\" in the clinical narrative). Furthermore, certain entities such as\"\n    },\n    {\n      \"title\": \"5.4.2 Performance Drop using Context\",\n      \"content\": \"Section 5.3.3 shows that in contrast to other entity types, there is a notable performance degradation for \\\"problem\\\" when context is provided. To in- vestigate, we observe the ground truth \\\"problem\\\" entities that are removed by the filter. Interestingly, we find that for most of them, the context specif- ically stated that the patient does not experience\"\n    },\n    {\n      \"title\": \"these problems. We then conduct further investi- gation on their polarity attribute, which contains information on whether the patient is experiencing medical problems (or taking certain medications, for instance). To clarify, if there are explicit men- tions that a patient does not have certain medical problems, they would be indicated as negatives. Otherwise, it is positive. We conduct an analysis of how entity polarity affects filter responses. We plot our results in Figure 5.\nThe \\\"negative\\\" polarity degrades perfor- mance. First, the dataset statistics in Table 6 show that almost a fifth of \u201cproblem": "ntities are \u201cnega- tive", "negative": "olarity causes the performance drop on the \"problem\" entity, as revealed by how a ma- jority of the rejected gold \"problem\" entities are \"negative\"."}, {"content": null}, {"title": "5.5 More Results", "content": "Given the space limit, we include more re- sults in the Appendix, including few-shot (Ap- pendix E), performance on a BERT-based model (Appendix F), and error analysis on CD perfor- mance drop (Appendix G)."}, {"title": "6 Conclusion and Future Work", "content": "In this work, we propose a novel EDF framework to tackle the clinical named entity recognition (NER) task. Our comprehensive experiments demonstrate the strength of our framework across different di- mensions. We also thoroughly investigate each framework component and provide several key in- sights. In future works, we hope to explore how to address the limitations of our work described in Section 7."}, {"title": "7 Limitations", "content": "First, we restrict our work to clinical narratives and have yet to explore how our framework gen- eralizes to other texts. In this work, we deliber- ately focus on how well the method generalizes to different datasets, which (1) tackle different and clinically significant (Lehman et al., 2022) entity types, (2) are collected from different institutions (thus different distributions), (3) are de-identified in different ways (e.g., masks used for the patient's sensitive information), (4) used different formats (e.g., header names and section organizations), etc. In fact, each patient is a unique case, and each of them can be treated as a separate domain (Yang et al., 2023). Thus, generalizing to these datasets is already a significant challenge. However, testing how well our framework generalizes beyond clini- cal narratives would be an interesting avenue. Note that our motivation for this framework is that we found some clinical entities are easier to identify through simpler terms. This is particularly true for clinical narratives since most entities that are of interest (Lehman et al., 2022) follow this assump- tion. Thus, we designed our framework based on the characteristics of entities inside clinical narra- tives, not the narratives themselves. This is the reason we hypothesize that our framework may work outside clinical narratives (with similar entity characteristics). We leave this to future work.\nSecond, we restrict our work to only open- sourced models and leave experiments on propri- etary models to future works. Most publicly avail- able clinical narratives are under restrictive licenses. Hence, we cannot simply use commercial models. Furthermore, using commercial models on clinical narratives requires de-identification (removing sen- sitive information), which is a significant process in itself (Johnson et al., 2016). In contrast, open- sourced models have more practical values (e.g., they can be deployed in the hospital's internal sys- tem without de-identification). In this work, we de- liberately use strong open-sourced models such as UniversalNER (Zhou et al., 2023), which performs better than ChatGPT (Brown et al., 2020). How- ever, how open-sourced models fare with other pro- prietary models on clinical NER is still unknown. We leave them to future works.\nThird, our work falls under the healthcare do- main, which is a high-stakes setting. Despite the reasonable performance, there is still a long way to reach the high requirements set by healthcare"}, {"title": "applications. Nevertheless, our work paves a poten- tial solution for the zero-shot clinical named entity recognition task.", "content": null}, {"title": "8 Ethic Statement", "content": "Our research is conducted on open, retrospective clinical datasets without human subject interven- tion and thus will not harm human subjects. Fur- thermore, the clinical domain is complex and re- quires evaluation beyond performance, particularly regarding safety and bias. Unfortunately, the clin- ical narratives in our datasets are not associated with specific patients, impeding such evaluations. Further evaluations and validations from clinical experts will be needed to translate research into the clinical decision-making process."}, {"title": "9 Acknowledgement", "content": "This publication was supported, in part, by the Na- tional Center for Advancing Translational Sciences of the National Institutes of Health under Grant Number UM1TR004548. The content is solely the responsibility of the authors and does not neces- sarily represent the official views of the National Institutes of Health. The computation resources enabling this project are provided by the Ohio Su- percomputing Center. The authors would like to thank the colleagues from the OSU NingLab for their constructive feedback."}, {"title": "I.1 UniNER", "content": "A virtual assistant answers questions from a user based on the provided text.\nUSER: Text: {input}\nASSISTANT: I've read this text.\nUSER: {instruction}\nASSISTANT:"}, {"title": "I.2 GNER", "content": "[INST] Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis. Output format is: word_1(label_1), word_2(label_2),\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. 0 (Outside) denotes words that are not part of a named entity.\n{instruction}\nSentence: {input} [/INST]"}, {"title": "I.3 Asclepius", "content": "You are an intelligent clinical languge model.\nBelow is a snippet of patient's discharge summary and a following instruction from healthcare professional.\nWrite a response that appropriately completes the instruction.\nThe response should provide the accurate answer to the instruction, while being concise.\n[Discharge Summary Begin]\n{input}\n[Discharge Summary End]\n[Instruction Begin]\n{instruction}\n[Instruction End]"}, {"title": "I.4 Llama2", "content": ""}]}