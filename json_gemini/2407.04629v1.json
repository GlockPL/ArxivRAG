{"title": "Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework", "authors": ["Reza Averly", "Xia Ning"], "abstract": "Clinical named entity recognition (NER) aims to retrieve important entities within clinical narratives. Recent works have demonstrated that large language models (LLMs) can achieve strong performance in this task. While previous works focus on proprietary LLMs, we investigate how open NER LLMs, trained specifically for entity recognition, perform in clinical NER. In this paper, we aim to improve them through a novel framework, entity decomposition with filtering, or EDF. Our key idea is to decompose the entity recognition task into several retrievals of sub-entity types. We also introduce a filtering mechanism to remove incorrect entities. Our experimental results demonstrate the efficacy of our framework across all metrics, models, datasets, and entity types. Our analysis reveals that entity decomposition can recognize previously missed entities with substantial improvement. We further provide a comprehensive evaluation of our framework and an in-depth error analysis to pave future works.", "sections": [{"title": "Introduction", "content": "Clinical narratives hold immense value for clinical experts (Tayefi et al., 2021; Mahbub et al., 2022; Raghavan et al., 2014; Rannikm\u00e4e et al., 2021), largely due to their wealth of information often in-accessible in the structured data of electronic health records (EHR) (Mahbub et al., 2023; Goodman-Meza et al., 2022; Kharrazi et al., 2018; Rannikm\u00e4e et al., 2020; Hernandez-Boussard et al., 2019; Boag et al., 2018). Their free format, however, causes significant challenges for healthcare systems to utilize. The richness of information trapped within the narratives, followed by its significance, has spurred a plethora of works in tackling the clinical information extraction problem within the clinical NLP community (Wang et al., 2018; Landolsi et al., 2023).\nOne key building block in clinical information extraction is named entity recognition (NER), focusing on identifying clinical concepts within these narratives. As surveyed (Wang et al., 2018), prior studies rely on either traditional natural language processing (NLP) techniques or supervised learning methods. Nevertheless, the former approach can be fragile, while the latter requires significant effort to annotate. In addition, supervised methods cannot simply scale for the large number of concepts available in the clinical domain (Bodenreider, 2004).\nIn light of this, Large Language Models (LLMs), with their strong capabilities for zero and few-shot learning (Chowdhery et al., 2023; Brown et al., 2020; Thoppilan et al., 2022; Touvron et al., 2023), serve as promising solutions for clinical NER. While previous works focus on LLMs trained on general tasks (Agrawal et al., 2022; Liu et al., 2023; Gero et al., 2023), here we focus on LLMs specifically trained for entity recognition, or open NER LLMs (Zhou et al., 2023; Ding et al., 2024). Inspired by their results in clinical domain (Zhou et al., 2023), outperforming even proprietary LLMs (Brown et al., 2020), we conduct deeper investigations in this study. Surprisingly, our preliminary experiment (Section 5.1) suggests a stark performance gap between retrieving different clinical entities (Figure 2). For instance, an open NER LLM called UniversalNER (Zhou et al., 2023) performs significantly better at extracting medications rather than clinical treatments (85.88% vs 53.81% Exact Match F1-scores). Upon closer inspection, we find these unidentified treatment entities can be effectively recognized by exploiting simpler, albeit specific, entity types. For example, by explicitly specifying \"medication\" rather than \"treatment\u201d as input, the model can capture a substantial portion of the previously unidentified medication-related treatment entities.\nBuilding upon this insight, we present a novel"}, {"title": "Related Work", "content": "framework, entity decomposition with filtering, or EDF, aimed at tackling clinical NER. To the best of our knowledge, we are the first to explore strategies to effectively use open NER LLMs in the clinical domain without using any samples. We draw inspiration from the divide-and-conquer paradigm (Knuth, 1998), which breaks down a complex problem into simpler sub-problems. Con-cretely, we posit that a direct retrieval of entities may be too complex for the model and instead propose to disentangle it into a series of retrievals through entity decomposition. Unlike the previous approach (Xie et al., 2023), entity decomposition breaks down the task by identifying through their sub-entity types, which, ideally, are easier to re-trieve. Nonetheless, entity decomposition alone is insufficient since some sub-entity types do not form strict subsets (further discussed in Section 3.2.3). To address this, we introduce a filtering mechanism in our framework to further improve performance. We illustrate them in Figure 1\n2 Related Work\n2.1 LLMs for Clinical NER\nLLMs are promising for many clinical tasks (Singhal et al., 2022; Agrawal et al., 2022; Clusmann et al., 2023). Concurrently, several works aim to improve their performance on clinical NER. For instance, Agrawal et.al. (Agrawal et al., 2022) proposes a guided prompt design along with a resolver to handle the structured output space required by NER, while others (Hu et al., 2024, 2023; Liu et al., 2023) use prompt engineering. Outside the clinical domain, several works tackle NER either by framing it as a sequence labeling task (Wang et al., 2023), using label decomposition and syntactic augmentation (Xie et al., 2023), or improving the structured label space (Li et al., 2024), similar to Agrawal et.al. (Agrawal et al., 2022). Most of these works focus on LLMs trained in handling diverse tasks such as ChatGPT (Brown et al., 2020).\nIn contrast, we focus on open NER LLMs (Zhou et al., 2023; Ding et al., 2024), which have two key differences. First, they are trained specifically for entity recognition tasks and do not require structured output space handling (Agrawal et al., 2022; Li et al., 2024). Second, their instruction-tuning mostly focused on the diversity of entities rather than the instructions (e.g., keeping the prompt constant), which may limit the efficacy of prompt engineering techniques. Furthermore, unlike previous works (Hu et al., 2024, 2023; Liu et al., 2023), our work does not fall under prompt engineer-ing. Notably, prompt engineering is limited to prompt-based models, while our work is model-agnostic and, thus, is applicable to BERT-based models (Zaratiana et al., 2023).\n2.2 Task Decomposition in LLMs\nThe idea of task decomposition, solving complex tasks through solving its constituent simpler sub-tasks, can be dated back to (Lazarou et al., 1998). Previous works propose task decompositions for LLMs to tackle complex problems (Zhou et al., 2022; Xie et al., 2023). Concurrent with our work, Xie et.al. (Xie et al., 2023) suggests decomposing NER into a multi-turn dialogue, asking the model one question for each label. However, some open NER LLMs (Zhou et al., 2023) can only extract one label at a time, thus limiting the efficacy of Xie et.al. (Xie et al., 2023). Here, we propose to decompose NER on entity-level rather than label-level. Concretely, we can further decompose each label into simpler labels. Our method also complements Xie et.al. (Xie et al., 2023) since these decompositions can be performed sequentially. Besides, our work aims to improve open NER LLMs, which have several key differences from other LLMs as briefly discussed in Section 2.1\n2.3 Open NER LLMs\nClinical narratives fall under domains with a large number of concepts and scarce annotations. Thus, developing open named entity recognition LLMs (Zhou et al., 2023; Ding et al., 2024) is timely and crucial research for clinical NER. Despite the progress, existing works focus on training the backbone models. Furthermore, these models present a unique challenge and cannot be treated similarly to other LLMs (Section 2.1). Our work paves a way to adapt them for clinical domains without finetuning."}, {"title": "Methodology", "content": "3 Methodology\n3.1 Problem Definition\nClinical narrative holds important entities about a patient's medical history. In this work, we aim to tackle clinical NER, focusing on extracting them. We frame the problem through the lens of a text-generative task. Let x be a clinical narrative, and further let t be the target entity type we want to extract. We aim to retrieve the target entities set Y"}, {"title": "ENTITY DECOMPOSITION WITH FILTERING", "content": "corresponding to t from x. To illustrate, if x is a patient's discharge summary and t = \"medication\", then the goal is to extract medication entities in the discharge summary. In this case, the ouput can be Y = {\"aspirin\u201d, \u201cmethanol\u201d, ...}.\n3.2 ENTITY DECOMPOSITION WITH FILTERING\nAs introduced in Section 1, directly retrieving the target clinical entities may be too challenging, particularly for models without domain-specific training, such as open NER LLMs. In this work, we propose to break the task into multiple retrievals of sub-entities instead. We define sub-entities as subsets of the target clinical entities. Concretely, let \\( \\hat{Y}_i \\) be a set of sub-entities corresponding to the i-th sub-entity type \\( s_i \\) and let \\( \\hat{Y} \\) be our complete set, where ideally \\( \\hat{Y}_i \\subseteq \\hat{Y} \\). The first part of our framework, entity decomposer, aims to iteratively collect \\( \\hat{Y}_i \\) to produce \\( \\hat{Y} \\), or \\( \\bigcup_{i=1}^N \\hat{Y}_i = \\hat{Y} \\). The last part of our framework, filter, involves removing disjoint sub-entity set from \\( \\hat{Y} \\). The filtered version \\( \\hat{Y}_f \\) then serves as the final output. We provide more details of our framework in the following sections.\n3.2.1 Entity Decomposer\nTo identify sub-entities \\( \\hat{Y}_i \\), it is essential to define their types. Let \\( D(t) = S \\) be an entity decomposer module, aimed to produce a set of N sub-entity types \\( S = \\{s_1, s_2, ..., s_N\\} \\) using the target entity type t. We note that these sub-entity types can either be manually curated (which may involve clinical experts) or obtained using existing"}, {"title": "Experimental Setup", "content": "tools such as a medical knowledge base (Bodenreider, 2004). Namely, clinical practitioners can resort to cost-effective approaches to construct D. To illustrate how the module works, here we take \"treatment\" as an example. In a patient's discharge summary, \u201ctreatment\u201d constitutes a myriad of entities, including medications, medical procedures, and more (see Figure 1). In this case, D aims to decompose t = \u201ctreatment\u201d into S = {\"medication\u201d, \u201cmedical procedure\", ...}.\n3.2.2 Open NER LLM\nAfter defining the sub-entity types S, the next step is to retrieve all the sub-entities \\( \\hat{Y}_i \\). In this work, we leverage an open NER LLM. We base its formal definition to (Zhou et al., 2023). Let f be an open NER LLM tasked to retrieve the sub-entities \\( \\hat{Y}_i \\) of \\( s_i \\) from the clinical narrative x. That is, \\( f(x, s_i) = \\hat{Y}_i \\). To construct the complete set \\( \\hat{Y} \\), the model would iteratively collect \\( \\hat{Y}_i \\) from each \\( s_i \\). This may be of concern, given that it requires multiple iterations. To address this, we also consider other variants (Ding et al., 2024; Zaratiana et al., 2023) of open NER LLMs, which we formally define as \\( f*(x, S) = \\hat{Y} \\). That is, in contrast to f, these variants are capable of simultaneously extracting multiple entity types at once, thereby reducing the whole iterative sub-entity retrievals to only one forward pass.\nNext, we provide our reasons for using an open NER LLM in our framework. First, open NER LLMs are versatile in recognizing arbitrary entities, which is vital given that sub-entities are not confined to specific entities. This stands in contrast to traditional supervised NER models, which"}, {"title": "Filter", "content": "are bounded by their predefined label sets. Second, in contrast to other LLMs, open NER LLMs are trained specifically for NER tasks, substantially reducing the effort of mapping LLM generative output to the structured output space of NER (Agrawal et al., 2022). In addition, our preliminary experiment in Section 5.1 suggests that open NER LLMs perform surprisingly well on recognizing basic clinical entities such as medications, making them strong candidates as our sub-entities retriever.\n3.2.3 Filter\nWe discuss the last step of our framework here. Formally, let \\( L(\\hat{Y}, t, k) = \\hat{Y}_f \\) be the filter module, where k denotes a context and \\( \\hat{Y}_f \\subseteq \\hat{Y} \\). In this framework, L aims to eliminate sub-entities within \\( \\hat{Y} \\) that do not fall under the target entity type t based on its context k. Similarly, L can also be viewed as a binary classifier, assigning a positive label if the entity corresponds to t, and a negative label otherwise. The filter module then aggregates and outputs the positively classified entities.\nHere, we discuss how some sub-entity types may not adhere to \\( \\hat{Y}_i \\subseteq \\hat{Y} \\). We formally define these atypical sub-entity types as \\( s_i \\) and their corresponding sub-entities as \\( \\hat{Y}_i \\), where \\( \\hat{Y} \\setminus \\hat{Y}_i \\neq 0 \\). Concretely, some sub-entities in \\( \\hat{Y}_i \\) are disjoint to \\( \\hat{Y} \\), or they fall outside \\( \\hat{Y} \\). Take \u201ctreatment\u201d and \u201cmedical procedure\" for example. While \u201ctreatment\u201d includes \u201cmedical procedure\", not all \"medical procedure\" qualify as a \u201ctreatment\u201d. For instance, in a clinical narrative, some medical procedures may serve purely for diagnosis rather than treatment. Thus, they do not fall under \"treatment\" and should be removed.\nNext, we provide the rationale behind context in the filter module. Unlike general domain entities, clinical entities can largely rely on context cues. To illustrate, consider \"adverse drug event (ADE)\u201d, a clinically significant entity as evidenced by its association with emergency visits and hospitalizations (Zed et al., 2008; Lazarou et al., 1998). By definition, \"ADE\u201d is an \"injury resulting from a medical intervention\" (Henry et al., 2020). In our framework, one of its sub-entity types may be \"injury\". However, for a filter to dicatate whether an injury corresponds to \u201cADE\u201d, it needs to be aware of other medical interventions and whether the injury stems from any of them. In other words, the filter requires the context in which the entity occurs to provide an accurate prediction. Moreover, our experiment suggests that while some clinical enti-\""}, {"title": "Experimental Setup", "content": "ties do not necessitate contextual information, they can derive benefit from it (see Section 5.3.3).\n4 Experimental Setup\nWe provide the experimental setup here and leave the details in the Appendix. All of the base models are available in huggingface*.\n4.1 Open NER LLMS\nWe take SOTA open NER LLMs in our experiment and further improve them. Per definition in Section 3.2.2, they may be categorized based on how many entity types can be extracted simultaneously. To this end, we use UniversalNER (Zhou et al., 2023) and GNER (Ding et al., 2024) as the representative for f and f*, respectively. Concretely, we use UNIVERSALNER-TYPE-7B and GNER-LLAMA-7B. Both of them are finetuned on PileNER dataset (Zhou et al., 2023), which is generated from GPT 3.5 (Brown et al., 2020). In contrast to other LLMs, open NER LLMs are trained on the diversity of entity types rather than the prompts (Zhou et al., 2023). Thus, we only experiment with their default prompts.\n4.2 Entity Decomposers\nHere, we experiment with different techniques to decompose entities. First, given that clinical narratives require specialized knowledge, we consider sub-entity types curated by clinical experts. Specifically, we take the annotation guidelines available from the datasets. Second, we use ChatGPT (Brown et al., 2020) to decompose clinical entities automatically. We draw our inspiration from the recent success of ChatGPT in the clinical domain (Agrawal et al., 2022; Singhal et al., 2022). Furthermore, using ChatGPT for entity decomposition is more cost-effective and scalable. Third, we utilize the Unified Medical Language System (UMLS) (Bodenreider, 2004), a medical knowledge base, for retrieving sub-entity types. We provide more details in the Appendix A.\n4.3 Filters\nWe use Asclepius (Kweon et al., 2023) and LlaMA-2 (Touvron et al., 2023) trained on clinical and general domains, respectively. Specifically, we use ASCLEPIUS-LLAMA2-7B and LLAMA-2-CHAT-7B versions. Given the inherent generative nature of LLMs, we restrict their"}, {"title": "Ablation Study", "content": "outputs to \"Yes/No\" responses (when applicable) using grammar-constrained decoding (Geng et al., 2023). This strategic constraint reduces the number of generated tokens, resulting in increased inference speed. By default, we prompt the model by asking \"Can {entity} be considered as a/an {entity_type}?\". We try different prompts in Section 5.3.4. For entities that require context, we use a simple preprocessing so that the context provides sufficient information to extract the clinical entities.\n4.4 Datasets\nWe focus our experiment on extracting concepts from publicly available clinical notes. We use ClinicalIE (Agrawal et al., 2022), i2b2 2010 (Uzuner et al., 2011), i2b2 2012 (Sun et al., 2013), i2b2 2018 Task 2 (Henry et al., 2020) and CLEF 2014 (Mowery et al., 2014) datasets in this paper. They are available in Harvard DBMI for i2b2 datasets, PhysioNet\u2020 (Goldberger et al., 2000) for CLEF 2014 and huggingface for ClinicalIE. By default, all clinical entities in our experiment datasets do not require context except for the i2b2 2018 dataset. We provide further details in the Appendix \u0412.\n4.5 Baselines and Metrics\nGiven the scarce methods to compare with, we use Xie et.al. (Xie et al., 2023) as our baseline. Concretely, we extract each entity type one at a time using UniversalNER and GNER. We also compare with UNIVERSALNER-ALL, trained with both PileNER and over 40 supervised datasets, including our experiment sets. We use Precision (P), Recall (R), and Exact Match F1-Score (F1) as evaluation metrics, similar to previous works.\n5 Experimental Results\nThroughout this section, we abbreviate the entity types in the tables as follows to save space: Tr for treatment, Pr for problem, Te for test, CD for clinical department, DD for disease/disorder, AD for adverse drug, and ADE for adverse drug event.\n5.1 Preliminary Experiment\nWe conduct a preliminary experiment to confirm that open NER LLMs perform better at recognizing sub-entities rather than the target entity types. For target entities, we use the i2b2 2012 dataset, which"}, {"title": "Ablation Study", "content": "contains decomposable entity types (i.e., entities can further be divided into sub-entities). For sub-entities, we use ClinicalIE, a medication extraction dataset.\nFigure 2 illustrates the result and confirms our hypothesis. That is, it is harder to recognize the target entity types (that are decomposable) compared to the sub-entity. For instance, we observe a stark difference between \"medication\" extraction and \"treatment\u201d extraction, where the former is a sub-entity of the other.\n5.2 Overall Performance\nWe present our results in Table 1 and Figure 3. For detailed numbers on precision and recalls, we leave them in the Appendix C.\nOn average, EDF outperforms baseline by 2.54% and 5.82% F1-score on UniversalNER and GNER, respectively. Interestingly, for some entity types, GNER performs similarly or even outperforms UniversalNER. This suggests that models that can recognize multiple entities simultaneously can benefit more from using our framework.\nEntity decomposition (ED) improves recall but decreases precision. As illustrated in Figure 3, we observe a consistent improvement in recall across diverse datasets and entity types for both models, suggesting that entity decomposition facilitates the identification of previously missing entities while being robust to the backbone models. For precision, however, we notice a drop in performance. As discussed in Section 3.2.3, some sub-entities may not form a subset of the target entities, causing performance degradation on precision. Further examination of the F1-score reveals a decline in overall performance, which justifies the necessity of incorporating a filtering mechanism.\nConversely, filtering (F) benefits precision but degrades recall. This shows contrasting results compared to entity decomposition across datasets, entity types, and models. The overall F1-score"}, {"title": "Statement", "content": "Table 1: F1-score performance (%) comparison between baseline (B) from Xie et.al. (Xie et al., 2023), Entity Decomposer (ED) only, Filter (F) only, supervised (UniNER-all) and EDF (Ours) methods across datasets, entity types, and models. We use Asclepius and the default prompt strategy (Section 3.2.3) for the filter and annotation guideline for the entity decomposer. We mark the best results in bold and second-best in underlined. Discussion in Section 5.2\nimprovement on EDF compared to using each component individually suggests that entity decomposition and filtering complement each other. This emphasizes the necessity to incorporate both of them.\nEDF is robust to out-of-distribution entities compared to supervised training. We want to emphasize that our method does not require any training as opposed to the supervised approach. We use UniversalNER+EDF and UniversalNER-all as comparison. Despite the performance gap, we observe that on entities not covered in the training label set (e.g., adverse drug or AD), EDF outperforms by more than 10% on the F1-score. This shows the robustness of our method.\n5.3 Ablation Study\nWe perform additional experiments to test the efficacy of our framework using different entity decomposers or filters. We focus on the overall performance or Fl-score. To reduce the cost of our experiments, we only experiment with the i2b2 2012 dataset, given their diversity in entity types. Unless otherwise specified, we use UniNER as the open NER LLM, annotation guideline for the entity decomposer module, and Asclepius for the filter.\n5.3.1 Entity Decomposers\nFirst, we conduct ablation study with different entity decomposers as described in Section 4.2 and present our results in Table 2. We do not experiment with ChatGPT and UMLS for the \"clinical department\" entity type since (1) ChatGPT is unable to produce reasonable sub-entity types and (2) we find there are no correspondence semantic types in UMLS.\nEDF is robust to entity decomposer module. We show that a general LLM and an existing medical knowledge base may serve as alternatives to clinical experts, as indicated by their competitive performance (e.g. 58.25% vs 58.09% between annotation and ChatGPT on treatment, respectively) in Table 2. This circumvents the necessity of having clinical experts to curate sub-entity types.\nWithout filters, UMLS outperforms other entity decomposers. Most sub-entity types in UMLS form subsets to the target entities; hence, a filter may not be necessary. Interestingly, for some enti-"}, {"title": "LLM Prompt Templates", "content": "Table 2: Ablation Study on Entity Decomposer.\nTable 4: Ablation Study on Filter Context.\nTable 3: Ablation Study on Filter Model.\nties, it performs better than vanilla prompt. This is significant since even without filtering, our framework, specifically entity decomposition, can still outperform the vanilla approach with proper curation of sub-entity types (i.e., making sure that the sub-entity types do not have disjointed sub-entities). We remark that this does not underscore the value of a filter module since UMLS can benefit more from integrating it, as shown in Table 2.\n5.3.2 Filter Models\nHere, we investigate how different filter models affect the overall performance of our framework. Specifically, we compare domain-specific and general LLMs. We present the results in Table 3\nClinical model is better at recognizing entities requiring clinical expertise. Specifically, we observe that it is superior to a general domain model in \u201ctreatment\u201d, \u201cproblem\u201d, and \u201ctest\u201d entities. For \"clinical department\u201d, however, they perform similarly. This is unsurprising since the former entities are heavily involved with entities requiring clinical-specific knowledge, while the latter is splintered\nwith entities that often appear in the general domain, such as hospitals.\n5.3.3 Filter Context\nAs discussed in Section 3.2.3, some clinical entities necessitate a context. Here, we investigate whether context helps for entities not requiring it. We compare the performance between filtering (1) without context, (2) including the sentence the entity appears in, and (3) providing the whole clinical narrative or document. We show the results in Table 4.\nContext may improve or hurt performance. Overall, we observe mixed results across different entity types, with and without an entity decom-poser. For instance, we observe slight improvements for \"treatment\" and \"test\" entities. On the other hand, the performance of the \"problem\" entity consistently drops the more context we provide. We provide further analysis in Section 5.4.2.\n5.3.4 Filter Prompts\nLLMs are often brittle to prompting strategies (Zhu et al., 2023). Rather than constructing different templates, we experiment with how incorporating the entity description into the filter affects our frame-work's performance. For \u201ctreatment\u201d, \u201cproblem\u201d"}, {"title": "Error Analysis", "content": "and \"test\", we use descriptions available in i2b2 2010 annotation guidelines. For \u201cclinical department\u201d, we provide our own definition. We put the prompts in Appendix D and present the results in Table 5.\nComplex entity description degrades performance. Our experiment shows a notable performance drop in all entity types except \u201cclinical department\u201d. We hypothesize that the descriptions provided in the guidelines may be too complex for the model to understand. In contrast, we observe more than 7% F1-score improvement on \u201cclinical department\u201d, which uses our handcrafted definition. Unlike the guidelines, we focus on the conciseness of the description.\n5.4 Error Analysis\n5.4.1 Entity Decomposition Missing Entities\nDespite the significant improvement in recall through entity decomposition, some entities remain unrecognizable. Thus, we analyze the potential sources of these errors. First, we calculate the percentage of entities fully absent from the predictions. To illustrate, if the label is \u201chis aspirin\u201d and the prediction is \u201caspirin\u201d, we do not deem it fully absent since the prediction partially captures the label. Figure 4 illustrates the percentage of fully absent entities for each entity type in the dataset.\nEntity decomposition significantly reduces the number of fully absent entities. For instance, only 5.5% entities are fully absent for \u201ctreatment\u201d in the i2b2 2010 dataset after entity decomposition. We observe improvements across all entity types.\nThe majority of fully absent entities are abbreviations and homonyms. For example, open NER LLM cannot capture \"CVA\", an abbreviation for \"cerebral vascular accident\", after entity decomposition. Another example is \u201cdelivery\", which carries nuanced meanings in different contexts (i.e., frequently referencing a \u201cchildbirth\" in the clinical narrative). Furthermore, certain entities such as"}, {"title": "LLM Hyperparameters", "content": "\"HD\" are both abbreviations and homonyms (i.e., high definition vs hemodialysis).\n5.4.2 Performance Drop using Context\nSection 5.3.3 shows that in contrast to other entity types, there is a notable performance degradation for \"problem\" when context is provided. To investigate, we observe the ground truth \"problem\" entities that are removed by the filter. Interestingly, we find that for most of them, the context specifically stated that the patient does not experience\nTable 6: i2b2 2012 Polarity Dataset Statistics\nthese problems. We then conduct further investigation on their polarity attribute, which contains information on whether the patient is experiencing medical problems (or taking certain medications, for instance). To clarify, if there are explicit mentions that a patient does not have certain medical problems, they would be indicated as negatives. Otherwise, it is positive. We conduct an analysis of how entity polarity affects filter responses. We plot our results in Figure 5.\nThe \"negative\" polarity degrades performance. First, the dataset statistics in Table 6 show that almost a fifth of \u201cproblem\u201d entities are \u201cnegative\", making it likely that these gold labels would be rejected. Furthermore, Figure 5 reveals that the \u201cnegative\u201d polarity causes the performance drop on the \"problem\" entity, as revealed by how a majority of the rejected gold \"problem\" entities are \"negative\".\n5.5 More Results\nGiven the space limit, we include more results in the Appendix, including few-shot (Appendix E), performance on a BERT-based model (Appendix F), and error analysis on CD performance drop (Appendix G).\n6 Conclusion and Future Work\nIn this work, we propose a novel EDF framework to tackle the clinical named entity recognition (NER) task. Our comprehensive experiments demonstrate the strength of our framework across different dimensions. We also thoroughly investigate each framework component and provide several key insights. In future works, we hope to explore how to address the limitations of our work described in Section 7."}, {"title": "Acknowledgements", "content": "7 Limitations\nFirst, we restrict our work to clinical narratives and have yet to explore how our framework generalizes to other texts. In this work, we deliberately focus on how well the method generalizes to different datasets, which (1) tackle different and clinically significant (Lehman et al., 2022) entity types, (2) are collected from different institutions (thus different distributions), (3) are de-identified in different ways (e.g., masks used for the patient's sensitive information), (4) used different formats (e.g., header names and section organizations), etc. In fact, each patient is a unique case, and each of them can be treated as a separate domain (Yang et al., 2023). Thus, generalizing to these datasets is already a significant challenge. However, testing how well our framework generalizes beyond clinical narratives would be an interesting avenue. Note that our motivation for this framework is that we found some clinical entities are easier to identify through simpler terms. This is particularly true for clinical narratives since most entities that are of interest (Lehman et al., 2022) follow this assumption. Thus, we designed our framework based on the characteristics of entities inside clinical narratives, not the narratives themselves. This is the reason we hypothesize that our framework may work outside clinical narratives (with similar entity characteristics). We leave this to future work.\nSecond, we restrict our work to only open-sourced models and leave experiments on proprietary models to future works. Most publicly available clinical narratives are under restrictive licenses. Hence, we cannot simply use commercial models. Furthermore, using commercial models on clinical narratives requires de-identification (removing sensitive information), which is a significant process in itself (Johnson et al., 2016). In contrast, open-sourced models have more practical values (e.g., they can be deployed in the hospital's internal system without de-identification). In this work, we deliberately use strong open-sourced models such as UniversalNER (Zhou et al., 2023), which performs better than ChatGPT (Brown et al., 2020). However, how open-sourced models fare with other proprietary models on clinical NER is still unknown. We leave them to future works.\nThird, our work falls under the healthcare domain, which is a high-stakes setting. Despite the reasonable performance, there is still a long way to reach the high requirements set by healthcare applications. Nevertheless, our work paves a potential solution for the zero-shot clinical named entity recognition task.\n8 Ethic Statement\nOur research is conducted on open, retrospective clinical datasets without human subject intervention and thus will not harm human subjects. Furthermore, the clinical domain is complex and requires evaluation beyond performance, particularly regarding safety and bias. Unfortunately, the clinical narratives in our datasets are not associated with specific patients, impeding such evaluations. Further evaluations and validations from clinical experts will be needed to translate research into the clinical decision-making process.\n9 Acknowledgement\nThis publication was supported, in part, by the National Center for Advancing Translational Sciences of the National Institutes of Health under Grant Number UM1TR004548. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. The computation resources enabling this project are provided by the Ohio Supercomputing Center. The authors would like to thank the colleagues from the OSU NingLab for their constructive feedback."}, {"title": "Appendix", "content": "Appendix\nA Entity Decomposers\nWe provide the details for each of our entity decomposition methods described in Section 4.2 here:\n\u2022 Manually curating a set of candidate types using expert-level knowledge. Here", "Tr": "Pr", "Te": "nd \u201cDD\u201d we take the annotation guidelines from i2b2 2010. For \"CD\"", "AD\u201d and \u201cADE\u201d, we use i2b2 2018 Task 2. We list the curated set in Section A.1.\n\u2022 Prompting an LLM for automatic generation. We prompt ChatGPT with \u201cYou are an intelligent clinical language model. Your job is to extract {entity_type} from a patient's discharge summary. What entities can be considered as {entity_type} in a discharge summary?\u201d for each entity type. For reproducibility, we present the results in Section A.2.\n\u2022 Utilizing an existing medical knowledge bank. We use the Unified Medical Language System (UMLS) since it contains standardized medical vocabulary for many clinical entities. Here, we take```json\n{\n  ": "itle", "Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework": "authors\": [\n    \"Reza Averly", "Xia Ning": "abstract\": \"Clinical named entity recognition (NER) aims to retrieve important entities within clinical narratives. Recent works have demonstrated that large language models (LLMs) can achieve strong performance in this task. While previous works focus on proprietary LLMs, we investigate how open NER LLMs, trained specifically for entity recognition, perform in clinical NER. In this paper, we aim to improve them through a novel framework, entity decomposition with filtering, or EDF. Our key idea is to decompose the entity recognition task into several retrievals of sub-entity types. We also introduce a filtering mechanism to remove incorrect entities. Our experimental results demonstrate the efficacy of our framework across all metrics, models, datasets, and entity types. Our analysis reveals that entity decomposition can recognize previously missed entities with substantial improvement. We further provide a comprehensive evaluation of our framework and an in-depth error analysis to pave future works.", "sections": [{"title": "Introduction", "content": "Clinical narratives hold immense value for clinical experts (Tayefi et al., 2021; Mahbub et al., 2022; Raghavan et al., 2014; Rannikm\u00e4e et al., 2021), largely due to their wealth of information often in-accessible in the structured data of electronic health records (EHR) (Mahbub et al., 2023; Goodman-Meza et al., 2022; Kharrazi et al., 2018; Rannikm\u00e4e et al., 2020; Hernandez-Boussard et al., 2019; Boag et al., 2018). Their free format, however, causes significant challenges for healthcare systems to utilize. The richness of information trapped within the narratives, followed by its significance, has spurred a plethora of works in tackling the clinical information extraction problem within the clinical NLP community (Wang et al., 2018; Landolsi et al., 2023).\nOne key building block in clinical information extraction is named entity recognition (NER), focusing on identifying clinical concepts within these narratives. As surveyed (Wang et al., 2018), prior studies rely on either traditional natural language processing (NLP) techniques or supervised learning methods. Nevertheless, the former approach can be fragile, while the latter requires significant effort to annotate. In addition, supervised methods cannot simply scale for the large number of concepts available in the clinical domain (Bodenreider, 2004).\nIn light of this, Large Language Models (LLMs), with their strong capabilities for zero and few-shot learning (Chowdhery et al., 2023; Brown et al., 2020; Thoppilan et al., 2022; Touvron et al., 2023), serve as promising solutions for clinical NER. While previous works focus on LLMs trained on general tasks (Agrawal et al., 2022; Liu et al., 2023; Gero et al., 2023), here we focus on LLMs specifically trained for entity recognition, or open NER LLMs (Zhou et al., 2023; Ding et al., 2024). Inspired by their results in clinical domain (Zhou et al., 2023), outperforming even proprietary LLMs (Brown et al., 2020), we conduct deeper investigations in this study. Surprisingly, our preliminary experiment (Section 5.1) suggests a stark performance gap between retrieving different clinical entities (Figure 2). For instance, an open NER LLM called UniversalNER (Zhou et al., 2023) performs significantly better at extracting medications rather than clinical treatments (85.88% vs 53.81% Exact Match F1-scores). Upon closer inspection, we find these unidentified treatment entities can be effectively recognized by exploiting simpler, albeit specific, entity types. For example, by explicitly specifying \"medication\" rather than \"treatment\u201d as input, the model can capture a substantial portion of the previously unidentified medication-related treatment entities.\nBuilding upon this insight, we present a novel"}, {"title": "Related Work", "content": "framework, entity decomposition with filtering, or EDF, aimed at tackling clinical NER. To the best of our knowledge, we are the first to explore strategies to effectively use open NER LLMs in the clinical domain without using any samples. We draw inspiration from the divide-and-conquer paradigm (Knuth, 1998), which breaks down a complex problem into simpler sub-problems. Con-cretely, we posit that a direct retrieval of entities may be too complex for the model and instead propose to disentangle it into a series of retrievals through entity decomposition. Unlike the previous approach (Xie et al., 2023), entity decomposition breaks down the task by identifying through their sub-entity types, which, ideally, are easier to re-trieve. Nonetheless, entity decomposition alone is insufficient since some sub-entity types do not form strict subsets (further discussed in Section 3.2.3). To address this, we introduce a filtering mechanism in our framework to further improve performance. We illustrate them in Figure 1\n2 Related Work\n2.1 LLMs for Clinical NER\nLLMs are promising for many clinical tasks (Singhal et al., 2022; Agrawal et al., 2022; Clusmann et al., 2023). Concurrently, several works aim to improve their performance on clinical NER. For instance, Agrawal et.al. (Agrawal et al., 2022) proposes a guided prompt design along with a resolver to handle the structured output space required by NER, while others (Hu et al., 2024, 2023; Liu et al., 2023) use prompt engineering. Outside the clinical domain, several works tackle NER either by framing it as a sequence labeling task (Wang et al., 2023), using label decomposition and syntactic augmentation (Xie et al., 2023), or improving the structured label space (Li et al., 2024), similar to Agrawal et.al. (Agrawal et al., 2022). Most of these works focus on LLMs trained in handling diverse tasks such as ChatGPT (Brown et al., 2020).\nIn contrast, we focus on open NER LLMs (Zhou et al., 2023; Ding et al., 2024), which have two key differences. First, they are trained specifically for entity recognition tasks and do not require structured output space handling (Agrawal et al., 2022; Li et al., 2024). Second, their instruction-tuning mostly focused on the diversity of entities rather than the instructions (e.g., keeping the prompt constant), which may limit the efficacy of prompt engineering techniques. Furthermore, unlike previous works (Hu et al., 2024, 2023; Liu et al., 2023), our work does not fall under prompt engineer-ing. Notably, prompt engineering is limited to prompt-based models, while our work is model-agnostic and, thus, is applicable to BERT-based models (Zaratiana et al., 2023).\n2.2 Task Decomposition in LLMs\nThe idea of task decomposition, solving complex tasks through solving its constituent simpler sub-tasks, can be dated back to (Lazarou et al., 1998). Previous works propose task decompositions for LLMs to tackle complex problems (Zhou et al., 2022; Xie et al., 2023). Concurrent with our work, Xie et.al. (Xie et al., 2023) suggests decomposing NER into a multi-turn dialogue, asking the model one question for each label. However, some open NER LLMs (Zhou et al., 2023) can only extract one label at a time, thus limiting the efficacy of Xie et.al. (Xie et al., 2023). Here, we propose to decompose NER on entity-level rather than label-level. Concretely, we can further decompose each label into simpler labels. Our method also complements Xie et.al. (Xie et al., 2023) since these decompositions can be performed sequentially. Besides, our work aims to improve open NER LLMs, which have several key differences from other LLMs as briefly discussed in Section 2.1\n2.3 Open NER LLMs\nClinical narratives fall under domains with a large number of concepts and scarce annotations. Thus, developing open named entity recognition LLMs (Zhou et al., 2023; Ding et al., 2024) is timely and crucial research for clinical NER. Despite the progress, existing works focus on training the backbone models. Furthermore, these models present a unique challenge and cannot be treated similarly to other LLMs (Section 2.1). Our work paves a way to adapt them for clinical domains without finetuning."}, {"title": "Methodology", "content": "3 Methodology\n3.1 Problem Definition\nClinical narrative holds important entities about a patient's medical history. In this work, we aim to tackle clinical NER, focusing on extracting them. We frame the problem through the lens of a text-generative task. Let x be a clinical narrative, and further let t be the target entity type we want to extract. We aim to retrieve the target entities set Y"}, {"title": "ENTITY DECOMPOSITION WITH FILTERING", "content": "corresponding to t from x. To illustrate, if x is a patient's discharge summary and t = \"medication\", then the goal is to extract medication entities in the discharge summary. In this case, the ouput can be Y = {\"aspirin\u201d, \u201cmethanol\u201d, ...}.\n3.2 ENTITY DECOMPOSITION WITH FILTERING\nAs introduced in Section 1, directly retrieving the target clinical entities may be too challenging, particularly for models without domain-specific training, such as open NER LLMs. In this work, we propose to break the task into multiple retrievals of sub-entities instead. We define sub-entities as subsets of the target clinical entities. Concretely, let \\( \\hat{Y}_i \\) be a set of sub-entities corresponding to the i-th sub-entity type \\( s_i \\) and let \\( \\hat{Y} \\) be our complete set, where ideally \\( \\hat{Y}_i \\subseteq \\hat{Y} \\). The first part of our framework, entity decomposer, aims to iteratively collect \\( \\hat{Y}_i \\) to produce \\( \\hat{Y} \\), or \\( \\bigcup_{i=1}^N \\hat{Y}_i = \\hat{Y} \\). The last part of our framework, filter, involves removing disjoint sub-entity set from \\( \\hat{Y} \\). The filtered version \\( \\hat{Y}_f \\) then serves as the final output. We provide more details of our framework in the following sections.\n3.2.1 Entity Decomposer\nTo identify sub-entities \\( \\hat{Y}_i \\), it is essential to define their types. Let \\( D(t) = S \\) be an entity decomposer module, aimed to produce a set of N sub-entity types \\( S = \\{s_1, s_2, ..., s_N\\} \\) using the target entity type t. We note that these sub-entity types can either be manually curated (which may involve clinical experts) or obtained using existing"}, {"title": "Experimental Setup", "content": "tools such as a medical knowledge base (Bodenreider, 2004). Namely, clinical practitioners can resort to cost-effective approaches to construct D. To illustrate how the module works, here we take \"treatment\" as an example. In a patient's discharge summary, \u201ctreatment\u201d constitutes a myriad of entities, including medications, medical procedures, and more (see Figure 1). In this case, D aims to decompose t = \u201ctreatment\u201d into S = {\"medication\u201d, \u201cmedical procedure\", ...}.\n3.2.2 Open NER LLM\nAfter defining the sub-entity types S, the next step is to retrieve all the sub-entities \\( \\hat{Y}_i \\). In this work, we leverage an open NER LLM. We base its formal definition to (Zhou et al., 2023). Let f be an open NER LLM tasked to retrieve the sub-entities \\( \\hat{Y}_i \\) of \\( s_i \\) from the clinical narrative x. That is, \\( f(x, s_i) = \\hat{Y}_i \\). To construct the complete set \\( \\hat{Y} \\), the model would iteratively collect \\( \\hat{Y}_i \\) from each \\( s_i \\). This may be of concern, given that it requires multiple iterations. To address this, we also consider other variants (Ding et al., 2024; Zaratiana et al., 2023) of open NER LLMs, which we formally define as \\( f*(x, S) = \\hat{Y} \\). That is, in contrast to f, these variants are capable of simultaneously extracting multiple entity types at once, thereby reducing the whole iterative sub-entity retrievals to only one forward pass.\nNext, we provide our reasons for using an open NER LLM in our framework. First, open NER LLMs are versatile in recognizing arbitrary entities, which is vital given that sub-entities are not confined to specific entities. This stands in contrast to traditional supervised NER models, which"}, {"title": "Filter", "content": "are bounded by their predefined label sets. Second, in contrast to other LLMs, open NER LLMs are trained specifically for NER tasks, substantially reducing the effort of mapping LLM generative output to the structured output space of NER (Agrawal et al., 2022). In addition, our preliminary experiment in Section 5.1 suggests that open NER LLMs perform surprisingly well on recognizing basic clinical entities such as medications, making them strong candidates as our sub-entities retriever.\n3.2.3 Filter\nWe discuss the last step of our framework here. Formally, let \\( L(\\hat{Y}, t, k) = \\hat{Y}_f \\) be the filter module, where k denotes a context and \\( \\hat{Y}_f \\subseteq \\hat{Y} \\). In this framework, L aims to eliminate sub-entities within \\( \\hat{Y} \\) that do not fall under the target entity type t based on its context k. Similarly, L can also be viewed as a binary classifier, assigning a positive label if the entity corresponds to t, and a negative label otherwise. The filter module then aggregates and outputs the positively classified entities.\nHere, we discuss how some sub-entity types may not adhere to \\( \\hat{Y}_i \\subseteq \\hat{Y} \\). We formally define these atypical sub-entity types as \\( s_i \\) and their corresponding sub-entities as \\( \\hat{Y}_i \\), where \\( \\hat{Y} \\setminus \\hat{Y}_i \\neq 0 \\). Concretely, some sub-entities in \\( \\hat{Y}_i \\) are disjoint to \\( \\hat{Y} \\), or they fall outside \\( \\hat{Y} \\). Take \u201ctreatment\u201d and \u201cmedical procedure\" for example. While \u201ctreatment\u201d includes \u201cmedical procedure\", not all \"medical procedure\" qualify as a \u201ctreatment\u201d. For instance, in a clinical narrative, some medical procedures may serve purely for diagnosis rather than treatment. Thus, they do not fall under \"treatment\" and should be removed.\nNext, we provide the rationale behind context in the filter module. Unlike general domain entities, clinical entities can largely rely on context cues. To illustrate, consider \"adverse drug event (ADE)\u201d, a clinically significant entity as evidenced by its association with emergency visits and hospitalizations (Zed et al., 2008; Lazarou et al., 1998). By definition, \"ADE\u201d is an \"injury resulting from a medical intervention\" (Henry et al., 2020). In our framework, one of its sub-entity types may be \"injury\". However, for a filter to dicatate whether an injury corresponds to \u201cADE\u201d, it needs to be aware of other medical interventions and whether the injury stems from any of them. In other words, the filter requires the context in which the entity occurs to provide an accurate prediction. Moreover, our experiment suggests that while some clinical enti-\""}, {"title": "Experimental Setup", "content": "ties do not necessitate contextual information, they can derive benefit from it (see Section 5.3.3).\n4 Experimental Setup\nWe provide the experimental setup here and leave the details in the Appendix. All of the base models are available in huggingface*.\n4.1 Open NER LLMS\nWe take SOTA open NER LLMs in our experiment and further improve them. Per definition in Section 3.2.2, they may be categorized based on how many entity types can be extracted simultaneously. To this end, we use UniversalNER (Zhou et al., 2023) and GNER (Ding et al., 2024) as the representative for f and f*, respectively. Concretely, we use UNIVERSALNER-TYPE-7B and GNER-LLAMA-7B. Both of them are finetuned on PileNER dataset (Zhou et al., 2023), which is generated from GPT 3.5 (Brown et al., 2020). In contrast to other LLMs, open NER LLMs are trained on the diversity of entity types rather than the prompts (Zhou et al., 2023). Thus, we only experiment with their default prompts.\n4.2 Entity Decomposers\nHere, we experiment with different techniques to decompose entities. First, given that clinical narratives require specialized knowledge, we consider sub-entity types curated by clinical experts. Specifically, we take the annotation guidelines available from the datasets. Second, we use ChatGPT (Brown et al., 2020) to decompose clinical entities automatically. We draw our inspiration from the recent success of ChatGPT in the clinical domain (Agrawal et al., 2022; Singhal et al., 2022). Furthermore, using ChatGPT for entity decomposition is more cost-effective and scalable. Third, we utilize the Unified Medical Language System (UMLS) (Bodenreider, 2004), a medical knowledge base, for retrieving sub-entity types. We provide more details in the Appendix A.\n4.3 Filters\nWe use Asclepius (Kweon et al., 2023) and LlaMA-2 (Touvron et al., 2023) trained on clinical and general domains, respectively. Specifically, we use ASCLEPIUS-LLAMA2-7B and LLAMA-2-CHAT-7B versions. Given the inherent generative nature of LLMs, we restrict their"}, {"title": "Ablation Study", "content": "outputs to \"Yes/No\" responses (when applicable) using grammar-constrained decoding (Geng et al., 2023). This strategic constraint reduces the number of generated tokens, resulting in increased inference speed. By default, we prompt the model by asking \"Can {entity} be considered as a/an {entity_type}?\". We try different prompts in Section 5.3.4. For entities that require context, we use a simple preprocessing so that the context provides sufficient information to extract the clinical entities.\n4.4 Datasets\nWe focus our experiment on extracting concepts from publicly available clinical notes. We use ClinicalIE (Agrawal et al., 2022), i2b2 2010 (Uzuner et al., 2011), i2b2 2012 (Sun et al., 2013), i2b2 2018 Task 2 (Henry et al., 2020) and CLEF 2014 (Mowery et al., 2014) datasets in this paper. They are available in Harvard DBMI for i2b2 datasets, PhysioNet\u2020 (Goldberger et al., 2000) for CLEF 2014 and huggingface for ClinicalIE. By default, all clinical entities in our experiment datasets do not require context except for the i2b2 2018 dataset. We provide further details in the Appendix \u0412.\n4.5 Baselines and Metrics\nGiven the scarce methods to compare with, we use Xie et.al. (Xie et al., 2023) as our baseline. Concretely, we extract each entity type one at a time using UniversalNER and GNER. We also compare with UNIVERSALNER-ALL, trained with both PileNER and over 40 supervised datasets, including our experiment sets. We use Precision (P), Recall (R), and Exact Match F1-Score (F1) as evaluation metrics, similar to previous works.\n5 Experimental Results\nThroughout this section, we abbreviate the entity types in the tables as follows to save space: Tr for treatment, Pr for problem, Te for test, CD for clinical department, DD for disease/disorder, AD for adverse drug, and ADE for adverse drug event.\n5.1 Preliminary Experiment\nWe conduct a preliminary experiment to confirm that open NER LLMs perform better at recognizing sub-entities rather than the target entity types. For target entities, we use the i2b2 2012 dataset, which"}, {"title": "Ablation Study", "content": "contains decomposable entity types (i.e., entities can further be divided into sub-entities). For sub-entities, we use ClinicalIE, a medication extraction dataset.\nFigure 2 illustrates the result and confirms our hypothesis. That is, it is harder to recognize the target entity types (that are decomposable) compared to the sub-entity. For instance, we observe a stark difference between \"medication\" extraction and \"treatment\u201d extraction, where the former is a sub-entity of the other.\n5.2 Overall Performance\nWe present our results in Table 1 and Figure 3. For detailed numbers on precision and recalls, we leave them in the Appendix C.\nOn average, EDF outperforms baseline by 2.54% and 5.82% F1-score on UniversalNER and GNER, respectively. Interestingly, for some entity types, GNER performs similarly or even outperforms UniversalNER. This suggests that models that can recognize multiple entities simultaneously can benefit more from using our framework.\nEntity decomposition (ED) improves recall but decreases precision. As illustrated in Figure 3, we observe a consistent improvement in recall across diverse datasets and entity types for both models, suggesting that entity decomposition facilitates the identification of previously missing entities while being robust to the backbone models. For precision, however, we notice a drop in performance. As discussed in Section 3.2.3, some sub-entities may not form a subset of the target entities, causing performance degradation on precision. Further examination of the F1-score reveals a decline in overall performance, which justifies the necessity of incorporating a filtering mechanism.\nConversely, filtering (F) benefits precision but degrades recall. This shows contrasting results compared to entity decomposition across datasets, entity types, and models. The overall F1-score"}, {"title": "Statement", "content": "Table 1: F1-score performance (%) comparison between baseline (B) from Xie et.al. (Xie et al., 2023), Entity Decomposer (ED) only, Filter (F) only, supervised (UniNER-all) and EDF (Ours) methods across datasets, entity types, and models. We use Asclepius and the default prompt strategy (Section 3.2.3) for the filter and annotation guideline for the entity decomposer. We mark the best results in bold and second-best in underlined. Discussion in Section 5.2\nimprovement on EDF compared to using each component individually suggests that entity decomposition and filtering complement each other. This emphasizes the necessity to incorporate both of them.\nEDF is robust to out-of-distribution entities compared to supervised training. We want to emphasize that our method does not require any training as opposed to the supervised approach. We use UniversalNER+EDF and UniversalNER-all as comparison. Despite the performance gap, we observe that on entities not covered in the training label set (e.g., adverse drug or AD), EDF outperforms by more than 10% on the F1-score. This shows the robustness of our method.\n5.3 Ablation Study\nWe perform additional experiments to test the efficacy of our framework using different entity decomposers or filters. We focus on the overall performance or Fl-score. To reduce the cost of our experiments, we only experiment with the i2b2 2012 dataset, given their diversity in entity types. Unless otherwise specified, we use UniNER as the open NER LLM, annotation guideline for the entity decomposer module, and Asclepius for the filter.\n5.3.1 Entity Decomposers\nFirst, we conduct ablation study with different entity decomposers as described in Section 4.2 and present our results in Table 2. We do not experiment with ChatGPT and UMLS for the \"clinical department\" entity type since (1) ChatGPT is unable to produce reasonable sub-entity types and (2) we find there are no correspondence semantic types in UMLS.\nEDF is robust to entity decomposer module. We show that a general LLM and an existing medical knowledge base may serve as alternatives to clinical experts, as indicated by their competitive performance (e.g. 58.25% vs 58.09% between annotation and ChatGPT on treatment, respectively) in Table 2. This circumvents the necessity of having clinical experts to curate sub-entity types.\nWithout filters, UMLS outperforms other entity decomposers. Most sub-entity types in UMLS form subsets to the target entities; hence, a filter may not be necessary. Interestingly, for some enti-"}, {"title": "LLM Prompt Templates", "content": "Table 2: Ablation Study on Entity Decomposer.\nTable 4: Ablation Study on Filter Context.\nTable 3: Ablation Study on Filter Model.\nties, it performs better than vanilla prompt. This is significant since even without filtering, our framework, specifically entity decomposition, can still outperform the vanilla approach with proper curation of sub-entity types (i.e., making sure that the sub-entity types do not have disjointed sub-entities). We remark that this does not underscore the value of a filter module since UMLS can benefit more from integrating it, as shown in Table 2.\n5.3.2 Filter Models\nHere, we investigate how different filter models affect the overall performance of our framework. Specifically, we compare domain-specific and general LLMs. We present the results in Table 3\nClinical model is better at recognizing entities requiring clinical expertise. Specifically, we observe that it is superior to a general domain model in \u201ctreatment\u201d, \u201cproblem\u201d, and \u201ctest\u201d entities. For \"clinical department\u201d, however, they perform similarly. This is unsurprising since the former entities are heavily involved with entities requiring clinical-specific knowledge, while the latter is splintered\nwith entities that often appear in the general domain, such as hospitals.\n5.3.3 Filter Context\nAs discussed in Section 3.2.3, some clinical entities necessitate a context. Here, we investigate whether context helps for entities not requiring it. We compare the performance between filtering (1) without context, (2) including the sentence the entity appears in, and (3) providing the whole clinical narrative or document. We show the results in Table 4.\nContext may improve or hurt performance. Overall, we observe mixed results across different entity types, with and without an entity decom-poser. For instance, we observe slight improvements for \"treatment\" and \"test\" entities. On the other hand, the performance of the \"problem\" entity consistently drops the more context we provide. We provide further analysis in Section 5.4.2.\n5.3.4 Filter Prompts\nLLMs are often brittle to prompting strategies (Zhu et al., 2023). Rather than constructing different templates, we experiment with how incorporating the entity description into the filter affects our frame-work's performance. For \u201ctreatment\u201d, \u201cproblem\u201d"}, {"title": "Error Analysis", "content": "and \"test\", we use descriptions available in i2b2 2010 annotation guidelines. For \u201cclinical department\u201d, we provide our own definition. We put the prompts in Appendix D and present the results in Table 5.\nComplex entity description degrades performance. Our experiment shows a notable performance drop in all entity types except \u201cclinical department\u201d. We hypothesize that the descriptions provided in the guidelines may be too complex for the model to understand. In contrast, we observe more than 7% F1-score improvement on \u201cclinical department\u201d, which uses our handcrafted definition. Unlike the guidelines, we focus on the conciseness of the description.\n5.4 Error Analysis\n5.4.1 Entity Decomposition Missing Entities\nDespite the significant improvement in recall through entity decomposition, some entities remain unrecognizable. Thus, we analyze the potential sources of these errors. First, we calculate the percentage of entities fully absent from the predictions. To illustrate, if the label is \u201chis aspirin\u201d and the prediction is \u201caspirin\u201d, we do not deem it fully absent since the prediction partially captures the label. Figure 4 illustrates the percentage of fully absent entities for each entity type in the dataset.\nEntity decomposition significantly reduces the number of fully absent entities. For instance, only 5.5% entities are fully absent for \u201ctreatment\u201d in the i2b2 2010 dataset after entity decomposition. We observe improvements across all entity types.\nThe majority of fully absent entities are abbreviations and homonyms. For example, open NER LLM cannot capture \"CVA\", an abbreviation for \"cerebral vascular accident\", after entity decomposition. Another example is \u201cdelivery\", which carries nuanced meanings in different contexts (i.e., frequently referencing a \u201cchildbirth\" in the clinical narrative). Furthermore, certain entities such as"}, {"title": "LLM Hyperparameters", "content": "\"HD\" are both abbreviations and homonyms (i.e., high definition vs hemodialysis).\n5.4.2 Performance Drop using Context\nSection 5.3.3 shows that in contrast to other entity types, there is a notable performance degradation for \"problem\" when context is provided. To investigate, we observe the ground truth \"problem\" entities that are removed by the filter. Interestingly, we find that for most of them, the context specifically stated that the patient does not experience\nTable 6: i2b2 2012 Polarity Dataset Statistics\nthese problems. We then conduct further investigation on their polarity attribute, which contains information on whether the patient is experiencing medical problems (or taking certain medications, for instance). To clarify, if there are explicit mentions that a patient does not have certain medical problems, they would be indicated as negatives. Otherwise, it is positive. We conduct an analysis of how entity polarity affects filter responses. We plot our results in Figure 5.\nThe \"negative\" polarity degrades performance. First, the dataset statistics in Table 6 show that almost a fifth of \u201cproblem\u201d entities are \u201cnegative\", making it likely that these gold labels would be rejected. Furthermore, Figure 5 reveals that the \u201cnegative\u201d polarity causes the performance drop on the \"problem\" entity, as revealed by how a majority of the rejected gold \"problem\" entities are \"negative\".\n5.5 More Results\nGiven the space limit, we include more results in the Appendix, including few-shot (Appendix E), performance on a BERT-based model (Appendix F), and error analysis on CD performance drop (Appendix G).\n6 Conclusion and Future Work\nIn this work, we propose a novel EDF framework to tackle the clinical named entity recognition (NER) task. Our comprehensive experiments demonstrate the strength of our framework across different dimensions. We also thoroughly investigate each framework component and provide several key insights. In future works, we hope to explore how to address the limitations of our work described in Section 7."}, {"title": "Acknowledgements", "content": "7 Limitations\nFirst, we restrict our work to clinical narratives and have yet to explore how our framework generalizes to other texts. In this work, we deliberately focus on how well the method generalizes to different datasets, which (1) tackle different and clinically significant (Lehman et al., 2022) entity types, (2) are collected from different institutions (thus different distributions), (3) are de-identified in different ways (e.g., masks used for the patient's sensitive information), (4) used different formats (e.g., header names and section organizations), etc. In fact, each patient is a unique case, and each of them can be treated as a separate domain (Yang et al., 2023). Thus, generalizing to these datasets is already a significant challenge. However, testing how well our framework generalizes beyond clinical narratives would be an interesting avenue. Note that our motivation for this framework is that we found some clinical entities are easier to identify through simpler terms. This is particularly true for clinical narratives since most entities that are of interest (Lehman et al., 2022) follow this assumption. Thus, we designed our framework based on the characteristics of entities inside clinical narratives, not the narratives themselves. This is the reason we hypothesize that our framework may work outside clinical narratives (with similar entity characteristics). We leave this to future work.\nSecond, we restrict our work to only open-sourced models and leave experiments on proprietary models to future works. Most publicly available clinical narratives are under restrictive licenses. Hence, we cannot simply use commercial models. Furthermore, using commercial models on clinical narratives requires de-identification (removing sensitive information), which is a significant process in itself (Johnson et al., 2016). In contrast, open-sourced models have more practical values (e.g., they can be deployed in the hospital's internal system without de-identification). In this work, we deliberately use strong open-sourced models such as UniversalNER (Zhou et al., 2023), which performs better than ChatGPT (Brown et al., 2020). However, how open-sourced models fare with other proprietary models on clinical NER is still unknown. We leave them to future works.\nThird, our work falls under the healthcare domain, which is a high-stakes setting. Despite the reasonable performance, there is still a long way to reach the high requirements set by healthcare applications. Nevertheless, our work paves a potential solution for the zero-shot clinical named entity recognition task.\n8 Ethic Statement\nOur research is conducted on open, retrospective clinical datasets without human subject intervention and thus will not harm human subjects. Furthermore, the clinical domain is complex and requires evaluation beyond performance, particularly regarding safety and bias. Unfortunately, the clinical narratives in our datasets are not associated with specific patients, impeding such evaluations. Further evaluations and validations from clinical experts will be needed to translate research into the clinical decision-making process.\n9 Acknowledgement\nThis publication was supported, in part, by the National Center for Advancing Translational Sciences of the National Institutes of Health under Grant Number UM1TR004548. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. The computation resources enabling this project are provided by the Ohio Supercomputing Center. The authors would like to thank the colleagues from the OSU NingLab for their constructive feedback."}, {"title": "Appendix", "content": "Appendix\nA Entity Decomposers\nWe provide the details for each of our entity decomposition methods described in Section 4.2 here:\n\u2022 Manually curating a set of candidate types using expert-level knowledge. Here, we refer to the annotation guidelines available in existing datasets since we believe they are curated by domain experts. For \u201cTr\u201d, \u201cPr\u201d, \u201cTe\u201d and \u201cDD\u201d we take the annotation guidelines from i2b2 2010. For \"CD\", we use i2b2 2012. For \"AD\u201d and \u201cADE\u201d, we use i2b2 2018 Task 2. We list the curated set in Section A.1.\n\u2022 Prompting an LLM for automatic generation. We prompt ChatGPT with \u201cYou are an intelligent clinical language model. Your job is to extract {entity_type} from a patient's discharge summary. What entities can be considered as {entity_type} in a discharge summary?\u201d for each entity type. For reproducibility, we present the results in Section A.2.\n\u2022 Utilizing an existing medical knowledge bank. We use the Unified Medical Language System (UMLS) since it contains standardized medical vocabulary for many clinical entities. Here, we take"}]}]}