{"title": "EEG Artifact Detection and Correction with Deep Autoencoders", "authors": ["David Aquilu\u00e9-Llorens", "Aureli Soria-Frisch"], "abstract": "EEG signals convey important information about brain activity both in healthy and pathological conditions. However, they are inherently noisy, which poses significant challenges for accurate analysis and interpretation. Traditional EEG artifact removal methods, while effective, often require extensive expert intervention. This study presents LSTEEG, a novel LSTM-based autoencoder designed for the detection and correction of artifacts in EEG signals. Leveraging deep learning, particularly LSTM layers, LSTEEG captures non-linear dependencies in sequential EEG data. LSTEEG demonstrates superior performance in both artifact detection and correction tasks compared to other state-of-the-art convolutional autoencoders. Our methodology enhances the interpretability and utility of the autoencoder's latent space, enabling data-driven automated artefact removal in EEG its application in downstream tasks. This research advances the field of efficient and accurate multi-channel EEG preprocessing, and promotes the implementation and usage of automated EEG analysis pipelines for brain health applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Electroencephalography (EEG) is a non-invasive and cost-effective approach to register the electrical activity generated in the brain. This activity is highly informative and is leveraged across a wide range of applications such as diagnosing brain diseases [1], [2], monitoring epilepsy [3]\u2013[5], determining quality of sleep [6], [7], affective computing [8], [9], and interacting with machines through Brain-Computer Interfaces [10], among others. However, the electrical potentials generated by neural activity present low amplitudes and are usually overshadowed by artifacts: spurious sources of high-amplitude electrical noise originated by ocular or muscular activity, electromagnetic interferences, incorrect EEG electrode contact, etc [11]. These artifacts hinder the adequate analysis of the brain recordings and should, therefore, be removed as accurately as possible, which is attained through the offline application of different methodologies. The occurrence of artifacts is greatly exacerbated in EEG studies in more naturalistic scenarios, which have been gaining popularity thanks to advancements in cost and portability of recording devices [12]\u2013[14]. Therefore, artifact removal is crucial to spread the usage of accessible, non-invasive brain monitoring in new application domains. Currently, EEG artifact removal is a time-consuming process requiring expert evaluation. Developing tools to automate this task can significantly alleviate the burden on researchers and clinical practitioners, facilitating automated EEG analysis. Specially in out-of-the-lab settings the performance of artefact correction methodologies is not always optimal and offer clear room for improvement. Furthermore, the implementation of real-time artefact removal paves the path to effectively use EEG in ergonomics, human-machine interaction, and clinical decision support systems, to name a few.\nClassical artifact removal techniques rely on identifying specific sources of noise to apply targeted strategies. Techniques such as filtering are effective against power line noise, which appear at very specific frequencies. However, removing other artifactual subsignals originated by e.g. eye blinks or muscle activation, requires more sophisticated methods. Among diverse Blind Source Separation methodologies [15], Independent Component Analysis (ICA) has been widely used as a reliable statistical algorithm to decompose an EEG recording into its multiple independent constituents [11], [16]. This approach allows to remove recurring artifacts, such as eye blinks [17], but requires expert knowledge on artefact characterization, careful preprocessing, significant computational resources for long recordings or large datasets, and does not allow the implementation of automated pipelines.\nIn recent years, multiple tools leveraging current advancements on deep learning have been developed to automate EEG artifact removal. Pion-Tonachini et al. [18] introduced ICLabel, a Convolutional Neural Network (CNN) designed to complement ICA by automatically classifying source signals. Despite its successful implementation to automate the pre-processing of large datasets, the method is still constrained by ICA's limitations, particularly its reliance on simple mapping functions between sources and channel recordings that may simplify the complex and non-linear nature of neuronal activity [19].\nThus, recent work has focused on developing deep neural networks, capable of learning highly complex non-linear relations to automatically remove artifacts from EEG signals. Initial efforts targeted single-channel EEG corrections, exemplified by Zhang and colleagues' work, introducing the EEGDenoiseNet benchmark dataset for artifact removal [20] and using it to train multiple networks for automated EEG segment correction. Following studies leveraged the EEGDenoiseNet dataset to train neural"}, {"title": null, "content": "networks able to separate the contaminated from the neurophysiological signal [21] and Generative Adversarial Networks (GANs) that learn to generate a clean epoch from a noisy input to deceive an adversarial discriminator network [22].\nHowever, this single-channel focus overlooks crucial information in the spatial domain, essential for accurately correcting artifacts across channels placed at different points on the scalp. Despite the higher computational demands and complexity, a multi-channel approach promises a more holistic and effective strategy for EEG artifact removal.\nSeveral works have implemented Convolutional Neural Networks (CNNs) for multi-channel artifact correction. Saba Sadiya et al. [23] applied an ensemble of CNNs to correct artifacts in EEG epochs after an artifact detection step, while Lopes and colleagues developed a Deep CNN to remove artifacts from long epilepsy recordings [24]. Nonetheless, Lopes et al's work highlights the inability of the network at removing large artifacts, indicating a need for an additional rejection step to remove segments where brain activity is totally masked by artifacts.\nMore recent research has been devoted to autoencoders, a particular type of deep neural network architecture. Its ability to compress and decompress data efficiently by learning a low-dimensional representation of input features in an unsupervised manner eases the implementation of automated correction algorithms circumventing the effort-intensive process of manual data labelling. The work in [19] presents IC-U-Net, an adaptation of the widely successful UNET architecture [25] for automatic multi-channel EEG artifact correction. By training the UNET with a dataset of raw and denoised signal pairs, created using ICLabel, the network learns to denoise EEG signals by minimizing the Mean Squared Error (MSE) between its outputs and the target denoised signals. Similarly, Lai and colleagues successfully implemented a lightweight autoencoder CNN [26] demonstrating comparable performance to IC-U-Net using a significantly smaller network architecture.\nIn this present study, we further advance the field of automated multi-channel EEG artifact removal through the development of LSTEEG (Starlab's Artifact Removal Autoencoder for EEG recordings), a novel LSTM-based Autoencoder designed for detection and correction of artifacual activity in EEG signals. Our novel implementation leverages LSTM layers, which are specifically designed to capture long-term non-linear dependencies in sequential data [27], [28] and have already demonstrated EEG processing capabilities for classification [29], [30] and denoising [20]. We supersede this last work by presenting the first LSTM autoencoder to work with multi-channel EEG, which incorporates the aforementioned advantages. LSTEEG learns to encode each EEG segment into a homogeneous low-dimensional latent space, thereby harnessing the benefits of AEs, such as improved interpretability, thanks to its lower-dimensional representations, and synthetic EEG sample generation, both clean and artifactual, which can be further exploited for data augmentation."}, {"title": null, "content": "Moreover, we propose a novel method to leverage the unsupervised capabilities of AEs in the context of artifact removal. Recognizing the importance of identifying instances of incomplete denoising, we incorporate an anomaly detection technique to detect EEG segments containing artifactual information, serving as a crucial step either before or after the correction process to guarantee thorough pre-processing.\nWe conduct a comparative performance evaluation of LSTEEG with recently developed state of the art convolutional AEs in two different EEG data sets. We demonstrate that our LSTEEG presents competitive performances in both artifact detection and correction tasks, while also learning meaningful low dimensional representations in its Latent Space. These representations add an interpretability layer for further understanding of what constitutes clean EEG signals, as well as to further extract data-driven neurophysiological features, which can be later used in downstream tasks.\nOur contribution is three-fold:\n- We present LSTEEG, a novel LSTM-based autoencoder network for EEG artifact detection and correction with a clearly defined compressed Latent Space.\n- We show that unsupervised training of autoencoders with clean EEG epochs, which does not require the usage of ground truth labels, allows for accurate classification of contaminated signals.\n- We propose multiple ways to study and interpret the learned Latent Space of our LSTM-based autoencoder, characterizing clean EEG with a data-driven approach while opening up novel ways to utilize the learned features for its further application."}, {"title": "II. METHODS", "content": ""}, {"title": "A. Artifact Detection", "content": "Anomaly Detection differs from the classical supervised classification approaches where examples of the different classes are given during the training procedure. Anomaly Detection approaches learn to characterize the \"normal\" class, from which it detects deviations during the inference phase. It includes data-driven algorithms that detect atypical patterns in observations, which are likely caused by a different mechanism [31]. Thus, we propose a novel approach to detecting EEG artifacts as an anomaly detection problem. Since labeled data in the context of artifactual EEG is expensive to generate, we propose an unsupervised approach based on autoencoders. Autoencoders are trained to minimize the reconstruction error between the outputs they produce and the input they have been fed. Thus, when an input has characteristics that significantly differ from the characteristics of the samples in their training data, the reconstruction is degraded. It is possible then to leverage the reconstruction error value as an anomaly metric, i.e used to classify the input as being an anomalous or regular observation through binary classification.\nTherefore, we train the implemented AEs with already pre-processed clean EEG data from the LEMON dataset,"}, {"title": null, "content": "fully described in section II-C.1. A 60/20/20 training/-validation/test hold-out partition is applied to the clean LEMON dataset for performance evaluation. We use 60% for training, 20% as a validation set for early stopping, and 20% for testing. During training, we aim to find the networks' weights that minimize the Mean Squared Error (MSE) between the AE's input and output epochs. Additionally, we merge the EEG epochs in the clean testing dataset with noisy epochs from the original LEMON data, to create the \"LEMON Clean/RawFiltered\" dataset, which we use to validate the artifact detection capabilities of the implemented networks.\nDetails on the training process and hyperparameters are provided in the Supplementary Information, Supp1.1.\nAfter training, we forward the evaluation epochs through the AEs. AEs are expected to produce accurate reconstructions for EEG epochs with similar characteristics to the training data (low reconstruction MSE). However, the networks will provide degraded reconstruction (high reconstruction MSE) for epochs containing artifacts, as these anomalies were not present in the training process. Thus, the reconstruction MSE becomes the predictive function used to classify epochs: lower MSE values indicate a higher probability of an epoch being clean while higher MSE values indicate a higher probability of being noisy. The Area Under the Receiver Operating Characteristic Curve (AUC) will be used to determine the predictive ability of the trained network."}, {"title": "B. Artifact Correction", "content": "In the context of artifact correction, our objective is to train the deep networks to eliminate artifacts from EEG signals while preserving the intrinsic brain activity. To achieve this task, we must take a supervised training approach, where each EEG epoch serves as input, and the network learns to approximate its output to a predetermined target, specifically the artifact-free version of the input epoch.\nWe generate this dataset by automatically denoising the LEMON dataset using ICLabel, following similar steps as in [19], thus creating an extensive collection of epoch pairs: the original, uncleaned input epoch and its corresponding, automatically denoised target epoch. It is worth mentioning that this dataset also includes epochs originally unaffected by noise, untouched by the automatic denoising process. These epochs are invaluable, as they provide the network with examples of clean signals, demonstrating cases where no corrective action is required.\nFollowing the pipeline used in the Artifact Detection problem, we partition the epoch pairs into training, validation and testing sets in a 60%/20%/20% partition. After training, we assess the performance of the models using the average root mean square error (RMSE) across the test set epochs. We quantitatively compare the error scores between the different implemented model to determine the best performing approaches. We will further examine the reconstruction capacity of the models by qualitatively analyzing the time and frequency domain visualizations of the reconstructed epochs."}, {"title": "C. Datasets", "content": "1) LEMON dataset\nThe LEMON dataset is a publicly available dataset that contains multiple physiological recordings of 227 healthy subjects, as well as the results of a battery of cognitive and psychological tests [32]. The resting state EEG has a duration of 16 minutes, using a 61-channel montage in a 10-10 electrode system. The recording is done with interleaved eyes-open and eyes-closed blocks of 60s during a resting-state task. The dataset includes both raw and pre-processed EEG signals."}, {"title": null, "content": "a) Pre-Processed LEMON for Artifact Detection\nIn order to train a network for artifact detection through anomaly detection, we will need a training data set as clean as possible. Thus, we use the pre-processed data that has been visually inspected and manually cleaned by Babayan et al.\nIn [32], the pre-processing pipeline applied consisted in downsampling the signals from 2500Hz to 250Hz and applying a bandpass filter between 1-45Hz (using an 8th order Butterworth filter). Outlier channels were rejected after visual inspection for frequent jumps/shifts in voltage and poor signal quality. Additionally, data intervals with extreme peak to peak deflections or large bursts of high frequency activity were also identified by visual inspection and removed. Finally, PCA and ICA were applied to remove components including eye movement, eye blinks or heartbeats. We further downsampled the EEG recordings to 200 Hz, for consistency with the EOG-synthetically contaminated dataset. Finally, as will be done with all the datasets used in this work, we select the 19 standard channels from the international 10-20 electrode system and divide the dataset in two-second-long epochs.\nWe split the LEMON data into three subject-stratified groups: training (60% of subjects), validation (20% of subjects), and testing (20% of subjects). We then populate the training, validation, and testing partitions by pooling the corresponding EEG epochs in each set."}, {"title": null, "content": "b) Automatically denoised LEMON for Artifact Correction\nTo train the implemented neural networks to automatically correct artifacts we need to build a dataset with noisy epochs as inputs and their corresponding cleaned epochs as targets. Regrettably, it is not possible to relate the raw and manually pre-processed signlas provided by Babayan et al. due to missing information about rejected data segments. Therefore, starting from the raw LEMON data, the input dataset, X, is obtained by downsampling the data to 200 Hz (for consistency with the rest of the used datasets) and applying a bandpass filter between 1 and 45 Hz. In parallel, we sample epochs from noisy recordings from the testing partition and merge them together with random epochs from the pre-processed LEMON testing partition, creating the \"LEMON Clean/RawFiltered\" dataset mentioned in Section II-A, to evaluate the artifact detection capabilities of the trained networks.\nFor the clean part, we repeat the downsampling and filtering of the raw data and later apply automatic rejection of Independent Components with ICLabel [18]. ICLabel is a deep-learning based tool that classifies independent components resulting from an ICA decomposition in several classes (\"brain\", \"muscle artifact\", \"eye blink\", \"heart beat\", \"line noise\", \"channel noise\", \"other\"). We make use of the provided classes, along with the provided classification probability to reject independent components. We study the following two automatic approaches:\n- Maintain only brain components: only those independent components that have been labeled as \"brain\u201d by the ICLabel tool with a probability over 80% are kept, the rest are removed from the EEG signal, obtaining $X_{Br}$, following the same approach as in [19].\n- Reject artifactual components: we also consider a more conservative approach where we only remove those independent components that have been labeled as artifact (\"muscle artifact\", \"eye blink\", \"heart beat\u201d, \u201dline noise\", \"channel noise\") with a probability over 90%, obtaining $X_{Ar}$. This is the default setting for the ICLabel plugin in the standard toolkit EEGLAB.\nAgain, we use the 19 standard channels in the international 10-20 electrode system and split the temporal dimension into two-second-long epochs."}, {"title": "2) EOG-synthetically contaminated dataset", "content": "The EOG-synthetically contaminated dataset consists of two different sets of data: one with clean EEG recordings, and one with EOG synthetically contaminated EEG recordings [33]. The clean EEG dataset, $X_{EOG}$, contains 1 minute of eyes-closed (EC) EEG recordings from 27 healthy subjects (14 males) at a sampling frequency of 200Hz. A montage with the 19 standard electrodes was used, in the 10-20 electrode system. The recordings were already filtered by the authors, applying a band-pass filter in the range 0.5-40Hz and a Notch filter at 50Hz. The synthetically contaminated set, i.e. $X'_{EOG}$, is obtained through the linear combination of EOG recordings with the clean EEG dataset. In this case, the EOG was recorded from the same subjects during an eyes-open (EO) task with four electrodes to obtain the vertical (VEOG) - through two electrodes and the horizontal (HEOG) components - through the other two. The EOG components are linearly combined with the clean EEG signals following:\n$X'_{jEOG}=X_{jEOG}+a_jVEOG+b_jHEOG$ (1)\nwhere j is the channel index and $a_j$ and $b_j$ are linear coefficients that account for the strength contribution of the EOG to each EEG channel. These coefficients follow a model further explained in [33]."}, {"title": "D. LSTEEG Network Structure", "content": "LSTM layers have been shown to perform well on physiological time signals [34], [35]. The developed LSTM-based autoencoder takes multi-channel two-second-long EEG epochs as input, aiming to achieve unsupervised feature extraction of underlying EEG characteristics. The multi-channel epoch is first embedded into the Latent Space (LS) by the encoder section of the network. The encoder processes the epoch via two concatenated LSTM layers, with $N_0$ = 50 and $N_1$ = 25 features in the hidden state. The output of the LSTM layers is flattened and projected through a fully connected layer into the LS, of dimension $N_{LS}$. While the $N_0$ and $N_1$ have been fixed, $N_{LS}$ is left as free parameter since it is the parameter with the most influence on the final reconstruction capacity of the network (see Supp1.2 in the Supplementary Information).\nThe embedded information is then reconstructed through the decoder, effectively reversing the encoding process using a fully connected layer, two concatenated LSTM layers, now with $N_1$ = 25 and $N_0$ = 50 respectively, and a final fully connected layer that is charged to project from the $N_0 \u00d7 N_T$ to the original $N_C \u00d7 N_T$."}, {"title": "E. Comparative Performance Evaluation with Convolutional Autoencoders", "content": "Recent advancements in automatic EEG denoising have leveraged CNNs, achieving state-of-the-art performance. To benchmark our LSTEEG model against existing techniques, we implemented two leading convolutional autoencoders: the UNET architecture [19] and CLEEGN [26]."}, {"title": "1) UNET", "content": "In [19] authors propose to adapt the UNET architecture to the denoising EEG problem. The UNET network is a popular Convolutional Neural Network (CNN) that is commonly used for biomedical image segmentation [25]. It has an AE structure (although not necessarily with information compression) in which the encoder network applies a series of convolution and downsampling operations while the decoder network applies a series of upsampling and convolution operations. For further details on the network implementation and performance on EEG denoising, please refer to [19]."}, {"title": "2) CLEEGN", "content": "The CLEEGN network is a lightweight convolutional autoencoder specifically developed for multi-channel inter-subject EEG reconstruction, that has strong decoding performances on BCI datasets [26]. It encodes and decodes the input information through a series of 1D convolutional filters, both in the channel and time dimensions. However, no compression of information takes place between the encoder and the decoder as the Latent Space has $N_F$ times the dimension of the input, being $N_F$ the number of temporal filters. The reader may refer to the CLEEGN original publication [26] for extended details."}, {"title": "F. Latent Space Exploration", "content": "During the training phase, LSTEEG's encoder learns to project the high-dimensional information in an EEG sample onto a lower-dimensional Latent Space in an unsupervised manner, allowing the model to autonomously learn relevant features and characteristics from the input data, i.e. from expected clean EEG. Therefore, probing the structure and characteristics of the learned LS can provide meaningful insight for data-driven knowledge discovery on the characterization of artefact-free EEG."}, {"title": "1) Latent Space Dimension Activations", "content": "We first aim to understand the activation patterns within the Latent Space to identify which EEG signal components most strongly influence each latent dimension. Specifically, the LSTEEG's encoder projects each EEG epoch ($x_e$) onto the LS through the mapping $x_e \u2192 f_E(x_e) \u2208 R^{N_{LS}}$. In this context we analyze the characteristics of $x_e$ that contribute to an increase or a decrease in the activation values at each dimension $j$ of the LS, $f(x_e)$.\nSpectral features, which characterize the signal's power distribution across different frequency bands, have been widely applied to analyze resting state EEG [36], [37]. Furthermore, the multi-channel nature of our methodology can be leveraged to investigate how spatial distributions of spectral features influence LS activation patterns.\nTherefore, to quantify the relationship between spectral features and LS activation, we calculate the relative band power for the five standard EEG bands (b) previously mentioned (Delta, Theta, Alpha, Beta, Gamma), across each channel (c) and epoch (e): $P_{b,c,e}$. We then construct an the activation map for each dimension in the LS through:\n$S^j_{b,c}= \\sum_{e=1}^{N_e}P_{b,c,e} f^j(x_e)$ (2)\nwhere $S^j_{b,c}$ is the spectral activation for dimension j, band b, and channel c. A topographical map is subsequently generated for each dimension j and band b. Each element's contribution to the activation is weighted by its corresponding LS encoding, $f^j_E(x_e)$, ensuring that the summation highlights patterns that produce the most significant activation values.\nIn this manuscript, we choose to visualize the Most Activated LS Dimensions (MADs). These are the dimensions characterized by the highest cumulative activation across all epochs. The cumulative activation for one dimension of the latent space j can be defined as $A_j = \\sum_{e=1}^{N_e} f^j(x_e)$. Therefore, the set of K-Most Activated Dimensinons (MADs) is defined as:\n$MAD_K = {j_1, j_2, ..., j_K | A_{j_1} \u2265 A_{j_2} \u2265 ... \u2265 A_{j_K}}$ (3)\nwith $j_1, j_2, ..., j_K \u2208 {1, 2, ..., N_{LS}}$. In this manner, we aim to focus the scope of the analysis on the features that most powerfully influence the neural representations within the model."}, {"title": "2) Linear Interpolation in the Latent Space", "content": "Linear interpolation within an autoencoder's LS is a powerful technique to assess the smoothness and generalization capabilities of the learned representations. By generating intermediate representations between two points in the LS and decoding them back to the input space, it is possible to determine whether the LS transitions smoothly and meaningfully between known data points. Smooth transitions are indicative of a model that is well-structured and has learned to generalize well across the data distribution [38], which is potentially beneficial for later downstream tasks and improved interpretability [39]. Finally, a regular and smooth LS allows to generate meaningful synthetic data, as points between actual samples retain realistic characteristics, which can be leveraged to expand datasets through data augmentation.\nWe therefore attempt to verify the regularity of the LS by randomly selecting two epochs ($x_a$ and $x_b$), obtaining their LS projections ($f_E(x_a)$ and $f_E(x_b)$), and sampling intermediate vectors that lie in the segment connecting $f_E(x_a)$ and $f_E(x_b)$.\nIn mathematical terms, let \u03bb represent the interpolation parameter that varies linearly from 0 to 1. To obtain M interpolated vectors we define $\u03bb_m = \\frac{m}{M}, m = 1, 2, . . ., M$.\nThen, the interpolated vectors, denoted as $z_{int}$, can be expressed as:\n$z^{int} = (1 - \u03bb_m) \u00b7 f_E(x_a) + \u03bb_m \u00b7 f_E(x_b)$ (4)\nFinally, these intermediate vectors represent are decoded $y^{int} = f_D(z^{int})$ to visualize them in the EEG space, for comparison with the real EEG samples $x_a$ and $x_b$."}, {"title": "III. RESULTS", "content": ""}, {"title": "A. Artifact Detection", "content": "Figure 2 shows the artifact detection ROCs when applying the four compared networks both on the EOG-synthetically contaminated (left panel) and the LEMON Clean/RawFiltered (right panel) datasets. In both cases the networks have been trained on the pre-processed LEMON dataset. As it can be observed, the AUC values are generally large, always surpassing 0.9, and indicating that the anomaly detection approach classifying noisy against clean epochs works satisfactorily for the implemented autoencoders.\nThe LSTEEG ($N_{LS}$ = 2000) and the UNET networks are the top performers on the Clean/RawFiltered dataset, which is the evaluation dataset most similar to the training one. Meanwhile, on the EOG-Synthetically Contaminated dataset, recorded with a different EEG set up in a different laboratory, the CLEEGN and LSTEEG ($N_{LS}$ = 500, but closely followed by $N_{LS}$ = 2000,) networks perform best.\nThese results demonstrate that the novel anomaly-detection-based EEG artifact rejection method is a powerful algorithm for detecting artifacts in EEG epochs, effectively generalizing to datasets recorded under different conditions from the training data. The four tested networks consistently achieve remarkable performance across both datasets, highlighting the robustness of the method."}, {"title": "B. Artifact Correction", "content": "We use the reconstruction error, i.e. the root mean squared error (RMSE) between the target epoch and the output of each network, as performance evaluation metric. The reconstruction errors averaged over the EEG epochs in the testing set are shown in Table I, for the four implemented networks when trained with the two different datasets discussed in Section II-C.1. The UNET architecture is the outperforming model when training the networks with $X_{Br}$, where we reject all independent components that ICLabel does not classify as brain class with p > 0.9. Our LSTEEG closely follows UNET in terms of performance, while CLEEGN shows both the largest and most variable error.\nIn Figure 3, a comparison of the correction capabilities for an EEG epoch containing a muscular and an ocular artifacts is shown. In the muscular artifact (high frequency burst of activity between 0.5 and 0.75s) one can see the difference in performance between the different networks: CLEEGN produces a denoised signal that is closer to the noisy input than the target signal. Meanwhile, LSTEEG and UNET produce outputs that follow closely the target signal, with the output of LSTEEG being slightly worse due to its smaller amplitude.\nWhen training the implemented models with the more conservative dataset $X_{Ar}$, where only those independent components labeled as artifacts with high probability are rejected, results change significantly. Table I shows the performance of the different models, where CLEEGN achieves the smallest reconstruction error, while UNET and LSTEEG ($N_{LS}$ = 2000) achieve almost identical performance. Additionally, CLEEGN achieves remarkably small variability in its RMSE values over the test set, while the rest of models show standard deviations of the order of 10 \u03bc\u03bd.\nSuch a difference in performance between the two datasets requires further investigation. Therefore, we visualize the EEG epoch previously discussed, now on the $X_{Ar}$ dataset, and compare the reconstruction of the different models in Figure 4. One can easily interpret the quantitative results by inspecting Figure 4: in the $X_{Ar}$ dataset, the \"Target Clean\" signals may be noisier, due to taking a more conservative approach with ICLabel and rejecting less, possibly artifactual, Independent Components. Although the number of epochs with a contaminated target signal may be small, they are enough to degrade the performance of the CLEEGN model, which learns to closely match the target epoch independently of its form or shape. However, the remaining implemented networks seem to be able to have learned to generalize better from the large corpus of clean epochs in $X_{Ar}$, and are able to correct the muscular artifact, thus producing larger RMSE values between the network output and the target epoch.\nFinally, by inspecting the dimensions of each layer's output in the implemented networks, we argue that the previously discussed behavior is a clear result of how feature learning takes place in each network. Hence looking at the dimensions at the output of each of the networks in Table II for a two-second-long input epoch of size $(N_C, N_T) = (19, 500)$, the output dimensions of CLEEGN and UNET are always larger than the input dimension, implying a lack of information compression, which hinders information-rich low-dimensional representations in the Latent Space, and therefore points to the absence of feature embedding and representation learning."}, {"title": "C. Interpretability of the Learned Latent Spa\u0441\u0435", "content": "The latent space is therefore worth studying due to the presence of representation learning achieved in LSTEEG."}, {"title": "1) Spatial Activations of the Latent Space Dimensions", "content": "We highlight the interpretability capabilities of the proposed LSTEEG autoencoder's LS by showcasing the spectral activation maps for the four Most Activated Latent Space Dimensions (MADs) described in equation 3.\nThe activation maps of the MADs shown in Figure 5 highlight some of thefeatures of the autoencoder that have been learned without supervision. One can readily notice that both the Delta and Alpha bands, closely followed by the Theta band, consistently exhibit large influences on the shown MADs, with clearly highlighted patterns. Meanwhile, the faster Gamma band tends to show smoother activation maps of much smaller amplitude.\nThis highlights the effectivity of this method at detecting different patterns of activity in the LS dimensions. The model does not prioritize encoding fast frequency information, instead focusing on encoding lower-frequency characteristics in the LS, an effect further analyzed in the discussion section.\nWhile gamma band information is lost, we can see that each dimension encodes clearly defined spectro-spatial features, such as general delta, alpha and beta, power (0th), mid-central Delta (1st), central delta and alpha (2nd), etc. It is also worth noting that spatial activations are not simplified: dimensions are not specialized for particular patterns or particular band powers. Instead, each dimension accumulates information from different band powers."}, {"title": null, "content": "This methodology underscores the interpretative potential of EEG-based autoencoders. In this study, our developed algorithm offers insights into understanding the fundamental features of clean EEG signals [40], and it holds promise for application on a wider range of datasets to elucidate key characteristics associated with brain-related diseases or cognitive processes."}, {"title": "2) Linear Interpolation in the Latent Space", "content": "The regularity of the learned LS is explored by decoding the sampled points from linearly interpolation between two embedded EEG epochs onto the LS. Figure 6 shows the decoded samples in EEG space.\nThe interpolated points exhibit a smooth transition from sample A to sample B, the decoded interpolation points being confined between the two original samples' amplitudes. These results provide empirical evidence that the learned LSTEEG's LS is smooth, robust and generalizable. These features reveal LSTEEG's potential to be exploited as a data augmentation tool. Interpolating samples between two noisy EEG signals can be useful to generate synthetically augmented and labelled datasets to improve deep learning techniques for automatic artifact detection. Furthermore, the structured LS can be leveraged to generate additional EEG samples from individuals, by interpolating the LS between their real samples, to enlarge datasets aiming to enhance classification algorithms for e.g. brain-related disease diagnosis, patient stratification, emotional recognition, or BCI applications."}, {"title": "IV. DISCUSSION", "content": "In this study", "problems": "artifact detection and artifact correction", "AEs": "CLEEGN and UNET. Our analyses demonstrate that LSTEEG and UNET consistently show similar and powerful denoising performances. Both networks are able to remove different types of noise in EEG epochs such as ocular and muscle artifacts, as well as high amplitude deflections, occasionally caused by movement in the EEG electrodes. Additionally, their correct performance is robust against datasets that contain leaked artifactual activity in the target epochs. In such cases, the CLEEGN network learns to correct the most frequent artifacts, such as eye blinks, but does not successfully remove sporadic artifacts.\nDue to its network architecture and absence of information funnel, CLEEGN does not aim to learn a meaningful representation of EEG signals during training, but instead learns to very accurately transform the input EEG segment into the desired target. It is, therefore, less robust to noise in the training data. Instead, the encoder-decoder structure of LSTEEG and UNET encourages the networks to learn the underlying features characterizing clean EEG recordings, allowing for a more robust noise correction, even when the target epochs contain incorrectly removed artifactual activity from incorrectly classified ICs.\nNonetheless, CLEEGN is an extremely light model configuration, with competitive performance, while our LSTEEG models are significantly larger. We believe that, in addition to the increased robustness against noisy datasets, the enhanced interpretability and smaller dimensionality of LSTEEG's LS justifies the larger size of our novel autoencoder. LSTEEG's LS improves interpretability and extends the applicability of the LSTEEG model to other areas of EEG research. Possible future applications exploiting the low-dimensional embedding include EEG data generation and unsupervised EEG feature learning. The proposed model can be straightforwardly extended into a beta-variational autoencoder, potentially enhancing the smoothness of the latent space to facilitate data generation processes and interpretability by disentangling the dataset's generative factors [38", "39": [41]}]}