{"title": "Web vs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students", "authors": ["Aayush Kumar", "Daniel Prol", "Amin Alipour", "Sruti Srinivasa Ragavan"], "abstract": "LLMs such as ChatGPT have been widely adopted by students in higher education as tools for learning programming and related concepts. However, it remains unclear how effective students are and what strategies students use while learning with LLMs. Since the majority of students' experiences in online self-learning have come through using search engines such as Google, evaluating AI tools in this context can help us address these gaps. In this mixed-methods research, we conducted an exploratory within-subjects study to understand how CS2 students learn programming concepts using both LLMs as well as traditional online methods such as educational websites and videos to examine how students approach learning within and across both scenarios. We discovered that students found it easier to learn a more difficult concept using traditional methods than using ChatGPT. We also found that students ask fewer follow-ups and use more keyword-based queries for search engines while their prompts to LLMs tend to explicitly ask for information.", "sections": [{"title": "1 INTRODUCTION", "content": "The advancement of generative artificial intelligence (AI) has introduced new tools and methods to support CS education. Large Language Models (LLMs) such as ChatGPT have gained popularity for their ability to assist learners through interactive and personalized guidance [2]. These systems can provide high-quality explanations [9, 18], generate examples, and answer questions in real-time, making them a promising resource for learning programming [14]. However, the path to their adoption for self-directed programming learning is replete with challenges: these tools sometimes generate inaccurate or misleading information, lack the ability to understand nuanced learner needs, and potentially foster overreliance on AI instead of critical thinking and independent problem solving [29]. Recent studies also highlight the importance of learners developing metacognitive abilities \u2013 the capacity to recognize what they know, identify gaps in their understanding, and actively address those gaps - to guide their own learning when using LLMs, but these skills might be challenging to grasp for novice learners of programming concepts. [2, 25]. Thus, the research on whether or not to adopt LLMs in learning contexts is divided: some studies such as [2, 4, 11, 14, 22, 32] indicate their promise while others caution against their use [25, 26]. There is also a paucity of empirical studies comparing learning with LLMs against traditional web-based learning (e.g., using video tutorials, documentation, and websites such as Stack Overflow) \u2013 both in terms of quantitative outcomes and qualitative behaviors. As a result, making informed decisions about learning choices becomes hard for both students and educators alike. In this paper, we begin to address this gap by comparing the effectiveness of learning programming concepts with AI vs. traditional web (NoAI) resources. Specifically, we conducted a within-subject lab study at IIT Kanpur, a large engineering university in India, involving 32 participants (CS2 students) learning new topics in programming concepts with LLMs and traditional web resources. We investigated the differences in the strategies learners adopt in both these information-seeking setups, along with their underlying reasons, and the effectiveness of their learning in terms of assessments. Our results suggest that the participants performed better on the post-study evaluations in the NoAI condition. We also observed a statistically significant difference in between the types of prompts in Al and queries under NoAI conditions. Our key contributions are: 1) a comparison of students' information-seeking behavior in learning tasks for two programming concepts"}, {"title": "2 RELATED WORK", "content": "Student-AI Interaction in Programming Education. Prather et al. [22] reviewed the usage of LLMs in computing education, noting their potential to improve learning experiences while cautioning against overreliance [26]. Early experiments with AI-based feedback mechanisms have shown promise in delivering helpful and high-quality feedback [4, 15, 19, 28], although challenges remain to balance automated assistance with learning objectives. Kazemitabaar et al. [14] categorized novice programming strategies into distinct patterns, finding that approaches combining manual coding with AI assistance yielded better results. Amoozadeh et al. [3] report on trust dynamics between students and generative AI, revealing how perceptions influence adoption and outcomes. Overall, this body of literature calls for the deliberate integration of Al in education to facilitate self-regulation and programming skills development. ChatGPT as a Learning Tool. ChatGPT's conversational interface has profoundly impacted programming education, as evidenced by the findings of Sun et al. [32]. Amoozadeh et al. [2] discuss how this interface enables personalized learning experiences and promotes student agency. Studies by Kazemitabaar et al. [14] and Guo et al.[11] highlight ChatGPT's effectiveness in debugging and concept clarification. Although Sun et al. [33] demonstrated that structured prompts improve learning outcomes, Nguyen et al. [20] identified specific barriers in prompt formulation and evaluation of solutions. This duality resonates with Prather et al.'s [23] findings that AI tools accelerate learning for some students while potentially hindering others. While such studies (as listed by Prather et al. [24]) have evaluated the use of LLMs as learning tools, they do not evaluate how learning with LLMs compares to using traditional self-study methods. A notable exception is Qureshi et al.'s study [27] comparing the results of a programming exercise between two groups of students: one that had access to ChatGPT and another that only had access to offline course material. Our study complements Qureshi et al.'s study by comparing ChatGPT with traditional self-study web resources for learning programming concepts. Online Information Seeking in Programming. Research on information seeking in the web has identified fundamental patterns in how developers learn programming online. Brandt et al. [7] identified the pattern of \u201copportunistic programming\" demonstrating how developers adapt online examples to their needs. Sellen et al. [30] characterized online information seeking and gathering behavior, providing a framework to understand search strategies. Ko et al. [16] extended this understanding by cataloging information needs during software development. Upadhyay et al. [34] contributed insights into how interface modality affects the information seeking behavior. These studies help us understand how programmers use and navigate between information sources on the web. Our work corroborates and extends this research by examining how online information-seeking behavior differs when using AI tools, providing insights into how these different approaches support programming learning in different ways."}, {"title": "3 RESEARCH QUESTIONS", "content": "Our specific research questions are:\n(1) Are there any differences, in terms of learning outcomes, between Al and NoAI treatments?\n(2) Are there differences in information seeking strategies in AI and NoAI treatments?"}, {"title": "4 METHODOLOGY", "content": "To answer the above questions, we conducted a user study with 32 participants, comparing their learning of programming concepts with and without AI. The study was approved by the Institutional Ethics Committee at IIT Kanpur, and all studies were conducted by the first author in January-February, 2024."}, {"title": "4.1 Study Design", "content": "Our study used an in-vitro, within-subject design. Each participant learned two different programming concepts, namely currying [12] and immediately invoked function expressions (IIFE) [10] in Javascript. They were instructed to learn one of these tasks using only ChatGPT (GPT 3.5 Turbo) and the other using traditional online resources (e.g web search, videos) starting with a Google search page, without the use of any Generative AI tools. We call these two conditions AI and NoAI respectively. Participants in both conditions had access to the Visual Studio Code (VSCode) IDE to run code if they decided. The task assignment to conditions and task order were balanced."}, {"title": "4.2 Participants and recruitment", "content": "In December 2023, we sent a recruiting email to the students' mailing list at IIT Kanpur inviting students to participate in a 1-hour study. The email included a sign-up form which contained questions about respondents' programming experience, programming courses taken, programming languages known, and generative AI usage. We received 71 responses before closing replies to the form. We then invited 32 respondents: 31 were undergraduate students who had taken CS1 and were eligible for CS2, and 1 was a non-CS graduate student (details of participants' majors are in Table 2) All participants were familiar with programming, and had prior experience using Generative AI (GenAI) tools, but did not know Javascript (Table 1)."}, {"title": "4.3 Study Protocol", "content": "Each study session started with a briefing and informed consent. Participants were then given 15 minutes to to learn the concept (with or without AI) followed by an assessment. They then moved on to the learning and assessment for the second task/condition. Participants were asked to think aloud during the learning task. For the assessment, participants took a conceptual-understanding quiz"}, {"title": "4.4 Tasks and Assessments", "content": "4.4.1 Learning tasks. Participants learnt two programming concepts associated with functional programming. All the pre-requisites for understanding these concepts (such as functions and variable scopes) were part of a mandatory CS1 course that all participants had taken. For each task, participants were provided with the following learning objectives to guide their learning: \"You should be able to answer the following questions through your learning: What is <concept> in programming? Why is <concept> used in programming? How is <concept> implemented in programming languages?\". This was to prevent participants from being derailed by the historical, mathematical or programming language implementation aspects of the concepts, as was the case in pilot studies. Since Javascript was the language used in our study, we \"recommended\" that participants \"use JavaScript as the medium to understand the concepts\".\n4.4.2 Assessment. We evaluated participants' learning of each task using a two-part assessment. The first part assessed conceptual understanding via a 5-question quiz, comprising of 4 true/false questions related to the syntax and purpose of the concept and 1 multiple choice question about the benefits of applying the concept. All questions also had a \"don't know\" option and participants were instructed before the quiz to avoid guessing and choose this option if they did not know the answer to a particular question. The second part of the assessment involved participants debugging and fixing a semantic error in a Javascript program to work according to the stated specification. The code only contained rudimentary Javascript structures and did not contain Javascript-specific idioms. A patch could contain up to five lines of code. We set a 10-minute time limit for this task based on pilot studies."}, {"title": "4.5 Data Collection and Analysis", "content": "4.5.1 Participants' artifacts. We gathered participants' solutions to the two assessments (i.e., quiz and debugging task) and scored them for further analysis.\nFor the quiz, each true/false question was awarded +1 or -1 depending on the correct / incorrect answer, while the multiple choice question (which contained multiple correct answers) was scored as +1, -1 based on all correct and incorrect answers respectively. A \"don't know\" response for any question received a score of 0. Thus, the possible quiz scores ranged from -8 to +8, with incorrect understanding carrying a greater penalty than \"I don't know\".\nFor the debugging task, we assigned three possible scores: 1) score=0, if the final code failed to produce desired output, 2) score=1 if the final code produced the desired output and the participant was able to correctly describe their bug fix, and 3) score=0.5, if the code produced the desired output but the participant was unable to explain their solution. We refer to these outcomes as Incorrect, Correct and Correct (LoU), where LoU stands for lack of understanding."}, {"title": "4.5.2 Task videos.", "content": "We recorded participants' video, audio and screen actions during the study. We qualitatively analyzed this data in two parts. First, we performed activity analysis to understand the time participants spent on different learning activities (Table 3). For this, we coded the videos in 30-second segments, with one or more codes per segment. Our codes consisted of programming, information seeking, and information learning [31], where learning was divided into theoretical learning and learning by example [7]. We also recorded the number of videos watched and websites visited by each participant. Second, we performed a qualitative analysis of search queries and LLM prompts to understand their nature (Table 4). We collected all queries and prompts along with their timings. We categorized them across three dimensions: follow-up, origin, and phrasing. As web searches often lead to follow-up queries [31], this analysis helps us examine if and how LLM prompts lead to follow-ups. The taxonomy for query phrasing has been adapted from Bolotova et al.'s [6] taxonomy for questions. We extended this taxonomy to queries not expressed as questions by adding two categories - Statement and Explanatory."}, {"title": "4.5.3 Reliability.", "content": "Two authors coded 4 out of 32 (>10%) videos independently to design an initial codebook. The interrater reliability was 87% on the Jaccard similarity index for activity analysis (Table 3) and was 89%, 83% and 81% on Cohen's Kappa [8] for the dimensions of origin, follow-up, and phrasing, respectively, in the query/prompt analysis (Table 4). We used the Jaccard index for activity analysis as the codes were not mutually exclusive and multiple code assignments per segment were allowed. Following this, the remaining videos were divided and coded by one of the two coders based on the agreed upon codebook."}, {"title": "5 RESULTS", "content": "5.1 RQ1: Post-study assessment results\n5.1.1 Debugging Task Performance. Figure 1 gives an overview of the results of the debugging task across tasks and treatments. The paired t-test did not find significant differences across the dimensions of concept learned (p-value = .08), or learning treatment (p-value = .57). We also did not find any significant differences on applying the Mann-Whitney U-test across the dimensions of learning treatment when considering the debugging task performance for currying (p-value = .53) and IIFE (p-value = .88). 5.1.2 Quiz Performance. Figure 2 presents the distribution of quiz scores for each concept and learning treatment, scored on a scale of [-8, +8]. We found that the quiz scores for currying (M = 0.66, SD = 2.52) were significantly lower (p = 0.0002) than IIFE (M = 2.69, SD = 2.10). The effect size, measured by Cohen's d, was d = -0.86, indicating a large effect. It also took participants more time to complete the currying quiz compared to IIFE (163.1 vs 97.21 seconds, on average, paired t-test p-value << .05). Across all participants and questions in the quiz, participants chose the \"don't know\" option 31 times (15.9% of all options chosen) in the quiz for currying, and 29 times (13.5% of all options chosen) in the quiz for IIFE. This may suggest that currying is a more difficult topic for participants to learn in the study condition than IIFE in general, or that the post-study quiz in currying task was harder than in IIFE. As a result, we compare the NoAI and AI conditions for each task."}, {"title": "AI vs. NoAI in Currying.", "content": "Our sample size was small (N = 16) and the currying quiz scores for the AI treatment were non-normal [Shapiro-Wilk (W = 0.84, p = 0.01)]. Therefore, we conducted a Mann-Whitney U-test and found statistical differences between the mean quiz scores in the AI and NoAI treatments (-0.69 vs. 2.0, p=.005). The effect size was large (r = 0.50).\nAI vs. NoAI in IIFE. In contrast, we did not find any significant difference in the IIFE quiz scores between the AI and NoAI conditions (p = 0.34). Here, we used an independent sample t-test since Shapiro-Wilk tests suggested normal distributions of IIFE quiz scores in AI (W = 0.92, p=.20) and NoAI (W = 0.93, p = .28) treatments.\nObservations: Participants learning with Al performed significantly worse than those learning through web search in the theoretical quiz for currying."}, {"title": "5.2 RQ2: Learning strategies", "content": "5.2.1 Activity Analysis. We qualitatively coded participant activities according to Table 3, and a distribution of the time they spent on various activities is presented in Table 6. In the NoAI treatment,"}, {"title": "5.2.2 Query Analysis.", "content": "Participants wrote a total of 148 search queries (Google + YouTube) in the NoAI treatment and a total of 237 prompts to ChatGPT. We coded these queries based on the classification scheme in Table 4. Table 5 lists these code occurrences in the web and ChatGPT conditions.\nParticipants began their learning episodes based on the learning objectives provided to them. Nearly half of all participants (15 out 32) used them directly as the starting point for prompting in the AI condition, in contrast to only 5 out of 32 in web search (NoAI). Most (18 out of 32) participants used keywords based on the learning objectives as the starting point for their web information foraging (code: Statement).\nAs participants progressed in their learning, querying differences surfaced again. Al prompts were often follow-ups to previous prompts, in contrast to those in web searches ($x^2$ (1, N = 385) = 6.93, p = .008). At the same time, a higher percentage of web queries were copied directly, from sources such as prior websites they had visited (Table 5). These differences in information seeking behaviors could be considered adaptations to the environment [21]. For example, ChatGPT offered byte-sized answers to specific questions and afforded easy follow-up questions in a more chatty manner, whereas web searches offered a longer list of diverse, potentially useful sources that participants had to sift through and choose to learn from. Drilling further into the differences in learning behaviors between the AI and NoAI environments, we evaluated the effects of learning environment and phrasing type (Table 4; query phrasing) on the number of queries/prompts issued by participants. A two-way ANOVA of the number of queries (without replication, with repeated measures for each participant) revealed significant effects"}, {"title": "6 THREATS TO VALIDITY", "content": "Internal. We randomized the order of tasks and treatments to minimize learning effects. We had two tasks with different difficulty levels and this stimulated diversity in learning tasks. Construct. We used quizzes and debugging tasks to evaluate participant learning, which are insufficient. However, we validated our questions and the debugging task in pilot studies to evaluate participants' knowledge on key concepts in the learning goals. External. Our results are based on a small sample size relative to the general student population. IIT Kanpur has a female:male ratio of 20%, and despite our best efforts we could not recruit a gender balanced sample. Only 20% of our participants were female. Replication of this study with a gender-balanced sample is needed to generalize our results to other environments. We also need further studies to generalize our findings to other kinds of learning tasks and situations (e.g., concepts in computer architecture or usability)."}, {"title": "7 DISCUSSION AND CONCLUDING REMARKS", "content": "In this paper, we provide a comparative study on how students used ChatGPT and Google to find material and learn new topics in programming in a self-guided manner. As ChatGPT and similar Al tools gain popularity, examining them as learning tools in the context of traditional online learning methods helps us gain insights into learners' interactions with them.\nPerform Fast and Learn Slow. ChatGPT and other LLMs can improve students' productivity in performing programming tasks [14], but their effectiveness in helping students learn and understand new concepts in unclear. In our study, while AI helped participants slightly more than web search in successfully debugging programs, a notable number of participants in the AI condition lacked understanding of their proposed fixes (Figure 1). Similarly, participants in the AI treatment performed poorly in the quiz for currying, the more difficult task, as compared to participants in the NoAI treatment, suggesting that the nature of productivity may be at odds with learning. While productivity emphasizes on the speed in completion of tasks, successful learning can be slow and tedious, and ChatGPT might not be the best tool for learning, especially in the absence of sufficient metacognitive abilities [5, 23].\nImportance of Example-based learning. Learning theories have emphasized the importance of examples in learning. Examples can provide learning opportunities for learners to imitate experts, discern analogies and contrast among different problems. In our study, participants across treatments showed a tendency to learn through examples - participants in the AI treatment spent 51% of their time learning by example while those in the NoAI treatment spent 40% of the time learning by example and an additional 10% learning through videos. This emphasizes the importance of incorporating high-quality examples in pedagogy.\nNeed for Holistic LLM Responses when Learning. Participants in our study tended to write keyword-based queries when using search engines, which led them to websites containing human-curated content about programming concepts. While using LLMs, participants wrote more questions and explicitly asked for information in their prompts. Since LLMs reply directly to these prompts, their responses might only cover the portion of information about the concept that is asked for in the prompt. Our results suggest that students are more likely to ask follow-ups and thus dive deeper into areas they have already explored previously while using LLMs. This might lead learners down a rabbit hole, preventing them from gaining a more complete understanding of the concept, thus explaining participants' poorer performance in Al condition for the harder task. Future research can explore how LLMs can understand student intent and provide more holistic responses rather than generating direct responses to prompts for exploratory learning. One approach would be to draw inspiration directly from web searches that afford learning from multiple perspectives by utilizing multiple LLM responses. In our study, while using web search, participants often visited more than one website for one search query (211 websites + videos, from 148 search queries).\nText is NOT enough! Modern LLMs generate high-quality textual responses to user prompts. They can provide examples as well as high-quality explanations of these examples [18]. However, participants in the NoAI treatment also expressed interest in non-textual learning resources, notably videos. Some participants chose to watch videos as they are more dynamic than text-based responses, and allow them to better understand the process behind using the concept. Others cited them to be more human. These preferences are consistent with prior work by Jackman et al. [13] on the benefits of YouTube as a tool to demystify complex concepts. Thus, when choosing generative AI tools as learning resources in educational settings, it is imperative to choose ones that allow for heterogenous"}, {"title": "content types for different learning preferences and styles.", "content": "For example, [1] can augment its generated textual content with links to video resources. However, there are opportunities for research and development in multimodal language models that can help in creating more effective learning content (e.g., videos, interactive visualizations) and human-like experiences for diverse subjects and learners[17]."}]}