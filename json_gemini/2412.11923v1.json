{"title": "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named Entity Detection", "authors": ["Sepideh Mamooler", "Syrielle Montariol", "Alexander Mathis", "Antoine Bosselut"], "abstract": "In-context learning (ICL) enables Large Language Models (LLMs) to perform tasks using few demonstrations, facilitating task adaptation when labeled examples are hard to obtain. However, ICL is sensitive to the choice of demonstrations, and it remains unclear which demonstration attributes enable in-context generalization. In this work, we conduct a perturbation study of in-context demonstrations for low-resource Named Entity Detection (NED). Our surprising finding is that in-context demonstrations with partially correct annotated entity mentions can be as effective for task transfer as fully correct demonstrations.\nBased off our findings, we propose Pseudo-annotated In-Context Learning (PICLe), a framework for in-context learning with noisy, pseudo-annotated demonstrations. PICLe leverages LLMs to annotate many demonstrations in a zero-shot first pass. We then cluster these synthetic demonstrations, sample specific sets of in-context demonstrations from each cluster, and predict entity mentions using each set independently. Finally, we use self-verification to select the final set of entity mentions. We evaluate PICLe on five biomedical NED datasets and show that, with zero human annotation, PICLe outperforms ICL in low-resource settings where limited gold examples can be used as in-context demonstrations.", "sections": [{"title": "1 Introduction", "content": "With in-context learning (ICL), Large Language Models (LLMs) can be adapted to perform many tasks using few demonstrations (Brown et al., 2020; Dong et al., 2022; Srivastava et al., 2023; Ye et al., 2023). This emergent property of LLMs is particularly beneficial in tasks where limited supervision data is available for fine-tuning models, such as in specialized domains where only expensive expert annotations can be relied upon to produce quality data (e.g., biomedical, clinical, and legal domains, among many others), and in situations where in-house proprietary datasets must be compiled with few available experts to perform the annotation.\nDespite its promise in these settings, ICL is highly sensitive to the choice of the demonstrations (Wang et al., 2024; Li and Qiu, 2023; Liu et al., 2021), and it remains unclear which characteristics of demonstrations are critical for successful task adaptation. Consequently, prior work has explored which demonstration characteristics lead to successful task adaptation in ICL (Min et al., 2022; Yoo et al., 2022; Wei et al., 2023), but these studies have focused on scalar-output tasks with a limited, predefined label space, such as classification. As a result, demonstration characteristics that optimize performance remain unclear for tasks that require structured, open-ended prediction such as Named Entity Detection (NED).\nIn NED, the goal is to identify all mentions of a specific type of entity within a given query, producing a structured output, with a label space effectively bounded only by the number of domain entities. In this work, we focus on NED given its high number of use cases (Navarro et al., 2023; Skylaki et al., 2020; Ushio et al., 2022), particularly in specialized domains where effective annotation is challenging, as (1) it requires considerable domain expertise, and (2) entities can change over time, introducing distribution shifts in supervised datasets.\nWe conduct a thorough analysis of demonstration properties that impact in-context adaptation in NED. First, we analyze the importance of the context-label correspondence of in-context demonstrations by introducing noise through various perturbations that preserve different aspects of this mapping. Second, we investigate the effects of partial correctness in demonstrations. In NED, partial correctness refers to annotations that differ from the complete list of entity mentions in the input query"}, {"title": "2 Related Work", "content": "What matters in in-context learning? In-context learning is remarkably effective for performing various NLP tasks with only a few task demonstrations appended to the prompt (Brown et al., 2020). However, despite a large body of work on designing novel in-context learning methods (e.g., Gao et al., 2021; Sorensen et al., 2022; Mishra et al., 2022), it is not yet fully understood what makes in-context learning effective, with multiple works demonstrating surprising variables, such as the impact of the demonstration order (Lu et al., 2022), the term frequencies of test examples in pretraining data (Razeghi et al., 2022), and basic output calibration (Zhao et al., 2021; Fei et al., 2023; Jiang et al., 2023b). Consequently, recent works explore how demonstration components might be separately responsible for in-context transfer. Min et al. (2022) show that in-context demonstrations serve to show the label space of demonstrations, the distribution of their input text, and their overall format. However, Yoo et al. (2022) perform quantifiable analysis on the impact of ground-truth label demonstrations on a larger set of tasks and datasets and find that ground-truth labels have substantial impacts on ICL performance. Wei et al. (2023) continue this line of work and show that the degree to which the label mapping influences task transfer depends on the scale of the model, and that smaller models are more capable of ignoring misaligned label mappings. Wang et al. (2023a) show similar results for CoT reasoning, finding that CoT is also possible without valid demonstrations, and that demonstrations that are relevant to the query and have the correct order of reasoning steps are more important for effective transfer.\nHowever, these works focus on classification tasks, which lack the concept of partial correctness; a label is either fully correct or entirely incorrect. In token-level tasks like NED, however, the list of annotated entities can be partially correct. We show that partially correct demonstrations can perform as effectively as fully correct ones a result not addressed by these prior works. Furthermore, contrary to Min et al. (2022)'s findings for classification tasks, we show that ICL demonstrations with fully incorrect labels are not effective in NED.\nPseudo-annotation. Pseudo-annotation is a popular semi-supervised learning method in many domains (Yang et al., 2022; Ye et al., 2024). It has recently been used for various NLP tasks to generate demonstrations for ICL (Wan et al., 2023a,b) and fine-tuning LLMs (Huang et al., 2023; Honovich et al., 2023; Wang et al., 2023b). Demonstrations'"}, {"title": "3 Experimental setup", "content": "The task of Named Entity Detection (NED) requires detecting all mentions of entities in a text. We formulate the task such that the language model is given a passage of text as part of a prompt and must predict the list of entities that are mentioned in the passage. Optionally, in few-shot settings (i.e., in-context learning), the prompt also contains several demonstrations, each including an example passage and a corresponding list of mentioned entities in the passage.\nDatasets. We consider five biomedical NED datasets with rich and comprehensive collections of diverse specialized entity types ChemProt contains annotations for extracting chemical compounds (drugs) and gene and protein-related objects (Taboureau et al., 2010). Originally, each sample of this dataset is a paragraph, but we split these paragraphs into sentences. We construct two datasets from ChemProt: ChemProt-Chem and ChemProt-Gene, for detecting chemicals and genes, respectively. BC5CDR contains biomedical abstracts annotated for chemical and disease extraction (Li et al., 2016). Similar to ChemProt, we conduct our experiments on two sub-portions, BC5-Chem and BC5-Disease. Finally, BC2GM contains biomedical abstracts annotated for the extraction of genes, proteins, and related entities (Smith et al., 2008).\nModels. We use three LLMs in our experiments: the proprietary GPT-3.5-Turbo model, and the open-source Mistral-7b-instruct (Jiang et al., 2023a) and Llama-2-7b-Chat (Touvron et al., 2023) models. In the remainder of the paper, we refer to them as Mistral and Llama2, respectively."}, {"title": "4 Do we need gold demonstrations?", "content": "In this section, we analyze which components of ICL demonstrations are critical for task transfer by studying the effect of fully incorrect (Section 4.1) and partially incorrect (Section 4.2) demonstrations in the in-context prompt. In all analyses, we use kNN demonstration retrieval (Liu et al., 2022).\n4.1 Input-output correspondence of in-context demonstrations\nPrior research shows that correct demonstrations are not imperative for priming models in classification tasks (Lyu et al., 2023), and that incorrect demonstrations are sufficient to show desired in-context transfer behavior, including domain relevance and annotation format.\nIn our analysis, we investigate essential demonstration attributes for successful in-context task transfer in NED by designing various corruption schemes, each targeting specific demonstration aspects. We compare performance under these corruptions to zero-shot prediction (No Demo) and standard ICL (Gold Label). We compare all settings using the same test samples and instructions prepended to the prompt.\nRandom ID Label: We replace ground-truth entity labels with random in-distribution entities. For each input sentence, every entity in the ground-truth annotation is replaced by an in-distribution (ID) entity randomly sampled from all labels in training samples of the dataset.\nRandom OOD Label: We replace entity labels in the ground-truth demonstrations with a random out-of-distribution (OOD) English word.\nCorrupted OOD Text: We replace the entity mentions in the text with random OOD English words.\nCorrupted OOD Text and Label: Similar to Corrupted OOD Text, but we replace ground-truth labels as well, such that the entities in the text and label match.\nCorrupted and Shuffled OOD Text, Corrupted and Shuffled OOD Text and Label: Same as their non-shuffled counterpart, but with randomly shuffling the words of the sentence.\nResults When looking at Mistral's performance averaged over all datasets, we find as expected that demonstrations with gold annotations consistently improve the performance over no demonstration"}, {"title": "4.2 Partially Correct ICL demonstrations", "content": "To further investigate the findings above, we perform a second study where we perturb demonstrations by modifying the context-label correspondence in a controlled manner. Specifically, we vary the correctness of the gold labels by applying different heuristic perturbations to the gold entity labels according to a perturbation factor $p \\in \\{0.1, 0.2, ...0.9\\}$:\nDeletion: each entity in the ground-truth annotation is deleted with probability $p$.\nSubstitution: each entity in the ground-truth annotation is substituted with a random in-distribution entity chosen from the dataset's label space with probability $p$.\nAddition and Substitution: for each entity in the ground-truth, an entity chosen randomly from the dataset's label space is added to the annotation with probability $p$; additionally, each ground-truth entity is substituted with a random entity from the same label space with probability $p$.\nDeletion and Substitution: each entity in the ground-truth is removed with probability $p$. The remaining entities are substituted with a random entity from the dataset with probability $p$.\nFollowing these perturbations, we report the precision, recall, and F1 score of the perturbed demonstrations (evaluated based off the initial gold demonstration labels) against the F1 score of downstream predictions for test samples that contain at least one entity in their gold annotations.\nResults Demonstrations subject to different perturbations may have similar demonstration F1"}, {"title": "5 In-context NED with pseudo-annotated demonstrations", "content": "In this section, we propose PICLe, a framework for pseudo-annotating unlabeled samples that can be leveraged for in-context learning. This framework consists of two stages In the first stage, we start with a set of unlabeled samples and prompt the model in a zero-shot pass to extract entities in each sample. Then, we improve the quality of these pseudo-annotations by prompting the model to verify each predicted entity (i.e., self-verification; Weng et al., 2023), and filter entities that are not of the correct entity type. We use k-means clustering to group the pseudo-annotated samples into $K$ clusters based off the embedding of their text and pseudo-annotations. Each cluster is used as an individual pool of demonstrations for the downstream NED task. In the second stage, we prompt the model $K$ times, each time choosing the demonstrations from one cluster of pseudo-annotated samples (a sampling method we refer to as Sp-k-means, i.e., Specialized k-means). Then, for each entity in the $K$ lists of predictions, we perform a self-verification step to verify if the entity has the correct type or not, and retain the extracted entities that have the correct entity type. In all of our experiments, we pseudo-annotate 1000 samples from the training set of the datasets with greedy decoding.\nPICLe performance We evaluate PICLe on the same biomedical NED datasets used for our analysis in Section 4 and compare PICLe's performance with standard ICL using gold demonstrations sampled from different demonstration pool sizes, representing various degrees of annotation scarcity. For baselines that use gold annotations as in-context examples, we initially sample demonstration pools of size $N$ from the full training set of each dataset, which range in size from 4.5K to 12.5K examples (Table 1). In scarce annotation settings, we then"}, {"title": "6 Conclusion", "content": "In this work, we study the demonstration attributes that enable in-context generalization for named entity detection. We find that the context-label semantic correspondence is crucial for effective in-context NED, and without this correspondence, in-context examples hurt performance, pushing it below zero-shot NED. However, our analysis demonstrates that partially correct demonstration label sets are just as effective as gold label sets, provided a sufficient number of correct label mappings are found in the demonstration. Based off these findings, we design an ICL framework, PICLe, for named entity detection that leverages LLMs to produce pseudo-annotated examples that can be used for in-context transfer. Our results on five biomedical NED datasets demonstrate that PICLe is more effective than zero-shot prediction and outperforms in-context learning with gold demonstrations when gold demonstrations are scarce."}, {"title": "7 Limitations", "content": "Single Task. This work introduces a method to alleviate annotation effort for named entity detection (NED) while achieving comparable performance to few-shot NED with human-labeled annotations. While this pipeline could be generalized to other tasks besides NED, the experiments presented in this paper are limited to this particular task. However, we demonstrate its effectiveness over a broad set of entity types. Similarly, further work is needed to generalize our conclusions on the partial correctness of demonstrations to all structured output tasks.\nSensitive applications. We apply our system to documents from the biomedical domain. The evaluation sets are drawn from abstracts from published articles. However, the tools we develop can be used to extract the same type of entities in sensitive documents. Our tools were not tested for these applications, and practitioners should be aware that performance on such different types of documents is not guaranteed to transfer.\nAnnotation bias. Annotated data can contain various forms of annotation bias, which lead trained models to make biased predictions when labeling entities based on the knowledge and beliefs of the annotators. This bias is usually alleviated following common annotation practices such as computing inter-rater agreement and having detailed annotation guidelines discussed with the annotators. However PICLe only uses models' pseudo-annotations, since we focus on domains for which expert annotation is challenging to obtain. Consequently, given the lack of interpretability and training data openness of the used LLMs, we cannot assess the reliability and fairness of the demonstrations."}, {"title": "A Reproducibility statement", "content": "Code. We plan to share the code for PICLe and all of our experiments. The decoding temperature is specified for each experiment in their corresponding section. For experiments with non-zero temperature, we use top_p=1, and max_tokens=512 in all experiments. All models were run on a single NVIDIA A100 GPU with 80 GB Memory, each inference run taking between 5 and 20 minutes depending on the dataset.\nData. The datasets we use are publicly available on the Huggingface platform.\nModels. As described in Section 3, we use two open-source models for our studies whose checkpoints can be found in the Huggingface model library: Mistral-7b-instruct and Llama-2-7b-Chat. We also conduct experiments using a proprietary LLM from OpenAI, gpt-3.5-turbo-0125, which unfortunately is subject to be updated (or removed from the API entirely) at any moment, limiting the long-term reproducibility of the results obtained with this tool. For supervised fine-tuning, we use the text encoder BiomedNLP-BiomedBERT-large.\nRandom seeds We repeat all of our experiments that involve randomization 5 times with the following seeds: 12345, 24690, 37035, 49380, 61725.\nPrompts Examples of the prompts used for the self-verification pass and NED are shown in Figures 9 and 10 respectively."}, {"title": "B Additional analysis for ICL demonstration", "content": "B.1 Corrupted random demonstrations\nIn addition to the perturbation schemes defined in the main body of the paper, we experiment with two additional methods:\nRandom OOD Label from Text: We replace ground-truth entity labels with words randomly selected from the sample's text that are not included in the ground-truth annotation (i.e., not a target entity).\nSwapped ID Labels: We swap entity labels in the ground-truth demonstrations with the entity labels of a randomly chosen sample in the training split. Contrary to Random ID Label where the number of entities is preserved, the number of entities in each annotation changes compared to the original ground-truth.\nFigure 5 shows results per dataset for all corruption schemes, with GPT-3.5-Turbo and Mistral models. Corrupted (and Shuffled) OOD Text (and Labels) consistently outperform zero-shot performance across all datasets."}, {"title": "B.2 Partially correct demonstrations", "content": "Figure 6 shows the evolution of the downstream F1 score depending on the number of entities in the demonstrations and the perturbation factor. As expected, an increased perturbation factor leads to a lower demonstration F1 and a lower downstream F1 (right side of the figure). Similarly, adding or removing entities in the demonstration labels leads to a lower downstream F1. However, with the same perturbation factor, perturbations that do not decrease the number of entities in the demonstration (Substitution and Addition and Substitution) lead to a much softer rate of performance loss. Similarly, to reach the same downstream performance as zero-shot (around 0.5 on average), removing one entity is enough, while at least two entities need to be added. This result supports the hypothesis that a way to increase downstream performance is to give preference to a higher recall and number of entities in the demonstration set. Figure 7 compares the precision and recall of demonstrations against the precision and recall of predictions."}, {"title": "C Additional results for PICLe", "content": "Ablation study We ablate each component of PICLe's pseudo-annotation and inference steps in Table 7. These results show the importance of the self-verification step in pseudo-annotation (row #3 vs #8), and inference (row #7 vs #8) for the downstream precision. Additionally, we observe that our proposed Sp-k-means leads to a higher recall and F1 compared to other demonstration sampling methods Results with different models Table 8 evaluates PICLe with different base language models for"}]}