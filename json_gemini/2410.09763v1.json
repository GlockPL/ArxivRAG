{"title": "EEG-based AI-BCI Wheelchair Advancement: A Brain-Computer Interfacing Wheelchair System Using Machine Learning Mechanism with Right and Left Voluntary Hand Movement", "authors": ["Biplov Paneru", "Bishwash Paneru", "Khem Narayan Poudyal"], "abstract": "This paper presents an Artificial Intelligence (AI) integrated novel approach to Brain-Computer Interface (BCI)-based wheelchair development, utilizing a voluntary Right Left Hand Movement mechanism for control. The system is designed to simulate wheelchair navigation based on voluntary right and left-hand movements using electroencephalogram (EEG) data. A pre-filtered dataset, obtained from an open-source EEG repository, was segmented into arrays of 19x200 to capture the onset of hand movements. The data was acquired at a sampling frequency 200Hz in the laboratory experiment. The system integrates a Tkinter-based interface for simulating wheelchair movements, offering users a functional and intuitive control system. Various machine learning models, including Support Vector Machines (SVM), XGBoost, random forest, and a Bi-directional Long Short-Term Memory (Bi-LSTM) attention-based model, were developed. The random forest model obtained 79% accuracy. Great performance was seen on the Logistic Regression model which outperforms other models with 92% accuracy and 91% accuracy on the Multi-Layer Perceptron (MLP) model. The Bi-LSTM attention-based model achieved a mean accuracy of 86% through cross-validation, showcasing the potential of attention mechanisms in BCI applications.", "sections": [{"title": "1. Introduction", "content": "Brain-Computer Interfaces (BCIs) represent a cutting-edge technology that facilitates direct communication between the human brain and external devices. In recent years, BCIs have been widely explored for assisting individuals with mobility impairments. This paper focuses on a novel BCI-based wheelchair control system that leverages EEG signals associated with voluntary hand movements. The system incorporates various machine learning models with various optimization techniques for hyper-parameter tuning and finally, shows an attention mechanism for enhancing the performance of Bi-directional Long Short-Term Memory (Bi-LSTM) networks, which are employed for EEG signal classification. To integrate the brain-computer interface (BCI) for the wheelchair, an analysis of brain activity is necessary-based on modern technology. The signs of brain activity can be obtained using a variety of techniques [1]. In order to help people with severe disabilities live their daily lives, new aids, gadgets, and assistive technologies are required, as demonstrated by the pandemic emergency of the coronavirus illness 2019 (COVID-19). Brain-Computer Interfaces (BCIs) that use electroencephalography (EEG) can help people who experience major health issues become more independent and participate in activities more easily. This can improve their general well-being and prevent deficits [2]. Because brain-computer interfaces allow users to manage a variety of devices without moving their limbs, they can have a significant positive impact on the lives of elderly or crippled persons [4]. Biopotentials generated by electrochemical processes in the human body have been extensively studied for their potential to be used in the control of human-machine interfaces (HMIs), such as wheelchairs with motors. Nevertheless, a lot of research in this area occasionally ignores important aspects, such as the demands of special users, who frequently lack the muscle mass and strength and paresis required to maneuver a wheelchair [5]. Recent years have seen the use of intelligent wheelchair technology to treat a number of mobility issues. Brain-computer interface (BCI) techniques are currently being explored in the development of electric wheelchairs. Because of its adaptability, using human brain control in wheelchairs for disabled individuals has garnered a lot of attention [7]. The majority of brain-computer interfaces (BCIs) on the market today are made to operate a single assistive technology, like a robotic arm, prosthetic limb, or wheelchair. Nevertheless, a lot of daily jobs call for a combination of capabilities that can only be achieved by combining many robotic devices [8]. The application of brain-computer interface (BCI) in rehabilitation has grown significantly. The brainwave-based wheelchair control systems, which are still at the research stage, have a number of drawbacks, such as a lack of attention to mental activity, complexity in neural behavior under various settings, and decreased precision [9]. The subject of biomedical engineering is always evolving, and as a result, designers of newer and more sophisticated BCI (brain-computer interface) systems must always explore for and discover new and inventive ways to create their systems [10]. Electroencephalography (EEG) is expected to have a significant impact on our lives. Research on brain-computer interface (BCI) and EEG-based health diagnostics of brain disorders is expanding. Nevertheless, these methods are utterly inadequate for designing an automated detection system and utilizing it within the BCI framework [15]. The brain-computer interface wheelchair design for individuals with physical disabilities is showcased. The electroencephalographic (EEG) data are received, processed, and classified as the foundation for the construction of the proposed system, which is subsequently used to control the wheelchair. Several experimental brain activity assessments have been conducted with wheelchairs controlled by human commands [16]. The incorporation of brain-computer interfaces (BCIs) into smart wheelchair (SW) technology represents a significant advancement in augmenting the mobility and self-sufficiency of persons with physical limitations. Direct brain-to-external device communication is made possible by BCI technology. While BCI systems present incredible potential for improving human-computer interaction and giving disabled people mobility solutions, they also bring up serious security, privacy, and safety issues that have not been fully explored by large-scale research [17]. Technologies known as brain-computer interfaces (BCIs) are designed to create a link between the human brain and other electronic systems. The potential of BCI technology has been shown by earlier studies employing non-invasive BCI to operate real-world things like wheelchairs and quadcopters as well as virtual objects like computer cursors and virtual helicopters [18]. Brain-machine interfaces, or BCIs, are gadgets that enable direct brain-to-machine communication. People with disabilities can make the most of this technology to increase their level of independence and to locate objects in their surroundings. Electroencephalography (EEG) is a non-invasive method of measuring information from the cortex that can be used to create such devices [19]. By deploying intelligent systems and minimizing human intervention, the marriage of artificial intelligence (AI) with signal processing is transforming the robotics and automation sector. The ability to read human brain signals using electroencephalography (EEG) has opened up new avenues for study into brain-computer interface (BCI) automation of machinery [22].\nThe goal of this paper is to present the development of a brainwave-controlled wheelchair, which will allow users to operate it solely with mental activity rather than requiring physical input. A Brain-Computer Interface (BCI), which enables communication between the brain and the wheelchair, is used in this study. Brainwaves, or electroencephalogram (EEG) data, are recorded using the Emotiv EPOC headset and sent wirelessly to a PC via Bluetooth. These impulses are processed by the PC program, which then interprets them as mental commands (e.g., forward, left). The wheelchair can then move in accordance with the user's thoughts thanks to the wireless transmission of these commands [1]. Modern EEG-based BCI applications, especially those that use motor-imagery (MI) data, are presented in this systematic review with regard to wheelchair control and mobility. It provides a comprehensive overview of the many research carried out since 2010, with an emphasis on the approaches employed for algorithm analysis, features extraction, features selection, and classification, as well as wheelchair components and performance assessment. The findings presented in this work have the potential to draw attention to novel research areas and draw attention to the shortcomings of the biological instruments currently used to treat individuals with severe disabilities [2]. The goal of this research is to create a smart wheelchair that is operated by the EEG signals of people with disabilities. It makes use of non-invasive Brain-Computer Interface (BCI) technology, in which brain impulses are recorded by a Brainsense EEG headset and then sent over Bluetooth to a computer. The wheelchair is controlled by orders derived from eye blinks and other artifacts. The wheelchair's motors are controlled by an Arduino, which receives commands from the system via MATLAB. With this configuration, disabled people can maneuver their wheelchair in various directions without the assistance of a caregiver [3]. This study describes the creation of an affordable electric wheelchair with a Brain-Computer Interface (BCI) for people who are seriously injured or paralyzed. This wheelchair is operated by EEG signals, specifically attention and blink detection, which are recorded by a wearable Neurosky MindWave Mobile device. This is in contrast to commercially available electric wheelchairs, which are costly and depend on joysticks or voice commands. With the system's \"Destination Mapping,\" the wheelchair generates a virtual map and uses EEG commands to navigate autonomously to specified areas. The wheelchair provides a practical and reasonably priced alternative, particularly for the elderly and disabled in underdeveloped nations [4]. This study suggests a low-cost manual-to-powered wheelchair conversion kit that runs on an LSTM network-based hybrid control system that uses EEG and EMG inputs. To identify user attention and navigation goals, it makes use of an inexpensive EEG headset and an EMG armband. After training using the LSTM network, the system obtained 83.33% accuracy for attention detection, 86.67% accuracy for navigation, and 97.3% accuracy overall. This technology improves the mobility of elderly and disabled people by offering a biopotential-based powered wheelchair control interface that is both ergonomic and reasonably priced [5]. This study describes the creation of an EEG-based brain-controlled wheelchair using Brain Computer Interface (BCI) and the NeuroSky Mind Wave EEG Headset. Patients who are quadriplegic are unable to move any body organs below their necks. By using this technology, people who are quadriplegic will be able to move around independently. The patient's fluctuating attention level regulates the wheelchair's movement. The patient can blink their eyes twice to turn on or off this device. The wheelchair's design incorporates a fuzzy image based on graphics to let patients adjust their level of concentration as needed [6]. In order to create a logical taxonomy, this study thoroughly examines current research on wheelchair control utilizing brain-computer interfaces (BCI) for people with disabilities. The study finds significant topics by examining 100 selected publications from major databases: framework design (4 articles), wheelchair control simulations (14 articles), real-time control systems (55 articles), BCI signal analysis (25 articles), and a review of concerns (1 article). Since 2007, studies have looked into a number of approaches to enhance wheelchair control based on BCI, discussing issues and suggesting fixes. The study highlights how these developments have affected society and attempts to direct future research by pointing out problems and inadequacies in this area [7]. The goal of this project is to control an integrated wheelchair robotic arm system using a unique, highly accurate hybrid brain-computer interface (BCI) based on electroencephalogram (EEG) and electrooculogram (EOG). By using left- and right-hand motor imagery (MI), the user can turn the wheelchair left or right. They can also create other commands for the wheelchair and robotic arm by moving their eyebrows and blinking their eyes. Five of the twenty-two subjects that took part in the MI training session finished the mobile self-drinking experiment, which was specifically created with strict accuracy standards. The outcomes indicated the possibility for BCI-controlled systems to be used in challenging daily tasks and proved that the suggested hBCI could give satisfactory control accuracy for a system that consists of many robotic devices [8]. This work describes a brainwave-controlled wheelchair system that responds to color stimuli through EEG data. Four directions (left, right, forward, and stop) were selected as stimuli, and the corresponding colors were red, green, blue, and yellow. To find the best rhythm and classifier, the analysis of EEG rhythms (alpha, beta, delta, and theta) was done, and time-frequency domain features were recovered. Through cross-validation, the study evaluated the performance of K-Nearest Neighbor (KNN), Support Vector Machine (SVM), Random Forest Classifier (RFC), and Artificial Neural Networks (ANN). According to the results, the beta rhythm worked the best, and the ANN classifier had the maximum accuracy of 82.5% [9]. With an emphasis on useful real-world applications, this work suggests a sophisticated Brain-Computer Interface (BCI) system incorporating EEG data processing for wheelchair control. Even when influenced by distortions such as simulated nervous tics, the system uses artificial intelligence to decode EEG signals, including facial emotions as control commands. It has two neural networks: one for identifying nervous tics and another for processing EEG signals to produce control commands. The system is appropriate for those with disabilities who use BCI-based wheelchair control since it is made to work in the presence of common interferences like radio waves and street noise [10]. With the development of a brain-computer interface (BCI) based electric wheelchair control system, users can now propel the wheelchair in four directions (forward, backward, left, and right) by just closing their eyes for more than a second, which causes an alpha-wave block to occur in the EEG. A BCI control panel, data acquisition, signal processing unit, and interface control circuit make up the system. The studies involved eight volunteers, with the greatest success control rate at 93.7% and a mean of 81.3%. The average information transfer rate was 8.10 bits per minute, with a maximum of 12.54 bits per minute for a single participant [11]. This essay focuses on the creation of a brain-controlled wheelchair that has two more control interfaces: an Android phone-based remote control and a joystick. Because all three controls work together, the user just needs to adjust the slide switch's status to switch the mode of operation. In this experiment, a single channel at the FP1 location is used to record EEG data using the Neurosky Mindwave Mobile headset. The main characteristics of the recorded EEG that are retrieved and recognized by an Android application are eye blinks and attention levels. In order to guarantee safety for users, the design also incorporates an ultrasonic sensor-based safety system that can identify impediments in all four directions [12]. An investigation into the control of an electric wheelchair using electroencephalograms (EEGs) is presented in this publication. The goal is to steer an electric wheelchair solely with electroencephalogram impulses. To put it another way, this is an effort to operate mechanical devices like wheelchairs via brain impulses. We have created a recursive training algorithm to produce recognition patterns from EEG signals in order to accomplish this goal. Our experimental findings show the effectiveness of the suggested recursive training technique and the feasibility of using only EEG data to control the direction of an electric wheelchair [13]. In order to improve interaction speed and accuracy, this study presents the CVAR-BCW, a brain-controlled wheelchair (BCW) that combines augmented reality (AR) and computer vision (CV) for autonomous target navigation. The translucent head-mounted display (HMD) of the CVAR-BCW is used to identify ambient targets through CV. After a target is chosen, the wheelchair moves toward it on its own. When manual control is preferable, there is also a semiautomatic option available. Twenty individuals without disabilities tested the system, and the results showed that it performed well indoors, with average accuracy of 83.6% for automatic and 84.1% for semiautomatic modes and a shorter target approach time of 42.4 seconds for automatic mode. An efficient, user-friendly interface with potential uses in disability assistance is provided by the CVAR-BCW [14. Work that is being presented covers the preliminary research on a few brain conditions in various scenarios that may be observed in this environment, and the detection system can generate control signals that can be sent for the motions of the wheelchair. Because these systems are compact, the detection and classification algorithms used in them must be quite basic in order to fit into the microcontroller's little memory. One such method, based on fuzzy classifications of the EEG data utilizing specific statistical signal properties, is presented in the paper [15]. The design of the categorization system based on fuzzy neural networks (FNN) is taken into consideration, with the wheelchair's control commands and the user's mental activity taking precedence. For brain-actuated control, an algorithm based on FNN design is employed. Test data is used to gauge the control system's performance after training data is used to develop the system. The wheelchair's direction and speed control commands are used to operate the device in real-world scenarios. The method employed in the research makes it possible to lower the likelihood of misclassification and increase the wheelchair's control precision [16]. The authors present a non-invasive brain-computer interface (BCI) device that records EEG data using a neuro-signal acquisition headset. The signals that enable precise control of the wheelchair are derived from particular brain activity that people have been trained to produce. The electrical activity of the brain is captured by EEG-based BCIs, which then convert these impulses into useful commands. This study's main goal is to show how the system can decipher particular thought patterns or mental commands given by the user by interpreting EEG signals. It hopes to translate these into precise wheelchair control commands by doing this. This procedure involves identifying navigational objectives that are unique to wheelchair operation, such as turning, moving ahead, or going backward [17]. It has not yet been demonstrated how to use non-invasive BCI to effectively control a robotic arm to do reach-and-grasp activities. In this study, we discovered that using a combination of two sequential low dimensional controls, a group of thirteen human participants could voluntarily regulate brain activity to control a robotic arm with good accuracy for tasks requiring numerous degrees of freedom. In just a few training sessions, the subjects were able to modulate their brain rhythms to effectively control the robotic arm's reaching, and they were able to sustain this control across several months. The results show that non-invasive BCI technology can be used to operate prosthetic limbs by humans [18]. In this work, a unique Brain-Computer Interface (BCI) system allowing mind-only control of a robotic arm is presented. Using an EMOTIV EPOC headset, four participants (ages 20-29) recorded their imagined movements (e.g., right hand, left hand, both hands, or feet). Within the sensorimotor rhythm frequency range of 8\u201322 Hz, the recorded brain activity was examined using Principal Component Analysis (PCA) and Fast Fourier Transform (FFT). After that, a Support Vector Machine (SVM) with a Radial Basis Function (RBF) was used to classify these features, and the outputs were converted into instructions for the robot arm to follow. With an accuracy of 85.45% on average, the system allowed the robot arm to move in four different directions [19]. In order to control a robotic arm, this study introduces the Brain Machine Interface (BMI) system, which is based on exploiting the brain electroencephalography (EEG) signals linked with three arm movements: close, open arm, and close hand. signals captured with an Emotive Epoc device from a single subject. In our experiment, we only employed four channels: AF3, which is located in the prefrontal cortex, and F7, F3, and FC5, which are located in the brain's supplementary motor cortex. Principal Component Analysis (PCA), Fast Fourier Transform (FFT), and Wavelet Transform (WT) were the three methods utilized for features extraction. The three jobs under consideration were classified using a multi-layer Perceptron Neural Network that was trained using a typical back propagation approach. Using the employed feature extraction strategies, classification rates of 91.1%, 86.7%, and 85.6% were attained, respectively [20]. Loss of motor function is an important issue that significantly lowers the quality of life for those who experience it. In this initiative, a different approach to enhancing the comfort level in everyday chores for individuals with upper limb motor function loss is offered [21]. This work investigates the use of artificial intelligence (AI) in a non-invasive Brain-Computer Interface (BCI) system to operate a robotic arm and help people with physical disabilities. To find the optimum performance for various users, the system examines several AI classification techniques, including Random Forest, KNN, Gradient Boosting, Logistic Regression, SVM, and Decision Tree. According to the results, Random Forest had the best accuracy at 76%, Gradient Boosting came in second at 74%, and Decision Tree had the lowest accuracy at 64%. The study also finds that individual variations in dominant frequencies and activation bandwidth in EEG signals impact the system's functionality [22]. This work investigates the use of artificial intelligence (AI) in a non-invasive Brain-Computer Interface (BCI) system to operate a robotic arm and help people with physical disabilities. To find the optimum performance for various users, the system examines several AI classification techniques, including Random Forest, KNN, Gradient Boosting, Logistic Regression, SVM, and Decision Tree. According to the results, Random Forest had the best accuracy at 76%, Gradient Boosting came in second at 74%, and Decision Tree had the lowest accuracy at 64%. The study also finds that individual variations in dominant frequencies and activation bandwidth in EEG signals impact the system's functionality [23]. This study proposes a wheelchair control system that uses electrical impulses, facial motions, eye blinks, and muscle contractions to be powered by EEG data from the human brain. The study concluded that because the Raspberry Pi can permit remote updates and establish a wireless connection via Bluetooth to a BCI EEG headset, it is the best microcontroller for the system. The necessity for repeated trips to the center is diminished by this feature. Because of the system's support for parallel processing, brain signal analysis is now quicker and more precise. The Brain-Sense gadget also analyzes the EEG data, and the system is compatible with security, IoT [30], and automation applications [24]. EEG-based BCI wheelchairs process the obtained signal and transmit commands to the wheelchair's motors through the use of wireless EEG devices that are readily available for purchase in combination with one or more types of BCI. Additionally, the signals that are gathered and processed vary depending on the input mechanism. While the intended output of various assistive devices differs significantly, similar processes take place in them. The majority of the examined studies, if not all of them, are still in the early phases of development, but they all project a positive future for BCI based on EEG. Furthermore, research strongly suggests hybrid BCI for a multimodal strategy to guarantee higher system safety and accuracy [25]. This work describes a Brain-Computer Interface (BCI) system that uses Steady State Visual Evoked Potential (SSVEP) to control the movement of a wheelchair in the following directions: forward, backward, left, right, and stop. The system generates SSVEPs using four distinct flashing frequencies, which LabVIEW displays on an LCD panel. Fast Fourier Transform (FFT) is used to interpret EEG signals from the occipital area. The accuracy of three classifiers, two of which are based on Artificial Neural Networks (ANN) and one on Support Vector Machines (SVM), is compared. Based on ten individuals' worth of testing, the findings demonstrate that the One-Against-All (OAA) multiclass SVM classifier works more accurately than the ANN classifiers [26]. In order to extract features from EEG data, a Long Short-Term Memory Convolutional Neural Network (LSTM-CNN) is used in this study to offer a unique EEG signal state recognition model. By forecasting signal states using both past and present probabilities, an actor-critic decision-making model is utilized to repair errors brought on by mental randomness or outside influences. Using the Unicorn Hybrid Black EEG device and OpenViBE for data transfer, the model is implemented in a hybrid BCI real-time control system for a BCI robot. The system achieves 87.20% offline accuracy and 93.12% online accuracy with a mean information transmission rate of 67.07 bits/min, according to experiments conducted with five individuals. The system performs better than other BCI control systems, proving its dependability and usefulness [27]. The goal of this project is to create a Brain-Machine Interface (BMI) based on EEG that can identify voluntary keystrokes in people with motor disabilities. The dataset was extracted from a Nature journal and segmented into 22 electrode arrays using band-pass filtering to eliminate non-significant channels. Three categories were created out of the data using ERP window-based segmentation: resting state, 'd' key press, and 'l' key press. After training a number of deep learning models to classify EEG signals, the SVC model achieved an accuracy of 90.42%. Strong performance was also demonstrated by other models, such as MLP, Catboost, KNN, and a Bi-Directional LSTM-GRU hybrid. Using the trained MLP model, a graphical user interface (GUI) was developed to replicate keystrokes in real time [28].\nThe objective of this study is to develop a system that allows users to control wheelchair movements using their brain signals, specifically the movements of their right and left hands. Decoding EEG data based on the right and left-hand voluntary movement of users is the first approach to such an application and by translating it into control commands, the system offers a promising solution for hands-free wheelchair navigation."}, {"title": "2. Methodology", "content": ""}, {"title": "2.1 Dataset Description", "content": "A standard 10/20 EEG cap (Electro-Cap International, USA) with 19 bridge electrodes in the 10/20 international configuration was used to help collect the EEG data. An EEG technician prepped each participant's head before the recording session, cleaning the scalp with a washing solution and combing the hair around the electrode sites. The subject then had the EEG cap put back on their head. The EEG dataset used in this study was obtained from a publicly available source, featuring recordings of brain signals captured during voluntary right and left-hand movements. The dataset consists of multichannel EEG signals recorded from 19 electrodes, which is standard in motor-related BCI experiments. Each trial in the dataset contains data corresponding to the onset of hand movements. A fixation point was visible to participants in the middle of the GUI screen. Each trial began with the presentation of an action signal lasting one second, which may be the left hand, right hand, left leg, right leg, tongue, or a circle signifying a passive reaction. All of the participants did not say anything at all. The goal of this BCI interaction paradigm was to use EEG signals to investigate the differentiation of voluntary motor movements before their physical manifestation. There were very few recording sessions available for this paradigm, and this line of inquiry was not pursued extensively in the paper [28, 29].\nThe action signal onset times (on-time) in 1 and 6 can be defined as the moments at which the \"marker\" value changed from 0 to a non-zero value for the purposes of the included data analysis.  A fragment of EEG data linked with a participant's provided mental imagery can be formed based on these times by selecting segments of the EEG data from the \"data\" array and applying the provided time offsets into time intervals before to and following the action signal on-time. These \"event data frames\" ought to be linked to particular mental imagery through the use of \"marker\" values that are presumed to exist right after the action signal's on-switch.\nParticipants were first shown action signals denoting one of the mental imagery exercises that needed to be done. Participant use of the visuals occurred once while the action signal was activated. Using EEG-1200 gear, the EEG signal corresponding to the implemented imagery was captured and stored using Neurofax recording software. The collected EEG data were stored and exported as an ASCII file for additional processing following the experiment. Matlab was used to import the ASCII data file and perform analysis [28].\nThe FreeForm-interaction GUI interface displayed in Fig. 2c was viewed by participants. The participants were instructed to quietly rest their hands on the computer keyboard and fix their eyes on the fixation point. At random intervals, participants were instructed to freely push the \"d\" or \"l\" keys with either their left or right hand. The reference point for the analysis of the EEG key-press waveforms was established by timing the key presses, which were recorded. The identification of the last key that was pressed and the total number of left and right keystrokes were recorded by eGUI and shown on the screen. The average number of left and right key presses that the participants were required to execute was comparable.."}, {"title": "2.2 Preprocessing", "content": "The dataset was pre-filtered to remove noise and artifacts, ensuring the signals were of high quality for classification. A band-pass filter was applied to retain the frequencies relevant to motor activity (typically between 1\u201330 Hz) as per the publication so, no extensive feature engineering was required but feature extraction required the ERP window based segmentation with help of 19 channels and 200 as sampling frequency a feature array in the form of 19*200 is developed. The dataset finally, after combining from SubjectC two different data forms, got 2776 rows after feature extracting with ERP onsets help.\nThe segmentation focused on capturing the onset of voluntary movements to ensure the extracted features corresponded to the initiation of either right-hand or left-hand movements. These preprocessed segments were then fed into the machine learning models for training and evaluation."}, {"title": "2.3 System Architecture", "content": ""}, {"title": "2.3.1 EEG Signal Classification", "content": "Three machine learning models were employed to classify the EEG signals associated with voluntary hand movements: Support Vector Machines (SVM), XGBoost, and a Bi-LSTM attention-based model. The system's architecture consists of the following components:\ni.\tEEG Signal Acquisition: The raw EEG signals are collected from the dataset and passed through preprocessing stages for noise removal and feature extraction. The signal is feature extracted using ERP windows based segmentation into 19*200 electrode array, where 19 represents the number of electrodes and 200 is the sampling frequency in which data was obtained in experiment.\nThe dataset used in this work is based on the FreeForm paradigm from the Nature paper [6], which focuses on the EEG signal-based discrimination of voluntary motor movements. The subjects interacted with an eGUI that allowed for FreeForm interaction, focusing on a fixation point and freely pressing the \"d\" or \"l\" keys with their left or right hand at random intervals. The neural activity changes that preceded key presses were captured by the EEG signals, which served as the foundation for further analysis [29].\nii.\tDataset feature extraction: The dataset is divided into three sets: Recording session of subject C: FREEFORM-SubjectC-151210-5St-LRHand.mat, similarly, FREEFORMSubjectB1511112StLRHand.mat and also FREEFORMSubjectC1512102StLRH and are three sets of data obtained from a recording session in the experiment.\nFreeForm-the identification of voluntary left- and right-hand movements\u2014is the recording session paradigm. There are two states of mental imagery (2St). The Left-Right Hand (LRHand) mnemonic is used during recording sessions.\nAs, seen in table 1. the total number of user keystroke events during the activity that corresponds to the stimuli or event during the keypress or resting state is referred to as the onsets in the EEG recording in Table 1. A crucial component of the classification problem is the onset information that was acquired. The subjectC dataset consisting up of single person activity is utilized in this research work.\nEach record in the dataset is distinguished by a unique alphanumeric identifier referred to as \"id.\" This identifier serves as a key element for record tracking and management. The \"nS\" parameter denotes the number of EEG data samples contained within each record, providing insight into the temporal dimension of the recorded neural signals. The \"sampFreq\" parameter specifies the sampling frequency of the EEG data, representing the rate at which data points are collected per unit of time. The \"marker\" field encapsulates the eGUI interaction record of the recording session, offering contextual information about user actions during the EEG data acquisition [28, 29]. During the preprocessing stage, trials are extracted from the 200 Hz EEG data sample and aligned with the key press events that were recorded. Event plots with markers are used to identify and visualize these events. ERP (Event-Related Potentials) plots and other additional analysis techniques are used to improve the signal quality and discriminate neural patterns. If the ERP results are not conclusive, more techniques such as filtering are investigated to improve the data quality. A deep learning model is trained for classification after preprocessing. The eGUI's \"0\" label denotes a resting stage, \"1\" denotes pressing the \"d\" key, and \"2\" denotes pressing the \"l\" key. The model is made to predict labels 0, 1, or 2. To guarantee reliable classification performance, the model's accuracy is validated during the training phase.\nThe trained model is integrated with a tkinter-based Graphical User Interface (GUI) to create this simulation. Based on the recorded EEG signals, the model predicts and simulates key presses and the GUI imitates the FreeForm-interaction GUI screen. Through the completion of this extensive procedure, which includes data collection and preprocessing, model training, validation, and simulation, the project hopes to aid in the creation of a dependable system for brain-computer interfaces. The ultimate objective is to offer a neurodegenerative disability support system that will improve the user's capacity to engage with digital platforms using an EEG-controlled wheelchair [29].\nWe plotted the event-related potentials of 19 electrode data and analyzed the neural activity patterns, related to key press. ERPs provide us with the average electrical activity of the brain in response to specific stimuli or events. The utilization of ERP plots aids in the identification of distinctive features and characteristics in the EEG data that correspond to motor planning and execution. These plots can reveal subtle changes in brain activity in the time domain, particularly in the moments immediately preceding the key presses. Analyzing ERPs helps in determining the temporal dynamics of the brain's response to motor intentions, which is crucial for feature extraction and model training.\nThe ERP plot is shown in fig.3. Shows the 21-channel EEG data plot with respect to movement onset, which provides us with the behavior of the events in the recording session. The sampling frequency of 200 Hz is utilized to grab the samples and make an analysis of EEG data. The event-related potential (ERP) waveforms of a (FREEFORMSubjectB1511112StL) during a freeform task are displayed in the figure above. Plotting of the ERP waveforms for 19 electrodes on the subject's scalp is done with the ERP amplitude on the y-axis and the time axis on the x-axis.\nThe ERP provides information about the subject's brain activity during the freeform task. The peaks and valleys in the waveforms indicate various electrical activity patterns, and the various colors correspond to various scalp electrodes. For instance, the Pz electrode, which is located at the top of the head, exhibits a positive peak at approximately 300 milliseconds, which is commonly connected to the ERP's P300 component. It is believed that the P300 component reflects the subject's focus on the freeform task.\nThe figure 4 image shows an event-related potential (ERP) waveform for a single channel (Pz) of a dataset. The ERP is a measure of the electrical activity of the brain in response to a specific event or stimulus. It is calculated by averaging the EEG data from multiple trials, which cancels out random noise and reveals the underlying brain response. The ERP waveform is typically characterized by a series of peaks and troughs, each of which is associated with a different stage of cognitive processing. For example, the P1 wave is thought to reflect the initial sensory processing of a stimulus, while the N1 wave is thought to reflect the attention to that stimulus. Later waves, such as the P300 and N400, are thought to reflect higher-level cognitive processes, such as decision-making and memory retrieval. The specific ERP waveform that is observed can vary as it is dependent on the type of stimulus or event that is presented. For example, the ERP waveform for a visual stimulus will be different from the ERP waveform for an auditory stimulus. The ERP waveform can also be affected by the individual's cognitive state, such as their attention level or fatigue [29]."}, {"title": "2.3.2 Model Development:", "content": "SVM: An effective supervised learning approach for regression and classification problems is called Support Vector Machine (SVM). It operates by determining the best hyperplane to maximize the margin between classes while dividing them into distinct classes in the feature space. SVM uses kernel functions to change the input space", "resilience.\nXGBoost": "Extreme Gradient Boosting", "Regression": "A statistical model called logistic regression is used to estimate the likelihood that an input falls into a specific category in binary classification tasks. Predicted values are mapped to a range between 0 and 1 using the logistic function, enabling interpretation as probabilities. The input features and the log-odds of the result are assumed to have a linear relationship by the model. Despite its ease of use, logistic regression is a fundamental method in statistical and machine learning and can be quite effective"}]}