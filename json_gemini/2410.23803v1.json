{"title": "Generative AI for Accessible and Inclusive Extended Reality", "authors": ["Jens Grubert", "Junlong Chen", "Per Ola Kristensson"], "abstract": "Artificial Intelligence-Generated Content (AIGC) has the potential to transform how people build and\ninteract with virtual environments. Within this paper, we discuss potential benefits but also challenges\nthat AIGC has for the creation of inclusive and accessible virtual environments. Specifically, we touch\nupon the decreased need for 3D modeling expertise, benefits of symbolic-only as well as multimodal\ninput, 3D content editing, and 3D model accessibility as well as foundation model-specific challenges.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence-Generated Content (AIGC) is impacting a growing number of individuals,\nprofessions, whole societies, and political systems. AIGC already allows for high-fidelity text\n[1], image [2], and video generation [3, 4] alongside further modalities (see recent survey [5]).\nIt has been projected to have a substantial impact on future XR (and Metaverse) experiences [6].\nText-to-3D generation [7] is particularly relevant for the generation of content for Extended\nReality (XR). We foresee that AIGC in general, and the creation and editing of 3D content in\nparticular can have a substantial impact on accessible and inclusive XR environments. Within\nthis paper, we discuss selected promises and potential challenges when using AIGC for creating\nand interacting with such environments."}, {"title": "2. Promises of AIGC for Inclusive and Accessible XR\nEnvironments", "content": ""}, {"title": "2.1. No 3D Modeling Expertise Necessary", "content": "Tools for immersive content generation and editing inside VR became popular in recent years\n(such as Gravity Sketch\u00b9) and complement more traditional desktop-based 3D computer graphics\ntools. Still, such tools typically require a substantial amount of expertise for the generation of\n3D models, which are of particular relevance for XR environments. AIGC-based tools promise\nto make 3D modeling easier to use through simple text-to-3D generation. While initial methods\n[8, 9] tended to have heavy computational demands requiring hours to generate individual\nobjects, substantial improvements have been made both in terms of computational needs [10, 11]\nand visual fidelity [12, 13, 14] with further advancements being published regularly. Integration\nof such tools directly inside an immersive environment, as recently proposed by Weid et al. [15],\ncould lower the entry barrier for immersive content generation, and, through this, contribute to\nmore accessible 3D content creation for virtual environments."}, {"title": "2.2. Symbolic Input as an Effective Interaction Paradigm", "content": "Related, the reliance on text as the main input for the creation of 3D assets potentially facilitates\naccessible (3D) content generation for user groups with limited manual dexterity. Even people\nwith limited speech abilities could potentially create 3D content through gaze-based text entry\n([16]) and then navigate and interact within the virtual environments using gaze (c.f., [17]). This\ncould contribute to the broader agenda of accessible XR by Design [18, 19]. Still, to allow for\naccessible XR for a wide range of users (e.g., also one with involuntary eye or limb movements\n[19]), multiple, ideally equivalent input modalities should be offered."}, {"title": "2.3. Using Multimodal Input to Reduce Interaction Complexity", "content": "Inspired by early work multimodal input [20, 21], we foresee chances that, using a combination\nof speech and pointing gestures, an AI-based system could support the efficient selection (and\nmanipulation) of objects, that would otherwise require tedious individual interactions. Imagine\na user standing inside a virtual or augmented living room that should be re-decorated. For\nexample, now, the user could recolor all objects in the room by pointing and stating \"make\nall objects with a texture like this [the user is pointing towards an object] brighter\". Similarly,\nreferencing and understanding real-world objects in augmented reality environments can now\nalready be facilitated through scanning of the physical environment, spatial representation (e.g.,\nas point cloud), and subsequent processing through large multimodal or large language model\n[22, 23]. Through this modifications or add-ons to existing physical objects could be achieved.\nFor example, a user could point towards a window and ask the system to generate a suitable\ncurtain. Similarly, a user could point towards a wall and ask the system to propose artwork that\nfits the room the user is in."}, {"title": "3. Challenges of AIGC for Inclusive and Accessible XR\nEnvironments", "content": ""}, {"title": "3.1. 3D Content Editing and 3D Model Accessibility", "content": "While there is a growing number of tools for the initial creation of 3D assets, subsequent editing\nof the created 3D assets still often relies on traditional workflows, hence, limiting its applicability\nfor accessible XR content generation. A few works already allow for subsequent editing of\ninitially created 3D scenes but typically rely on specific spatial scene representations such as\nneural radiance fields (NERFS) or using Gaussian splatting (GS), e.g., [25, 26, 27]. Challenges\nremain in also enabling text-guided editing for traditional 3D object representations such as\nmeshes or integrating NERF/GS scene representations in traditional rendering pipelines.\nFurther, even today, manually created 3D content poses accessibility challenges through the\nlack of 3D content metadata [18], which might be even further complicated through automated\n3D asset creation. This metadata is of importance, for example, to allow for alternative rendering\nmethods such as haptic or auditory rendering for people with visual impairments. Hence, in\nthe future, there should be a focus on how to utilize, e.g., large language models, in the creation\nprocess to generate meaningful metadata alongside the actual 3D representation. Otherwise, in\nthe future, AIGC without metadata for alternative rendering methods could jeopardize carefully\nd designed accessible VR or AR environments. For example, partially empty spaces or holes\n(e.g., a generated chair or table) in an otherwise accessible virtual room could degrade the user\nexperience."}, {"title": "3.2. Foundation Models", "content": "While foundation models in AI are a key driver for the recent success of AI in general and\ncontent generation in particular, they come with their own set of challenges.\nFor one, bias in foundation models is an insufficiently addressed challenge for text and\nmultimodal foundation models [28, 29]. It remains an open question how to balance the urge\nfor inclusive 2D and 3D content generated content without replicating (e.g., gender or racial)\nstereotypes [30, 31] with concerns about the reliability of generated content regarding (e.g.,\nhistorical) facts. Recent media attention to Google's Gemini model generating biased depiction\nof humans (e.g., putting people of color in Nazi-Era uniforms [32]) exemplifies this challenge. It\nseems plausible that similar issues can arise when generating 3D content as common text-to-3D\ngenerators rely indeed on text-to-image generation first, followed by an uplifting process. But\neven, when using text-to-3D diffusion directly (e.g., [13]) bias in the underlying 3D model\ndatabases such as Objaverse-XL [33], remains challenging, specifically when considering using\nsuch content for the creation of inclusive XR environments"}, {"title": "4. Conclusion and Future Work", "content": "Within this paper, we looked at how AI-based content generation might benefit but also challenge\nthe creation of and interaction in inclusive and accessible virtual environments. While some\nbenefits, such as easier and more accessible content generation seem graspable, the potential\nrisks of employing AIGC-based systems should not be underestimated. Some of those risks (such\nas bias) relate to underlying issues of using foundational models in the first place. However, it\nis important to be aware of how those problems could manifest themselves in inclusive and\naccessible virtual environments in the future. As one of our next steps, we plan to focus on\nsupporting efficient content selection and manipulation to facilitate accessible XR environments.\nWe will also explore the possibility of incorporating multimodal manipulation techniques to\ncounterbalance the negative effects of AIGC-based techniques with the accuracy and precision\nadvantages of traditional input techniques."}]}