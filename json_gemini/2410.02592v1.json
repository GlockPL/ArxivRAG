{"title": "IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of Both Driver and Passengers", "authors": ["Zihan Fang", "Zheng Lin", "Senkang Hu", "Hangcheng Cao", "Yiqin Deng", "Xianhao Chen", "Yuguang Fang"], "abstract": "Recently, in-car monitoring has emerged as a promising technology for detecting early-stage abnormal status of the driver and providing timely alerts to prevent traffic accidents. Although training models with multimodal data enhances the reliability of abnormal status detection, the scarcity of labeled data and the imbalance of class distribution impede the extraction of critical abnormal state features, significantly deteriorating training performance. Furthermore, missing modalities due to environment and hardware limitations further exacerbate the challenge of abnormal status identification. More importantly, monitoring abnormal health conditions of passengers, particularly in elderly care, is of paramount importance but remains underexplored. To address these challenges, we introduce our IC3M, an efficient camera-rotation-based multimodal framework for monitoring both driver and passengers in a car. Our IC3M comprises two key modules: an adaptive threshold pseudo-labeling strategy and a missing modality reconstruction. The former customizes pseudo-labeling thresholds for different classes based on the class distribution, generating class-balanced pseudo labels to guide model training effectively, while the latter leverages cross-modality relationships learned from limited labels to accurately recover missing modalities by distribution transferring from available modalities. Extensive experimental results demonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy, precision, and recall while exhibiting superior robustness under limited labeled data and severe missing modality.", "sections": [{"title": "I. INTRODUCTION", "content": "Traffic accidents result in a significant number of fatalities and injuries each year. Association for Safe International Road Travel shows that nearly 1.35 million people die in road crashes every year, or an average of 3,287 deaths per day [1]. At this rate, traffic accidents is ranked as the fifth leading cause of death by 2030 [2]. According to [3], one of the leading cause to traffic accidents is the abnormal status of driver (e.g., distraction [4], stress [5], fatigue [6], and health emergency [7]), accounting for about 87.7% of the overall road crashes. Those abnormal statuses greatly impair driver's hazard perception and decision-making ability [8], hence, the early detection of a driver's abnormal status is imperative to avoid traffic accidents. Therefore, the effective in-car monitoring systems that can identify abnormal status at early stage and reminder a driver in advance have acquired immense popularity recently [9]\u2013[11].\nTo achieve a comprehensive and reliable monitoring system, different sensing modalities need to be leveraged collaboratively as a single modality alone is not always informative for status detection [12]. Cameras could provide precise records for facial expression (e.g., eye blinking and yawning) but perform poorly in insufficient lighting and occlusion [13]\u2013[15], whereas wearable sensors provide robust monitoring for physiological features (e.g., respiration rates and heartbeats) against adverse conditions [16]. By integrating diverse data from various sensing modalities, the system captures complementary information from different perspectives, significantly enhancing the reliable perception in a vehicle. In general, as depicted in Fig. 1, a vehicle trains a model to recognize a driver's abnormal status with multimodal data and then notifies the driver to take necessary actions in advance.\nNevertheless, performing in-car status monitoring upon multimodal data encounters several major challenges on informative representation learning. First, the large amount of well-labelled data is usually difficult to acquire. Since the evaluation of abnormal status always heavily relies on a user's feeling with vague definition, the unreliable and subjective evaluation leads to label inconsistency and omission [8], [17]. However, current in-car monitoring frameworks [11], [12], [18] are typically based on supervised learning which depends on vast datasets with high-quality labelled data to learn the robust feature representations [19]\u2013[26]. The limited labeled data impedes model ability to discover the underlying relationships of feature representations, thus posing a significant barriers to extract informative feature representations efficiently. Meanwhile, recognizing abnormal status with the imbalanced data quantity in different classes is challenging."}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "The unpredictability to foresee and record abnormal events leads to a smaller amount of data for abnormal status compared to normal status [27], while the artificially generated data may not fully capture the wide range of possible patterns [28]. The imbalanced class distribution for model training makes it hard to learn characteristics of abnormal status, causing model to misclassify data as normal status, which may jeopardize the lives of people in the car without prompt detection of abnormal status.\nMoreover, the acquisition of full-modality data in a vehicle system is also a challenge as sensing limitations, vibration of a vehicle, lighting condition and occlusion all contribute to the possible data lost [29]. Unfortunately, the existing multimodal learning frameworks assume that the complete data samples with all modalities are always available. The lack of auxiliary data handicaps the training model to distinguish similar activities like blinking eyes or fainting and feeling hot or under stress, hence the reliability of informative representation learning could not be guaranteed. Furthermore, the substantial differences in dimensions and patterns among modalities also cause distinct representations in feature space [30], posing significant challenge to recover the important details for the missing modality.\nAlthough continuous monitoring of the driver is paramount, prompt responses to abnormal health conditions (e.g., cardiovascular disease and stroke) of other passengers is also non-trivial. However, the health monitoring of passengers has been relatively overlooked in current in-car monitoring system compared to driving safety. Due to the long-time driving, driving under time pressure, and nighttime driving, the abnormal status of passengers may not be noticed by others in time [8], putting them, particularly the elderly, at a greater risk to their health as cardiovascular disease accounts for more than 40% of total deaths and 85% of deaths due to chronic diseases occurs in those older than 70 years [31], [32]. As a result, continuously monitoring the status of both driver and other passengers in a vehicle is highly desired for driving safety and elderly care as well.\nTo simultaneously monitor the abnormal status of a driver and passengers, rotating camera emerges as a viable solution by continuously changing its viewing angles to provide broader coverage [33]\u2013[35]. Therefore, mounting a rotating camera on the rear-view mirror enables efficient in-car detection of driver and passengers, guaranteeing continuous detection of them within its expanded field of view. Despite the promising potential of rotating cameras, the implementation of rotating cameras significantly degrades the reliability and efficiency of multi-object monitoring: i) Turning the angle of the camera introduces noise, resulting in low-quality data collected. ii) Camera rotation causes severe modality missing when passengers are out of covering range. iii) The distorted facial images from different angles and perspectives enlarge the distribution gap between visual and physiological feature representations, necessitating effective alignment of different modalities for modality reconstruction.\nIn-car multimodal monitoring has been extensively investigated for several years. However, none of the existing multimodal sensing systems [11], [12], [18] is capable of learning informative representations with limited high-quality labels and severe missing modalities to monitor multiple objects concurrently. Moreover, existing efforts only focus on monitoring the driver whereas the status of other passengers are neglected, failing to fully utilize the perception capabilities of a vehicle to enhance the overall safety. Hence, it is imperative to design a multi-object monitoring system that effectively learns informative representations with limited labels under severe modality missing.\nIn this paper, we propose a cutting-edge in-car monitoring system, named in-car multimodal multi-object monitoring (IC3M), to recognize the abnormal status at the early stage of critical health conditions for both driver and passengers. IC3M comprises two well-designed modules, namely, adaptive threshold pseudo-labeling and missing modality reconstruction. The adaptive threshold training is designed to generate high-confidence data (i.e., unlabeled data with confidence higher than the threshold) through adaptive threshold adjustment to achieve more balanced class distribution for model training. As for the low confidence data, we introduce it into model training through contrastive learning, facilitating feature extraction for informative feature representations and thereby accelerating convergence. To keep the model performance when some sensing data is missing, we propose a novel modality reconstruction network to restore modality-specific features across the distribution mapping from the available modality to the missing modality. The tailored meta learning framework is utilized for generalization enhancement with limited labels. We summarize our main contributions as follows.\n\u2022 We propose an adaptive threshold pseudo-labeling scheme to dynamically adjust confidence threshold based on class quantity with the incorporation of low-confidence unlabeled data in model training.\n\u2022 We design a feature reconstruction network to learn the distribution mapping through limited labels according to cross-modality relationships.\n\u2022 Our proposed framework is a more general model which is not limited in this in-car monitoring scenario, and could be applicable to other multimodal systems.\n\u2022 We conduct extensive experiments to illustrate that IC3M achieves higher detection accuracy and convergence speed than state-of-the-art benchmarks, demonstrating the effectiveness of our well-designed adaptive threshold pseudo-labeling and modality reconstruction modules.\nThis paper is organized as follows. Sec. II motivates the design of IC3M by revealing the limitations in current in-car status monitoring system. Sec. III presents the system design of IC3M. Sec. IV introduces system implementation, and experimental setup, followed by performance evaluation in Sec. V. Related works and technical limitations are discussed in Sec. VI. Finally, conclusions and future remarks are presented in Sec. VII."}, {"title": "III. SYSTEM DESIGN", "content": "To better motivate the design of IC3M, we first provide extensive pilot measurements to elaborate on the challenges that limited labelled data and modality missing pose on in-car\nIn this section, we elaborate our IC3M framework, a new system for monitoring the status of both drivers and passengers with multimodal sensors in the car. Motivated by the insights from Sec. II, our key idea is to fully utilize the information from limited labels and large amount of unlabeled data through contrastive semi-supervised learning with adaptive threshold for class balance, while also reconstructing the missing modality to bridge the distribution gap so as to restore the complementary information for reliable status identification. In the following, we first define our problem concretely, and then describe details of the system architecture."}, {"title": "A. Problem Statement and Overview", "content": "Our IC3M is designed to simultaneously monitor the status of both the driver and other passengers with continuously collected multimodal data, thereby enhancing not only driving safety to prevent accidents but also passenger safety to promptly respond to health emergency. The abnormal statuses for the driver and passengers exhibit distinct facial and physiological patterns at the early stage, such as twisted facial expression, difficulty breathing, and irregular heartbeat [8], [9]. Therefore, as shown in Fig. 7, we utilize multiple sensors (camera and wearable devices for the driver and passengers, and vehicle sensors only for the driver) to capture these early signs in different perspectives and train models for each person, providing early identification and timely alarms of potential issues. Associating specific rotation angles with different persons allows passengers in different locations to be distinguished with a rotating camera.\nAlthough status detection has been extensively explored for in-car monitoring, the limitation of reliability and efficiency for informative representation learning emphasized in Sec. I restrict the learning ability of the training model. As illustrated in Sec. II, the limited labeled data and class imbalance hamper training efficiency while modality missing leaves detection results unreliable, forcing traditional algorithms to yield only comparable or even inferior performance. Especially, the use of rotating camera exacerbates the challenges by increasing the feature distribution gap and raising the possibility of low quality data and information lost.\nOur design consists of the following two crucial modules: adaptive threshold pseudo-labeling and missing modality reconstruction. First, we design an adaptive threshold adjustment module to dynamically adjust the confidence threshold for each class, effectively selecting high-confidence data to guide model training within imbalanced class distribution (Sec. III-B1). We also devise the contrastive loss to further leverage the rich information contained in low-confidence data, facilitating informative feature extraction with limited labels (Sec. III-B2). Second, we develop a modality reconstruction module to reconstruct the modality-specific features by analyzing the correlations between various modality distributions (Sec. III-C1) and transfer complementary information through mapping relationships (Sec. III-C2) to achieve accurate recovery with limited labeled data (Sec. III-C4). Fig. 8 shows the overall system architecture of our IC3M."}, {"title": "B. Adaptive Threshold Pseudo-Labeling", "content": "Recalling Sec. II-A, labeled data is difficult to acquire in our IC3M because of the unreliable subjective evaluation. A small amount of labeled data may fail to capture the full feature distribution, leading to poor model performance on unseen data. To address this, semi-supervised learning (SSL) has emerged as an effective approach which leverages abundant unlabeled data to enhance performance with limited labeled samples. The core idea of SSL is to exploit valuable information in a large amount of unlabeled data by generating reliable labels for these unlabeled examples, effectively increasing the labeled dataset available for training. This is achieved through two key techniques: pseudo-labeling [41], [42], which selects trustworthy labels to unlabeled data, and consistency regularization [43], [44], which ensures that the model's predictions remain consistent between the weakly and strongly augmented versions of the same data.\nSpecifically, given a multimodal dataset X, we treat $x_i^m$ as m-th modality in the i-th multimodal sample, where $X = {X_1, X_2, ..., X_N}$ contains N multimodal data samples and each sample $x_i = {x_i^m | m = 1,..., M}$ consists of synchronized sensing data from M modalities. SSL computes the prediction distribution $p(y_i|a(x_i)))$ from unlabeled data $x_i$ with weak feature augmentation $a(\u00b7)$ (e.g., the concatenation of features from different modalities) to select pseudo labels. The excess of the highest prediction probability $max(p(y_i|a(x_i))))$ above a confidence threshold $\\tau$ indicates that the pseudo label is reliable as the unlabeled data provides informative features. Consequently, this unlabeled data $x_i$ with its pseudo label $\\hat{y}_i = argmax(p(y_i|a(x_i)))$ is added to the labeled dataset $D_l$ for subsequent model training as follows\n$D_l = D_l \\cup {x_i, \\hat{y}_i, m = 1, ..., M}$ (1)\nAccording to consistency regularization, the model should output similar predictions when feeding perturbed versions of the same data [45]. To this end, the cross-entropy function is enforced to the selected pseudo label $\\hat{y}_i$ and the model prediction $p(y_i|A(x_i))$ with a strongly-augmented version $A(\u00b7)$ of $x_i$ (e.g., adding noise for physiological data and distortion for facial information), expecting the model to output consistent results. Therefore, the unsupervised loss for high-confidence unlabeled data can be represented as\n$L_{pl} = \\sum_{i=1}^{N_u} [1(max(p(y_i|A(x_i))) \\geq \\tau))H(\\hat{y}_i, p(y_i|A(x_i))))] $ (2)\nwhere $N_u$ is the number of samples in the unlabeled dataset and $\\tau$ represents a predefined confidence threshold that controls the quality and quantity of pseudo labels generated from unlabeled data.\nMeanwhile, for labeled data, the model predicts labels from strongly augmented features and updates parameters using the standard cross-entropy loss $L_{cls}$.\n$L_{cls} = \\sum_{i=1}^{N_l} H(y_i, p(y_i|A(x_i))))$ (3)\nwhere $N_l = N - N_u$ is the number of labels. Finally, with the balance weight $\\lambda$ for unsupervised loss $L_{pl}$, the overall training loss $L_{ssl}$ is typically as\n$L_{ssl} = L_{cls} + \\lambda L_{pl}$ (4)\nRecalling in Sec. II-A that the scarcity of data from abnormal status leads to an imbalanced class distribution. Training model with class imbalance causes model to prioritize the detection normal states while neglecting features from the class of abnormal status, making it difficult to learn informative features for abnormal status. However, in high-risk applications such as medical diagnostics, the inability to detect abnormal status promptly is particularly dangerous and even endangers lives, emphasizing the necessity of training models to identify abnormal status despite class imbalance. To address this issue, as illustrated in Fig. 9, we propose an adaptive adjustment of the confidence threshold in semi-supervised learning, which generates high-confidence, class-balanced pseudo labels for model training.\nMoreover, to prevent involving incorrect model predictions for pseudo labels, modern SSL algorithms [45]\u2013[47] only leverage unlabeled data with artificial labels above a predefined confidence threshold (typically selected as 0.95) for model training while those below threshold remain as low-confidence unlabeled data. However, excluding low-confidence unlabeled samples in model update restricts the efficiency of informative feature extraction, requiring a large training rounds to reach competitive results. Therefore, we further explore a confidence-based training scheme that fully utilizes the unlabeled data with low confidence, facilitating feature extraction with the rich information in those low-confidence unlabeled data."}, {"title": "1) Adaptive threshold adjustment:", "content": "In semi-supervised learning, the threshold is crucial for selecting high-confidence pseudo labels. However, a fixed confidence threshold is used for pseudo-labeling in traditional semi-supervised learning algorithms [45]\u2013[47], which excludes a significant portion of the unlabeled data from model training at the early stage when the class confidence is generally low, hindering model ability to learn informative feature representations. Recent works [48], [49] have addressed this issue by dynamically adjusting the threshold for each class based on the number of pseudo labels, thus adapting to various training stages and enhancing model performance across extensive semi-supervised learning benchmarks. In practice, both the unlabeled and labeled data suffer from a severe class imbalance due to the data scarcity in abnormal status, making it inadequate to adjust the threshold solely based on the number of pseudo labels generated from the unlabeled data. Even if the selected pseudo labels are class-balanced, the total label quantities among various classes remain imbalanced when combining the original labels with the selected pseudo labels.\nTherefore, we design an adaptive threshold adjustment scheme for pseudo-labeling with balanced class distribution and quantity. Our IC3M aligns the class quantity by adjusting the threshold for each class according to the total number of labeled data and pseudo labels selected from the unlabeled data, thereby mitigating the impact of class imbalance on model training.\nMore specifically, at the t-th training round, the number of labels for class c is given by $o_l^t(c) = \\sum_{i=1}^{N_l} 1(y = c)$ and the number of pseudo labels selected for class c is calculated as\n$\\sigma_u^t (c) = \\sum_{i=1}^{N_u} [1(max(p(y_i|a(x_i)) \\geq \\tau_t(c)))\n1(argmax(p(y_i|a(x_i)) = c)]$ (5)\nwhere $\\tau_t(C)$ is the adaptive threshold for class c and $1(\u00b7)$ represents the indicator function. After one round training, the total number of labels in class c is calculated as\n$\\gamma_t(c) = \\sigma_u^t(c) + \\sigma_l^t (c)$ (6)\nThen, we calculate the proportion of the label quantity in class c as its cumulative distribution to reflect the class distribution imbalance. Define the total number of classes as C, the cumulative distribution of class c is represented as\n$P_t(c) = \\frac{\\sum_{i=1}^c \\gamma_t(c)}{\\sum_{i=1}^C \\gamma_t(c)}$ (7)\nTo achieve a balanced class distribution during training, we utilize KL divergence as a guidance for threshold adjustment. By quantifying the deviation between the current cumulative distribution and the target class-balanced distribution, the model adjusts thresholds to select varying numbers of samples for different classes, encouraging a closer class quantity across classes. For the current empirical distribution of pseudo labels $P(c) = p_t(c)$ and the class-balanced target distribution $Q(c) = 1/C$, the KL divergence is defined as\n$D_{KL}(P||Q) = \\sum_{C} P(c) log \\frac{P(c)}{Q(c)}$ (8)\nLastly, the adaptive pseudo-labeling threshold for class c is\n$\\tau_t(c) = min\\{(p_t(c) + \\tau - D_{KL}(P_t)), \\tau_h\\}$ (9)\nwhere $\\tau$ is the predefined base threshold (e.g., 0.95) and $\\tau_h$ is the upper bound of threshold (usually set to 0.95) which avoids threshold saturation (i.e., exceed 1.0) to leave out most of potentially useful data for training.\nImportantly, KL divergence is crucial for balancing data distribution, with high KL divergence reflecting large class distribution differences. The threshold $\\tau_t(c)$ for minority class is appropriately lowered to generate more pseudo labels, while a higher $p_t(c)$ for majority classes ensures their thresholds are less influenced by high KL divergence. This approach facilitates a gradual alignment of the training class distribution with the target class-balanced distribution, thereby promoting a more balanced class quantity in the next training round."}, {"title": "2) Semi-supervised training with contrastive learning:", "content": "Recalling Sec. III-B1, high-confidence unlabeled data (with the highest prediction probability exceeding the threshold) are selected as pseudo labels for training with more balanced class distribution through dynamic threshold adjustment, allowing model to better extract informative features for each class. However, the remaining low-confidence unlabeled data, despite less distinctive, still contain valuable features that contribute additional information to improve status classification. Current approaches [45]\u2013[49] only rely on high-confidence data for training. However, in the early training stage, limited high-confidence pseudo labels typically represent only easily classifiable samples, failing to capture the full data distribution and resulting in model bias towards well-represented classes. In contrast, incorporating low-confidence unlabeled data in model training provides more diverse training samples, encouraging the model to explore hard-to-classify features and gradually improve performance. Therefore, including low-confidence data in training is essential to further enhance feature extraction with limited labels.\nLabels for low-confidence unlabeled data remain undetermined due to the unreliable model predictions. Fortunately, contrastive learning, an emerging unsupervised learning approach that does not require labels, can effectively learn effective feature representations by pulling the features of unlabeled samples from the same latent class together and pushing the features of different classes apart. Incorporating contrastive learning into model significantly enhance data diversity by leveraging the large amount unlabeled data for discriminative feature extraction, thereby facilitating the feature extraction from hard-to-classify samples.\nMore specifically, we treat pairs of the augmented multimodal features {a(xi), A(xi)} from the same sample xi as the positives pt, versus pairs of the multimodal features from different samples {a(xj), A(xj)} (j \u2260 i,j \u2208 {1, ..., N}) as negatives pj. By minimizing the contrastive loss formulated below, the positive features are brought together while the negative features are separated apart for data without labels.\n$L_{con} = -log\\frac{e^{d(p_i^+)}}{ \\sum_{k=1}^{S}e^{d(p_k^-)}}$ (10)\nwhere S is the number of negative samples (e.g., the batch size) and d() is the cosine similarity adjusted by a hyper-parameter T (typically 0.05), which can be formulated as\n$d({p_i}) = exp(\\frac{a(x_i)\\cdot A(x_i)}{||a(x_i)||\\cdot ||A(x_i)|| T})$ (11)\nWith the contrastive loss Leon to learn discriminative feature representations, our IC3M fully utilizes low-confidence model prediction (max(p(yi|a(xi))) < r) to boost the model performance. To this end, by combining the contribution of labeled data and high-confidence unlabeled data to the model, the overall training loss for status detection is defined as\n$L_{ssl} = L_{cls} + \\lambda_p L_{pl} + \\lambda_c L_{con}$ (12)\nwhere Ap and Ae are weight parameters that control the contributions of unlabeled data in model training."}, {"title": "C. Missing Modality Reconstruction", "content": "Recalling to Sec. II-B that a vehicle system cannot always access full-modality data, we propose a modality reconstruction network to recover the feature representations of the missing modality. Multimodal data shares similar semantic meanings from different perspectives, hence through learning the hidden relationship between modalities, the missing modality can be reconstructed according to other available modalities [37]. However, existing methods usually recover the missing data directly across the common feature extraction [50]\u2013[53], which overlooks different complementary information"}, {"title": "1) Distribution approximation:", "content": "To better motivate our design of modality reconstruction module, we deploy MMIN [50], one of the state-of-the-art multimodal networks for handling missing modalities, to learn the missing heartbeat representations from camera samples in the Stressors dataset and then test the recovery performance on heartbeat samples. We apply PCA to the features extracted from heartbeat and camera samples and visualize the first two PCA components to illustrate their feature distribution. Fig. 10a demonstrates that there is an evident gap in the feature distribution of different modalities and Fig. 10b reveals that only relying on the common information in available modalities results in inconsistent recovery between the recovered and true data. Since the distribution gap impedes the direct estimation of the missing data, it is imperative to propose a new data imputation technique to handle this issue.\nTherefore, instead of directly generating the missing data based on the shared features, we reconstruct distribution of missing modality to restore the modality-specific feature representation with the correlations between modality distributions. By recovering data based on the cross-modality feature transformation from available data, the generated data contains the shared information among modalities and the complementary information of the missing modality as well, hence mitigating the distribution gap between the recovered and the true data. We build a reconstruction module to learn the feature mapping function $F_{m'}$ for distribution transferring. Without loss of generality, below we only demonstrate the reconstruction of the modality m from other available modalities m' ($m' \\neq m, m' \\in {1, ..., M}$), where the feature representations $Z^{m'}$ are extracted by the feature encoder from modality $m'$ independently. To maximize the mutual information that ensures informative feature relationships to be transferred from available modalities $Z^{m'}$ to the missing modality $z^m$, the objective is to minimize the conditional entropy $H(Z^m|Z^{m'})$ in the feature space across the data distribution. Higher entropy implies greater uncertainty in data recovery, thus the reconstruction objective is denoted as\n$min H(Z^m|Z^{m'}) = min E_{p(z^m,z^{m'})} (-log P(Z^m|Z^{m'}))$ (13)\nAs shown in Fig. 11, we first estimate the informative structure representations of the missing modality and other available modalities to preserve their modality-specific characteristics, and then train a feature mapping network to reconstruct the missing modality based on approximated distribution. Finally, the modality reconstruction is achieved by recovering the feature representations of missing samples with the averaged combination of distribution transferring from multiple modalities. The model strives to maintain key features of the missing modality by leveraging cross-modality correlations from the distribution mapping, thus ensuring that the recovered data retains its modality-specific features. The extensively studied\n\n The extensively studied paradigm for data recovery [50]-[53] focuses on directly recovering missing data from available modalities by discovering the hidden relationship between modalities through common feature extraction. However, shared information between modalities cannot fully encompass the distinct complementary information of each modality since different modalities exhibit inconsistent feature distributions due to their distinctive characteristics as shown in Fig. 10. This means that the modality-specific feature representations cannot be reconstructed when simply relying on shared information for recovery, leading to inaccurate recovered data. To compensate for the distribution gap caused by modality-specific information, we utilize the limited data samples from the missing modality in the labeled dataset to exploit its distribution characteristics. By analyzing the correlations between various modality distributions, we reconstruct the characteristics of missing modality from the restored distribution, thereby enabling accurate data recovery."}, {"title": "2) Feature mapping:", "content": "The redundant information in high-dimensional sensor data makes it hard to visualize similarities between data samples, complicating the task of capturing the intrinsic structure of the data distribution. Fortunately, many research efforts reveals that data distributions of images and time-series signals often lie in low-rank subspaces [54]\u2013[56], which retain essential information while eliminating redundancy, providing a more concise representation of the data structure. Therefore, constructing a low-rank subspace representation of modality distributions is pivotal for effectively extracting informative modality characteristics.\nPCA [57], a well-known technology for dimension reduction, selects principal components by identifying the directions with higher singular values in the data space, ensuring that the most important characteristics of the original data distribution are retained. Inspired by this observation, our IC3M utilizes limited labeled data from the missing modality to approximate its distribution through PCA for low-rank subspace construction. This subspace representation preserves key distribution characteristics, serving as a guidance for subsequent feature mapping and data recovery.\nDenote the feature representations of the m-th modality after the feature encoder as $Z^m = {z_i^m, i = 1, ..., N}$ where $Z^m \\in R^{N\\times F}$ with the feature length of F, we perform PCA on all N samples in $Z^m$ to extract the distribution structure of the missing modality. The approximated distribution $P^m$ projected on the low-rank subspace contains the main characteristics of modality distribution, which is represented as $P^m = Z^mV^m$ where $V^m \\in R^{F\\times K}$ (K \u226a N) is the principal component matrices, representing the low-rank subspace of modality m. We select the first K principal components (e.g. K = 4), keeping the simplified yet effective representation for distribution approximation.\nThe complementary information specific to each modality causes the distribution gap between modalities, challenging data recovery with only shared information. Camera rotation further exacerbates the feature distribution differences between visual and physiological modalities, intensifying the challenge of data recovery through common features alone. To address this issue, we design a feature mapping network that learns the relationships between the distributions of different modalities. Instead of recovering missing data from common information between modalities, our IC3M transfers complementary information through mapping relationships, allowing for the reconstruction of missing modality characteristics from available modalities. By estimating the distribution of missing modality, the modality-specific features could be restored in recovered data, thereby mitigating the distribution gap and improving recovery accuracy."}, {"title": "3) Objective function:", "content": "Despite the distinct features emphasized by each modality leading to inconsistent distributions, different modalities share similar semantics as they capture various aspects of the same event within a common context. This semantic similarity unleashes the potential for distribution transformation between modalities by aligning their semantic representations. Specifically, using the distribution approximations of various modalities obtained through PCA, we learn a feature mapping function $F_{m'}$ that maps the distributions of available modalities $P^{m'}$ to the missing modality $P^m$ based on semantic similarity, expressed as $P^m= P^{m'}F_{m'}$. This distribution mapping transfers the complementary information across feature distributions from different modalities, thereby bridging the distribution gaps between them.\nTo recover missing data, we first reconstruct the distribution of missing modality through feature mapping to restore the modality-specific feature representation, and then estimate the original data based on its low-rank subspace. Since the principal components $V^m$ allows the projection of approximated distribution $P^m$ back to the original data distribution $Z^m$ through $Z^m = P^m(V^m)^T$, the reconstruction of the missing modality $\u017c^{m}$ can be estimated as follows:\n$\u017c^{m} = (P^{m'}F_{m'})(V^m)^T = Z^{m'}(V^{m'}F_{m'})(V^m)^T$ (14)\nWhen multiple modalities are available for feature mapping, our IC3M averages the estimated distributions from each modality. By integrating information from all modalities, this averaging process helps smooth out outliers and biases present in distribution transformations, hence providing a more stable and reliable estimation. We implement the mapping relationship $F_{m'}$ as a neural network with trainable parameters $W_m$, thus for each missing data sample $x^m$, its recovered feature representation $zin$ can be expressed as the average of feature mappings from other modalities $z^{m'}$, which is given by\n$zin = \\frac{1}{M-1} \\sum_{m'} z^{m'}W_m.(V^m)^T$ (15)\nwhere Wm is used to convert the feature representations zm'\nof other modalities into the feature representations zm ofmissing modality.\nSince the low-rank subspace Vm of the missing modalityretains its distribution characteristics, the recovered data canalign with missing modality distribution, overcoming the lossof modality-specific information due to distribution inconsistency among modalities and thus accurately restoring the data.The recovered data im, along with other available data zm',are then fed to the adaptive threshold pseudo-labeling modulein Sec. III-B for further training to identify abnormal statuswith the semi-supervised learning framework."}, {"title": "IV. IMPLEMENTATION AND EXPERIMENTAL SETUP", "content": "4) Meta training framework: By training the feature mapping network to learn the optimal parameters Wm, the distribution transformation could better capture the relationships between modalities, thereby achieving more accurate modality reconstruction. However, severe modality missing causes the data distribution mismatch with the training data, requiring the feature mapping network to extract more generalized distribution relationships applicable to various feature distributions. Moreover, image distortion increases the diversity and inconsistency of features, imposing higher demands on the alignment of consistent semantic representations across different modalities. As discussed in Sec. II-A, the amount of labeled data is limited for training, hence improving the model ability to learn the distribution mapping relationships within limited training data becomes more challenging.\nAs an emerging paradigm recently, meta learning [59", "62": "aims to guide the model in acquiring prior knowledge on how to learn new knowledge with only a few training samples. Rather than training on a single data distribution, the model is exposed to a diverse set of data distributions, allowing it to recognize the common patterns across them. This exposure enables the model to leverage similar patterns when encountering new data distributions, thereby reducing the need for extensive training data. Taking its advantage of the fast learning capacity in few-shot training [60"}, {"62": "we adopt meta-training to learn the informative mapping relationship among different modality distributions efficiently.\nTo learn feature mapping relationships effectively, IC3M evaluates the performance of modality reconstruction network on various batches of modality-complete samples within limited labeled data. The modality reconstruction model seeks to minimize the overall loss across those data according to Lrecover in Eqn. (18), thereby learning generalized distribution mapping relationships that facilitate quick adaptation to new data and efficient recovery across diverse distributions.\nTherefore, during meta-training, we encourage the semi-supervised learning framework to identify distinctive features of different status in the presence of modality missing. The semi-supervised learning framework is first initialized with the current parameters of the modality reconstruction network from a well-optimized starting point, and then updates its parameters according to Lssl in Eqn. (12) using both the available data and the recovered data from the missing modality. This initialization ensures that the learned cross-modality relationships are leveraged for data recovery, providing pre-trained knowledge to enhance the generalization of status recognition from limited labeled data. Finally, the modality reconstruction model is evaluated with the updated parameters of the semi-supervised learning model, and the overall loss for jointly optimizing both models is expressed as\n$L_{all} = L_{ssl} + L_{recover}$ (19)\nThe complete training procedure of our IC3M is outlined in Algorithm. 1. With the limited labeled data, we apply the meta-training framework to simultaneously optimize the semi-supervised learning model and the modality reconstruction network with the overall loss Lall, exploring the discriminative information for status classification with accurate data recovery. For the missing data, we first utilize the modality reconstruction module to predict its feature representations, and then apply to the proposed confidence-based semi-supervised learning framework. The high-confidence pseudo labels are selected by the adaptive threshold adjustment which mitigates class imbalance, while the remaining low-confidence unlabeled data are fully utilized with contrastive learning, facilitating feature extraction for informative feature representation.\nIn this section, we demonstrate the detailed implementation of our IC3M system in the in-car status monitoring application using Stressors dataset [17", "63": "."}]}