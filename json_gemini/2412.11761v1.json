{"title": "Harnessing Language for Coordination: A Framework and Benchmark for LLM-Driven Multi-Agent Control", "authors": ["Timoth\u00e9e Anne", "Noah Syrkis", "Meriem Elhosni", "Florian Turati", "Franck Legendre", "Alain Jaquier", "Sebastian Risi"], "abstract": "Abstract-Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. A promising but largely under-explored area is their potential to facilitate human coordination with many agents. Such capabilities would be useful in domains including disaster response, urban planning, and real-time strategy scenarios. In this work, we introduce (1) a real-time strategy game benchmark designed to evaluate these abilities and (2) a novel framework we term HIVE. HIVE empowers a single human to coordinate swarms of up to 2,000 agents using natural language dialog with an LLM. We present promising results on this multi-agent benchmark, with our hybrid approach solving tasks such as coordinating agent movements, exploiting unit weaknesses, leveraging human annotations, and understanding terrain and strategic points. However, our findings also highlight critical limitations of current models, including difficulties in processing spatial visual information and challenges in formulating long-term strategic plans. This work sheds light on the potential and limitations of LLMs in human-swarm coordination, paving the way for future research in this area. The HIVE project page, which includes videos of the system in action, can be found here: hive.syrkis.com.", "sections": [{"title": "I. INTRODUCTION", "content": "The growing capabilities of Large Language Models (LLMs) have opened up new frontiers in artificial intelligence, including enabling human-AI collaboration across diverse and complex domains [1]\u2013[3]. While much of the existing research focuses on LLMs' proficiency in tasks like natural language understanding and generation, their potential for coordinating is a new area of exploration [4], [5]. This ability can be particularly critical in scenarios such as disaster response, urban planning, and strategy games, where efficient coordination of multiple agents can significantly impact outcomes.\nThis paper introduces a novel framework called HIVE (Hybrid Intelligence for Vast Engagements) designed to facilitate such coordination by enabling natural language-based control of thousands of agents in real-time. Leveraging the strengths of LLMS, HIVE translates high-level human instructions into detailed operational plans for agent swarms. Specifically, after receiving a high-level strategy from the player in natural language, HIVE generates a plan using a simple domainspecific language. This plan assigns each unit a target position on the map and a behavior tree, which takes an intelligent action based on its local observations at each step. Fig. 1 illustrates an example where HIVE responds to the player's prompt and devises a successful plan in a defense scenario."}, {"title": "II. RELATED WORK", "content": "Recent work has demonstrated the potential of large language models (LLMs) and vision language models (VLMs) in complex strategic games. Cicero [3] showed that LLMs can achieve human-level performance in Diplomacy through natural language negotiation and tactical coordination. SwarmBrain [6], an agent for StarCraft II, combines high-level LLMbased strategic planning with low-level non-LLM-based tactical execution. These works demonstrate that LLMs can reason about and execute strategic decisions in game environments.\nHowever, controlling multiple units in real-time strategy (RTS) games presents unique challenges. A proposed approach is hierarchical command and control architecture, tested on StarCraft, combining high-level and low-level reinforcement learning agents, attempting to balance micro and macro management (i.e., we want an overall strategy and quick low-level decision) [7]. Recent work has also used behavior trees for low-level actions along with LLMs for high-level control [8].\nFurthering the ability to conduct AI research in the RTS domain, the StarCraft Multi-Agent Challenge (SMAC) environment [9] established key benchmarks for multi-agent control [10], [11]. SMACv2 [12] extends this work, introducing procedurally generated scenarios requiring generalizations. Recently, JaxMARL's SMAX [13] has enabled easy parallelization in an SMAC-like environment, lowering the barrier of entry for research into AI applied in an RTS setting. However, while these frameworks provide foundations for developing multi-agent control systems, they focus on tactical execution rather than strategic planning, leaving the integration with higher-level decision making under-explored.\nThe development of LLM-based multi-agent systems has seen significant recent research. A comprehensive survey discussing key aspects of agent profiling, communication, and environment interaction has also been written [14].\nAgentCoord [15] is a visual interface designed for coordination strategies in multi-agent collaboration to solve a joint goal. Remaining challenges in multi-agent LLM systems include effective task allocation, robust reasoning, and efficient memory management [16]. A key limitation across these systems is the computational overhead of LLM inference, which makes real-time applications challenging without significant architectural compromises (partly motivating hybrid systems like SwarmBrain in which low-level execution is done without the LLM).\nRecent research has explored how LLMs handle strategic reasoning tasks. Zhang et al. [5] surveyed the strategic reasoning capabilities of LLMs, highlighting their ability to understand game structures and adapt to different contexts. Kram\u00e1r et al. [17] investigated how artificial agents can use communication for better cooperation in strategic games (Diplomacy). Lor\u00e8 and Heydari [4] analyzed how different LLMs are affected by contextual framing (i.e. the superficial story we tell about the game) and game structure.\nHowever, in sufficiently complex and dynamic environments, LLMs and VLMs struggle to perform well [18], sometimes even performing worse when visual information, like an image of the game map, is included. LLMs struggle with physical common-sense reasoning in 3D environments, performing worse than human children on basic spatial reasoning tasks [19]. Further, long-range reasoning problems are exasperated as these models are prone to hallucinations [20]. Though algorithms has been proposed to fortify against hallucination in multi-step reasoning by dividing up reasoning steps, the problem of long-range causal reasoning remains unsolved [20]. This is particularly problematic in strategy games, where decisions must build on each other, and errors compound over time. AndroidArene [21] is a benchmark, and environment that evaluates an LLM's ability to navigate an operating system (e.g. using a smartphone with apps), showing this to be a current limitation of LLMs.\nAdditionally, models struggle to generalize beyond their training data when moving towards geospatial modalities, sparking the need for dedicated Vision-Language GeoFoundation models [22]. This limitation is particularly relevant for strategy games involving spatial reasoning and map awareness, where current models often fail to develop coherent long-term strategies based on terrain and unit positioning (and sometimes struggle to identify basic landmarks). This echoes recent research that shows VLM often struggle with basic tasks (e.g., do two circles intersect?) that are trivial for humans, and a necessity for spatial reasoning [23]."}, {"title": "III. THE HIVE APPROACH", "content": "Our approach, HIVE, allows a player to control thousands of units in a strategy game through a natural language LLMbased dialog. We hypothesis that this type of human-machine collaboration should be able to alleviate some of the aforementioned shortcomings of purely LLM-based approaches.\nFig. 2 presents an overview of its main components. (a) The player can communicate in natural language and add markers to the map to give absolute positions (Sec. III-C). (b) The LLM writes a plan using a domain-specific language that controls the behavior of all units (Sec. III-D1). (c) The plan assigns a target position and behavior tree to each unit when created or triggered by objective checking (Sec. III-D2). (d) Each unit evaluates its behavior tree using local observation to determine an action (Sec. III-B). Actions in our game are either attacking an enemy unit or moving. (e) After each step, the game checks the current plan's objectives (Sec. III-D3). If some are achieved, the plan is rolled forward, and the active steps are updated. This module also checks for global winning and losing conditions specific to each scenario. (f) At the start of the discussion, the LLM receives information about the types of each unit present (Sec. III-E). When triggered by the player, the LLM also receives a precomputed textual description of the map (see Sec. IV-F for a discussion of the geo-visual abilities of LLMs) and the current health and position of each unit. (g) After each game step, the units' positions are plotted on the map and displayed on the player's screen (Sec. III-A6).\nOur implementation allows the player to interact with HIVE during plan execution, potentially enabling updates as the plan unfolds. However, for this initial evaluation of the framework, we chose not to utilize this feature to allow for a simpler analysis. Another provided functionality is to restart the same scenario after a game to try again and improve the current score. In such cases, HIVE retains the discussion history but does not include information about the previous game, primarily due to the limited context size of the model. We will now go over the different HIVE components in more detail."}, {"title": "A. The multi-agent game", "content": "The game is a top-down strategy game where the player commands an army of several hundred to thousands of units against an opposing army of comparable size. Players control their army exclusively by interacting with HIVE, which develops a plan that determines each unit's behavior at every step. The game also allows us to create various scenarios by using different maps and setting global objectives.\n1) Global objectives: There are two kinds of objectives. One is the elimination objective, where the goal is to eliminate a set of enemy units (in this paper, the global elimination objectives require the elimination of all enemy units, but HIVE can choose to target a specific set when writing a plan). The other one is the position objective, where the goal is to reach a given position with at least one or all units (in this paper, the position objectives for the scenarios require reaching a given location with at least one unit. In contrast, position objectives in a plan require the whole group of units to reach it). These objective specifications allow for creating scenarios with more offensive, defensive, or stealth components. Once either side has achieved its global objective, the game ends.\n2) The units: The framework allows us to create any type of unit parametrized by its move speed, max health, attack damage, and attack range. In this paper, we used three types of units. The spearman a slow close range unit with high health. The archer a long range unit with low health. The cavalry a fast close range unit with medium health.\nThose three types of units allow for rock-paper-scissor dynamics, as a group of archers easily eliminate spearmen thanks to their longer range, a group of cavalry easily eliminate a group of archers by coming in contact faster than they get killed, and a group of spearmen easily wins against a group of cavalry because of similar damage but higher health. Tab. II in the appendix details the characteristics of those units.\n3) The observation: Each unit knows the position and health of every unit inside its sight range (set to 15 m) and also has access to a distance map, i.e. a matrix containing the distance to the unit's target position from anywhere on the map. This enables it to follow the shortest path to its target position.\n4) The actions: At each step, each unit takes one action: do nothing, move to any position in sight reachable given the unit's speed (2D continuous action), or attack an enemy unit in sight (i.e. in sight range with no visual obstacle in between) that is in attack range (discrete action).\n5) The terrain: The game allows one to easily implement different types of terrain. In this paper, we use four types:\n\u2022 Normal: the units can move and see through it;\n\u2022 Forest: the units can move but not see through it;\n\u2022 Water: the units can see over it but not move through it;\n\u2022 Building: the units cannot move or see through it.\nThose allow the creation of diverse maps and scenarios with strategic features such as choke points over bridges (normal terrain over water) or stealth opportunities by hiding in forests.\n6) The visualization: At each time step, the game plots the units' positions over the map using different shades of colors (blues for the player's units and reds for the enemy units) and shapes (square for the spearmen, circle for the archers, and triangle for the cavalry). The player does not receive information about each unit's health (i.e. it would be difficult to assess the health of hundreds of units in real-time [25]).\n7) The main loop: The main loop of the game is implemented in JAX [24] and proceeds as follows:\n1) Apply the attack action of each attacking unit;\n2) Apply the move action of each moving unit while checking for collision with building or water terrain;\n3) Push units in collision with each other;\n4) Check for collision with a building or water terrain;\n5) Compute the distance between each unit, considering the line of sight with building and forest terrain."}, {"title": "B. Behavior Trees", "content": "The framework includes an important middle layer between the high-level instruction of the player written down as a plan and the low-level actions taken by the units. We choose behavior trees [26] for this purpose. They offer a convenient way to control agents while being easy to design and understand. They are composed of four different types of nodes:\n\u2022 Sequence node S: stops at the first failed node;\n\u2022 Fallback node F: stops at the first successful node;\n\u2022 Condition node C: evaluates a condition given the unit observation and returns failure or success;\n\u2022 Action node A: tries to perform an action given the unit observation and returns failure or success.\nHIVE evaluates each active behavior tree sequentially, as they correspond to different JAX instructions. Still, each behavior tree evaluation is done in parallel over its assigned units. This evaluation slows down as more behavior trees become active and their size (i.e. number of nodes) increases.\nFor this paper, we designed a list of condition and action nodes parametrized by qualifiers (see Sec. C in the appendix for the full Lark grammar). The available conditions are:\n\u2022 Is there an ally or enemy of a given type in sight?\n\u2022 Is there an ally or enemy of a given type in sight that is or could be in attack range in one, two or three steps?\n\u2022 Am I or could I be in attack range in one, two, or three steps of an ally or enemy of a given type in sight?\n\u2022 Am I of a given type?\n\u2022 Do I have health below a 75%, 50%, or 25% threshold?\n\u2022 Am I in a forest?\nThe available actions are:\n\u2022 Do nothing;\n\u2022 Move toward the target position (following the shortest path with some noise) up to a given threshold relative to the unit sight range and move speed;\n\u2022 Move toward the closest or farthest or weakest or strongest or random ally or enemy unit in sight of a given type (without optimal path guaranty);\n\u2022 Attack the (closest or farthest or weakest or strongest or random) enemy in attack range."}, {"title": "C. The interface between the player and HIVE", "content": "The player can only control their army through an interface with HIVE. Its front end is composed of an LLM that receives natural language prompts from the player. HIVE can reply to the player prompts by answering questions or asking some.\nControlling many units to go to different positions on a map can be conveniently and quickly done through natural language commands. For example, with large unit numbers, asking the system to send units to cover all bridges can be faster than using a traditional mouse interface. However, sometimes it can still be useful to communicate a specific absolute map position to the model. This is why HIVE includes the ability for the player to click on the map to add markers labeled with a letter that can then be referenced in the dialog with the model.\nThe core component of HIVE is an LLM. The tasks' specifications are instructed via its context, allowing us to easily test different LLMs for their ability to perform in our multi-agent benchmark. The LLM inference is the most timeconsuming part of our system, as the model may take between a few seconds and several dozen to respond (see Fig. 7). In this paper, the game starts only when the LLM answer is received, and the plan is parsed and applied."}, {"title": "D. The plan", "content": "1) Writing a plan with our Domain Specific Language: HIVE uses a domain-specific language to write a plan that follows the player's high-level commands. A plan is composed of several steps. A step is composed of several components:\n\u2022 A numerical label to refer to it;\n\u2022 A list of prerequisite steps that need to be achieved before this step (e.g., waiting for two groups to reach their assigned locations before ordering them to attack);\n\u2022 An objective (similar to the global objectives) that is used to infer if the step is achieved, active, or inactive;\n\u2022 A list of groups of units that dictates the behaviors of each unit concerned by the step.\nA group of units is composed of a list of unit IDs, a target position, and a behavior tree. The step is valid only if each unit is included in no more than one group.\nWe instruct the LLM (using a System prompt) on the rules to write a plan with a simple example and a short list of mistakes to avoid. The actual instruction message used for Fig. 1 is detailed in the appendix Sec. A1. The plan written by the LLM is parsed and an error is raised if it is not grammatically correct or if the units' or behavior trees' IDs are incorrect.\n2) Applying the plan: When the plan is created or when the active steps change, HIVE passes through each active step and assigns a behavior tree and target position to each unit. If a unit is present in several active steps, the last one will be assigned. If it does not have a new assignment, it keeps its previous assignment. If a unit never receives an assignment, its default behavior is to do nothing. This part of the system can also be time-consuming since (1) each target position requires computing a breadth-first search on the full map, and (2) the reassignment of the behavior trees and the steps' objectives checking requires a new compilation of the JAX functions.\n3) Checking the plan: After each step in the game, HIVE checks the global and active step objectives to see if one side is victorious, if the plan is fully unrolled, or if new steps become active. In this paper, we stopped the game if the plan was fully unrolled, but no side was victorious, as we suppose that this was a flaw in the plan's design. HIVE could also ask the player to present new instructions."}, {"title": "E. LLM Game Information", "content": "The LLM receives both information about each unit and information about the map, which we detail below.\n1) Unit information: The LLM gets a complete description of the unit types and the current compositions of both sides. After each player prompt, it receives the units' health and positions. An example is in the appendix Sec. A2.\n2) Textual description of the map: During preliminary experiences, we tried using visuals but found that current allpurpose LLMs are inefficient in understanding a top-down map and can not give accurate positioning of the units, terrain, or landmarks. Thus, we rely on a precomputed description of the map that gives the absolute position of the different terrain types (forest and rivers) and the bridges' positions. Sec. IV-F compares HIVE's abilities with the textual prompts and with different variations of inputting an image of the map. Fig. 11 and 12 show two examples of such descriptions."}, {"title": "IV. EXPERIMENTS", "content": "We test HIVE's effectiveness in following the player's command using five ability tests on four scenarios (Fig. 4).\nIn all scenarios, the enemy units' behavior is identical: the spearmen and cavalry units attack in close combat, and the archers attack in long range, moving away from close combat. The maps were designed with the INKARNATE map-making platform (https://inkarnate.com/).\n1) Coordinate: The player controls 1000 units (500 spearmen and 500 archers) and must eliminate the opposite army of 1000 spearmen from the north. This tests the ability to coordinate many units with two types on a simple terrain. With the map being 150 m wide and each unit only able to see 15 m ahead, the challenge is to spread the units on the battlefield. The other challenge is efficiently placing the archers behind the spearmen to minimize casualties.\n2) Exploit weakness: The player controls 750 units (250 spearmen, 250 archers, and 250 cavalry) and must eliminate the opposing army of equal composition. The map is divided into four quadrants by vertical and horizontal rivers crossed by bridges. The opposite army is divided by types on three quadrants, the player's army starting in the last one. This tests the ability to exploit the rock-paper-scissor dynamics to win while minimizing casualties. This demands that HIVE estimate the opposite army battalions' positions and unit type, and send the unit type strong against it. HIVE is given the strengths and weaknesses of each unit type as in-context information.\n3) Follow markers & Exploit terrain: The player controls 300 spearmen starting on the northeast corner and must bring at least one to an objective position in the southwest corner. The units must cross a bridge guarded by 1200 opposite units (600 spearmen and 600 archers). The challenge is to efficiently use the cover of the trees (the units cannot fight in the forest) to minimize casualties. We use this scenario for two ability tests: Follow markers, which tests the ability to follow absolute positions given by the player as markers, and Exploit terrain, which tests the ability to exploit the terrain by following the player's textual description of the path to follow.\n4) Strategize points: The player controls 700 units (350 spearmen and 350 archers) and must prevent the opposite army of 900 spearmen from reaching the center of the camp (the fire camp in the center of the island). The opposite units are divided in the northeast and southwest corners and are designed to split and path through each bridge. This tests the ability to split the army on strategic points correctly."}, {"title": "B. The tested LLMs", "content": "HIVE is agnostic to which LLM is being used (apart from the change in API to query the model). We evaluate HIVE using six different models, to determine which one works best together with a human user when tested on our ability tests. We chose state-of-the-art closed-source models from GPT-4o and Claude families and one small open model as a baseline:\n\u2022 4o: gpt-4o-2024-11-20 [27];\n\u2022 4o-mini:gpt-4o-mini-2024-07-18 [27];\n\u2022 oi-mini: 01-mini-2024-09-12 [27]:\n\u2022 Sonnet: claude-3-5-sonnet-20241022 [28];\n\u2022 Haiku: claude-3-5-haiku-20241022 [28];\n\u2022 Llama3 (8B): Llama3 (8B) [29].\nApart from oi-mini, which is a preview with a fixed temperature of 1, all models are used with a temperature of 0."}, {"title": "C. HIVE ability tests", "content": "While experimenting, we found that the LLMs' proposed plans were susceptible to just one word. To evaluate them, we decided to generate 10 slightly different prompts for each of the five abilities. For example, to test the coordination ability, we query HIVE with \"Make a plan that forms as many squads as you think necessary to cover the middle row of the map to eliminate all the enemies as quickly as possible. Ensure to place our archers so that they are protected by our close range units.\". The slight variations are the orders of saying \"Design a plan,\" \"Write down a plan,\u201d or \u201cMake a plan,\" using \u201cNorth/South/East/West\u201d or \u201cTop/Down/Left/Right,\u201d or saying \"archers units\" or \"long-distance units.\" All the prompts are detailed in the appendix Sec. F. HIVE waits for the LLM's answer, parses a plan from it, and, if it is valid, applies it to the game until one side achieves its objective or a timeout is reached. Otherwise, the attempt is flagged as invalid.\nFig. 5 shows snapshots of a successful plan execution made by HIVE using different LLMs with the corresponding prompt from the player. The actual plans are detailed in the appendix Sec B. Tab. I presents the strict successes of HIVE for each modality. Fig. 6 presents a continuous evaluation of the performance of the plans made by HIVE. Using the percentage of enemies eliminated for the abilities (a), (b), and (e), and the distance to objective position for (c) and (d).\nFig. 8 shows the result of the execution in terms of winning, losing, timeout, and achieving the plan objectives without achieving the winning objective; the plan being invalid and HIVE returning no plan. A video showing all the plans being executed is available here: hive.syrkis.com.\nHive solves all ability tests, highlighting that it effectively possesses all the proposed abilities. It can effectively split the units on a battlefield, putting the long range units in the rear to increase efficiency. Additionally, it can recognize clusters of enemy units with specific weaknesses and exploit them."}, {"title": "D. How does HIVE scale up with the number of units", "content": "To see how HIVE scales with the number of units, we performed another evaluation on the Coordinate test, varying the number of units from 200 to 4000. We stopped at 4000 due to the current hardware limitation of the machine running the experiment. As the game's main loop needs to be compiled, the size of the distance matrix between each unit grows quadratically with the number of units. For this study, we only tested on 4o and Sonnet, which showed the best performances. Fig. 9 shows the result of the plan made by HIVE. There is no significant conclusion regarding the success rate as there is too much noise due to the inherent variance of the LLMs. Still, 4o's number of invalid and empty plans increases as the number of agents increases. This is not observed for Sonnet, again highlighting its superior performance.\nOne limitation with the scaling for the current implementation is that HIVE sends the position and health of each unit to the LLM, in the context window, and the context size is limited (128,000 tokens for 4o and 200,000 for Sonnet). To get an idea, for 4o, the context went from 4,271 tokens with 200 units to 38,475 with 4,000 units (with an affine scaling), i.e. approximately 9 tokens per unit, which gives a hard limit of 128,000/9 = 14,222 units for 4o. Similarly, for Sonnet, it gives a hard limit of 200,000/9 = 22, 222 units."}, {"title": "E. Can HIVE win alone?", "content": "An interesting question is how well HIVE performs without a human's help. To answer this question, we use ten variations of the prompt: \"First, analyze the situation to find a good strategy to win this mission, then write down the corresponding plan.\" (the variations are detailed in the appendix Sec. G) on each of the four scenarios for 4o and Sonnet as they were the best LLMs. HIVE always gives the scenario description so that the LLM knows the global objective.\nFig. 10 shows the comparison between HIVE with and without the player's help. For Coordinate and Exploit terrain both models no longer wins without the human help. For Strategize Points Sonnet conserves its three wins but both models have lower median performances. This shows that HIVE is better with human help, confirming the benefits of a hybrid intelligence approach. Additionally, when comparing the median performance, both Coordinate and Exploit Weakness seem to be easier ability tests for HIVE alone as they only require sending units toward the enemies. In comparison, Exploit terrain and Strategic points require exploiting the map features to win and depend on more long-term planning."}, {"title": "F. Does HIVE need the textual description of the map?", "content": "We evaluate HIVE's vision abilities by replacing the textual description of the map with the image. As it has to come up with precise target positions, we also compared two different ways to improve the image-to-coordinate: using a grid with x and y axis and using a scaffolding [30]. We only tested the vision on two ability tests that required it: Exploit terrain and Strategize points. Fig. 11 and 12 show the four kinds of inputs HIVE uses as the description of the map. Fig. 13 shows the continuous measure of performance. Without the textual description, HIVE's performance decreases. For Exploit terrain, 4o seems to be able to use the raw image while Sonnet performance completely drops. For Strategize points, Sonnet won once with the raw image and twice with the scaffolding, compared to 3 times with the textual description, but the lower median performances."}, {"title": "V. DISCUSSION", "content": "HIVE using Llama3-(8B) has very poor results, mainly due to sending invalid plans. This may be solved by finetuning it to our specific DSL. HIVE would need to rely on big generalist models that one can only call via an API, increasing the response time. Those models know many things that are irrelevant to our task. By taking a smaller model and fine-tuning it, HIVE could run locally and, using dedicated hardware, could even become fast enough to run continuously alongside the game to give real-time feedback to the player or adjust the plan as the game unrolls.\nFine-tuning could also improve LLMs' poor ability to read the map. Another solution would be to use a specialized module to looks at a map and returns a textual description of its interesting features. For example, the module could display the location of different terrains and landmarks that the player can intuitively use to communicate placements.\nEarly experiments showed that generalist LLMs were not good at writing their behavior trees with our out-of-distribution DSL grammar. Fine-tuning an LLM for a specific grammar and repertoire of condition and action nodes could drastically extend the diversity of behaviors available to the player.\nNonetheless, in addition to the cost of fine-tuning an LLM, it also reduces the applicability of HIVE. These new challenges were never present in any training dataset of the models. Nevertheless, HIVE using Sonnet won more than the total loss (19 against 14 over 50 games). A generalist model could allow HIVE to perform satisfyingly on other massively multi-agent games, such as worker and resource management games. The main change would be the nodes in the behavior trees."}, {"title": "VI. CONCLUSION", "content": "In this paper, we present a new challenge for LLMs as human assistants to control up to two thousand units in a strategy game. We propose a new framework, HIVE, to allow a player to give high-level commands that an LLM translates into a long-term plan that controls the behavior of each unit. We showed that generalist LLMs such as Claude Sonnet and GPT-4o can handle such tasks but are still sensitive to slight changes in the player's prompts. Complimentary experiments showed that HIVE requires human help to get the best performance and that generalist LLMs' visual capacity to use an out-of-distribution map for terrain and landmark locations is still to be improved. This work opens many interesting avenues for improving LLMs' capacities to collaborate with humans, such as improving their map-reading abilities, reducing their sensitivity to prompts, and increasing their long-term planning."}, {"title": "B. LLM answer and plan to Fig. 5 examples", "content": "1) Coordination with Sonnet:\nI'll help you create a plan to effectively eliminate the enemies. Looking at the positions, I notice that all enemy units are currently in the Northern Forest (y-coordinates between 135-149), and they will need to move south through the middle of the map.\nLet's analyze our forces:\nWe have 500 spearmen (IDs 0-499)\nWe have 500 archers (IDs 500-999)\nGiven the map's width (150) and our need to protect archers, I suggest creating three main battle groups across the middle of the map, each with both spearmen and archers. The spearmen will form a protective front line while archers attack from behind.\nHere's the detailed plan:\nBEGIN PLAN\nStep 0:\nprerequisites: []\nobjective: position\nunits: [0:167]\ntarget position: (25, 75)\nbehavior: attack_in_close_range any\nunits: [167:334]\ntarget position: (75, 75)\nbehavior: attack_in_close_range any\nunits: [334:500]\ntarget position: (125, 75)\nbehavior: attack_in_close_range any\nunits: [500:667]\ntarget position: (25, 65)\nbehavior: attack_in_long_range any\nunits: [667:834]\ntarget position: (75, 65)\nbehavior: attack_in_long_range any\nunits: [834:1000]\ntarget position: (125, 65)\nbehavior: attack_in_long_range any"}, {"title": "1) (a) Coordinate:", "content": "0. Make a plan that forms as many squads as you think necessary to cover the central row of the battlefield to eliminate all the enemies as fast as possible. Judisiously place our long range units so that they are not in close combat.\n1. Design a plan that forms as many group of units as you think necessary to cover the central row of the battlefield to eliminate all the enemies as quickly as possible. Judisiously place our long range units so that they protected by our close range units.\n2. Write down a plan that forms as many group of units as you think necessary to cover the central row of the map to eliminate all the enemies as fast as possible. Ensure to place our long range units so that they are not in close combat."}]}