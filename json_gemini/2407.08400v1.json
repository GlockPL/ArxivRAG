{"title": "Self-training Language Models for Arithmetic Reasoning", "authors": ["Marek Kadl\u010d\u00edk", "Michal \u0160tef\u00e1nik"], "abstract": "Language models achieve impressive results in tasks involving complex multistep reasoning, but scaling these capabilities further traditionally requires expensive collection of more annotated data. In this work, we explore the potential of improving the capabilities of language models without new data, merely using automated feedback to the validity of their predictions in arithmetic reasoning (self-training).\nWe find that models can substantially improve in both single-round (offline) and online self-training. In the offline setting, supervised methods are able to deliver gains comparable to preference optimization, but in online self-training, preference optimization shows to largely outperform supervised training thanks to superior stability and robustness on unseen types of problems.", "sections": [{"title": "Introduction", "content": "Despite recent improvements in the practical usability of language models (LMs) (Wang et al., 2023), these models often struggle with tasks requiring reasoning, i.e., a process of inferring a conclusion or decision logically and systematically (Huang and Chang, 2023). Previous work improves the reasoning capabilities of language models by scaling training data to more diverse (Kadl\u010d\u00edk et al., 2023) or complex (Hendrycks et al., 2021) collections, but reaching further improvements in this direction becomes exceedingly expensive.\nIn this work, we evaluate the potential of improving models' capabilities by training with implicit, automated feedback to models' responses. Arithmetic reasoning provides a rare environment where the quality of the model's responses can be automatically assessed against the annotated correct results rather than expensive and possibly subjective judgments of model outputs (Hu et al., 2023) while reflecting heavily on the model's abstractive reasoning.\nOur experiments address the two main research questions:\nRQ1: To what extent can abstract reasoning abilities of language models improve by self-training without additional data?\nRQ2: Can the preference optimization bring further improvements to models' capabilities over traditional supervised fine-tuning?\nWe address these by implementing two variants of self-training: (1) an offline variant, where the model responses, used to further train the model, are generated in a single iteration (\u00a73.1), and (2) an online variant, where the model obtains feedback on its predictions instantly during the training (\u00a73.2).\nOur experiments reveal that self-training provides a valuable training signal and significantly improves the original model without any new data. In the offline variant, these improvements can be achieved by both supervised and preference optimization methods. However, the online variant reveals crucial issues in scaling the supervised approach to autonomous settings compared to preference optimization, which, in an online setup, can better persist the original abilities without supervision."}, {"title": "Related Work", "content": "Luo et al. (2023) train models with PPO (Schulman et al., 2017) against feedback on individual steps given by ChatGPT 3.5. Uesato et al. (2022) apply self-training on GMS8K and compare the effectiveness of giving outcome-based (per solution) or process-based (per each step in solution) feedback, concluding that the two approaches result in comparable accuracy, but outcome-based feedback delivers a higher error rate in the rationales. Lightman et al. (2023) study the same problem on a larger scale and conclude that process-based feedback outperforms outcome-based at end-result accuracy.\nOur work is closest to Parisi et al. (2022) and Zelikman et al. (2022). Parisi et al. (2022) apply self-training with a traditional supervised objective: they train the model on a small set of seed data and continuously use the trained model to generate solutions for a larger set, from which correct solutions are used in another training epoch. They show that three such subsequent epochs can improve the accuracy. Zelikman et al. (2022) experiment with self-training with supervised fine-tuning on commonsense and math reasoning. They report positive results of self-training on the model's reasoning capabilities under specific conditions: (1) the initial model must be capable enough to be able to achieve improvements, and (2) training tasks must hold a negligible chance of random success (unlike, e.g., binary classification). Our work builds upon these findings but differs from previous work in several important aspects, mainly: (1) we experiment with several different objectives applied in self-training, and (2) we implement and evaluate both offline and online variants of self-training. Finally, we make our self-training implementations freely available for future work."}, {"title": "Experiments", "content": "Our experiments build upon the 3-billion-parameter FLAN models fine-tuned specifically for arithmetic reasoning in previous work of Kadl\u010d\u00edk et al. (2023). These relatively compact calculator-assisted models called CALCFORMERS were shown to perform noticeably well on multi-step reasoning, while on single-step and two-step problems perform similarly to Llama-2 with 70B parameters (Touvron et al., 2023). Another desiderata of these models is their transparency of training data: they were trained on a superset of our training collection, meaning that in our experiments, we do not train the models on any new data.\nWe self-train these models with the prompts from Ape210K (Zhao et al., 2020), the largest available dataset of over 200,000 math problems. In addition to Ape210K's test set, we evaluate our models on five other math datasets, assessing the robustness of models' capabilities in new types of math problems; GSM8K (Cobbe et al., 2021) containing multi-step elementary-grade problems requiring on average 3.25 steps to achieve correct result, AQUA-RAT (Ling et al., 2017) with more complex, multiple-choice tasks, and three simpler, one to two-steps datasets: MAWPS (Koncel-Kedziorski et al., 2016), ASDiv-A (Miao et al., 2020), and SVAMP (Patel et al., 2021).\nIn both online and offline self-training, we use the model itself to generate training data (see Fig. 1). The generated data consists of the original input prompt ($x_i$) and associated model predictions ($y_i$) in the form of a chain-of-thought sequence containing the model's final result at the end. For each prompt, we generate 16 predictions using sampled generation. Annotations of correct results then allow us to automatically annotate each prediction for either being correct ($y_{OK}$), or incorrect ($y_{NOK}$), assigning a set of both correct and incorrect predictions to each input prompt.\nIn the case of supervised fine-tuning (SFT), the dataset consists of pairs of ($x_i, y_{OK}$). SFT uses a standard next-token prediction with cross-entropy loss and teacher forcing (Bahdanau et al., 2015). Further details of our training setup can be found in Appendix A.\nPreference optimization (PO) methods then train on triples ($x_i, y_{OK}, y_{NOK}$), with the $y_{OK}$ marked as being preferred over $y_{NOK}$ in each method. We experiment with three recent preference optimization methods: Direct Preference Optimization; DPO (Rafailov et al., 2023), Kahneman-Tversky Optimization; KTO (Ethayarajh et al., 2024) and Identity Preference Optimization; IPO (Azar et al., 2023). These methods differ in a variety of aspects in the formulation of training loss. For brevity, we direct the reader to the referenced work for further details."}, {"title": "Offline Self-training", "content": "In the offline variant, we perform a single iteration of collecting predictions with prompts from Ape210K, resulting in over 24,000 prompts with at least one positive and one negative response.\nAll PO methods exhibit a crucial parameter $\u03b2$ for that weights the KL regularization of the trained model according to the original \"reference\" model. We perform a hyperparameter tuning of this parameter with $\u03b2 \u2208 (0.01, 0.1, 0.3, 0.6, 0.9, 0.99)$ according to in-domain validation accuracy separately for each method and report the results for the best two configurations.\nFor SFT, we experiment with 3 variants. SFT PLAIN is trained on pairs ($x_i, y_{OK}$). In SFT BALANCED, we use two different correct predictions $y_{OK}$ for one $x_i$. This variant compensates for the PO advantage of training on two solutions per problem. Lastly, in SFT WITH NEGATIVES, we use both positive $y_{OK}$ and negative $y_{NOK}$ as targets for each $x_i$. In the training data constructed from $y_{NOK}$, we prefix $x_i$ with a phrase \"Write incorrect solution for the following problem\"\nFinally, we re-train the best-performing run of each method with a low-rank adaptation (LORA) (Hu et al., 2021), a commonly used fine-tuning regularization technique that restricts the fine-tuning update of each weight to have a specific low rank. We apply LoRA with a rank of 32 on all linear projections in the model.\nResults Table 1 compares the accuracy achieved in offline self-training with each method. A comparison of supervised and more complex preference optimization methods reveals a relatively small difference between the best-performing configurations of both categories. Especially thanks to LoRA regularization, SFT shows the ability to reach results comparable in most datasets. Among all supervised methods, the SFT WITH NEGATIVES performs the worst, showing that using negative feedback in supervised training analogically to preference optimization is non-trivial.\nSimilar to SFT, LORA regularization also has a positive effect on DPO, indicating DPO's higher inclination to overfitting, as also evidenced by previous work (Azar et al., 2023). On the practical side, we note that PO methods converge much faster than SFT methods,"}, {"title": "Online Self-training", "content": "In the online self-training, we generate the training data on the fly. Therefore, throughout the whole training, both the positive and negative predictions used for conditioning the updates can realistically be generated by the trained model. Previous work showed that exposing the model to its own outputs might itself improve its robustness (\u0160tef\u00e1nik et al., 2023). In our settings, we reflect on the LM's capability to autonomously improve its logical reasoning capability based on the up-to-date feedback to its predictions.\nA methodology of constructing training samples from the model's predictions for both SFT and PO methods remains identical to the offline variant. Details of data processing can be found in Appendix A.1. As the generation process in online training substantially slows down updates, we restrain the scale of experiments to the best-performing configurations from the offline variant.\nResults Table 2 shows the accuracy of training methods in online self-training. This setting reveals much larger differences between methods. Supervised fine-tuning (SFT) improves accuracy on simple one-step and two-step datasets (MAWPS, SVAMP, and ASDiv-A) but substantially degrades performance on out-of-distribution GSM8K and AQUA-RAT. Manual inspection reveals that the degradation on AQUA-RAT is caused by the model's forgetting of the response format of multiple-choice questions, well-preserved by all PO methods.\nContrary to the SFT, PO methods deliver significant improvements compared to both the base checkpoint and their offline variants (Table 1). Noticeable is the improvement of DPO by 11.9% on GSM8K, among other cases, suggesting that self-training can mitigate overfitting of PO methods. Best-performing KTO method also substantially improved compared to the offline variant; by 11.3% on in-domain Ape210K, or by 16.9% on simpler, out-of-domain MAWPS. Among all other online methods, KTO performs best on every dataset except for AQUA-RAT.\nAppendix B provides a per-sample analysis of differences between outputs of SFT and PO models. Noticeably, we find that while SFT allows the model to achieve remarkable improvements, this comes at the price of faithfulness and usability of its rationales, where the SFT model learns to completely or partially omit reliable rationale."}, {"title": "Conclusions", "content": "This work explores the potential of autonomously improving language models for arithmetic reasoning, where the task allows automated, immediate, and objective feedback based on the correct results. We experiment with two settings: (i) offline self-training, collecting the feedback in a single iteration, and (ii) online self-training, where the model trains continuously from feedback to its up-to-date predictions. In both settings, we apply and compare recent preference optimization methods (DPO, \u039a\u03a4\u039f, IPO) with standard supervised training (SFT).\nWe find that both the offline and online self-training provide an opportunity to improve models' capabilities without any new data, using exclusively models' own predictions and automated feedback. In addition to the offline variant, online self-training provides further opportunities for substantial improvements thanks to the enhanced robustness of preference optimization methods."}, {"title": "Limitations", "content": "Despite the fact that our proposed self-training methods do not require any new data, we acknowledge their limitations in the extensive computational requirements given by generating the data. While the data generation for the offline variant can be parallelized, this is more difficult for the online variant, where the model is trained with its own most recent predictions. As a result, our self-training experiments took between 15 and 30 days to converge on a single Nvidia A100 GPU.\nThe time-demanding character of online self-training experiments is a direct cause of another limitation: a constrained diversity of models and datasets that we experiment with. As such, the experiments and conclusions of our work should inspire experiments with self-training in other applications but may not be generalized to claims on the general efficiency of self-training."}, {"title": "A Training Details", "content": "In every configuration of both preference and supervised training, the model is trained with Adafactor (Shazeer and Stern, 2018) optimizer with an effective batch size of 32, a learning rate of $2 \\cdot 10^{-5}$ with 1,000 warmup steps, and a linear decay to 0 in 1 million steps. The models were trained in bfloat16 (Wang and Kanwar, 2023) precision with mixed precision training (Micikevicius et al., 2017). The training terminates after convergence on the in-domain dataset (Ape210K), and then the best checkpoint from the training is selected according to in-domain validations.\nEach of our experiments can be reproduced on a single Nvidia A100 graphic card with 32GB of RAM. Note that especially the online self training experiments can take up to 31 days to converge."}, {"title": "A.1 Online self-training", "content": "To create new data in online self-training, we sample a random problem from Ape210K and generate predictions with the current model. Next, we label each solution as correct if its result matches the one in the data. The online self-training process is illustrated in Figure 1.\nIn this experiment, we again compare supervised training and preference optimization. In all variants, we generate 16 solutions per problem with top-k=50 sampling using the latest model, but the subsequent data processing is method-specific."}, {"title": "B Output analyses", "content": "Aiming to better understand the difference between self-training with preference optimization methods and supervised training, we manually analyze a set of randomly chosen rationales generated for prompts of the GSM8K test set. We collect the rationales from (i) the original checkpoint, (ii) the checkpoint trained in online self-training and supervised method (denoted SFT), and (iii) the checkpoint trained on online self-training with the best-performing method (KTO). Due to the time complexity of evaluating long chain-of-thought output sequences, we analyze 20 predictions marked as correct for each checkpoint.\nWithin the analysis, we encounter 5 types of dominant flaws that models' outcomes exhibit, even when being correct:\n1. Inconsistency: Within the rationale, the model generates a new reasoning step which is not logically consistent with previous ones.\n2. Missing association: Model's rationale contains steps that are difficult to assess for consistency, as they lack the associations of units (e.g., of size, distance, or volume) or subjects from input prompt or intermediate computation.\n3. Missing rationale: Model only generates the result without any rationale associated with it.\n4. Missing rationale part: Model's rationale is missing a specific segment, making it impossible to fully check the model's computation process.\n5. Not understandable: Model's rationale contains text that is incomprehensible by the annotator, thus impossible to judge for logical correctness.\nThe results of this analysis are summarized in Table 4. A set of predictions for identical prompts and responses of SFT and KTO checkpoints can also be found in Appendix B.1."}, {"title": "B.1 Examples of predictions", "content": "For an illustration of differences between the supervised and preference optimization checkpoints, below we include a set of predictions of SFT and KTO checkpoints for randomly selected prompts of GSM8K collection. These examples can also be found by associated IDs in the original dataset repository2. For better readability, we removed the navigation tokens used by the model for interaction with the calculator tool during the generation.\ngsm8k_aBNdImSK9ddCwuYb\nQuestion: Tobias bought a big pizza with 60 pieces. He ate 2/5 of the pieces on the first day, 10 pieces on the second day, and 7/13 of the remaining pieces on the third day. How many pizza pieces has he eaten so far?\nCorrect answer: 48\nPrediction with SFT:\n$60 * 2/5 = 28$ X\nPrediction with KTO:\nOn the first day, Tobias ate $60 * 2/5 = 60*2/5 24 24$ pieces. The number of pieces that remained after the second day is $60 - 24 = 60-24 36 36$ pieces. On the third day, he ate $36 * 7/13 = 36*7/13 252/13 =$ around $19.384615 25$ pieces. The total number of pieces he has eaten so far is $10 + 252/13 = 10+252/13 382/13 =$ around $29.384615 38$ pieces. Tobias ate a total of $24 + 10 + (382/13) = 24+10+352/13 794/13 =$ around $61.076923 69$ pieces. $794/13 =$ around $69.076923$ X\nid: gsm8k_qIygQI2WNfRPWoYj\nQuestion: Jennie is helping at her mom's office. She has a pile of 60 letters needing stamps, and a pile of letters already stamped. She puts stamps on one-third of the letters needing stamps. If there are now 30 letters in the pile of already-stamped letters, how many were in that pile when Jennie began?\nCorrect answer: 10\nPrediction with SFT:\n$60 / 3 = 40$ X\nPrediction with KTO:\nJennie stamps $60 / 3 = 60/3 20 20$ letters. Then the number of letters still needing stamps is $60 - 20 = 60-20 40 40$. The number of letters already stamped is $30 - 20 = 30-20 10 10$. There were $40 + 10 = 40+10 50 50$ letters in the pile when Jennie began. $50$ X\nid: gsm8k_0oOjz5Ub66DF4inZ\nQuestion: There are 6 trees in Chris's yard. Ferdinand has half the number of trees that Chris has. Harry has 5 more than twice the number of trees that Ferdinand has. How many more trees are in Harry's yard than Ferdinand's yard?\nCorrect answer: 8\nPrediction with SFT:\n$9$ X\nPrediction with KTO:\nHalf the number of trees that Chris has is $6$ trees / $2 = 6/2 3 3$ trees. Twice the number of trees that Ferdinand has is $3$ trees * $2 = 3*2 6 6$ trees. Harry has $5$ trees + $6$ trees = $6+5 11 11$ trees. There are $11$ trees - $6$ trees = $11-6 5 5$ more trees in Harry's yard than in Ferdinand's yard. $5 5 5$ more trees in Harry's yard than in Ferdinand's yard. $5 5 5$ more trees in Harry's yard than in Ferdinand's yard. $11 11 5$ more trees than in Ferdinand's yard. $11-6 5 5$ more trees in Harry's yard. $5$ X"}]}