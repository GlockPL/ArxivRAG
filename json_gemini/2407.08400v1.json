{"title": "Self-training Language Models for Arithmetic Reasoning", "authors": ["Marek Kadl\u010d\u00edk", "Michal \u0160tef\u00e1nik"], "abstract": "Language models achieve impressive results\nin tasks involving complex multistep reason-\ning, but scaling these capabilities further\ntraditionally requires expensive collection\nof more annotated data. In this work, we\nexplore the potential of improving the ca-\npabilities of language models without new\ndata, merely using automated feedback to\nthe validity of their predictions in arith-\nmetic reasoning (self-training).\nWe find that models can substantially im-\nprove in both single-round (offline) and on-\nline self-training. In the offline setting, su-\npervised methods are able to deliver gains\ncomparable to preference optimization, but\nin online self-training, preference optimiza-\ntion shows to largely outperform supervised\ntraining thanks to superior stability and ro-\nbustness on unseen types of problems.", "sections": [{"title": "Introduction", "content": "Despite recent improvements in the practical\nusability of language models (LMs) (Wang\net al., 2023), these models often struggle with\ntasks requiring reasoning, i.e., a process of in-\nferring a conclusion or decision logically and\nsystematically (Huang and Chang, 2023). Pre-\nvious work improves the reasoning capabilities\nof language models by scaling training data\nto more diverse (Kadl\u010d\u00edk et al., 2023) or com-\nplex (Hendrycks et al., 2021) collections, but\nreaching further improvements in this direction\nbecomes exceedingly expensive.\nIn this work, we evaluate the potential of im-\nproving models' capabilities by training with\nimplicit, automated feedback to models' re-\nsponses. Arithmetic reasoning provides a rare\nenvironment where the quality of the model's\nresponses can be automatically assessed against\nthe annotated correct results rather than ex-\npensive and possibly subjective judgments of\nmodel outputs (Hu et al., 2023) while reflecting\nheavily on the model's abstractive reasoning.\nOur experiments address the two main re-\nsearch questions:\nRQ1: To what extent can abstract reasoning\nabilities of language models improve by self-\ntraining without additional data?\nRQ2: Can the preference optimization bring\nfurther improvements to models' capabilities\nover traditional supervised fine-tuning?\nWe address these by implementing two vari-\nants of self-training: (1) an offline variant,\nwhere the model responses, used to further\ntrain the model, are generated in a single iter-\nation (\u00a73.1), and (2) an online variant, where\nthe model obtains feedback on its predictions\ninstantly during the training (\u00a73.2).\nOur experiments reveal that self-training pro-\nvides a valuable training signal and significantly"}, {"title": "Related Work", "content": "Luo et al. (2023) train models with PPO (Schul-\nman et al., 2017) against feedback on individ-\nual steps given by ChatGPT 3.5. Uesato et al.\n(2022) apply self-training on GMS8K and com-\npare the effectiveness of giving outcome-based\n(per solution) or process-based (per each step\nin solution) feedback, concluding that the two\napproaches result in comparable accuracy, but\noutcome-based feedback delivers a higher error\nrate in the rationales. Lightman et al. (2023)\nstudy the same problem on a larger scale and\nconclude that process-based feedback outper-\nforms outcome-based at end-result accuracy.\nOur work is closest to Parisi et al. (2022)\nand Zelikman et al. (2022). Parisi et al. (2022)\napply self-training with a traditional super-\nvised objective: they train the model on a\nsmall set of seed data and continuously use\nthe trained model to generate solutions for a\nlarger set, from which correct solutions are\nused in another training epoch. They show\nthat three such subsequent epochs can improve\nthe accuracy. Zelikman et al. (2022) experi-\nment with self-training with supervised fine-\ntuning on commonsense and math reasoning.\nThey report positive results of self-training on\nthe model's reasoning capabilities under spe-\ncific conditions: (1) the initial model must be\ncapable enough to be able to achieve improve-\nments, and (2) training tasks must hold a neg-\nligible chance of random success (unlike, e.g.,\nbinary classification). Our work builds upon\nthese findings but differs from previous work\nin several important aspects, mainly: (1) we\nexperiment with several different objectives ap-\nplied in self-training, and (2) we implement and\nevaluate both offline and online variants of self-\ntraining. Finally, we make our self-training im-\nplementations freely available for future work."}, {"title": "Experiments", "content": "Our experiments build upon the 3-billion-\nparameter FLAN models fine-tuned specifically\nfor arithmetic reasoning in previous work of\nKadl\u010d\u00edk et al. (2023). These relatively compact\ncalculator-assisted models called CALCFORM-\nERS were shown to perform noticeably well\non multi-step reasoning, while on single-step\nand two-step problems perform similarly to\nLlama-2 with 70B parameters (Touvron et al.,\n2023). Another desiderata of these models is\ntheir transparency of training data: they were\ntrained on a superset of our training collection,\nmeaning that in our experiments, we do not\ntrain the models on any new data.\nWe self-train these models with the prompts\nfrom Ape210K (Zhao et al., 2020), the largest\navailable dataset of over 200,000 math prob-\nlems. In addition to Ape210K's test set,\nwe evaluate our models on five other math\ndatasets, assessing the robustness of models'\ncapabilities in new types of math problems;\nGSM8K (Cobbe et al., 2021) containing multi-\nstep elementary-grade problems requiring on\naverage 3.25 steps to achieve correct result,\nAQUA-RAT (Ling et al., 2017) with more com-\nplex, multiple-choice tasks, and three simpler,\none to two-steps datasets: MAWPS (Koncel-\nKedziorski et al., 2016), ASDiv-A (Miao et al.,\n2020), and SVAMP (Patel et al., 2021).\nIn both online and offline self-training, we\nuse the model itself to generate training data\n(see Fig. 1). The generated data consists of\nthe original input prompt ($x_i$) and associated\nmodel predictions ($y_i$) in the form of a chain-of-\nthought sequence containing the model's final\nresult at the end. For each prompt, we gen-\nerate 16 predictions using sampled generation.\nAnnotations of correct results then allow us\nto automatically annotate each prediction for\neither being correct ($y_{OK}$), or incorrect ($Y_{NOK}$),\nassigning a set of both correct and incorrect\npredictions to each input prompt.\nIn the case of supervised fine-tuning (SFT),\nthe dataset consists of pairs of ($x_i, y_{OK}$). SFT\nuses a standard next-token prediction with\ncross-entropy loss and teacher forcing (Bah-\ndanau et al., 2015). Further details of our\ntraining setup can be found in Appendix A.\nPreference optimization (PO) methods then\ntrain on triples ($x_i, y_{OK}, Y_{NOK}$), with the $y_{OK}$"}, {"title": "Offline Self-training", "content": "In the offline variant, we perform a single itera-\ntion of collecting predictions with prompts from\nApe210K, resulting in over 24,000 prompts\nwith at least one positive and one negative\nresponse.\nAll PO methods exhibit a crucial param-\neter $\\beta$ that weights the KL regulariza-\ntion of the trained model according to the\noriginal \"reference\" model. We perform a hy-\nperparameter tuning of this parameter with\n$\\beta \\in (0.01, 0.1, 0.3, 0.6, 0.9, 0.99)$ according to\nin-domain validation accuracy separately for\neach method and report the results for the best\ntwo configurations.\nFor SFT, we experiment with 3 variants.\nSFT PLAIN is trained on pairs ($x_i, y_{OK}$). In\nSFT BALANCED, we use two different correct\npredictions $y_{OK}$ for one $x_i$. This variant com-\npensates for the PO advantage of training on\ntwo solutions per problem. Lastly, in SFT\nWITH NEGATIVES, we use both positive $y_{OK}$\nand negative $y_{NOK}$ as targets for each $x_i$. In\nthe training data constructed from $y_{NOK}$, we\nprefix $x_i$ with a phrase \"Write incorrect solu-\ntion for the following problem\"\nFinally, we re-train the best-performing run\nof each method with a low-rank adaptation\n(LORA) (Hu et al., 2021), a commonly used fine-\ntuning regularization technique that restricts\nthe fine-tuning update of each weight to have a\nspecific low rank. We apply LoRA with a rank\nof 32 on all linear projections in the model.\nResults Table 1 compares the accuracy\nachieved in offline self-training with each\nmethod. A comparison of supervised and more\ncomplex preference optimization methods re-\nveals a relatively small difference between the\nbest-performing configurations of both cate-\ngories. Especially thanks to LoRA regulariza-\ntion, SFT shows the ability to reach results\ncomparable in most datasets. Among all super-\nvised methods, the SFT WITH NEGATIVES per-\nforms the worst, showing that using negative\nfeedback in supervised training analogically to\npreference optimization is non-trivial.\nSimilar to SFT, LORA regularization also\nhas a positive effect on DPO, indicating DPO's\nhigher inclination to overfitting, as also evi-\ndenced by previous work (Azar et al., 2023).\nOn the practical side, we note that PO meth-\nods converge much faster than SFT methods,"}, {"title": "Online Self-training", "content": "In the online self-training, we generate the\ntraining data on the fly. Therefore, through-\nout the whole training, both the positive and\nnegative predictions used for conditioning the\nupdates can realistically be generated by the\ntrained model. Previous work showed that\nexposing the model to its own outputs might\nitself improve its robustness (\u0160tef\u00e1nik et al.,\n2023). In our settings, we reflect on the LM's\ncapability to autonomously improve its logical\nreasoning capability based on the up-to-date\nfeedback to its predictions.\nA methodology of constructing training sam-\nples from the model's predictions for both SFT\nand PO methods remains identical to the of-\nfline variant. Details of data processing can\nbe found in Appendix A.1. As the generation\nprocess in online training substantially slows\ndown updates, we restrain the scale of exper-\niments to the best-performing configurations\nfrom the offline variant.\nResults Table 2 shows the accuracy of train-\ning methods in online self-training. This set-\nting reveals much larger differences between\nmethods. Supervised fine-tuning (SFT) im-\nproves accuracy on simple one-step and two-\nstep datasets (MAWPS, SVAMP, and ASDiv-\nA) but substantially degrades performance on\nout-of-distribution GSM8K and AQUA-RAT.\nManual inspection reveals that the degradation\non AQUA-RAT is caused by the model's forget-\nting of the response format of multiple-choice\nquestions, well-preserved by all PO methods.\nContrary to the SFT, PO methods deliver\nsignificant improvements compared to both the\nbase checkpoint and their offline variants (Ta-\nble 1). Noticeable is the improvement of DPO\nby 11.9% on GSM8K, among other cases, sug-\ngesting that self-training can mitigate overfit-\nting of PO methods. Best-performing KTO\nmethod also substantially improved compared\nto the offline variant; by 11.3% on in-domain\nApe210K, or by 16.9% on simpler, out-of-\ndomain MAWPS. Among all other online meth-\nods, KTO performs best on every dataset ex-\ncept for AQUA-RAT.\nAppendix B provides a per-sample analy-\nsis of differences between outputs of SFT and\nPO models. Noticeably, we find that while\nSFT allows the model to achieve remarkable\nimprovements, this comes at the price of faith-\nfulness and usability of its rationales, where the\nSFT model learns to completely or partially\nomit reliable rationale."}, {"title": "Conclusions", "content": "This work explores the potential of au-\ntonomously improving language models for\narithmetic reasoning, where the task allows\nautomated, immediate, and objective feedback\nbased on the correct results. We experiment\nwith two settings: (i) offline self-training, col-\nlecting the feedback in a single iteration, and\n(ii) online self-training, where the model trains\ncontinuously from feedback to its up-to-date\npredictions. In both settings, we apply and\ncompare recent preference optimization meth-\nods (DPO, K\u03a4\u039f, IPO) with standard super-\nvised training (SFT).\nWe find that both the offline and online\nself-training provide an opportunity to im-\nprove models' capabilities without any new\ndata, using exclusively models' own predic-\ntions and automated feedback. In addition to\nthe offline variant, online self-training provides\nfurther opportunities for substantial improve-\nments thanks to the enhanced robustness of\npreference optimization methods."}, {"title": "Limitations", "content": "Despite the fact that our proposed self-training\nmethods do not require any new data, we ac-\nknowledge their limitations in the extensive\ncomputational requirements given by generat-\ning the data. While the data generation for the\noffline variant can be parallelized, this is more\ndifficult for the online variant, where the model\nis trained with its own most recent predictions.\nAs a result, our self-training experiments took\nbetween 15 and 30 days to converge on a single\nNvidia A100 GPU.\nThe time-demanding character of online self-\ntraining experiments is a direct cause of an-\nother limitation: a constrained diversity of\nmodels and datasets that we experiment with.\nAs such, the experiments and conclusions of\nour work should inspire experiments with self-\ntraining in other applications but may not be\ngeneralized to claims on the general efficiency\nof self-training."}, {"title": "A Training Details", "content": "In every configuration of both preference and\nsupervised training, the model is trained with\nAdafactor (Shazeer and Stern, 2018) optimizer\nwith an effective batch size of 32, a learning rate\nof $2 \\cdot 10^{-5}$ with 1,000 warmup steps, and a linear\ndecay to 0 in 1 million steps. The models were\ntrained in bfloat16 (Wang and Kanwar, 2023)\nprecision with mixed precision training (Micike-\nvicius et al., 2017). The training terminates\nafter convergence on the in-domain dataset\n(Ape210K), and then the best checkpoint from\nthe training is selected according to in-domain\nvalidations.\nEach of our experiments can be reproduced\non a single Nvidia A100 graphic card with\n32GB of RAM. Note that especially the online\nself training experiments can take up to 31 days\nto converge."}, {"title": "Online self-training", "content": "To create new data in online self-training, we\nsample a random problem from Ape210K and\ngenerate predictions with the current model.\nNext, we label each solution as correct if its\nresult matches the one in the data. The online\nself-training process is illustrated in Figure 1.\nIn this experiment, we again compare super-\nvised training and preference optimization. In\nall variants, we generate 16 solutions per prob-\nlem with top-k=50 sampling using the latest\nmodel, but the subsequent data processing is\nmethod-specific."}, {"title": "Output analyses", "content": "Aiming to better understand the difference be-\ntween self-training with preference optimiza-\ntion methods and supervised training, we man-\nually analyze a set of randomly chosen ratio-\nnales generated for prompts of the GSM8K\ntest set. We collect the rationales from (i) the\noriginal checkpoint, (ii) the checkpoint trained\nin online self-training and supervised method\n(denoted SFT), and (iii) the checkpoint trained\non online self-training with the best-performing\nmethod (KTO). Due to the time complexity\nof evaluating long chain-of-thought output se-\nquences, we analyze 20 predictions marked as\ncorrect for each checkpoint.\nWithin the analysis, we encounter 5 types of\ndominant flaws that models' outcomes exhibit,\neven when being correct:\n1. Inconsistency: Within the rationale, the\nmodel generates a new reasoning step\nwhich is not logically consistent with pre-\nvious ones.\n2. Missing association: Model's rationale\ncontains steps that are difficult to assess\nfor consistency, as they lack the associa-\ntions of units (e.g., of size, distance, or\nvolume) or subjects from input prompt or\nintermediate computation.\n3. Missing rationale: Model only generates\nthe result without any rationale associated\nwith it.\n4. Missing rationale part: Model's ratio-\nnale is missing a specific segment, making\nit impossible to fully check the model's\ncomputation process.\n5. Not understandable: Model's rationale\ncontains text that is incomprehensible by\nthe annotator, thus impossible to judge\nfor logical correctness."}, {"title": "Examples of predictions", "content": "For an illustration of differences between the\nsupervised and preference optimization check-\npoints, below we include a set of predictions\nof SFT and KTO checkpoints for randomly\nselected prompts of GSM8K collection. These\nexamples can also be found by associated IDs\nin the original dataset repository2. For better\nreadability, we removed the navigation tokens\nused by the model for interaction with the cal-\nculator tool during the generation.\ngsm8k_aBNdImSK9ddCwuYb\nQuestion: Tobias bought a big pizza with\n60 pieces. He ate 2/5 of the pieces on the first\nday, 10 pieces on the second day, and 7/13 of\nthe remaining pieces on the third day. How\nmany pizza pieces has he eaten so far?\nCorrect answer: 48\nPrediction with SFT:\n$60 * 2/5 = 28$ X\nPrediction with KTO:\nOn the first day, Tobias ate $60 * 2/5$\n= $60*2/5$ 24 24 pieces. The num-\nber of pieces that remained after the\nsecond day is 60 - 24 = 60-24 36 36\npieces. On the third day, he ate 36\n* 7/13 = $36*7/13$ 252/13 = around\n19.384615 25 pieces. The total num-\nber of pieces he has eaten so far is\n10 + 252/13 = 10+252/13 382/13 =\naround 29.384615 38 pieces. Tobias\nate a total of 24 + 10 + (382/13)\n= 24+10+352/13 794/13 = around\n61.076923 69 pieces. 794/13 = around\n69.076923 X\nid: gsm8k_qIygQI2WNfRPWoYj\nQuestion: Jennie is helping at her mom's\noffice. She has a pile of 60 letters needing\nstamps, and a pile of letters already stamped.\nShe puts stamps on one-third of the letters\nneeding stamps. If there are now 30 letters in\nthe pile of already-stamped letters, how many\nwere in that pile when Jennie began?\nCorrect answer: 10\nPrediction with SFT:\n$60 / 3 = 40$ X\nPrediction with KTO:\nJennie stamps 60 / 3 = $60/3$ 20 20\nletters. Then the number of letters\nstill needing stamps is 60 - 20 = 60-20\n40 40. The number of letters already\nstamped is 30 - 20 = 30-20 10 10.\nThere were 40 + 10 = 40+10 50 50\nletters in the pile when Jennie began.\n50 X\nid: gsm8k_0oOjz5Ub66DF4inZ\nQuestion: There are 6 trees in Chris's yard.\nFerdinand has half the number of trees that\nChris has. Harry has 5 more than twice the\nnumber of trees that Ferdinand has. How many\nmore trees are in Harry's yard than Ferdinand's\nyard?\nCorrect answer: 8\nPrediction with SFT:\n$9$ X\nPrediction with KTO:\nHalf the number of trees that Chris\nhas is 6 trees / 2 = $6/2$ 3 3 trees.\nTwice the number of trees that Ferdi-\nnand has is 3 trees * 2 = $3*2$ 6 6 trees.\nHarry has 5 trees + 6 trees = 6+5\n11 11 trees. There are 11 trees - 6\ntrees = 11-6 5 5 more trees in Harry's\nyard than in Ferdinand's yard. 5 5\n5 more trees in Harry's yard than in\nFerdinand's yard. 5 5 5 more trees\nin Harry's yard than in Ferdinand's\nyard. 11 11 5 more trees than in Fer-\ndinand's yard. 11-6 5 5 more trees in\nHarry's yard. 5 X"}]}