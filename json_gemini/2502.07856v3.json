{"title": "MRS: A FAST SAMPLER FOR MEAN REVERTING DIFFUSION BASED ON ODE AND SDE SOLVERS", "authors": ["Ao Li", "Wei Fang", "Hongbo Zhao", "Le Lu", "Ge Yang", "Minfeng Xu"], "abstract": "In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFES (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.", "sections": [{"title": "INTRODUCTION", "content": "Diffusion models have emerged as a powerful class of generative models, demonstrating remarkable capabilities across a variety of applications, including image synthesis and video generation. In these applications, controllable generation is very important in practice, but it also poses considerable challenges. Various methods have been proposed to incorporate text or image conditions into the score function of diffusion models, whereas Mean Reverting (MR) Diffusion offers a new avenue of control in the generation process. Previous diffusion models (such as DDPM ) simulate a diffusion process that gradually transforms data into pure Gaussian noise, followed by learning to reverse this process for sample generation . In contrast, MR Diffusion is designed to produce final states that follow a Gaussian distribution with a non-zero mean, which provides a simple and natural way to introduce image conditions. This characteristic makes MR Diffusion particularly suitable for solving inverse problems and potentially extensible to multi-modal conditions. However, the sampling process of MR Diffusion requires hundreds of iterative steps, which is time-consuming."}, {"title": "BACKGROUND", "content": "In this section, we briefly review the basic definitions and characteristics of diffusion probabilistic models and mean-reverting diffusion models."}, {"title": "DIFFUSION PROBABILISTIC MODELS", "content": "According to , Diffusion Probabilistic Models (DPMs) can be defined as the solution of the following It\u00f4 stochastic differential equation (SDE), which is a stochastic process \\({x_t}_{t \\in [0,T]}\\) with \\(T > 0\\), called forward process, where \\(x_t \\in \\mathbb{R}^D\\) is a D-dimensional random variable.\n\\[dx = f(x,t)dt + g(t)dw.\\]\nThe forward process performs adding noise to the data \\(x_0\\), while there exists a corresponding reverse process that gradually removes the noise and recovers \\(x_0\\). Anderson (1982) shows that the reverse of the forward process is also a solution of an It\u00f4 SDE:\n\\[dx = [f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)]dt + g(t)dw,\\]\nwhere \\(f\\) and \\(g\\) are the drift and diffusion coefficients respectively, \\(w\\) is a standard Wiener process running backwards in time, and time \\(t\\) flows from \\(T\\) to 0, which means \\(dt < 0\\). The score function \\(\\nabla_x \\log p_t(x)\\) is generally intractable and thus a neural network \\(s_\\theta(x,t)\\) is used to estimate it by optimizing the following objective :\n\\[\\theta^* = \\arg \\min_\\theta E_{t \\sim U(0,T)} E_{x(t) \\sim p(x_t|x_0)} E_{x_0} \\left[ \\left\\| s_\\theta(x_t, t) - \\nabla_{x_t} \\log p(x_t|x_0) \\right\\|_2^2 \\right].\\]\nwhere \\(\\lambda(t): [0,T] \\rightarrow \\mathbb{R}^+\\) is a positive weighting function, \\(t\\) is uniformly sampled over \\([0, T]\\), \\(x_0 \\sim p_0(x)\\) and \\(x_t \\sim p(x_t|x_0)\\). To facilitate the computation of \\(p(x_t|x_0)\\), the drift coefficient \\(f(x, t)\\) is typically defined as a linear function of \\(x\\), as presented in Eq.(4). Based on the inference by S\u00e4rkk\u00e4 & Solin (2019) in Section 5.5, the transition probability \\(p(x_t|x_0)\\) corresponding to Eq.(4) follows Gaussian distribution, as shown in Eq.(5).\n\\[dx = f(t)xdt + g(t)dw,\\]\n\\[p(x_t|x_0) \\sim \\mathcal{N} \\left( x_t; x_0 e^{\\int_0^t f( \\tau) d \\tau}, \\int_0^t e^{2 \\int_\\xi^t f( \\tau) d \\tau} g^2( \\xi) d \\xi \\right).\\]\nSong et al. (2020b) proved that Denoising Diffusion Probabilistic Models and Noise Conditional Score Networks can be regarded as discretizations of Variance Preserving SDE (VPSDE) and Variance Exploding SDE (VESDE), respectively. As shown in Table 1, the SDEs corresponding to the two most commonly used diffusion models both follow the form of Eq.(4)."}, {"title": "MEAN REVERTING DIFFUSION MODELS", "content": "Luo et al. (2023b) proposed a special case of It\u00f4 SDE named Mean Reverting SDE (MRSDE), as follows:\n\\[dx = f(t) (\\mu - x) dt + g(t)dw,\\]\nwhere \\(\\mu\\) is a parameter vector that has the same shape of variable \\(x\\), and \\(f(t), g(t)\\) are time-dependent non-negative parameters that control the speed of the mean reversion and stochastic volatility, respectively. To prevent potential confusion, we have substituted the notation used in the original paper (Luo et al., 2023b). For further details, please refer to Appendix B. Under the assumption that \\(g^2(t)/f(t) = 2\\sigma_\\infty^2\\) for any \\(t \\in [0, T]\\) with \\(T > 0\\), Eq.(6) has a closed-form solution, given by\n\\[x_t = x_0 e^{-\\int_0^t f(\\tau) d \\tau} + \\mu (1 - e^{-\\int_0^t f(\\tau) d \\tau}) + \\sigma_\\infty \\sqrt{1 - e^{-2 \\int_0^t f(\\tau) d \\tau}} z,\\]\nwhere \\(\\sigma_\\infty\\) is a positive hyper-parameter that determines the standard deviation of \\(x_t\\) when \\(t \\rightarrow \\infty\\) and \\(z \\sim \\mathcal{N}(0, I)\\). Note that \\(x_t\\) starts from \\(x_0\\), and converges to \\(\\mu + \\sigma_\\infty z\\) as \\(t \\rightarrow \\infty\\). According to Anderson (1982)'s result, we can derive the following reverse-time SDE:\n\\[dx = [f(t) (\\mu - x) - g^2(t) \\nabla_x \\log p_t(x)] dt + g(t)dw.\\]\nSimilar to DPMs, the score function in Eq.(8) can also be estimated by score matching methods . Once the score function is known, we can generate \\(x_0\\) from a noisy state \\(x_t\\). In summary, MRSDE illustrates the conversion between two distinct types of data and has demonstrated promising results in image restoration tasks."}, {"title": "FAST SAMPLERS FOR MEAN REVERTING DIFFUSION WITH NOISE PREDICTION", "content": "According to Song et al. (2020b), the states \\(x_t\\) in the sampling procedure of diffusion models correspond to solutions of reverse-time SDE and PF-ODE. Therefore, we look for ways to accelerate sampling by studying these solutions. In this section, we solve the noise-prediction-based reverse-time SDE and PF-ODE, and we numerically estimate the non-closed-form component of the solution, which serves to accelerate the sampling process of MR diffusion models. Next, we analyze the sampling method currently used by MR Diffusion and demonstrate that this method corresponds to a variant of discretization for the reverse-time MRSDE."}, {"title": "SOLUTIONS TO MEAN REVERTING SDES WITH NOISE PREDICTION", "content": "Ho et al. (2020) reported that score matching can be simplified to predicting noise, and Song et al. (2020b) revealed the connection between score function and noise prediction models, which is\n\\[\\nabla_{x_t} \\log p(x_t|x_0) = - \\frac{\\epsilon_\\theta(x_t, \\mu, t)}{\\sigma_t},\\]\nwhere \\(\\sigma_t = \\sigma_\\infty \\sqrt{1 - e^{-2 \\int_0^t f(\\tau) d \\tau}}\\) is the standard deviation of the transition distribution \\(p(x_t|x_0)\\). Because \\(\\mu\\) is independent of \\(t\\) and \\(x\\), we substitute \\(\\epsilon_\\theta(x_t, \\mu, t)\\) with \\(\\epsilon_\\theta(x_t, t)\\) for notation simplicity. According to Eq.(9), we can rewrite Eq.(8) as\n\\[dx = \\left[f(t) (\\mu - x) + \\frac{g^2(t)}{\\sigma_t} \\epsilon_\\theta(x_t, t)\\right] dt + g(t)dw.\\]\nUsing It\u00f4's formula (in the differential form), we can obtain the following semi-analytical solution:"}, {"title": "SOLUTIONS TO MEAN REVERTING ODES WITH NOISE PREDICTION", "content": "Song et al. (2020b) have illustrated that for any It\u00f4 SDE, there exists a probability flow ODE, sharing the same marginal distribution \\(p_t(x)\\) as a reverse-time SDE. Therefore, the solutions of PF-ODEs are also helpful in acceleration of sampling. Specifically, the PF-ODE corresponding to Eq.(10) is\n\\[\\frac{dx}{dt} = f(t) (\\mu - x) + \\frac{g^2(t)}{2\\sigma_t} \\epsilon_\\theta(x_t, t).\\]\nThe aforementioned equation exhibits a semi-linear structure with respect to \\(x\\), thus permitting resolution through the method of \"variation of constants\". We can draw the following conclusions:"}, {"title": "POSTERIOR SAMPLING FOR MEAN REVERTING DIFFUSION MODELS", "content": "In order to improve the sampling process of Mean Reverting Diffusion, Luo et al. (2024b) proposed the posterior sampling algorithm. They define a monotonically increasing time series \\({t_i}_{i=1}^T\\) and the reverse process as a Markov chain:\n\\[p(x_{1:T} | x_0) = p(x_T | x_0) \\prod_{i=2}^T p(x_{i-1} | x_i, x_0) \\text{ and } x_T \\sim \\mathcal{N}(0, I),\\]\nwhere we denote \\(x := x_t\\) for simplicity. They obtain an optimal posterior distribution by minimizing the negative log-likelihood, which is a Gaussian distribution given by\n\\[p(x_{i-1} | x_i, x_0) = \\mathcal{N}(x_{i-1} | \\mu_i(x_i, x_0), \\beta_i I),\\]\n\\[\\mu_i (x_i, x_0) = \\frac{(1 - \\alpha_{i-1}^2)\\alpha_i}{1 - \\alpha_i^2 \\alpha_{i-1}^2} (x_i - \\mu) + \\frac{\\alpha_{i-1} (1 - \\alpha_i^2)}{1 - \\alpha_i^2 \\alpha_{i-1}^2} (x_0 - \\mu) + \\mu,\\]\n\\[\\beta_i = \\frac{(1 - \\alpha_{i-1}^2)(1 - \\alpha_i^2)}{1 - \\alpha_i^2 \\alpha_{i-1}^2},\\]\nwhere \\(\\alpha_i = e^{-\\int_0^{t_i} f(\\tau) d\\tau}\\) and \\(x_0 = (x_i - \\mu - \\sigma_i \\epsilon_\\theta(x_i, \\mu, t_i)) / \\alpha_i + \\mu\\). Actually, the reparameterization of posterior distribution in Eq.(19) is equivalent to a variant of the Euler-Maruyama discretization of the reverse-time SDE (see details in Appendix A.2). Specifically, the Euler-Maruyama method computes the solution in the following form:\n\\[x_t = x_s + \\int_s^t \\left[ f(\\tau) (\\mu - x_\\tau) + \\frac{g^2(\\tau)}{\\sigma_\\tau} \\epsilon_\\theta(x_\\tau, \\tau) \\right] d\\tau + \\int_s^t g(\\tau) dw,\\]\nwhich introduces approximation errors from both the analytical term and the non-linear component associated with neural network predictions. In contrast, our approach delivers an exact solution for the analytical part, leading to reduced approximation errors and a higher order of convergence."}, {"title": "FAST SAMPLERS FOR MEAN REVERTING DIFFUSION WITH DATA PREDICTION", "content": "Unfortunately, the sampler based on noise prediction can exhibit substantial instability, particularly with small NFEs, and may perform even worse than posterior sampling. It is well recognized that the Taylor expansion has a limited convergence domain, primarily influenced by the derivatives of the neural networks. In fact, higher-order derivatives often result in smaller convergence radii. During the training phase, the noise prediction neural network is designed to fit normally distributed Gaussian noise. When the standard deviation of this Gaussian noise is set to 1, the values of samples can fall outside the range of [-1,1] with a probability of 34.74%. This discrepancy results in numerical instability in the output of the neural network, causing its derivatives to exhibit more pronounced fluctuations (refer to the experimental results in Section 5 for further details). Consequently, the numerical instability leads to very narrow convergence domains, or in extreme cases, no convergence at all, which ultimately yields awful sampling results.\nLu et al. (2022b) have identified that the choice of parameterization for either ODEs or SDEs is critical for the boundedness of the convergent solution. In contrast to noise prediction, the data prediction model focuses on fitting \\(x_0\\), ensuring that its output remains strictly confined within the bounds of [-1,1], thereby achieving high numerical stability."}, {"title": "SOLUTIONS TO MEAN REVERTING SDES WITH DATA PREDICTION", "content": "According to Eq.(7), we can parameterize \\(x_0\\) as follows:\n\\[\\epsilon_\\theta(x_t, t) = \\frac{x_t - \\alpha_t x_\\theta(x_t, t) - (1 - \\alpha_t) \\mu}{\\sigma_t},\\]"}, {"title": "SOLUTIONS TO MEAN REVERTING ODES WITH DATA PREDICTION", "content": "By substituting Eq.(21) into Eq.(15), we can obtain the following ODE parameterized by data prediction.\n\\[\\frac{dx}{dt} = \\left( \\frac{g^2(t)}{2 \\sigma_t^2} - f(t) \\right) x + \\left[f(t) - \\frac{g^2(t)}{2 \\sigma_t^2} \\right] (1 - \\alpha_t) \\mu - \\frac{g^2(t)}{2 \\sigma_t^2} \\alpha_t x_\\theta (x_t,t).\\]\nThe incorporation of the parameter \\(\\mu\\) does not disrupt the semi-linear structure of the equation with respect to \\(x\\), and \\(\\mu\\) is not coupled to the neural network. This implies that analytical part of solutions can still be derived concerning both \\(x\\) and \\(\\mu\\). We present the solution below (see Appendix A.1 for a detailed derivation)."}, {"title": "TRANSFORMATION BETWEEN THREE KINDS OF PARAMETERIZATIONS", "content": "There are three mainstream parameterization methods. Ho et al. (2020) introduced a training objective based on noise prediction, while Salimans & Ho (2022) proposed parameterization strategies for data and velocity prediction to keep network outputs stable under the variation of time or log-SNR. All three methods can be regarded as score matching approaches with weighted coefficients. To ensure our proposed algorithm is compatible with these parameterization strategies, it is necessary to provide transformation formulas for each pairs among the three strategies.\nThe transformation formula between noise prediction and data prediction can be easily derived from Eq.(7):\n\\[x_\\theta(t) = \\frac{x_t - (1 - \\alpha_t) \\mu - \\sigma_t \\epsilon_\\theta(t)}{\\alpha_t} = \\frac{x_t - \\alpha_t x_\\theta(t) - (1 - \\alpha_t) \\mu}{\\sigma_t}.\\]\nFor velocity prediction, we define \\(\\phi_t := \\arctan(\\frac{\\sigma_t}{\\alpha_t})\\), which is slightly different from the definition of Salimans & Ho (2022). Then we have \\(\\alpha_t = \\sigma_\\infty \\sin \\phi_t\\) and hence\n\\[v_t = \\frac{dx_t}{d\\phi_t} = \\mu \\sin \\phi_t - x_0 \\sin \\phi_t + \\sigma_\\infty \\cos(\\phi_t) \\epsilon_\\theta(t).\\]"}, {"title": "EXPERIMENTS", "content": "In this section, we conduct extensive experiments to show that MR Sampler can significantly speed up the sampling of existing MR Diffusion. To rigorously validate the effectiveness of our method, we follow the settings and checkpoints from Luo et al. (2024a) and only modify the sampling part. Our experiment is divided into three parts. Section 5.1 compares the sampling results for different NFE cases. Section 5.2 studies the effects of different parameter settings on our algorithm, including network parameterizations and solver types. In Section 5.3, we visualize the sampling trajectories to show the speedup achieved by MR Sampler and analyze why noise prediction gets obviously worse when NFE is less than 20."}, {"title": "MAIN RESULTS", "content": "Following Luo et al. (2024a), we conduct experiments with ten different types of image degradation: blurry, hazy, JPEG-compression, low-light, noisy, raindrop, rainy, shadowed, snowy, and inpainting (see Appendix D.1 for details). We adopt LPIPS and FID as main metrics for perceptual evaluation, and also report PSNR and SSIM for reference. We compare MR Sampler with other sampling methods, including posterior sampling and Euler-Maruyama discretization. We take two tasks as examples and the metrics are shown in Figure 2. Unless explicitly mentioned, we always use MR Sampler based on SDE solver, with data prediction and uniform \\(\\lambda\\). The complete experimental results can be found in Appendix D.3. The results demonstrate that MR Sampler converges in a few (5 or 10) steps and produces samples with stable quality. Our algorithm significantly reduces the time cost without compromising sampling performance, which is of great practical value for MR Diffusion."}, {"title": "EFFECTS OF PARAMETER CHOICE", "content": "In Table 2, we compare the results of two network parameterizations. The data prediction shows stable performance across different NFEs. The noise prediction performs similarly to data prediction with large NFEs, but its performance deteriorates significantly with smaller NFEs. The detailed analysis can be found in Section 5.3. In Table 3, we compare MR Sampler-ODE-d-2 and MR Sampler-SDE-d-2 on the inpainting task, which are derived from PF-ODE and reverse-time SDE respectively. SDE-based solver works better with a large NFE, whereas ODE-based solver is more effective with a small NFE. In general, neither solver type is inherently better."}, {"title": "ANALYSIS", "content": "Sampling trajectory. Inspired by the design idea of NCSN , we provide a new perspective of diffusion sampling process. Song & Ermon (2019) consider each data point (e.g., an image) as a point in high-dimensional space. During the diffusion process, noise is added to each point \\(x_0\\), causing it to spread throughout the space, while the score function (a neural network) remembers the direction towards \\(x_0\\). In the sampling process, we start from a random point by sampling a Gaussian distribution and follow the guidance of the reverse-time SDE (or PF-ODE) and the score function to locate \\(x_0\\). By connecting each intermediate state \\(x_t\\), we obtain a sampling trajectory. However, this trajectory exists in a high-dimensional space, making it difficult to visualize. Therefore, we use Principal Component Analysis (PCA) to reduce \\(x_t\\) to two dimensions, obtaining the projection of the sampling trajectory in 2D space. As shown in Figure 3, we present an example. Previous sampling methods (Luo et al., 2024b) often require a long path to find \\(x_0\\), and reducing NFE can lead to cumulative errors, making it impossible to locate \\(x_0\\). In contrast, our algorithm produces more direct trajectories, allowing us to find \\(x_0\\) with fewer NFEs."}, {"title": "CONCLUSION", "content": "We have developed a the fast sampling algorithm of MR Diffusion. Compared with DPMS, MR Diffusion is different in SDE and thus not adaptable to existing training-free fast samplers. We propose MR Sampler for acceleration of sampling of MR Diffusion. We solve the reverse-time SDE and PF-ODE derived from MRSDE and find a semi-analytical solution. We adopt the methods of exponential integrators to estimate the non-linear integral part. Abundant experiments demonstrate that our algorithm achieves small errors and fast convergence. Additionally, we visualize sampling trajectories and explain why the parameterization of noise prediction does not perform well in the case of small NFEs."}, {"title": "Limitations and broader impact", "content": "Despite the effectiveness of MR Sampler, our method is still inferior to distillation methods within less than 5 NFEs. Additionally, our method can only accelerate sampling, but cannot improve the upper limit of sampling quality."}, {"title": "APPENDIX", "content": "We include several appendices with derivations, additional details and results. In Appendix A, we provide derivations of propositions in Section 3 and 4, equivalence between posterior sampling and Euler-Maruyama discretization, and velocity prediction, respectively. In Appendix B, we compare the notations used in this paper and MRSDE . In Appendix C, we list detailed algorithms of MR Sampler with various orders and parameterizations. In Appendix D, we present details about datasets, settings and results in experiments. In Appendix E, we provide an in-depth discussion on determining the optimal NFE."}, {"title": "DERIVATION DETAILS", "content": ""}, {"title": "PROOFS OF PROPOSITIONS", "content": "Proposition 1. Given an initial value \\(x_s\\) at time \\(s \\in [0,T]\\), the solution \\(x_t\\) at time \\(t \\in [0,s]\\) of Eq.(10) is\n\\[x_t = \\frac{A_t}{A_s} x_s + (1 - \\frac{A_t}{A_s}) \\mu + \\frac{A_t}{\\alpha_s} \\int_s^t \\frac{g^2(\\tau)}{\\alpha_\\tau \\sigma_\\tau} \\epsilon_\\theta(x_\\tau, \\tau) d\\tau + \\sqrt{\\frac{A_t^2}{\\alpha_s^2} \\int_s^t g^2(\\tau) d\\tau} z,\\]\nwhere \\(A_t := e^{-\\int_0^t f(\\tau) d \\tau}\\) and \\(z \\sim \\mathcal{N}(0, I)\\).\nProof. For SDEs in the form of Eq.(1), It\u00f4's formula gives the following conclusion:\n\\[d\\phi(x,t) = \\frac{\\partial \\phi(x,t)}{\\partial t} dt + \\frac{\\partial \\phi(x,t)}{\\partial x} [f(x,t)dt + g(t)dw] + \\frac{1}{2} \\frac{\\partial^2 \\phi(x, t)}{\\partial x^2} g^2(t)dt,\\]\nwhere \\(\\phi(x, t)\\) is a differentiable function. And we define\n\\[\\phi(x,t) = x e^{\\int_0^t f(\\tau) d \\tau}.\\]\nBy substituting \\(f(x,t)\\) and \\(g(t)\\) with the corresponding drift and diffusion coefficients in Eq.(10), we obtain\n\\[d\\phi(x,t) = \\mu f(t) e^{\\int_0^t f(\\tau) d \\tau} dt + e^{\\int_0^t f(\\tau) d \\tau} \\left[\\frac{g^2(t)}{\\sigma_t} \\epsilon_\\theta(x,t) dt + g(t) dw\\right].\\]\nAnd we integrate both sides of the above equation from \\(s\\) to \\(t\\):\n\\[\\phi(x,t) - \\phi(x,s) = \\mu \\left(e^{\\int_0^t f(\\tau) d \\tau} - e^{\\int_0^s f(\\tau) d \\tau}\\right) + \\int_s^t e^{\\int_0^\\xi f(\\tau) d \\tau} \\frac{g^2(\\xi)}{\\sigma_\\xi} \\epsilon_\\theta(x,\\xi) d\\xi + \\int_s^t e^{\\int_0^\\xi f(\\tau) d \\tau} g(\\xi) dw.\\]\nNote that \\(w\\) is a standard Wiener process running backwards in time and we have the quadratic variation \\((dw)^2 = -d\\tau\\). According to the definition of \\(\\phi(x, t)\\) and \\(A_t\\), we have\n\\[\\frac{x_t}{A_t} - \\frac{x_s}{A_s} = \\mu \\left(\\frac{1}{A_s} - \\frac{1}{A_t}\\right) + \\int_s^t \\frac{g^2(\\tau)}{A_\\tau \\sigma_\\tau} \\epsilon_\\theta(x, \\tau) d\\tau + \\int_s^t \\frac{g(\\tau)}{A_\\tau} dw,\\]\nwhich is equivalent to Eq.(31).\nProposition 2. Given an initial value \\(x_s\\) at time \\(s \\in [0,T]\\), the solution \\(x_t\\) at time \\(t \\in [0,s]\\) of Eq.(15) is\n\\[x_t = \\frac{A_t}{A_s} x_s + (1 - \\frac{A_t}{A_s}) \\mu + \\frac{A_t}{2} \\int_s^t \\frac{g^2(\\tau)}{\\alpha_\\tau \\sigma_\\tau} \\epsilon_\\theta(x, \\tau) d\\tau,\\]\nwhere \\(A_t := e^{-\\int_0^t f(\\tau) d \\tau}\\).\nProof. For ODEs which have a semi-linear structure as follows:\n\\[\\frac{dx}{dt} = P(t)x + Q(x, t),\\]\nthe method of \"variation of constants\" gives the following solution:\n\\[x(t) = e^{\\int_s^t P(\\tau) d\\tau} \\left[\\int_s^t Q(x, \\tau) e^{-\\int_0^\\tau P(\\xi) d\\xi} d\\tau + C\\right].\\]\nBy simultaneously considering the following two equations\n\\begin{cases}\nx(t) = e^{\\int_s^t P(\\tau) d\\tau} \\left[\\int_s^t Q(x, \\tau) e^{-\\int_0^\\tau P(\\xi) d\\xi} d\\tau + C\\right], \\\\\nx(s) = e^{\\int_s^t P(\\tau) d\\tau} \\left[\\int_s^t Q(x, \\tau) e^{-\\int_0^\\tau P(\\xi) d\\xi} d\\tau + C\\right], \n\\end{cases}\nand eliminating \\(C\\), we obtain\n\\[x(t) = x(s) e^{\\int_s^t P(\\tau) d\\tau} + \\int_s^t Q(x, \\tau) e^{\\int_0^\\xi P(\\xi) d\\xi} d\\tau.\\]\nNow we compare Eq.(15) with Eq.(34) and let\n\\[P(t) = -f(t)\\]\nand\n\\[Q(x,t) = f(t)\\mu + \\frac{g^2(t)}{2\\sigma_t} \\epsilon_\\theta(x_t, t).\\]\nTherefore, we can rewrite Eq.(35) as\n\\[x_t = x_s e^{-\\int_s^t f(\\tau) d\\tau} + \\int_s^t e^{\\int_0^\\xi f(\\xi) d\\xi} \\left[f(\\tau)\\mu + \\frac{g^2(\\tau)}{2\\sigma_t} \\epsilon_\\theta(x_t, t)\\right] d\\tau\\]\n\\[= x_s e^{-\\int_s^t f(\\tau) d\\tau} + \\mu (1 - e^{-\\int_0^t f(\\tau) d\\tau}) + \\int_s^t e^{\\int_0^\\xi f(\\xi) d\\xi} \\frac{g^2(\\tau)}{2\\sigma_\\tau} \\epsilon_\\theta(x, \\tau) d\\tau,\\]\nwhich is equivalent to Eq.(33).\nProposition 3. Given an initial value \\(x_s\\) at time \\(s \\in [0,T]\\), the solution \\(x_t\\) at time \\(t \\in [0,s]\\) of Eq.(22) is\n\\[x_t = \\frac{\\sigma_t e^{-(\\lambda_t - \\lambda_s)}}{ \\sigma_s} x_s + \\mu \\left(1 - \\frac{A_t e^{-2(\\lambda_t - \\lambda_s)}}{A_s} \\right) + \\frac{ \\sigma_t}{A_s} \\left[ 2A_t \\int_{\\lambda_s}^{\\lambda_t} e^{-2(\\lambda_t - \\lambda)} x_\\theta(x_\\lambda, \\lambda) d\\lambda + \\sigma_t \\sqrt{1 - e^{-2(\\lambda_t - \\lambda_s)}} z \\right],\\]\nwhere \\(z \\sim \\mathcal{N}(0, I)\\).\nProof. According to Eq.(32), we define\n\\[u(t) = \\frac{g^2(t)}{2 \\sigma_t^2} - f(t)\\]\nand\n\\[v(x,t) = x e^{\\int_s^t u(\\tau) d\\tau}.\\]\nWe substitute \\(f(x, t)\\) and \\(g(t)\\) in Eq.(32) with the corresponding drift and diffusion coefficients in Eq.(22), and integrate both sides of the equation from \\(s\\) to \\(t\\):\n\\[x_t - x_s e^{\\int_s^t u(\\tau) d\\tau} = \\mu \\int_s^t \\left[e^{\\int_\\xi^t f(\\tau) d\\tau} - \\frac{\\sigma_t}{\\alpha_\\tau} \\frac{f(\\tau)}{\\sigma_\\tau} \\alpha_\\tau \\right] d\\tau + \\int_s^t \\epsilon_\\theta(x, \\tau) \\frac{g^2(\\tau)}{\\sigma_\\tau} d\\tau.\\]\nWe can rewrite \\(g^2(\\tau)\\) as Eq.(12) and obtain\n\\[\\int_s^t e^{u(\\tau) d\\tau} = exp\\left( -2 \\int \\frac{\\alpha_\\tau}{\\sigma_\\tau} f(\\tau) d\\tau \\right) = \\frac{\\sigma_t}{\\alpha_t} e^{-(\\lambda_t - \\lambda_s)} \\frac{\\alpha_s}{\\sigma_s}.\\]"}]}