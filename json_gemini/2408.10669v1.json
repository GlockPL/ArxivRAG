{"title": "Tensor tree learns hidden relational structures in data to construct generative models", "authors": ["Kenji Harada", "Tsuyoshi Okubo", "Naoki Kawashima"], "abstract": "Based on the tensor tree network with the Born machine framework, we propose a general method\nfor constructing a generative model by expressing the target distribution function as the quantum\nwave function amplitude represented by a tensor tree. The key idea is dynamically optimizing the\ntree structure that minimizes the bond mutual information. The proposed method offers enhanced\nperformance and uncovers hidden relational structures in the target data. We illustrate potential\npractical applications with four examples: (i) random patterns, (ii) QMNIST hand-written digits,\n(iii) Bayesian networks, and (iv) the stock price fluctuation pattern in S&P500. In (i) and (ii),\nstrongly correlated variables were concentrated near the center of the network; in (iii), the causality\npattern was identified; and, in (iv), a structure corresponding to the eleven sectors emerged.", "sections": [{"title": "Introduction", "content": "Generative models thrive on the adaptability of archi-\ntectures tailored to the data's characteristics. The archi-\ntecture is often chosen manually, such as using RNNs for\ntime series and sequential data. However, in some cases,\nthe network structure is automatically selected. For ex-\nample, a fully connected neural network is optimized by\npruning parameters, such as the weight matrices, to re-\nflect the correlations among data elements [1]. Typically,\nthe architecture of a generative model is expressed as a\nnetwork that represents connections between data ele-\nments. A high performance is achieved by manually or\nautomatically optimizing this network structure to suit\nthe data characteristics. Therefore, learning the rela-\ntional structure of the target data is important to obtain\nbetter generative models.\nRecently, a method of using the quantum state mea-\nsurement process as a generative model was proposed\n2. From Born's rule, it follows that the probability of\nobtaining a measurement output x is the squared am-\nplitude of the wave function of a quantum state. This\nprobability model for generative modeling is called the\nBorn machine. In [2] and [3], a tensor network was pro-\nposed to represent the wave function in the Born ma-\nchine. A tensor network is illustrated as a network of\ntensors, with a connection representing the partial con-\ntraction between tensors. It has been utilized in various\nmachine learning methods, such as classifiers [4], neural\nnetworks [5], and reinforcement learning [6]. In [2] and\n3], a one-dimensional chain (called tensor train) and a\nbalanced tree were adopted for the tensor network repre-\nsenting a wave function, respectively. The differences in\nthe performance of resulting generative models suggest\nthe importance of choosing the network structure. How-\never, how we can choose the best network structure for a\ngiven data remains unclear.\nIn this study, we improve the scheme in [2] and [3]: The\nkey idea is to adopt a general tree structure for the net-\nwork and adaptively reconnect its branches to reflect the\ntarget data's nature better. The reconnection is based\non combining and decomposing local tensors, as shown\nin Fig.1(a). We choose a decomposition guided by the\nmutual information to minimize information flow. If the\nnetwork structure does not fit the relational data struc-\nture, the information requires passing through unneces-\nsarily long paths to achieve the same level of learning. In\ncontrast, a better-fitting tree structure demands less in-\nformation flow (i.e., the bond mutual information) with-\nout degrading the learning quality. Though a tree net-\nwork is not the most general network structure, it strikes\na balance between flexibility (e.g., the tensor train and\nbalanced tree are special cases of the tree structure) and\ncomputational efficiency (e.g., computational time is pro-\nportional to the number of the variables). Compared to\nthe conventional parameter adjustment approaches, such\nas neural network pruning, the sparsely connected net-\nwork allows the proposed method to use fewer parameters\nduring learning.\nTo demonstrate the virtue of the proposed method, we\napplied it to four data sets: (i) artificial random patterns,\n(ii) images of handwritten digits, (iii) causally connected\nrandom variables (Bayesian network), and (iv) the fluc-\ntuation pattern of stock prices in S&P 500 index. The\nresults show that the proposed generative model outper-\nforms conventional methods when no prior knowledge of\nthe data structure is used. In addition, the tree structure\nafter the learning/optimization process provides insights\ninto the (sometimes hidden) relational data structure,\nsuch as strong correlations between bits or pixels for arti-\nficial random patterns and images of handwritten digits,\ncausal dependencies among random variables of Bayesian\nnetwork, and the \"11-sector\" categories of stocks in S&P\n500 index. The proposed method automatically learns\nthe hidden relational target data structure and reflects it\nin the network structure, making it a powerful tool for\ndata analysis even without prior knowledge of the infor-\nmation structure."}, {"title": "RESULTS", "content": "Optimization of tensors and network structure in a\nBorn machine with a tensor tree\nWe introduce a general tree structure in the Born ma-\nchine framework in which the model distribution $p_\\theta$ is\ndefined by the Born rule,\n$p_\\theta(x) = \\frac{|\\langle x | \\psi_\\theta \\rangle|^2}{\\langle \\psi_\\theta | \\psi_\\theta \\rangle},$  (1)\nwhere $|\\psi_\\theta \\rangle$ is the quantum state of the machine. Here,\nwe replace a real quantum system with a tensor network\nwith tree structure to represent $|\\psi_\\theta \\rangle$. Generally, a ten-\nsor network is a partial contraction of the product of\ntensors. It is depicted as a graph, where the nodes (i.e.,\ncircles or ellipses) represent tensors and the lines their\nindices. A line connecting two nodes indicates an index\nto be contracted, referred to as a bond. The dimension of\nan index, i.e., how many possible values it may take, is\ncalled the bond dimension. We consider a tensor network\nwith no loop, which we call tensor tree. Each component\ntensor has three indices.\nIn the Born machine with a tensor tree, the mutual\ninformation that measures mutual dependence between\ntwo groups of outputs has a strict limit. By cutting a\nbond in a tensor tree, we decompose it into two sub-\ntrees. In addition, outputs are divided into two groups,\none for each subtree. For example, for the network in\nFig.1(b), by cutting a thick bond, the machine's outputs\nare divided into parts A and B. We call the mutual in-\nformation between A and B as bond mutual information\n(BMI) and is given as $I(A, B)$. $I(A, B)$ is bounded from\nabove by the entanglement entropy of $|\\psi\\rangle$ for the same\nbipartition [7, 8]. In addition, the entanglement entropy\nfor the bipartition by a bond cut is less than the loga-\nrithm of the bond dimension of the cut bond[9]. There-\nfore, $I(A, B)$ is smaller than the logarithm of the bond\ndimension, $\\chi$:\n$I(A, B) = H(A) + H(B) - H(A, B) \\le \\ln(\\chi),$  (2)\nwhere $H()$ is the Shanon entropy.\nThese conditions provide the following guiding prin-\nciple in optimizing the tree structure: when the bond\ndimension is fixed, avoid trees in which the BMI is close\nto the upper bound set by the bond dimension, which\nsuggests putting two strongly correlated points close to\neach other on the tree. After combining two connected\ntensors in a tensor tree, we can decompose it in three\nways, as shown in Fig.1 (a). If we choose the least BMI\ndecomposition, we may rearrange two strongly correlated\npoints close to each other. Therefore, we propose the\nbranch-reconnection procedure on a tensor tree, as shown\nin Fig.1 (b). We refer to this method of adjusting the\ntree structure of the tensor network guided by BMI as\nthe adaptive tensor tree (ATT) method."}, {"title": "Examples", "content": "To demonstrate the efficiency of the ATT method, we\napply it to various datasets, namely random patterns,\nhandwritten digits images, data with probabilistic de-\npendencies generated by Bayesian networks, and the real\ndata on stock price fluctuation patterns in S&P500 index.\n1. Random patterns and images of handwritten digits\nGenerally, we would like a generative model to mem-\norize finite random bit patterns accurately while effec-\ntively capturing the strong correlation among the bits.\nAlthough, generative modeling with a tensor train has\nstruggled to learn long random sequences, in [3], it was\ndemonstrated that these issues can be resolved by a bal-\nlanced tensor tree, indicating the importance of the right\nchoice of the tensor network in addressing this problem.\nWe consider a particularly challenging problem with the\ntarget data representing a long-range correlation to ex-\nplore this issue further. Here, the data to be learned\nconsists of 10 sequences of 128 bits. While the left- and\nright-most 32 bits are generated as mutually independent\nrandom binary numbers, the bits in the intermediate part\nare fixed at 0 for all ten patterns, as illustrated in a tensor\ntrain in Fig.2(a). Since ten is less than the total number\nof possible bit patterns, it creates a strong correlation\nbetween the left- and right-most parts.\nStarting with the tensor train as the initial tensor net-\nwork, we compare two cases: (i) the network fixed as\nthe initial tensor train, and (ii) the network modified\nby the ATT method. When the network is adaptively\nmodified, the negative log-likelihood (NLL) for random\npatterns converges to the expected lower bound, i.e.,\nln(10)(Fig.2(b)). However, the NLL of tensor trains con-\nverges to local minimum values larger than ln(10). Fig.2\n(c) shows a particular realization of the final network\nstructure. The color of the circles represents their origi-\nnal positions, as shown in Fig.2(a). The red and blue cir-\ncles, initially far apart, are tightly clustered in the final\ntree, reflecting their strong correlation. The irrelevant\nintermediate bits (blueish green or ocher) are excluded\nfrom it and barely connected (i.e., connected by low-BMI\nedges).\nIn the previous example, the relevant random variables\nare separated in the tensor train, making the train struc-\nture disadvantageous compared to the structure derived\nby branch-reconnection. Below, we consider another sce-\nnario where the relational structure among the random\nvariables is not immediately apparent, namely, 2D im-\nages. The balanced tensor tree outperforms the tensor\ntrain for images because its network structure aligns with\nthe geometrical relationship among pixels [3]. However,\nto construct a chain or balanced tree of random bits (pix-\nels), we need to know their spatial location, i.e., they re-\nquire some prior knowledge about the target data. Here,\nwe compare the present method with fixed network meth-\nods under the condition of no prior knowledge. We apply\nthem to images of handwritten digits from the QMNIST\ndataset [10] using an initial random tensor tree or random\npixel permutation (see the METHODS section for the de-\ntails), which eliminates prior knowledge of the dataset's\nimplicit geometrical relationship.\nFig.2(e) displays the final values of NLL for the test\nhandwritten digit images. Comparing the results of the\nfixed balanced tree with (crosses) and without (solid\ncircles) permutation shows the significant effect of the\nprior knowledge about the spatial arrangement of the\npixels. In contrast, network optimization with branch-\nreconnection (blue and red) shows performance compa-\nrable to the balanced tree for the bond dimension around\n10, even with the initial random tree. Fig.2(d) illustrates\nan optimized tensor tree network. The edges with strong\nBMI are concentrated near the center of the optimized\nnetwork. We measure the distance from the center to\nan open edge representing a pixel to analyze the prop-\nerties of the optimized network. In Fig.2(f), the color\nmap shows pixel ranking based on the distance from the\ncenter. The compact region near the network's center is\nalso the central region in the 2D images. Similar to the\ncase of the separated random sequences, the generative\nmodel automatically learns the relevant relational struc-\nture among the random variables and places them close\ntogether on the tensor network.\n2. Random variables of Bayesian networks\nA Bayesian network [11] is a stochastic model rep-\nresenting causal dependencies among random variables\nthrough a directed acyclic graph. A node means a ran-\ndom variable and a directed arrow is drawn from nodes\non which a pointed random variable depends. Bayesian\nnetworks are used for modeling complex systems, such as\nmedical diagnoses, financial forecasting, or risk analysis,\nwhere multiple variables interact in complicated ways.\nHowever, constructing a Bayesian network from data\nis challenging, as this task, called structure learning [12],\nis an NP-hard problem. In a special case, if any random\nvariable is causally dependent on only one random vari-\nable, the Bayesian network structure can be represented\nas a directed branching tree. In this case, Chow and Liu\n13] proposed an algorithm to construct the Bayesian net-\nwork as the maximum spanning tree from the pair-wise\nmutual information of all possible pairs. Generally, a\nBayesian network with no loop structure is called a poly-\ntree. In this case, the topology can be constructed as a\nmaximum spanning tree [14].\nTo the ATT method, the task of the Bayesian tree con-\nstruction poses another case where random variables are\nmutually and causally related. While it is not clear, a pri-\nori, whether the ATT method always produces the \"cor-\nrect\" tree, i.e., the Bayesian tree used in generating the\ndata sets, we can at least expect that the correct tree is\nthe stable solution of our method. We explain this as fol-\nlows. The present algorithm decomposes the composite\ntensor into two pairs of smaller tensors (Fig.1(a)). If two\nstrongly related random variables of the four (with sub-\ntrees below them) are separated by this decomposition,\nit causes a large BMI between the two pairs. Therefore,\nselecting such a pairing from the three possible pairings\nis unlikely. In the selected pairing, the causal depen-\ndency will be stronger within each pair, not between the\npairs. Hence, if we start from the topology of the poly-\ntree exactly reflecting the correct Bayesian network, the\nbranch-reconnection procedure will not change the tree\nstructure. Thus, the correct tree is the optimal solution\nfor the ATT method.\nWe tested the ATT method using the data generated\nfrom three Bayesian polytree networks. In Fig.3(a-c), the\nBayesian network is shown by nodes with digits and ar-\nrows. The optimized tensor tree is illustrated by circles\nand bonds. A node with a digit represents a random\nvariable corresponding to an open index of the tensor\ntree. An arrow indicates the cause-to-effect direction be-\ntween two random variables. The network in Fig.3(a)\nis a simple sequence with no branching. Fig.3(b) and\n(c) depict scenarios containing a branch and collision in\nthe Bayesian network structure, respectively. The opti-\nmized tensor tree, starting from random tensor trees, suc-\ncessfully captures the corresponding topology of Bayesian\nnetworks for all cases, though our results do not detect\nthe direction of dependencies explicitly.\n3. Stock price fluctuation patterns in S&P 500 index\nTo evaluate the effectiveness of the ATT method for\nreal data with an unknown structure, we built a gener-\native model for the fluctuation pattern of stock prices.\nWe focused on the fluctuation pattern of stock prices\nlisted on S&P 500, a widely recognized stock price in-\ndex. We analyzed 436 stocks that have been a part of\nS&P 500 index since 2010. We convert the change rate\nof each stock price into a binary value: 1 if it is higher\nthan the average for all stocks and 0 otherwise. Thus,\nour dataset comprises 3589 samples, each represented by\na 436-dimensional vector with binary components. To\nevaluate the generalization ability of the resulting gener-\native models, we use half of the sample for training and"}, {"title": "DISCUSSION", "content": "The proposed method has shown satisfactory perfor-\nmance when applied to artificial and real data sets. The\noptimization is done in tensor elements and the tree\nstructure is guided by BMI. Thus, the network structure\nafter training reflects the information flow behind the\ngiven data set; even without prior knowledge, the method\nsuccessfully identified concealed information paths and\nadjusted the network structure to match them. The\nmethod provides a new systematic approach to gener-\native modeling, adapting to the information flow hidden\nbehind the given data sets.\nIn the examples in the present study, we fixed the bond\ndimension of each edge in the tensor tree. We also defined\nthe BMI by cutting the corresponding edge to measure\nthe amount of information flow. The relation between the\nbond dimension and BMI is analogous to that between\nthe flow capacity of a water pipe and actual flow through\nit. Therefore, we can adjust the bond dimension depend-\ning on BMI. For small BMI, the bond dimension can\nbe reduced without harm. This modification could fur-\nther compress the tensor tree generative models, which\ncan be useful in future practical applications. As spe-\ncial cases, we may consider the Born machines using a\ntensor train and balanced tensor tree with automatically\nadjusted bond dimensions.\nWe let the tensor tree represent a wave function in the\nBorn machine. The Born machine framework has the fol-\nlowing advantages. First, in principle, it may be imple-\nmented as a real quantum circuit, qualitatively improv-\ning upon any classical generative modeling. Though we\nhave not explored this possibility in the present article,\nthis is one of the attractive directions of future research.\nSecond, it guarantees the positivity of the weights, which\nis the basis of the proposed method. However, we may\nconsider other alternatives, such as the steepest descent\nby differentiating with respect to the logarithm of ten-\nsor elements rather than the tensor elements themselves.\nIn addition, improvement in the optimization details is a\nsubject of future work."}, {"title": "METHODS", "content": "Tree tensor network representation for generative\nmodeling\nThe proposed approach is based on optimizing a gen-\neral tree structure for generative modeling. In the Born\nmachine framework, the model distribution $p_\\theta(x)$ is de-\nfined by the Born rule in (1) as the squared amplitude\nof the wave function of a Born machine. Then, we intro-\nduce a tensor tree to represent the amplitude of the wave\nfunction [9]. The tensor train and balanced tree used in\n2] and [3], respectively, are special cases of the general\ntree structure. Based on a tree structure, we can effi-\nciently calculate various exact contractions and marginal\ndistributions using recursive procedures.\nWe can define a canonical form for a tensor tree [15] to\ndetermine the optimal truncation from local calculations.\nWe first choose the root edge to construct the canonical\nform of a given tensor tree. Then, all the tensors be-\ncome isometries by recursively applying singular value\ndecomposition (SVD) starting from the leaves. Finally,\nwe obtain the matrix A on the root edge. Using SVD,\nthe root's position can be moved to any neighboring edge.\nOur method chooses the edge to be updated as the root.\nAfter modifying the local connection and updating the\ntensors around the root edge, we move the root position\nto the neighboring edge that has not been updated for\nthe longest time."}, {"title": "Statistical estimation of mutual information", "content": "From the definition of the mutual information in (2),\nthe mutual information for a bipartition into sub-systems\nA and B is rewritten as\n$I(A, B) = \\sum_{(a,b)} p(a, b) \\ln \\frac{p(a, b)}{p(a)p(b)},$  (3)\nwhere a and b are configurations on sub-systems A and\nB, respectively, $p(a,b) = p_\\theta(x = (a,b))$ is a joint dis-\ntribution, and $p(a) = \\Sigma_b p(a,b)$ and $p(b) = \\Sigma_a p(a, b)$\nare marginal distributions. We can efficiently compute\nthe values of $p(a)$ and $p(b)$ utilizing a tensor tree rep-\nresentation of a joint distribution in the canonical form.\nHowever, estimating the mutual information in a tensor\ntree state is challenging due to the need to calculate the\nsummation for all configurations (a,b). Moreover, the\nnumber of configurations grows exponentially with the\nsystem size.\nTo estimate the mutual information approximately, we\nreplace the average over the joint distribution with that\nover the empirical data distribution as\n$I(A, B) \\approx I_{data} (A, B) = \\frac{1}{|M'|} \\sum_{(a,b) \\in M'}  \\ln \\frac{p(a, b)}{p(a)p(b)},$  (4)\nwhere M' is the ensemble of data samples. The estimator\n(4) may be negative if the empirical data distribution\n$t_{emp}(a, b)$ differs from the joint distribution $p(a, b)$.\nAn alternative method involves estimating the average\nof $\\ln[p(a, b)/p(a)p(b)]$ using Monte Carlo sampling. If a\nBorn machine has a tree structure, it can efficiently calcu-\nlate the marginal distribution of a random variable given\nthe other random variable states. The conditional distri-\nbution is then used to sample random variables perfectly.\nRepeating these steps can generate an independent sam-\nple (a, b) with probability p(a, b). Consequently, we can\nefficiently do Monte Carlo estimation of (3) as (4) with\nM', the ensemble of samples generated from $p(a, b)$ di-\nrectly. However, during learning, we only used the first\nmethod due to the low quality of the Born machine at\nthe early stage."}, {"title": "Branch-reconnection algorithm", "content": "The detail of our branch-reconnection in Fig.1(b) is as\nfollows:\n(i) Initialize the network and all tensor elements.\n(ii) Start from a randomly selected \"virtual\" bond.\n(Here, a virtual bond does not directly represent\nan original variable.)\n(iii) Contract the bond and obtain a 4-leg tensor. Im-\nprove the combined tensor to approximate the em-\npirical distribution of the data better.\n(iv) Estimate BMI by sampling for each of the three\nways of decomposition (Fig.1(a)) and choose the\none with the smallest BMI. Before computing BMI\nfor each decomposition, the component tensors are\nimproved.\n(v) Replace the 4-leg tensor with the chosen decompo-\nsition.\n(vi) If the termination condition is satisfied, terminate.\n(vii) Move to a bond connected to one of the two new\ntensors and return to (iii).\nWe utilized a reconnection-based decomposition of a\ncomposite tensor to calculate the ground state of a quan-\ntum model with a tensor tree[16]. We improve the tensor\nin (iii) and (iv) by employing gradient-descent updates to\nminimize NLL."}, {"title": "Training details", "content": "In steps (iii) and (iv) in the branch-reconnection algo-\nrithm, we improve the combined tensor and new tensors\nin the decomposition by employing gradient-descent up-\ndates to minimize NLL [2, 3]. Assuming that the target\ndistribution of generative modeling is an empirical data\ndistribution, $t_{emp}$, NLL is defined as\n$\\mathcal{L} = \\frac{1}{M} \\sum_{x \\in M}  \\ln[p_\\theta(x)] = H(t_{emp}) + D_{KL}(t_{emp}||p_\\theta),$  (5)\nwhere M is the ensemble of data samples and $D_{KL}$ is\na Kullback-Leibler divergence. In this study, M is the\nsame as M' in (4). We perform a single gradient-descent\nupdate in the procedure (iii) and ten ones in (iv), with\nlearning rates of 0.05 for random patterns and 0.001 for\nothers. For data samples M, we use the batch of train-\ning data samples for random patterns and S&P 500 stock\nprice fluctuation patterns and a mini-batch of size 1000\nfor others. If the mini-batch size is small, instability oc-\ncurs in estimating mutual information. We set the inter-\nval for changing the mini-batch to 1000 gradient-descent\nupdates for the handwritten digits and updated the new\nmini-batch after all edges for the Bayesian networks. The\nNLL for test data is useful for checking the overfitting of\nlearning data. For the Bayesian network data, we showed\nthe optimized network with lower NLL for test data in\nFig.3 because it fluctuates in the optimization process\nwhen changing a mini-batch.\nWe use the QMNIST dataset [10] for the images of\nhandwritten digits. It is an improved version of the\nMixed National Institute of Standards and Technology\ndatabase (MNIST) dataset[17], a collection of handwrit-\ning digits from 0 to 9 on 28 \u00d7 28 pixels. In QMNIST, the\nnumber of test images increases to the number of learning\nimages, 60000. The intensity of a pixel ranges from 0 to"}]}