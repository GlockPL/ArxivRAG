{"title": "Tensor tree learns hidden relational structures in data to construct generative models", "authors": ["Kenji Harada", "Tsuyoshi Okubo", "Naoki Kawashima"], "abstract": "Based on the tensor tree network with the Born machine framework, we propose a general method for constructing a generative model by expressing the target distribution function as the quantum wave function amplitude represented by a tensor tree. The key idea is dynamically optimizing the tree structure that minimizes the bond mutual information. The proposed method offers enhanced performance and uncovers hidden relational structures in the target data. We illustrate potential practical applications with four examples: (i) random patterns, (ii) QMNIST hand-written digits, (iii) Bayesian networks, and (iv) the stock price fluctuation pattern in S&P500. In (i) and (ii), strongly correlated variables were concentrated near the center of the network; in (iii), the causality pattern was identified; and, in (iv), a structure corresponding to the eleven sectors emerged.", "sections": [{"title": "Introduction", "content": "Generative models thrive on the adaptability of architectures tailored to the data's characteristics. The architecture is often chosen manually, such as using RNNs for time series and sequential data. However, in some cases, the network structure is automatically selected. For example, a fully connected neural network is optimized by pruning parameters, such as the weight matrices, to reflect the correlations among data elements [1]. Typically, the architecture of a generative model is expressed as a network that represents connections between data elements. A high performance is achieved by manually or automatically optimizing this network structure to suit the data characteristics. Therefore, learning the relational structure of the target data is important to obtain better generative models.\nRecently, a method of using the quantum state measurement process as a generative model was proposed [2]. From Born's rule, it follows that the probability of obtaining a measurement output x is the squared amplitude of the wave function of a quantum state. This probability model for generative modeling is called the Born machine. In [2] and [3], a tensor network was proposed to represent the wave function in the Born machine. A tensor network is illustrated as a network of tensors, with a connection representing the partial contraction between tensors. It has been utilized in various machine learning methods, such as classifiers [4], neural networks [5], and reinforcement learning [6]. In [2] and [3], a one-dimensional chain (called tensor train) and a balanced tree were adopted for the tensor network representing a wave function, respectively. The differences in the performance of resulting generative models suggest the importance of choosing the network structure. However, how we can choose the best network structure for a given data remains unclear.\nIn this study, we improve the scheme in [2] and [3]: The key idea is to adopt a general tree structure for the network and adaptively reconnect its branches to reflect the target data's nature better. The reconnection is based on combining and decomposing local tensors, as shown in Fig.1(a). We choose a decomposition guided by the mutual information to minimize information flow. If the network structure does not fit the relational data structure, the information requires passing through unnecessarily long paths to achieve the same level of learning. In contrast, a better-fitting tree structure demands less information flow (i.e., the bond mutual information) without degrading the learning quality. Though a tree network is not the most general network structure, it strikes a balance between flexibility (e.g., the tensor train and balanced tree are special cases of the tree structure) and computational efficiency (e.g., computational time is proportional to the number of the variables). Compared to the conventional parameter adjustment approaches, such as neural network pruning, the sparsely connected network allows the proposed method to use fewer parameters during learning.\nTo demonstrate the virtue of the proposed method, we applied it to four data sets: (i) artificial random patterns, (ii) images of handwritten digits, (iii) causally connected random variables (Bayesian network), and (iv) the fluctuation pattern of stock prices in S&P 500 index. The results show that the proposed generative model outperforms conventional methods when no prior knowledge of the data structure is used. In addition, the tree structure after the learning/optimization process provides insights into the (sometimes hidden) relational data structure, such as strong correlations between bits or pixels for artificial random patterns and images of handwritten digits, causal dependencies among random variables of Bayesian network, and the \"11-sector\" categories of stocks in S&P 500 index. The proposed method automatically learns the hidden relational target data structure and reflects it in the network structure, making it a powerful tool for data analysis even without prior knowledge of the information structure."}, {"title": "Results", "content": "Optimization of tensors and network structure in a Born machine with a tensor tree\nWe introduce a general tree structure in the Born machine framework in which the model distribution $p_\\theta$ is defined by the Born rule,\n$p_\\theta(x) = \\frac{|\\langle x | \\Psi_\\theta \\rangle|^2}{\\langle \\Psi_\\theta | \\Psi_\\theta \\rangle}.$ (1)\nwhere $|\\Psi_\\theta \\rangle$ is the quantum state of the machine. Here, we replace a real quantum system with a tensor network with tree structure to represent $|\\Psi_\\theta \\rangle$. Generally, a tensor network is a partial contraction of the product of tensors. It is depicted as a graph, where the nodes (i.e., circles or ellipses) represent tensors and the lines their indices. A line connecting two nodes indicates an index to be contracted, referred to as a bond. The dimension of an index, i.e., how many possible values it may take, is called the bond dimension. We consider a tensor network with no loop, which we call tensor tree. Each component tensor has three indices.\nIn the Born machine with a tensor tree, the mutual information that measures mutual dependence between two groups of outputs has a strict limit. By cutting a bond in a tensor tree, we decompose it into two subtrees. In addition, outputs are divided into two groups, one for each subtree. For example, for the network in Fig.1(b), by cutting a thick bond, the machine's outputs are divided into parts A and B. We call the mutual information between A and B as bond mutual information (BMI) and is given as $I(A, B)$. $I(A, B)$ is bounded from above by the entanglement entropy of $|\\psi\\rangle$ for the same bipartition [7, 8]. In addition, the entanglement entropy for the bipartition by a bond cut is less than the logarithm of the bond dimension of the cut bond[9]. Therefore, $I(A, B)$ is smaller than the logarithm of the bond dimension, $\\chi$:\n$I(A, B) = H(A) + H(B) - H(A, B) \\leq \\ln(\\chi),$ (2)\nwhere $H()$ is the Shanon entropy.\nThese conditions provide the following guiding principle in optimizing the tree structure: when the bond dimension is fixed, avoid trees in which the BMI is close to the upper bound set by the bond dimension, which suggests putting two strongly correlated points close to each other on the tree. After combining two connected tensors in a tensor tree, we can decompose it in three ways, as shown in Fig.1 (a). If we choose the least BMI decomposition, we may rearrange two strongly correlated points close to each other. Therefore, we propose the branch-reconnection procedure on a tensor tree, as shown in Fig.1 (b). We refer to this method of adjusting the tree structure of the tensor network guided by BMI as the adaptive tensor tree (ATT) method."}, {"title": "Examples", "content": "To demonstrate the efficiency of the ATT method, we apply it to various datasets, namely random patterns, handwritten digits images, data with probabilistic dependencies generated by Bayesian networks, and the real data on stock price fluctuation patterns in S&P500 index.\n1. Random patterns and images of handwritten digits\nGenerally, we would like a generative model to memorize finite random bit patterns accurately while effectively capturing the strong correlation among the bits. Although, generative modeling with a tensor train has struggled to learn long random sequences, in [3], it was demonstrated that these issues can be resolved by a balanced tensor tree, indicating the importance of the right choice of the tensor network in addressing this problem. We consider a particularly challenging problem with the target data representing a long-range correlation to explore this issue further. Here, the data to be learned consists of 10 sequences of 128 bits. While the left- and right-most 32 bits are generated as mutually independent random binary numbers, the bits in the intermediate part are fixed at 0 for all ten patterns, as illustrated in a tensor train in Fig.2(a). Since ten is less than the total number of possible bit patterns, it creates a strong correlation between the left- and right-most parts.\nStarting with the tensor train as the initial tensor network, we compare two cases: (i) the network fixed as the initial tensor train, and (ii) the network modified by the ATT method. When the network is adaptively modified, the negative log-likelihood (NLL) for random patterns converges to the expected lower bound, i.e., ln(10)(Fig.2(b)). However, the NLL of tensor trains converges to local minimum values larger than ln(10). Fig.2 (c) shows a particular realization of the final network structure. The color of the circles represents their original positions, as shown in Fig.2(a). The red and blue circles, initially far apart, are tightly clustered in the final tree, reflecting their strong correlation. The irrelevant intermediate bits (blueish green or ocher) are excluded from it and barely connected (i.e., connected by low-BMI edges).\nIn the previous example, the relevant random variables are separated in the tensor train, making the train structure disadvantageous compared to the structure derived by branch-reconnection. Below, we consider another scenario where the relational structure among the random variables is not immediately apparent, namely, 2D images. The balanced tensor tree outperforms the tensor train for images because its network structure aligns with the geometrical relationship among pixels [3]. However, to construct a chain or balanced tree of random bits (pixels), we need to know their spatial location, i.e., they require some prior knowledge about the target data. Here, we compare the present method with fixed network methods under the condition of no prior knowledge. We apply them to images of handwritten digits from the QMNIST dataset [10] using an initial random tensor tree or random pixel permutation (see the METHODS section for the details), which eliminates prior knowledge of the dataset's implicit geometrical relationship.\nFig.2(e) displays the final values of NLL for the test handwritten digit images. Comparing the results of the fixed balanced tree with (crosses) and without (solid circles) permutation shows the significant effect of the prior knowledge about the spatial arrangement of the pixels. In contrast, network optimization with branch-reconnection (blue and red) shows performance comparable to the balanced tree for the bond dimension around 10, even with the initial random tree. Fig.2(d) illustrates an optimized tensor tree network. The edges with strong BMI are concentrated near the center of the optimized network. We measure the distance from the center to an open edge representing a pixel to analyze the properties of the optimized network. In Fig.2(f), the color map shows pixel ranking based on the distance from the center. The compact region near the network's center is also the central region in the 2D images. Similar to the case of the separated random sequences, the generative model automatically learns the relevant relational structure among the random variables and places them close together on the tensor network."}, {"title": "Random variables of Bayesian networks", "content": "A Bayesian network [11] is a stochastic model representing causal dependencies among random variables through a directed acyclic graph. A node means a random variable and a directed arrow is drawn from nodes on which a pointed random variable depends. Bayesian networks are used for modeling complex systems, such as medical diagnoses, financial forecasting, or risk analysis, where multiple variables interact in complicated ways.\nHowever, constructing a Bayesian network from data is challenging, as this task, called structure learning [12], is an NP-hard problem. In a special case, if any random variable is causally dependent on only one random variable, the Bayesian network structure can be represented as a directed branching tree. In this case, Chow and Liu [13] proposed an algorithm to construct the Bayesian network as the maximum spanning tree from the pair-wise mutual information of all possible pairs. Generally, a Bayesian network with no loop structure is called a polytree. In this case, the topology can be constructed as a maximum spanning tree [14].\nTo the ATT method, the task of the Bayesian tree construction poses another case where random variables are mutually and causally related. While it is not clear, a priori, whether the ATT method always produces the \"correct\" tree, i.e., the Bayesian tree used in generating the data sets, we can at least expect that the correct tree is the stable solution of our method. We explain this as follows. The present algorithm decomposes the composite tensor into two pairs of smaller tensors (Fig.1(a)). If two strongly related random variables of the four (with subtrees below them) are separated by this decomposition, it causes a large BMI between the two pairs. Therefore, selecting such a pairing from the three possible pairings is unlikely. In the selected pairing, the causal dependency will be stronger within each pair, not between the pairs. Hence, if we start from the topology of the polytree exactly reflecting the correct Bayesian network, the branch-reconnection procedure will not change the tree structure. Thus, the correct tree is the optimal solution for the ATT method.\nWe tested the ATT method using the data generated from three Bayesian polytree networks. In Fig.3(a-c), the Bayesian network is shown by nodes with digits and arrows. The optimized tensor tree is illustrated by circles and bonds. A node with a digit represents a random variable corresponding to an open index of the tensor tree. An arrow indicates the cause-to-effect direction between two random variables. The network in Fig.3(a) is a simple sequence with no branching. Fig.3(b) and (c) depict scenarios containing a branch and collision in the Bayesian network structure, respectively. The optimized tensor tree, starting from random tensor trees, successfully captures the corresponding topology of Bayesian networks for all cases, though our results do not detect the direction of dependencies explicitly."}, {"title": "Stock price fluctuation patterns in S&P 500 index", "content": "To evaluate the effectiveness of the ATT method for real data with an unknown structure, we built a generative model for the fluctuation pattern of stock prices. We focused on the fluctuation pattern of stock prices listed on S&P 500, a widely recognized stock price index. We analyzed 436 stocks that have been a part of S&P 500 index since 2010. We convert the change rate of each stock price into a binary value: 1 if it is higher than the average for all stocks and 0 otherwise. Thus, our dataset comprises 3589 samples, each represented by a 436-dimensional vector with binary components. To evaluate the generalization ability of the resulting generative models, we use half of the sample for training and the other half for evaluating the generalization ability by measuring the NLL.\nAlthough the stocks are categorized into 11 sectors and even finer classifications according to the \"Global Industry Classification System,\" we do not feed the generative model with such prior knowledge. We start from a random tree and let the tree evolve using the branch-reconnection algorithm described above. Fig.3(d) represents the NLL values after learning has converged with a given bond dimension. For comparison, we also conduct training without branch-reconnections (i.e., sticking to the initial random tree throughout learning). A clear improvement achieved by the branch-reconnections can be observed in the NLL for training and testing data sets. The discrepancy between the training and testing data is the measure of the generalization performance (the smaller the better). The figure shows that the generalization performance of the generative model is at its maximum around the bond dimension 4. The bond dimension larger than five shows over-fitting.\nFig.3(e) shows the optimized network structure with a bond dimension of 5. The companies in S&P 500 are classified into 11 sectors according to their business areas. The color of each circle indicates the sector to which the corresponding company belongs. The color of an edge indicates the amplitude of the BMI; purple and blue indicate strong and weak BMI, respectively. Some features of the resulting tree are independent of the initial condition. For example, companies in the same sector tend to be close and form almost single-colored sub-trees. The sector information was not used in learning, while the clustering feature is solely the result of learning. The relative location of single-colored sub-trees depends on the initial condition and is not reproducible. Regardless of the initial condition, companies in the \"utility\" sector (black) always form a sub-tree. The edge connecting the utility sub-tree to the main tree is weaker than that of edges within the sector, indicating a weak correlation with other sectors. This likely reflects the nature of the utility sector, where the demand and supply dynamics of its products, such as electricity, gas, and water, operate independently of the activities of other sectors."}, {"title": "Discussion", "content": "The proposed method has shown satisfactory performance when applied to artificial and real data sets. The optimization is done in tensor elements and the tree structure is guided by BMI. Thus, the network structure after training reflects the information flow behind the given data set; even without prior knowledge, the method successfully identified concealed information paths and adjusted the network structure to match them. The method provides a new systematic approach to generative modeling, adapting to the information flow hidden behind the given data sets.\nIn the examples in the present study, we fixed the bond dimension of each edge in the tensor tree. We also defined the BMI by cutting the corresponding edge to measure the amount of information flow. The relation between the bond dimension and BMI is analogous to that between the flow capacity of a water pipe and actual flow through it. Therefore, we can adjust the bond dimension depending on BMI. For small BMI, the bond dimension can be reduced without harm. This modification could further compress the tensor tree generative models, which can be useful in future practical applications. As special cases, we may consider the Born machines using a tensor train and balanced tensor tree with automatically adjusted bond dimensions.\nWe let the tensor tree represent a wave function in the Born machine. The Born machine framework has the following advantages. First, in principle, it may be implemented as a real quantum circuit, qualitatively improving upon any classical generative modeling. Though we have not explored this possibility in the present article, this is one of the attractive directions of future research. Second, it guarantees the positivity of the weights, which is the basis of the proposed method. However, we may consider other alternatives, such as the steepest descent by differentiating with respect to the logarithm of tensor elements rather than the tensor elements themselves. In addition, improvement in the optimization details is a subject of future work."}, {"title": "Methods", "content": "Tree tensor network representation for generative modeling\nThe proposed approach is based on optimizing a general tree structure for generative modeling. In the Born machine framework, the model distribution $p_\\theta(x)$ is defined by the Born rule in (1) as the squared amplitude of the wave function of a Born machine. Then, we introduce a tensor tree to represent the amplitude of the wave function [9]. The tensor train and balanced tree used in [2] and [3], respectively, are special cases of the general tree structure. Based on a tree structure, we can efficiently calculate various exact contractions and marginal distributions using recursive procedures.\nWe can define a canonical form for a tensor tree [15] to determine the optimal truncation from local calculations. We first choose the root edge to construct the canonical form of a given tensor tree. Then, all the tensors become isometries by recursively applying singular value decomposition (SVD) starting from the leaves. Finally, we obtain the matrix A on the root edge. Using SVD, the root's position can be moved to any neighboring edge. Our method chooses the edge to be updated as the root. After modifying the local connection and updating the tensors around the root edge, we move the root position to the neighboring edge that has not been updated for the longest time."}, {"title": "Statistical estimation of mutual information", "content": "From the definition of the mutual information in (2), the mutual information for a bipartition into sub-systems A and B is rewritten as\n$I(A, B) = \\sum_{(a,b)} p(a, b) \\ln \\frac{p(a, b)}{p(a)p(b)},$ (3)\nwhere a and b are configurations on sub-systems A and B, respectively, $p(a,b) = p_\\theta(x = (a,b))$ is a joint distribution, and $p(a) = \\sum_b p(a,b)$ and $p(b) = \\sum_a p(a, b)$ are marginal distributions. We can efficiently compute the values of p(a) and p(b) utilizing a tensor tree representation of a joint distribution in the canonical form. However, estimating the mutual information in a tensor tree state is challenging due to the need to calculate the summation for all configurations (a,b). Moreover, the number of configurations grows exponentially with the system size.\nTo estimate the mutual information approximately, we replace the average over the joint distribution with that over the empirical data distribution as\n$I(A, B) \\approx I_{data}(A, B) = \\frac{1}{|M'|} \\sum_{(a,b) \\in M'} \\ln \\frac{p(a, b)}{p(a)p(b)},$ (4)\nwhere M' is the ensemble of data samples. The estimator (4) may be negative if the empirical data distribution $T_{emp}(a, b)$ differs from the joint distribution $p(a, b)$.\nAn alternative method involves estimating the average of $\\ln[p(a, b)/p(a)p(b)]$ using Monte Carlo sampling. If a Born machine has a tree structure, it can efficiently calculate the marginal distribution of a random variable given the other random variable states. The conditional distribution is then used to sample random variables perfectly. Repeating these steps can generate an independent sample (a, b) with probability p(a, b). Consequently, we can efficiently do Monte Carlo estimation of (3) as (4) with M', the ensemble of samples generated from p(a, b) directly. However, during learning, we only used the first method due to the low quality of the Born machine at the early stage."}, {"title": "Branch-reconnection algorithm", "content": "The detail of our branch-reconnection in Fig.1(b) is as follows:\n(i) Initialize the network and all tensor elements.\n(ii) Start from a randomly selected \"virtual\" bond. (Here, a virtual bond does not directly represent an original variable.)\n(iii) Contract the bond and obtain a 4-leg tensor. Improve the combined tensor to approximate the empirical distribution of the data better.\n(iv) Estimate BMI by sampling for each of the three ways of decomposition (Fig.1(a)) and choose the one with the smallest BMI. Before computing BMI for each decomposition, the component tensors are improved.\n(v) Replace the 4-leg tensor with the chosen decomposition.\n(vi) If the termination condition is satisfied, terminate.\n(vii) Move to a bond connected to one of the two new tensors and return to (iii).\nWe utilized a reconnection-based decomposition of a composite tensor to calculate the ground state of a quantum model with a tensor tree[16]. We improve the tensor in (iii) and (iv) by employing gradient-descent updates to minimize NLL."}, {"title": "Training details", "content": "In steps (iii) and (iv) in the branch-reconnection algorithm, we improve the combined tensor and new tensors in the decomposition by employing gradient-descent updates to minimize NLL [2, 3]. Assuming that the target distribution of generative modeling is an empirical data distribution, $T_{emp}$, NLL is defined as\n$\\mathcal{L} = \\frac{1}{M} \\sum_{x \\in M} \\ln[p_\\theta(x)] = H(T_{emp}) + D_{KL}(T_{emp}||p_\\theta),$ (5)\nwhere M is the ensemble of data samples and $D_{KL}$ is a Kullback-Leibler divergence. In this study, M is the same as M' in (4). We perform a single gradient-descent update in the procedure (iii) and ten ones in (iv), with learning rates of 0.05 for random patterns and 0.001 for others. For data samples M, we use the batch of training data samples for random patterns and S&P 500 stock price fluctuation patterns and a mini-batch of size 1000 for others. If the mini-batch size is small, instability occurs in estimating mutual information. We set the interval for changing the mini-batch to 1000 gradient-descent updates for the handwritten digits and updated the new mini-batch after all edges for the Bayesian networks. The NLL for test data is useful for checking the overfitting of learning data. For the Bayesian network data, we showed the optimized network with lower NLL for test data in Fig.3 because it fluctuates in the optimization process when changing a mini-batch.\nWe use the QMNIST dataset [10] for the images of handwritten digits. It is an improved version of the Mixed National Institute of Standards and Technology database (MNIST) dataset[17], a collection of handwriting digits from 0 to 9 on 28 \u00d7 28 pixels. In QMNIST, the number of test images increases to the number of learning images, 60000. The intensity of a pixel ranges from 0 to 255. As in [3], by padding zero intensity pixels around an original image, we embed an original image into 32 \u00d7 32 pixels. We make a binary bit from a pixel's intensity. If the intensity is larger than 127, the bit is set to one; otherwise, it is set to zero. The final image consists of $32^2$ bits.\nIn the Bayesian networks, each random variable is binary. We generate the target data according to the causal relation illustrated in Fig.3 (a-c). When there is a single cause, say A, we generate the result bit B with the conditional probability $P(B = A|A) = r$. In contrast, when there are multiple causes, we first calculate the intermediate bit using the exclusive-or of all the cause bits and generate the result bit with the same conditional probability. The present article's correlation rater is fixed to be 0.8. In all cases, we use a bond dimension of 4 for a tensor tree.\nThe pseudo-program is terminated at (vi) when the number of iterations reach a prefixed number, N. In the random patterns, handwritten digits, Bayesian networks, and S&P 500 examples, we choose $N = 2000, 10^6, 3000$ and $2 \\times 10^6$, respectively.\nThe initial random tree structure influences the results. Therefore, we perform five optimizations to assess the variance of NLLs in Fig.2(b) and (e), and Fig.3(d). We generated an initial random tree as follows:\n(i) Regard each random variable as a \"terminal tree\" consisting of only one node (leaf) with no edges. Let S be the set of all such single-node trees.\n(ii) Randomly take two elements from the list S and connect their roots to a new node to form a single bigger tree. Let the new node be the new tree's root, and put the new tree back into the list S.\n(iii) Until the list S contains only one element, repeat step (ii).\nThe final element in S is a tree with the random variables associated with its leaves."}]}