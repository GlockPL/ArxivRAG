{"title": "Foundations of Large Language Models", "authors": ["Tong Xiao", "Jingbo Zhu"], "abstract": "Large language models originated from natural language processing, but they have undoubtedly become one of the most revolutionary technological advancements in the field of artificial intelligence in recent years. An important insight brought by large language models is that knowledge of the world and languages can be acquired through large-scale language modeling tasks, and in this way, we can create a universal model that handles diverse problems. This discovery has profoundly impacted the research methodologies in natural language processing and many related disciplines. We have shifted from training specialized systems from scratch using a large amount of labeled data to a new paradigm of using large-scale pre-training to obtain foundation models, which are then fine-tuned, aligned, and prompted.", "sections": [{"title": "Preface", "content": "Large language models originated from natural language processing, but they have undoubtedly become one of the most revolutionary technological advancements in the field of artificial intelligence in recent years. An important insight brought by large language models is that knowledge of the world and languages can be acquired through large-scale language modeling tasks, and in this way, we can create a universal model that handles diverse problems. This discovery has profoundly impacted the research methodologies in natural language processing and many related disciplines. We have shifted from training specialized systems from scratch using a large amount of labeled data to a new paradigm of using large-scale pre-training to obtain foundation models, which are then fine-tuned, aligned, and prompted.\nThis book aims to outline the basic concepts of large language models and introduce the related techniques. As the title suggests, the book focuses more on the foundational aspects of large language models rather than providing comprehensive coverage of all cutting-edge methods. The book consists of four chapters:\n\u2022 Chapter 1 introduces the basics of pre-training. This is the foundation of large language models, and common pre-training methods and model architectures will be discussed here.\n\u2022 Chapter 2 introduces generative models, which are the large language models we commonly refer to today. After presenting the basic process of building these models, we will also explore how to scale up model training and handle long texts.\n\u2022 Chapter 3 introduces prompting methods for large language models. We will discuss various prompting strategies, along with more advanced methods such as chain-of-thought reasoning and automatic prompt design.\n\u2022 Chapter 4 introduces alignment methods for large language models. This chapter focuses on instruction fine-tuning and alignment based on human feedback.\nIf readers have some background in machine learning and natural language processing, along with a certain understanding of neural networks like Transformers, reading this book will be quite easy. However, even without this prior knowledge, it is still perfectly fine, as we have made the content of each chapter as self-contained as possible, ensuring that readers will not be burdened with too much reading difficulty.\nIn writing this book, we have gradually realized that it is more like a compilation of \"notes\" we have taken while learning about large language models. Through this note-taking writing style, we hope to offer readers a flexible learning path. Whether they wish to dive deep into a specific area or gain a comprehensive understanding of large language models, they will find the knowledge and insights they need within these \"notes\".\nWe would like to thank the students in our laboratory and all our friends who have shared with us their views on large language models and helped with corrections of errors in writing. In particular, we wish to thank Weiqiao Shan, Yongyu Mu, Chenglong Wang, Kaiyan Chang, Yuchun Fan, Hang Zhou, Xinyu Liu, Huiwen Bao, Tong Zheng, Junhao Ruan, and Qing Yang."}, {"title": "Notation", "content": "a variable\na row vector or matrix\nf(a) function of a\nmax f(a) maximum value of f (a)\narg maxa f (a) value of a that maximizes f(a)\nx input token sequence to a model\nxj input token at position j\ny output token sequence produced by a model\nyi output token at position i\n\u03b8 model parameters\nPr(a) probability of a\nPr(a|b) conditional probability of a given b\nPr(b) probability distribution of a variable given b\nPro(a) probability of a as parameterized by @\nht hidden state at time step t in sequential models\nH matrix of all hidden states over time in a sequence\nQ, K, V query, key, and value matrices in attention mechanisms\nSoftmax(A) Softmax function that normalizes the input vector or matrix A\nL loss function\nD dataset used for training or fine-tuning a model\ngradient of the loss function L with respect to the parameters \u03b8\nKL(p || q) KL divergence between distributions p and q"}, {"title": "Contents", "content": "1 Pre-training\n1.1 Pre-training NLP Models\n1.1.1 Unsupervised, Supervised and Self-supervised Pre-training\n1.1.2 Adapting Pre-trained Models\n1.2 Self-supervised Pre-training Tasks\n1.2.1 Decoder-only Pre-training\n1.2.2 Encoder-only Pre-training\n1.2.3 Encoder-Decoder Pre-training\n1.2.4 Comparison of Pre-training Tasks\n1.3 Example: BERT .\n1.3.1 The Standard Model\n1.3.2 More Training and Larger Models\n1.3.3 More Efficient Models\n1.3.4 Multi-lingual Models\n1.4 Applying BERT Models\n1.5 Summary\n2 Generative Models\n2.1 A Brief Introduction to LLMs\n2.1.1 Decoder-only Transformers\n2.1.2 Training LLMs\n2.1.3 Fine-tuning LLMs\n2.1.4 Aligning LLMs with the World\n2.1.5 Prompting LLMs\n2.2 Training at Scale\n2.2.1 Data Preparation\n2.2.2 Model Modifications\n2.2.3 Distributed Training\n2.2.4 Scaling Laws\n2.3 Long Sequence Modeling\n2.3.1 Optimization from HPC Perspectives\n2.3.2 Efficient Architectures\n2.3.3 Cache and Memory\n2.3.4 Sharing across Heads and Layers"}, {"title": "1 Pre-training", "content": "The development of neural sequence models, such as Transformers [Vaswani et al., 2017], along with the improvements in large-scale self-supervised learning, has opened the door to universal language understanding and generation. This achievement is largely motivated by pre-training: we separate common components from many neural network-based systems, and then train them on huge amounts of unlabeled data using self-supervision. These pre-trained models serve as foundation models that can be easily adapted to different tasks via fine-tuning or prompting. As a result, the paradigm of NLP has been enormously changed. In many cases, large-scale supervised learning for specific tasks is no longer required, and instead, we only need to adapt pre-trained foundation models.\nWhile pre-training has gained popularity in recent NLP research, this concept dates back decades to the early days of deep learning. For example, early attempts to pre-train deep learning systems include unsupervised learning for RNNs, deep feedforward networks, autoencoders, and others [Schmidhuber, 2015]. In the modern era of deep learning, we experienced a resurgence of pre-training, caused in part by the large-scale unsupervised learning of various word embedding models [Mikolov et al., 2013b; Pennington et al., 2014]. During the same period, pre-training also attracted significant interest in computer vision, where the backbone models were trained on relatively large labeled datasets such as ImageNet, and then applied to different downstream tasks [He et al., 2019; Zoph et al., 2020]. Large-scale research on pre-training in NLP began with the development of language models using self-supervised learning. This family of models covers several well-known examples like BERT [Devlin et al., 2019] and GPT [Brown et al., 2020], all with a similar idea that general language understanding and generation can be achieved by training the models to predict masked words in a huge amount of text. Despite the simple nature of this approach, the resulting models show remarkable capability in modeling linguistic structure, though they are not explicitly trained to achieve this. The generality of the pre-training tasks leads to systems that exhibit strong performance in a large variety of NLP problems, even outperforming previously well-developed supervised systems. More recently, pre-trained large language models have achieved a greater success, showing the exciting prospects for more general artificial intelligence [Bubeck et al., 2023].\nThis chapter discusses the concept of pre-training in the context of NLP. It begins with a general introduction to pre-training methods and their applications. BERT is then used as an example to illustrate how a sequence model is trained via a self-supervised task, called masked language modeling. This is followed by a discussion of methods for adapting pre-trained sequence models for various NLP tasks. Note that in this chapter, we will focus primarily on the pre-training paradigm in NLP, and therefore, we do not intend to cover details about generative large language models. A detailed discussion of these models will be left to subsequent chapters."}, {"title": "1.1 Pre-training NLP Models", "content": "The discussion of pre-training issues in NLP typically involves two types of problems: sequence modeling (or sequence encoding) and sequence generation. While these problems have different forms, for simplicity, we describe them using a single model defined as follows:\no = g(x0, x1, ..., xm;0) = g\u03b8(x0, x1, ..., xm) (1.1)\nwhere {x0, x1, ..., xm} denotes a sequence of input tokens, x0 denotes a special symbol ((s) or [CLS]) attached to the beginning of a sequence, g(\u00b7;\u03b8) (also written as g\u03b8(\u00b7)) denotes a neural network with parameters \u03b8, and o denotes the output of the neural network. Different problems can vary based on the form of the output o. For example, in token prediction problems (as in language modeling), o is a distribution over a vocabulary; in sequence encoding problems, o is a representation of the input sequence, often expressed as a real-valued vector sequence.\nThere are two fundamental issues here.\n\u2022 Optimizing @ on a pre-training task. Unlike standard learning problems in NLP, pre-training does not assume specific downstream tasks to which the model will be applied. Instead, the goal is to train a model that can generalize across various tasks.\n\u2022 Applying the pre-trained model g\u03b8(.) to downstream tasks. To adapt the model to these tasks, we need to adjust the parameters \u03b8 slightly using labeled data or prompt the model with task descriptions.\nIn this section, we discuss the basic ideas in addressing these issues."}, {"title": "1.1.1 Unsupervised, Supervised and Self-supervised Pre-training", "content": "In deep learning, pre-training refers to the process of optimizing a neural network before it is further trained/tuned and applied to the tasks of interest. This approach is based on an assumption that a model pre-trained on one task can be adapted to perform another task. As a result, we do not need to train a deep, complex neural network from scratch on tasks with limited labeled data. Instead, we can make use of tasks where supervision signals are easier to obtain. This reduces the reliance on task-specific labeled data, enabling the development of more general models that are not confined to particular problems.\nDuring the resurgence of neural networks through deep learning, many early attempts to achieve pre-training were focused on unsupervised learning. In these methods, the parameters of a neural network are optimized using a criterion that is not directly related to specific tasks. For example, we can minimize the reconstruction cross-entropy of the input vector for each layer [Bengio et al., 2006]. Unsupervised pre-training is commonly employed as a preliminary step before supervised learning, offering several advantages, such as aiding in the discovery of better local minima and adding a regularization effect to the training process [Erhan et al., 2010]. These benefits make the subsequent supervised learning phase easier and more stable.\nA second approach to pre-training is to pre-train a neural network on supervised learning tasks. For example, consider a sequence model designed to encode input sequences into some"}, {"title": "1.1.2 Adapting Pre-trained Models", "content": "As mentioned above, two major types of models are widely used in NLP pre-training.\n\u2022 Sequence Encoding Models. Given a sequence of words or tokens, a sequence encoding model represents this sequence as either a real-valued vector or a sequence of vectors, and obtains a representation of the sequence. This representation is typically used as input to another model, such as a sentence classification system."}, {"title": "1.1.2.1 Fine-tuning of Pre-trained Models", "content": "For sequence encoding pre-training, a common method of adapting pre-trained models is fine-tuning. Let Encode\u03b8(\u00b7) denote an encoder with parameters \u03b8, for example, Encode\u03b8() can be a standard Transformer encoder. Provided we have pre-trained this model in some way and obtained the optimal parameters \u03b8\u0302 , we can employ it to model any sequence and generate the corresponding representation, like this\nH = Encode\u03b8\u0302(x) (1.2)\nwhere x is the input sequence {x0, x1, ..., xm}, and H is the output representation which is a sequence of real-valued vectors {h0, h1, ..., hm}. Because the encoder does not work as a standalone NLP system, it is often integrated as a component into a bigger system. Consider, for example, a text classification problem in which we identify the polarity (i.e., positive, negative,"}, {"title": "1.1.2.2 Prompting of Pre-trained Models", "content": "Unlike sequence encoding models, sequence generation models are often employed independently to address language generation problems, such as question answering and machine translation, without the need for additional modules. It is therefore straightforward to fine-tune these models"}, {"title": "1.2 Self-supervised Pre-training Tasks", "content": "In this section, we consider self-supervised pre-training approaches for different neural architectures, including decoder-only, encoder-only, and encoder-decoder architectures. We restrict our discussion to Transformers since they form the basis of most pre-trained models in NLP. However, pre-training is a broad concept, and so we just give a brief introduction to basic approaches in order to make this section concise."}, {"title": "1.2.1 Decoder-only Pre-training", "content": "The decoder-only architecture has been widely used in developing language models [Radford et al., 2018]. For example, we can use a Transformer decoder as a language model by simply removing cross-attention sub-layers from it. Such a model predicts the distribution of tokens at a position given its preceding tokens, and the output is the token with the maximum probability. The standard way to train this model, as in the language modeling problem, is to minimize a loss function over a collection of token sequences. Let Decoder\u03b8(\u00b7) denote a decoder with parameters \u03b8. At each position i, the decoder generates a distribution of the next tokens based on its preceding tokens {x0, ..., xi}, denoted by Pro(\u00b7|xo, ..., xi) (or p i+1 for short). Suppose we have the gold-standard distribution at the same position, denoted by p gold . For language modeling, we can think of p gold as a one-hot representation of the correct predicted word. We then define a loss function L(p i+1 , p gold ) to measure the difference between the model prediction and the true prediction. In NLP, the log-scale cross-entropy loss is typically used.\nGiven a sequence of m tokens {x0, ..., xm}, the loss on this sequence is the sum of the loss over the positions {0, ..., m \u2212 1}, given by\nLoss\u03b8(x0, ..., xm) = m\u22121 \u2211L(p \u03b8i+1, pgoldi=0 = m\u22121 \u2211LogCrossEntropy (p \u03b8i+1, pgold)\ni=0 (1.5)\nwhere LogCrossEntropy(\u00b7) is the log-scale cross-entropy, and p gold is the one-hot representation of xi+1 .\nThis loss function can be extended to a set of sequences D. In this case, the objective of pre-training is to find the best parameters that minimize the loss on D\n\u03b8\u0302 = arg min \u2211Loss\u03b8(x) (1.6)\n\u03b8 x\u2208D\nNote that this objective is mathematically equivalent to maximum likelihood estimation, and can be re-expressed as\n\u03b8\u0302 = arg max \u2211log Pr\u03b8(x) (1.7)\n\u03b8 x\u2208D\n= arg max \u2211 m\u22121 log Pro(xi+1|x0, ..., xi)\n\u03b8 x\u2208D i=0\nWith these optimized parameters \u03b8\u0302 , we can use the pre-trained language model Decoder\u03b8\u0302(\u00b7) to compute the probability Pr\u03b8\u0302(xi+1|x0, ..., xi) at each position of a given sequence."}, {"title": "1.2.2 Encoder-only Pre-training", "content": "As defined in Section 1.1.2.1, an encoder Encode\u03b8(\u00b7) is a function that reads a sequence of tokens x = x0...xm and produces a sequence of vectors H = ho...hm . Training this model is not straightforward, as we do not have gold-standard data for measuring how good the output of the real-valued function is. A typical approach to encoder pre-training is to combine the encoder with some output layers to receive supervision signals that are easier to obtain. shows a common architecture for pre-training Transformer encoders, where we add a Softmax layer on top of the Transformer encoder. Clearly, this architecture is the same as that of the decoder-based language model, and the output is a sequence of probability distributions\np0W,\u03b8\n:\npmW,\u03b8\n= SoftmaxW(Encodex\u03b8(x)) (1.9)"}, {"title": "1.2.2.1 Masked Language Modeling", "content": "One of the most popular methods of encoder pre-training is masked language modeling, which forms the basis of the well-known BERT model [Devlin et al., 2019]. The idea of masked language modeling is to create prediction challenges by masking out some of the tokens in the input sequence and training a model to predict the masked tokens. In this sense, the conventional language modeling problem, which is sometimes called causal language modeling, is a special case of masked language modeling: at each position, we mask the tokens in the right-context, and predict the token at this position using its left context. However, in causal language modeling we only make use of the left-context in word prediction, while the prediction may depend on tokens in the right-context. By contrast, in masked language modeling, all the unmasked tokens are used for word prediction, leading to a bidirectional model that makes predictions based on both left and right-contexts."}, {"title": "1.2.2.2 Permuted Language Modeling", "content": "While masked language modeling is simple and widely applied, it introduces new issues. One drawback is the use of a special token, [MASK], which is employed only during training but not at test time. This leads to a discrepancy between training and inference. Moreover, the auto-encoding process overlooks the dependencies between masked tokens. For example, in the above example, the prediction of x2 (i.e., the first masked token) is made independently of x6 (i.e., the second masked token), though x6 should be considered in the context of x2.\nThese issues can be addressed using the permuted language modeling approach to pre-training [Yang et al., 2019]. Similar to causal language modeling, permuted language modeling involves making sequential predictions of tokens. However, unlike causal modeling where predictions follow the natural sequence of the text (like left-to-right or right-to-left), permuted language modeling allows for predictions in any order. The approach is straightforward: we determine an order for token predictions and then train the model in a standard language modeling manner, as described in Section 1.2.1. Note that in this approach, the actual order of tokens in the text remains unchanged, and only the order in which we predict these tokens differs from standard language modeling. For example, consider a sequence of 5 tokens x0x1x2x3x4. Let ei represent the embedding of xi (i.e., combination of the token embedding and positional embedding). In standard language modeling, we would generate this sequence in the order of x0 \u2192 x1 \u2192 x2 \u2192 x3 \u2192 x4. The probability of the sequence can be modeled via a generation process."}, {"title": "1.2.2.3 Pre-training Encoders as Classifiers", "content": "Another commonly-used idea to train an encoder is to consider classification tasks. In self-supervised learning, this is typically done by creating new classification challenges from the unlabeled text. There are many different ways to design the classification tasks. Here we present two popular tasks.\nA simple method, called next sentence prediction (NSP), is presented in BERT's original paper [Devlin et al., 2019]. The assumption of NSP is that a good text encoder should capture the relationship between two sentences. To model such a relationship, in NSP we can use the output of encoding two consecutive sentences Senta and Sentb to determine whether Sentb is the next sentence following Senta. For example, suppose Senta = 'It is raining.' and Sentb = 'I need an umbrella .'. The input sequence of the encoder could be\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\nwhere [CLS] is the start symbol (i.e., x0) which is commonly used in encoder pre-training, and [SEP] is a separator that separates the two sentences. The processing of this sequence follows a standard procedure of Transformer encoding: we first represent each token xi as its corresponding embedding ei, and then feed the embedding sequence {eo, ..., em} into the encoder to obtain the output sequence {ho, ..., hm}. Since ho is generally considered as the representation of the entire sequence, we add a Softmax layer on top of it to construct a binary classification system. This process is illustrated as follows\ntoken: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nembedding: eo e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11\nencoding: ho h1 h2 h3 h4 h5 h6 h7 h8 h9 h10 h11\nSoftmax Is Next or Not?"}, {"title": "1.2.3 Encoder-Decoder Pre-training", "content": "In NLP, encoder-decoder architectures are often used to model sequence-to-sequence problems, such as machine translation and question answering. In addition to these typical sequence-to-sequence problems in NLP, encoder-decoder models can be extended to deal with many other problems. A simple idea is to consider text as both the input and output of a problem, and so we can directly apply encoder-decoder models. For example, given a text, we can ask a model to output a text describing the sentiment of the input text, such as positive, negative, and neutral.\nSuch an idea allows us to develop a single text-to-text system to address any NLP problem. We can formulate different problems into the same text-to-text format. We first train an encoder-decoder model to gain general-purpose knowledge of language via self-supervision. This model is then fine-tuned for specific downstream tasks using targeted text-to-text data."}, {"title": "1.2.3.1 Masked Encoder-Decoder Pre-training", "content": "In Raffel et al. [2020]'s T5 model, many different tasks are framed as the same text-to-text task. Each sample in T5 follows the format\nSource Text \u2192 Target Text\nHere \u2192 separates the source text, which consists of a task description or instruction and the input given to the system, from the target text, which is the response to the input task. As an example, consider a task of translating from Chinese to English. A training sample can be expressed as\n[CLS] Translate from Chinese to English: \u4f60\u597d! \u2192 (s) Hello!\nwhere [CLS] and (s) are the start symbols on the source and target sides, respectively.\nLikewise, we can express other tasks in the same way. For example\n[CLS] Answer: when was Albert Einstein born? \u2192 (s) He was born on March 14, 1879.\n[CLS] Simplify: the professor, who has has published numerous papers in his field, will be giving a lecture on the topic next week. \u2192 (s) The experienced professor will give a lecture next week.\n[CLS] Score the translation from English to Chinese. English: when in Rome, do as the Romans do. Chinese: \u4eba\u5728\u7f57\u9a6c\u5c31\u50cf\u7f57\u9a6c\u4eba\u4e00\u6837\u505a\u4e8b\u3002\u2192 (s) 0.81\nwhere instructions are highlighted in gray. An interesting case is that in the last example we"}, {"title": "1.2.4 Comparison of Pre-training Tasks", "content": "So far, we have discussed a number of pre-training tasks. Since the same training objective can apply to different architectures (e.g., using masked language modeling for both encoder-only and encoder-decoder pre-training), categorizing pre-training tasks based solely on model architecture does not seem ideal. Instead, we summarize these tasks based on the training objectives.\n\u2022 Language Modeling. Typically, this approach refers to an auto-regressive generation procedure of sequences. At one time, it predicts the next token based on its previous context.\n\u2022 Masked Language Modeling. Masked Language Modeling belongs to a general mask-predict framework. It randomly masks tokens in a sequence and predicts these tokens using the entire masked sequence."}, {"title": "1.3 Example: BERT", "content": "In this section, we introduce BERT models, which are among the most popular and widely used pre-trained sequence encoding models in NLP."}, {"title": "1.3.1 The Standard Model", "content": "The standard BERT model, which is proposed in Devlin et al. [2019]'s work, is a Transformer encoder trained using both masked language modeling and next sentence prediction tasks. The loss used in training this model is a sum of the loss of the two tasks.\nLOSSBERT = LOSSMLM + LOSSNSP (1.17)\nAs is regular in training deep neural networks, we optimize the model parameters by minimizing this loss. To do this, a number of training samples are collected. During training, a batch of"}, {"title": "1.3.1.1 Loss Functions", "content": "In general, BERT models are used to represent a single sentence or a pair of sentences, and thus can handle various downstream language understanding problems. In this section we assume that the input representation is a sequence containing two sentences Senta and SentB, expressed as\n[CLS] Senta [SEP] SentB [SEP]\nHere we follow the notation in BERT's paper and use [SEP] to denote the separator.\nGiven this sequence, we can obtain LossMLM and LossNSP separately. For masked language modeling, we predict a subset of the tokens in the sequence. Typically, a certain percentage of the"}, {"title": "1.3.1.2 Model Setup", "content": "As shown in Figure 1.6, BERT models are based on the standard Transformer encoder architecture. The input is a sequence of embeddings, each being the sum of the token embedding, the positional embedding, and the segment embedding.\nei = x + epos + eseg (1.20)\nBoth the token embedding (x) and positional embedding (epos) are regular, as in Transformer models. The segment embedding (eseg) is a new type of embedding that indicates whether a token belongs to Senta or SentB. This can be illustrated by the following example.\nToken [CLS] It is raining . [SEP] I need an umbrella [SEP]\nepos PE(0) PE(1) PE(2) PE(3) PE(4) PE(5) PE(6) PE(7) PE(8) PE(9) PE(10) PE(11)\neseg eA eA eA eA eA eA eB eB eB eB eB eB\nThe main part of BERT models is a multi-layer Transformer network. A Transformer layer consists of a self-attention sub-layer and an FFN sub-layer. Both of them follow the post-norm architecture: output = LNorm(F(input) + input), where F(\u00b7) is the core function of the sub-layer (either a self-attention model or an FFN), and LNorm(\u00b7) is the layer normalization unit. Typically, a number of Transformer layers are stacked to form a deep network. At each position of the sequence, the output representation is a real-valued vector which is produced by the last layer of the network.\nThere are several aspects one may consider in developing BERT models.\n\u2022 Vocabulary Size (|V|). In Transformers, each input token is represented as an entry in a vocabulary V. Large vocabularies can cover more surface form variants of words, but may lead to increased storage requirements.\n\u2022 Embedding Size (de). Every token is represented as a de-dimensional real-valued vector. As presented above, this vector is the sum of the token embedding, positional embedding, and segment embedding, all of which are also de-dimensional real-valued vectors.\n\u2022 Hidden Size (d). The input and output of a sub-layer are of d dimensions. Besides, most of the hidden states of a sub-layer are d-dimensional vectors. In general, d can be roughly viewed as the width of the network.\n\u2022 Number of Heads (nhead). In self-attention sub-layers, one needs to specify the number of heads used in multi-head self-attention. The larger this number is, the more sub-spaces attention is performed. In practical systems, we often set nhead \u2265 4.\n\u2022 FFN Hidden Size (dffn). The size of the hidden layer of the FFNs used in Transformers is typically larger than d. For example, a typical setting is dffn = 4d. For larger Transformers, such as recent large models, dffn may be set to a very large value."}, {"title": "1.3.2 More Training and Larger Models", "content": "BERT is a milestone model in NLP, sparking many subsequent efforts to improve it. One direction is to scale up the model itself, including increasing training data and developing larger models.\nROBERTa, an extension of the standard BERT model, is an example of such efforts [Liu et al., 2019]. It introduces two major improvements. First, simply using more training data and more compute can improve BERT models without need of changing the model architectures. Second, removing the NSP loss does not decrease the performance on downstream tasks if the training is scaled up. These findings suggest exploring a general direction of pre-training: we can continue to improve pre-training by scaling it up on simple pre-training tasks.\nA second approach to improving BERT models is to increase the number of model parameters. For example, in He et al. [2021]'s work, a 1.5 billion-parameter BERT-like model is built by increasing both the model depth and hidden size. However, scaling up BERT and various other pre-trained models introduces new challenges in training, for example, training very large models often becomes unstable and difficult to converge. This makes the problem more complicated, and requires careful consideration of various aspects, including model architecture, parallel computation, parameter initialization, and so on. In another example, Shoeybi et al. [2019] successfully trained a 3.9 billion-parameter BERT-like model, where hundreds of GPUs were used to manage the increased computational demands."}, {"title": "1.3.3 More Efficient Models", "content": "Compared to its predecessors, BERT is a relatively large model for the time it was proposed. This increase in model size results in larger memory requirements and a consequent slowdown in system performance. Developing smaller and faster BERT models is part of the broader challenge of building efficient Transformers, which has been extensively discussed in Tay et al. [2020]'s work and Xiao and Zhu [2023]'s work. However, a deeper discussion of this general topic is beyond the scope of our current discussion. Here we instead consider a few efficient variants of BERT.\nSeveral threads of research are of interest to NLP researchers in developing efficient BERT models. First, work on knowledge distillation, such as training student models with the output of well-trained teacher models, shows that smaller BERT models can be obtained by transferring knowledge from larger BERT models. Given that BERT models are multi-layer networks with several different types of layers, knowledge distillation can be applied at different levels of representation. For example, beyond distilling knowledge from the output layers, it is also possible to incorporate training loss that measures the difference in output of hidden layers between teacher models and student models [Sun et al., 2020; Jiao et al., 2020]. Indeed, knowledge distillation has been one of the most widely-used techniques for learning small pre-trained models.\nSecond, conventional model compression methods can be directly applied to compress BERT models. One common approach is to use general-purpose pruning methods to prune the Transformer encoding networks [Gale et al., 2019]. This generally involves removing entire layers"}, {"title": "1.3.4 Multi-lingual Models", "content": "The initial BERT model was primarily focused on English. Soon after this model was proposed, it was extended to many languages. One simple way to do this is to develop a separate model for each language. Another approach, which has become more popular in recent work on large language models, is to train multi-lingual models directly on data from all the languages. In response, multi-lingual BERT (mBERT) models were developed by training them on text from 104 languages. The primary difference from monolingual BERT models is that mBERT models use larger vocabularies to cover tokens from multiple languages. As a result, the representations of tokens from different languages are mapped into the same space, allowing for the sharing of knowledge across languages via this universal representation model.\nOne important application of multi-lingual pre-trained models is cross-lingual learning. In the cross-lingual setting, we learn a model on tasks in one language, and apply it to the same tasks in another language. In cross-lingual text classification, for example, we fine-tune a multi-lingual pre-trained model on English annotated documents. Then, we use the fine-tuned model to classify Chinese documents.\nAn improvement to multi-lingual pre-trained models like mBERT is to introduce bilingual data into pre-training. Rather than training solely on monolingual data from multiple languages, bilingual training explicitly models the relationship between tokens in two languages. The resulting model will have innate cross-lingual transfer abilities, and thus can be easily adapted to different languages. Lample and Conneau [2019] propose an approach to pre-training cross-lingual language models (XLMs). In their work, a cross-lingual language model can be trained in either the causal language modeling or masked language modeling manner. For masked language modeling"}, {"title": "1.4 Applying BERT Models", "content": "Once a BERT model is pre-trained, it can then be used to solve NLP problems. But BERT models are not immediately ready for performing specific downstream tasks. In general, additional fine-tuning work is required to make them adapt. As a first step, we need a predictor to align the output of the model with the problem of interest. Let BERT\u03b8(\u00b7) be a BERT model with pre-trained parameters \u03b8, and Predict\u03c9(\u00b7) be a prediction network with parameters \u03c9. By integrating the prediction network with the output of the BERT model, we develop a model to tackle the downstream tasks. This model can be expressed as\ny = Predict\u03c9(BERT\u03b8(x)) (1.21)\nwhere x is the input and y is the output that fits the problem. For example, in"}]}