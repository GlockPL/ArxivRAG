{"title": "Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy", "authors": ["Zhenyu Guan", "Xiangyu Kong", "Fangwei Zhong", "Yizhou Wang"], "abstract": "Diplomacy is one of the most sophisticated activities in human society. The complex interactions among multiple parties/ agents involve various abilities like social reasoning, negotiation arts, and long-term strategy planning. Previous AI agents surely have proved their capability of handling multi-step games and larger action spaces on tasks involving multiple agents. However, diplomacy involves a staggering magnitude of decision spaces, especially considering the negotiation stage required. Recently, LLM agents have shown their potential for extending the boundary of previous agents on a couple of applications, however, it is still not enough to handle a very long planning period in a complex multi-agent environment. Empowered with cutting-edge LLM technology, we make the first stab to explore Al's upper bound towards a human-like agent for such a highly comprehensive multi-agent mission by combining three core and essential capabilities for stronger LLM-based societal agents: 1) strategic planner with memory and reflection; 2) goal-oriented negotiate with social reasoning; 3) augmenting memory by self-play games to self-evolving without any human in the loop.", "sections": [{"title": "1 Introduction", "content": "Diplomacy, as a cornerstone of international relations, is an intricate and multifaceted activity that lies at the heart of human society's most complex interactions. It encompasses a wide array of skills and strategies, including social reasoning, negotiation, and long-term planning, to navigate the intricate web of relationships and alliances between multiple parties. Mirroring this complexity, the board game Diplomacy[59] involves seven players to control European powers, presenting a complex strategic challenge that requires both sophisticated negotiation and strategic planning to triumph.\nThe AI community has shown an increasing interest in the deployment of AI agents to master such games [45, 26, 29, 15, 36, 28]. The recent breakthrough [6] has turned into press diplomacy, which allows communication between players. However, the previous methods [6] heavily rely on domain- specific human data, leading to its poor generalization to other scenarios/ applications. The question then arises: Can we build an AI agent that excels in the art of diplomacy without relying on domain-specific human data?\nRecently, agents based on the Large Language Model(LLM) have emerged as a promising devel- opment for AI agents. The previous applications on personal assistants [32], robotics [10, 64], and video games [48] have shown the surprising ability of LLM-based agents in communication and planning, benefiting from the emergent ability of common sense reasoning, in-context/ few-shot learning, and sophisticated natural language processing on LLMs. However, diplomacy presents a unique set of challenges. It not only requires planning long-horizon strategic [40] and communi- cating with natural language, but also reasoning and adopting the complex social dynamics with partial observations, including gaining trust and reputation, building rapport, detecting deception, and assessing the reliability of other players.\nIn this work, we aim to make the first attempt to explore LLMs' potential to develop a human-like AI diplomacy agent. We name the agent Richelieu in memorizing a pivotal figure in European history who had enduring impacts on French politics, foreign affairs, and state building. To achieve this goal, we have identified three core and essential capabilities that are crucial for building an LLM-based societal agent: 1) Social reasoning This is the basic function for a social agent to interact with others, particularly for adapting to the dynamic changes in the nation's intentions and relationships. 2) Balance long- and short-term planning Diplomacy often requires a delicate balance between short-term tactics and long-term strategies. An effective AI agent must be able to recognize and weigh the immediate consequences of its actions against the potential long-term outcomes. 3) Powerful memory A robust memory system is a critical component of learning and improvement. The AI agent must be able to recall and integrate information from past negotiations and actions to inform its current and future decision-making processes. This endows the agent with the ability to evolve. 4) Profound reflection An AI agent capable of profound reflection can analyze its own decisions, learn from its memory experience, and adapt its strategies accordingly. By integrating these three capabilities, the agent can operate at the highest level of diplomatic sophistication, outperforming the state-of-the-art AI diplomats [6].\nOur contributions can be summarized in three-fold: 1) We introduced a new paradigm for building AI diplomacy agents, compared to previous work (Fig. 1). The agent can self-evolve by generating experience via self-play games, without any task-specific human data. 2) We demonstrate the superior performance of our agent playing against the SOTA method, e.g., Cicero [6], that relies on a large- scale human demonstration for training. 3) We further analyze the effectiveness of each module in our agent and the generalization of our agent in adopting different LLMs, such as GPT4.0 and Llamma 3."}, {"title": "2 Related work", "content": "AI Diplomacy. The game involves seven players controlling different powers in Europe. In each turn, players can negotiate for cooperation before making moves to take as much supply centers as they can. Apparently, this challenging strategy task requires both complex negotiation skills and superior planning capability for player agents to achieve final victory. So far, most AI research on this task remain focused on the planning strategies (a.k.a. No-Press Diplomacy where no communication channels are allowed). The setting remains challenging considering its enormous action space of $10^{21}$ to $10^{64}$ per turn (compared with Chess, which has much fewer than 100 actions per turn). No wonder existing efforts rely on human data to play the game. Among the methods, one typical research is DipNet [39] which uses supervised and reinforcement learning. Based on DipNet, BRPI [3], SearchBot [18], DORA [5], and KL-Regularized search (Diplodocus) [24] were conducted. Until very recently, research has also emerged for the full-setting of Diplomacy, or Press Diplomacy where players are allowed to communicate with each before making their moves in each turn. Such studies [13][6][25][29] mainly benefit from the recent thriving language models."}, {"title": "3 Problem Statement", "content": "The Diplomacy game [59, 8] is set in pre-World War I Europe and involves each player (agent) representing one of the seven Great Powers of Europe, such as Germany, France, England, Italy, Austria-Hungary, Russia, and Turkey. Each player has a set of military units, including armies and fleets, which they can move and use to capture other supply centers. The ultimate goal for the agent is to control a majority of the total supply centers on the board by the end of the game's Fall phase. It's important to note that it is not won by eliminating other players or their units; it is won by controlling the requisite number of supply centers. This often involves forming and breaking alliances, negotiating, and sometimes betraying other players to achieve one's own goals.\nIn each turn, [58, 53] the agent $i$ gets the current state $s_t \\in S$, the actions of other players from the previous turn $\\bar{a}_{t-1}^i$, and the messages $\\bar{m}_{t-1}^i$ from other players during this turn's negotiations. The state $s_t$ for the environment includes the ownership of each territory on the map by a particular country and where the armies of each country are located. Based on this information, the agent needs to engage in negotiations with other players, sending messages $m_t^{i, -1}$ to chat with other players, and then take the actions $a_t^i$ in this turn. The possible actions an agent can take $a \\in A$ are commands to the armies, such as moving into an adjacent territory, supporting another unit, or holding a position. Actions can also include diplomatic moves, such as proposing or withdrawing from an alliance, although these are less formalized in the game mechanics.[39, 22]"}, {"title": "4 Self-Evolving LLM-based Diplomat", "content": "We have constructed a comprehensive framework with modules for memory management, social reasoning, strategic planning, negotiation, decision-making, memory update, and self-evolving to fully leverage the capabilities of LLMs. Richelieu starts by setting up with map details, game rules, domain knowledge, and the long-term goal.[74, 58, 53] At each turn, the agent will run in the following steps: 1) Social Reasoning: First of all, the agent undergoes a comprehensive analysis of the game state $s_t$ to build the social belief, including the intention of other players and their relationship $t \\in \\Phi_\\eta$.[71, 19] 2) Planner with Reflection: Then, the agent proposes sub-goals $X \\in X$ that is strategically aligned with the long-term goals $Y$, with the social belief and refining the proposed goal with experience $\\eta_\\tau \\in H_m$ abstract from the memory $M$ via self-reflection.[51, 57] 3) Negotiator: To achieve the sub-goals, the negotiator will start a dialogue session with some players, and evaluate their trueness $\\S$ by referring to their words $m_{t-1}^{i, -1}$, the current state $s_t$, their sincerity and the experience $\\xi$.[1, 7] 4) Actor: After negotiation, the actor decides its course of action $a_i$, based on the sub-goal $x_i$ and updated social state $s_{t+1}$, marking the end of that turn. 5)\nMemory Management: The state of the current turn $s_t$, the content of negotiations $m_t$, the actions taken by all players $a_t \\in A_n$, and the sub-goals set forth $x_t$ are all logged within the memory as $\\mu \\in M$. This logged data serves as a historical experience, guiding Richelieu's subsequent actions in future turns [20, 73]. 6) Self-evolution: The agent's evolution is highly dependent on the diversity of experiences stored in its memory. As this diversity grows, so does the agent's capability. Without human demonstrations, we employ multi-agent self-play games, i.e., our agents respectively control all the countries to simulate and acquire diverse experiences for self-evolving. Notably, the agent can further evolve during testing to adapt to different players."}, {"title": "4.1 Social Reasoning", "content": "There are no permanent enemies, no permanent allies. The relationship among countries is dynami- cally changing upon the evolving global state. However, it is difficult to determine the appropriate allies and enemies with partial observation. For example, there is uncertainty about the intentions of potential allies, which could lead to betrayal at pivotal moments. Consequently, we need to identify the intention and relationship of the current state by social reasoning to shape the social belief [71, 19].\n1) Modeling Relationship: Before setting sub-goals, Richelieu evaluates its relations with other players, identifying enemies such as aggressive nations, vulnerable neighbors for expansion, and those with long-term potential threats. It also seeks out potential allies to counter these threats. [46, 72] Simultaneously, Richelieu also tries to identify potential allies that could be instrumental in countering these adversaries. By isolating the analysis of inter-player relationships as a discrete element, Riche- lieu strategically exploits the actions of other players in subsequent stages of the game to reach its goals. 2) Inferring Intention: The social belief is also used by the planner, ensuring that Richelieu's sub-goals are formulated with a comprehensive consideration of the behaviors and intentions of other intelligent agents within the game.[14, 21] Richelieu's sub-goals will particularly emphasize on those who are identified as potential adversaries or allies, fostering more effective collaboration with potential allies and participation in strategic opposition against adversaries. Furthermore, the insights gleaned from this analysis are instrumental in the subsequent negotiation phases. They are employed to assess the authenticity of the statements made by other players, as well as to aid Richelieu in reaching cooperative agreements."}, {"title": "4.2 Strategic Planner with Reflection", "content": "The strategic planner specifies the sub-goals, which serves as an intermediary between immediate actions and the overarching goal of securing victory in the game. That is because we observe that LLMs are often characterized by their propensity to prioritize short-term gains in decision-making processes, with a notable deficiency in incorporating the future into their strategic calculations. [41, 70]For example, it is common for a non-neighboring country to become too powerful. Formally, $X_t \\leftarrow SR(s_t, \\Phi_t, Y)$ where $x_t = (x_1, x_2, \\dots, x_n)$ represents the proposed sub-goals and other players' intention that we inferred, $\\Phi \\in \\Phi^n$ represents the inferred relationship on the social belief. These goals may encompass a range of tactical considerations, such as the containment of a formidable rival's advancement or the strategic expansion in a particular direction to consolidate power.\nReflection with Memory. We further develop a reflection mechanism to enhance the rationality and effectiveness of our agent's sub-goals in achieving long-term goals.[33] This reflection mechanism relies on the past experiences to critique and enhance proposed sub-goals. We employ a similarity- based function to find relevant historical experiences that match the current game state from its memory. This function considers two factors: goal similarity and state similarity, to select the most comparable experiences. The process can be written as: $\\eta_\\tau \\leftarrow \\eta_\\tau(s_t, X_t, M)$, where $\\eta_\\tau \\in H_m$. In practice, considering the limited context windows of LLM, we retrieve the most analogous experiences from the memory based on these metrics. Experiences with high evaluative scores reinforce successful strategies and support the continuity of existing sub-goals. On the other hand, lower scores indicate areas that need improvement and prompt the necessary adjustments. As our agent, Richelieu, undergoes more training sessions, its reflection abilities improve. The growing pool of historical experiences consistently enhances its performance."}, {"title": "4.3 Negotiator and Actor", "content": "By chatting with other players, the goal of the negotiation is to update the social belief according to the received words and reach the sub-goal by manipulating other's intentions, such as securing cooperative agreements with other nations, terminating ongoing conflicts with a specific country, or deterring the formation of alliances directed against its interests.[37, 67] However, it is difficult to reach a consensus, as the interests and strategies of the various nations often conflict, and trust between players can be scarce, making it challenging to establish and maintain cooperative agreements. In this case, we argue that the negotiator should identify the true intentions and relationship of the opponent before generating the words for the negotiation.\nTo fully utilize the power of LLMs, we construct a social reasoning flow for negotiation, as shown in Figure 3. During the negotiation process, we guide Richelieu to consider the veracity of what other players said and their true intentions, and in conjunction with our established sub-goals and analysis of our relationships with other players, to negotiate and form alliances with potential allies and attempt to deceive enemies.[60, 35]\nTo counteract the challenge of non-binding agreements and potential deception, we incorporate a discrete module dedicated to the assessment of the veracity of statements made by other players during negotiations. To determine the truthiness of other players' statements $\\psi$, three main factors are considered. The most important is the consistency between the player's sub-goals $\\chi_i$ that our agent inferred before and the intentions conveyed through his statements $m_t^{i, -i}$. To aid in the judgment, our agent also goes through the memory to retrieve the consistent experiences $\\E_t$. Additionally, the player's overall honesty score $\\Upsilon_i$ is taken into account. Hence, we get the truthiness of the opponent $j$: $\\S = g(s_t, \\chi_i, m_t^{i, -i}, \\Phi_t, \\Upsilon_j, \\E_t)$, where $\\xi = \\omega(s_t, m_t^{i, -i}, M)$. With such a reasoning flow, our agent can adeptly navigate diplomatic discourse. After the negotiation, the actor will get the updated social beliefs and choose a specific action for the army."}, {"title": "4.4 Memory Management and Self-Evolution", "content": "This memory is the foundation of the framework that accumulates the historical experience of the agent and summarizes them for other modules.[17, 30, 66, 23] It supports other modules, such as planner and negotiator, to provide long-tail experiences.\nRaw Experience Management. Specifically, the memory module is tasked with the acquisition and archival of historical data, encompassing the observed game state $s_t$ at each turn, its sub-goals $\\chi_i$, the messages during the negotiation $m_t$, and the actions of all the players $a_t$. Subsequently, the raw experience is summarized in a shorter content with an evaluation $\\Alpha_t \\in A$ of the proposed sub-goals and an assessment of the credibility of other players $\\gamma_i \\in \\Gamma$. $\\Alpha_t$ serves to reflect upon the agent's sub-goals. It evaluates whether sub-goals are reasonable based on the subsequent state and long-term goals $Y$. As the game progresses, it is continuously updated in response to changes in the state $\\Alpha_t \\leftarrow f(\\chi_i, Y, s)$, where $s = (s_t, s_{t+1}, ..., s_T)$. The formula represents the update of the evaluation $\\Alpha_t$ for the sub-goal in turn $t$ by the memory in turn $T$. The updates will cease when there is a fundamental change in the sub-goal compared to the goal at turn $t$. This prevents subsequent decisions from impacting the assessment of the current decision-making. We employ $\\Upsilon_i \\in \\Gamma$ to evaluate the credibility of player $j$ and utilize $\\tau_j^t \\in \\{0, 1\\}$ to denote the truthfulness, i.e., whether the statements made by the player $j$ during the negotiation process at time $t$ are truthful. The truthiness of player $j$'s statements is updated according to the memory from the previous turns, $\\tau_j^t \\leftarrow \\tau(s_t, s_{t+1}, a_t^i, m_t^{i, -i})$. The credibility of player $j$ $\\gamma_j$ will be updated based on player $j$'s statements $\\tau_j$, written as $\\gamma_j \\leftarrow \\rho(\\psi_j, \\tau_j^{t-1})$. Players' credibility $\\gamma$ is a short-term memory that is applicable only to the current turn. Other data collected or generated constitutes long-term memory. These data will be combined to form a history $\\mu \\in M$, and then is incorporated into memory.\nAcquisition Experience via Self-Play Games. Self-play allows the agent to accumulate more experiences for self-evolution.[33, 69] After training, when Richelieu is faced with a certain state, it can draw on a larger pool of similar historical experiences. Diverse evaluations enable Richelieu to reflect more comprehensively on the strategies it currently devises, leading to a stronger optimization of decision making. As self-play continues, the acquisition of new and better historical experiences by Richelieu will diminish. This means that Richelieu's capabilities will not improve indefinitely. At the same time, as the memory grows, selecting appropriate historical experiences becomes a new challenge. The chosen $m$ experience may be almost identical, which could actually reduce the amount of useful information available to Richelieu. As shown in Figure 5, Richelieu's performance against Cicero [6] becomes better with increasing training iterations. With the accumulation of experiences, Richelieu's win rate exhibited a steady increase with accumulated training iterations, ultimately plateauing at a stable performance level. In contrast, the defeated rate showed a consistent decrease, approaching an asymptotic value. These observations confirm the effectiveness of self-play in Richelieu's evolution."}, {"title": "5 Experiment", "content": "In the experiments, our goal is to answer the following questions: 1) Mastery of Non-Press Diplomacy: Can our agent master the non-press diplomacy against baselines? 2) Competing with State-of-the-Art: Can our agent surpass the performance of the current state-of-the-art agents in press diplomacy? 3) Compatibility with LLMs: Can our self-evolving framework be compatible with different LLMs? 4) Contribution of Framework Modules: Do the individual modules within our framework contribute to the overall improvement of our agent's performance?\nEnvironment. The widely-used open source Diplomacy game platform introduced by [39] is adopted for evaluating Richelieu against other models. It is easy to switch between no-press (with communication/negotiation between players) and press (no communication between players) games based on this platform, facilitating comparison on both settings. The platform also contains over 10,000 human game data on which previous approaches are trained. Note that our method does not need them. In each game, a model will play the role of one randomly selected country to compete against countries controlled by other methods. It wins if occupying all the supply centers and loses vice versa.\nEvaluation Metrics. We evaluate the models based on the results of multiple rounds of games. In each round, the model is randomly assigned a country to control. Typically, 1000 rounds are played to obtain the average results. We evaluate the models in two metrics. One is based on the win rate, Most SC rate, survived rate, and defeated rate. There are four possible outcomes for each country in the game. If a country loses all its supply centers (SC), it is eliminated and recorded as \u201cdefeated\". If a country occupies 18 or more out of 34 supply centers, the game ends, and that country is recorded as \"win\", while other countries are recorded as \"defeated\". In other cases, the game ends in a draw. The country with the most supply centers is recorded as \"Most SC\", the countries that have been eliminated are recorded as \"defeated\", and the other countries are recorded as \"Survived\". The other is based on the scores obtained by the models after multiple rounds of competition. To compare the capabilities of multiple models, we use C-Diplo Argir[4], a scoring system. This system is used in many international diplomacy competitions. The scoring method is as follows: If a player wins by occupying 18 or more supply centers, the player scores 93 points, and each of the other six players scores 1 point. If the game ends in a draw, the player with the most centers scores 37 points. The second player with the most centers scores 14 points. The third player with the most centers scores 7 points. Each player scores 1 point per center owned. Each player also scores 1 point for participating. In this way, regardless of the game outcome, a total of 99 points will be distributed among the players in each game.\nBaselines. We select six previous models as baselines for comparison. Among them, Cicero[6] by Meta is a diplomacy model with a negotiation module. The other five are no-press diplomacy models, including the SL-DipNet and RL-DipNet [39], the BRPI [3], the SearchBot [18], and the DORA[5].\""}, {"title": "5.2 Results", "content": "Massively Play with Baselines on no-press setting. We let Richelieu compete with the other six models including Cicero[6], SL-DipNet and RL-DipNet [39], BRPI [3], SearchBot [18], and DORA[5] on No-Press Diplomacy, in which players make moves without communication. Figure 4 indicates that Richelieu outperforms other previous models relying on human game data. In contrast, Richelieu does not need such data but outperforms these methods by a clear margin, which demonstrates the outstanding planning capability of Richelieu.\nPlay against Cicero on press setting. We also evaluate Richelieu through competition against Cicero in the challenging scenario where negotiation is enabled. Specifically, we randomly assign three countries to one model and the remaining four to another. After playing several rounds of the game, the win rate, most SC rate, survived rate, and the defeated rate is calculated using a weighted average for evaluation. Table 1 demonstrates the competitive performance of Richelieu in comparison to Cicero. Richelieu's win rate is approximately 0.7% higher than Cicero's. If the Most SC rate is also taken into account, Richelieu is about 2% higher than Cicero. At the same time, Richelieu's loss rate is also 0.6% lower. According to our scoring system, Richelieu's score is about 10% higher than Cicero's. This is nontrivial especially when Richelieu is trained in a self-play game without humans and the opponents are trained with the data from human players.\nGeneralization of self-evolving framework to diverse LLMs. Llamma 3 To demonstrate the effectiveness of our framework in a variety of LLM, we conducted experiments using four models: GPT4.0, ERNIE Bot, Spark Desk, and Llamma 3. The experimental results show that, despite variations in Richelieu's performance due to the inherent differences in the capabilities of these LLMs, as illustrated in Figure 5, our framework and training approach significantly enhance the capabilities of all large language models. After training, the win rate using GPT4.0 increased from 1.5% lower than Cicero's to about 0.7% higher than Cicero's. The win rate using llama3 increased from 2.3% lower than Cicero's to almost equal to Cicero's. The win rates using Models Spark Desk and ERNIE Bot increased from 3% and 4% lower than Cicero's to 0.7% and 1.6% lower than Cicero's, respectively. This indicates the generalization of a self-evolving framework to various LLMs.\nAblation Study. We conduct comprehensive ab- lation studies on Richelieu by analyzing the ben- efit of incorporating Richelieu's various mod- ules, like planners or memory, into basic LLMs. The results are shown in Table 2. As can be seen, direct use of vanilla LLM yields relatively poor results. Richelieu's performance obtains steady and significant improvement by incorporating each individual module. This indicates that Richelieu is able to leverage other players' ac- tions during decision-making and consider both short-term and long-term benefits. Additionally, Richelieu's negotiation ability has been signif- icantly improved, allowing it to effectively ex- press intentions to cooperate with other players and avoid deception during negotiations. And after self-play, Richelieu's experience makes it perform better."}, {"title": "6 Example Cases", "content": "As is shown in Figure 6, Richelieu controls France. At this time, France is at war with Austria over control of the Apennine Peninsula. However, Russia is on the verge of victory in its war against Turkey, which will lead to significant territorial expansion for Russia. Although France and Russia currently do not share a border, are not at war, and have no conflicts of interest, Richelieu foresees Russia becoming the most threatening enemy in the future. Therefore, Richelieu sets a sub-goal of weakening Russia.\nIn the subsequent negotiation phase, Richelieu proactively proposes ending the war with Austria, despite holding an advantage in this conflict. Richelieu promises Austria that if it ceases hostilities and attacks Russia, Richelieu will assist Austria in defending against any attacks from England. The negotiations are successful. Austria accepted Richelieu's proposal, and the two countries reached an agreement to exchange the supply centers of Napoli and Munich.\nDuring the action phase, Austria moves its troops from Venice to Apulia in preparation for capturing Napoli in the next turn, while the rest of its forces are repositioned to the eastern regions bordering Russia to defend against Russian attacks and compete for supply centers. French units occupy Munich and prepare to advance on Russian territories such as Berlin. Meanwhile, French units support Austria in the Holland and Belgium regions.\nAs shown in Figure 7, Richelieu controls Germany. During the negotiation phase, England proposed a ceasefire to Germany and invited Germany to form an alliance to jointly attack France. England hoped to cease the war with Germany in Holland and Belgium. Subsequently, German units would support England in attacking Brest, and then England would utilize its fleets to assist Germany in attacking Spain and Portugal. Richelieu suspected that England was deceiving Germany, as England was likely to attack territories in the north such as Belgium and Berlin after German units were redirected to support Brest. Therefore, we pretended to accept England's alliance proposal during the negotiation process. However, at the same time, we sought out France and expressed our willingness to cease hostilities, allowing France to focus entirely on defending against England's attacks. In the action phase, England's actions confirmed Richelieu's suspicions. England attacked Belgium from Holland, but because Richelieu didn't move units in Belgium, England's attack failed."}, {"title": "7 Conclusion", "content": "In this paper, we introduce Richelieu, a self-evolving LLM-based agent for AI diplomacy. Our model enables hierarchical planning for multi-agent tasks and utilizes a memory module for reflective optimization. Our model does not require human data and can evolve through self-play. It ultimately outperforms existing models like Cicero in the Diplomacy. Our ablation study demonstrates the effectiveness of the modules we have established. By conducting experiments using different LLMs, we validate the generalization of our framework to various LLMs. We believe that the use of LLM-based agents will become an effective approach in social science in the future."}, {"title": "8 Limitations and Future Work", "content": "Our study is subject to certain limitations. We utilize diplomacy as the platform for constructing our model. However, the space of actions within diplomacy is constrained, whereas the decision-making space in real-world diplomacy is virtually boundless. In Diplomacy, apart from the negotiation information exchanged between players, all other information is public and certain. Conversely, real-world diplomacy operates within a framework of incomplete information.\nThe potential applications of such an AI agent are vast, ranging from simulated diplomatic environ- ments to real-world assistance and analysis. In future research, we intend to develop a more realistic game space, characterized by incomplete information and multi-player games, to enhance and refine our model further. We will also extend the framework to other multi-agent scenarios, including embodied interactions [75, 11, 9], sensor networks [54, 61, 38, 31], and video games [49, 34]. This framework can also be employed to develop various applications. For instance, in the fields of economics and finance, we intend to utilize it to create business analytics and negotiation models."}, {"title": "A Implementation Details", "content": "A.1 Rules of Diplomacy Game\n\u2022 You need to occupy as many supply centers as possible. If you occupy 18 or more supply centers, you will win the game directly. If you lose all your supply centers, you will be eliminated immediately.\n\u2022 The units consist of armies and fleets. Armies can only move to adjacent areas, while fleets can move to adjacent sea zones or coastal areas and can move along the coast.\n\u2022 To occupy a supply center, your units must move into that area in the autumn.\n\u2022 When a unit moves to an area, if another unit is in the destination or if other units are also moving to that destination, the move fails, resulting in a standoff. In such cases, you can seek support from units in adjacent areas to the destination. If another unit moves into the region from which support is coming, the support is cut off. The unit with the most support moves into the area, while other units must retreat to an adjacent province or disband. If there is no place to retreat, the unit must disband. Fleets can transport armies across sea zones from one coastal region to another. However, if another fleet moves into that sea zone, the transport is cut off.\n\u2022 The number of units a country can have cannot exceed the number of supply centers it controls. If the number of supply centers decreases, excess units must be disbanded. Each autumn, new units can be built at supply centers. Coastal supply centers can produce fleets or armies, while others can only produce armies. [22]\nA.2 Domain Knowledge\nRichelieu can adopt a strategy of allying with distant countries while attacking neighboring ones to occupy adjacent territories and achieve rapid expansion. Richelieu should pay attention to the Balance of Power by forming alliances with other countries or supporting weaker states to prevent any single country or alliance from becoming too powerful. [12] To this end, Richelieu can also adopt a strategy of attacking distant countries while allying with nearby ones, sacrificing short-term benefits to avoid the emergence of future hegemonic states that could threaten his own survival. When facing multiple enemies, Richelieu can find ways to divide other countries and incite wars among them. Whether in offense or defense, Richelieu should actively choose suitable allies. Richelieu can also introduce a third party to achieve goals such as ceasefire, alliance, or joint attack. To achieve alliances or ceasefires, Richelieu can sacrifice some interests to the other party as long as the ultimate benefits are greater. Others may lie and deceive [27]; their words in negotiations are not binding. Richelieu must avoid being deceived or betrayed. At the same time, Richelieu can also actively deceive others to achieve his own goals.[42, 2]\nA.3 Prompt Templates\nFor the convenience of reproducing the results of the experiments of this paper, here we give the prompt template of different modules of Richelieu.\n1) INIT\n1\n2\nYou will control {country} and compete with six other countries on the map for supply centers.\nThe map consists of different regions and sea areas. Their adjacency relationships are shown in the matrix. The numbers for the regions and sea areas are\n3\n4\nDifferent regions are occupied by different countries. The ownership of the regions is shown in the matrix.\nThe region Berlin,\nare supply centers.\n5\nYou need to follow these rules\n6\nTo help you achieve victory, these diplomatic strategies might be of assistance."}, {"title": "2) Social Reasoning", "content": "1\n2\nFrance occupies Portugal Ruhr, Paris, Burgundy,\nFrance has armies in Brest, Belgium, And France has fleets in Mid Atlantic, England Channel,\n3\nEngland\n4\n5\n6\n7\nBased on the current state, what do you think are the current strategic intentions of the other countries?\nWhich country do you think needs to be attacked or weakened the most right now?\nAnd which country do you think is most suitable for you to ally with in order to deal with this country?"}, {"title": "3) Planner with Reflection", "content": "1\n2\n3\nIn the current state, with {ally and enemy}, what sub-goal do you think should be set for {country} ?\nI have found some useful historical experiences for you. Please reflect on and optimize your sub-goal based on these historical experiences.\nThe sub-goal you formulated when {state} was to {sub-goal}. The eventual result was {future}. The evaluation for this sub-goal is {score}."}]}