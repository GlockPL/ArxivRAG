{"title": "Jet: A Modern Transformer-Based Normalizing Flow", "authors": ["Alexander Kolesnikov", "Andr\u00e9 Susano Pinto", "Michael Tschannen"], "abstract": "In the past, normalizing generative flows have emerged as a promising class of generative models for natural images. This type of model has many modeling advantages: the ability to efficiently compute log-likelihood of the input data, fast generation and simple overall structure. Normalizing flows remained a topic of active research but later fell out of favor, as visual quality of the samples was not competitive with other model classes, such as GANS, VQ-VAE-based approaches or diffusion models. In this paper we revisit the design of the coupling-based normalizing flow models by carefully ablating prior design choices and using computational blocks based on the Vision Transformer architecture, not convolutional neural networks. As a result, we achieve state-of-the-art quantitative and qualitative performance with a much simpler architecture. While the overall visual quality is still behind the current state-of-the-art models, we argue that strong normalizing flow models can help advancing research frontier by serving as building components of more powerful generative models.", "sections": [{"title": "Introduction", "content": "In this paper we explicitly do not attempt to devise the new state-of-the art image modeling approach or propose a new paradigm. Instead, we revisit the long known but recently neglected class of models for generative modeling: coupling-based normalizing flows. Normalizing flows have important capabilities that make them a useful tool for modern generative modeling.\nOn a high-level, a normalizing flow model learns a bijective (and thus invertible) mapping $g$ from the input space to the latent space, where the latent space follows a simple distribution, e.g. Gaussian distribution. A complex bijective transformation $g$ can be constructed by stacking multiple coupling blocks, which are bijective and invertible in closed form by design and are parametrized by deep neural networks.\nNormalizing flow models can be directly trained by computing data log-likelihood in the simple (e.g. Gaussian) latent space after the learnable and differentiable mapping $g$ is applied to transform training examples. For data generation, the inverse transformation $g^{-1}$ is readily available, which can be used to map easy-to-sample Gaussian latent space to the samples from the target distribution. The two explicit, differentiable and losseless mappings $g$ and $g^{-1}$ can be used as building blocks for more complex generative systems. For example, normalizing flows are used as a critical component of more complex systems: recent examples include (Chen et al., 2016; Kingma et al., 2016; Tschannen et al., 2024a) and (Tschannen et al., 2024b) that leverage normalizing flows to facilitate image modeling with VAEs and autoregressive transformers, respectively. This motivates us to revisit the normalizing flow model class.\nIn the mid-to-late 2010s normalizing flow models were a topic of active research. NICE (Dinh et al., 2014) was the early normalizing flow model for images. It introduced the main building block behind normalizing"}, {"title": "Method", "content": "In the section we introduce the Jet model. We first introduce the architecture, then describe training procedure and important implementation details."}, {"title": "Jet model", "content": "The Jet model has a very simple high-level structure. First, the input image is split into $K$ flat patches, flattened into a sequence of vectors, and then we repeatedly apply affine coupling layers (Dinh et al., 2017). We illustrate the Jet model architecture in Figure 1."}, {"title": "Training and optimization objective.", "content": "We now recall the fundamentals of the normalizing flow model training. The key assumption behind the normalizing flows is that the target density $p(x)$ can be modeled as a simple distribution, $p_z(z)$ (e.g. standard Gaussian) after using a bijective transformation $g: x \\rightarrow z$ to transform the original data. Applying the \"change of variable\" identity from the basic probability calculus we obtain a tractable model for the probability density function:\n$p(x) = p_z(g(x)) |det \\frac{\\partial g(x)}{\\partial x}|$"}, {"title": "Inverse transformation and image generation", "content": "It is easy to obtain the inverse of the above transformation in closed form. Notably, the ViT function that computes bias and scale terms does not need to be inverted:\n$x_1 = y_1$\n$x_2 = \\frac{y_2}{\\sigma(s(x_1)) \\cdot m} - b(x_1)$\nThe computational complexity of computing the inverse is exactly the same as computing the normalizing flow itself. New images can be sampled by first sampling the target density (i.e. Gaussian noise) and then applying the inverse transformation."}, {"title": "Initialization", "content": "Careful initialization is essential for training a deep normalizing flow model with a large number of coupling blocks. We employ a simple yet very effective initialization scheme. The final linear projection of the ViT $f$ is initialized with zero weights. As a result, predicted bias values, $b(x_1)$ are 0. The scale values are equal to $\\sigma(0) = 0.5$. When we set $m = 2$, then the all scaling factors become equal to 1.\nAs a result, the Jet model behaves as identity function at initialization. Empirically, we find this sufficient to ensure stable optimization in the beginning of training. Consequently, we do not need to add \"ActNorm\" layer that is commonly used in the normalizing flow literature to achieve a similar effect."}, {"title": "Dimension splitting", "content": "We explore various options for channel spitting. One option is to perform channel-wise split, by splitting the channels of each image patch into the two equal groups (Dinh et al., 2014; Kingma & Dhariwal, 2018). The splitting is random within each coupling layer and is fixed ahead of time (independently for each layer). This is a simple-to implement-strategy that ensures diverse channel mixing."}, {"title": "Experiments", "content": "Throughout the paper, we keep our experimental setup simple and unified. For the ViT architecture, we follow the original paper Dosovitskiy et al. (2021). Our ViT models inside the coupling layers do not have initial patchification or final pooling and linear projections. Unless stated otherwise, we set the patch size such that the total number of patches is equal to 256.\nFor the optimizer we use AdamW (Loshchilov et al., 2017). We set second momentum $\\beta_2$ parameter to 0.95 to stabilize training. We use a cosine learning rate decay schedule.\nDatasets. We perform experiments on three datasets: Imagenet-1k, Imagenet-21k and CIFAR-10, across two input resolutions: 32 \u00d7 32 and 64 \u00d7 64 (except for CIFAR-10). To downsample Imagenet-1k images we follow the standard protocol (Chrabaszcz et al., 2017) to ensure a correct comparison to the prior art (i.e. we use the preprocessed data provided by (Chrabaszcz et al., 2017) where available). To downsample Imagenet-21k images we use TensorFlow resize operation with method set to AREA. For CIFAR-10 we use the original dataset resolution. Importantly, to make sure our results are comparable to the literature, we do not perform any data augmentations."}, {"title": "Main results", "content": "We conduct extensive sweep that includes Jet models trained across varying computational capacity, data size (ImageNet-1k and ImageNet-21k) and resolutions (32\u00d732 and 64\u00d764). For the model capacity sweep we explore the following configurations (approximately spanning 2 orders of magnitude in compute intensity):"}, {"title": "Ablations", "content": "In this section we ablate key design choices for the Jet model. Our default ablation setting is moderately-sized Jet model, trained on ImageNet-1k 64 \u00d7 64 for 200 epochs. We set the total depth to 32 coupling layers, the ViT depth to 2 blocks and width to 512 dimensions. In our scaling study this configuration was reasonably close to the optimal setup, while being sufficiently fast for the extensive ablations sweeps. We use negative log-likelihood reported in bpd as the main ablation metric."}, {"title": "Coupling types", "content": "As presented in Section 2.5, we consider 4 different ways to split the channels in a coupling block: one is a channel-wise method (splits the channels into two parts) and three patch-wise methods (alternating rows, alternating columns or \"checkerboard\" splitting). Inspired by the prior literature, where channel-wise couplings were more prominent, we explore the following design space: M repeated channel-wise couplings followed by 1 spatial coupling. The spatial coupling is either fixed to one of the methods, or alternates between 3 types. We vary M from 0 to 5. Overall, the above choice space gives rise to 25 configurations."}, {"title": "Coupling layers vs ViT depth", "content": "Jet has two depth parameters: number of coupling layers and ViT depth within each coupling layer. The total compute is roughly proportional to the product of these two depths. We observe a very interesting interplay between these parameters, see Figure 2b representing ImageNet-21k 32 \u00d7 32, which is a detailed view of Figure 3.\nSpecifically, we observe that scaling the number of coupling layers, while keeping shallow ViT models (e.g. depth 1) results in an unfavorable compute-performance trade-off. It appears that ViT depth of at least 4-6 is the necessary condition for the Jet model to stay close to the frontier. For example, a model with 32 coupling layers and ViT depth 4 has roughly the same compute requirements as a model with 128 coupling layers and ViT depth 1. However, the former performs much better than the latter for fixed compute."}, {"title": "ViT vs CNNs", "content": "To ablate the use a ViT instead of a CNN block, we conduct a similar sweep to our main sweep on ImageNet-1k 64 x 64 but using a CNN architecture (specifically we use the CNN architrecture from (Kolesnikov et al., 2020)). This time sweeping the following settings for the CNN setup: model depth in {16,32,64}, CNN block depth in {1,...,8}, block embedding dimension in {256, 512, 768, 1024, 1536}. Block dimension 1536 was not used for model depth 64 due to significant memory costs. The results in Figure 2a show that the CNN-based variant lags significantly behind the ViT-based one.\nWe anticipate that gap can be reduced by using multiscale architectures, as commonly done in the literature (Dinh et al., 2017; Kingma & Dhariwal, 2018). However, in this paper we strive to simplify design and exclude multiscale architectures."}, {"title": "Coupling implementation", "content": "We investigate two common approaches for implementing coupling layers: masking or pairing. To be concrete, let's assume that we implement a coupling layer that splits the input spatially into two gropus of patches. In one approach, which we name \"masking\" mode, we feed the K patches to the ViT block but mask with zeros the ones corresponding to the $x_2$ group. At the output we use only the ones corresponding to the $x_2$ group and ignore the output of the patches corresponding to the $x_1$ group. One potential issue with this method is that it weakens the residual connections as the tokens from which we predict the output are tokens which observe zero as input.\nAnother approach we consider is a \"pairing\" mode in which we establish a pairing between input and output patches (or embeddings). For example when using a vertical-stripes pattern, the outputs of the ViT block for a patch in the $x_1$ group will predict the scale and bias for a patch in the $x_2$ group (e.g. to the patch below). This would make the ViT block processes only N/2 patches.\nWe experiment with these two implementation types while sweeping the M:1 channel:spatial coupling ratios as in Section 3.2.1. The results presented in Table 2a indicate pairing to be superior, though the impact becomes smaller as one increases the number of channel couplings which do not depend on this design decision. We observe that both methods perform very similarly, with the pairing being slightly ahead of masking, especially for the scenario when only spatial couplings are used. Thus, for the Jet model, we default to using pairing mode."}, {"title": "Invertible dense layers and activation normalization", "content": "Glow (Kingma & Dhariwal, 2018) introduces two components to improve the performance of normalizing flows: (1) a learnable, invertible dense layer which replaces the fixed permutation used to split the channels for each coupling; (2) an activation normalization layer with a scalar and bias parameters per channel similar to batch normalization."}, {"title": "Uniform dequantization vs dequantization flow", "content": "Flow++ (Ho et al., 2019) introduces a variational dequantization scheme to normalizing flows. Concretely, it proposes to replace the uniform dequantization noise added to the input with an image-conditional, learned noise distribution modeled by another normalizing flow. We ablate this component by training a 64 layer Jet model with 16-layer dequantization flow and compare it with an 80-layer base Jet model. The image conditioning of the dequantization flow was implemented by adding cross-attention layers to the ViT-blocks to the input. We observe no significant improvements when using the dequantization flow component."}, {"title": "Related work", "content": "NICE (Dinh et al., 2014) popularized coupling-based normalizing flows with the introduction of the additive coupling layer. RealNVP (Dinh et al., 2017) then increased the flow's expressivity by using affine coupling layers in combination with a multiscale architecture, and (Kingma & Dhariwal, 2018; Hoogeboom et al., 2019; Sukthanker et al., 2022) proposed additional specialized invertible layers for image modeling. Flow++ (Ho et al., 2019) demonstrated improvements from learning the dequantization noise distribution along with the flow model.\nAnother class of likelihood-based generative models are autoregressive models which flatten the (sub)pixels of an image into a sequence. Autoregressive modeling is enabled by using CNNs (Van den Oord et al., 2016b;a; Salimans et al., 2016) or transformers (Parmar et al., 2018; Chen et al., 2020). Kolesnikov & Lampert (2017); Menick & Kalchbrenner (2019) improved performance of autoregressive models with hierarchical modeling (e.g. over color depth or resolution). While obtaining better results than normalizing flows, autoregressive models are also much slower and do not scale to large resolutions as they require a forward-pass per (sub)pixel.\nIn the context of normalizing flows, autoregressive dependency patterns between latent variables are a popular approach to improve modeling capabilities of normalizing flows (Kingma et al., 2016; Papamakarios et al., 2017; Huang et al., 2018). Bhattacharyya et al. (2020) combined autoregressive modeling with a multiscale architecture. Concurrently to this work, Zhai et al. (2024) proposed a combination of the transformer-based autoregressive flow."}, {"title": "Conclusion", "content": "The Jet model revisits normalizing flows with a focus on simplicity and performance. While eliminating complex components such as multiscale architectures and invertible layers, Jet achieves state-of-the-art results across benchmarks while maintaining a straightforward design.\nWe see normalizing flows, and Jet in particular, as a useful tool for advancing generative modeling. Due to its simple structure and lossless guarantees, it can serve as a building block for powerful generative systems. One recent example is (Tschannen et al., 2024b), which leverages a normalizing flow to enable end-to-end autoregressive modeling of raw high-resolution images. We anticipate more progress in this area and believe that the Jet model will prove itself a powerful normalizing flow component that can be used out-of-the-box for a variety of applications."}]}