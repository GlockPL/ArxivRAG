{"title": "Inconsistencies In Consistency Models: Better ODE Solving Does Not Imply Better Samples", "authors": ["No\u00ebl Vouitsis", "Rasa Hosseinzadeh", "Brendan Leigh Ross", "Valentin Villecroze", "Satya Krishna Gorti", "Jesse C. Cresswell", "Gabriel Loaiza-Ganem"], "abstract": "Although diffusion models can generate remarkably high-quality samples, they are intrinsically bottlenecked by their expensive iterative sampling procedure. Consistency models (CMs) have recently emerged as a promising diffusion model distillation method, reducing the cost of sampling by generating high-fidelity samples in just a few iterations. Consistency model distillation aims to solve the probability flow ordinary differential equation (ODE) defined by an existing diffusion model. CMs are not directly trained to minimize error against an ODE solver, rather they use a more computationally tractable objective. As a way to study how effectively CMs solve the probability flow ODE, and the effect that any induced error has on the quality of generated samples, we introduce Direct CMs, which directly minimize this error. Intriguingly, we find that Direct CMs reduce the ODE solving error compared to CMs but also result in significantly worse sample quality, calling into question why exactly CMs work well in the first place. Full code is available at: https://github.com/layer6ai-labs/direct-cms.", "sections": [{"title": "1 Introduction", "content": "In recent years, diffusion models (DMs) [44, 14] have become the de facto standard generative models [22] for many perceptual data modalities such as images [34, 40, 36, 5], video [15, 2, 53, 52], and audio [20, 39, 17]. Despite their successes, an inherent drawback of diffusion models stems from their iterative sampling procedure, whereby hundreds or thousands of function calls to the diffusion model are typically required to generate high-quality samples, limiting their practicality in low-latency settings. A prominent approach for improving the sampling efficiency of diffusion models is to subsequently distill them into models capable of few-step generation [24, 41, 31, 27, 1, 9, 56, 55, 42]. Among the works in this vein, consistency models (CMs) [49] have garnered attention due to their simple premise as well as their ability to successfully generate samples with only a few steps. CMs leverage the ordinary differential equation (ODE) formulation of diffusion models, called the probability flow (PF) ODE, that defines a deterministic mapping between noise and data [48]. The goal of consistency model distillation is to train a model (the student) to solve the PF ODE of an existing diffusion model (the teacher) from all points along any ODE trajectory in a single step. The loss proposed by Song et al. [49] to train CMs does not directly minimize the error against an ODE solver; the solver is mimicked only at optimality and under the assumptions of arbitrarily flexible networks and perfect optimization. We thus hypothesize that the error against the ODE solver can be further driven down by directly solving the PF ODE at each step using strong supervision from the teacher, which we call a direct consistency model (Direct CM). Although Direct CMs are more expensive to train than standard CMs, they provide a relevant tool to probe how well CMs solve the PF ODE and how deviations from an ODE solver affect sample quality. We perform controlled experiments to compare CMs and Direct CMs using a state-of-the-art and large-scale diffusion model from the Stable Diffusion family [36], SDXL [30], as the teacher model for distillation. We show that Direct CMs perform better at solving the PF ODE but, surprisingly, that they translate to noticeably worse sample quality. This unexpected result challenges the conception that better ODE solving necessarily implies better sample quality, a notion that is implicitly assumed by CMs and its variations alike [47, 7, 10, 57, 21]. Our findings serve as a counterexample to this statement, thus calling into question the community's understanding of ODE-based diffusion model distillation and its implications on sample quality. Since CMs achieve larger ODE solving error, we surmise that other confounding factors contribute to their improved sample quality. We thus call for additional investigation to clarify this seemingly paradoxical behaviour of ODE-based diffusion model distillation."}, {"title": "2 Background and Related Work", "content": "Diffusion Models The overarching objective of diffusion models is to learn to reverse a noising process that iteratively transforms data into noise. In the limit of infinite noising steps, this iterative process can be formalized as a stochastic different equation (SDE), called the forward SDE. The goal of diffusion models amounts to reversing the forward SDE, hence mapping noise to data [48].\nFormally, denoting the data distribution as p0, the forward SDE is given by\n$dxt = \\mu(x, t)dt + \\sigma(t)dWt, \\quad Xo \\sim Po,$\nwhere t \u2208 [0, T] for some fixed T, \u03bc and \u03c3 are hyperparameters, and Wt denotes a multivariate Brownian motion. We denote the implied marginal distribution of xt as pt; the intuition here is that, with correct choice of hyperparameters, pr is almost pure noise. Song et al. [48] showed that the following ODE, referred to as the probability flow (PF) ODE, shares the same marginals as the forward SDE,\n$dxt = (\\mu(x, t) - \\frac{\\sigma^2(t)}{2} \\nabla_{\\mathbf{x}} \\log p_t(\\mathbf{x})) \\, dt,$\nwhere $ \\nabla_{\\mathbf{x}} \\log p_t $ is the (Stein) score function. In other words, if the PF ODE is started at x0 ~ po, then xt ~ pt. Under standard regularity conditions, for any initial condition x0 this ODE admits a unique trajectory (xt)t\u2208[0,T] as a solution. Thus, any point x\u0165 uniquely determines the entire trajectory, meaning that Equation 2 implicitly defines a deterministic mapping f* : (xt,t,t') \u2192 xt, which can be computed by solving Equation 2 backward through time whenever t > t'. In principle this function can be used to sample from po, since f*(xT, T, 0) will be distributed according to po if xT ~ pr. In practice this cannot be done exactly, and three approximations are performed. First, the score function is unknown, and diffusion models train a neural network s(xt, t) to approximate it, i.e., s(xt, t) \u2248 \u2207 log pt (xt). This approximation results in the new PF ODE, sometimes called the empirical PF ODE,\n$dxt = (\\mu(x, t) - \\frac{\\sigma^2(t)}{2} s(x,t)) \\, dt,$\nwhose solution function we denote as fs. Second, computing f(xT, T, 0) still requires solving an ODE, meaning that a numerical solver must be used to approximate it. We denote the solution of a numerical ODE solver as fsolver, and a single step of the solver from time t to time t' as \u03a6(., t, t'). More formally, discretizing the interval [0, T] as 0 = to < \u2026 < tn = T, we have that whenever n > m, fsolver(Xtn,tn, tm) is defined recursively as fsolver (\u2717tn,tn,tm) = xtm where Xti-1 = \u03a6(xti, ti, ti\u22121) for i = n, n \u2212 1, ..., m+1 with xt = xtn. Lastly, pr is also unknown, but since it is very close to pure noise, it can be approximated with an appropriate Gaussian distribution \u0440\u0442.\nIn summary, by leveraging the empirical PF ODE, samples from a diffusion model can be obtained as fsolver (XT, T, 0), where x\u012b ~ pr. If the approximations made throughout are accurate, then fsolver \u2248 fs \u2248 f* and pr\u2248 pr, so that samples from the model resemble samples from po [4]. Despite their ability to generate high-quality samples, an inherent drawback of DMs is rooted in their sampling procedure, since computing I requires a function call to s; the iterative refinement of denoised samples to generate high-quality solution trajectories is computationally intensive."}, {"title": "Consistency Models", "content": "Consistency models [49] leverage the PF ODE formulation of DMs to enable few-step generation. They can be used either for DM distillation or trained standalone from scratch; we only consider distillation in our work since the score function of pre-trained DMs gives us a tool to directly study the effect of ODE solving on CMs. Given a trained diffusion model s with a corresponding fsolver, the idea of consistency model distillation is to train a neural network fe such that fo(xtn, tn) \u2248 fsolver (Xtn, tn, 0) for all n \u2208 {1, . . ., N }. In other words, CMs aim to learn a function to mimic the solver of the empirical PF ODE, thus circumventing the need to repeatedly evaluate s during sampling. CMs learn fe by enforcing the self-consistency property, meaning that for every xt and xt, along the same trajectory, fe(xtn,tn) and fe(xt,, tn') are encouraged to match. More specifically, CMs are trained by minimizing the consistency distillation loss,\n$LCD := \\mathbb{E}_{x_0\\sim p_0, n \\sim U[1,N], x_{t_n} \\sim P_{t_n|0}(\\cdot|x_0)} \\, [\\lambda(t_n) d(f_{\\theta}(x_{t_n}, t_n), f_{\\bar{\\theta}}(x_{t_{n-1}}, t_{n-1}))],$\nwhere ptio is the transition kernel corresponding to Equation 1, x > 0 is a weighting function treated as a hyperparameter, d is any distance, \u0113 is a frozen version of 0, and Xtn\u22121 = \u03a6(xtn,tn,tn\u22121). Since the transition kernel is given by a known Gaussian, the above objective is tractable. CMs parameterize fe in such a way that fo (x0, 0) = x0 holds. This property is referred to as the boundary condition, and prevents Equation 4 from being pathologically minimized by fe collapsing onto a constant function.\nDuring sampling, CMs can use one or multiple function evaluations of fe, enabling a trade-off between computational cost and sample quality. For example, if given a budget of two function evaluations, rather than produce a sample as fe(xT,T), one could run Equation 1 until some time tn' starting from fe(xT, T) to produce xt\u201e,, and then output fo(xt,, tn') as the sample. This idea generalizes to more function evaluations, although note that fo(xt,, tn') and xt, do not belong to the same ODE trajectory as fe (xT, T) and xy due to the added noise from the forward SDE."}, {"title": "3 Direct Consistency Models", "content": "In Equation 4, x0 and xt do not belong to the same ODE trajectory since noise is added to obtain xt from xo via the forward SDE's transition kernel. Thus, it would not make sense to enforce consistency by minimizing d(fe(xtn, tn), xo), and Equation 4 is used instead. While Song et al. [49] theoretically showed that perfectly minimizing Equation 4 with an arbitrarily flexible fo results in fo(xtn,tn) = fsolver (Xtn,tn, 0), in practice it has been observed that CMs can be difficult to optimize, with slow convergence or, in some cases, divergence [10, 7]. We attribute this behaviour to what we call \u201cweak supervision\" in the CM loss, namely that fe is not directly trained to map xt to the origin of its ODE trajectory. The constraint that the CM should map any point on the ODE trajectory to the trajectory's origin is only weakly enforced through the boundary condition parameterization of fe. Only at time t\u2081 does the objective directly encourage mapping points xt\u2081 to the trajectory's origin. The network fe must therefore first learn to map slightly noised data back to the origin before that constraint can be properly enforced for noisier inputs at larger timesteps. We depict this behaviour in Figure 1 (left).\nIn order to assess the impact of ODE solving on CMs, we put forth a more intuitive and interpretable variation of its loss as\n$LDirect := \\mathbb{E}_{x_0\\sim p_0, n \\sim U[1,N], x_{t_n} \\sim P_{t_n|0}(\\cdot|x_0)} [\\lambda(t_n) d(f_{\\theta}(x_{t_n}, t_n), f_{solver}(x_{t_n}, t_n, 0))],$\nwhere we directly enforce that all points along a trajectory map to its origin, rather than providing only weak supervision as in CMs; see Figure 1 (right). We see this loss as the smallest possible modification to CMs resulting in the direct matching of the model and the solver. Note that unlike standard CMs, Direct CMs do not require enforcing the boundary condition in the parameterization of fe to prevent collapse, although it is of course still valid to do so. While this loss requires solving the ODE for n steps at each iteration and is therefore more computationally expensive than Equation 4, we only propose this formulation for comparative purposes rather than suggesting its use in practice.\nAs we will show, Equation 5 does indeed solve the empirical PF ODE better than Equation 4 but, intriguingly, it translates to worse sample quality. We define the ODE solving error E as the expected distance between the ODE solver's solution and the CM's prediction with the same initial noise, i.e.,\n$\\mathcal{E} := \\mathbb{E}_{x_T \\sim p_T} [d(f_{\\theta}(x_T, T), f_{solver}(x_T, T, 0))].$"}, {"title": "4 Experiments", "content": "Training For all of our experiments, we aim to compare CMs and Direct CMs using large-scale and state-of-the-art DMs trained on Internet-scale data to better reflect the performance of these models in practical real-world settings. Hence, we select SDXL [30] as the DM to distill, a text-to-image latent diffusion model [36] with a 2.6 B parameter U-Net backbone [37], capable of generating images at a 1024 px resolution. Classifier-free guidance [13] is commonly used to improve sample quality in text-conditional DMs, so we augment s in Equation 3 as s(xt, t, c, w), where c is the text prompt and w is the guidance scale, following Luo et al. [25]. When distilling a DM, it is common to initialize the student network from the weights of the teacher network so that, in effect, distillation is reduced to a fine-tuning task which requires much less data and resources. We further leverage modern best practices for efficient fine-tuning using low-rank adapters [16, 26]. We use a high-quality subset of the LAION-5B dataset [43] called LAION-Aesthetics-6.5+ for training similar to Luo et al. [25]. To ensure a controlled comparison of CMs and Direct CMs, the only component in the code that we modify is the loss. See Appendix A.1 for a list of training hyperparameters.\nEvaluation We perform quantitative comparisons using metrics that measure ODE solving quality as well as image quality. For ODE solving, we use E (Equation 6, lower is better) which is only valid for single-step generation.\u00b9 For image metrics, we use Fr\u00e9chet Distance on Inception (FID [12],"}, {"title": "5 Conclusions and Future Work", "content": "Although consistency models have achieved success in distilling diffusion models into few-step generators, we find that there exists a gap between their theory and practice. Solving the PF ODE is central to the theoretical motivation of CMs, but we show that we can solve the same PF ODE more accurately using Direct CMs while generating samples of noticeably worse quality. Naturally, we question what additional underlying factors might be contributing to the effectiveness of CMs, and call for additional research from the community to bridge this observed gap between solving the PF ODE and generating high-quality samples. We finish by putting forth some potential explanations: (i) since our experiments are carried out with latent diffusion models, the ODEs are defined on the corresponding latent space, and it could be that the closeness to the solver's solutions observed in Direct CMs is undone after decoding to pixel space; (ii) if the pre-trained diffusion model failed to closely approximate the true score function (as could be the case when the true score function is unbounded [29, 23, 22]) then f, & f*, meaning that even if a model closely approximates fsolver and thus fs, it need not be the case that it also properly approximates f*; and (iii) although both the CM and Direct CM objectives (Equation 4 and Equation 5, respectively) are meant to mimic the solver fsolver at optimality, in practice this optimum is never perfectly achieved, and the CM objective might inadvertently provide a beneficial inductive bias which improves sample quality."}, {"title": "A Appendix", "content": "A.1 Hyperparameters\nWe provide a list of default hyperparameter values in Table 2. We only train a small number of LoRA blocks [16] following Luo et al. [26], and find that metric and loss curves stabilized around 250 training steps. To enforce the boundary condition in CMs, we follow Song et al. [49] and parameterize fo(xtn,tn) = Cskip(tn)Xtn + Cout(tn)Fe(xtn,tn) where Fe(xtn, tn) in our case is the SDXL U-Net backbone with learnable LoRA blocks, and Cskip(tn) and Cout(tn) are differentiable functions such that Cskip(0) = 1 and Cout(0) = 0. We set the values of Cskip(tn) and Cout(tn) following Luo et al. [25] (see Table 2), and note that this choice is roughly equivalent to a step function where, for n \u2265 1, Cskip (tn) \u2248 0 and Cout(tn) \u2248 1. As mentioned in the main text, Direct CMs do not require enforcing a boundary condition by construction, but we parameterize them identically to CMs in order to ensure controlled experiments so that we can attribute any differences between them solely to differences in the loss. All experiments were performed on a single 48GB NVIDIA RTX 6000 Ada GPU.\nA.2 Additional Quantitative Results\nWe provide additional quantitative image analysis for two- and four-step generation in Table 3. In almost all cases, these results demonstrate that CMs generate higher quality images than Direct CMs akin to the single-step generation case. Although these results suggest that for four steps Direct CMs slightly outperform CMs in terms of FD-DINO and CLIP score, qualitative comparisons of generated images between both models (see examples in Figure 2 and Figure 5) quickly reveal that images from CMs have noticeably higher quality. We thus attribute the discrepancy either to imperfections in generative model evaluation metrics as observed by Stein et al. [50], or to these metrics not perfectly matching aesthetic quality and being affected by additional confounders (e.g., FD-DINO scores are meant to reflect image diversity in addition to image aesthetics).\nA.3 Additional Qualitative Results"}]}