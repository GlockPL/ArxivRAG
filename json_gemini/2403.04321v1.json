{"title": "Discriminative Probing and Tuning for Text-to-Image Generation", "authors": ["Leigang Qu", "Wenjie Wang", "Yongqi Li", "Hanwang Zhang", "Liqiang Nie", "Tat-Seng Chua"], "abstract": "Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a bonus of the discriminative adapter, a self-correction mechanism can leverage discriminative gradients to better align generated images to text prompts during inference. Comprehensive evaluations across three benchmark datasets, including both in-distribution and out-of-distribution scenarios, demonstrate our method's superior generation performance. Meanwhile, it achieves state-of-the-art discriminative performance on the two discriminative tasks compared to other generative models.", "sections": [{"title": "1. Introduction", "content": "Text-to-image generation (T2I) aims to synthesize high-quality and semantically-relevant images to a given free-form text prompt. In recent years, the rapid development of diffusion models [23, 54] has ignited the research enthusiasm for content generation, leading to a significant leap in T2I [47, 50, 52]. However, due to the weak compositional reasoning capabilities, current T2I models still suffer from the Text-Image Misalignment problem [30], such as attribute binding [16], counting error [44], and relation confusion [44] (see Fig. 1), especially in complicated multi-object generation scenes.\nTwo lines of work have made remarkable progress in improving text-image alignment for T2I models. The first line proposes to intervene in cross-modal attention activations guided by linguistic structures [16] or test time optimization [4]. However, they heavily rely on the inductive bias for manipulating attention structures, often necessitating expertise in vision-language interaction. This expertise is not easily acquired and lacks flexibility. In contrast, another research line [17, 44] borrows LLM's linguistic comprehension and compositional abilities for layout planning, and then incorporates layout-to-image models (e.g., GLIGEN [34]) for controllable generation. Although these methods mitigate misalignment issues like counting error, they heavily rely on intermediate states, e.g., bounding boxes, for layout representation. The intermediate states may not adequately capture fine-grained visual attributes, and can also accumulate errors in this two-stage paradigm. Furthermore, the intrinsic compositional reasoning abilities of T2I models are still inadequate.\nTo tackle these issues, we aim to promote text-image alignment by directly catalyzing the intrinsic compositional reasoning of T2I models, without depending on the inductive bias for attention manipulation or intermediate states. Richard Feynman famously stated, \u201cWhat I cannot create, I do not understand,\u201d underscoring the significance of understanding in the process of creation. This motivates us to consider enhancing the understanding abilities of T2I models to facilitate their text-to-image generation. As illustrated in Fig. 1, T2I models are more likely to generate an image with correct semantics if they can distinguish the alignment difference between the text prompt and the two images with minor semantic variations.\nIn light of this, we propose to examine the understanding abilities of T2I models by two discriminative tasks. First, we probe the discriminative global matching ability\u00b9 of T2I models on Image-text Matching (ITM) [18, 43], a representative task to evaluate fundamental text-image alignment. The second discriminative task inspects the local grounding ability of T2I models. One representative task is Referring Expression Comprehension (REC) [68], which examines the fine-grained expression-object alignment within an image. Based on the two tasks, we aim to 1) probe the discriminative abilities of T2I models, especially the compositional semantic alignment, and 2) further improve their discriminative abilities for better text-to-image generation.\nToward this end, we propose a Discriminative Probing and Tuning (DPT) paradigm to examine and improve text-image alignment of T2I models in a two-stage process. 1) To probe the discriminative abilities, DPT incorporates a Discriminative Adapter to do the ITM and REC tasks based on the semantic representations [29] of T2I models. For example, DPT may take the feature maps from U-Net of diffusion models [50] as semantic representations. And 2) in the second stage, DPT further improves the text-image alignment by means of parameter-efficient"}, {"title": "2. Related Work", "content": "Text-to-Image Generation. Over the past decades, great efforts on Variational Autoencoders [65], Generative Adversarial Networks [64, 69], and Auto-regression Models [10, 46, 67] have been dedicated to generating high-quality images with text conditions. Recently, there has been a flurry of interest in Diffusion Probabilistic Models (DMs) [23, 54] due to their stability and scalability. To further improve the generation quality, large-scale models such as DALL-E 2 [47], Imagen [52], and GLIDE [40], emerged to synthesize photorealistic images. This work mainly focuses on diffusion models and especially takes the open-sourced Stable Diffusion (SD) [50] as the base model.\nImproving Text-Image Alignment. Despite the thrilling success, current T2I models still suffer from Text-Image Misalignment issues [1, 9, 19], especially in complex scenes requiring compositional reasoning [37]. Several pioneering efforts were made to introduce guidance to intervene in internal features of SD to stimulate the high-alignment generation. For example, StructureDiffusion [16] parses prompts into tree structures and incorporates them with cross-attention representations to promote compositional generation. Attend-and-Excite [4] manipulates cross-attention units to attend to all textual subject tokens and enhance the activations in attention maps. Despite the notable"}, {"title": "3. Method", "content": "In this section, we introduce the DPT paradigm to probe and enhance the discriminative abilities of foundation T2I models. As shown in Fig. 2, DPT consists of two stages, i.e., Discrimination Probing and Discrimination Tuning, as well as a self-correction mechanism in Sec. 3.3.\n3.1. Stage 1 - Discriminative Probing\nIn the first stage, we aim to develop a probing method to explore \"How powerful are discriminative abilities of recent T2I models?\". To this end, we first select representative T2I models and semantic representations, and then consider adapting the T2I models to do discriminative tasks.\nStable Diffusion for Discriminative Probing. Considering SD is open-sourced and one of the most powerful and popular T2I models, we select its different versions (see Sec. 4.2) as representative models to probe the discriminative abilities. To make generative diffusion models semantically focused and efficient, SD [50] performs denoising in a latent low-dimensional space. It includes VAE [27], Text Encoder of CLIP [45], and U-Net [51]. The U-Net serves as a neural backbone for denoising score matching in the latent space, composed of three parts, i.e., down blocks, mid blocks, and up blocks. During training, given a positive image-text pair (x, y), SD first encodes image x with the VAE encoder and adds noise $\\epsilon \\sim N(0, 1)$ to obtain the latent $z_t = h(x, t)$ at timestep t. Thereafter, SD employs U-Net to predict the added noise and optimizes the model parameters by minimizing the L2 loss between the ground-truth noise and the predicted one.\nSemantic Representations. It is non-trivial to leverage T2I models such as SD to do discriminative tasks. Fortunately, recent work [29] demonstrates that diffusion models have a meaningful semantic latent space although they were originally designed for denoising [23] or score estimation [55]. Besides, a series of pioneering work [2, 8, 31, 63] shows the validity and even superiority of representations extracted from U-Net of SD to be qualified to discriminative tasks. Inspired by these studies, we consider utilizing semantic representations from the U-Net of SD to do discriminative tasks via a discriminative adapter.\nDiscriminative Adapter. We propose a lightweight discriminative adapter, which relies on the semantic representations of SD to handle discriminative tasks. Inspired by DETR [3], we implement the discriminative adapter with the Transformer [58] structure, including a Transformer encoder and a Transformer decoder. Besides, we adopt a fixed number of randomly initialized and learnable queries to adapt the framework to specific discriminative tasks.\nConcretely, given a noisy latent $z_t$ at a sampled timestep t and a prompt y, we first feed them into U-Net and extract a 2D feature map $F_t \\in \\mathbb{R}^{h \\times w \\times d}$ from one of the intermediate blocks\u00b2, where h, w, and d denote the height, width, and dimension, respectively. Formally, we extract $F_t$ via\n$F_t = \\text{UNet}_l(z_t, \\text{CLIP}(y), t),$\nwhere $\\text{UNet}_l$ refers to the operation of extracting the feature maps in the l-th block of U-Net. Afterward, we combine $F_t$ with learnable position embeddings [12] and timestep embeddings [50] of t via additive fusion, and then flatten it into the semantic representation $F_t \\in \\mathbb{R}^{hw \\times d}$. For simplicity, we will omit the subscript t in the following.\nTo probe the discriminative abilities, we feed F into the Transformer encoder $\\text{Enc}(\\cdot)$, and then perform interaction between the encoder output and some learnable queries $Q = \\{q_1, ..., q_v \\}$ with $q_i \\in \\mathbb{R}^d$ in the Transformer decoder $\\text{Dec}(\\cdot, \\cdot)$. The whole process is formulated as\n$Q^* = f(F; W_a, Q) = \\text{Dec}(\\text{Enc}(F), Q)$\nwhere f() abstracts to the discriminative adapter with parameters $W_a$ and Q. $W_a$ includes the parameters in Enc and Dec. The queries Q serve as a bridge between visual representations and downstream discriminative tasks, which attends the encoded semantic representation $F_t$ via cross-attention [58] of the decoder for downstream tasks. Thanks to multiple queries in Q, the query representations"}, {"title": "3.2. Stage 2 - Discriminative Tuning", "content": "In the second stage, we propose to improve the generative abilities, especially text-image alignment, by optimizing T2I models in a discriminative tuning manner. Most prior work [2, 63] only views SD as a fixed feature extractor for segmentation tasks due to its fine-grained semantic representation power but overlooks the potential back-feeding of discrimination to generation. Besides, though a recent study [28, 62] fine-tunes the SD model using discriminative objectives, it only pays attention to specific downstream tasks (e.g., ITM) and ignores the effect of tuning on generation. The advancement of discrimination may sacrifice the original generative power. In this stage, we mainly focus on enhancing generation, but also investigate the superior limit of discrimination under the premise of priority generation. It may shed new light on giving full play to the versatility of"}, {"title": "3.3. Self-Correction", "content": "Equipping the T2I model with the discriminative adapter enables the whole model to execute discriminative tasks. As a bonus of using the discriminative adapter, we propose a self-correction mechanism to guide high-alignment generation during inference. Formally, we update the latent $z_t$ aiming to enhance the semantic similarity between $z_t$ and the prompt y through gradients:\n$\\tilde{z_t} = z_t + \\eta \\frac{\\partial z_t}{\\partial s(z_t, y)},$\nwhere the guidance factor $\\eta$ control the guidance strength. $\\frac{\\partial s(z_t, y)}{\\partial z_t}$ represents the gradients from the discriminative adapter to the latent $z_t$. Afterward, we predict the noise by feeding $\\tilde{z_t}$ into U-Net and then obtain $z_{t-1}$ for generation."}, {"title": "4. Experiments", "content": "We conduct extensive experiments to evaluate the generative and discriminative performance of DPT, justify its effectiveness, and conduct an in-depth analysis.\n4.1. Experimental Settings\nBenchmarks. During training, we adopt the training set of MSCOCO [35] for ITM and three commonly used datasets [68], i.e., RefCOCO, RefCOCO+, and RefCOCOg for REC. To evaluate the text-image alignment, we utilize five benchmarks: COCO-NSS1K [44], CC-500 [16], ABC-6K [16], TIFA [25], and T2I-CompBench [14]. According to the distribution differences of textual prompts between the training set and the test sets, we adopt three settings, i.e., In-Distribution (ID) and Out-of-Distribution (OOD) [57] on COCO-NSS1K and CC-500, respectively, and Mixed Distribution (MD) on ABC-6K, TIFA, and T2I-CompBench. More details can be found in Appendix B.1.\nEvaluation Metrics. Following the existing baselines [4, 16, 44], we adopt CLIP score [21] and BLIP score\u00b3 [33]"}, {"title": "A. More Results and Analysis", "content": "A.1. Ablation Study on DPT\nTo further explore the effectiveness of the proposed DPT paradigm, we compare it with the traditional denoising tuning method based on the MSE loss function. Concretely, we delve into all four combinations of these two and evaluate the ID and OOD generation performance on the basis of two versions of SD, i.e., SD-v1.4 and SD-v2.1. The experimental results are reported in Tab. 5. We have the following observations. 1) MSE could improve the alignment and quality under the ID setting, but it may not be helpful and even harmful to the alignment under the OOD setting. 2) DPT consistently enhances alignment performance across both ID and OOD settings, with minimal impact on the original image quality. And 3) by combining MSE and DPT objectives, we may get a good trade-off between alignment and quality, and achieve a better performance on the BLIP-ITM evaluation metric which may focus on more details.\nA.2. Ablation Study on Probed Blocks\nAs shown in Tab. 6, we carry out extensive experiments to study the effect of different probed blocks of U-Net on generation performance. The results show that DPT achieves the best performance when probing the up3 block.\nA.3. Comprehensive Evaluation on COCO-NSS1K\nThe COCO-NSS1K dataset [44] was constructed to evaluate five categories of abilities for T2I models, including counting, spatial relation reasoning, semantic relation reasoning, complicated relation reasoning, and abstract imagination. To delve into these categories, we compare the proposed methods, including DPT and DPT + SC, with SD-v1.4 and SD-v2.1, as shown in Fig. 6. Our method consistently improves the alignment performance in all categories compared with the state-of-the-art SD-v2.1. Besides, the self-correction module could further align the generated images with prompts, especially in the semantic relation category.\nA.4. Impact of Discriminative Tuning Steps\nAs shown in Fig. 7, Fig. 8, and Fig. 9, we study the generation and discrimination performance based on SD-v2.1 and SD-v1.4, and the comparison between the two versions, respectively. We discuss the results from the discrimination and generation aspects as follows.\nDiscriminative Tuning for Discrimination On the one hand, the grounding performance is continuously improved with the tuning step increases, while the matching perfor-"}, {"title": "A.5. Impact of Rank Numbers in LoRA", "content": "The rank number in LoRA determines the number of extra parameters introduced to the discriminative tuning stage compared with the first stage. To explore the influence of the rank numbers on the generation performance, we compare different rank numbers, from 0 to 128, as shown in Tab. 7. The results reflect that DPT achieves the best performance when using 4 rank numbers on most evaluation metrics. More rank numbers do not bring further improvement, which may be attributable to the scale of tuning data."}, {"title": "A.6. Impact of Layers of Discriminative Adapter", "content": "The total parameters of DPT also depend on the transformer layers of the discriminative adapter. We conduct experiments by using different numbers of layers. Note that we keep the same number of layers in encoders and decoders for each experiment. The results are reported in Tab. 8. In general, the best generation performance can be achieved when using 4 layers. Besides, the alignment performance is always better than SD-v2.1 (i.e., the experiment with 0 layer), which further verifies the effectiveness of DPT."}, {"title": "A.7. Impact of Denosing Objective", "content": "In the raw SD model, only the denoising objective with the MSE form is used to model the data distribution for image synthesis. To further the interplay of DPT and MSE objectives, we perform more experiments by combining them and taking different values of the coefficient of MSE. As shown in Tab. 9, we observe that the simultaneous use of"}, {"title": "B.2. Baselines", "content": "To verify the effectiveness of our method on T2I, we carry out experiments based on SD-v1.4 and SD-v2.1, and compare DPT with SD [50] and recent alignment-oriented T2I baselines including LayoutLLM-T21 [44], StructureDiffusion [16], Attend-and-Exite [4], DiffusinoITM [28], VP-Gen [13] and LayoutGPT [17]."}, {"title": "B.3. Implementation Details", "content": "B.3.1 Settings for Discriminative Probing and Tuning\nIn the first stage, we perform discriminative tuning by pre-training for 60k steps with a batch size of 220 and a learning rate of 1e-4. After that, we perform discriminative tuning in the second stage for 6k steps with a batch size of 8 and a learning rate of 1e-4. We use the AdamW [39] optimizer with betas as (0.9,0.999) and weight decay as 0.01. We perform the gradient clip with 0.1 as the maximum norm. We select the model checkpoint by using a validation set for alignment-oriented generation. Specifically, we collect this validation set from MS-COCO, following COCO-NSS1K [44]. Based on this set, we generate images conditioned on prompts and then compute the CLIP score between them. The model with the highest CLIP score in this validation set is chosen for testing. Besides, we accumulate gradients in each 8 steps in this stage. Using a single A100 (40G), the discriminative probing requires about 3 days for the first stage, and the discriminative tuning requires about 1 day for the second stage.\nB.3.2 Settings for Discriminative Adapter\nWe use the 1-layer Transformer encoder and the 1-layer Transformer decoder to implement the discriminative adapter introduced in Sec. 3.1. The feature map with the shape 1280 \u00d7 8 \u00d7 8 in the medium layer of U-Net is flattened and fed into the encoder. The hidden dimensions of attention layers and feedforward layers are set to 256 and 2048, respectively. The number of attention heads is 8. We use 110 learnable queries in total, 10 for global matching, and the other 100 for local grounding, i.e., N = 110 and M = 10. The temperature factor \u03c4 for contrastive learning in Eqn. (3) and Eqn. (4) is learnable and initialized to 0.07. To make a balance between different grounding objectives in Eqn. (7), we set \u03bb0, \u03bb1, \u03bb2, and \u03bb3 as 1, 5, 2, and 1, respectively. The same setting is also used for maximum matching in Eqn. (5). During inference, we use 0.5 as the default guidance strength of self-correction, i.e., \u03b7 = 0.5.\nB.3.3 Implementation of Baselines\nWe run the codes of SD-v1.45 and SD-v2.16 in the huggingface open-source community. Besides, we run the code of LayoutLLM-T2I7 and load the open checkpoint to evaluate its performance on COCO-NSS1K. Because of the training-free property, StructureDiffusion and Attend-and-Excite can be directly executed for evaluation. As for DiffusionITM [28], we implement it based on the open-sourced code10 and rename it as HN-DiffusionITM considering we perform contrastive learning based on the hard negative samples. For a fair comparison, we re-train HN-DiffusionITM using our training data and adopt the DDIM"}]}