{"title": "Multiple Invertible and Partial-Equivariant Function for Latent Vector Transformation to Enhance Disentanglement in VAES", "authors": ["Hee-Jun Jung", "Jaehyoung Jeong", "Kangil Kim"], "abstract": "Disentanglement learning is a core issue for understanding and re-using trained information in Variational AutoEncoder (VAE), and effective inductive bias has been reported as a key factor. However, the actual implementation of such bias is still vague. In this paper, we propose a novel method, called Multiple Invertible and partial-equivariant transformation (MIPE-transformation), to inject inductive bias by 1) guaranteeing the invertibility of latent-to-latent vector transformation while preserving a certain portion of equivariance of input-to-latent vector transformation, called Invertible and partial-equivariant transformation (IPE-transformation), 2) extending the form of prior and posterior in VAE frameworks to an unrestricted form through a learnable conversion to an approximated exponential family, called Exponential Family conversion (EF-conversion), and 3) integrating multiple units of IPE-transformation and EF-conversion, and their training. In experiments on 3D Cars, 3D Shapes, and dSprites datasets, MIPE-transformation improves the disentanglement performance of state-of-the-art VAEs.", "sections": [{"title": "1 Introduction", "content": "Disentanglement learning to learn more interpretable representations is broadly useful in artificial intelligence fields such as classification Singla et al. (2021), zero-shot learning Tenenbaum (2018), and domain adaptation Li et al. (2019); Zou et al. (2020). The disentangled representation is defined as a change in a single dimension, which corresponds to unique semantic information. Several works have been conducted based on this framework.\nA major model for enhancing the disentanglement learning is Variational AutoEncoder (VAE) Kingma & Welling (2013). Based on VAE, unsupervised disentangled representation learning has been elaborated Higgins et al. (2017); Chen et al. (2018); Kim & Mnih (2018); Jeong & Song (2019); Li et al. (2020) through the factorizable variations and control of uncorrelatedness of each dimension of representations. Moreover, VAE models that handle the shape of prior as a Gaussian mixture Dilokthanakul et al. (2016b) or von Mises-Fisher Davidson et al. (2018) were also developed, but the disentanglement is still incomplete. As a critical point, there is a report that unsupervised disentanglement learning is impossible without inductive bias Locatello et al. (2019)."}, {"title": "2 Related Work", "content": "Recently, such inductive bias has been introduced in various perspectives on transformation of latent vector space. Intel-VAE Miao et al. (2022) proposed the benefit of invertible transformation of the space to another latent space to provide better data representation, which includes hierarchical representations. Group theory based bias also shows significant improvement on disentanglement Zhu et al. (2021); Yang et al. (2022), whose definition follows Higgins et al. (2018a), which is based on the group theory. The works show that equivariant transformation between input and latent vector space has a key role of disentanglement.\nInspired by the above works, we propose a Multiple Invertible and partial-equivariant transformation (MIPE-transformation) method, which is simply insertable to VAEs. The partial-equivariance is defined by Romero & Lohit (2022) and we follow its definition: $f(g \\cdot x) = g \\cdot f(x) \\forall x \\in X, \\forall y \\in G', G' \\subset G$, where G' is a subset of group G (f is partially equivariant to group G). First, we assume that an encoder is partial-equivariant and we call it an encoder equivariance condition. The method adopts the matrix exponential to hold the invertible property of latent-to-latent (L2L) vector transformation. Then, we constrain the L2L transformation to a symmetric matrix exponential to be partial-equivariant to a subgroup between latent and transformed latent space. Because it extends the encoder to be partial-equivariant to a subgroup between input and transformed latent space. The IPE-transformation generates an uncertain form of latent vector distributions, so we provide a training procedure to force them to be close to an exponential family, called exponential family conversion (EF-conversion). This conversion enables the uncertain distribution to work in the typical training framework of VAEs. Then, we mathematically show that the multiple uses of IPE-transformation work as \u03b2 parameters Higgins et al. (2017) controlled for enhancing disentanglement learning. Also, we propose the implicit semantic mask to induce a semantic mask in the latent vector space, different to Yang et al. (2020). In experiments with quantitative and qualitative analysis, MIPE-transformation shows significant improvement in disentangled representation learning in 3D Cars, 3D Shapes, and dSprites tasks. Our main contributions are summarized as follows."}, {"title": "3 Method", "content": "The overview of a VAE equipped with MIPE-transformation is shown in Figure 1. The MIPE-transformation has three main components: 1) IPE-transformation Unit to transform latent vectors with invertible and partial-equivariant properties, 2) EF-conversion Unit to extend VAEs to learn the exponential family distribution of"}, {"title": "3.1 Invertible and Partial-Equivariant Function for L2L Transformation", "content": ""}, {"title": "3.1.1 Invertible Property by Using Matrix Exponential", "content": "To guarantee the invertible property of IPE-transformation, we use a function $\\psi(\\cdot) = e^{M * .}$ for the transformation, where M is in n \u00d7 n real number matrix set $M_n(R)$ Xiao & Liu (2020). The operator * is matrix multiplication, and $e^M = \\sum_{k=0}^\\infty \\frac{M^k}{k!}$. Our motivation is to use the benefits of injecting explicit inductive bias for disentanglement Locatello et al. (2019); Miao et al. (2022). InteL-VAE effectively extracts hierarchical representation, which includes low-level features (affect to a specific factor) and high-level features (affect to complex factors) with an invertible transformation function Miao et al. (2022)."}, {"title": "3.1.2 Why Should L2L Transformation Be Equivariant?", "content": "Let's consider equivariant function between the input and transformed latent vector space, directly used for a decoder in the VAE frameworks. All L2L transformations do not extend the encoder equivariance condition to the relation between input and transformed latent space. This problem is more precisely shown in Figure 2, which illustrates partial equivariance condition over the input space X, latent vector space Z, and its transformed latent vector space $\\tilde{Z}$ with a corresponding group of symmetries $G_I, G_L$, and $G_T$, respectively. In the VAEs literature, it has not been reported to restrict L2L transformation to guarantee equivariant function between two spaces, so we propose a solution to guarantee at least a part of symmetries to be equivariant."}, {"title": "3.1.3 Equivariance Property with Symmetric Matrix Exponential", "content": "To enhance the equivariance of L2L transformation, we set M of $\\psi(\\cdot)$ to a symmetric matrix. We show that 1) a group with the constraint guarantees equivariance of $\\psi(\\cdot)$ over the specific group, 2) $\\psi(\\cdot)$ being equivariant over subset of symmetries between the input space and transformed latent vector space, and 3) the constraint increases the probability of $\\psi(\\cdot)$ to be in the group (equal to be equivariant over the subset of symmetries).\nWe particularly call the transformations as symmetries Higgins et al. (2022) to distinguish them from IPE- and I2L-transformations. For the generality of our method, we consider an arbitrary VAE model that has no restriction on creating intersections to any set as Figure 2.\nIn the next, we show that matrix exponential with symmetric matrix partially preserves encoder equivariance condition better than other matrices."}, {"title": "Proposition 3.1.", "content": "Any $\\psi(\\cdot) \\in G_S$, notated as $\\psi_{G_S}(\\cdot)$, is equivariant to group $G_S$."}, {"title": "Proof.", "content": "The group $G_S$ is closed to matrix multiplication, and its element is always a symmetric matrix by definition. Then, any two elements in $G_S$ are commutative because if matrix multiplication of two symmetric matrices is symmetric then both are commutative. As a result, $\\psi_{G_S}(\\cdot)$ and group elements of $G_S$ are commutative ($G_S$ is an abelian group). Because of the commutativity, $\\psi_{G_S}(g_S \\circ z) = e^S g_S z = g_S e^S z = g_S \\psi_{G_S}(z)$ for $g_S \\in G_S$ if the group action $\\circ$ is set to matrix multiplication, where $\\psi_{G_S} \\in G_S$. This equation satisfies the general definition of an equivariant function that a function f(.) is equivariant if $f(g \\circ z) = g \\circ f(z)$ for all g in a group G by matching f, g, and G to $\\psi_{G_S}, g_S$, and $G_S$, respectively."}, {"title": "Proposition 3.2.", "content": "If $q_\\phi$ is equivariant over defined on group of symmetries $G_O$ and $G_q$, then $\\psi_{G_S}(q_\\phi(\\cdot))$ is equivariant to symmetries in $G_\\Lambda$ corresponding to $G_S \\cap G_\\Lambda$ and $G_\\Gamma$ corresponding to $G_S \\cap G_\\Lambda$ by the equivariance of $q_\\phi$."}, {"title": "Proof.", "content": "The function $\\psi_{G_S}(\\cdot)$ is an equivariant function over group elements in $G_S \\cap G_\\Lambda$ by Proposition 3.1. Then, the composite function, $\\psi_{G_S}(\\cdot)$ and $q_\\phi$, is an equivariant function of $G_\\Lambda$ corresponding to $G_S \\cap G_\\Lambda$ and $G_\\Gamma$ corresponding to $G_S \\cap G_\\Lambda$. Let $g_l$ be a group element in $G_S \\cap G_\\Lambda$, and $g_\\Gamma$ is a group element in $G_\\Gamma$ corresponding to $G_S \\cap G_\\Lambda$. More details are in Appendix C.1."}, {"title": "Then,", "content": ""}, {"title": "Proposition 3.3.", "content": "$Pr(\\psi_{e_S}(\\cdot) \\in G_S) > Pr(\\psi_{E_M}(\\cdot) \\in G_S) > Pr(\\psi_{M}(\\cdot) \\in G_S)$."}, {"title": "Proof.", "content": "All $e_S \\in E_S$ are in $E_M$ since $Sym_n(R) \\subset M_n(R)$. However, $E_M \\notin E_S$ because $e_S$ is always symmetric, but $e_M$ can be an asymmetric matrix. Therefore $E_M \\subsetneq E_S$. Therefore, the probability"}, {"title": "3.1.4 Relation Between $\\psi(\\cdot)$ and Disentanglement", "content": "In addition to invertible and partial-equivariant properties, our IPE-transformation also guarantees zero Hessian matrix, which enhances disentanglement without any additional loss of Peebles et al. (2020). Hessian matrix of the transformation $\\nabla^2\\psi(z) = \\nabla_z(\\nabla_z e^{Mz}) = 0$ because of the irrelevance of M to z. By this property, $\\psi(\\cdot)$ leads that independently factorizes each dimension Peebles et al. (2020), and it injects group theory based inductive bias simultaneously. This is because the group decomposition of z space $G = G_1 \\times G_2 \\times \\dots \\times G_k$ corresponds to group decomposition of the transformed latent vector space $G' = G'_1 \\times G'_2 \\times \\dots \\times G'_k$ such that each $G'_i$ is fixed by the action of all the $G_j$ for $j \\neq i$ Yang et al. (2022); Higgins et al. (2018b). This correspondence of decomposition is expected to transfer the independence between dimensions of z to the $\\tilde{z}$ space of Higgins et al. (2018a)."}, {"title": "3.2 Exponential Family Conversion for Unknown Prior", "content": "In VAE frameworks, the Gaussian normal distribution is applied as a prior. However, a prior from data is usually unknown and may not follow the Gaussian distribution Miao et al. (2022). As a solution, we present a training procedure for VAEs to build an exponential family distribution from a latent variable of an arbitrary distribution. Then, we introduce training losses obtained from the unit IPE-transformation function and EF-conversion."}, {"title": "3.2.1 Elements of Exponential Family Distribution Settings", "content": "First, we set sufficient statistics T(\u00b7), log-normalizer A(\u00b7), and carrier or base measure B(\u00b7) are deterministic functions by maximizing conjugate prior for parameter \u03be. To determine the natural parameter of posterior and prior $\\theta_{\\tilde{z}_m}$, and $\\theta_{\\xi_m}$, we use a natural parameter generator (NPG) designed by multi-layer perceptron Charpentier et al. (2022a). As introduced in Bishop (2006); Charpentier et al. (2022a), we assume exponential family always admits a conjugate prior:"}, {"title": "Then,", "content": "$q(\\theta | \\xi, \\nu) = exp(\\nu \\theta^T \\xi - \\nu A(\\theta) + B'(\\xi, \\nu)),$                                                                                    (1)\nwhere B'(.) is a normalize coefficient and v is evidence, and it is expressed by prior natural parameter \u03be. However, generated natural parameter $\\Theta_m$ is not guaranteed as the appropriate parameter of the exponential family corresponds to conjugate prior. To satisfy this condition, we assume observation is a set of independent"}, {"title": "3.2.2 Distribution Approximation As an Exponential Family", "content": "The procedure represents a posterior distribution in the exponential family by adopting the following form:"}, {"title": "Then,", "content": "$p(\\theta | X, \\xi,\\nu) \\propto exp\\left(\\theta^T (\\sum_{n=1}^N T(x_n) + \\nu\\xi) - A(\\Theta)\\right),$                                                                         (2)\nwhere sufficient statistics T(\u00b7) and log-normalizer, A(\u00b7) are known functions, samples $X = \\{x_1, x_2,..., x_n\\}$ from distribution, and natural parameter of posterior $\\theta$ and of prior \u03be Bishop (2006). The functions T(\u00b7), and A(\u00b7) are deterministic functions to maximize posterior distribution. The evidence is implemented as learnable parameters $\\nu\\in R^{n \\times n}$. The natural parameter is generated by a multi-layer perceptron as Charpentier et al. (2022a). This general form approximating an exponential family distribution with learnable parameters can extend VAEs to use a wider distribution for latent variables by simply matching X to generated latent variables. After IPE-transformation, we can apply the form by using the $\\tilde{z}_m, \\Theta_{\\tilde{z}_m}$, and $\\Theta_{\\xi_m}$ for X, \u03b8, and \u03be, respectively."}, {"title": "3.2.3 EF Similarity Loss", "content": "We added a loss to converge the unrestricted distributions of $\\tilde{z}$ to the power density function of the exponential family by constraining the posterior maximization as:"}, {"title": "Then,", "content": "maximize $log p(\\Theta_{\\tilde{z}_m} | z_m, \\Theta_{\\xi_m}, \\nu_m)$ s.t. $D_{KL}(f_x(x | \\Theta_{\\tilde{z}_m}) || f_x(x | \\Theta_{\\xi_m})) \\ge 0$                                                                           (3)\n$\\Rightarrow \\mathcal{L}_s(z_m, \\xi_m) = log p(\\Theta_{\\tilde{z}_m} | z_m, \\Theta_{\\xi_m}, \\nu_m) + \\lambda_m D_{KL}(f_x(x | \\Theta_{\\tilde{z}_m}) || f_x(x | \\Theta_{\\xi_m}))$                                               (4)\n$\\Rightarrow \\mathcal{L}_{el} := ||\\nabla_{\\lambda_m, \\Theta_{\\xi_m}, \\Theta_{\\tilde{z}_m}} \\mathcal{L}_s ||_2^2 = 0.$                                                                                   (5)\nThe notation $\\Theta_k$ is a generated natural parameter by a given $k \\in \\{z, \\xi\\}$, and $f_x(x | \\theta)$ is a power density function of the exponential family. Moreover, $\\lambda_m$ is a trainable parameter for optimizing the Lagrange multiplier, and $D_{KL}(f_x(x | \\Theta_{\\tilde{z}_m}) || f_x(x | \\Theta_{\\xi_m}))$ is a KL divergence of the exponential family."}, {"title": "3.2.4 KL Divergence for Evidence of Lower Bound", "content": "The KL divergence of Gaussian distribution Kingma & Welling (2013) is computed using mean and variance, which are the parameters of a Gaussian distribution. To introduce a loss as the KL divergence of Gaussian distribution, we compute KL divergence of the exponential family in Eq. 2 using the learnable parameter T(.) and A(.) with given natural parameter $\\Theta_{\\tilde{z}}$ and $\\Theta_{\\xi}$, expressed as:"}, {"title": "Then,", "content": "$\\mathcal{L}_{kl} := D_{KL}(f_x(x | \\Theta_{\\tilde{z}_m}) || f_x(x | \\Theta_{\\xi_m}))$\n$= A(\\Theta_{\\xi}) - A(\\Theta_{\\tilde{z}}) + \\Theta^T_{\\xi} \\nabla_{\\Theta_{\\tilde{z}}} A(\\Theta_{\\tilde{z}}) - \\Theta^T_{\\tilde{z}} \\nabla_{\\Theta_{\\xi}} A(\\Theta_{\\xi}).$                                                                                    (6)"}, {"title": "3.2.5 KL Divergence Calibration Loss", "content": "To reduce the error between the approximation and true matrix for the matrix exponential Bader et al. (2019), we add a loss to minimize the difference of their KL divergence measured by mean squared error (MSE) as:"}, {"title": "Then,", "content": "$\\mathcal{L}_{cali} = MSE(D_{KL}(q_\\phi(z|x) || p_\\theta(z)), D_{KL} (f_x(x | \\Theta_{\\tilde{z}_m}) || f_x(x | \\Theta_{\\xi_m}))),$                                                     (7)\nwhich is the KL divergence calibration loss ($\\mathcal{L}_{cali}$)."}, {"title": "3.2.6 Implicit Semantic Mask", "content": "We propose an implicit semantic mask to improve disentanglement learning. We apply mask matrix M which consists of 0 or 1 element to log-normalizer to prevent less effective weight flow as:"}, {"title": "Then,", "content": "$M_{ij} = \\begin{cases} 1 & \\text{if } |W_{ij} | \\ge \\overline{W_{ij}} - \\lambda |W_{ij}|  \\\\ 0 & \\text{otherwise}  \\end{cases}$                                                                                     (8)\nwhere W is the weight of log-normalizer, \u03bb is a hyper-parameter, $\\overline{|W_{ij}|}$, and $\\sigma_{|W_{ij}|}$ are the mean, and standard deviation of weight respectively. Previous work Yang et al. (2020) utilizes a semantic mask in input space directly, but we inject the semantic mask implicitly on the latent space."}, {"title": "3.3 Integration for Multiple IPE-Transformation and EF-Conversion", "content": "We mathematically extend IPE-transformation to MIPE-transformation, which is the equivalent process of \u03b2-VAE to enhance disentanglement. Each IPE-transformation function operates independently, then the reconstruction error for objective function is defined as:"}, {"title": "Then,", "content": "$\\mathcal{L}_{rec} := \\frac{1}{k}\\sum_{i=1}^{k}\\int q_\\phi(z|x)log p_\\theta(x|\\psi_i(z))dz = \\frac{1}{k} \\sum_{i=1}^{k} \\mathbb{E}_{q_\\psi,\\psi_i(z)|x} log p_\\theta(x|\\psi_i(z)),$                                     (9)"}, {"title": "Then,", "content": "$\\mathcal{L}'(\\phi, \\theta, \\psi_{i\\in [1,k]}; x) = \\frac{1}{k}\\sum_{i=1}^{k} \\mathbb{E}_{q_{\\phi,\\psi_i}(z|x)} log p_\\theta (x|\\psi_i(z)) - \\sum_{i=1}^{k} D_{KL} (q_{\\phi,\\psi_i}(z|x) || p_{\\psi_i}(z)).$             (10)\nHowever, following Eq. 10, k samples are generated, and each sample is disentangled for different factors. We implement the output as the average of the sum of the k samples to obtain a single sample with a superposition effect from k samples. Moreover, the KL divergence term in Eq. 10 represents that increasing number of MIPE-transformation is equal to increasing \u03b2 hyper-parameter in \u03b2-VAE Higgins et al. (2017).\nThe VAEs equipped with MIPE-transformation (MIPET-VAEs) can be trained with the following loss:"}, {"title": "Then,", "content": "$\\mathcal{L}(\\phi, \\theta, \\psi_{i\\in [1,k]}; x) = \\mathcal{L}_{rec} - \\mathcal{L}_{kl} - \\mathcal{L}_{el} - \\mathcal{L}_{cali}.$                                                                            (11)"}, {"title": "4 Experiment Settings", "content": ""}, {"title": "4.0.1 Models", "content": "As baseline models, we select VAE, \u03b2-VAE, \u03b2-TCVAE, and CLG-VAE. These models are compared to their extension to adopt MIPET, abbreviated by adding the MIPET prefix. We apply the proposed method to"}, {"title": "4.0.2 Datasets", "content": "We compare well-known VAEs to CHIC-VAES: VAE, \u03b2-VAE, \u03b2-TCVAE, and CLG-VAE on the following data sets with 1) dSprites Matthey et al. (2017) which consists of 737,280 binary 64 \u00d7 64 images of dSprites with five independent ground truth factors(number of values), i.e. shape(3), orientation(40), scale(6), x-position(32), and y-position(32). 2) 3D Shapes Burgess & Kim (2018) which consists of 480,000 RGB 64 \u00d7 64 \u00d7 3 images of 3D Shapes with six independent ground truth factors: shape(4) orientation(15), scale(8), wall color(10), floor color(10), and object color(10). 3) 3D Cars Reed et al. (2015) which consists of 17,568 RGB 64 \u00d7 64 \u00d7 3 images of 3D Shapes with three independent ground truth factors: car models(183), azimuth directions(24), and elevations(4)."}, {"title": "4.0.3 Training", "content": "We set 256 mini-batch size in the datasets (dSprites, 3D Shapes, and 3D Cars), Adam optimizer with learning rate 4 \u00d7 10\u22124, \u03b21 = 0.9, \u03b22 = 0.999, and epochs from {30,67, 200} as a common setting for all the comparative methods. For the comparison, we follow training and inference on the whole dataset. We train each model for 30, 67, and 200 epochs on the dSprites, 3D Shapes, and 3D Cars, respectively, as introduced in Kim & Mnih (2018); Ren et al. (2022). We tune \u03b2 from {1,2,4,10} and {4,6} for \u03b2-VAE and \u03b2-TCVAE, respectively. We set the dimension size of the latent vectors from {6,10} for 10 on dSprites and 3D Cars datasets and 6 for 3D Shapes, but we set 10 for CLG-VAE because it sets 10 dimensions size on 3D Shapes in Zhu et al. (2021). Regarding the CLG-VAE, we fix Adecomp, Ahessian, and forward group features as 40, 20, and 0.2, respectively. Because the hyper-parameters showed the best result in Zhu et al. (2021). We set group reconstruction from {0.2, 0.5, 0.7}. For Control-VAE, we set the maximum KL divergence value from {10,12,..., 20}. In addition, we set masking ratio \u03bb from {0.0, 0.5, . . . 2.0,\u221e}. To check the impact of MIPE-transformation, we do not consider the Groupified VAE because the latter is implemented with an extended decoder (different capacity)."}, {"title": "4.0.4 Evaluation", "content": "We conduct experiments on NVIDIA A100, RTX 2080 Ti, and RTX 3090. We set 100 samples to evaluate global empirical variance in each dimension and run it a total of 800 times to estimate the FVM score introduced in Kim & Mnih (2018). For the MIG Chen et al. (2018), SAP Kumar et al. (2018), and DCI Eastwood & Williams (2018), we follow default values introduced in Michlo (2021), training and evaluation 100 and 50 times with 100 mini-batches, respectively. We evaluate four disentanglement metrics for a less biased understanding of the actual states of disentanglement."}, {"title": "5 Results and Discussion", "content": ""}, {"title": "5.1 Quantitative Analysis", "content": ""}, {"title": "5.1.1 Disentanglement Metrics", "content": "We set the number of IPE-transformation functions to be equal to balancing hyper-parameter \u03b2 on \u03b2-VAE because of Eq. 11. The number of IPE-transform functions of \u03b2-TCVAE is 3. However, in the case of CLG-VAE, we set it to 1 because its approach is based on the group theory, not directly controlling a KL divergence term such as \u03b2-VAE. We average each model performance value with 40, 20, 60, and 30 cases in VAES, \u03b2-TCVAES, Control-VAE and CLG-VAEs, respectively.\nAs shown in Table 2, MIPET-VAEs disentanglement performance is broadly improved with four metrics on each dataset. In particular, most FVM results significantly affect the model performance and stability on all datasets. Therefore, our proposed method obtains a specific dimension that corresponds to a specific single factor. These results imply that applied to MIPE-transformation functions on VAEs elaborate disentangled representation learning.\nWe additionally estimate the p-value of each metrics over models in Table 3. Previous work shows the average case of each models Yang et al. (2022)."}, {"title": "5.1.2 Sensitivity to the Number of IPE-transformation and EF-conversion", "content": "We investigate the impact of the MIPE-transformation function. As presented in Table. 4, MIPE- transformation is better than IPE-transformation for disentanglement learning on each dataset. Indeed, MIPET-\u03b2-VAEs results more generally and clearly show the impact of the MIPE-transfomation function. Our derivation in Section C.1 clearly explains MIPE-transformation impact. This result shows the impact of the multiple uses of IPE-transformation and EF-conversion."}, {"title": "5.1.3 Impact of Implicit Semantic Mask", "content": "We set masking hyper-parameter \u5165 from {0.0,0.5,2.0,\u221e}, and each model has different A for best case. In Table 5, VAE and CLG-VAE with masked log-normalizer show better and well-balanced results than the models without masking, which implies improvement of disentanglement."}, {"title": "5.1.4 Ablation Study", "content": "We conduct an ablation study to evaluate the separate impact of equivariant property and the EF-conversion. We have already presented the impact of the multiple uses of IPE-transform and EF-conversion in the previous paragraph. We evaluate the impact of the other properties by setting MIPE-transformation 1) without equivariant (w/o E), which is implemented as an asymmetric matrix, and 2) without EF-conversion (w/o EF). To exclude group theory interference with other methods, we select \u03b2-VAE and \u03b2-TCVAE. As the results are shown in Table 6, most of the results show that MIPET-VAEs performance is better than other cases. In particular, MIPET (w/o EF) results are lower than MIPET (w/o E) results and are clearly shown in all cases."}, {"title": "5.2 Qualitative Analysis", "content": "We randomly sample an image for each dimension of the latent vector space and creates 10 variants of its generated latent vector by selecting values from {-2, 2} with 10 intervals for the dimension, then generate their corresponding output images. For the generation, we select \u03b2-TCVAE (6), which shows the best FVM scores in dSprites dataset. Thereafter, we evaluate the semantic roles of each dimension before and after applying MIPE-transformation function.\nIn Figure 4, \u03b2-TCVAE struggles with y-position and rotation, as shown on the 6th row, and with scale and shape represented on the 7th row. On the contrary, MIPET-\u03b2-TCVAE separates y-position and rotation factor (10th, and 7th rows), also the activated dimensions of MIPET-\u03b2-TCVAE are not overlapped with each factor. Applied our method on \u03b2-TCVAE shows better disentangled representation on dSprites dataset. These results also show that our proposed method improves disentangled representation learning."}, {"title": "6 Conclusion", "content": "In this paper, we address the problem of injecting inductive bias for learning unsupervised disentangled representations. To build the bias in VAE frameworks, we propose MIPE-transformation composed of 1) IPE-transformation for the benefits of invertibility and partial-equivariant for disentanglement, 2) a training loss and module to adapt unrestricted prior and posterior to an approximated exponential family, and 3) integration of multiple units of IPE-transformation function and EF-conversion for more expressive bias. The method is easily equipped on state-of-the-art VAEs for disentanglement learning and shows significant improvement on dSprites, 3D Shapes, and 3D Cars datasets. We expect that our method can be applied to more VAEs, and extended to downstream applications. Our work is limited to holding partial equivariance of I2L transformation, so more direct methods to induce it can be integrated in the future."}, {"title": "A Appendix", "content": ""}, {"title": "B Preliminaries", "content": ""}, {"title": "B.1 Group Theory", "content": "Binary operation: Binary operation on a set S is a function that * : S\u00d7S \u2192 S, where \u00d7 is a cartesian product.\nGroup: A group is a set G together with binary operation *, that combines any two elements ga and gb in G, such that the following properties:"}, {"title": "Then,", "content": "\u2022 closure: ga, gb \u2208 G \u21d2 ga * gb \u2208 G.\n\u2022 Associativity: \u2200ga, gb, gc \u2208 G, s.t. (ga * gb) * gc = ga * (gb * gc).\n\u2022 Identity element: There exists an element e \u2208 G, s.t. \u2200g \u2208 G, e * g = g * e = g.\n\u2022 Inverse element: \u2200g \u2208 G,\u2203g\u22121 \u2208 G: g * g\u22121 = g\u22121 * g = e."}, {"title": "Group action:", "content": "Let (G, *) be a group and set X, binary operation \u00b7 : G \u00d7 X \u2192 X, such that following properties:"}, {"title": "Then,", "content": "\u2022 Identity: e \u00b7 x = x, where e \u2208 G, x \u2208 X.\n\u2022 Compatibility: \u2200ga, gb \u2208 G, x \u2208 X, (ga * gb) \u00b7 x = ga \u00b7 (gb \u00b7 x)."}, {"title": "Equivariant map:", "content": "Let G be a group and X1, X2 be two sets with corresponding group action of G in each sets: TgX1 , TgX2 , where g \u2208 G. Then a function f : X1 \u2192 X2 is equivariant if f(TgX1 \u00b7 X1) = TgX2 \u00b7 f(X1)."}, {"title": "Partial Equivariance:", "content": "Romero & Lohit (2022):"}]}