{"title": "NESTQUANT: NESTED LATTICE QUANTIZATION FOR MATRIX PRODUCTS AND LLMS", "authors": ["Semyon Savkin", "Eitan Porat", "Or Ordentlich", "Yury Polyanskiy"], "abstract": "Post-training quantization (PTQ) has emerged as a critical technique for efficient deployment of large language models (LLMs). This work proposes NESTQUANT, a novel PTQ scheme for weights and activations that is based on self-similar nested lattices. Recent work have mathematically shown such quantizers to be information-theoretically optimal for low-precision matrix multiplication. We implement a practical low-complexity version of NestQuant based on Gosset lattice, making it a drop-in quantizer for any matrix multiplication step (e.g., in self-attention, MLP etc). For example, NestQuant quantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving perplexity of 6.6 on wikitext2. This represents more than 55% reduction in perplexity gap with respect to unquantized model (perplexity of 6.14) compared to state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various LLM evaluation benchmarks also show a reduction in performance degradation induced by quantization.", "sections": [{"title": "Introduction", "content": "There are three principal goals of post-training quantization (PTQ). First, reducing the number of bits per parameter allows for loading big models on cheap GPUs with limited memory, thus democratizing access to LLMs. This requires \"weights-only\" quantization algorithms of which the most popular are AWQ, GPTQ, and QuIP (see references in Section 2.2).\n\nThe second goal of PTQ is to accelerate inference. In LLMs most of the compute is spent multiplying matrices. Multiplying a pair of such matrices requires $2n^3$ FLOPs and $Rn^2$ bytes to exchange between the core and memory (here and below R designates the number of bits required to store each entry of a vector/matrix). So when matrices are large (such as during the pre-fill phase when the prompt is processed) the GPU is compute-bound, while when n is small (such as during generation) the GPU becomes memory-bound. To achieve this goal one needs to reduce R by quantizing both weights and the KV cache.\n\nThe third goal of PTQ is to accelerate inference of giant LLMs that require hosting each layer on a separate GPU (pipelining parallelism). For this goal one needs to quantize activations passed from one layer to the next to reduce the communication bottleneck.\n\nWhile quantization of weights to R = 3, 4 and even R = 2 bits has been achieved with minimal loss of quality, quantiza-"}, {"title": "Prior work", "content": "We briefly survey prior work, which we separate into work by information theorists and by the ML community."}, {"title": "Information-theoretic quantization", "content": "Rate R quantization of an information source X in $R^n$ is the operation of encoding it to nR bits, from which a decoder can produce a reconstruction $\\hat{X} \\in R^n$ that has small distortion with respect to X. The most popular distortion criterion is the quadratic loss, where the expected distortion is defined as $D = E||X \u2013 \\hat{X}||^2$, and here we restrict attention to this loss. Characterization of the optimal tradeoff between R and D is a classic topic in information theory, e.g. [2, Part V]."}, {"title": "LLM quantization", "content": "One of the directions of prior research on LLM quantization is addressing the issue of activation outliers that hinder the quantization quality. These outliers are present in certain dimensions of activations, weights and KV cache. In LLM.int8() of [12], these outlier dimension are kept unquantized. In SmoothQuant [13] authors balance the scale of outliers between weights and activations by modifying LayerNorm's diagonal matrices.\n\nGoing to random rotations, by rewriting matrix product $AB = (AU)(U^T B)$ for an orthogonal matrix U, one gets matrices with much more Gaussian entries (few outliers) and can apply standard quantization algorithms. Some of the multiplications by U can be merged with the weights (i.e. do not require additional runtime FLOPs), while the rest are applied at runtime. For the latter, matrices U should have structure to enable fast multiplication. For example, QuaRot [14] uses randomized Hadamard matrices as coordinate transformations, which can be applied to a vector of size n in O(n log n) additions. SpinQuant [15] uses a rotation parametrization with four orthogonal matrices $R_1, R_2, R_3, R_4$, where $R_1$ and $R_2$ can be arbitrary orthonormal matrices, and $R_3$ and $R_4$ should have a fast multiplication algorithm. The authors use Cayley SGD [16] to optimize $R_1$ and $R_2$ for minimization of the quantization error, while the matrices $R_3$ and $R_4$ are chosen to be random Hadamard."}, {"title": "Outline of NestQuant approach", "content": "In this section we outline the main components of our approach. A detailed description is brought in the following section.\n\nWhen designing a quantizer, one needs to make some assumptions about the distribution of the source that will be fed to it. While weights (and sometimes activations) can be well-approximated by Gaussians, their magnitude are wildly varied. Thus, one employs two ideas: normalization and random rotation.\n\nNormalization: In most of the literature, the normalization is done by taking an input vector of large dimension n (e.g. n = 4096 for Llama-3), dividing by the $L_\\infty$ norm to get entries to be in [-1,1] and then applying uniform quantization. This is suboptimal for two reasons: one is that uniform quantization induces error that is distributed uniformly on the small cube, which is suboptimal from the MSE point of view. Second reason, much more serious, is known as the shaping gain and demonstrated on Fig. 2. When entries of the vector are Gaussian, it will typically lie inside the black circle. Thus those grid elements outside of it will almost never be used, wasting bitspace.\n\nInstead, we use normalization by the $L_2$-norm (see Algorithm 3) and then use points inside the Voronoi region of a Gosset lattice, which as Fig. 2 (right) demonstrates wastes a lot fewer bitstrings for rare vectors, thus allowing us to use finer grids.\n\nRandom rotation: When input to the quantizer significantly differs from the presumed model (of iid Gaussians), performance can become quite poor. As discussed previously, multiplying by a random orthoghonal matrix U provides an excellent fix. Specifically, UX vector becomes uniform on the n-sphere of radius $\\sqrt{n}$, and small chunks of this vector have distribution very close to Gaussian iid. In particular, the total variation between any subset of d coordinates and N(0, Ia) is $O(d^2/n)$ [4], such that for $d = o(\\sqrt{n})$ what we quantize is effectively iid Gaussian.\n\nComplexity of lattice quantization: In order to explain our choice of nested lattice quantizer, we need to carefully balance several requirements. One of the key ones is complexity. It is known that finding (even approximating) a nearest lattice point is a well-known cryptographic assumption [22]. Thus, we are not suggesting to directly operate on n-dimensional lattices. Instead, we partition the n-vector into sections, each of dimension d and apply lattice quantization to d-subvectors. Equivalently, our vector quantizers for $R^n$ are constructed as Cartesian products of vector quantizers of small dimension d (we will take d = 8 for all experiments).\n\nGranular and overload quantization errors: There are two different sources of errors for lattice quantizers. The first is called granular quantization error, and is related to the second moment of the lattice Voronoi region. A common way to measure the"}, {"title": "Detailed Method", "content": ""}, {"title": "Nested lattice codebook", "content": "In this section, we describe the construction for a Vector Quantization (VQ) codebook of size $q^d$ for quantizing an d-dimensional vector, where q is an integer parameter. To quantize a vector, we find the closest codebook element by Euclidean norm. We describe efficient encoding and decoding algorithms to a quantized representation in $Z_q^d$.\n\nLet A be a lattice in $R^d$ with generator matrix G. We define the coordinates of a point $x \\in A$ to be an integer vector v such that x = Gv. Each point $P\\in A$ has a corresponding Voronoi region $V_A(P)$, for which P is the closest point in A with respect to $L^2$ metric. To define the codebook, we consider the scaled lattice qA. Then:\n\nDefinition 4.1. x \u2208 A belongs to codebook C iff $x \\in V_{qA}(0)$. Let v be the coordinates of x. Then, the quantized representation of x is $Q(x) := v \\mod q$. Note that $Q$ is a bijection between C and $Z_q^d$\n\nUsing this representation, we can describe the encoding and decoding functions, assuming the point x we are quantizing is in $V_{qA}(0)$. We will also need an oracle $Q_A(x)$, which maps x to the closest point in A to x."}, {"title": "Matrix quantization", "content": "When quantizing a matrix, we normalize its rows, and quantize each block of d entries using the codebook. The algorithm 3 describes the quantization procedure for each row of the matrix."}, {"title": "LLM quantization", "content": "Recall that we apply a rotation matrix H to every weight-activation pair of a linear layer without changing the output of the network. Let n be the number of input features to the layer.\n\n\u2022 If n = $2^k$, we set H to be Hadamard matrix obtained by Sylvester's construction\n\n\u2022 Otherwise, we decompose n = $2^k m$, such that m is small and there exists a Hadamard matrix $H_1$ of size m. We construct Hadamard matrix $H_2$ of size $2^k$ using Sylvester's construction, and set U = $H_1 \\otimes H_2$.\n\nNote that it's possible to multiply an r \u00d7 n matrix by H in O(rn log n) in the first case and O(rn(log n + m)) in the second case, which is negligible to other computational costs and can be done online.\n\nIn NestQuant, we quantize all weights, activations, keys, and values using Algorithm 3. We merge the Hadamard rotation with the weights and quantize them. We also apply the Hadamard rotation and quantization to the activations before linear layers. We"}, {"title": "Optimal scaling coefficients", "content": "One of the important parts of the algorithm is finding the optimal set of $\u03b2_i$. Given the distribution of 8-vectors that are quantized via a codebook, it is possible to find an optimal set of given size exactly using a dynamic programming approach, which is described in Appendix D."}, {"title": "Algorithm summary", "content": "Here we describe the main steps of NestQuant.\n\n1. Collect the statistics for LDLQ. For each linear layer with in-dimension d, we compute a d \u00d7 d matrix H.\n\n2. We choose an initial set of scaling coefficients $\u03b2$, and for each weight we simulate LDLQ quantization with these coefficients, getting a set of 8-dimensional vectors to quantize.\n\n3. We run a dynamic programming algorithm described in Appendix D on the 8-vectors to find the optimal \u03b2-values for each weight matrix.\n\n4. We also run the dynamic programming algorithm for activations, keys, and values for each layer. To get the distribution of 8-vectors, we run the model on a small set of examples.\n\n5. We quantize the weights using LDLQ and precomputed \u03b2.\n\n6. During inference, we quantize any activation before it's passed to the linear layer, and any KV cache entry before it is saved.\n\nNote the complete lack of fine-tuning needed to make our method work."}, {"title": "Experiments", "content": ""}, {"title": "Simulated Data", "content": "We compared the mean $L_2$ loss per entry of SpinQuant to the uniform $L_\\infty$-scaling quantizer (used in SpinQuant and other methods). The mean $L_2$ loss per entry for the product of two matrices $A \\in R^{n \\times k}$, $B \\in R^{m \\times k}$ is computed as $\\frac{||AB^T-\\hat{A}\\hat{B}^T||_2}{nm}$. We set n = k = m = 4096 and sampled two matrices A, B with unit normal distribution $A_{ij}, B_{ij} \\sim N(0, 1)$. We compare to the lower bound from (1).\n\nFor NestQuant, we do a grid search over (q, k). For given q and k, we find the best subset in { $\\beta_i$ for m = 1, 2, . . ., 50} of scaling coefficients $\u03b2$ of size k using the algorithm from Appendix D. Then we calculate the expected bits per entry computed as $\\log_2q + \\sum_{i=1}^k P(\u03b2_i) \\log_2 p(\u03b2_i)$ where $p(\u03b2_i)$ is the probability that the i'th beta is used in quantization. In Figure 3, we plot the efficient frontier of bits per entry vs root mean $L_2$ loss."}, {"title": "Llama results", "content": "We quantize Llama-3-8B model [26] using different values of q. We choose the number of scaling coefficients (k) to be equal to 4, the Section 5.5 explains the rationale behind this choice. More details on the hyperparameter choice of the experiments are in Appendix E. For each experiment, we compute the number of bits per entry similar to Section 5.1, but for the setup of compressed \u03b2 indices, we run the zstd compression algorithm instead of using the entropy of the distribution. As our evaluation metric, we use the perplexity on the validation split of wikitext2 with context length 2048.\n\nWe also perform the evaluation of NestQuant on various zero-shot benchmarks: ARC-Easy and ARC-Challenge [27], Hellaswag [28], [29], and Winogrande [30]. The results on 4-bit models with comparisons to other models are summarized in Table 1."}, {"title": "Comparison to other methods", "content": "We compare our method to the current state-of-the-art method SpinQuant. On the WikiText2 dataset, we computed the perplexity scores of the quantized models on context size 2048. Our method demonstrates superior perplexity scores by a high margin. On W4KV4A4 (4-bit weights, KV-cache, and activations) quantization of Llama 3-8B we achieve a perplexity score of 6.6 compared to the reported score of 7.3 in SpinQuant (See table 1). Impressively, our method outperforms SpinQuant, without the need of learned rotations. Even without LDLQ, we achieve a perplexity score of 6.8, which is still better than SpinQuant."}, {"title": "Results for other models", "content": "Here, we show the results of NestQuant on the newer 1B parameter model LLama3.2-1B. We do experiments in the same setups as for the Llama-3-8B model, computing the wikitext2 perplexity."}, {"title": "The choice of k", "content": "The value of k, i.e. the number of scaling coefficients is an important hyperparameter of the algorithm. With an increase of k, we decrease the quantization error by allowing each vector to be quantized to the lattice point with a proper scaling. However, it increases the bitrate and makes the encoding slower, since we need to try a larger number of scaling coefficients.\n\nWe used k = 3, 4, 5, 8 to quantize Llama-3-8B across different values of q, plotting the resulting perplexity against bitrate in Figure 5. We can see that using k = 3 leads to a suboptimal performance of the quantization scheme, while the performances of k = 4, 5, 8 are comparable. In our experiments, we use k = 4, because lower having k results in faster encoding.\n\nMore ablation studies for LDLQ and the choice of rotation are described in Appendix F."}, {"title": "Impact statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "Gosset oracle", "content": "In this section, we discuss the algorithm for finding the nearest neighbour in $E_8$ lattice and estimate its performance in FLOPs (Floating Point Operations). We note that $E_8 = D_8 \\cup D_8 + \\frac{1}{2}$, where $D_8$ contains vectors in $Z^8$ with even sum of coordinates. To compute $V_{E_8}(x)$, we compute two candidate points: $x_1 = V_{D_8}(x)$ and $x_2 = V_{D_8+\\frac{1}{2}}(x)$, and choose the one that has smaller $L^2$ distance to x.\n\nTo get $V_{D_8}(x)$, we can round each coordinate to the nearest integer. If the sum of rounded coordinates is odd, we need to \"flip\" the rounding direction of the coordinate for which the flip would cost the least. Note that finding the closest point in $V_{D_8+\\frac{1}{2}}$ works the same, but the rounding grid now contains half-integers, not integers.\n\nIn algorithm 5, we first round our vector down (getting d) and compute the mask (g) of whether it's optimal to round up for $D_8$. We note that the optimal rounding for $D_8 + \\frac{1}{2}$ is d + 0.5, while the optimal rounding for $D_8$ is d + g.\n\nWe want to understand whether rounding to $D_8$ or $D_8 + \\frac{1}{2}$ is better. Let $dist_i$ be the absolute distance from the i-th entry $x_i \\in [d_i, d_i + 1]$ to the middle of this integer segment $d_i + 0.5 = x_{2,i}$. We note that the contribution of this point to the MSE for $D_8$ is $(0.5 - dist_i)^2$, while for $D_8 + \\frac{1}{2}$ is $dist_i^2$. The difference is: $0.25 - dist_i + dist_i^2 - dist_i^2 = 0.25 - dist_i$. If the sum of this value over i is negative (i.e. $\\sum dist_i > 2$), it's optimal to quantize to $D_8$, otherwise to $D_8 + \\frac{1}{2}$. In pseudocode, we store $\\sum dist_i$ as $\u0394$\n\nWe note that we should check the constraint that the sum of coordinates in $D_8$ is even, and if it is not, \"flip\" one of the rounding directions. The optimal coordinate to flip can be determined through $dist_i$, and the new value of flipped coordinate - through g. We also need to update $\u0394$ given that the MSE difference changes.\n\nThe full pseudocode of the algorithm is in Algorithm 5."}, {"title": "Crude estimate of runtime of quantized matrix", "content": "We give an estimate of how the NestQuant runtime affects the overall efficiency of the model. We note that the runtime of encoding and decoding one 8-vector with one scaled codebook is dominated by running the oracle for the Gosset lattice, described in Algorithm 5. By counting the arithmetic operations and comparisons equally, we get that one run of the oracle takes approximately 15 operations per vector entry. We should also add no more than 3 arithmetic operations in Algorithms 1 and 2, and multiplication by the generator matrix G, or its inverse $G^{-1}$. Given the structure of the matrices, we can spend less operations on multiplying them by vectors using prefix sum technique for $G^{-1}$ and ignoring zeros for G. So, the multiplication takes around 2 operations per vector entry. In total, we do 20 FLOPs per dequantizing one matrix entry, and 20k FLOPs for quantization, since we need to run the algorithm for k values of $\u03b2$. We note that in weight quantization the encoding time is not important since it's done offline, and in KV cache quantization encoding happens one time per token, while decoding happens with any additional query to the KV cache.\n\nRecall that in generation phase the LLM runtime is memory-bound, i.e. getting the weights from memory takes significantly more time than computing the matrix-vector product. Let's compute the total running time of loading matrix of size m \u00d7 n from memory, which can be decomposed into the following running times:\n\n1. Loading the matrix from memory $\\frac{mnRT_{load}}{8}$ seconds, where R is the rate (bits/entry) and $T_{load}$ is the time to load a byte (in seconds)\n\n2. Decoding the matrix $N_{decode} \\cdot mn \\cdot T_{flop}$ where $N_{decode}$ is the number of operations per entry and $T_{flop}$ is the time to apply a floating point operation to a byte.\n\nIn total $T_{total} = mn(\\frac{RT_{load}}{8} + T_{flop}N_{decode})$ compared to mn $2T_{load}$ (assume 16-bit baseline), so the speedup is $\\frac{2T_{load}}{\\frac{RT_{load}}{8} + T_{flop}N_{decode}}$."}, {"title": "Dynamic programming for optimal \u03b2", "content": "Recall that instead of using one instance of lattice codebook C, we use a union of codebooks C scaled to different coefficients. Specifically, our final codebook C is parameterized by coefficients $\u03b2_1 < \u03b2_2 \u2264 ... \u2264 \u03b2_k$, and is equal to:\n\n$C = \u03b2_1C \\cup \u03b2_2C \\cup... \\cup \u03b2_kC$\n\nGiven a set of 8-vectors to quantize, we can find the set of \u03b2 that minimizes reconstruction error using a dynamic programming algorithm, which is described in Appendix D.\n\nWhen quantizing a vector to the i-th scaled codebook, we could either get a small granular error when the vector is in $V_{\u03b2_iA}(0)$, or a large overload error otherwise. If we use a codebook with smaller $\u03b2$, we have larger chance of overload error, but the expected magnitude of granular error is smaller due to the volume of Voronoi region being smaller (Figure 7). We can have two strategies for encoding:\n\n1. First-\u03b2: Use the smallest $\u03b2$, which does not result in an overflow error.\n\n2. Opt-\u03b2: Try all the values of $\u03b2$, and choose the one that has the smallest reconstruction MSE.\n\nEven though Opt-\u03b2 should provide smaller error, the definition of First-\u03b2 will be useful for us. We can note that the difference between error for Opt-\u03b2 and First-\u03b2 is not very siginificant (Table D). Moreover, First-\u03b2 can be used to determine the optimal set of $\u03b2_i$ to use."}, {"title": "Llama experiment details", "content": "We choose the train split of the Wikitext2 [31] dataset as a calibration dataset for computing H, and evaluate the model on the vali-dation split, computing the perplexity metric. For step 2 in the algorithm (Section 4.5), we select $\u03b2$ = [3.5, 4.5, 6.0, 14.5, 25.0]/q, because it is the $\u03b2$ we get when optimizing them for weight quantization without consideration of LDLQ. The overall universe of betas contains values from 1 to 40 with spacing ranging from 0.25 to 2. For running DP on activations, keys, and values, we run the model on a batch of 6 full-length sequences, which is sufficient for this low-dimensional hyperparameter.\n\nWhen choosing maximum beta for given distribution, we add a margin of $\\frac{3.0}{q}$ for weights and $\\frac{4.0}{q}$ to the maximum beta needed to have 0 overload errors on known data to account for potential overload errors in unknown data. While small number of overload error does not affect perplxity significantly, we still aim to minimize their probability.\n\nWhen computing perplexity for Wikitext2 with given context length, we average the perplexities for all the positions, which is standard for other works in quantization of LLMs."}, {"title": "Ablation studies", "content": "We found LDLQ to be useful in improving the quality of quantized model. In table 5, we compare the wikitext2 perplexity of models with and without LDLQ.\n\nWhile Hadamard matrices from Sylvester construction are commonly used in other works (QuIP#, Quarot), there are multiple ways to construct a fast rotation for the case when dimension is not a power of 2 (such as the down projection in MLP of Llama-3). We tested three possible options for rotation on q = 14, k = 4, W + KV + A quantization."}]}