{"title": "Can Hallucination Correction Improve Video-Language Alignment?", "authors": ["Lingjun Zhao", "Mingyang Xie", "Paola Cascante-Bonilla", "Hal Daum\u00e9 III", "Kwonjoon Lee"], "abstract": "Large Vision-Language Models often generate hallucinated content that is not grounded in its visual inputs. While prior work focuses on mitigating hallucinations, we instead explore leveraging hallucination correction as a training objective to improve video-language alignment. We introduce HACA, a self-training framework learning to correct hallucinations in descrip-tions that do not align with the video content. By identifying and correcting inconsistencies, HACA enhances the model's ability to align video and textual representations for spatio-temporal reasoning. Our experimental results show consistent gains in video-caption binding and text-to-video retrieval tasks, demonstrat-ing that hallucination correction-inspired tasks serve as an effective strategy for improving vi-sion and language alignment.", "sections": [{"title": "Introduction", "content": "Aligning representations across modalities involves creating joint embeddings that map visual and linguistic features to a shared space, enabling the model to assess their similarity. This is crucial for tasks including cross-modal retrieval (Xu et al., 2016a), mitigating hallucinations (Jiang et al., 2024), compositional reasoning (Cascante-Bonilla et al., 2024), visual-to-text generation (Li et al., 2022), visual question answering (Shen et al., 2022), and visual-language navigation (Zhao et al., 2021). While progress has been made in aligning image representations with text (Li et al., 2020; Zeng et al., 2022; Wang et al., 2023a; Lin et al., 2025), advancements in video-language alignment remain limited. Videos pose unique challenges due to their rich spatio-temporal information, involving multiple entities and scenes that dynamically interact and change over time. Video-language"}, {"title": "Related Work", "content": "In addition to the video-language alignment ap-proaches discussed in \u00a7 1, several methods leverage a contrastive learning objective to learn a shared video-language embedding space (Xue et al., 2023; Rasheed et al., 2023; Girdhar et al., 2023; Zhu et al., 2024), and Bagad et al. (2023) further introduces a contrastive loss in Video-LLMs to enforce time-order consistency. However, most of these models lack robustness to semantically plausible manipula-tions (Park et al., 2022). Yuksekgonul et al. (2023) also finds that applying a contrastive objective to video-caption datasets does not promote the model's ability to capture fine-grained details. In contrast, our approach encourages Video-LLMs to capture more nuanced semantic mismatches by learning to correct hallucinations, extending beyond sentence-level hallucination detection. More discussion is provided in Appendix B."}, {"title": "HACA: Hallucination Correction for Video-language Alignment", "content": "To investigate whether hallucination correction can improve video-language alignment, we introduce HACA, a fine-tuning objective for Video-LLM as a sequence-to-sequence generation task."}, {"title": "Preliminaries: Video-LLMs", "content": "Video-LLMs typically consist of three parts: i) a visual encoder to map images and videos to visual representations; ii) an LLM that takes text instruc-tions as inputs to generate text responses; and iii) an adapter between visual and text representations. Our approach finetunes (ii) the text decoder and (iii) the adapter, freezing (i) the visual encoder.\nPre-training. A Video-LLM Me parameterized by @ takes a textual question or instruction Q, a video V as input, and generates a text response A = (A1, A2, \u2026\u2026\u2026, AT) autoregressively using a decoder-based language model (LLM) as output, by esti-mating a conditional distribution M(A | Q, V). This is achieved by training the model using the maximum-likelihood estimation (MLE) objective:\nL(0) = \u2211\u2211log \u03a3\u03a3 Me (At | A<t, Q, V)\nDtrain t=1\nwhere At is t-th word of the text response, and A<t are the first t - 1 words of the response. The dataset Dtrain consists of samples in the form (Q, A, V).\nFine-tuning with entailment. Following Bansal et al. (2024), we finetune the Video-LLM using an entailment task, where the text input Q is for-matted as an entailment question as Q(W) = Does this caption accurately describe the video? Caption: {W}. In this task, the output of the model A is Yes or No (Figure 2 (a)). Given a dataset Dtrain consisting of ground-truth answers A for Q(W) and V, the model is fine-tuned to have a better estimation of Me(Yes | Q(W), V) and Me (No | Q(W), V) using the MLE objective:\nLent (0) = \u2211 log Mg(A | Q(W), V)\nDtrain"}, {"title": "Learning from Hallucination Correction", "content": "Building on the work of Bansal et al. (2024), the Video-LLM takes the question Q and the video V as input to determine whether a text description W entails the video (similar to \u00a7 3.1). However, in our"}, {"title": "Masking Correction as Augmentation", "content": "We also incorporate a masking correction task as data augmentation (Figure 2 (c)), where an in-struction Q prompts the Video-LLM to make cor-rections to a masked caption W, teaching the model to generate a corrected caption that con-tains a sequence of words W = (W1, W2, ..., Wn) as its answer by estimating conditional probability M(\u0174 | Q(W), V). Specifically, Q is a func-tion that formats the text instruction as Q(W) = Please correct this caption to accurately describe the video. Caption: W, where W is masked from \u0174: W = (w\u2081, [MASK], ..., Wn), by randomly masking 45% of the content words in the ground truth video description \u0174.\nWe finetune the model using two objectives: the MLE objective to estimate the probability for mask-ing correction Mo(\u0174|Q(W), V), and the HACA objective (Eq 3). The model is tasked with provid-ing responses corresponding to different instruc-tions."}, {"title": "Experimental Setup", "content": "Data (detailed in \u00a7A.5). We train HACA using videos and their ground-truth and contrastive de-scriptions from VideoCon (Bansal et al., 2024), generating 115,536 (video, description, correction) triplets for training and 8,312 for validation, which is used for model selection. Synthetic contrast captions are also used to fine-tune the baseline en-tailment task with the same dataset sizes.\nWe evaluate our trained models on text-to-video retrieval using the temporally-challenging SSv2-Temporal (Sevilla-Lara et al., 2021) and action-intensive SSv2-Events (Bagad et al., 2023) datasets. Additionally, we evaluate our models on composi-tional ability over time using the VELOCITI bench-mark (Saravanan et al., 2024). Each video in the dataset includes a correct caption and an incorrect one.\nBaselines. (i) Pretrained Video-LLMs: we em-ploy two pre-trained models with different ar-chitectures, Video-LLaVA (Lin et al., 2023) and VideoChat2 (Li et al., 2023b). More details in \u00a7A.4. (ii) Entailment: we fine-tune the pretrained Video-LLMs using the entailment task described in \u00a73.1. More details about the implementation are in \u00a7A.1.\nEvaluation metrics. We report the accuracy on the VELOCITI benchmark as the proportion of examples in which the positive video-caption pair"}, {"title": "Analysis", "content": "Performance on text-to-video retrieval. HACA consistently outperforms both the pretrained model and the entailment fine-tuned model, as illustrated in Figure 3. This demonstrates HACA's abil-ity to effectively capture the rich temporal infor-mation present in videos. On the SSv2-Events dataset, while the entailment objective yields per-formance comparable to the pretrained Video-LLaVA, HACA achieves better results on this action-intensive dataset, despite being fine-tuned on the same amount of data. Additional compar-isons with other models are provided in \u00a7A.2.\nPerformance on video-language binding. Table 1 shows that, on average, both Video-LLaVA and VideoChat2 fine-tuned with the HACA objec-tive outperform the pre-trained models and those fine-tuned with the entailment objective. Masking correction further boosts performance through data augmentation. The Agent Coref test evaluates a model's ability to link events to specific agents, a misalignment type absent in the VideoCon dataset, where actions are always tied to one agent. Conse-quently, the pretrained Video-LLaVA outperforms its fine-tuned versions, with HACA marginally ex-ceeding the entailment baseline. The Chrono test measures a model's ability to detect reversed event order. While VideoCon includes such data, our results show that models fine-tuned on the entail-ment objective perform similarly to the pretrained model. Although HACA slightly underperforms the entailment objective, it excels on SSv2-Events, involving multiple events.\nHACA consistently outperforms baseline mod-els in all Action tests: Action Adv (replacing an action with one not in the video), Action Bind (re-placing an action within the same video), and Ac-tion Modif (replacing the manner with a plausible modifier). This highlights HACA's robust ability to distinguish actions in videos, requiring under-standing of complex spatio-temporal relationships between the video and its description. HACA also excels in Agent Iden and Agent Bind, showcasing its effectiveness in identifying and binding entities through the right relationship.\nQualitative examples. Figure 4 presents an ex-ample where HACA outperforms the entailment baseline on the VELOCITI dataset, and delivers ac-"}, {"title": "Conclusion", "content": "Video understanding through language is vital for applications like human-robot interaction and au-tonomous driving. We propose a novel approach to enhance video-language alignment by connecting it to the hallucination problem in visual-language models, paving the way for future advancements."}, {"title": "Limitations", "content": "Our proposed method assumes the availability of ground-truth video caption annotations for fine-tuning using hallucination correction. Additionally, the method assumes a clear separation between the parameters of the video representations and those of the language model, as we freeze the video en-coder parameters during fine-tuning to align video-language representations. Another limitation is that our approach has not been evaluated on long videos, due to the limitation of computational resources. We envision future work in this direction."}, {"title": "Implementation Details", "content": "Fine-tuning from pretrained models. We use the visual representations for video and language model embeddings pretrained from Video-LLMs to perform instruction fine-tuning using different objectives, including HACA (\u00a73.2), entailment (\u00a73.1), and masking correction (\u00a73.3). During finetuning, visual representations are frozen, and the embeddings from the visual-text adapter layers and LLM are learnable.\nHyperparameters and computation. For Video-LLaVA, we finetune our models for 3 epochs, using a learning rate of 2e-4 and AdamW optimizer. We also use a LoRA adapter (Hu et al., 2022) of rank 128 and alpha 256. Since we freeze the video encoder, the number of trainable parameters is significantly reduced to 241M for Video-LLaVA. The number of video frames processed per video is 8, with a batch size of 8, using 2 RTXA6000 GPUs, for a total of ~ 72 hours.\nFor VideoChat2, we finetune our models for 3 epochs, using a learning rate of 2e-5 and AdamW optimizer. We use a LoRA adapter (Hu et al., 2022) of rank 16 and alpha 32. We also freeze the visual encoders and reduce the number of trainable parameters to 193M. The number of video frames processed per video is 8, with a batch size of 2, using 1 RTXA6000 GPU, for around ~ 72 hours.\nTools. We implement our models with Pytorch 2.0.1, Huggingface Transformers 4.31.0, scikit-learn 1.2.2. We use SciPy 1.6.0 to find content words from ground truth video description by excluding words with part-of-speech tags: AUX, SYM, DET, PUNCT."}, {"title": "Ablation studies", "content": "Performance on text-to-video retrieval. In Table 3, HACA (\u00a73.2) demonstrates competitive per-formance on SSv2 downstream tasks, surpassing the pretrained model by up to 5.7 mAP points and outperforming the model fine-tuned with the entailment task by up to 2.0 mAP points. Masking correction augmentation typically enhances video-language alignment when jointly trained with HACA or the entailment task.\nEffect of different mask ratios. Table 4 shows the performance when jointly finetuning Video-LLaVA using HACA and masking correction task (\u00a73.3) with different masking ratio. The results indicate that using masking ratio of 45% achieves higher average accuracy.\nComparing HACA and natural language explanations. To assess the effectiveness of HACA as a finetuning task, we compare it against natural language explanations (NLE) generated by external natural language inference models (Bansal et al., 2024), used alongside the entailment task. We fine-tune Video-LLaVA with both the entailment and NLE training objectives and report the results in Table 5. Our findings show that HACA outperforms Video-LLaVA trained with entailment and NLE objectives, even without our proposed masking objective."}, {"title": "Additional Qualitative Analysis", "content": "Figure 5 shows additional success and failure cases of HACA and the other models we tested."}, {"title": "Pretrained Video-LLMs.", "content": "We use two pre-trained Video-LLMs with different model architectures.\nVideo-LLaVA. Video-LLaVA (Lin et al., 2023) consists of LanguageBind (Zhu et al., 2024) encoders for the visual inputs, a large language model (Team, 2023), visual projection layers and a word embedding layer. It is finetuned via visual instruction tuning with 665k image-text pairs from LLaVA 1.5 (Liu et al., 2023b) and a 100k video-text instruction set from Video-ChatGPT (Maaz et al., 2023). We use this model under their Apache License 2.0.\nVideoChat2. VideoChat2 (Li et al., 2023b) performs a progressive multi-modal training for three stages. In the first stage, it is trained to aling the visual encoder with a Querying Transformer (Q-Former) (Li et al., 2022) which acts as an information bottleneck between the image and textual encoders and distill relevant information to the textual context. The second stage connects the visual encoder with a pretrained LLM. In the third stage, finetunes the model via instruction tuning, using 5 different tasks including: captioning, conversations, visual question answering, reasoning and classification, with data coming from LLaVA (Liu et al., 2023b), VideoChat (Li et al., 2023a), VideoChatGPT (Maaz et al., 2023), COCO Captions (Lin et al., 2014), WebVid (Bain et al., 2021), YouCook (Das et al., 2013), OK-VQA (Marino et al., 2019), AOK-VQA (Schwenk et al., 2022), DocVQA (Mathew et al., 2021), CLEVR (Johnson et al., 2017), CLEVRER (Yi et al., 2020) and NExT-QA (Xiao et al., 2021) among others. We use this model under their MIT License."}, {"title": "Datasets", "content": "VideoCon. VideoCon is constructed by generating contrastive video captions and explanations for different subset of videos (Xu et al., 2016b; Wang et al., 2019; Hendricks et al., 2018). This dataset contains seven misaligned types that include replacement of objects, actions, attributes, counts and relations, and adds hallucinations (i.e. unrelated but plausible information). We use this dataset under their MIT License.\nVELOCITI. The duration of the video clips in the dataset is 10 seconds, and has dense text annotations on action and role descriptions. The perception-based tests require discriminating video-caption pairs"}, {"title": "Related Work", "content": "Alignment in Video-Language Models is fundamental for the logical integration of video and textual information. To align both modalities, prior work has focused on pre-training models with different objectives to capture the temporal dynamics in video. While these self-supervised correction objectives are highly effective during pre-training (Li et al., 2023c; Wang et al., 2022; Zhu et al., 2024; Ge et al., 2022), fine-tuning is typically required to adapt Video-LLMs to specific downstream tasks (Li et al., 2023a; Zhang et al., 2023; Bansal et al., 2024) (e.g., classification, retrieval, or question answering). However, these objectives rely on coarse-grained alignment labels and do not provide detailed feedback for resolving inconsistencies between video and language.\nHallucination Correction methods aim to mitigate the generation of content that does not align with the data a model was trained on, or the model describes content that does not exist in the provided input (Huang et al., 2024). Orthogonal to our proposed method, LURE (Zhou et al., 2024) uses statistical analysis to identify and rectify errors in generated descriptions, addressing co-occurrence, uncertainty, and positional factors via masking. In our work, we randomly mask the video description so that the model is required to output the corrected sentence, which is also conditioned in the input video via visual entailment. Yin et al. (2023); Wang et al. (2023b) uses external models and measures to correct hallucinations to be consistent with images or videos. Zhou et al. (2021); Liu et al. (2023a); Xiao et al. (2024); Zhao et al. (2024) create a synthetic dataset to train a specialized model to detect and correct hallucinations. Dale et al. (2022); Huang et al. (2024) shows promising results on correcting hallucinations without an external model for machine translation and image captioning. In our work, we investigate leveraging hallucination as a training objective to improve video-language alignment, by exploring the potential of using a video-LLM model itself to correct hallucinations through fine-tuning on a synthetic dataset."}]}