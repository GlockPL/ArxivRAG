{"title": "Towards Multilingual LLM Evaluation for Baltic and Nordic languages: A study on Lithuanian History", "authors": ["Yevhen Kostiuk", "Oxana Vitman", "\u0141ukasz Gaga\u0142a", "Artur Kiulian"], "abstract": "In this work, we evaluated Lithuanian and general history knowledge of multi-lingual Large Language Models (LLMs) on a multiple-choice question-answering task. The models were tested on a dataset of Lithuanian national and general history questions translated into Baltic, Nordic, and other languages (English, Ukrainian, Arabic) to assess the knowledge sharing from culturally and historically connected groups. We evaluated GPT-40, LLaMa3.1 8b and 70b, QWEN2.5 7b and 72b, Mistral Nemo 12b, LLaMa3 8b, Mistral 7b, LLaMa3.2 3b, and Nordic fine-tuned models (GPT-SW3 and LLaMa3 8b).\nOur results show that GPT-40 consistently outperformed all other models across language groups, with slightly better results for Baltic and Nordic languages. Larger open-source models like QWEN2.5 72b and LLaMa3.1 70b performed well but showed weaker alignment with Baltic languages. Smaller models (Mistral Nemo 12b, LLaMa3.2 3b, QWEN 7B, LLaMa3.1 8B, and LLaMa3 8b) demonstrated gaps with LT-related alignment with Baltic languages while performing better on Nordic and other languages. The Nordic fine-tuned models did not surpass multilingual models, indicating that shared cultural or historical context alone does not guarantee better performance.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) provide a functional framework for tackling various natural language processing (NLP) tasks, such as question-answering , machine translation and so on. However, LLMs have shown less reliable results for low-resource languages due to the smaller fraction of available data in comparison to English and a few other widely spoken languages.\nBenchmarking multilingual LLMs across languages is essential for evaluating their capabilities. However, the availability of high-quality, culturally aligned datasets remains a challenge. This need for culturally aligned high-quality datasets becomes even more critical when evaluating historical knowledge, where ensuring linguistic and cultural fairness adds a layer of complexity.\nVerifying comparability of the results on historical knowledge QA datasets requires that a single set of historical events is queried in all languages. The choice of that set is likely to be biased to events more represented in widely spoken languages. Conversely, events that are more region- or cultural-specific are less likely to occur in the benchmarks. Finding and addressing these gaps is important to improve the fairness of LLMs and highlight historical and cultural biases.\nIn this work, we focus on evaluating multilingual LLMs on Lithuanian and general history. Our goal is to determine how LLMs perform on Lithuanian history exam questions when prompted in different languages and explore the alignment between languages and historical awareness, particularly within the Nordic and Baltic language groups.\nOur contribution is the following:\n\u2022 We automatically translated publicly available Lithuanian history exam question-answering dataset into Nordic (Danish, Finnish, and Swedish), Baltic (Estonian and Latvian), and other (Arabic, Ukrainian, and English) languages and partially manually evaluated it.\n\u2022 We tested GPT-40 ,"}, {"title": "Related Work", "content": "Pre-trained LLMs have exhibited a remarkable ability to encode and retrieve factual and common knowledge across different languages . However, there is a notable variation in model performance across languages, with a strong shift toward high-resource languages , particularly languages with Latin scripts .\nThe datasets used for benchmarking multilingual LLMs are created using either one of the two approaches: human annotation or translating existing annotated datasets using LLMs .\nAlthough datasets created by human annotators provide accurate translations and task-specific precision, they require considerable investment of both time and finances .\nOn the other hand, with an advancement of LLMs, the translation performance of automatic tools has been significantly boosted lately. For example, ChatGPT demonstrates fewer errors with the launch of the GPT-4 engine, even for distant languages . The quality control research on the DeepL translation tool found that DeepL\u00b9 performed well in terms of translation accuracy, fluency, and naturalness, reaching an overall semantic similarity score 94.13.\nThis improvement elevated the creation of benchmark datasets on various tasks. DeepL was used for creating the X-FACT multilingual factual knowledge dataset translated in 25 languages . In the research , five well-known datasets of various tasks were translated by DeepL into 21 European languages. LLMs with different numbers of parameters were evaluated on newly introduced datasets. The authors observed that models generally achieve higher performance on Romance and Germanic languages compared to Slavic languages.\nChatGPT was utilized to translate the 158K English instructions into 26 languages, including 7 low-resource languages . The data was used to instruction-tune LLM for multiple languages using reinforcement learning from human feedback. The resulting framework, Okapi, was also evaluated on datasets translated by ChatGPT from English into 26 selected languages.\nRelying on the aforementioned approaches, in our work we utilized ChatGPT and DeepL to create a dataset with special consideration on Baltic and Nordic languages. To ensure the quality of translation for each language, 100 samples of English version and it's corresponding translation were manually evaluated by native speakers. On average, 77% of such language pairs were approved.\nWe offer evaluation on Lithuanian history exam tasks, assessing the ability of multilingual LLMs to exhibit particular cultural and historical knowledge and evaluating the alignment of this knowledge within regional language groups."}, {"title": "Methodology", "content": "In this paper, we investigate performance consistency of LLMs within Nordic and Baltic language groups on the Lithuanian history exams questions. We hypothesized that the LLMs perform better in this domain, when presented with questions in languages from Nordic and Baltic groups than from other due to the cultural, linguistic and historical similarities."}, {"title": "Results and Discussion", "content": "For each LLM, we grouped its results per language group into Nordic (Danish, Finnish, and Swedish), Baltic (Lithuanian, Latvian, Estonian), and multilingual (Ukrainian, English, and Arabic). The accuracy scores per language and averages scores per language group are presented in Tables 1 and 2 and on Figures 3, 4, and 5.\nOur results demonstrated that all the models except for GPT-40 obtained better scores for general history questions rather than for LT-related ones. This observation is expected due to a biased training datasets for such models towards English-centric data.\nThe largest evaluated model, GPT-40, performed consistently better than other models for LT-related and general history questions in all language groups. The model achieved a maximum average score of 0.88 for LT-related history questions for the Baltic group (BLT) and performed similarly for Nordic languages (NRD) with a score of 0.87, though it showed slightly weaker performance in the multilingual group (MLT), scoring 0.84. These results suggest better knowledge representation for Nordic and Baltic language groups in LT-related history exams. Among the individual languages, English and Lithuanian were the best-performing languages for both LT-related and general history questions.\nThe 70b group of models (QWEN2.5 72b and LLaMa3.1 70b) demonstrated second best performance across all the types of questions. QWEN2.5 showed lower accuracy for Baltic languages on average, obtaining similar scores for MLT and NRD groups. Also, in both types of questions, QWEN2.5 showed similar trends of receiving lower scores in Estonian and Latvian, but higher scores for Nordic languages. Additionally, its performance was better in general questions across all languages, but when it comes to the alignment with LT-related, model was able to output better results for English, Swedish, and Danish rather than for Lithuanian or Baltic languages. In contrast, LLaMa3.1 70b did not performed at par with diff language groups. The results are similar for all languages in all questions with Arabic being the weakest and Lithuanian with English slightly stronger than others.\nIn case of Mistral Nemo 12b, the model scored the smallest scores comparing to other, even smaller (7-8b, 3b) models. It showed similar results across all language groups, obtaining the same average accuracy scores (36%) on Baltic and Nordic group on LT-related questions and a better performance for Nordic group on general questions than for Baltic. The average of MLT group was better, even though neither score was higher than 64%.\nLLaMa3 8b, QWEN2.5 7b, and LLaMa3.1 8b demonstrated a weaker performance when tested on BLT group across all questions. Using Lithuanian showed a better results. Similarly, Swedish and Danish helped QWEN2.5 7b obtain a bet-"}, {"title": "Conclusion", "content": "This study evaluated the performance of Large Language Models (LLMs) on Lithuanian historical multiple-choice question-answering tasks, focusing on Baltic, Nordic, and other language groups. The models were evaluated on the Lithuanian national history related (LT-related) questions and a general history questions.\nOur findings showed that GPT-40 consistently outperformed all other tested models across languages, achieving the highest scores for LT-related and general history questions, with slightly better results for Baltic and Nordic languages. Among open-source models, larger models QWEN2.5 72b and LLaMa3.1 70b performed well but did not match GPT-40, especially in Baltic languages. Smaller models, including Mistral Nemo 12b, LLaMa3.2 3b, QWEN 7B, LLaMa3.1 8B, and LLaMa3 8b demonstrated weaker results with Baltic languages, including Lithuanian, while performing better in Nordic and multilingual groups. Nordic fine-tuned models performed consistently across their supported languages but failed to surpass general multilingual models, even within their specialized domain. These findings highlight that shared cultural or historical context alone does not guarantee better model performance. To bridge these gaps, further efforts are needed to develop targeted datasets and fine-tuning strategies to improve LLM alignment with less-resourced languages like those in the Baltic language group."}]}