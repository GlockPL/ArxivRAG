{"title": "IFADAPTER: INSTANCE FEATURE CONTROL FOR GROUNDED TEXT-TO-IMAGE GENERATION", "authors": ["Yinwei Wu", "Xianpan Zhou", "Bing Ma", "Xuefeng Su", "Kai Ma", "Xinchao Wang"], "abstract": "While Text-to-Image (T2I) diffusion models excel at generating visually appealing images of individual instances, they struggle to accurately position and control the features generation of multiple instances. The Layout-to-Image (L2I) task was introduced to address the positioning challenges by incorporating bounding boxes as spatial control signals, but it still falls short in generating precise instance features. In response, we propose the Instance Feature Generation (IFG) task, which aims to ensure both positional accuracy and feature fidelity in generated instances. To address the IFG task, we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances feature depiction by incorporating additional appearance tokens and utilizing an Instance Semantic Map to align instance-level features with spatial locations. The IFAdapter guides the diffusion process as a plug-and-play module, making it adaptable to various community models. For evaluation, we contribute an IFG benchmark and develop a verification pipeline to objectively compare models' abilities to generate instances with accurate positioning and features. Experimental results demonstrate that IFAdapter outperforms other models in both quantitative and qualitative evaluations. Project Page: https://ifadapter.github.io/.", "sections": [{"title": "1 INTRODUCTION", "content": "The advent of diffusion models has revolutionized the field of Text-to-Image (T2I) synthesis (Ho et al., 2020; Podell et al., 2023; Baldridge et al., 2024; Betker et al., 2023; Rombach et al., 2022; Yang et al., 2023a). Despite their exceptional performance in generating high-quality images of single instances, these models remain limited in generating images containing multiple instances."}, {"title": "2 RELATED WORK", "content": "Layout-to-Image Generation. In the early stages, Layout-to-Image (Layout-to-Image) works primarily hinged on Generative Adversarial Networks (GANs) (Sun & Wu, 2019; 2021; Li et al., 2021; He et al., 2021; Wang et al., 2022; Sylvain et al., 2021). Novel modules and techniques have been proposed to address specific challenges in existing methods, such as object-to-object relations (He et al., 2021; Sylvain et al., 2021), object appearance (Sun & Wu, 2021; He et al., 2021), and handling interactions between bounding boxes (Sylvain et al., 2021; Li et al., 2021; Wang et al., 2022). Nevertheless, withthe rising tide of diffusion-based methods in the generative field, incorporating diffusion techniques into Layout-to-Image methods (Cheng et al., 2023; Zheng et al., 2023; Li et al., 2023; Wang et al., 2024c; Zhou et al., 2024b;a; Xie et al., 2023; Xiao et al., 2023; Chen et al., 2024; Yang et al., 2023b; Avrahami et al., 2023) has led to significant improvements in the quality, diversity, and controllability of generated images. LayoutDiffusion (Zheng et al., 2023) constructs a structural image patch with region information and transforms it into a unified form fused with the layout. LayoutDiffuse (Cheng et al., 2023) proposed an adapter based on layout attention and task-aware prompts. MIGC (Zhou et al., 2024b;a) utilizes an instance enhancement attention mechanism for precise shading. GLIGEN (Li et al., 2023) and InstanceDiffusion (Wang et al., 2024c) pioneer Layout-to-Image generation of open-sets by using the conceptual knowledge of SD pretrained models to inject object locations and appearance into features through custom attention. Some works explore the training-free method for Layout-to-Image generation by guiding the attention maps of the diffusion model during the generative process (Xiao et al., 2023; Chen et al., 2024) or implementing spatial constraints in the denoising process (Xie et al., 2023).\nControllable Diffusion Models. The emergence of diffusion models has significantly propelled advancements in the field of image generation. Controllable Diffusion Models utilize a wide variety of control conditions to generate images with specific content, leading to a proliferation of applications. Semantic control enables precise manipulation of image attributes or features in the generation process by referencing text (Rombach et al., 2022; Saharia et al., 2022b; Ramesh et al., 2022; Chen et al., 2023a) or images (Tang et al., 2023; Saharia et al., 2022a). Spatial control provides fine-grained control over the content in specific regions, such as segmentation-guided (Bar-Tal et al., 2023; Couairon et al., 2023; Wu et al., 2024a), sketch-guided (Voynov et al., 2023), and depth-guided methods (Kim et al., 2022). Recent efforts have concentrated on integrating these spatial control conditions into a unified framework for text-to-image generation, including approaches such as ControlNet (Zhang et al., 2023; Zhao et al., 2024), Composer (Huang et al., 2023), and Adapter-based (Mou et al., 2024) methods. ID and style control emphasize maintaining the consistency of user-specified identity or style in generated images, tuning-based methods guide diffusion models to generate the specified content by fine-tuning (Hu et al., 2021; Ruiz et al., 2023), while tuning-free methods (Ye et al., 2023; Huang et al., 2024; Wang et al., 2024b; Li et al., 2024; Hertz et al., 2024; Wang et al., 2024a) injecting coded condition embedding in the denoising process."}, {"title": "3 APPROACH", "content": ""}, {"title": "3.1 PRELIMINARIES", "content": "Latent Diffusion Model. Our method is applied over a pretrained T2I diffusion model, more specifically, a T2I latent diffusion model (LDM) (Rombach et al., 2022). The generation process of the LDM can be regarded as stepwise denoising from a initial Gaussian noise $z \\sim N(0, I)$, conditioned on a textual prompt y. The training objective is to minimize the following LDM loss:\n$L_{LDM} = E_{z\\sim N(0,I),y,t} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, E(y))||^2]$,\nwhere the $e_\\theta$ is parameterized as a UNet (Ronneberger et al., 2015) and t is the denoising timestep. E is a pretrained text encoder, used to encode y into text embeddings."}, {"title": "Cross Attention", "content": "In the LDM, text embeddings guide the direction of generation via cross attention (Vaswani, 2017), which can be represented using the following equation:\n$Attention(Q, K, V, M) = Softmax(\\frac{Q K^T}{\\sqrt{d}} + M)V$,\nthe Q represents the image latent code obtained after being projected through a MLP (Multi-Layer Perceptron), while K and V are similarly derived from text embeddings. M is a mask used to adjust attention scores, and d represents the dimensionality of the hidden vector, which helps stabilize the training process."}, {"title": "3.2 PROBLEM DEFINITION", "content": "In the Instance Feature Generation task, the LDM requires additional conditioning on a set of local descriptors c = {(r1,l1), . . ., (rn, ln)}. ri represents the designated generation position for the i-th instance, in [x, y, w, h] form. li is the corresponding phrase that describes the features of the i-th instance. Our method differs from others in that li incorporates detailed, extended descriptions of the instance, including aspects such as mixed colors, complex textures. With e serving as auxiliary conditions, the LDM should be able to generate instances with high fidelity in both position and features."}, {"title": "3.3 IFADAPTER", "content": "In this work, the IFAdapter is designed to control the generation of instance features and positions. We employ the open-source Stable Diffusion (SD) (Rombach et al., 2022; Podell et al., 2023) as the base model. To address the issue of instance feature loss, we introduce appearance tokens as a supplement to the high-frequency information, as discussed in Sec. 3.3.1. Furthermore, to incorporate a stronger spatial prior for more accurate control over position and features, we use appearance tokens to construct an instance semantic map that guides the generation process, as elaborated in Sec. 3.3.2."}, {"title": "3.3.1 APPEARANCE TOKENS", "content": "L2I SD enables the generation of grounded instances by incorporating local descriptions and location as additional conditions. Existing approaches (Li et al., 2023; Zhou et al., 2024b; Wang et al., 2024c) typically utilize the contextualized token (the End of Text, EoT token) produced by the pretrained CLIP text encoder (Radford et al., 2021) to guide the generation of instance features. Although the EoT token plays a crucial role in foreground generation, it is primarily used to generate coarse structural content (Wu et al., 2024b; Chen et al., 2024) and requires additional tokens to complement high-frequency details. As a result, existing L2I methods that discard all other tokens"}, {"title": "3.3.2 INSTANCE SEMANTIC MAP-GUIDED GENERATION", "content": "Along with ensuring the generation of detailed instance features, the IFG task also requires instances to be generated at designated locations. Previous method (Li et al., 2023) uses sequential grounding tokens as conditions, which lack robust spatial correspondence, potentially leading to issues such as feature misplacement and leakage. Therefore, in our work, we introduce a map called the Instance Semantic Map (ISM) as a stronger guiding signal. Since the generation of all instances is guided by the ISM, two major considerations must be addressed when constructing the map: (1) generating detailed and accurate features for each instance while avoiding feature leakage, and (2) managing overlapping regions where multiple instances are present. To address these concerns, we first generate each instance in isolation and then aggregate them in the overlapping regions. The following sections will provide a detailed explanation of these processes.\nPer-instance Feature Generation. Avoiding interference from extraneous features is crucial for the precise generation of high-quality instance details. To achieve this objective, we first generate the semantic map of each instance individually. Specifically, for the i-th instance, we transform its corresponding location ri into the following mask mi:\n$m_i(x, y) = \\begin{cases}0 & \\text{if } [x, y] \\in R_i \\\\ -\\infty & \\text{if } [x, y] \\notin R_i\\end{cases}$,\nwhere Ri represents the coordinates within the region indicated by ri. By employing Eq. 2, we can obtain the semantic map si for the i-th instance:\nsi = Attention(Hi, K, V, mi),\nwhere Hi represents the appearance tokens of i-th instance, as obtained from Eq. 3.\nGated Semantic Fusion. After obtaining the semantic maps for each instance, the next step is to blend these maps to derive the final ISM, as shown in Fig. 2 (b). A critical issue to consider during the map integration process is how to represent the features of a latent pixel when it is associated with multiple instances. Previous method (Jia et al., 2024) average the features of multiple"}, {"title": "3.4 LEARNING PROCEDURE", "content": "During training, we freeze the parameters of the SD, training only the IFAdapter. The loss function used for training is the LDM loss with instance-level condition incorporated:\n$L_{IFA} = E_{z\\sim N(0,1),y,t}[||\\epsilon - \\epsilon_{\\theta}(z_t, t, E(y)), c||^2]$\nTo enable our method to perform classifier-free guidance (CFG) (Ho & Salimans, 2022) during the inference phase, we randomly set the global condition y and local condition c to 0 during training."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 IMPLEMENTATION DETAILS", "content": "We described the basic setup for training our model. For more details, please refer to the appendix.\nTraining dataset. We use the COCO2014 (Lin et al., 2014) dataset and a 1 million subset from LAION 5B (Schuhmann et al., 2022) as our data sources. Following previous methods (Wang et al., 2024c; Zhou et al., 2024b), we utilize Grounding-DINO (Liu et al., 2023) and RAM (Zhang et al., 2024) to annotate the instance positions within the images. We then employ the state-of-the-art visual language models (VLMs) QWen (Bai et al., 2023) and InternVL (Chen et al., 2023b) to generate captions for the images and individual instance. Training details. We use SDXL (Podell et al., 2023), known for its strong detail generation capabilities, as our base model. The IFAdapter is applied to a subset of SDXL's mid-layers and decoder layers, which significantly contribute to foreground generation. We trained the IFAdapter using the AdamW (Loshchilov et al., 2017) optimizer with a learning rate of 0.0001 for 100,000 steps and a batch size of 160. During training, there was a 15% chance of dropping the local description and a 30% chance of dropping the global caption. For inference, we used the EulerDiscreteScheduler (Karras et al., 2022) with 30 sample steps and set the classifier-free guidance (CFG) scale to 7.5."}, {"title": "4.2 EXPERIMENTAL SETUP", "content": "Baselines. We compared our approach with previous SOTA L2I methods, including training-based methods InstanceDiffusion (Wang et al., 2024c), MIGC (Zhou et al., 2024b), and GLIGEN (Li et al., 2023), as well as the training-free methods DenseDiffusion (Kim et al., 2023) and MultiDiffusion (Bar-Tal et al., 2023).\nEvaluation dataset. Following the previous setup (Li et al., 2023; Zhou et al., 2024b; Wang et al., 2024c), we constructed the COCO IFG benchmark on the standard COCO2014 dataset. Specifically, we annotate the locations and local descriptions in the validation set using the same approach as in the training data. Each method is required to generate 1,000 images for validation.\nEvaluation Metrics. For the validation of the IFG task, it is imperative that the model generates instances with accurate features at the appropriate locations.\n\u2022 Instance Feature Success Rate. To verify spatial accuracy and description-instance consistency, we propose the Instance Feature Success (IFS) rate. The calculation of the IFS rate involves two steps. Step 1, Spatial accuracy verification: We begin by using Grounding-DINO to detect the positions of each instance. Next, we compute the Intersection over Union (IoU) between the detected positions and the Ground Truth (GT) positions, selecting the GT with the highest IoU as the corresponding match for that instance. If the highest IoU is less than 0.5, the instance generation is considered unsuccessful. Step 2, Local feature accuracy verification: Previous methods (Avrahami et al., 2023; Zhou et al., 2024b) primarily employ local CLIP for verifying local features. However, CLIP focuses on overall semantics and is not well-suited for capturing fine visual details (Yuksekgonul et al., 2023). Therefore, we utilize VLMs in conjunction with the prompt engineering technique to achieve more precise verification of local details. For each local region identified in Step 1, we prompt the VLMs to determine whether the content within the cropped region aligns with the corresponding description. If the VLM confirms that the content matches the prompt, the instance is marked as successful. The Instance Foreground Success (IFS) rate is then calculated as the ratio of successful instances to the total number of instances. Additionally, we report the Grounding-DINO Average Precision (AP) score to independently validate the positional accuracy of instance location generation.\n\u2022 Fr\u00e9chet Inception Distance (FID). FID (Heusel et al., 2017) measures image quality by calculating the feature similarity between generated and real images. We compute the FID using the validation set of COCO2017.\n\u2022 Global CLIP Score. The global caption of the image primarily describes the overall semantics of the image. Therefore, we use the CLIP score to evaluate Image-Caption Consistency."}, {"title": "4.3 COMPARISON", "content": ""}, {"title": "4.3.1 QUANTITATIVE ANALYSIS", "content": "Tab. 1 presents our qualitative results on the IFG benchmark, including metrics of IFS Rate, Spatial accuracy, and the Image Quality.\nIFS Rate. To calculate the IFS rate, we utilize three state-of-the-art (SOTA) vision-language models (VLMs): QWenVL (Bai et al., 2023), InternVL (Chen et al., 2023b), and CogVL (Wang et al., 2023). This multi-model approach ensures a more comprehensive and rigorous validation. As shown in Tab 1, our model outperforms the baseline models in all three IFS rate metrics. The introduction of appearance tokens and the incorporation of dense instance descriptions in training have significantly enhanced our model's ability to generate accurate instance details. It is worth noting that InstanceDiffusion achieves a higher IFS rate compared to other baselines. This is likely due to its training dataset also contains dense instance-level descriptions. This observation further underscores the necessity of high-quality instance-level annotations.\nSpatial Accuracy. As can be observed from Tab 1, IFAdapter achieves the best results in Grounding-DINO AP. This success can be attributed to our map-guided generation design, which incorporates additional spatial priors, leading to more accurate generation of instance locations."}, {"title": "4.3.2 QUALITATIVE ANALYSIS", "content": "In Fig. 1(a), we present generation results for a scene with multiple complex instances. We further evaluate the models' ability to generate instances with diverse features in Fig. 3. As shown, our method demonstrates the highest level of fidelity across various types of instance details."}, {"title": "4.4 USER STUDY", "content": "Although VLMs can verify instance details to a certain extent, a gap remains compared to human perception. Therefore, we invited professional annotators for further validation."}, {"title": "4.5 INTEGRATION WITH COMMUNITY MODELS", "content": "Thanks to the plug-and-play design of the IFAdapter, it can impose spatial control on pretrained diffusion models without significantly compromising the style or quality of the generated images. This capability enables the IFAdapter to be effectively integrated with various community diffusion models and LoRAs (Hu et al., 2021). As illustrated in Fig. 4, we applied IFAdapter to several community"}, {"title": "4.6 ABLATION STUDY", "content": "In our method, appearance tokens are introduced to address the shortcomings of EoT tokens in generating high-frequency details. This ablation study primarily explores the roles of these two token types in instance generation."}, {"title": "5 CONCLUSION", "content": "In this work, we propose IFAdapter to exert fine-grained, instance-level control on pretrained Stable Diffusion models. We enhance the model's ability to generate detailed instance features by introducing Appearance Tokens. By utilizing Appearance Tokens to construct an instance semantic map, we align instance-level features with spatial locations, thereby achieving robust spatial control. Both qualitative and quantitative results demonstrate that our method excels in generating detailed instance features. Furthermore, due to its plug-and-play nature, IFAdapter can be seamlessly integrated with community models as a plugin without the need for retraining."}]}