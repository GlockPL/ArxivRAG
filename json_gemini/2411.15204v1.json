{"title": "LABEL DISTRIBUTION SHIFT-AWARE PREDICTION REFINEMENT FOR TEST-TIME ADAPTATION", "authors": ["Minguk Jang", "Hye Won Chung"], "abstract": "Test-time adaptation (TTA) is an effective approach to mitigate performance degradation of trained\nmodels when encountering input distribution shifts at test time. However, existing TTA methods\noften suffer significant performance drops when facing additional class distribution shifts. We first\nanalyze TTA methods under label distribution shifts and identify the presence of class-wise confusion\npatterns commonly observed across different covariate shifts. Based on this observation, we introduce\nlabel Distribution shift-Aware prediction Refinement for Test-time adaptation (DART), a novel TTA\nmethod that refines the predictions by focusing on class-wise confusion patterns. DART trains a\nprediction refinement module during an intermediate time by exposing it to several batches with\ndiverse class distributions using the training dataset. This module is then used during test time to\ndetect and correct class distribution shifts, significantly improving pseudo-label accuracy for test\ndata. Our method exhibits 5-18% gains in accuracy under label distribution shifts on CIFAR-10C,\nwithout any performance degradation when there is no label distribution shift. Extensive experiments\non CIFAR, PACS, OfficeHome, and ImageNet benchmarks demonstrate DART's ability to correct\ninaccurate predictions caused by test-time distribution shifts. This improvement leads to enhanced\nperformance in existing TTA methods, making DART a valuable plug-in tool.", "sections": [{"title": "Introduction", "content": "Deep learning has achieved remarkable success across various tasks, including image classification [1, 2, 3] and natural\nlanguage processing [4, 5]. However, these models often suffer from significant performance degradation when there\nis a substantial shift between the training and test data distributions [6, 7, 8]. Test-time adaptation (TTA) methods\n[9, 10, 11, 12, 13] have emerged as a prominent solution to mitigate performance degradation due to distribution shifts.\nTTA methods allow trained models to adapt to the test domains using only unlabeled test data. In particular, an approach\nknown as BNAdapt [14, 15], which substitutes the batch statistics in the batch normalization (BN) layers of a trained\nclassifier with those from the test batch, has proven to be effective in adapting to input distribution shifts in scenarios\nsuch as image corruption. Consequently, numerous TTA methods [12, 13, 16, 17] are built upon the BNAdapt strategy.\nHowever, recent studies [17, 18, 19] have shown that the effectiveness of the BNAdapt strategy diminishes or even\nbecomes harmful when both the class distribution and the input distribution in the test domain shift at test time. This\nreduction in effectiveness is due to BNAdapt's reliance on batch-level data statistics, which are influenced not only by\nclass-conditional input distributions but also by the configuration of data within the test batches. For instance, when a\ntest batch predominantly contains samples from a few head classes, the batch statistics become biased towards these\nclasses. To address these limitations, some recent methods have been designed to lessen the dependency of test-time\nadaptation strategies on batch-level statistics and to tackle class distribution shifts with additional adjustments. For\ninstance, NOTE [18] employs instance-aware batch normalization, which diverges from traditional batch normalization,\nand uses a prediction-balanced memory to simulate a class-balanced test data stream. SAR [19] implements batch-\nagnostic normalization layers, such as group or layer norm, complemented by techniques designed to drive the model"}, {"title": "Label distribution shifts on T\u0422\u0410", "content": "TTA methods are extensively investigated to mitigate input data distribution shifts, yet the impact of label distribution\nshifts on these methods is less explored. In this section, we theoretically and experimentally analyze the impact of\nlabel distribution shifts on BNAdapt [14, 15], a foundational strategy for many TTA methods [12, 13, 16, 17]. We also\nintroduce a new metric designed to detect label distribution shifts at test time using only unlabeled test samples.\nImpact of label distribution shifts on TTA We begin with a toy example to explore the impact of test-time\ndistribution shifts on a classifier trained for a four-class Gaussian mixture distribution. We assume that the classifier\nis adapted at test time by a mean centering function, modeling the effect of batch normalization in the BNAdapt.\nConsider the input distribution for class i as $N(\\mu_i, \\sigma^2I_2)$, where $\\mu_1 = (d, \\beta d)$, $\\mu_2 = (-d, \\beta d)$, $\\mu_3 = (d, -\\beta d)$, and\n$\\mu_4 = (-d, -\\beta d)$ with $d \\in \\mathbb{R}^2$ and $\\beta > 1$. Assume uniform priors $\\mathbb{P}_{tr} = [1/4, 1/4, 1/4, 1/4]$ for training. At test\ntime, we consider shifts in both input and label distributions. Consider the covariate shift by $\\Delta \\in \\mathbb{R}^2$ as studied in\nprior works [23, 24]. This changes the input distribution for each class to $N(\\mu_i + \\Delta, \\sigma^2I_2)$. Additionally, let the\nclass distribution shift to $\\mathbb{P}_{te} = [p, 1/4, 1/4, 1/2 - p]$ for some $p \\in [1/4, 1/2)$. Let $h(\\cdot)$ denote a Bayes classifier for\nthe training distribution, and Norm($\\cdot$) represent a mean centering function, mimicking the batch normalization at test\ntime. When $p = 1/4$ (i.e., $\\mathbb{P}_{tr} = \\mathbb{P}_{te}$), the mean centering effectively mitigates the test-time input distribution shift,\nrestoring the original performance of $h(\\cdot)$. However, when $p > 1/4$ (i.e., $\\mathbb{P}_{tr} \\neq \\mathbb{P}_{te}$), the BN-adapted classifier, modeled\nby $h(\\cdot) := h($Norm($\\cdot$)), begins to exhibit performance degradation with a distinct class-wise confusion pattern. This\npattern reflects both the spatial distances between class means and the severity of the label distribution shifts:\n(#1) The misclassification probability from the major class (class 1) to a minor class is higher than the reverse:\n$\\mathbb{P}_{x \\sim N(\\mu_1 + \\Delta, \\sigma^2I_2)}[h(x) = i] > \\mathbb{P}_{x \\sim N(\\mu_i + \\Delta, \\sigma^2I_2)}[h(x) = 1], \\forall i \\neq 1$.\n(#2) The misclassification patterns are influenced by the spatial relationships between classes, e.g., the prob-\nability of misclassifying from class 1 to other classes is higher for those that are spatially closer:\n$\\mathbb{P}_{x \\sim N(\\mu_1 + \\Delta, \\sigma^2I_2)}[h(x) = 2] > \\mathbb{P}_{x \\sim N(\\mu_1 + \\Delta, \\sigma^2I_2)}[h(x) = 3] > \\mathbb{P}_{x \\sim N(\\mu_1 + \\Delta, \\sigma^2I_2)}[h(x) = 4]$."}, {"title": "Prediction refinement module for TTA", "content": "We introduce a prediction refinement module that can detect test-time class distribution shifts and modify the predictions\nof the trained classifiers to effectively reverse the class-wise confusion patterns.\nPrediction refinement module go Our core idea is that if the prediction refinement module gf experiences batches\nwith diverse class distributions before the test time, it can develop the ability to refine inaccurate predictions resulting\nfrom label distribution shifts. Based on this insight, we train 9 during the intermediate time, between the training and\ntesting times, by exposing it to several batches with diverse class distributions from the training datasets. The module go\ntakes as inputs the average pseudo-label distribution $\\mathbb{P}_B = \\frac{1}{|B|}\\sum_{x \\in B} \\text{softmax}(f_{\\bar{\\theta}}(x)) \\in \\mathbb{R}^K$ and prediction variance\n$d_B = \\frac{1}{|B|}\\sum_{(x)} \\mathbb{E}_{x\\in B}D(u, \\text{softmax}(f_{\\bar{\\theta}}(x))) \\in \\mathbb{R}$ for each batch, to detect the label distribution shifts from uniformity.\nThe module then outputs a square matrix $W_B \\in \\mathbb{R}^{K \\times K}$ and a vector $b_B \\in \\mathbb{R}^{K}$, which together transform the model's\nlogits $f_{\\bar{\\theta}}(x)$ for a sample $x \\in B$ to $f_{\\bar{\\theta}}(x)W_B +  b_B$.\nTraining of 9 During the intermediate time, we use the labeled training dataset $D$ as the intermediate dataset $D_{int}$,\nassuming that D is available while the test dataset $D_{test}$ remains unavailable, as is common in previous TTA settings\n[27, 28, 29]. For example, we use CIFAR-10 dataset during the intermediate time on CIFAR-10C-imb benchmark.\nTo create batches with diverse class distributions during the intermediate time, we employ a Dirichlet distribution\n[18, 30]. Batches sampled through i.i.d. sampling typically have class distributions resembling a uniform distribution.\nIn contrast, batches sampled using the Dirichlet distribution exhibit a wide range of class distributions, including\nlong-tailed distributions, as illustrated in Fig. 6 of Appendix A. If the original training dataset exhibits an imbalanced\nclass distribution, as seen in datasets like PACS and OfficeHome (Fig. 5), this imbalance can inadvertently influence the\nclass distributions within batches. To mitigate this, we create a class-balanced intermediate dataset $D_{int}$ by uniformly\nsampling data from each class.\nThe training objective of go for a Dirichlet-sampled batch $B_{Dir} \\subset D_{int}$ is formulated as follows:\n$\\mathcal{L}_{imb}(\\phi) = \\mathbb{E}_{(x,y)\\in B_{Dir}} [CE(y, \\text{softmax}(f_{\\bar{\\theta}}(x)W_{imb}(\\phi) + b_{imb}(\\phi)))]$\nfor $W_{imb}(\\phi), b_{imb}(\\phi) = g_{\\phi}(\\mathbb{P}_{imb},d_{imb})$ where $\\mathbb{P}_{imb}$ is the averaged pseudo label distribution, $d_{imb}$ is the prediction\ndeviation of the batch $B_{Dir}$, and CE denotes the cross-entropy loss. During the intermediate time, $g_{\\phi}$ is optimized to\nminimize the CE loss between the ground truth labels of the training samples and the modified softmax probability.\nDuring this time, the parameters of the trained classifier fe are not updated, but the batch statistics in the classifiers are\nupdated as in BNAdapt. Moreover, since go training is independent of the test domains, e.g. test corruption types, we\ntrain go only once for each classifier. Therefore, it requires negligible computation overhead (Appendix A).\nRegularization for go When there is no label distribution shift, there are no significant class-wise confusion patterns\nthat the module needs to reverse (as shown in Fig. 7). However, when using only equation 1, the module may become"}, {"title": "Experiments", "content": "Benchmarks We consider two types of input data distribution shifts: synthetic and natural. Synthetic distribution\nshifts are artificially created using data augmentation techniques, such as image corruption with Gaussian noise. In\ncontrast, natural distribution shifts arise from changes in image style, for instance, from artistic to photographic styles.\nWe evaluate synthetic distribution shifts on CIFAR-10/100C and ImageNet-C [26], and natural distribution shifts on\nCIFAR-10.1 [31], PACS [32], and OfficeHome [33] benchmarks. For synthetic distribution shifts, we apply 15 different\ntypes of common corruption, each at the highest severity level (i.e., level 5). To evaluate the impact of class distribution\nshifts, we use long-tailed distributions for CIFAR-10C (class imbalance ratio p) and online imbalanced distributions for\nCIFAR-100C and ImageNet-C (imbalance ratio IR). For datasets with a large number of classes, such as CIFAR-100C\nand ImageNet-C, the test batch size is often smaller than the number of classes, making the distribution of test batches\nsignificantly different from the assumed long-tailed distributions. Therefore, to evaluate the impact of class distribution\nshifts, we consider test batches modeled by online label distribution shifts, as described in Sec. 2 (with further details in\nSec. A.1). For PACS and OfficeHome, we use the original datasets, which inherently include natural label distribution\nshifts across domains (Fig. 5).\nBaselines Our method can be used as a plug-in for baseline methods that utilize test data predictions. We test the\nefficacy of DART combined with following baselines: (1) BNAdapt [15] corrects batch statistics using test data; (2)\nTENT [12] fine-tunes BN layer parameters to minimize the prediction entropy of test data; (3) PL [16] fine-tunes\nthe trained classifier using confident pseudo-labeled test samples; (4) NOTE [18] adapts classifiers while mitigating\nthe effects of non-i.i.d test data streams through instance-aware BN and prediction-balanced reservoir sampling; (5)\nDELTA [13] addresses incorrect BN statistics and prediction bias with test-time batch renormalization and dynamic\nonline reweighting; (6) ODS [17] estimates test data label distribution via Laplacian-regularized maximum likelihood\nestimation and adapts the model by weighting infrequent and frequent classes differently. (7) LAME [9] modifies\npredictions with Laplacian-regularized maximum likelihood estimation using nearest neighbor information in the\nclassifier's embedding space. (8) SAR [19] adapts models to lie in a flat region on the entropy loss surface. More details\nabout baselines are available in Appendix A."}, {"title": "Discussion and Conclusion", "content": "We proposed DART, a method to mitigate test-time class distribution shifts by accounting for class-wise confusion\npatterns. DART trains a prediction refinement module during an intermediate time to detect label distribution shifts\nand correct predictions of BN-adapted classifiers. Our experiments demonstrate DART's effectiveness across various\nbenchmarks, including synthetic and natural distribution shifts. DART's intermediate-time training incurs additional\ncosts compared to traditional TTA. However, these costs are relatively minimal, especially considering the significant\nperformance improvements it offers when combined with traditional TTA methods (Table 1). The low cost is partly\ndue to using the existing training dataset, rather than requiring an auxiliary dataset, and its short runtime. This paper\nhighlights and effectively addresses the negative impact of test-time label distribution shifts on existing TTA methods.\nAs label distribution shifts are common in real-world scenarios, we expect future studies to build on our approach to\nenhance real-world scalability by making models robust to test-time shifts in both input and label distributions.\nBroader Impact Statement\nThis paper highlights and effectively resolves the negative impact of test-time label distribution shifts to existing TTA\nmethods. Since the label distribution shifts between the training and test domains are common in the real world, we\nexpect further studies that are robust to test-time shifts on both input data and label distributions to be built on our work\nto enhance real-world scalability."}, {"title": "Appendix", "content": "A Implementation details\nA.1 Details about dataset\nWe consider two types of input data distribution shifts: synthetic and natural distribution shifts. The synthetic and\nnatural distribution shifts differ in their generation process. Synthetic distribution shift is artificially generated by data\naugmentation schemes including image corruption like Gaussian noise and glass blur. On the other hand, the natural\ndistribution shift occurs due to changes in image style transfer, for example, the domain is shifted from artistic to\nphotographic styles.\nFor the synthetic distribution shift, we test on CIFAR-10/100C and ImageNet-C, which is created by applying 15 types\nof common image corruptions (e.g. Gaussian noise and impulse noise) to the clean CIFAR-10/100 test and ImageNet\nvalidation datasets. We test on the highest severity (i.e., level-5). CIFAR-10/100C is composed of 10,000 generic\nimages of size 32 by 32 from 10/100 classes, respectively. ImageNet-C is composed of 50,000 generic images of size\n224 by 224 from 1,000 classes. The class distributions of the original CIFAR-10/100C and ImageNet-C are balanced.\nThus, we consider two types of new test datasets to change the label distributions between training and test domains.\nFirst, we consider CIFAR-10-LT, which has long-tailed class distributions, as described in Section 2. We set the number\nof images per class to decrease exponentially as the class index increases. Specifically, we set the number of samples\nfor class k as $n_k = n(1/\\rho)^{k/(K-1)}$, where n and p denote the number of samples of class 0 and the class imbalance\nratio, respectively. We also consider the test set of inversely long-tailed distribution in Figure 1. Specifically, we set the\nnumber of samples for class k as $n_k = n(1/\\rho)^{(K-1-k)/(K-1)}$, where n and p denote the number of samples of class\nK - 1 and the class imbalance ratio, respectively. However, unlike CIFAR-10-LT, each test batch of CIFAR-100C-LT\nand ImageNet-C-LT tend to not have imbalanced class distributions, since the test batch size (e.g., 32 or 64) is set to be\nsmaller than the number of classes (100 and 1,000). Thus, we also consider CIFAR-100C-imb and ImageNet-C-imb,\nwhose label distributions keep changing during test time, as described in Section 2. These datasets are composed of K\nsubsets, where K is the number of classes. We assume a class distribution of the k-th subset as $[p_1, p_2, ..., p_K]$, where\n$p_k = p_{max}$ and $p_i = p_{min} = (1 - p_{max})/(K - 1)$ for i \u2260 k. Let IR = $p_{max}/p_{min}$ represent the imbalance ratio. Each\nsubset consists of 100 samples from the CIFAR-100C and ImageNet-C test set based on the above class distribution.\nThus, the new test set for CIFAR-100C/ImageNet-C is composed of 10,000/100,000 samples, respectively. Additionally,\nwe shuffle the subsets to prevent predictions based on their order.\nFor the natural distribution shift, we test on CIFAR-10.1-LT, PACS, and OfficeHome benchmarks. CIFAR-10.1 [31] is\na newly collected test dataset for CIFAR-10 from the TinyImages dataset [42], and is known to exhibit a distribution\nshift from CIFAR-10 due to differences in data collection process and timing. Since the CIFAR-10.1 has a balanced\nclass distribution, we construct a test set having a long-tailed class distribution, named CIFAR-10.1-LT, similar to\nCIFAR-10C-LT. PACS benchmark consists of samples from seven classes including dogs and elephants in four domains:\nphoto, art, cartoon, and sketch. In PACS, we test the robustness of classifiers across 12 different scenarios, each using\nthe four domains as training and test domains, respectively. The OfficeHome [33] benchmark is one of the well-known\nlarge-scale domain adaptation benchmarks, comprising images from four domains (art, clipart, product, and the real\nworld). Each domain consists of images belonging to 65 different categories. The number of data samples per class\nranges from a minimum of 15 to a maximum of 99, with an average of 70. This ensures that the label distribution\ndifferences between domains are not substantial, as depicted in Figure 5. The data generation/collection process of\nPACS and OfficeHome benchmarks is different across domains, resulting in differently imbalanced class distribution, as\nillustrated in Figure 5.\nA.2 Details about pre-training\nWe use ResNet-26 for CIFAR datasets, and ResNet-50 for PACS, OfficeHome, and ImageNet benchmarks as backbone\nnetworks. We use publicly released trained models and codes for a fair comparison. Specifically, for CIFAR-10/100"}, {"title": "Detailed explanation about DART-split", "content": "We propose a variant of DART, named DART-split, for large-scale benchmarks, which splits the prediction refinement\nmodule go into two parts 941 and 942. In this setup, 961 takes the prediction deviation as an input to detect label\ndistribution shifts, while 962 generates affine transformations using the averaged pseudo label distribution. Specifically,\n961 takes the prediction deviation of each batch B as an input, and outputs a severity score sp = \u03c3(941 (dB)) ranging\nfrom 0 to 1, with higher values indicating more severe shifts. On the other hand, 962 uses the averaged pseudo label\ndistribution PB as an input to produce an affine transformation W\u00df and b\u00df, effectively correcting predictions affected by\nthe label distribution shifts. DART-split can address the scaling challenges faced by the original DART as the number\nof classes, K, increases.\nAt the intermediate time, g\u2081 and g2 are trained individually for detecting label distribution shifts and generating\naffine transformation, respectively, as detailed in Algorithm 3. 941 is trained to classify batches to either severe label\ndistribution shift (output close to 1) or no label distribution shift (output close to 0), taking the prediction deviation as\nan input. Specifically, g\u00f8\u2081 is trained to minimize BCE(0, \u03c3(941 (dbal))) + BCE(1,0(941 (dimb))), where o is a sigmoid\nfunction, BCE is the binary cross entropy loss, and dbal, dimb are the prediction deviation values of the batches i.i.d.\nsampled and Dirichlet-sampled, respectively. 941 is composed of a simple single layer, based on the observation in\nFig. 4 that the prediction deviation almost linearly decreases as the imbalance ratio increases. On the other hand, 942, is\ncomposed of a two-layer MLP and focuses on generating affine transformations solely from the averaged pseudo label\ndistribution, following the training objectives similar to those of the original DART's go.\nDuring testing, each test batch B is first evaluated by 941 to determine whether there is a severe label distribution shift\nor not. If sp = \u03c3(941 (dp)) exceeds 0.5, indicating a severe label distribution shift, the predictions for B are adjusted\nusing the affine transformations from 942, i.e., for a test data x \u2208 B, the prediction is modified from softmax(fo(x)) to\nsoftmax(fo(x) WB + b\u00df). Otherwise, we do not modify the prediction.\nFor ImageNet-C, the output dimension of 962 increases as the number of classes increases, becoming more challenging\nto learn and generate a higher-dimensional square matrix W and b for large-scale datasets. To address this challenge,\nwe modify the prediction refinement module to produce W with all off-diagonal entries set to 0 and fix all entries of b\nto 0 for ImageNet benchmarks. Moreover, to alleviate the discrepancy in the softmax prediction confidence between\nthe training and test datasets, we store the confidence of the pre-trained classifier's softmax probabilities and perform\nsoftmax temperature scaling to ensure that the prediction confidence of the pre-trained classifier are maintained for the\ntest dataset for each test corruption for ImageNet-C. Specifically, for the pre-trained classifier fe, training dataset D,\nand test dataset Dtest, we aim to find T that achieves the following:\n$\\mathbb{E}_{(x,\u00b7) \\sim \\mathcal{D}} [\\text{max}_k \\text{softmax}(f_{\\theta_0}(x))_k] = \\mathbb{E}_{\\hat{x} \\sim \\mathcal{D}_{test}} [\\text{max}_k \\text{softmax}(f_{\\theta_0}(\\hat{x})/T)_k]$.\nBy Taylor series approximation, we can re-write equation 5 as in [47]:\n$\\mathbb{E}_{(x,\u00b7) \\sim \\mathcal{D}} [\\frac{1+\\text{max}_k f_{\\theta_0}(x)_k}{\\sum_{k'} (1+f_{\\theta_0}(x)_{k'})}] = \\mathbb{E}_{\\hat{x} \\sim \\mathcal{D}_{test}} [\\frac{1+\\text{max}_k f_{\\theta_0}(\\hat{x})_k/T}{\\sum_{k'} (1+f_{\\theta_0}(\\hat{x})_{k'}/T)}]$.\nWith the assumption that the sum of logits, $\\sum_{k'} f_{\\theta_0}(\\cdot)_{k'}$, are constant $l_{tr,sum}$ and $l_{te,sum}$ for any training and test data,\nrespectively, we can re-write equation 6 and compute T as\n$\\frac{1+\\mathbb{E}_{\\hat{x} \\sim \\mathcal{D}_{test}} [\\text{max}_k f_{\\theta_0}(\\hat{x})_k/T]}{K+l_{te,sum}/T} = \\frac{1+\\mathbb{E}_{(x,\u00b7) \\sim \\mathcal{D}} [\\text{max}_k f_{\\theta_0}(x)_k]}{K+l_{tr,sum}}$.\nSolving above equation, we can obtain softmax temperature T as:\nT = \\frac{\\mathbb{E}_{\\hat{x} \\sim \\mathcal{D}_{test}} [\\text{max}_k f_{\\theta_0}(\\hat{x})_k] \\cdot l_{te,sum}}{K} \\frac{K+l_{tr,sum}}{1+\\mathbb{E}_{(x) \\sim \\mathcal{D}} [\\text{max}_k f_{\\theta_0}(x)_k]}.\nwhere K is the number of classes. We store the average of the maximum logits and the sum of logits of the pre-trained\nclassifier on the training data, and use them to calculate the softmax temperature T at test time. For each test batch, we"}]}