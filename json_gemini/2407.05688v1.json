{"title": "Learning with Alignments: Tackling the Inter- and Intra-domain Shifts for Cross-multidomain Facial Expression Recognition", "authors": ["Yuxiang Yang", "Yuanyuan Xu", "Lu Went", "Xi Wu", "Yan Wang", "Xinyi Zeng", "Jiliu Zhou"], "abstract": "Facial Expression Recognition (FER) holds significant importance in human-computer interactions. Existing cross-domain FER methods often transfer knowledge solely from a single labeled source domain to an unlabeled target domain, neglecting the comprehensive information across multiple sources. Nevertheless, cross-multidomain FER (CMFER) is very challenging for (i) the inherent inter-domain shifts across multiple domains and (ii) the intra-domain shifts stemming from the ambiguous expressions and low inter-class distinctions. In this paper, we propose a novel Learning with Alignments CMFER framework, named LA-CMFER, to handle both inter- and intra-domain shifts. Specifically, LA-CMFER is constructed with a global branch and a local branch to extract features from the full images and local subtle expressions, respectively. Based on this, LA-CMFER presents a dual-level inter-domain alignment method to force the model to prioritize hard-to-align samples in knowledge transfer at a sample level while gradually generating a well-clustered feature space with the guidance of class attributes at a cluster level, thus narrowing the inter-domain shifts. To address the intra-domain shifts, LA-CMFER introduces a multi-view intra-domain alignment method with a multi-view clustering consistency constraint where a prediction similarity matrix is built to pursue consistency between the global and local views, thus refining pseudo labels and eliminating latent noise. Extensive experiments on six benchmark datasets have validated the superiority of our LA-CMFER.", "sections": [{"title": "1 Introduction", "content": "Facial Expression Recognition (FER) endeavors to discern human expressions and emotional states, playing a pivotal role in human-computer interactions [1]. Nowadays, FER has gained notable promotion thanks to diverse deep learning (DL) algorithms [2, 3, 4, 5, 46, 47, 48] and well-annotated FER datasets [6, 7, 8]. However, these FER works [9, 10] typically operate under the assumption that both training and testing samples come from the same dataset (domain) and inherently share an identical data distribution. In practice, their classification accuracy often drops sharply due to the great discrepancy in data distribution (i.e., inter-domain shifts) [11, 12] when applied in different scenes, making them incapable of tackling the cross-domain problem settings.\nTo alleviate the inter-domain shifts, unsupervised Cross-Domain FER (CDFER) [13, 14, 49] has been introduced, aiming to extract domain-invariant features from a single labeled source domain to accurately classify the samples in an unlabeled target domain. Various techniques, such as adversarial learning [11, 15, 16] and metric learning [12, 17, 18], have been explored to reduce the distribution disparity between the two domains. However, these methods primarily focus on leveraging data information from a single source, while overlooking the valuable facial knowledge buried in multiple source domains. In reality, there are multiple labeled source datasets collected with diverse conditions, such as different acquisition environments, ethnic characteristics, etc. Leveraging these sources can significantly increase the number of training samples and further provide more comprehensive and discriminative feature representations of facial expressions, thus contributing to better generalization ability. Subsequently, unsupervised Cross-Multidomain FER (CMFER) [19] is motivated to effectively transfer richer facial knowledge from multiple labeled source domains to the unlabeled target domain.\nA simple solution to CMFER tasks is to regard different sources as a single source and directly apply CDFER methods. Regretfully, as the number of sources increases, these approaches often exhibit sub-optimal performance or even degradation owing to the growing complexity of data distributions [19, 20]. Therefore, developing a well-designed CMFER method is quite imperative. Delving deeply into CMFER, there are two main challenges. The first one is the above-mentioned inter-domain shifts across different domains due to their inherent data distribution difference. Specifically, as shown in Fig.1, with different shooting conditions, AffectNet [6] (i.e., T) contains both gray and color images, while FER-2013 [21] (i.e., Sm) only includes gray images. Besides, JAFFE [22] (i.e., Sn) has notable racial characteristics where all the facial images come from Japanese women. These severe inter-domain shifts disrupt the effective extraction of domain-invariant facial expression representations and hinder smooth knowledge transfer from source domains to the target domain. The second challenge for CMFER is the intra-domain shifts that occur within the target domain. As shown in Fig.1, the subtle differences and intrinsic ambiguities in human expressions (marked by red borders) may lead to lower inter-class distinctions. Therefore, the intra-domain shifts are likely to cause the model to produce uncertain category predictions and acquire inaccurate semantic information, severely damaging the prediction robustness.\nIn this paper, we propose a novel Learning with Alignments for Cross-Multidomain Facial Expression Recognition (LA-CMFER) framework to simultaneously tackle the inter- and intra-domain shifts. In LA-CMFER, we consider both essential global and local features for facial expressions and employ a shared dual-branch structure across multiple source domains and the target domain. Then, to address the inter-domain shifts, a dual-level (i.e., sample-level and cluster-level) inter-domain alignment is developed to encourage the model to pay more attention to the hard-to-align samples with higher uncertainty and generate a more well-clustered feature space based on the predicted labels. As for the intra-domain shifts, a multi-view intra-domain alignment is devised with a muti-view clustering consistency constraint to form a discriminative target feature space, and a multi-view voting scheme to gain the high-quality pseudo labels for an effective information interaction between the local and global branches, respectively. Both inter- and intra-domain alignments act with united strength to comprehensively extract discriminative facial representations from multiple source domains and promote the classification in the target domain. The main contributions of this paper are three-fold:\n\u2022 We present a Learning with Alignments for Cross-Multidomain Facial Expression Recognition (LA-CMFER) framework to tackle both inter- and intra-domain shifts and utilize beneficial knowledge from multiple source domains to address the challenging CMFER task.\n\u2022 To mitigate the inter-domain shift, we introduce a dual-level inter-domain alignment method that prioritizes hard-to-align samples and fosters a well-clustered feature space. As for the intra-domain shift, we design a muti-view clustering consistency constraint between the global and local branches to gain a concentrated target feature space for alleviating the interference of noisy samples.\n\u2022 Extensive experiments on six commonly used FER benchmark datasets verify the superior performance of our method compared to other state-of-the-art approaches."}, {"title": "2 Related Works", "content": "2.1 Cross-domain FER\nCDFER follows the fundamental principles of Unsupervised Domain Adaptation (UDA) [23], aiming to apply the classifier trained with a single labeled source domain to classify samples in an unlabeled target domain. Numerous studies have focused on CDFER tasks, which can be categorized into two groups: metric-based approaches vs. adversarial-based approaches. Concretely, metric-based approaches [12, 13 17, 18, 24] construct proper distance functions to extract more discriminative facial features for reducing cross-domain distribution variation. For instance, Ni et al. [24] combined metric learning with dictionary learning to alleviate the transfer facial expression recognition issue. Zhang et al. [13] presented a local-global discriminative subspace transfer learning (LGDSTL) method where a local-global graph is used as distance metric. Besides, adversarial-based approaches [11, 15, 16, 25] mine the domain-invariant facial features with adversarial training. To illustrate, Xie et al. [15] embedded a graph representation propagation with adversarial learning and presented an adversarial"}, {"title": "2.2 Multi-source Domain Adaptation", "content": "Multi-source Domain Adaptation (MDA) [26] holds the assumption that data can be gathered from diverse source domains with different distributions, which is a more practical but challenging task compared to single-source domain adaptation. Pioneer studies [27, 28, 50] have theoretically confirmed that the target distribution can be represented as a weighted combination of source distributions. To further ensure effective adaptation, the key lies in overcoming the inter-domain shifts caused by diverse data distributions across domains. Along with this philosophy, Zhao et al. [29] used adversarial training to align the target and source distributions. Zhu et al. [30] developed an MDA framework with multiple domain-specific classifiers to mine the domain-invariant features and use a maximum mean discrepancy (MMD) loss [31] to ease the inter-domain shift. Li et al. [32] devised a feature filtration network to selectively align features across domains.\nUnlike existing MDA works in natural image classification, the CMFER tasks are more difficult. Besides the inter-domain shifts, the subtle differences between facial expressions also bring about"}, {"title": "3 Methodology", "content": "3.1 Preliminaries and Framework Overview\nIn the problem definition of CMFER, there are N labeled source domains {Sn}Nn=1 and an unlabeled target domain T, with a total of K shared facial expression classes. Concretely, Sn denotes the n-th\nFER dataset DSnn = {(xnSn i , ynn)Sn }n=1 where xinn is the i-th labeled source image and yn\u2208 {0,1}K represents its one-hot real label. Correspondingly, the target domain T contains a single unlabeled\ndataset DT = {xti }Ti=1, where xti denotes the i-th unlabeled target\nimage. CMFER tries to train a deep model with DT and {DSn}Nn=1 to accurately predict the expression labels of xti in DT.\nThe overview of our LA-CDFER framework is illustrated in Fig. 2, which includes a shared encoder E with both global and local branches. Specifically, the global/local branch is equipped with respective global/local feature alignment network AG/AL and feature classifier CG/CL. Notably, AG and AL hold different model"}, {"title": "3.2 Global and Local Branches", "content": "Given the notable importance of both global and local features, LA-CMFER adopts a dual-branch architecture. As shown in Fig. 2, fed with the source image xinSn , encoder E first obtains its compact\nembedding enSn , which is then processed into a global feature frG,iSn using the alignment network AG. To capture finer details essential\nfor recognizing subtle differences in full-face images, enSn is further subdivided into four attentional regions: top-left (etlSin), top-right\n(etrSin), bottom-left (eblSin), and bottom-right (ebrSin). These embeddings en\nL,iSn = {etlLSi, etrLSi, eblLSi, ebrLSi} separately undergo\nprocessing by AL and are then concatenated to form a local feature\nfrL,iSn. Finally, the classifiers CG and CL generate predictions P\nG,iSn =\nCG(frG,iSn) and PrL,iSn = CL(frL,iSn). Similarly, when processing the target\nimage xti , the same procedure is followed, resulting in two\npredictions from the two branches, i.e., PtG,i = CG(AG(E(xti )) and\nPtL,i = CL(AL(E(xti )). These two branches make predictions from the complementary global-local views for both source and target\nsamples, thus better detecting the beneficial expression knowledge."}, {"title": "3.3 Dual-level Inter-domain Alignment", "content": "To tackle the inter-domain shifts between source and target distributions, LA-CMFER presents the dual-level (sample- and cluster-level) inter-domain alignment to comprehensively extract\nthe domain-invariant facial features. This is gained by prioritizing hard-to-align samples with higher uncertainty and grouping\nsamples based on their category information.\nSample-level Inter-Domain Alignment. Existing CDFER works\nusually use the discrepancy-based constraints (e.g., MMD [30, 31,\n52], triplet loss [4], and adversarial loss [11, 25]) to minimize the inter-domain shift. Concretely, given the source distribution P Sn(x, y) and target distribution PT (x, y), the traditional MMD\nloss [31] can be formulated as follows:\nLMMD(P Sn , P T) = Du(P Sn, PT) = ||\u03c6S n\u2212 \u03c6 T||\n (1)\n1\nB\na\n\u03c6a = \u03a3\nB\nai=1\n\u03c6xi,\n(2)\nwhere \u03c6xi denotes the average feature mappings of input samples\nwithin the batch Ba to the reproducing Kernel Hilbert Space H and a represents the target domain (T) or the n-th source (Sn).\nDespite its satisfactory performance, traditional MMD usually averages the distances among samples with uniform weights, thus\nneglecting the varying importance of different samples in the\nalignment. In the CMFER task, some hard-to-align samples,\nexhibiting higher uncertainty and located near decision boundaries,\nmay offer greater insights into the domain alignment and\nknowledge transfer, thus warranting increased attention. Thus, we\npropose the sample-level inter-domain (SID) alignment with a\nhardness-aware weighting function. Specifically, given an image\nxi and its prediction Pu, with J probability values {Puj }j=1,\nwhere M = G or L denotes different branches and J denotes the\nnumber of classes, we use a measurement term \u03a9M (\u00b7) to adaptively evaluate the instantaneous hardness of xi in the current iteration:\n\u03a9M(xi ) = \u2211Jj=1 1[Puj \u2260 Max(Pui ) or j \u2260 rce](Pa ), (3)\nwhere 1[] is a binary indicator and Max() represents the\nmaximum function. From both local and global views, \u03a9M(\u00b7) aims to exclude the maximum probability in xti or set the real class\nelement (rce) to 0 in xinSn and then evaluate the L2-norm of the\nremaining elements. This prioritizes uncertain target sample xti or\nsource sample xinSn by assigning them with higher weights.\nSubsequently, \u03a9M(xi ) is normalized in Ba to determine the relative weights HM(xi ) to facilitate batch-wise optimization:\nHM(xi ) =\n\u03a9M (xi )\n\u2211x\u2208Ba \u03a9M(x).\n(4)\nFinally, HM (\u00b7) is utilized to modify the traditional MMD loss as\nour sample-level inter-domain alignment loss Lssid:\nLsid(P Sn , P T) = ||\u03c6S n\u2212 \u03c6 T||2 ,\n(5)\n\u03c6a =\n1\nB\na\n\u2211B\nai=1 HM(xi )\u00b7 \u03c6xi.\n(6)\nNotably, HM(\u00b7) can be seamlessly integrated into other discrepancy-based constraints in a plug-and-play way for providing\nuseful perceptions about sample importance.\nCluster-level Inter-domain Alignment. Existing discrepancy-based inter-domain alignments [30, 31] may blindly align samples with different classes closer, potentially impeding the model from\nlearning discriminative features. Thus, we introduce a cluster-level inter-domain alignment to better group samples based on their\ncategories. We use the real label yn=k and pseudo label \u0177ti =d to\ndenote the category attributes of the source sample xinSn and target\nsample xti , respectively. Then, we minimize the distances between\nsource-target samples of the same category (d = k) and maximize\nthem for samples of different categories (d \u2260 k). This process is\nsummarized as a cluster-level inter-domain alignment loss Lcid:"}, {"title": "3.4 Multi-view Intra-domain Alignment", "content": "Multi-view Clustering Consistency Constraint: The intra-domain shifts within the FER target domain are typically\ncharacterized by noisy samples near the decision boundary, which\nmay lead to a dispersed target feature space, thereby introducing\nprediction bias. To tackle this, we propose a novel multi-view\nclustering consistency constraint, aiming to shape a discriminative\nand concentrated target feature space. In this space, clusters with\nthe same class predicted by classifiers of different branches should\nremain similar, while those of different classes should be distinct.\nSpecifically, given the predictions P GB and P LB of all target\nsamples in the target batch BT from the global and local branches, we build a Multi-view Prediction Consistency (MPC) matrix to\nevaluate the consistency between these predictions with the\nfollowing formulation:\nMP C = (P L)T P G,\n(10)\nDrawing on the cluster assumption [33], we consider target samples in BT as a specific class (i.e., class c) and their\ncorresponding prediction probabilities in BT as a batch-wise cluster representation for class c, which is captured by the c-th column in\nPG or PB. To reach the intra-domain alignment, we enhance the\nconsistency among cluster representations for identical class\nassignments (i.e., higher diagonal values in MPC) and\ninconsistency among them with different class assignments (i.e.,\nlower off-diagonal values in MPC). This process can be represented\nas the following multi-view clustering consistency loss Lmcc:\nLmcc = (S(||MP C|| \u2212 I) + S(||MP Ct|| \u2212 1)),\n(11)\nwhere S(\u00b7) calculates the sum of absolute values across all matrix\nelements, I \u2208 RK\u00d7K represents an identity matrix, and ||\u00b7|| denotes\na normalization function [34]. Minimizing Lmcc facilitates the well-separated clusters of different expressions in the target feature\nspace and encourages consistent predictions from dual branches.\nMulti-view Voting Scheme: Considering the high-quality pseudo\nlabels can provide more accurate guidance, we propose a multi-view scheme with two voting conditions to select them: (i) global and local branch makes consistent predictions from different views,\ni.e., argmax(PtiG) = argmax(PtLi ) ; (ii) at least one branch\nachieves higher confidence than a threshold \u03c4, i.e., Max(PtG,i) > \u03c4\nor Max(PtLi ) > \u03c4. When the two points are all satisfied, the high-quality pseudo-label \u1ef9ti is selected. Additional supervision is then\nprovided to PtiG and PtLi using a multi-view voting loss Lmvv:\nLmvv =\n1\n|BT|\n\u2211|BT|i=1\n1[MV (PtG,i , PtLi )] \u00d7 [CE(PtG,i , \u1ef9ti) +\nCE(PtLi , \u1ef9ti)],\n(12)\nwhere MV (\u00b7) is a function to determine whether the two voting conditions are simultaneously met. By incorporating Lmcc and\nLmvv, LA-CMFER is guided to achieve a meaningful knowledge\ninteraction between the two branches, promoting mutual\nenhancement and facilitating the development of a more robust\ntarget feature space."}, {"title": "3.5 Training Objective", "content": "The whole training objective contains five parts: (1) supervised loss Lsup for labeled source samples, (2) inter-domain loss Linter, (3) multi-view clustering consistency loss Lmcc, and (4) multi-view voting loss Lmvv for unlabeled target samples.\nThe supervised loss Lsup can be represented as follows:\nLsup =\n1\nN\n\u2211\nNn=1 \u2211|Sn|\ni=1[CE(PG,iSn, yG,iSn) + CE(PL,iSn, yL,iSn)],\n(13)\nwhere CE(\u00b7) denotes the cross-entropy loss.\nFinally, the training objective of LA-CMFER is formulated as:\nLtotal = Lsup + \u03b1Linter + \u03b2Lmcc + \u03b3Lmvv,\n(14)\nwhere \u03b1, \u03b2, and \u03b3 are three weighted hyper-parameters.\nThe training stage of LA-CMFER is summarized in Algorithm 1. In the inference stage, we take the prediction with a higher\nprobability from the two branches as the final result [51]."}, {"title": "4 Experiments and Result Analysis", "content": "4.1 Experimental Setup\nDatasets. To verify the performance of the proposed method, we\nfollow the experiment settings in [19] and involve six commonly\nused FER datasets, including three lab-controlled datasets CK+ [7],\nJAFFE [22], and Oulu-CASIA [35] and three Internet-collected\nlarge-scale field datasets AffectNet [6], RAF-DB [8], and FER- 2013 [21]. Specifically, CK+ provides 593 annotated video\nsamples from 123 subjects and uses the last three frames with six\nbasic emotions (i.e., anger, disgust, fear, surprise, happy, and sad)\nand neutral emotions, finally gaining a total of 1,236 images.\nJAFFE includes 213 facial expression images from 10 Japanese\nwomen. Oulu-CASIA (Oulu) captures 2988 face images by\nchoosing the last three frames with normal lighting for basic\nexpressions and the first frame for neutral expressions. AffectNet\nstands as the largest facial expression dataset to date. Following\n[19], we curate up to 5,000 facial images for each emotion category,\nresulting in a total of 33,793 training images and 3,500 testing\nimages. RAF-DB is a real-world FER dataset that has 12,271 and\n3,068 images for training and testing, respectively. FER-2013,\ncompiled via the Google image search engine, contains 35,887\nimages of different facial expressions and uses 28,709 images for\ntraining and 3,589 images for testing. Intuitively, some samples\nfrom each dataset are given in Fig. 3.\nFor label settings, we select the subset with six basic and neutral\nemotion labels from each dataset, and the training and testing sets\nof the lab-controlled datasets are the same. In our experiments, each\ndataset is sequentially regarded as a domain and we utilize the\nsymbol '\u2192' to represent that dataset A is the target domain while\nother datasets serve as source domains.\nImplementation Details. We conduct all our experiments on two\nGeForce RTX 3090 GPUs with the PyTorch toolbox. Same as [19],\nface images are first detected and aligned by the RetinaFace [36]\nand further resized to 224 \u00d7224. We use the conv1~conv3 layers"}, {"title": "4.2 Comparison Experiments", "content": "To verify the effectiveness of the proposed method, we compare\nour method with state-of-the-art (SOTA) Single-source DA\nmethods (SDA) and MDA methods. For SDA methods, two\nprotocols are adopted: (1) Single Best, which reports the best result\namong all source domains, and (2) Source Combine, which naively\ncombines all source domains and then uses the SDA methods.\nSpecifically, Source-Only refers to directly transferring the model\ntrained in source domains to the target domain. We select (i) the\ntraditional SDA methods: DAN [4], DANN [41], ADDA [42], and"}, {"title": "4.3 Analytical Experiments", "content": "Contributions of Key Components: To study the contributions of\nkey components in our LA-CMFER, we progressively conduct\nablation experiments on all FER datasets. For clarity, \u2018Baseline\u2019\nmeans directly transferring the dual-branch model trained in the\nsource domains to the target domain with only supervised loss Lsup.\n\u2018MMD\u2019 indicates using the traditional MMD loss to assist the network to alleviate the inter-domain shift. The experimental\nresults are shown in Tab.2 where we use the initial letters of the\ndataset as its reference for the space limitations. As seen, each\nconstraint loss positively contributes to performance enhancements\nin most cases. With MMD, (B) achieves 8.29% mean promotion by narrowing the inter-domain shift within the dual-branch framework.\nCompared to (B), the sample-level inter-domain alignment with\nhardness-aware weighting function H(\u00b7), i.e., (C), further enhances\nthe accuracy from 65.11% to 67.03%. Besides, with the additional cluster-level alignment, (D) gains notable promotions on \u2018\u2192\nJAFFE\u2019 (\u21911.41%) and \u2018\u2192Oulu\u2019 (\u21911.33%) tasks, respectively. Then, with Lmcc and Lmvv in the multi-view intra-domain alignment, we obtain mean accuracies of 68.67% and 69.32%, respectively.\nImpact of Different Inter-domain Alignment Strategies: To\nexplore the impact of different inter-domain alignment strategies, we compare our dual-level inter-domain alignment with the widely used adversarial training strategy [19]. To accomplish adversarial training, rather than employing our dual-level inter-domain alignment, we use two discriminators following our global and local branches to distinguish whether the features come from the source domain or the target domain (denoted as 'Adv variant'). As seen in Tab.3, our dual-level inter-domain alignment achieves enhancements on all tasks, particularly for the \u2018\u2192O\u2019 (\u21914.65%), \u2018\u2192\nA\u2019 (\u21915.27%), and \u2018\u2192F\u2019 (\u21915.38%) tasks. These results strongly reveal that, unlike adversarial training, which only aligns multiple\ndomains at a feature level, our method can prioritize hard samples with abundant knowledge and further leverage label information, thus enabling a more fine-grained alignment with higher accuracy.\nFurther, in the domain alignment, the distances among the data distributions of multiple domains can give a direct evaluation of the alignment effectiveness. Thus, we adopt several distance metrics,\ninvolving (A) intra-class L2 distance (intra-L2), (B) intra-class variance (intra-var), (C) inter-class L2 distance (inter-L2), and (D)\ndistance ratio r(r= inter-L2/intra-L2). The comparisons among the Adv variant, DUML [19], and our LA-CMFER are shown in Tab.4. As seen, compared to both the Adv variant and DUML, our LA-CMFER achieves the least intra-class distances (0.69 for sources and 1.08 for target) and the largest inter-class distances (1.50 for sources and 1.32 for target). Such remarkable superiorities validate that our LA-CMFER can effectively promote inter-class variability and intra-class compactness, thus reaching the best accuracy (69.32%).\nAnalysis of the Global-Local Branch Architecture: To capture both the global knowledge from the full image and local expression details, LA-CMFER is built with a global-local branch architecture. To study its superiority, we compared five different architectures: (i) only global branch (denoted as 'Global'), (ii) only local branch"}]}