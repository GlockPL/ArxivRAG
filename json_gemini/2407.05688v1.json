{"title": "Learning with Alignments: Tackling the Inter- and Intra-domain Shifts for Cross-multidomain Facial Expression Recognition", "authors": ["Yuxiang Yang", "Yuanyuan Xu", "Lu Went", "Xi Wu", "Yan Wang", "Xinyi Zeng", "Jiliu Zhou"], "abstract": "Facial Expression Recognition (FER) holds significant importance in human-computer interactions. Existing cross-domain FER methods often transfer knowledge solely from a single labeled source domain to an unlabeled target domain, neglecting the comprehensive information across multiple sources. Nevertheless, cross-multidomain FER (CMFER) is very challenging for (i) the inherent inter-domain shifts across multiple domains and (ii) the intra-domain shifts stemming from the ambiguous expressions and low inter-class distinctions. In this paper, we propose a novel Learning with Alignments CMFER framework, named LA-CMFER, to handle both inter- and intra-domain shifts. Specifically, LA-CMFER is constructed with a global branch and a local branch to extract features from the full images and local subtle expressions, respectively. Based on this, LA-CMFER presents a dual-level inter-domain alignment method to force the model to prioritize hard-to-align samples in knowledge transfer at a sample level while gradually generating a well-clustered feature space with the guidance of class attributes at a cluster level, thus narrowing the inter-domain shifts. To address the intra-domain shifts, LA-CMFER introduces a multi-view intra-domain alignment method with a multi-view clustering consistency constraint where a prediction similarity matrix is built to pursue consistency between the global and local views, thus refining pseudo labels and eliminating latent noise. Extensive experiments on six benchmark datasets have validated the superiority of our LA-CMFER.", "sections": [{"title": "1 Introduction", "content": "Facial Expression Recognition (FER) endeavors to discern human expressions and emotional states, playing a pivotal role in human-computer interactions [1]. Nowadays, FER has gained notable promotion thanks to diverse deep learning (DL) algorithms [2, 3, 4, 5, 46, 47, 48] and well-annotated FER datasets [6, 7, 8]. However, these FER works [9, 10] typically operate under the assumption that both training and testing samples come from the same dataset (domain) and inherently share an identical data distribution. In practice, their classification accuracy often drops sharply due to the great discrepancy in data distribution (i.e., inter-domain shifts) [11, 12] when applied in different scenes, making them incapable of tackling the cross-domain problem settings.\nTo alleviate the inter-domain shifts, unsupervised Cross-Domain FER (CDFER) [13, 14, 49] has been introduced, aiming to extract domain-invariant features from a single labeled source domain to accurately classify the samples in an unlabeled target domain. Various techniques, such as adversarial learning [11, 15, 16] and metric learning [12, 17, 18], have been explored to reduce the distribution disparity between the two domains. However, these methods primarily focus on leveraging data information from a single source, while overlooking the valuable facial knowledge buried in multiple source domains. In reality, there are multiple labeled source datasets collected with diverse conditions, such as different acquisition environments, ethnic characteristics, etc. Leveraging these sources can significantly increase the number of training samples and further provide more comprehensive and"}, {"title": "2 Related Works", "content": "2.1 Cross-domain FER\nCDFER follows the fundamental principles of Unsupervised Domain Adaptation (UDA) [23], aiming to apply the classifier trained with a single labeled source domain to classify samples in an unlabeled target domain. Numerous studies have focused on CDFER tasks, which can be categorized into two groups: metric-based approaches vs. adversarial-based approaches. Concretely, metric-based approaches [12, 13 17, 18, 24] construct proper distance functions to extract more discriminative facial features for reducing cross-domain distribution variation. For instance, Ni et al. [24] combined metric learning with dictionary learning to alleviate the transfer facial expression recognition issue. Zhang et al. [13] presented a local-global discriminative subspace transfer learning (LGDSTL) method where a local-global graph is used as distance metric. Besides, adversarial-based approaches [11, 15, 16, 25] mine the domain-invariant facial features with adversarial training. To illustrate, Xie et al. [15] embedded a graph representation propagation with adversarial learning and presented an adversarial"}, {"title": "2.2 Multi-source Domain Adaptation", "content": "Multi-source Domain Adaptation (MDA) [26] holds the assumption that data can be gathered from diverse source domains with different distributions, which is a more practical but challenging task compared to single-source domain adaptation. Pioneer studies [27, 28, 50] have theoretically confirmed that the target distribution can be represented as a weighted combination of source distributions. To further ensure effective adaptation, the key lies in overcoming the inter-domain shifts caused by diverse data distributions across domains. Along with this philosophy, Zhao et al. [29] used adversarial training to align the target and source distributions. Zhu et al. [30] developed an MDA framework with multiple domain-specific classifiers to mine the domain-invariant features and use a maximum mean discrepancy (MMD) loss [31] to ease the inter-domain shift. Li et al. [32] devised a feature filtration network to selectively align features across domains.\nUnlike existing MDA works in natural image classification, the CMFER tasks are more difficult. Besides the inter-domain shifts, the subtle differences between facial expressions also bring about"}, {"title": "3 Methodology", "content": "3.1 Preliminaries and Framework Overview\nIn the problem definition of CMFER, there are N labeled source domains {Sn}_{n=1}^{N} and an unlabeled target domain T, with a total of K shared facial expression classes. Concretely, S_{n} denotes the n-th FER dataset \\mathcal{D}_{S^{n}} = {{(x_{i}^{S_{n}}, y_{i}^{S_{n}})}}_{i=1}^{|S_{n}|} where x_{i}^{S_{n}} is the i-th labeled source image and y_{n} \\in {0,1}^{K} represents its one-hot real label. Correspondingly, the target domain T contains a single unlabeled dataset \\mathcal{D}_{T} = {x_{i}^{T}}_{i=1}^{|T|}, where x_{i}^{T} denotes the i-th unlabeled target image. CMFER tries to train a deep model with \\mathcal{D}^{T} and {\\mathcal{D}^{S^{n}}}_{n=1}^{N} to accurately predict the expression labels of x_{i}^{T} in \\mathcal{D}^{T}.\nThe overview of our LA-CDFER framework is illustrated in Fig. 2, which includes a shared encoder E with both global and local branches. Specifically, the global/local branch is equipped with respective global/local feature alignment network A_{G}/A_{L} and feature classifier C_{G}/C_{L}. Notably, A_{G} and A_{L} hold different model"}, {"title": "3.2 Global and Local Branches", "content": "Given the notable importance of both global and local features, LA-CMFER adopts a dual-branch architecture. As shown in Fig. 2, fed with the source image x_{i}^{S_{n}}, encoder E first obtains its compact embedding e_{i}^{S_{n}}, which is then processed into a global feature f_{G,i}^{S_{n}} using the alignment network A_{G}. To capture finer details essential for recognizing subtle differences in full-face images, e_{i}^{S_{n}} is further subdivided into four attentional regions: top-left (e_{tl,i}^{S_{n}}), top-right (e_{tr,i}^{S_{n}}), bottom-left (e_{bl,i}^{S_{n}}), and bottom-right (e_{br,i}^{S_{n}}). These embeddings e_{n} = {e_{tl,i}^{S_{n}}, e_{tr,i}^{S_{n}}, e_{bl,i}^{S_{n}}, e_{br,i}^{S_{n}}} separately undergo processing by A_{L} and are then concatenated to form a local feature f_{L,i}^{S_{n}}. Finally, the classifiers C_{G} and C_{L} generate predictions P_{G,i}^{S_{n}} = C_{G}(f_{G,i}^{S_{n}}) and P_{L,i}^{S_{n}} = C_{L}(f_{L,i}^{S_{n}}). Similarly, when processing the target image x_{i}^{T}, the same procedure is followed, resulting in two predictions from the two branches, i.e., P_{G,i}^{T} = C_{G}(A_{G}(E(x_{i}^{T}))) and P_{L,i}^{T} = C_{L}(A_{L}(E(x_{i}^{T}))). These two branches make predictions from the complementary global-local views for both source and target samples, thus better detecting the beneficial expression knowledge."}, {"title": "3.3 Dual-level Inter-domain Alignment", "content": "To tackle the inter-domain shifts between source and target distributions, LA-CMFER presents the dual-level (sample- and cluster-level) inter-domain alignment to comprehensively extract the domain-invariant facial features. This is gained by prioritizing hard-to-align samples with higher uncertainty and grouping samples based on their category information.\nSample-level Inter-Domain Alignment. Existing CDFER works usually use the discrepancy-based constraints (e.g., MMD [30, 31, 52], triplet loss [4], and adversarial loss [11, 25]) to minimize the inter-domain shift. Concretely, given the source distribution \\mathcal{P}_{S^{n}}(x, y) and target distribution \\mathcal{P}_{T}(x, y), the traditional MMD loss [31] can be formulated as follows:\n\n\\mathcal{L}_{MMD}(\\mathcal{P}_{S^{n}}, \\mathcal{P}_{T}) = D_{\\mathcal{H}}(\\mathcal{P}_{S^{n}}, \\mathcal{P}_{T}) = ||\\phi_{\\mathcal{S^{n}}} - \\phi_{\\mathcal{T}}||^{2},\n\n\\phi_{\\alpha} = \\frac{1}{B_{a}}\\sum_{B_{a}i=1} \\varphi_{i},\\newline\nwhere \\varphi_{i} denotes the average feature mappings of input samples within the batch B_{a} to the reproducing Kernel Hilbert Space \\mathcal{H} and \\alpha represents the target domain (T) or the n-th source (S^{n}).\nDespite its satisfactory performance, traditional MMD usually averages the distances among samples with uniform weights, thus neglecting the varying importance of different samples in the alignment. In the CMFER task, some hard-to-align samples, exhibiting higher uncertainty and located near decision boundaries, may offer greater insights into the domain alignment and knowledge transfer, thus warranting increased attention. Thus, we propose the sample-level inter-domain (SID) alignment with a hardness-aware weighting function. Specifically, given an image x and its prediction P_{u}, with J probability values {P_{j}^{u}}_{j=1}^{J}, where M = G or L denotes different branches and J denotes the number of classes, we use a measurement term \\Omega_{u}(\\cdot) to adaptively evaluate the instantaneous hardness of x in the current iteration:\n\n\\Omega_{M}(x) = \\sum_{j=1}^{J} \\mathbb{1}[P_{j} \\neq Max(P_{i}) \\text{ or } j \\neq rce] \\lVert P_{j} - \\mathbb{1}_{j=rce}(P_{j}) \\rVert^{2},\n\nwhere \\mathbb{1}[\\cdot] is a binary indicator and Max(\u00b7) represents the maximum function. From both local and global views, \\Omega_{u}(\\cdot) aims to exclude the maximum probability in x_{i}^{T} or set the real class element (rce) to 0 in x_{i}^{S_{n}} and then evaluate the L2-norm of the remaining elements. This prioritizes uncertain target sample x_{i}^{T} or source sample x_{i}^{S_{n}} by assigning them with higher weights. Subsequently, \\Omega_{M}(x) is normalized in B_{a} to determine the relative weights H_{M}(x_{i}) to facilitate batch-wise optimization:\n\nH_{M}(x_{i}) = \\frac{\\Omega_{M}(x)}{\\sum_{x \\in B_{a}} \\Omega_{M}(x)}.\n\nFinally, H_{M}(\\cdot) is utilized to modify the traditional MMD loss as our sample-level inter-domain alignment loss \\mathcal{L}_{sid}:\n\n\\mathcal{L}_{sid}(\\mathcal{P}_{S^{n}}, \\mathcal{P}_{T}) = ||\\phi_{\\mathcal{S^{n}}}^{\\mathcal{H}} - \\phi_{\\mathcal{T}}^{\\mathcal{H}}||^{2},\n\n\\phi_{\\alpha}^{\\mathcal{H}} = \\frac{1}{B_{a}}\\sum_{B_{a}i=1} H_{M}(x_{i})\\cdot \\varphi_{i}.\n\nNotably, H_{M}(\\cdot) can be seamlessly integrated into other discrepancy-based constraints in a plug-and-play way for providing useful perceptions about sample importance.\nCluster-level Inter-domain Alignment. Existing discrepancy-based inter-domain alignments [30, 31] may blindly align samples with different classes closer, potentially impeding the model from learning discriminative features. Thus, we introduce a cluster-level inter-domain alignment to better group samples based on their categories. We use the real label y_{i}^{S_{n}}=k and pseudo label \\hat{y}_{i}^{T} =d to denote the category attributes of the source sample x_{i}^{S_{n}} and target sample x_{i}^{T}, respectively. Then, we minimize the distances between source-target samples of the same category (d = k) and maximize them for samples of different categories (d \\neq k). This process is summarized as a cluster-level inter-domain alignment loss \\mathcal{L}_{cid}:"}, {"title": "3.4 Multi-view Intra-domain Alignment", "content": "Multi-view Clustering Consistency Constraint: The intra-domain shifts within the FER target domain are typically characterized by noisy samples near the decision boundary, which may lead to a dispersed target feature space, thereby introducing prediction bias. To tackle this, we propose a novel multi-view clustering consistency constraint, aiming to shape a discriminative and concentrated target feature space. In this space, clusters with the same class predicted by classifiers of different branches should remain similar, while those of different classes should be distinct. Specifically, given the predictions \\mathcal{P}_{GB} and \\mathcal{P}_{LB} of all target samples in the target batch B^{T} from the global and local branches, we build a Multi-view Prediction Consistency (MPC) matrix to evaluate the consistency between these predictions with the following formulation:\n\nMPC = (\\mathcal{P}_{LB})^{'}\\mathcal{P}_{GB}.\n\nDrawing on the cluster assumption [33], we consider target samples in B^{T} as a specific class (i.e., class c) and their corresponding prediction probabilities in B^{T} as a batch-wise cluster representation for class c, which is captured by the c-th column in \\mathcal{P}_{GB} or \\mathcal{P}_{LB}. To reach the intra-domain alignment, we enhance the consistency among cluster representations for identical class assignments (i.e., higher diagonal values in MPC) and inconsistency among them with different class assignments (i.e., lower off-diagonal values in MPC). This process can be represented as the following multi-view clustering consistency loss \\mathcal{L}_{mcc}:\n\n\\mathcal{L}_{mcc} = \\frac{1}{2}(\\mathcal{S}(\\lVert MPC \\rVert - I) + \\mathcal{S}(\\lVert MPC^{'} \\rVert - 1)),\n\nwhere \\mathcal{S}(\\cdot) calculates the sum of absolute values across all matrix elements, I \\in \\mathbb{R}^{K \\times K} represents an identity matrix, and \\lVert \\cdot \\rVert denotes a normalization function [34]. Minimizing \\mathcal{L}_{mcc} facilitates the well-separated clusters of different expressions in the target feature space and encourages consistent predictions from dual branches.\nMulti-view Voting Scheme: Considering the high-quality pseudo labels can provide more accurate guidance, we propose a multi-view scheme with two voting conditions to select them: (i) global"}, {"title": "3.5 Training Objective", "content": "The whole training objective contains five parts: (1) supervised loss \\mathcal{L}_{sup} for labeled source samples, (2) inter-domain loss \\mathcal{L}_{inter}, (3) multi-view clustering consistency loss \\mathcal{L}_{mcc}, and (4) multi-view voting loss \\mathcal{L}_{mvv} for unlabeled target samples.\nThe supervised loss \\mathcal{L}_{sup} can be represented as follows:\n\n\\mathcal{L}_{sup} = \\sum_{n=1}^{N} \\frac{1}{|B_{S^{n}}|} \\sum_{S^{n}}{i=1}^{|B_{S^{n}}|} [CE(P_{G,i}^{S^{n}}, y_{i}^{S^{n}}) + CE(P_{L,i}^{S^{n}}, y_{i}^{S^{n}})],\n\nwhere CE(\u00b7) denotes the cross-entropy loss.\nFinally, the training objective of LA-CMFER is formulated as:\n\n\\mathcal{L}_{total} = \\mathcal{L}_{sup} + \\alpha\\mathcal{L}_{inter} + \\beta\\mathcal{L}_{mcc} + \\gamma\\mathcal{L}_{mvv},\n\nwhere \u03b1, \u03b2, and \u03b3 are three weighted hyper-parameters.\nThe training stage of LA-CMFER is summarized in Algorithm 1. In the inference stage, we take the prediction with a higher probability from the two branches as the final result [51]."}, {"title": "4 Experiments and Result Analysis", "content": "4.1 Experimental Setup\nDatasets. To verify the performance of the proposed method, we follow the experiment settings in [19] and involve six commonly used FER datasets, including three lab-controlled datasets CK+ [7], JAFFE [22], and Oulu-CASIA [35] and three Internet-collected large-scale field datasets AffectNet [6], RAF-DB [8], and FER-2013 [21]. Specifically, CK+ provides 593 annotated video samples from 123 subjects and uses the last three frames with six basic emotions (i.e., anger, disgust, fear, surprise, happy, and sad) and neutral emotions, finally gaining a total of 1,236 images. JAFFE includes 213 facial expression images from 10 Japanese women. Oulu-CASIA (Oulu) captures 2988 face images by choosing the last three frames with normal lighting for basic expressions and the first frame for neutral expressions. AffectNet stands as the largest facial expression dataset to date. Following [19], we curate up to 5,000 facial images for each emotion category, resulting in a total of 33,793 training images and 3,500 testing images. RAF-DB is a real-world FER dataset that has 12,271 and 3,068 images for training and testing, respectively. FER-2013, compiled via the Google image search engine, contains 35,887 images of different facial expressions and uses 28,709 images for training and 3,589 images for testing. Intuitively, some samples from each dataset are given in Fig. 3.\nFor label settings, we select the subset with six basic and neutral emotion labels from each dataset, and the training and testing sets of the lab-controlled datasets are the same. In our experiments, each dataset is sequentially regarded as a domain and we utilize the symbol '\u2192' to represent that dataset A is the target domain while other datasets serve as source domains.\nImplementation Details. We conduct all our experiments on two GeForce RTX 3090 GPUs with the PyTorch toolbox. Same as [19], face images are first detected and aligned by the RetinaFace [36] and further resized to 224 \u00d7224. We use the conv1~conv3 layers"}, {"title": "4.2 Comparison Experiments", "content": "To verify the effectiveness of the proposed method, we compare our method with state-of-the-art (SOTA) Single-source DA methods (SDA) and MDA methods. For SDA methods, two protocols are adopted: (1) Single Best, which reports the best result among all source domains, and (2) Source Combine, which naively combines all source domains and then uses the SDA methods. Specifically, Source-Only refers to directly transferring the model trained in source domains to the target domain. We select (i) the traditional SDA methods: DAN [4], DANN [41], ADDA [42], and"}, {"title": "4.3 Analytical Experiments", "content": "Contributions of Key Components: To study the contributions of key components in our LA-CMFER, we progressively conduct ablation experiments on all FER datasets. For clarity, \u2018Baseline\u2019 means directly transferring the dual-branch model trained in the source domains to the target domain with only supervised loss \\mathcal{L}_{sup}. \u2018MMD\u2019 indicates using the traditional MMD loss to assist the network to alleviate the inter-domain shift. The experimental results are shown in Tab.2 where we use the initial letters of the dataset as its reference for the space limitations. As seen, each constraint loss positively contributes to performance enhancements in most cases. With MMD, (B) achieves 8.29% mean promotion by narrowing the inter-domain shift within the dual-branch framework. Compared to (B), the sample-level inter-domain alignment with hardness-aware weighting function H(\u00b7), i.e., (C), further enhances the accuracy from 65.11% to 67.03%. Besides, with the additional cluster-level alignment, (D) gains notable promotions on \u2192JAFFE' (\u21911.41%) and '\u2192Oulu' (\u21911.33%) tasks, respectively. Then, with \\mathcal{L}_{mcc} and \\mathcal{L}_{mvv} in the multi-view intra-domain alignment, we obtain mean accuracies of 68.67% and 69.32%, respectively.\nImpact of Different Inter-domain Alignment Strategies: To explore the impact of different inter-domain alignment strategies, we compare our dual-level inter-domain alignment with the widely used adversarial training strategy [19]. To accomplish adversarial training, rather than employing our dual-level inter-domain alignment, we use two discriminators following our global and local branches to distinguish whether the features come from the source domain or the target domain (denoted as 'Adv variant'). As seen in Tab.3, our dual-level inter-domain alignment achieves enhancements on all tasks, particularly for the '\u2192O' (\u21914.65%), \u2018\u2192A' (\u21915.27%), and '\u2192F' (\u21915.38%) tasks. These results strongly reveal that, unlike adversarial training, which only aligns multiple domains at a feature level, our method can prioritize hard samples with abundant knowledge and further leverage label information, thus enabling a more fine-grained alignment with higher accuracy.\nFurther, in the domain alignment, the distances among the data distributions of multiple domains can give a direct evaluation of the alignment effectiveness. Thus, we adopt several distance metrics, involving (A) intra-class L2 distance (intra-L2), (B) intra-class variance (intra-var), (C) inter-class L2 distance (inter-L2), and (D) distance ratio r(r= inter-L2/intra-L2). The comparisons among the Adv variant, DUML [19], and our LA-CMFER are shown in Tab.4. As seen, compared to both the Adv variant and DUML, our LA-CMFER achieves the least intra-class distances (0.69 for sources and 1.08 for target) and the largest inter-class distances (1.50 for sources and 1.32 for target). Such remarkable superiorities validate that our LA-CMFER can effectively promote inter-class variability and intra-class compactness, thus reaching the best accuracy (69.32%).\nAnalysis of the Global-Local Branch Architecture: To capture both the global knowledge from the full image and local expression details, LA-CMFER is built with a global-local branch architecture. To study its superiority, we compared five different architectures: (i) only global branch (denoted as 'Global'), (ii) only local branch"}, {"title": "5 Conclusion", "content": "In this paper, we present the LA-CMFER framework to achieve the CMFER task by addressing both inter- and intra-domain shifts. We first propose a dual-level (i.e., sample- and cluster-level) inter-domain alignment method to narrow the inter-domain shifts by prioritizing hard-to-align samples and sufficiently using the class attributes. Then, to tackle the intra-domain shifts, a multi-view intra-domain alignment method is introduced to promote consistency between the global and local branches. Extensive experiments have verified the superiority of our LA-CMFER."}]}