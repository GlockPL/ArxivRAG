{"title": "Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM", "authors": ["Junxiao Ma", "Jingjing Wang", "Jiamin Luo", "Peiying Yu", "Guodong Zhou"], "abstract": "Prior studies on Video Anomaly Detection (VAD) mainly focus on detecting whether each video frame is abnormal or not in the video, which largely ignore the structured video semantic information (i.e., what, when, and where does the abnormal event happen). With this in mind, we propose a new chat-paradigm Multi-scene Video Abnormal Event Extraction and Localization (M-VAE) task, aiming to extract the abnormal event quadruples (i.e., subject, event type, object, scene) and localize such event. Further, this paper believes that this new task faces two key challenges, i.e., global-local spatial modeling and global-local spatial balancing. To this end, this paper proposes a Global-local Spatial-sensitive Large Language Model (LLM) named Sherlock, i.e., acting like Sherlock Holmes to track down the criminal events, for this M-VAE task. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module and a Spatial Imbalance Regulator (SIR) to address the two challenges respectively. Extensive experiments on our M-VAE instruction dataset show the significant advantages of Sherlock over several advanced Video-LLMs. This justifies the importance of global-local spatial information for the M-VAE task and the effectiveness of Sherlock in capturing such information.", "sections": [{"title": "1 Introduction", "content": "Video Understanding is a foundational task in artificial intelligence, which focuses on analyzing and interpreting the content of videos to enable various applications, including video classification, ac- tivity recognition, and scene understanding [40, 58, 59]. As a crit- ical branch of video understanding, Video Anomaly Detection (VAD) [20], which aims to automatically detect abnormal videos, has garnered significant research attention due to its wide range of ap- plications in criminal activity detection and disaster response [56]. Prior studies on VAD mainly focus on detecting whether each video frame is abnormal or not in the video [20, 29, 41, 56]. However, these studies overlook targeting at determining the underlying video se- mantic structure, i.e., \u201cwhat is the abnormal type, where they have occurred, which people or things are involved\u201d with a given video. Motivated by these, this paper proposes a novel Multi-scene Video Abnormal Event Extraction and Localization (M-VAE) task, aiming at localizing abnormal events (i.e., starting and ending times of the anomaly) and extracting event quadruples (i.e. [subject of the event, event type, object of the event, scene of the event]) through a chat paradigm. Take an example of Street scene in Figure 1 (a), within 23s to 25s, a man bends down and pries the lock, then drives away from the street and the abnormal event quadruple is [people, steal, car, street]. Different scene (i.e., Residence scene) is also shown in Figure 1 (b). Within 15s to 17s, a man vandalizes a sculpture at one's residence and the quadruple is [people, Vandalism, Sculpture, Residence]. This structured processing for abnormal videos can sig- nificantly improve the practicality and efficiency of video anomaly localization systems. In fields such as real-time abnormal event monitoring that require high reliability and precision monitoring, using such structured processing can quickly search and screen for the required abnormal elements, which provides more convenient and intuitive evidence for further processing. Therefore, it is worth- while to address this new task. Nevertheless, we believe that this new task faces two key challenges.\nFor one thing, it is challenging to model the global-local spa- tial information (named global-local spatial modeling challenge). Existing video understanding models [34, 38, 54] mainly focus on modeling general global information. However, local spatial infor- mation in our M-VAE task is often crucial compared to general global information, which are highly discriminative and essential for precise identification. Taking Figure 1 (a) as an example, the local spatial information, such as action (bend down), object rela- tions (<man, near, car>), and background (street), can help better identify abnormal events. However, those local spatial information (e.g., actions, object relations, backgrounds) have different heteroge- neous representations (i.e., different model structures and encoders). Therefore, a single, fixed-capacity transformer-based model, often makes it difficult to capture those critical local spatial information in videos. Recently, the Mixture of Expert (MoE) [18, 23] paradigm has demonstrated scalability in multi-modal heterogeneous repre- sentation fusion tasks [18, 23, 24]. Inspired by this, a well-behaved model for our task should adopt the MoE paradigm to not only con- sider global spatial information but also emphasize the importance of local spatial information.\nFor another, a straightforward approach is to employ a basic Mix- ture of Expert (MoE) mechanism [18, 23, 24] to treat global spatial"}, {"title": "2 Related Work", "content": "\u2022 Video Anomaly Detection. Video Understanding is a rapidly evolving research field which encompasses several tasks, includ- ing video grounding [40, 58, 59], spatial-temporal detection [13] and so on. As an important branch of video understanding, pre- vious studies on Video Anomaly Detection (VAD) can be catego- rized into unsupervised, weakly-supervised, and fully-supervised categories. Unsupervised approaches focus on leveraging recon- struction techniques to identify anomalies [15, 20, 62, 64]. Weakly- supervised methods have shown promising results in identify- ing abnormal frames [11, 36, 57, 60, 72]. Fully-supervised meth- ods are scarce due to the expensive frame-level annotations re- quired [7, 10, 12, 19, 53, 55, 70]. Different from the above studies, our Sherlock model aims to target at determining the underlying video semantic structure, providing a structured quadruple that goes beyond previous methods, facilitating the rapid detection and early warning of abnormal events in real-time.\n\u2022 Event Extraction (EE) focuses on extracting structured infor- mation from given types of information. Traditional EE methods mainly extract from text documents [21, 25, 35, 37, 52]. Recently, many studies [2, 44, 66-68] generate similar event structures from visual image data. Different from all the above studies, we are the first to focus on extracting the abnormal event from videos and constructing a quadruple dataset, incorporating information from"}, {"title": "3 Our Sherlock Model", "content": "In this paper, we propose a Sherlock model to address the M-VAE task. Figure 2 illustrates the framework of Sherlock, which is composed of two core components (i.e., the Global-local Spatial- enhanced MoE (GSM) module (sec 3.1) for the global-local spa- tial modeling challenge and the Spatial Imbalance Regulator (SIR) (sec 3.2) for the global and local spatial balancing challenge). Subse- quently, we present our training strategies to enhance the ability of understanding spatial information (sec 3.3).\nBackbone. We choose Video-LLaVA [38] and its visual encoder LanguageBind [73] as the core framework. Video-LLaVA, which is optimized with a mixed dataset of images and videos, demonstrates leading performance across most image and video benchmarks. We employ Video-LLaVA as the backbone to explore the potential of Video-LLMs in extracting and localizing abnormal events.\nTask Formulation. Given a video V for M frames, each frame is labeled with 1 or 0, where 1 and 0 represent whether this frame conveys an abnormal event. The goal of M-VAE is to interactively generate the quadruple (sub, type, obj, sce) for each event along with the corresponding timestamp sta and end, where sub, type, obj, sce, sta and end are the subject, event type, object, scene, start time and end time of the abnormal event. As shown in Figure 1 (a), a man steals a car at street from 23s to 25s. Therefore, the output of our M-VAE task is {23s, 25s, (people, steal, car, street)}."}, {"title": "3.1 Global-local Spatial-enhanced MoE Module", "content": "As shown in Figure 2, we design a Global-local Spatial-enhanced MoE (GSM) Module for the global-local spatial modeling challenge. Inspired by Mixture-of-Experts (MoE) [24], we design three Local Spatial Experts (i.e., Local Action Expert, Local Object Relation"}, {"title": "3.2 Spatial Imbalance Regulator", "content": "After modeling the spatial information, we design a Spatial Imbal- ance Regulator (SIR) including a Gated Spatial Balancing Loss (GSB) for the global-local spatial balancing challenge, detailed as follows.\n\n\nwhere Nlocal is the number of local expert. gglobal is the weight of global expert. The first term of Eq.(7) is balancing between local experts, and the second term is balancing between local and global experts. The weights of four experts have already balanced when the loss is optimized to a minimum. This regulation achieves a better balance among all experts, reducing the impact of data imbalance, which effectively addresses the global-local balancing challenge.\nFinally, the overall loss of Sherlock can be represented as:\n\nwhere a is the hyper-parameter that controls the strength of Lgate, and LD is the next-token prediction loss of Video-LLMs."}, {"title": "3.3 Training Strategies for Sherlock", "content": "In order to enhance the ability of understanding spatial information, we design a two-stage training process. Stage 1 is to enhance the ability of understanding spatial information and Stage 2 is to address the M-VAE task, detailed as follows.\nStage 1. Pre-Tuning for spatial understanding. As shown in Figure 2, we first pre-tune Video-LLaVA using four high-quality"}, {"title": "4 Experimental Settings", "content": "4.1 Instruction Data Construction\nThe training pipeline of Sherlock contains two stages. As shown in Figure 4, for each stage, we construct the corresponding instruction dataset for better tuning.\nFor Stage 1. We construct a special understanding dataset based on Ref-L4 [3], HumanML3D [16], RSI-CB [32] and COCO [39]. Specifically, we manually design an instruction for each type of spatial information, for instance: Instruction: \"Judge the action of the characters in the image. Describe the image region  in the image. Judge the background of the image. Describe the image\". As HumanML3D has 25K videos with an average duration of 1 second, and we take 8 frames per second. For the data balance, we randomly select 20K images or frames from each dataset.\nFor Stage 2. We construct an M-VAE instruction dataset based on CUVA [10], which primarily consists of surveillance videos, with an average duration of 80 seconds per video. As this dataset in- cludes five detailed video Q-A tasks (i.e., timestamp, classification, reason, result, and description tasks), it is highly beneficial for con- structing our M-VAE dataset. 1) For abnormal event quadruples, constructing quadruples involves two steps. First, we collect an- swers from the reason, result, and description tasks in CUVA for each video. Subsequently, we construct initial quadruples through ChatGPT [49] based on the answers to these tasks, with the instruc- tion: \"Please extract the subject, object, and scene of the event based on the responses below\". Second, we create multiple candidate sets for subjects, objects, and scenes in quadruple. Specifically, for subjects and objects elements, we manually construct a set of around 40 for subjects and objects and filter elements based on this set. For event types elements, we adopt the 11 categories (i.e., Fighting, Animals, Water, Vandalism, Accidents, Robbery, Theft, Pedestrian, Fire, Violations, and Forbidden) from CUVA as the event types. For"}, {"title": "4.2 Baselines", "content": "In this paper, we select several advanced Video-LLMs as base- lines which are introduced as follows. VideoChat [47] employs Q-Former [33] to map visual representations to Vicuna [8]. Video- ChatGPT [47] integrates LLMs with CLIP [51] for video represen- tations. Valley [46] employs a temporal modeling module to bridge visual and textual modes. PandaGPT [54] utilizes ImageBind [14] to demonstrate cross-modal capabilities. mPLUG-Owl [65] intro- duces a visual abstractor module to align different modes. Chat- UniVi [26] merges visual tokens with semantic meanings. Video- LLAVA [38] conducts joint training on images and videos. To ensure a fair comparison, we re-implement these models using their re- leased codes in our experiments, with all LLMs sized at 7B."}, {"title": "4.3 Evaluation Metrics", "content": "M-VAE focuses on extracting event quadruples and locating ab- normal events from videos, requiring evaluation metrics in three aspects (i.e., extract event quadruples, locate abnormal events, and classify abnormal events). For the extraction performance, we measure our model through three perspectives. 1) Single: perfor- mance of generating each individual element. 2) Pair: performance of generating the element pair, i.e., Subject-Type pair, Object-Type pair, Subject-Scene pair, Object-Scene pair. 3) Quadruple Gener- ation: performance of generating the complete event quadruple.\nFollowing the prior works [30], the performance is evaluated with Macro-F1. Furthermore, we use T5-based and GPT-based metrics based on Video-bench [48] especially for LLM. For localization performance, we use the mAP@tIoU metric [71], calculated by mean Average Precision (mAP) at different IoU thresholds from 0.1 to 0.3 with 0.1 intervals. For classification performance, we refer to the traditional anomaly classification task [17, 45, 61] for anomaly classification metric, which mainly determines whether each video frame is abnormal or not in the video. We prefer Recall over Precision and report F2 [71] as another classification metric. Furthermore, our model focuses on accurately distinguishing abnor- mal events. As shown in Figure 1, it's better to mark all timestamps as abnormal than to miss any. So we prioritize false negative rates (FNRs): FNRs = num of false-negative frame which is the rate of mis- num of positive frame labeling an abnormal event frame as normal. In addition, t-test is used to evaluate the significance of the performance."}, {"title": "4.4 Implementation Details", "content": "In our experiments, we utilize open-source codes to obtain experi- mental results of all the baselines in Table 2. The hyper-parameters of these baselines remain the same setting reported by their public papers. For both Stage 1 and 2, we use a batch size of 16 and train for 1 epoch with the AdamW [43] optimizer and a cosine learning rate decay schedule with a warm-up period. The initial learning rate is 2e-5. The hyper-parameter a in L is set to 0.4. We tune the Video-LLaVA model using LoRA [22]. The LoRA matrix dimension, dropout rate, and dropout rate are 16, 64, and 0.05 respectively. Experiments are run on a single NVIDIA A100 GPU with 40GB memory. Stage 1 training takes about 16 hours, Stage 2 takes 60 hours, and inference takes about 8 hours."}, {"title": "5 Results and Discussions", "content": "5.1 Experimental Results\nTable 2 and Table!4 shows the performance comparison of different models on our M-VAE task, and we can see that: For extraction performance, our Sherlock model outperforms all baselines, with"}, {"title": "5.2 Contributions of Each Key Component", "content": "In order to further investigate the contributions of different modules of Sherlock, we conduct an ablation study on our Sherlock model. As shown in Table 2, w/o AE, w/o ORE, w/o BE, w/o GE, w/o EG, and w/o pre-tuning represent without four Spatial Experts, Expert Gate, and pre-tuning stage in sec 3.2 respectively.\nEffectiveness Study of Global and Local Spatial Expert. From Table 2, we can see that: The performance of w/o AE, w/o ORE, w/o BE and w/o GE degrades in all metrics, with an average decrease of 7.54 (p-value < 0.01), 7.57 (p-value < 0.01), 4.37 (p-value < 0.01), and 5.68 (p-value < 0.01) in FNRs, F2, average map@tIoU, and average event extraction metrics. This confirms the importance of global and local information in extracting and localizing abnormal events, and Sherlock can better model those information well.\nEffectiveness Study of Spatial Imbalance Regulator. From Table 2, we can see that: 1) Compared with Sherlock, w/o EG shows poorer performance in all metrics, with a decrease of FNRs, F2, average map@tIoU, and average extraction performance by 15.34 (p-value < 0.01), 16.52 (p-value < 0.01), 8.62 (p-value < 0.05) and 10.36 (p-value < 0.01), respectively. This demonstrates the ef- fectiveness of GSM in global-local spatial modeling and encourages us to consider handling heterogeneity issues between spatial in- formation in the manner of MoE. 2) From Table 2, we can see that compared to performance of w/o SIR, the performance of w/o MG is poorer, with FNRs, F2, average map@tIoU, and average event extraction metrics decreasing by 1.94 (p-value < 0.05), 3.9 (p-value"}, {"title": "5.3 Convergence Analysis and Practical Assessment for Sherlock", "content": "In order to analyze the convergence of Sherlock, we record the loss of baseline Video-LLMs, Sherlock, and its variant without specific components over various training steps. The results are shown in Figure 5 and we can see that: 1) Sherlock demonstrates the fastest convergence compared to other Video-LLMs. At the convergence point, the loss of Sherlock is 1.05, while Video-LLaVA is 2.06. This underscores the high efficiency of Sherlock over other advanced Video-LLMs. 2) Sherlock demonstrates the fastest convergence compared to its variant without specific components in Figure 5. This justifies that the spatial information along with GSM and SIR can accelerate the convergence process, which further encourages us to consider the spatial information in the M-VAE task.\nTo assess practicality, we analyze the FNRs of Sherlock for each scene. As shown in Table 3, we can observe that in every scene, Sherlock outperforms other Video-LLMs. This indicates that the possibility of misclassifying abnormal events as normal events is minimized, thereby demonstrating the importance of global and local spatial modeling of Sherlock. We also analyze the average inference time in seconds for a one-minute video. As shown in Figure 7 (b), Sherlock does not perform much differently from the other models in terms of inference time. This is reasonable, as some studies confirm that the MoE architecture can improve efficiency"}, {"title": "5.4 Qualitative Analysis for Sherlock", "content": "As shown in Figure 8, we visualize and compare Sherlock with other Video-LLMs. We randomly select two samples from our dataset and ask these models to Analyze the following video and localize the timestamp and extract the quadruple of the abnormal events. From the figure, we can see that: 1) Accurately localizing ab- normal events and extracting correct quadruples is a huge challenge. For instance, example 2 captures a segment from 9s to 15s, where identifying the collision of the truck at road is challenging, 2) Com- pared with other advanced Video-LLMs, Sherlock shows excellent performance in localizing abnormal events. In example 1, Sherlock outperforms other models in terms of accuracy. In example 2, it outperforms PandaGPT in terms of accuracy and can generate a correct quadruple. This further demonstrates the effectiveness of Sherlock in precisely extracting and localizing abnormal events."}, {"title": "6 Conclusion", "content": "In this paper, we firstly propose a new M-VAE task and a con- structed instruction dataset, making a significant contribution to future research on abnormal events. Secondly, we propose a Global- local Spatial-sensitive LLM named Sherlock to assist in localizing and extracting abnormal event quadruples. This model includes a Global-local Spatial-enhanced MoE module and Spatial Imbalance Regular to model and balance spatial information. In the end, our experimental results demonstrate the outstanding performance of Sherlock. In future work, we hope to consider the relationships be- tween events and enrich our tasks with event inference to improve the performance of extraction. In addition, we also hope to improve the interpretability of our model by providing explanations for each abnormal event."}]}