{"title": "rule4ml: An Open-Source Tool for Resource Utilization and Latency Estimation for ML Models on FPGA", "authors": ["Mohammad Mehdi Rahimifar", "Hamza Ezzaoui Rahali", "Audrey C. Therrien"], "abstract": "Implementing Machine Learning (ML) models on Field-Programmable Gate Arrays (FPGAs) is becoming increasingly popular across various domains as a low-latency and low-power solution that helps manage large data rates generated by continuously improving detectors. However, developing ML models for FPGAs is time-consuming, as optimization requires synthesis to evaluate FPGA area and latency, making the process slow and repetitive. This paper introduces a novel method to predict the resource utilization and inference latency of Neural Networks (NNs) before their synthesis and implementation on FPGA. We leverage HLS4ML, a tool-flow that helps translate NNs into high-level synthesis (HLS) code, to synthesize a diverse dataset of NN architectures and train resource utilization and inference latency predictors. While HLS4ML requires full synthesis to obtain resource and latency insights, our method uses trained regression models for immediate pre-synthesis predictions. The prediction models estimate the usage of Block RAM (BRAM), Digital Signal Processors (DSP), Flip-Flops (FF), and Look-Up Tables (LUT), as well as the inference clock cycles. The predictors were evaluated on both synthetic and existing benchmark architectures and demonstrated high accuracy with R\u00b2 scores ranging between 0.8 and 0.98 on the validation set and sMAPE values between 10% and 30%. Overall, our approach provides valuable preliminary insights, enabling users to quickly assess the feasibility and efficiency of NNs on FPGAs, accelerating the development and deployment processes. The open-source repository can be found at https://github.com/IMPETUS-UdeS/rule4ml, while the datasets are publicly available at https://borealisdata.ca/dataverse/rule4ml.", "sections": [{"title": "1. Introduction", "content": "Neural Networks (NNs) are a subset of Machine Learning (ML) models, popularized due to their ability to model complex patterns and relationships in data. These algorithms are transforming several fields such as healthcare, finance, manufacturing, and entertainment by enabling predictive analytics, automating processes, and enhancing decision-making [1,2]. In particular, the surge in data generated by advanced detectors over the past decade has created a need for data compression at the source, a task at which NNs have proven highly effective [3,4]. However, the high data rates from instrumentation require processing units with minimal latency to ensure real-time performance from NN implementation [5].\nCompared to common processing units such as Central Processing Units (CPU) and Graphics Processing Units (GPU), Field Programmable Gate Arrays (FPGAs) provide a large number of Input/Output (I/O) ports, and their architectures are designed to fully exploit pipelining and parallelism, enabling continuous and concurrent computing [6]. The implementation of FPGA-based Edge Machine Learning (EdgeML) is an active area of research, particularly targeting instrumentation applications that require minimal latency. For instance, a fast inference of boosted decision trees on FPGAs near the sensor is used in particle physics applications [7]. Ultra-low-latency and low-power neural networks with convolutional layers are also employed on FPGAs for various instrumentation applications [8]. Additionally, FPGA-embedded systems are being developed for ML-based tracking and triggering in electron-ion collider experiments [9].\nDeveloping NNs on FPGA using traditional Hardware Description Language (HDL) requires significant time and effort, as well as a high understanding of hardware architecture [10]. Accordingly, some tool-flows have been developed to ease the implementation of NNs on FPGA. The tool-flows translate NNs implemented using common ML libraries into High-Level Synthesis (HLS) code, enabling FPGA implementation with minimum hardware knowledge. In our previous review [11], we determined that HLS4ML [12] stands out as the current best option, considering its wide support for different NN architectures, and its ability to adapt to the application requirements and achieve low power and low latency, both essential for EdgeML solutions and instrumentation applications. However, in its current state, HLS4ML cannot indicate whether an NN will fit the targeted board before synthesis. This is a significant limitation, as larger NNs may require several hours to synthesize, and the resource utilization report is only available after the synthesis phase. The resource utilization report details the usage of Block RAM (BRAM), Digital Signal Processors (DSP), Flip-Flops (FF), and Look-Up Tables (LUT), revealing whether the network implementation exceeds the capacities of a board. Additionally, HLS4ML currently lacks a latency approximation for the NNS before synthesis, which is crucial for projects with strict timing constraints. In this context, latency refers to the number of clock cycles required for a single inference of"}, {"title": "2. Background", "content": "HLS4ML stands out among several NN-to-FPGA implementation tool-flows for its exceptional performance in key areas: minimal latency, efficient resource utilization, user-friendliness, and configuration flexibility [11]. The effectiveness of HLS4ML has been demonstrated in various research projects implementing diverse NNs on FPGAs. For example, a real-time semantic segmentation on FPGAs for autonomous vehicles with minimal latency was achieved using HLS4ML [14]. Ultra-low latency Recurrent Neural Network (RNN) inference on FPGAs for physics applications has also been studied [3]. Additionally, HLS4ML was used to explore efficient compression algorithms at the edge for real-time data acquisition in a billion-pixel X-ray camera in one of our previous works [1].\nFigure 1 illustrates how HLS4ML translates an NN from standard Python code into HLS code ready for FPGA implementation. The first step involves designing an NN architecture for the targeted task. This can be done using Keras or PyTorch libraries, possibly using the Open Neural Network Exchange (ONNX) format. Tools such as QK-"}, {"title": "3. Methodology", "content": "To train regression models capable of predicting both the resource utilization and the inference latency of an HLS4ML-synthesized NN, we generated and synthesized a diverse collection of NN architectures. For accurate predictions, the training dataset must encompass a wide range of architecture-related parameters, including various input and output sizes, different numbers of layers and neurons, along with various types of layers and operations: matrix multiplication, non-linear activations, skip connections, batch normalization, etc. However, for the current iteration, we choose to exclude computationally taxing operations such as convolution, speeding up the synthesis and data generation processes.\nThe parameters of a synthetic NN are randomly selected from their respective ranges listed in Table 1. A few additional HLS4ML-related parameters are required for the synthesis:\nPrecision: Specifies the total number of bits used to represent each of the inputs, outputs, weights, and biases of layers.\nReuse Factor: Determines the number of times an FPGA multiplier is reused, controlling the level of pipelining, and directly impacting the latency and resources.\nStrategy: Chooses whether to prioritize resource or latency optimization.\nBoard: Selects the target board, for which the synthesis report is generated.\nThe value ranges and their increments were selected to encapsulate a wide variety of NNs, targeting different tasks (prediction, classification, clustering, etc.), all while making sure the architectures remain within reasonable bounds, both in terms of duration and feasibility of the synthesis. While HLS4ML's precision and strategy parameters can be configured on a per-layer basis, we chose to assign a global value across all layers of the generated network, limiting the otherwise massive number of possible combinations. The reuse factor, however, can still change from one layer to another since its maximum value is bounded by a layer's input and output sizes.\nIn addition to the primary parameters listed in Table 1, probability functions dictate the use of certain layers, such as skip connections and batch normalization. These probabilities are based on the network's size and complexity, enhancing the dataset's representation of real-world architectures. The dataset contains more than 15,000 synthetic NNs, generated within approximately 28,800 CPU-core hours. The synthesis ran in parallel on 2 x Intel Xeon Processor E5-2660 v2 and 2 x Intel Xeon Processor E5-2680 v4. The project repository\u2020 includes the code used for data generation, while the datasets\u2021 can be accessed separately."}, {"title": "3.2. Feature engineering", "content": "As networks are generally structured in a sequence of layers, all architecture-related data is inherently layer-dependent, where each layer can have its own set of parameters. While some parameters are universally applicable, such as the layer type (e.g., fully connected, activation, batch normalization) and its input and output sizes, others such as the number of neurons, bias, and trainable parameters are exclusive to certain layers within the architecture. Consequently, there are two main approaches for training the predictors: either choose a regression model architecture capable of handling the sequential nature of the inputs or convert the raw data into a format compatible with a wider range of architectures.\nSome ML models are inherently designed to operate on sequential data: architectures such as RNNs and Transformers can capture local and global dependencies within sequences [16,17]. At first glance, these models might appear well-suited to the sequential nature of our data. However, training RNNs can be challenging due to vanishing and exploding gradients [18], while Transformers are prone to be overly complex and fail to achieve state-of-the-art performance on some tasks [19]. Alternatively, extracting and creating meaningful network-level features from the raw sequential data enables the use of regression ML models like decision trees, random forests, or Multi-Layer Perceptron (MLP).\nWhile these simpler models are easier to train, tune, and interpret, ultimately our goal is to achieve the highest accuracy for the predictions. The evaluation of the models will be the object of the next section, using both sequential raw inputs and extracted features. The rest of this section will focus on feature engineering, extracting network-level features from the original layer-wise data.\nThe raw features are distilled into more meaningful engineered features mainly through aggregation and statistical averages. Additional features are calculated from the number of fixed-point operations required for a layer's implementation in HLS4ML. The fixed-point operations include addition, multiplication, logical operations, and lookup operations, i.e. retrieving a value from a pre-computed table using an index. Engineered features, extracted from the raw data for each network in the dataset, are listed below:\nAggregated Features: The total counts of dense layers, batch normalization layers, skip layers, dropout layers, and the count of each type of activation.\nStatistical Features: The average number of parameters, inputs, outputs, and the average reuse factor of fully connected layers specifically, as they are expected to be the most computationally intensive.\nFixed-point Operations: The single counts of addition, multiplication, logical, and lookup operations used across the entire architecture, multiplied by the HLS4ML precision.\nMeanwhile, the dependent resource variables (BRAM, DSP, FF, and LUT), having unbounded and considerably different ranges of values, are scaled down to a normalized interval representing the NN's utilization percentage of the target board's available resources. The lower limit of the range is 0%, while the upper bound is set to 200%,"}, {"title": "3.3. Model selection", "content": "considering possible inaccuracies within the synthesis reports. We expect NNs using more than 200% of any resource to fail the implementation phase, regardless of potential post-synthesis optimizations. Therefore, we consider prediction accuracy irrelevant for those values, limiting them to the set upper bound.\nFurthermore, we calculated the Spearman correlation [20] to study dependencies between features and prediction variables. The correlation matrix depicted in Figure 2 illustrates the relationship between the engineered features and the resources and latency values. For the generated dataset, the resources and latency (cycles) primarily correlate with features extracted from fully connected layers, plus the summed numbers of fixed-point operations. As expected, the reuse factor appears to have a higher positive correlation with clock cycles and a negative correlation with resources. The low correlation between resource utilization and reuse factor can be attributed to our dataset's composition. The dataset is dominated by larger boards (ZCU102 and Alveo-u200) and lower precisions (2 and 8-bit). These factors inherently lead to lower resource consumption, reducing the impact of reuse on overall resource utilization. Other unexpected trends are also noticeable: the softmax layer seems highly correlated with the DSPs despite its HLS4ML implementation mainly relying on lookup tables.\nTo evaluate the approaches, we trained regression models both with and without the engineered features. Specifically, we experimented with Long Short-Term Memory (LSTM) and Transformers [21], Random Forests [22], Gradient Boosted Trees [23], and MLP models. Feature engineering was applied to extract meaningful inputs for simpler models, while the sequential ones (LSTM and Transformer) were trained on the raw, layer-based data. The results of the preliminary training are summarized in Table 2, highlighting the performance differences between the different models."}, {"title": "4. Results", "content": "To assess the models' predictive performance, an initial evaluation is conducted on a test set of 500+ models split from the synthetic data, generated according to Table 1. The results are depicted in Figure 4 using box plots, with the resource predictions on the left (a) and latency prediction on the right (b). As illustrated in (a), the median error for all resources is very low, suggesting that the predictions are generally accurate. The mean prediction errors for LUT is around 10%, while the errors for BRAM, DSP, and FF are even lower. The Interquartile Range (IQR), the range between the first (Q1) and third quartiles (Q3), is also tight, indicating that most predictions are closely packed around the median. Additionally, the outliers for all resources seem mostly skewed toward lower values, closer to the Q3 + 1.5x IQR range.\nThe right box plot shows the prediction errors for latency, measured in the number of cycles. Similar to the resources box plots, the median and mean are low and within the acceptable range. However, the IQR is large since the errors are not measured in percentages but rather using absolute values, resulting in higher prediction variability."}, {"title": "5. Discussion", "content": "For resource prediction, we consider an error lower than 10% to be acceptable. According to predictions on the synthetic test set (Figure 4), we found that, on average, 81% of resource predictions fall within this acceptable range. Breaking it down by resource type, 80% of BRAM predictions, 89% of DSP predictions, 88% of FF predictions, and 66% of LUT predictions are within the 10% error threshold. Our results reveal that the LUT prediction errors are generally higher than the rest of the resources (Figure 4). This is primarily because LUT is the most used resource, with its ground truth covering a wide range of values. For example, even a configuration with low precision and high reuse that shows negligible usage of BRAM, DSP, and FF can still employ a significant amount of the FPGA's LUTs. In contrast, the furthest outliers of the FF errors indicate that the corresponding model might struggle to make accurate FF predictions. We attribute this to two reasons: the distribution of FF values in the dataset is heavily skewed, where most values are close to the lower and upper thresholds (0% and 200%)."}, {"title": "6. Conclusion", "content": "Implementing ML models on FPGAs is becoming common across various domains, leveraging FPGA's reduced latency compared to alternative processing units for accelerating ML inference tasks. However, developing NNs for FPGAs is significantly time-consuming. HLS4ML accelerates this process but lacks the ability to inform users if an NN fits the targeted board before synthesis. This is problematic as larger networks can take hours to synthesize, with resource utilization and latency reports available only after a successful synthesis. Each failed synthesis further slows down the development process.\nThis paper demonstrates the efficiency of using ML models to predict both resource utilization and latency reports for NNs prior to FPGA implementation. The prediction models, tested on both generated NNs and existing benchmark architectures, showed high accuracy, achieving low average errors and high R\u00b2 scores across various dependent variables. While our results reveal commendable prediction accuracy and minimal error for resources and latency, our current scope is confined to predicting FCNNs. Consid-"}, {"title": "Funding", "content": "This research was undertaken, in part, thanks to funding from the Canada Research Chairs Program. A. C. Therrien holds the Canada Research Chair in Real-Time Intelligence Embedded for High-Speed Sensors."}, {"title": "Disclosures", "content": "The authors declare no conflicts of interest."}, {"title": "Ethics statement", "content": "This paper reflects the authors' research and analysis in a truthful and complete manner."}, {"title": "Data availability", "content": "The data that support the findings of this study is publicly available."}]}