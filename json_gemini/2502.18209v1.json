{"title": "LAG: LLM agents for Leaderboard Auto Generation on Demanding", "authors": ["Jian Wu", "Jiayu Zhang", "Dongyuan Li", "Linyi Yang", "Aoxiao Zhong", "Renhe Jiang", "Qingsong Wen", "Yue Zhang"], "abstract": "This paper introduces Leaderboard Auto Generation (LAG), a novel and well-organized framework for automatic generation of leaderboards on a given research topic in rapidly evolving fields like Artificial Intelligence (AI). Faced with a large number of AI papers updated daily, it becomes difficult for researchers to track every paper's proposed methods, experimental results, and settings, prompting the need for efficient automatic leaderboard construction. While large language models (LLMs) offer promise in automating this process, challenges such as multi-document summarization, leaderboard generation, and experiment fair comparison still remain under exploration. LAG solves these challenges through a systematic approach that involves the paper collection, experiment results extraction and integration, leaderboard generation, and quality evaluation. Our contributions include a comprehensive solution to the leaderboard construction problem, a reliable evaluation method, and experimental results showing the high quality of leaderboards.", "sections": [{"title": "1 Introduction", "content": "The explosive growth of scientific publications has created both unprecedented opportunities and significant challenges for researchers seeking to stay abreast of state-of-the-art methods (Bornmann et al., 2020; Wang et al., 2024; \u015eahinu\u00e7 et al., 2024). Leaderboard platforms, such as NLP-progress\u00b9 and Papers-With-Code\u00b2 have become invaluable by offering comprehensive overviews of recent research developments, highlighting ongoing trends, and identifying future directions. However, the large amount of daily papers makes it increasingly difficult to update these leaderboards automatically and promptly. Figure 1 illustrates two pressing issues:\nFirst, the number of LLM-related articles submitted to arXiv has surged dramatically-from 2022 to 2025, with over 20,000 submissions in 2024 alone. Second, even as new methods continuously emerge, leaderboards, such as the one for Multi-hop Question Answering on the HotpotQA(Yang et al., 2018) dataset, remain stagnant, with the latest method dating back to 2023. These observations highlight a serious issue: the rapid accumulation of daily scientific publications often outpaces the capability of researchers to keep up with cutting-edge research and state-of-the-art methods, emphasizing the growing need for more efficient methods to generate the latest and useful leaderboards.\nPrior efforts have attempted to address this gap. A line of work (Hou et al., 2019; Kardas et al., 2020) has proposed leaderboard construction methods that directly extract scientific entities from individual NLP papers, and construct a static leaderboard without updating and maintenance. Semi-supervised scientific NER, proposed by Li et al. (2023), focuses on extracting scientific entities from both tables and text. \u015eahinu\u00e7 et al. (2024) introduce SCILEAD, a manually-curated Scientific Leaderboard dataset, including 27 leaderboards derived from 43 NLP papers. However, all previous methods have been limited to the extracted scientific entities and only give a static snapshot after extracting information from a narrow selections.\nWe consider using LLMs such as GPT-4 (Achiam et al., 2023), Qwen (Yang et al., 2024), and O1-preview which have demonstrated exceptional performance across diverse NLP tasks, especially in the long-context scenario (Chen et al., 2023a,b; Wang et al., 2023b) to automatically generate leaderboards based on given research topic.\nDirectly applying LLMs to this task still faces several key challenges. First, Limited Paper Coverage: It is challenging for human to search for all papers on a certain scientific topic, due to the overwhelming number of constantly emerging publications. "}, {"title": "2 Related Work", "content": "LLM for Scientific Research. In the realm of LLMs, several studies have explored using LLMs for improving work efficiency in scientific research. Baek et al. (2024) and Yang et al. (2023) proposed a multi-agent-based scientific idea generation method to boost AI-related research. To evaluate the quality of LLM-generated ideas, Si et al. (2024) introduced a comprehensive human evaluation metric. Wang et al. (2023a) proposed SciMON, a method that uses LLMs for scientific literature retrieval. Wang et al. (2024) proposed an Auto-Survey to automatically generate scientific surveys based on the given research topic. The AI Scientist, Lu et al. (2024) introduced a fully automated, prompt-driven research pipeline. To make LLM-generated ideas more diverse and practical, Weng et al. (2024) proposed CycleResearcher, an iterative self-rewarding framework that allows the LLM to refine its ideas continuously, enhancing both diversity and practicality in research proposal generation. However, no previous research focused on the leaderboard generation for researchers to search, organize, and compare the state-of-the-art methods rapidly and fairly based on a certain research topic.\nLeaderboard Construction. Table 1 illustrates the differences between the previous work and LAG. First of all, previous work builds leaderboards by using data sources such as NLP-progress or Papers-With-Code. However, these sources lack rigorous quality assurance, such as standardizing scientific entities across different leaderboards and ensuring complete coverage of relevant publications. Instead, we choose arXiv, which is a free distribution service and an open-access archive for nearly 2.4 million scholarly articles in different domains, providing a large amount of publications for researchers. Similar to our work, Hou et al. (2019), Kardas et al. (2020), and Singh et al. (2024) extract \u201cTask\u201d, \u201cDataset\u201d, \u201cModel\u201d along with the experiment result entities as TDM triples to build a leaderboard. Yang et al. (2022) and KABENAMUALU et al. (2023) leverage the pre-defined TDM triples in an extraction process similar to Hou et al. (2019). Since these approaches require a pre-defined taxonomy of TDM triples, they are incompatible with realistic task definitions. In short, none of the previous work is adaptable to constantly emerging benchmarks driven by new research and innovation. Moreover, none of the studies extract the experiment settings as additional information to generate leaderboards, which results in a lack of fair comparison. In scientific research, experiment settings are important for educators or users to reproduce the experimental results claimed in scientific publications. In this work, we address the aforementioned problems. Specifically, we (1) dynamically download scientific publications and generate up-to-date leaderboards based on the given scientific topic and the specific date; (2) extract experiment settings as part of leaderboards for fair comparison; (3) apply a Multi-Agent-as-Judge to evaluate leaderboard quality."}, {"title": "3 Methods", "content": "Figure 2 depicts LAG, which consists of four stages: Paper Collection and Split, Table Extraction and Classification, Table Unpacking and Integration, and Leaderboard Generation and Evaluation. Each stage is meticulously designed to address specific challenges associated with leaderboard generation, thereby enhancing the efficiency and quality of the resulting leaderboards. The whole process is iterated several times (e.g., five times) to generate a high-quality leaderboard."}, {"title": "3.1 Paper Collection and Split", "content": "Utilizing the off-the-shelf tools \u00b3, LAG first searches and retrieves a set of papers $P_{init}$ = {$P_1$, $P_2$, ..., $P_N$} from arXiv and downloads LaTeX code files related to a specific scientific research topic T. Then, we specify a certain date and filter out all papers published before the date. The filtering stage is important for ensuring that the generated leaderboards are grounded in the most relevant and recent research. Moreover, since the search tool just identifies only the keywords in the paper title and abstract, which can lead to a significant amount of noisy data, we also introduce a retrieval model to filter out papers that are irrelevant to the given topic and retrieve topic-related papers. The set of filtered papers $P_{filtered}$ = {Retrieval{$P_1$, $P_2$, ..., $P_u$}} is used to generate the leaderboards, ensuring comprehensive coverage of the topic and logical structure. Due to the extensive number of relevant papers retrieved and filtered during this stage, the total input length of $P_{filtered}$ often exceeds the maximum input length of LLMs. Since most of the LaTeX content is unproductive for generating leaderboards, we split the LaTeX code into several sections based on the structure of each paper. Most tables, table-related descriptions, experiment results, and experiment settings are located in the \u201cExperiment\" section, which contains the key information for generating leaderboards. Consequently, we select all \"Experiment\" sections as well as all tables {Table1, Table2, ..., Tableu} and all table-related descriptions {$D_1$, $D_2$, ...$D_u$}, extracted from all papers, as input for the next stage."}, {"title": "3.2 Table Extraction and Classification", "content": "Typically, a scientific paper, such as those in the natural language processing domain, contains several types of tables, including \u201cMain Results\u201d, \u201cAblation Study\", and \u201cOthers\u201d. The \u201cMain Results\" tables are the most important tables in the paper, which illustrate the novelty, contributions, and effectiveness of the proposed methods or models by comparing the experiment results of the proposed method with other baselines. We utilize these tables for leaderboard generation. The \"Ablation Study\" tables examine the effect of damaging or removing certain components in a controlled setting to investigate all possible outcomes of system failure. The \u201cOthers\u201d tables are the tables that illustrate the supplementary information of the experiments. For example, some tables illustrate the dataset statistics of the benchmark used in the experiments, while other tables list the results of \u201cCase Study\u201d and \u201cError Analysis\". To address this, we propose an agent that uses the In-Context Learning method (Dong et al., 2022) to manually select one table from each of the three different types. The agent then prompts LLMs to classify the table types and only keeps the \"Main Experiments\u201d tables and their descriptions as the final input. The $i$th table types can be described as: LLM(Tablei, $D_i$; Prompt) \u2192 Table type.\nIn practice, the most intrinsic approach is to divide Stage 2 into the following sequential steps: (1) Extract all tables and their associated captions from the LaTeX code. (2) Classify the extracted tables according to predefined table types. (3) Extract metrics, performance values, and experimental settings related to the proposed model from tables categorized as \u201cMain Results\". Moreover, each of these three steps necessitates the use of LLM APIs, and repeated reference to certain table contents further exacerbates the substantial waste of tokens. To address this issue, we create the agent following the few-shot Chain of Thought (CoT) prompting process, enabling it to classify and extract information from identified \"Main Results\" tables in a single dialogue round. Specifically, in the requested JSON output, we additionally set the key points as follows: \"number of tables (Int)\", \"classification of tables (Dict)\" and \"selected table's index (Int)\"."}, {"title": "3.3 Table Unpacking and Integration", "content": "Following the table extraction and classification phase, each table Tablei is sent into the LLM to extract the core information. To build a useful and high-quality leaderboard, we define four types of scientific terms: Datasets, Metrics, Experiment Results, and Experiment Settings. For datasets, we use LLMs to count the frequency in all filtered papers $P_{filtered}$ of each dataset under a certain research topic and retain the top-K (K=5) datasets with the highest frequency of occurrence in scientific papers. For the rest of the three scientific terms, we utilize LLMs to extract from given Tablei with a related table description $D_i$. After scientific term extraction, we recombined them into a quintuple, including the paper title as the unique identification ID. Each paper can produce one quintuple and finally we get a raw leaderboard with M quintuples from M filtered papers. The raw leaderboard is reranked on the basis of the experiment results."}, {"title": "3.4 Leaderboard Generation and Evaluation", "content": "After we obtain K leaderboards based on top-K frequent datasets, the final stage involves a quality evaluation based on our pre-defined four criteria, which is shown in Table 4 in Appendix. Each leaderboard is assigned three scores based on \u201cCoverage\u201d, \u201cLatest\u201d and \u201cStructure\u201d. Since a research topic may contain several datasets, the \"Multi-Aspect\" is the average quality score that is used to evaluate the LLM-generated leaderboards for each dataset. The best leaderboard is chosen from N candidates. LLMs critically examine the leaderboards in several aspects. The final output of Leaderboard is $L_{best}$ = Evaluate($L_{ca1}$, $L_{ca2}$, ..., $L_{caN}$).\nThe methodology outlined here, from paper collection to leaderboard evaluation, ensures that LAG effectively addresses the complexities of leaderboard generation in the AI domain using advanced LLM agents. We provide Pseudo-code for easily understanding, which is shown in Algorithm 1."}, {"title": "4 Experiments", "content": "We designed experiments for LAG, aiming to answer four questions: RQ-1: Can LAG address the paper coverage issue and generate fair leaderboards by incorporating the latest baselines? RQ-2: Can LAG reduce time consumption? RQ-3: Is the evaluation consistent between LAG and human experts? RQ-4: Is each proposed component of LAG useful?"}, {"title": "4.1 Experimental Setup", "content": "We evaluated LAG's performance by testing its ability to generate leaderboards for specific topics across various complex settings."}, {"title": "4.1.1 Evaluation Metrics", "content": "We use two metrics to evaluate the quality (topic-related and leaderboard content) and speed of leaderboard generation, respectively, in response to the three challenges mentioned in the introduction. (1) Topic-related Quality: The aforementioned arXiv crawler employs regular expression matching in the abstract section to identify papers related to specified topics. While this method is efficient, it is relatively rudimentary and cannot guarantee that all retrieved papers meet our requirements. The quality of these papers not only directly affects the final leaderboard, but low-quality candidate papers can also significantly prolong the time required for construction. Therefore, it is essential to evaluate the quality of the retrieved articles. We evaluate the quality of content from the following two aspects. (i) Recall: It measures whether all items in the generated leaderboard are related to the given research topic. (ii) Precision: It identifies irrelevant items, ensuring that the items in the generated leaderboards are pertinent and directly support the given research topic.\n(2) Leaderboard Content Quality: The evaluation metric of leaderboard content quality includes four aspects. Each aspect is judged by LLMs according to a 5-point, calibrated by human experts. The evaluation criteria are listed in Table 4. (i) Coverage: Assess each paper represented on the LAG-generated leaderboards encapsulates all aspects of the topic. (ii) Latest: Test whether all papers represented on the LAG-generated leaderboards are latest. (iii) Structure: Evaluate the logical organization and determine whether LAG leaderboards are missing any items. (iv) Multi-aspect: Average score of the previous three criteria for LAG-generated leaderboards. (3) Leaderboard Construction Speed: Manually building a leaderboard is a time-consuming and laborious task. This process can be divided into the following main components: $T_r$ (search for papers on a specific topic), $T_b$ (browse all retrieved articles and develop several highly frequent datasets), $T_f$ (filter candidate articles based on the selected datasets), $T_e$ (read and extract information), and $T_c$ (the integration and construction time). And the total time consumption can be calculated as:\n$T_{manual}$ = $T_r$ + $T_b$ + $T_f$ + $T_e$ + $T_c$. (1)\nGiven L denotes the length of the leaderboard, $N_{retrieved}$ number of retrieved articles, $N_{filtered}$ number of articles retained, and P the proportion of valid articles with $P = \\frac{N_{filtered}}{N_{retrieved}}$. We find that $T_b$ and $T_f$ are strongly correlated with leaderboard length L and the Topic-related quality:\n{$T_b$, $T_f$} \\propto $\\frac{L}{\\frac{P \\cdot N_{retrieved}}{L \\cdot N_{retrieved}} - \\frac{L}{N_{filtered}}}$. (2)\nWhile $T_r$ is relatively fixed, $T_e$ and $T_c$ usually only have a positive correlation with L.\nFor LAG, we barely account for all the invocation time of the agents' API calls. Compared to manual work, which often takes several days, LAG reduces the total time cost in the minute level. This is largely attributed to the task decomposition conducted in this paper, the division of labor and scheduling among agents, and the superior performance of the LLMs."}, {"title": "4.1.2 Baselines", "content": "We employ proprietary and open-source LLMs in our experiments and set the temperature to 0.7 for proprietary models. For proprietary models, we adopt GPT-40 (Achiam et al., 2023), and the O1-preview. For open-source LLMs, we adopt Qwen2.5-7B and Qwen2.5-14B (Yang et al., 2024). We provide a detailed illustration of our designed prompts for different stages in Appendix A."}, {"title": "4.2 Experiment Results", "content": "4.2.1 Performance Comparison (RQ-1)\nTopic-related Quality Evaluation: Table 2 illustrates the Topic-related Quality LAG achieved a recall of 67.58% and a precision of 70.33% with 20 items, indicating that it successfully retrieved a large proportion of relevant papers while maintaining a low rate of irrelevant ones. This performance is crucial for ensuring that the generated leaderboards are both comprehensive and accurate. The high precision and recall scores illustrate that LAG could help solve the paper coverage problem.\nFair Comparison: To ensure fair comparison, LAG extracted all experiment settings as part of the LAG-generated leaderboards. We provide a detailed case study of LAG-generated leaderboards with experiment settings in Appendix B.\nContent Quality Evaluation: Table 2 presents the results of leaderboard quality generated by LAG and the baselines. LAG consistently achieved high scores across all evaluation metrics, particularly in terms of Coverage and Latest, indicating its ability to include a wide range of relevant and recent papers. For example, at a leaderboard length of 20 items, LAG achieved a Coverage score of 4.12 and a Latest score of 3.96, approaching human performance (4.72 and 4.68, respectively). While manual leaderboards scored slightly higher in content quality, LAG significantly reduced the time required for leaderboard generation, demonstrating its efficiency."}, {"title": "4.2.2 Efficiency Analysis (RQ-2)", "content": "Construction Speed: LAG dramatically reduced the time required to generate leaderboards compared to manual methods. For instance, generating a 20-item leaderboard with LAG took approximately 120 seconds, while manual construction took over 18 minutes. This speed advantage makes LAG a practical tool for researchers who need up-to-date leaderboards in rapidly evolving fields. The high speed of LAG illustrates that LAG could help generate high-quality leaderboards timely."}, {"title": "4.2.3 Meta Evaluation (RQ-3)", "content": "To verify the consistency between our proposed LLM evaluation strategy and human evaluation, we conduct a correlation evaluation involving human experts and our automated evaluation method. Human experts judge pairs of generated leaderboards to determine which one is superior. We compare the judgments made by our method against those made by human experts. Specifically, we provide experts with the same scoring criteria used in our evaluation for reference. The experts rank the 20 LAG-generated leaderboards, and we compare these rankings with those generated by the LLM using the Pearson Correlation Coefficient to measure consistency between human and LLM evaluations.\nThe results of this meta-evaluation are presented in Figure 4. The table shows the Pearson Correlation Coefficient values, indicating the degree of correlation between the rankings given by each LLM and the human experts. The Pearson correlation coefficient values indicate a strong positive correlation between the quality scores provided by the LLM and those given by human experts, with the Ol-preview achieving the highest correlation at 0.76. These results suggest that our evaluation method aligns well with human preferences, providing a reliable proxy for human evaluation."}, {"title": "4.2.4 Ablation Study (RQ-4)", "content": "To understand the contribution of each component in LAG, we conducted an ablation study by removing key components of LAG as follows: (1) LAG w/o Table Classification: We removed the table classification step, which led to a slight decrease in Structure and Multiaspect scores, indicating that classifying tables is essential for maintaining a logical and well-organized leaderboard. (2) LAG w/o Refinement: We disabled the Refinementing step, which resulted in a minor drop in Coverage and Latest scores, suggesting that Refinementing helps refine the leaderboard by ensuring that only the most relevant and recent papers are included. As shown in Table 3, the results of the ablation study confirm that each component of the LAG plays a crucial role in achieving the generation of the high-quality leaderboard."}, {"title": "5 Conclusion", "content": "We introduce LAG, a novel agent framework leveraging large language models to automatically generate the latest, and high-quality leaderboards based on given research topics. LAG addresses key challenges including paper coverage, fair comparison, and timeliness through a systematic approach involving paper collection and split, table extraction and classification, table unpacking and integration, and leaderboard generation and evaluation. Experiments showed that LAG can automatically generate new, high-quality leaderboards in a relatively short time and match human performance topic-related quality and content qulaity. This advancement offers a scalable and effective solution for synthesizing the latest leaderboards, providing a valuable tool for researchers in rapidly evolving fields like artificial intelligence."}, {"title": "Limitations", "content": "One limitation of LAG is its reliance on the quality of the retrieved papers. While our topic-related quality metrics are strong, there is still room for improvement in ensuring that all relevant papers are included. Future work could explore more sophisticated retrieval models to further enhance the coverage of the generated leaderboards. Another limitation is, a specific dataset may contain several evaluation metrics, and different papers may use different metrics to evaluate proposed models' performance, bringing challenges for leaderboard generation and baseline comparison."}]}