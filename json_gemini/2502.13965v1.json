{"title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs", "authors": ["Michael Luo", "Xiaoxiang Shi", "Colin Cai", "Tianjun Zhang", "Justin Wong", "Yichuan Wang", "Chi Wang", "Yanping Huang", "Zhifeng Chen", "Joseph E. Gonzalez", "Ion Stoica"], "abstract": "Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program.\nTo address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) as autonomous agents enhance their problem solving capabilities by scaling their inference computation\u2014that is, increasing the number of output tokens or LLM calls [9, 12,22,30,31,65]. With more calls and tokens, LLMs endow agents with improved reasoning [19,76,84, 85], planning and search capabilities [56,91], self-reflection from prior experiences [34,64,87], and collaboration between multiple agents [20,78,95]. These techniques enable agents to effectively navigate external environments via tools [54,59, 85] and solve complex tasks, such as autonomously browsing the web [27,83, 92], resolving GitHub issues [29, 74, 80], and proving difficult math problems [18,35].\nThe rise of inference-time techniques and agentic applications signifies a shift from static, specialized LLM applications [15, 41] to highly dynamic, general agentic programs [37,78,90]. More precisely, an agentic program is a dynamic execution workflow, represented by a directed acyclic graph (DAG), that consists of LLM calls from one or more agents, and external interrupts, which include tool calls (i.e. external API calls), generic code execution, or human inputs (\u00a72). We assume that the LLM invocation pattern of programs emerges only at runtime, making it difficult to fully know or predict the entire graph in advance.\nAutellix introduces a novel framework that leverages global, program-level statistics, such as program's cumulative execution time on an engine, to minimize waiting times and improve engine throughput. We propose two non-clairvoyant scheduling algorithms that assume no prior workload knowledge of programs: PLAS (Program-Level Attained Service) for single-threaded programs and ATLAS (Adaptive Thread-Level Attained Service) for multi-threaded programs represented as general, dynamic DAGs. PLAS prioritizes LLM calls based on the current cumulative service, or execution times, of their source program. Generalizing PLAS, ATLAS prioritizes LLM calls based on the maximum cumulative service time across all threads in the same program, which sorts calls based on their program's critical path [88]. Beyond reduced wait times, ATLAS decreases program's makespans by prioritizing critical LLM calls that would otherwise block programs' progress (\u00a74).\nPrograms comprised of tens to hundreds of LLM calls impose significant demands to the serving systems with a"}, {"title": "2 Background & Related Work", "content": "To detail relevant context for Autellix, we provide a brief overview of the emergent Al agent infrastructure and its applications, split between the LLM serving layer (\u00a7 2.1) and higher-level agentic layer (\u00a7 2.2), as depicted in Figure 3."}, {"title": "2.1 LLM Serving Layer", "content": "LLM Inference Process. Large language models (LLMs), which drive chatbots and AI applications, predominantly utilize the Transformer architecture [71], including decoder-only models such as GPT, Claude, and LLaMA [10, 28, 69, 70]. For each request, LLM inference operates in two stages: the prefill phase, which converts the input prompt into intermediate token states, and the decoding phase, where new tokens are generated auto-regressively, one at a time, based on prior token sequences. To reduce computation, LLM serving systems leverage KV-cache, which stores intermediate token states to accelerate token generation [36, 86].\nLLM Serving. LLM serving systems manage both the routing of LLM calls across engines and the execution of LLM calls within each engine (Fig. 3). Within an engine, recent innovations in LLM serving mirror concepts rooted in traditional operating systems (OS), such as memory management, kernel optimization, and scheduling [43, 67]. Existing solutions, such as vLLM, integrate virtual memory and paging techniques to reduce KV-cache fragmentation [36], introduce shared memory to cache prefixes across LLM requests [41, 90], and manage cache hierarchies between GPU, CPU, and disk [62, 63, 94]. Other techniques improve GPU kernel implementations to accelerate self-attention [16], pipeline different operators [94], and implement better tensor or pipeline parallelism [39, 77]. Finally, LLM engines can leverage better scheduling, such as binpacking prefills and decodes together [2] and preempting LLM requests [77], to improve response times. Across multiple LLM engines, serving systems employ load-balancing techniques like live migration [68], disaggregate prefills and decodes [57], construct prefix trees [66], and migrate KV caches across engines [41] to meet request SLOs and improve tail latencies.\nOverall, the above techniques optimize for independent LLM requests, equivalent to a function-call in a general program. Instead, Autellix focuses on program-level optimizations, particularly scheduling\u2014akin to how traditional OSs manage entire processes across CPU cores."}, {"title": "2.2 Agentic Layer", "content": "Agentic Programs. Above the LLM inference layer, developers build sophisticated agentic programs to orchestrate interactions between agents, tools, and humans (Fig. 3). Specifically, this work focuses on LLM agents, defined as a tuple consisting of a system prompt specifying the agent's role and the LLM model class\u00b9. Similar to traditional OS processes and interrupts, agentic programs either interact directly with the LLM serving layer via LLM calls or engage in external interrupts-time spent outside an LLM engine. Specifically, agents can interact with tools to execute generic functions or external APIs, enabling control over environments such as databases, robotic systems, or the internet [6,54,59,60,83,93]. Most importantly, agentic orchestration frameworks [50], such as LangChain [15,37] and Autogen [78], provide developers with primitives to manage a program's control flow, determining when to execute agents, invoke tools, or request human input. Such primitives adhere to general programming semantics, including conditional statements, loops, error handling, and terminal conditions [32, 37, 78, 90]. Finally, programs maintain a global history of outputs across agents, tools, and humans [37,41,53, 81]. For instance, LLM-based chatbots accumulate messages between LLM agents' outputs and humans' inputs [48]. Importantly, Autellix does not modify the program layer. Instead, it dynamically builds an internal state of the program's execution graph (DAG) when the program runs, which is stored in a process table (\u00a75).\nAgentic Applications. Beyond standard chatbots (Fig. 1a), agentic applications, or instantiations of programs, automate or assist with complex tasks, including web or user-interface (UI) navigation (e.g. OpenAI's Operator) [6, 27, 47, 92], resolving Github issues [29, 74, 80], solving IMO-level problems [18, 35], fact-checking and summarizing claims from multiple sources (Fig. 1c) [23, 41], and enabling precise robotic control [58]. Many applications scale inference time compute the number of LLM calls and, correspondingly, total decode tokens to improve their performance on complex tasks. These test-time methods include: step-by-step reasoning to decompose tasks [55, 76], explicit thought injection to guide reasoning [85], planning or searching to explore possible solutions [8, 84, 91], self-critique to evaluate actions [42,89], self-reflection to learn from failures [34, 64], and multi-agent collaboration [21, 78]. In particular, a single-threaded Reasoning and Acting (ReAct) agent, which"}, {"title": "3 Motivation", "content": "Today's AI agent infrastructure decouples LLM serving systems from agentic programs (\u00a72). As organizations shift from serving LLM queries to higher-level AI applications, LLM engines must optimize for program-level objectives, such as response times, or end-to-end latencies [41]. Formally, a single-threaded program's end-to-end latency comprises three components: (1) waiting time, the total queuing time of a program's LLM calls on the engine; (2) execution time, the cumulative feedforward time of LLM calls; and (3) interceptions, time spent waiting for external interrupts such as tool calls or human input. Since component (3) is unrelated to LLM serving, this section identifies problems and opportunities to reduce waiting (\u00a73.1) and execution times (\u00a73.2), subsequently addressed in the design of Autellix's scheduling policies (\u00a74)."}, {"title": "3.1 Program-level Wait Times", "content": "Figure 5 shows that across various agentic workloads-from classic chatbots to ReAct and MCTS programs-the majority of a program's time is spent waiting as load increases. Hence,"}, {"title": "3.2 Program-level Execution Times", "content": "A program's execution time largely depends on how efficiently the LLM engine manages the prefill and decoding phases. In agentic workloads, which often feature long, cumulative prefills, Autellix focuses on optimizing prefill performance. Specifically, significant portions of prefill computation can be eliminated through prefix caching. This technique stores and reuses relevant key-value (KV) cache entries such as the system prompt across LLM requests [41,90].\nData Locality. Figure 7 illustrates the average cache-hit rate as a function of input length. The cache-hit rate is defined as the percentage of precomputed input tokens in the LLM engine's KV cache for an incoming LLM call. Notably, within a single program, cache-hit rates remain above 90% across all input lengths, indicating that LLM calls within the same program share identical contexts. In contrast, when considering different programs, the cache-hit rate decays exponentially with input length, suggesting that programs only share the system prompt. These results suggest that LLM serving systems across engines should consider a program's data locality, as much of its KV cache can be reused for future LLM requests."}, {"title": "4 Autellix Design", "content": "We present Autellix's overall architecture (\u00a74.1) and then explore its two key components: (1) a program-aware scheduler (\u00a74.2) designed to reduce both call-level and program-level blocking, and (2) a data locality-aware load balancer (\u00a74.3)."}, {"title": "4.1 Overview", "content": "Autellix is a higher-level serving engine designed for agentic programs rather than individual LLM requests. Autellix focuses on three primary objectives: (1) improving overall program's end-to-end latency, for users, (2) maximizing GPU utilization for providers, and (3) mitigating program starvation to improve fairness, measured via 95th and 99th percentile latencies.\nAssumptions. Autellix is non-clairvoyant; it assumes no knowledge of program arrivals, the structure of executed workflows, or general workload distributions. When a program arrives, its execution DAG is initially unknown; Autellix dynamically constructs an internal representation (IR) as the program runs. This flexibility enables Autellix to generalize to any program that invokes LLM calls on the underlying engine. While prior work [41] submits static LLM applications to the engine, Autellix assumes users run general Python programs on their local machines, which invoke Autellix's backend (\u00a7 5).\nArchitecture. Figure 8 illustrates Autellix's overall architecture. Unlike existing LLM engines, which assumes LLM calls are stateless, Autellix is stateful: programs execute from the user's local machine, establish a session with the Autellix, and issue LLM calls over time with an associated session ID. We further detail the low-level implementation in Section 5. When a session starts, Autellix adds a corresponding entry to a global process table (\u00a74.2). This table tracks program metadata, including total service time, thread-level metadata, and waiting times across programs' LLM calls. Both the engine-level scheduler (\u00a74.2) and stateful load balancer (\u00a74.3) leverage the table to schedule LLM calls for the next decoding batch and route LLM calls to an engine based on their program's data locality."}, {"title": "4.2 Program-Aware Scheduler", "content": "We present a general, efficient scheduler designed to minimize programs' response times, or end-to-end latencies, without a-priori knowledge. To mitigate head-of-line blocking at both the program and call levels, Autellix assigns priorities to calls based on program-level statistics (e.g., total accumulated runtime, \u00a74.2.1) and dynamically preempts calls (\u00a74.2.2)."}, {"title": "4.2.1 Program-level Prioritization", "content": "To implement program-level prioritization effectively, Autellix relies on a global process table that tracks essential program metrics, enabling more informed scheduling decisions across both single- and multi-threaded programs.\nProcess Table. Inspired by traditional operating systems, Autellix maintains a global process table that records the state of all running programs. When a new program arrives, Autellix adds a corresponding entry; when the program completes, this entry is removed. Each program entry in the process table tracks the following metrics:\n\u2022 Service time: For single-threaded programs, this is the cumulative execution time of all completed calls on the LLM engine's model executor. For multi-threaded programs, it is the longest observed critical path's execution time.\n\u2022 Waiting time: The time spent in the LLM engine's scheduler queue-used for anti-starvation.\n\u2022 Engine ID(s): The engine(s) that the program is currently running on-used for Autellix's load-balancer. (\u00a7 4.3).\n\u2022 Threads Metadata: Each thread corresponds to an active LLM call. Hence, we keep track of a program's active LLM calls and their individual arrival, waiting, and service times.\n\u2022 Most recent call arrival: The last time a new LLM call arrived for this program used for tracking stale programs.\n\u2022 Most recent call completion: The last time an LLM call finished-used for detecting long external interrupts.\nWhen a program's LLM call completes, the table is updated accordingly. With the process table, the scheduler can reason about the global state of each program to schedule LLM calls.\nSingle-Threaded Programs. Scheduling policies like Shortest-Job-First (SJF) and Shortest-Remaining-Processing-Time (SRPT) minimize response times optimally in single- and multi-server settings [4,25]. However, these require exact knowledge of program runtimes, violating Autellix's non-clairvoyance assumption. Instead, the Least-Attained-Service (LAS) algorithm [45], widely used in information-agnostic settings such as data center networking [7, 14] and deep learning clusters [26], offers a practical alternative.\nWe introduce Program-Level Attained Service, or PLAS, extending LAS to programs. For a single-threaded program, its service time is the total runtime of all prior completed LLM calls. Formally, if the jth LLM call cj with program ID of cj.id is submitted, PLAS assigns a priority p(cj) to cj based on the sum of all runtimes, tk, of all prior LLM calls with the same ID:\n$p(c_j) = \\sum_{k<j \\\\ c_k.id=c_i.id} t_k$ (1)\nHere, large priority values mean lower priority. To reduce computation, the scheduler reads the program's total service time from the process table (Line 11). When an LLM call completes, its program's total service time is updated (Line 4). Thus, PLAS naturally favors calls from programs that have received less total service, helping shorter programs finish earlier and reducing response times.\nMulti-Threaded Programs. Unlike single-threaded programs, multi-threaded programs are modeled as dynamic DAGs of LLM calls. Unfortunately, a program's completion time is dictated by the DAG's critical path-the longest sequence of dependent calls from start to finish, illustrated in Figure 9. No matter how many parallel LLM calls an engine can process, the program only terminates when all calls along the critical path have finished. Furthermore, without considering critical paths, schedulers achieve sub-optimal completion times for programs; in Figure 9, the DAG's makespan increases from 11 to 14 units.\nTo address this, we introduce Adaptive Thread-Level At-tained Service (ATLAS), a pragmatic generalization of PLAS, that prioritizes calls based on their service times along their programs' critical paths. ATLAS aims to assign each newly arrived call cj a priority p(cj) based on the priorities and completed service times of its parents P(cj) in the same program:\n$p(c_j) = \\begin{cases} 0 & \\text{if } c_j \\text{ is root} \\\\ max_{c_k \\\\in P(c_j)} {p(c_k) + t_k} & \\text{otherwise} \\end{cases}$ (2)\nHere, tk is the execution time of a parent call ck. By recursively combining parent priorities and runtimes, p(cj) estimates the longest chain of accumulated service time leading to cj, providing a non-clairvoyant estimation of the critical path.\nHowever, achieving both objectives-favoring short programs while also prioritizing the longest, critical-path threads is nontrivial. To solve this, ATLAS maintains a single scalar per program in its process table: the longest observed critical path. Each active LLM call in a program inherits this value as its initial priority, and upon call completion, updates the scalar only if its own critical path is longer (Line 4). This simple mechanism continuously refines the program's critical path estimate without tracking dependencies between LLM calls. Consequently, ATLAS favors programs and LLM calls with shorter critical paths, effectively approximating a Least-Attained-Service policy for dynamic DAGs. Furthermore, as all calls of a given program derive their priorities from the same entry, the scheduler"}, {"title": "4.2.2 Preemptive Scheduling", "content": "Autellix assigns priorities to each LLM call based on their program's history. However, scheduling and preempting programs based on continuous priorities can degrade into worst-case round-robin scheduling [14], which performs worse than FCFS, and incur unnecessary context switches, including frequent KV-cache swaps between CPU and GPU [77]. To avoid this, Autellix discretizes priorities into a finite set of queues, akin to multi-level feedback queues (MLFQ) in operating systems [5, 14, 26].\nMulti-level Program-based Scheduling Autellix bins and discretizes LLM calls' priorities into K queues (Q1, Q2,..., \u03a9\u03ba), where priorities decrease from Q1 to QK. Each queue Q\u00a1 covers a priority range [Q, Qhi), with Q\u00b9\u00ba = 0, Q = \u221e, and Q1 = Qhi.\nIn Figure 10, when an LLM call arrives, Autellix looks up its program's priority p(c), based on the process table (\u2460, Line 11). Unlike traditional MLFQ, where new calls all start at the highest priority queue Q1, LLM calls are assigned to the ith queue based on discretized priorities, p(c) \u2208 [Q, Qhi) (2, Line 12). Subsequently, calls receive the queue's time quantum and execute in FCFS order within their queue (Line 13, 35). Once a call exhausts its quantum, it is demoted to a lower priority queue (\u2462, Lines 20-23). If the call waits too long, Autellix employs anti-starvation mechanisms, described next (\u2463, Lines 24-30). Finally, when a call completes decoding, it updates the process table (Lines 16-18).\nAnti-Starvation. Discrete prioritization, or MLFQ-style algorithms, incurs the starvation of long, low-priority programs [14, 26, 77]. Simple anti-starvation techniques such as promoting calls that have waited past a threshold-reduces Autellix to naive MLFQ, where long program's LLM calls, which are now in Q1, interrupt short programs [5,77]. Hence, we also utilize the process table to measure program-level starvation. Concretely, for a program p, Autellix promotes call c to Q1 if the ratio of total waiting time (Wtotal = Wp+Wc) to service time (Ttotal = Tp +Tc) exceeds a threshold \u03b2:\n$\\frac{W_{\\text{total}}}{T_{\\text{total}}} \\geq \\beta$\nVarying \u1e9e presents a trade off between programs' average response times and fairness. After promotion, only We and Te, or the calls' wait and run time, are set to zero, to ensure programs' threads, or active LLM calls, are likely all promoted together (Line 29).\nMemory Management. With preemptive scheduling, LLM engines must handle a large volume of concurrent LLM calls, leading to frequent GPU-CPU transfers as KV-cache blocks are repeatedly swapped to serve different requests [77]. Prior work mitigates this swapping overhead by proactively swapping KV-cache for the next iteration of LLM requests while processing the current ones [77]. However, Autellix is synchronous and requires real-time updates for each call's time quantum and the process table. Instead, Autellix employs two key optimizations to reduce both the frequency and overhead of GPU-CPU swapping respectively.\nFirst, Autellix reduces total swaps by adopting multi-step scheduling, running the scheduler once every N decoding steps rather than at every step. As some requests may complete early, our scheduler overprovisions queued requests already on the GPU, ensuring that new requests are immediately added when some requests finish before N steps. Second, Autellix employs a more efficient GPU-CPU swap kernel. Instead of calling separate asynchronous transfers for each block, our kernel gathers all KV blocks into a contiguous buffer and transfers them in one operation\u2014increasing PCIe bandwidth by reducing fragmentation, reducing per-block overhead, and lowering end-to-end swap latency (\u00a75)."}, {"title": "4.3 Load Balancer", "content": "As agentic workloads scale, deploying multiple engine replicas is necessary. However, distributing requests without considering data locality yields suboptimal performance [66]. Our analysis for agentic workloads (\u00a73.2) highlights a critical distinction between short and long requests. Short requests below 2048 tokens achieve high cache hit rates (\u226575%) across any engine, due to common system prompts. Enforcing data locality for these requests offers negligible gains and risks skewing engine utilization when large, parallel programs dominate specific engines. Thus, simply balancing short requests across the least-loaded engines preserves performance with minimal overhead. Conversely, longer requests are far more sensitive to their programs' data locality. Their substantial prefix overlap with a given program significantly reduces recomputation when consistently routed to the same engine, justifying occasional queuing delays.\nWhile prior work relies on complex prefix trees to quantify data locality [66], our simple method dynamically routes short requests to the least-loaded engine and pins longer requests to their programs' corresponding engines.\nAlgorithm 2 formalizes this approach, and our evaluation shows that Autellix's load balancer improves both throughput and latency across heterogeneous workloads (\u00a76)."}, {"title": "5 Implementation", "content": "Autellix is a multi-engine LLM inference serving system comprising a frontend, scheduler, and load balancer\u2014totaling 5k lines of Python and CUDA/C++ code.\nFrontend. Autellix's frontend extends OpenAI's Chat Com-pletion and vLLM's Python APIs [36,49] to provide a stateful interface that appears stateless to developers. Users simply import Autellix's library into their Python applications, and upon program initialization, Autellix automatically issues a start_session request to the backend. This operation returns a unique session identifier and creates a corresponding entry in the process table. Subsequent LLM calls are transparently annotated with the appropriate session, program, and thread IDs before being dispatched to the backend. When the program completes or encounters an error, Autellix invokes end_session, removing the associated entry from the process table. As a research prototype, the current frontend lacks safeguards against user modification of the underlying package; addressing this limitation remains future work.\nLLM Engine. Autellix builds on vLLM v0.6.1 [36]. To keep changes localized, we modify only the scheduler by integrating new policies (PLAS, ATLAS, and MLFQ) and memory swapping kernels for efficiency. This ensures straightforward experimentation and clear attribution of performance gains. The scheduler follows the algorithm described in the previous section (\u00a74). We've also noticed in vLLM, each Key-Value (KV) block is transferred individually via cudaMemcpyAsync, creating small fragmented transfers that underutilize PCIe bandwidth and incur high overhead such as repeated DMA setups. To address this, we allocate a host buffer and consolidate all KV blocks into a single contiguous chunk, enabling one bulk transfer. The results are shown in the next section (\u00a76)."}, {"title": "Multi-engine.", "content": "vLLM currently lacks the ability to manage multiple LLM engines at the same time. To better evaluate our load balancing strategy, we built AsyncMultiLLMEngine atop of AsyncLLMEngine. Each LLM engine replica runs in a dedicated Python process, and a coordinating meta-engine manages these replicas via standard inter-process communication (IPC) primitives such as mp. Queue and mp. Pipe. When the meta-engine receives a request, it assigns the request to the appropriate replica, returning a future-like object to the frontend without blocking. The selected engine process executes the task asynchronously and sends the completed result back through the IPC channel. Upon receiving the result, the meta-engine resolves the future and provides the output to the frontend. This design allows multiple requests to be processed in parallel, with the meta-engine acting as a non-blocking coordinator that handles routing, resource assignment, and result collection."}, {"title": "6 Evaluation", "content": "In this section, we analyze representative agentic workloads, evaluate Autellix's performance against state-of-the-art LLM serving systems, and ablate its design choices."}, {"title": "6.1 Workloads", "content": "Our real-world experiments evaluate Autellix over four representative agentic workloads, which widely vary in the number of decode tokens, prefill tokens, and the LLM calls (Fig. 11).\nChatbot Agent: ShareGPT [1]. The ShareGPT dataset comprises of user-generated conversational inputs and outputs, typical for chatbot applications. The number of LLM calls follows a long-tailed distribution with a mean of 6.66 and a max of 80 (Fig. 11d). ShareGPT's conversational nature"}, {"title": "6.2 Experimental Setup", "content": "Models & Testbed. We evaluate on three models: LLaMA-3.1-8B, 70B and Falcon-180B, running on 1, 4, and 8 GPUs, respectively. Experiments are conducted on a GCP Compute Engine a2-ultragpu-8g instance with eight A100-SXM4-80GB GPUs connected via NVLink, 1360 GB host memory, PCIe-4.0\u00d716, and 2 TB of disk space.\nMetrics. Existing LLM serving systems focus on request-level metrics, such as Time-to-First-Token (TFTT) and Time-per-Output-Token (TPOT), also referred to as token latency [36,77,94]. However, these metrics overlook end-to-end latency for agentic programs. To that end, we introduce program-level token latency, defined as the total program response time divided by the number of tokens generated2. A high-throughput system for programs should retain low program-level latency during high request rates. For simplicity, we refer to our metric as latency throughout the evaluation.\nBaselines. Our evaluation considers three baselines. All baselines, including Autellix, use the same max batch size.\n\u2022 vLLM [36]. vLLM is the state-of-the-art, high throughput LLM serving system that integrates continuous batch-ing [86] and PagedAttention [36] to reduce KV cache fragmentation. Its default scheduling policy is FCFS, which is application-unaware and suffers from call-level and program-level HoL blocking. We use vLLM v0.6.1.\n\u2022 VLLM-opt. An optimized version of vLLM that enables chunk-prefill [3], prefix-caching [41,90], and multi-step scheduling. Based on vLLM's blogpost [72], it's performance closely matches SGLang [90] and TensorRT [46].\n\u2022 MLFQ. On top of vLLM-opt, it implements preemption via the Multi-Level Feedback Queue algorithm [77]. This baseline ablates the impact of program and call-level blocking."}, {"title": "6.3 End-to-End Single-Engine Performance", "content": "In Figure 12, we evaluate the end-to-end performance of Autellix against three baselines and four workloads: ShareGPT, BFCL, LATS, and Mixed. Across all workloads, Autellix consistently achieves the highest throughput given same token latency. Conversely, vLLM performs worst due to its lack of prefix caching, which results in expensive re-computation of cumulative state (Fig. 7) for LLM calls in the same program. Across workloads, the relative performance between vLLM-opt, MLFQ, and Autellix varies.\nThe first two rows plot the latencies for single-threaded workloads, ShareGPT and BFCL. vLLM and vLLM-opt's FCFS scheduling causes severe head-of-line (HoL) blocking, which increases latencies as arrival rates increase. In contrast, MLFQ, a preemptive algorithm, mitigates call-level HoL, improving throughput by 1.5\u00d7 over vLLM-opt. However, at high load, it still suffers from program-level HoL. By employing PLAS to tackle both call- and program-level HoL, Autellix achieves up to 8x throughput of vLLM, twice that of vLLM-opt, and a 1.5x improvement over MLFQ under heavy load.\nThe third row presents results for the multi-threaded LATS workload. Autellix outperforms vLLM, MLFQ, and vLLM-opt by up to 5x, 2.5x, and 2x, respectively. Notably, MLFQ's preemptive scheduling, which benefits single-threaded programs, is less effective in multi-threaded settings. By aggressively prioritizing shorter requests, MLFQ inadvertently disrupts threads in the same program, exacerbating program-level HoL blocking and stalling overall progress. Autellix's ATLAS policy holistically optimizes resource allocation across all threads, maintaining balanced progress and sustaining high throughput under heavy multi-threaded workloads.\nThe fourth row of Figure 12 illustrates performance on mixed workloads. Autellix achieves up to 15x higher throughput than vLLM, 5.5\u00d7 higher than MLFQ, and 4\u00d7 higher than VLLM-opt. Since Autellix reduces program and call-level blocking, Autellix performs better as programs' heterogeneity, or the diversity of LLM calls and decode lengths, increases.\nTail latency. Preemptive scheduling strategies can reduce average latency but risk increasing tail latency by starving long-running programs. Figure 13 reports the 95th (P95) and 99th (P99) percentile latencies across different workloads on LLaMA-3.1-8B. For ShareGPT, MLFQ significantly improves average latency compared to vLLM-opt (Fig. 12), but exhibits poor P95/99 tail latencies. In contrast, for BFCL, MLFQ outperforms vLLM-opt in both cases. In 7 of 8 scenarios, Autellix maintains consistently lower tail latencies than MLFQ and vLLM-opt and improves throughput by up to 1.7x for P95/99 tail latencies, demonstrating robust performance gains in both average and tail performance metrics."}, {"title": "6.4 End-to-End Multi-Engine Performance", "content": "To evaluate the effectiveness of Autellix's data locality-aware load balancer (\u00a74.3), we compare it against two widely used load balancing strategies under identical scheduling policies (PLAS, ATLAS) for the sake of fairness:\n\u2022 Round Robin. Requests are assigned to engines in cyclic order ensuring an even distribution of request counts-which is the default load-balancer policy for Kubernetes [33]. This strategy ignores data locality, resulting in costly KV cache misses and high recomputation overheads.\n\u2022 Least Used. Requests are assigned to the engine with the lowest number of LLM calls in the system, effectively balancing engine workloads. However, like Round Robin, it neglects data locality and incurs frequent KV recomputations.\nWe conduct experiements using four replicas of LLaMA3.1-8B and two replicas of LLaMA3.1-70B with the ShareGPT and LATS workloads. The results, shown in Figure 14, demonstrate the Autellix's effectiveness in maintaining low average and tail latencies across all configurations. Autellix delivers up to 1.4x higher throughput compared to both baselines. The benefit is more pronounced in ShareGPT workload, where chat history reuse significantly amplifies KV-cache locality. These advantages become even more evident as the number of replicas increases, as a larger pool of engines reduces the likelihood of a request being routed to one with it's locality.\nScalability. To evaluate the scalability of Autellix, we assess its performance as the number of engine replicas increases under various latency requirements, using the ShareGPT workload with the LLaMA3.1-8B model. Figure 15 shows linear scaling in all cases. Leveraging program-level load balancing, Autellix effectively scales horizontally without data locality overhead, making it a robust solution for large-scale LLM deployments."}, {"title": "6.5 Ablations", "content": "We ablate Autellix over different scenarios, including offine batch inference, timing breakdown, and various design choices, such as the swap kernel. All experiments run LLaMA3.1-8B [24] over ShareGPT [1] and LATS [91]."}, {"title": "6.5.1"}]}