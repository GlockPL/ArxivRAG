{"title": "STREAMINGBENCH: ASSESSING THE GAP FOR MLLMS TO ACHIEVE STREAMING VIDEO UNDER-STANDING", "authors": ["Junming Lin", "Zheng Fang", "Chi Chen", "Zihao Wan", "Fuwen Luo", "Peng Li", "Yang Liu", "Maosong Sun"], "abstract": "The rapid development of Multimodal Large Language Models (MLLMs) has expanded their capabilities from image comprehension to video understanding. However, most of these MLLMs focus primarily on offline video comprehension, necessitating extensive processing of all video frames before any queries can be made. This presents a significant gap compared to the human ability to watch, listen, think, and respond to streaming inputs in real time, highlighting the limitations of current MLLMs. In this paper, we introduce StreamingBench, the first comprehensive benchmark designed to evaluate the streaming video understanding capabilities of MLLMs. StreamingBench assesses three core aspects of streaming video understanding: (1) real-time visual understanding, (2) omni-source understanding, and (3) contextual understanding. The benchmark consists of 18 tasks, featuring 900 videos and 4,500 human-curated QA pairs. Each video features five questions presented at different time points to simulate a continuous streaming scenario. We conduct experiments on StreamingBench with 13 open-source and proprietary MLLMs and find that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and GPT-4o perform significantly below human-level streaming video understanding capabilities. We hope our work can facilitate further advancements for MLLMs, empowering them to approach human-level video comprehension and interaction in more realistic scenarios.", "sections": [{"title": "INTRODUCTION", "content": "The rapid evolution of Multimodal Large Language Models (MLLMs) has significantly reshaped the field of Artificial Intelligence (Yang et al., 2023; Reid et al., 2024; Liu et al., 2024c;a). Current advanced MLLMs (Reid et al., 2024; Wang et al., 2024a; Yao et al., 2024) have already demonstrated exceptional performance in video understanding tasks, excelling on existing video benchmarks (Fu et al., 2024; Wang et al., 2024b; Zhou et al., 2024; Ataallah et al., 2024). Moreover, several pioneering studies (Chen et al., 2024a; Zhang et al., 2024a; Wu et al., 2024) have focused on improving the ability of MLLMs to comprehend real-time online video streams, pushing the boundaries of their applicability and efficiency in dynamic environments. In the industry, streaming video understanding has also attracted significant attention, with OpenAI's GPT-4o (OpenAI, 2024) as a prominent example that demonstrates human-like perception and understanding of streaming inputs.\nDespite the recognition of the importance of streaming video understanding for MLLMs, most existing video understanding benchmarks (Fu et al., 2024; Wang et al., 2024b; Zhou et al., 2024) are"}, {"title": "RELATED WORK", "content": "Video MLLMs. Recently, the development of advanced MLLMs has shifted from single image understanding to video comprehension (Reid et al., 2024; Wang et al., 2024a; Yao et al., 2024; Lin et al., 2023; Chen et al., 2024b; Li et al., 2024a). These video MLLMs typically work by converting entire videos into visual tokens that can be processed by LLMs, through sampling and encoding video frames. However, these models are limited to offline video understanding rather than real-time, real-world streaming video comprehension. In contrast, GPT-4o (OpenAI, 2024) explores the potential for human-like perception and understanding of streaming inputs. There are also several"}, {"title": "STREAMINGBENCH", "content": "We identify three key distinctions between a streaming video understanding benchmark and traditional offline video benchmarks: (1) the inclusion of real-time queries that can appear at any point during the video stream, rather than solely at the end; (2) the consideration of synchronized visual and audio content, mirroring real-world video streams; and (3) the reflection of the complex and dynamic context of video streams, encompassing the evaluation of streaming interactions beyond conventional isolated question answering. Based on these distinctions, we design three task categories: Real-Time Visual Understanding, Omni-Source Understanding and Contextual Understanding. Each category mainly addresses one of these distinctions and evaluates specific core capabilities essential for streaming video comprehension."}, {"title": "REAL-TIME VISUAL UNDERSTANDING", "content": "The tasks in this category aim to assess the ability of a model to perceive, comprehend, and reason based on the visual content of video streams. Each question is accompanied by a timestamp that indicates the time of the query and ensures that it only pertains to the visual content preceding that specific moment. To emphasize the real-time nature of the questions, they include specific time indicators such as \u201cright now\", \"just now\", or \"currently\u201d. As a result, the same question asked at different times may yield different answers.\nThere are 10 tasks that belong to this category: (1) Object Perception (OP): Detect and identify specific objects within the video. (2) Causal Reasoning (CR): Analyze cause-and-effect relationships in events. (3) Clips Summarization (CS): Summarize main content in specific video clips. (4) Attribute Perception (ATP): Identify and categorize object or individual attributes. (5) Event Understanding (EU): Recognize and describe sequences of events. (6) Text-Rich Understanding (TR): Interpret and explain text-rich content within the video. (7) Prospective Reasoning (PR): Predict future events based on current video context. (8) Spatial Understanding (SU): Understand and describe spatial relationships and locations. (9) Action Perception (ACP): Identify and describe specific actions in the video. (10) Counting (CT): Count occurrences of specific objects or actions. These tasks cover the main visual understanding tasks and effectively evaluate the ability of MLLMs to understand visual information in real-time in streaming scenarios. For deterministic evaluations, all test examples are presented as multiple-choice questions with four distinct options each. For examples of each task, please refer to Appendix D."}, {"title": "OMNI-SOURCE UNDERSTANDING", "content": "These tasks evaluate the capability of a model to process visual and audio content in a video stream simultaneously, especially focusing on the ability to integrate information from video and audio content, or align them temporally. There are four tasks in this category:"}, {"title": "CONTEXTUAL UNDERSTANDING", "content": "These tasks focus on assessing the ability of MLLMs to provide accurate responses based on complex context within a continuous video stream. Such context includes not only the redundant information presented throughout the video, but also the the streaming interactions such as prior question-answer pairs or conditions for late proactive outputs. Overall, there are four contextual understanding tasks. The first two involve filtering information from the redundant context:"}, {"title": "EXPERIMENTS", "content": "In this section, we present the experimental setup, evaluation results, and analysis of Streaming-Bench. We evaluate 13 open-source and proprietary MLLMs, highlighting their strengths and limitations in streaming video scenarios. Building on these initial findings, we then conduct additional in-depth analytical experiments to further explore their performance, aiming to facilitate further advancements for MLLMs in enhancing its streaming video understanding capabilities."}, {"title": "SETTINGS", "content": "We evaluate three proprietary MLLMs: GPT-4o (OpenAI, 2024), Gemini 1.5 Pro (Reid et al., 2024), and Claude 3.5 Sonnet (Anthropic, 2024), alongside 10 high-performing open-source video MLLMs: Video-LLaMA2 (Zhang et al., 2023), MiniCPM-V 2.6 (Yao et al., 2024), InternVL-V2 (Chen et al., 2024c), Video-CCAM (Fei et al., 2024), LongVA (Zhang et al., 2024b), LLaVA-OneVision (Li et al., 2024a), VILA-1.5 (Fang et al., 2024), Kangaroo (Liu et al., 2024d), LLaVA-NeXT-Video (Liu et al., 2024b), and Qwen2-VL (Wang et al., 2024a). We adhere to the official configurations of most MLLMs for frame extraction from the videos, as detailed in Appendix A.1.\nSince current MLLMs lack the ability to accept streaming video input, we convert each streaming task into an offline task for evaluation except for the proactive output task. During the evaluation process, each video is clipped into the segment from the beginning to the timestamp when the question is asked. Then the model answers the question based on this video segment in an offline way. We use accuracy as the evaluation metric for all multiple-choice questions.\nFor SQA, the basic evaluation process and metric are consistent with other tasks. The only difference is that contextual information, i.e., previous QA pairs should be additionally included. For simplicity, we attach the history of question-answer pairs before the current question to expand the input as: \u201c{Timestamp1}: {QA1} ...; Answer the question accordingly: {current question}\u201d.\nFor the Proactive Output task, most models cannot be directly evaluated, as they lack the ability to autonomously provide output without prompts. To address this, we implement a polling strategy: we define an interval spanning several seconds before and after the ground truth timestamp (the moment when the model is expected to output). During this interval, the model is queried every second with the question \"Is it the right time to output?\" This continues until the model responds with \u201cYes.\u201d At that point, the model is prompted to provide the relevant keywords, and this moment is recorded as the actual output timestamp. A question in the PO task is considered accurately resolved only if the difference between the actual output timestamp and the ground truth timestamp is less than two seconds. The average accuracy across all queries is then computed and used as the performance metric for the PO task. Please refer to Appendix A.2 for more evaluation protocals."}, {"title": "RESULTS ON STREAMINGBENCH", "content": "The performance of 13 open-source and proprietary models on the 18 tasks of StreamingBench is presented in Table 2. The results indicate that all three proprietary models outperform the best-performing open-source model, LLaVA-OneVision, with Gemini 1.5 pro achieving the highest score of 67.07%. Among the open-source models, LLaVA-OneVision ranks first with a score of 56.36%, followed closely by Qwen2-VL and MiniCPM-V 2.6, which achieve scores of 54.14% and 53.85%, respectively. For comparison, we sample 10% of the tasks from each of the 18 tasks for human evaluation. The average human score across 18 tasks is 91.66%. Even the best-performing MLLMs, Gemini 1.5 Pro, lags significantly behind human performance.\nThe results demonstrate that all models perform well on real-time visual understanding tasks, but exhibit generally poor performance on omni-source understanding and contextual understanding tasks. This suggests that the models' ability to understand offline video transfers effectively to real-time visual understanding, but they struggle with tasks that require audio information for omni-source understanding and tasks that demand consideration of contextual information in scenarios with streaming interactions or high-redundancy visual inputs for contextual understanding. This"}, {"title": "MODEL PERFORMANCE ON DIFFERENT VIDEO LENGTHS", "content": "We further investigate the impact of video length on the model capabilities of streaming video understanding. As most current MLLMs can process minute-level videos, we choose 60 seconds as a threshold to distinguish between short and long videos, and compare the models' performance on both. We focus on the top three open-source models with the highest performance in real-time visual understanding. The results, as shown in Table 3, indicate that all models perform worse overall on videos longer than 60 seconds compared to their performance on shorter videos. However, Qwen2-VL stands out by demonstrating better performance on long videos than short ones in the tasks of Causal Reasoning (CR) and Clip Summarization (CS). This highlights the need for improvements in the ability of MLLMs to effectively process longer video content."}, {"title": "CONCLUSION", "content": "In this work, we introduce StreamingBench, the first comprehensive benchmark designed to assess the streaming video understanding capabilities of MLLMs. StreamingBench consists of 900 videos and 4,500 QA pairs, covering 18 tasks across three main categories that evaluate key aspects of streaming video comprehension. Experiments with 13 state-of-the-art MLLMs reveal that even the best-performing model Gemini 1.5 Pro still falls significantly short of human-level performance. Additionally, we analyze the performance gap and propose potential directions for improving MLLMs. We hope that our work will contribute to the development of future AI systems with improved performance in real-world scenarios."}]}