{"title": "Speaker Emotion Recognition: Leveraging Self-Supervised Models for Feature Extraction Using Wav2Vec2 and HuBERT", "authors": ["Pourya Jafarzadeh", "Amir Mohammad Rostami", "Padideh Choobdar"], "abstract": "Speech is the most natural way of expressing ourselves as humans. Identifying emotion from speech is a nontrivial task due to the ambiguous definition of emotion itself. Speaker Emotion Recognition (SER) is essential for understanding human emotional behavior. The SER task is challenging due to the variety of speakers, background noise, complexity of emotions, and speaking styles. It has many applications in education, healthcare, customer service, and Human-Computer Interaction (HCI). Previously, conventional machine learning methods such as SVM, HMM, and KNN have been used for the SER task. In recent years, deep learning methods have become popular, with convolutional neural networks and recurrent neural networks being used for SER tasks. The input of these methods is mostly spectrograms and hand-crafted features. In this work, we study the use of self-supervised transformer-based models, Wav2Vec2 and HuBERT, to determine the emotion of speakers from their voice. The models automatically extract features from raw audio signals, which are then used for the classification task. The proposed solution is evaluated on reputable datasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results show the effectiveness of the proposed method on different datasets. Moreover, the model has been used for real-world applications like call center conversations, and the results demonstrate that the model accurately predicts emotions.", "sections": [{"title": "1. Introduction", "content": "Speech is one of the primary ways of expressing emotions. Therefore, a system that can recognize, interpret, and respond to the emotions expressed in speech is highly valuable. Emotions influence both the vocal characteristics and the linguistic content of speech. In recent years, enormous efforts have been devoted to developing methods for automatically identifying human emotions from speech signals, a field known as speaker emotion recognition (SER). SER systems have many applications including human-machine interactions [11, 41], medicine [32], and psychology [60]. Moreover, evaluation of the conversations in a call centre is one of the major applications of SER, the manager of a call centre can analyse the performance of the operators. The operators are able to consider the feeling of the customers during conversation to handle necessary cases.\nIn all of the classification problems one of the main step is feature extraction. Before using deep learning most of the conventional machine learning methods used hand-crafted features speech recognition including Mel-frequency cepstral coefficients (MFCCs, pitch, zero-crossing, Fourier transform, and energy of signals [51, 43, 1, 44, 40, 46]. After growing deep learning usage in speech processing tasks, the models extracted features automatically by deep-based models. They used convolutional neural networks (CNNs) to extract spatial features from input, the input was mostly spectrograms. Also, they used recurrent neural networks (RNNs) to evaluate temporal features. The models extracted local information and long-term contextual dependencies [3, 33, 57, 2, 58, 30, 56, 10, 36, 53]. Additionally, the attention mechanism is another popular method that has been used in different fields such as speech recognition [10], visual object classification [36] and document classification [53]. It is a powerful concept that helps models focus on specific parts of the input sequence when generating each part of the output sequence. In the SER problem, models try to ignore silence frames and other parts of the utterance that do not carry emotional"}, {"title": "2. Related Works", "content": "Different machine learning algorithms have been used to construct a good classifier for emotion classification. As mentioned, conventional machine learning methods such as support vector machines (SVM) [20, 8, 46, 45], k-nearest neighbours (KNN) [27], and decision tree [29] have been used for emotion recognition. In [27], two utterances were aligned in the emotion space defined by emotograms using Multi-Dimensional Dynamic Time Warping (MD-DTW) [18]. A KNN classifier was then applied to assign a final emotion class label based on the MD-DTW measure. KNN assigns a label to a test utterance based on the labels of its k nearest neighbors, with the final label determined by a majority vote among the neighbors. Each of the classifiers has its own advantages and disadvantages. These methods mainly use hand-crafted features such as pitch, energy, zero-crossing rate, Mel-filterbank features, and MFCCs. These are often referred to as Low-Level Descriptors (LLD). Since MFCC models the human auditory perception system, it is the most popular spectral feature. Moreover, CNN-based models take the signal's spectrogram as input, which is a visual representation of the spectrum of frequencies of a signal as it varies with time [2, 58]. In [2], spectrograms were generated from the input speech signal, followed by the use of three convolutional layers and three fully connected layers to extract features from the spectrogram images. A spectrogram is a visual representation of signal strength over time at various frequencies present in a waveform. It is depicted as a two-dimensional graph, with time on the horizontal axis, frequency on the vertical axis, and the amplitude of the frequency components at a specific time represented by the intensity or color at each point. The spectrogram is computed from the speech signal by applying the Fast Fourier Transform (FFT), resulting in a time-frequency representation. Some methods apply CNNs to extract local features from the input, then use LSTM layers to learn long-term dependencies from the extracted local features [57, 9]. In [57], two CNN-LSTM networks were constructed: one 1D CNN-LSTM network and one 2D CNN-LSTM network, designed to learn local and global emotion-related features from speech and log-mel spectrograms, respectively. Both networks share a similar architecture, consisting of four Local Feature Learning Blocks (LFLBs) and an LSTM layer. Each LFLB, which primarily includes one convolutional layer and one max-pooling layer, is designed to capture local correlations while extracting hierarchical features. The LSTM layer is then used to learn long-term dependencies from these local features. These networks, which combine the strengths of CNNs and LSTMs, capitalize on the advantages of both architectures while mitigating their individual limitations. RNNs, however, are inherently sequential models that do not allow parallelization of their computations, this bottleneck is particularly evident when processing large datasets with long sequences [47]. The authors in [3, 33] have found that spectrograms are more effective than LLD features because LLD features are already decorrelated. One of the advantages of deep-based models is feature extraction; they can extract useful and important features dur-"}, {"title": "3. Proposed Methodology", "content": "A key element of emotion recognition is the ability to extract more distinguishing speech features. In the proposed framework, features are extracted by transformers instead of traditional feature engineering techniques. In this study HuBERT and Wav2Vec2.0 have been used for feature extraction. Both models are transformer-based, and they extract both acoustic features and language modelling from raw audio signals in their architectures. In the following sections we go into more details about two models."}, {"title": "3.1. HuBERT", "content": "Hidden-Unit BERT (HuBERT) provides aligned target labels for BERT-like prediction losses using an offline clustering step (figure 2). It is proposed to overcome three distinct problems in the self-supervised approach: the presence of multiple sound units per input utterance, the absence of lexicon during the pre-training phase, and the absence of explicit segmentation of sound units [19]. In HuBERT, prediction loss is applied to masked regions, forcing a combined acoustic and language model to be learned for unmasked inputs. The model is pre-trained on the Librispeech [39] (960h) and Librilight [26] (60Kh) benchmarks and it outperformed state-of-the-art methods, it presented in three different models three model sizes pre-trained with HuBERT: BASE (90M parameters), LARGE (300M), and X-LARGE (1B). The authors in [19] made two decisions regarding mask prediction: how to mask and where to apply the prediction loss. For the first decision,$p\\%$ of the timesteps are randomly selected as start indices and spans of 1 steps are masked using the same strategies as SpanBERT [25] and wav2vec 2.0 [4]. To address the second decision, cross-entropy loss computed over masked and unmasked time steps as$L_m$and$L_u$. Similarly to language modelling, if the loss is computed only over masked time steps, the model must predict the targets for the unseen frames based on the context. It forces the model to learn both the acoustic representation of unmasked segments and the long-range temporal structure of speech data."}, {"title": "3.2. Wav2Vec 2.0", "content": "The model is a transformer-based model trained with a self-supervised method to extract features from raw audio signals [4] (figure 3). It operates by first converting raw audio waveforms into meaningful feature representations using a CNN and a GELU activation function [16] to capture latent speech representations $z_1, z_2, ..., z_T$ for T time steps. These initial features are then fed into a transformer network [5], which is trained using a contrastive loss objective to differentiate between correct and incorrect quantized representations of the audio signal [17]. Using this training method, Wav2Vec 2.0 can learn rich, contextualised embeddings from unlabeled speech data, effectively building context representations over continuous speech and capturing"}, {"title": "3.3. Model architecture", "content": "The input to our model is a raw audio file, which is sent to the feature extractor module (HUBERT or Wav2Vec 2.0). After extracting the features, we have a matrix with the size of n\u00d7768. Depending on the length of the input audio, the length varies. The longer the audio file, the more features we have with the size of 768. In order to project the output of the feature extractor, we calculated the mean of the features across each row, so we will have a vector with a size of 768. Then, we apply two feed-forward layers for classification. The size of the final layer is equal to the number of classes that are compatible with the dataset's classes (figure 1). In the following section, we will report the performance of the proposed model on reputable datasets."}, {"title": "4. Experimental Results", "content": "We performed SER on five reputable datasets that encompass different languages and emotions to assess the performance of transformer-based models."}, {"title": "4.1. Datasets", "content": "There are 1440 speech audio samples in the RAVEDESS [31] dataset, recorded by 24 professional actors. Two semantically neutral US English phrases are read while revealing eight emotions (neutral, calm, happiness, sadness, anger, fear, disgust, surprise). AESDD (Actuated Emotional Speech Dynamic Database) [49, 50] is a publicly available dataset for speech emotion recognition. There are utterances of acted emotional speech in the Greek language. There are approximately 500 utterances from five actors. The database contains utterances with five emotions: anger, disgust, fear, happiness, and sadness. EMODB [7] is a German database with high-quality audio recordings from 10 actors (5 male and 5 female) who produce 10 German utterances (5 short and 5 long sentences) with 7 emotions (anger, neutral, anger, boredom, happiness, sadness, disgust). Some emotional expressions have two versions recorded by the same author. Thus, the database provides about 535 sentences. The Surrey Audio-Visual Expressed Emotion (SAVEE) [23] database has been recorded as a prerequisite for the development of an automatic emotion recognition system. The database consists of recordings from 4 male actors of 7 different emotions (anger, disgust, fear, happiness, sadness, surprise, and neutral), and 480 British English utterances in total. The sentences were chosen from the standard TIMIT corpus and phonetically balanced for each emotion. The data were recorded in a visual media lab with high-quality audio-visual equipment, processed and labelled. The SHEMO [38] dataset includes 3000 semi-natural utterances, equivalent to 3 h and 25 min of speech data extracted from online radio plays. The dataset covers speech samples of 87 native-Persian speakers for five basic emotions including anger, fear, happiness, sadness, and surprise, as well as a neutral. It should be noted that we excluded the fear utterances from our test due to their small number in the database, as was done by the authors in [38]."}, {"title": "4.2. Experiments and Results", "content": "Several experiments were carried out on the explained dataset by the proposed model. We split the datasets into 80% and 20% for train and test respectively, we kept the ratio of the classes in the test set. The confusion matrix is one of the best methods for showing the evaluation of the methods for the SER problem since we can analyze the missed classifications. For this study, we applied large models of HUBERT and Wave2Vec 2.0.\nOur work aims to recognize speech emotion"}, {"title": "5. Conclusion", "content": "This paper proposes transformer-based feature extractors as a way of solving the SER problem. Using transformer-based models, we can learn many useful temporal and spatial features from raw audio files that are very useful for emotion recognition. Our claim is supported by experimental results from benchmark datasets. The two models performed well on standard and famous datasets, and their performance was acceptable. This shows the significance and effectiveness of the proposed system for SER using self-supervised methods. Moreover, our model performed well on noisy data such as that from call centers, which is relevant to real-world applications. Although the self-supervised models presented in this paper have demonstrated improved performance in speaker emotion recognition, there are still opportunities for further enhancing the model. We can use CNN models in the following of the output of the feature extraction part. In this case, we calculated the mean of the features. By using CNN models we can capture a more meaningful relationship between the features rather than a mean measurement. Also, performance could be further improved by augmenting the training dataset with noise, SpecAugment, and room impulse responses (RIRs)."}]}