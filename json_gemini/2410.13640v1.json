{"title": "LATENT SPACE CHAIN-OF-EMBEDDING\nENABLES OUTPUT-FREE LLM SELF-EVALUATION", "authors": ["Yiming Wang", "Pei Zhang", "Baosong Yang", "Derek F. Wong", "Rui Wang"], "abstract": "LLM self-evaluation relies on the LLM's own ability to estimate response correct-\nness, which can greatly improve its deployment reliability. In this research track,\nwe propose the Chain-of-Embedding (CoE) in the latent space to enable LLMs to\nperform output-free self-evaluation. CoE consists of all progressive hidden states\nproduced during the inference time, which can be treated as the latent thinking\npath of LLMs. We find that when LLMs respond correctly and incorrectly, their\nCoE features differ, these discrepancies assist us in estimating LLM response cor-\nrectness. Experiments in four diverse domains and seven LLMs fully demonstrate\nthe effectiveness of our method. Meanwhile, its label-free design intent without\nany training and millisecond-level computational cost ensure real-time feedback in\nlarge-scale scenarios. More importantly, we provide interesting insights into LLM\nresponse correctness from the perspective of hidden state changes inside LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have significantly enhanced their ability to generalize across diverse\nscenarios (Brown et al., 2020; Achiam et al., 2023; GLM et al., 2024). However, their outputs can\nsometimes be unstable, leading to incorrect responses that may threaten social safety. Therefore, label-\nfree LLM self-evaluation estimating the correctness of LLM responses fully through LLMs' own\ncapabilities has emerged as a crucial research area. It can provide real-time response monitoring\nand feedback in large-scale employments, enhancing the reliability of LLMs (Sun et al., 2024).\nPopular self-evaluation research in the era of LLMs focuses more on output-based forms (Zhang\net al., 2023). Two typical paradigms that do not assess the internal states of LLMs involve directly\nasking LLMs to express confidence in their responses through well-designed prompts (Lin et al.,\n2022a; Tian et al., 2023), and generating multiple responses by perturbing prompts (Gao et al., 2024)\nor decoding sampling (Wang et al., 2023) to calculating the response consistency (Xiong et al., 2024).\nBesides the two types, other methods basically draw on uncertainty estimation concepts from the era\nof deep neural networks, leveraging output logits or probability distributions to gauge the confidence\nof model responses (Malinin & Gales, 2020; Si et al., 2022; Huang et al., 2023; Kuhn et al., 2023).\nRecently, some research has revealed that the latent space of LLMs contains a substantial amount of\nuntapped hidden state information, they can largely reflect response correctness (Azaria & Mitchell,\n2023; Liu et al., 2023; Duan et al., 2024), and are usually more interpretable than LLM output (Li\net al., 2024a). However, these output-free research often require correctness labels 0/1 for training\nprobing classifiers to extract features from hidden states (Burns et al., 2022; Sky et al., 2024; Su et al.,\n2024). This contradicts our goal of being \"label-free\" and limits the generalization capabilities on\nunseen data. To further expand this research line, we consider a challenging but valuable question:\nHow to solely utilize hidden states to estimate the LLM response correctness without any label?\nTo answer this question, we start from the perspective of human thinking: In the cognitive theory,\nhuman thinking is accomplished collaboratively by intuitive thinking (system 1) and deliberative\nthinking (system 2) (Evans, 2003): correct thinking tends to activate system 2 to produce more\ndeliberative thinking paths, while incorrect thinking tends to be affected by system 1 to make more\nrapid and direct thinking paths (Kahneman, 2011). This cognitive phenomenon means that human\nthinking paths may differ when responding correctly and incorrectly.\nNext, we draw an analogy about the latent thinking path from humans to LLMs: Some research\n(Peters et al., 2018; Tenney, 2019; Jawahar et al., 2019; Chen et al., 2020) has demonstrated that"}, {"title": "2 CHAIN-OF-EMBEDDING REFLECTS RESPONSE CORRECTNESS", "content": "large Transformer-based (Vaswani et al., 2017) language models generate text representations by\nfirst encoding morphological and syntactic information in the lower layers, then progressing to more\ncomplex semantic information in the higher layers. This means that the hidden state changes can\nmirror the interpretable progressive thinking of LLMs in the latent space. Moreover, some LLM\nmechanistic studies also utilize multiple hidden states to explore the thinking states at different stages\nof LLMs (Ye et al., 2024). Therefore, we can treat the progressive hidden states as the latent thinking\npath of LLMs, which we term Chain-of-Embedding (CoE), as shown in Figure 1.\nBased on these analyses, we boldly migrate human thinking patterns to LLMs and make the following\nassumption: CoE discrepancies may happen when LLMs generate correct and incorrect responses.\nStarting from this assumption, our paper\nstructure and contributions are as follows:\nIn Section 2, we first explore the CoE dis-\ncrepancy when LLMs respond correctly\nand incorrectly by quantifying its features\nto demonstrate this assumption. In Section\n3, we propose a comprehensive CoE-based\nmetric for label-free LLM self-evaluation.\nIn Section 4, we verify the performance\nof our CoE method from four diverse do-\nmains that are popular in the LLM ability\ntest: Mathematics, Reasoning, Knowledge,\nand Understanding, to demonstrate the effectiveness of CoE for self-evaluation. In Section 5, we\nconduct theoretical analyses further to present more insights about the effectiveness of our method."}, {"title": "Problem Statement.", "content": "Label-free LLM self-evaluation aims to estimate whether the LLM response\nto a given input question is correct or not fully through LLMs' own capabilities without relying on\nany true label, external tool, and supervised trainer (Chen & Mueller, 2023; Li et al., 2024c). We\ndenote the language model as $f$. For each sample $(x, y) \\leftrightarrow (question, true answer)$, the output of\nthe language model is denoted as $\\hat{y} = f(x)$. Then this sample is associated with a decision score\n$s = S(x, \\hat{y}, f)$, where $S(\\cdot)$ is a decision function derived solely from the question, LLM output, and\nlanguage model information\nwithout reference to the true label $y$. The domain of this function\nencompasses the entire sample and model space. An ideal decision function aims to achieve the\nfollowing goal: a higher score indicates a greater likelihood of a correct answer.\nTherefore, the self-evaluation task can be formulated as a binary classification problem. Let $\\gamma$ be\nthe decision threshold, for a question, whether the LLM response is correct can be discriminated as\nthe instruction function $\\chi(y) = Correct \\ if \\ s > \\gamma \\ else \\ Incorrect$. The core goal is to find an optimal\nthreshold to improve the classification accuracy."}, {"title": "2.1 DEFINITION: LATENT SPACE CHAIN-OF-EMBEDDING (COE)", "content": "Formalization. We start by formalizing the CoE under the language model $f$. Assume the model\nhas $L$ hidden layers, we can decompose $f$ into the following ordered sub-modules:\n$f = f_{head} \\circ f_L \\circ \\ldots \\circ f_l \\circ \\ldots \\circ f_1 \\circ f_{emb}$.\n(1)\nIn Eq.1, $f_{head}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{|V|}$ is the final classification layer, $f_{emb}: \\mathbb{R}^{|V|} \\rightarrow \\mathbb{R}^d$, which can also be\ndenoted as $f_0$, is the embedding mapping layer (0-th layer), and each $f_l(1 \\leq l \\leq L) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ is\nthe intermediate hidden layer. Here $d$ is the embedding dimensions and $V$ is the model vocabulary.\nGiven a question $x$ as input to $f$, the output $\\hat{y}$ consists of $T$ tokens $\\hat{y}_1\\hat{y}_2...\\hat{y}_T$. For the $t$-th token, we\ndenote its hidden state at layer $l(0 \\leq l \\leq L)$, i.e., the $t$-th output embedding of function $f_l \\circ \\ldots \\circ f_1 \\circ f_0$,\nas $z_t^l$. Following the definitions in Ren et al. (2022); Wang et al. (2024), we define the average\nembedding at layer $l$ as $h_l = \\frac{1}{T}\\sum_{t=1}^T z_t^l$, which represents the $l$-th sentence hidden state. Then, the\nCoE is expressed as a progressive chain $H$ of all sentence hidden states formalized as follows:\n$H = \\underset{Input State}{h_0} \\rightarrow \\underset{Intermediate Hidden States}{h_1 \\rightarrow \\ldots \\rightarrow h_{L-1}} \\rightarrow \\underset{Output State}{h_L}$.\n(Chain-of-Embedding)"}, {"title": "Feature Definition.", "content": "After formalizing the CoE, we need to quantify its features so that we can\nutilize them to validate the assumption proposed in Section 1: \u201cCoE discrepancies may happen when\nLLMs generate correct and incorrect responses\". We can create a continuous CoE trajectory in the\nlatent space by performing segmented linear interpolation (simply connecting adjacent states) on the\nCoE. To measure the trajectory feature, its geometric information is the most fundamental dimension\n(Helland-Hansen & Hampson, 2009; Rintoul & Wilson, 2015), which usually includes magnitude and\nangle that can reflect the distance and direction changes produced during the trajectory wandering.\nWe briefly examine the practical significance of these two features in measuring the LLMs' thinking\npath. The magnitude feature is undoubtedly the direct feature of the thinking path curvature. In\ncontrast, while the angle feature does not explicitly represent the thinking path curvature, the cosine\nvalue between two embeddings indicates semantic similarity (Rahutomo et al., 2012). This suggests\nthat the angle feature can indirectly reflect the thinking path feature at the semantic modeling level.\nNow, we first define changes in magnitude and angle between each adjacent state pair $(h_l, h_{l+1})(0 \\leq\nl \\leq L - 1)$. The magnitude change $M(h_l, h_{l+1})$ is quantified using the L2-norm, while the angle\nchange $A(h_l, h_{l+1})$ is derived by indirectly calculating the cosine value between the two vectors,\nand then transformed using the arc cosine function. The two measures are formalized as follows:\n$M(h_l, h_{l+1}) = ||h_{l+1} - h_l||_2, \\ \\ A(h_l, h_{l+1}) = arccos(\\frac{h_{l+1}^T h_l}{||h_{l+1}||_2||h_l||_2})$.\n(2)\nSubsequently, the magnitude and angle features of the whole CoE trajectory, denoted as $Mag(H)$\nand $Ang(H)$, can be defined as the average changes in magnitude and angle between each adjacent\nstate pair. The two features are formalized as follows:\n$Mag(H) = \\frac{1}{L} \\sum_{l=0}^{L-1} \\frac{M(h_l, h_{l+1})}{Z_{Mag}}, \\ \\ Ang(H) = \\frac{1}{L} \\sum_{l=0}^{L-1} \\frac{A(h_l, h_{l+1})}{Z_{Ang}}$.\n(3)\nIn Eq.3, to reduce potential sample biases, we set the range scaling factors $Z_{Mag} = M(h_0, h_1)$ and\n$Z_{Ang} = A(h_0, h_1)$ for the following reason: The positions in the input space and output space of\ndifferent samples may vary, if the input and output of one sample are far apart in the latent space,\nits trajectory naturally has a longer wandering distance. By setting scaling factors, we convert the\nabsolute magnitude and angle changes of each adjacent state pair into relative changes, specifically,\nthe changes of $(h_l, h_{l+1})$ is relative to the changes of the input-output states $(h_0, h_1)$, thereby\navoiding measurement noise caused by inherent differences between samples."}, {"title": "2.2 How COE REFLECTS RESPONSE CORRECTNESS", "content": "Setup. After defining CoE and its features, we aim to explore the generalized impact of CoE on the\nLLM response correctness. Therefore, we focus on four popular domains: Mathematics, Reasoning,\nKnowledge, Understanding, with each domain set MATH, TheoremQA, MMLU, and Belebele\nas domain datasets separately. Dataset details and citations can be found in Section 4.1. In each\ndomain dataset, we divide the correct and incorrect samples into two sets, with each corresponding\nto a feature set, denoted as $V_+ = \\{(Mag_i, Ang_i)\\}_{i=1}^{n_+}$ and $V_- = \\{(Mag_i, Ang_i)\\}_{i=1}^{n_-}}$. Additionally,\nwe use the Qwen2-7B-Instruct (Yang et al., 2024) model as the backbone. These settings apply to\nall experimental analyses in this section (Figure 2 and 3) and will not be repeated hereafter.\nCoE Feature Distribution Discrepancy. Now, we quantify the CoE feature discrepancies between\ncorrect and incorrect sample sets for each domain. Due to each sample having two CoE features,\nwe employ 2D kernel density estimation to calculate the probability density function (PDF) $f_V$ for\neach feature set $V = \\{(Mag_i, Ang_i)\\}_{i=1}^{n}$, which represent the CoE feature distribution of $V$. We use\nGaussian kernel for PDF estimation, expressed as:\n$f_V (Mag, Ang) = \\frac{1}{n \\pi h^2} \\sum_{i=1}^{n} \\frac{1}{2 \\pi} exp\\{-\\frac{[(Mag - Mag_i)^2 + (Ang - Ang_i)^2]}{2h^2}\\}$,\n(4)\nwhere $h$ is the bandwidth, which we set to 1 to maintain consistency with the default value in\nthe Python sklearn library (Pedregosa et al., 2011). We implement Eq.4 using the sklearn\nlibrary. In each domain, we independently derive the PDFs $f_{V_+}$ and $f_{V_-}$ for $V_+$ and $V_-$ using Eq.4, as\nillustrated in Figure 2. We find that in all domains, the distributions of correct and incorrect samples\ndo not overlap and show significant discrepancies, with conclusions described below:"}, {"title": "3 COE SCORE FOR OUTPUT-FREE LLM SELF-EVALUATION", "content": "In Section 2, we have quantified CoE features and highlighted discrepancies between correct and\nincorrect samples. Next, we wish to create a self-evaluation metric combining the two features, so as\nto detect the LLM response correctness using a comprehensive CoE feature. However, the feature\ncombination is not simple because of their inconsistent magnitudes. We propose two ways as follows:\nCoE-R: Real-Space Combination. A straightforward method is to calculate a numerical summa-\ntion of the two features. Though adding the two real numbers may seem unmeaningful due to their\ndiffering magnitudes, our focus is on the metric relative trends rather than exact numbers, so this way\ncan preserve the metric usability without considering the feature relevance.\nIn Section 2.2, we have found that $Ang(H)$ is inversely proportional to the response correctness, so\nfor each adjacent state pair, we use $1 - A(h_l, h_{l+1})/A(h_0, h_1)$ as the direction change measure, then\nadd it with $M(h_l, h_{l+1})/M(h_0, h_1)$. By removing extraneous constants, we derive $CoE-R(H)$\nscore by averaging all adjacent state pair changes with the real-space combination way:\n$CoE-R(H) = \\sum_{l=0}^{L-1} \\frac{1}{L} (\\frac{M(h_l, h_{l+1})}{M(h_0, h_1)} - \\frac{A(h_l, h_{l+1})}{A(h_0, h_1)})$.\n(5)"}, {"title": "CoE-C: Complex-Space Combination.", "content": "However, while the exact values of the two features may\nnot be critical, they can significantly interfere with each other, particularly if one is abnormally large.\nThis interference can weaken the overall impact of another feature, resulting in the instability of the\nCoE-R metric. Therefore, we aim to combine the two features more seamlessly.\nThe magnitude and angle features enable a clear association of complex numbers in the complex\nplane, with each point uniquely represented by its complex magnitude and complex argument.\nTherefore, for each adjacent state pair, We combine $M(h_l, h_{l+1})$ and $A(h_l, h_{l+1})$ into a new feature\npoint $C(h_l, h_{l+1})$ on the complex plane, where $M(h_l, h_{l+1})$ represents the complex magnitude and\n$A(h_l, h_{l+1})$ represents the complex argument. This feature point $C(h_l, h_{l+1})$ can be expressed as:\n$\\begin{aligned}C(h_l, h_{l+1}) & = M(h_l, h_{l+1})e^{i \\cdot A(h_l, h_{l+1})} \\\\\n& = M(h_l, h_{l+1}) cos(A(h_l, h_{l+1})) + i \\cdot M(h_l, h_{l+1}) sin(A(h_l, h_{l+1})),\\end{aligned}$ (6)\nwhere $i$ is the imaginary unit. Each adjacent state pair corresponds to one feature point, we then\naverage these $L$ feature points by separately averaging their real and imaginary parts. The magnitude\nof this averaged point yields the final $CoE-C(H)$ score with the complex-space combination way:\n$CoE-C(H) = \\sqrt{ [\\frac{1}{L} \\sum_{l=0}^{L-1} Re(C(h_l, h_{l+1}))]^2 + [\\frac{1}{L} \\sum_{l=0}^{L-1} Im(C(h_l, h_{l+1}))]^2}  = \\sqrt{ [\\frac{1}{L} \\sum_{l=0}^{L-1} M(h_l, h_{l+1}) cos(A(h_l, h_{l+1}))]^2 + [\\frac{1}{L} \\sum_{l=0}^{L-1} M(h_l, h_{l+1}) sin(A(h_l, h_{l+1}))]^2}$. (7)"}, {"title": "4 EXPERIMENTAL VERIFICATION", "content": "Setup. We select six datasets across four domains for our self-evaluation experiments. These\ndomains reflect the four critical dimensions of LLM capabilities (Zheng et al., 2024; Huang et al.,\n2024): (1) GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) for the Mathematics\ndomain; (2) CommonsenseQA (Talmor et al., 2019) and TheoremQA (Chen et al., 2023) for the\nReasoning domain; (3) MMLU (Hendrycks et al., 2020) for the Knowledge domain; (4) Belebele\n(Bandarkar et al., 2023) for the Understanding domain. Dataset details are shown in Appendix C.1.\nLanguage Model. We use instruction-based models due to their ability to follow instructions,\neffectively addressing diverse user needs. We mainly adopt 7B+ parameter models with the Zero-Shot-\nCoT generation paradigm (Wei et al., 2022; Kojima et al., 2022), including Llama2-7B (Touvron"}, {"title": "4.2 MAIN RESULTS (TABLE 1)", "content": "Method Comparisons. First, our methods achieve SOTA performances across almost all sce-\nnarios. In four domains, our CoE achieves an average improvement of 8.30%, 5.55%, and"}, {"title": "4.3 EXTENDED ANALYSIS", "content": "First, we further analyze the effectiveness of our CoE methods.\nComponent Ablation. CoE scores consist of two components: magnitude and angle. To assess\ntheir impact on the combination metric, we conduct ablation studies. Table 2 presents the AUROC\nresults for four 7B+ parameter models. We observe that in 14 out of 16 settings, the combination\nmetric outperforms the individual components, indicating a positive influence from both components.\nFurthermore, when anomalies arise \u2014 such as in Mathematics and Knowledge domains with Qwen1.5-\n7B CoE-R is more affected by these anomalies, whereas CoE-C demonstrates greater robustness.\nAs a result, CoE-C offers more stable performance for real-world applications.\nTask Difficulty Exploration. Tasks within the same domain can vary in difficulty, likely affecting\nmetric performances. In our setup, we select two datasets of different difficulty levels for Math-\nematics and Reasoning domains, respectively. In particular, GSM8K and CommonsenseQA are\nlow-difficulty datasets; MATH and TheoremQA are high-difficulty datasets for they require at least\ncollege knowledge. Details are shown in Appendix C.1. Figure 5 shows the AUROC results with the\nQwen2-7B-Instruct model, where CoE has a slight edge in low-difficulty tasks, but demonstrates a\nsignificant advantage in high-difficulty tasks, and outperforms other baselines by large points. This\nindicates that CoE is more discriminative on more difficult tasks, which may be because the thinking\npaths are more complex on difficult tasks, increasing the potential informational features for CoE.\nSecondly, we analyze the reliability and stability of applying CoE methods in real-world scenarios.\nData Ratio Robustness. Self-evaluation differs from other classification tasks because the ratio of\npositive to negative samples in each scenario is not balanced, it entirely depends on the accuracy a\nof LLM responses. If we denote the number of positive samples in one dataset as $s+$ and negative\nsamples as $s$, then the ratio can be expressed as $s_+ : s_- = a$. To assess the performance robustness\nof CoE under different data ratios, we match all the CoE results in Table 1 with the response accuracy\nof the corresponding models on the corresponding datasets, then observe the AUROC results under\ndifferent data ratios. Figure 6 shows the results, there is no significant performance drop in any\nparticular area, especially in areas where a < 0.2 and a > 0.8, indicating that data imbalance does\nnot adversely affect performances. This suggests that CoE is robust against varying data ratios.\nHigh Deployment Efficiency. Existing methods have significant efficiency bottlenecks. Excluding\nthe base LLM inference: For sampling-based methods, LLMs must perform at least one additional\ninference, so the inference time is the lower bound of its execution cost. For sampling-free output-\nbased methods, they almost all require the output probability distribution, so the SoftMax computation"}, {"title": "5 THEORETICALLY REVISIT COE-C AND COE-R", "content": "Monotonicity Analysis. In Section 3, we pointed out that CoE-R lacks physical significance as\na metric. However, its linearity effectively captures the monotonicity of both magnitude and angle\nfeatures. In contrast, the monotonicity of these two features of CoE-R is not as apparent. We assume\n$n$ feature points, each represented as a pair of magnitude and angle $(L_i, a_i)$ for $1 \\leq i \\leq n$, with\n$L=[L_i]_{i=1}^n$ and $a=[a_i]_{i=1}^n$. The final CoE feature is denoted as $F(L, a)$. We consider the\nincrements $\\Delta L$ applied to $L_i$ and $\\Delta a$ applied to $a_i$, with the CoE feature increments being\n$\\begin{aligned}\\Delta F(L_i) & = F(L_i + \\Delta L, L_{-i}, a) - F(L_i, L_{-i}, a), \\\\\n\\Delta F(a_i) & = F(a_i + \\Delta a, a_{-i}, L) - F(a_i, +\\Delta a, a_{-i}, L)\\end{aligned}$\n(8)\nFor CoE-R, its final feature $F_R$ = $\\sum_{i=1}^n \\frac{L_i}{n}$ ensures $\\Delta F_R(L_i) = \\frac{\\Delta L}{n} > 0$ and $\\Delta F_R(a_i) = \\frac{-\\Delta a}{n} < 0$. In contrast, The situation on CoE-C is relatively complex, we formalize its final feature\n$F_C$ and computer its feature increments $\\Delta F_C(L_i)$ and $\\Delta F_C(a_i)$ by following Eq.8 as below:\n$\\begin{aligned}F_C(L, a) & = \\sqrt{ [\\frac{1}{n} \\sum_{j=1}^n L_j cos a_j]^2 + [\\frac{1}{n} \\sum_{j=1}^n L_j sin a_j]^2} = \\frac{1}{n} \\sqrt{\\sum_{j}L_j^2 + \\sum_{k, t, k \\neq t} 2 L_k L_t cos(a_k - a_t)},\\end{aligned}$\n(9)\n$\\begin{aligned}\\Delta F_C(L_i) &= \\frac{\\sqrt{\\sum_{j=1, j \\neq i} L_j^2 + (L_i + \\Delta L)^2 + \\sum_{k, t, k \\neq t \\neq i} 2 L_k L_t cos(a_k - a_t) + \\sum_{j, j \\neq i} 2(L_i + \\Delta L) L_j cos(a_i - a_j)}}{\\frac{n^2}{F_C(L_i + \\Delta L, L_{-i}, a) + n^2F_C(L, a)}}  \\\\&=\\frac{\\Delta L (2L_i + \\Delta L + \\sum_{j, j \\neq i} 2L_j cos(a_i - a_j))}{n^2F_C(L_i + \\Delta L, L_{-i}, a) + n^2F_C(L, a)}}\n\\end{aligned}$\n(10)\nSee Appendix B.2.1 for the derivation of Eq.10. In practical inference, we statistically find that more\nthan 98% of the cases fall within the range of $a_i$ between 0 and $\\pi/2$, this can also be intuitive in\nFigure 3. Consequently, it is nearly always true that $\\Delta F_C(L_i) > 0$. Additionally, the angle difference\nbetween correct and incorrect trajectories is often greater than the angle difference within a single\ntrajectory, namely $|a_i - a_j|$. When $\\Delta a$ causes $a_i$ to deviate from the current class feature, it tends\nto be sizable. As a result, $sin (a_i - a_j + \\frac{^{\\Delta a}}{2})$ will be greater than 0, leading to $\\Delta F_C(a_i) < 0$.\nTherefore, in practical scenarios, the CoE-C monotonicity of both magnitude and angle features is\nconsistent with CoE-R and satisfies conclusions drawn in Section 2.2."}, {"title": "Why CoE-C is More Robust Than CoE-R?", "content": "In Section 3, we pointed out that CoE-C may be more\nsensitive to outliers. This claim has been verified in the ablation study presented in Section 4.3. Here,\nwe delve into the fundamental reasons behind the metric robustness from a theoretical perspective.\nWe already know that the magnitude changes of the CoE trajectory of a correct sample is more\nsignificant, which means that for an incorrect sample, if one $L_i$ of a feature point appears abnormally\nlarge, it will be easily misclassified as a correct sample. Therefore, if one CoE feature can better\ncontrol the increment when facing this situation, it will reduce the risk of misclassification. Formally,\nwe compare $\\Delta F_R(L_i)$ and $\\Delta F_C(L_i)$, the smaller one CoE metric possesses stronger robustness.\nWe first deflate the lower bound of $\\Delta F_C(L_i)$ by fixing the principal element $L_i$:\n$\\overline{F_C(L_i, a)} = \\frac{1}{n^2} (L_i + \\sum_{j, j \\neq i} L_j cos(a_i - a_j))^2 =  \\frac{1}{n^2}(L_i^2 + \\sum_{j, j \\neq i} 2 L_i L_j cos(a_i - a_j) +  \\sum_{j \\neq i} L_j^2 ) = \\overline{F_C}(L_i)$.\n(11)\nThen, we use this deflation bound to further deflate the $\\Delta F_C(L_i)$ of Eq.8:\n$\\begin{aligned}\\Delta F_C(L_i) \\leq \\frac{\\Delta L (2L_i + \\Delta L + \\sum_{j, j \\neq i} 2L_j cos(a_i - a_j))}{n^2 \\cdot (L_i + \\Delta L + \\sum_{j, j \\neq i} L_j cos(a_i - a_j)) + n^2 \\cdot  (L_i + \\sum_{j, j \\neq i} L_j cos(a_i - a_j))}\n= \\frac{\\Delta L}{n}\\end{aligned}$\n(12)\nWe find that the right side of Eq.12 is exactly $F_R(L_i)$, which implies $F_C(L_i) \\leq F_R(L_i)$, proving\nthat CoE-C is more robust than CoE-R. The complete derivation can be found in Appendix B.2.2."}, {"title": "6 RELATED WORK AND DISCUSSION", "content": "Our research focuses on label-free self-evaluation", "label-free\u201d\ncondition, and we present a detailed discussion about these related works in Appendix A.1.\nFrom the perspective of research ideas, our research involves the usage of hidden state information.\nMany existing studies typically utilize hidden states to train correctness-label-based probing classifiers.\nThey can learn useful hidden state features of specific datasets or error types, but their generalization\nability on out-of-distribution (OOD) data is unpredictable. Despite the differing research intents,\nconsidering the overlap of research ideas, we also conduct simple comparisons with them.\nWe select two recent works": 1, "datasets": 1}]}