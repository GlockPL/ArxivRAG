{"title": "Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis", "authors": ["Yousef Yeganeh", "Ioannis Charisiadis", "Marta Hasny", "Martin Hartenberger", "Bj\u00f6rn Ommer", "Nassir Navab", "Azade Farshad", "Ehsan Adeli"], "abstract": "Scaling by training on large datasets has been shown to enhance the quality and fidelity of image generation and manipulation with diffusion models; however, such large datasets are not always accessible in medical imaging due to cost and privacy issues, which contradicts one of the main applications of such models to produce synthetic samples where real data is scarce. Also, finetuning on pre-trained general models has been a challenge due to the distribution shift between the medical domain and the pre-trained models. Here, we propose Latent Drift (LD) for diffusion models that can be adopted for any fine-tuning method to mitigate the issues faced by the distribution shift or employed in inference time as a condition. Latent Drifting enables diffusion models to be conditioned for medical images fitted for the complex task of counterfactual image generation, which is crucial to investigate how parameters such as gender, age, and adding or removing diseases in a patient would alter the medical images. We evaluate our method on three public longitudinal benchmark datasets of brain MRI and chest X-rays for counterfactual image generation. Our results demonstrate significant performance gains in various scenarios when combined with different fine-tuning schemes. The source code of this work will be publicly released upon its acceptance.", "sections": [{"title": "1. Introduction", "content": "In recent years, high-resolution image generation models such as the latent diffusion model (LDM) [43] have gained popularity for their ability to generate photorealistic images from prompts. These models are trained on large datasets such as LAION-5B [46], consisting of billions of primarily natural images with corresponding captions. Leveraging such models for medical image generation can be impactful where publicly available data, especially annotated, are scarce. Several factors contribute to the scarcity of medical data, including privacy concerns, the cost of collecting data from clinically ill patients, and the rarity of some diseases [49]. Additionally, conditional diffusion models hold valuable potential for counterfactual tasks like disease progression, aging, gender alteration, etc.\nAlthough such models are trained on all types of data, their main objective is to produce photorealistic images, and the diversity is not limited by any means. This is often in contradiction with the nature of medical images, which are restricted to certain shape templates; for instance, bony structures like a skull are required to keep their shape, while soft tissues can be altered due to aging or disease progression. Fine-tuning techniques for diffusion models have been recently developed [13, 26, 44], in which a minute number of images (3-5) are used to introduce the new concept to the model; few works have investigated the usability of such methods for medical image generation [3, 7, 44] without success for generating brain Magnetic Resonance Images (MRIs).\nIn this work, we introduce a generalized formal definition of conditioning in diffusion models and pose it as a counterfactual explanation optimization problem [50], which is a more restrictive way of conditioning suited for medical applications, see some examples in Fig. 1. We demonstrate that by employing a pre-trained diffusion model at the inference time, without any fine-tuning, one can generate similarities by fixing the noise distribution as a conditioning factor from different text prompts, as can be seen in Fig. 2. Referred to as Latent Drifting (LD), our proposed method poses a min-max optimization problem, aiming to match the learned distribution of pre-trained models to a new distribution represented by finite accessible samples without accessing their training data when fine-tuning diffusion models. The latent space adds to the traditional conditions, e.g., text or image, and its underlying distribution functions as an additional hyperparameter and a conditioning factor. This would enable the model to make a trade-off between diversity and the desired condition while training. By employing Latent Drifting, the model can adapt to the new domain more efficiently. Our proposed method boosts the image generation performance of SOTA approaches in medical image generation and manipulation using diffusion models by a large margin. Latent Drifting closes the distribution gap between the pre-trained models dataset(s) and the target medical domain in contrast to the prior work in medical imaging with diffusion models [25, 37, 38, 55] and Generative Adversarial Networks (GANs) [22, 39, 41, 42, 53] which were trained from scratch. The contributions of this work are as follows: (1) We propose a formal definition of conditioning for diffusion models based on the min-max optimization of a counterfactual formulation suitable for fine-tuning models on medical imaging data, (2) Our proposed Latent Drifting is agnostic to fine-tuning approaches and can be flexibly adapted for enhancing the distribution matching of pre-trained models on arbitrary data to the target data distribution, (3) We generate counterfactual medical images conditioned on text and image while preserving image quality and fidelity to the conditions. In addition, classifiers trained on synthetic images generated by LD show improved test accuracy on the real datasets, (4) We evaluate Latent Drifting on two longitudinal brain MR datasets [27, 52], and a Chest-Xray dataset [19]. These datasets contain different stages of patients, which makes them a fit for counterfactual image generation evaluations."}, {"title": "2. Related Work", "content": "Image generation. Image generation is a task that involves creating realistic and diverse images either unconditionally [14, 23, 24] or from various inputs, such as text, sketches, or graphs. It has many applications, such as image editing, synthesis, and manipulation [8-12, 21, 54]. Image generation has been largely driven by GANs [15] and diffusion models [32]. In particular, conditional image generation [30] has been explored, which allows controlling the content and style of the generated images based on different modalities. Conditional image generation methods are generally categorized into the following: Image-to-image translation transforming an image from one domain to another, such as Pix2Pix [20] and CycleGAN [59]; Semantic image generation, which produces images from an input semantic map[5, 35, 51]; Layout-based image generation generates images from bounding boxes and class labels for each scene instance [48, 58]; and text-conditioned generative models [28, 33, 57]. Recently, image generation has been revolutionized by the introduction of LDMS [43], which enables unconditional and conditional high-resolution image generation and editing.\nImage Editing. Another related task is image manipulation, which allows modifying an existing image according to some user input, such as a mask, a sketch, or a graph."}, {"title": "3. Method", "content": "We propose a method for generating counterfactual medical images using general pre-trained diffusion models. The diffusion model is either conditioned on a text prompt for text-to-image generation or an image and text for image-to-image translation. We are given a dataset D of images I and labels c, where I, c \u2208 D, and the labels c define the counterfactual element which corresponds to disease vs. healthy, or the patient's information such as age, gender, etc. Using a pre-trained diffusion model DM, our goal is to fine-tune the DM for the prompt-based medical image generation and manipulation task. The diffusion model is parameterized by \u03b8."}, {"title": "3.1. Diffusion Model", "content": "Given a data distribution xo ~ q(xo), the forward Markov process generates a sequence of random variables X1,X2,...,xT with transition kernel q(xt|xt\u22121). Then, based on the chain rule and the Markov property, we can factorize the joint distribution of X1,X2,...,XT conditioned on xo, denoted as q(x1,...,xT|x0), into:\nq(x1,..., XT|\u0445\u043e) = \u03a0t=1 T q(xt|xt\u22121). \nIn the forward process, noise is introduced into data until the distribution of latent space matches the Gaussian noise distribution p(x\u0442) with the mean of \u00b5 and variance of o, allowing to obtain xt in models like DDPM [17], where xt = \u221a\u0101txo + \u221a1 \u2013 \u0101tet with at := 1 - Bt and at = \u03a0t=0&st + 1 and \u0454 ~ N(0, I). In the reverse process, for generating new data samples, diffusion models start by sampling a noise vector from the prior distribution p(x), then gradually removing noise by running a learnable Markov chain in the reverse time direction. Specifically, the reverse Markov chain is parameterized by a prior distribution p(x\u012b) and a learnable transition kernel Po(Xt-1|Xt).\nThe learnable transition kernel po(xt-1|Xt) takes the form of\nPo(Xt-1|Xt) = N(xt\u22121; \u03bc\u00f8 (xt, t), \u03a3\u03b8(xt, t)),"}, {"title": "3.2. Conditioning", "content": "The diffusion model DM is trained on a denoising objective of the form.\nEx,c,e,tWt||20(Atx + \u03c3\u03c4\u03b5, c) \u2013 x||2,\nwhere (x,c) are data-conditioning pairs, t ~ U([0,1]), \u20ac ~ N(0, I), and at, 6t, wt are functions of t that influence sample quality. Intuitively, \u00eee is trained to denoise Zt = ax + \u03c3\u03c4\u03b5 into x using a squared error loss, weighted to emphasize certain values of t. Sampling such that the ancestral sampler [17] and DDIM [47] start from pure noise 21 ~ N(0, I) and iteratively generate points zt\u2081,..., Ztr, where 1 = t1 > \uff65\uff65\uff65 > tr = 0, modeled by p\u04e9(xt-1|Xt, C)."}, {"title": "3.3. Latent Drifting", "content": "We define LD as a hyperparameter in diffusion models to adapt the learned distribution of a pre-trained model De to a new data distribution DGT. LD is represented by a signed scalar value 8 and is introduced to the diffusion process. For the fine-tuning through Latent Drifting, LD is added to the target zy of the forward process, as well as the reverse processes:\nPo(xt-1|Xt) = N(xt\u22121; \u03bc\u04e9(xt, t) + \u03b4, \u03a3o(x, t)).\nFor example, in Fig. 2 LD is applied to reverse process only in inference time showcasing how it can control the style of synthetic images, and Fig. 3 to both forward and reverse process for fine-tuning. The discrepancy between the generated data distribution De and the target data distribution DGT can be quantified by a distance function d(DGT, De). In the absence of the training set used to fine-tune the pre-trained model, this distance can be estimated via Monte Carlo sampling of generated samples from De and existing samples in DGT. Here, we use L1norm as the distance function between the synthetic samples and the target dataset, as further explained in the algorithm in the supplement. The diversity of generated samples in a diffusion model is ensured by the stochastic element introduced by N(\u03bc,\u03c3). However, in the reverse Markov chain process of a conditional model po(zt\u22121|zt, c), N(\u03bc,\u03c3) remains unchanged despite the introduction of condition c. This phenomenon is demonstrated in Fig. 3 by visualizing the generated samples, the channel-wise data distribution, and the latent space distribution with and without Latent Drifting. As it can be seen, the data and latent distribution have a high variance when fine-tuned without LD, while the model with LD reaches a stable point that is resilient and robust to distribution shift. We hypothesize that the final latent variable z\u012b must be considered as part of the condition, and with our proposed d, we can modify \u00b5 to ensure that the learned representations of De accurately reflect the target data distribution DGT."}, {"title": "3.4. Counterfactual Image Generation", "content": "A sample x is labeled as y = f(x), such that (x, y) \u2208 DGT, and a counterfactual sample x' is a synthetically generated by a generative model Ge(x) that is highly similar to x but has different enough features such that it can be labeled differently y' = f(x'). This can be achieved by minimizing two contradictory loss values, resembling a min-max problem with the following objective function [50]:\nL(x, x', y', 1) = min [x. (lo(f(x'), y'))]\n+ min [lin (x,x')]\nwhere lin(x,x') ensures the similarity of the original instance x and the counterfactual instance x', and lo(f(x'), y') is a term to minimize the model's prediction for the counterfactual instance f(x') and the desired outcome y'. Inherently, lin x based on the definition of counterfactual conditions. The weighting factor A controls the trade-off between achieving the desired outcome and maintaining similarity to the original instance; hence, the model is optimized by minimizing lin, which leads to x'. Conversely, higher values of X, (1 in our counterfactual experiments) facilitate additional conditioning by introducing the latent drifting parameter & affecting new sampled latent value x' for the new data points."}, {"title": "Latent Drifting in Diffusion Models", "content": "A diffusion model with parameters 0, pre-trained on a large dataset of natural images De, generates samples by sampling from a latent space (z ~ N(0, I)). Standard fine-tuning on a limited dataset of medical images DGT assumes that model parameters & can directly transform to \u03b8'. However, the significant distribution shift between natural and medical images prevents the direct adaptation of generic foundation diffusion models. LD reframes this as a counterfactual generation problem, positing that 0 and 0' belong to a more general domain. Therefore, in a large pre-trained model De resembling a general domain, a latent space z', similar to z, exists, such that the model generates samples from DGT when conditioned on z'. In Eq. (5), if (x = 0), the Desired Outcome Fidelity term is eliminated, resulting in (z = z') (standard fine-tuning). However, if (x > 0), the minimum value of 8 is found via grid search to minimize the distance function. Fine-tuning then maximizes the probability that the model produces results belonging to the target domain. Here, we employed the L1norm as the distance function and tune hyperparameter \u03b4."}, {"title": "4. Experiments", "content": "To show the effectiveness of our method in medical image generation and manipulation, we adopted several methods: Stable Diffusion [43] with various fine-tuning schemes, namely Custom Diffusion [26], Dreambooth [44], and Textual Inversion [7] for text-to-image generation, and Pix2Pix Zero [36], and InstructPix2Pix [2] for text-conditioned image-to-image generation."}, {"title": "4.1. Experimental Setup", "content": "For all the experiments, the SD-v1.4 model pre-trained on the LAION-5B dataset [46] was employed. The Stable Diffusion framework is based on three main elements: A text encoder (here based on CLIP [40]), a latent diffusion model consisting of an Autoencoder, and a text-conditioned U-Net. Training all of these elements can be computationally expensive. Thus, fine-tuning methods have been developed that focus on a particular element of the Stable Diffusion framework. We evaluate four fine-tuning methods with and without LD, namely textual inversion, DreamBooth, Custom Diffusion, and Stable Diffusion basic fine-tuning (fine-tuning the denoising U-Net while freezing the rest of the components). While the first three methods only require a couple of samples for introducing a new concept, the latter requires a large amount of data. The implementation details and dataset preprocessing are provided in the supplement.\nDatasets To fine-tune and evaluate our method, we utilize three medical imaging datasets. For the experiments on brain MR imaging, we utilize the ADNI-1 [52] and OASIS-3 [27] datasets which include longitudinal MR scans of Mild Cognitive Impairment (MCI), Alzheimer's Disease (AD) or Cognitively Normal (CN) patients. For the chest x-ray experiments, we used the CheXpert [19] dataset, which is a large dataset containing over 224K chest radiographs of four categories: no finding (healthy), Cardiomegaly, Pleural Effusion, and Pneumonia. The datasets were preprocessed, and the final datasets consisted of 3269 scans (414 AD, 634 MCI, 2214 CN) for the brain dataset and 800 uniformly random samples from the chest x-ray dataset. The details of data preprocessing are reported in the supplementary material. All models for brain MR generation are evaluated on 200 samples, and the models on chest X-rays are evaluated on 400 test samples.\nEvaluation Metrics For the evaluation of the image realism, we calculate the Fr\u00e9chet Inception Distance (FID) [16] and Kernel Inception Distance (KID) [1] between the synthetically generated samples and our test set. Additionally, we train binary classification models (CN/AD) on a Resnet18 architecture using 600 synthetically generated brain MR (300 AD, 300 CN) slices and test them on real test sets. The chest x-ray classification model is trained based on the model obtained from LibAUC [56]. We report the area under the receiver operating characteristic curve (AUC) from the classification on the real test set.\nPrompt Generation Since the model performance can be highly dependent on the prompting style, we experiment with different prompting styles: Simple, where the same prompt template is used for all training images, and Di-"}, {"title": "4.2. Results", "content": "4.2.1. Medical Image Generation\nWe present the results of conditional medical image generation with and without our proposed method LD, in Tab. 1, and Fig. 5. In Fig. 4, we show examples of brain MR images generated by the different methods combined with LD"}, {"title": "Effect of Latent Drifting", "content": "Qualitatively, Fig. 5 shows a significant improvement of the visual realism across all methods when using a drift of 0.1. The background is consistently black as in real brain MR images; the shape of the brain becomes more realistic, and the white and gray matter structure improves. For an analytical evaluation, we calculated the FID between our test data and 200 synthetically generated images from each method (100 CN, 100 AD). The results in Tab. 1 demonstrate that LD improves the ability of the model to generate realistic MRI slices for both healthy brains and brains with Alzheimer's disease. For this reason, all following experiments were done with LD."}, {"title": "4.2.2. Medical Image Manipulation", "content": "The counterfactual conditioning for medical image manipulation is based on pairs of text and images for the two tasks of aging and disease conditioning. We employ the In-"}, {"title": "Age-conditioned Manipulation", "content": "The InstructPix2Pix model is based on Stable Diffusion [43]. We use three different Stable Diffusion models as the starting point. 1) Stable Diffusion model with basic fine-tuning, 2) Stable Diffusion model fine-tuned using Custom Diffusion [26], and 3) the original Stable Diffusion model. The input to the model includes an image together with an editing prompt, while the output is the edited image (e.g., Fig. 8). Tab. 2 shows the performance of the different starting point models with three prompting styles. As can be seen, the numerical prompts achieve the best overall performance. The Custom Diffusion model achieved a much lower performance than the other models, while the model based on the original Stable Diffusion performs slightly worse."}, {"title": "Disease-conditioned Manipulation", "content": "We use the Pix2Pix Zero model with a basic fine-tuned Stable Diffusion model to generate healthy brain MRIs from ones diagnosed with Alzheimer's Disease and vice versa. We generate the counterfactual images by negating the ground truth label of the 200 test samples and conditioning the model on the negated label value and the source image. We compute image quality metrics, as well as the AUC, using a disease classification model trained on 600 real brain MRI slices (300 AD, 300 CN). Additionally, we determine the Structure Similarity Inced (SSIM) between the target and the source image to determine how well the identity of the source image is retained. The qualitative results in Fig. 6, illustrate four examples from our two editing directions: from AD to CN and from CN to AD, respectively. When transitioning from AD to CN, the model primarily reduces the size of the ventricles. Conversely, in the CN to AD transformation, the ventricle size increases, accompanied by a worsening of brain atrophy."}, {"title": "4.2.3. Ablation Study", "content": "We provide different ablation studies on the effect of different parameters and components of our model. In this section, we show the effect of different prompt styles on the model performance. In the supplementary material, we ablate the effect of 8 and \u0442 parameters."}, {"title": "Prompt Style", "content": "We evaluated four varieties of prompting style, namely Simple and Diverse, both with and without the patient information (PI) in Tab. 3 and Tab. 4. In all scenarios, the models fine-tuned with Diverse prompts and patient information (PI) performed the best."}, {"title": "5. Conclusions", "content": "In this work, we analyzed the generalizability of transferring the knowledge from a pre-trained diffusion model on the natural image domain to another domain with a distribution shift, such as medical imaging. We proposed Latent Drifting, an approach for adapting the diffusion model during the fine-tuning process to the target domain by optimizing the latent space. Latent Drifting enables the model to generate realistic medical images, while taking advantage of the pretraining domain. We evaluated our method on the counterfactual medical image generation task using text prompts on multiple medical datasets and showed its advantage when combined with SOTA text-to-image and image-to-image generation models."}]}