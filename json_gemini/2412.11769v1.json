{"title": "\"Does it Chug?\" Towards a Data-Driven Understanding of Guitar Tone Description", "authors": ["Pratik Sutar", "Jason Naradowsky", "Yusuke Miyao"], "abstract": "Natural language is commonly used to describe instrument timbre, such as a \"warm\" or \"heavy\" sound. As these descriptors are based on human perception, there can be disagreement over which acoustic features correspond to a given adjective. In this work, we pursue a data-driven approach to further our understanding of such adjectives in the context of guitar tone. Our main contribution is a dataset of timbre adjectives, constructed by processing single clips of instrument audio to produce varied timbres through adjustments in EQ and effects such as distortion. Adjective annotations are obtained for each clip by crowdsourcing experts to complete a pairwise comparison and a labeling task. We examine the dataset and reveal correlations between adjective ratings and highlight instances where the data contradicts prevailing theories on spectral features and timbral adjectives, suggesting a need for a more nuanced, data-driven understanding of timbre.", "sections": [{"title": "Introduction", "content": "The study of music, whether through performance or appreciation, takes us on an ever-deepening journey to understand its many complexities. Among these complexities is the characteristic sound of the instruments, a property known as timbre. Within circles of musicians and music aficionados, unique vocabularies emerge to help articulate the subtle and intricate characteristics of instrument sounds. While common terms like bright or dark might resonate with a wide audience, others such as dry, fat, lush, and round introduce further nuance and intricacy. These terms, rich in nuance, aim to bridge the gap between the physical experience of sound and its emotional impact. However, a challenge arises in establishing a shared understanding of these descriptors: What defines the qualities that constitute a dry or fat sound? And more importantly, how can we navigate the subjective nature of sound perception to agree on what these terms truly signify?\nTo better understand how timbre adjectives are invented, and how online communities reach a consensus on their meanings, we construct a new dataset of aligned audio clips with varying timbres, annotated with adjective labels and pairwise comparison among the clips. Our study focuses on a single instrument: the electric guitar, motivated by (a) its extensive use across a broad spectrum of contemporary musical genres, (b) the presence of a rich community of online discussion forums for guitar enthusiasts that have given rise to many unique timbral adjectives (what does it mean to chug? What is a brown sound?), and (c) while the instrument inherently contributes certain timbral characteristics, it is predominantly the application of additional processing (effects, amplification) that shapes the sound into distinct timbres. This instrument choice enables us to apply different processing to a given guitar performance, creating many recordings where the timbre differs but the musical content remains constant. This approach allows us to isolate and study the effects of timbre independently from other factors. We release all code and dataset to facilitate additional research and aid the development of language and music creation systems, such as prompt-based music generation."}, {"title": "Related Works", "content": "The study of how we describe timbre, and the ways in which we create or borrow words to facilitate it, has a long history. Relevant to this work, it has been empirically found that experts, over a prolonged period of practice and exposure to various timbres, develop an ability to acutely distinguish between finer timbral variations and develop a sophisticated vocabulary to communicate them. Studies support that experts rely more on timbral differences when communicating about novel sounds, though the creative use of words is not limited to experts.\nAlso relevant to our work is how words are invented, or often borrowed from other contexts to fulfill a new role as a timbral descriptor. Among many studies on this topic, a recent study proposes a categorization of the origins of instrument timbre descriptors into seven classes. The descriptors in our proposed dataset are sufficiently diverse to have examples from each of these categories. Similar to our work,  use crowdsourcing to gather timbre annotations for recordings of audio effects, such as equalizers. Our work differs in that we focus on a variety of timbre for a single instrument and collect pairwise comparisons, and we construct our annotator pool of participants from online enthusiast communities.\nA widely used quantitative method for studying perceptual qualities of timbre involves rating sound stimuli on a verbal scale. One approach is the Semantic Differential (SD) technique , where each question involves rating adjective pairs that have opposing meanings, e.g., dark-bright, smooth-rough, etc. Due to the use of verbal scales, SD studies suffer from issues like polysemy and non-exact antonymy (bright-dull in , bright-dark in ). A common solution is to use unipolar rating scales , which are bounded by an attribute (e.g. soft) and its negation (e.g. not soft). Of note to our study is that while many adjectives have obvious opposites, many others do not. We thus argue that the creation of larger data is necessary, in order to enable a data-driven understanding of these terms.\nAn alternative to verbal scales are dissimilarity studies, in which participants rate differences between pairs of sounds. Techniques like multidimensional scaling (MDS) are then used to produce a spatial arrangement where distances between points correspond to these dissimilarity ratings. The latent dimensions of MDS can then be correlated with the physical characteristics of the sound."}, {"title": "Dataset Creation", "content": "The dataset creation process involves three key steps, (1) collecting a comprehensive set of adjectives for describing tone from online communities, (2) generating audio recordings that encompass a broad range of timbres, and (3) annotating the recordings via crowdsourcing using an online interface. As our dataset consists of nuanced timbral distinctions within a singular instrument class, all data is of electric guitar recordings."}, {"title": "Collecting Timbre Descriptors", "content": "In this work, we aim to study how timbre and tone are discussed more informally, evolving as the need develops, in the niche or online communities discussing specific music tones, genres, or styles. Thus, we turn to those communities themselves to know which adjectives are commonly used outside the established literature. We begin by crawling the internet for articles discussing guitar timbre words, using keyword searches of the form \u201ca(n) x sound/tone\" for a given adjective x. We also engage with these communities to gather additional suggestions. This process resulted in a set of 110 adjectives, which are presented in the appendix A."}, {"title": "Creating the Audio Files", "content": "To study a diverse set of timbral descriptors, it is necessary to generate a diverse set of instrument audio recordings such that they could foreseeably be described using a wide range of the adjectives gathered in the preceding step. We approach this problem using a two-step process, first generating unprocessed guitar sounds in a variety of genres (diverse content), and then processing them with different signal processing chains to yield a variety of sounds (diverse timbre).\nFirst, we record a series of unprocessed signals, also known as direct input (DI), from an electric guitar without any sound shaping. We hypothesize that some timbral descriptions may only apply to specific genres or styles of playing. For instance, very percussive and fast rhythm playing is unlikely to be described as chimey regardless of the instrument timbre. Therefore, to capture a variety of playing styles, we collect a number of recordings from three different guitar players, one amateur and two professional.\nWe manually sample segments from these recordings, aiming to select short segments representing a diverse set of styles and dynamics. The final set of DI contained 12 recording segments with content ranging from slow arpeggios, simple chords, aggressive-style rhythm playing, and fast soloing. Each segment is approximately 10 seconds in length, 44.1kHz monaural audio.\nWe then process each DI using a different FX chain to achieve a diverse set of timbres. For this, we use a commercial plugin (Helix Native) which emulates various effects, amplifiers, and cabinets. To ensure that these chains generate desirable sound, we utilize the included presets, which are specific parameter settings designed by manufacturers of audio plugins, artists, or other users to achieve a specific tone of interest. We process each of the 12 DI clips using the 80 preset effects to produce 960 audio samples. A complete list of the presets can be found in the appendix B. The processing of audio signals is performed using REAPER."}, {"title": "Annotation Interface Design", "content": "We design a web interface for collecting annotations, in which we collect three types of annotations."}, {"title": "Pairwise annotations", "content": "The annotator is presented with two samples, A and B, in random order. For a given adjective X, the annotator is asked to choose: (1) A is more X than B, (2) A is less X than B, (3) Both audio samples are equally X, or (4) to skip the question.\nEach audio sample A and B is based on the same DI recording, and thus their musical content is identical. This allows the user to focus solely on the differences in timbre, and to minimize the confounding aspects of other acoustic factors, such as pitch and loudness, which have been noted to affect the perception of timbre.\nThe benefit of the ranked comparison is that it allows us to gather data about very precise timbral relationships, e.g., in situations where the overall sound of timbre A vs. B is presumably much closer than that of previous work, where such clips would represent different instruments entirely. Second, ranking directly supports important practical use cases, such as \u201cIn which of these songs is the sound of guitar more X?\".\n\""}, {"title": "Label annotations", "content": "Pairwise rank comparison can be an extremely informative annotation, but because we must arrange comparisons randomly in order to avoid imparting any bias to the study, some ranked comparisons will be less useful and irrelevant. The ternary nature of our ranked comparison (an (A, B, X) tuple) may also lead to sparsity. In order to counteract this and ensure more information-per-recording, we also collect label annotations. After the annotator has made a ranked comparison, the annotator is asked to select any adjectives from the adjective list that may apply to the selected clip of the pairwise annotation."}, {"title": "Custom Annotations", "content": "A final source of annotations is an open text field, where annotators may enter any other adjectives that apply to the selected clip and are not contained in the adjective list. These adjectives aren't included in the annotation list but are retained in the dataset for future research."}, {"title": "Collecting Annotations", "content": "We seek to understand more nuanced descriptions of tone that arise in online communities under the need to describe increasingly specific timbral qualities. By the very nature of the study, a pool of general annotators (like those commonly hired via Mechanical Turk) is not appropriate for the study, as they lack the expertise and experience in discussing these sounds. Instead, we enlist volunteers from online guitar and music enthusiast communities by incentivizing participation using an online raffle system. In total, we collect 2038 annotations from 38 participants. In addition to timbral annotations, we also record participant information, such as where they heard about the study, and how many years of experience they have playing the guitar. Notably, 87% of our annotators have more than 10 years of experience playing guitar."}, {"title": "Unifying Annotations", "content": "As we collect multiple types of annotation on the level of individual clips, we present a method to unify the annotations and provide a single score for each clip-adjective combination (which can then be averaged over clips to provide a score between any preset/timbre and adjective). For pairwise comparisons, models like Bradley-Terry can be used, however, as we also include multi-label annotations on clips, we instead present a simple graph-based algorithm that combines the two types of annotations for its potential future use.\nFor every adjective in the label annotations, we add a constant \\( \\delta \\) to the presets labeled with the adjective, representing a single \u201cunit\u201d of adjective-preset correlation. Working with these ratings, we utilize pairwise annotations to discover and enhance the greater than or less than relationships among the data. For every adjective, we find the set of presets, \\( \\{H\\} \\), with the highest label annotation score. From the pairwise comparison data, we then find the relationships where A is rated less than B and \\( A \\in \\{H\\} \\). In alignment with such pairwise comparisons, we adjust the score of B to be greater than A by a constant, \\( \\Delta \\). We then infer scores lower than the lowest label annotation score. We repeat this inference process until no new higher or lower preset is found. In the case of ties, we prioritize the pairwise annotation data over the label annotations. We release these scores with the dataset."}, {"title": "Analysis", "content": ""}, {"title": "Presets By Adjectives", "content": "The table 1 shows presets corresponding most to a sampling of adjectives. Evaluating the correctness of a dataset of this type is difficult, as by its very nature there is no gold standard to refer to. However, we find many of the highly correlated presets correspond well to known descriptions of the sounds they are modeled on. For instance, 07B Line6 Litigator, which is ranked in the dataset as being most correlated to warm, is based on a Dumble Overdrive amplifier, which is expertly described as having a \u201cvery open and uncompressed feel, overdrive without fuzz, warm sustaining cleans, and of course that saxophone-like midrange and sing that these amps are famous for\u201d. We encourage the reader to listen to the clips for a better understanding of the extent to which these presets relate to these adjectives."}, {"title": "Novel Findings", "content": "Existing work, utilizing unaligned audio of different instruments, has identified spectral features that correlate with the perception of acoustic properties, which we describe using timbral adjectives. The annotations of our dataset allow us to revisit these claims and assess how well they agree with the crowdsourced consensus. We provide one case study on brightness and its relationship to the spectral centroid. We find that in pairs of clips which should be ranked as A > B with respect to existing theories, crowdsourced workers ranked them differently. Visualizations of these relationships are presented in the appendix C.2. We argue that these findings are evidence that further analysis into the acoustic causes of human perception of these properties is necessary."}, {"title": "Inter Annotator Agreement", "content": "As we aim to compare a variety of audio samples pairwise, across many adjectives, the number of possible comparisons is very high. And because annotators needed experience with the instrument, we're limited by how many possible data samples we can get, which naturally leads to sparsity and limits the ability to conduct inter-annotator agreement. However, amongst the 6 instances where we found multiple responses on the same annotation question, in only one case did the annotators disagree about the ranking of the clips."}, {"title": "Conclusions", "content": "In this work, we present a dataset that focuses on very fine-grained differences in timbre, isolating them from other factors by generating recordings of different timbres based on shared DIs, containing identical musical content. We find that human assessments sometimes differ from previously established correlations between coarse acoustic features and the perception of adjectives, supporting the need for a more nuanced understanding of acoustic correlates of these descriptors in the context of guitar music. Furthermore, this understanding will also yield practical improvements in prompt-based conditional audio generation, timbre-based music retrieval, and natural language interfaces for musical tools."}, {"title": "Further Analysis", "content": ""}, {"title": "Label Frequencies", "content": "Figure 1 shows the most frequent 35 labels. Among the most annotated labels, we find a frequency of annotation of 20-40 times. Even among the top labels, we observe a good diversity in timbre, although there seems to be some skew towards heavier genres. This may be a bias in our dataset stemming from uniformly sampling the Helix presets, many of which are geared toward metal and rock genres. These labels cover all the categories proposed in the comprehensive taxonomy study , some examples from each of the categories are Aggressive, Dull from Affect; Round, Full from Matter; Bright, Sharp from CMC; Boomy, Twangy from Mimesis; Muffled, Saturated from Action; Ringing, Muted from Acoustics; and Buzzy, Fizzy from Onomatopoeia. This diversity underscores the richness and complexity of timbral descriptions in our dataset."}, {"title": "Case Study: Spectral Centroid", "content": "\"Brightness\", which is a commonly studied timbral descriptor, dating back at least to and has more recently been correlated to the center of mass of the spectrum, often referred to as the spectral centroid. While this result holds generally in our dataset, and recordings with higher spectral centroids are more likely to be labeled as \u201cbright\u201d, we also observe many confounding factors. The rows of Figure 2 show spectrograms of pairwise comparison between two clips from our dataset where the left clip was annotated as less bright than the right one. In the top-row comparison, the spectrogram with the higher spectral centroid is indeed considered brighter, but in the second (bottom) comparison, the relationship does not hold.\nWhy is this the case? Although existing work on correlating spectral features to acoustic properties and adjectives provides a general approach, we hypothesize that other factors should be considered when correlating the acoustic feature to timbral adjectives. In the case of brightness, features like F0 and Harmonic-to-Noise ratio (HNR) may play a role. However, the difficulty of understanding the interactions between these features and how they relate to brightness supports the notion that a more data-driven (or machine learning-driven approach) may be necessary."}, {"title": "Cross-Correlation", "content": "We also perform a cross-correlation analysis between the clips and adjective labels (the most correlated adjectives are shown in the heatmap in Figure 3). We again observe the most frequent annotations pertaining to heavy or distorted sounds, but we can also observe the extent to which some adjectives may function as synonyms or are otherwise highly correlated. For instance, perhaps unsurprisingly, \u201cdistorted\u201d and \"dirty\" apply to the same clips. But a \u201cfull\u201d clip is one that is also \u201cdistorted\u201d and \u201cdirty\u201d, but also \"thick\" and often \u201cdark\u201d. In the absence of additional evidence, this method of defining less understood adjectives in terms of more understood adjectives can help find a more general consensus of meaning for new or unknown words. However, the data can also be used for a more focused study of the audio features based on contrastive examples (for instance, where a recording is labeled as \u201cthick\u201d but not \"full\") which can help identify which acoustic properties are most associated with the adjective, and to what extent adjectives are true synonyms."}, {"title": "Limitations", "content": "The constructed dataset provides a unique resource for researchers seeking to study the relationship between timbral descriptions and guitar sounds. However, there are limitations to note. Among them, in the era of big data, the number of annotations is relatively small. This is a consequence of the necessity that annotators be experienced in guitar playing and participants in online discussion forums. We present ways of smoothing these statistics to help enable their use in future research, but some estimates may be better represented than others. As there is no objective grounding of these terms, it is difficult to assess the extent to which this is true.\nA second concern is that our online approach to data collection allowed users to listen to the clips in their own environments, which may differ significantly from one user to another. However, previous crowdsourcing of timbre descriptions from audio clips have made similar assumptions. Our addition of pairwise comparison is designed to further mitigate the effect of the environment on labeling, as it establishes a relationship between two recordings."}]}