{"title": "PULMOFUSION: ADVANCING PULMONARY HEALTH WITH EFFICIENT MULTI-MODAL FUSION", "authors": ["Ahmed Sharshar", "Yasser Attia", "Mohammad Yaqub", "Mohsen Guizani"], "abstract": "Traditional remote spirometry lacks the precision required for effective pulmonary monitoring. We present a novel, non-invasive approach using multimodal predictive models that integrate RGB or thermal video data with patient metadata. Our method leverages energy-efficient Spiking Neural Networks (SNNs) for the regression of Peak Expiratory Flow (PEF) and classification of Forced Expiratory Volume (FEV1) and Forced Vital Capacity (FVC), using lightweight CNNs to overcome SNN limitations in regression tasks. Multimodal data integration is improved with a Multi-Head Attention Layer, and we employ K-Fold validation and ensemble learning to boost robustness. Using thermal data, our SNN models achieve 92% \u00b1 2% accuracy on a breathing-cycle basis and 99.5% \u00b1 0.5% patient-wise. PEF regression models attain Relative RMSEs of 0.11 \u00b1 0.05 (thermal) and 0.26 \u00b1 0.07 (RGB), with an MAE of 4.52% for FEV1/FVC predictions, establishing state-of-the-art performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Asthma and Chronic Obstructive Pulmonary Disease (COPD) pose significant challenges to global health [1], affecting approximately 339 million people and leading to over 3.91 million deaths annually, with COPD accounting for 6% of global mortality [2]. ID-19 pandemic has highlighted the critical need for efficient and remote lung health assessment methods [3]. Traditional methods such as spirometry, which measures key parameters like Forced Vital Capacity (FVC), Forced Expiratory Volume in one second (FEV1), and Peak Expiratory Flow (PEF), are often limited by issues of cost, accessibility, and hygiene, particularly in low-resource environments [4], prompting the exploration of non-invasive, continuous monitoring technologies through smartphones and wearable devices [5]. However, all the previous techniques lack generalization and fail to incorporate specific patient-related personal data.\nRecent innovations in this domain, such as thermal detection [6], advanced PPG [7], and optical imaging [8] have shown potential with reduced error rates, despite challenges like model overfitting and contrast issues [9]. Mobile applications like SpiroSmart and SpiroCall offer accessible lung function assessments but face robustness issues, with Mean Absolute Errors (MAEs) of 5.1% and 7.2%, respectively [10], [11]. Mobile thermal imaging respiratory oscillometry has shown notable accuracy in diagnosing pulmonary dysfunction, especially deep learning (DL) [12], [13]. UbiLung's passive monitoring approach has demonstrated efficacy in disease classification, achieving a 7.47% MAE in FEV1/FVC ratio estimation [14]. Although ideally, such solutions should run on a low resource setting, most DL-based solutions require high computational resources.\nSpiking Neural Networks (SNNs) are bio-inspired, energy-efficient neural networks that process temporal data by mimicking efficiently the human brain. Their use extends beyond healthcare to fields like biometric security and brain-inspired computing [15]. SNNs' application in medical data analysis underscores their potential to revolutionize medical diagnostics in low resource settings [16].\nThis paper proposes an innovative approach to lung health assessment, leveraging either RGB or thermal video data with patient metadata, including height, age, athletic activity, and smoking status, to enhance model accuracy. Central to our approach is the innovative use of SNNs and the video-efficient model, highlighting their versatility and energy efficiency in healthcare. Furthermore, we conduct a comparative analysis of SNN- and CNN-based models, focusing on their accuracy and efficiency performance for classification and regression. Our contributions are:\n\u2022\n\u2022\n\u2022\nWe introduce PulmoFusion, an end-to-end lung health assessment model utilizing regression and classification. This approach incorporates data augmentation, multi-head attention, and ensemble learning.\nTo the best of our knowledge, we are the first to introduce SNN to analyze thermal videos and efficiently integrate multi-modal thermal or RGB videos along with metadata for lung health assessment.\nWe achieved state-of-the-art performance on FEV1/FVC"}, {"title": "II. METHODOLOGY", "content": "We propose classification and regression models to assess lung health accurately. In addressing the complexity problem, where the measurements of an entire breathing-blow cycle are crucial, our approach necessitates a shift from conventional image models to a video-based learning model. Therefore, X3D model is used as our vanilla (base) model."}, {"title": "II-A. Spiking Neural Networks (SNNs) Models", "content": "SNNs excel in energy efficiency and temporal precision by processing data with spike timing, closely modeling biological neurons. Central to our SNN model is the Leaky Integrate-and-Fire (LIF) neuron, which dynamically updates its membrane potential ml)(t) at any time t in layer l as:\n$m^{(l)} (t) = \\beta v^{(l)} (t - 1) + W^{(l)}s^{(l-1)} (t)$, (1)\nwhere $W^{(e)}$ are learnable weights, $\u03b2$ is the decay rate, $s^{(l\u22121)}(t)$ represents input spikes from the previous layer, and $v^{(l)}(t)$ is the membrane potential at t. If $m^{(l)}(t)$ surpasses a threshold $V_{th}^{(l)}$, a spike $s^{(l)}(t)$ is emitted:\n$s^{(l)} (t) = H (m^{(l)} (t) - V_{th}^{(l)}) = \\begin{cases}1, & \\text{if } m^{(l)} (t) \\geq V_{th}^{(l)},\\\\0, & \\text{otherwise.}\\end{cases}$ (2)\nAfter firing, the neuron's membrane potential resets:\n$v^{(l)} (t) = m^{(l)} (t) \u2013 s^{(l)} (t)V_{th}^{(l)}$. (3)\nTo address challenges in gradient-based optimization due to non-differentiable spikes, we apply a surrogate gradient that approximates H' with a differentiable function, enhancing backpropagation efficiency. Despite recent advances, SNNs remain limited for regression tasks, especially in critical applications like healthcare. Thus, we focus on classification tasks where SNNs are more reliable.\nOur PulmoFusion Convolutional SNN (CSNN) model processes video sequences to identify spatiotemporal patterns.\nFrames are normalized and rate-coded into binary spike trains $U_{ij} \\sim Bernoulli(1, p_{ij})$, where $p_{ij}$ depends on pixel intensity. This rate-coded data flows through a 2D spiking convolutional layer, pooling, and a flat layer, with final classification based on spike frequency. In our multimodal approach, video data and metadata are converted into spike trains and fused through a fully connected layer, allowing comprehensive feature integration for enhanced analysis.\nFor SNN models, both thermal and RGB model architectures are used as in Figure 1 with one multi-modal for thermal, while RGB needs ensemble models, consisting of four models with diverse convolutional filters and parameters. This ensemble methodology effectively balanced computational efficiency with predictive performance, significantly enhancing diagnostic accuracy."}, {"title": "II-B. CNN Model- Unimodel and Multimodel", "content": "We investigate the X3D model small [17] for CNN regression and classification due to its ability to extend 2D images to handle spatial, temporal, width, and depth expansions. The model's expansion strategy includes adjustments in temporal duration ($\u03b3$ : t), frame rate ($\u03b3$ : \u03c4), spatial resolution ($\u03b3$ : s), network width ($\u03b3$ : w), bottleneck width ($\u03b3$ : b), and depth ($\u03b3$: d), enabling computational effective 3D video analysis.\nWe developed models for distinct data types: thermal and RGB videos, finding solid correlations between participant metadata-such as smoking habits, age, height, and athletic status and pulmonary health. Building on these insights, we designed a multimodal model that integrates video data with metadata, enhancing predictive accuracy.\nTo optimize data fusion, we tested two approaches: a basic Dense Layer for straightforward integration and a Multi-Head Attention Layer for more nuanced merging of CNN and metadata features. This attention mechanism allows the model to focus on critical features and deeper correlations.\nOur model, illustrated in Figure 2, uses an X3D CNN backbone to extract spatial-temporal features from videos. In contrast, metadata features are processed through a fully"}, {"title": "III. DATASET", "content": "To overcome the lack of availability of multi-modalities datasets for lung health assessment, we collected a novel dataset assembled using 60 volunteers [18]. These participants provided a wide range of data in terms of age (15-75 years), weight (51.2-102.7 kg), and height (154-189 cm), contributing to the dataset's diversity. They underwent two sessions: a resting state and a post-exercise state, each for 1 minute, generating 120 data points per type. The dataset contains RGB and thermal videos, heart rate, smartwatch electrocardiogram (ECG), blood pressure, and Peak Flow & Asthma Meter, which we used as our ground truth values.\nThe dataset also includes detailed metadata with personal and health-related information such as age, height, smoking duration, athlete status, seasonal cough indicator, lung past problems, lung genetic problems, and inbody data.\nTo enhance lung health analysis, estimated values for FVC and FEV1 were included and calculated using NHANES III study reference equations [19]. These values are used to determine if the person was healthy or not. If the FEV1/FVC percentage for the measured (using flow meter during data collection) over the predicted one (from the equations) 70% it means the subject is normal; otherwise, it implies there is an abnormality. This is a standard measurement by the American Lung Association [20]. This standard processing and value is used clinically to determine the patient's health status. PEF values were derived from the Nunn and Gregg equation [21], which are compared with the measured ones to detect how healthy the participant is. The dataset was collected under the supervision of expert doctors.\nThe experimental protocol ensured data integrity and consistency through a two-phase collection process, with steps for vital signs measurement, smartwatch ECG recording, respiratory flow assessment, and simultaneous thermal and RGB video recording. Video synchronization was achieved using a timestamp camera application. The post-processing stage involved meticulous manual inspection of videos, facial isolation using OpenCV for RGB and thermal recordings. The final dataset contained 2,424 segmented videos, where each segment is just one full breathing cycle."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "Our experimental methodology was designed with two primary goals: regression, to estimate PEF and evaluate the FEV1/FVC ratio, and classification, to detect abnormalities using the FEV1/FVC ratio with a delineation threshold of 70% for pulmonary dysfunction (38 subjects healthy, 22 not).\nWe enhanced dataset generalization by allocating 80% for training and 20% for testing based on individual subjects. As the data size is not large, we utilized a pre-trined X3D model to finetune along with 5-fold cross-validation to ensure distinct subject sets across training and testing phases and spatial and temporal data augmentation. Video clips were standardized to 30 frames and resized to 224 \u00d7 224 pixels.\nThe dataset included segmented video clips, each representing a unique respiratory cycle. Despite consistent metadata, we observed variability in respiratory performance. We implemented a post-processing technique to improve model accuracy by averaging respiratory metrics by participant to mitigate natural variability for the regression model. A majority voting scheme was implemented in the classification models, combining evaluations from sub-videos to determine a comprehensive diagnosis for each subject.\nFor SNN, models were optimized using ADAM optimizer and MSE count loss while CNN used ADAM and MSE loss for regression and categorical cross entropy for classification."}, {"title": "V. RESULTS & DISCUSSION", "content": "Table I presents a detailed comparison between thermal and RGB classification models, highlighting the thermal models' superior accuracy per breathing cycle. However, both models enhanced across all metrics upon patient-wise aggregation, illustrating the significant role of data aggregation. While multi-modal CNNs outperform SNNs in initial performance, this gap diminishes considerably on a patient-wise level, showcasing comparable effectiveness. SNNs distinguish themselves with faster inference, processing patient data in just 0.2 seconds versus CNN's 1.3 seconds, underscoring SNNs' advantage for rapid processing.\nThe RGB model's accuracy for abnormal and normal conditions was 80.41% and 83.23% breathing-cycle-wise, respectively, which surged to 100% patient-wise. The Thermal model showed a similar trend, starting from 90.72% (abnormal) and 94.75% (normal) to reaching 100% accuracy for patient-wise. Notably, the thermal data outperformed RGB due to its ability to capture changes in mask heat distribution, reflecting variations in exhaled air volume.\nTable II delineates the performance metrics of our regression models utilizing thermal and RGB imaging data, including Relative RMSE, Relative MAE, and Pearson Correlation coefficients. Our initial single-model efforts, leveraging video data, established a baseline with a Relative RMSE of 0.30 and a Pearson Correlation of 0.72. Despite demonstrating reasonable correlation, these results underscored the necessity for more accurately capturing video data.\nTo advance our model's performance, we employed techniques such as data augmentation to diminish error rates by diversifying the dataset and enhancing its generalization ability. Adopting a multi-modal incorporating metadata significantly enhanced the model's input. By implementing multi-head attention mechanisms, we enabled the model to concentrate precisely on relevant data features, thereby improving its ability to recognize complex patterns.\nWe achieved the best results by adding ensemble learning to the multi-modal with Multi-Head Attention multi, achieving notable results with a Relative RMSE of 0.13. We experimented with the best model for FEV1/FVC prediction and achieved MAE of 4.52%, representing state-of-the-art results. Our model's robustness ensures superior generalization capacity, making it highly applicable."}, {"title": "VI. CONCLUSION", "content": "We proposed a multi-modal, non-invasive pulmonary health evaluation approach that integrates RGB or thermal videos with patient metadata to assess lung function. By combining the complementary strengths of SNNs and CNNs within a unified framework, our methodology achieved notable advancements in accuracy, efficiency, and energy consumption. The results highlight the potential of thermal imaging, offering more precise insights into respiratory patterns. However, the reliance on high-quality, manually segmented datasets remains a critical bottleneck, potentially limiting scalability and real-world applicability. While our small participant pool and the scarcity of SNN regression models restrict generalizability, these limitations underscore the need for automated data preprocessing techniques, larger datasets, and further exploration of SNNs in regression tasks.\nFuture work addressing these issues can help unlock the broader potential of this approach, making it more practical for widespread adoption."}, {"title": "VII. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This work involved human subjects or animals in its research. This study was performed in line with the principles of the Declaration of Helsinki. Approval of all ethical and experimental procedures and protocols was granted by a written consent form signed by each participant for data usage for scientific research purposes only."}, {"title": "VIII. ACKNOWLEDGMENT", "content": "No funding was received for conducting this study. The authors have no relevant financial or non-financial interests to disclose."}]}