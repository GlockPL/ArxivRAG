{"title": "What Factors Affect Multi-Modal In-Context Learning? An In-Depth Exploration", "authors": ["Libo Qin", "Qiguang Chen", "Hao Fei", "Zhi Chen", "Min Li", "Wanxiang Che"], "abstract": "Recently, rapid advancements in Multi-Modal In-Context Learning (MM-ICL) have achieved notable success, which is capable of achieving superior performance across various tasks without requiring additional parameter tuning. However, the underlying rules for the effectiveness of MM-ICL remain under-explored. To fill this gap, this work aims to investigate the research question: \u201cWhat factors affect the performance of MM-ICL?\u201d To this end, we investigate extensive experiments on the three core steps of MM-ICL including demonstration retrieval, demonstration ordering, and prompt construction using 6 vision large language models and 20 strategies. Our findings highlight (1) the necessity of a multi-modal retriever for demonstration retrieval, (2) the importance of intra-demonstration ordering over inter-demonstration ordering, and (3) the enhancement of task comprehension through introductory instructions in prompts. We hope this study can serve as a foundational guide for optimizing MM-ICL strategies in future research.", "sections": [{"title": "Introduction", "content": "Recently, Large Language Models (LLMs) have demonstrated remarkable advancements, showcasing proficiency in a wide range of tasks [Zhao et al., 2023a, Qin et al., 2023, 2024, Hu et al., 2023, Pan et al., 2023]. Notably, advanced LLMs exhibit the emergence of novel capabilities such as In-Context Learning (ICL) [Wei et al., 2022a, Dong et al., 2022, Zhuang et al., 2023], which optimize task performance by incorporating demonstrations into input prompts [Giannou et al., 2023, Li et al., 2023d, Wies et al., 2023, Zhou et al., 2022]. In particular, multi-modal in-context-learning (MM-ICL) is capable of utilizing multi-modal demonstrations to quickly adapt to the downstream task without parameter tuning [Yin et al., 2023, He et al., 2023, Zhang et al., 2024, Li and Lu, 2024].\nIn the literature, a series of works emerge to enhance MM-ICL. Specifically, Gong et al. [2023] manually create a general template with multiple images and corresponding responses during instruction-tuning (IT) stage to improve MM-ICL. Tsimpoukelli et al. [2021], Li et al. [2023b], Doveh et al. [2024] and Zhao et al. [2024] develop task-specific MM-ICL templates during the IT stage, further extending its capabilities across more domains. Li et al. [2023a] introduce OtterHD, adapting MM-ICL for high-definition image tasks. Furthermore, Sun et al. [2023] and Tian et al. [2024] explore the potential of MM-ICL in the image generation tasks. Jin et al. [2024] provide compelling evidence for the effectiveness of MM-ICL in comprehending game instructions. Zong et al. [2024] and Shukor et al. [2024] develop fine-grained benchmarks and evaluate the MM-ICL in classification tasks.\nWhile significant progress has been witnessed in MM-ICL, the existing work still mainly focuses on how to optimize MM-ICL, ignoring the underlying factors that influence its effectiveness and performance. Such gap impedes a comprehensive understanding of the mechanisms and performance determinants of MM-ICL, thereby limiting further exploration and research in this field. Motivated by this, this paper aims to systematically investigate the research question: What factors affect the performance of MM-ICL?, hoping to offer a unified view and guideline for researchers to build better MM-ICL. Specifically, as illustrated in Figure 1, the MM-ICL process comprises three steps: demonstration retrieval, demonstration ordering, and prompt construction. Therefore, We systematically investigate the following sub-questions: (a) how to select multi-modal demonstrations (Sec. 3.1); (b) how to order multi-modal demonstrations (Sec. 3.2); and (c) how to construct MM-ICL prompts (Sec. 3.3) to this end. To achieve this, we conduct detailed experiments on MM-ICL using 20 strategies across 4 tasks with 6 representative vision large language models (VLLMs).\nThrough extensive investigations, the main findings are as follows:\n\u2022 Multi-modal alignment is the bottleneck for MM-ICL. Our analysis confirms that, on average, multi-modal retrieval methods outperform single-modal ones. Furthermore, multi-modal alignment in VLLMs has a greater impact on MM-ICL effectiveness than parameter size, identifying alignment as the key limitation in both backbone structure and demonstration quality.\n\u2022 Intra-demonstration ordering holds greater importance than inter-demonstration ordering. Our investigation first indicates that the intra-demonstration ordering, particularly the ordering of modalities, greatly influences model performance more than demonstration arrangement.\n\u2022 Introductory instruction guides better task understanding for MM-ICL. To construct a comprehensive MM-ICL prompt, it is essential to include introductory instructions preceding the demonstrations. This approach consistently enhances the performance of MM-ICL campared with summative instruction placed after demonstrations, and intra-demonstration instruction."}, {"title": "Background", "content": "In this work, we formally present the prompt building process for MM-ICL. As depicted in Figure 1, the process of prompt building for MM-ICL involves three sequential stages:\n(1) Demonstration Retrieval: The core MM-ICL requires retrieval to obtain demonstrations that can help MM-ICL. Formally, given a validation dataset $V_n = \\{x_1, x_2,..., x_n\\}$, each multi-modal sample $x_i$ includes textual input $I_{txt}^i$, visual input $I_{vis}^i$, and output $O_i$. For a specific test query q,\nthis step aims to identify a subset of relevant demonstrations $C_k = \\{x_{\\pi_j}\\}_{j=1}^k$, where $x_{\\pi_j} \\in V_n$.\n(2) Demonstration Ordering: Researches [Lu et al., 2022b, Wu et al., 2023, Xiang et al., 2024] show that LLMs are highly sensitive to the order of demonstrations. Thus, arranging these demonstrations effectively is crucial for MM-ICL. After retrieving relevant demonstrations, we must rearrange the sequence $L_k = [x_{\\sigma_j}]_{j=1}^k$, which will be used to construct the prompt.\n(3) Prompt Construction: Previous research indicates that using delimiters and instructions can significantly enhance textual ICL capabilities [Min et al., 2022, Qin et al., 2023]. Therefore, the final core step is to transform the ordered demonstrations into a structured prompt P, incorporating delimiters and instructions to optimize MM-ICL."}, {"title": "What Factors Affect Multi-modal In-Context Learning?", "content": "The efficacy of ICL heavily depends on the quality of the retrieved demonstrations C, which provide essential prior knowledge for MM-ICL. As illustrated in Figure 2, the retrieval process encompasses"}, {"title": "Exploration of MM-ICL Demonstration Retrieval", "content": "three key steps: (1) Sample Representation, (2) Sample Comparison, and (3) Sample Selection. In this section, we conduct a systematic analysis of how various strategies for sample representation, comparison, and selection affect MM-ICL task performance.\nSample Representation. It involves defining an encoder (Encoder(\u00b7)) to map each input sample $x_j \\in V$ and user query q into a shared representation space:\n$h_j = Encoder(x_j).$ \t(1)\nSpecifically, we evaluate various encoder architectures across modalities, focusing on the impact of visual encoder ($Encoder_{vis}$), text encoder ($Encoder_{txt}$), and multi-modal encoder ($Encoder_{multi}$)\non model performance.\nSample Comparison. After deriving the representations, we employ a metric M to evaluate the quality $Q_j$ of the sample $h_j$ in comparison to the query representation $h_q$ and the dataset samples $h_j$:\n$Q_j = M(h_q, h_j).$ \t(2)\nSpecifically, we explore various comparison metrics, including cosine similarity $M_{cos}$ [Liu et al., 2022a], L2 similarity $M_{L2}$ [Liu et al., 2022a], and semantic diversity $M_{div}$ [Li and Qiu, 2023a], to assess sample quality and understand the correlation with model performance.\nSample Selection. After quality assessments, we apply a selection criterion S to identify the k most advantageous samples $x_{\\pi_j}$ for inclusion in the demonstration set C:\n$C = \\{x_{\\pi_j}|x_{\\pi_j} \\in S(q, Q_j), j \\leq k\\}.$ \t(3)\nSample selection is guided by factors such as domain information [He et al., 2023], demonstration style [Agrawal et al., 2023], and token distance [Liu et al., 2022a]. Specifically, we systematically examine samples from both in-domain and out-of-domain collections. And we also assess the impact of image style on the selected demonstrations. Further, we investigate the token distance between modalities to understand its effects on sample selection for MM-ICL."}, {"title": "Exploration of MM-ICL Demonstration Ordering", "content": "Following Lu et al. [2022b] and Wu et al. [2023], the order of the demonstration set C significantly impacts MM-ICL performance. As shown in Figure 3, this section explores two key aspects:\nIntra-demonstration Ordering. The sequence within a demonstration, especially modalities (e.g., text and image), is an important component that might affect the MM-ICL capabilities. Therefore,"}, {"title": "Exploration of MM-ICL Prompt Construction", "content": "VLLMs are highly sensitive to input instructions [Kojima et al., 2022, Qin et al., 2023]. Inspired by this, to enhance task comprehension, we incorporate different instructions to explore the performance influence for MM-ICL. Formally, we construct instruction methods $I(\u00b7)$ that describe the task and position them within the prompt. The prompt construction process is:\n$P = I(\\delta(x_{\\sigma_1}), \\delta(x_{\\sigma_2}),..., \\delta(x_{\\sigma_k})),$ \t(6)\nSpecifically, as shown in Figure 4, we explore three instruction categories to bolster MM-ICL process:\n\u2022 Introductory Instruction ($I_{intro}$) refers to the initial guidance that offers an overview of the task prior to any demonstrations. As shown in Figure 4 (a), this instruction, denoted as $T_{intro}$, is positioned at the start of the ordered demonstration sequence, L."}, {"title": "Experimental Setup", "content": "Following the setting of Li et al. [2023c], we systematically explore 4 tasks, including image-caption, visual question answering (VQA), image classification, and chain-of-thought reasoning, which come from M\u00b3IT [Li et al., 2023c] and M\u00b3CoT [Chen et al., 2024b] (as shown in Tables 2), providing a universal paradigm can help researchers conduct unified and fairer comparisons and studies within a unified framework. In order to evaluate the MM-ICL performance accurately, we use two indicators for each task. Following Zhang et al. [2019], Li et al. [2023b], and Zong et al. [2024], we use CIDER [Vedantam et al., 2015] and BertScore [Zhang et al., 2019] as image-caption metrics. Since M\u00b3IT includes various VQA tasks with free-form answers, inspired by the success of free-form and precise answer hybrid evaluation in machine reading comprehension, following Rajpurkar et al. [2016], Zhang et al. [2019], we adapt Token-F1 [Rajpurkar et al., 2016] and BertScore as visual question answering (VQA) metrics (The correlation analysis of the indicators and accuracy as shown in Table 3). Following Li et al. [2023c,b], we use accuracy and F1 score as indicators of image classification. Following Lu et al. [2022a], Golovneva et al. [2022] and Qin et al. [2023], we use accuracy and reasoning alignment score [Golovneva et al., 2022] (RAS) as indicators of reasoning.\nTo ensure rigorous experimental control, we established a baseline using a multi-modal encoder for data representation and cosine similarity for sample comparison, limiting retrieval to within the same task. This baseline ranks samples based on similarity, with a delimiter and a 3-shot setting (see Appendix A for details). In addition, all open source models complete inference on 2 A100 80G. For all experiments, we select top-p from {0.95, 1} and adjust the temperature parameter within [0, 1]. Among them, temperature is the main error variable in this work."}, {"title": "Empirical Analysis of Factors Affecting MM-ICL", "content": "Multi-modal alignment is the bottleneck for MM-ICL in both backbones and demonstrations.\nTo evaluate the impact of semantic representation in different modalities for MM-ICL, we assessed three distinct encoders: ROBERTa [Liu et al., 2019] as a textual encoder for Textual Retriever,\nCLIP-Vision Encoder [Radford et al., 2021] for Visual Retriever, and BridgeTower [Xu et al., 2023] as multi-modal encoder for Multi-Modal Retriever. As illustrated in Table 1, multi-modal retrieval consistently outperforms zero-shot, randomly selected, and single-modality methods, highlighting the advantages of multi-modal semantic learning for MM-ICL. What's more, as shown in Table 1, our results reveal that increasing model parameters from 8 billion to over 100 billion does not significantly enhance performance, suggesting that beyond parameter size, multi-modal context understanding and alignment are more crucial for MM-ICL than model scaling. Our analysis demonstrates that multi-modal alignment is the critical factor in both the backbone and demonstrations.\nCurrent multi-modal encoders still lack modeling of multi-modal logic. Actually, multi-modal retrieval attains better performance in many scenarios like Image Caption and VQA. However, our experiments show that textual retrieval works well for classification and reasoning tasks. Based on the qualitative analysis, we observe that due to the semantic richness of the labels and rationales, textual retrieval can obtain more similar samples. However, the current multi-modal retrieval struggles with complex text semantics, often favoring image similarity. This aligns with recent work [Tong et al., 2023, 2024, Fei et al., 2024c], which is valuable for future exploration.\nMulti-modal context diminishes the necessity of careful demonstration selection. As shown in Table 1, adding relevant demonstrations slightly improves performance, but the gains are less"}, {"title": "Sample Representation", "content": "Multi-modal alignment is the bottleneck for MM-ICL in both backbones and demonstrations.\nTo evaluate the impact of semantic representation in different modalities for MM-ICL, we assessed three distinct encoders: ROBERTa [Liu et al., 2019] as a textual encoder for Textual Retriever,\nCLIP-Vision Encoder [Radford et al., 2021] for Visual Retriever, and BridgeTower [Xu et al., 2023] as multi-modal encoder for Multi-Modal Retriever. As illustrated in Table 1, multi-modal retrieval consistently outperforms zero-shot, randomly selected, and single-modality methods, highlighting the advantages of multi-modal semantic learning for MM-ICL. What's more, as shown in Table 1, our results reveal that increasing model parameters from 8 billion to over 100 billion does not significantly enhance performance, suggesting that beyond parameter size, multi-modal context understanding and alignment are more crucial for MM-ICL than model scaling. Our analysis demonstrates that multi-modal alignment is the critical factor in both the backbone and demonstrations.\nCurrent multi-modal encoders still lack modeling of multi-modal logic. Actually, multi-modal retrieval attains better performance in many scenarios like Image Caption and VQA. However, our experiments show that textual retrieval works well for classification and reasoning tasks. Based on the qualitative analysis, we observe that due to the semantic richness of the labels and rationales, textual retrieval can obtain more similar samples. However, the current multi-modal retrieval struggles with complex text semantics, often favoring image similarity. This aligns with recent work [Tong et al., 2023, 2024, Fei et al., 2024c], which is valuable for future exploration.\nMulti-modal context diminishes the necessity of careful demonstration selection. As shown in Table 1, adding relevant demonstrations slightly improves performance, but the gains are less"}, {"title": "Sample Comparison", "content": "To further analyze the influencing factors of MM-ICL in sample retrieval, this study employs similarity and diversity metrics, which help assess how MM-ICL processes sample similarities and differences, enhancing our understanding of its mechanisms. See Appendix B for more details and results."}, {"title": "Sample Selection", "content": "Domain interval matters for sample selection. Prior research highlights the critical role of domain relevance in enhancing ICL performance. Inspired by this, we employ the multi-modal retriever to select samples from both in-domain and out-of-domain pools. Figure 7 (a) shows a nearly 4% performance drop when out-of-domain demonstrations are included, underscoring the necessity of in-domain demonstrations for optimal MM-ICL.\nVisual style is not a crucial factor in sample selection. Although stylistic similarity in text samples is known to bolster ICL, its effect on the visual modality remains ambiguous. Utilizing CLIP for image classification, we investigate the impact of stylistic coherence in multi-modal samples on MM-ICL performance. As depicted in Figure 7 (b), significant enhancements are observed solely in the VQA task, while captioning and classification show minimal effects and reasoning tasks decline. This indicates that diverse visual styles are not crucial in general MM-ICL.\nToken distances between modalities need to be considered for different tasks to improve sample selection. For textual ICL, excessive token distance between samples can impede performance [Liu"}, {"title": "Exploration of MM-ICL Demonstration Ordering", "content": "Intra-demonstration ordering significantly impacts performance. Within the demonstration, organizing the ordering, especially the relationship between modalities is a crucial topic. We investigate this by arranging inputs and outputs across modalities using three methods: text input\u2192text output image input (Text-Image), text input\u2192image input\u2192text output (Text-Image-Text), and image input text input\u2192text output (Image-Text). As shown in Figure 8 (a), positioning the image at the start significantly enhances model performance. This suggests that presenting visual information first improves multi-modal comprehension, thereby boosting its learning abilities.\nInter-demonstration ordering demonstrates minimal impacts. Following Lu et al. [2022c], we investigate how the order of demonstration presentation influences model efficacy. We explore various strategies: random rearrangement, a \"similar-last\" approach where samples similar to the query are shown last, and a \"similar-first\" approach where similar samples are presented first. Figure 8 (b) illustrates that inter-demonstration ordering has a negligible impact on MM-ICL performance. This suggests the order-robustness, with the presentation sequence having minimal effect."}, {"title": "Empirical Analysis of MM-ICL Prompt Construction", "content": "Introductory Instruction is consistently effective for better MM-ICL. To investigate the impact of inserting task-related instructions within prompts, we conduct the following experiment on three categories of instruction: Introductory Instruction, Summative Instruction, and Intra-demonstration Instruction. As depicted in Figure 9, our analysis indicates that introductory instructions stably"}, {"title": "Related Work", "content": "Recent advancements in vision large language models (VLLMs) have achieved great success in various vision-language tasks [Yin et al., 2023, Wu et al., 2024a,b, Wang et al., 2024, Fei et al., 2024b]. Initially, VLLMs lack Multi-modal In-context Learning (MM-ICL) capabilities. To address this, researchers explore incorporating MM-ICL directly into the training phase. This involves constructing training samples with multi-modal interleaved data by manual and general templates, which unlock the MM-ICL capability [Alayrac et al., 2022, Awadalla et al., 2023]. Building on this, Li et al. [2023b], Doveh et al. [2024] and Zhao et al. [2024] extend the MM-ICL to construct a series of task-specific templates, which improves generalization for MM-ICL. Further, Li et al. [2023a] introduce OtterHD and adapt the former process for high-definition images. The potential of MM-ICL is further explored in scene text recognition, image generation, and game instructions [Zhao et al., 2023b, Sun et al., 2023, Jin et al., 2024]."}, {"title": "Discussion", "content": "Broader Impacts. Our work is the first to systematically explore the factors influencing MM-ICL. We aim to enhance the understanding of MM-ICL mechanisms and guide future developments in this field. Additionally, our findings could foster a more comprehensive comprehension of MM-ICL within the community. For social impact, this research may influence the creation of more effective multi-modal large language models and relevant applications.\nLimitations & Future Work. Due to time and cost constraints, this work is limited to the exploration of image and text modalities. In future research, we can extend our exploration to video modal ICL and multi-lingual MM-ICL scenarios. Another limitation of this work involves the insufficient consideration of certain image instructions, such as grounding or the inclusion of additional arrows. These aspects often require more complex human input and are not adequately supported by most current models."}, {"title": "Conclusion", "content": "This study is the first to systematically explore MM-ICL by identifying key performance determinants. Our experiments with 6 models and 20 factors across 4 tasks show that multi-modal retrieval significantly outperforms single-modal approaches and the intra-demonstration ordering critically influences learning efficacy. Additionally, incorporating task-specific instructions into prompts enhances model performance. We hope these findings will refine our understanding of MM-ICL mechanisms and guide more effective developments and future research in this evolving field."}, {"title": "The Implement Details for Standard Baseline", "content": "To ensure rigorous control of experimental variables, we establish a standard baseline for our study. This baseline utilizes a multi-modal encoder for data representation and cosine similarity for sample comparison, with retrieval restricted to the same task. The following sections provide detailed insights into the implementation of this baseline."}, {"title": "Demonstration Retrieval Implementation for Baseline", "content": "We employ BridgeTower [Xu et al., 2023] as a multi-modal encoder to represent the data in a unified embedding space. This encoder integrates both visual and textual information, prioritizing single modality to capture the rich semantic content present in images."}, {"title": "Sample Representation", "content": "It involves defining an encoder (Encoder(\u00b7)) to map each input sample $x_j \\in V$ and user query q into a shared representation space:\n$h_j = Encoder(x_j).$ \t(1)\nSpecifically, we evaluate various encoder architectures across modalities, focusing on the impact of visual encoder ($Encoder_{vis}$), text encoder ($Encoder_{txt}$), and multi-modal encoder ($Encoder_{multi}$)\non model performance."}, {"title": "Sample Comparison", "content": "After deriving the representations, we employ a metric M to evaluate the quality $Q_j$ of the sample $h_j$ in comparison to the query representation $h_q$ and the dataset samples $h_j$:\n$Q_j = M(h_q, h_j).$ \t(2)\nSpecifically, we explore various comparison metrics, including cosine similarity $M_{cos}$ [Liu et al., 2022a], L2 similarity $M_{L2}$ [Liu et al., 2022a], and semantic diversity $M_{div}$ [Li and Qiu, 2023a], to assess sample quality and understand the correlation with model performance."}, {"title": "Sample Selection", "content": "After quality assessments, we apply a selection criterion S to identify the k most advantageous samples $x_{\\pi_j}$ for inclusion in the demonstration set C:\n$C = \\{x_{\\pi_j}|x_{\\pi_j} \\in S(q, Q_j), j \\leq k\\}.$ \t(3)\nSample selection is guided by factors such as domain information [He et al., 2023], demonstration style [Agrawal et al., 2023], and token distance [Liu et al., 2022a]. Specifically, we systematically examine samples from both in-domain and out-of-domain collections. And we also assess the impact of image style on the selected demonstrations. Further, we investigate the token distance between modalities to understand its effects on sample selection for MM-ICL."}, {"title": "Demonstration Ordering Implementation for Baseline", "content": "By default, we utilize the methodology for ordering demonstrations within our baseline model. By default, we adopt a text-after-image (Text-Image) approach for intra-demonstration sorting. This means that, within a single demonstration, textual information is positioned after the corresponding image. This ordering is chosen based on preliminary findings suggesting that such a sequence aids in better contextual understanding and retention of the demonstrated information.\nFurthermore, for the ordering of inter-demonstration sequences, we employ a similarity-based method. This method ranks demonstrations according to their similarity to the query, with more similar demonstrations placed higher in the order. The similarity is determined using a metric that assesses the alignment of key features between the query and the demonstrations. This approach ensures that the most relevant and contextually aligned demonstrations are prioritized, potentially enhancing the model's performance and the user's comprehension."}, {"title": "Prompt Construction Implementation for Baseline", "content": "To ensure consistency and comparability in our baseline, we introduce both a delimiter and a 3-shot setting (following Wei et al. [2022b], Qin et al. [2023]). The delimiter serves to clearly demarcate different segments of the input data, preventing any potential confusion or overlap between distinct portions of the input. This clear separation is crucial for the model to accurately process and understand the structure of the data it receives.\nThe 3-shot setting, on the other hand, involves providing three examples for each task within the prompt. This approach is designed to stabilize the learning process by presenting the model with sufficient contextual information. By offering three examples, we strike a balance between providing enough context to guide the model's understanding and avoiding the cognitive overload that might occur with too many examples. This setting not only enhances the model's performance but also ensures a more robust and reliable learning process."}, {"title": "Baseline Prompt", "content": "In the context of using Vision-and-Language Large Models (VLLMs), it is essential to carefully structure the input prompts to ensure accurate processing. The prompt format typically used is illustrated below:\n[REQUEST] % Shot 1\n<Visual Input Tvis> <Textual Input Itxt >\n[RESPONSE]\n<Textual Output Ivis>\n[REQUEST] % Shot 2\n<Visual Input Tyis> <Textual Input Itxt >\n[RESPONSE]\n<Textual Output Iyis>\n[REQUEST] % Shot 3\n<Visual Input Lis> <Textual Input Ixt >\n[RESPONSE]\n<Textual Output Tis>\n[REQUEST] % User Query\n<Visual Input Tuis> <Textual Input Itxt>"}, {"title": "Metric Calculation", "content": "Compute the cosine similarity between ha and hj using the formula:\n$M_{cos}(h_q, h_j) = \\frac{h_qh_j}{||h_q||||h_j||}$ \t(7)\nCalculate the L2 similarity by computing the negative Euclidean distance between ha and hj:\n$M_{L2}(h_q, h_j) = - ||h_q - h_j||^2$ \t(8)\nSince Euclidean distance measures dissimilarity, we use the negative value to represent similarity, where a higher value indicates greater similarity."}, {"title": "Comparison and Analysis", "content": "Comparing the results obtained using different metrics ($M_{cos}$, $M_{L2}$, $M_{div}$) provides a comprehensive understanding of their effectiveness and suitability for specific applications. It is essential to analyze the trade-offs associated with each metric and interpret the results to draw meaningful conclusions about sample quality and relevance.\nAs shown in Figure 12, cosine similarity, which measures directional semantic alignment, emerges as the superior metric in MM-ICL compared to L2 similarity. This observation is supported by the findings of Deza et al. [2009] and Steck et al. [2024], who highlight that MM-ICL prioritizes semantic directional consistency over complete semantic alignment. Cosine similarity's ability to capture the nuances of directional alignment allows for more precise interpretations of semantic relationships within the data, making it particularly effective for MM-ICL tasks.\nIn contrast, Figure 13 illustrates that while diversity, as measured by $M_{div}$, enhances performance in text-based in-context learning, our experiments reveal limited improvement in MM-ICL tasks. This finding suggests that diversity may not directly correlate with better performance in MM-ICL. The limited impact of diversity on MM-ICL performance could be attributed to the specific nature of multi-modal data, where the interplay between different modalities requires a more nuanced approach than simply maximizing diversity.\nFurther analysis of these metrics reveals the inherent trade-offs between them. For instance, while cosine similarity offers advantages in maintaining semantic directional consistency, it may not capture the full extent of semantic similarity that L2 similarity can provide. On the other hand, L2 similarity, though comprehensive in measuring complete alignment, might lack the precision needed for tasks that rely heavily on directional semantic cues. Similarly, while diversity is beneficial in certain"}, {"title": "The Implement Details for Demonstration Sampling", "content": "To examine the effect of demonstration sample quantity on model performance, as shown in Figure 14, we select a subset of k' demonstrations from the demonstration list Lk to the prompt, where k' is the number of retrieved demonstrations. Formally, the prompt construction process is defined as:\n$P = I(\\delta(x_{\\sigma_1}), \\delta(x_{\\sigma_2}),..., \\delta(x_{\\sigma_{k'}}))$ \t(9)\nWe systematically evaluate the influence of varying k' on MM-ICL performance."}, {"title": "The Implement Details for Delimiter Injection", "content": "To distinctly separate inputs and outputs within demonstrations $x_i$, as shown in Figure 15, we leverage special delimiter markers. Delimiters like [Request] and [Response] are strategically placed before the inputs and outputs, respectively. Formally, delimiter injection function & maps inputs and outputs to the prompting sequences:\n$\\delta(x_i) = \\text{[Request]} \\oplus I_i \\oplus \\text{[Response]} \\oplus O_i,$\t(10)\nwhere $I_i$ and $O_i$ denotes the input and output for the sample $x_i$, respectively. In addition, $\\oplus$ represents string concatenation operation."}, {"title": "The Implement Details for Instruction Injection", "content": "Visual Language Models (VLLMs) are known to be highly sensitive to input instructions, as demonstrated by Kojima et al. [2022] and Qin et al. [2023]. Inspired by this observation, we aim to enhance task comprehension in Multi-Modal In-Context Learning (MM-ICL) by incorporating various instruc-tions to explore their influence on performance. Formally, we develop instruction methods, denoted as I(\u00b7), which describe the task and are integrated into the prompt construction process. The prompt P is constructed as follows:\n$P = I(\\delta(x_{\\sigma_1}), \\delta(x_{\\sigma_2}),..., \\delta(x_{\\sigma_n})),$ \t(11)\nwhere $\\delta(x_{\\sigma_i})$ represents the transformation of the i-th demonstration example."}, {"title": "Chain-of thought reasoning tasks require", "content": "Chain-of-thought reasoning tasks require a more complex interaction between visual and textual information. The prompt encourages users to break down their reasoning process into clear, logical steps, each supported by specific details from the image or text.\nFurthermore, we explore three categories of instructions to enhance the MM-ICL process:\nIntroductory Instruction ($I_{intro}$) This instruction provides an overview of the task before presenting any demonstrations. As depicted in Figure 4 (a), the introductory instruction $T_{intro}$ is positioned at the beginning of the ordered demonstration list L. This setup aims to set the context for the subsequent examples. Specifically, the overall prompt template is as follows:\n<Instruction $T_{intro}$>\n[DEMONSTRATIONS]\n[REQUEST] % Shot 1\n<Visual Input Tvis> <Textual Input Itxt >\n[RESPONSE]\n<Textual Output Ivis >\n[QUERY]\n[REQUEST] % User Query\n<Visual Input Tuis> <Textual Input Itxt>\nSummative Instruction ($I_{sum}$) This instruction offers a summary after the examples, guiding the model to apply the learned concepts to real-world problems. As shown in Figure 4 (b), the summative instruction I is added at the end of the demonstration list L. This helps in reinforcing the learning objectives and expected outcomes. Specifically, the overall prompt template is as follows:\n<Instruction Lintro>\n[DEMONSTRATIONS]\n[REQUEST] % Shot 1\n<Visual Input Tris> <Textual Input Itxt>\n[RESPONSE]\n<Textual Output Ivis >\nIn summary, <Instruction Isum>\n[QUERY]\n[REQUEST] % User Query\n<Visual Input Tuis> <Textual Input Itxt>\nIntra-demonstration Instruction ($I_{intra}$) This instruction embeds task instructions within each example, assisting the model in understanding the task requirements during the learning process. As illustrated in Figure 4 (c), the intra-demonstration instruction I is included within each demonstration Xi in the list L. This method ensures that the task instructions are continuously reinforced throughout the learning process. Specifically, the overall prompt template is as follows:"}, {"title": "Prompt Robust", "content": "In our preliminary experiments, we observed that variations in prompts do not significantly alter the overall conclusions. Specifically, we employed multiple prompts-differing in instructions and delimiters-while maintaining equivalent semantic content but varying linguistic expression. As demonstrated in Table 4, the influence of these different prompts on the results is minimal. This suggests that our findings are robust to changes in prompt formulation, thereby supporting the reliability of the experimental outcomes."}]}