{"title": "Anchored Preference Optimization and Contrastive Revisions:\nAddressing Underspecification in Alignment", "authors": ["Karel D'Oosterlinck", "Winnie Xu", "Chris Develder", "Thomas Demeester", "Amanpreet Singh", "Christopher Potts", "Douwe Kiela", "Shikib Mehri"], "abstract": "Large Language Models (LLMs) are often\naligned using contrastive alignment objec-\ntives and preference pair datasets. The in-\nteraction between model, paired data, and\nobjective makes alignment a complicated\nprocedure, sometimes producing subpar re-\nsults. We study this and find that (i) prefer-\nence data gives a better learning signal when\nthe underlying responses are contrastive,\nand (ii) alignment objectives lead to better\nperformance when they specify more con-\ntrol over the model during training. Based\non these insights, we introduce Contrastive\nLearning from AI Revisions (CLAIR), a\ndata-creation method which leads to more\ncontrastive preference pairs, and Anchored\nPreference Optimization (APO), a control-\nlable and more stable alignment objective.\nWe align Llama-3-8B-Instruct using var-\nious comparable datasets and alignment ob-\njectives and measure MixEval-Hard scores,\nwhich correlate highly with human judg-\nments. The CLAIR preferences lead to the\nstrongest performance out of all datasets,\nand APO consistently outperforms less con-\ntrollable objectives. Our best model, trained\non 32K CLAIR preferences with APO, im-\nproves Llama-3-8B-Instruct by 7.65%,\nclosing the gap with GPT-4-turbo by 45%.", "sections": [{"title": "Introduction", "content": "Aligning language models with preferences is a\ncritical component in LLM development, signif-\nicantly enhancing model capabilities, safety, and\nadherence to human values (Christiano et al.,\n2017; Ouyang et al., 2022; Bai et al., 2022). These\npreferences can be expressed through preference\npairs (output y < yw for input x), which offer\na richer signal than individual outputs and enable\nmore expressive learning objectives. Recently,\ncontrastive learning objectives have made align-\nment more accessible (Rafailov et al., 2024b).\nDespite these advantages, alignment outcomes\ncan be suboptimal (Eisenstein et al., 2023; Feng\net al., 2024; Park et al., 2024). In this paper, we\nreason through the nature of alignment, focusing\non (i) the preference signal expressed by the data,\nand (ii) the training dynamics of contrastive objec-\ntives. We find that across both these axes, conven-\ntional alignment methods are underspecified. To\nsolve this, we argue that (i) preference data should\nbe minimally contrastive, and (ii) alignment ob-\njectives should account for distinct alignment sit-\nuations (see Figure 1). This sheds light on sub-\noptimal alignment outcomes. For example, we\nshow in Section 5 how a model aligned using high-\nquality outputs can actually degrade if the pairs\ndiffer in multiple uncontrolled aspects.\nThese insights lead to two new contributions.\nFirst, we introduce Contrastive Learning from AI\nRevisions (CLAIR), a method for creating prefer-"}, {"title": "Underspecification in Alignment", "content": "The alignment procedure creates complex inter-\nactions between the target model, the preference\ndataset, and the alignment objective. The present\nsection reflects on failure cases of all alignment\nefforts which start from preferences. The section\ndiscussed data and objective respectively.\nGiven a collection of prompts X, a preference\ndataset is a set of triples (x, yw, Y\u0131), where yw and\ny\u0131 are, respectively, a winning (more preferred)\nand losing (less preferred) response to prompt x.\nThe preference signal in such a dataset is essen-\ntially expressed by the difference between win-\nning and losing outputs, illustrated in Figure 1 A.\nHowever, paired outputs can differ in many as-\npects, some of which are spurious and thus irrele-\nvant to the preference. These spurious differences\nwill generally create a challenging credit assign-\nment problem. Outputs which are minimally con-\ntrastive differ along fewer axes, resulting in less\nspurious differences. Thus, the more contrastive\npreference pairs are, the more clear the learn-\ning signal becomes. Existing preference datasets\nvary meaningfully in their contrastiveness. For ex-\nample, in the Stanford Human Preferences dataset\n(Ethayarajh et al., 2022), two outputs in a pair are\nsimply responses to the same Reddit post, and thus\nthey are not guaranteed to be especially compara-\nble. An ideal preference dataset would consist of\na very controlled difference between either exam-\nple. This insight leads us to CLAIR (Section 3).\nPreference triples only specify that one output is\nbetter than another. This creates ambiguity, since\nit is not known if the more preferred answer was\nactually good. To see how this can impact align-\nment, suppose we have a dataset of triples where\nYw tends to score 8/10 on some quality scale and y\u0131\ntends to score 6/10. A target model that generally\nscores 9/10 may become worse if the likelihood of\nYw would increase during training, as illustrated in\nFigure 1 B. Therefore, alignment training needs\nto be aware of how desirable any individual an-\nswer is, regardless of its preference relation-\nship. To take a salient example, \u224880% of win-\nning outputs in UltraFeedback (Cui et al., 2024)\nare generated by a less performant model than\nLlama-3-8B-Instruct (as measured by Chatbot\nArena Elo; Chiang et al. 2024). Naively aligning\nLlama-3-8B-Instruct on this dataset may thus\nworsen performance. Examples like this one lead\nus to Anchored Preference Optimization (APO;\nSection 4).\nIn summary, current alignment approaches are\nunderspecified along two key axes: (i) preferences\nmay be weakly expressed due to non-contrastive\ndata, and (ii) alignment objectives need to account"}, {"title": "Contrastive Learning from Revisions", "content": "We now introduce Contrastive Learning from AI\nRevisions (CLAIR), a general procedure for cre-\nating minimally contrasting preference pairs.\nLet M be the target model we will align. Given\na prompt x, we sample the losing output y\u0131 directly\nfrom the model. Then, we use a Reviser to mini-\nmally revise and improve y\u0131, resulting in the win-\nning output Yw:\n$YI = M(x)$ \n$Yw = Reviser(x, y\u0131)$.\nIn this work, we use a stronger LLM to per-\nform revisions, prompted to enhance the clar-\nity, correctness, and engagement of the output\n(prompts and dataset details given in Appendix A).\nFigure 2 shows an example triple created using\nthis method. The losing output was generated\nby Llama-3-8B-Instruct and revised by GPT4-\nturbo. The revision keeps most of the initial output\nintact, while improving details. Recently, Dubey\net al. (2024) used human revisions in the develop-\nment of the 11ama-3.1 model family, though their\nprocess seems oriented towards enhancing quality\ndifferences rather than creating minimal contrasts.\nCLAIR differs markedly from more familiar ap-\nproaches to collecting preference data. For exam-\nple, in the on-policy judge paradigm (as used in\nReinforcement Learning from AI Feedback; Bai\net al. 2022), two generations are sampled from\nM(x), and a Judge (often another LLM) decides\nwhich is the winner and which the loser:\n$Y1, Y2 = M(x), M(x)$ \n$Yw, Y\u0131 = Judge(x, Y1, Y2)$.\nWe use this approach as one of our baselines,\nwith a prompt comparable to the revision prompt\nused by CLAIR. Additionally, we consider an off-\npolicy judge versions of (2) where the outputs are\ngenerated by models other than the target model:\n$Y1, Y2 = M'(x), M'(x)$ \n$Yw, Y\u0131 = Judge(x, Y1, Y2)$.\nBoth the on-policy and off-policy judge ap-\nproaches provide useful comparison points for\nCLAIR. In addition, we evaluate a baseline that\nhelps us understand the role of contrastiveness in\nparticular. For CLAIR, the Reviser is generally\na stronger model than the model we are aligning.\nThis means that the winning examples yw are al-\nways generated by a stronger model. To decouple\nthis factor from the contrastiveness induced by the\nrevision process, we also evaluate a baseline that\nwe call Stronger Preferred, where the stronger\nmodel provides the winning example for each pair\nwithout revision:\n$Yl = M(x)$ \n$Yw = Stronger(x)$"}, {"title": "Anchored Preference Optimization", "content": "A preference triple (x, yw, Y\u0131) expresses the be-\nlief that yw is a more preferred output than y\u0131\nfor prompt x. Alignment objectives use this re-\nlationship to align a model. Different objectives\nachieve this in very different ways, with deep con-\nsequences for the alignment process.\nDirect Preference Optimization (DPO; Rafailov\net al. 2024b) is a widely used and empirically suc-\ncessful alignment objective. The core stipulation\nof DPO is that the likelihood change of winning\noutputs during training needs to be greater than the\nlikelihood change of losing outputs. This likeli-\nhood change for a prompt and output is denoted as\nthe reward ro(x, y), which captures the log-ratio\nof likelihoods between the model during training\n\u03c0\u03c1(x | y) and the model before training, also\ncalled reference, #ref(x | y):\n$ro(x, y) = Blog \\frac{\u03c0\u03c1(y | x)}{Tref(y | x)}$\nHere, \u1e9e is a hyperparameter which scales this log-\nratio. This leads to the following DPO objective:\n$LDPO(X, Y\u03c9, \u03a5\u03b9; \u03b8) = \u2013 logo (ro(x, yw) \u2013 ro (x, y))$ \nThe DPO authors report that the gradient of this\nobjective intuitively leads to an increased winning\nlikelihood and decreased losing likelihood. How-\never, this is only one possibility out of three dis-\ntinct scenarios. Alternatively, DPO can increase\nthe winning likelihood more than it increases the\nlosing likelihood, or decrease the winning likeli-\nhood less than it decreases the losing likelihood\n(Feng et al., 2024). These scenarios may end up\nproducing vastly different models. As discussed\nin Section 2, a winning output is not necessarily\nbetter than what the model produces before align-\nment. In this case, DPO may hurt performance if\nit increases the likelihood of undesirable outputs.\nTo help researchers navigate these interactions,\nwe introduce Anchored Preference Optimization\n(APO). In essence, APO is a family of alignment\nobjectives which offer fine-grained control over\neach of the rewards, thus controlling the absolute\nincrease or decrease in likelihood during training.\nIn this paper, we focus in particular on variants\nthat we call APO-zero and APO-down:\n$LAPO (x, Y\u03c9, \u03b9; \u03b8) = -\u03c3(ro(X,Yw)) + \u03c3(re(x,y1))$ \n$Lapon (x, \u03c8\u03c9, \u03b3\u03b9; \u03b8) = \u03c3(ro(X,Yw)) -\u03c3 (ro(X,Yw) \u2013 ro(x, y))$"}, {"title": "Alignment Experiments", "content": "To study the effectiveness of CLAIR and APO, we\nalign Llama-3-8B-Instruct across the four com-\nparable preference datasets described in Section 3,\ncreated from 32K UltraFeedback prompts. We\nuse GPT4-turbo to act as Judge or Reviser when\ncreating these datasets. For every dataset, we align\nthe model using the four different objectives de-\nscribed in Section 4. Additionally, we consider\nSupervised Fine-Tuning (SFT) on only the win-\nning outputs as a baseline alignment objective."}, {"title": "Evaluation Methodology", "content": "Human judgments are ultimately the best indicator\nof how well a model is aligned with human pref-\nerences. Chatbot Arena (Chiang et al., 2024) uses\nthousands of pairwise human judgements to pro-\nduce a ranking of model performance. However,\ncollecting these judgments can be prohibitively\nexpensive. To overcome this obstacle, we measure\nmodel performance through a benchmark which\ncorrelates highly with this Chatbot Arena ranking.\nAt the time of writing, MixEval-Hard (Ni\net al., 2024) is the benchmark with the high-\nest Chatbot Arena correlation (0.98 rank correla-\ntion). MixEval-Hard features hard queries with\nknown answers across a wide range of domains\nand uses a GPT3.5-turbo (Brown et al., 2020;\nOuyang et al., 2022) model to evaluate if pre-\ndicted answers correspond with this ground-truth.\nThis makes MixEval-Hard more grounded in hu-\nman knowledge and significantly cheaper to run\ncompared to other popular evaluation frameworks\nsuch as AlpacaEval (Li et al., 2023; Dubois\net al., 2024). Under the hood, MixEval-Hard\nutilizes queries sampled from MATH (Hendrycks\net al., 2021), BBH (Suzgun et al., 2023), DROP (Dua\net al., 2019), GSM8k (Cobbe et al., 2021), AGIEval\n(Zhong et al., 2024), TriviaQA (Joshi et al., 2017),\nMBPP (Austin et al., 2021), MMLU, (Hendrycks et al.,"}, {"title": "Training Specifications", "content": "Llama-3-8B-Instruct is trained for a total of\n18 epochs on each preference dataset and align-\nment objective, with a checkpoint saved every sin-\ngle epoch. The \u1e9e hyperparameter, common to\nall alignment objectives except SFT, is set to 0.1."}, {"title": "Results", "content": "We report the maximal and mean MixEval-Hard\nimprovement over all checkpoints from the same\ntraining run. This helps us understand both the\nbest-case and average impact of alignment across\nthe entire training procedure. This analysis is sum-\nmarized in Table 2 for every dataset and objective;\nwe now discuss these results in more detail."}, {"title": "Preference Data", "content": "To assess the quality of a particular dataset, we\nconsider the performance of that dataset when\npaired with its best objective. Using the APO-\nzero objective, the contrastive CLAIR dataset\nleads to the greatest improvement. Specifically,\nCLAIR leads to the greatest maximal improve-\nment of +7.65% and the greatest average improve-\nment of +2.93% out of all our experiments. This\nimprovement of +7.65% closes the relative gap\nwith GPT4-turbo by 45% using only 32K pairs.\nWe noted in Section 1 that uncontrolled con-\ntrastiveness can degrade model performance. We\nsee this dramatically in the results for the Stronger\nPreferred dataset, which heavily degrades model\nperformance. Like CLAIR, this dataset has all\nwinning outputs produced by a stronger model.\nUnlike CLAIR, though, its examples provide no\nguarantee of relevant minimal contrasts. Thus, the\ncontrastiveness induced by the CLAIR revision\nprocess is a major driver of performance.\nBoth on-policy judge and off-policy judge\ndatasets lead to improved performance when\npaired with their best alignment objective, but on-\npolicy preferences lead to better performance\ncompared to off-policy preferences. This is in-\ntuitive; judgments about the target model's outputs\nare in general more relevant.\nA breakdown of performance in function of\nMixEval-Hard's constituent benchmarks is given\nin Appendix B for every dataset."}, {"title": "Alignment Objectives", "content": "Anchored Preference Optimization (APO) con-\nsistently leads to the greatest performance in-\ncrease for every preference dataset, with the ex-\nception of the Stronger Preferred dataset, where all\ncontrastive objectives deteriorate the model. The\nrelation between the preference dataset and the tar-\nget model controls which variant of APO is best\nfor any dataset, as predicted in Section 2. APO-\ndown results in the best performance when win-\nning outputs are generally worse than the tar-\nget model, as is the case for the off-policy judge\ndataset. APO-zero is the best objective when\nwinning outputs are generally better than the\ntarget model, as is the case for CLAIR and on-\npolicy judge datasets. The difference between\nalignment objectives is less salient for the on-\npolicy judge dataset as compared to CLAIR, since\nwinning on-policy judge outputs are only slightly\nbetter than Llama-3-8B-Instruct on average.\nWinning CLAIR outputs may be vastly better than\nLlama-3-8B-Instruct since they are produced\nby a stronger model, making the different in align-\nment objectives more noticeable."}, {"title": "Analysis", "content": "To more deeply understand how the target model\nis changed during training, we can study the trajec-\ntories of winning/losing likelihoods and rewards\non held-out preferences. Figure 4 plots these tra-\njectories for the APO-down, APO-zero, and DPO\nexperiments on each preference dataset, using 100\nheld-out preference pairs from that dataset."}, {"title": "Preference Data", "content": "First, we observe that the likelihoods help char-\nacterize the type of preference dataset. In the\non-policy judge dataset, all answers are sampled\nfrom the target model and thus have a high likeli-\nhood. The off-policy variant has no answers com-\ning from the target model, and hence all likeli-\nhoods are low. Both CLAIR and Stronger Pre-\nferred have losing outputs with high likelihood and\nwinning outputs with low likelihood.\nAny initial discrepancy between log-likelihoods\nis normalized by the reward, which tracks changes\nin likelihood and thus starts at exactly 0. The\nmargin between winning and losing reward indi-\ncates how much more the winning likelihood in-\ncreased during training. Positive reward margins\ncan still produce negative log-likelihood margins,\nif any initial disparity between winning/losing\nlog-likelihood is not overcome. This ends up be-\ning the case for our CLAIR dataset.\nThe training dynamics for CLAIR and Stronger"}, {"title": "Alignment Objectives", "content": "All three alignment objectives display systematic\nbehavior across each dataset. APO-zero consis-\ntently leads to the greatest winning and losing re-\nwards. APO-down consistently produces the low-\nest rewards. Both of these behaviors are as in-\ntended. DPO has a slightly more complicated\ndynamic, which is nonetheless consistent across\ndatasets. In the initial steps of training, DPO\ntracks the behavior of APO-zero (high rewards)\nbefore following APO-down (low rewards) dur-\ning the remainder of training. This explains why\ndownstream DPO performance correlates most\nwith APO-down. However, DPO is never the best\nmethod on any dataset, because it falls between the\ndistinct modes of APO-zero and APO-down.\nTraining models with contrastive alignment ob-\njectives is considerably more complex that con-\nventional supervised fine-tuning. The result is de-\npendent on the semantics of the alignment objec-\ntive, the contrastive signal in the training data, and\nthe relationship between data quality and target\nmodel. Our results show that paying attention to\nthe interplay between these attributes is essential."}, {"title": "Related Work", "content": "We now characterize relevant alignment efforts\nand outline how they relate to Contrastive Learn-\ning from AI Revisions (CLAIR) and Anchored\nPreference Optimization (APO).\nReinforcement Learning from Human or AI\nFeedback (RLHF/RLAIF; Ouyang et al. 2022;\nBai et al. 2022; Yuan et al. 2024) is a technique\nused to align models with human preferences.\nFundamentally, these approaches first train a re-\nward model using preference judgments and sub-\nsequently optimize a Language Model for this re-\nward using Reinforcement Learning (Schulman\net al., 2017). To side-step the need for an ex-\nplicit reward model, Direct Preference Optimiza-\ntion (DPO; Rafailov et al. 2024b) aligns an LM\ndirectly using a contrastive training objective.\nWe articulated two core insights concerning\n(i) the role of contrastive preference data, and\n(ii) the need to anchor alignment depending on\nmodel and data. These insights translate to any\neffort which uses comparative preferences."}, {"title": "Conclusion", "content": "Alignment performance is significantly impacted\nby (i) the contrastiveness of the preference pairs\nand (ii) the relationship between model and\ndata. We introduce Contrastive Learning from\nAI Revisions (CLAIR), a data-creation method\nwhich produces more contrastive preference pairs,\nand Anchored Preference Optimization (APO),\na family of alignment objectives with tailored\ntraining dynamics. Our experiments aligning\nLlama-3-8B-Instruct show that CLAIR prefer-\nences lead to the highest performance improve-\nment, and APO methods consistently outperform\nconventional alignment objectives."}, {"title": "Prompts", "content": "The prompts we use for the Reviser and Judge function of Equation 1 and 2 are given in Table 3. Both\nprompts contain instructions to prefer more clear, more correct, and more engaging outputs. The Reviser\nprompt creates a preference pair by minimally revising and improving an output according to these\npreferences. Instead, the Judge prompt selects a more preferred output given two candidate answers."}, {"title": "Preference Pair Filtering", "content": "We reject revisions or judgments if the LLM failed to follow formatting guidelines specified in the\nrevising or judging prompt. Additionally, we reject revisions if they altered the length of the original\noutput too much; we found this mainly happens when the LLM misunderstands the revision prompt.\nStarting from the same 32K instructions sampled from UltraFeedback, this procedure creates 29K\nCLAIR pairs, 29K Stronger Preferred pairs, 29K off-policy Judge pairs, and 32k on-policy Judge pairs.\nWe adapted the code by Williams (2023) to efficiently query closed-source LLMs in parallel over API."}]}