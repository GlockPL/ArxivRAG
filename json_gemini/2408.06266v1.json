{"title": "Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment", "authors": ["Karel D'Oosterlinck", "Winnie Xu", "Chris Develder", "Thomas Demeester", "Amanpreet Singh", "Christopher Potts", "Douwe Kiela", "Shikib Mehri"], "abstract": "Large Language Models (LLMs) are often aligned using contrastive alignment objectives and preference pair datasets. The interaction between model, paired data, and objective makes alignment a complicated procedure, sometimes producing subpar results. We study this and find that (i) preference data gives a better learning signal when the underlying responses are contrastive, and (ii) alignment objectives lead to better performance when they specify more control over the model during training. Based on these insights, we introduce Contrastive Learning from AI Revisions (CLAIR), a data-creation method which leads to more contrastive preference pairs, and Anchored Preference Optimization (APO), a controllable and more stable alignment objective. We align Llama-3-8B-Instruct using various comparable datasets and alignment objectives and measure MixEval-Hard scores, which correlate highly with human judgments. The CLAIR preferences lead to the strongest performance out of all datasets, and APO consistently outperforms less controllable objectives. Our best model, trained on 32K CLAIR preferences with APO, improves Llama-3-8B-Instruct by 7.65%, closing the gap with GPT-4-turbo by 45%.", "sections": [{"title": "Introduction", "content": "Aligning language models with preferences is a critical component in LLM development, significantly enhancing model capabilities, safety, and adherence to human values (Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022). These preferences can be expressed through preference pairs (output $y_l \\prec y_w$ for input x), which offer a richer signal than individual outputs and enable more expressive learning objectives. Recently, contrastive learning objectives have made alignment more accessible (Rafailov et al., 2024b).\nDespite these advantages, alignment outcomes can be suboptimal (Eisenstein et al., 2023; Feng et al., 2024; Park et al., 2024). In this paper, we reason through the nature of alignment, focusing on (i) the preference signal expressed by the data, and (ii) the training dynamics of contrastive objectives. We find that across both these axes, conventional alignment methods are underspecified. To solve this, we argue that (i) preference data should be minimally contrastive, and (ii) alignment objectives should account for distinct alignment situations (see Figure 1). This sheds light on suboptimal alignment outcomes. For example, we show in Section 5 how a model aligned using high-quality outputs can actually degrade if the pairs differ in multiple uncontrolled aspects.\nThese insights lead to two new contributions. First, we introduce Contrastive Learning from AI Revisions (CLAIR), a method for creating prefer-"}, {"title": "Underspecification in Alignment", "content": "The alignment procedure creates complex interactions between the target model, the preference dataset, and the alignment objective. The present section reflects on failure cases of all alignment efforts which start from preferences. The section discussed data and objective respectively.\nGiven a collection of prompts X, a preference dataset is a set of triples $(x, y_w, y_l)$, where $y_w$ and $y_l$ are, respectively, a winning (more preferred) and losing (less preferred) response to prompt x. The preference signal in such a dataset is essentially expressed by the difference between winning and losing outputs, illustrated in Figure 1 A. However, paired outputs can differ in many aspects, some of which are spurious and thus irrelevant to the preference. These spurious differences will generally create a challenging credit assignment problem. Outputs which are minimally contrastive differ along fewer axes, resulting in less spurious differences. Thus, the more contrastive preference pairs are, the more clear the learning signal becomes. Existing preference datasets vary meaningfully in their contrastiveness. For example, in the Stanford Human Preferences dataset (Ethayarajh et al., 2022), two outputs in a pair are simply responses to the same Reddit post, and thus they are not guaranteed to be especially comparable. An ideal preference dataset would consist of a very controlled difference between either example. This insight leads us to CLAIR (Section 3).\nPreference triples only specify that one output is better than another. This creates ambiguity, since it is not known if the more preferred answer was actually good. To see how this can impact alignment, suppose we have a dataset of triples where $y_w$ tends to score 8/10 on some quality scale and $y_l$ tends to score 6/10. A target model that generally scores 9/10 may become worse if the likelihood of $y_w$ would increase during training, as illustrated in Figure 1 B. Therefore, alignment training needs to be aware of how desirable any individual answer is, regardless of its preference relationship. To take a salient example, \u224880% of winning outputs in UltraFeedback (Cui et al., 2024) are generated by a less performant model than Llama-3-8B-Instruct (as measured by Chatbot Arena Elo; Chiang et al. 2024). Naively aligning Llama-3-8B-Instruct on this dataset may thus worsen performance. Examples like this one lead us to Anchored Preference Optimization (APO; Section 4).\nIn summary, current alignment approaches are underspecified along two key axes: (i) preferences may be weakly expressed due to non-contrastive data, and (ii) alignment objectives need to account"}, {"title": "Contrastive Learning from Revisions", "content": "We now introduce Contrastive Learning from AI Revisions (CLAIR), a general procedure for creating minimally contrasting preference pairs.\nLet M be the target model we will align. Given a prompt x, we sample the losing output $y_l$ directly from the model. Then, we use a Reviser to minimally revise and improve $y_l$, resulting in the winning output $y_w$:\n$y_l = M(x)$\n$y_w = Reviser(x, y_l)$.\nIn this work, we use a stronger LLM to perform revisions, prompted to enhance the clarity, correctness, and engagement of the output (prompts and dataset details given in Appendix A). Figure 2 shows an example triple created using this method. The losing output was generated by Llama-3-8B-Instruct and revised by GPT4-turbo. The revision keeps most of the initial output intact, while improving details. Recently, Dubey et al. (2024) used human revisions in the development of the 11ama-3.1 model family, though their process seems oriented towards enhancing quality differences rather than creating minimal contrasts.\nCLAIR differs markedly from more familiar approaches to collecting preference data. For example, in the on-policy judge paradigm (as used in Reinforcement Learning from AI Feedback; Bai et al. 2022), two generations are sampled from M(x), and a Judge (often another LLM) decides which is the winner and which the loser:\n$y_1, y_2 = M(x), M(x)$\n$y_w, y_l = Judge(x, y_1, y_2)$.\nWe use this approach as one of our baselines, with a prompt comparable to the revision prompt used by CLAIR. Additionally, we consider an off-policy judge versions of (2) where the outputs are generated by models other than the target model:\n$y_1, y_2 = M'(x), M'(x)$\n$y_w, y_l = Judge(x, y_1, y_2)$.\nBoth the on-policy and off-policy judge approaches provide useful comparison points for CLAIR. In addition, we evaluate a baseline that helps us understand the role of contrastiveness in particular. For CLAIR, the Reviser is generally a stronger model than the model we are aligning. This means that the winning examples $y_w$ are always generated by a stronger model. To decouple this factor from the contrastiveness induced by the revision process, we also evaluate a baseline that we call Stronger Preferred, where the stronger model provides the winning example for each pair without revision:\n$y_l = M(x)$\n$y_w = Stronger(x)$"}, {"title": "Anchored Preference Optimization", "content": "A preference triple $(x, y_w, y_l)$ expresses the belief that $y_w$ is a more preferred output than $y_l$ for prompt x. Alignment objectives use this relationship to align a model. Different objectives achieve this in very different ways, with deep consequences for the alignment process.\nDirect Preference Optimization (DPO; Rafailov et al. 2024b) is a widely used and empirically successful alignment objective. The core stipulation of DPO is that the likelihood change of winning outputs during training needs to be greater than the likelihood change of losing outputs. This likelihood change for a prompt and output is denoted as the reward $r_\\theta(x, y)$, which captures the log-ratio of likelihoods between the model during training $\\pi_\\theta(x | y)$ and the model before training, also called reference, $\\pi_{ref}(x | y)$:\n$r_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y | x)}{\\pi_{ref}(y | x)}$\nHere, $\u03b2$ is a hyperparameter which scales this log-ratio. This leads to the following DPO objective:\n$L_{DPO}(x, y_\\omega, y_\\iota; \\theta) = - \\log \\sigma (r_\\theta(x, y_\\omega) - r_\\theta (x, y_\\iota))$\nThe DPO authors report that the gradient of this objective intuitively leads to an increased winning likelihood and decreased losing likelihood. However, this is only one possibility out of three distinct scenarios. Alternatively, DPO can increase the winning likelihood more than it increases the losing likelihood, or decrease the winning likelihood less than it decreases the losing likelihood (Feng et al., 2024). These scenarios may end up producing vastly different models. As discussed in Section 2, a winning output is not necessarily better than what the model produces before alignment. In this case, DPO may hurt performance if it increases the likelihood of undesirable outputs.\nTo help researchers navigate these interactions, we introduce Anchored Preference Optimization (APO). In essence, APO is a family of alignment objectives which offer fine-grained control over each of the rewards, thus controlling the absolute increase or decrease in likelihood during training. In this paper, we focus in particular on variants that we call APO-zero and APO-down:\n$L_{APO}^{zero}(x, y_\\omega, y_\\iota; \\theta) = - \\sigma(r_\\theta(x,y_\\omega)) + \\sigma(r_\\theta(x,y_\\iota))$\n$L_{APO}^{down} (x, y_\\omega, y_\\iota; \\theta) = \\sigma(r_\\theta(x,y_\\omega)) - \\sigma (r_\\theta(x,y_\\omega) \u2013 r_\\theta(x, y_\\iota))$"}, {"title": "Alignment Experiments", "content": "To study the effectiveness of CLAIR and APO, we align Llama-3-8B-Instruct across the four comparable preference datasets described in Section 3, created from 32K UltraFeedback prompts. We use GPT4-turbo to act as Judge or Reviser when creating these datasets. For every dataset, we align the model using the four different objectives described in Section 4. Additionally, we consider Supervised Fine-Tuning (SFT) on only the winning outputs as a baseline alignment objective."}, {"title": "Evaluation Methodology", "content": "Human judgments are ultimately the best indicator of how well a model is aligned with human preferences. Chatbot Arena (Chiang et al., 2024) uses thousands of pairwise human judgements to produce a ranking of model performance. However, collecting these judgments can be prohibitively expensive. To overcome this obstacle, we measure model performance through a benchmark which correlates highly with this Chatbot Arena ranking.\nAt the time of writing, MixEval-Hard (Ni et al., 2024) is the benchmark with the highest Chatbot Arena correlation (0.98 rank correlation). MixEval-Hard features hard queries with known answers across a wide range of domains and uses a GPT3.5-turbo (Brown et al., 2020; Ouyang et al., 2022) model to evaluate if predicted answers correspond with this ground-truth. This makes MixEval-Hard more grounded in human knowledge and significantly cheaper to run compared to other popular evaluation frameworks such as AlpacaEval (Li et al., 2023; Dubois et al., 2024). Under the hood, MixEval-Hard utilizes queries sampled from MATH (Hendrycks et al., 2021), BBH (Suzgun et al., 2023), DROP (Dua et al., 2019), GSM8k (Cobbe et al., 2021), AGIEval (Zhong et al., 2024), TriviaQA (Joshi et al., 2017), MBPP (Austin et al., 2021), MMLU, (Hendrycks et al.,"}, {"title": "Training Specifications", "content": "Llama-3-8B-Instruct is trained for a total of 18 epochs on each preference dataset and alignment objective, with a checkpoint saved every single epoch. The $\u03b2$ hyperparameter, common to all alignment objectives except SFT, is set to 0.1.\nPrompt and responses are truncated to 512 tokens each. Each model is trained using an effective batch size of 16 across one node of 8 NVIDIA H100 GPUs, using the RMSProp optimizer with a learning rate of 2 \u00d7 10-7, linearly decaying to 0 over the 18 epochs. All training is implemented using the TRL library (von Werra et al., 2020)."}, {"title": "Results", "content": "We report the maximal and mean MixEval-Hard improvement over all checkpoints from the same training run. This helps us understand both the best-case and average impact of alignment across the entire training procedure. This analysis is summarized in Table 2 for every dataset and objective; we now discuss these results in more detail."}, {"title": "Preference Data", "content": "To assess the quality of a particular dataset, we consider the performance of that dataset when paired with its best objective. Using the APO-zero objective, the contrastive CLAIR dataset leads to the greatest improvement. Specifically, CLAIR leads to the greatest maximal improvement of +7.65% and the greatest average improvement of +2.93% out of all our experiments. This improvement of +7.65% closes the relative gap with GPT4-turbo by 45% using only 32K pairs.\nWe noted in Section 1 that uncontrolled contrastiveness can degrade model performance. We see this dramatically in the results for the Stronger Preferred dataset, which heavily degrades model performance. Like CLAIR, this dataset has all winning outputs produced by a stronger model. Unlike CLAIR, though, its examples provide no guarantee of relevant minimal contrasts. Thus, the contrastiveness induced by the CLAIR revision process is a major driver of performance.\nBoth on-policy judge and off-policy judge datasets lead to improved performance when paired with their best alignment objective, but on-policy preferences lead to better performance compared to off-policy preferences. This is intuitive; judgments about the target model's outputs are in general more relevant.\nA breakdown of performance in function of MixEval-Hard's constituent benchmarks is given in Appendix B for every dataset."}, {"title": "Alignment Objectives", "content": "Anchored Preference Optimization (APO) consistently leads to the greatest performance increase for every preference dataset, with the exception of the Stronger Preferred dataset, where all contrastive objectives deteriorate the model. The relation between the preference dataset and the target model controls which variant of APO is best for any dataset, as predicted in Section 2. APO-down results in the best performance when winning outputs are generally worse than the target model, as is the case for the off-policy judge dataset. APO-zero is the best objective when winning outputs are generally better than the target model, as is the case for CLAIR and on-policy judge datasets. The difference between alignment objectives is less salient for the on-policy judge dataset as compared to CLAIR, since winning on-policy judge outputs are only slightly better than Llama-3-8B-Instruct on average. Winning CLAIR outputs may be vastly better than Llama-3-8B-Instruct since they are produced by a stronger model, making the different in alignment objectives more noticeable."}, {"title": "Analysis", "content": "To more deeply understand how the target model is changed during training, we can study the trajectories of winning/losing likelihoods and rewards on held-out preferences. Figure 4 plots these trajectories for the APO-down, APO-zero, and DPO experiments on each preference dataset, using 100 held-out preference pairs from that dataset."}, {"title": "Preference Data", "content": "First, we observe that the likelihoods help characterize the type of preference dataset. In the on-policy judge dataset, all answers are sampled from the target model and thus have a high likelihood. The off-policy variant has no answers coming from the target model, and hence all likelihoods are low. Both CLAIR and Stronger Preferred have losing outputs with high likelihood and winning outputs with low likelihood.\nAny initial discrepancy between log-likelihoods is normalized by the reward, which tracks changes in likelihood and thus starts at exactly 0. The margin between winning and losing reward indicates how much more the winning likelihood increased during training. Positive reward margins can still produce negative log-likelihood margins, if any initial disparity between winning/losing log-likelihood is not overcome. This ends up being the case for our CLAIR dataset.\nThe training dynamics for CLAIR and Stronger Preferred look very similar, yet the downstream performance on MixEval-Hard is completely different. This is because contrastive alignment objectives will exploit any difference between winning and losing outputs to decrease loss. Most of these differences in CLAIR are directly related to improving performance, because CLAIR itself is a minimally contrastive dataset. Many of the differences in Stronger Preferred may not be relevant."}, {"title": "Alignment Objectives", "content": "All three alignment objectives display systematic behavior across each dataset. APO-zero consistently leads to the greatest winning and losing rewards. APO-down consistently produces the lowest rewards. Both of these behaviors are as intended. DPO has a slightly more complicated dynamic, which is nonetheless consistent across datasets. In the initial steps of training, DPO tracks the behavior of APO-zero (high rewards) before following APO-down (low rewards) during the remainder of training. This explains why downstream DPO performance correlates most with APO-down. However, DPO is never the best method on any dataset, because it falls between the distinct modes of APO-zero and APO-down.\nTraining models with contrastive alignment objectives is considerably more complex that conventional supervised fine-tuning. The result is dependent on the semantics of the alignment objective, the contrastive signal in the training data, and the relationship between data quality and target model. Our results show that paying attention to the interplay between these attributes is essential."}, {"title": "Related Work", "content": "We now characterize relevant alignment efforts and outline how they relate to Contrastive Learning from AI Revisions (CLAIR) and Anchored Preference Optimization (APO).\nReinforcement Learning from Human or AI Feedback (RLHF/RLAIF; Ouyang et al. 2022; Bai et al. 2022; Yuan et al. 2024) is a technique used to align models with human preferences. Fundamentally, these approaches first train a reward model using preference judgments and subsequently optimize a Language Model for this reward using Reinforcement Learning (Schulman et al., 2017). To side-step the need for an explicit reward model, Direct Preference Optimization (DPO; Rafailov et al. 2024b) aligns an LM directly using a contrastive training objective.\nWe articulated two core insights concerning (i) the role of contrastive preference data, and (ii) the need to anchor alignment depending on model and data. These insights translate to any effort which uses comparative preferences.\nFor the remainder of this review, we focus on contrastive methods which directly align an LM on preference data (of which Wang et al. 2024 provide a detailed overview).\nChanging the LM more/less: Amini et al. (2024) and Wu et al. (2024a) recognize that preference pairs can vary. Both works study how much more preferred the winning output is, and seek to incorporate this into the objective by changing the model more / less depending on this preference strength. Using the difference in gold rewards as a substitute for preference strength, Amini et al. (2024) add an instance-level margin to the contrastive objective while Wu et al. (2024a) scale the $\u03b2$ parameter at a batch-level. Other works also utilize a margin in the contrastive loss, but specify this as a static hyperparameter (Zhao et al., 2023; Azar et al., 2024; Meng et al., 2024). These contributions complement our own; they focus on how much a model should change, whereas CLAIR creates better learning signals and APO more fully specifies the intended training dynamics.\nControlling training dynamics: The tendency of DPO to decrease the winning likelihood has been remarked and analyzed in several works (Feng et al., 2024; Pal et al., 2024). Some works use an additional loss term to explicitly increasing the likelihood of winning outputs (Hong et al., 2024; Pentyala et al., 2024; Adolphs et al., 2023; Zhao et al., 2023; Xu et al., 2024). While these methods can be seen as variants of Anchored Preference Optimization, they do not recognize the need to anchor the objective differently depending on dataset and model, and they do not offer methods that explicitly decrease the winning likelihood when required. Both Rafailov et al. (2024a) and Azar et al. (2024) generalize a set of alignment methods, but neither allow for any anchoring.\nLearning from unpaired data: Ethayarajh et al. (2024) and Richemond et al. (2024) use unpaired examples and rewards for alignment instead of paired examples. Zhang et al. (2024) and Duan et al. (2024) operate solely on undesirable examples in this unpaired setting. In contrast, our work exclusively operates on paired preferences. However, the core insights of APO do apply to unpaired data. For example, Ethayarajh et al. (2024) use binary desired/undesired labels for each answer. We argue this desirability is inherently relative to the model: the same example of desirable behavior used to improve a weak model may actually be an example of undesirable behavior compared to a stronger model, causing the need for anchoring.\nLength-controlled optimization: Preference pairs created through a judging paradigm can be biased towards preferring more verbose answers (Saito et al., 2023). To prevent aligned models from inheriting this bias, Meng et al. (2024) and Park et al. (2024) explicitly control for the length of generations during training. These constraints on generation length can be seamlessly integrated into APO methods as well. In addition, CLAIR revisions could further help with these efforts to reduce the verbosity bias. For example, the Reviser could be designed to not increase length.\nReference-free optimization: Several objectives have opted to directly optimize the contrastive relation between winning/losing likelihoods instead of rewards, removing the need for a secondary reference model (Meng et al., 2024; Zhao et al., 2023; Hong et al., 2024; Xu et al., 2024). Since all these methods are contrastive, the insights from CLAIR and APO directly apply. Additionally, the CLAIR dataset used in our experiments may shed light on the nature of reference-free optimization. Figure 4 shows that our models are sufficiently aligned on the CLAIR dataset when considering rewards, but the absolute likelihood of losing outputs is still greater. This is due to the initial discrepancy in likelihoods produced by the revision process.\nIterative optimization: Updating the reference model during training can improve results (Kim et al., 2024; Rosset et al., 2024; Wu et al., 2024b). All of these insights are applicable to our work."}, {"title": "Conclusion", "content": "Alignment performance is significantly impacted by (i) the contrastiveness of the preference pairs and (ii) the relationship between model and data. We introduce Contrastive Learning from AI Revisions (CLAIR), a data-creation method which produces more contrastive preference pairs, and Anchored Preference Optimization (APO), a family of alignment objectives with tailored training dynamics. Our experiments aligning Llama-3-8B-Instruct show that CLAIR preferences lead to the highest performance improvement, and APO methods consistently outperform conventional alignment objectives."}, {"title": "Prompts", "content": "You are a teacher and your task is to minimally improve a student's answer. I will give you a {{task}} and a {{student_solution}}. Your job is to revise the {{student_solution}} such that it is clearer, more correct, and more engaging. Copy all non-corrected parts of the student's answer. Do not allude to the {{corrected_student_solution}} being a revision or a correction in your final solution.\\n\\n{{task}}:  \n\\n{{student_solution}}:  \\n\\n-\\n\\nLet's first think step by step with a {{teacher_reasoning}} to decide how to improve the {{student_solution}}, then give the {{corrected_student_solution}}. Mention the {{teacher_reasoning}} and {{corrected_student_solution}} identifiers to structure your answer.\\n\\n\nYou are a teacher and your task is to pick the best student's answer. The best answer is the most clear, most correct, and most engaging answer. I will give you a {{task}} and {{student_solution_1}} and {{student_solution_2}}. Your final answer must contain [1] if {{student_solution_1}} was best, else [2].\\n\\n{{task}}:  \\n\\n{{student_solution_1}}:  \\n\\n{{student_solution_2}}:  \\n\\n\\n\\nLet's first think step by step with a {{teacher_reasoning}} to decide which solution is better, and then answer [1] or [2].\\n\\n"}, {"title": "Preference Dataset Creation", "content": "The prompts we use for the Reviser and Judge function of Equation 1 and 2 are given in Table 3. Both prompts contain instructions to prefer more clear, more correct, and more engaging outputs. The Reviser prompt creates a preference pair by minimally revising and improving an output according to these preferences. Instead, the Judge prompt selects a more preferred output given two candidate answers."}, {"title": "Preference Pair Filtering", "content": "We reject revisions or judgments if the LLM failed to follow formatting guidelines specified in the revising or judging prompt. Additionally, we reject revisions if they altered the length of the original output too much; we found this mainly happens when the LLM misunderstands the revision prompt. Starting from the same 32K instructions sampled from UltraFeedback, this procedure creates 29K CLAIR pairs, 29K Stronger Preferred pairs, 29K off-policy Judge pairs, and 32k on-policy Judge pairs. We adapted the code by Williams (2023) to efficiently query closed-source LLMs in parallel over API."}]}