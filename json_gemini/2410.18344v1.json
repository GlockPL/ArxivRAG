{"title": "Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned and Retrieval-Augmented Generation Models", "authors": ["Fengchen Liu", "Jordan Jung", "Wei Feinstein", "Jeff D'Ambrogia", "Gary Jung"], "abstract": "This paper introduces a novel approach to enhancing closed-domain Question Answering (QA) systems, focusing on the specific needs of the Lawrence Berkeley National Laboratory (LBL) Science Information Technology (ScienceIT) domain. Utilizing a rich dataset derived from the ScienceIT documentation, our study embarks on a detailed comparison of two fine-tuned large language models and five retrieval-augmented generation (RAG) models. Through data processing techniques, we transform the documentation into structured context-question-answer triples, leveraging the latest Large Language Models (AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, Google Gemini-Pro) for data-driven insights. Additionally, we introduce the Aggregated Knowledge Model (AKM), which synthesizes responses from the seven models mentioned above using K-means clustering to select the most representative answers. The evaluation of these models across multiple metrics offers a comprehensive look into their effectiveness and suitability for the LBL ScienceIT environment. The results demonstrate the potential benefits of integrating fine-tuning and retrieval-augmented strategies, highlighting significant performance improvements achieved with the AKM. The insights gained from this study can be applied to develop specialized QA systems tailored to specific domains.", "sections": [{"title": "1 INTRODUCTION", "content": "Rapid advancements in Natural Language Processing (NLP) and deep learning have enhanced the capability of Question Answering (QA) systems to utilize both structured and unstructured data effectively. These systems are increasingly critical in diverse domains, from customer support to academic research, particularly in closed-domain applications known for their precision and domain-specific focus.\nAt the Lawrence Berkeley National Laboratory (LBL), the ScienceIT department addresses the IT needs of the scientific community, utilizing extensive documentation and resources. This work leverages data from the LBL ScienceIT documentation website, processed to support our QA system. By integrating technologies like Google Cloud VertexAI, AWS Bedrock, OpenAI, and Meta LLAMA with Retrieval Augmented Generation (RAG) [11, 16] through the LangChain framework, we fine-tune multiple models for enhanced QA performance. In addition to these models, we propose the Aggregated Knowledge Model (AKM), which synthesizes responses from the various fine-tuned and RAG models using K-means clustering [12, 13] to select the most representative answers. This novel approach aims to improve the overall performance and reliability of QA systems in the LBL ScienceIT domain.\nThis paper presents a comparative study of seven models, analyzing their performance and potential in the LBL ScienceIT domain."}, {"title": "1.1 Motivation and Benefits", "content": "\u2022 Streamlining Information Retrieval for Researchers: The LBL researchers require efficient access to ScienceIT documentation to support their work, necessitating systems that provide quick, precise information retrieval.\n\u2022 Elevating User Experience through Accessible Expertise: The QA system enhances user experience by providing rapid and accurate responses, improving efficiency and focus on primary research tasks.\n\u2022 Sustainability in Knowledge Dissemination: Our QA system offers a sustainable solution for information dissemination, evolving with the expanding needs of the ScienceIT ecosystem.\n\u2022 Bridging Informational Gaps: The initiative promotes accessible, accurate information for all researchers, ensuring inclusivity and comprehensive knowledge access within the Berkeley Lab."}, {"title": "2 RELATED WORK", "content": "The evolution of QA systems began with Stanford's BASEBALL in 1961, a rule-based linguistic model interfaced with a database [10]. Advancements in deep learning have significantly impacted QA system development across various domains, notably through the use of neural networks like CNNs [15], LSTMs [23], and the transformational architecture of transformers such as BERT and GPT-2 [6, 7, 21]. Modern QA approaches include:\n\u2022 Information Retrieval QA, utilizing search engines for answer retrieval [5].\n\u2022 NLP QA, employing NLP and machine learning for response deduction [8].\n\u2022 Knowledge Base QA, using structured datasets for precise answers [24].\n\u2022 Hybrid QA, combining multiple methods for enhanced results [17].\n\u2022 Retrieval-Augmented Generation QA, integrating retrieval with generative methods for contextually accurate responses [11, 16].\n\u2022 Open Domain and Closed Domain QA, catering to general or specific informational needs [2, 14].\nThis diversity highlights the evolution from singular, rule-based models to complex, multifaceted systems that blend retrieval, NLP, and domain-specific methodologies."}, {"title": "3 DATA", "content": "Generating meaningful question-answer pairs for a closed-domain QA system presents a significant challenge, particularly in ensuring the relevance and accuracy of the pairs to the specific domain. To address this challenge, we turned to the capabilities of the Language Model (LLM). Specifically, we employed GCP VertexAI's PaLM2(chat-bison@001)[9]. Given a context paragraph, this LLM was tasked with producing concise and pertinent question-answer pairs. The model's ability to understand the essence of a paragraph and distill it into questions and answers was crucial in meeting this challenge. The depth and breadth of the context paragraph determined the number of question-answer pairs generated, with an average context yielding about 5 pairs. In total, our approach resulted in approximately 2800 tuples of context, question, and answer for our data-driven experiments."}, {"title": "4 MODELS", "content": "In the domain of Question Answering (QA), the choice of the underlying model plays a pivotal role in determining the system's efficiency and accuracy. Given the specific requirements of the LBL ScienceIT domain, we ventured into a comprehensive exploration of both conventional and contemporary models. Our endeavor was twofold: fine-tuning Large Language Models (LLMs) and harnessing the power of Retrieval Augmented Generation (RAG) models. This section delineates the specifics of each model we employed.\nFigure [1] shows a systematic workflow to tackle QA challenges. The process begins with document preparation which involves loading the data, fragmenting it into digestible segments (discussed in Section 3), and then encoding these segments into numerical vectors using the AWS Bedrock Titan [4], GCP PaLM2 [9], Meta LLAMA [20] and OpenAI GPT-4 [1] embeddings. Once transformed, as shown in Figure[1]A, these vectors are cataloged in an index to streamline future retrievals. When a query is presented, as shown in Figure[1]B, its embedding is generated and juxtaposed with the indexed embeddings to pinpoint the most pertinent document segments. These identified segments are then incorporated into the context of the prompt dispatched to the AWS Bedrock, GCP PaLM2 (text-bison-001), the self-hosting Meta LLaMA models, OpenAI GPT-4 and Google Gemini-Pro[3]. This ensures that the response is not only relevant but also deeply contextual, drawing directly from the information within the retrieved document segments."}, {"title": "4.1 The Enhanced ScienceIT QA System", "content": ""}, {"title": "4.1.1 Two fine-tuned models:", "content": "\u2022 Model 1: Fine-tuned on (question, answer) pairs.\nModel\u2081: {(qi, ai)} \u2192 \u00e2\u00a1\nWhere qi is the question, ai is the ground truth answer, and \u00e2\u00a1 is the predicted answer.\n\u2022 Model 2: Fine-tuned on (question, context, answer) triples.\nModel2: {(qi, ci, ai)} \u2192 \u00e2\u00a1\nWhere ci is the context.\nBoth models were fine-tuned using 4x NVIDIA A100 80GB GPUs."}, {"title": "4.1.2 Five RAG models:", "content": "\u2022 Embedding Generation: For each document segment dj, generate an embedding ej using the respective LLM's embedding model.\nej = Embed(dj)\n\u2022 Vector Store Creation: Store all embeddings E = {e1, e2, ..., en} in a vector database.\n\u2022 Query Processing: Generate an embedding for the query qi:\neq\u2081 = Embed(qi)"}, {"title": "4.2 Aggregated Knowledge Model (AKM)", "content": "To enhance our Domain Knowledge Question Answering system, we introduced the Aggregated Knowledge Model (AKM) as the 8th model. This model leverages the strengths of seven individual LLM models, including fine-tuned versions of Google PaLM2 and various Retrieval Augmented Generation (RAG) models from AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, and Google Gemini-Pro.\nThe AKM evaluates responses from these models by utilizing K-means clustering [12, 13] with k = 1 to identify the most representative answer for each question. The specific steps involved are as follows:"}, {"title": "4.2.1 Vectorization.", "content": "Each predicted answer A\u00a1 from the models is converted into a TF-IDF [19] vector vi:\nV = [V1, V2, ..., V7]"}, {"title": "4.2.2 Clustering.", "content": "We apply K-means clustering with k = 1 to these vectors to find the centroid c of the cluster:\nc = 1/7* sum(vi) for i=1 to 7"}, {"title": "4.2.3 Distance Calculation.", "content": "For each vector vi, we calculate its Euclidean distance to the centroid c:\ndi = ||Vi - c||"}, {"title": "4.2.4 Selection.", "content": "The answer corresponding to the vector with the smallest distance to the centroid is selected as the final response:\nAfinal = Aarg min\u2081 di\nThis approach ensures that the chosen answer is the most central and representative of the combined knowledge from all models. By synthesizing diverse perspectives and reducing the impact of outlier responses, the AKM provides more accurate and reliable answers."}, {"title": "5 EXPERIMENTS", "content": "The primary goal of our experiments was to assess the efficacy of various model architectures for our Closed Domain QA System. Approximately 2800 (context, question, answer) tuples were used, with 80% allocated for model fine-tuning and 20% for validation to ensure thorough learning and unbiased assessment.\nEvaluation was conducted using a testing dataset comprising approximately 560 ScienceIT domain knowledge questions. Each question was processed by the first seven models (two fine-tuned models and five RAG models) with the LLM temperature set to 0.5. The eighth model, AKM, aggregated the responses from these seven models to generate its answer. This entire process was repeated 100 times, resulting in a total of 56000 samples for evaluation. The performance metrics for each model were computed as the mean and standard deviation (in Table[1], and Appendix A) from these samples, ensuring robust and reliable results."}, {"title": "5.1 Evaluation Metrics", "content": "We utilized BLEU Scores to evaluate n-gram accuracy, ROUGE Scores to assess recall, precision, and F1 metrics, and STS (Semantic Textual Similarity) to examine the semantic similarity between model-generated and reference answers, emphasizing STS's relevance due to language variability [18]."}, {"title": "5.2 Experimental Comparison", "content": "In addition to comparing the performance of all eight models, we also experimented with several methods for the eighth model (AKM) to select the most representative answer:"}, {"title": "5.2.1 TF-IDF and Cosine Similarity.", "content": "We first vectorized the answers using TF-IDF [19] and computed the cosine similarity between each pair of answers. However, this method did not yield the best performance, as the cosine similarity metric alone was insufficient to capture the nuanced differences between the answers."}, {"title": "5.2.2 Embeddings and Mean Embedding (using BERT).", "content": "In another approach, we utilized pre-trained BERT embeddings [6] to represent each answer. We then computed the mean embedding of all answers and selected the answer whose embedding was closest to this mean. Although this method showed improvement over cosine similarity, it still fell short in accurately identifying the most representative answers due to the high dimensionality and complexity of the embeddings."}, {"title": "5.2.3 Clustering (using K-means).", "content": "The most effective method was K-means clustering [12, 13]. By clustering the TF-IDF [19] vectors of the predicted answers and selecting the answer closest to the centroid, we were able to consistently identify the most representative answer. This method outperformed the previous approaches by better capturing the central tendency of the answer distributions."}, {"title": "6 ANALYSIS", "content": "Our diverse selection of models yielded varied results, with significant insights into their performance across different metrics, as showcased in Figure [2] and Table[1] (for each model's metrics distribution, please see Appendix A Figure 3). BLEU and ROUGE scores indicated specific strengths in text alignment and recall capabilities, while STS scores highlighted semantic similarities effectively handled by our models. Notably, Models with Retrieval-Augmented Generation (RAG) features, such as Model 6 (OpenAI GPT-4) and Model 7 (Google Gemini-Pro), showed strong performances in semantic contexts, aligning closely with reference answers, showcasing the impact of RAG on model performance.\nThe introduction of the Aggregated Knowledge Model (AKM) further improved performance by effectively synthesizing the strengths of the individual models. From the evaluation metrics in Table[1], it is evident that the AKM model consistently achieved higher scores across various metrics, indicating its robustness and effectiveness in providing accurate and contextually relevant answers. This improvement, with an overall performance increase of over 8%, can be attributed to the AKM's ability to aggregate and balance the diverse responses from multiple models, capturing the central tendency and reducing the impact of outlier predictions.\nThe use of K-means clustering in the AKM proved to be particularly effective. By vectorizing the predicted answers and identifying the centroid of the cluster, the AKM could select the most representative answer for each question. This method outperformed others, such as TF-IDF with cosine similarity and BERT embeddings with mean embedding, by better capturing the overall distribution of the answers and selecting the one closest to the centroid."}, {"title": "7 CONCLUSION", "content": "This study explored various models in Closed Domain Question Answering (QA) Systems, demonstrating the distinct advantages of Retrieval Augmented Generation models. The introduction of the Aggregated Knowledge Model (AKM) marked a significant improvement, with an overall performance increase of over 8%, surpassing the previously best-performing OpenAI GPT-4 with RAG. The AKM model leveraged the strengths of multiple models to provide more accurate and reliable answers, resulting in a higher average evaluation score across multiple metrics.\nThe results emphasize the need for tailoring model architecture and fine-tuning strategies to specific domains. The AKM model, by aggregating responses from various LLMs and RAG models, highlights the importance of integrating diverse model capabilities and effective retrieval mechanisms to enhance performance."}]}