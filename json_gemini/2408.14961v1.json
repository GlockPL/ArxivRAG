{"title": "CVPT: Cross-Attention help Visual Prompt Tuning\nadapt visual task", "authors": ["Lingyun Huang", "Jianxu Mao", "Yaonan Wang", "Junfei Yi", "Ziming Tao"], "abstract": "In recent years, the rapid expansion of model sizes has led to large-scale pre-trained\nmodels demonstrating remarkable capabilities. Consequently, there has been a trend\ntowards increasing the scale of models. However, this trend introduces significant\nchallenges, including substantial computational costs of training and transfer to\ndownstream tasks. To address these issues, Parameter-Efficient Fine-Tuning (PEFT)\nmethods have been introduced. These methods optimize large-scale pre-trained\nmodels for specific tasks by fine-tuning a select group of parameters. Among\nthese PEFT methods, adapter-based and prompt-based methods are the primary\ntechniques. Specifically, in the field of visual fine-tuning, adapters gain prominence\nover prompts because of the latter's relatively weaker performance and efficiency.\nUnder the circumstances, we refine the widely-used Visual Prompt Tuning (VPT)\nmethod, proposing Cross Visual Prompt Tuning (CVPT). CVPT calculates cross-\nattention between the prompt tokens and the embedded tokens, which allows us to\ncompute the semantic relationship between them and conduct the fine-tuning of\nmodels exactly to adapt visual tasks better. Furthermore, we introduce the weight-\nsharing mechanism to initialize the parameters of cross-attention, which avoids\nmassive learnable parameters from cross-attention and enhances the representative\ncapability of cross-attention. We conduct comprehensive testing across 25 datasets\nand the result indicates that CVPT significantly improves VPT's performance\nand efficiency in visual tasks. For example, on the VTAB-1K benchmark, CVPT\noutperforms VPT over 4% in average accuracy, rivaling the advanced adapter-based\nmethods in performance and efficiency. Our experiments confirm that prompt-\nbased methods can achieve exceptional results in visual fine-tuning. The code is\navailable at https://github.com/Xlgsyzp/CVPT/tree/main", "sections": [{"title": "Introduction", "content": "Increasing the scale of the models is a common method to enhance the model's performance\n[35][9][28][29]. In recent years, with the rapid development of computing devices, model sizes\nhave significantly increased [45][6][17][47]. For instance, the number of parameters in the GPT\nseries developed by OpenAI has surged from 117 million to 1.8 trillion in just five years [36][37][2].\nThe rapidly increasing number of parameters will lead to the problem of immense computational\noverhead. Therefore, adapting those models to downstream tasks with the full-tuning method will\nincur enormous costs. To resolve this issue, the PEFT approach has been proposed [20][27][1][38][5].\nPEFT adapts those large-scale pre-trained models to downstream tasks in a more efficient way by\nfine-tuning a subset of the models that contains much fewer parameters. Two mainstream methods\nwithin PEFT are Adapter [19] and Prompt [27]. During the training process, the Adapter inserts\nadapters into each transformer block and tunes those adapters, while the Prompt inserts prompt tokens\ninto the embedded tokens to update the prompt tokens.\nVPT, a prompt-based method is first introduced by Jia et al. [22] for visual fine-tuning tasks. Never-\ntheless, research on the adapter-based method is prominent due to its superior performance. Although\nsome works have improved the performance of VPT [21][13][7], it is still challenging to match the\neffectiveness to that of adapter-based methods. There appears to be a consensus that prompt-based\nmethods underperform adapter-based methods in the visual domain. But is this the case?\nWe conduct extensive experiments and analyses on VPT to uncover the reasons for its weaker\nperformance compared to the Adapter. According to our experiments, we consider that the primary\nreason for the performance difference between VPT and adapters is that VPT's deployment directly\napplies that used in NLP tasks [27], without any adaptation to visual tasks. In NLP tasks, prompts\nusually contain rich semantic information that guides the fine-tuning process of the model. However,\nin visual tasks, prompts lack representation information. Therefore, it is necessary for VPT to use an\nabundant amount of prompts to fine-tune models. However, the design of VPT leads to computational\ninefficiency and redundancy, as well as the disruption of the self-attention between embedded tokens\n3.1. As the graph follows 1, VPT shows a significant decrease in performance and an increase in\ncosts when given a large number of prompts. Considering that, we think that VPT is unusable when\ngiven a large number of prompts.\nTo handle the problem, we redesign VPT and in-\ntroduced Cross Visual Prompt Tuning (CVPT). For\nthe prompt tokens in CVPT, we calculate the cross-\nattention with the embedded tokens and add the result\nas residuals to the embedded tokens. This approach\navoids the computational complexity of self-attention\nthat is quadratically related to the number of prompts\nand allows prompts to focus on the embedded token\nto adapt to downstream tasks more efficiently. Addi-\ntionally, by maintaining consistency in token dimen-\nsions throughout the computation process, the results of\ncross-attention can be directly summed with embedded\ntokens as residuals and do not introduce additional com-\nputational overhead for subsequent MLP. Furthermore,\nwe share the weights of the self-attention layer with\nthe cross-attention layer during loading checkpoints,\nkeeping the cross-attention layer frozen alongside the\nself-attention layer, which eliminates the requirement\nfor additional learned parameters for the cross-attention,\nand utilizes the encoded information in self-attention\nto help the fine-tuning of the model.\nWe validate the effectiveness of our method on 25 datasets, the results show that the CVPT achieves\na significant improvement in performance and efficiency compared to the VPT. CVPT shows an\naverage 4% improvement in accuracy on the 19 VTAB-1K datasets, 1% on the 5 FGVC datasets,\nand 3% on the ADE20K dataset. Additionally, if given fewer prompt tokens, CVPT achieves a\ncomparable performance with other advanced PEFT methods which significantly outperforms the\nother prompt-based methods and needs fewer learnable parameters. If a large number of prompts\nis allowed, our CVPT outperforms the SOTA methods on FGVC and ADE20K datasets. Besides,\nalthough a large number of prompts are inserted, it does not introduce too much extra computational\noverhead compared to VPT.\nFinally, we explore the impact of the deployment's position and the effectiveness of the weight-\nsharing mechanism. The improvement on the model can be fully illustrated by the experimental\nresults above, indicating that prompt-based methods can also rival SOTA adapter-based methods.\nOverall, our contributions are as follows:\n\u2022 We provide a detailed analysis of the application of VPT to visual tasks, and propose that its\ndrawback can be summarised in three points which are lack of adaptation, computational\ninefficiency and redundancy, destruction of self-attention."}, {"title": "Related Work", "content": "PEFT. In the era of CNN, making bigger and deeper models was an effective way to improve\nperformance [26][16][43]. With the rise of transformers, this trend became even more popular.\nThe introduction of ChatGPT further cemented the goal of the community to develop larger and\nmore powerful models. However, limited by their scale, despite their powerful performance and\ngenerality, these large models are difficult to adapt downstream tasks by using traditional paradigms\n(full-tuning). Consequently, NLP researchers first proposed PEFT methods. Their works demonstrate\nthat fine-tuning just a small number of parameters in a large-scale pre-trained model can achieve\nnearly the same performance as full-tuning. Encouraged by the success in NLP, researchers began\nto apply PEFT to large-scale vision models on different visual tasks [8][44]. After development in\nthe past several years, the mainstream PEFT methods can be broadly categorized into adapter-based\nmethods and Prompt-based methods.\nAdapter. Jie et al. [19] proposed inserting adapters into the network to efficiently fine-tune the\nmodel. These adapters are commonly a small network that usually contains an upsampling layer\nand a downsampling layer. The input is multiplied with a scaling factor $\\gamma$ after passing through the\nupsampling and downsampling layers and then the result is added as a residual to the input. The\ngeneral form of adapter can be expressed as:\n$X_{out} = X_{in} + \\gamma(W_{up}(W_{down}(X_{in}))),$   (1)\nwhere $X_{in}$ denotes the input of Adapter, $\\gamma$ represents the scaling factor of Adapter, and $W_{up}$ and\n$W_{down}$ correspond to the upsampling layer and downsampling layer, respectively. Some works did\nsome adaption to visual tasks based on Adapter, developing several variants such as AdaptFormer\n[4], LoRA [20] and RepAdapter [30], etc. These adapter-based methods dominate the field of visual\nfine-tuning.\nPrompt. Prompt was originally used in the field of NLP which is added to the input text for\ncomprehension tasks. Lester et al. [27] proposed treating the prompt as a continuous vector and\nfine-tuning the model by updating its gradients. Jia et al. [22] introduced this concept to visual\nfine-tuning for the first time, naming it VPT. As shown in Fig.3, the embedded tokens are spliced with\nthe prompt tokens before entering each transformer block, allowing it to participate in every layer of\nthe network within the transformer block. Before entering the next transformer block, the prompt\ntokens of the previous layer are discarded, and new prompt tokens are spliced with the embedded\ntoken again (VPT-Deep). This can be formulated as shown below:\n$[Z_{i}, E_{i}] = L_{i}([Z_{i-1}, P_{i-1}, E_{i-1}]),$   (2)\nwhere the red and blue indicate learnable and frozen parameters, respectively. $P$ denotes a learnable\nd-dimensional vector, X is the CLS token, and E is the patched image. Although there are improved\nvariants based on VPT, such as E2VPT [13], EXPRESS [7] and DAM-VP [21], a performance gap\nremains between prompt-based and adapter-based approaches."}, {"title": "Method", "content": "3.1 Analysis of previous VPT\nFirstly, we analyze VPT deeply to explore why it is not better than adapter in terms of performance\nand efficiency, our analysis follows three points:\nLack of adaptation to visual tasks. In NLP, each token represents an actual word with rich semantic\ninformation. Therefore, the processing of concatenating prompt tokens and embedded tokens is\nnatural and suitable for NLP tasks. However, in visual tasks, tokens represent image patches and\ncontain sparse semantic information compared to those in NLP. Therefore, simply splicing the prompt\ntokens with the embedded tokens may not provide sufficient guidance information. Additionally,\nvisual tasks often require a deeper understanding of spatial relationships and structural features of an\nimage, which are difficult to achieve with prompt tokens.\nComputational inefficiency and redundancy. When computing self-attention, the attention between\neach token and all other tokens needs to be calculated. Its computational complexity is $n^2$, where\nn is the number of embedded tokens. If m represents the number of inserted prompt tokens, the\ncomputational complexity of self-attention in VPT can be expressed as $(n + m)^2$. This increases\nthe computational overhead significantly, especially when using a larger number of prompt tokens.\nAdditionally, we found that prompt tokens are involved in the MLP computation process, which not\nonly adds computational overhead but also does not impact the results. Our experiments show that\nremoving the prompt token after self-attention does not affect the results.\nDestruction of self-attention between embedded tokens. After softmax, the sum of the weights\nof all tokens is normalized to 1. Whereas, due to the addition of the prompt tokens, the sum of\nthe weights of the embedded tokens is reduced by the prompt tokens, which corresponds to the\nweakening of the representation ability of the self-attention between embedded tokens. Since the\nprompt token is eventually removed, this is equivalent to multiplying the self-attention result between\nthe embedded tokens by a factor which less than one. To explore how large this effect is, we set the\nnumber of prompts to 1,5,20,50,100,150,196 respectively, and visualize the tensor after the softmax\nfunction, the results are shown in Fig.2 below."}, {"title": "Cross Visual Prompt Tuning", "content": "Cross-Attention. Unlike self-attention [40], which computes the relationship between each element\nin the input sequence, cross-attention computes attention on two different sequences to process the\nsemantic relationship between them [3]. For example, in translation tasks, cross-attention is used to\ncompute the attention weights between the source language sentence and the target language sentence.\nIn our method, we introduce cross-attention to handle the semantic relationship between embedded\ntokens and prompt tokens, guiding the fine-tuning of the model. Specifically, the input of cross-\nattention consists of two parts: $X_1$ and $X_2$, in which $X_1 \\in R^{n\\times d_1}$ and $X_2 \\in R^{m\\times d_2}$. And $X_1$ serves\nas the query set and $X_2$ serves as the key-value set. We set $Q = X_1W_Q$ and $K = V = X_2W_K$, and\nthen the cross-attention can be expressed as follows:\n$CrossAttention(X_{1}, X_{2}) = Softmax(\\frac{QK}{\\sqrt{d_k}}) V.$   (3)\nIn which $W_Q \\in R^{d_{1}\\times d_k}$ and $W_K \\in R^{d_{2}\\times d_k}$ are learned projection matrix, $d_k$ is the dimension of\nvalue-key set. In our methods, $d_1 = d_2 = d_k$. And the shape of output is $n \\times d_k$, which is consistent\nwith $X_1$.\nCross Visual Prompt Tuning. We redesign the prompt to better adapt visual tasks and proposed\nCVPT. Our approach, as illustrated in Fig.3, follows the VPT, the main parameters of the network\nremain frozen, and only the final classification layer and the prompt are trainable. The key difference\nis that we allow the prompt token to perform cross-attention with the embedded tokens and the result\nof cross-attention is added with the embedded tokens as residuals. This operation helps prompts adapt\nvisual tasks a lot, and we demonstrate how significant this improvement is in Sec.4.2. Specifically,\nfor any input $x_i$ of a transformer block, the forward flow can be represented as follows:\n$X_{1} = X_{i} + SA(LN_{1}(X_{i})),$   (4)\n$X_{2} = X_{1} + CA(X_{1}, Prompt),$   (5)\n$X_{out} = X_{2} + MLP(LN_{2}(X_{2})),$   (6)\nwhere blue denotes frozen parameters and red denotes trainable parameters, SA denotes self-attention,\nCA denotes cross-attention, and LN denotes layer normalization.\nIn CVPT, we only introduce linear computational overhead associated with the number of prompt\ntokens. It allows CVPT to use a large number of prompt tokens to improve its performance by\nintroducing an acceptable overhead. Furthermore, CVPT preserves the original procedure of self-\nattention, keeping the complete representation ability of embedded tokens. We demonstrate the\nimprovement over VPT in terms of performance and efficiency in Sec.3.3. Finally, we set embedded\ntokens as query set and prompt tokens as key-value set, so that we can maintain the unity of the\nnumber of channels, allowing the result of cross-attention to be directly summed with the input as a\nresidual term.\nWeight-sharing mechanism. The utilization of cross-attention, which requires a large number\nof learnable parameters (usually $\\geq$ 30% model's parameter number), leads to a major challenge\nin computational overhead. Therefore, if the parameters of them are tunable, the computational\noverhead of CVPT will even rival those using full-tuning. Therefore, we introduce the weight-sharing\nmechanism. Due to the structure of cross-attention equals to that of self-attention, we consider that\nthe weight of self-attention is also instructive for the fine-tuning of cross-attention. Thus, we initialize\nthe weight of cross-attention with the parameters of self-attention when loading checkpoints. It\navoids the introduction of a huge number of learnable parameters in cross-attention and keeps the\nefficiency of our CVPT. We explore the impact of weight-sharing in 4.3 and demonstrate that frozen\ncross-attention is even more effective than learnable cross-attention."}, {"title": "Comparison with VPT", "content": "Performance improvement. To investigate how much improvement CVPT makes and the effect\nof the number of prompts on performance, we use different numbers of prompt tokens and conduct\nexperiments on VTAB-1K using VPT and CVPT, respectively. The results are shown in the following\nThese results show that our CVPT achieves better performance in almost every case except the number\nof prompts equals 1. As we analyzed in Section 3.1, VPT represents a pool absolute performance\non account of the lack of adaptation to visual tasks. Besides, due to the corruption of self-attention\nbetween embedded tokens, when given a larger number of prompt tokens, VPT shows significant\nperformance degradation or even crashes. In contrast, our CVPT avoids suffering from these problems.\nAdditionally, its performance improves as the number of prompt tokens increases. All these results\nabove indicate that cross-attention between prompt tokens and embedded tokens helps prompts\nadapting the visual tasks and instruct the model's fine-tuning more exactly.\nEfficiency improvement. To explore the improvement in efficiency of CVPT, we also recorded the\namount of GPU memory occupied by VPT and CVPT during training and testing as well as the total\ncomputation of the two when conducting the above experiments, and the results are shown in Fig.4\nfollows:\nIt can be seen that our CVPT has made significant improvements in efficiency compared to VPT\nespecially given a large amount of prompt tokens. Although it requires slightly more GPU memory\nduring testing compared to full-tuning which is marginal compared to VPT. Additionally, the weight-\nsharing mechanism allows for targeted optimization in engineering applications, letting cross-attention\nand self-attention share memory, further widening the efficiency gap with VPT. Moreover, the careful\ndesign of CVPT prevents explosive growth in memory and computation as the number of prompts\nincreases. This means we can improve the performance of CVPT by increasing the number of\nprompts, which is more computationally efficient than other methods.\nIn summary, our CVPT significantly improves the performance and efficiency of VPT by\nintroducing cross-attention and the weight-sharing mechanism, especially given a larger number\nof prompts. Therefore, it allows us to introduce more prompts to the prompt-based method in an\nefficient manner, thus improving its performance. We will demonstrate how much this improvement\nis and compare it with the SOTA methods in the next section."}, {"title": "Experiment", "content": "4.1 Experimental settings\nDatasets. We evaluate our CVPT on both image classification and semantic segmentation tasks to\nverify its effectiveness. The specific datasets involved in our work are presented in the following.\n\u2022 VTAB-1K. VTAB-1K comprises 19 datasets from different domains, classified into\nthree main categories: the Natural group (natural images captured by standard cameras)\n[25][32][10][34], the Specialized group (professional images captured by specialized equip-\nment, such as medical and remote sensing images) [41][18], and the Structured group\n(synthetic images from artificial environments). Each task contains only 1,000 training\nsamples [23][12][31]. This is a primary metric for evaluating PEFT's performance.\n\u2022 FGVC. FGVC consists of five fine-grained visual classification benchmarks, including CUB-\n200-2011 [42], NABirds [39], Oxford Flowers [33], Stanford-Dogs [24] and Stanford-Cars\n[11]. Unlike VTAB-1K, the datasets in FGVC benchmarks are complete.\n\u2022 ADE20K. ADE20K [50] contains more than 25,000 images and is primarily used for scene\nperception, parsing, segmentation, multi-object recognition, and semantic understanding.\nThis adaptation is challenging due to the huge gap between the objectives of pretraining and\ndownstream tasks.\nBaseline. We primarily use CVPT to compare with the following methods: (1) Full-tuning, (2)\nAdapter and its improved variants such as LoRA, Adaptformer, RepAdapter, and SPT, and (3) VPT\nand its variants, including E2VPT, EXPRESS and so on.\nTraining. We use the ViT-Base-16 model as our main model and AdamW as our optimizer. The\nother settings and training strategies follow those used in VPT. To avoid extensive hyperparameter\nsearch, we only select the number of prompts from [1, 5, 10, 20] for VTAB-1K. Besides, we use\nsingle NVIDIA 3090 on VTAB-1K and FGVC benchmark, and use NVIDIA 3090 \u00d7 8 on ADE20k.\n4.2 Comparison with the SOTA\nVTAB-1K. We compared our method with other baseline methods on the VTAB-1K benchmark. The\nexperimental results are shown in Table.2, where we report the top-1 accuracy of these methods. In\nthe table, we divide the prompt-based methods into one group and the other methods into another\ngroup. The bold values in each group represent the best accuracy.\nWe first compare our method with other prompt-based methods. The results of our experiments show\nthat our method achieved the best performance among prompt-based methods in 16 out of 19 datasets,\nsignificantly outperforming VPT and other VPT-based methods. Notably, CVPT achieves the highest\naccuracy in all datasets within the structured group, indicating that the addition of cross-attention"}, {"title": "Ablation Studies", "content": "The impact of the location of the Cross-Attention (CA). We conducted experiments with the\nfollowing five positions to explore the optimal deployment of CA, and the results of the experiments\nare displayed in Table.5:\nWe can see that inserting in prompt tokens after self-attention (SA) is the best way to perform.\nHowever, if a slight performance decrease is acceptable, we can choose position 2 to insert in parallel\nto improve the efficiency of the operation (this improvement is also slight).\nThe impact of weight-sharing between CA and SA. We set CA to be learnable (without weight-\nsharing) and frozen (with weight-sharing) respectively to investigate the impact of weight-sharing.\nThe results on VTAB-1K and FGVC are shown in Table.5 below:\nWe find that setting CA to tunable adds a significant number of parameters, substantially increasing\ncomputational overhead. Despite the slight performance gain it brings on VTAB-1K, it lags behind\nthe frozen CA substantially in FGVC. Therefore, We believe that the parameters of SA are valuable\nfor guiding the fine-tuning of CA. Especially, when dealing with a complete dataset of a certain size,\nsuch as FGVC, the weight-sharing mechanism can better utilize the pre-trained capabilities of the\nmodel, thereby improving performance."}, {"title": "Conclusion", "content": "In this paper, we explore the current mainstream prompt-based method VPT deeply and analyze the\nreasons why it performs poorly. Consequently, we propose a simple and effective PEFT method,\nCVPT, which introduces the cross-attention module to compute the cross-attention between the prompt\ntokens and embedded tokens thus instructing the model's fine-tuning. What more, the weights of\ncross-attention are come from self-attention, avoiding introducing an enormous number of additional\ntrainable parameters and achieving better performance. We conducted extensive experiments on\n25 datasets, and the results demonstrate that CVPT achieves SOTA performance. Additionally,\nwe conducted extensive ablation experiments on CVPT, demonstrating the impact of introducing\ncross-attention and weight-sharing, as well as its efficiency and performance improvements over VPT.\nWe hope our work will inspire prompt-based PEFT methods in the future. One limitation of our work\nis that CVPT does not explore new strategies for the initialization of prompt tokens. In VPT, the\nauthor made a complete comparison of different initialization methods. In our work, we take the\nsame strategy with VPT. However, we still think the optimized specific initialization method is better\nthan the general methods VPT used. Besides, this initialization will also help us understand how\nprompts help the model's fine-tuning."}]}