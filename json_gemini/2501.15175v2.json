{"title": "Option-ID Based Elimination For Multiple Choice Questions", "authors": ["Zhenhao Zhu", "Bulou Liu", "Qingyao Ai", "Yiqun Liu"], "abstract": "Multiple choice questions (MCQs) are a popular and important task for evaluating large language models (LLMs). Based on common strategies people use when answering MCQs, the process of elimination (PoE) has been proposed as an effective problem-solving method. Existing methods to the PoE generally fall into two categories: one involves having the LLM directly select the incorrect options, while the other involves scoring the options. However, both methods incur high computational costs and often perform worse than methods that directly answer the MCQs with the option IDs. To address this issue, this paper proposes a PoE based on option ID. Specifically, our method eliminates option by selecting the option ID with the lowest probability. We conduct experiments with 10 different LLMs in zero-shot settings on 7 publicly available datasets. The experimental results demonstrate that our method significantly improves the LLM's performance. Further analysis reveals that the sequential elimination strategy can effectively enhance the LLM's reasoning ability. Additionally, we find that sequential elimination is also applicable to few-shot settings and can be combined with debias methods to further improve LLM's performance.", "sections": [{"title": "Introduction", "content": "MCQs are a common assessment tool that plays a crucial role in evaluating the reasoning and comprehension abilities of LLMs. Due to their simplicity and structured nature, MCQs are widely used in various contexts such as standardized testing and machine reading comprehension. Therefore, how to improve LLMs' performance on MCQs tasks has become an important area of research.\nWhen people solve MCQs, they often use a special strategy named the process of elimination (PoE), which involves first identifying and discarding obviously incorrect options, and then progressively narrowing down the range of possible answers until the correct one is identified. Based on this idea, previous studies have implemented the PoE to LLMs to enhance their MCQs answering capability. Existing PoE for LLMs can be broadly divided into two categories: one involves directly having the LLMs identify and select the incorrect options (Balepur et al., 2024), while the other involves scoring each option and eliminating those with lower scores (Ma and Du, 2023). However, these methods both have significant limitations. First, it has been shown that the PoE based on incorrect options is not applicable to current LLMs. Second, the PoE based on option scoring requires multiple forward inferences for each option, which is computationally expensive. Furthermore, the effectiveness of it is often not as high as that directly answer the MCQs with the option IDs (Robinson and Wingate, 2023).\nTo address these issues, this study introduces an option-ID based PoE (PoEID). This method does not require scoring each option individually; instead, it eliminates option by selecting the option ID with the lowest probability in a single forward inference, thereby simplifying the reasoning process and significantly reducing computational overhead. To validate the effectiveness and generalizability of our method, we conduct experiments with 10 different LLMs in zero-shot settings on 7 publicly available datasets. The experimental results show that PoEID significantly improves the LLM's performance on MCQs tasks, especially on datasets related to logical reasoning. In addition, we further analyze the failure cases of the LLM when directly selecting incorrect options, and identify that these failures mainly stem from intrinsic limitations of the LLM itself. We then analyze various elimination strategies introduced in our study, and the results demonstrate that the sequential elimination strategy (PoEseq) (Figure 1) offers a significant advantage. Finally, we found that the PoEID not only enhances the LLM's reasoning abilities but also performs well in few-shot settings, and can be combined with debias methods to further improve the LLM's performance. Specifically, our contributions are threefold:\n\u2022 We propose the PoEID. Instead of scoring each option individually, this method eliminates the most likely incorrect option through a single forward inference, thus significantly simplifying the reasoning process and effectively reducing computational overhead.\n\u2022 We show that the PoEID significantly improves the LLM's performance on MCQs tasks, particularly achieving better results on datasets related to logical reasoning.\n\u2022 We find that the PoEseq exhibits a clear advantage over all elimination strategies. It not only enhances reasoning capabilities but also performs well in few-shot settings, and when combined with debias methods, it further improves the LLM's performance."}, {"title": "Related Work", "content": "In recent years, LLMs based on the Transformer (Vaswani et al., 2017) architecture have become the mainstream method. The introduction of models such as the GPT series has accelerated the development of the pre-training-fine-tuning paradigm, demonstrating the remarkable performance of large-scale pre-trained language models across a variety of downstream tasks. Notably, models like GPT-3 (Brown et al., 2020) have pushed the boundaries of LLM potential, achieving state-of-the-art performance across multiple tasks. In recent years, with the emergence of studies such as LLAMA (Touvron et al., 2023a; Touvron et al., 2023b), researchers have further explored scaling laws and efficient training techniques, enhancing both the performance and efficiency of LLMs. Despite their impressive performance across a wide range of tasks, these LLMs still face challenges, including issues related to model interpretability, bias and fairness (Gallegos et al., 2024), and their reliance on domain-specific knowledge."}, {"title": "MCQs", "content": "MCQs, due to their structured format and ease of evaluation, have become widely adopted in many applications centered around LLMs. For instance, in the automatic evaluation frameworks proposed by (Chiang et al., 2023; Zheng et al., 2023b), GPT-4 (OpenAI, 2023) is provided with a question and two candidate answers and is required to determine which answer is more accurate. This approach leverages the reasoning and comparison capabilities of LLMs to automate the evaluation process, thereby reducing the reliance on human annotations. Additionally, MCQs are commonly used in standard benchmark tests to assess the knowledge and reasoning abilities of language models, such as MMLU (Hendrycks et al., 2020). Despite the widespread use of MCQs in evaluation, recent research has raised concerns about their robustness in the context of LLMs. For example, Chang et al. (2024) points out that LLMs exhibit high sensitivity to subtle variations in question phrasing or answer ordering, leading to inconsistent performance across different scenarios. Similarly, Yang et al. (2024) demonstrates that LLMs often display over-confidence in incorrect answers, particularly when distractors are semantically similar to the correct answer."}, {"title": "Methodology", "content": "For MCQs, we use q to denote the question, oi to the option ID (e.g. A,B,C,D), yi to the option, and x to the concatenation of the option IDs and the corresponding options."}, {"title": "PoE", "content": "For MCQs, two popular methods are (1) directly answer the question and (2) find the correct answer through PoE. The direct answering treats all options equally and relies on the model to directly predict the most likely correct answer. However, this method can be unreliable when the question involves complex relationships between the options and the question, particularly when distractors are closely related to the correct answer. In contrast, PoE is commonly used by people to solve MCQs. This method involves progressively eliminating obviously incorrect options, narrowing down the set of possible answers until the correct one is identified. The application of PoE can significantly enhance the model's ability to handle ambiguous or difficult questions. Given the strong capabilities of LLM in understanding context, inferring relationships, and reasoning over a large space of potential answers, applying PoE to LLM offers a promising opportunity to improve performance on MCQs tasks. In contrast to traditional PoE of calculating the probabilities of the options, we use the probabilities of the option IDs as a substitute. After the elimination of options, the option IDs are updated, e.g. when option C is eliminated, the original D will be updated as C. Specially, our method ensures that the relative positions of the remaining options remain unchanged after the elimination of options."}, {"title": "Elimination Strategies", "content": "In this section, we introduce three elimination strategies."}, {"title": "Eliminating One Option at a Time (POEID)", "content": "First, we compute the probability for each option ID and select the option corresponding to the ID with the lowest probability as the eliminated option.\n$Yeli = arg \\min P(o_i | q, x), i \\in \\{1,2,3,4\\}$\nThen, we remove the eliminated option from x, resulting in a new set $x_{new}$. Finally, we recompute the probabilities for the option IDs based on $x_{new}$ to obtain the answer.\n$answer = arg \\max P(o_i | q, x_{new}), i \\in \\{1,2,3\\}$"}, {"title": "Eliminating Two Options at a Time (POEID)", "content": "Similar to the strategy in Section 3.3.1, the only difference is that, in the first step of elimination, two options are selected.\n$Yeli = \\begin{cases} arg \\min P(o_i | q, x), arg \\min P(o_j | q, x) \\\\ i, j \\in \\{1,2,3,4\\} \\end{cases}$"}, {"title": "Sequential Elimination (PoEseq)", "content": "The first step is the same as the strategy in Section 3.3.1. But after this step, we perform further elimination based on the $x_{new}$.\n$Yeli = arg \\min P(o_i | q, x_{new}), i \\in \\{1,2,3\\}$"}, {"title": "Experimental Setup", "content": "To cover a broad range of domains, we conduct experiments on seven 4-options datasets. These datasets are categorized into two groups:\n\u2022 Commonsense reasoning primarily require general world knowledge, intuitive understanding and linguistic reasoning. We use OpenBookQA(Mihaylov et al., 2018), and three sub-datasets selected from the BIG-Bench tasks(Srivastava et al., 2023): Multiemo, Metaphor Understanding (MU) and Phrase Relatedness (PR).\n\u2022 Logical reasoning focus on structured reasoning, causal inference and the application of scientific principles. We use ARC-Challenge(Clark et al., 2018), and two sub-datasets selected from the BIG-Bench tasks: Physics and Conceptual Combinations (CC).\nFor each dataset, we prioritize sampling from the test set. And if the number of samples exceeds 500, we randomly sample 500 examples; otherwise, we use all available samples."}, {"title": "LLM Selection and Configuration", "content": "\u2022 Selection We select 10 mainstream LLMs, covering different parameter scales. Specifically, these include: Llama-3-8B(Grattafiori et al., 2024), Llama-3.1-8B(Grattafiori et al., 2024), Falcon3-1B/7B/10B(Team, 2024), Mistral-7B-Instruct-v0.2/0.3(Mistral, 2024), Qwen2-7B(qwe, 2024), OLMO-2-1124-7B(OLMo et al., 2024) and Phi-3-medium-128k-instruct (14B)(Abdin et al., 2024). Since We need to obtain output probabilities, all of these LLMs are open-source and available on the HuggingFace platform.\n\u2022 Configuration The temperature hyperparameter for all LLMs is set to 0.1. And We use fixed prompts to ensure the consistency of experimental conditions. The specific content of the prompt can be found in Appendix A. Unless otherwise specified, all experiments are conducted in zero-shot settings."}, {"title": "Baselines", "content": "We select 9 common methods as baselines for comparison. These baselines are categorized into four groups."}, {"title": "Option Scoring Method", "content": "\u2022 Language Model (LM)(baseline in Zhao et al., 2021) LM simply selects the option with the highest probability. Thus, LM can be be written as follows:\n$arg \\max P(y_i | q)$\nFor causal language models, e.g. GPT, $P(y_i | q)$ can be further decomposed as:\n$P(y_i | q) = \\prod_{j=1}^{l_i} P(y_j | q, y_{1}, ..., y_{j}^{-1})$\nwhere $y_j$ is the jth token of $y_i$ and $l_i$ is the number of tokens in $y_i$.\n\u2022 Average Log Probability (AVG)(Brown et al., 2020) AVG takes the logarithm of the probabilities based on the LM and performs normalization.\n$arg \\max \\frac{1}{l_i} log P(y_i|q)$\n\u2022 Domain Conditional PMI (PMIDC)(Holtzman et al., 2021) PMIDC reweights the option score by calculating the probability of the option in the specific task domain, allowing different valid answers to compete fairly.\n$arg \\max log \\frac{P(y_i | q)}{P(y_i | q_{domain})}$\nWhere $q_{domain}$ is the string \"Answer:\" in our study.\n\u2022 Channel (Min et al., 2022) Channel computes the probability of the question given the option, assuming that $P(y_i | q) \\propto P(q | Y_i)$.\n$arg \\max \\frac{1}{l_i} log P(q | Yi)$\n\u2022 All-Options Log Probability (AOLP) (baseline in Ma and Du, 2023) AOLP calculates the probability of a specific option given all options.\n$arg \\max log P(Yi | q, x)$"}, {"title": "Option-ID Based Method", "content": "\u2022 Multiple Choice Prompt (MCP)(Robinson and Wingate, 2023) MCP assigns an ID to each option and selects the answer based on the probabilities of IDs.\n$arg \\max P(o_i | q, x)$"}, {"title": "Generative Method", "content": "\u2022 Text Generation (TG)(Wei et al., 2022; Wang et al., 2023b; Kojima et al., 2022) TG first generates a piece of text based on the input, and then extracts the answer from the generated text using manually designed specific rules.\n$text = generate(q, x)$\n$answer = extract(text)$"}, {"title": "PoE Method", "content": "\u2022 Explicit Elimination (EE)(Balepur et al., 2024) EE uses TG in both steps. First, Let the LLM select the incorrect option, then remove it from x, and finally answer. Similar to Section 3.3, EE also employs three strategies.\n$Incorrect option = extract(text)$\n$answer = extract(text_{new})$\n\u2022 Implicit Elimination (IE)(Ma and Du, 2023) IE first computes $logP(y_i | q, x)$, and then calculates an average probability for all options. Options with probabilities below this threshold are replaced with a special string: [MASK]. Then the probabilities of all options are recalculated, with the probabilities of the replaced options set to negative infinity.\n$Y_{mask} = \\{y_i | log P(y_i | q, x) < avg\\}$\n$arg \\max log P(Yi | q, X_{mask})$"}, {"title": "Experimental Results", "content": "AOLP outperforms other option scoring methods in overall performance, indicating that it is more effective in assisting LLMs with scoring when other options are considered as input. Generative methods exhibit significant instability, primarily due to different prompts and hyperparameters such as temperature, top-p significantly affect the generated text. Additionally, it is challenging to design a universal optimal answer extraction rule that applies to all scenarios. MCP demonstrates superior performance on 6/7 datasets across all baselines, suggesting that LLMs possess strong symbolic binding capabilities. The poor performance of EE indicates that it is not applicable to current LLMs. Our method achieves outstanding results across all datasets and LLMs (Figure 2), particularly when the PoEID is applied, where its performance is especially remarkable. We also observe that in datasets involving logical reasoning, such as the CC, Physics, our method show a significantly greater improvement compared to commonsense reasoning datasets (e.g. OpenBookQA). This aligns with people behavior when answering MCQs: when faced with complex reasoning, people often improve their accuracy through PoE.\nFurthermore, from a computational cost perspective, all baselines except for MCP require high computational costs. For instance, IE and option scoring methods require multiple forward inferences for each option. In contrast, our method and MCP only require a single forward inference, significantly reducing computational complexity. Therefore, our method not only outperforms others in terms of effectiveness but also offers higher computational efficiency."}, {"title": "The Effectiveness of Elimination Strategies", "content": "Although our method outperforms IE in overall performance, this does not imply that our elimination strategies are necessarily superior to IE's elimination strategy. The reason is that IE uses $log P(y_i | q, x)$ as the evaluation metric, while we employ $P(o_i | q, x)$ for assessment. Based on this, we improve IE and propose two new elimination strategies. Specifically, $IE_{new}$ uses $log P(o_i | q, x)$ as the evaluation metric.\n$Y_{mask} \\{y_i | log P(o_i | q, x) < avg\\}$\nOne of the new strategies is to replace the using [MASK] option in $IE_{new}$ with direct eliminating the options. We refer this strategy as Log.\n$Y_{eli} = \\{y_i | log P(o_i | q, x) < avg\\}$\nThe other is based on the three strategies mentioned in Section 3.3, which replace options with [MASK] instead of eliminating them. We refer this strategy as NoLog.\n$Y_{mask} = arg \\min P(o_i | q, x)$"}, {"title": "Enhancing LLM's Reasoning Ability with POEID", "content": "In the Figure 2, it can be observed that the PoEID significantly outperforms the other two strategies. Therefore, we hypothesize that the step-by-step reasoning method of sequential elimination can enhance the LLM's reasoning ability. To validate this hypothesis, we select two 5-options datasets Logical Deduction (LD) and Riddle Sense (RS)(Lin et al., 2021). We randomly sample 500 examples from each dataset. And we select Llama-3.1-8B, OLMO-2-1124-7B, Phi-3-medium-128k-instruct to conduct the experiment.\nIn the Figure 3, We observe that the PoEseq consistently shows an improvement in accuracy across both datasets. Specifically, as the number of eliminated options increases, the LLM's accuracy also improves. Furthermore, we find that the POEseq still performs better on logical reasoning (LD) compared to commonsense reasoning (RS). These results suggest that the PoEseq helps enhance LLM's performance by effectively narrowing the option space. This is in line with our hypothesis that step-by-step reasoning is beneficial for enhancing the LLM's reasoning ability."}, {"title": "The Causes of EE Failure", "content": "In an ideal scenario, the process by which the LLM selects the correct and incorrect answers should exhibit consistency. However, the poor performance in EE indicates that the LLM still lacks sufficient capability in choosing the incorrect options. That being said, we argue that this failure cannot be solely attributed to intrinsic factors of the LLM. As mentioned in Section 5.1, it is difficult to design a set of optimal answer extraction rules applicable to all scenarios. Based on this, we aim to further investigate the root causes of EE failures, to clarify whether they stem from external factors or from the inherent limitations of the LLM itself. We improve the EE and result in EEnew:\n$Incorrect option = arg \\max P(o_i | q, x)$\nWe select Falcon3-7B/10B, OLMo-2-1124-7B and conduct experiments on ARC-Challenge, OpenBookQA, MU.\nIn the table 2, We observe a significant decline in performance for the EEnew compared to the MCP. By combining the experimental results of EE, we speculate that this performance drop is primarily due to intrinsic factors of the LLM. Additionally, we found that in the EEnew, the accuracy further decreases when performing sequential elimination compared to when only selecting one incorrect option. This suggests that errors propagate throughout the step-by-step reasoning process. We believe above phenomenon is closely related to the LLM's training, which predominantly involves correct answers. As a result, there are fewer samples of incorrect options, limiting the LLM's ability to understand and identify error options."}, {"title": "PoE with Debias Methods", "content": "Zheng et al. (2023a) has shown that LLMs exhibit token bias when performing MCQs task, meaning that the LLMs tends to assign more probability mass to certain option ID token a prior when predicting the answer from the option IDs. Therefore, it is natural to consider whether the two debias methods (cyclic and pride) proposed in this study could be combined with the PoEID to further improve the LLM's performance. To confirm this, we select Llama-3.1-8B, Falcon3-7B/10B, Phi-3-medium-128k-instruct and conduct experiments on ARC-Challenge, OpenBookQA.\nSpecifically, for the cyclic method, in each round, we first obtain the corrected option ID probabilities based on the cyclic permutation. Then, elimination is performed according to these probabilities, continuing until only one option remains.\n$Yeli = arg \\min P_{cyclic} (O_i | q, x)$\nFor the pride method, in each round, we first obtain the probabilities of the original option IDs. Then, we correct these probabilities based on the prior probabilities. Finally, elimination is performed, continuing until only one option remains. Specially, we newly sample examples equivalent to 20% of the above datasets' size to compute the Prior.\n$Yeli = arg \\min P(o_i | q, x)/Prior(o_i)$"}, {"title": "Few-Shot", "content": "To further validate the generality and robustness of our method, we conduct additional experiments under different shot size settings beyond the zero-shot scenario, specifically considering few-shot settings with K = 1, K = 5, and K = 10. For comparison, we select two baseline methods: MCP and IE. MCP, which performs the best under the zero-shot settings, serves as a strong reference for comparison. IE is also chosen as a baseline to assess whether our method can still outperform it in the few-shot settings. We evaluate Llama-3.1-8B, Falcon3-10B and Phi-3-medium-128k-instruct on the ARC-Challenge and Physics datasets, and average the results across three random seeds (0, 1, and 2) to ensure the reliability of our findings.\nIn the Table 4, our experimental results demonstrate that our method continues to outperform the others. In contrast, the overall performance of the IE method remains significantly lower than both our method and MCP. These results reinforce the robustness of our method, demonstrating that it maintains superior performance even under different shot size conditions."}, {"title": "Conclusion", "content": "In this paper, we propose a novel PoE leveraging option IDs, which simplifies the reasoning process and significantly reduces computational overhead, offering a clear advantage over existing methods. Through zero-shot experiments on 7 publicly available datasets, we demonstrate that the our method can notably improve LLM's performance, particularly in tasks that require logical reasoning. Furthermore, we conduct a systematic analysis of various elimination strategies, with the results indicating that the PoEseq has a clear advantage in answering MCQs. It not only enhances the LLM's reasoning capabilities but also performs well in few-shot settings. When combined with debias methods, it further improves the overall performance of the LLM. Future work will focus on optimizing the method and exploring its generalization ability across a wider range of tasks."}, {"title": "Limitations", "content": "Our study has two main limitations. First, we employ fixed prompts without optimizing them, which may constrain the LLM's performance on specific tasks. Second, we don't explore whether incorporating more complex prompting methods, such as Chain of Thought, could further enhance the LLM's reasoning abilities and performance. Future research could focus on optimizing these two aspects."}]}