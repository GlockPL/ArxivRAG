{"title": "Performance Evaluation of Large Language Models in Statistical Programming", "authors": ["Xinyi Song", "Kexin Xie", "Lina Lee", "Ruizhe Chen", "Jared M. Clark", "Hao He", "Haoran He", "Jie Min", "Xinlei Zhang", "Simin Zheng", "Zhiyang Zhang", "Xinwei Deng", "Yili Hong"], "abstract": "The programming capabilities of large language models (LLMs) have revolutionized automatic code generation and opened new avenues for automatic statistical analysis. However, the validity and quality of these generated codes need to be systematically evaluated before they can be widely adopted. Despite their growing prominence, a comprehensive evaluation of statistical code generated by LLMs remains scarce in the literature. In this paper, we assess the performance of LLMs, including two versions of ChatGPT and one version of Llama, in the domain of SAS programming for statistical analysis. Our study utilizes a set of statistical analysis tasks encompassing diverse statistical topics and datasets. Each task includes a problem description, dataset information, and human-verified SAS code. We conduct a comprehensive assessment of the quality of SAS code generated by LLMs through human expert evaluation based on correctness, effectiveness, readability, executability, and the accuracy of output results. The analysis of rating scores reveals that while LLMs demonstrate usefulness in generating syntactically correct code, they struggle with tasks requiring deep domain understanding and may produce redundant or incorrect results. This study offers valuable insights into the capabilities and limitations of LLMs in statistical programming, providing guidance for future advancements in AI-assisted coding systems for statistical analysis.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Background and Motivation", "content": "Artificial intelligence (AI) has become increasingly important in many areas, and its growth continues at an unprecedented pace. In recent years, the rise of large language models (LLMs) has led to significant advancements in AI-generated coding tools such as ChatGPT (OpenAI et al. 2024) and Llama (Touvron et al. 2023). For instance, LLMs can generate statistical code in SAS (SAS Institute Inc. 2025) and R (R Development Core Team 2025). These exciting developments offer the potential to reduce manual coding efforts, enhance accessibility to statistical analysis for non-statisticians, and potentially pave the way toward fully automated statistical analysis.\nWhile LLMs can generate codes based on their generative models, the validity and quality of those generated code require thorough investigation, which is particularly important for non-expert users who lack expertise in statistical programming. Non-expert users may find it challenging to evaluate the correctness and reliability of AI-generated code and its outputs. Therefore, it is important to establish a systematic evaluation framework to investigate the quality of AI-generated code and ensure the results are reliable before using them in the workplace. This motivates us to evaluate the performance of LLMs in statistical programming by leverage the human evaluation from domain experts.\nHuman evaluation is the essential starting point for this effort. Although automatic metrics, such as those commonly used in natural language processing (NLP), can be employed to assess generated code, their applicability to statistical programming remains uncertain. Concerns persist about the reliability and trustworthiness of these metrics in this specific context. Consequently, human evaluation can serve as the gold standard for assessing the quality of generated statistical code.\nHuman evaluation of generated code is essential but comes with its challenges. The primary challenge is the significant demand for expert hours, as code can be complex and difficult to assess. We selected SAS as the starting point for human evaluation. SAS programming is a powerful tool for data management and analysis, widely used in industries such as finance and pharmaceutical industry. Its well-structured syntax facilitates evaluation. However, writing SAS code and interpreting its results require a deep understanding of programming syntax and statistical models, which can be challenging for those without substantial expertise. In this study, considerable human labor hours have been dedicated to expert evaluation of statistical code generated by LLMs, marking a pioneering effort in the literature. Specifically, more than 10 participants, consisting of statistics faculty and PhD candidates, have contributed to the evaluation, devoting hundreds of hours to the study."}, {"title": "1.2 Related Literature and Contribution of This Work", "content": "Generative AI has profoundly influenced the field of programming by enabling automated code generation and assisting with complex tasks. Early approaches to code generation leveraged sequence-to-sequence models to translate problem descriptions in natural language into executable code. Ling et al. (2016) introduced latent predictor networks to map text to code, while Yin and Neubig (2017) proposed a syntactic neural network model that constructs abstract syntax trees, ensuring syntactic correctness. The transformer architecture, presented by Vaswani et al. (2017), revolutionized NLP by effectively capturing long-range dependencies. Building on this foundation, GPT 2.0 and GPT 3.0, pre-trained on extensive web corpora, achieved remarkable performance across diverse NLP tasks (Brown et al. 2020), paving the way for the development of LLMs.\nLLMs, such as ChatGPT, have the capability to automatically generate code in various programming languages using natural language prompts (Mollick 2022; Idrisov and Schlippe 2024). A significant milestone in AI code generation was the development of OpenAI Codex, an LLM specifically trained on code datasets. Built upon Codex, GitHub Copilot serves as an AI-powered coding assistant, providing code suggestions for various tasks. Additionally, Meta AI introduced the Llama family of models, trained on extensive text datasets, followed by the release of Code Llama (Rozi\u00e8re et al. 2023), which is fine-tuned on code-specific data to enhance its proficiency in code generation. These LLMs are widely used and significantly enhance programming efficiency."}, {"title": "1.3 Overview", "content": "The overarching goal of this paper is to evaluate the performance of LLMs in statistical programming, specifically in the context of SAS coding, through human expert assessments. We investigate the performance of three LLMs: GPT 3.5, GPT 4.0, and Llama 3.1 70B. To begin, we manually curated a set of 207 statistical analysis tasks, each consisting of a problem description, dataset details, and human-verified SAS code. These tasks were then input into the LLMs, which generated SAS code for each task. Human experts evaluated the generated code based on various criteria, including correctness, effectiveness, readability, executability, and the accuracy of the output results. Statistical analyses were conducted on the evaluation scores to address research questions regarding the performance of LLMs in statistical programming. The specific objectives of this study are as follows: 1) provide an overall summary of the performance of LLMs in statistical programming with SAS; 2) identify the strengths and weaknesses of LLMs; 3) compare the performance of different LLMs (i.e., ChatGPT versus Llama) and examine whether the version of the LLM makes a difference (i.e., ChatGPT 3.5 versus 4.0); and 4) explore potential areas for improvement in automatic statistical analysis."}, {"title": "2 Study Design and Data Collection", "content": ""}, {"title": "2.1 Overview of The Study Design", "content": "We first provide an overview of the study design under the context of the SAS programming because of its well-structured syntax. We assess the performance of three LLMs, GPT 3.5, GPT 4.0, and Llama 3.1 70B, as introduced in Section 2.3. A set of 207 statistical analysis tasks was compiled, and the LLMs were tasked with generating SAS code to address these tasks. The generated code was evaluated based on three groups of criteria: code quality, executability, and output quality, detailed in Section 2.4. The study design is illustrated in Figure 1, which includes the evaluation setup (a) and the rating procedure (b). Detailed explanations of the steps shown in Figure 1 are provided in the following sections."}, {"title": "2.2 Statistical Analysis Tasks", "content": "To evaluate the capability of LLMs in generating code, we designed a series of statistical analysis tasks. The research team manually collected datasets and statistical analysis tasks from various online public sources. These tasks cover a wide range of basic statistical analyses across various fields, including biology, medicine, engineering, and social sciences. In total, 65 datasets (in CSV format) were gathered along with their descriptions. From these datasets, 207 statistical analysis tasks were compiled, forming a problem description and obtaining"}, {"title": "2.3 Large Language Models", "content": "LLMs are complex deep learning networks pre-trained on vast amounts of data, enabling them to understand and generate human-like text. For each individual statistical analysis task, the LLM can generate the SAS code based on the problem description, data description, and the associated data. The evaluation procedure in this study primarily involves two versions of ChatGPT (i.e., 3.5 and 4.0) developed by OpenAI (OpenAI et al. 2024), and one version of Llama (i.e., 3.1 70B) developed by Meta AI (Touvron et al. 2023). We give a brief introduction to those LLMs in the sections to follow."}, {"title": "2.3.1 ChatGPT", "content": "ChatGPT, short for Chat Generative Pre-trained Transformer, is a conversational AI system developed by OpenAI. ChatGPT can assist users with various tasks by engaging in natural language interactions. Built on a generative pre-trained transformer architecture and trained on vast amounts of data, ChatGPT can engage in dynamic conversations, answer questions, and aid in problem-solving. In coding and programming, ChatGPT has evolved through multiple versions, each bringing improved problem-solving abilities. GPT-3.5, released in March 2022, utilizes a transformer model with dense attention layers within an LLM framework. It is particularly effective for quick code generation and debugging in"}, {"title": "2.3.2 Llama", "content": "The Llama series (Large Language Model Meta AI), developed by Meta AI, consists of open-source LLMs designed for various NLP tasks, including code generation. Llama 1, released in February 2023, was Meta's first open-access LLM. Its successor, Llama 2, released in July 2023, is fully open-source and widely used for specialized NLP tasks. Rozi\u00e8re et al. (2023) introduced Code Llama, an open-source language model built on Llama 2. It outperforms previous open-source models and is competitive with proprietary models like OpenAI's Codex on several code generation benchmarks. However, challenges remain, such as complex context understanding, dependency on prompt quality (Chen et al. 2021), and potential security vulnerabilities (Wong et al. 2023).\nLlama 3.1 is designed to enhance capabilities in multilingual understanding, coding, reasoning, and tool usage (Dubey et al. 2024). It is available in multiple versions: Llama 3.1 8B, Llama 3.1 70B, and Llama 3.1 405B, containing 8 billion, 70 billion, and 405 billion parameters, respectively. Each version is trained with different parameter counts and context lengths, impacting the quality of generated code across various tasks. Llama 3.1 8B is suitable for moderate code generation tasks, Llama 3.1 70B offers higher accuracy for more complex analytical tasks, and Llama 3.1 405B is ideal for advanced coding scenarios requiring deep contextual understanding or multi-task programming. This study focuses on evaluating the performance of Llama 3.1 70B for statistical programming tasks."}, {"title": "2.4 Evaluation Criteria", "content": "For the LLM-generated code, our evaluation aims to provide a comprehensive assessment on programming quality and effectiveness. Specifically, the evaluation criteria consists of three major assessment components: Group 1, Code Correctness and Readability; Group 2, Executability; and Group 3, Output Correctness and Quality. A total of 10 questions (criteria) are designed to evaluate these components, with each criterion worths 5 points, leading to a maximum score of 50 points. The detailed questionnaire for raters is provided in Appendix A.\nEach criterion is assessed using a consistent five-point rating scale, ranging from \u201cStrongly Disagree\" (1) to \u201cStrongly Agree\" (5), ensuring a standardized approach to evaluating various aspects of the code. Zero points may also be awarded if no output is generated. This comprehensive evaluation method guarantees a systematic and adaptable process suitable for diverse coding scenarios.\nThe first group, code correctness and readability, focuses on code quality and it has five questions with a total of 25 points. In this category, raters examine the SAS code to assess the technical accuracy and correctness of the data step and model structure. Specific aspects such as dataset names, variable names, model options, and output options are verified for accuracy. Raters also evaluate whether the code is well-organized, logically structured, and easy to follow. Additionally, the assessment consider whether the code avoids unnecessary repetition in logic, variable declarations, and output generation, thus minimizing redundancy.\nThe second group, code executability, consists of two questions worth a total of 10 points. This section evaluates whether the code generated by the LLM can execute successfully without producing errors or warning messages. If the code runs flawlessly, it automatically receives the full 10 points, bypassing further evaluation in this category.\nThe third group, output correctness and quality, evaluates output quality through three questions, totaling 15 points. Raters determine whether the output generated by the SAS code includes the necessary and accurate results for the statistical problem. The outputs must be clear, concise, and directly address the statistical analysis task. Additionally, the outputs should be free from duplicated results or excessive verbosity that does not contribute meaningful insights."}, {"title": "2.5 Rating Scores", "content": "As illustrated in Figure 1(a), for each LLM, we provide a statistical analysis task consisting of a problem description and a data description, and request LLM to generate SAS code for the analysis. The rating scores are collected through a structured and systematic process as illustrated in Figure 1(b). This process involves nine raters, denoted as {R\u2081, R\u2082, . . ., R\u2089}, who are selected based on their prior experience with SAS coding and statistical analy-"}, {"title": "3 Analysis of Rating Scores and Results", "content": "For each answer (i.e., SAS code) generated by an LLM and for each criterion, three raters are assigned to evaluate it with a score. The average of the three scores is then calculated and used for analysis. Using the notation in (1), the criterion level score is computed as,\n$\\begin{aligned}\n\\overline{x}_{i, k}^{M} &=\\frac{1}{3} \\sum_{j=1}^{3} x_{i, j, k}^{M} \t k=1,2,3,4,5, \\\\\n\\overline{x}_{i, k}^{M} &=\\frac{1}{3} \\sum_{j=4}^{6} x_{i, j, k}^{M} \t k=6,7, \\text { and } \\\\ \\overline{x}_{i, k}^{M} &=\\frac{1}{3} \\sum_{j=7}^{9} x_{i, j, k}^{M} \t k=8,9,10,\n\\end{aligned}$\nfor a given model M and criterion Qk. The group level score is computed as,\n$\\begin{aligned}\nx_{i, G 1}^{M} &=\\sum_{k=1}^{5} \\overline{x}_{i, k}^{M}, \t x_{i, G 2}^{M}=\\sum_{k=6}^{7} \\overline{x}_{i, k}^{M}, \\text { and } x_{i, G 3}^{M}=\\sum_{k=8}^{10} \\overline{x}_{i, k}^{M}\n\\end{aligned}$\nwhere i = 1, . . ., 207, M \u2208 {M\u2081, M\u2082, M\u2083}. The total score is computed as,\n$\\begin{aligned}\nx_{i}^{M} &=x_{i, G 1}^{M}+x_{i, G 2}^{M}+x_{i, G 3}^{M}, \t i=1, \\ldots, 207, M \\in\\left\\{M_{1}, M_{2}, M_{3}\\right\\} .\n\\end{aligned}$\nIn the following sections, we will perform data analysis for the total score, group score, and criterion score, respectively. Additionally, we will assess rater variability."}, {"title": "3.1 Total Score Analysis", "content": "In this section, we conduct a statistical analysis of the total score to evaluate the overall performance of the LLMs in statistical programming. Using the total score defined in (4), the average total score across the three LLMs and all 207 tasks is 37.387 out of 50 (74.774%) with a standard deviation (SD) of 10.977. Figure 3(a) visualizes the mean total scores with error bars representing the SD for each of the three LLMs. The results indicate that the three"}, {"title": "3.2 Group Score Analysis", "content": "As mentioned earlier, the 10 rating criteria are categorized into three groups: code quality, code executability, and output quality. This section analyzes the rating scores for each group separately. In summary, for Group 1, the mean score is 23.513 out of 25 (94.052%) with an SD of 1.309; for Group 2, the mean is 6.120 out of 10 (61.202%) with an SD of 4.423; and for Group 3, the mean is 7.753 out of 15 (51.690%) with an SD of 6.290. Figure 4 displays the mean group scores with error bars representing the SD. Note that the score ranges differ across the groups.\nFrom these summary statistics, we observe that LLMs perform well in Group 1, indicating that they can generate SAS code that appears to be correct to raters most of the time, with an average score of 94.052%. However, the average score drops significantly when the code is"}, {"title": "3.3 Individual Criterion Score Analysis", "content": "In this section, we examine the individual criterion scores across three LLMs. A total of 10 criteria are evaluated. Radar plots are used to visualize the mean and SD of these scores. Figure 5 displays the radar plot of the mean criterion scores for the three LLMs, while Figure 6 shows the radar plot of the SD of these scores. The distributions of the individual criterion scores are provided in Figure 8 in Appendix B.\nFrom the mean radar plot, Q5: code conciseness and Q8: output correctness emerge as key contributors to the group and total scores. Interestingly, Llama tends to provide multiple solutions, which, while leading to redundancy, also increases its chances of delivering correct answers. For code executability, GPT 4.0 performs best in both Q6: variable/dataset errors and Q7: other errors/warnings. The SD radar plot reveals high variability in the individual criteria within Groups 2 and 3, highlighting the challenge of achieving consistent performance in executability and output quality.\nSimilarly, we perform formal regression modeling and conduct statistical tests for individual criterion scores. For a given criterion k, the following regression model is fitted to the"}, {"title": "3.4 Rater Variability Assessment", "content": "As mentioned earlier, each rater blindly reviews the code and outputs from each model and assigns scores based on predefined rating rubrics. For each attribute, three raters are involved in the evaluation process independently. However, challenges arise due to potential differences in rater \u201cbaselines\" or biases. For instance, one rater might be more lenient, consistently giving higher scores, while another could be more critical and generally assign lower scores. Therefore, it is crucial to assess rater variability to account for potential biases and ensure a fair evaluation process.\nIn literature, random effects models have been commonly used to assess rater variability. We use the raw score $x_{i,j,k}^{M_l}$ as shown in Figure 1 and consider the following random effects model,\n$x_{i, j, k}^{M_{l}} = \\beta_0 + \\beta_1 z_{1ijkl} + \\beta_2 z_{2ijkl} + \\gamma_1 c_{1ijkl} + \\cdots + \\gamma_9 c_{9ijkl} + w_j + \\epsilon_{ijkl}.$\nHere, we treat the LLM and individual criterion as categorical variables in the fixed effects, with coefficients \u03b2's and \u03b3's and the corresponding dummy variables z\u1d62\u2c7c\u2096\u2097's and c\u1d62\u2c7c\u2096\u2097's. The raters are modeled as random effects, denoted by w\u2c7c. The random effect w\u2c7c ~ N(0, \u03c3_w^2), while the error term \u03b5\u1d62\u2c7c\u2096\u2097 ~ N(0,\u03c3\u00b2). This model allows each rater j to have their own intercept, effectively capturing potential rater-to-rater variability.\nThe model is fitted using the R package lme4 (Bates et al. 2015) and statistical tests were done with the R package lmerTest (Kuznetsova et al. 2017). Table 4 presents the parameter estimates for the random effects model, based on all individual criterion scores from all raters. The estimated variance for the random effect is 0.0014. In comparison, the estimated variance for the error term is 2.8027. The negligible variance of the random effect indicates that rater variability is minimal compared to the variability in the error term, demonstrating a high level of consistency among the raters."}, {"title": "3.5 Rater Comments and Areas for Improvement", "content": "During the rating process, each rater was asked to summarize the problems they encountered. These insights provide valuable feedback on areas where LLMs can be improved. For Group 1, LLMs sometimes produce excessive code by offering multiple solutions for the same problem or including unnecessary steps beyond the problem's scope. This issue is particularly noticeable in Llama. The generated code is occasionally overly complex and poorly organized, using unnecessarily complicated proc steps. Some SAS code includes incorrect model or output options, leading to incompatibility with proc steps or failure to produce the required results. Models sometimes use incorrect dataset and variable names. Although rare, the models occasionally misunderstand the problem entirely, generating incorrect code."}, {"title": "4 Conclusions", "content": "In this paper, we propose a systematic framework to evaluate the performance of LLMs in statistical programming, with a focus on SAS programming. We developed a comprehensive set of evaluation criteria and invested significant human effort in the rating process. Our"}]}