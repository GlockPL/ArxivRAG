{"title": "SciCode: A Research Coding Benchmark Curated by Scientists", "authors": ["Minyang Tian", "Luyu Gao", "Shizhuo Dylan Zhang", "Xinan Chen", "Cunwei Fan", "Xuefei Guo", "Roland Haas", "Pan Ji", "Kittithat Krongchon", "Yao Li", "Shengyan Liu", "Di Luo", "Yutao Ma", "Hao Tong", "Kha Trinh", "Chenyu Tian", "Zihan Wang", "Bohao Wu", "Yanyu Xiong", "Shengzhu Yin", "Minhui Zhu", "Kilian Lieret", "Yanxin Lu", "Genglin Liu", "Yufeng Du", "Tianhua Tao", "Ofir Press", "Jamie Callan", "Eliu Huerta", "Hao Peng"], "abstract": "Since language models (LMs) now outperform average humans on many challenging tasks, it is becoming increasingly difficult to develop challenging, high-quality, and realistic evaluations. We address this by examining LM capabilities to generate code for solving real scientific research problems. Incorporating input from scientists and AI researchers in 16 diverse natural science sub-fields, including mathematics, physics, chemistry, biology, and materials science, we create a scientist-curated coding benchmark, SciCode. The problems naturally factorize into multiple subproblems, each involving knowledge recall, reasoning, and code synthesis. In total, SciCode contains 338 subproblems decomposed from 80 challenging main problems, and it offers optional descriptions specifying useful scientific background information and scientist-annotated gold-standard solutions and test cases for evaluation. Claude3.5-Sonnet, the best-performing model among those tested, can solve only 4.6% of the problems in the most realistic setting. We believe that SciCode demonstrates both contemporary LMs' progress towards realizing helpful scientific assistants and sheds light on the building and evaluation of scientific AI in the future.", "sections": [{"title": "Introduction", "content": "The development of evaluations in tandem with language models (LMs) has substantially contributed to the rapid advancement of these models [29, 11, 7, 25, 81, 27, 72]. Because LMs now surpass the performance of most humans except domain experts, evaluating them becomes increasingly challenging. Many established benchmarks struggle to keep pace with the advancements in LM performance and have quickly become saturated [91, 14, 70, 58], leading to discrepancies between the models' perceived and actual capabilities [36]. As a consequence, researchers are developing synthetic challenging benchmarks, often involving models in the construction of evaluation instances. For example, some subsample instances from existing benchmarks that cannot be solved by current models [93, 82], or augment them to construct more challenging evaluations [21, 44, 49]. However, it is unclear whether such efforts accurately reflect real-world applications and the models' performance in practical scenarios. Realistic, high-quality, and challenging evaluations are crucial for the continued advancement of LMs."}, {"title": "SciCode", "content": "This section examines the design principles and annotation process we chose for SciCode, describing: research-level coding problems from various natural science fields (\u00a72.1); how we decomposed main problems into multiple, simpler subproblems (\u00a72.2); our design choices for the annotation process (\u00a72.3); and various evaluation setups that SciCode facilitates (\u00a72.4)."}, {"title": "Challenging and Realistic Scientific Coding Problems", "content": "SciCode sources challenging and realistic research-level coding problems across natural science disciplines, including mathematics, physics, chemistry, biology, and material science, covering a total of 16 subfields. This diverse selection ensures a comprehensive representation of the natural sciences, where extensive code development is essential.\nSciCode is mainly drawn from the scripts that scientists use in their everyday workflow. Many of these have been used in one or more publications, demonstrating their robustness and correctness. However, they are primarily for internal use, which means that they are seldomly open-sourced and often poorly annotated. Consequently, unlike general-domain coding problems, natural science problems have less exposure in most current LMs' training data. This offers a unique opportunity to evaluate the models' ability to generalize to less familiar contexts. In total, SciCode consists of 80 main problems, decomposed into 338 subproblems."}, {"title": "A Main Problem with Multiple Subproblems", "content": "In their everyday workflow, scientists often decompose a complex problem into multiple smaller, more manageable parts. They may write relatively independent code for each part and then integrate these parts into a complete solution to the main problem. In developing our dataset, we leverage this natural and intuitive structure and further standardize our dataset by instructing the scientists to adhere to the following format.\nMain Problem A main problem is a primary task that needs to be addressed. It defines the overall objective of the research and guides the direction of the study. The main problem encompasses all subproblems, with detailed instructions on required inputs and expected outputs articulated in a docstring block. With the main problem defined, scientists have sufficient guidance to solve the task.\nSubproblem Decomposition Subproblems focus on questions derived from the main problem. They decompose the complex main problem into smaller, more manageable parts, enabling a more detailed and systematic investigation. Detailed docstrings for each subproblem describe the required input and expected output, ensuring clarity and aiding in accurate code generation. This structured decomposition simplifies problem-solving and facilitates a more granular evaluation of the models' scientific coding capabilities."}, {"title": "Data Annotation", "content": "This process consists of three main stages:\n(1) Problem selection: Deciding on question topics related to the research domain (\u00a72.3.1).\n(2) Evaluation design: Designing both numerical and domain-specific test cases to ensure the problem's validity (\u00a72.3.2).\n(3) Problem validation: Iterating on the problems through three rounds of revisions to further enhance question design (\u00a72.3.3).\nWe now examine the design choices for each stage."}, {"title": "Problem Selection", "content": "Throughout the research project cycle, various coding needs arise, such as data processing, fitting, and plotting. To use SciCode, scientists select the problems that require intense scientific knowledge and reasoning to optimally test LM's science capability. This approach ensures that both the breadth and depth of frontier research are addressed. We focus on:\n\u2022 Numerical methods. Analytical forms are usually impossible to achieve for very com-plicated systems. Therefore, scientists must derive numerical models and algorithms that describe physical phenomena [9, 74, 35, 77, 32], chemical reactions [22, 23, 87], biological systems [96, 97, 95, 83, 16, 15, 51, 89], or statistical behaviors[79, 69, 57, 55, 50, 24, 47].\n\u2022 Simulation of systems. In fields of natural science, scientists write code to simulate systems and processes. These simulations are based on theoretical principles and empirical data,"}, {"title": "Evaluation Design", "content": "To facilitate evaluation, we have scientist annotators use only widely adopted and well-documented packages such as NumPy, SciPy, and SymPy when writing the solution code for their problems, as shown in Figure 4.\nOur test suite involves two key components. (1) Numerical tests list input-output pairs to check if the generated code produces the same outputs as ground truth. (2) Domain-specific test cases, introduced as an additional stage, evaluate whether model-generated solutions align with scientists' practical needs and further ensure the correctness and applicability of each solution within its specific field. These tests are extracted from real scientific workflows: scientists must design domain-specific test cases to verify code accuracy by reproducing results published in academic papers or matching analytical solutions derived from theoretical models. For example, we reproduce the phase transition at around kT/J = 2.269 for the 2D square Ising model problem [63], derive the surface plasmon mode in a 2D layered electron gas [10, 32], verify the ballistic Brownian motion in optical tweezer [46], etc. By doing so, we validate that the code not only functions correctly but also accurately represents the underlying scientific problem.\nOverall, the evaluation design aims to balance the fidelity of the scientific problem with the practicality of the evaluation process, ensuring that the solutions are both accurate and accessible."}, {"title": "Problem Validation for Quality Control", "content": "We conduct three rounds of validation and revision for each problem:\n(1) In-domain scientist validation. At least two scientists in the same research domain cross-check the question design, solution code, and domain-specific test cases, providing detailed feedback. The scientists who design the workflows iterate on them based on this feedback to ensure the problems are scientifically accurate.\n(2) Out-of-domain scientist validation. One scientist from a different domain reviews the question design to ensure it is clear and that the information provided is precise and sufficient to solve the problem (e.g., all scientific constants are given). This helps to identify any assumptions that might be unclear to those outside the immediate field of study.\n(3) GPT-4 validation. GPT-4 assists with the final review round. The previously validated sub-questions are input to GPT-4 to generate code solutions. Scientists perform error analysis for the generated solutions and redesign the numerical test cases if necessary to prevent false positives.Based on the code solutions from GPT-4, the scientist may also revise the entire workflow a third time to addressany potential ambiguity.\nThis multi-round validation approach ensures that the problems are scientifically rigorous, clear, and unambiguous, facilitating accurate and effective evaluation."}, {"title": "Various Types of Evaluations", "content": "SciCode offers unique opportunities for evaluating LMs across diverse settings, comprehensively testing their coding capabilities."}, {"title": "Experiments", "content": "Prompts. We evaluate our model using zero-shot prompts. We keep the prompts general and design different ones for different evaluation setups only to inform the model about the tasks. We keep prompts the same across models and fields, and they contain the model's main and sub-problem instructions and code for previous subproblems. We also instruct the model to recall useful knowledge when gold background knowledge is not provided. \u00a7A.1 presents an example."}, {"title": "Evaluated Models", "content": "Since SciCode is a challenging benchmark, we mainly consider strong language models.\n\u2022 GPT-40 [65]: An optimized version of GPT-4 [64] by OpenAI with multi-modal capability.\n\u2022 GPT-4-Turbo: A faster and more cost-effective variant of GPT-4 [64]. We use the 'gpt-4-turbo-2024-04-09' snapshot.\n\u2022 Claude3.5-Sonnet [4]: The latest model from the Claude 3.5 family from Anthropic.\n\u2022 Claude3-Opus [3]: The most capable model from the Claude 3 family from Anthropic.\n\u2022 Claude3-Sonnet [3]: The second most capable model from the Claude 3 family.\n\u2022 Gemini 1.5 Pro [85]: A model from the Gemini 1.5 family by Google and the largest with open access at the time of writing.\n\u2022 Llama-3-70B-Instruct [2]: The instruction-tuned version of the largest available model from the Llama-3 family.\n\u2022 Mixtral-8x22B-Instruct [33]: The instruction-tuned version of Mistral AI's largest publicly accessible Mixture-of-Expert Model.\n\u2022 Deepseek-Coder-v2 [102]: Mixture-of-Experts (MoE) code language model continue pre-trained on DeepSeek-V2\n\u2022 Qwen2-72B-Instruct [86]: The largest instruction-tuned Qwen-2 model."}, {"title": "Main Results", "content": "Table 2 presents results under the standard setup. For the easier subproblem-level evaluation, the state-of-the-art models we test solve 14%-26% of the subproblems. Among them, Claude3.5-Sonnet achieves the best performance, with a 26.0% pass@1 rate. However, all models perform much worse on the more realistic and challenging main problem evaluation. Claude3.5-Sonnet still performs the best in this setting, but with only a 4.6% pass@1 rate.\nThese results show that SciCode is a difficult benchmark for current LMs. Consistent with our observations on proprietary models, open-weight LMs under test also showed their lack of capabilities in solving any main problem despite being able to solve a number of sub-problems correctly."}, {"title": "Additional Results with Other Evaluation Settings", "content": "Providing gold scientific background knowledge. Table 3 presents results when background text authored by scientists is provided to the LMs and generated solutions to previous subproblems are used. This setting evaluates both the models' capabilities to faithfully follow the instructions provided in the background as well as their code-generation performance. The \u0394 columns indicate performance differences compared to the standard setup.\nAll models substantially improve performance for both subproblem and main problem evaluations when given scientific background knowledge. For the subproblem evaluation, Claude3.5-Sonnet and GPT-40 perform the best, both with a 35.4% pass@1 rate. GPT-4-Turbo benefits the most from the provided scientific background and reasoning with an increase of 10.8%. Open models improve less compared to proprietary models which might indicate weaker Instruction following capability. Interestingly, the comparison between Llama-3-70B-Instruct and Mixtral-8x22B-Instruct reveals a trend that differs from the standard setup: Llama-3-70B-Instruct benefits more from the scientific background knowledge and reaches the performance of Mixtral-8x22B-Instruct in this setting.\nFor the main problem evaluation, the trend remains similar to the standard setup. Claude3.5-Sonnet performs best, with a 12.3% pass@1 rate, followed closely by GPT-40 and GPT-4-Turbo at 9.2%. GPT-40, GPT-4-Turbo, and Claude3.5-Sonnet improve most from background content, at 7.7%. Nonetheless, all models still fall short of satisfactory performance even with the background knowledge provided. This reaffirms that SciCode is challenging even when focusing on code generation rather than testing the models' scientific knowledge."}, {"title": "Related Work", "content": "Language models for code. Code has long been an active field of research, and code LMs have co-evolved with foundation LMs since the era of BERT [17]. Earlier works include CodeBert [19] and CodeT5 [94], while Codex [12] arguably kick-started the LLM era for code-generation models. Since Codex, the field has experienced rapid growth in quantity and quality of large code generation models, including specially trained models like Codegen [60], StarCoder models [45, 52], and generalist models with code adapation [12] such as CodeLlama [75], CodeQwen [8], and DeepSeek-Coder [25]. As code generation gains more attention and becomes increasingly useful, contemporary generalist models often include non-trivial coding capabilities [66, 85].\nEvaluating code generation. Before the emergence of very capable code synthesis models, when most models struggled to produce executable code, datasets like CoNaLa typically included n-gram-based metrics [100]. Soon after model capabilities improved, execution-based evaluation gained in popularity [28, 7, 11]. While n-gram or general text-based evaluation still exists, we opted to omit them from SciCode due to obvious limitations of surface form matching in scientific coding.\nCode generation benchmarks now take various forms. For simple function completion, MBPP [7] and HumanEval [11] are two widely used benchmarks that contain basic programming questions, mainly evaluating LMs' ability to turn natural language instructions into Python programs. Other benchmarks assess the models' competence in real-world programming scenarios, such as writing data science code [42, 101], repository-level code completion [18], and more complex tasks in real-world software engineering [34]. Though our work is similar to MTPB [60] in terms of using a multi-turn setup, our subproblem instructions correspond to a high-level task, while theirs correspond to specific code actions (e.g., replace X with Y in the string).\nLanguage models for science. Scientific tasks are complex due to their demands for reasoning and knowledge. However, Recent advances in general and specialized language models have revolutionized the processing of text and other data modalities, such as molecules and proteins, in scientific fields. Galactica [84], a general-purpose scientific model, can perform tasks like citation prediction, scientific reasoning, document generation, and molecular property prediction. Many models focus on one single domain or task, like math (e.g., Minerva [43] and Deepseek-Math [78] ), protein structure prediction (e.g., ESM-2 [48]), medical reasoning (e.g., Med-PaLM [80], BioGPT [53]), and others."}, {"title": "Conclusion", "content": "We introduce SciCode, a scientific research benchmark curated by professional natural scientists. We designed SciCode for scientific problem evaluation and collected problems representing 16 diverse domains. By assessing SciCode with ten contemporary state-of-the-art AI models, we demonstrated that our benchmark is within reach but remains very challenging. We believe SciCode will serve as a helpful guideline for building future code language models for varied scientific applications."}, {"title": "Appendix", "content": ""}, {"title": "Prompt", "content": "PROBLEM DESCRIPTION:\nYou will be provided with problem steps along with background knowledge necessary for solving the problem. Your task will be to develop a Python solution focused on the next step of the problem-solving process.\nPROBLEM STEPS AND FUNCTION CODE:\nHere, you'll find the Python code for the initial steps of the problem-solving process. This code is integral to building the solution.\n{problem_steps_str}\nNEXT STEP - PROBLEM STEP AND FUNCTION HEADER:\nThis part will describe the next step in the problem-solving process. A function header will be provided, and your task is to develop the Python code for this next step based on the provided description and function header.\n{next_step_str}\nDEPENDENCIES:\nUse only the following dependencies in your solution. Do not include these dependencies at the beginning of your code.\n{dependencies}\nRESPONSE GUIDELINES:\n1. Start with the scientific background required for the next step, formatted as a comment.\n2. Then write the complete and executable Python program for the next step in a single block.\n3. Your response should focus exclusively on implementing the solution for the next step, adhering closely to the specified function header and the context provided by the initial steps.\n4. DO NOT include previous function code, example usage or test code in your response.\n5. Ensure your response is in the format of \u201cpython\u201c and includes the necessary background as a comment at the top.\nExample:\nBackground: [Here, insert the necessary scientific knowledge required for the next step.]\n[Insert the Python code here based on the provided function header and dependencies.]"}, {"title": "SciCode Full Problem Example", "content": ""}, {"title": "Example Main Problem", "content": "1. Generate an array of Chern numbers for the Haldane model on a hexagonal lattice by sweeping the following parameters: the on-site energy to next-nearest-neighbor coupling constant ratio (m/t) and the phase (4) values. Given the lattice spacing a, the nearest-neighbor coupling constant t\u2081, the next-nearest-neighbor coupling constant t\u2082, the grid size d for discretizing the Brillouin zone in the k\u2081 and k\u2081, directions (assuming the grid sizes are the same in both directions), and the number of sweeping grid points N for m/t\u2082 and \u03c6."}, {"title": "Example Subproblems", "content": ""}, {"title": "Question", "content": "1.1 Write a Haldane model Hamiltonian on a hexagonal lattice, given the following parameters: wavevector components k\u2081 and k\u2081, (momentum) in the x and y directions, lattice spacing a, nearest-neighbor coupling constant t\u2081, next-nearest-neighbor coupling constant t\u2082, phase \u03c6 for the next-nearest-neighbor hopping, and the on-site energy m."}, {"title": "Background", "content": "We denote {a} are the vectors from a B site to its three nearest-neighbor A sites, and {b} are next-nearest-neighbor distance vectors, then we have\na\u2081 = (0, a),\na\u2082 =\n$\\begin{pmatrix}\n\\frac{\\sqrt{3}a}{2} \\\\\n\\frac{a}{2}\n\\end{pmatrix}$,\na\u2083 =\n$\\begin{pmatrix}\n\\frac{\\sqrt{3}a}{2} \\\\\n-\\frac{a}{2}\n\\end{pmatrix}$,\nb\u2081 = a\u2082 - a\u2081 = \n$\\begin{pmatrix}\n\\sqrt{3}a \\\\\n-\\frac{3a}{2}\n\\end{pmatrix}$,\nb\u2082 = a\u2083 - a\u2081 = (\\sqrt{3}a, 0),\nb\u2083 = a\u2081 - a\u2082 = \n$\\begin{pmatrix}\n\\frac{\\sqrt{3}a}{2} \\\\\n\\frac{3a}{2}\n\\end{pmatrix}$\nThen the Haldane model on a hexagonal lattice can be written as\nH(k) = doI + d\u2081\u03c3\u2081 + d\u2082\u03c3\u2082 + d\u2083\u03c3\u2083\ndo = 2t\u2082 cos \u03c6 \u2211 cos(k \u22c5 b\u1d62)\n= 2t\u2082 cos \u03c6 \\Bigg[ cos(\\sqrt{3}k\u2093 a) + cos\\Bigg(\\frac{\\sqrt{3}k\u2093 a}{2} + \\frac{3k\u1d67 a}{2}\\Bigg) + cos\\Bigg(\\frac{\\sqrt{3}k\u2093 a}{2} - \\frac{3k\u1d67 a}{2}\\Bigg)\\Bigg]\nd\u2081 = t\u2081 \u2211 cos(k \u22c5 a\u1d62)\n$\\sqrt{3}k\u2093 a$\n= t\u2081 \\Bigg[ cos(k\u1d67 a) + cos\\Bigg(\\frac{k\u2093 a}{2} + k\u1d67 a\\Bigg) + cos\\Bigg(-\\frac{3k\u2093 a}{2} - \\frac{k\u1d67 a}{2}\\Bigg)\\Bigg]\nd\u2082 = t\u2081 \u2211 sin(k \u22c5 a\u1d62)\n$\\sqrt{3}k\u2093 a$\n= t\u2081 \\Bigg[ sin (k\u1d67 a) + sin\\Bigg(\\frac{k\u2093 a}{2} + k\u1d67 a\\Bigg) - sin\\Bigg(-\\frac{3k\u2093 a}{2} - \\frac{k\u1d67 a}{2}\\Bigg)\\Bigg]\nd\u2083 = m - 2t\u2082 sin \u03c6 \u2211 sin(k \u22c5 b\u1d62)\nm - 2t\u2082 sin \u03c6 \\Bigg[ sin(\\sqrt{3}k\u2093 a) + sin\\Bigg(\\frac{\\sqrt{3}k\u2093 a}{2} + \\frac{3k\u1d67 a}{2}\\Bigg) + sin\\Bigg(\\frac{\\sqrt{3}k\u2093 a}{2} - \\frac{3k\u1d67 a}{2}\\Bigg)\\Bigg]\nwhere \u03c3\u1d62 are the Pauli matrices and I is the identity matrix."}, {"title": "Question", "content": "1.2 Calculate the Chern number using the Haldane Hamiltonian, given the grid size d for discretizing the Brillouin zone in the k\u2081 and k\u2081, directions (assuming the grid sizes are the same in both directions), the lattice spacing a, the nearest-neighbor coupling constant t\u2081, the next-nearest-neighbor coupling constant t\u2082, the phase p for the next-nearest-neighbor hopping, and the on-site energy m."}, {"title": "Background", "content": "Here we can discretize the two-dimensional Brillouin zone into grids with step $dk_x = dk_y = \\delta$. If we define the U(1) gauge field on the links of the lattice as $U_\\mu(k_l) := \\frac{\\langle\\eta(k_l)|\\eta(k_l+\\hat{\\mu})\\rangle}{|\\langle\\eta(k_l)|\\eta(k_l+\\hat{\\mu})\\rangle|}$, where $|\\eta(k_l)\\rangle$ is the eigenvector of Hamiltonian at $k_l$, $\\hat{\\mu}$ is a small displacement vector in the direction $\\mu$ with magnitude $\\delta$, and $k_l$ is one of the momentum space lattice points l. The corresponding curvature (flux) becomes\n$F_{xy}(k_l) := ln [U_x(k_l)U_y(k_l + \\hat{x})U_x^{-1}(k_l + \\hat{y})U_y^{-1}(k_l)]$\nand the Chern number of a band can be calculated as\n$C = \\frac{1}{2\\pi} \\sum_l F_{xy}(k_l)$,\nwhere the summation is over all the lattice points l. Note that the Brillouin zone of a hexagonal lattice with spacing $a$ can be chosen as a rectangle with $0 \\leq k_x \\leq k_{x0} = \\frac{2\\pi}{\\sqrt{3}a}$,  $-\\frac{4\\pi}{3a} \\leq k_y \\leq k_{y0} = \\frac{4\\pi}{3a}$."}, {"title": "Question", "content": "1.3 Make a 2D array of Chern numbers by sweeping the parameters: the on-site energy to next-nearest-neighbor coupling ratio (m/t\u2082 from -6 to 6 with N samples) and phase (\u03c6 from -n to n with N samples) values. Given the grid size 8 for discretizing the Brillouin zone in the k\u2081 and ky directions (assuming the grid sizes are the same in both directions), the lattice spacing a, the nearest-neighbor coupling constant t\u2081, and the next-nearest-neighbor coupling constant t2."}, {"title": "Example Domain Specific Test Cases", "content": "Both the k-space and sweeping grid sizes are set to very rough values to make the computation faster, feel free to increase them for higher accuracy.\nAt zero on-site energy, the Chern number is 1 for $\u03c6 > 0$, and the Chern number is -1 for $\u03c6 < 0$.\nFor complementary plots Figure 9, we can see that these phase diagrams are similar to the one in the original paper: Fig.2 in Haldane, F. D. M. (1988). To achieve a better match, decrease all grid sizes.\nCompare the following three test cases. We can find that the phase diagram is independent of the value of t\u2081, and the ratio of t2/t1, which is consistent with our expectations."}]}