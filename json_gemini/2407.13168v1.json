{"title": "SciCode: A Research Coding Benchmark Curated by Scientists", "authors": ["Minyang Tian", "Luyu Gao", "Shizhuo Dylan Zhang", "Xinan Chen", "Cunwei Fan", "Xuefei Guo", "Roland Haas", "Pan Ji", "Kittithat Krongchon", "Yao Li", "Shengyan Liu", "Di Luo", "Yutao Ma", "Hao Tong", "Kha Trinh", "Chenyu Tian", "Zihan Wang", "Bohao Wu", "Yanyu Xiong", "Shengzhu Yin", "Minhui Zhu", "Kilian Lieret", "Yanxin Lu", "Genglin Liu", "Yufeng Du", "Tianhua Tao", "Ofir Press", "Jamie Callan", "Eliu Huerta", "Hao Peng"], "abstract": "Since language models (LMs) now outperform average humans on many challenging tasks, it is becoming increasingly difficult to develop challenging, high-quality, and realistic evaluations. We address this by examining LM capabilities to generate code for solving real scientific research problems. Incorporating input from scientists and AI researchers in 16 diverse natural science sub-fields, including mathematics, physics, chemistry, biology, and materials science, we create a scientist-curated coding benchmark, SciCode. The problems naturally factorize into multiple subproblems, each involving knowledge recall, reasoning, and code synthesis. In total, SciCode contains 338 subproblems decomposed from 80 challenging main problems, and it offers optional descriptions specifying useful scientific background information and scientist-annotated gold-standard solutions and test cases for evaluation. Claude3.5-Sonnet, the best-performing model among those tested, can solve only 4.6% of the problems in the most realistic setting. We believe that SciCode demonstrates both contemporary LMs' progress towards realizing helpful scientific assistants and sheds light on the building and evaluation of scientific AI in the future.", "sections": [{"title": "Introduction", "content": "The development of evaluations in tandem with language models (LMs) has substantially contributed to the rapid advancement of these models [29, 11, 7, 25, 81, 27, 72]. Because LMs now surpass the performance of most humans except domain experts, evaluating them becomes increasingly challenging. Many established benchmarks struggle to keep pace with the advancements in LM performance and have quickly become saturated [91, 14, 70, 58], leading to discrepancies between the models' perceived and actual capabilities [36]. As a consequence, researchers are developing synthetic challenging benchmarks, often involving models in the construction of evaluation instances. For example, some subsample instances from existing benchmarks that cannot be solved by current models [93, 82], or augment them to construct more challenging evaluations [21, 44, 49]. However, it is unclear whether such efforts accurately reflect real-world applications and the models' performance in practical scenarios. Realistic, high-quality, and challenging evaluations are crucial for the continued advancement of LMs."}, {"title": "SciCode", "content": "This section examines the design principles and annotation process we chose for SciCode, describing: research-level coding problems from various natural science fields (\u00a72.1); how we decomposed main problems into multiple, simpler subproblems (\u00a72.2); our design choices for the annotation process (\u00a72.3); and various evaluation setups that SciCode facilitates (\u00a72.4)."}, {"title": "Challenging and Realistic Scientific Coding Problems", "content": "SciCode sources challenging and realistic research-level coding problems across natural science disciplines, including mathematics, physics, chemistry, biology, and material science, covering a total of 16 subfields. This diverse selection ensures a comprehensive representation of the natural sciences, where extensive code development is essential.\nSciCode is mainly drawn from the scripts that scientists use in their everyday workflow. Many of these have been used in one or more publications, demonstrating their robustness and correctness. However, they are primarily for internal use, which means that they are seldomly open-sourced and often poorly annotated. Consequently, unlike general-domain coding problems, natural science problems have less exposure in most current LMs' training data. This offers a unique opportunity to evaluate the models' ability to generalize to less familiar contexts. In total, SciCode consists of 80 main problems, decomposed into 338 subproblems."}, {"title": "A Main Problem with Multiple Subproblems", "content": "In their everyday workflow, scientists often decompose a complex problem into multiple smaller, more manageable parts. They may write relatively independent code for each part and then integrate these parts into a complete solution to the main problem. In developing our dataset, we leverage this natural and intuitive structure and further standardize our dataset by instructing the scientists to adhere to the following format.\nMain Problem A main problem is a primary task that needs to be addressed. It defines the overall objective of the research and guides the direction of the study. The main problem encompasses all subproblems, with detailed instructions on required inputs and expected outputs articulated in a docstring block. With the main problem defined, scientists have sufficient guidance to solve the task.\nSubproblem Decomposition Subproblems focus on questions derived from the main problem. They decompose the complex main problem into smaller, more manageable parts, enabling a more detailed and systematic investigation. Detailed docstrings for each subproblem describe the required input and expected output, ensuring clarity and aiding in accurate code generation. This structured decomposition simplifies problem-solving and facilitates a more granular evaluation of the models' scientific coding capabilities."}, {"title": "Data Annotation", "content": "This process consists of three main stages:\n(1) Problem selection: Deciding on question topics related to the research domain (\u00a72.3.1).\n(2) Evaluation design: Designing both numerical and domain-specific test cases to ensure the problem's validity (\u00a72.3.2).\n(3) Problem validation: Iterating on the problems through three rounds of revisions to further enhance question design (\u00a72.3.3).\nWe now examine the design choices for each stage."}, {"title": "Problem Selection", "content": "Throughout the research project cycle, various coding needs arise, such as data processing, fitting, and plotting. To use SciCode, scientists select the problems that require intense scientific knowledge and reasoning to optimally test LM's science capability. This approach ensures that both the breadth and depth of frontier research are addressed. We focus on:\n\u2022 Numerical methods. Analytical forms are usually impossible to achieve for very com-plicated systems. Therefore, scientists must derive numerical models and algorithms that describe physical phenomena [9, 74, 35, 77, 32], chemical reactions [22, 23, 87], biological systems [96, 97, 95, 83, 16, 15, 51, 89], or statistical behaviors[79, 69, 57, 55, 50, 24, 47].\n\u2022 Simulation of systems. In fields of natural science, scientists write code to simulate systems and processes. These simulations are based on theoretical principles and empirical data,"}, {"title": "Evaluation Design", "content": "To facilitate evaluation, we have scientist annotators use only widely adopted and well-documented packages such as NumPy, SciPy, and SymPy when writing the solution code for their problems, as shown in Figure 4.\nOur test suite involves two key components. (1) Numerical tests list input-output pairs to check if the generated code produces the same outputs as ground truth. (2) Domain-specific test cases, introduced as an additional stage, evaluate whether model-generated solutions align with scientists' practical needs and further ensure the correctness and applicability of each solution within its specific field. These tests are extracted from real scientific workflows: scientists must design domain-specific test cases to verify code accuracy by reproducing results published in academic papers or matching analytical solutions derived from theoretical models. For example, we reproduce the phase transition at around kT/J = 2.269 for the 2D square Ising model problem [63], derive the surface plasmon mode in a 2D layered electron gas [10, 32], verify the ballistic Brownian motion in optical tweezer [46], etc. By doing so, we validate that the code not only functions correctly but also accurately represents the underlying scientific problem.\nOverall, the evaluation design aims to balance the fidelity of the scientific problem with the practicality of the evaluation process, ensuring that the solutions are both accurate and accessible."}, {"title": "Problem Validation for Quality Control", "content": "We conduct three rounds of validation and revision for each problem:\n(1) In-domain scientist validation. At least two scientists in the same research domain cross-check the question design, solution code, and domain-specific test cases, providing detailed feedback. The scientists who design the workflows iterate on them based on this feedback to ensure the problems are scientifically accurate.\n(2) Out-of-domain scientist validation. One scientist from a different domain reviews the question design to ensure it is clear and that the information provided is precise and sufficient to solve the problem (e.g., all scientific constants are given). This helps to identify any assumptions that might be unclear to those outside the immediate field of study.\n(3) GPT-4 validation. GPT-4 assists with the final review round. The previously validated sub-questions are input to GPT-4 to generate code solutions. Scientists perform error analysis for the generated solutions and redesign the numerical test cases if necessary to prevent false positives.Based on the code solutions from GPT-4, the scientist may also revise the entire workflow a third time to addressany potential ambiguity.\nThis multi-round validation approach ensures that the problems are scientifically rigorous, clear, and unambiguous, facilitating accurate and effective evaluation."}, {"title": "Various Types of Evaluations", "content": "SciCode offers unique opportunities for evaluating LMs across diverse settings, comprehensively testing their coding capabilities."}, {"title": "Experiments", "content": "Prompts. We evaluate our model using zero-shot prompts. We keep the prompts general and design different ones for different evaluation setups only to inform the model about the tasks. We keep prompts the same across models and fields, and they contain the model's main and sub-problem instructions and code for previous subproblems. We also instruct the model to recall useful knowledge when gold background knowledge is not provided. \u00a7A.1 presents an example."}, {"title": "Evaluated Models", "content": "Since SciCode is a challenging benchmark, we mainly consider strong language models.\n\u2022 GPT-4O [65]: An optimized version of GPT-4 [64] by OpenAI with multi-modal capability.\n\u2022 GPT-4-Turbo: A faster and more cost-effective variant of GPT-4 [64]. We use the 'gpt-4-turbo-2024-04-09' snapshot.\n\u2022 Claude3.5-Sonnet [4]: The latest model from the Claude 3.5 family from Anthropic.\n\u2022 Claude3-Opus [3]: The most capable model from the Claude 3 family from Anthropic.\n\u2022 Claude3-Sonnet [3]: The second most capable model from the Claude 3 family.\n\u2022 Gemini 1.5 Pro [85]: A model from the Gemini 1.5 family by Google and the largest with open access at the time of writing.\n\u2022 Llama-3-70B-Instruct [2]: The instruction-tuned version of the largest available model from the Llama-3 family.\n\u2022 Mixtral-8x22B-Instruct [33]: The instruction-tuned version of Mistral AI's largest publicly accessible Mixture-of-Expert Model.\n\u2022 Deepseek-Coder-v2 [102]: Mixture-of-Experts (MoE) code language model continue pre-trained on DeepSeek-V2\n\u2022 Qwen2-72B-Instruct [86]: The largest instruction-tuned Qwen-2 model."}, {"title": "Main Results", "content": "Table 2 presents results under the standard setup. For the easier subproblem-level evaluation, the state-of-the-art models we test solve 14%-26% of the subproblems. Among them, Claude3.5-Sonnet achieves the best performance, with a 26.0% pass@1 rate. However, all models perform much worse on the more realistic and challenging main problem evaluation. Claude3.5-Sonnet still performs the best in this setting, but with only a 4.6% pass@1 rate.\nThese results show that SciCode is a difficult benchmark for current LMs. Consistent with our observations on proprietary models, open-weight LMs under test also showed their lack of capabilities in solving any main problem despite being able to solve a number of sub-problems correctly."}, {"title": "Additional Results with Other Evaluation Settings", "content": "Providing gold scientific background knowledge. Table 3 presents results when background text authored by scientists is provided to the LMs and generated solutions to previous subproblems are used. This setting evaluates both the models' capabilities to faithfully follow the instructions provided in the background as well as their code-generation performance. The \u0394 columns indicate performance differences compared to the standard setup.\nAll models substantially improve performance for both subproblem and main problem evaluations when given scientific background knowledge. For the subproblem evaluation, Claude3.5-Sonnet and GPT-4O perform the best, both with a 35.4% pass@1 rate. GPT-4-Turbo benefits the most from the provided scientific background and reasoning with an increase of 10.8%. Open models improve less compared to proprietary models which might indicate weaker Instruction following capability. Interestingly, the comparison between Llama-3-70B-Instruct and Mixtral-8x22B-Instruct reveals a trend that differs from the standard setup: Llama-3-70B-Instruct benefits more from the scientific background knowledge and reaches the performance of Mixtral-8x22B-Instruct in this setting.\nFor the main problem evaluation, the trend remains similar to the standard setup. Claude3.5-Sonnet performs best, with a 12.3% pass@1 rate, followed closely by GPT-4O and GPT-4-Turbo at 9.2%. GPT-4O, GPT-4-Turbo, and Claude3.5-Sonnet improve most from background content, at 7.7%. Nonetheless, all models still fall short of satisfactory performance even with the background knowledge provided. This reaffirms that SciCode is challenging even when focusing on code generation rather than testing the models' scientific knowledge."}, {"title": "Related Work", "content": "Language models for code. Code has long been an active field of research, and code LMs have co-evolved with foundation LMs since the era of BERT [17]. Earlier works include CodeBert [19] and CodeT5 [94], while Codex [12] arguably kick-started the LLM era for code-generation models. Since Codex, the field has experienced rapid growth in quantity and quality of large code generation models, including specially trained models like Codegen [60], StarCoder models [45, 52], and generalist models with code adapation [12] such as CodeLlama [75], CodeQwen [8], and DeepSeek-Coder [25]. As code generation gains more attention and becomes increasingly useful, contemporary generalist models often include non-trivial coding capabilities [66, 85].\nEvaluating code generation. Before the emergence of very capable code synthesis models, when most models struggled to produce executable code, datasets like CoNaLa typically included n-gram-based metrics [100]. Soon after model capabilities improved, execution-based evaluation gained in popularity [28, 7, 11]. While n-gram or general text-based evaluation still exists, we opted to omit them from SciCode due to obvious limitations of surface form matching in scientific coding.\nCode generation benchmarks now take various forms. For simple function completion, MBPP [7] and HumanEval [11] are two widely used benchmarks that contain basic programming questions, mainly evaluating LMs' ability to turn natural language instructions into Python programs. Other benchmarks assess the models' competence in real-world programming scenarios, such as writing data science code [42, 101], repository-level code completion [18], and more complex tasks in real-world software engineering [34]. Though our work is similar to MTPB [60] in terms of using a multi-turn setup, our subproblem instructions correspond to a high-level task, while theirs correspond to specific code actions (e.g., replace X with Y in the string).\nLanguage models for science. Scientific tasks are complex due to their demands for reasoning and knowledge. However, Recent advances in general and specialized language models have revolution-ized the processing of text and other data modalities, such as molecules and proteins, in scientific fields. Galactica [84], a general-purpose scientific model, can perform tasks like citation prediction, scientific reasoning, document generation, and molecular property prediction. Many models focus on one single domain or task, like math (e.g., Minerva [43] and Deepseek-Math [78] ), protein structure prediction (e.g., ESM-2 [48]), medical reasoning (e.g., Med-PaLM [80], BioGPT [53]), and others."}, {"title": "Conclusion", "content": "We introduce SciCode, a scientific research benchmark curated by professional natural scientists. We designed SciCode for scientific problem evaluation and collected problems representing 16 diverse domains. By assessing SciCode with ten contemporary state-of-the-art AI models, we demonstrated that our benchmark is within reach but remains very challenging. We believe SciCode will serve as a helpful guideline for building future code language models for varied scientific applications."}, {"title": "Appendix", "content": ""}, {"title": "Prompt", "content": ""}]}