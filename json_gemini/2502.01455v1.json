{"title": "Temporal-consistent CAMs for Weakly\nSupervised Video Segmentation in Waste Sorting", "authors": ["Andrea Marelli", "Luca Magri", "Federica Arrigoni", "Giacomo Boracchi"], "abstract": "In industrial settings, weakly supervised (WS) methods are\nusually preferred over their fully supervised (FS) counterparts as they\ndo not require costly manual annotations. Unfortunately, the segmen-\ntation masks obtained in the WS regime are typically poor in terms\nof accuracy. In this work, we present a WS method capable of produc-\ning accurate masks for semantic segmentation in case of video streams.\nMore specifically, we build saliency maps that exploit the temporal co-\nherence between consecutive frames in a video, promoting consistency\nwhen objects appear in different frames. We apply our method in a\nwaste-sorting scenario, where we perform weakly supervised video seg-\nmentation (WSVS) by training an auxiliary classifier that distinguishes\nbetween videos recorded before and after a human operator, who man-\nually removes specific wastes from a conveyor belt. The saliency maps\nof this classifier identify materials to be removed, and we modify the\nclassifier training to minimize differences between the saliency map of a\ncentral frame and those in adjacent frames, after having compensated ob-\nject displacement. Experiments on a real-world dataset demonstrate the\nbenefits of integrating temporal coherence directly during the training\nphase of the classifier. Code and dataset are available upon request.", "sections": [{"title": "Introduction", "content": "With the escalation of global waste production, it has become critical to improve\nmodern waste management systems. In particular, waste sorting involves the sep-\naration of specific types of recyclable waste, which usually consists in manually\nremoving objects of different materials from a conveyor belt, where only a specific\nmaterial must remain. Machine vision systems and deep learning have emerged\nas promising solutions to automatize these processes, aiming to enhance the ef-\nficiency and accuracy of waste management to reduce human error and to lower\noperational costs [9,11]. These advancements significantly contribute to more\nsustainable and environmentally friendly practices. Unfortunately, Fully super-\nvised (FS) segmentation methods, which are known for their efficacy in these\ntasks, require extensive pixel-level annotations for training. These annotations\nmust be obtained by manually segmenting a large number of images, and they\nare extremely costly to produce."}, {"title": "Related Works", "content": "The landscape of segmentation methods based on neural networks can be orga-\nnized into two main categories: i) FS approaches, that require pixel-level anno-\ntated datasets for precise segmentation, and ii) WS ones, that exploit image-\nlevel annotations and are definitely more practical for large-scale applications.\nIn waste sorting, existing datasets also reflect this classification. After presenting\nan overview of these two categories, we will focus on WS methods that, as in\nour scenario, take as input videos instead of images.\nFully supervised image segmentation approaches span a wide range of methods,\nfrom simpler region-proposal [5, 8, 10, 12, 30] and fully convolutional networks\n[1,6, 25, 43] to transformers [40]. The success of these methods heavily depends\non the availability of precise pixel-level annotation for training. In waste sorting,"}, {"title": "Problem formulation", "content": "We frame our waste sorting problem as a WS segmentation task where the\ntraining data is a set of images collected by the cameras C\u2081 and C2 as shown in\nFigure 1. We refer to images collected before the human intervention as \"before\"\nimages and to those collected after it as \"after\" images. Also, we refer to objects\nthat the human operator must remove as \"illegal\u201d objects while to all the objects\nthat must remain on the belt as \"legal\" objects. Given an RGB image $X \\in$\n$R^{wxhx3}$ of the conveyor belt, with values normalized between [0,1], we aim at\nsegmenting the illegal objects that the operator must remove. As illustrated in\nFigure 2a, this consist in estimating for the image X a semantic segmentation\nmask $M_X \\in A^{wxh}$ defined as:\n$M_X (r, c) = y \\text{ if pixel at position } (r, c) \\text{ in } X \\text{ belongs to an}$"}, {"content": "$\text{object of class } y \\in A,$\n(1)\nwhere $A = {0,1}$ is the set of illegal objects and background respectively. Note\nthat in this formulation legal objects are segmented together with the back-\nground.\nWe make the following assumptions: the training set is composed by videos\ncaptured by cameras C\u2081 and C2, which are labelled as \"before\" or \"after\" respec-\ntively. Thus, we are in a WS setting, i.e., we only know which frames belong to"}, {"title": "Proposed Solution", "content": "Inspired by zerowaste-w [2], we train an auxiliary classifier to distinguish be-\ntween videos taken before and after human intervention. The classifier learns to\nidentify the before video thanks to the presence of illegal objects that are instead\nabsent in the after videos. A saliency map of each individual before frame would\nroughly highlight the regions corresponding to illegal objects, but the resulting\nsegmentation masks might not be very accurate. Thus, to boost the accuracy\nof saliency maps, we exploit both the spatial and the temporal coherence of the\nvideos, operating on triplets of consecutive frames Xt-1, Xt, Xt+1 as outlined in\nFig. 4. As a first step, we remove the background from the images of our dataset\nin a pre-processing step described in Section 4.1 (Fig. 3). As shown in Fig. 4,\nthe background-removed frames are processed through a pre-trained backbone\nnetwork (ResNet50) to extract features (Sec. 4.2), to be handled by two dif-\nferent modules. The spatial module (Sec. 4.3) implements the principles of\nPuzzle-CAM [17] and returns the reconstructed feature space $f_{puzzle}$, obtained\nby splitting the central frame Xt into local patches $X_{t}^{i}$ and by merging back\ntheir feature spaces $f_{t}^{i}$ computed on individual patches (Fig. 5). The temporal\nmodule (Sec. 4.4) operates along the temporal dimension of videos. It takes as\ninput the adjacent frames Xt\u22121 and Xt+1 and, by exploiting optical flow, it rec-\nonciles the warped mask Mt\u22121 and Mt+1 and into a central, single, fused $M_{fused}$\n(Fig. 6). These two modules produce different outputs, which are then compared\nagainst the classifier's output with two reconstruction losses. This process forces\nthe classifier to generate consistent saliency maps at spatial and temporal levels\n(Fig. 4)."}, {"title": "Pre-processing", "content": "All \"before\" images share the same camera C1, lighting conditions, and belt\nsection, resulting in having all similar backgrounds. The same holds for \"after\"\nimages. However, \"before\" and \"after\" backgrounds are very different from each\nother. Unfortunately, this condition results in the classifier focusing on the back-\nground instead of the features of the objects. For this reason, we preliminary\nsegment foreground objects from the background in our dataset. For both be-\nfore and after videos, we estimated a background by computing the pixel-wise\nmedian image across all grayscale frames. For each frame, the distance of every\npixel with respect to the background estimator is then computed. Pixels signifi-\ncantly different from the estimator are so considered as foreground, resulting in\na binary mask that is then applied to the RGB images."}, {"title": "Feature Extraction and Classification Loss", "content": "As shown in Fig. 4, each frame Xt is processed through a pre-trained ResNet50\nbackbone F with a classification head that reduces the number of final feature\nmaps to A, corresponding to the three classes after, before, and background.\nThe output of the backbone is the feature space ft:\n$f_t = F(X_t)$.\n(2)\nwhich is then processed by a Global Average Pooling (GAP) layer G to produce\nthe prediction vector $\u0109 = \u03c3(G(f_t))$ used for image classification. We utilize a\nmulti-label soft margin loss for this task. For notational convenience, we define\nz as:\n$z=$\n(3)\n$cls(z, z) = -log(z)$\n(4)"}, {"title": "Spatial Module", "content": "Following PuzzleCAM [17], our architecture (Fig. 5) is designed to promote\nspatial coherence of saliency maps when extracting features from a single image\nas follows. The Spatial Module processes the central frame Xt to match its\nfeatures with those extracted from its patches. More specifically, from an input\nimage Xt of size w \u00d7 h, the tiling module generates non-overlapping tiled patches\n${X_{t}^{11}, X_{t}^{1,2}, X_{t}^{2,1}, X_{t}^{2,2}}$ of size ${h \\over 2} \\times {w \\over 2}$. Next, we extract $f_{t}^{i}$ feature spaces for\neach $X_{t}^{i}$ as described in (2). Finally, the merging module assembles all $f_{t}^{i}$ into\na single feature space $f_{puzzle}$ that has the same shape as ft, the feature space of\nthe original image Xt (Fig. 5). Using the GAP layer G described in Section 4.2,\nwe map $f_{puzzle}$ into a prediction vector $\u0109_{puzzle} = G(f_{puzzle})$. Using (3) and (4)\nwe compute a new classification loss as\n$L_{p-cls} = cls(\u0109_{puzzle}, z),$\n(6)"}, {"title": "Temporal Module", "content": "The main contribution introduced by our work is the temporal module, through\nwhich we compute temporal consistent saliency maps: this module processes a\ntriplet of frames Xt\u22121, Xt, and Xt+1 in a joint classification network employing\ntemporal coherence between the saliency maps of the frames (Fig. 6).\nCAM generation. First, from Xt-1 and Xt+1, we extract feature spaces ft-1\nand ft+1 as in (2). Then, for every frame of the triplet Xt\u22121, Xt, Xt+1, as done\nin [17], we use a ReLU activation function to compute the saliency map M for\nthe class y the input images belong to:\n$M = ReLU(f[y]),$\n(8)\nwhere f[y] represent the y-th channel of a feature space f. The computed map\nM is then normalized by dividing it by its maximum value. We produce the\nsaliency maps for every frame in triplet Xt-1, Xt, and Xt+1 obtaining Mt-1,\nMt, and Mt+1, respectively.\nOptical Flow Warping and CAM Fusion. As next step, we align the saliency maps\nusing DICL-FLow [36], and in practice we compute the optical flows between\nconsecutive frames Xt and Xt+1 and use these flows to warp the lateral maps to\nthe central one:\n$M_{warped} = Warp(M_i, Flow(X_t, X_i))$\n(9)\nfor $i \\in {t \u2212 1, t + 1}$,\nwhere Flow(Xt, Xi) denotes the optical flow between frame Xt and frame Xi.\nThe warped maps for frames Xt\u22121 and Xt+1 are then fused keeping the pixel-wise\nmaximum:\n$M_{fused} = max(M_{warped}^{t-1}, M_{warped}^{t+1})$\n(10)"}, {"title": "Final Loss Design", "content": "To summarize, as illustrated in Fig. 4, we train our network by minimizing\na loss function that combines the losses from both the Spatial (PuzzleCAM)\nand the Temporal modules. The final loss function $L_{total}$ is the sum of the\nclassification losses given by Eq. (5) and (6), and the reconstruction losses given\nby (7) and (11), namely:\n$L_{total} = L_{cls} + L_{p-cls} + \u03b1L_{spatial} + \u03b2L_{temporal}$.\n(12)\nwhere \u03b1 and \u1e9e are regularization terms that weight respectively the spatial and\ntemporal coherence components given."}, {"title": "Experiments", "content": "This section is devoted at assessing the benefits of our solution on both seg-\nmentation and classification tasks. After describing our dataset, we present a\ncomparative analysis of our approach against baseline methods, demonstrat-\ning the effectiveness of exploiting both temporal coherence in segmentation and\nbackground removal in classification tasks."}, {"title": "Datasets and Competitors", "content": "We evaluate our method on our custom-collected dataset (named SERUSO and\navailable upon request), which consists of 3682 images, divided into 36 videos for\nthe \"after\" class and 32 videos for the \"before\" class. Specifically, there are 1836\n\"after\" images and 1846 \"before\" images, each with a resolution of 2400 \u00d7 2400\npixels. Cameras have been installed to monitor a conveyor belt containing objects\nmade from PET materials, including transparent, bluish and opaque PET. The\noperators remove any object but semi-transparent colored PET ones. As a result,\nthe \"before\" images captured the initial, mixed material flow, while the \"after\"\nimages contain primarily semi-transparent colored PET objects with occasional\nanomalies (see Fig. 7 for an example). A total of 364 images were manually la-\nbeled by segmenting \"illegal\" objects in the \"before\" images. These segmentation\nmasks were used exclusively for testing. We also performed additional experi-\nments on the Zerowaste-w dataset [2], extending the range of analyzed methods\nbeyond those used by the authors. We benchmarked our method against the\napproach used by the authors of Zerowaste-w, namely PuzzleCAM [17], as well\nas other CAM-based methods, including Grad-CAM and its extension incor-\nporating temporal coherence (Frame-to-frame [23]). In addition, we conducted\nablation studies to assess the impact of each component of our method."}, {"title": "Results", "content": "All experiments were conducted on a workstation equipped with an Nvidia RTX\nA6000 GPU. The images were re-scaled to 512 \u00d7 512 as the network inputs and\nthe dataset was split into training and validation sets with an 80% and 20%"}, {"title": "Conclusions", "content": "We addressed the challenging task of industrial waste sorting using a WSVS ap-\nproach. We proposed a novel method that drives a classifier to produce temporal\nconsistent saliency maps for objects appearing in different frames. Experiments\nand ablation studies demonstrated that the use of temporal coherence directly\nin the classifier's training phase effectively improves the classifier's ability to\ngenerate saliency maps, outperforming the mIoU of other saliency maps-based\nmethods. The results obtained in our dual-camera setup are very promising and\nsuggest that this approach can be applied to other industrial processes with sim-\nilar settings, where it is necessary to manually separate specific objects from a\nheterogeneous stream e.g. in product quality control processes, where anoma-\nlous elements need to be removed from a stream of objects, such as damaged or\nfaulty products. Given that saliency maps are currently computed during the in-\nference phase using only a single frame, future work explores including adjacent\nframes in the map computation at inference time as well. Also, as a next step,\nthe segmentation masks obtained can be used as pseudo-labels to supervise a\nFS segmentation network, to improve the segmentation performance."}]}