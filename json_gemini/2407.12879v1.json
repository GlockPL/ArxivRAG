{"title": "Large Visual-Language Models Are Also Good Classifiers: A Study of In-Context Multimodal Fake News Detection", "authors": ["Ye Jiang", "Yimin Wang"], "abstract": "Large visual-language models (LVLMs) exhibit exceptional performance in visual-language reasoning across diverse cross-modal benchmarks. Despite these advances, recent research indicates that Large Language Models (LLMs), like GPT-3.5-turbo, under-achieve compared to well-trained smaller models, such as BERT, in Fake News Detection (FND), prompting inquiries into LVLMs' efficacy in FND tasks. Although performance could improve through fine-tuning LVLMs, the substantial parameters and requisite pre-trained weights render it a resource-heavy endeavor for FND applications. This paper initially assesses the FND capabilities of two notable LVLMS, CogVLM and GPT4V, in comparison to a smaller yet adeptly trained CLIP model in a zero-shot context. The findings demonstrate that LVLMs can attain performance competitive with that of the smaller model. Next, we integrate standard in-context learning (ICL) with LVLMs, noting improvements in FND performance, though limited in scope and consistency. To address this, we introduce the In-context Multimodal Fake News Detection (IMFND) framework, enriching in-context examples and test inputs with predictions and corresponding probabilities from a well-trained smaller model. This strategic integration directs the LVLMs' focus towards news segments associated with higher probabilities, thereby improving their analytical accuracy. The experimental results suggest that the IMFND framework significantly boosts the FND efficiency of LVLMs, achieving enhanced accuracy over the standard ICL approach across three publicly available FND datasets.", "sections": [{"title": "1. Introduction", "content": "The rapid dissemination of fake news across online platforms has presented tangible challenges [1]. Among the strategies to address these challenges, automated fake news detection (FND), designed to autonomously identify inaccurate or deceptive news articles from authentic ones, has proven to be a viable solution in previous studies [2, 3, 4].\nTraditional methods predominantly depend on modeling the textual semantics of news articles, and have revealed inadequacies in addressing the multi-faced nature of fake news [5]. Therefore, multimodal approaches have incorporated pre-trained small\u00b9 language models (SLMs, e.g., BERT [6]) with visual models (VMs, e.g., ResNet [7]) for establishing cross-modal representation, resulting in a more thorough performance for FND [8, 9]. While SLMs are normally pre-trained on open-domain datasets, for example, BERT was originally trained on Wikipedia, they are inherently limited in processing fake news that demands expertise beyond their training scope, and so as that of in the VMs [10].\nRecent developments in large visual-language models (LVLMs), trained on extensive collections of text-image pairs covering a broad spectrum of global knowledge [11, 12], have shown a profound comprehension of cross-modal reasoning and demonstrated proficiency in identifying commonplace occurrences [13]. However, LVLMs are deficient in forgery-specific knowledge [14, 15], which undermines their efficacy in FND. This limitation could be ascribed to (1) media manipulation disrupting the coherence between various modalities, thereby leading to semantic inconsistencies [16]; and (2) the tendency of altered images to exhibit noticeable artifact indications [17]. Meanwhile, the pre-trained weights of LVLMs, like GPT-4, are normally inaccessible. The immense size of LVLMs also makes it challenging to fine-tune for specific tasks even if the model checkpoints are released.\nAlternatively, in-context learning (ICL) empowers LVLMS to acquire the capability to perform downstream tasks through conditioning on prompts composed of input-output samples, circumventing the need for explicit gradients updating [18]. Although previous studies [19, 14] have suggested that large language models (LLMs) might underperform the traditional SLMs in textual-based FND, the potential of ICL in aiding LVLMS for FND remains under-explored. To address the above limitations, this paper presents a framework In-context Multimodal Fake News Detection (IMFND), aimed at exploring three pivotal research questions:"}, {"title": "3. In-context multimodal fake news detection", "content": "In-context multimodal fake news detection (IMFND) framework includes three phases sequentially: (1) Conducting few-shot training on in-context examples using a smaller multimodal model; (2) Integrating predictions and their probabilities from the pre-trained smaller model into the in-context examples and test inputs; (3) Employing the constructed in-context examples and test inputs with LVLMs for final prediction. The overall workflow is shown in Figure 1(c).\n3.1. Few-shot learning with a smaller model\nThe standard in-context methodology integrates the ground-truth label with the text input directly, encouraging LVLMs to concentrate on data specific to the task. However, the outputs generated by LVLMs remain characterized by a lack of confidence and are frequently accompanied by uncertainty (e.g., \"I'm sorry, I can't assist with verifying the authenticity of news articles\").\nTo address this, the IMFND first trains a smaller model to be able to generate the predictions and their probabilities for the in-context examples and test inputs. Specifically, a smaller model learns from n samples of a labeled dataset $(t_i, m_i, y_i)$, where $t_i, m_i$ are the text and the image inputs respectively and $i \\in [1, n]$ (i.e., n samples per class), passing to a pre-trained frozen multimodal encoder. Inspired by [48], which posits that few-shot samples from a singular modality may inadequately encapsulate the entirety of a concept class, each training sample is augmented to encompass five distinct feature representations: 1) a solely text feature, denoted as $f_t$; 2) a solely image feature, denoted as $f_m$; 3) the aggregation of L2-normalized features $f_c = [f_t \\bigoplus f_m]$, where $\\bigoplus$ denotes the operation of concatenation; 4) an image-to-text cross-attention feature, denoted as $f_{mt}$; 5) a text-to-image cross-attention feature, denoted by $f_{tm}$.\nThe cross-attention $CrossAtt()$ involves interchanging the textual query $f_t$ with the imagery query $f_m$ to derive the cross-attended feature $f_{mt}$ is, and is defined as:\n$f_{mt} = CrossAtt_{tm}(f_m, f_t, f_t) = softmax(\\frac{f_m f_t^T}{\\sqrt{d}}) f_t$\nConversely, interchanging the imagery query $f_m$ with the textual query $f_t$ facilitates the acquisition of the cross-attended feature $f_{tm}$ as outlined:\n$f_{tm} = CrossAtt_{mt}(f_t, f_m, f_m) = softmax(\\frac{f_t f_m^T}{\\sqrt{d}}) f_m$\nwhere d denotes the dimensionality of the smaller model. Subsequently, the multimodal features are comprised of the aforementioned five features, each feature is then passed through a linear classifier MLP to deduce five inferred probabilities. Finally, we concatenate the five deduced probabilities into a singular input for a meta-linear MLP classifier, thereby facilitating the formulation of the final prediction:"}, {"title": "3.2. In-context examples reconstruction", "content": "Following the training of the smaller model with few-shot data, in-context examples are then reconstructed, aiding LVLMs in acquiring the FND-specific knowledge imparted by the smaller model. Specifically, the well-trained smaller model is initially deployed to generate predictions and their corresponding probabilities for in-context examples, which are comprised of instances randomly selected from the training set. Notably, the predictions and their probabilities, derived from the linear probing of the $f_t$ and $f_m$ features, are also utilized in formulating in-context examples. Table 1(c) showcases the final prompt example of IMFND, in comparison to the standard ICL prompt example presented in Table 1(a).\nThe constructed in-context examples serve to augment the LVLMs' comprehension of the interplay between the inputs, ground-truth labels, and the expert knowledge of the smaller model, allowing the LVLMs to confidently produce the final predictions. Meanwhile, probabilities also act as an indicator of the smaller model's uncertainty regarding its predictions. Integrating probabilities into the context enables the LVLMs to rely on predictions when the smaller model exhibits high confidence and to exercise caution in instances of uncertainty. Additionally, probabilities can direct the LVLM's focus to more challenging in-context examples, facilitating learning from these complex examples and potentially enhancing FND performance overall."}, {"title": "3.3. Test inputs reconstruction", "content": "Following the assembly of the in-context examples, the constructed test input which encompasses both the predicted label and its probability, is concatenated with the test input and forms a comprehensive input for the LVLMs, as shown in Table 1(d). Finally, the complete IMFND algorithm can be summarized in Algorithm 1."}, {"title": "4. Experiments", "content": "4.1. Data\nTo evaluate the FND capabilities of the LVLMs, three publicly accessible datasets have been selected, encompassing a broad spectrum of fake news content and specifically including two languages: English and Chinese.\nPolitiFact [2] contains a collection of political news items, each classified as fake or real by specialized assessors, forming"}, {"title": "4.2. Large visual-language models", "content": "Two publicly accessible LVLMs are utilized in the experiments:\nCogVLM [21] is an open-source visual-language model that combines 10 billion visual parameters and 7 billion language parameters. It is designed for image understanding and multi-turn dialogue. We utilize the CogVLM-17B version (i.e.,"}, {"title": "4.3. Baselines", "content": "The few-shot FND performances of IMFND with the above two LVLMs are compared against an unimodal BERT and a multimodal CLIP with linear probing as discussed in Section 3.1:\nCLIP-LP is the variant of the CLIP model, which employs pre-trained models from OpenAI CLIP (ViT-B-32) [22] and Chinese CLIP (ViT-B-16) [50] for the extraction of text and image features respectively, and then passing to a linear probing (LP) layer for classification. The dimensionality of the linear probing is set to 512, aligning with the output dimension of the CLIP encoders.\nBERT is fine-tuned directly on few-shot examples using bert-base-uncased 4 and bert-base-chinese 5. The Huggingface Trainer is utilized to fine-tune the BERT models.\nFor all baselines, the AdamW optimization is utilized, configured with a learning rate of le-3 and a decay rate of 1e-2. Training spans 20 epochs, with the selection of the checkpoint being based on the best test accuracy. An early stopping is applied, utilizing a patience of three epochs. The performance results represent the mean across five seeds, disclosed through both accuracy and macro-f1 metrics."}, {"title": "5. Results", "content": "5.1. Performance comparison and ablation study\nTable 3 presents the accuracy and macro-f1 scores achieved by integrating IMFND with both CogVLM and GPT-4V, compared with the performance of unimodal BERT and multimodal CLIP-LP in few-shot learning settings.\nComparing with the smaller models. The IMFND significantly enhances the FND efficacy of LVLMs over smaller models across various few-shot settings, with notable improvements (i.e., with 15% increase in average accuracy and 16.5%"}, {"title": "5.3. Stability test", "content": "Given that the selection of in-context examples can significantly influence model performance in the few-shot settings, all experiments presented in Table 3 are executed using five randomly selected seeds. The stability of each model is evaluated by calculating the standard deviation of accuracies across diverse few-shot configurations, as shown in Figure 2.\nIn general, observations indicate a decrease in the standard deviation for each model concurrent with an increase in the number of in-context examples, suggesting enhanced stability with the increment of training samples. Additionally, the overall standard deviations for the IMFND consistently fall below"}, {"title": "5.4. Case study", "content": "Figure 3 showcases responses generated from CogVLM and GPT4V, employing either IMFND or standard ICL prompting methods.\nObservations indicate that the predictions and their probabilities from the smaller model guide the LVLM to concentrate on specific segments of the news content, thereby aiding in the accuracy of predictions. For example, a 98% confidence level by the image classifier in affirming the news image as fake prompts the LVLM to intensify its focus on evaluating the news images, as shown in the first example of Figure 3(a).\nFurthermore, IMFND prompting not only augments the responses generated from the LVLM, leading to a more comprehensive analysis of the text and image from news article, but also can override inaccurately classified predictions arising from standard ICL prompting, as demonstrated in the second example in Figure 3(a).\nTo enhance our comprehension of the limitations inherent to the LVLM, we also conduct an error analysis focusing on prevalent errors within both IMFND and ICL prompting. Manual examination of these inaccuracies revealed that error classifications by the smaller model could also lead the LVLM to generate incorrect predictions. For example, an image classifier's prediction that news is fake with a 71% confidence level prompted CogVLM to deem it fake news, in contrast, the standard ICL arrived at the correct prediction uninfluenced by the smaller model's misleading information, as illustrated in the first example of Figure 3(b).\nAdditionally, it is observed that the presence of credible sources, (e.g., the 'Demoncratic Underground', 'Politico', 'CNN') within the image or text cues LVLMs to classify the content as genuine news articles, as demonstrated in the second example of Figure 3(a) as well as in that of Figure 3(b)."}, {"title": "6. Discussion", "content": "The experimental results highlight the proficiency of LVLMs in applications that demand an understanding of complex real-world contexts. Subsequently, we will contextualize these findings within the framework of our three research questions:\nRQ(1) What is the effectiveness of LVLMs in identifying multimodal fake news?\nContrary to the conclusions drawn by [19, 14], which suggested that LLMs might not effectively replace SLMs in FND, our experimental findings indicate that LVLMs also possess substantial capability as detectors for complex real-world task,"}, {"title": "7. Conclusion", "content": "This study introduces the IMFND framework, designed to enhance FND across three datasets, by leveraging LVLMs. Initially, a zero-shot evaluation of LVLMs and the CLIP model for FND reveals that LVLMs are capable of attaining competitive results, distinct from those observed with traditional LLMs. Subsequent findings demonstrate that while standard ICL can enhance LVLM performance, the degree of improvement remains limited and variable. Finally, it is shown that incorporating predictions and their probabilities from a well-trained smaller model into LVLMs can significantly augment"}, {"title": "8. Limitations", "content": "This study acknowledges certain constraints, including: 1) The comparative analysis of LVLMs is confined to two models (i.e., CogVLM and GPT4V), a limitation imposed by budgetary and hardware constraints; 2) Due to the time-intensive of prompting LVLMs with five random seeds, only a single prompting strategy has been evaluated. The exploration of prompt engineering design remains a subject for future research; 3) CogVLM's performance may be limited by the hardware restriction of employing 4-bit quantization, suggesting potential for improved results with a full-precision configuration."}]}