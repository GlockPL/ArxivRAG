{"title": "Large Visual-Language Models Are Also Good Classifiers: A Study of In-Context Multimodal Fake News Detection", "authors": ["Ye Jiang", "Yimin Wang"], "abstract": "Large visual-language models (LVLMs) exhibit exceptional performance in visual-language reasoning across diverse cross-modal benchmarks. Despite these advances, recent research indicates that Large Language Models (LLMs), like GPT-3.5-turbo, under-achieve compared to well-trained smaller models, such as BERT, in Fake News Detection (FND), prompting inquiries into LVLMS' efficacy in FND tasks. Although performance could improve through fine-tuning LVLMs, the substantial parameters and requisite pre-trained weights render it a resource-heavy endeavor for FND applications. This paper initially assesses the FND capabilities of two notable LVLMS, CogVLM and GPT4V, in comparison to a smaller yet adeptly trained CLIP model in a zero-shot context. The findings demonstrate that LVLMs can attain performance competitive with that of the smaller model. Next, we integrate standard in-context learning (ICL) with LVLMs, noting improvements in FND performance, though limited in scope and consistency. To address this, we introduce the In-context Multimodal Fake News Detection (IMFND) framework, enriching in-context examples and test inputs with predictions and corresponding probabilities from a well-trained smaller model. This strategic integration directs the LVLMs' focus towards news segments associated with higher probabilities, thereby improving their analytical accuracy. The experimental results suggest that the IMFND framework significantly boosts the FND efficiency of LVLMs, achieving enhanced accuracy over the standard ICL approach across three publicly available FND datasets.", "sections": [{"title": "1. Introduction", "content": "The rapid dissemination of fake news across online platforms has presented tangible challenges [1]. Among the strategies to address these challenges, automated fake news detection (FND), designed to autonomously identify inaccurate or deceptive news articles from authentic ones, has proven to be a viable solution in previous studies [2, 3, 4].\nTraditional methods predominantly depend on modeling the textual semantics of news articles, and have revealed inadequacies in addressing the multi-faced nature of fake news [5]. Therefore, multimodal approaches have incorporated pre-trained small\u00b9 language models (SLMs, e.g., BERT [6]) with visual models (VMs, e.g., ResNet [7]) for establishing cross-modal representation, resulting in a more thorough performance for FND [8, 9]. While SLMs are normally pre-trained on open-domain datasets, for example, BERT was originally trained on Wikipedia, they are inherently limited in processing fake news that demands expertise beyond their training scope, and so as that of in the VMs [10].\nRecent developments in large visual-language models (LVLMs), trained on extensive collections of text-image pairs covering a broad spectrum of global knowledge [11, 12], have shown a profound comprehension of cross-modal reasoning and demonstrated proficiency in identifying commonplace occurrences [13]. However, LVLMs are deficient in forgery-specific knowledge [14, 15], which undermines their efficacy in FND. This limitation could be ascribed to (1) media manipulation disrupting the coherence between various modalities, thereby leading to semantic inconsistencies [16]; and (2) the tendency of altered images to exhibit noticeable artifact indications [17]. Meanwhile, the pre-trained weights of LVLMs, like GPT-4, are normally inaccessible. The immense size of LVLMs also makes it challenging to fine-tune for specific tasks even if the model checkpoints are released.\nAlternatively, in-context learning (ICL) empowers LVLMS to acquire the capability to perform downstream tasks through conditioning on prompts composed of input-output samples, circumventing the need for explicit gradients updating [18]. Although previous studies [19, 14] have suggested that large language models (LLMs) might underperform the traditional SLMs in textual-based FND, the potential of ICL in aiding LVLMS for FND remains under-explored. To address the above limitations, this paper presents a framework In-context Multimodal Fake News Detection (IMFND), aimed at exploring three pivotal research questions:"}, {"title": "2. Related work", "content": "Previous studies on FND can be classified into three primary categories: image-based, text-based and multimodal approaches.\nImage-based approach aims to utilize traces of edits to ascertain the authenticity of visual content. Certain research focuses on the spatial domain, identifying artifact traces using techniques such as blending [17], achieving patch consistency [23], and implementing reconstruction methods [24].\nText-based methods have concentrated on the analysis of textual statistical features, such as length, punctuation, and exclamation marks [25], along with metadata attributes like likes and shares [26, 27], to manually detect fake news. Recently, deep learning strategies have predominantly employed BiLSTM [28, 29], GNNs [30], and pre-trained models [31, 32, 33, 34] for text feature analysis.\nMultimodal approaches amalgamate cross-modal features to construct semantic representations. For example, MCAN [8] utilizes several co-attention layers to enhance the integration of textual and visual features for FND. CAFE [9] measures cross-modal ambiguity by evaluating the Kullback-Leibler (KL) divergence between distributions of unimodal features. LIIMR [35] identifies the modality with higher confidence for FND. COOLANT [36] aims to refine the alignment between image and textual representations, applying contrastive learning to achieve precise semantic alignment and cross-modal fusion for understanding inter-modality correlations. However, the efficacy of such methodologies is constrained due to their reliance on substantial volumes of annotated data and their typical training within isolated systems. This overlooks the potential benefits of assimilating and leveraging the expansive knowledge encompassed within LVLMs.\nLLMs [37, 20] have demonstrated exceptional capabilities in a range of downstream tasks. Recently, efforts have been made to expand LLMs' abilities to interpret visual signals. For example, LLaVA [12] and Mini-GPT4 [38] initially enable the alignment of image-text features, subsequently optimizing for visual instruction tuning. PandaGPT [39] utilizes a straightforward linear layer to connect ImageBind [40] with the Vicuna model [41], thereby facilitating multimodal inputs. CogVLM [21] narrows the divide between the frozen pre-trained language model and the image encoder through a trainable visual expert module integrated within the attention and Feedforward Neural Network (FFN) layers.\nAlthough LVLMs have achieved significant advances on diverse cross-modal benchmarks, recent studies [19, 14] suggest that LLM, such as GPT-3.5, still underperforms well-trained smaller models, such as BERT, in FND task, raising the question of whether LVLMs similarly fall short in FND performance. However, while fine-tuning LVLMs could enhance performance, the extensive parameters necessary for LVLMs make it a resource-intensive process for FND.\nICL, as an alternative, utilizes LLMs for new tasks without modifying the model's parameters and was first introduced in the GPT-3 model [18]. It has been successfully applied to downstream tasks, including machine translation [42] and data generation [43]. Recent LVLMs, developed on the foundation of LLMs, have also demonstrated this ICL capability [44, 45, 46]. For example, the demonstration of precise labels can significantly affect the performance of ICL in specific contexts [47].\nHowever, ICL on LVLMs varies due to the supplementary visual information in the demonstrations and the variation in model components. In this paper, we focus on ICL applied to LVLMs in the FND task, seeking to ascertain which component of information within the multimodal fake news demonstrations is of paramount importance."}, {"title": "3. In-context multimodal fake news detection", "content": "In-context multimodal fake news detection (IMFND) framework includes three phases sequentially: (1) Conducting few-shot training on in-context examples using a smaller multimodal model; (2) Integrating predictions and their probabilities from the pre-trained smaller model into the in-context examples and test inputs; (3) Employing the constructed in-context examples and test inputs with LVLMs for final prediction. The overall workflow is shown in Figure 1(c).\nThe standard in-context methodology integrates the ground-truth label with the text input directly, encouraging LVLMs to concentrate on data specific to the task. However, the outputs generated by LVLMs remain characterized by a lack of confidence and are frequently accompanied by uncertainty (e.g., \"I'm sorry, I can't assist with verifying the authenticity of news articles\").\nTo address this, the IMFND first trains a smaller model to be able to generate the predictions and their probabilities for the in-context examples and test inputs. Specifically, a smaller model learns from n samples of a labeled dataset $(t_i, m_i, y_i)$, where $t_i, m_i$ are the text and the image inputs respectively and $i \\in [1, n]$ (i.e., n samples per class), passing to a pre-trained frozen multimodal encoder. Inspired by [48], which posits that few-shot samples from a singular modality may inadequately encapsulate the entirety of a concept class, each training sample is augmented to encompass five distinct feature representations: 1) a solely text feature, denoted as $f_t$; 2) a solely image feature, denoted as $f_m$; 3) the aggregation of L2-normalized features $f_c = [f_t \\oplus f_m]$, where $\\oplus$ denotes the operation of concatenation; 4) an image-to-text cross-attention feature, denoted as $f_{mt}$; 5) a text-to-image cross-attention feature, denoted by $f_{tm}$.\nThe cross-attention $CrossAtt()$ involves interchanging the textual query $f_t$ with the imagery query $f_m$ to derive the cross-attended feature $f_{mt}$ is, and is defined as:\n$f_{mt} = CrossAtt_{tm}(f_m, f_t, f_t) = softmax(\\frac{f_m f_t^T}{\\sqrt{d}}) f_t$ (1)\nConversely, interchanging the imagery query $f_m$ with the textual query $f_t$ facilitates the acquisition of the cross-attended feature $f_{tm}$ as outlined:\n$f_{tm} = CrossAtt_{mt}(f_t, f_m, f_m) = softmax(\\frac{f_t f_m^T}{\\sqrt{d}}) f_m$ (2)\nwhere d denotes the dimensionality of the smaller model. Subsequently, the multimodal features are comprised of the aforementioned five features, each feature is then passed through a linear classifier MLP to deduce five inferred probabilities. Finally, we concatenate the five deduced probabilities into a singular input for a meta-linear MLP classifier, thereby facilitating the formulation of the final prediction:\n$y = softmax(MLP(f_t \\oplus f_m \\oplus f_c \\oplus f_{mt} \\oplus f_{tm})$ (3)\nThe ultimate goal is to allocate a binary classification label of $y_i \\in \\{0, 1\\}$, in which 0 denotes real news and 1 denotes fake news. Therefore, the few-shot smaller model aims to minimize the Cross-entropy loss L:\n$L = -(y_i log(\\hat{y}) + (1 - y_i) log(1 - \\hat{y}))$ (4)\nwhere $\\hat{y}$ is the model inference from the meta-linear classifier MLP after Softmax.\nFollowing the training of the smaller model with few-shot data, in-context examples are then reconstructed, aiding LVLMs in acquiring the FND-specific knowledge imparted by the smaller model. Specifically, the well-trained smaller model is initially deployed to generate predictions and their corresponding probabilities for in-context examples, which are comprised of instances randomly selected from the training set. Notably, the predictions and their probabilities, derived from the linear probing of the $f_t$ and $f_m$ features, are also utilized in formulating in-context examples. \nThe constructed in-context examples serve to augment the LVLMs' comprehension of the interplay between the inputs, ground-truth labels, and the expert knowledge of the smaller model, allowing the LVLMs to confidently produce the final predictions. Meanwhile, probabilities also act as an indicator of the smaller model's uncertainty regarding its predictions. Integrating probabilities into the context enables the LVLMs to rely on predictions when the smaller model exhibits high confidence and to exercise caution in instances of uncertainty. Additionally, probabilities can direct the LVLM's focus to more challenging in-context examples, facilitating learning from these complex examples and potentially enhancing FND performance overall.\nFollowing the assembly of the in-context examples, the constructed test input which encompasses both the predicted label and its probability, is concatenated with the test input and forms a comprehensive input for the LVLMs. Finally, the complete IMFND algorithm can be summarized in Algorithm 1."}, {"title": "4. Experiments", "content": "To evaluate the FND capabilities of the LVLMs, three publicly accessible datasets have been selected, encompassing a broad spectrum of fake news content and specifically including two languages: English and Chinese.\nPolitiFact [2] contains a collection of political news items, each classified as fake or real by specialized assessors, forming a component of the benchmarking initiative FakeNewsNet. By employing the provided data crawling scripts to exclude news items lacking images or possessing invalid image URLs, a total of 198 multimodal news articles are curated.\nGossipCop [2] contains entertainment narratives evaluated on a 0 to 10 scale, where narratives with scores below five are designated as fake news according to the creator of FakeNewsNet. Applying the same retrieval techniques to those used for PolitiFact, a compilation of 6,805 news articles is collected.\nWeibo [49] represents a dataset derived from Chinese social media platforms, encapsulating a multimodal assortment of fake news that includes both textual and visual elements. Real news articles were collected from a credible outlet (Xinhua News), while fake articles were sourced from Weibopiyao, Weibo's sanctioned platform for countering rumors, which compiles information via either public contribution or formal discrediting endeavors. Adhering to the pre-processing techniques employed in previous research [36], this process yielded a total of 7,853 Chinese news articles.\nNotably, a news article may contain several images. To identify the most relevant image, the cosine similarity between each image and the associated text is computed. The image-text pair exhibiting the greatest similarity is retained, as ascertained by the pre-trained CLIP model. The statistics of the pre-processed dataset are shown in Table 2.\nFor each dataset, we first randomly stratified split 20% of data as the test set. Next, the remaining 80% of data are further sampled within five random seeds, to evaluate the robustness of the few-shot performances. For each seed, we conduct n-shot classification, where $n \\in \\{1,3, 5\\}$ and n is the number of samples per class. Consequently, the efficacy of LVLMs is assessed on the designated 20% test set.\nTwo publicly accessible LVLMs are utilized in the experiments:\nCogVLM [21] is an open-source visual-language model that combines 10 billion visual parameters and 7 billion language parameters. It is designed for image understanding and multi-turn dialogue. We utilize the CogVLM-17B version (i.e., cogvlm-chat-hf)\u00b2 to conduct the experiments. To facilitate local execution of the experiments within the bounds of hardware limitations, 4-bit quantization was implemented at the time of loading CogVLM, aimed at reducing the memory demands during inference.\nGPT-4V [20] enables the GPT to process images and respond to inquiries pertaining to them. The experiments use the GPT-4V model through the official OpenAI API3.\nIn preliminary experiments, it is noted that employing higher temperature settings (e.g., 0.8 for LVLMs) led to inadequate classification performance due to the infusion of increased randomness into the LVLM's responses. Following the hyperparameter settings described by [19], a reduced temperature setting (i.e., 0.2) is utilized for the LVLMs to enhance response stability, thereby augmenting reproducibility."}, {"title": "5. Results", "content": "Table 3 presents the accuracy and macro-f1 scores achieved by integrating IMFND with both CogVLM and GPT-4V, compared with the performance of unimodal BERT and multimodal CLIP-LP in few-shot learning settings.\nComparing with the smaller models. The IMFND significantly enhances the FND efficacy of LVLMs over smaller models across various few-shot settings, with notable improvements (i.e., with 15% increase in average accuracy and 16.5% in average macro-f1) observed in GPT4V. Furthermore, it is observed that augmenting the number of in-context examples further amplifies this enhancement, indicating the FND abilities of LVLMs can be stimulated by integrating the additional information into the context.\nWe also investigate the performances between the smaller models (i.e., BERT vs. CLIP-LP) in the few-shot settings. Interestingly, although CLIP-LP demonstrates superior overall accuracy compared to BERT, BERT surpasses CLIP-LP in all 1-shot settings, particularly evident in the Weibo dataset. This observation suggests a potential detriment to the FND performance induced by the quality of image modality.\nComparing with the LVLMs. To investigate if the IMFND can be adapted to different LVLMs, the performances of the IMFND are also compared among two LVLMs. Generally, the GPT4V-IMFND outperforms the CogVLM-IMFND in all few-shot settings. This might be attributed to 1) the utilization of 4-bit quantization in CogVLM-IMFND for local deployment, which could potentially result in lower precision during prediction; 2) the size of CogVLM (i.e., 10B of vision model plus 7B of language model) is relatively much smaller than that of the GPT4V. Overall, the empirical evidence suggests that the IMFND framework can be effortlessly integrated with various LVLMs, consistently enhancing performance on the FND task.\nWe conduct several ablation experiments aimed at investigating the impact of key components in IMFND. This assessment involves evaluating the framework's performance across various complete and partial configurations. In each experiment, IMFND is selectively utilized by systematically removing different components. The results illustrate the performance decay of IMFND in the absence of each component across most configurations, highlighting the significance of each key module within IMFND.\nInitially, a minor reduction in both accuracy and macro-f1 score is observed when the prediction probabilities from the smaller model's in-context examples are eliminated (i.e., w/o proba). This suggests that the prediction probabilities from the smaller model enhance the confidence of LVLMs for decision-making. Further elimination of the smaller model's predictions from the context (i.e., w/o proba & pred) converts the framework into a standard ICL scenario. This results in a marked performance decline, with accuracy falling by 7.9% and 8.7%\nTable 4 demonstrates the zero-shot performances in the LVLMs and the CLIP-LP. Notably, the zero-shot application of CLIP-LP involves immobilizing the pre-trained CLIP model and appending a linear projection layer atop the merged text and image features derived from their respective encoders. The zero-shot configuration for LVLMs directly prompts the model with the inquiry: \"Read this news and its image, do you think this is real or fake news? Just answer if it's real or fake. image News: text\".\nObservations indicate that the LVLMs can achieve comparable performances to those of the zero-shot CLIP-LP. Specifically, the LVLMs exhibit superior performance on Politifact compared to GossipCop and Weibo. This disparity may be partly due to the sequence length constraints of LVLMs (e.g., 2096 tokens for CogVLM) which are typically longer than those of the CLIP model (defaulting to 77 tokens). The average of 2,148 tokens per Politifact article, as shown in Table 2, significantly exceeding that of GossipCop and Weibo, permits LVLMs to analyze more textual data compared to the latter"}, {"title": "5.3. Stability test", "content": "Given that the selection of in-context examples can significantly influence model performance in the few-shot settings, all experiments presented in Table 3 are executed using five randomly selected seeds. The stability of each model is evaluated by calculating the standard deviation of accuracies across diverse few-shot configurations\nIn general, observations indicate a decrease in the standard deviation for each model concurrent with an increase in the number of in-context examples, suggesting enhanced stability with the increment of training samples. Additionally, the overall standard deviations for the IMFND consistently fall below those of the smaller models across all datasets, illustrating that the IMFND facilitates more stable performance outcomes compared to the smaller models in few-shot settings.\nWe argue that the superior stability observed in the IMFND model derives from incorporating reference predictions of the CLIP-LP model. Such a strategy enables LVLMs to focus on enhancing and, as required, consulting CLIP's predictions, thus diminishing the variances induced by diverse in-context examples."}, {"title": "5.4. Case study", "content": "Figure 3 showcases responses generated from CogVLM and GPT4V, employing either IMFND or standard ICL prompting methods.\nObservations indicate that the predictions and their probabilities from the smaller model guide the LVLM to concentrate on specific segments of the news content, thereby aiding in the accuracy of predictions. For example, a 98% confidence level by the image classifier in affirming the news image as fake prompts the LVLM to intensify its focus on evaluating the news images, as shown in the first example of Figure 3(a).\nFurthermore, IMFND prompting not only augments the responses generated from the LVLM, leading to a more comprehensive analysis of the text and image from news article, but also can override inaccurately classified predictions arising from standard ICL prompting, as demonstrated in the second example in Figure 3(a).\nTo enhance our comprehension of the limitations inherent to the LVLM, we also conduct an error analysis focusing on prevalent errors within both IMFND and ICL prompting. Manual examination of these inaccuracies revealed that error classifications by the smaller model could also lead the LVLM to generate incorrect predictions. For example, an image classifier's prediction that news is fake with a 71% confidence level prompted CogVLM to deem it fake news, in contrast, the standard ICL arrived at the correct prediction uninfluenced by the smaller model's misleading information\nAdditionally, it is observed that the presence of credible sources, (e.g., the 'Demoncratic Underground', 'Politico', 'CNN') within the image or text cues LVLMs to classify the content as genuine news articles"}, {"title": "6. Discussion", "content": "The experimental results highlight the proficiency of LVLMs in applications that demand an understanding of complex real-world contexts. Subsequently, we will contextualize these findings within the framework of our three research questions:\nRQ(1) What is the effectiveness of LVLMs in identifying multimodal fake news?\nContrary to the conclusions drawn by [19, 14], which suggested that LLMs might not effectively replace SLMs in FND, our experimental findings indicate that LVLMs also possess substantial capability as detectors for complex real-world task"}, {"title": "7. Conclusion", "content": "This study introduces the IMFND framework, designed to enhance FND across three datasets, by leveraging LVLMs. Initially, a zero-shot evaluation of LVLMs and the CLIP model for FND reveals that LVLMs are capable of attaining competitive results, distinct from those observed with traditional LLMs. Subsequent findings demonstrate that while standard ICL can enhance LVLM performance, the degree of improvement remains limited and variable. Finally, it is shown that incorporating predictions and their probabilities from a well-trained smaller model into LVLMs can significantly augment their FND capabilities, ensuring robust accuracy across diverse LVLMs. Moreover, the proposed IMFND, leveraging standard ICL without necessitating fine-tuning or gradient updates, exhibits flexible adaptability to various LVLMs, with the choice of the smaller model also being versatile."}, {"title": "8. Limitations", "content": "This study acknowledges certain constraints, including: 1) The comparative analysis of LVLMs is confined to two models (i.e., CogVLM and GPT4V), a limitation imposed by budgetary and hardware constraints; 2) Due to the time-intensive of prompting LVLMs with five random seeds, only a single prompting strategy has been evaluated. The exploration of prompt engineering design remains a subject for future research; 3) CogVLM's performance may be limited by the hardware restriction of employing 4-bit quantization, suggesting potential for improved results with a full-precision configuration."}]}