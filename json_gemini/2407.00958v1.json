{"title": "Universal Approximation Theory: The basic theory for large language models", "authors": ["Wei Wang", "Qing Li"], "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large-scale Transformer networks have quickly become the leading approach for advancing natural language processing algorithms. Built on the Transformer architecture, these models enable interactions that closely mimic human communication and, equipped with extensive knowledge, can even assist in guiding human tasks. Despite their impressive capabilities and growing complexity, a key question remains \u2013 the theoretical foundations of large language models (LLMs). What is it about the Transformer architecture that makes it so effective for powering intelligent language applications, such as translation and coding? What underlies LLMs' ability for In-Context Learning (ICL)? How does the LORA scheme enhance the fine-tuning of LLMs? And what supports the practicality of pruning LLMs? To address these critical questions and explore the technological strategies within LLMs, we leverage the Universal Approximation Theory (UAT) to offer a theoretical backdrop, shedding light on the mechanisms that underpin these advancements.", "sections": [{"title": "1 Introduction", "content": "In recent years, the rapid emergence of LLMs in the field of artificial intelligence has undoubtedly become one of the most notable advancements within the domain. The core allure of these models stems from their extraordinary capabilities in language processing. Language, as a unique crystallization of human intelligence, serves not only as the external reflection of thought but also as the bridge for communication, the cornerstone for the dissemination of knowledge, and the continuation of civilization, profoundly shaping the identity of humans as a unique species. Thus, endowing machines with the ability to understand and generate language marks a significant leap towards the realization of true artificial intelligence. The emergence of models such as the ChatGPT (Radford and Narasimhan, 2018; Brown et al., 2020; Achiam et al., 2023), the Llama (Touvron et al., 2023), and the PaLM (Chowdhery et al., 2023) vividly demonstrates this point.\nA distinctive feature of LLMs is their immense parameter size (Achiam et al., 2023; Touvron et al., 2023; Chowdhery et al., 2023; Chen et al., 2021; Zeng et al., 2022), often amounting to hundreds of billions or even trillions (for instance, GPT-3's (Brown et al., 2020) 175 billion parameters and PaLM's (Chowdhery et al., 2023) 540 billion parameters). This vast parameter scale lays the foundation for their exceptional language processing capabilities and enables them to exhibit almost human-like traits, such as ICL (Brown et al., 2020; Dong et al., 2022), instruction following (Sanh et al., 2021; Ouyang et al., 2022; Wei et al., 2021), and multi-step reasoning (Wei et al., 2022). Notably, these colossal models are predominantly trained by tech giants like Google and Microsoft using large-scale GPU clusters (Zhao et al., 2023), sparking a research fervor on how to efficiently fine-tune them with limited GPU resources. The advent of Lora (Hu et al., 2021) fine-tuning technology has provided an effective pathway for this, allowing for the fine-tuning of large models under resource constraints without the need to adjust all parameters of the original model comprehensively. Moreover, model pruning techniques (Sun et al., 2023; Ma et al., 2023) are crucial for deploying large models in resource-constrained environments, aiming to reduce the model size for operation on smaller devices. Faced with the challenge of processing long texts, such as generating summaries or answering questions based on extensive documents\u2014which traditionally requires substantial computational resources\u2014technologies like LongLora (Chen et al., 2023) have been developed to tackle the difficulties of processing long contextual texts, further expanding the application boundaries of LLMs. Today, LLMs possess a diverse range of functionalities, from translation and text summarization to automatic code generation, demonstrating their versatility.\nDespite LLMs being on a fast track of development towards higher intelligence and reliability, the theoretical foundation behind them remains largely unexplored, shrouded in clouds of uncertainty. The scientific community is actively seeking to uncover the inner mechanisms behind their powerful capabilities, including analyses of ICL mechanism (Xie et al., 2021; Min et al., 2022), among others. Against this backdrop, we establish the UAT (Cybenko, 2007; Hornik et al., 1989) as the mathematical essence of LLMS, and utilize the UAT lens to clarify pivotal technologies and phenomena within contemporary LLMs, aiming to illuminate this central theoretical conundrum from a fresh perspective. Our contributions are as follows:\n\u2022 We prove that the Transformer is the tangible embodiment of UAT.\n\u2022 We deliver a rigorous scientific explanation of Transformer-based LLMs through the UAT.\n\u2022 We explain the characteristics of LLMs, such as ICL, instruction following, multi-step reasoning, and the technologies applied within LLMs like Lora, pruning, and LLMs' strong generalization capabilities.\n\u2022 We offer insights into the future development of LLMs.\nOur article is structured as follows: In Section 2, we begin by introducing the UAT and propose that to demonstrate the Transformer's adherence to UAT, it is necessary to show that both Linear and Multi-Head Attention (MHA) can be represented in the form of matrix-vector multiplication. In Section 3, we establish that the Transformer belongs to the category of UAT. Our proof strategy unfolds starting with Section 3.1, where we present the idea to express Linear and MHA as matrix-vector products. In Section 3.2, we detail the computational methods required for the proof. Sections 3.3 and 3.4 are dedicated to proving that both Linear and MHA can indeed be represented as matrix-vector operations. Further in Section 4, we leverage UAT to theoretically elucidate some fundamental issues (Generalization 4.1, ICL 4.2) and techniques (Pruning 4.3 and LoRA 4.4) associated with LLMs. Finally, we conclude with a summary of the existing challenges and potential future developments for LLMs in Section 4.5."}, {"title": "2 The Universal Approximation Theory", "content": "Up to now, the UAT (Cybenko, 2007) is the most widely recognized fundamental theory in deep learning. However, this theorem is only applicable to the simplest form of neural networks, the multilayer perceptron (MLP) (Cybenko, 2007; Popescu et al., 2009). Due to the increased complexity of Transformer networks, they cannot be mathematically expressed in the same form as UAT. Thus, the theory has not yet been extended to Transformer networks. Our aim in this paper is to unify Transformer networks under the framework of UAT, thereby standardising their mathematical representation. Before unifying their mathematical forms, we provide a brief overview of the theorem, which was initially proposed by Cybenko (2007). This theorem encompasses numerous conclusions and proof details. Although it has been further developed, its fundamental mathematical form remains unchanged. Therefore, this paper explains the theory based on the UAT form presented by Cybenko (2007). Theorem 2 from Cybenko (2007) states that if \u03c3 is any continuous sigmoidal function, then finite sums of the following form:\n$$G(x) = \\sum_{j=1}^{N} \\alpha_j \\sigma (W_jx + \\theta_j)$$\n(1)\nis dense in C (In). Here, Wj \u2208 Rn and \u03b1j, \u03b8 \u2208 R are fixed. For any f\u2208 C (In) and \u03b5 > 0, there exists a function G(x):\n$$|G(x) \u2212 f(x)| < \\varepsilon \\text{ for all } x \u2208 I_n.$$\n(2)\nThis implies that, when N is sufficiently large, a neural network can approximate any continuous function on a closed interval. Hornik et al. (1989) further demonstrates that multilayer feedforward networks also conform to the UAT, capable of approximating arbitrary Borel measurable functions. Observing Equation (1), where the function G(x) yields a scalar output in R, the scenario expands naturally when G(x) maps to Rm, requiring the approximation in each dimension. It becomes evident that to accommodate this multidimensional output, a simple extension to Equation (1) suffices: the transformation matrix Wj is revised to the space Rnxm, the bias term \u03b8j is recast as a vector in Rm, and \u03b1j is reshaped into a diagonal vector. Nevertheless, in these demonstrations, the theorem does"}, {"title": "3 UAT for Transformer", "content": "In the previous section, we have outlined our fundamental goal to unify the Transformer model under the framework of UAT by converting Linear and MHA operations in the Transformer into matrix-vector forms. In this section, we will use the Matrix-Vector method to transform Transformer operations into matrix times vector forms."}, {"title": "3.1 Matrix-Vector Method", "content": "Before delving into transforming Linear and MHA into their matrix-vector format, we introduce the Matrix-Vector Method, which will subsequently be employed to cast both Linear and MHA operations into a unified matrix-vector format. This method constitutes a strategic realignment of input data and corresponding parameters of various transformations within the network, as illustrated in Figure 1. The underlying principle is as follows: both the input (x) and output (y) data are reconfigured uniformly through a transformation TD into column vectors (x' and y', while parameter tensor (W) is reorganized into matrix form (W'). A critical requirement is that identical operations throughout the LLMs network adhere to the same restructuring scheme, thereby eliminating the need for additional transformations on intermediate feature data; they can directly be represented as column vectors. Additionally, by default, matrix variables in the original formulas are represented in bold, such as x, and in the Matrix-Vector form, corresponding variables are denoted with a prime symbol (') in the upper right corner, such as x'. Elements within matrices are represented by corresponding lowercase letters with subscripts, for example, xi. The Matrix-Vector Method can be succinctly encapsulated as follows:\n$$y = T(x|W) \\rightarrow y' = W'x'$$\n(3)\nIt is obvious that TD and Tp are not fixed, we could design various kinds of ways to do those. For convenience, we present a methodology tailored for Linear in Section 3.3 and an approach for MHA in Section 3.4, thereby illustrating the adaptability and application of the Matrix-Vector Method across different components of the Transformer architecture."}, {"title": "3.2 Diamond Matrix Multiplication", "content": "While our goal is to transform Linear and M\u041d\u0410 modules into a matrix-vector format, this process is complex. In order to describe this transformation process clearly, we propose a new computational method called the Diamond Multiplication Method, denoted by the symbol \u25c7.\n$$\\begin{pmatrix}\nW_{1,1} & W_{1,2} & \\cdots & W_{1,n} \\\\\nW_{2,1} & W_{2,2} & \\cdots & W_{2,n} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nW_{m,1} & W_{m,2} & \\cdots & W_{m,n}\n\\end{pmatrix} \\diamond \\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_m\n\\end{pmatrix} = \\begin{pmatrix}\nW_{1,1}x_1 + W_{31} * x_2 + \\cdots + W_{m,1}x_m \\\\\nW_{1,2}x_1 + W_{2,2}x_2 + \\cdots + W_{m,2}x_m \\\\\n\\vdots \\\\\nW_{1,n}x_1 + W_{2,n}x_2 + \\cdots W_{mn}x_m\n\\end{pmatrix}$$\n(4)\nLet W\u2208 R(m,n) and x \u2208 R(m,1), the Diamond Matrix Multiplication is defined as y = W \u25c7 x, where y \u2208 R(m,1). The computation procedure involves element-wise multiplication and summation of corresponding elements from left to right across the columns of matrix W with vector x. The sum of the multiplication of the ith column of W with x is assigned as the ith element of y. A detailed calculation process is provided in Eq. ((4)). The computation process for y = x \u25ca W is identical to that of W\u25ca x, implying that W \u25ca x = x\u25c7W. However, it should be noted that the Diamond Matrix Multiplication does not possess the properties of associativity and distributivity.\nFurthermore, based on its computation method, it can be inferred that the Diamond Matrix Multiplication is related to conventional matrix multiplication, specifically: W \u25ca x = WTx. Additionally, when W\u2081 and W2 are square matrices, we can derive the following relationships: W1 \u25c7 [x\u25c7W2] = WW1x and [W1\u25c7x]\u25c7W2 = W[\u25c7W2\u25c7x. Detailed derivations can be found in Appendix A."}, {"title": "3.3 Matrix-Vector Method for Linear", "content": "In this section, we present a way to transform Linear operation into matrix-vector format. Figure 2 illustrates this process: Figure 2.a shows the linear transformation of multi-channel input: Wx = y. Figure 2.b provides a specific example of Figure 2.a, Figure 2.c converts the linear transformation in Figure 2.b into the corresponding matrix-vector representation: W' \u25c7x' = y'. Thus, the linear transformation can be represented in matrix-vector form as follows:\n$$x_{i+1} = W_ix^i \\rightarrow x'_{i+1} = W'_i \\diamond x'^i = (W^i_T) x'^i$$\n(5)\nHere, xi \u2208 R(N,M) and xi+1 \u2208 R(N,M) represent the input and output of layer i, respectively, while Wi \u2208 R(N,N) represents the parameters of layer i. x', x'i+1, and W'i are generated based on xi, xi+1, and Wi using the Matrix-Vector Method. The derivation of the general form can be found in Appendix B.1."}, {"title": "3.4 Matrix-Vector Method for MHA", "content": "We now employ the Matrix-Vector Method to elucidate the inner workings of the MHA. The mechanism is defined by the following equation:\n$$H = MultiHead(Q, K, V)\n= Concat (H_1, ..., H_h) W_O$$\n(6)\n$$Attention(xW_{Q_i} = Q_i, x_iW_{K_i} = K_i,\nx_iW_{V_i} = V_i) = softmax(\\frac{Q_i K_i^T}{\\sqrt{M}})V_i$$\n(7)\n$$= H_iV_i = H_i[x_iW_{V_i}] = H_i$$\nHere, h represents the number of attention heads, and the input x \u2208 R(N,M) is divided into x1,..., xh based on h. The parameters WQi, WKi, and Wvi correspond to xi. The whole process of MHA can be represented in Figure 3. Figure 3.a represnets that the input x is split into X1...X8 based on the number of heads. Figure 3.b represents xiWQi = Qi, xiWKi = Ki, xiWVi = Vi. Figure 3. c, d, e represent the KT process of softmax (QK) V; and we omit \u221a M. Figure 3.f represents Concat (H1,..., Hh) Wo.\n\nFigure 3.g represents the whole process by a matrix multiplication of (WHVO)Tx' = H', where WHVO is generated based H1H8, WV1 Wv8 and Wo. It means that we could represent the whole complex MHA into a matrix multiplication. Next, we give the proof of this process.\nOur objective is to express the MHA as (W')HV\u25cb x' = H'. Prior to transforming the MHA into the matrix-vector form, we need to conduct a comprehensive analysis and clearly define the research object. In Eq. (7), Concat (H1,..., Hr) describes an engineering process that requires mathematical representation. The learning process for the input x primarily consists of two parts: V1,...,Vh and H\u2081, ..., \u0397h. H\u00bf is derived based on the parameters WQi and WKi, and can be directly regarded as parameters matrix now, a deep explanation is given in Section 3.5.\n\nFigure 4.a represents the computation process of H1V1 = H1,..., HhVh = H, and in Figure 4.b, we convert it into the matrix-vector form W'WHX' = H', where W'WH is generated from H1,...,H8 and WV1,...,Wv8. In Figure 4.c, we present a simple example, Figure 4.c.1 depicts Hi [xiWvi], while Figure 4.c.2 represents the matrix-vector format of Figure 4.c.1. More ditails could be found in Appendix B.2.\nFigure 5 illustrates the parameter transformation scenario after incorporating Wo. Figure 5.a demonstrates an example of H Concat (\u01241,..., \u0124 = Wo, while Figure 5.b rewrites Figure 5.a as H' \u25ca W' = H'. Consequently, based on Figures 4 and 5, the entire MHA can be expressed as WHO\u25c7x' = H', where W HVO = WWH W. So the whole MHA can be written as:\n$$H = (W^T_{HVO}) (x^i)'$$\n(8)\nHere, xi \u2208 R(NM,1) and \u1f29 \u2208 R(NM,1) are the input and output of ith layer, respectively, while WHVO E R(NM,NM) denotes matrices generated in accordance with H, Wy and Wo. In this manner, we have expressed MHA as a matrix-vector multiplication. This matrix multiplication representation provides a more concise way to express MHA mechanism.\nBased on the derivations above and referring to Figure 4, we can gain a deeper understanding of the learning process in MHA. In particular, MHA can be regarded as a specialized form of linear transformation, which involves firstly dividing x into x1...xh based on the number of heads, then transforming x\u2081 into x, and subsequently utilizing H, W'vi, and W\u2081 to generate the corresponding parameters WHVoi for x."}, {"title": "3.5 What makes Transformer to be the winner in LLMs?", "content": "The Transformer architecture is founded on two pivotal components: FFN and MHA. FFN is comprised of linear operations. In Sections 3.3 and 3.4, we have showcased the matrix-vector representations for both Linear and MHA. In this section, we delve into why the Transformer has emerged as a key model in the domain of LLMs.\nFrom a mathematical perspective, both the MLP and the Transformer serve as concrete implementations of the UAT. Through the matrix-vector approach, it is straightforward to demonstrate that convolutional networks also realize the UAT, thus sharing the same foundational mathematical principles. However, the question arises: Why does the Transformer hold a dominant position in LLMs? The answer lies in the ingenious design of the MHA. Unlike traditional linear operations, the parameters of MHA are dynamic, adjusting in response to the input data. While the UAT endows models with a powerful approximation capability, fixing the parameters in linear operations means that the function that an MLP can approximate remains unchanged, as illustrated in Figure 6.a. In contrast, the Transformer can adjust its parameters by H, based on the input. These parameters, H, should be regarded as being generated in response to the input, as depicted in Figure 6.b. This adaptability implies that the approximated function can vary with the input. This characteristic also explains the strong generalization capability of Transformer-based neural networks, as they can approximate different functions tailored to the requirements of various tasks, such as those needed for language translation versus mathematical problem-solving."}, {"title": "4 Discussion", "content": "Leverranging on our proof in the previous section that the Transformer is the tangible embodiment of UAT, in this section we address the following critical problems and emplore the technical strategies of LLMs: Why does the Transformer architecture possess such power in enabling intelligent language models, such as translation and program-ming? What enables LLMs' capacity for ICL? How does the LoRA scheme effectively fine-tune LLMs? What justifies the feasibility of pruning LLMs?"}, {"title": "4.1 The fundamental reason for the power of Transformers in LLMS", "content": "Theoretically, we have established that Transformer networks act as concrete implementations of the UAT, leveraging its potent capability to approximate any Borel measurable function, thus ensuring the high-dimensional function approximation advantage of models based on UAT. While UAT possesses immense function approximation abilities, it inherently lacks the capacity of approximating multiple functions simultaneously. However, language tasks are by nature diverse, potentially requiring the approximation of different functions based on the input. For instance, summarizing, translating, and continuing the same text content, the function space remains entirely the same except for minor variations in prompt words. Without the ability to dynamically approximate functions based on input, but merely fitting based on a general function trend, it would lead to approximating the same function, hence producing identical or similar output. Therefore, LLMs need to distinguish and adapt to these nearly identical functions, thereby dynamically generating response functions based on input. The MHA mechanism in Transformers provides LLMs with the capability to dynamically approximate relevant functions based on input. In particular, MHA mechanism allows for the dynamic adjustment of UAT's parameters according to the input, thereby laying the theoretical foundation for Transformer-based LLMs to simultaneously tackle multifaceted tasks such as translation, continuation, summarization, code generation, and solving mathematical problems."}, {"title": "4.2 What enables LLMs to possess ICL capability?", "content": "Contextual interaction, as the core capability of LLMs, permeates every phase from training and fine-tuning to prediction. ICL, multi-step reasoning, and instruction following are intuitive manifestations of this contextual interaction. Leveraging their context-sensitive interaction capabilities, LLMs can exhibit behaviors consistent with ICL, multi-step inference, and instruction following, which are tailored based on contextual cues. Furthermore, contemporary LLMs are evolving towards handling longer contexts; however, this advancement is accompanied by a dramatic increase in computational resource demands due to text expansion. A notable solution to this challenge is LongLORA (Chen et al., 2023), which fundamentally also utilizes contextual interaction mechanisms to facilitate training on extended sequences.\nSo, how does this contextual interaction capability arise within LLMs? The formula H (WHVO) (x\u00b2)' in Figure 5 reveals this mode of contextual interaction. Since (WHVO)T represents a dense matrix (almost devoid of zero elements and whose internal elements are highly correlated), each element in H encapsulates comprehensive information from both preceding and subsequent contexts. This learning of holistic contextual information constitutes the foundation of contextual interaction within LLMs."}, {"title": "4.3 What justifies the feasibility of pruning LLMs?", "content": "Due to the massive size of parameters in LLMs and the subsequent high demand for computational resources, pruning LLMs is pivotal for their deployment. A legitimate question to ask is why LLMs are amenable to pruning? The rationale lies in the presence of excessively low-weight parameters in certain layers of LLMs. To understand this, we can directly analyze it from the perspective of the formula underlying the UAT:\n$$\\left| \\sum_{j=1}^{N} \\alpha_j \\sigma (W_j^T x + \\theta_j) - f(x) \\right| < \\varepsilon $$\n(9)\nfor all x \u0395 In. Let's assume A = {1,2...\u039d}, \u039b\u2081\u222a A2 = A, A10 A2 = \u00d8 and \u03a3\u03f5\u03bb\u03b9 \u03b1\u03c3 \u2192 0. Then we have:\n$$\\left| \\sum_{j \\epsilon \\Lambda_1} \\alpha_j \\sigma (W_j^T x + \\theta_j)\n+ \\sum_{j \\epsilon \\Lambda_2} \\alpha_j \\sigma (W_j^T x + \\theta_j) - f(x) \\right| < \\varepsilon $$\n(10)\nSince | \u03a3\u03b5\u03bb\u2081 \u03b1\u03c3 (Wx+0;) \u2192 0, we have the following inequality:\n$$\\left| \\sum_{j \\epsilon \\Lambda_1} \\alpha_j \\sigma (W_j^T x + \\theta_j) - f(x) \\right|\n- |\\sum_{j \\epsilon \\Lambda_2} \\alpha_j \\sigma (W_j^T x + \\theta_j) || < \\varepsilon $$\n(11)\nTherefore, we have:\n$$\\left| \\sum_{j=1}^{N_1} \\alpha_j \\sigma (W_j^T x + \\theta_j) - f(x) \\right|$$\n$$\n< \\varepsilon + | \\sum_{j=1}^{N_2} \\alpha_j \\sigma (W_j^T x + \\theta_j) |$$\n(12)\nHence, when parameters in certain layers are small enough, we can directly remove those layers since their impact on the final result is minimal."}, {"title": "4.4 How does the LoRA scheme effectively fine-tune LLMs?", "content": "Owing to the substantial computational resources required for training LLMs and their strong generalization capabilities, we believe the focus should be redirected towards efficiently leveraging pre-trained models. Repeatedly training models from scratch incurs considerable computational overhead, rendering the endeavor of repurposing well-trained models in novel tasks both practically valuable and resource-saving. A currently prominent solution addressing this issue is the LORA methodology.\nThe LoRA scheme, illustrated in Figure 7.a, adopts a streamlined approach where T\u2081 symbolically represents the transformations within the network. Given our prior establishment that these transformations can be cast into matrix-vector operations, from the perspective of the UAT, LORA fundamentally constitutes a layer-wise fine-tuning tailored to specific task characteristics. This fine-tuning mechanism is depicted in Figures 7.b.1 and Figure 7.b.2, where W Tamend signifies the parameters post-refinement, thereby embodying an adaptive calibration of the UAT's underlying parameters to enhance task-specific performance."}, {"title": "4.5 Rethinking LLMs", "content": "The capabilities of LLMs have become so formidable that their language processing prowess is increasingly approaching human levels, prompting a core question: How do LLMs differ from humans in language processing? Figure 8 elucidates the contrast between the patterns employed by LLMs and humans. Both start with language encoding-humans through a character-based system and LLMs via numerical arrays\u2014essentially, there is little difference at this foundational level. Given the polysemy of words, context determination is crucial: humans understand context through the activation and transmission of neurons in the human brain, while LLMs use patterns of parameter activation to approximate corresponding functions. Here, the activation of network parameters parallels the neural electrical signalling in the brain, and the approximation of functions corresponds to human comprehension of semantics. From this perspective, the difference in language processing between humans and LLMs seems minimal.\nWe posit that the main differences lie in human autonomy and multimodal perception. Human autonomy enables self-directed learning, allowing knowledge to be validated within real-world contexts and personal experiences, whereas LLMs are confined to function fitting based on the majority of corpora in their datasets, with the goal of optimizing loss minimization. Human multimodality encompasses interpreting the world through senses such as vision, hearing, taste, and touch, infusing vocabulary with rich meanings, while LLMs are inherantly adept to to single-modality numerical representations.\nAnother significant issue in LLMs pertains to the segmentation of word embeddings within the MHA, as shown in Figure 4.a. Does this fragmentation still capture the full meaning of words comprehensively? If there are eight attention heads, does that mean a word has eight different encodings, suggesting a form of modality diversity, which then interacts through MHA to extract contextual insights? Furthermore, by considering LLMs as implementations of UAT, can we directly compute the weights necessary for function approximation from the data, thereby significantly reducing training overhead?"}, {"title": "5 Conclusion", "content": "In this paper, we delve into the theoretical underpinnings of LLMs, demonstrating that contemporary LLMs, primarily constructed with Transformer architectures, embody concrete manifestations of the UAT. The remarkable generalization prowess exhibited by LLMs is attributed to their MHA modules, which enable the adaptation to and approximation of diverse functions based on the presented input data. Contextual interaction emerges as a paramount capability for LLMs, manifesting such abilities as ICL, instruction following, and contextual reasoning. These competencies are enabled by the Transformer's innate capacity to learn from context.\nExpanding upon this understanding, we have provided a rigorous theoretical grounding for key techniques employed in LLMs, including LoRA for efficient fine-tuning and pruning for model compression, elucidating their effectiveness through the lens of the UAT. By leveraging the theoretical framework provided by the UAT, not only can existing methodologies be explained but also avenues for the future evolution of LLMs are illuminated."}, {"title": "Limitation", "content": "Our work primarily delves into explaining a select few characteristics (ICL, Instruction following, and multi-step reasoning) and techniques (such as LORA and Pruning) that we perceive as pivotal within current LLMs. While numerous advancements have been made to Transformer-based LLMs, their fundamental underpinnings largely align with the principles outlined herein. Nevertheless, due to space constraints, this paper does not aspire to exhaustively address every issue or attribute present across all LLMs. Instead, it focuses on elucidating those deemed most significant, thereby offering a concentrated insight into the core of LLMs. It is important to acknowledge that our scope, though carefully curated, is by no means exhaustive, and further exploration is warranted to gain a comprehensive understanding of the extensive landscape of LLMs enhancements and challenges."}]}