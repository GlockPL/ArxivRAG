{"title": "Generalizable Spacecraft Trajectory Generation via Multimodal Learning with Transformers", "authors": ["Davide Celestini", "Amirhossein Afsharrad", "Daniele Gammelli", "Tommaso Guffanti", "Gioele Zardini", "Sanjay Lall", "Elisa Capello", "Simone D'Amico", "Marco Pavone"], "abstract": "Effective trajectory generation is essential for reliable on-board spacecraft autonomy. Among other approaches, learning-based warm-starting represents an appealing paradigm for solving the trajectory generation problem, effectively combining the benefits of optimization- and data-driven methods. Current approaches for learning-based trajectory generation often focus on fixed, single-scenario environments, where key scene characteristics, such as obstacle positions or final-time requirements, remain constant across problem instances. However, practical trajectory generation requires the scenario to be frequently reconfigured, making the single-scenario approach a potentially impractical solution. To address this challenge, we present a novel trajectory generation framework that generalizes across diverse problem configurations, by leveraging high-capacity transformer neural networks capable of learning from multimodal data sources. Specifically, our approach integrates transformer-based neural network models into the trajectory optimization process, encoding both scene-level information (e.g., obstacle locations, initial and goal states) and trajectory-level constraints (e.g., time bounds, fuel consumption targets) via multimodal representations. The transformer network then generates near-optimal initial guesses for non-convex optimization problems, significantly enhancing convergence speed and performance. The framework is validated through extensive simulations and real-world experiments on a free-flyer platform, achieving up to 30% cost improvement and 80 % reduction in infeasible cases with respect to traditional approaches, and demonstrating robust generalization across diverse scenario variations.", "sections": [{"title": "I. INTRODUCTION", "content": "Future in-orbit servicing, assembly, manufacturing, and logistics operations will demand increasingly autonomous rendezvous capabilities. Achieving reliable autonomy requires efficient trajectory optimization, enabling systems to compute state and control trajectories that simultaneously satisfy constraints and optimize mission objectives. In the context of space operations, trajectory optimization must balance two conflicting requirements [1], [2]. On one hand, autonomous rendezvous necessitates strict performance and safety guarantees. On the other hand, computational resources are severely limited, as space-grade processors are orders of magnitude less powerful than their commercial, Earth-bound counterparts. Such computational constraints are further are further exacerbated by the inherently non-convex nature of most relevant trajectory generation problems, making them hard to solve efficiently and reliably onboard an Autonomous Vehicle (AV) [3], [4].\nGiven the aforementioned challenges, there is a growing interest in applying Machine Learning (ML) techniques to enhance traditional trajectory generation methods. This interest in mainly motivated by two factors. First, learning-based techniques offer an appealing paradigm for solving problems characterized by stochastic spacecraft dynamics, multi-step decision-making, and potentially highly non-convex objectives [5], [6]. Second, the computational overhead of using trained ML models during inference is low, potentially compatible with the limited computational capabilities available onboard spacecrafts [6], [7]. However, learning-based methods are often sensitive to distribution shifts in unpredictable ways, whereas optimization-based approaches are more readily characterized in terms of robustness and out-of-distribution behavior. As a result, the deployment of learning-based methods in real-world, safety-critical applications-such as in-orbit Rendezvous and Proximity Operations (RPO)\u2013has been limited.\nRecent work has demonstrated that learning-based warm-starting of numerical optimization is an appealing approach to solving the trajectory generation problem. In this paradigm, ML models provide a close-to-optimal initial guess for a downstream optimization problem, enabling hard constraint satisfaction while efficiently converging to a local optimum near the provided warm-start [8]\u2013[11]. Nonetheless, current approaches are often limited to a single-scenario setting, where most scenario characteristics and problem requirements, such as constraints on the spacecraft state (e.g., target waypoints, obstacles' and keep-out zones' configuration) or performance specifications (e.g., final time requirements), remain fixed across problem instatiations. Consequently, the single-scenario approach limits the range of applications that can be addressed through learning-based trajectory optimization methods.\nTo overcome these limitations, this paper extends the framework introduced in [8], [9] by endowing the transformer model to process comprehensive, multimodal scene representations. By incorporating detailed information about spatial features, dynamic constraints, and performance requirements, the augmented model can generalize to diverse scenario variations.\nThe contributions of this paper are threefold:\n\u2022 We extend the Autonomous Rendezvous Transformer (ART) to handle scenario variations in both the physical"}, {"title": "II. RELATED WORK", "content": "Our work builds upon previous approaches that employ Artifical Intelligence (AI) for spacecraft on-board autonomy [8], [9], [11]\u2013[14], and methods that exploit high-capacity neural network models for control [15]\u2013[17].\nPrior work on Al for spacecraft autonomy can be broadly classified into two categories. The first category focuses on learning representations for action policies, value functions, or reward models [12], [13], [18] using techniques from Reinforcement Learning (RL) or Supervised Learning (SL). Approaches in this category often lack guarantees on performance and constraint satisfaction, and are hindered by the computational expense of simulating high-fidelity spacecraft dynamics, particularly in online RL settings. The second category leverages learning-based components to warm-start sequential optimization solvers [8], [9], [11]. This strategy enables hard constraint satisfaction while achieving faster convergence to a local optimum in the neighborhood of the provided warm-start. However, despite the theoretical appeal of warm-starting optimization solvers, existing methods are typically designed for a single-scenario setting. In this context, most key scene characteristics are assumed to be fixed, and the ML model receives only partial information regarding the spatial features and performance requirements related to a specific task. In practice, this drastically reduces the robustness of the overall framework even in the face of minor problem reconfigurations (e.g., executing the same trajectory with a different final time constraint). To address these limitations, this work extends [8]\u2013[10] by endowing the transformer model with the ability to process diverse input modalities and more accurately represent a given trajectory optimization problem. By doing so, we improve the adaptability and robustness of the framework to variations in problem settings.\nOur method is closely related to recent efforts that leverage high-capacity generative models for control. For instance, prior studies have demonstrated how transformers [15], [16] and diffusion models [17], trained via SL on pre-collected trajectory data, can be effective for model-free feedback control [15] and discrete model-based planning [16]. However, such methods have two primary drawbacks. First, they often ignore non-trivial state-dependent constraints, which are prevalent in practical applications and pose significant challenges for purely learning-based methods. Second, they do not fully exploit the information available to system designers and practitioners, such as approximate knowledge of the system dynamics.\nIn contrast, our method addresses both of these shortcomings by (i) integrating online trajectory optimization to enforce non-trivial constraint satisfaction, and (ii) adopting a model-based approach in transformer-based trajectory generation leveraging available approximations of the system dynamics to improve autoregressive generation."}, {"title": "III. METHODOLOGY", "content": "Let us consider the time-discrete Optimal Control Problem (OCP):\n$\\begin{aligned}\n\\underset{x(t_i), u(t_i)}{\\text{minimize}} \\quad & J = \\sum_{i=1}^{N} J(x(t_i), u(t_i)) \\\\ \n\\text{subject to} \\quad & x(t_{i+1}) = f(x(t_i), u(t_i)) \\quad \\forall i \\in [1, N], \\\\ \n& x(t_i) \\in \\mathcal{X}_{t_i}, \\quad u(t_i) \\in \\mathcal{U}_{t_i} \\quad \\forall i \\in [1, N], \n\\end{aligned}$ (1)\nwhere $x(t_i) \\in \\mathbb{R}^{n_x}$ and $u(t_i) \\in \\mathbb{R}^{n_u}$ are the $n_x$-dimensional state and $n_u$-dimensional control vectors, respectively, $J: \\mathbb{R}^{n_x+n_u} \\to \\mathbb{R}$ defines the cost function, $f: \\mathbb{R}^{n_x+n_u} \\to \\mathbb{R}^{n_x}$ represents the system dynamics, $\\mathcal{X}_{t_i}$ and $\\mathcal{U}_{t_i}$ are the state and control constraint sets associated to the specific scenario of interest of the optimization problem, and where $N \\in \\mathbb{N}$ defines the number of discrete time instants $t_i$ over the full OCP horizon $T_f$. Our approach, as introduced in [8], [9], generates the state and control sequences $X = (x_1,...,x_N), U = (u_1,...,u_N)$ by combining two components:\n$(X, \\hat{U}) \\sim p_{\\theta}(X, U \\mid \\sigma_0),$ (2)\n$(X, U) = Opt (x(t_1), X, \\hat{U}),$ (3)\nwhere $p_{\\theta}(\\cdot)$ denotes the conditional probability distribution over trajectories given an initial condition $\\sigma_0$, learned by a transformer model with parameters $\\theta$, $Opt$ denotes the trajectory optimization problem, and $X, U$ denote the predicted state and control trajectories provided as initial guesses to the optimization problem. As discussed in the remainder of this section, the initial condition $\\sigma_0$ is used to inform trajectory generation in the form of, e.g., an initial state $x(t_1)$, a goal state $x(t_N)$, scene descriptions, or other performance-related metrics (e.g., time constraints).\nIn this section, we first delve into the details of the multimodal trajectory representation employed for transformer training. We then discuss the transformer architecture and training scheme devised to improve its effectiveness in generating close-to-optimal trajectories."}, {"title": "A. Multimodal Trajectory Representation", "content": "A key element to enable reliable generalization to variations in the problem settings is the representation of trajectory data as sequences suitable for modeling by a transformer network [8]. Let us consider a pre-collected dataset of trajectories in the form $T_{raw} = (X_1, U_1, r_1,..., X_N, U_N, r_N)$, where $x_i$ and $u_i$ denote the available state estimate and control signal at time $t_i$, respectively, and $r_i = -J(x(t_i), u(t_i))$ is the instantaneous reward (the negative of the cost function). We define the following multimodal trajectory representation for transformer training:\n$T = (x_1, P_1, S_1, u_1, ..., x_N, P_N, S_N, u_N),$ (4)"}, {"title": "", "content": "where $x_1$ is the initial state, and $S_i$, $P_i$ are sets of scene and performance descriptors providing quantitative information regarding the scenario of interest for Problem (1). Both $S$ and $P$ can be represented through diverse modalities. For instance, $S$ may include various forms of environmental information, such as vector-space representations $X_v$ (e.g., positions and sizes of obstacles, waypoints, or other relevant spatial features), visual data $X_i$ (e.g., camera images or depth maps), and sensor readings $X_s$. Similarly, $P$ may include diverse metrics that define the desired performance of the trajectory. Specifically, we define $G_i$ as the goal or target state, i.e., the state we wish to reach by the end of the trajectory (which can be either constant during the entire trajectory or time-dependent). Another performance metric of interest is the time-to-go $T_i$, expressed as the time remaining until the end of the trajectory, i.e., $T_i = T_f \u2013 t(i)$. We further refer to $R_i$ and $C_i$ as the reward-to-go and constraint-violation-budget evaluated at $t_i$, respectively. Formally, as in [8], we define $R_i$ and $C_i$ to express future optimality and feasibility of the trajectory as:\n$\\begin{aligned}\nR(t_i) &= \\sum_{j=i}^{N} r_j, \\\\\nC(t_i) &= \\sum_{j=i}^{N} C(t_j),\n\\end{aligned}$ (5)\nwhere $C(t_j)$ is an indicator function that checks for constraint violations at time $t_j$:\n$C(t_j) = \\begin{cases}\n1, & \\text{if } (x(t_j), u(t_j)) \\notin \\mathcal{X}_{t_j} \\times \\mathcal{U}_{t_j}, \\\\\n0, & \\text{otherwise}.\n\\end{cases}$ (6)\nThis enriched trajectory representation allows the transformer model to capture the causal relationships between states, scene and performance descriptors, and control profiles, thus representing a core design principle when targeting effective generalization across these dimensions. Crucially, the representation defined in Eq. (4) maintains all the benefits discussed in [9], namely: (i) during training, all performance parameters can be readily computed from raw trajectory data using Eq. (5) and Eq. (6) for $R_i$ and $C_i$, respectively, and by setting $G_i = x_N$, $T_i = T_f$, and (ii) during inference, according to Eq. (2), it allows the user to condition the generation of predicted state and control trajectories $X$ and $\u00db$ through user-defined initial conditions $\\sigma_0 = (x_1, P_1, S_1)$. For instance, given an initial state $x_1$ and corresponding scene description $S_1$, the user may query the transformer by defining a specific choice of $P_1$, e.g., a goal state, final time, cost, or combination thereof.\nTo clarify the notation introduced above, consider the following example:"}, {"title": "B. Transformer architecture for multimodal representation", "content": "In this section, we introduce our proposed transformer architecture and the training scheme designed to handle the multimodal trajectory representation outlined in Section III-A. Our approach is based on two primary architectural components:\nMultimodal Encoder. To effectively process multimodal input sequences, the transformer leverages a three-step architecture. First, each input modality is mapped to a shared embedding space using modality-specific encoders. Depending on the input type, such as camera images $X_i$, vector-field representations $X_v$, etc., the encoder is represented by a differentiable mapping, which may be pre-trained (e.g., ResNet [19] backbone for image data). Second, the resulting embeddings are processed by a GPT model with a causal attention mask [16]. Given a maximum context length $K$, the transformer uses the last $K$ embeddings to update its internal representations. Finally, the processed embeddings are mapped to the corresponding output domain via modality-specific decoders, which typically employ linear transformations. Such transformations project the high-dimensional embeddings onto lower-dimensional outputs, such as state, control, or performance metrics.\nDiverse final times and variable-length sequences. Generating state and control trajectories described by varying final times requires the transformer to produce output sequences of arbitrary lengths. Drawing inspiration from standard practices in Large Language Models (LLM) architectures, we train the transformer model on sequences of arbitrary lengths by randomly selecting sub-sequences of length $K$ from the training data. Once trained, the transformer can generate arbitrary sequence lengths by utilizing the last $K$ elements of each modality to autoregressively predict the next sequence element until the $N$-th element is reached. This approach directly addresses the limitations of previous works [8], [9], which were restricted to fixed sequence lengths where $K = N."}, {"title": "C. Pre-training", "content": "Let us introduce the training scheme used to learn the parameters $\u03b8$ of the transformer model. At a high level, three main processes are required to obtain a functional transformer model: dataset generation, model training, and test-time inference.\nDataset generation. The first step in our methodology involves generating a dataset suitable for effective transformer"}, {"title": "IV. EXPERIMENTS", "content": "The goal of our experimental evaluation is to address the following key questions: (1) Can ART generate effective warm-starts for trajectory optimization in novel scenarios? (2) How does data diversity influence ART's ability to generalize across different conditions? (3) What are the critical design choices necessary to ensure robust generalization?\nIn this section, we evaluate the performance of our proposed approach using a free-flyer testbed. We first describe the free-flyer dynamics and problem specifications, followed by a detailed explanation of the experimental setup and the transformer architecture implemented. We then present simulation results that demonstrate statistically significant improvements in trajectory planning across a range of time constraints and operational scenarios. Finally, we provide an analysis of selected trajectories executed on the real robotic platform."}, {"title": "A. Free-flyer dynamics and problem specification", "content": "We consider a free-flyer system designed to emulate orbital rendezvous maneuvers in a two-dimensional plane [9]. The system is equipped with eight on-off thrusters arranged to provide independent control over both translational and rotational motion, as illustrated in Fig. 1a). By leveraging compressed air, the system achieves near-frictionless movement over a granite surface.\nLet $Oxy$ denote the global Cartesian reference frame. The state vector of the system is defined as $x := (r,\u03c8,v,\u03c9) \u2208 \\mathbb{R}^6$, where $r, v \u2208 \\mathbb{R}^2$, $\u03c8,\u03c9 \u2208 \\mathbb{R}$ are the position, velocity, heading angle and angular rate of the free-flyer, respectively. Furthermore, the control input vector in the global reference frame is given by $u := R_{GB}(\u03c8)\u039b\u2206V \u2208 \\mathbb{R}^3$, where $\u2206V \u2208 \\mathbb{R}^8$ represents the impulsive delta-Vs applied by each thruster, $\u039b \u2208 \\mathbb{R}^{3\u00d78}$ is the thruster configuration matrix, and $R_{GB} \u2208 \\mathbb{R}^{3\u00d73}$ is the rotation matrix from the body-fixed reference frame of the free-flyer $OxbYb$ to the global frame $Oxy$. Due to actuation limits, the delta-Vs applied by each thruster are limited to $0 < \u2206V < \u2206V_{max}$, where $\u2206V_{max} = T\u2206t/m$, and $T, \u2206t"}, {"title": "B. Experimental design", "content": "In our experiments, we aim to isolate (i) the benefits of the initial guess, and (ii) the effects of various design decisions on generalization. Therefore, we keep the formulation and solution algorithm for the OCP fixed across scenarios. Specifically, we leverage Sequential Convex Programming (SCP) to solve the trajectory optimization problem and evaluate the benefits of different warm-starting approaches. For all"}, {"title": "C. Simulation experiments", "content": "In this section, through extensive simulation, we answer the three primary questions introduced in Section IV.\n(1) Can ART generate effective warm-starts for trajectory optimization in novel scenarios?\nTo determine whether the proposed architecture can learn near-optimal behavior and generalize to novel scenarios, we first evaluate the performance of warm-starting the SCP with ART models trained on datasets whereby the scene descriptors (i.e., obstacle configurations and final time specifications) are uniformly sampled from a set of operating conditions of interest. We refer to these models as ART-Rs and ART-Rt for obstacle configurations and final time specifications, respectively. The comparisons are made by varying either the final time or the operational scenario. In the former case, evaluations occur at seven different final times Tf, ranging from 40 s to 100 s in increments of 10 s. In the latter"}, {"title": "(2) How does data diversity influence ART's ability to generalize across different conditions?", "content": "To study the impact of diversity in the training dataset on the learning process, we repeat the aforementioned evaluation process for all the models presented in Table I. Concretely, these models differ in the number of scenarios used during training, ranging from a single scenario across all training trajectories (i.e., -1t and -1s), to a different scenario for every training trajectory (i.e., -Rt and -Rs). Results for generalization"}, {"title": "(3) How does the choice of scene representation influence generalization to novel scenarios?", "content": "In our experiments, we employed a vector-space scene representation leveraging relative obstacle information. Specifically, at each timestep, we denote $\\mathcal{X}_{vi} = [v_1,..., v_i]$, with $v_i^{(m)} = [r_i - r_i^{(m)}, ||r_i - r_i^{(m)}||_2 - R^{(m)}]$, and $r_i^{(m)}$, $R^{(m)}$ being the coordinates of the center and the radius of the $m$-th obstacle expressed in $Oxy$, respectively. This choice results in substantially improved generalization over using e.g., the absolute position of the obstacles. Specifically, let us consider an alternative version of ART-1s, whereby we utilize only absolute obstacle information, i.e. $X_v^{(m)} = [r_i^{(m)}, R^{(m)}]$. Statistical analyses indicate that the ART-1s model trained with absolute obstacle information substantially deteriorates its performance compared to its relative-observation counterpart- +77% average cost, +49% average number of SCP iterations, +290% average infeasibility rate. These findings illustrate that relative scene representations are crucial to maximize the generalization capabilities of ART, even when trained on a single scenario. More broadly, similar insights are likely to apply beyond obstacle representations. Therefore, selecting the most effective input representations should be a key consideration in the design of learning-based trajectory generation approaches."}, {"title": "D. Experimental results", "content": "Finally, the effectiveness of the proposed framework is tested on the real free-flyer platform introduced in Sec. IV-A. Specifically, we employ the best models identified through simulations in Sec. IV-C to warm-start the SCP optimizer and we vary either the obstacles' configuration (ART-Rs) or the final time constraint (ART-4t) of Problem (9). The planned trajectory is then tracked by a downstream PID controller with $K_p = diag([2.0, 2.0, 0.2])$, $K_p = diag ([10.0, 10.0, 0.4])$, $K_i = diag([0, 0, 0])$.\nQualitative results for both types of constraints generalization are shown in Fig. 5. In the context of obstacle avoidance constraint generalization (left), we test ART-Rs in two of the three held-out scenarios introduced in Sec. IV-C. It is evident that ART-Rs demonstrates a robust capability to adapt the generation process in response to the current configuration of obstacles. This results in a final SCP solution that strategically leverages advantageous features of the scenario, e.g. the diagonal corridor shown in Fig. 5 (left), rather than navigating through narrower spaces between obstacles. The cumulative firing time of all the eight thrusters of the platform is 88.34s.\nIn the context of time constraints generalization (right), we assess the warm-starting performance of ART-4t utilizing two different final times, 40 s and 80 s. In both cases, ART-4t provides an effective warm-start to the SCP, converging to two similar paths. The main differences between the two solutions is clearly visible in Fig. 6, depicting the reference velocities computed by SCP ART for the free-flyer, as well as its real velocities achieved using the PID controller. Crucially, the specification of a larger final time constraint ($T_f = 80$ s)"}, {"title": "V. CONCLUSIONS", "content": "In conclusion, this paper presents a novel trajectory generation framework which leverages the capabilities of transformer-based neural networks to address challenges related to multi-scenario spacecraft operations. By integrating multimodal data into the learning routine, the proposed approach allows one to generate near-optimal trajectory guesses, improving the efficiency and robustness of traditional optimization methods. Specifically, the experimental results showcased i) how the proposed framework can generate effective warm-starts for trajectory optimization in previously unseen scenarios, ii) the impact of data diversity, and iii) the impact of different scene representations on generalization capabilities. The presented results highlight the potential of transformer-based models to overcome the limitations of single-scenario trajectory generation frameworks, and underline their applicability to a broader range of space operations. Future research interests primarily focus on three key areas. First, this research paves the way for extending the current framework to a broader class of complex trajectory optimization problems in spacecraft applications, such as in-orbit servicing, assembly, manufacturing, and logistics operations. Second, enhancing the ART framework by introducing ad-hoc runtime monitors to provide stronger safety guarantees, particularly in detecting erroneous warm-starts from ART. Third, exploring the robustness of the proposed methodology in the presence of uncertainties, such as those arising from navigation, actuation, and unmodeled system dynamics."}]}