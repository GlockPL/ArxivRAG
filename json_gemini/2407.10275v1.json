{"title": "Cross-Lingual Multi-Hop Knowledge Editing \u2013 Benchmarks, Analysis and a Simple Contrastive Learning based Approach", "authors": ["Aditi Khandelwal", "Harman Singh", "Hengrui Gu", "Tianlong Chen", "Kaixiong Zhou"], "abstract": "Large language models are often expected to constantly adapt to new sources of knowledge and knowledge editing techniques aim to efficiently patch the outdated model knowledge, with minimal modification. Most prior works focus on monolingual knowledge editing in English, even though new information can emerge in any language from any part of the world. We propose the Cross-Lingual Multi-Hop Knowledge Editing paradigm, for measuring and analyzing the performance of various SoTA knowledge editing techniques in a cross-lingual setup. Specifically, we create a parallel cross-lingual benchmark, CROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive analysis over various knowledge editing techniques uncover significant gaps in performance between the cross-lingual and English-centric setting. Following this, we propose a significantly improved system for cross-lingual multi-hop knowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and generate knowledge editing framework, where a retriever is formulated to recall edited facts and support an LLM to adhere to knowledge edits. We develop language-aware and hard-negative based contrastive objectives for improving the cross-lingual and fine-grained fact retrieval and verification process used in this framework. Extensive experiments on three LLMs, eight languages, and two datasets show CLEVER-CKE's significant gains of up to 30% over prior methods.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are seeing an increasing adoption across users having different cultural and linguistic background, and need to be up to date about the ever-changing knowledge in the world for maintaining their utility and reliability in various applications. Due to the ever increasing compute and data requirements to train these models, there has been a surge in the development of knowledge editing techniques to modify the language models in an efficient way, such that they adhere to the world dynamics.\nPrior work on knowledge editing has largely focused on editing LLMs in a monolingual setting (Zhong et al., 2023; Gu et al., 2024), where both user queries and edited facts are expressed in the form of English. These works can be grouped into two categories: parameter-update and parameter-preserving methods. The former directly updates the parameters within LLMs for updating knowledge about the edited facts through meta-learning, fine-tuning, or knowledge locating (De Cao et al., 2021; Dai et al., 2022; Mitchell et al., 2022a; Meng et al., 2022a,b). The later approach freezes the parameters and explicitly stores the edited facts in an external memory and retrieves them for answering user queries (Zhong et al., 2023; Gu et al., 2024; Mitchell et al., 2022c; Hartvigsen et al., 2023). Existing monolingual knowledge editing techniques aren't broadly applicable since new knowledge can emerge in different languages. Some works have made progress in this direction (Beniwal et al., 2024; Xu et al., 2023a; Si et al., 2024), but they have considered a simplistic setting of assuming the edited facts as independent without any multi-hop rippling consequences on entailed reasoning process, and are primarily focused on parameter-modifying based editing methods.\nThere has only been a limited focus on the realistic case of cross-lingual multi-hop knowledge editing (see Fig 1), where the edited knowledge can come in through users who communicate in different languages. Further, much of edited knowledge often has a rippling effect on other facts of the world. For example, the club change of Messi affects deduction process of question \u201cindicating a superficial word matching rather than a contextual grasp of the entities involved.\" This knowledge editing setting, which we argue is important to study, is challenging since the model needs to transfer knowledge about fact edits between different languages, while also reasoning about the facts which are modified as a consequence to the given edit. Poor knowledge transfer between languages can lead to error propagation across reasoning steps which can increase failure cases of model editing.\nIn this work, we formulate the notion of cross-lingual multi-hop knowledge-editing and analyze existing approaches for their editing ability in different languages, following which a simple yet highly effective approach is designed. Specifically,\nWe create one of the first benchmark datasets for measuring cross-lingual multi-hop knowledge editing capabilities of knowledge editing methods. Besides parameter-update based approaches, we contribute strong retrieval-based baselines for knowledge editing and provide a comprehensive analysis.\nWe provide a detailed analysis and find significant gaps in the performance of methods for cross-lingual knowledge editing. The gaps are mainly due to challenges in accurately recalling fact edits made in language other than input query.\nTo bridge such gap, we design a competitive method, termed as Contrastive Language-aware Verification for Cross-lingual Knowledge Editing (CLEVER-CKE), for improving performance of cross-lingual multi-hop knowledge editing. Our approach is based on decomposing a multi-hop question in a particular language into sub-questions and retrieving fact edits (if any) from memory using a cross-lingual retriever, which is integrated for answering sub-questions. In particular, the cross-lingual retriever is regularized by novel language-guided and hard-negative based contrastive losses, which leads to improved language and fine-grained sentence understanding of the edits, leading to high quality cross-lingual retrievals. CLEVER-CKE improves over previous SoTA by up-to 30% increase in knowledge editing accuracy when tested on multiple LLMs, datasets and languages."}, {"title": "2 Cross-lingual Multi-hop Editing", "content": "Following prior work (Zhong et al., 2023), a fact is defined as a triplet (s, r, o), where s is the subject, o is the object, and r is the relation (e.g., Shakespeare, author of, Hamlet). Given that a parametric LLM can become outdated or incorrect, knowledge editing is required to be performed on it. An edited fact stores information about updated knowledge of an existing fact and is denoted as e = (s, r, o*), where the object is replaced with a new one o*.\nCross-Lingual Knowledge Editing. Each knowledge fact or edit is assumed to be represented in natural language. Let $\\mathcal{T} : E \\rightarrow L$ be a function which takes any fact e \u2208 E (e.g., Shakespeare, author of, Hamlet) and converts it into a natural language statement, (e.g., Shakespeare is the author of Hamlet). All the facts and edits can be represented in a variety of languages {L\u2081, L\u2082,...} via functions such as {$\\mathcal{T}_{L1}$, $\\mathcal{T}_{L2}$, ...}. For example, an edit e =(Shakespeare, author of, Lolita) can be written as $\\mathcal{T}_{de}(e)$ = Shakespeare ist der Autor von Lolita in German and $\\mathcal{T}_{en}(e)$ = Shakespeare is the author of Lolita in English.\nWe consider a collection of n fact edits in the diverse languages: $E = {e^{L\u2081}_{1}, e^{L\u2082}_{2},..., e^{L_i}_{n_i}}$, where L\u2081, L\u2082,..., L\u1d62 are different languages for e.g., German, Hindi, Swahili, etc. A language model f is said to be edited with new knowledge facts if the model generations adheres to all the edits present in E. The model is required to seamlessly transfer knowledge about an edit in one language to answer queries in other languages.\nMulti-Hop Editing and Evaluation. We follow Zhong et al. (2023) for evaluating knowledge editing via multi-hop question answering. Consider $e^{L\u2081} = (s^{L\u2081}_{1}, r^{L\u2081}_{1}, o^{L\u2081}_{1})$, an edited fact in language L\u2081. Also consider a chain of facts $P = ((s^{L\u2082}_{1}, r^{L\u2082}_{1}, o^{L\u2082}_{1}), ..., (s^{L\u2082}_{h}, r^{L\u2082}_{h}, o^{L\u2082}_{k}))$, where object of a fact is the subject for the next fact. Any edit to the first fact $(s^{L\u2081}_{1}, r^{L\u2081}_{1}, o^{*L\u2081}_{1})$ will likely have a rippling effect and change the subsequent facts in the chain, and we expect a successfully edited model to be aware of all such entailed changes.\nFor evaluating models in a cross-lingual multi-"}, {"title": "3 CROLIN-MQUAKE Benchmark", "content": "We develop one of the first parallel cross-lingual for measuring the knowledge editing capabilities of the existing approaches. A parallel benchmark has the same test examples across all the languages, enabling a direct comparison between them. For this, we use existing datasets measuring the multi-hop model editing in English: MQUAKE-CF and MQuAKE-T released by Zhong et al. (2023), which have counterfactual edits and real-world temporal edits respectively. We translate one fact edit per example in these datasets using Google Translate (Google) into 7 languages with diverse writing scripts across medium to high resourcedness - German, Spanish, Chinese, Rissian, Hindi, Bengali, Swahili. This results in the benchmark: Cross-Lingual Multi-Hop QnA for Knowledge Editing (CROLIN-MQUAKE). It has two datasets, CROLIN-MQUAKE-CF and CROLIN-MQUAKE-T, each having 8 languages, and 3k and 1.8k parallel examples (same examples in all languages) per language, respectively. The translations are verified by human experts proficient in particular languages and evaluation of BLEU score (Papineni et al., 2002) using backtranslation. We find that the translation is highly accurate, since we study medium to high resource languages. See Section A.2 for more details.\nConcurrently, Wei et al. (2024) created a multilingual knowledge editing dataset using Wikipedia, offering translocalized knowledge but lacking parallel multilingual examples like ours. CROLIN-MQUAKE enables comparing the knowledge editing performance difference across languages directly without being affected by the variation of test sets between different languages."}, {"title": "4 Benchmark Analysis on Cross-Lingual Multi-hop Knowledge Editing", "content": "LLMs. We use SoTA propriety and open-source LLMs: ChatGPT (Schulman et al., 2022), LLaMa-2-7B (Touvron et al., 2023b), Vicuna-1.5-7B (Chiang et al., 2023) as backbones to evaluate cross-lingual multi-hop knowledge editing.\nEvaluation Metrics. We use multi-hop accuracy proposed by Zhong et al. (2023) which measures the accuracy of the final answer of a multi-hop question. We also adopt hop-wise answering accuracy for checking the correctness of intermediate reasoning steps, as proposed by Gu et al. (2024).\nNew Baselines. Based on existing work, we contribute strong baselines for the new editing setup:\n\u2022 MeLLo-CL: We modify the existing method of MeLLo (Zhong et al., 2023) by replacing the monolingual retriever used in their system with a multilingual retriever. This minimal modification allows the system to retrieve the cross-lingual edits. MeLLo-CL is a simple retrieval-based knowledge editing approach: LLM first breaks down a multi-hop question into various sub-questions and for each sub-question, the retriever then recalls the most relevant fact from an external memory. The LLM disambiguates if the retrieved fact is useful for answering the question or not.\n\u2022 PokeMQA-CL: PokeMQA is similar to MeLLo but consists of a conflict disambiguator for retrieving as well as classifying if a fact is useful to answer a sub-question. Following PokeMQA, we train this disambiguator using BCE loss with negative sampling for retrieving the close edits, given a decomposed sub-question. However, our training dataset now consists of translated version of the training dataset used in PokeMQA. This training set contains all 8 languages (the multilingual setting) or English along with one of the 7 non-English languages (the bilingual setting)."}, {"title": "5 CLEVER-CKE for Knowledge Editing", "content": "For overcoming limitations in cross-lingual multi-hop knowledge editing, we design CLEVER-CKE, a cross-lingual and light-weight model editor that seamlessly integrates into any backbone LLM, without changing its parameters. CLEVER-CKE is inspired by memory-based and retrieval-augmented knowledge editing methods (Zhong et al., 2023; Gu et al., 2024; Mitchell et al., 2022b) for mutlihop question answering. CLEVER-CKE follows the following procedure: Given an input query, it a) decomposes the multi-hop question into multiple sub-questions for getting to the final answer, and for answering each sub-question b) retrieves a relevant fact from the edit memory, c) disambiguates whether the retrieved new knowledge is relevant to answering the sub-question, and d) continues the model generation process based on that. In this work, we primarily aim at showing the importance of having a high-quality retriever for the retrieve-and-verify steps at b) and c) described as follows. See Fig. 3 for an overview.\nMemory of Fact Edits: CLEVER-CKE explicitly stores a set of knowledge edits E in a memory F. Each edit triplet e = (s, r, o) \u2208 E is converted to a natural language statement in either English or another language using English or translated templates present in CROLIN-MQUAKE. This creates a multilingual edited fact memory.\nSub-question Decomposition: Given a multi-hop question Q, LLM is prompted using in-context examples to decompose it into various sub-questions Qsub = {q\u2081, q\u2082,...}. Note that Q and the language model generation is assumed to be in English in our work whereas the edited fact memory can contain both English and non-English knowledge edits. The LLM is instructed to answer the generated sub-questions as follows.\nRetrieve-and-Verify: For each sub-question q, CLEVER-CKE retrieves the top-1 candidate r \u2208 F using cosine similarity. Verification process then answers the question: Does r help answer q? The answer to this is yes if cos(f(r), f(q)) \u2265 t where cos(.) is the cosine similarity function, f(.) \u2208 Rd is the retriever embedding and t is a threshold (hyperparameter). In this case, r is passed to the LLM which uses it for generating the answer to the sub-question. If cos(f(r), f(q)) < t, only the LLM's internal knowledge is used to answer the question. Following this, LLM will move on to answering the next sub-question. Note that here, the disambiguation of whether r is useful or not, happens external to the LLM, reducing its reasoning complexity.\nCLEVER-CKE Retriever Training: Motivated by gaps found in Section 4, we create new objectives for training the retriever for improving fine-grained and cross-lingual representations. We then show that our simple losses provide significant gains in knowledge editing performance.\nSemantic Distinction Loss: We employ a contrastive, triplet margin loss $L_{SD}$ for improving fine-grained cross-lingual retrieval. Assuming an edits e = (s, r, o), we obtain its natural language forms $T_{L\u2081}(e)$, $T_{L\u2082}(e)$ in languages L\u2081, L\u2082 respectively. This creates a positive pair for the triplet loss. We generate hard negatives for $T_{en}(e)$ in English by replacing an edits' subject, object, or both object with random entities, with a probability of 0.33 each. This process involves extracting all relations in MQUAKE dataset and prompting the GPT-3.5 model to suggest head/tail entities for these relations. We then randomly sample any generated head/tail (or both) for replacement in an edit containing the corresponding relation. Following this, the hard negative example $T_{en}(e_{neg})$ is translated to L\u2081 and hence a negative pair ($T_{L\u2081}(e)$, $T_{L\u2081}(e_{neg})$ is obtained. The loss function is formulated as:\n$L_{SD} = max(d(f(T_{L\u2081}(e)), f(T_{L\u2082}(e))\\)\n\\(-d(f(T_{L\u2081}(e)), f(T_{L\u2081}(e_{neg})) + \u03b1, 0). \\tag{1}$\nf(\u00b7) represents the retriever embedding, d(\u00b7) represents the distance function, and \u03b1 is a gate hyperparameter. $L_{SD}$ promotes learning the fine-grained knowledge about subject, relation and object in a cross-lingual setting and encourages the model to distinguish the semantic nuances in different edits. This mitigates the redundant selection of edits with significant word overlap.\nCross-Lingual Edit Consistency Loss: We employ a contrastive, triplet margin loss $L_{CLEC}$ focused on improving cross-lingual retrieval. Here, the anchor is $Q_{en}$, a question in English. The edited fact for answering that question, $T_{L\u2081}(e)$, serves as the positive example, and a random edit $T_{L\u2082}(e_{rand})$ forms the negative example:\n$L_{CLEC} = max(d(f(Q_{en}), f(T_{L\u2081}(e))\\)\n\\(-d(f(Q_{en}), f(T_{L\u2082}(e_{rand})) + \u03b1, 0). \\tag{2}$\nBCE Loss: Following (Gu et al., 2024; Mikolov et al., 2013) we add a binary cross-entropy loss in the cross-lingual setting as a baseline loss for training the retriever for retrieving edits in a cross-lingual setting. The negative BCE Loss function takes questions in English and their corresponding edited facts in one of the seven languages as input. We then compute the L2 norm between these edits and questions, and sample 20 negatives. The loss function L is defined similar to Gu et al. (2024):\n$L_{BCE} = log g(T_{L\u2081}(e), f(Q_{en}))\\)\n\\(- E_{qn~Pn}(q) [log(1 - g(T_{L\u2081}(e), q_{n}))], \\tag{3}$\nwhere $P_n$ is a uniform over each mini-batch, and g(.) = exp(d(.)).\n$L_{CLEC}$ and $L_{BCE}$ encourage it to differentiate between edits in different languages and enhance its ability to handle multilingual knowledge editing tasks effectively. The total loss we use is then:\n$L_{total} = L_{SD} + L_{CLEC} + L_{BCE}. \\tag{4}$"}, {"title": "5.1 Performance of CLEVER-CKE", "content": "We train the retriever with the above losses on a dataset of 8 languages and measure performance on the CROLIN-MQUAKE. In Table 1, on average across languages and across different LLMs, CLEVER-CKE improves over previous methods by up-to 5.7% in accuracy on CROLIN-MQUAKE-CF and we see a much larger increase in the hop-accuracy which suggests faithful reasoning. On the real world temporal dataset CROLIN-MQUAKE-T, we see a significant increase of about 30% accuracy and more than 25% in hop-accuracy metric. Performance gains are large and consistent or better for larger and more capable models like ChatGPT, as compared to LLaMa-2/Vicuna-1.5. Refer to Figure 8 which illustrates an example where other methods make errors, while CLEVER-CKE correctly answers the question.\nPerformance across n-hops: We compare the performance of MeLLo, PokeMQA and CLEVER-CKE in answering n-hop questions, n \u2208 2, 3, 4 using CROLIN-MQUAKE-CF dataset and ChatGPT as the LLM. As shown in Fig. 4, CLEVER-CKE outperforms PokeMQA-CL and MeLLo-CL with an average performance increase of 30.7% for 2-hop questions, 22.6% for 3-hop questions, and 5% for 4-hop questions. Fig. 6 presents language-wise accuracies for these methods for n-hop questions, showing the superior performance of CLEVER-CKE compared to other methods.\nBilingual vs Multilingual retriever: To compare performance differences with increasing the number of languages, we trained PokeMQA-CL and CLEVER-CKE's retrievers in a bilingual setting using English and the target language. See Fig 5 for results. As expected, on average the bilingual setting has greater performance than the multilingual setting, potentially due to interference of multiple languages in the multilingual setting. We interestingly observe that this gap is minimal in the case of CLEVER-CKE, compared to PokeMQA-CL. This is because CLEVER-CKE's losses lead to better cross-lingual knowledge transfer leading to reduced interference of languages and more generalization. This observation generalizes across LLMs and datasets we tested on. Language-wise performance comparison of the two retriever setups for PokeMQA and CLEVER-CKE using ChatGPT, LLaMa-2-7B and Vicuna-1.5-7B are in Tables 6-11."}, {"title": "6 Related Works", "content": "Cross-lingual knowledge editing. Recent studies have shifted focus to the multilingual capabilities of SOTA LLMs like LLaMA (Touvron et al., 2023a), ChatGPT (Schulman et al., 2022), and GPT-4 (OpenAI, 2023). Wang et al. (2023a) investigated cross-lingual knowledge editing and its impact on different target languages using a synthetic dataset. (Si et al., 2024) introduced Multilingual Patch Neuron (MPN) for efficient cross-lingual knowledge synchronization, showing enhanced performance on single-hop XNLI and XFEVER datasets. (Xu et al., 2023b) proposed a framework for language anisotropic editing, facilitating simultaneous cross-lingual model editing. (Beniwal et al., 2024) explored the cross-lingual model editing (XME) paradigm, revealing performance limitations in multilingual LLMs for hypernetwrok based parameter-modifying methods. (Wang et al., 2023b) presented Retrieval-augmented Multilingual Knowledge Editing (ReMaKE), a model-agnostic knowledge editing method designed for multilingual settings. ReMaKE retrieves new knowledge from a multilingual knowledge base and concatenates it with prompts to update LLMs. Most works assume edited facts are independent without any multi-hop consequences of these edits, and focus on parameter update based methods. We focus on parameter-preserving methods, and the more complex setting of multi-hop editing in a cross-lingual setup. See A.1 for more."}, {"title": "7 Conclusion", "content": "In this paper, we contributed a benchmark having parallel multilingual examples for evaluating cross-lingual multi-hop knowledge editing. We provide new baselines and a detailed analysis of SOTA knowledge editing methods and find various gaps in existing methods, particularly in the cross-lingual setting. Motivated by this, we propose a generic, simple and highly effective method, CLEVER-CKE, for improving the knowledge editing capabilities of parameter-preserving, retrieval augmented editing methods. CLEVER-CKE improves cross-lingual and fine-grained retrieval in the case of knowledge editing, by introducing language aware and hard-negative mining based contrastive losses to train retrievers. Improved retrieval leads to precise knowledge retrieval and reduced error propagation in the multi-hop reasoning setting. CLEVER-CKE is parameter-preserving in terms of the LLM weights, and uses a lightweight retriever with low latency as compared to methods like Zhong et al. (2023)."}, {"title": "8 Limitations", "content": "Our analysis and methods has some limitations. Firstly, although CROLIN-MQUAKE is a parallel cross-lingual benchmark, it predominantly contains fact edits related to English-speaking knowledge changes, while the edits could be localized to any part of the world in practice. This reliance on translation rather than trans-localization may lead to gaps in accurately understanding regional and local fact edits. However, having parallel data in all languages is advantageous to accurately measure per-language performance without confounding factors. Secondly, our method is primarily focused on the retriever component and does not address the inherent inaccuracies of the LLMs. This includes issues such as understanding and generation capabilities of LLMs in different languages, correctly breaking down multi-hop questions into sub-questions, accurately extracting the final answer in the desired language. Lastly, our analysis is currently limited to a broad range of medium to high-resource languages. Extending this analysis to low-resource languages presents a significant challenge due to the inaccuracies in translation, which can hinder the proper representation and understanding of facts in low resource languages. Improving translation accuracy and extending our work to low-resource languages is part of our future work."}, {"title": "A.1 Related Work", "content": "Knowledge editing methods: Knowledge editing can be broadly classified intro two groups. 1) Parameter-modifying based editing which locates the parameters related to factual knowledge and subsequently modify them (De Cao et al., 2021; Dai et al., 2022; Mitchell et al., 2022a; Meng et al., 2022a,b). These method requires an error-prone analytic step to identify parameters, which might be model-specific and not efficient. 2) Parameter-preserving based editing keeps the model parameters frozen and explicitly stores the fact edits in an external memory, for retrieval and external validation (Zhong et al., 2023; Gu et al., 2024; Mitchell et al., 2022c; Hartvigsen et al., 2023). some recent works like that of (Hernandez et al., 2023) have also explored a decoding time approach for editing knowledge.\nCross-lingual knowledge editing. Recent studies have shifted focus to the multilingual capabilities of SOTA LLMs like LLaMA (Touvron et al., 2023a), ChatGPT (Schulman et al., 2022), and GPT-4 (OpenAI, 2023). Wang et al. (2023a) investigated cross-lingual knowledge editing and its impact on different target languages using a synthetic dataset. (Si et al., 2024) introduced Multilingual Patch Neuron (MPN) for efficient cross-lingual knowledge synchronization, showing enhanced performance on single-hop XNLI and XFEVER datasets. (Xu et al., 2023b) proposed a framework for language anisotropic editing, facilitating simultaneous cross-lingual model editing. (Beniwal et al., 2024) explored the cross-lingual model editing"}, {"title": "A.2 Verification of Translated Data in CROLIN-MQUAKE", "content": "A.2.1 Human Verification of Translation\nWe randomly selected 50 edits in four lan-guages\u2014German, Chinese, Hindi, and Ben-gali\u2014and had the translations verified by expert human annotators to ensure accuracy. For each sample, we provided two sentences: one in English and its translation in the respective language. The annotators were asked to verify whether the seman-tic information was consistent between the two sen-tences. Given the brevity of the edit sentences, the potential for translation errors was minimal. Only one sample from Hindi in the CROLIN-MQUAKE-CF dataset encountered an issue during transla-"}, {"title": "A.3 Training Details", "content": "We employ the training dataset to train the retriever component of the CLEVER-CKE framework, using the same training set as utilized in training PokeMQA-CL (Gu et al., 2024). Subsequently, we translate this dataset into seven other languages and generate hard negatives following the method outlined in Section 5. The training dataset contains 6688 samples along with translations into 8 languages and hard-negative pairs for each edit in the dataset, both of which is created by us for training CLEVER-CKE's retriever. For training the multilingual retriever, we utilize data from all languages,"}, {"title": "A.4 Method Details", "content": "We finetuned distilbert-base-multilingual-cased (Sanh et al., 2019) with approximately 130.7M parameters from the HuggingFace transformers library on the training data we created by translation and hard negative mining for the edits as described in Section 5 using our designed training objectives for the retriever. We used held out 20% of the samples for the validation set and used Adam optimizer to update the parameters during training."}, {"title": "A.5 CROLIN-MQUAKE Benchmark Statistics", "content": "See Table 5 for the dataset statistics of our benchmark CROLIN-MQUAKE, which we create in this work and use it for evaluating the cross-lingual multi-hop knowledge editing capabilities of various model editing techniques. Languages studied in this work and supported by CROLIN-MQUAKE are English, German, Spanish, Hindi, Swahili, Bengali, Russian, Chinese."}, {"title": "A.6 Prompts for LLM inference", "content": "To help the LLM break down questions into sub-questions, generate answers for the subquestions, and extract the final answer, we provide four in-context example demonstrations. These examples include edits from different languages based on the edits made. We include a mix of 2, 3, and 4-hop example demonstrations in the prompt. Below, we present an example demonstration for a prompt used for edits in German and Swahili. In these demonstrations, the text written in blue represents the updated fact from the edited fact memory, and the text written in teal indicates the answer extraction.\nHere is the 3-hop question example demonstration used in the prompt when edits are made in German:\nFollowing is the 2-Hop example demonstration when edits are made in Swahili:"}, {"title": "A.7 Compute Resources", "content": "We performed all experiments using 8 NVIDIA A100 80 GB GPUs. The training duration for the retriever, including both bilingual and multilingual retrievers for both PokeMQA-CL and CLEVER-CKE, was approximately 2 hours per run. Inference tasks took between 4 to 6 hours to complete when using ChatGPT as the LLM in the case of CLEVER-CKE, and between 10 to 24 hours with Llama-2-7b and Vicuna-1.5. Each MeLLo baseline run varied in duration from 8 to 24 hours, depending on the language and the LLM used."}, {"title": "A.8 Error Analysis", "content": "Figure 7 presents real examples of errors made by different methods. The first column displays errors related to incorrect retrieval, where the model fails to understand the context of the subquestion and either retrieves a fact with some word overlap with the subquestion or a random edit. The second column shows instances where the LLM makes mistakes in breaking down the subquestion. In the first example, it deviates from the question, asking when Giles Gilbert Scott died, and then in the third hop, it just repeats the original question. The second example of this column contains an example where the LLM fails to adhere to the strict pattern of the prompt, misunderstands the context, and generates incorrect information, causing a cascading effect of errors. The third column highlights errors specific to the MeLLomethod, where the LLM struggles to disambiguate between the generated answer and the retrieved fact. In the first example of this column, the retrieved fact contradicts the generated answer, but the LLM fails to identify the correct entity from the generated answer/retrieved fact after resolving the contradiction, leading to a wrong answer. In the second example, although the retrieved fact does not contradict the generated answer, the LLM incorrectly perceives it as a contradiction, resulting in a mistake.\nOur method, CLEVER-CKE, addresses and improves upon these errors, as demonstrated in Figure 8. In the same question scenario, where MeLLo-CL exhibits a contradiction error highlighted in yellow and red, and PokeMQA-CL makes a retrieval error based on word overlap, our method follows all the correct steps, leading to the accurate final answer."}, {"title": "A.9 Licensing", "content": "The baseline methods ROME, MEMIT, FT, MeLLo, and PokeMQA are distributed under the MIT License. Similarly, the datasets MQUAKE-CF and MQUAKE-T are available under the MIT License. The models Vicuna-1.5-7B (v1.5) and distilbert-base-multilingual-cased are released under the Apache License 2.0, while LLaMa-2-7B is licensed under the LLAMA 2 Community License."}]}