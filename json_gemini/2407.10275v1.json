{"title": "Cross-Lingual Multi-Hop Knowledge Editing \u2013 Benchmarks, Analysis and a Simple Contrastive Learning based Approach", "authors": ["Aditi Khandelwal", "Harman Singh", "Hengrui Gu", "Tianlong Chen", "Kaixiong Zhou"], "abstract": "Large language models are often expected to constantly adapt to new sources of knowledge and knowledge editing techniques aim to efficiently patch the outdated model knowledge, with minimal modification. Most prior works focus on monolingual knowledge editing in English, even though new information can emerge in any language from any part of the world. We propose the Cross-Lingual Multi-Hop Knowledge Editing paradigm, for measuring and analyzing the performance of various SoTA knowledge editing techniques in a cross-lingual setup. Specifically, we create a parallel cross-lingual benchmark, CROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive analysis over various knowledge editing techniques uncover significant gaps in performance between the cross-lingual and English-centric setting. Following this, we propose a significantly improved system for cross-lingual multi-hop knowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and generate knowledge editing framework, where a retriever is formulated to recall edited facts and support an LLM to adhere to knowledge edits. We develop language-aware and hard-negative based contrastive objectives for improving the cross-lingual and fine-grained fact retrieval and verification process used in this framework. Extensive experiments on three LLMs, eight languages, and two datasets show CLEVER-CKE's significant gains of up to 30% over prior methods.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are seeing an increasing adoption across users having different cultural and linguistic background, and need to be up to date about the ever-changing knowledge in the world for maintaining their utility and reliability in various applications. Due to the ever increasing compute and data requirements to train these models, there has been a surge in the development of knowledge editing techniques to modify the language models in an efficient way, such that they adhere to the world dynamics.\nPrior work on knowledge editing has largely focused on editing LLMs in a monolingual setting (Zhong et al., 2023; Gu et al., 2024), where both user queries and edited facts are expressed in the form of English. These works can be grouped into two categories: parameter-update and parameter-preserving methods. The former directly updates the parameters within LLMs for updating knowledge about the edited facts through meta-learning, fine-tuning, or knowledge locating (De Cao et al., 2021; Dai et al., 2022; Mitchell et al., 2022a; Meng et al., 2022a,b). The later approach freezes the parameters and explicitly stores the edited facts in an external memory and retrieves them for answering user queries (Zhong et al., 2023; Gu et al., 2024; Mitchell et al., 2022c; Hartvigsen et al., 2023). Existing monolingual knowledge editing techniques aren't broadly applicable since new knowledge can emerge in different languages. Some works have made progress in this direction (Beniwal et al., 2024; Xu et al., 2023a; Si et al., 2024), but they have considered a simplistic setting of assuming the edited facts as independent without any multi-hop rippling consequences on entailed reasoning process, and are primarily focused on parameter-modifying based editing methods.\nThere has only been a limited focus on the realistic case of cross-lingual multi-hop knowledge editing (see Fig 1), where the edited knowledge can come in through users who communicate in different languages. Further, much of edited knowledge often has a rippling effect on other facts of the world. For example, the club change of Messi affects deduction process of question \u201cindicating a superficial word matching rather than a contextual grasp of the entities involved.\" This knowledge editing setting, which we argue is important to study, is challenging since the model needs to transfer knowledge about fact edits between different languages, while also reasoning about the facts which are modified as a consequence to the given edit. Poor knowledge transfer between languages can lead to error propagation across reasoning steps which can increase failure cases of model editing.\nIn this work, we formulate the notion of cross-lingual multi-hop knowledge-editing and analyze existing approaches for their editing ability in different languages, following which a simple yet highly effective approach is designed. Specifically, We create one of the first benchmark datasets for measuring cross-lingual multi-hop knowledge editing capabilities of knowledge editing methods. Besides parameter-update based approaches, we contribute strong retrieval-based baselines for knowledge editing and provide a comprehensive analysis. We provide a detailed analysis and find significant gaps in the performance of methods for cross-lingual knowledge editing. The gaps are mainly due to challenges in accurately recalling fact edits made in language other than input query. To bridge such gap, we design a competitive method, termed as Contrastive Language-aware Verification for Cross-lingual Knowledge Editing (CLEVER-CKE), for improving performance of cross-lingual multi-hop knowledge editing. Our approach is based on decomposing a multi-hop question in a particular language into sub-questions and retrieving fact edits (if any) from memory using a cross-lingual retriever, which is integrated for answering sub-questions. In particular, the cross-lingual retriever is regularized by novel language-guided and hard-negative based contrastive losses, which leads to improved language and fine-grained sentence understanding of the edits, leading to high quality cross-lingual retrievals. CLEVER-CKE improves over previous SoTA by up-to 30% increase in knowledge editing accuracy when tested on multiple LLMs, datasets and languages."}, {"title": "2 Cross-lingual Multi-hop Editing", "content": "Following prior work (Zhong et al., 2023), a fact is defined as a triplet $(s, r, o)$, where $s$ is the subject, $o$ is the object, and $r$ is the relation (e.g., Shakespeare, author of, Hamlet). Given that a parametric LLM can become outdated or incorrect, knowledge editing is required to be performed on it. An edited fact stores information about updated knowledge of an existing fact and is denoted as $e = (s, r, o^*)$, where the object is replaced with a new one $o^*$.\nCross-Lingual Knowledge Editing. Each knowledge fact or edit is assumed to be represented in natural language. Let $\\mathcal{T}: \\mathcal{E} \\rightarrow \\mathcal{L}$ be a function which takes any fact $e \\in \\mathcal{E}$ (e.g., Shakespeare, author of, Hamlet) and converts it into a natural language statement, (e.g., Shakespeare is the author of Hamlet). All the facts and edits can be represented in a variety of languages $\\{\\mathcal{L}_1, \\mathcal{L}_2, ...\\}$ via functions such as $\\{\\mathcal{T}_{\\mathcal{L}_1}, \\mathcal{T}_{\\mathcal{L}_2}, ...\\}$. For example, an edit $e =(Shakespeare, author of, Lolita)$ can be written as $\\mathcal{T}_{de}(e)$ = Shakespeare ist der Autor von Lolita in German and $\\mathcal{T}_{en}(e)$ = Shakespeare is the author of Lolita in English.\nWe consider a collection of $n$ fact edits in the diverse languages: $\\mathcal{E} = \\{e^{\\mathcal{L}_1}_1,e^{\\mathcal{L}_2}_2,..., e^{\\mathcal{L}_i}_{n_i}\\}$, where $\\mathcal{L}_1, \\mathcal{L}_2,..., \\mathcal{L}_i$ are different languages for e.g., German, Hindi, Swahili, etc. A language model $f$ is said to be edited with new knowledge facts if the model generations adheres to all the edits present in $\\mathcal{E}$. The model is required to seamlessly transfer knowledge about an edit in one language to answer queries in other languages.\nMulti-Hop Editing and Evaluation. We follow Zhong et al. (2023) for evaluating knowledge editing via multi-hop question answering. Consider $e^{\\mathcal{L}_1} = (s_1, r_1, o^*_1)$, an edited fact in language $\\mathcal{L}_1$. Also consider a chain of facts $\\mathcal{P} = ((s_1, r_1, o_1), (s_2, r_2, o_2), ..., (s_h, r_h, o_h))$, where object of a fact is the subject for the next fact. Any edit to the first fact $(s_1, r_1, o^*_1)$ will likely have a rippling effect and change the subsequent facts in the chain, and we expect a successfully edited model to be aware of all such entailed changes.\nFor evaluating models in a cross-lingual multi-hop setting, we make use of multi-hop questions such as $Q^{\\mathcal{L}_n}$, in language $\\mathcal{L}_n$ which is different from $\\mathcal{L}_{1...k}$. The question asks about the head entity $s_1$ for which the answer is $o_h$ before editing. After editing, the fact chain changes to $\\mathcal{P}^* = ((s_1, r_1, o^*_1), (s_2, r_2, o^*_2), ..., (s_h, r_h, o^*_h))$ since edits in the first fact can effect the subsequent facts it's linked to. For answering $Q^{\\mathcal{L}_n}$ after editing, the model has to account for this rippling effect, and provide the final answer as $o^*_h$. For this, model has to transfer knowledge of the edited fact and the answer, between languages $\\mathcal{L}_{1...k}$ and $\\mathcal{L}_n$, while correctly reasoning about fact edits via $\\mathcal{P}^*$."}, {"title": "3 CROLIN-MQUAKE Benchmark", "content": "We develop one of the first parallel cross-lingual for measuring the knowledge editing capabilities of the existing approaches. A parallel benchmark has the same test examples across all the languages, enabling a direct comparison between them. For this, we use existing datasets measuring the multi-hop model editing in English: MQUAKE-CF and MQuAKE-T released by Zhong et al. (2023), which have counterfactual edits and real-world temporal edits respectively. We translate one fact edit per example in these datasets using Google Translate (Google) into 7 languages with diverse writing scripts across medium to high resourcedness - German, Spanish, Chinese, Rissian, Hindi, Bengali, Swahili. This results in the benchmark: Cross-Lingual Multi-Hop QnA for Knowledge Editing (CROLIN-MQUAKE). It has two datasets, CROLIN-MQUAKE-CF and CROLIN-MQUAKE-T, each having 8 languages, and 3k and 1.8k parallel examples (same examples in all languages) per language, respectively. The translations are verified by human experts proficient in particular languages and evaluation of BLEU score (Papineni et al., 2002) using backtranslation. We find that the translation is highly accurate, since we study medium to high resource languages. See Section A.2 for more details.\nConcurrently, Wei et al. (2024) created a multilingual knowledge editing dataset using Wikipedia, offering translocalized knowledge but lacking parallel multilingual examples like ours. CROLIN-MQUAKE enables comparing the knowledge editing performance difference across languages directly without being affected by the variation of test sets between different languages."}, {"title": "4 Benchmark Analysis on Cross-Lingual Multi-hop Knowledge Editing", "content": "LLMs. We use SoTA propriety and open-source LLMs: ChatGPT (Schulman et al., 2022), LLaMa-2-7B (Touvron et al., 2023b), Vicuna-1.5-7B (Chiang et al., 2023) as backbones to evaluate cross-lingual multi-hop knowledge editing.\nEvaluation Metrics. We use multi-hop accuracy proposed by Zhong et al. (2023) which measures the accuracy of the final answer of a multi-hop question. We also adopt hop-wise answering accuracy for checking the correctness of intermediate reasoning steps, as proposed by Gu et al. (2024).\nNew Baselines. Based on existing work, we contribute strong baselines for the new editing setup:\n\u2022 MeLLo-CL: We modify the existing method of MeLLo (Zhong et al., 2023) by replacing the monolingual retriever used in their system with a multilingual retriever. This minimal modification allows the system to retrieve the cross-lingual edits. MeLLo-CL is a simple retrieval-based knowledge editing approach: LLM first breaks down a multi-hop question into various sub-questions and for each sub-question, the retriever then recalls the most relevant fact from an external memory. The LLM disambiguates if the retrieved fact is useful for answering the question or not.\n\u2022 PokeMQA-CL: PokeMQA is similar to MeLLo but consists of a conflict disambiguator for retrieving as well as classifying if a fact is useful to answer a sub-question. Following PokeMQA, we train this disambiguator using BCE loss with negative sampling for retrieving the close edits, given a decomposed sub-question. However, our training dataset now consists of translated version of the training dataset used in PokeMQA. This training set contains all 8 languages (the multilingual setting) or English along with one of the 7 non-English languages (the bilingual setting).\nMulti-hop knowledge editing performance heavily depends on the language of edits. As can be seen in the Figure 2, the gaps in average accuracy between English and other language edits are 10% and 11.7% for methods MeLLo-CL and PokeMQA-CL, respectively, highlighting the significant drop in cross-lingual knowledge editing setup. Performance of MeLLo-CL varies significantly across the different scripts. For language written in Latin scripts, the accuracy is ~20%. In contrast, for languages written in non-Latin scripts such as Devanagari, Chinese, or Cyrillic, the accuracy drops to ~11%. Another observation is that, in case of edits made in Swahili, despite being a low-resource language, it outperforms more resource-rich languages like Chinese, Russian, and Hindi. This suggests that script plays a crucial role in cross-lingual knowledge editing and retrieval. The reason is intuitive, i.e., Latin script languages have a higher presence in most pretraining data which leads to better tokenization and better representation in LLMs; whereas the non-Latin script languages suffer from high tokenization fertility and less effective representation in the model (Ahia et al., 2023; Singh et al., 2024).\nParameter-modifying based knowledge editing performs poorly in the cross-lingual setting. Methods that update the parameters of the model, such as ROME, MEMIT, FT, perform significantly worse in the cross-lingual setting, achieving an accuracy under 5.0% (average across languages), as shown in Table 1. One key issue is that knowledge edits may not transfer effectively across different languages just via model weights, leading to inconsistent and inaccurate retrievals. Further, the problem is exacerbated due to cascading error propagation in a multi-hop setting. Hence the parameter-modifying methods struggle to reliably edit the LLM across languages and multi-hop contexts. This highlights the need for memory-based approaches that rely on an external edit memory, like our contributed baselines, MeLLo-CL and PokeMQA-CL, which can cross-lingually retrieve the relevant edits on the fly when inferring from an LLM. These approaches substantially improve performance up to nearly 30% on CROLIN-MQUAKE compared to parameter-modifying based methods.\nKnowledge editing performance based on retriever training technique. MeLLo-CL retrieves the edited fact from the memory using mContriever and employs an LLM to disambiguate between the generated answer and the retrieved fact and hence ascertains if the generated fact needs any update or not. On the other hand, the current state-of-the-art knowledge editing method in English, PokeMQA-CL, uses a retrieve-then-verify approach, which offloads the knowledge disambiguation to the retriever. This retriever is a light-weight and fine-tuned distilbert-base model trained on a (sub-question,edit) pair dataset using binary cross-entropy loss with negative sampling. It retrieves the closest edits (in fact memory) to a sub-question and scores it for whether the edit answers the question or not (called verification or disambiguation). If it does, then it uses this new knowledge as the answer to the sub-question in the n-th hop step and performs in-context editing. PokeMQA-CL outperforms MeLLo-CL on in the monolingual (English) setting, with a much smaller retriever as shown in Gu et al. (2024), however, when trained with multilingual data, we find that it significantly underperforms MeLLo-CL in most languages including English as shown in Fig. 2. MeLLo-CL underperforms in Hindi and Bengali \u2013 languages with scripts very different from Latin, even though it's retriever is trained with 100+ languages.\nQualitative analysis of errors. We examine the error cases of MeLLo-CL and PokeMQA-CL for knowledge edits made in two languages: English and Hindi. Our analysis identifies two primary types of errors made by these methods. The first type is a) incorrect retrieval, where the retrieved information is not relevant to input queries. The second type is b) incorrect LLM response, where a LLM either makes a mistake in extracting the final answer or errors in decomposing the question into subquestions. Additionally, MeLLo-CL exhibits c) contradiction error where the LLM makes mistake at the contradiction step.\n\u25cf MeLLo-CL: When edits are made in English, 63.3% of the samples are correct, 29.3% have the contradiction error, 3.6% have Incorrect retrieval, and 3.6% have the incorrect LLM response. For edits made in Hindi, 33.3% of the samples are correct, 60% exhibit an error combination of incorrect retrieval and subsequent contradiction error, where the model first makes an incorrect retrieval and then fails in the contradiction step and 6.6% of erroneous samples are due to the incorrect LLM response. In the CROLIN-MQUAKE-CF case when the multilingual edited fact memory containing edits in English and Hindi, MeLLo-CL's retriever rarely retrieves edits in Hindi, indicating a limitation in its multilingual capabilities. The limitation of MeLLo-CL lies in its retriever-then-contradict mechanism which is up to the LLM.\n\u25cf PokeMQA-CL: When edits are made in English, 53.3% of the samples are correct and 46.3% have the incorrect retrieval error. When edits are made in Hindi, 43.3% are correct, 51% have errors due to the incorrect retrieval and 5.6% are due to the incorrect LLM response. The limitation of PokeMQA-CL lies in its reliance on a bag-of-words model for retrieval. For instance, when presented with the sub-question \u201cWho is the head of state of the USA?", "The head of state of Mongolia is Kh\u00fcrels\u00fckh Ukhnaa.\" This example underscores that PokeMQA-CL prioritizes facts with the highest word overlap, specifically \u201chead of state": "ndicating a superficial word matching rather than a contextual grasp of the entities involved.\nWhen trained in a cross-lingual setting, PokeMQA-CL exacerbates the issue of bag-of-words retrieval. For example, for the sub-question \u201cWhere was Bob Dylan born?", "Bob Dylan was born in the city of Nankoku\" in English. However, if the same edit is made in German, it retrieves \u201cBob Dylan spricht die Sprache von Malayalam": "Bob Dylan speaks the language of Malayalam). This issue is a likely a consequence of high word overlap in retriever's internal translation process and is a limitation of current systems. Section 4 hints signficant gapS between English-only and cross-lingual case, and that proper knowledge retrieval technique is critical to the performance of cross-lingual knowledge editing."}, {"title": "5 CLEVER-CKE for Knowledge Editing", "content": "For overcoming limitations in cross-lingual multi-hop knowledge editing, we design CLEVER-CKE, a cross-lingual and light-weight model editor that seamlessly integrates into any backbone LLM, without changing its parameters. CLEVER-CKE is inspired by memory-based and retrieval-augmented knowledge editing methods (Zhong et al., 2023; Gu et al., 2024; Mitchell et al., 2022b) for mutlihop question answering. CLEVER-CKE follows the following procedure: Given an input query, it a) decomposes the multi-hop question into multiple sub-questions for getting to the final answer, and for answering each sub-question b) retrieves a relevant fact from the edit memory, c) disambiguates whether the retrieved new knowledge is relevant to answering the sub-question, and d) continues the model generation process based on that. In this work, we primarily aim at showing the importance of having a high-quality retriever for the retrieve-and-verify steps at b) and c) described as follows. See Fig. 3 for an overview.\nMemory of Fact Edits: CLEVER-CKE explicitly stores a set of knowledge edits $\\mathcal{E}$ in a memory $\\mathcal{F}$. Each edit triplet $e = (s, r, o) \\in \\mathcal{E}$ is converted to a natural language statement in either English or another language using English or translated templates present in CROLIN-MQUAKE. This creates a multilingual edited fact memory.\nSub-question Decomposition: Given a multi-hop question $Q$, LLM is prompted using in-context examples to decompose it into various sub-questions $Q_{sub} = \\{q_1, q_2,...\\}$. Note that $Q$ and the language model generation is assumed to be in English in our work whereas the edited fact memory can contain both English and non-English knowledge edits. The LLM is instructed to answer the generate sub-questions as follows.\nRetrieve-and-Verify: For each sub-question $q$, CLEVER-CKE retrieves the top-1 candidater $\\in \\mathcal{F}$ using cosine similarity. Verification process then answers the question: Does r help answer q? The answer to this is yes if $\\cos(f(r), f(q)) \\geq t$ where $\\cos(.)$ is the cosine similarity function, $f(.) \\in \\mathbb{R}^{d}$ is the retriever embedding and t is a threshold (hyperparameter). In this case, r is passed to the LLM which uses it for generating the answer to the sub-question. If $\\cos(f(r), f(q)) < t$, only the LLM's internal knowledge is used to answer the question. Following this, LLM will move on to answering the next sub-question. Note that here, the disambiguation of whether r is useful or not, happens external to the LLM, reducing its reasoning complexity.\nCLEVER-CKE Retriever Training: Motivated by gaps found in Section 4, we create new objectives for training the retriever for improving fine-grained and cross-lingual representations. We then show that our simple losses provide significant gains in knowledge editing performance.\nSemantic Distinction Loss: We employ a contrastive, triplet margin loss $\\mathcal{L}_{SD}$ for improving fine-grained cross-lingual retrieval. Assuming an edits $e = (s, r, o)$, we obtain its natural language forms $\\mathcal{T}_{L_1}(e), \\mathcal{T}_{L_2}(e)$ in languages $L_1, L_2$ respectively. This creates a positive pair for the triplet loss. We generate hard negatives for $\\mathcal{T}_{en}(e)$ in English by replacing an edits' subject, object, or both object with random entities, with a probability of 0.33 each. This process involves extracting all relations in MQUAKE dataset and prompting the GPT-3.5 model to suggest head/tail entities for these relations. We then randomly sample any generated head/tail (or both) for replacement in an edit containing the corresponding relation. Following this, the hard negative example $\\mathcal{T}_{en}(e_{neg})$ is translated to $L_1$ and hence a negative pair $(\\mathcal{T}_{L_1}(e), \\mathcal{T}_{L_1}(e_{neg})$ is obtained. The loss function is formulated as:\n$\\mathcal{L}_{SD} = \\max(d(f(\\mathcal{T}_{L_1}(e)), f(\\mathcal{T}_{L_2}(e))) -d(f(\\mathcal{T}_{L_1}(e)), f(\\mathcal{T}_{L_1}(e_{neg})) + \\alpha, 0).$ (1)\n$f(\\cdot)$ represents the retriever embedding, $d(.)$ represents the distance function, and $\\alpha$ is a gate hyperparameter. $\\mathcal{L}_{SD}$ promotes learning the fine-grained knowledge about subject, relation and object in a cross-lingual setting and encourages the model to distinguish the semantic nuances in different edits. This mitigates the redundant selection of edits with significant word overlap.\nCross-Lingual Edit Consistency Loss: We employ a contrastive, triplet margin loss $\\mathcal{L}_{CLEC}$ focused on improving cross-lingual retrieval. Here, the anchor is $Q_{en}$, a question in English. The edited fact for answering that question, $\\mathcal{T}_{L_1}(e)$, serves as the positive example, and a random edit $\\mathcal{T}_{L_2}(e_{rand})$ forms the negative example:\n$\\mathcal{L}_{CLEC} = \\max(d(f(Q_{en}), f(\\mathcal{T}_{L_1}(e)) -d(f(Q_{en}), f(\\mathcal{T}_{L_2}(e_{rand})) + \\alpha, 0).$ (2)\nBCE Loss: Following (Gu et al., 2024; Mikolov et al., 2013) we add a binary cross-entropy loss in the cross-lingual setting as a baseline loss for training the retriever for retrieving edits in a cross-lingual setting. The negative BCE Loss function takes questions in English and their corresponding edited facts in one of the seven languages as input. We then compute the $L_2$ norm between these edits and questions, and sample 20 negatives. The loss function $\\mathcal{L}$ is defined similar to Gu et al. (2024):\n$\\mathcal{L}_{BCE} = \\log g(\\mathcal{T}_{L_1}(e), f(Q_{en})) -\\mathbb{E}_{qn\\sim Pn(q)} [\\log(1 - g(\\mathcal{T}_{L_1}(e), qn))],$ (3)\nwhere $P_n$ is a uniform over each mini-batch, and $g(.) = \\exp(d(.)).$\n$\\mathcal{L}_{CLEC}$ and $\\mathcal{L}_{BCE}$ encourage it to differentiate between edits in different languages and enhance its ability to handle multilingual knowledge editing tasks effectively. The total loss we use is then:\n$\\mathcal{L}_{total} = \\mathcal{L}_{SD} + \\mathcal{L}_{CLEC} + \\mathcal{L}_{BCE}.$ (4)"}, {"title": "5.1 Performance of CLEVER-CKE", "content": "We train the retriever with the above losses on a dataset of 8 languages and measure performance on the CROLIN-MQUAKE. In Table 1, on average across languages and across different LLMs, CLEVER-CKE improves over previous methods by up-to 5.7% in accuracy on CROLIN-MQUAKE-CF and we see a much larger increase in the hop-accuracy which suggests faithful reasoning. On the real world temporal dataset CROLIN-MQUAKE-T, we see a significant increase of about 30% accuracy and more than 25% in hop-accuracy metric. Performance gains are large and consistent or better for larger and more capable models like ChatGPT, as compared to LLaMa-2/Vicuna-1.5. Refer to Figure 8 which illustrates an example where other methods make errors, while CLEVER-CKE correctly answers the question.\nPerformance across n-hops: We compare the performance of MeLLo, PokeMQA and CLEVER-CKE in answering n-hop questions, $n \\in {2, 3, 4}$ using CROLIN-MQUAKE-CF dataset and ChatGPT as the LLM. As shown in Fig. 4, CLEVER-CKE outperforms PokeMQA-CL and MeLLo-CL with an average performance increase of 30.7% for 2-hop questions, 22.6% for 3-hop questions, and 5% for 4-hop questions.\nBilingual vs Multilingual retriever: To compare performance differences with increasing the number of languages, we trained PokeMQA-CL and CLEVER-CKE's retrievers in a bilingual setting using English and the target language. As expected, on average the bilingual setting has greater performance than the multilingual setting, potentially due to interference of multiple languages in the multilingual setting. We interestingly observe that this gap is minimal in the case of CLEVER-CKE, compared to PokeMQA-CL. This is because CLEVER-CKE's losses lead to better cross-lingual knowledge transfer leading to reduced interference of languages and more generalization. This observation generalizes across LLMs and datasets we tested on."}, {"title": "6 Related Works", "content": "Cross-lingual knowledge editing. Recent studies have shifted focus to the multilingual capabilities of SOTA LLMs like LLaMA (Touvron et al., 2023a), ChatGPT (Schulman et al., 2022), and GPT-4 (OpenAI, 2023). Wang et al. (2023a) investigated cross-lingual knowledge editing and its impact on different target languages using a synthetic dataset. (Si et al., 2024) introduced Multilingual Patch Neuron (MPN) for efficient cross-lingual knowledge synchronization, showing enhanced performance on single-hop XNLI and XFEVER datasets. (Xu et al., 2023b) proposed a framework for language anisotropic editing, facilitating simultaneous cross-lingual model editing. (Beniwal et al., 2024) explored the cross-lingual model editing (XME) paradigm, revealing performance limitations in multilingual LLMs for hypernetwrok based parameter-modifying methods. (Wang et al., 2023b) presented Retrieval-augmented Multilingual Knowledge Editing (ReMaKE), a model-agnostic knowledge editing method designed for multilingual settings. ReMaKE retrieves new knowledge from a multilingual knowledge base and concatenates it with prompts to update LLMs. Most works assume edited facts are independent without any multi-hop consequences of these edits, and focus on parameter update based methods. We focus on parameter-preserving methods, and the more complex setting of multi-hop editing in a cross-lingual setup."}, {"title": "7 Conclusion", "content": "In this paper, we contributed a benchmark having parallel multilingual examples for evaluating cross-lingual multi-hop knowledge editing. We provide new baselines and a detailed analysis of SOTA knowledge editing methods and find various gaps in existing methods, particularly in the cross-lingual setting. Motivated by this, we propose a generic, simple and highly effective method, CLEVER-CKE, for improving the knowledge editing capabilities of parameter-preserving, retrieval augmented editing methods. CLEVER-CKE improves cross-lingual and fine-grained retrieval in the case of knowledge editing, by introducing language aware and hard-negative mining based contrastive losses to train retrievers. Improved retrieval leads to precise knowledge retrieval and reduced error propagation in the multi-hop reasoning setting. CLEVER-CKE is parameter-preserving in terms of the LLM weights, and uses a lightweight retriever with low latency as compared to methods like Zhong et al. (2023)."}, {"title": "8 Limitations", "content": "Our analysis and methods has some limitations. Firstly, although CROLIN-MQUAKE is a parallel cross-lingual benchmark, it predominantly contains fact edits related to English-speaking knowledge changes, while the edits could be localized to any part of the world in practice. This reliance on translation rather than trans-localization may lead to gaps in accurately understanding regional and local fact edits. However, having parallel data in all languages is advantageous to accurately measure per-language performance without confounding factors. Secondly, our method is primarily focused on the retriever component and does not address the inherent inaccuracies of the LLMs. This includes issues such as understanding and generation capabilities of LLMs in different languages, correctly breaking down multi-hop questions into sub-questions, accurately extracting the final answer in the desired language. Lastly, our analysis is currently limited to a broad range of medium to high-resource languages. Extending this analysis to low-resource languages presents a significant challenge due to the inaccuracies in translation, which can hinder the proper representation and understanding of facts in low resource languages. Improving translation accuracy and extending our work to low-resource languages is part of our future work."}, {"title": "A Appendix", "content": "A.1 Related Work\nKnowledge editing methods: Knowledge editing can be broadly classified intro two groups. 1) Parameter-modifying based editing which locates the parameters related to factual knowledge and subsequently modify them (De Cao et al.", "methods": "With the advances in generative language technologies powered by Large Language Models (LLMs; Brown et al."}, {"title": "Cross-Lingual Multi-Hop Knowledge Editing \u2013 Benchmarks, Analysis and a Simple Contrastive Learning based Approach", "authors": ["Aditi Khandelwal", "Harman Singh", "Hengrui Gu", "Tianlong Chen", "Kaixiong Zhou"], "abstract": "Large language models are often expected to constantly adapt to new sources of knowledge and knowledge editing techniques aim to efficiently patch the outdated model knowledge, with minimal modification. Most prior works focus on monolingual knowledge editing in English, even though new information can emerge in any language from any part of the world. We propose the Cross-Lingual Multi-Hop Knowledge Editing paradigm, for measuring and analyzing the performance of various SoTA knowledge editing techniques in a cross-lingual setup. Specifically, we create a parallel cross-lingual benchmark, CROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive analysis over various knowledge editing techniques uncover significant gaps in performance between the cross-lingual and English-centric setting. Following this, we propose a significantly improved system for cross-lingual multi-hop knowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and generate knowledge editing framework, where a retriever is formulated to recall edited facts and support an LLM to adhere to knowledge edits. We develop language-aware and hard-negative based contrastive objectives for improving the cross-lingual and fine-grained fact retrieval and verification process used in this framework. Extensive experiments on three LLMs, eight languages, and two datasets show CLEVER-CKE's significant gains of up to 30% over prior methods.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are seeing an increasing adoption across users having different cultural and linguistic background, and need to be up to date about the ever-changing knowledge in the world for maintaining their utility and reliability in various applications. Due to the ever increasing compute and data requirements to train these models, there has been a surge in the development of knowledge editing techniques to modify the language models in an efficient way, such that they adhere to the world dynamics.\nPrior work on knowledge editing has largely focused on editing LLMs in a monolingual setting (Zhong et al., 2023; Gu et al., 2024), where both user queries and edited facts are expressed in the form of English. These works can be grouped into two categories: parameter-update and parameter-preserving methods. The former directly updates the parameters within LLMs for updating knowledge about the edited facts through meta-learning, fine-tuning, or knowledge locating (De Cao et al., 2021; Dai et al., 2022; Mitchell et al., 2022a; Meng et al., 2022a,b). The later approach freezes the parameters and explicitly stores the edited facts in an external memory and retrieves them for answering user queries (Zhong et al., 2023; Gu et al., 2024; Mitchell et al., 2022c; Hartvigsen et al., 2023). Existing monolingual knowledge editing techniques aren't broadly applicable since new knowledge can emerge in different languages. Some works have made progress in this direction (Beniwal et al., 2024; Xu et al., 2023a; Si et al., 2024), but they have considered a simplistic setting of assuming the edited facts as independent without any multi-hop rippling consequences on entailed reasoning process, and are primarily focused on parameter-modifying based editing methods.\nThere has only been a limited focus on the realistic case of cross-lingual multi-hop knowledge editing (see Fig 1), where the edited knowledge can come in through users who communicate in different languages. Further, much of edited knowledge often has a rippling effect on other facts of the world. For example, the club change of Messi affects deduction process of question \u201cindicating a superficial word matching rather than a contextual grasp of the entities involved.\" This knowledge editing setting, which we argue is important to study, is challenging since the model needs to transfer knowledge about fact edits between different languages, while also reasoning about the facts which are modified as a consequence to the given edit. Poor knowledge transfer between languages can lead to error propagation across reasoning steps which can increase failure cases of model editing.\nIn this work, we formulate the notion of cross-lingual multi-hop knowledge-editing and analyze existing approaches for their editing ability in different languages, following which a simple yet highly effective approach is designed. Specifically, We create one of the first benchmark datasets for measuring cross-lingual multi-hop knowledge editing capabilities of knowledge editing methods. Besides parameter-update based approaches, we contribute strong retrieval-based baselines for knowledge editing and provide a comprehensive analysis. We provide a detailed analysis and find significant gaps in the performance of methods for cross-lingual knowledge editing. The gaps are mainly due to challenges in accurately recalling fact edits made in language other than input query. To bridge such gap, we design a competitive method, termed as Contrastive Language-aware Verification for Cross-lingual Knowledge Editing (CLEVER-CKE), for improving performance of cross-lingual multi-hop knowledge editing. Our approach is based on decomposing a multi-hop question in a particular language into sub-questions and retrieving fact edits (if any) from memory using a cross-lingual retriever, which is integrated for answering sub-questions. In particular, the cross-lingual retriever is regularized by novel language-guided and hard-negative based contrastive losses, which leads to improved language and fine-grained sentence understanding of the edits, leading to high quality cross-lingual retrievals. CLEVER-CKE improves over previous SoTA by up-to 30% increase in knowledge editing accuracy when tested on multiple LLMs, datasets and languages."}, {"title": "2 Cross-lingual Multi-hop Editing", "content": "Following prior work (Zhong et al., 2023), a fact is defined as a triplet $(s, r, o)$, where $s$ is the subject, $o$ is the object, and $r$ is the relation (e.g., Shakespeare, author of, Hamlet). Given that a parametric LLM can become outdated or incorrect, knowledge editing is required to be performed on it. An edited fact stores information about updated knowledge of an existing fact and is denoted as $e = (s, r, o^*)$, where the object is replaced with a new one $o^*$.\nCross-Lingual Knowledge Editing. Each knowledge fact or edit is assumed to be represented in natural language. Let $\\mathcal{T}: \\mathcal{E} \\rightarrow \\mathcal{L}$ be a function which takes any fact $e \\in \\mathcal{E}$ (e.g., Shakespeare, author of, Hamlet) and converts it into a natural language statement, (e.g., Shakespeare is the author of Hamlet). All the facts and edits can be represented in a variety of languages $\\{\\mathcal{L}_1, \\mathcal{L}_2, ...\\}$ via functions such as $\\{\\mathcal{T}_{\\mathcal{L}_1}, \\mathcal{T}_{\\mathcal{L}_2}, ...\\}$. For example, an edit $e =(Shakespeare, author of, Lolita)$ can be written as $\\mathcal{T}_{de}(e)$ = Shakespeare ist der Autor von Lolita in German and $\\mathcal{T}_{en}(e)$ = Shakespeare is the author of Lolita in English.\nWe consider a collection of $n$ fact edits in the diverse languages: $\\mathcal{E} = \\{e^{\\mathcal{L}_1}_1,e^{\\mathcal{L}_2}_2,..., e^{\\mathcal{L}_i}_{n_i}\\}$, where $\\mathcal{L}_1, \\mathcal{L}_2,..., \\mathcal{L}_i$ are different languages for e.g., German, Hindi, Swahili, etc. A language model $f$ is said to be edited with new knowledge facts if the model generations adheres to all the edits present in $\\mathcal{E}$. The model is required to seamlessly transfer knowledge about an edit in one language to answer queries in other languages.\nMulti-Hop Editing and Evaluation. We follow Zhong et al. (2023) for evaluating knowledge editing via multi-hop question answering. Consider $e^{\\mathcal{L}_1} = (s_1, r_1, o^*_1)$, an edited fact in language $\\mathcal{L}_1$. Also consider a chain of facts $\\mathcal{P} = ((s_1, r_1, o_1), (s_2, r_2, o_2), ..., (s_h, r_h, o_h))$, where object of a fact is the subject for the next fact. Any edit to the first fact $(s_1, r_1, o^*_1)$ will likely have a rippling effect and change the subsequent facts in the chain, and we expect a successfully edited model to be aware of all such entailed changes.\nFor evaluating models in a cross-lingual multi-hop setting, we make use of multi-hop questions such as $Q^{\\mathcal{L}_n}$, in language $\\mathcal{L}_n$ which is different from $\\mathcal{L}_{1...k}$. The question asks about the head entity $s_1$ for which the answer is $o_h$ before editing. After editing, the fact chain changes to $\\mathcal{P}^* = ((s_1, r_1, o^*_1), (s_2, r_2, o^*_2), ..., (s_h, r_h, o^*_h))$ since edits in the first fact can effect the subsequent facts it's linked to. For answering $Q^{\\mathcal{L}_n}$ after editing, the model has to account for this rippling effect, and provide the final answer as $o^*_h$. For this, model has to transfer knowledge of the edited fact and the answer, between languages $\\mathcal{L}_{1...k}$ and $\\mathcal{L}_n$, while correctly reasoning about fact edits via $\\mathcal{P}^*$."}, {"title": "3 CROLIN-MQUAKE Benchmark", "content": "We develop one of the first parallel cross-lingual for measuring the knowledge editing capabilities of the existing approaches. A parallel benchmark has the same test examples across all the languages, enabling a direct comparison between them. For this, we use existing datasets measuring the multi-hop model editing in English: MQUAKE-CF and MQuAKE-T released by Zhong et al. (2023), which have counterfactual edits and real-world temporal edits respectively. We translate one fact edit per example in these datasets using Google Translate (Google) into 7 languages with diverse writing scripts across medium to high resourcedness - German, Spanish, Chinese, Rissian, Hindi, Bengali, Swahili. This results in the benchmark: Cross-Lingual Multi-Hop QnA for Knowledge Editing (CROLIN-MQUAKE). It has two datasets, CROLIN-MQUAKE-CF and CROLIN-MQUAKE-T, each having 8 languages, and 3k and 1.8k parallel examples (same examples in all languages) per language, respectively. The translations are verified by human experts proficient in particular languages and evaluation of BLEU score (Papineni et al., 2002) using backtranslation. We find that the translation is highly accurate, since we study medium to high resource languages. See Section A.2 for more details.\nConcurrently, Wei et al. (2024) created a multilingual knowledge editing dataset using Wikipedia, offering translocalized knowledge but lacking parallel multilingual examples like ours. CROLIN-MQUAKE enables comparing the knowledge editing performance difference across languages directly without being affected by the variation of test sets between different languages."}, {"title": "4 Benchmark Analysis on Cross-Lingual Multi-hop Knowledge Editing", "content": "LLMs. We use SoTA propriety and open-source LLMs: ChatGPT (Schulman et al., 2022), LLaMa-2-7B (Touvron et al., 2023b), Vicuna-1.5-7B (Chiang et al., 2023) as backbones to evaluate cross-lingual multi-hop knowledge editing.\nEvaluation Metrics. We use multi-hop accuracy proposed by Zhong et al. (2023) which measures the accuracy of the final answer of a multi-hop question. We also adopt hop-wise answering accuracy for checking the correctness of intermediate reasoning steps, as proposed by Gu et al. (2024).\nNew Baselines. Based on existing work, we contribute strong baselines for the new editing setup:\n\u2022 MeLLo-CL: We modify the existing method of MeLLo (Zhong et al., 2023) by replacing the monolingual retriever used in their system with a multilingual retriever. This minimal modification allows the system to retrieve the cross-lingual edits. MeLLo-CL is a simple retrieval-based knowledge editing approach: LLM first breaks down a multi-hop question into various sub-questions and for each sub-question, the retriever then recalls the most relevant fact from an external memory. The LLM disambiguates if the retrieved fact is useful for answering the question or not.\n\u2022 PokeMQA-CL: PokeMQA is similar to MeLLo but consists of a conflict disambiguator for retrieving as well as classifying if a fact is useful to answer a sub-question. Following PokeMQA, we train this disambiguator using BCE loss with negative sampling for retrieving the close edits, given a decomposed sub-question. However, our training dataset now consists of translated version of the training dataset used in PokeMQA. This training set contains all 8 languages (the multilingual setting) or English along with one of the 7 non-English languages (the bilingual setting).\nMulti-hop knowledge editing performance heavily depends on the language of edits. As can be seen in the Figure 2, the gaps in average accuracy between English and other language edits are 10% and 11.7% for methods MeLLo-CL and PokeMQA-CL, respectively, highlighting the significant drop in cross-lingual knowledge editing setup. Performance of MeLLo-CL varies significantly across the different scripts. For language written in Latin scripts, the accuracy is ~20%. In contrast, for languages written in non-Latin scripts such as Devanagari, Chinese, or Cyrillic, the accuracy drops to ~11%. Another observation is that, in case of edits made in Swahili, despite being a low-resource language, it outperforms more resource-rich languages like Chinese, Russian, and Hindi. This suggests that script plays a crucial role in cross-lingual knowledge editing and retrieval. The reason is intuitive, i.e., Latin script languages have a higher presence in most pretraining data which leads to better tokenization and better representation in LLMs; whereas the non-Latin script languages suffer from high tokenization fertility and less effective representation in the model (Ahia et al., 2023; Singh et al., 2024).\nParameter-modifying based knowledge editing performs poorly in the cross-lingual setting. Methods that update the parameters of the model, such as ROME, MEMIT, FT, perform significantly worse in the cross-lingual setting, achieving an accuracy under 5.0% (average across languages), as shown in Table 1. One key issue is that knowledge edits may not transfer effectively across different languages just via model weights, leading to inconsistent and inaccurate retrievals. Further, the problem is exacerbated due to cascading error propagation in a multi-hop setting. Hence the parameter-modifying methods struggle to reliably edit the LLM across languages and multi-hop contexts. This highlights the need for memory-based approaches that rely on an external edit memory, like our contributed baselines, MeLLo-CL and PokeMQA-CL, which can cross-lingually retrieve the relevant edits on the fly when inferring from an LLM. These approaches substantially improve performance up to nearly 30% on CROLIN-MQUAKE compared to parameter-modifying based methods.\nKnowledge editing performance based on retriever training technique. MeLLo-CL retrieves the edited fact from the memory using mContriever and employs an LLM to disambiguate between the generated answer and the retrieved fact and hence ascertains if the generated fact needs any update or not. On the other hand, the current state-of-the-art knowledge editing method in English, PokeMQA-CL, uses a retrieve-then-verify approach, which offloads the knowledge disambiguation to the retriever. This retriever is a light-weight and fine-tuned distilbert-base model trained on a (sub-question,edit) pair dataset using binary cross-entropy loss with negative sampling. It retrieves the closest edits (in fact memory) to a sub-question and scores it for whether the edit answers the question or not (called verification or disambiguation). If it does, then it uses this new knowledge as the answer to the sub-question in the n-th hop step and performs in-context editing. PokeMQA-CL outperforms MeLLo-CL on in the monolingual (English) setting, with a much smaller retriever as shown in Gu et al. (2024), however, when trained with multilingual data, we find that it significantly underperforms MeLLo-CL in most languages including English as shown in Fig. 2. MeLLo-CL underperforms in Hindi and Bengali \u2013 languages with scripts very different from Latin, even though it's retriever is trained with 100+ languages.\nQualitative analysis of errors. We examine the error cases of MeLLo-CL and PokeMQA-CL for knowledge edits made in two languages: English and Hindi. Our analysis identifies two primary types of errors made by these methods. The first type is a) incorrect retrieval, where the retrieved information is not relevant to input queries. The second type is b) incorrect LLM response, where a LLM either makes a mistake in extracting the final answer or errors in decomposing the question into subquestions. Additionally, MeLLo-CL exhibits c) contradiction error where the LLM makes mistake at the contradiction step.\n\u25cf MeLLo-CL: When edits are made in English, 63.3% of the samples are correct, 29.3% have the contradiction error, 3.6% have Incorrect retrieval, and 3.6% have the incorrect LLM response. For edits made in Hindi, 33.3% of the samples are correct, 60% exhibit an error combination of incorrect retrieval and subsequent contradiction error, where the model first makes an incorrect retrieval and then fails in the contradiction step and 6.6% of erroneous samples are due to the incorrect LLM response. In the CROLIN-MQUAKE-CF case when the multilingual edited fact memory containing edits in English and Hindi, MeLLo-CL's retriever rarely retrieves edits in Hindi, indicating a limitation in its multilingual capabilities. The limitation of MeLLo-CL lies in its retriever-then-contradict mechanism which is up to the LLM.\n\u25cf PokeMQA-CL: When edits are made in English, 53.3% of the samples are correct and 46.3% have the incorrect retrieval error. When edits are made in Hindi, 43.3% are correct, 51% have errors due to the incorrect retrieval and 5.6% are due to the incorrect LLM response. The limitation of PokeMQA-CL lies in its reliance on a bag-of-words model for retrieval. For instance, when presented with the sub-question \u201cWho is the head of state of the USA?", "The head of state of Mongolia is Kh\u00fcrels\u00fckh Ukhnaa.\" This example underscores that PokeMQA-CL prioritizes facts with the highest word overlap, specifically \u201chead of state": "ndicating a superficial word matching rather than a contextual grasp of the entities involved.\nWhen trained in a cross-lingual setting, PokeMQA-CL exacerbates the issue of bag-of-words retrieval. For example, for the sub-question \u201cWhere was Bob Dylan born?", "Bob Dylan was born in the city of Nankoku\" in English. However, if the same edit is made in German, it retrieves \u201cBob Dylan spricht die Sprache von Malayalam": "Bob Dylan speaks the language of Malayalam). This issue is a likely a consequence of high word overlap in retriever's internal translation process and is a limitation of current systems. Section 4 hints signficant gapS between English-only and cross-lingual case, and that proper knowledge retrieval technique is critical to the performance of cross-lingual knowledge editing."}, {"title": "5 CLEVER-CKE for Knowledge Editing", "content": "For overcoming limitations in cross-lingual multi-hop knowledge editing, we design CLEVER-CKE, a cross-lingual and light-weight model editor that seamlessly integrates into any backbone LLM, without changing its parameters. CLEVER-CKE is inspired by memory-based and retrieval-augmented knowledge editing methods (Zhong et al., 2023; Gu et al., 2024; Mitchell et al., 2022b) for mutlihop question answering. CLEVER-CKE follows the following procedure: Given an input query, it a) decomposes the multi-hop question into multiple sub-questions for getting to the final answer, and for answering each sub-question b) retrieves a relevant fact from the edit memory, c) disambiguates whether the retrieved new knowledge is relevant to answering the sub-question, and d) continues the model generation process based on that. In this work, we primarily aim at showing the importance of having a high-quality retriever for the retrieve-and-verify steps at b) and c) described as follows. See Fig. 3 for an overview.\nMemory of Fact Edits: CLEVER-CKE explicitly stores a set of knowledge edits $\\mathcal{E}$ in a memory $\\mathcal{F}$. Each edit triplet $e = (s, r, o) \\in \\mathcal{E}$ is converted to a natural language statement in either English or another language using English or translated templates present in CROLIN-MQUAKE. This creates a multilingual edited fact memory.\nSub-question Decomposition: Given a multi-hop question $Q$, LLM is prompted using in-context examples to decompose it into various sub-questions $Q_{sub} = \\{q_1, q_2,...\\}$. Note that $Q$ and the language model generation is assumed to be in English in our work whereas the edited fact memory can contain both English and non-English knowledge edits. The LLM is instructed to answer the generate sub-questions as follows.\nRetrieve-and-Verify: For each sub-question $q$, CLEVER-CKE retrieves the top-1 candidater $\\in \\mathcal{F}$ using cosine similarity. Verification process then answers the question: Does r help answer q? The answer to this is yes if $\\cos(f(r), f(q)) \\geq t$ where $\\cos(.)$ is the cosine similarity function, $f(.) \\in \\mathbb{R}^{d}$ is the retriever embedding and t is a threshold (hyperparameter). In this case, r is passed to the LLM which uses it for generating the answer to the sub-question. If $\\cos(f(r), f(q)) < t$, only the LLM's internal knowledge is used to answer the question. Following this, LLM will move on to answering the next sub-question. Note that here, the disambiguation of whether r is useful or not, happens external to the LLM, reducing its reasoning complexity.\nCLEVER-CKE Retriever Training: Motivated by gaps found in Section 4, we create new objectives for training the retriever for improving fine-grained and cross-lingual representations. We then show that our simple losses provide significant gains in knowledge editing performance.\nSemantic Distinction Loss: We employ a contrastive, triplet margin loss $\\mathcal{L}_{SD}$ for improving fine-grained cross-lingual retrieval. Assuming an edits $e = (s, r, o)$, we obtain its natural language forms $\\mathcal{T}_{L_1}(e), \\mathcal{T}_{L_2}(e)$ in languages $L_1, L_2$ respectively. This creates a positive pair for the triplet loss. We generate hard negatives for $\\mathcal{T}_{en}(e)$ in English by replacing an edits' subject, object, or both object with random entities, with a probability of 0.33 each. This process involves extracting all relations in MQUAKE dataset and prompting the GPT-3.5 model to suggest head/tail entities for these relations. We then randomly sample any generated head/tail (or both) for replacement in an edit containing the corresponding relation. Following this, the hard negative example $\\mathcal{T}_{en}(e_{neg})$ is translated to $L_1$ and hence a negative pair $(\\mathcal{T}_{L_1}(e), \\mathcal{T}_{L_1}(e_{neg})$ is obtained. The loss function is formulated as:\n$\\mathcal{L}_{SD} = \\max(d(f(\\mathcal{T}_{L_1}(e)), f(\\mathcal{T}_{L_2}(e))) -d(f(\\mathcal{T}_{L_1}(e)), f(\\mathcal{T}_{L_1}(e_{neg})) + \\alpha, 0).$ (1)\n$f(\\cdot)$ represents the retriever embedding, $d(.)$ represents the distance function, and $\\alpha$ is a gate hyperparameter. $\\mathcal{L}_{SD}$ promotes learning the fine-grained knowledge about subject, relation and object in a cross-lingual setting and encourages the model to distinguish the semantic nuances in different edits. This mitigates the redundant selection of edits with significant word overlap.\nCross-Lingual Edit Consistency Loss: We employ a contrastive, triplet margin loss $\\mathcal{L}_{CLEC}$ focused on improving cross-lingual retrieval. Here, the anchor is $Q_{en}$, a question in English. The edited fact for answering that question, $\\mathcal{T}_{L_1}(e)$, serves as the positive example, and a random edit $\\mathcal{T}_{L_2}(e_{rand})$ forms the negative example:\n$\\mathcal{L}_{CLEC} = \\max(d(f(Q_{en}), f(\\mathcal{T}_{L_1}(e)) -d(f(Q_{en}), f(\\mathcal{T}_{L_2}(e_{rand})) + \\alpha, 0).$ (2)\nBCE Loss: Following (Gu et al., 2024; Mikolov et al., 2013) we add a binary cross-entropy loss in the cross-lingual setting as a baseline loss for training the retriever for retrieving edits in a cross-lingual setting. The negative BCE Loss function takes questions in English and their corresponding edited facts in one of the seven languages as input. We then compute the $L_2$ norm between these edits and questions, and sample 20 negatives. The loss function $\\mathcal{L}$ is defined similar to Gu et al. (2024):\n$\\mathcal{L}_{BCE} = \\log g(\\mathcal{T}_{L_1}(e), f(Q_{en})) -\\mathbb{E}_{qn\\sim Pn(q)} [\\log(1 - g(\\mathcal{T}_{L_1}(e), qn))],$ (3)\nwhere $P_n$ is a uniform over each mini-batch, and $g(.) = \\exp(d(.)).$\n$\\mathcal{L}_{CLEC}$ and $\\mathcal{L}_{BCE}$ encourage it to differentiate between edits in different languages and enhance its ability to handle multilingual knowledge editing tasks effectively. The total loss we use is then:\n$\\mathcal{L}_{total} = \\mathcal{L}_{SD} + \\mathcal{L}_{CLEC} + \\mathcal{L}_{BCE}.$ (4)"}, {"title": "5.1 Performance of CLEVER-CKE", "content": "We train the retriever with the above losses on a dataset of 8 languages and measure performance on the CROLIN-MQUAKE. In Table 1, on average across languages and across different LLMs, CLEVER-CKE improves over previous methods by up-to 5.7% in accuracy on CROLIN-MQUAKE-CF and we see a much larger increase in the hop-accuracy which suggests faithful reasoning. On the real world temporal dataset CROLIN-MQUAKE-T, we see a significant increase of about 30% accuracy and more than 25% in hop-accuracy metric. Performance gains are large and consistent or better for larger and more capable models like ChatGPT, as compared to LLaMa-2/Vicuna-1.5. Refer to Figure 8 which illustrates an example where other methods make errors, while CLEVER-CKE correctly answers the question.\nPerformance across n-hops: We compare the performance of MeLLo, PokeMQA and CLEVER-CKE in answering n-hop questions, $n \\in \\{2, 3, 4\\}$ using CROLIN-MQUAKE-CF dataset and ChatGPT as the LLM. As shown in Fig. 4, CLEVER-CKE outperforms PokeMQA-CL and MeLLo-CL with an average performance increase of 30.7% for 2-hop questions, 22.6% for 3-hop questions, and 5% for 4-hop questions.\nBilingual vs Multilingual retriever: To compare performance differences with increasing the number of languages, we trained PokeMQA-CL and CLEVER-CKE's retrievers in a bilingual setting using English and the target language. As expected, on average the bilingual setting has greater performance than the multilingual setting, potentially due to interference of multiple languages in the multilingual setting. We interestingly observe that this gap is minimal in the case of CLEVER-CKE, compared to PokeMQA-CL. This is because CLEVER-CKE's losses lead to better cross-lingual knowledge transfer leading to reduced interference of languages and more generalization. This observation generalizes across LLMs and datasets we tested on."}, {"title": "6 Related Works", "content": "Cross-lingual knowledge editing. Recent studies have shifted focus to the multilingual capabilities of SOTA LLMs like LLaMA (Touvron et al., 2023a), ChatGPT (Schulman et al., 2022), and GPT-4 (OpenAI, 2023). Wang et al. (2023a) investigated cross-lingual knowledge editing and its impact on different target languages using a synthetic dataset. (Si et al., 2024) introduced Multilingual Patch Neuron (MPN) for efficient cross-lingual knowledge synchronization, showing enhanced performance on single-hop XNLI and XFEVER datasets. (Xu et al., 2023b) proposed a framework for language anisotropic editing, facilitating simultaneous cross-lingual model editing. (Beniwal et al., 2024) explored the cross-lingual model editing (XME) paradigm, revealing performance limitations in multilingual LLMs for hypernetwrok based parameter-modifying methods. (Wang et al., 2023b) presented Retrieval-augmented Multilingual Knowledge Editing (ReMaKE), a model-agnostic knowledge editing method designed for multilingual settings. ReMaKE retrieves new knowledge from a multilingual knowledge base and concatenates it with prompts to update LLMs. Most works assume edited facts are independent without any multi-hop consequences of these edits, and focus on parameter update based methods. We focus on parameter-preserving methods, and the more complex setting of multi-hop editing in a cross-lingual setup."}, {"title": "7 Conclusion", "content": "In this paper, we contributed a benchmark having parallel multilingual examples for evaluating cross-lingual multi-hop knowledge editing. We provide new baselines and a detailed analysis of SOTA knowledge editing methods and find various gaps in existing methods, particularly in the cross-lingual setting. Motivated by this, we propose a generic, simple and highly effective method, CLEVER-CKE, for improving the knowledge editing capabilities of parameter-preserving, retrieval augmented editing methods. CLEVER-CKE improves cross-lingual and fine-grained retrieval in the case of knowledge editing, by introducing language aware and hard-negative mining based contrastive losses to train retrievers. Improved retrieval leads to precise knowledge retrieval and reduced error propagation in the multi-hop reasoning setting. CLEVER-CKE is parameter-preserving in terms of the LLM weights, and uses a lightweight retriever with low latency as compared to methods like Zhong et al. (2023)."}, {"title": "8 Limitations", "content": "Our analysis and methods has some limitations. Firstly, although CROLIN-MQUAKE is a parallel cross-lingual benchmark, it predominantly contains fact edits related to English-speaking knowledge changes, while the edits could be localized to any part of the world in practice. This reliance on translation rather than trans-localization may lead to gaps in accurately understanding regional and local fact edits. However, having parallel data in all languages is advantageous to accurately measure per-language performance without confounding factors. Secondly, our method is primarily focused on the retriever component and does not address the inherent inaccuracies of the LLMs. This includes issues such as understanding and generation capabilities of LLMs in different languages, correctly breaking down multi-hop questions into sub-questions, accurately extracting the final answer in the desired language. Lastly, our analysis is currently limited to a broad range of medium to high-resource languages. Extending this analysis to low-resource languages presents a significant challenge due to the inaccuracies in translation, which can hinder the proper representation and understanding of facts in low resource languages. Improving translation accuracy and extending our work to low-resource languages is part of our future work."}, {"title": "A Appendix", "content": "A.1 Related Work\nKnowledge editing methods: Knowledge editing can be broadly classified intro two groups. 1) Parameter-modifying based editing which locates the parameters related to factual knowledge and subsequently modify them (De Cao et al., 2021; Dai et al., 2022; Mitchell et al., 2022a; Meng et al., 2022a,b). These method requires an error-prone analytic step to identify parameters, which might be model-specific and not efficient. 2) Parameter-preserving based editing keeps the model parameters frozen and explicitly stores the fact edits in an external memory, for retrieval and external validation (Zhong et al., 2023; Gu et al., 2024; Mitchell et al., 2022c; Hartvigsen et al., 2023). some recent works like that of (Hernandez et al., 2023) have also explored a decoding time approach for editing knowledge.\nCross-lingual knowledge editing. Recent studies have shifted focus to the multilingual capabilities of SOTA LLMs like LLaMA (Touvron et al., 2023a), ChatGPT (Schulman et al., 2022), and GPT-4 (OpenAI, 2023). Wang et al. (2023a) investigated cross-lingual knowledge editing and its impact on different target languages using a synthetic dataset. (Si et al., 2024) introduced Multilingual Patch Neuron (MPN) for efficient cross-lingual knowledge synchronization, showing enhanced performance on single-hop XNLI and XFEVER datasets. (Xu et al., 2023b) proposed a framework for language anisotropic editing, facilitating simultaneous cross-lingual model editing. (Beniwal et al., 2024) explored the cross-lingual model editing (XME) paradigm, revealing performance limitations in multilingual LLMs for hypernetwrok based parameter-modifying methods. (Wang et al., 2023b) presented Retrieval-augmented Multilingual Knowledge Editing (ReMaKE), a model-agnostic knowledge editing method designed for multilingual settings. ReMaKE retrieves new knowledge from a multilingual knowledge base and concatenates it with prompts to update LLMs. Most works assume edited facts are independent without any multi-hop consequences of these edits, and are primarily focused on parameter updating based methods. We focus on parameter-preserving methods, and the more complex setting of multi-hop editing in a cross-lingual setup.\nMulti-Hop QA and prompting methods: With the advances in generative language technologies powered by Large Language Models (LLMs; Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022; OpenAI et al., 2023; Tay et al., 2023; Google, 2023), complex and multi-hop QA tasks are often handled by a prompt based and retrieval augmented approach (Press et al., 2022; Yao et al., 2023; Khattab et al., 2022). Works that tackle multi-hope knowledge editing have started to use this retrieve-then-generate framework to effeciently perform knowledge editing in an in-context setting, without changing the parameters of the base LLM, and have achieved SoTA performance on knowledge editing. Given their success, we"}]}]}