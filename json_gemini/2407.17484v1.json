{"title": "A SURVEY OF ACCESSIBLE EXPLAINABLE ARTIFICIAL\nINTELLIGENCE RESEARCH", "authors": ["Chukwunonso Henry Nwokoye", "Maria J. P. Peixoto", "Akriti Pandey", "Lauren Pardy", "Mahadeo Sukhai", "Peter R. Lewis"], "abstract": "The increasing integration of Artificial Intelligence (AI) into everyday life makes it essential to ex-\nplain AI-based decision-making in a way that is understandable to all users, including those with\ndisabilities. Accessible explanations are crucial as accessibility in technology promotes digital in-\nclusion and allows everyone, regardless of their physical, sensory, or cognitive abilities, to use these\ntechnologies effectively. This paper presents a systematic literature review of the research on the ac-\ncessibility of Explainable Artificial Intelligence (XAI), specifically considering persons with sight\nloss. Our methodology includes searching several academic databases with search terms to capture\nintersections between XAI and accessibility. The results of this survey highlight the lack of research\non Accessible XAI (AXAI) and stress the importance of including the disability community in XAI\ndevelopment to promote digital inclusion and accessibility and remove barriers. Most XAI tech-\nniques rely on visual explanations, such as heatmaps or graphs, which are not accessible to persons\nwho are blind or have low vision. Therefore, it is necessary to develop explanation methods through\nnon-visual modalities, such as auditory and tactile feedback, visual modalities accessible to persons\nwith low vision, and personalized solutions that meet the needs of individuals, including those with\nmultiple disabilities. We further emphasize the importance of integrating universal design principles\ninto AI development practices to ensure that AI technologies are usable by everyone.", "sections": [{"title": "1 Introduction and Motivation", "content": "Explainable Artificial Intelligence (XAI) refers to the development of Al systems that offer clear and easy-to-\nunderstand explanations for the decisions or actions they take [1]. With AI's ever-increasing importance in our daily\nlives, it is crucial to ensure that these explanation technologies are accessible to everyone, including those with sight\nloss and other disabilities, to promote digital inclusion. We argue that while the field of XAI is rapidly growing, there\nis a lack of focus on accessibility in its design. To address this, we propose the notion of Accessible Explainable\nArtificial Intelligence (AXAI) in this paper. Achieving AXAI will remove barriers and ensure that clear and under-\nstandable explanations of AI actions and decisions can help people effectively use these technologies in their daily\nlives. Emphasizing the importance of AXAI can encourage the adoption of universal design practices, ensuring that\nAI technologies are designed to be used by everyone, regardless of their physical, sensory or cognitive abilities.\nBesides ethical principles [2], the essentiality of explanations for attaining transparency, trust, reliability or dependabil-\nity for AI systems has been strongly emphasized in the European Union's Ethics Guidelines for Trustworthy AI [3].\nIn fact, research evidence has shown that counterfactual explanations can assist comprehension and contesting of AI\ndecisions by citizens [4]. Also, Bahalul et al. [5] highlighted the \u201cright to explanation\" (Goodman and Flaxman, [6])\nalongside AI system's explainability as underscored by the General Data Protection Regulation. In the context of XAI,\nthis \"right to explanation\u201d is yet to be sufficiently defined and discussed for disability (visual, speech, etc.). Consid-\nering Ali, et al., [7], explainability gives a comprehensible explanation of the model's choice, thus expressing what is\nhappening inside the model. Additionally, explanation implies \u201can understanding of how a model works, as opposed to\nan explanation of how the world works\u201d [8]. Nevertheless, it is challenging to define exactly what constitutes an expla-\nnation. There have been several successful attempts at defining explanations in XAI research [5], [7], [9]. For instance,\nBahalul et al. [5] presented the accepted meanings and explanations for several domains of research, but the research"}, {"title": "AXAI Research", "content": "unfortunately lacks an accessibility and disability perspective. The affected domains include e-commerce, human re-\nsource management, digital assistants, e-governance, healthcare, media and entertainment, education, transportation,\nfinance, and social networking.\nGoing by the meanings and styles of the domains mentioned above, it is necessary to define and discuss explanations\nand XAI presentations from a disability point of view. This is premised on the positions of Ali et al. [7], which asserted\nthat several academic research groups have studied the idea of explainability in decision-making by AI. However, every\nresearch community takes a different perspective on the problem and offers interpretations with distinct connotations.\nIn this discourse, two significant communities seem to emerge, the technology and the disability community. By\nthe technology community, we are referring to groups, meetings, conferences, researches, developments, initiatives\nand endeavours, whether academic or industry, aimed at promoting AI, XAI, machine learning (ML), deep learning\n(DL). Additionally, the disability community focuses on groups, meetings, conferences, researches, developments,\ninitiatives and endeavours aimed at making technology accessible for individuals living with any form of disability. By\nimplication, the technical aspects of XAI are the focus of the technology community, while human-centred aspects\ntowards accessibility are the focus of the disability community. Through extensive XAI research, the technology\ncommunity has made immense progress in defining and implementing explanations for various domains [5], [7]. So,\nour focus is on the disability community.\nTherefore, to ensure the inclusion of the disability community in the XAI discourse, this paper also aims to connect\nfindings to XAI and tools for accessibility. We also sought promising applications/adaptations from highly related\nstudies in human-centred computing and accessibility, for instance, some identified papers for visual disability. When\nwe say visual disability, it is crucial to understand that we mean persons who are blind, Deaf-Blind, with low vision,\nas well as individuals with visual processing disabilities. To achieve this, we conducted a literature survey to evaluate\nhow the accessibility of AI explanations, especially for people with sight loss, has been addressed."}, {"title": "2 A Literature Survey of AXAI", "content": "Many XAI techniques rely on visual explanations, such as heatmaps or graphs [10], which are not accessible to people\nwho are blind or have low vision. As a result, making complex AI explanations understandable and accessible through\nnon-visual modalities, such as audio or haptic feedback, is a significant challenge. Furthermore, the accessibility needs\ncan differ significantly among people with visual impairments, necessitating personalized solutions. After reviewing\nacademic literature, we present our methodology, results, analysis, and discussions related to AXAI."}, {"title": "2.1 Methodological Approach", "content": "We conducted a search for relevant literature on accessibility and explainable artificial intelligence using reputable\ndatabases. For this purpose, we have selected databases based on the quality and scope of the papers indexed by each.\nThis is particularly important for interdisciplinary fields like AI and accessibility. The following databases have been\nchosen:\n\u2022 Scopus is known for its broad interdisciplinary coverage, including a range of high-quality journals in science,\ntechnology, medicine, arts and humanities.\n\u2022 ISI Web of Science has a collection of high-impact publications, which is essential for capturing cutting-edge\nresearch in all areas of science and technology, including studies on explainable AI and accessibility.\n\u2022 IEEE Digital Library provides access to high-quality technical literature on AI, machine learning, and their\napplications in accessibility and inclusion, specializing in engineering and technology.\n\u2022 Google Scholar aims to include relevant works beyond traditional journals, covering a wide range of aca-\ndemic sources of diverse levels of quality.\n\u2022 El Compendex is a comprehensive engineering database that ensures technical literature coverage in specific\nengineering areas related to AI and accessibility.\n\u2022 ACM Digital Library provides access to research in assistive technology and explainable AI, as it is focused\non computing and information technology literature.\n\u2022 CINAHL & PubMed offer access to an extensive collection of health-related literature.\nWe created search strings for our selected databases to capture the intersection between Explainable AI (XAI) and\nAccessibility, ensuring the inclusion of relevant work for people with different visual disabilities. Our search strings\ninclude key terms and synonyms, such as accessibility, disability, visual impaired, sight loss, low vision, blind, and\ndeaf blind, to cover the range of terminologies used in the literature."}, {"title": "AXAI Research", "content": "Search strings:\n\u2022 (\"accessib*\" OR \"inclusiv*\") AND (\u201cexplainabl*\" OR \u201cXAI\" OR \u201ctransparent\" OR \u201cinterpretab*\") AND\n(\"artificial intelligence\" OR \"machine learning\" OR \"deep learning\") AND (\"explanations\" OR \u201cnarration\").\n\u2022 (\u201cblind\" OR \"low vision\" OR \u201cdeaf blind\" OR \u201csight loss\" OR \u201cvision loss\") AND (\u201cexplainabl*\" OR \u201cXAI\"\nOR \"transparent\") AND (\u201cartificial intelligence\" OR \u201cmachine learning\" OR \"deep learning\").\n\u2022 (\"disabil*\" OR \"visual impairment\") AND (\u201cexplainabl*\" OR \u201cXAI\" OR \"transparent\") AND (\u201cartificial\nintelligence\" OR \"machine learning\" OR \"deep learning\").\nWe conducted a systematic review using the Parsifal \u00b9 tool to manage papers and exclude duplicates from multiple\ndatabases. After using the specific search strings above and excluding duplicates, we initially selected 727 papers from\n2019 to 2024. We then selected eight articles based on their title and abstract and analyzed them individually, leading\nto only four relevant papers.\nWhile the search string resulted in a large number of papers, many were in fact not related to AI technologies that are\naccessible and explainable for individuals with sight loss. We therefore used an exclusion criteria to remove works\nthat do not address the accessibility of AI explanations, or that develop AI-based technologies for accessibility but\nwithout considering the importance of explainability.\nAs an example of the removed papers based on our exclusion criteria, consider the work titled \u201cArtificial Intelligence,\nAccessible and Assistive Technologies\" [11]. That paper discussed the potential of artificial intelligence (AI) to en-\nhance accessibility and assistive technologies for individuals with disabilities. However, it fails to address transparency\nand explanation of AI decision-making. As a result, we did not incorporate that paper and similar ones in our research.\nThe exclusion also occurred for research papers that utilize AI-based technologies to create activities or products that\nare only accessible to particular groups of people rather than those with sight loss. For example, the study \"AI Enabled\nTutor for Accessible Training\" [12], which developed an AI tool to offer job training to individuals with disabilities,\nspecifically the deaf and hard of hearing community, and similar research works were disregarded.\nOther examples of exclusion can be seen in the use of the term \u201caccessible", "Accessible Video Analytics: the Use Case of Basketball\" [13], and something easy to use for the\ngeneral public, but not explicitly designed for sight loss people, as in \u201cBuilding XAI-Based Agents for IoT Systems\\\"\n[14].\nAdditionally, we ensured that any work discussing XAI without addressing accessibility was excluded from our study.\nFor example, \\\"XAI for intrusion detection system: comparing explanations based on global and local scope\\\" [15] and\n\\\"Explainable Artificial Intelligence (XAI) User Interface Design for Solving a Rubik's Cube\\\" [16] were not included\nin our analysis.\"\n    },\n    {\n      \"title\": \"2.2 Key Papers and Their Contributions\",\n      \"content\": \"As a result of our research, we have selected four papers: two from 2020 and two from 2023. The following text\nprovides a summary of these four papers on AXAI Experiences.\nOne of the papers, titled \\\"Designing Accessible, Explainable AI (XAI) Experiences\\\" [17], explores the intersection\nof accessibility and XAI. It emphasizes the significance of making complex AI and ML models understandable and\"\n    },\n    {\n      \"title\": \"AXAI Research\",\n      \"content\": \"accessible, focusing on two main areas: accessibility at the interface and tailoring explanations to diverse and changing\nuser needs. This article uses aging-in-place and mental health support as case studies to illustrate the challenges of\nmaking XAI more accessible, particularly for those with specific needs.\nThe paper \\\"Accessible Cultural Heritage through Explainable Artificial Intelligence\\\" [18] aims to improve accessibility\nin cultural heritage using XAI. By incorporating XAI techniques, the goal is to make art and cultural experiences more\ninclusive, especially for marginalized communities. The paper discusses the challenges in making cultural heritage\naccessible to everyone, including people with disabilities, and suggests using XAI to bridge the gap. The proposed\nmethodology uses generative and multimodal models, NLP, and image captioning to create more informative and\nengaging art experiences. The primary conclusion is that XAI can play a significant role in making cultural heritage\nmore accessible and engaging for diverse audiences, including those who are visually impaired.\nThe paper titled \u201cIncreasing Transparency to Design Inclusive Conversational Agents (CAs)\\\" [19] explores how AI-\nbased CAs can be used to include marginalized and vulnerable populations such as people with mobility, visual or\nhearing disabilities, older adults, and those with mental illnesses. The paper highlights the importance of increasing\ntransparency in CAs to be more inclusive and address misperceptions. It also emphasizes challenges such as person-\nalizing transparency levels and creating human-centred knowledge on transparency and explainability. Ultimately, the\narticle concludes that enhancing transparency in CAs can improve learning and performance, address privacy concerns,\nand promote inclusion.\nThe article \\\"A Transparent CAPTCHAS Verification System for Cloud-Based Smart and Secure Applications\\\" [20] in-\ntroduces a novel CAPTCHA verification system designed for cloud-based applications. The system employs advanced\ndeep-learning techniques to convert image CAPTCHAS into text, utilizing Explainable AI (XAI) for transparency in\nthe conversion process. This approach enhances accessibility for individuals with disabilities by providing an inclusive\nverification method online. The system shows promise in CAPTCHA-based authentication, improving security and\nuser experience, especially for those with accessibility needs.\nIn order to gain insights from the selected research papers, we established correlations among them, as described in\n[21]. By doing so, we aimed to identify potential trends and patterns in accessible explainable artificial intelligence\n(XAI). This approach helps us to understand which topics are gaining popularity and where gaps in the research might\nexist. Correlating articles is a crucial step in systematic reviews and meta-analyses, which enable us to synthesize\nthe results of various studies to form comprehensive conclusions on a given topic, as explained in Booth et al. [21].\nAdditionally, considering the specific field of XAI, the work [22] provides valuable insights into the current state and\nfuture directions of explainable AI, underscoring the relevance of identifying patterns and gaps in this rapidly evolving\narea.\"\n    },\n    {\n      \"title\": \"2.3 A Comprehensive Analysis of AXAI Research\",\n      \"content\": \"Analyzing the selected papers , we identified common themes related to accessibility, inclusion, and explain-\nable artificial intelligence (XAI). However, each paper presented specific focuses and contributions.\nThe choice of variables for analysis among the papers is justified by their range and depth in understand-\ning the studies. \u201cMethodological Approach\\\", \\\"Technologies or Tools Used\\\", and \\\"Proposed Design\\\" highlight how\nresearch is conducted. They reflect on the validity and applicability of the results. As discussed in Practice of Social\nResearch [23], design research and methodologies are crucial for obtaining reliable data. The relevance and impact\nof studies in diverse settings and specific groups can be determined by considering the \u201cTarget Audience\\\" and \\\"Paper\nContext\\\". This is an important point emphasized by [24], who stresses the significance of including diversity, accessi-\nbility, and the context in the search. Additionally, the \u201cPaper Contribution\\\" and \\\"Core Themes\\\" clarify the research's\ninnovation, added value, and central topics. These factors are fundamental in evaluating the contribution of each article\nto the field of study.\nThe papers are strictly correlated in several aspects. The user-centred methodology is a cornerstone in\nall four articles, with additional convolutional neural network (CNN) techniques in the [20] study. The user-centric\napproach prioritizes the needs, experiences and feedback of end users. This methodology is fundamental to developing\nsolutions that are not only technically viable but also practical, useful and accessible to the intended users.\nThe papers use a significant variety of technologies and tools, such as generative and multimodal AI, AI-powered\nvoice assistants, and deep learning techniques for transparent CAPTCHAS. Two papers ([17] and [20]) are directly\ncorrelated regarding the use of explainable techniques, while two others ([17] and [19]) are related due to the use of\nconversational technologies. This diversity of technology reflects the potential of various AI approaches to improve\naccessibility and inclusion and the specific challenges they seek to solve within their application context.\"\n    },\n    {\n      \"title\": \"AXAI Research\",\n      \"content\": \"scriptions. The third [19] highlights the importance of transparency in conversational agents for inclusion, suggesting\npersonalization of transparency. The fourth [20] introduces a transparent CAPTCHA verification system, using deep\nlearning to transform image CAPTCHAs into text, improving accessibility.\nThe papers analyzed highlight the growing importance of accessibility and explainability or transparency in AI, focus-\ning on diverse applications and contexts. One trend is the search for personalized and inclusive solutions that meet\ndifferent user needs. A noted gap is the need for more research and effort into how AI explanations can be adapted for\nusers with different types of accessibility and deeper integration of it with existing assistive technologies.\"\n    },\n    {\n      \"title\": \"2.4 XAI Taxonomic Analysis/Relationships - Accessibility and Disability\",\n      \"content\": \"It has been established above that XAI discussions are lacking the accessibility and disability perspective, but where\nit is possible, our analyses would include human centered aspects that might enhance accessibility for persons with\nvisual disability. In this section, we aim to relate our findings to XAI taxonomies published in reputable venues\nsuch as Elsevier Science Direct, Web of Science and IEEE Xplore Digital Library. Several works on XAI taxonomy\nwere identified and collected and used for this analysis. The exclusion criteria for this search include: conference\npapers, articles prior to 2020, articles that did not include taxonomies or extensive classifications of XAI. Conference\npapers were excluded because we only want to utilize sources that provided sufficient depth in the discourse of XAI\ntaxonomies and classifications. To further describe depth here, we mean the articles that reviewed and classified both\nXAI algorithms, technologies/systems and assessment approaches for users.\nTo conduct the analysis for the taxonomic relationships of the above findings on accessibility and XAI, we commence\nwith the paper by Wolf et al. [17], which is actually a newsletter and not research/technology paper, so eliciting actual\nXAI technologies required some investigation. The reason behind this investigation is that it used case studies and\nreflections, XAI and AI/ML was described in broad terms and later discussed in line with the age-in-place and mental\nhealth administration. However, the article based its assertions on visual explanations (VE) while citing the work by\nHendricks et al. [31]. So, an in-depth analysis of this work provided us the necessary information to understand the\nclassification of the AI-based approach used. Ali et al. [7] mentioned VEs under section 7.3, i.e., \u201cJoint prediction\nand explanation\\\", for dataset training while comparing Park et al., [32]'s model of explainability to the Teaching\nExplanations for Decisions (TED) framework. Deep Neural Networks (DNN) was employed by Hendricks et al. [31]\nfor VEs in the context of recognition and detection of objects. Specifically, this method demands explanations in\na textual format alongside the class labels during training to be able to provide distinct class-dependent VEs of the\ninputted image predictions during testing.\nAdditionally, the possibilities of utilizing VEs were again discussed under Randomized Input Sampling to Provide\nExplanations (RISE) and Gradient-weighted CAM (Grad-CAM) within Ali et al [7]'s proposed taxonomy for post-\nhoc explainability (PHE). It is noteworthy that this form of explainability is organized around six key attributes and\nmethods namely: example-based explanation, visualization, knowledge extraction, attribution, game theory and neural\nmethods. RISE and CAM are classified under attribution methods. In Arrieta et al. [25], VE's was also discussed under\npost-hoc explainability. In addition, DNNs was discussed under several sections which include Hybrid explainable\nmodels, Explainability through regularization, RISE, Back propagation methods and Model distillation, etc. The\nextensive usage of DNNs is premised on the fact that it is very useful when obtaining meaningful information from\nvery complicated datasets and that networks with considerable depths facilitates improved decision-making compared\"\n    },\n    {\n      \"title\": \"AXAI Research\",\n      \"content\": \"to shallow ones. Since the article is a newsletter of accessible XAI reflections without usability assessments, it was\nnot possible to discuss any of the mentioned XAI assessment approaches in Ali et al. [7].\nThe paper by Diaz-Rodriguez and Pisoni [18] posed issues, important research questions and hypothesis for art acces-\nsibility using XAI approaches. Here, we present taxonomic analysis of identified XAI methods alongside the relevant\npapers of XAI taxonomy and classification. Hypothesis 1 is about providing assistance through AI-based models that\ncan generate content- and interpretation-wise explanations. The consideration for viable solution is image captioning.\nAli et al. [7] did not include this concept in their taxonomy. However, captioning (image, video) and its techniques\nwere discussed and implemented in Stangl et al. [33]. While hypothesis 2 asked whether XAI can explain art, research\nquestion 2 asked if XAI can both produce and distinguish between explanations of content and form. The consider-\nation for viable solution is visual question answering (VQA). VQA in ref [7] was briefly mentioned under section\n7.3 i.e., \\\"Joint prediction and explanation\\\". Research question 3 asked if art explanations can be produced on request\nthrough VQA \u2013 this concept is also on-demand interaction implemented in Stangl et al. [33] using AD systems. The\nlast method identified in the paper is Generative adversarial networks (GANs) and multimodal CNN presented under\nthe section for explaining visual art. While the concept of CNN was widely seen in Ali et al. [7], we did not find\nGAN and multimodal CNN. The article is a review paper for art accessibility using XAI and it lacked discussion of\nassessments, therefore was not included here.\nThe paper by Motta and Quaresma [19] involved AI-based conversational agents and voice assistants. Intelligent\nagents that aid conversations are not widely mentioned in Ali et al. [7], perhaps due to their focus on technical\naspects of XAI developments rather than human-centered needs and requirements. The only relevant mention of AI\nagents was under assessment of explanations and specifically, under \\\"Cognitive psychological measures": "Here, the\ntheme is user understanding of AI-based agents and algorithms. Further investigations of references revealed the study\nby Penney et al. [34] titled \"Toward foraging for understanding of StarCraft agents\". This paper is game-based as\nparticipants played the real-time strategy game and it is noteworthy that post-hoc explainability of ref [7] included\nthe game-theory methods, which are Shapley Values and Shapley Additive Explanation. The rationale behind the lack\nof more information on agents within Ali et al. [7] is because their interest was unfortunately not accessibility and\ninclusion. Generally, from our discussions it may seem that the concepts of voice assistants and audio descriptions are\nsame because of sound. Truly, there are significant similarities which are accessibility (improvements for persons who\nare visually impaired through additional interaction modes and conveying the requisite information); NLP (understand\nquestions or user commands for voice assistants or produce the right ADs) and involves AI/ML algorithms. Both\ntechnologies differ in terms of functionality and focus of the content, for example, Siri/Alexa can be used for lots of\npurposes, but ADs are for describing visual content.\nThe article by Shah et al. [20] focused on XAI and enhancing transparency and inclusion for persons with disabil-ity by the implementation of a Completely Automated Public Turing Test to Tell Computers and Humans Apart\n(CAPTCHAS) Verification System. However, a Connectionist Temporal Classification (CTC) layer was incorpo-\nrated into their system. This makes it easier to learn from sequence to sequence, enabling direct predictions of the\nCAPTCHA text and eliminating the need for intricate alignment. Shah et al. [20] utilized the CTC approach, and the\naim is to create the textual version of the CAPTCHA while training by using the text labels and the knowledge that\nhas been learnt. The proposed system applied AI methods such as LSTMRNN and CNN, which were also identified\nin Ali et al. [7] taxonomy but the CTC layer, which implements XAI was absent. It is noteworthy that text genera-\ntion may be somewhat similar to functions which include; image caption generation, text summarization and text on\nscreen [33]. Text production and processing are involved in all activities, whether they are producing descriptions in\nplain language, compressing text, and displaying information graphically. Each of them make use of natural language\nprocessing approaches, and some of the tasks may require the use of ML algorithms."}, {"title": "3 Promising Avenues for Exploring AXAI: Accessible Human-Centered Computing\nTechniques for Explainability", "content": "In this section, we provide a summary of promising avenues for exploring human-centered computing techniques for\nexplainability. This is because, we believe these avenues hold potential to impact the lives of individuals who are blind\nand partially-sighted.\nFirst, we refer to accessible human-centred computing techniques for explainability, wherein findings include confer-\nence papers published in the ACM SIGACCESS Conference on Computers and Accessibility and others. Generally,\nthese types of systems have progressed considerably, merging knowledge representation and reasoning, natural lan-\nguage processing, computer vision, and XAI. The findings are as follows: [33], [35], [36], [37], [38]. Note that these\nindividuals who were referred to as blind and low vision (BLV)."}, {"title": "AXAI Research", "content": "Second, the lives of people with visual impairments were seemingly improved by audio description (AD) systems,\naccording to research on accessibility and HCC [33]. However, within the framework of this research, this emphasizes\nhow important it is to comprehend and investigate audio XAI technological advances. The pertinent audio XAI tech-\nniques that may be applied to the development of audio explanations were enumerated by Akman et al. [39]. These\nmethods include: Layer-Wise Relevance Propagation (LRP), Discrete Fourier Transform-Layer-Wise Relevance Prop-\nagation (DFT-LRP); Cough-Local Interpretable Model-agnostic Explanations (Cough-LIME); loudness/ Non- nega-\ntive Matrix Factorization (NMF) decomposition, Spleeter, Surrogate self-explainable model NMF decomposition; and\nthe combination of LIME, Causal, Statistical Fault Localisation (SFL).\nThe analysis of these techniques using the format above (findings and taxonomic analysis) will be explored in subse-\nquent research. However, it seems clear that these techniques present promising avenues to solve complex issues of\naccessibility for individuals who are blind and partially-sighted."}, {"title": "4 Conclusion, Recommendations and Research Road-map", "content": "The motivation for this review was drawn from the documented issues and barriers faced by individuals with visual\ndisabilities [40], [41], [42], [43], [44], [45]. These barriers are particularly evident in technologically advanced settings\nwhere essential information is communicated visually, resulting in social, educational, and career marginalization and\nperhaps affecting their independence, autonomy, and well-being [33].\nThe current state of XAI research and accessibility can be compared to outdated approaches in designing and develop-\ning applications, web pages, and digital tools, with no accessibility guidelines for developers to follow. In recent times,\nthe importance of adhering to guidelines cannot be overstated [46], [47]. Therefore, we advocate for the creation of\naccessibility guidelines for the development of AI-based systems."}, {"title": "4.1 New research directions for \u03a7\u0391\u0399", "content": "Considering our findings and their implications, we now provide ideas and important research directions for enhancing\nthe accessibility of XAI for the disability community.\n1. We emphasized the importance of involving the disability community in XAI discourse. However, our pa-\nper focused mainly on visual disabilities such as blindness and low vision, suggesting the need for further\nresearch into other types of disabilities. For some mental and sensory disabilities, we can only imagine the\ncomplexities it might entail considering the complex XAI landscape.\n2. With further research and implementation, if transitioning from audio descriptions to audio explanations\nproves to be practical and effective, there may be a necessity to consider the processes involved in authoring\nand evaluating audio explanations (AE). However, we need to understand the meaning of \"minimum\" in\nMVD [33] and how it improves access for the disability community, with explanations that would positively\nimpact them. This will depend on extended research confirming the significance of the phrase \"minimum\nviable explanations\" to disability and accessibility. It refers to understanding the least amount of explanation\nthat produces the best results, as it has been found that too much information can cause cognitive overload,\nreducing a user's trust and understanding of the system[48, 49].\n3. In addition to the subjective nature of explanation, Pisoni et al. [50] aligned with advocating for more thoughts\ntowards interpretability [51] and the expertise of the audience [52] in question. Since individuals with visual\ndisability constitute the population of interest here, there is a need to consider literacy level while conceiving\nand implementing explanations. Based on the papers reviewed, potential future research could concentrate\non creating multisensory interfaces that can explain AI decisions and tailor AI explanations to better cater to\nusers' needs with a broader range of disabilities and preferences. It is worth exploring how AI explanations\ncan be more contextual and situational, adapting dynamically to the user's context. Furthermore, exploring\ninteractive approaches, where users can request additional or more detailed clarification about AI decisions,\ncould enhance comprehension and trust in the technology.\n4. The role of human-in-the-loop (HITL) [37] approach for explanations, XAI, accessibility and disability needs\nto be investigated, since Stangl et al. [33] had successes using it for verification and revision of AI-generated\ncaptions and descriptions. While Stangl et al. [33] confirmed that the HITL approach has made AI-based AD\nmore acceptable [53], they added that the process nonetheless demands infrastructure, funding, and time to\noperate at scale and may offer variable results.\n5. In our search to discover approaches to assist persons with visual disabilities, a system such as FitVid came\nto the fore. FitVid [54] is a platform that facilitates video contents that are both customizable and responsive."}, {"title": "AXAI Research", "content": "In the context herein, extended research is required to evaluate if XAI models can be made accessible using\nFitVid. They discussed \u201cadvanced accessibility with content customization\u201d and the basic idea involves\nenhancing accessibility for several circumstances and conditions. There were specific mentions of disability\npopulations such as low vision, colour blindness and dyslexia as candidates of content adaptation. We believe\nthat this is an attempt at inclusivity, which proposes the addition of color palette settings for people with color\nblindness rather than a just dark theme.\n6. Inclusion of users who are living with visual disabilities is very important for the XAI/accessibility research.\nSeveral approaches for assessing XAI-based systems were provided in Ali et al. [7], and we believe they\nmay positively impact this area of research depending on the available funding, time and infrastructure. As\na matter of fact, studying the effectiveness of different explanations and how users interact with them in real-\nworld scenarios could provide valuable insights. Involving users with disabilities in the XAI design process\nthrough co-creation can also lead to more inclusive and practical solutions.\nFor individuals who are blind, deaf blind, or with other forms of vision loss, it is essential to develop techniques\nthat utilize sound narratives or auditory descriptions [55] to clarify Al's functioning and decisions. Additionally, it\nis crucial to ensure that AI explanations are compatible and can be integrated with assistive technologies currently\nused, such as screen readers. Making AI systems' explanations and operations understandable by people with visual\ndisabilities increases their autonomy, allowing them to make informed decisions based on a clear understanding of\nhow technologies work and impact their lives.\nUpon analyzing the papers in the previous sections, it was found that there has been a lack of attention given to provid-\ning explanations of AI for people with sight loss. To address this issue, the notion of Accessible Explainable Artificial\nIntelligence, AXAI, has been proposed. Achieving AXAI will ensure that AI systems are easily understandable and\naccessible to everyone, thus promoting digital inclusion. It is crucial to advance research and development in this area\nbecause it is not just a matter of accessibility but also of equity and inclusion. We must commit to research, devel-\nopment, and collaborative testing with the sight loss community to create solutions that meet their specific needs and\npreferences."}]}