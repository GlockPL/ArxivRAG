{"title": "A SURVEY OF ACCESSIBLE EXPLAINABLE ARTIFICIAL INTELLIGENCE RESEARCH", "authors": ["Chukwunonso Henry Nwokoye", "Maria J. P. Peixoto", "Akriti Pandey", "Lauren Pardy", "Mahadeo Sukhai", "Peter R. Lewis"], "abstract": "The increasing integration of Artificial Intelligence (AI) into everyday life makes it essential to explain AI-based decision-making in a way that is understandable to all users, including those with disabilities. Accessible explanations are crucial as accessibility in technology promotes digital inclusion and allows everyone, regardless of their physical, sensory, or cognitive abilities, to use these technologies effectively. This paper presents a systematic literature review of the research on the accessibility of Explainable Artificial Intelligence (XAI), specifically considering persons with sight loss. Our methodology includes searching several academic databases with search terms to capture intersections between XAI and accessibility. The results of this survey highlight the lack of research on Accessible XAI (AXAI) and stress the importance of including the disability community in XAI development to promote digital inclusion and accessibility and remove barriers. Most XAI techniques rely on visual explanations, such as heatmaps or graphs, which are not accessible to persons who are blind or have low vision. Therefore, it is necessary to develop explanation methods through non-visual modalities, such as auditory and tactile feedback, visual modalities accessible to persons with low vision, and personalized solutions that meet the needs of individuals, including those with multiple disabilities. We further emphasize the importance of integrating universal design principles into AI development practices to ensure that AI technologies are usable by everyone.", "sections": [{"title": "1 Introduction and Motivation", "content": "Explainable Artificial Intelligence (XAI) refers to the development of Al systems that offer clear and easy-to-understand explanations for the decisions or actions they take [1]. With AI's ever-increasing importance in our daily lives, it is crucial to ensure that these explanation technologies are accessible to everyone, including those with sight loss and other disabilities, to promote digital inclusion. We argue that while the field of XAI is rapidly growing, there is a lack of focus on accessibility in its design. To address this, we propose the notion of Accessible Explainable Artificial Intelligence (AXAI) in this paper. Achieving AXAI will remove barriers and ensure that clear and understandable explanations of AI actions and decisions can help people effectively use these technologies in their daily lives. Emphasizing the importance of AXAI can encourage the adoption of universal design practices, ensuring that AI technologies are designed to be used by everyone, regardless of their physical, sensory or cognitive abilities.\nBesides ethical principles [2], the essentiality of explanations for attaining transparency, trust, reliability or dependability for AI systems has been strongly emphasized in the European Union's Ethics Guidelines for Trustworthy AI [3]. In fact, research evidence has shown that counterfactual explanations can assist comprehension and contesting of AI decisions by citizens [4]. Also, Bahalul et al. [5] highlighted the \u201cright to explanation\" (Goodman and Flaxman, [6]) alongside AI system's explainability as underscored by the General Data Protection Regulation. In the context of XAI, this \"right to explanation\u201d is yet to be sufficiently defined and discussed for disability (visual, speech, etc.). Considering Ali, et al., [7], explainability gives a comprehensible explanation of the model's choice, thus expressing what is happening inside the model. Additionally, explanation implies \u201can understanding of how a model works, as opposed to an explanation of how the world works\u201d [8]. Nevertheless, it is challenging to define exactly what constitutes an explanation. There have been several successful attempts at defining explanations in XAI research [5], [7], [9]. For instance, Bahalul et al. [5] presented the accepted meanings and explanations for several domains of research, but the research unfortunately lacks an accessibility and disability perspective. The affected domains include e-commerce, human resource management, digital assistants, e-governance, healthcare, media and entertainment, education, transportation, finance, and social networking.\nGoing by the meanings and styles of the domains mentioned above, it is necessary to define and discuss explanations and XAI presentations from a disability point of view. This is premised on the positions of Ali et al. [7], which asserted that several academic research groups have studied the idea of explainability in decision-making by AI. However, every research community takes a different perspective on the problem and offers interpretations with distinct connotations. In this discourse, two significant communities seem to emerge, the technology and the disability community. By the technology community, we are referring to groups, meetings, conferences, researches, developments, initiatives and endeavours, whether academic or industry, aimed at promoting AI, XAI, machine learning (ML), deep learning (DL). Additionally, the disability community focuses on groups, meetings, conferences, researches, developments, initiatives and endeavours aimed at making technology accessible for individuals living with any form of disability. By implication, the technical aspects of XAI are the focus of the technology community, while human-centred aspects towards accessibility are the focus of the disability community. Through extensive XAI research, the technology community has made immense progress in defining and implementing explanations for various domains [5], [7]. So, our focus is on the disability community.\nTherefore, to ensure the inclusion of the disability community in the XAI discourse, this paper also aims to connect findings to XAI and tools for accessibility. We also sought promising applications/adaptations from highly related studies in human-centred computing and accessibility, for instance, some identified papers for visual disability. When we say visual disability, it is crucial to understand that we mean persons who are blind, Deaf-Blind, with low vision, as well as individuals with visual processing disabilities. To achieve this, we conducted a literature survey to evaluate how the accessibility of AI explanations, especially for people with sight loss, has been addressed."}, {"title": "2 A Literature Survey of AXAI", "content": "Many XAI techniques rely on visual explanations, such as heatmaps or graphs [10], which are not accessible to people who are blind or have low vision. As a result, making complex AI explanations understandable and accessible through non-visual modalities, such as audio or haptic feedback, is a significant challenge. Furthermore, the accessibility needs can differ significantly among people with visual impairments, necessitating personalized solutions. After reviewing academic literature, we present our methodology, results, analysis, and discussions related to AXAI."}, {"title": "2.1 Methodological Approach", "content": "We conducted a search for relevant literature on accessibility and explainable artificial intelligence using reputable databases. For this purpose, we have selected databases based on the quality and scope of the papers indexed by each. This is particularly important for interdisciplinary fields like AI and accessibility. The following databases have been chosen:\n\u2022 Scopus is known for its broad interdisciplinary coverage, including a range of high-quality journals in science, technology, medicine, arts and humanities.\n\u2022 ISI Web of Science has a collection of high-impact publications, which is essential for capturing cutting-edge research in all areas of science and technology, including studies on explainable AI and accessibility.\n\u2022 IEEE Digital Library provides access to high-quality technical literature on AI, machine learning, and their applications in accessibility and inclusion, specializing in engineering and technology.\n\u2022 Google Scholar aims to include relevant works beyond traditional journals, covering a wide range of academic sources of diverse levels of quality.\n\u2022 El Compendex is a comprehensive engineering database that ensures technical literature coverage in specific engineering areas related to AI and accessibility.\n\u2022 ACM Digital Library provides access to research in assistive technology and explainable AI, as it is focused on computing and information technology literature.\n\u2022 CINAHL & PubMed offer access to an extensive collection of health-related literature.\nWe created search strings for our selected databases to capture the intersection between Explainable AI (XAI) and Accessibility, ensuring the inclusion of relevant work for people with different visual disabilities. Our search strings include key terms and synonyms, such as accessibility, disability, visual impaired, sight loss, low vision, blind, and deaf blind, to cover the range of terminologies used in the literature."}, {"title": "2.2 Key Papers and Their Contributions", "content": "As a result of our research, we have selected four papers: two from 2020 and two from 2023. The following text provides a summary of these four papers on AXAI Experiences.\nOne of the papers, titled \"Designing Accessible, Explainable AI (XAI) Experiences\" [17], explores the intersection of accessibility and XAI. It emphasizes the significance of making complex AI and ML models understandable and accessible, focusing on two main areas: accessibility at the interface and tailoring explanations to diverse and changing user needs. This article uses aging-in-place and mental health support as case studies to illustrate the challenges of making XAI more accessible, particularly for those with specific needs.\nThe paper \"Accessible Cultural Heritage through Explainable Artificial Intelligence\" [18] aims to improve accessibility in cultural heritage using XAI. By incorporating XAI techniques, the goal is to make art and cultural experiences more inclusive, especially for marginalized communities. The paper discusses the challenges in making cultural heritage accessible to everyone, including people with disabilities, and suggests using XAI to bridge the gap. The proposed methodology uses generative and multimodal models, NLP, and image captioning to create more informative and engaging art experiences. The primary conclusion is that XAI can play a significant role in making cultural heritage more accessible and engaging for diverse audiences, including those who are visually impaired.\nThe paper titled \u201cIncreasing Transparency to Design Inclusive Conversational Agents (CAs)\" [19] explores how AI-based CAs can be used to include marginalized and vulnerable populations such as people with mobility, visual or hearing disabilities, older adults, and those with mental illnesses. The paper highlights the importance of increasing transparency in CAs to be more inclusive and address misperceptions. It also emphasizes challenges such as personalizing transparency levels and creating human-centred knowledge on transparency and explainability. Ultimately, the article concludes that enhancing transparency in CAs can improve learning and performance, address privacy concerns, and promote inclusion.\nThe article \"A Transparent CAPTCHAS Verification System for Cloud-Based Smart and Secure Applications\" [20] introduces a novel CAPTCHA verification system designed for cloud-based applications. The system employs advanced deep-learning techniques to convert image CAPTCHAS into text, utilizing Explainable AI (XAI) for transparency in the conversion process. This approach enhances accessibility for individuals with disabilities by providing an inclusive verification method online. The system shows promise in CAPTCHA-based authentication, improving security and user experience, especially for those with accessibility needs.\nIn order to gain insights from the selected research papers, we established correlations among them, as described in [21]. By doing so, we aimed to identify potential trends and patterns in accessible explainable artificial intelligence (XAI). This approach helps us to understand which topics are gaining popularity and where gaps in the research might exist. Correlating articles is a crucial step in systematic reviews and meta-analyses, which enable us to synthesize the results of various studies to form comprehensive conclusions on a given topic, as explained in Booth et al. [21]. Additionally, considering the specific field of XAI, the work [22] provides valuable insights into the current state and future directions of explainable AI, underscoring the relevance of identifying patterns and gaps in this rapidly evolving area."}, {"title": "2.3 A Comprehensive Analysis of AXAI Research", "content": "Analyzing the selected papers in Table 2, we identified common themes related to accessibility, inclusion, and explainable artificial intelligence (XAI). However, each paper presented specific focuses and contributions.\nThe choice of variables for analysis among the papers in Table 2 is justified by their range and depth in understanding the studies. \u201cMethodological Approach\", \"Technologies or Tools Used\", and \"Proposed Design\" highlight how research is conducted. They reflect on the validity and applicability of the results. As discussed in Practice of Social Research [23], design research and methodologies are crucial for obtaining reliable data. The relevance and impact of studies in diverse settings and specific groups can be determined by considering the \u201cTarget Audience\" and \"Paper Context\". This is an important point emphasized by [24], who stresses the significance of including diversity, accessibility, and the context in the search. Additionally, the \u201cPaper Contribution\" and \"Core Themes\" clarify the research's innovation, added value, and central topics. These factors are fundamental in evaluating the contribution of each article to the field of study.\nThe papers in Table 2 are strictly correlated in several aspects. The user-centred methodology is a cornerstone in all four articles, with additional convolutional neural network (CNN) techniques in the [20] study. The user-centric approach prioritizes the needs, experiences and feedback of end users. This methodology is fundamental to developing solutions that are not only technically viable but also practical, useful and accessible to the intended users.\nThe papers use a significant variety of technologies and tools, such as generative and multimodal AI, AI-powered voice assistants, and deep learning techniques for transparent CAPTCHAS. Two papers ([17] and [20]) are directly correlated regarding the use of explainable techniques, while two others ([17] and [19]) are related due to the use of conversational technologies. This diversity of technology reflects the potential of various AI approaches to improve accessibility and inclusion and the specific challenges they seek to solve within their application context.\""}, {"title": "2.4 XAI Taxonomic Analysis/Relationships - Accessibility and Disability", "content": "It has been established above that XAI discussions are lacking the accessibility and disability perspective, but where it is possible, our analyses would include human centered aspects that might enhance accessibility for persons with visual disability. In this section, we aim to relate our findings to XAI taxonomies published in reputable venues, such as Elsevier Science Direct, Web of Science and IEEE Xplore Digital Library. Several works on XAI taxonomy were identified and collected and used for this analysis. The exclusion criteria for this search include: conference papers, articles prior to 2020, articles that did not include taxonomies or extensive classifications of XAI. Conference papers were excluded because we only want to utilize sources that provided sufficient depth in the discourse of XAI taxonomies and classifications. To further describe depth here, we mean the articles that reviewed and classified both XAI algorithms, technologies/systems and assessment approaches for users. \nTo conduct the analysis for the taxonomic relationships of the above findings on accessibility and XAI, we commence with the paper by Wolf et al. [17], which is actually a newsletter and not research/technology paper, so eliciting actual XAI technologies required some investigation. The reason behind this investigation is that it used case studies and reflections, XAI and AI/ML was described in broad terms and later discussed in line with the age-in-place and mental health administration. However, the article based its assertions on visual explanations (VE) while citing the work by Hendricks et al. [31]. So, an in-depth analysis of this work provided us the necessary information to understand the classification of the AI-based approach used. Ali et al. [7] mentioned VEs under section 7.3, i.e., \u201cJoint prediction and explanation\", for dataset training while comparing Park et al., [32]'s model of explainability to the Teaching Explanations for Decisions (TED) framework. Deep Neural Networks (DNN) was employed by Hendricks et al. [31] for VEs in the context of recognition and detection of objects. Specifically, this method demands explanations in a textual format alongside the class labels during training to be able to provide distinct class-dependent VEs of the inputted image predictions during testing.\nAdditionally, the possibilities of utilizing VEs were again discussed under Randomized Input Sampling to Provide Explanations (RISE) and Gradient-weighted CAM (Grad-CAM) within Ali et al [7]'s proposed taxonomy for post-hoc explainability (PHE). It is noteworthy that this form of explainability is organized around six key attributes and methods namely: example-based explanation, visualization, knowledge extraction, attribution, game theory and neural methods. RISE and CAM are classified under attribution methods. In Arrieta et al. [25], VE's was also discussed under post-hoc explainability. In addition, DNNs was discussed under several sections which include Hybrid explainable models, Explainability through regularization, RISE, Back propagation methods and Model distillation, etc. The extensive usage of DNNs is premised on the fact that it is very useful when obtaining meaningful information from very complicated datasets and that networks with considerable depths facilitates improved decision-making compared to shallow ones. Since the article is a newsletter of accessible XAI reflections without usability assessments, it was not possible to discuss any of the mentioned XAI assessment approaches in Ali et al. [7].\nThe paper by Diaz-Rodriguez and Pisoni [18] posed issues, important research questions and hypothesis for art accessibility using XAI approaches. Here, we present taxonomic analysis of identified XAI methods alongside the relevant papers of XAI taxonomy and classification. Hypothesis 1 is about providing assistance through AI-based models that can generate content- and interpretation-wise explanations. The consideration for viable solution is image captioning. Ali et al. [7] did not include this concept in their taxonomy. However, captioning (image, video) and its techniques were discussed and implemented in Stangl et al. [33]. While hypothesis 2 asked whether XAI can explain art, research question 2 asked if XAI can both produce and distinguish between explanations of content and form. The consideration for viable solution is visual question answering (VQA). VQA in ref [7] was briefly mentioned under section 7.3 i.e., \"Joint prediction and explanation\". Research question 3 asked if art explanations can be produced on request through VQA \u2013 this concept is also on-demand interaction implemented in Stangl et al. [33] using AD systems. The last method identified in the paper is Generative adversarial networks (GANs) and multimodal CNN presented under the section for explaining visual art. While the concept of CNN was widely seen in Ali et al. [7], we did not find GAN and multimodal CNN. The article is a review paper for art accessibility using XAI and it lacked discussion of assessments, therefore was not included here.\nThe paper by Motta and Quaresma [19] involved AI-based conversational agents and voice assistants. Intelligent agents that aid conversations are not widely mentioned in Ali et al. [7], perhaps due to their focus on technical aspects of XAI developments rather than human-centered needs and requirements. The only relevant mention of AI agents was under assessment of explanations and specifically, under \"Cognitive psychological measures\u201d. Here, the theme is user understanding of AI-based agents and algorithms. Further investigations of references revealed the study by Penney et al. [34] titled \"Toward foraging for understanding of StarCraft agents\". This paper is game-based as participants played the real-time strategy game and it is noteworthy that post-hoc explainability of ref [7] included the game-theory methods, which are Shapley Values and Shapley Additive Explanation. The rationale behind the lack of more information on agents within Ali et al. [7] is because their interest was unfortunately not accessibility and inclusion. Generally, from our discussions it may seem that the concepts of voice assistants and audio descriptions are same because of sound. Truly, there are significant similarities which are accessibility (improvements for persons who are visually impaired through additional interaction modes and conveying the requisite information); NLP (understand questions or user commands for voice assistants or produce the right ADs) and involves AI/ML algorithms. Both technologies differ in terms of functionality and focus of the content, for example, Siri/Alexa can be used for lots of purposes, but ADs are for describing visual content.\nThe article by Shah et al. [20] focused on XAI and enhancing transparency and inclusion for persons with disability by the implementation of a Completely Automated Public Turing Test to Tell Computers and Humans Apart (CAPTCHAS) Verification System. However, a Connectionist Temporal Classification (CTC) layer was incorporated into their system. This makes it easier to learn from sequence to sequence, enabling direct predictions of the CAPTCHA text and eliminating the need for intricate alignment. Shah et al. [20] utilized the CTC approach, and the aim is to create the textual version of the CAPTCHA while training by using the text labels and the knowledge that has been learnt. The proposed system applied AI methods such as LSTMRNN and CNN, which were also identified in Ali et al. [7] taxonomy but the CTC layer, which implements XAI was absent. It is noteworthy that text generation may be somewhat similar to functions which include; image caption generation, text summarization and text on screen [33]. Text production and processing are involved in all activities, whether they are producing descriptions in plain language, compressing text, and displaying information graphically. Each of them make use of natural language processing approaches, and some of the tasks may require the use of ML algorithms."}, {"title": "3 Promising Avenues for Exploring AXAI: Accessible Human-Centered Computing Techniques for Explainability", "content": "In this section, we provide a summary of promising avenues for exploring human-centered computing techniques for explainability. This is because, we believe these avenues hold potential to impact the lives of individuals who are blind and partially-sighted.\nFirst, we refer to accessible human-centred computing techniques for explainability, wherein findings include conference papers published in the ACM SIGACCESS Conference on Computers and Accessibility and others. Generally, these types of systems have progressed considerably, merging knowledge representation and reasoning, natural language processing, computer vision, and XAI. Note that these individuals who were referred to as blind and low vision (BLV).\nSecond, the lives of people with visual impairments were seemingly improved by audio description (AD) systems, according to research on accessibility and HCC [33]. However, within the framework of this research, this emphasizes how important it is to comprehend and investigate audio XAI technological advances. The pertinent audio XAI techniques that may be applied to the development of audio explanations were enumerated by Akman et al. [39]. These methods include: Layer-Wise Relevance Propagation (LRP), Discrete Fourier Transform-Layer-Wise Relevance Propagation (DFT-LRP); Cough-Local Interpretable Model-agnostic Explanations (Cough-LIME); loudness/ Non-negative Matrix Factorization (NMF) decomposition, Spleeter, Surrogate self-explainable model NMF decomposition; and the combination of LIME, Causal, Statistical Fault Localisation (SFL).\nThe analysis of these techniques using the format above (findings and taxonomic analysis) will be explored in subsequent research. However, it seems clear that these techniques present promising avenues to solve complex issues of accessibility for individuals who are blind and partially-sighted."}, {"title": "4 Conclusion, Recommendations and Research Road-map", "content": "The motivation for this review was drawn from the documented issues and barriers faced by individuals with visual disabilities [40], [41], [42], [43], [44], [45]. These barriers are particularly evident in technologically advanced settings where essential information is communicated visually, resulting in social, educational, and career marginalization and perhaps affecting their independence, autonomy, and well-being [33].\nThe current state of XAI research and accessibility can be compared to outdated approaches in designing and developing applications, web pages, and digital tools, with no accessibility guidelines for developers to follow. In recent times, the importance of adhering to guidelines cannot be overstated [46], [47]. Therefore, we advocate for the creation of accessibility guidelines for the development of AI-based systems."}, {"title": "4.1 New research directions for \u03a7\u0391\u0399", "content": "Considering our findings and their implications, we now provide ideas and important research directions for enhancing the accessibility of XAI for the disability community.\n1. We emphasized the importance of involving the disability community in XAI discourse. However, our paper focused mainly on visual disabilities such as blindness and low vision, suggesting the need for further research into other types of disabilities. For some mental and sensory disabilities, we can only imagine the complexities it might entail considering the complex XAI landscape.\n2. With further research and implementation, if transitioning from audio descriptions to audio explanations proves to be practical and effective, there may be a necessity to consider the processes involved in authoring and evaluating audio explanations (AE). However, we need to understand the meaning of \"minimum\" in MVD [33] and how it improves access for the disability community, with explanations that would positively impact them. This will depend on extended research confirming the significance of the phrase \"minimum viable explanations\" to disability and accessibility. It refers to understanding the least amount of explanation that produces the best results, as it has been found that too much information can cause cognitive overload, reducing a user's trust and understanding of the system[48, 49].\n3. In addition to the subjective nature of explanation, Pisoni et al. [50] aligned with advocating for more thoughts towards interpretability [51] and the expertise of the audience [52] in question. Since individuals with visual disability constitute the population of interest here, there is a need to consider literacy level while conceiving and implementing explanations. Based on the papers reviewed, potential future research could concentrate on creating multisensory interfaces that can explain AI decisions and tailor AI explanations to better cater to users' needs with a broader range of disabilities and preferences. It is worth exploring how AI explanations can be more contextual and situational, adapting dynamically to the user's context. Furthermore, exploring interactive approaches, where users can request additional or more detailed clarification about AI decisions, could enhance comprehension and trust in the technology.\n4. The role of human-in-the-loop (HITL) [37] approach for explanations, XAI, accessibility and disability needs to be investigated, since Stangl et al. [33] had successes using it for verification and revision of AI-generated captions and descriptions. While Stangl et al. [33] confirmed that the HITL approach has made AI-based AD more acceptable [53], they added that the process nonetheless demands infrastructure, funding, and time to operate at scale and may offer variable results.\n5. In our search to discover approaches to assist persons with visual disabilities, a system such as FitVid came to the fore. FitVid [54] is a platform that facilitates video contents that are both customizable and responsive.\nIn the context herein, extended research is required to evaluate if XAI models can be made accessible using FitVid. They discussed \u201cadvanced accessibility with content customization\u201d and the basic idea involves enhancing accessibility for several circumstances and conditions. There were specific mentions of disability populations such as low vision, colour blindness and dyslexia as candidates of content adaptation. We believe that this is an attempt at inclusivity, which proposes the addition of color palette settings for people with color blindness rather than a just dark theme.\n6. Inclusion of users who are living with visual disabilities is very important for the XAI/accessibility research. Several approaches for assessing XAI-based systems were provided in Ali et al. [7], and we believe they may positively impact this area of research depending on the available funding, time and infrastructure. As a matter of fact, studying the effectiveness of different explanations and how users interact with them in real-world scenarios could provide valuable insights. Involving users with disabilities in the XAI design process through co-creation can also lead to more inclusive and practical solutions.\nFor individuals who are blind, deaf blind, or with other forms of vision loss, it is essential to develop techniques that utilize sound narratives or auditory descriptions [55] to clarify Al's functioning and decisions. Additionally, it is crucial to ensure that AI explanations are compatible and can be integrated with assistive technologies currently used, such as screen readers. Making AI systems' explanations and operations understandable by people with visual disabilities increases their autonomy, allowing them to make informed decisions based on a clear understanding of how technologies work and impact their lives.\nUpon analyzing the papers in the previous sections, it was found that there has been a lack of attention given to providing explanations of AI for people with sight loss. To address this issue, the notion of Accessible Explainable Artificial Intelligence, AXAI, has been proposed. Achieving AXAI will ensure that AI systems are easily understandable and accessible to everyone, thus promoting digital inclusion. It is crucial to advance research and development in this area because it is not just a matter of accessibility but also of equity and inclusion. We must commit to research, development, and collaborative testing with the sight loss community to create solutions that meet their specific needs and preferences."}]}