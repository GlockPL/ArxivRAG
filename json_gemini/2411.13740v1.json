{"title": "Federated Continual Learning for Edge-AI: A Comprehensive Survey", "authors": ["ZI WANG", "FEI WU", "FENG YU", "YURUI ZHOU", "JIA HU", "GEYONG MIN"], "abstract": "Edge-AI, the convergence of edge computing and artificial intelligence (AI), has become a promising paradigm that enables the deployment of advanced AI models at the network edge, close to users. In Edge-AI, federated continual learning (FCL) has emerged as an imperative framework, which fuses knowledge from different clients while preserving data privacy and retaining knowledge from previous tasks as it learns new ones. By so doing, FCL aims to ensure stable and reliable performance of learning models in dynamic and distributed environments. In this survey, we thoroughly review the state-of-the-art research and present the first comprehensive survey of FCL for Edge-AI. We categorize FCL methods based on three task characteristics: federated class continual learning, federated domain continual learning, and federated task continual learning. For each category, an in-depth investigation and review of the representative methods are provided, covering background, challenges, problem formalisation, solutions, and limitations. Besides, existing real-world applications empowered by FCL are reviewed, indicating the current progress and potential of FCL in diverse application domains. Furthermore, we discuss and highlight several prospective research directions of FCL such as algorithm-hardware co-design for FCL and FCL with foundation models, which could provide insights into the future development and practical deployment of FCL in the era of Edge-AI.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep Learning (DL) has emerged as a leading approach in artificial intelligence (AI), with demonstrable efficacy across various scientific fields, including computer vision, natural language processing, and speech recognition [1]. DL utilises artificial neural networks with multiple hidden layers to model high-level abstractions and learn complex patterns and representations from data [2]. In recent years, the proliferation of DL applications has catalysed advancements in various sectors, exemplified by their role in assisting medical diagnostics [3], enhancing autonomous driving systems [4], and accelerating genomics research [5]. However, traditional implementations of DL rely on cloud computing systems with centralised servers and data storage, which can raise privacy concerns when collecting user data, incur high communication costs, and increase latency between servers and clients. To address these challenges, edge computing has emerged as a promising approach, which is a distributed computing paradigm that brings computation and storage closer to data sources, rather than relying on centralised cloud-based data processing. This paradigm shift can significantly reduce the latency and cost, making it suitable for data-intensive and latency-sensitive AI applications. Therefore, the convergence of edge computing and AI gives rise to Edge-AI, which aims to enable real-time AI applications powered by edge computing.\nEdge-AI employs a popular distributed machine learning approach called federated learning (FL) [6], which allows collaborative DL model training across clients while keeping the data localised. To achieve this, a coordinating server distributes the global model to participating clients, which then train the model using their local data. By aggregating processed parameters such as gradients rather than raw data from each client on the coordinating server, FL ensures the overall training performance and effectiveness of the global model while complying with data security regulations [7, 8] such as the General Data Protection Regulation (GDPR) and the Data Protection Act (DPA), addressing growing concerns about user privacy in AI applications.\nFL research has mainly focused on model convergence under non-independent and identically distributed (non-IID) data [9], model aggregation [10], security and privacy [11], resource optimisation and incentive mechanisms [12], etc. Furthermore, most FL works assume that the training dataset of clients is sampled from a static data distribution [13] and available from the beginning of the training [14]. Whereas, in real-world scenarios, the progressive data collection, the distribution of data, the class of samples, and the number of tasks can change over time, bringing significant challenges to the model adaptability [15].\nRecently, continual learning (CL), also known as incremental learning (IL) or lifelong learning (LL), has become an important approach for learning and accumulating knowledge from a continual stream of data [13]. Thus, integrating the concept of CL into the FL framework, known as Federated Continual Learning (FCL), leverages the strengths of both FL and CL to establish a robust foundation for Edge-AI in dynamic and distributed environments. However, continual learning from a series of new tasks can cause the model to experience significant performance degradation on previously learned tasks, a phenomenon known as catastrophic forgetting (CF) [13]. FCL deteriorates this problem as FL allows clients to join and leave the learning process arbitrarily. Furthermore, the heterogeneity of FL clients leads local models to learn diverse knowledge, exacerbating catastrophic forgetting in the global model during the aggregation of these local models. Recent studies (e.g., [15-18]) have proposed solutions to tackle these challenges, giving rise to an emerging research field that is increasingly attracting attention."}, {"title": "1.1 Related Surveys", "content": "In recent years, comprehensive surveys for FL and CL have been conducted separately [19\u201323]. For federated learning, Zhang et al. [19] surveyed FL in the IoT domain and explored FL-empowered IoT applications such as healthcare, smart city, and autonomous driving. Ye et al. [20] focused on the challenges of heterogeneous FL from five perspectives: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity and additional challenges. For continual learning, Van et al. [21] reviewed CL methods and summarised three types of CL as a common framework to cross-compare the performances of various methods. Lange et al. [22] surveyed works on CL for task classification, categorising them into replay-based, regularisation-based, and parameter isolation methods, based on how task information is stored and used throughout the learning process. Masana et al. [23] focused on class-incremental learning and categorized the existing CL methods for image classification into regularisation, rehearsal, and bias-correction methods. They also provided an extensive experimental evaluation of those methods for image classification tasks.\nThese surveys are focused on separate areas of FL and CL. None of them has systematically investigated the challenges and solutions proposed in the emerging paradigm of FCL, especially in the Edge-AI environments. Recently, Yang et al. [24] conducted a survey of FCL from the perspective of knowledge fusion. They proposed two frameworks, namely synchronous and asynchronous FCL, for addressing the spatial-temporal catastrophic forgetting challenge in FCL with knowledge fusion. Different from their work, our survey thoroughly investigates and categorizes the existing FCL methods in Edge-AI based on three task characteristics: federated class continual learning, federated domain continual learning, and federated task continual learning. In sections 2, 3, and 4, these taxonomies will be explained in more detail."}, {"title": "1.2 Aim and Contributions", "content": "This survey aims to comprehensively investigate the state-of-the-art research on FCL to provide an in-depth and consolidated review. From the perspectives of different task characteristics in FCL, we thoroughly review the background, challenges, and methods of FCL. Furthermore, we explore existing FCL-empowered applications for Edge-AI. This survey also provides an in-depth discussion about future research directions, motivating researchers to address important open challenges in FCL and offering insights that could inspire future advancement in Edge-AI. To the best of our knowledge, this paper is the first comprehensive survey of federated continual learning for Edge-AI.\nThe main contributions of this survey are summarised as follows:\n\u2022 We present a comprehensive review and clear taxonomy of the state-of-the-art FCL research based on different task characteristics: federated class continual learning, federated domain continual learning, and federated task continual learning, including a large number of papers in this rapidly expanding research field. The taxonomy, definitions, challenges, and advantages and disadvantages of the representative methods are thoroughly discussed.\n\u2022 We provide a review and summary of current real-world applications empowered by FCL, such as intelligent transportation systems, intelligent medical systems, IoT, and digital twins, highlighting the versatility and potential of FCL for making real-world impact.\n\u2022 We deliberate upon and posit several open research challenges including the lack of universal benchmarks, explainability, algorithm-hardware co-design, and FCL with foundation models, while proposing prospective directions that could inspire the research community to advance the field of FCL for its rapid development and wide deployment in the era of Edge-AI."}, {"title": "1.3 Survey Organisation", "content": "The overview of the survey is shown in Fig. 1, and the remainder of this paper is structured as follows. Section 2 first provides a detailed definition of dynamically adding new classes in FCL. Then, four types of approaches for this problem are categorized, and we elucidate the interrelationships among these approaches. Section 3 analyses four types of solutions to the problem of domain drift in FCL. Section 4 analyses current popular approaches in federated task continual learning. Section 5 investigates various applications empowered by FCL. Section 6 discusses several important open challenges and highlights exciting future research directions in FCL for Edge-AI. Finally, Section 7 concludes this survey."}, {"title": "2 FEDERATED CLASS CONTINUAL LEARNING", "content": "The first FCL scenario that we categorized is federated class continual learning (FCCL). Specifically, the objective of class continual learning (CCL) is to discriminate between incrementally observed new classes. For instance, a well-trained model in the CCL setting should distinguish all classes, such as 'birds' and 'dogs' in the first task, 'tigers' and 'fish' in the second task [25]. However, CCL is regarded as a challenging setting since the task identity is not provided. Moreover, by integrating into the federated learning paradigm where clients gather data on new classes in a streaming manner, FCCL leads to exacerbated forgetting challenge of old classes during training [16, 17, 26]. Specifically, there are two challenges in FCCL:\n\u2022 Challenge 1: intra-task forgetting: In scenarios where a client is not involved in a particular training round, the newly aggregated global model is at risk of experiencing a performance drop to retain knowledge previously contributed by that client's data. Consequently, this can result in unsatisfying performance when applying the global model to the local data of the non-participating client.\n\u2022 Challenge 2: inter-task forgetting: When clients train models with new tasks, the performance of new global degrades on old tasks.\nTo provide a clear understanding of the FCCL problem, we first formalize its definition as follows. Given a global server $S_g$ and $M$ clients in the FL process, in each federated training round $r \\in \\{1, ..., R\\}$, each client $m_k (k \\in [1, ..., M])$ trains its local model parameters $\\theta_{m_k}$ by using a"}, {"title": "2.1 Generative Replay", "content": "Replay is a critical strategy to recreate or preserve representations of old classes and combine them with available training data to address the catastrophic forgetting problem in FCCL.\nShenaj et al. [18] proposed a federated learning system with prototype aggregation for continual representation (FedSpace), which utilized class prototypes in feature space for each old class as replay and contrastive learning to preserve previous knowledge to avoid too divergent behaviour between different clients. Specifically, each client receives the initialized pre-training model over a custom-generated fractal dataset on the server side. The client then computes the prototypes of each class, and aggregates them with a weights parameter followed by prototype augmentation. Furthermore, they introduced a prototype-based loss and an additional loss function based on contrastive learning for clients' optimization.\nHendryx et al. [27] proposed the federated prototypical networks to facilitate more efficient sequential learning of new classes building on the prototypical networks. Specifically, it can enhance model performance through replaying feature vectors representative classes. However, it overlooks the non-IID data distribution across these distinct clients. Recent works demonstrated that this can be overcome via generative replay.\nGenerative replay (GR), typically implemented through generative adversarial networks (GANs) [28], acts as a data replay method by modelling the class distribution of real samples and then synthesizing instances. However, adapting GR to FCL settings is not straightforward. To solve this challenge, Qi et al. [29] discovered empirically that the unstable learning process from distributed training on non-IID data using standard federated learning algorithms can significantly impair GR-based models' performance. In response, they proposed FedCIL through model consolidation and consistent enforcement. On the server side, the global model is initialized with combined parameters and a collection of classification heads from various clients, then consolidated using instances synthesized by client generators. This can avoid failure caused by simply merging the parameters originating from clients with imbalanced new class data. To enforce consistency on the client side, a consistency loss is applied to the output logits of the client's classification module during local training.\nBabakniya et al. [30, 31] introduced the mimicking federated continual learning (MFCL), a method akin to FedCIL, designed to compensate for the lack of old data through generative replay and mitigate forgetting. In MFCL, the generative model which encourages to synthesize more uniform and balanced class images is trained on the server side in a data-free manner. This enables MFCL"}, {"title": "2.2 Parameter Regularization", "content": "Parameter regularization in FCCL is crucial to achieve the dual goals of adapting to new tasks or data distributions while preserving previously acquired knowledge. This method evaluates the significance of each network parameter and assigns greater weight to more critical parameters, thus minimizing catastrophic forgetting.\nHowever, it often cannot accumulate comprehensive data on all classes due to the limited storage capacity of FL clients. To tackle this challenge, Dong et al. [16] proposed the global-local forgetting compensation (GLFC) model. This work targets local forgetting caused by the class imbalance in local clients by implementing a class-aware gradient compensation loss and a class-semantic relation distillation loss. These losses aim to balance the forgetting of old classes and maintain consistent inter-class relations across tasks. Moreover, a proxy server is introduced to select the best old global model for aiding each client's local training to address global forgetting caused by the non-IID distribution of classes among clients. Further, a gradient-based prototype sample communication mechanism is developed to safeguard the privacy of communications between the proxy server and clients. Then, Dong et al. [32] extended GLFC and proposed the local-global anti-forgetting (LGA), which surpasses GLFC by efficiently performing local anti-forgetting on old classes. They proposed a category-balanced gradient-adaptive compensation loss and a category gradient-induced sematic distillation loss to solve local catastrophic forgetting on old categories. A proxy server is designed to collect perturbed prototype images of new classes, which can help select the best old model for global anti-forgetting via self-supervised prototype augmentation. Compared to the experiments in GLFC, this work conducted more detailed experiments on representative datasets under various FCCL settings and metrics such as top-1 accuracy, F1 score, and recall.\nApart from these two works, Dong et al. [33] observed that challenges in federated incremental semantic segmentation (FISS) are heterogeneous forgetting of old classes from both intra-client and inter-client perspectives. Therefore, they developed the forgetting-balanced learning (FBL) model to tackle these challenges. Specifically, they introduced a forgetting-balanced semantic compensation loss and a forgetting-balanced relation consistency loss to handle intra-client heterogeneous forgetting across old classes, guided by confidently generated pseudo-labels through adaptive class-balanced pseudo-labelling. Then, a task transition monitor is designed to surmount inter-client heterogeneous forgetting, enabling new class recognition under privacy protection and storing the latest old model for global relation distillation.\nInspired by the connection between client drift in FL caused by clients' unbalanced classes and catastrophic forgetting for old classes in CL, Legate et al. [34] introduced local client forgetting problem. Motivated by the balanced softmax cross-entropy method for CL, they applied a re-weighted softmax (WSM) for the loss function of each client based on its class distribution. Their"}, {"title": "2.3 Parameter Decomposition", "content": "Parameter decomposition used in FCCL is a method where the model's parameters are usually divided into shared global parameters and task-specific parameters, which capture the general knowledge among all learned tasks and informative knowledge for tasks with specific classes. This enables the learned global model to adapt to new class-incremental tasks without losing the previous knowledge learned from previous tasks.\nMotivated by additive parameter decomposition (APD) [39], Yoon et al. [40] proposed FedWeIT. They decomposed the model parameters into dense global parameters and sparse task-specific parameters to maximize the knowledge transfer between clients while minimizing the interference of irrelevant knowledge from other clients and communication costs. Further, they divided task-specific parameters into local base parameters and task-adaptive parameters, which capture the general knowledge for each client and each task with specific classes per client, respectively. Moreover, the sparse mask is applied to select only relevant base parameters for the knowledge of specific classes to lower the number of parameters transferred, thus reducing communication costs.\nSimilar to FedWeIT, Zhang et al. [41] proposed cross-FCL based on parameter decomposition inspired by APD [39] and several cross-edge strategies, which is a cross-edge FCL algorithm to enable cross-edge devices to continually learn tasks without forgetting. They used parameter decomposition by only aggregating base parameters from given tasks with specific classes to solve the challenge of knowledge interference from model aggregation in FL and from inter-task knowledge in CL. In addition, to tackle the cross-edge initial decision for usage between the local model and global model, several different cross-edge strategies including discard, replace, finetune, fusion and EWC fusion are proposed for different task relationships.\nDifferent from the above methods, Luo et al. [42] proposed an FCL framework called FedKNOW, which continually extracts and integrates the knowledge of signature tasks, featuring the concept of signature tasks that are the most dissimilar tasks identified from local past tasks. Specifically, each client consists of a knowledge extractor, a gradient restorer and a gradient integrator. FedKNOW first retained the top-ranked weight parameters that are extracted as specified tasks' knowledge"}, {"title": "2.4 Prompt-based methods", "content": "Recent trends involve designing FCCL approaches using a pre-trained Vision Transformer (ViT) as a backbone. The ViT adapts its representational capability to streaming class-incremental data through a continual learning method known as \u2018learning to prompt', by dynamically incorporating a set of learned model embeddings, i.e., prompts.\nHalbe et al. [43] first formulated intra-task forgetting and inter-task forgetting in FCCL. To mitigate forgetting while minimizing communication costs, protecting client privacy, and enhancing client-level computational efficiency, they proposed HePCo, a prompt-based data-free FCCL method. In HePCo, each client performs decomposed prompting, where prompts holding class-specific task information are used to solve corresponding local tasks. The final prompt is obtained with a weighted summation by the cosine scores of all these prompts. During local learning, each client learns the key and prompt matrices along with the classifier while keeping the ViT backbone frozen. Then, a few parameters including key, prompt weights and classifier weights are transferred to the server, which lowers communication overhead and safeguards client privacy by preventing local model inversion. On the server side, the latent generator, which takes as input a class label encoded using an embedding layer and a noise vector sampled from the standard normal distribution is trained for the current and previous tasks. Once generator training is finished, data-free distillation in the latent space is then employed to combat intra-task forgetting for the current task through finetuning the server model while using pseudo-data corresponding to past tasks helps mitigate inter-task forgetting.\nBagwe et al. [44] pointed out that the challenge in implementing prompting techniques in FCCL is the unbalanced class distribution of distributed clients, which can cause biased learning performance and slow convergence, while asynchronous task appearances further deteriorate it. They proposed Fed-CPrompt by incorporating asynchronous prompting learning and contrastive and continual loss (C2Loss) to alleviate inter-task forgetting and inter-client data heterogeneity. Fed-CPrompt allows class-specific task prompts aggregation in parallel by taking advantage of task synchronicity. C2Loss is designed to accommodate discrepancies due to biased local training between clients in environments with heterogeneous data distribution and to curb the forgetting effect via enforcing distinct task-specific prompts construction.\nDiffering from previous methods, motivated by the intuition that task-irrelevant prompts may contain potential common knowledge to enhance the embedded features, Liu et al. [45] integrated three types of prompts (i.e., task-specific, task-similar prompts and task-irrelevant prompts) into image feature embedding. This strategy effectively preserves both old and new knowledge within local clients, thereby addressing the issue of catastrophic forgetting. Additionally, it ensures the thorough integration of knowledge related to the same task across different clients via sorted and aligned the task information in the prompt pool. This effectively mitigates the non-IID problem, which arises due to class imbalances among various clients engaged in the same incremental task."}, {"title": "2.5 Knowledge Distillation", "content": "Knowledge Distillation (KD) [46], initially developed for transferring knowledge from larger and complex models to smaller and compact models, has gained widespread use in FCCL. It enables"}, {"title": "2.6 Summary and analysis of FCCL approaches", "content": "The major contribution of FCCL methods categorised above is summarised in Table 1. To further indicate the relation between these representative methods, Fig. 2 distinguishes these methods using generative replay (purple), parameter regularization (yellow), parameter decomposition (orange), prompt-based methods (green) and knowledge distillation (blue). Moreover, as indicated by the dashed boxes, three common strategies and methods, i.e., auxiliary datasets, the data-free manner with distillation, and the rehearsal-free method, are utilized in several FCCL methods. From this diagram, we can see that:\n\u2022 The impact of GLFC [16] on subsequent research and the integration of techniques from different fields are evident.\n\u2022 The impact of LwF [47], iCaRL [36] and APD [39] upon the field of FCCL is huge.\n\u2022 With the rapid development of generative models, techniques represented by GAN have emerged in the field of replay-based FCCL, which especially satisfies the users' data privacy protection needs in a data-free manner.\n\u2022 Benefiting from the advanced capabilities and rapid developments of foundation models in representation and transferability, a notable emergence of innovative FCL methods incorporating well-pre-trained models, such as ViT-based methods, have progressively surfaced [43, 45, 52].\nThese approaches offer various solutions and pivotal insights to address the challenges encountered in FCCL which is still in its nascent stage."}, {"title": "3 FEDERATED DOMAIN CONTINUAL LEARNING", "content": "In this section, we delve into various strategies aimed at addressing challenges in the second FCL scenario, namely Federated Domain Continual Learning (FDCL). In FDCL, \u2018domain' typically refers"}, {"title": "3.1 Domain Data Supplementation", "content": "The approach of domain data supplementation aims to achieve local dataset expansion by incorporating the data distribution from other clients. It can address the issue of poor generalization caused by dataset isolation from different clients and the absence of old data from the local client. A viable strategy involves employing data synthesis techniques to create proxy datasets that mimic others' domains by supplementing data from other clients while maintaining privacy [53-55]. Additionally, direct storage of old data also serves as an effective way as it supplements old data from the local client to remember the previous domain [56-59].\nSupplementing data from other clients. Inspired by feature extraction in image frequency domain space, Liu et al. [53] proposed federated domain generalization (FedDG), which exchanges part of frequency information across clients to supplement data in a privacy-conscious manner. Specifically, FedDG decomposes the amplitude (i.e., low-level distribution) and phase signals (i.e., high-level semantics) by fast fourier transform (FFT). Based on this, an 'amplitude distribution bank' is created for client data sharing, where each client generates new signals by interpolating the local amplitude with data from the shared bank while maintaining the local phase signal constant. These interpolated signals are transformed by the inverse fourier transform (IFT) to create a proxy dataset as complements. FedDG implicitly synthesizes data from other clients, bridging the gap between local and global models."}, {"title": "3.2 Domain Knowledge Learning", "content": "The approach of domain knowledge learning considers the potential privacy risks associated with proxy datasets. It prioritizes the direct knowledge transfer over the data transfer approach to address inter-domain differences among different clients and intra-domain drift within the local client. In the case of inter-domain knowledge learning, the strategy focuses on efficiently transferring knowledge from other clients to the local client [15, 60, 61]. In the case of intra-domain knowledge learning, it emphasizes the application of previously acquired knowledge to adapt the local domain drift [62-64].\nInter-domain knowledge learning. To solve heterogeneity and catastrophic forgetting problems in distributed domains, Huang et al. [60] presented a method based on federated cross-correlation and continual learning. The authors built a cross-correlation matrix across different clients with an unlabeled public dataset and exploited knowledge distillation techniques in the local updating process. A new loss function is proposed by federated cross-correlation learning that boosts model similarity while accounting for model diversity. The generated loss function is combined with a dual-domain knowledge distillation-based loss function, where the latest model is computed from a mixture of the learned global model and the previous local model.\nSimilarly using the knowledge distillation method, Ma et al. [15] implemented continual federated learning with distillation (CFeD) at both the client and server side with different learning objectives for different clients. The essence of CFeD lies in using the learned model in the previous domain to predict a proxy dataset and then utilising the predictions as pseudo-labels to retain knowledge in currently inaccessible domains. Moreover, CFeD also presents a server distillation mechanism specifically designed to address within-task forgetting in different domains. It involves adjusting the aggregated global model to imitate the output from both the previous global domain and the current local domain model.\nDifferent from the above methods, Wang et al. [61] developed a complex lifetime federated meta-reinforcement learning (LFMRL) algorithm, which leveraged prior obtained knowledge from federated meta-learning to quickly adapt to new domains. Specifically, LFMRL devises a knowledge fusion algorithm that integrates federated meta-learning and dual-attention deep reinforcement learning to update local gradient data and generate shared models. Additionally, LFMRL also designs an efficient knowledge transfer mechanism for the rapid learning of new domains in new environments. This approach enhances the generalization of the model not only in the known domains but also in the unknown domains."}, {"title": "3.3 Domain Model Enhancement", "content": "The approach of domain model enhancement emphasizes improving the models of local clients to mitigate the issue of memory loss. It enhances the model's capabilities of resistance to forgetting and adaptability by integrating innovative network structures or techniques. According to whether the structure of the model is fixed, existing works based on this approach can be divided into two categories: fixed structure [65\u201368] and non-fixed structure [69\u201373]. This approach aims to leverage the learning capabilities of the client model to retain the memory of the old domain while seamlessly integrating new domain knowledge.\nFixed Structure. De Caro et al. [65] facilitated effective learning in dynamic environments by employing echo state networks (ESNs) and intrinsic plasticity (IP). The authors introduced the FedIP algorithm to optimize the processing of stationary data in a federated learning setting and adapt the learning rules of IP. In non-stationary scenarios, FedCLIP extends FedIP by updating memory buffers and sampling the mini-batches. Zhang et al. [66] exploited synaptic intelligence (SI) for weight updating to maintain the memory of the previous domain. They also added a structural regularization loss term that integrates knowledge from other local models to tune the global model towards a global optimum while minimizing weight variance. Chathoth et al. [67] observed that non-IID data distributions have a significant impact on the performance of differential privacy (DP) stochastic algorithms. To counteract this issue, they integrated DP with SI to meet the privacy requirements of each client. In particular, they mitigated catastrophic forgetting by adding a quadratic SI loss to the objective function to minimize modifications to parameters that affect the previous model. Furthermore, they improved the (\u03b5, \u03b4)-DP training method for a cohort-based DP setting, tailoring it to meet the distinct privacy requirements of each cohort. To bridge continual learning with federated learning and improve the robustness of different clients to non-IID problems, Ma et al. [68] introduced the elastic federated learning (EFL) framework. It integrates an elasticity term that constrains the volatility of crucial parameters, as determined by the Fisher information matrix, within the local objective function. Moreover, they employed scaling aggregation coefficients to counteract convergence degradation. The framework is further optimized through sparsification and quantization techniques, effectively compressing both upstream and downstream communications.\nNon-fixed Structure. Drawing on insights from predictive coding in neuroscience, where updating the parameters of only some of the active heads can prevent the other inactive heads from being forgotten, Bereska et al. [69] proposed an approach based on reservoir computing in"}, {"title": "3.4 Domain Weight Aggregation", "content": "The approach of domain weight aggregation assesses and reorganizes the relationships between clients based on the uploaded model weights. Researches based on this approach can be divided into two key parts, non-uniform weight aggregation [74\u201376] and reorganization relationships of clients [77, 78]. The first part concentrates on directly optimizing the weight aggregation process to improve the model's generalization capabilities, while the second one emphasizes indirectly influencing the weight aggregation process by organizing relationships between clients and further considers adaptation to dynamically changing environments.\nNon-uniform weight aggregation Zhang et al. [74] devised an FL-friendly generalization adjustment (GA) method that combines a genetic algorithm with domain flatness constraint to determine the best weights for each client. Specifically, the flatness of each domain is evaluated by the difference in generalization between the global and local models. Meanwhile, domain weights are dynamically adjusted during server aggregation. Dupuy et al. [75] showed that in natural language understanding (NLU) training, non-uniform device selection based on the number of interactions improves model performance, with benefits increasing over time. Wang et al. [76] introduced the orthogonal gradient aggregation (OGA) method instead of uniform weight aggregation, which updates gradients orthogonally to prior parameter spaces to prevent catastrophic forgetting in domain transfer, thereby retaining old knowledge and enhancing privacy. Although this approach solves the problem of generalization of data from different source domains (known domains), the generalization performance for unknown domains and domain drift problems still needs to be discussed thoroughly.\nReorganization relationships of clients. Mawuli et al. [77] proposed a semi-supervised federated learning approach on evolving data streams (SFLEDS) that addresses domain drift and privacy protection. Their proposed method utilized a distributed prototype-based technique that uses k-means clustering to group data stream instances into micro-clusters. Then, an error-driven technique is employed to capture inter-and-intra domain drift. Specifically, it efficiently performs"}, {"title": "3.5 Summary and analysis of FDCL approaches", "content": "This section summarizes and analyzes the four types of FDCL approaches mentioned above, addressing domain generalization and domain drift adaptation from a unique perspective. In summary:\n\u2022 Domain Data Supplementation. This approach emphasizes strengthening the data-level complements to mitigate data scarcity and diversity issues. However, the use of synthetic datasets may pose a risk of privacy leakage, and there is the possibility of using synthetic data to infer the distribution of the original data.\n\u2022 Domain Knowledge Learning. This approach focuses on leveraging acquired knowledge for better application across different clients and drift issues. Despite the benefits, the complexity of distillation methods may lead to additional computational and communication costs, which may be detrimental to the implementation of some resource-constrained edge devices.\n\u2022 Domain Model Enhancement. This approach aims at improving the model's resistance to forgetting. Specific model enhancement methods may be effective in small datasets or simple tasks, while their generalizability and adaptability to other complex tasks are inconclusive.\n\u2022 Domain Weight Aggregation. This approach prioritizes the optimization of weight relationships between clients. It proves beneficial for known domains, however, its performance in generalizing to unknown domains deserves further investigation.\nEach approach contributes valuable insights for overcoming challenges in FDCL. Nonetheless, they are also accompanied by specific limitations that require further attention."}, {"title": "4 FEDERATED TASK CONTINUAL LEARNING", "content": "In this section, we will investigate the emerging challenges and state-of-the-art research of the third FCL scenario, namely Federated Task Continual Learning (FTCL).\nIn this scenario, the local clients learn a set of distinct tasks over time for which the task identity is explicitly provided, i.e., the learning algorithm is clear about which task will be executed. Then global aggregation is performed in multiple rounds to generate a global model enabling the distribution and update of the knowledge for different tasks across clients.\nNt\nSimilar to the previous sections, we first provide a clear formalisation of the FTCL problem. Considering a global server S and C distributed clients in a federated framework, each client $C_i \\in \\{C_1, ..., C_c\\}$ learns a local model on its private task dataset $TD_{C_i}$, with task sequence $\\{1, ..., t, ..., T\\}$, where $TD_{C_i} = \\{(x,y)\\}_{i=1}^{N_t}$ is a labeled dataset for task t with $N^t$ instances of x and its label y. In FTCL, there is no relationship among the datasets $TD_{C_i}$ across all clients. In each federated training round $r \\in \\{1, ..., R\\}$, each client $c_i$ updates its model parameters $\\theta_i^r$ by using task dataset $TD_{C_i}$ in a task continual learning setting and accelerate the current task learning with learned knowledge from the past tasks. Then, each client $c_i$ transmits updated model parameters $\\theta_i^r$ after training to the server S, and the server S aggregates them into the global parameter $\\theta$. To integrate the task knowledge across all clients. Finally, the server S distributes the global parameter $\\theta$ to all"}, {"title": "4.1 Regularization-based methods", "content": "In this category, the solution is characterized by adding explicit regularization terms to balance the old and new tasks. Bakman et al. [79] aimed at addressing the global catastrophic forgetting problem in FTCL under realistic assumptions that do not require access to past data samples. The authors compared and analyzed the conventional regularization-based approaches and proposed a federated orthogonal training (FOT) framework. FOT uses their proposed FedProject average method in the aggregation to make the global updates of new tasks orthogonal to previous tasks\u2019 activation principal subspace to decrease the performance disruption on old tasks. Their evaluation was compared with state-of-the-art methods, and the results indicated that FOT alleviates global forgetting while maintaining high accuracy performance with negligible extra communication and computation costs."}, {"title": "4.2 Architecture-based methods", "content": "To better solve the inter-task interference problem in FTCL, constructing specific modules or adding different parameters in the architecture is an effective and flexible solution that can explicitly help. Wang et al. [80] focused on the scenario where both data privacy and high-performance image reconstruction are required in multi-institutional collaborations. The authors proposed a peer-to-peer federated continual learning network called icP2P-FL to alleviate catastrophic forgetting with reduced communication costs. icP2P-FL uses the cyclic task-incremental continual learning mechanism across multiple institutions as the FTCL setting. The authors also designed an intermediate controller that includes two modules, the performance assessment module (PAM)"}, {"title": "4.3 Replay-based methods", "content": "In FTCL, replay-based methods include saving samples in memory and approximating and recovering old data distributions which are then used to rehearse knowledge in training current tasks. Zizzo et al. [82", "83": "found most existing FCL work neglected the maintenance or consolidation of old knowledge, resulting in performance degradation on previous tasks. Therefore, the authors defined this problem under the FTCL setting and designed a federated probability memory replay (FedPMR) framework"}]}