{"title": "Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers", "authors": ["Zhichao Geng", "Dongyu Ru", "Yang Yang"], "abstract": "Learned sparse retrieval, which can efficiently perform retrieval through mature inverted-index engines, has garnered growing attention in recent years. Particularly, the inference-free sparse retrievers are attractive as they eliminate online model inference in the retrieval phase thereby avoids huge computational cost, offering reasonable throughput and latency. However, even the state-of-the-art (SOTA) inference-free sparse models lag far behind in terms of search relevance when compared to both sparse and dense siamese models. Towards competitive search relevance for inference-free sparse retrievers, we argue that they deserve dedicated training methods other than using same ones with siamese encoders. In this paper, we propose two different approaches for performance improvement. First, we introduce the IDF-aware FLOPS loss, which introduces Inverted Document Frequency (IDF) to the sparsification of representations. We find that it mitigates the negative impact of the FLOPS regularization on search relevance, allowing the model to achieve a better balance between accuracy and efficiency. Moreover, we propose a heterogeneous ensemble knowledge distillation framework that combines siamese dense and sparse retrievers to generate supervisory signals during the pre-training phase. The ensemble framework of dense and sparse retriever capitalizes on their strengths respectively, providing a strong upper bound for knowledge distillation. To concur the diverse feedback from heterogeneous supervisors, we normalize and then aggregate the outputs of the teacher models to eliminate score scale differences. On the BEIR benchmark, our model outperforms existing SOTA inference-free sparse model by 3.3 NDCG@10 score. It exhibits search relevance comparable to siamese sparse retrievers and client-side latency only 1.1x that of BM25.", "sections": [{"title": "1 Introduction", "content": "Information retrieval(IR) and question answering (QA) are fundamental tasks in the realm of information processing, widely employed in various web applications. Lexical-based algorithms such as TF-IDF and BM25 were once the dominant approach. These algorithms utilize inverted indexes, which was proven to be efficient. However, due to issues such as vocabulary mismatch [43] and the lack of contextual information, their semantic retrieval capabilities are limited. In contrast, siamese dense retrievers have overcome the limitations of traditional lexical-based methods and have become the mainstream approach for semantic retrieval [34]. Nonetheless, the ANN algorithm requires a substantial amount of memory leading to a significant trade-off between search relevance and resource consumption [18, 28]. The interpretability of dense retrievers is also questioned. Recent years, learned sparse retrieval is proposed to address this obstacle and gained increasing attention [6, 10, 12]. This approach predicts token weights based on their semantics with context information. It expands token set with generative models [29, 30] or masked language model heads [1, 10-12, 22, 23, 27, 44], thereby addressing the vocabulary mismatch problem. Since sparse embeddings can be integrated with inverted indexes, the retrieval process of learned sparse models is highly efficient without compromising recall. Moreover, learned sparse retrieval offers better interpretability because the contribution of each token can be intuitively understood by human.\nAmong the learned sparse models, the inference-free architecture is particularly attractive to search applications. This architecture degenerates online model inference for query encoding into simple tokenization, significantly reducing end-to-end search latency and the associated model deployment costs.\nEarly work like DEEPCT [6] and Doc2Query [29] attempted to associate additional information with the original documents. However, search relevance could not be trained in an end-to-end manner. Proposed by Formal et al. [10], the SPLADE-doc architecture has achieved SOTA performance among inference-free retrievers. It predicts the token weights and expands the tokens with similar semantics. The search relevance and sparsity are tuned via end-to-end training. However, even the latest SOTA inference-free model, SPLADE-v3-Doc [23], exhibits a significant gap in search relevance when compared with siamese sparse retrievers. On the BEIR benchmark, the average NDCG@10 score of SPLADE-v3-Doc is 4.7 lower"}, {"title": "2 Related Work", "content": "Recent years, the use of language models to generate dense embeddings for text representations has become prevalent in QA and IR [5, 19, 32, 34]. Continuous efforts have been made to improve the training methodologies for dense retrieval models, such as negative sampling [32, 41] and knowledge distillation [16, 17, 25]. To enhance the generalization capability of dense retrieval models, numerous studies explore pre-training techniques on text embeddings. Some works [13, 14, 39] design auxiliary tasks to enrich the dense embeddings from models, while another tributary of previous work pre-train the model directly on constructed text pairs [3, 24, 37], including unsupervised and weakly supervised data.\nKnowledge distillation [15] utilizes the soft labels from teacher models to facilitate a more effective training process for student models so as to improve the accuracy. Researchers strive to select teacher models with inherent advantages, such as larger parameters size or superior architectures, to ensure the best possible knowledge transfer. Hofst\u00e4tter et al. [16] proposes using a cross-encoder reranker as the teacher model for the siamese dense retriever during fine-tuning. However, in the context of pre-training, the significantly larger data volume makes the use of cross-encoders prohibitively expensive.\nPre-training is a widely adopted technique to enhance the accuracy and generalization capabilities of dense retrievers. The contrastive InfoNCE loss is applied to massive amounts of unsupervised or weakly-supervised data. Previous work [38] illustrates that the contrastive InfoNCE loss improves the dense representation from the perspectives of alignment and uniformity. However, for the inference-free architecture, the asymmetric sparse representation is unaware of the query distribution, and the token weight is simply an importance measure for that token. Consistent with our experiments, pre-training with the InfoNCE loss does not improve the model as expected on dense retrievers.\nThe challenges associated with knowledge distillation and pre-training methodologies limit the application of these techniques to inference-free sparse retrievers."}, {"title": "2.2 Sparse Retrieval", "content": "Learned sparse retrievers have gained increasing attention due to their ability to perform semantic search while retaining the advantages of traditional lexical-based retrieval methods. DEEPCT [6] employs the BERT model to fit the token weights computed by heuristic rules. It modifies the term frequency, thereby influencing the match score in the BM25 algorithm. However, DEEPCT does not address the vocabulary mismatch issue inherent in lexical-based retrieval. Doc2query [30] and docTTTTTquery [29] tackle this issue by using generative models to predict potential queries for the"}, {"title": "3 Preliminary", "content": "Our work is built upon the SPLADE-doc-distill model. In this section, we'll introduce the details of the baseline method."}, {"title": "3.1 Model", "content": "SPLADE-doc-distill is an expansion model that predicts the estimated importance of tokens in the vocabulary space. The model reuses the masked language model head to generate a similarity distribution over the token vocabulary. Utilizing BERT as its backbone, the model operates with a sparse vector of 30,522 dimensions. It utilizes max pooling to aggregate the predictions across different positions and uses ReLU activation to sparsify the representation and ensure the weights are non-negative. SPLADE-doc-distill has an asymmetric architecture, which means it only performs model inference on documents. For an input document, let $w_{ij}$ represent the weight of token $j$ at position $i$. The formula to get the sparse representation is as follows:\n$w_j = \\max_i \\log(1 + \\text{ReLU}(w_{i,j}))$\n(1)"}, {"title": "3.2 Training", "content": "The optimization objective of SPLADE combines the ranking loss $L_{rank}$ and sparse regularization loss $L_{FLOPS}$, ensuring an end-to-end training approach that simultaneously optimizes the search relevance and representation sparsification. For SPLADE-doc-distill, the training loss $L$ is formulated as follows:\n$L = L_{rank} + \\lambda_a L_{FLOPS}$\n(3)\n$L_{FLOPS} = \\sum_{j \\in V} \\bar{a}_j^2$, where $a = \\frac{1}{N} \\sum_{i=1}^N \\text{ReLU}(W_j^{d_i})$\n(4)\nwhere $\\lambda_a$ is the weight for FLOPS regularizer and $\\bar{a}_j$ is the average weight of token j in the batch. The FLOPS regularizer penalizes the dimensions with high average weights in the representation to achieve the sparsity. In the context of text retrieval, tokens occurs more frequently and tokens with larger weights get more penalty. For given triplets $(q, d^+, d^-)$ consisting of a query, a positive document, and a hard negative document, SPLADE-doc-distill utilizes the KL divergence function to fit the match score differences produced by the model on the given triplets to the predicted results of the cross-encoder teacher model, thereby performing knowledge distillation."}, {"title": "4 Method", "content": ""}, {"title": "4.1 IDF-aware FLOPS", "content": "For the vanilla SPLADE-doc-distill model, according to Equation 4, we note that the FLOPS loss imposes a uniform penalty on different tokens. This penalty increases in a quadratic way with the average weights of token j, while the growth rate of the match score on token j is linear. Although the initial intention of the FLOPS regularizer was to sparsify the representation, one unintended consequence is that it prevents the model from generating excessively large weight for a single token. For semantically important tokens that are always retained once they occurs, FLOPS loss put an extra restriction on their weights.\nTherefore, we propose the IDF-aware FLOPS, which is a simple yet highly effective approach. We multiply the model output with token IDF values as the token weight. When computing the FLOPS loss, we exclude the IDF values and only apply constraints on the model output. The IDF values are involved in the calculation of the match score during both training and inference phases.\nFormally, we modify the token weight and FLOPS loss formulas for Equation 1 and Equation 4 to the following:\n$w_j = \\text{IDF}_j \\max_i \\log(1 + \\text{ReLU}(w_{i,j}))$\n(5)"}, {"title": "4.2 Ensemble Heterogeneous Knowledge\nDistillation", "content": "Pre-training is a widely adopted technique to improve the performance and generalization of retrieval models. To further boost the search relevance, we pre-train the model on an extensive corpus encompassing both unsupervised and weakly-supervised datasets. Subsequently, we fine-tune the model on a high-quality labeled dataset, specifically the MS MARCO dataset. To construct an effective optimization objective, we employ knowledge distillation techniques. The primary challenge lies in generating efficient supervisory signals for the large-scale pre-training data. For an input"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Settings", "content": ""}, {"title": "5.1.1 Training Data.", "content": "For the pre-training phase, we utilized a subset of the datasets collected by the Sentence Transformers project. Detailed datasets are listed in Table 2. Following the last paragraph in Section 4.2, samples where the positive document is not ranked among the top 10 results are filtered out. In each training step, we take the positive document and 7 hard negative documents for every query. For the S2ORC and WikiAnswers datasets, we only sample a portion of them to prevent the large sample size from dominating the pre-training dataset. Ultimately, there are 5359292 queries and their corresponding hard negative documents. For the fine-tuning phase, we utilize the MS MARCO passage ranking dataset. This dataset comprises 8,841,823 passages and 502,939 queries in the training set. For each query, we sample the top 100 hard negative documents to facilitate knowledge distillation."}, {"title": "5.1.2 Model Training.", "content": "Co-Condenser [14] is hired as the backbone, which is of the same size with BERT-base model. The IDF values for tokens are calculated using the documents of MS MARCO dataset. If a token is not present in the dataset, its value is set to 1. The query IDF representation remains frozen throughout the training and evaluation processes."}, {"title": "5.1.3 Indexing and Evaluation.", "content": "In this paper, a lexical search engine called OpenSearch, is employed to construct the inverted index and perform the retrieval process as well. By leveraging the OpenSearch neural sparse feature, we can seamlessly integrate the writing and searching processes for custom learned sparse models. For evaluation metrics, we use the implementation of BEIR python toolkit to caculate the MRR, NDCG and recall rate. During evaluation, we use the IDF values derived from the MS MARCO dataset. The max input length is set to 512."}, {"title": "5.2 Search Relevance Evaluation", "content": ""}, {"title": "5.2.1 In-domain Performance.", "content": "As we fine-tune the model on MS MARCO dataset, we report the in-domain performance on this dataset. Following Formal et al. [12], we report the MRR@10 and Recall@1000 on MS MARCO dev set. We also report the NDCG@10 and Recall@1000 for the TREC DL 2019 evaluation set. We train the SPLADE-doc model using knowledge distillation techniques described by Formal et al. [10] as baseline, and reference it as SPLADE-doc-distill in other sections. The results for baseline models are extracted from corresponding papers. The baseline models contain siamese dense encoders including ANCE [40], TCT-ColBERT [25], ColBERTv2 [36], RocketQA [32], RocketQAv2 [35], CoCondenser [14], TAS-B [17]. And we also include sparse retrievers including BM25, SparTerm [1], DEEPCT [6], doc2query-T5 [29] and SPLADE-series models [10-12, 23]. SPLADE-v3-Doc applies knowledge distillation on SPLADE-doc. It also employs tricks to improve the model's in-domain search relevance, and these tricks are orthogonal to what we proposed in this paper.\nThe experiment results indicate that our model achieves SOTA in-domain performance among all inference-free retrievers. Additionally, the proposed model shrinks the performance gap between the inference-free retrievers and siamese sparse retrievers. Since there are trade-offs between search relevance and retrieval"}, {"title": "5.2.2 Out-of-Domain Performance on BEIR.", "content": "The purpose of out-of-domain(OOD) benchmarking is to test the model's generalization capacity in a zero-shot fashion, which represents a quantity of real production scenarios, especially for ones with limited resource for building a relevance tuning pipeline. Following the work of Formal et al. [11], Lassance et al. [23], we evaluate our model on a readily available subset of 13 datasets from the BEIR benchmark, excluding CQADupstack, BioASQ, Signal-1M, TREC-NEWS, and Robust04.\nOn the BEIR benchmark, our model's zero-shot search relevance substantially outperforms other inference-free sparse retrievers, surpassing SPLADE-v3-Doc by a significant margin of 3.3 NDCG@10 score. Our model's search relevance is comparable to SOTA siamese sparse retrievers and even outperforms strong siamese retrievers such as SPLADE-v3-DistilBERT and ColBERTv2. This result demonstrates that our model exhibits superior generalization capabilities. Moreover, we discovered that our model maintains stronger robustness in OOD settings compared to in-domain settings."}, {"title": "5.3 Retrieval Efficiency", "content": ""}, {"title": "5.3.1 Theoretical FLOPS number.", "content": "FLOPS is the regularizer which controls the end-2-end efficiency of sparse retrieval. By adjusting"}, {"title": "5.3.2 End-to-end search performance.", "content": "To evaluate the efficiency of our model in real production settings, the benchmark is conducted on a distributed OpenSearch cluster. The end-to-end search performance is measured, where end-to-end refers to the process of sending a raw text search request and marking it complete upon receiving the search response. All workloads are included, such as tokenizer inference, network traffic etc. We compare our method with BM25 in terms of the 99th percentile (P99) search latency and average search throughput under different concurrency levels by adjusting the client number. By default, BM25 in OpenSearch employs heuristic optimizations, such as block-max WAND [7], while learned sparse retrievers do not have these optimizations. We employ a simple heuristic optimization rule: first, we search for a preliminary result set using tokens with high IDF values, then we rerank the result set using all tokens. The preliminary for this optimization rule is the involvement of IDF values. This optimization can boost search performance with negligible impact on search relevance. The results are listed in Table 5. Our method achieves an efficiency very close to that of BM25. The heuristic optimization rule boost the search latency and throughput for our method about 10 percentage. When both employ heuristic optimizations, the latency of our method is approximately 1.1x that of BM25."}, {"title": "5.4 IDF-Aware FLOPS", "content": ""}, {"title": "5.4.1 Impact on search relevance.", "content": "To assess the impact of IDF-aware FLOPS on the zero-shot search relevance, we conduct an ablation study employing different IDF settings on BEIR benchmark. In the default setting, we use fixed IDF values derived from MS MARCO dataset. We also conduct experiments using IDF values from the corresponding BEIR datasets. With different settings, we examined the impact of (1) employing IDF-aware FLOPS in training phase (2) retrieval with IDF values derived from different sources. We conduct these experiments on our model and SPLADE-doc-distill.\nThe experiment results are shown in Table 6. From the experiment results, we obtain several conclusions: (1) The IDF-aware FLOPS boost the model search relevance at large margin. Training and inference with IDF, both our model and SPLADE-doc-distill achieve much better search relevance. (2) For models without pre-training phase, using IDF derived from the test set has better search relevance compared with fixed IDF derived from training data. However, if the model has undergone extensive pre-training on large-scale data using the fixed IDF, the conclusion is the opposite."}, {"title": "5.4.2 Impact on retrieval efficiency and index size.", "content": "Since the IDF-aware FLOPS applies larger penalty on low IDF tokens, experiments are conducted to measure its impact on retrieval efficiency. We conducted experiments to quantify the relationship between the expansion rate(average token number per expanded documents), retrieval efficiency (FLOPS number), and search relevance for our model. The comparison is made between our model with IDF-aware FLOPS and our model trained without IDF-aware FLOPS. The experimental results are depicted in Figure 3. The experimental findings demonstrate that the IDF-aware FLOPS substantially augments the retrieval efficiency. For models with similar expansion rate, the FLOPS number of those trained with IDF-aware FLOPS is much smaller."}, {"title": "5.5 Heterogeneous Knowledge Distillation", "content": "We conducted an ablation study on the components of our proposed heterogeneous knowledge distillation to demonstrate their effectiveness. The detailed results are shown in Table 7, yielding several notable findings: Knowledge distillation is a more effective optimization objective than the naive InfoNCE loss. Utilizing supervision signals from one or more teacher models brings at least a 0.86 pts improvement (TAS-B) compared to model without pre-training, while the InfoNCE loss only improves 0.31 pts."}, {"title": "6 Conclusion", "content": "In this paper, we proposed two novel approaches to significantly improve the search relevance of inference-free learned sparse retrievers while maintaining high efficiency. We introduced IDF-aware FLOPS to mitigate underestimation of important tokens and increase sparsity, boosting both relevance and efficiency. We also developed a heterogeneous ensemble knowledge distillation framework leveraging strong dense and sparse retrievers for pre-training, enhancing generalization. Extensive experiments validate our methods' effectiveness. Our model achieves SOTA performance among inference-free sparse retrievers on BEIR, while maintaining end-to-end latency only 1.1x that of BM25."}]}