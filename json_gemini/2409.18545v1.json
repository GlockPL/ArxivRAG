{"title": "An Epistemic Human-Aware Task Planner which Anticipates Human Beliefs and Decisions*", "authors": ["Shashank Shekhar", "Anthony Favier", "Rachid Alami"], "abstract": "We present a substantial extension of our Human-Aware Task Planning framework, tailored for scenarios with intermittent shared execution experiences and significant belief divergence between humans and robots, particularly due to the uncontrollable nature of humans. Our objective is to build a robot policy that accounts for uncontrollable human behaviors, thus enabling the anticipation of possible advancements achieved by the robot when the execution is not shared, e.g., when humans are briefly absent from the shared environment to complete a sub-task. But, this anticipation is considered from the perspective of humans who have access to an estimated robot's model. To this end, we propose a novel planning framework and build a solver based on AND/OR search, which integrates knowledge reasoning, including situation assessment by perspective taking. Our approach dynamically models and manages the expansion and contraction of potential advances while precisely keeping track of when (and when not) agents share the task execution experience. The planner systematically assesses the situation and ignores worlds that it has reason to think are impossible for humans. Overall, our new solver can estimate the distinct beliefs of the human and the robot along potential courses of action, enabling the synthesis of plans where the robot selects the right moment for communication, i.e. informing, or replying to an inquiry, or defers ontic actions until the execution experiences can be shared. Preliminary experiments in two domains one novel and one adapted demonstrate the framework's effectiveness.", "sections": [{"title": "1 Introduction", "content": "Studies in psychology and cognitive science within the domain of joint actions suggest that humans consider each other's actions and beliefs, indicating that they model each other's tasks when planning [24,17,25]. Therefore, it is important if not key for success to be able to estimate or anticipate situations of divergence in beliefs and how that can be detrimental to collaborative activities.\nIn joint action scenarios, where partners work toward a shared goal, individuals often form expectations of their partner's actions based on their own mental"}, {"title": "2 The Cube Organization Case Study", "content": "Figure 2 illustrates the task of organizing cubes into boxes. The shared HR task requires that cubes from different tables be placed into separate boxes.\nSay only H is capable of moving around and exhibits unpredictable behavior (nondeterminism), such as moving to the other table (ot) to retrieve cubes, while R may continue to act. From the H's perspective, R may move some or all of the cubes from the main table (mt) and place them into one of the boxes, or it may choose to take no action at all. Upon returning to the main table mt, H may discover that some, none, or all of the cubes originally on mt are missing, indicating that they have been placed in one of the boxes.\nIf R places some cubes from mt into one of the boxes, H will only learn about this decision upon encountering transparent boxes. But when opaque, R has several options: it can communicate, wait for H to inquire, or select a remaining cube of mt to place in the correct box when H and R are co-present.\nPlanning is done from the robot's perspective, taking into account R's and H's task models. The human collaborator has an approximation of the robot's model, which enables them to anticipate the robot's action. We later provide more details on these models and about their accuracy and falsity."}, {"title": "3 Background", "content": "Dynamic Epistemic Logic (DEL). We focus on epistemic languages (LK), a state ($s$ \u2014 comprising a set of worlds $w_i$), an action ($a$ \u2014 comprising a set of events $e_i$), and state transitions (via the cross product $\\otimes$ operator) as derived from the literature [2,3], with necessary simple adjustments for our needs. For other basic concepts like indistinguishability and equivalence relation, perspective shift, and truth of epistemic formulas, readers are referred to the cited literature.\nHere, we focus on the essential DEL concepts necessary to build the framework, using examples from use case study. Recall the requirements for the task."}, {"title": "4 The EHATP Planning Framework", "content": "We consider that the human maintains an estimated model for the robot $M^H_R$, which can be incorrect compared to $M_R$.\nThe epistemic \u041d\u0410\u0422\u0420 (\u0415\u041d\u0410\u0422P) framework considers three models: $M_R$, $M_H$ and $M^{RH}$. While $M_R$ guides the planning of R's actions and $M_H$ helps estimate and emulate H's decisions and actions. But, using $M^{RH}$, H \"expects\" and \u201cpredicts\u201d certain robot behavior (from their own perspective) both, respectively, when they are co-present and when they are not. Note that, each model has their own dedicated components like Bel(.) and tn as defined earlier.\nThe majority of the models' components remain static, but for each model, its task network (tn) and belief ($Bel(\\phi)$) components are dynamic, where $\\phi$ denotes an agent (or agent perspective). Except for belief, we assume that components like the robot's action model and task network are accurately estimated by H. This allows us to focus on the key aspects relevant to this paper. For other incorrectly estimated components of $M_R$, we suspect a possible generalization utilizing concepts developed in [28] and intend to explore this in the future."}, {"title": "Planning Workflow", "content": "We focus on only the dynamic parts. The initial epistemic state $s_0$ (with the only world to begin with and that is also the designated world $w_d$) is provided as an input. In general, each world $w_i$ in an epistemic state $s_i$ represents $((Bel(R), tn_R), (Bel(H), tn_H), (Bel(RH), tn_{RH}))$. It also includes the only designated world $w_d$ always known to R. Note that these worlds are indistinguishable for H, but human knows that the robot can always distinguish them and that the robot can identify $w_d$. Also, the human knows that, if $w_j$ is the designated world, then $Bel^{RH}_{wj}$, is the reality as they do not have access to the facts appearing in $Bel^{R}_{wj}$. Here, we consider that $Bel(H)$ is equal to $Bel(RH)$, but they can be different from $Bel(R)$ and can contain false (human) beliefs.\nThe robot, an epistemic state $s_i$ and possible worlds $w_j$ in it are considered. We compute the set of all possible primitive actions, computed by all feasible decompositions, based on $(Bel(R), tn_R)_{ij}$, and whether it is different than the set of primitive actions based on the allowed decompositions w.r.t. $(Bel(RH), tn_{RH})_{ij}$. The idea is to align these decompositions, w.r.t. each $w_j$, in a way that the human can correctly estimate the progress the robot may achieve, thus utilizing the human's capacity for anticipating. If there is a difference, we identify the relevant facts in $Bel^{R}_{wj}$ that need to be corrected in $Bel^{RH}_{wj}$, to align the decompositions. To achieve that, we adapt our earlier approach presented in [14]. That is, one can plan minimal communication, possible to schedule ahead of time during offline planning when communication is allowed. Eventually, communication will also fix $Bel^{H}_{wj}$, accordingly. However, $Bel^{H}_{wj}$ and $Bel^{RH}_{wj}$ can still have non-relevant false beliefs compared to the ground truth $(Bel^{R}_{wj})$.\nNext, the planner computes the R's next real action based on its task network $tn^R_{wd}$ in the designated world $w_d$ of $s_i$, we call it the designated event. It also computes other non-designated events based on respective decompositions in each world $w_j$ of $s_i$. (An event and a possible real action including noops are used interchangeably.) In other words, the planner computes a set of all possible decompositions based on what H can anticipate, that means by taking into account each $(Bel(RH), tn_{RH})_{ij}$. These are all the anticipated events that can"}, {"title": "Executing an Epistemic Action in a State", "content": "Based on the cross-product operation ($\\otimes$), the state transition is computed as $s_{i+1} = s_i \\otimes a_i$. In our planning algorithm (Algorithm 1, Line 8), we model the scenario as follows: if H&R are co-present, then H can distinguish between the actual event (the real action performed by R) and other estimated events. Otherwise, H perceives each event as a possible action by R. When co-present, H assesses the execution of R's real action, thus narrowing down the possibilities over $w'_j$'s in $s_{i+1}$ captured by (ref Fig. 3).\nWithin each world of the new epistemic state, belief components, i.e., $Bel(R)$, $Bel(H)$, and $Bel(RH)$ are updated corresponding to the possible robot action (either real or anticipated) that is a part of epistemic action $a_i$. Also, the task networks concerning $M_R$ and $M^{RH}$ are updated in each world, accordingly."}, {"title": "When The Human Acts", "content": "H acts only if their next real action, w.r.t. a possible decomposition, is applicable in all possible worlds. I.e., for each $w_j$ in $s_{i+1}$, applicability of the action is examined in every $(Bel(H), tnh)_{i+1,j}$. Two key issues at this stage are: First, humans can act based on a false belief (if consistent throughout all the worlds), or a true belief w.r.t. the ground truth in every $w_j$. We handle false belief scenarios the way it is addressed in the literature, that is, by finding out relevant belief divergence and handling it via communication [14]. Second, we also know that a boolean variable, $p$, that H is uncertain about at this stage, which holds only in some worlds, is due to disrupted shared execution experiences. If $p$ is a precondition of the task refinement process, then H can initiate communication, or R can inform H about $p$. And, if co-present, R can also act to implicitly share $p$'s value such that there is some correlation between that action and $p$. Here, we focus on explicit communication, while sharing $p$'s value by changing the environment is left for the future."}, {"title": "Handling H&R Communication", "content": "We introduce two types of actions and they become a part of the deliberation process. First, ask-p human inquires about p from R, and, second inform-p - R informs them of the status of $p$.\nAt this stage, we create two specialized versions of state $s_{i+1}$: one prioritizing human inquiries, ask-p, and the other prioritizing robot updates, inform-p. Communication tasks are adjusted into respective networks appropriately."}, {"title": "Situation Assessment", "content": "Assessing the status of a state property depends on a broader context, which determines whether it can be observed or only inferred by attending the action execution affecting it. Knowledge rules were used to address this aspect [27]. For example, H can view the current status of the variable inside(cr, box1) as true if they meet the requirements of the rule's antecedent formula, e.g., being at the main table, box\u2081 is transparent, and cr is inside box1."}, {"title": "5 AND/OR Search based EHATP Planner", "content": "Algorithm 1 takes the EHATP problem as input, producing an output as either a failure or an optimal worst case joint solution. It is an implementation of the classic AND/OR search using rooted graphs. When the root node is DONE, the joint solution policy is extracted (extract_joint_solution()), in Lines 17 & 18.\nWe consider the root node (root_epi_state) and the subsequent actor, either R or H, to begin the plan exploration (Line 3). Within the loop, in Line 6, we select a node/state from queue, and next call the Situation Assessment( ) subroutine. At this stage, the planner already knows whether agents were co-present and whether H assessed the designated event. It ignores the worlds distinguishable from the designated world (Definition 1). The scenario where a human transitions to the R's location and subsequently becomes co-present is particularly interesting. Another significant subroutine, Expand(), previously discussed in the EHATP framework's planning workflow, is invoked in Line 8. The children created after R expands the popped node are AND nodes. Conversely, when H expands the popped node, OR nodes are created."}, {"title": "Runtime Analysis of Reasoning in EHATP", "content": "In the worst-case scenario, roughly, the runtime is influenced by the robot's available choices (m) in the absence of the human at each step, as these choices are crucial for updating the human mental model ($M_H$) correctly. This is then multiplied by the number of choices (b) the human has to progress with the task when they are copresent.\nWe introduce a parameter K, which represents the maximum #actions R can perform when H&R are not copresent. So, the runtime complexity can be O(bxmK) from the point they separate and reunite again, in terms of epistemic state exploration s.t. the maximum number of possible worlds in a state is m. We assume $M_H$ and $M_R$ are aligned at this stage when they separate."}, {"title": "6 Related Work", "content": "Human Robot Collaboration (HRC): Generating the robot's behavior while considering the existence of humans, known as human-aware planning and decision-making [6,1,31,18,20,8,7]. Also, it can do reasoning for task allocation [23,22]. Communication is an essential key to successful HRC, which is used to align an agent's belief, clarify its decision or action, fix errors, etc. [29,20]. We extend this research line but have not found studies addressing human anticipation and divergent beliefs in disrupted execution experiences.\nModels, Planning Approaches, and Solutions: Several planning models are applied in the context of HRC planning, including HTNs [19,23,5], POMDPs [30,23,31], AND/OR graphs [9], etc. HTNs use both abstract and non-abstract tasks to form hierarchical networks, while AND/OR graphs cover causal links among subtasks and depth-first search is used in planning [15].\nEpistemic Planning: The epistemic planning framework, in [3], holds promise for capturing key elements of ToM in autonomous robots. For HRC, the framework lays the groundwork for implicit coordination through perspective shifts [10]. By adapting this framework and focusing on the robot's perspective, it may serve as a basis for addressing the core problem we have aimed at with the shared mental model [21], albeit without assuming imperfectly estimated model ($M_H$)."}, {"title": "Qualitative Analysis", "content": "In our use case domain, we explore different plan traces the planner can come up with depending on scenarios that arise. We start with two cubes, cr and cw, placed initially on tables mt and ot, respectively. Initially, there is only one designated world, $w_d$, in the initial epistemic state, $s_0$. The environment otherwise remains unchanged. H can decide to go and retrieve the white cube, while the robot begins to work on other parts of the shared task.\nTwo plan traces are shown in Figure 4. \u0397 starts to execute. H&R are co-present and the boxes are opaque. (SA is shown only at relevant places.)\nLet us focus on (a): after the human shifts focus to ot, both agents are not co-present until they reunite later in the trace, during which they act simultaneously. (In this situation, agents must be at the same table and simultaneously focus on it to be considered co-present.) In the first broad rectangular box, the human moves to ot. They anticipate that the robot may have picked cr or done nothing, but in reality, the robot picks cr, resulting in two possibilities that will be maintained within the robot. Similarly, in the following box, the human picks cw at ot and anticipates that if the robot had picked cr, it could have placed it in one of the boxes or held onto it, or cr is still on the table. Together, these create four possibilities, with the reality being that cr is inside box1. At this point, the robot currently has no feasible action to execute, and the shared task has been not achieved yet, too. Upon the human's return, as per their initial agreement on K, the robot has prepared itself with four possible worlds (with a designated"}, {"title": "7.1 Experiments", "content": "Analyzing the Impact of K and Non-Determinism. Algorithm 1 highlights a rapid growth in the size of the epistemic state in terms of the number of worlds which directly correlates with K that is the maximum #actions the robot can perform when the experience is not shared. The sequencing of actions significantly influences the range of potential worlds H expects to see.\nK is considered to assess its impact on the planner's performance. We assume that whenever the shared execution experience is disrupted, R can execute a maximum of K actions, including the option of doing nothing. For example, when the human is away to fetch the cube and has a fixed length and sequence of actions to perform. The exact number of real ontic actions R performs ranging from 0 to K, including which of those allowed ones and their potential sequences, will depend on the scenario at hand, environment dynamics (e.g., the observability factor), and the optimization criteria. The option for the robot to limit its real actions whenever required is integrated into the task description, aligning with the turn-taking nature of the underlying planner. Consequently, the planner is engineered to optimize the robot's policy tree branching on uncontrollable human choices, including a communication action, to meet our objective."}, {"title": "8 Conclusion", "content": "Our framework allows the robot to implement a ToM not only at execution time but also at planning time and hence explores what would be the beliefs of the human and the robot depending on which course of action. This is done thanks to the use of epistemic reasoning, the notion of shared experience, and observable and non-observable facts, which allow anticipation of H's situation assessment along the various non-deterministic shared plan traces of H and R.\nR can adapt its choices to H's diverging beliefs over time, e.g. by choosing to communicate to inform H or elicit an action, or a particular context to act.\nWe acknowledge that scaling such abilities can pose complexity challenges for planners, which can be evident in [3]. Hence, we take care to precisely identify the context in which our approach can be effectively used which is dealing in a refined manner with short-term interactions and intricate H&R face-to-face situations. Also, we intend to test the current system in different domains with realistic H&R co-activities. We aim to enhance planner's practical efficiency and explore incremental task planning.\nUser Study: We tested with users the HATP framework, which supports execution concurrency and demonstrated the robot's ability to adapt to non-deterministic human behaviors [11,12]. Although this study is not for testing advanced epistemic reasoning of EHATP, it offers valuable insights and tools.\nBuilding on these findings, we are evaluating the EHATP framework, which incorporates features such as second-order theory of mind and belief divergence."}]}