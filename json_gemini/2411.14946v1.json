{"title": "RELIABLE EVALUATION OF ATTRIBUTION MAPS IN CNNS: A PERTURBATION-BASED APPROACH", "authors": ["Lars Nieradzik", "Henrike Stephani", "Janis Keuper"], "abstract": "In this paper, we present an approach for evaluating attribution maps, which play a central role in interpreting the predictions of convolutional neural networks (CNNs). We show that the widely used insertion/deletion metrics are susceptible to distribution shifts that affect the reliability of the ranking. Our method proposes to replace pixel modifications with adversarial perturbations, which provides a more robust evaluation framework. By using smoothness and monotonicity measures, we illustrate the effectiveness of our approach in correcting distribution shifts. In addition, we conduct the most comprehensive quantitative and qualitative assessment of attribution maps to date. Introducing baseline attribution maps as sanity checks, we find that our metric is the only contender to pass all checks. Using Kendall's \\( \\tau \\) rank correlation coefficient, we show the increased consistency of our metric across 15 dataset-architecture combinations. Of the 16 attribution maps tested, our results clearly show SmoothGrad to be the best map currently available. This research makes an important contribution to the development of attribution maps by providing a reliable and consistent evaluation framework. To ensure reproducibility, we will provide the code along with our results.", "sections": [{"title": "Introduction", "content": "The explainability of learned models is a crucial, yet mostly unachieved property towards a general utilization and acceptance of machine learning techniques Gilpin et al. [2018]. In order to succeed, many practical applications require solutions that provide not only high test accuracy, but also robustness, uncertainty estimates for predictions, and ways to make decision processes understandable for humans. Although recent deep learning methods have been very successful in achieving great advances in terms of model accuracy on many different tasks Zar\u00e1ndy et al. [2015], the other properties are mostly still the subject of increasing research efforts.\nA current standard tool for the visualization of decision processes in neural networks are so-called Attribution Methods Zhang et al. [2021a]. In the case of Computer Vision tasks with image input data, attribution methods are used to compute Attribution Maps (AM) (also known as Saliency Maps Zhang et al. [2021a]) of individual input images on a given trained model. AMs provide a human-interpretable visualization of the image regions' weighted impact on the model, allowing intuitive explanations of the complex internal mappings of model predictions. However, the qualitative"}, {"title": "Related Work", "content": ""}, {"title": "Attribution methods", "content": "Since the literature on attribution methods has become extensive in recent years, and many approaches are based on earlier methods, we retreat to the discussion and evaluation of some of the most widely used AM techniques in the context of CNNs."}, {"title": "Evaluation of attribution methods", "content": "Given the large number of available attribution methods introduced in the previous section, users are left with the question which AM method to choose for their application (see fig. 1). However, most publications that introduce novel AM methods are providing only qualitative visualizations of their results, showing selected input samples in direct comparison with other methods. Up to date, there is no commonly accepted benchmark that would provide a quantitative metric or ranking of attribution map algorithms for a given task. In the following, we review the few"}, {"title": "A Perturbation-Based Evaluation Method", "content": "First, let us establish the need for introducing an evaluation function. As we have visually demonstrated in fig. 1, saliency maps can exhibit significant variations. To precisely quantify these differences, we evaluate the similarity between individual attribution maps (AM). To achieve this, we utilize the Pearson correlation coefficient, defined as:\n\\( r_{xy} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}} \\),\nwhere \\( x_i \\) is the \\( i^{th} \\) pixel in the AM and \\( \\bar{x} \\) is the sample mean. Similarly, \\( y_i \\) is the \\( i^{th} \\) pixel of the second AM. The output range of \\( r_{xy} \\) is [-1, 1], with 1 indicating the highest similarity and \u2264 0 a low similarity. \\( r_{xy} = 0 \\) is intuitively random noise and \\( r_{xy} = -1 \\) an \"inverted\" saliency map.\nWe assessed the similarity across a subset of 1000 images from ImageNet, employing both the ResNet-50 and ConvNeXt-tiny architectures. In fig. 2, we present the results for ResNet-50, revealing an average similarity of 48%. Remarkably, for ConvNeXt-tiny, we observed an even lower similarity of 22%.\nGiven the substantial variation among these attribution maps, it becomes imperative to define a metric for evaluating which AMs most accurately represent what CNNs are truly \"seeing\"."}, {"title": "Discussion and Outlook", "content": "This paper addresses the pressing challenge of evaluating attribution methods such as class activation maps (CAMs) within the context of convolutional neural networks, particularly focusing on the reliability and consistency of current evaluation measures. The observed lack of correlation in the ranking of attribution maps (AMs) across different datasets underscores the complexity of this task and the limitations of existing evaluation techniques.\nOur research underscores that the unreliability of many current evaluation measures arises from the significant distribution shifts introduced by common methods like masking. These shifts disrupt the stability and accuracy of evaluation outcomes, raising the need for a more robust evaluation framework. To tackle this issue, we introduce a novel approach that replaces masking with adversarial perturbations. This innovative modification maintains the original image distribution, leading to more consistent and accurate ranking of attribution methods.\nOur proposed approach not only enhances the reliability of ranking but also aligns closely with expected results, providing a more trustworthy evaluation process. The demonstrated ability of our method to yield anticipated low scores for baseline methods like the Canny edge detector further supports its credibility.\nMoreover, the inclusion of diverse neural network architectures in our evaluation adds to the generalizability of our findings. This observation underscores the robustness and versatility of our methodology, which has practical implications across various real-world applications.\nThe introduction of our perturbation-based evaluation method brings the concept of monotonicity and smoothness to the evaluation of attribution methods. The absence of significant fluctuations in the probability, as shown in our paper, further contributes to the reliability of our approach.\nIn terms of future outlook, there are several avenues for further research. Exploring the applicability of our method to even more complex and diverse datasets and architectures could provide deeper insights into its robustness and generalizability. For example, transformers and other sequence-based architectures could be considered for further work.\nIn conclusion, this paper contributes a significant step forward in addressing the challenges of evaluating attribution methods for deep learning models. By introducing a novel perturbation-based approach that addresses distribution shifts and maintains the original image distribution, we provide a more reliable and consistent framework for assessing attribution methods. This advancement holds the potential to enhance the credibility and applicability of attribution methods across diverse domains and applications in the realm of machine learning.\nLimitations. A limitation of both our method and AMs in general is the case when non-existence of objects determines the CNN decision. Consider the case when the class is assigned when some object is not being in the image. Then there is no region to attack, and there are no salient pixels. Both our evaluation function and the AM would fail. Another"}]}