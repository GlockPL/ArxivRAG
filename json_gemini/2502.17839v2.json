{"title": "Say Less, Mean More:\nLeveraging Pragmatics in Retrieval-Augmented Generation", "authors": ["Haris Riaz", "Ellen Riloff", "Mihai Surdeanu"], "abstract": "We propose a simple, unsupervised method\nthat injects pragmatic principles in retrieval-\naugmented generation (RAG) frameworks such\nas Dense Passage Retrieval (Karpukhin et al.,\n2020) to enhance the utility of retrieved con-\ntexts. Our approach first identifies which sen-\ntences in a pool of documents retrieved by RAG\nare most relevant to the question at hand, cover\nall the topics addressed in the input question\nand no more, and then highlights these sen-\ntences within their context, before they are pro-\nvided to the LLM, without truncating or alter-\ning the context in any other way. We show\nthat this simple idea brings consistent improve-\nments in experiments on three question an-\nswering tasks (ARC-Challenge, PubHealth and\nPopQA) using five different LLMs. It notably\nenhances relative accuracy by up to 19.7% on\nPubHealth and 10% on ARC-Challenge com-\npared to a conventional RAG system.", "sections": [{"title": "1 Introduction", "content": "Retrieval-augmented generation (RAG) (Lewis\net al., 2020) has emerged as a solution to the lim-\nited knowledge horizon of large language models\n(LLMs). RAG combines \u201cpre-trained parametric\nand non-parametric memory for language genera-\ntion,\u201d (Lewis et al., 2020) with the non-parametric\nmemory typically retrieved from large collections\nof documents. RAG has been shown to dramati-\ncally improve the performance of LLMs on vari-\nous question-answering and reasoning tasks (see\nsection 2). However, we argue that RAG often\noverwhelms the LLM with too much information,\nonly some of which may be relevant to the task\nat hand. This contradicts Grice's four maxims of\neffective communication (Grice, 1975), which state\nthat the information provided should be \u201cas much\nas needed, and no more\u201d and that it should be \"as\nclear, as brief\u201d as possible. The four maxims are\nenumerated as follows: (1) Maxim of Quantity: Pro-\nvide as much information as needed, but no more;\n(2) Maxim of Quality: Be truthful; avoid giving\ninformation that is false or unsupported; (3) Maxim\nof Relation: Be relevant, sharing only information\npertinent to the discussion; (4) Maxim of Manner:\nBe clear, brief, and orderly; avoid obscurity and\nambiguity. While these maxims were originally for-\nmulated in the context of human communication,\nwe argue that they are also applicable in a RAG\nsetting.\nWe propose a simple, unsupervised method that\ninjects pragmatics in any RAG framework. In\nparticular, our method: (a) identifies which sen-\ntences in a pool of documents retrieved by RAG\nare most relevant to the question at hand (maxim of\nrelation), and cover all the topics addressed in the\ninput question and no more (maxim of quantity and\nmanner); and (b) highlights these sentences within\ntheir original contexts before they are provided to\nthe LLM.\nThe contributions of our paper are:\n(1) We introduce a strategy to introduce pragmat-\nics into any RAG method such as Dense Passage\nRetrieval (Karpukhin et al., 2020). To our knowl-\nedge, we are the first to investigate the impact of\npragmatics for RAG.\n(2) We evaluate the contributions of pragmatics\nin RAG on three datasets: ARC-Challenge (Clark\net al., 2018), PubHealth (Kotonya and Toni, 2020)\nand PopQA (Mallen et al., 2022) and with five\ndifferent LLMs ranging from 1B to 7B parame-\nters: Mistral-7B-Instruct-v0.1 (Jiang et al., 2023a),\nAlpaca-7B (Taori et al., 2023), Llama2-7B-chat\n(Touvron et al., 2023), Qwen2.5-3B (Team, 2024)"}, {"title": "2 Related Work", "content": "Since it was first proposed (Lewis et al., 2020),\nRAG has become an essential arrow in the quiver\nof LLM tools. However, many of the proposed\nRAG approaches rely on supervised learning to\njointly optimize the retrieval component and the\nLLM (Lewis et al., 2020; Guu et al., 2020; Xu et al.,\n2024; Kim and Lee, 2024, inter alia) or to decide\n\"when to retrieve\" (Asai et al., 2024). Instead, our\napproach is training free: it uses a set of unsuper-\nvised heuristics that approximate Grice's maxims\n(refer to Section 1). Part of our method is similar\nto Active-RAG, which also reformulates the input\nquery (Jiang et al., 2023b). However, unlike Active-\nRAG, we use pragmatics to reformulate the input\nquery and retrieve evidence for it, instead of rely-\ning on LLM probabilities. Our work is also similar\nto (Xu et al., 2024) and (Sarthi et al., 2024), which\nalso touch on pragmatics by reducing the quantity\nof text presented to the LLM through summariza-\ntion. However, the method used in (Xu et al., 2024)\nis supervised. Furthermore, both of these methods\nexhibit considerably higher overhead compared to\nour proposed approach, which relies on simple yet\nrobust heuristics.\nOur method adopts a pre-retrieval reasoning ap-\nproach that is complementary to post-retrieval rea-\nsoning approaches such as (Trivedi et al., 2023;\nKim et al., 2023), which reason after document re-\ntrieval. Further, we do not focus on reasoning about\nwhether the retrieval was useful or not (Islam et al.,\n2024). For example, current approaches that incor-\nporate reasoning into the QA task, such as rStar (Qi\net al., 2024), use an LLM to guide MCTS, where\neach intermediate step in the tree is verified by an-\nother LLM. (Jiang et al., 2024) demonstrate that,\nrather than relying solely on the LLM's paramet-\nric knowledge, retrieved contexts can also enhance\ntree search. Another reasoning-based approach,\nSTaR (Zelikman et al., 2022), employs an LLM\nto iteratively generate and refine a training set of\nrationales. The LLM is then fine-tuned on these"}, {"title": "3 Approach: Combining Step-Back\nReasoning With Pragmatic Retrieval", "content": "Conceptually, our approach is a simple plug-and-\nplay extension that emphasizes important informa-\ntion in any standard RAG setup (as shown in Fig-\nure 1). In this paper, we apply our extension to a\ncollection of documents retrieved by a dense pas-\nsage retriever (DPR) (Izacard et al., 2021).\nWe\nadapt the unsupervised iterative sentence retriever\nproposed by Yadav et al. (2020) to identify impor-\ntant sentences in the documents retrieved by RAG\nwith DPR, as follows: (1) Given a query and as-\nsociated passages retrieved by DPR, the query is\nfirst conjoined with a more abstract step-back ver-\nsion of itself created by a step-back LLM (Zheng\net al., 2024). (2) In the first sentence retrieval it-\neration, this conjoined query is used to retrieve a\nset of relevant evidence sentences from the corre-\nsponding passages (see Eqs. 1 and 2). (3) In the\nnext iteration(s), the query is reformulated to focus\non missing information, i.e., query keywords not\ncovered by the current set of retrieved evidence sen-\ntences (see Eq. 3) and the process repeats until all\nquestion phrases are covered. As such, this strategy\nimplements Grice's maxims of relation (because\nthe evidence sentences are relevant to the question),\nquantity, and manner (because we identify as many\nsentences as needed to cover the question and no\nmore).\nBy aggregating sets of retrieved evidence sen-\ntences across iterations, this retrieval strategy al-"}, {"title": "3.1 Step-Back Query Expansion", "content": "In this work, we employ Step-Back Prompting\n(Zheng et al., 2024), a simple technique to inte-\ngrate LLM driven reasoning into the retrieval pro-\ncess. A step-back prompt elicits from the LLM\nan abstract, higher-level question derived from the\noriginal query, encouraging higher-level reasoning\nabout the problem. For example, a step-back ver-\nsion of the query: \"As bank president, Alex Sink\neliminated thousands of Florida jobs while taking\nover $8 million in salary and bonuses. True or\nFalse?\" could be: \"What were the actions taken by\nAlex Sink as bank president?\". We hypothesize that\nstep-back queries, representing a more generalized\nquery formulation, when utilized as initialization\nseeds for the iterative retrieval (refer to Figure 1),\nwill generate a more diverse yet still relevant set of\ncandidate evidence sentences. For multiple-choice\nquestions (MCQs), we generate step-back answer\nchoices for each option, combining them with the\nstep-back query to guide retrieval. This approach\nintroduces an additional dimension of parallelism\nin constructing evidence chains for MCQs. The\nstep-back prompts used to elicit multi-hop reason-\ning follow the Knowledge QA template from Zheng\net al. (2024)"}, {"title": "3.2 Parallel Iterative Evidence Retrieval", "content": "Computing an alignment score between queries\nand documents is a critical step in any retrieval\nsystem. Keeping in mind the Gricean maxim's\nof quality and relation (Section 1), which empha-\nsize relevance and factual grounding, we leverage\n$s(Q, P_{j})=\\sum_{i=1}^{Q} \\text{align}(q_{i}, P_{j})$\n$\\text{align}(q_{i}, P_{j})=\\max_{k=1}^{P_{j}} \\text{cosSim}(q_{i}, p_{k})$\n$Q_{r}(i)=t(Q) - \\bigcup_{S_{k} \\in S_{i}} t(s_{k})$"}, {"title": "4 Results", "content": "Evaluation & Datasets We evaluate our method\non the test sets of ARC-Challenge (a MCQ rea-\nsoning dataset), PubHealth (a fact verification\ndataset about public health) & PopQA (open-\ndomain question-answering). For closed-tasks"}, {"title": "5 Analysis", "content": "When Does Pragmatics Help? Our error anal-\nysis indicates that leveraging pragmatics is effec-\ntive when answering the query requires connecting\nfacts along a causal path to deduce the answer (as\nshown in the example of Good Evidence in Table\n6, appendix A).\nWe also observe that highlighted evidence often\nfunctions as implicit few-shot exemplars, facilitat-\ning analogical reasoning. For instance, given the\nquestion \"In the design process, what is an example\nof a trade-off?\", our method highlights two analo-\ngous scenarios: a career decision (\u201c$50,000 salary\nworker sacrificing income to pursue medical train-\ning with the goal of increasing their future income\nafter becoming a doctor\") and a biological princi-\nple (\"beneficial trait changes linked to detrimental\nones\"). We hypothesize that such examples stim-\nulate the model's in-context learning capabilities,\npossibly explaining the observed 10% relative im-\nprovement in OLMo-1B's performance on ARC-C.\nHowever, our method exhibits a few limitations\nin specific scenarios (refer to Table 7, appendix\nB). First, it fails to highlight relevant evidences for\nqueries which require arithmetic manipulation or\ncomparison of physical quantities, as these tasks de-\npend more on mathematical reasoning than factual\nknowledge. Second, it struggles with complex lin-\nguistic phenomena, particularly negation patterns.\nFor example, consider the question: \u201cWhich hu-\nman activities would have a positive effect on the\nnatural environment?\u201d Most retrieved passages fo-\ncus on negative environmental impacts, reflecting\ntheir prevalence in real world corpora. The task\nhere requires identifying contrary evidence from\nthe long tail of the distribution, but our unsuper-\nvised retrieval heuristics do not account for such\nsemantic inversions.\nLastly, we find that for factoid QA tasks like\nPopQA, evidence highlighting can slightly degrade\nperformance compared to DPR, likely because\nthese tasks rely more on the model's parametric"}, {"title": "6 Conclusions", "content": "We present an unsupervised method that enhances\nretrieval-augmented generation (RAG) by high-\nlighting key sentences in retrieved documents. We\nfind that this approach can improve QA perfor-\nmance across 3 different datasets and 5 different\nLLMs."}, {"title": "Limitations", "content": "This study investigates the effectiveness of prag-\nmatics in enhancing Retrieval Augmented Gener-\nation (RAG) systems. Our evaluation, however,\nis limited to a comparison against standard Dense\nPassage Retriever (DPR) and BM25 baselines. The\nproposed method has potential for integration with\nmore sophisticated RAG systems, such as those\ndeveloped by Asai et al. (2024); Xu et al. (2024);\nSarthi et al. (2024). Our assessment encompasses\nthree datasets, but a more comprehensive evalua-\ntion would involve a broader range of single-hop\nand multi-hop tasks. Moreover, there are several\nscenarios which our approach does not cover, such\nas handling linguistic phenomena like negation,\nmathematical reasoning tasks and reconciling re-\ntrieved contexts that are ambiguous. Our current\napproach is also limited by the fact that it is unsu-\npervised and query reformulation is mostly driven\nby a bag-of-words. One could trivially improve\nquery reformulation by using an LLM, or using a\nweakly supervised strategy that fine-tunes an LLM\nto retrieve pragmatic evidence (using supervision\nfrom the current retriever) via a joint loss that learns\nto retrieve evidence sentences while simultaneously\nanswering the query correctly (motivated by the rel-\nevance estimator and answer marginalization losses\nproposed by Kim and Lee (2024)). We leave the\nexploration of supervised pragmatic RAG methods\nas future work.\nWhile we hypothesize that our retrieved & high-\nlighted justifications constitute \u201cshallow chains of\nthought\" which are faithfully utilized by the Large\nLanguage Model in its generations, this assertion\nremains to be formally validated through rigorous\nanalysis."}, {"title": "A Human Evaluation of Evidence Quality", "content": "For each query, we categorize its corresponding\nset of highlighted evidence(s) as \u201cbad\u201d (score: 0)\nwhen it includes completely irrelevant sentences\nor sentences within contexts that are somewhat\nrelated to the query but fail to provide any mean-\ningful support in addressing it. In the case of fact-\nchecking datasets like PubHealth, we also classify\nhighlighted evidence as \u201cbad\u201d if it appears to sup-\nport a claim but overlooks negations in the sur-\nrounding context that would ultimately refute the\nclaim.\nHighlighted evidence is categorized as \u201cmedium\"\n(score: 0.5) when it consists of sentences situated in\nrelevant contexts that may allow the correct answer\nto be inferred indirectly in some instances but lack\nthe direct or explicit support needed to answer the\nquery.\nHighlighted evidence is categorized as \"good\"\n(score: 1) when it includes a sufficient number\nof sentences that directly address the query while\nensuring no confounding factors (e.g., negations in\nthe surrounding context) are overlooked."}, {"title": "B Low Quality Evidence", "content": "In Table 7, we include some examples of retrieved"}]}