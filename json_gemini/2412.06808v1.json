{"title": "Effect of Adaptive Communication Support on Human-AI Collaboration", "authors": ["Shipeng Lius", "FNU Shrutika*", "Boshen Zhang*", "Zhehui Huang*", "Feifei Qian\u00a7\u2021"], "abstract": "Effective human-AI collaboration requires agents to adopt\ntheir roles and levels of support based on human needs, task\nrequirements, and complexity. Traditional human-AI team-\ning often relies on a pre-determined robot communication\nscheme, restricting teamwork adaptability in complex tasks.\nLeveraging strong communication capabilities of Large Lan-\nguage Models (LLMs), we propose a Human-Robot Teaming\nFramework with Multi-Modal Language feedback (HRT-\nML), a framework designed to enhance human-robot inter-\naction by adjusting the frequency and content of language-\nbased feedback. HRT-ML framework includes two core mod-\nules: a Coordinator for high-level, low-frequency strategic\nguidance, and a Manager for task-specific, high-frequency\ninstructions, enabling passive and active interactions with hu-\nman teammates. To assess the impact of language feedback in\ncollaborative scenarios, we conducted experiments in an en-\nhanced Overcooked-AI game environment with varying lev-\nels of task complexity (easy, medium, hard) and feedback\nfrequency (inactive, passive, active, superactive). Our results\nshow that as task complexity increases relative to human ca-\npabilities, human teammates exhibited stronger preference to-\nwards robotic agents that can offer frequent, proactive sup-\nport. However, when task complexities exceed the LLM's ca-\npacity, noisy and inaccurate feedback from superactive agents\ncan instead hinder team performance, as it requires human\nteammates to increase their effort to interpret and respond\nto the large amount of communications, with limited perfor-\nmance return. Our results offers a general principle for robotic\nagents to dynamically adjust their levels and frequencies of\ncommunications to work seamlessly with human and achieve\nimproved teaming performance.", "sections": [{"title": "Introduction", "content": "Human-robot collaboration has been extensively studied and\napplied across diverse scenarios, demonstrating strong po-\ntential towards enhanced efficiency and performance (Jahan-\nmahin et al. 2022; Chuah and Yu 2021; Park et al. 2020;\nGordon et al. 2020; Xiao et al. 2020; Liu et al. 2023b;\n2024). As task complexity increases, agent adaptability be-\ncomes increasingly essential for seamless teamwork. Pre-\nvious work has developed methods and tools for robot to\nadapt their actions based on inferred human objectives (Liu\net al. 2024), trust level (Chen et al. 2020), and individual\npreferences (B\u0131y\u0131k et al. 2022). Recent work also begin to\nincorporate language-based feedback (\u00d6zdemir et al. 2022;\nSharma et al. 2022) to enable more direct communications\nand lower user barriers. However, robot communications in\nthese approaches primarily focused on relative simple com-\nmands such as \"pick up the book\u201d or \u201cmove left a little\",\nwhich do not reflect the level of human-robot communica-\ntions required in real-world applications.\nRecent advancements in large language models (LLMs)\nhave brought powerful reasoning (Zhang et al. 2023; 2024;\nAgashe, Fan, and Wang 2023; Guan et al. 2023), natu-\nral language understanding (Liu et al. 2023a; Wu et al.\n2023), contextual awareness (Deng et al. 2023), and gen-\neralization capabilities (Ge et al. 2024), enabling more ad-\nvanced, prolonged communications (Bubeck et al. 2023;\nOuyang et al. 2022; Hong et al. 2023). These methods\nhas empowered agents to process ambiguous and com-\nplex instructions from human (Liu et al. 2023a), engage in\nmore natural and dynamic conversations (Wu et al. 2023;\nHou, Tamoto, and Miyashita 2024), and learn from a diverse\nset of inputs (Ge et al. 2024; Sun et al. 2024).\nHowever, even in these LLM-enhanced communications,\nhuman teammates continue to play a predominant role in\nrequesting specific tasks and providing suggestions during\ncollaboration (Liu et al. 2023a). Schoenegger et al. (2024)'s\nstudy suggested that state-of-the-art LLMs can often match\nor surpass human performance in various domains. Based\non these results, we hypothesize that allowing LLM agents\nto more proactively participate in (Tanneberg et al. 2024)\nor even initiate communications with human teammates can\nenhance teaming performance and efficiency.\nTo test this hypothesis and systematically evaluate the im-\npact of different forms of language feedback provided by\nrobots on collaboration efficiency and human satisfaction,\nin this study we develop HRT-ML, a human-robot teaming\nframework incorporating multi-modal language feedback\nto support dynamic, context-aware teaming styles. To en-\nable the robot to provide effective language feedback, HRT-\nML comprises two main modules: a Coordinator, which\nmanages overall collaboration strategies and delivers low-\nfrequency or passive instructions and feedback, and a Man-\nager, which determines appropriate subtasks based on the\ncoordinated plan at each stage, offering high-frequency in-"}, {"title": "Testbed: Overcooked-AI", "content": "To test the influence of language feedback on human-\nrobot collaboration, we chose Overcooked-AI (Carroll et al.\n2019), a platform designed to assess multi-agent coordina-\ntion skills. In this game, players are motivated to collaborate\nactively to maximize their score by completing orders within\na time limit. A score of 60 points was awarded when the cor-\nrect soup is served. Partial points will be awarded to incom-\nplete or incorrect soups based on the number of missing/in-\ncorrect components. To cook a soup, chefs need to finish\nspecific subtasks in sequence according to the recipes (see\nFig. 1A), and to finish subtasks, chefs need to move and in-\nteract with the environment. This process can be applied to\nany collaborative setting: first, reasoning through subtasks\nto achieve the overall goal, then selecting low-level atomic\nactions to complete each subtask.\nSubtasks The subtasks in an overcooked environment for\na single agent can be broken down into three main parts:\n1. Gathering Ingredients: Chefs must first pick up the cor-\nrect ingredients, such as onions or tomatoes, and place\nthem into the cooking pot according to the recipe require-\nments.\n2. Cooking: Once the ingredients are placed in the pot, the\nagent has to start cooking. A timer on the pot signals when\nthe soup is ready to be served.\n3. Serving: When the soup is ready, the chefs must collect\nand clean the dish from the dish dispenser, pour the soup\ninto the dish, and deliver it to the serving location.\nIn multi-agent collaborative scenarios, each agent has a dif-\nferent path cost for completing each subtask, such as pick-\ning up an onion versus a tomato. Some subtasks may be un-\nachievable for certain agents. Furthermore, by decompos-\ning a subtask into multiple smaller subtasks that can be per-\nformed by multiple agents, the overall time cost can be re-\nduced. For example, in Fig. 1, the green agent might place\nthe onion at the red cross point (5,2), allowing the blue agent\nto pick it up from there and add it to the pot.\nAtomic Action To finish a specific subtask, such as pick-\ning up an onion, the agent must execute a sequence of atomic\nactions, including movement commands like up, down, left,\nright, stay, and interact for picking up or placing objects."}, {"title": "Human-Robot Teaming Framework with Multi-Modal Language Feedback", "content": "To provide adaptive language feedback in human-robot col-\nlaboration, HRT-ML includes two core components: the Co-\nordinator and the Manager. The Coordinator leads high-\nlevel strategy discussions with humans, generating a final\ncoordination plan and offering low-frequency feedback. In\ncontrast, the Manager handles detailed subtask allocation\nand provides high-frequency feedback to guide human play-\ners. Humans and other agents will receive the feedback and\nexecute low-level actions.\nCoordinator\nThe Coordinator is responsible for designing overall col-\nlaboration strategies and discussing them with the human\npartner. To support this, we employed a structured chain-of-\nthought approach to make the suggestions and discussions\nmore effective. It begins by retrieving relevant information,\nincluding the layout, the overall collaboration goal, rules,\nand the agent's state formatted as a text description (Fig-\nure 2) using a prompt template, and then queries GPT-40\nwith it. It first analyzes the possible subtasks required to\nachieve the overall goal, then evaluates the difficulty of each\nsubtask for each agent, and generates a plan that maximizes\ncollaboration efficiency. For example, based on the environ-\nment (Fig. 1B), our Coordinator suggests: Given the lay-\nout, the human (blue agent) should focus on picking up the\ntomato and serving the dish, while I will handle the onion\nand place the dish at location (5, 2) or (6, 2). We note that\nthe coordination querying is made conversational, allowing\nhumans to continuously revise the plan through continuous\ndiscussion after reviewing the plan. During these coordina-\ntion discussions, the game remains paused until the human\nchooses to end the conversation. The conversation can be\ninitiated proactively by the Coordinator at a low frequency\nor by the human. When a human request is made, we incor-\nporate their suggestions into the coordination plan prompt\nfor initially querying GPT-4, explicitly asking it to evalu-\nate the feasibility of these preferences and propose a corre-\nsponding coordination plan.\nManager The Manager assigns subtasks at each stage,\nproviding high-frequency support to guide the human to-\nward the final goal. First, the Manager uses a subtask filter\nto identify feasible subtasks based on the agent's and envi-\nronment's current states, selecting from the potential subtask\nlist from the overall plan created by the Coordinator. Next,\nit converts the selected subtasks, coordination plan, current\nagent states, and environment state into a language-based\nprompt, querying GPT-4 to generate the next subtask. This\nquery is generated immediately upon completing the current\nsubtask, with an average latency of ~ 1.2 seconds. The de-\ntermined subtask in every query will then be converted into\nlanguage feedback and sent to humans.\nGreedy Planner Once given the target subtask, the human\ndetermines the atomic actions needed to complete it. For col-\nlaborative tasks with an autonomous agent, a greedy planner\nusing Depth First Search (DFS) finds the optimal path with\nthe lowest action cost to finish the subtask.\nMulti-Modal feedbacks\nBuilding on the proposed HRT-ML framework, we intro-\nduce agents that provide four different language feedbacks\ndescribed as follows: Inactive Feedback, Passive Feedback,\nActive Feedback, and Superactive Feedback.\n\u2022 Inactive Feedback agent (IFA): The IFA collaborates with\nhumans without language communication and coordina-\ntion. Only the Manager generates target subtasks for the\ngreedy planner.\n\u2022 Passive Feedback agent (PFA): The PFA starts to provide\npassive feedback only when a human requests. The hu-\nman player takes the role of the leader, while the agent"}, {"title": "Data collection", "content": "In this section, we aim to explore the agent's ability to pro-\nvide language feedback to help improve human satisfaction\nand teaming efficiency. Based on the proposed HRT-ML,\nwe consider four types of language feedback and three dif-\nferent layouts: easy, medium, and difficult, with increasing\ntask difficulty and map complexity (Fig. 3). Human partic-\nipants will play on each layout paired with an autonomous\nagent with varying levels of language feedback. Further de-\ntails will be provided in the following section.\nLayouts with different complexities\nTo test the performance of the four agents and the influence\nof language feedback, we implemented four overcooked\nmaps (Fig. 3). One of the maps is an introductory map, de-\nsigned for participants to get familiar with the game and\noperation. The other three maps have varying levels of dif-\nficulty, which we refer to as easy, medium, and hard. The\nmore challenging maps have more \u201cdead-ends\", requiring\nhumans and agents to have a better coordination strategy to\nmaintain the team efficiency. Furthermore, the hard map in-\ntroduces complexity in task orders by requiring multiple in-\ngredients, which demands that humans and the agent reason\nabout orders containing both tomatoes and onions.\nParticipants\nIn this study, we recruited 16 participants (9 male and 7 fe-\nmale) aged between 23 and 33 to evaluate the performance\nof collaborative agents. We selected participants with vary-\ning levels of familiarity with digital agents in game environ-\nments: 10 participants with self-reported video game time\nbetween 1-10 hours per week, 5 participants reported 10-\n20 hours per week, and 1 reported 20-30 hours per week.\nAdditionally, 13 of the 16 participants reported prior famil-\niar with Large Language Models (LLMs) and embodied lan-\nguage agents, while 3 of the 16 participants reported no prior\nknowledge or experiences with language agents.\nProcedures\nParticipants were first asked to complete a consent form per\nInstitutional Review Board (IRB) protocol. Subsequently,\nprior to the formal trials they were given an opportunity to\nplay with the four agents in an introduction map (Fig. 3) to\nexplore various language feedback styles. During this intro-\nduction phase participants were guided through system oper-\nations, game rules, and agent functionalities. After the intro-\nduction phase, participants proceeded to independently col-\nlaborate with each of the four agent types, on easy, medium,\nand hard layouts (Fig. 3), to complete the maximum score\nwithin the given time limit (60 s for each layout). For each\nparticipant, the scenario (i.e., agent type \u00d7 layout difficulty)\nwas set to show up in a randomized order. We collected a\ntotal of 192 experiment trials, 12 trials per participant.\nWe collected game scores and step-by-step action logs\nfor each experiment shown in (Fig. 3). After completing the\nteaming scenario, participants were also asked to fill out a\nsurvey rating their satisfaction, engagement, and trust level\nfor each experiment, on a seven-point Likert scale. Partic-\nipants were also asked to specify their preferred language\nfeedback level for each layout.\nAdditionally, participants were asked to provide improve-\nment suggestions on the language feedback provided by\nagents (e.g., \"If you were to play with this agent in an Over-\ncooked game competition, what changes or improvements\nwould you suggest for the agent's feedback?\u201d), and state the\nreason for their satisfaction ratings. For more details, the full\nquestionnaire is available at 1.\""}, {"title": "Results and Discussion", "content": "The purpose of our data collection and analysis was to test\nthe following hypotheses: An increased level of language\nsupport will result in an increase in the perceived level of\ntrustworthiness and intelligence of the agent, and improve\noverall team performance.\nSurprisingly, our data suggested while language feedback\ncould indeed facilitate human trust, perceived agent intel-\nligence, and team efficiency, the desired level of language\nsupport exhibited a different relationship than hypothesized.\nWe report our findings in the subsequent sections.\nLanguage feedback builds human trust and\nperceived intelligence\nTrust is a key factor in human-AI collaboration, shaping\nhuman experiences and significantly influencing long-term\ncollaboration efficiency (Chen et al. 2020). In this section,\nwe report the perceived levels of trust and intelligence after\nparticipants teamed with four types of agents across three\nlayouts. Both intelligence and trust levels were measured\nusing a 7-point Likert scale, ranging from \"Very Untrust-\nworthy\" (or \"Very Unintelligent\u201d) to \u201cVery Trustworthy\" (or\n\"Very Intelligent\"). We found that as the agent's support\nlevel increased from Inactive to Superactive, intelligence rat-\ning increased monotonically (Fig. 4). A similar trend was\nobserved in the trust ratings. This suggested that as hypothe-\nsized, active communication can facilitate building trust and\npreceived intelligence. Another interesting observation was\nthat, as the agents become more active, the standard devi-\nation for trust ratings and intelligent ratings also increased.\nThis suggested that both trust and intelligence levels also\nbecome more influenced by individual human preferences\nwhen the agent takes on a more active role in assisting.\nAppropriate Language feedback improves\ncollaboration efficiency\nWe used the game score to evaluate the team perfromance\nand collaboration efficiency between humans and agents.\nOverall, the team scored more points in easy layout with\nall agent types (53.6 in average) as compared to the medium\n(30.6 in average) and hard (27.3 in average) layouts. This\nis not surprising, as the easy layout has simpler maps and\nfewer subtasks, requiring minimal coordination to complete.\nIn contrast, medium and hard layouts introduced more com-\nplex subtasks and dependencies, requiring greater coordina-\ntion and team effort, which could increase the chance of er-\nrors during the task and result in lower scores.\nWe found that the team performance with \"active\" agents\n(PFA, AFA, and SFA), which engaged in language feed-\nback and coordination with human, achieved higher scores\non almost all difficulty levels than the \"inactive\" agent (IFA),\nwhich did not engage in any communication (Fig. 5). In ad-\ndition, as the difficulty level increases from easy to hard, the\nteam performance with the passive agent (PFA) decreased\nsignificantly, from close to 40 points to around 10 points\n(approximately 75% of performance drop). This result sug-\ngested that as hypothesized, language feedback can play a\nsignificant role in human-robot teaming, especially in com-\nplex tasks.\nHowever, the team performance was not always better\nwith more active agents. In the easy layout, team with PFA\nperformed best, scoring 83.6 in average, significantly higher\nthan IFA, AFA, and SFA (Fig. 5). It was expected that PFA\nperformed better than IFA in the easy task, as it provided ef-\nfective support with minimal interference, enabling partici-\npants to benefit from its assistance as needed and allowing\nhumans to take the lead. This was also supported by user\nfeedback from the survey \"Robot listening to the user\nand following the commands would help the game better.\nIn terms of intelligence, passive robots work better but still\nlack user assistance\". Unexpectedly, while AFA and SFA\noffered more active feedback, they achieved lower scores\nthan the PFA (Fig. 5), suggesting that constant communi-\ncation was less effective for simple tasks, and may even dis-\ntract humans. As the complexity increases to the medium\nlevel, the team with SFA exhibited a huge increase in score\n(Fig. 5), exceeding the performance of PFA, implying that as\ntask complexity increased, the frequent support provided by\nSFA became more valuable. Similarly, in the hard layout, the\nSFA and AFA demonstrated superior performance as com-\npared to IFA and PFA (Fig. 5), underscoring the value of\nactive guidance and high-frequency support in facilitating\ncollaboration and helping improve task execution in com-\nplex tasks.\nInterestingly, despite SFA's higher frequency of active\nsupport compared to AFA, overall performance still de-\ncreased due to the high complexity of the hard layout com-\npared to AFA. Participant feedback highlighted this chal-\nlenge, e.g., \"the robot/agent is not as smart as me,\u201d and, \u201cI\nhave to give it a long instruction set, and I am more intelli-\ngent than it in the hard layout.\u201d One interpretation of these\nresponses was that participants found the agent's support in-\nsufficient for complex tasks. As a result, instead of reduc-\ning cognitive load, the frequent suggestions from the agent\nrequired human to carefully think about responses, which\nultimately increased cognitive demands and decreased team\nefficiency. Another interpretation is that psychologically, hu-\nmans may have the preference to demonstrate and maintain\nintellectual superiority in challenging tasks when teaming\nwith Al agents. As a result, how AI agents communicate\nsuggestions may greatly influence humans' acceptance rate\nand team efficiency.\nOverall, our results revealed that, the language feed-\nback provided by LLMs can boost human-robot collabora-\ntion efficiency and increase human satisfaction. However,\nthe proactiveness and frequency of the language feedback\nshould be provided based on the task complexity and the ca-\npabilities of LLMs."}, {"title": "Humans don't always prefer the best-performing agents", "content": "Interestingly, our data suggested that human does not al-\nways prefer the agent type that helped achieve the highest\ngame scores. For example, even though the IFA achieved\nthe lowest scores on easy layout (Fig. 5), over 50% of par-\nticipants reported IFA as their preferred agent among the\nfour types (Fig. 6A). Similarly, even though the SFA out-\nperformed PFA by almost two folds in terms of team score\n(Fig. 5), 56% of participants selected the PFA as their pre-\nferred agent, and 0% of them preferred SFA (Fig. 6A).\nWe believe this preference shift can be explained by\nthe flow theory (Csikszentmihalyi 2000; Chen et al. 2024),\nwhich states that people feel most engaged when task com-\nplexity aligns with their skill level, and robots can help\nachieve this balance by adjusting their level of support. In\nthe easy layout, the task was not significantly beyond hu-\nman's skill level, and the need for additional help to main-\ntain engagement was low. Therefore, while the active agents\ncan provide help, this help did not significantly influence hu-\nman's engage level and satisfaction rate (Fig. 6B). As a re-\nsult, the non-active agent, which requires the lowest level\nof cognitive load (\u201ccost\") was preferred. As the difficulty\nlevel increases, however, the gap between task complexity\nand human skill level increases, resulting a disruption to\nthe task-capability balance and reduced engagement. Mean-\nwhile, the additional feedback and support from the more\nactive agents, such as assigning subtasks (e.g., \"pick up the\nonion from (x, x).\u201d, can reduce the coordination and plan-\nning effort on the human, and restore the task-capability\nbalance and enhance human's feel of engagement and joy.\nAt this point, the cost of communication became negligible\nas compared to the need for sense of achievement, making\nmore active agents more desirable in these challenging sce-\nnarios (Fig. 6B).\""}, {"title": "Adaptively assigning subtasks and providing language feedback", "content": "Our findings revealed the importance for LLM agents to\nadapt their language feedback by considering the relation-\nship between task complexity, $T_h$, human capability, $C_h$, and\nthe LLM's capability, $C_l$. Below we propose a simple adap-\ntation strategy for LLM agents to select their support level\nand language feedback frequency:\n\u2022 $C_h > T_h$ and $C_l < T_h$: Here human capability surpasses\nthe task's complexity, and the LLM capability is not suf-\nficient to address the challenging tasks without human\nguidance. Based on our results, a passive (PFA) to rel-\native infrequent (AFA) feedback style would allow the\nagent to provide sufficient support to improve team per-\nformance and request human help when needed, while\nkeeping communication frequency to a minimal to avoid\noverhead on the human side. This way, human teammates\nwith higher capability level could guide the agent on high-\nlevel coordination strategies and specific subtask execu-\ntion, keeping them engaged.\n\u2022 $C_h < T_h$ and $C_l > T_h$: Here task complexity exceeds hu-\nman capability, while LLM is fully capable of executing\nthe task. In this case, extra active agent feedback and sup-\nport (SFA) are crucial for maintaining team performance,\nand help reduce the gap between human capability and\ntask complexity.\n\u2022 $C_h < T_h$ and $C_l < T_h$: Here task complexity exceeds\nhuman capability. However, LLM capability is also not\nsufficient to address the challenging task neither. In this\ncase, language feedback from LLM is often not useful in\nresolving the challenges that human is struggling with,\nand can even be misleading. High-frequency communi-\ncations from LLM in this scenario would require addi-\ntional effort from human to respond, potentially further\nheightening the anxiety that human is already experienc-\ning (Lenzner et al. 2010), decreasing team performance\nand human engagement. Our results suggested that a more\npassive (PFA) or relative infrequent (AFA) feedback style\nwould result in better teaming performance in this case.\n\u2022 $C_h > T_h$ and $C_l > T_h$: Here both human and LLM capa-\nbilities surpass the task's complexity. The active feedback\nstyle (AFA) could allow the human and agents to com-\nmunicate their needs at a comfortable pace, and improve\ncollaboration efficiency."}, {"title": "Conclusion", "content": "In this work, we introduced HRT-ML, a flexible human-\nrobot teaming framework designed to provide adaptive\ncommunication feedback to humans at varying levels\nand frequencies. The HRT-ML framework comprises two\ncore modules: a Coordinator for high-level, low-frequency\nstrategic guidance and a Manager for task-specific, high-frequency instructions, allowing collaborating with humans\nacross four distinct feedback styles: Inactive, Passive, Ac-\ntive, and Superactive.\nOur user study results demonstrated that language-based\nfeedback from LLMs can significantly enhance collabora-\ntion performance and foster human trust, and that as task\ncomplexity increases, more frequent, proactive support is\ndesired. However, our study also revealed that it is critical\nfor the agent to select their language feedback frequency\nbased on task complexity, human capability, and agent ca-\npability. Overly frequent feedback in simple tasks or from\nless capable agents does not effectively increase team per-\nformance and satisfaction, and could even increase effort\nand reduce human engagement. Based on these findings, we\nproposed a simple principle that allows agents to adapt their\nlanguage feedback style according to perceived task chal-\nlenges, human capabilities, and LLM capabilities."}, {"title": "Limitations and future work", "content": "In this work, we focused on investigating the effect of agent\nfeedback frequency on team performance. As such, each\nagent was set to a constant active level, and cannot dynam-\nically adjust their level of support and language feedback\nthroughout the task. In real-world scenarios, task complexity\nand human skill levels for different sub task can vary dynam-\nically, requiring agents to adjust their communications and\nbehaviors accordingly. Future work should explore methods\nfor adaptive agent to estimate human cognitive load, capa-\nbilities, and engagement, and adjust LLM feedback in real\ntime to enable better team performance and adaptability. The\nresults from our study provide the basis for designing and\nimplementing such real-time feedback adjustments. Going\nforward, these adaptive response and feedback capabilities\ncan empower future LLM agents to flexibly support human\nneeds in a wide variety of task complexities, fostering true\npartnerships and enhancing teamwork outcomes."}]}