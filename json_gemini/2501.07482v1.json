{"title": "TiEBe: A Benchmark for Assessing the Current Knowledge of Large Language Models", "authors": ["Thales Sales Almeida", "Giovana Kerche Bon\u00e1s", "Jo\u00e3o Guilherme Alves Santos", "Hugo Abonizio", "Rodrigo Nogueira"], "abstract": "In a rapidly evolving knowledge landscape and the increasing adoption of large language models, a need has emerged to keep these models continuously updated with current events. While existing benchmarks evaluate general factual recall, they often overlook two critical aspects: the ability of models to integrate evolving knowledge through continual learning and the significant regional disparities in their performance. To address these gaps, we introduce the Timely Events Benchmark (TiEBe), a dataset containing over 11,000 question-answer pairs focused on globally and regionally significant events. TiEBe leverages structured retrospective data from Wikipedia, enabling continuous updates to assess LLMS' knowledge of evolving global affairs and their understanding of events across different regions. Our benchmark demonstrates that LLMs exhibit substantial geographic disparities in factual recall, emphasizing the need for more balanced global knowledge representation. Furthermore, TiEBe serves as a tool for evaluating continual learning strategies, providing insights into models' ability to acquire new information without forgetting past knowledge.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have rapidly become central to numerous ap- plications [3, 15, 7], prompting continuous efforts to refine and update them. Keeping these models' knowledge timely and accurate as the world's events un- fold has grown increasingly important. Continual pretraining [28, 25, 4] has emerged as a promising paradigm for systematically integrating new informa- tion, ensuring that models remain current with ongoing global affairs. However, despite clear interest in dynamically updating LLMs, there remains a shortage"}, {"title": "Related work", "content": "As large language models (LLMs) continue to improve, there is growing interest in evaluating their ability to comprehend and recall factual knowledge about the world. While many studies have investigated LLMs' capacity for general factual recall [13, 18, 21], it has become evident that this ability varies signif- icantly based on the geographic or cultural context of the data. For instance, WorldBench [14] highlights regional disparities in LLM performance, demon- strating that their ability to recall facts can differ depending on the region. This benchmark focuses on question-answering tasks involving economic and social statistics of various countries. However, our work takes a different ap- proach and focuses on notorious events-historical and significant occurrences- associated with specific countries or with global impact. By emphasizing events, our dataset captures a unique dimension of factual knowledge not explored by"}, {"title": "Methodology", "content": "In this section we decribe the pipeline for creating TiEBe, which is illustrated in Figure 1."}, {"title": "Data Collection", "content": "For data collection, we leveraged existing Wikipedia retrospective pages; these pages list the most notorious events of a given year in a given country or world- wide. An example of such a page, \"2021 in China,\" is shown in Figure 2. In order to study factual recall through a wide range of time, we chose to use retrospec- tive pages from 2015 up to 2024. Beyond that, we selected retrospective pages from five different countries-Brazil, China, Portugal, the USA, and France-as well as from a global context (referred to as \"World\"); with this diverse set of event contexts, we aim to study geographical bias in LLM knowledge."}, {"title": "Generation of Question-Answer Pairs", "content": "From these news documents, acquired from the citation links, we generate syn- thetic question-answer pairs about the events they discuss. The goal is to use these pairs to test whether a model recalls the information contained in the original source news document. Note that the source document is only used to generate the QA pairs that is, the LLMs being tested will not have access to it. The number of questions generated for each country and region, as summa- rized in Table 1, amounts to a total of 11,236 questions. We chose to create the QA pairs exclusively in English, despite some of the documents used being in other languages, for two primary reasons. First, this approach allows us to focus on the factual recall of the models and not their multilingual capabilities. Second, the QA pair generator is a model primar- ily optimized for English. Asking it to generate QAs in other languages could introduce translation inaccuracies, potentially compromising the dataset's qual- ity. To create the QA pairs, we use GPT-40-2024-08-06, providing it with the event description, the news document referenced in the event, and the date of the mentioned event. The full prompt is shown in Appendix A.1. Examples of the generated QA pairs for different regions are shown in Table 2. We observe that the questions cover a wide range of topics, ranging from political events to scientific milestones."}, {"title": "Model Evaluation", "content": "We selected 5 different models for our evaluation: two models with open weights, Qwen2-70B [26] and Llama-3-70B [2], and three commercial models: Sabia-3- 2024-12-11 [1], Mistral-large-2411 and GPT-40-2024-08-06 [16]. Many of these models have a non-American focus. For example, Qwen focuses on Chinese data, and Sabia-3 reports training mainly on Brazillian data. With this varied selection of models, we aim to investigate whether these language-specific biases improve model performance in their respective regions. All models are evaluated in the same manner. Each question is provided to the LLM as a zero-shot prompt. We then use an LLM-as-judge [5, 29, 12] to evaluate the answer. In this study, we use GPT-40-2024-08-06 as the judge. The judge receives the question, the candidate's answer provided by the LLM, and the expected answer created previously in our QA generation process. The judge then decides if the provided answer is correct or not. The full prompt used for the judge can be found in Appendix A.2."}, {"title": "Results", "content": "This section will discuss the results of the 5 tested models in the TiEBe dataset, exploring overall accuracy and their regional and temporal performance."}, {"title": "Models performance", "content": "Table 3 shows the overall accuracy of each tested model in our dataset. We can see a significant gap between GPT-40 and the other models, achieving 10% accuracy points over the second-best model in Mistral-large. Mistral-large shows a slightly superior performance compared to the other three models, all of which show a very similar performance overall, around 50% of accuracy. Another important aspect we explored was the difference in our models' performance depending on the events' origin. Table 3 also shows the overall accuracy of each tested model in each region of our dataset. GPT-40 consistently outperforms all other models across every region, with Mistral-large securing second place in all regions except China. Meanwhile, Sabi\u00e1-3, Qwen2-70B, and Llama3-70B often compete for the bottom three posi- tions. Despite their rankings, the models exhibit significantly different average performances depending on the region of origin of the events. For instance, the USA and world regions achieve higher averages, around 60%, while all other regions show an average at least 20 percentage points lower. We attribute this disparity to two main factors. First, LLM training datasets tend to prioritize English-language documents over other languages[19, 20]. Second, studies have"}, {"title": "Performance over time", "content": "We also explore how these models perform when answering questions related to news from various time ranges. Figure 3 presents the accuracy of all models across three time periods. Detailed results for each model by year are provided in Table 6 in the Appendix. All the tested models have a training data cutoff in 2023; GPT-40 and Mistral-large specify the cutoff data as october of 2023, the others models do not specify a month for their cutoff date. First, the data reveals that model performance remains relatively consistent between the periods 2015-2018 and 2019-2022, suggesting that the models do not exhibit a strong performance bias toward either older or more recent events within this range. However, a marked decline in performance is observed across all models for the period 2023-2024. This decline is expected, as the questions increasingly require knowledge of events occurring after the models' respective cutoff dates."}, {"title": "The effects of model specialization", "content": "Most of the models chosen for our experiments incorporate languages other than English in their training data; the Sabi\u00e1-3 family of models is trained mainly in Brazilian Portuguese, the Qwen models include a significant amount of Chi- nese in their pretraining, and Mistral Large claims support in English, French, Spanish, German, and Italian, mainly European languages. While Llama3 does include multilingual data in the pretraining, English is still the main focus. As for GTP40, little is known about their training data."}, {"title": "Future work", "content": "Our work introduces a pipeline for creating QA datasets oriented by major events listed in Wikipedia retrospective pages. While this allows us to have some level of confidence in the relevance of the events, it limits the approaches to countries with a well-developed retrospective page with numerous events. Future work could aim to retrieve events from other sources, allowing for a more general QA pipeline. Language is another aspect not explored in this work. While we study the factual recall of events from different regions and from models that originated in different regions, we limited the questions to English, possibly creating an advantage for English-centric models. Further studies exploring the use of other languages can provide further insights into the matter. Beyond that, the models tested in this work are limited due to budget con- straints; the publication of the dataset will allow future works to further explore the event knowledge of LLMs over multiple countries and possibly be a tool for bridging the gap observed between the USA and other countries."}, {"title": "Conclusion", "content": "We introduced TiEBe, a collection of over ten thousand QA pairs about events spanning six geographical regions from 2015 to 2024, and the pipeline used for its creation, which allows for progressive dataset updates as time passes, making our dataset a tool in continual learning. Finally, we explored the factual knowledge of various LLMs using our dataset and noticed a high gap in the performance of all models between events in the USA and other countries;"}, {"title": "Appendix", "content": ""}, {"title": "Prompts for QA generation", "content": "You are an assistant responsible for creating pairs of questions and answers based on news articles. These question-answer pairs will be used to construct a dataset for evaluating knowledge from the past. Your task is to create up to {n_questions} questions and their corresponding answers based on the infor- mation in the news article. The questions should be clear and understandable, even for those who have not read the article. Avoid asking about information that is constantly changing or lacks a definitive answer, such as the current death toll of an event or the present status of a specific situation. Focus on questions that will remain relevant in the future. Use the past tense in the questions. Avoid starting with \"What is...\" or referring to ongoing events or situations. Refrain from asking about the current status of a particular subject, such as an agreement or situation that may change over time. Additionally, avoid overly specific questions. Instead, focus on broader and more meaningful information about significant events. Keep in mind that the reader will not have access to the article itself, so do not reference the article directly (e.g. \"according to the article\"). Emphasize the key information the article provides, and specify the point in time when an event occurred, if necessary. Write the questions and answers in English, regardless of the language of the article."}, {"title": "Prompts for model generation", "content": "Answer the following question: \"{question}\"\nIf necessary, consider the context of {region}, Provide your response in the following format:\n\"Answer: your answer\""}, {"title": "Prompts for model evaluation", "content": "I will provide a question, an expected answer, and the candidate's answer. Your task is to verify if the candidate's answer is correct. The expected answer is the ground truth, so if the candidate's answer contradicts the expected answer or refuses to answer, it is incorrect.\nQuestion: \"{question}\"\nExpected answer: \"{expected_answer}\"\nCandidate answer: \"{model_answer}\"\nAnswer in the format\nReasoning: (your reasoning)\nCorrect: (yes\u2014no)"}, {"title": "Dataset statistics", "content": ""}, {"title": "Full results", "content": ""}]}