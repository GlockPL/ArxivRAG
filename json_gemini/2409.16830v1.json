{"title": "OffRIPP: Offline RL-based Informative Path Planning", "authors": ["Srikar Babu Gadipudi", "Srujan Deolasee", "Siva Kailas", "Wenhao Luo", "Katia Sycara", "Woojun Kim"], "abstract": "Informative path planning (IPP) is a crucial task in robotics, where agents must design paths to gather valuable information about a target environment while adhering to resource constraints. Reinforcement learning (RL) has been shown to be effective for IPP, however, it requires environment interactions, which are risky and expensive in practice. To address this problem, we propose an offline RL-based IPP framework that optimizes information gain without requiring real-time interaction during training, offering safety and cost-efficiency by avoiding interaction, as well as superior performance and fast computation during execution-key advantages of RL. Our framework leverages batch-constrained reinforcement learning to mitigate extrapolation errors, enabling the agent to learn from pre-collected datasets generated by arbitrary algorithms. We validate the framework through extensive simulations and real-world experiments. The numerical results show that our framework outperforms the baselines, demonstrating the effectiveness of the proposed approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Informative path planning (IPP) is a critical problem in robotics and autonomous systems, where the goal is to design a path that enables an agent to acquire valuable information about a quantity of interest within a given environment while adhering to resource constraints, such as a robot's battery life. IPP has numerous applications, including environmental monitoring [1], search and rescue operations [2], and precision agriculture [3]. The challenge is to ensure that the agent accurately approximates the true interest map.\nTraditional methods for IPP, [4], [5], rely on sampling techniques that, while effective, can be computationally intensive and time-consuming. They typically require significant computational resources and often struggle to scale in large, complex environments. Consequently, there is growing interest in leveraging reinforcement learning (RL) as a potential solution to the IPP problem [6], [7], [8]. RL offers a promising avenue for IPP by enabling agents to learn optimal policies through interactions with the environment. However, conventional RL approaches require extensive real-time interactions, making training costly and potentially hazardous, especially in safety-critical environments. To address this, offline RL, which trains an agent using only pre-collected"}, {"title": "II. BACKGROUND AND RELATED WORKS", "content": ""}, {"title": "A. Offline Reinforcement Learning", "content": "RL trains an agent through interaction with an environment, and the procedure is formulated as a Markov Decision Process, defined by the tuple < S, A, P,r,y >, where S is the state space, A is the action space, P is the transition probability function, r is the reward function, and y is the discount factor. Specifically, at each time step t, the agent's policy selects an action $a_t \\in A$ based on the current state $s_t \\in S$. The environment responds by providing a reward $r_t = r(s_t,a_t)$ and transitioning to the next state $S_{t+1} \\sim P(S_{t+1}|S_t, a_t)$. By repeating this procedure in an online learning fashion, the policy is trained to maximize the expected return, $E[\\sum_{t=0}^\\infty \\gamma^t r_t]$. Here, to evaluate and improve the policy, most RL algorithms estimate the value function under the current policy, such as $Q^\\pi(s,a) = E_\\pi[\\sum_{t=0}^\\infty \\gamma^t r_t | s, a]$, which is the expected return over the trajectory that follows policy \\pi after executing action a in state s. One way to estimate this is temporal-difference (TD) learning [11], which can be written as\n$Q^{\\pi} (s, a) = E_{s, \\pi}[r + \\gamma Q^{\\pi} (s', \\pi(s'))],$ (1)\nwhere s' is the next state. The policy is trained to maximize the estimated value function. Thus, accurate estimation of the value function is crucial, as it directly affects policy learning.\nOffline RL trains an agent on a fixed dataset previously collected from arbitrary policies, without any interaction with the environment during training [12], [9], [13]. By avoiding interaction, which is often risky and expensive in real-world applications, offline RL enables the development of a decision-making agent that maximizes the expected sum of rewards. A significant challenge in offline RL is extrapolation error caused by the mismatch between the dataset and the actual state-action distribution of the current policy. This leads to inaccurate value estimates [10]. For example, in TD learning, if a state-action pair (s', \\pi(s')) is not included in the dataset, the estimate of the value function for this state-action pair can be highly inaccurate. In other words, the expectation in Eq. 1 has a large approximation error, which accumulates in the estimate of $Q^\\pi (s, a)$ in Eq. 1. Thus, a greater mismatch between the dataset and the state-action distribution of the current policy increases the likelihood of approximation error.\nTo handle extrapolation error, most offline RL algorithms make the learning policy similar to the behavior policy, which was used to generate the dataset [10], [14], [9], in order to reduce the mismatch. For instance, Conservative Q-learning penalizes Q-values for actions not seen in the"}, {"title": "B. Informative Path Planning", "content": "The goal of IPP is to find a trajectory $\\psi^*$ that maximizes the information gain within the given budget [17], [18], [19], and the objective function is written as\n$\\psi^* = \\arg \\max_\\psi I(\\psi), \\text{ s.t. } C(\\psi) \\leq B, $ (2)\nwhere $I : \\psi \\rightarrow \\mathbb{R}^+$ and $C : \\psi \\rightarrow \\mathbb{R}^+$ represent the information gain and cost along trajectory $\\psi$. $B \\in \\mathbb{R}^+$ is the limit of the budget. The type of information gain depends on the domain, e.g., fire intensity in the wildfire domain. To find a solution for IPP, traditional IPP algorithms utilize sampling methods [4], [20], [21], [1]. However, they require significant computational resources for planning, which limits their practicality in real-world applications.\nRL-based IPP: RL has been utilized to learn an IPP solver to address the aforementioned problem of traditional IPP solvers and further enhance performance [22], [6], [8]. RL-based IPP algorithms consist of three components: constructing a representation of the entire search map, modeling environmental phenomena, and training an RL agent to select a path that maximizes information gain. As an example, CATNIPP [6] constructs a representation to cover a continuous 2D search domain using a probabilistic roadmap (PRM) [23], which reduces the complexity of the search space. At the start of each episode, the PRM generates a graph G = (V, E) with nodes V and edges E, where each node is connected to k neighbors, and the agent is initialized at a random node. At each time step, the agent moves to one of its neighboring nodes and observes the environmental phenomenon at its new location. To model this phenomenon across the search space, Gaussian Process (GP) regression, a non-parametric Bayesian method that uses statistical inference to capture relationships between data points, is used [24], [25]. Specifically, the mean and covariance of a test location $X^*$, given the observed locations X, are inferred as: $\\mu^* = K(X^*,X)[K(X,X) + \\sigma^2 I]^{-1}(Y - \\mu(X))$, $P^* = K(X^*,X^*) - K(X^*,X)[K(X,X) + \\sigma^2 I]^{-1}K(X,X)^T$, where K(., .) is the kernel function, $\\sigma_f^2$ is a hyperparameter of the GP. At each time step, the agent predicts the environmental phenomenon based on the GP and uses this prediction as input to the RL policy. After observing the environmental phenomenon at the new location, the GP is updated based on the new observation value. The output of the GP is incorporated into the graph, referred to as the GP-augmented graph. The RL policy uses the agent's current location, budget, trajectory history, and the GP-augmented graph, as inputs. The RL agent then selects one of the neighboring nodes. To train the agent to maximize the expected reduction in GP uncertainty, the reward function is designed as the normalized information gain, $r_t = (Tr(P_{t-1}) \u2013 Tr(P_t))/Tr(P_{t-1})$,"}, {"title": "III. OUR APPROACH", "content": "We propose an offline RL-based Informative Path Planning algorithm, OffRIPP, that trains an IPP solver using existing datasets without requiring interaction with the environment. By avoiding costly and risky real-time interactions, OffRIPP is applicable to real-world scenarios. In addition, OffRIPP can be combined with any arbitrary RL-based IPP algorithm, including CATNIPP [6] and 3D RL-based IPP [8]."}, {"title": "A. Problem Formulation", "content": "Dataset. The dataset D is assumed to be generated by arbitrary algorithms and consists of D episodes, with each episode containing sequences of states, actions, and rewards. Here, the states include a graph where each node contains a position and the agent's prediction of the environment phenomenon, as well as the planning state, which includes the current agent position and the remaining budget. The actions represent the next node the agent moves to, and the rewards correspond to the GP uncertainty.\nBy utilizing the dataset, our goal is to train an RL agent that plans a path maximizing the information gain under the budget constraints. Note that given that the dataset includes PRMs and predictions of environmental phenomena through GP regression, our approach does not require generating these components during training but instead leverages the given dataset. Upon completion of the RL agent's training, we deploy it in the test environment for evaluation."}, {"title": "B. Offline RL-based Informative path planning", "content": "It is known that naive training of RL algorithms, such as PPO [26], on the dataset can lead to inaccuracies in value estimation, resulting in unstable learning, as discussed in Sec. II-A. We observed that CATNIPP, which relies on PPO, fails to train on the dataset, as we will discuss in Sec. IV-C. In order to address this instability, we leverage a key concept from batch-constrained Q-learning (BCQ) [10], which restricts the actions selected by the policy during training to a subset of the given dataset. To achieve this, a model is introduced to generate actions similar to those in the batch, which are then used to build a policy with the Q-function. Consequently, OffRIPP consists of two components: an RL agent and a behavior policy approximator. To boost sample efficiency, both modules share the entire neural network-based function approximators, except for the final layer.\n1) Behavior Policy Approximation: We build an approximation of the behavior policy, $\\pi_{\\theta_\\beta}$, to ensure that the learning policy avoids selecting actions that are not supported by the dataset. Since we only have access to the dataset generated by the behavior policy, and not the behavior policy itself, we utilize imitation learning, which minimizes the negative log-likelihood function. The loss function for the behavior policy approximator, parameterized by $\\theta_\\beta$, is written as:\n$L(\\theta_\\beta) = -E_{(s,a)\\sim D} [\\log \\pi_{\\theta_\\beta} (a|s)],$ (3)\nwhere (s, a) is the state-action pair in the dataset. We pretrain the behavior policy approximator before policy learning.\n2) Policy Learning: We build a policy based on the Q-function, $Q_{\\theta_Q}(s, a)$, and train it using the TD update. Here, in contrast to Q-learning, which employs a greedy policy, represented as $\\pi(s) = \\arg \\max_a Q(s, a)$, following BCQ, we use the behavior policy approximator to construct a policy based on the Q-function as follows:\n$\\pi(s) = \\arg \\max_{\\hat{a}} Q_{\\theta_Q}(s, \\hat{a}),$ (4)\n$\\hat{a}: \\pi_{\\theta_\\beta}(\\hat{a}|s) > \\tau$\nwhere $\\tau$ is the threshold that defines how much the learning policy deviates from the behavior policy. In other words, we restrict the policy to generate actions that the behavior policy is likely to generate with a probability above the threshold $\\tau$. Note that $\\tau$ = 0 corresponds to fully imitating the behavior policy, while $\\tau$ = 1 corresponds to following the greedy policy. The rationale behind this is to avoid using unseen state-action pairs, which can lead to extrapolation errors. Based on this policy, we train the Q-function using TD update and the corresponding loss function is written as\n$L(\\theta_Q) = E_{(s,a,r,s')\\sim D} [(r + \\gamma \\max_{\\hat{a}: \\pi_{\\theta_\\beta}(\\hat{a}|s') > \\tau} Q_{\\theta_Q} (s', \\hat{a}) - Q_{\\theta_Q} (s,a))]^2,$ (5)\nwhere $Q_{\\theta_Q'}$ represents the parameters of the target Q-function, which is a delayed update of the Q-function, providing stable targets to reduce instability during training [27].\n3) Network Architecture.: The RL policy and the behavior policy approximator share the entire network except for the final layer. The shared network architecture follows the design of either CATNIPP [6] or the RL-based IPP [8], and any existing RL-based IPP architecture can be integrated. Here, we explain an example based on CATNIPP. The shared network takes an augmented graph as input, where each node contains the position and the mean and variance of the GP, to generate an effective representation that captures the relationships between nodes, making the system spatially aware. This augmented graph is then processed by positional encoding. Additionally, the planning state, which includes the remaining budget and executed trajectory, is combined with"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we validate the effectiveness of our proposed approach, OffRIPP, in two different simulated environments: a 2D light-intensity IPP task and a 3D fruit identification task, as well as in a real-world robotic environment. Since our work is the first to apply offline RL to IPP, we generate diverse datasets with varying performance levels, comprising both optimal and sub-optimal data, using existing RL-based IPP algorithms and a traditional IPP solver for evaluation. We believe the generated dataset will also make a valuable contribution to the research community."}, {"title": "A. Experiment Setup", "content": "1) Light-intensity Environment: Adopted from [6], the goal of this environment is to predict the light-intensity, which is the interest in the context of IPP, across the 2D map. The light intensity is constructed using 8 to 12 random 2-dimensional Gaussian distributions within the unit square [0, 1]2. The light-intensity is illustrated in Fig. 3a. At the start of each episode, the agent's initial prediction is initialized as a uniform distribution. During dataset collection, both the agent's start and destination positions are randomly generated within the same unit square. The environment is discretized using a PRM with a fixed number of nodes, 400, and the"}, {"title": "B. Training Details", "content": "To train OffRIPP, we use the Adam optimizer [28], a batch size of 256, and an update frequency of 100 for the target network. For the threshold $\\tau$ in Eq. 4, we perform hyperparameter tuning in the range (0,1). OffRIPP trains for 1 epoch on the dataset, requiring an average of 3 hours in the light-intensity environment and 18 hours in the 3D fruit identification environment when using a workstation"}, {"title": "C. Experimental Results", "content": "1) Light-intensity Environment: For the evaluation of the proposed framework, OffRIPP, we consider four baselines: (i) Greedy Planning, where the agent selects its next node from its neighbors based on the highest entropy of the GP; (ii) Randomized Anytime Orienteering (RAOr) [5], a sampling-based heuristic approach. Note that Greedy Planning and RAOr do not require training, so we report their performances regardless of the dataset; (iii) Behavior Cloning (BC), which imitates the behavioral policy. We use the CATNIPP architecture for BC and train it by optimizing the negative log-likelihood function; and (iv) CAtNIPP [6] is trained offline, where the CATNIPP agent is trained using PPO on the dataset. Besides the baselines, we also report the performance of the dataset, which reflects the quality of the data used to train the models, to validate if the trained model is able to outperform the model that generated the dataset. Note that the behavior policy is trained online, which makes comparisons with OffRIPP unfair.\nWe provide the performance of the algorithms under three types of datasets, as described in Sec. IV-A, with three different budgets of 6, 8, and 10. For the evaluation metric, we use the average trace of the covariance matrix over 50 different instances of the environment, which is the optimization objective in RL and is also used as a comparison metric in [6]. The corresponding results are shown in Table I. It is observed that OffRIPP consistently produces the lowest covariance trace values across all budget instances,"}, {"title": "D. Analysis", "content": "1) Performance with respect to the dataset: The performance of OffRIPP is influenced by both the quality and quantity of the dataset, as OffRIPP depends on the given dataset without generating new data based on the current policy. In Table I and Table II, it is observed that performance improves as the performance of the behavior"}, {"title": "V. CONCLUSION", "content": "In this paper, we present an offline RL-based IPP framework that enables training an RL agent to find a path that maximizes information gain without environment interactions during training. Based on the IPP architecture, which incorporates a graph, environment modeling using a GP, and attention mechanisms, we model an approximated behavior policy that imitates the policy used to generate the dataset, along with a Q-function. We then build a policy that chooses an action that maximizes the Q value, using the approximated behavior policy to select from among the actions likely found in the dataset. We show that OffRIPP outperforms the baselines, including the traditional IPP solver, in terms of both performance and planning time across two different environments. Additionally, we validate the effectiveness of OffRIPP in a real-robot system. Future work includes learning multi-agent IPP policies from a dataset."}]}