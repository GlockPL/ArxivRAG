{"title": "On the Sample Complexity of a Policy Gradient Algorithm with Occupancy Approximation for General Utility Reinforcement Learning", "authors": ["Anas Barakat", "Souradip Chakraborty", "Peihong Yut", "Pratap Tokekar", "Amrit Singh Bedi"], "abstract": "Reinforcement learning with general utilities has recently gained attention thanks to its ability\nto unify several problems, including imitation learning, pure exploration, and safe RL. However,\nprior work for solving this general problem in a unified way has mainly focused on the tabular\nsetting. This is restrictive when considering larger state-action spaces because of the need to estimate\noccupancy measures during policy optimization. In this work, we address this issue and propose to\napproximate occupancy measures within a function approximation class using maximum likelihood\nestimation (MLE). We propose a simple policy gradient algorithm (PG-OMA) where an actor updates\nthe policy parameters to maximize the general utility objective whereas a critic approximates the\noccupancy measure using MLE. We provide a sample complexity analysis of PG-OMA showing that\nour occupancy measure estimation error only scales with the dimension of our function approximation\nclass rather than the size of the state action space. Under suitable assumptions, we establish first\norder stationarity and global optimality performance bounds for the proposed PG-OMA algorithm\nfor nonconcave and concave general utilities respectively. We complement our methodological and\ntheoretical findings with promising empirical results showing the scalability potential of our approach\ncompared to existing tabular count-based approaches.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning with general utilities (RLGU) has emerged as a general framework to unify a\nrange of RL applications where the objective of the RL agent cannot be simply cast as a standard\nexpected cumulative reward (Zhang et al., 2022). For instance, in imitation learning, the objective is\nto learn a policy by minimizing the divergence between the state-action occupancy measure induced\nby the policy and expert demonstrations (Ho and Ermon, 2016). In pure exploration, the goal is to\nlearn a policy to explore the state space in a reward-free setting by maximizing the entropy of the\nstate occupancy measure induced by the agent's policy (Hazan et al., 2019). Other examples include\nrisk-averse and constrained RL (Garcia and Fern\u00e1ndez, 2015), diverse skills discovery (Eysenbach et al.,\n2019), and experiment design (Mutny et al., 2023).\nIt is well known that the standard RL objective can be written as a linear functional of the occupancy\nmeasure. To capture all the aforementioned applications, the RLGU objective is a possibly nonlinear\nfunctional of the state action occupancy measure induced by the policy (Zhang et al., 2022). Due to non-\nlinearity, policy gradient algorithms for solving RLGU problems face the major bottleneck of occupancy\nmeasure estimation. Prior works (Hazan et al., 2019; Zhang et al., 2022) have focused on the tabular\nsetting where the state action occupancy measure needs to be estimated for each state action pair using\nMonte Carlo estimation via sampling trajectories. However, this setting is restrictive for larger state and\nactions spaces where tabular methods will become intractable due to the curse of dimensionality. This\nscalability issue stands as an important challenge to overcome to establish RLGU as a general unified\nframework for which efficient algorithms exist to solve its larger state space instances. We refer\nthe reader to Figure 1 for an illustration of the challenge motivating our work. Our goal is to address"}, {"title": "2 Problem Formulation", "content": "MDP with General Utility. Consider a discrete-time discounted Markov Decision Process (MDP)\n(S, A, P, F, \u03c1, \u03b3), where S and A are finite state and action spaces respectively, P : S \u00d7 A \u2192 A(S) is the\nstate transition probability kernel, F : M(X) \u2192 R is a general utility function defined over the space\nof measures M(X) on the product state-action space X := S \u00d7 A, \u03c1 is the initial state distribution,\nand \u03b3\u2208 (0,1) is the discount factor. A stationary policy \u03c0 : S \u2192 \u2206(A) maps each states \u2208 S to a\ndistribution \u03c0(\u00b7|s) over the action space A. The set of all stationary policies is denoted by \u03a0. At each\ntime step t\u2208 N in a state st \u2208 S, the RL agent chooses an action at \u2208 A with probability \u03c0(at|st)\nand then environment transitions to a state st+1 \u2208 S with probability P(st+1|st, at) . We denote by P\u03c1,\u03c0\nthe probability distribution of the Markov chain (st, at)t\u2208N induced by the policy with initial state\ndistribution \u03c1. We use the notation \u0395\u03c1,\u03c0 (or often simply E) for the associated expectation. We define for\nany policy \u03c0\u2208 II the (normalized) state and state-action occupancy measures d\u03c0 \u2208 M(S), \u03bb\u03c0 \u2208 M(S\u00d7A)\nrespectively as:\n$d^{\\pi}(s) := (1 - \\gamma) \\sum_{t=0}^{+ \\infty} \\gamma^t P_{\\rho,\\pi}(S_t = s); \\quad  \\lambda^{\\pi}(s,a) := d^{\\pi}(s) \\pi(a|s).$\n(1)\nThe general utility function F assigns a real to each occupancy measure \u03bb\u03c0 induced by a policy \u03c0\u2208 II.\nWe note that \u03bb\u03c0 will also be seen as a vector of the Euclidean space R|S|\u00b7|A|. In the rest of this work,\nwe will consider a class of policies parameterized by a vector \u03b8\u2208 Rd for some fixed integer d\u2208 N. We\nshall denote by \u03c0\u03b8 \u2208 II such a policy in this class.\nPolicy optimization. The goal of the RL agent is to find a policy \u03c0\u03b8 solving the problem:\n$\\max_{\\theta \\in \\mathbb{R}^d} F(\\lambda^{\\pi_{\\theta}}),$\n(2)\nwhere \u03bb\u03c0 is defined in (1), F is a smooth function supposed to be upper bounded and F\u2217 is used to denote\nthe maximum in (2). The agent has access to trajectories of finite length H generated from the MDP\nunder the initial distribution \u03c1 and the policy \u03c0\u03b8. In particular, provided a time horizon H and a policy \u03c0\u03b8\nwith \u03b8 \u2208 Rd, the learning agent can simulate a trajectory \u03c4 = (s0,a0,\u00b7\u00b7\u00b7, sH\u22121,aH\u22121) from the MDP\nwhen the state transition kernel P is unknown. This general utility problem was described, for instance,\nin Zhang et al. (2021) (see also Kumar et al. (2022)). Recall that the standard RL problem corresponds\nto the particular case where the general utility function is a linear function, i.e., F(\u03bb\u03c0) = \u27e8r, \u03bb\u03c0\u27e9 for\nsome vector r\u2208 R|S|\u00b7|A|, in which case we recover the expected return function as an objective:\n$V^{\\pi_\\theta}(r) := E_{\\rho, \\pi_\\theta} \\Big[\\sum_{t=0}^{+\\infty} \\gamma^t r(s_t, a_t)\\Big].$\n(3)\nExamples. We provide two motivating examples of the RLGU framework as follows."}, {"title": "3 Policy Gradient Algorithm with Occupancy Measure Approx- imation (PG-OMA)", "content": "In this section, we propose a policy gradient algorithm to solve the policy optimization problem (2)\nwith general utilities for larger state-action spaces. We start by elaborating on the challenges faced to\nsolve such a large-scale problem. Section 3.1 mainly contains known material from the recent literature\n(Zhang et al., 2021), we report it here separately from the problem formulation in section 2 to motivate\nour algorithmic design. The rest of the section presents our algorithmic contributions."}, {"title": "3.1 Policy Gradient Theorem and Challenges for Large-scale RLGU", "content": "Policy Gradient for RLGU. Following the exposition in Zhang et al. (2021); Barakat et al. (2023),\nwe derive the policy gradient for the general utility objective. For convenience, we use the notation \u03bb(\u03b8)\nfor \u03bb\u03c0\u03b8. Since the cumulative reward can be rewritten more compactly V\u03c0\u03bf (r) = \u27e8\u03bb\u03c0\u03b8 ,r\u27e9, it follows from\nthe policy gradient theorem that:\n$\\big[\\nabla_{\\theta} V^{\\pi_{\\theta}}(r)\\big]_s = \\nabla_\\theta \\Big(\\sum_{t=0}^{\\infty} \\gamma^t \\sum_{s'}{P_{\\rho,\\pi_{\\theta}}(s_t = s, a_t)log(\\pi_{\\theta}(a_t|s_t))}\\Big)$\n(4)\nwhere Ve(0) is the Jacobian matrix of the vector mapping \u03bb(\u03b8). Using the chain rule, we have\n$\\nabla_{\\theta}F(\\lambda^{\\pi_{\\theta}}(\\theta)) = [\\nabla_\\theta \\lambda^{\\pi_{\\theta}}(\\theta)]^T \\nabla_{\\lambda} F(\\lambda^{\\pi_{\\theta}}(\\theta)) = \\nabla_{\\theta}V^{\\pi_{\\theta}}(r)|_{r = \\nabla_{\\lambda} F(\\lambda^{\\pi_{\\theta}})}.$\n(5)\nThe classical policy gradient in the standard RL setting uses rewards which are obtained via interaction\nwith the environment. In RLGU, there is no reward function but rather a pseudoreward \u2207\u03bbF(\u03bb(\u03b8))\ndepending on the unknown occupancy measure induced by the policy.\nStochastic Policy Gradient. In view of performing a stochastic policy gradient algorithm, we would\nlike to estimate the policy gradient VeF(\u03bb(\u03b8)) in (5). We can use the standard reinforce estimator\nsuggested by Eq. (4). Define for every reward function r (which is also seen as a vector in R|S|\u00d7|A|),\nevery \u03b8\u2208 Rd and every H-length trajectory \u03c4 simulated from the MDP with policy \u03c0\u03b8 and initial\ndistribution \u03c1 the (truncated) policy gradient estimate:\n$g(\\theta, r, \\tau) =  \\Big( \\sum_{t=0}^{H-1} r(s_t, a_t) \\Big)  \\Big(  \\sum_{h=t}^{H-1}  \\nabla_{\\theta} \\log \\pi_{\\theta}(a_h|s_h) \\Big).$\n(6)\nGiven (5), we also need to estimate the state-action occupancy measure \u03bb(\u03b8) (when F is nonlinear)\u00b9. \nPrior work has exclusively focused on the tabular setting using a Monte-Carlo estimate of this occu-"}, {"title": "3.2 Occupancy Measure Estimation", "content": "In this section, we address the challenge of occupancy measure estimation in large state action spaces.\nGiven a policy \u03c0\u03b8, our goal is to estimate the unknown occupancy measure d\u03c0\u03b8 induced by this policy\nusing state samples obtained from executing the policy. Since the normalized occupancy measure is a\nprobability distribution, we propose to perform maximum likelihood estimation. Before presenting this\nprocedure, we elaborate on the motivation behind approximating the occupancy measure by a parame-\nterized distribution in a given function class of neural networks for example.\nMotivation. Besides the practical motivation of using distribution approximation to scale to larger\nstate-action space settings, we provide some theoretical motivation. Recall that action-value functions\nare linear in the feature map for linear (or low-rank) MDPs for solving standard cumulative sum RL\nproblems (see Proposition 2.3 in Jin et al. (2019)). Similarly, it turns out that state-occupancy measures\nare linear (or affine in the discounted setting) in density features in low-rank MDPs. We refer the reader\nto Appendix B for a proof of this statement (see also Lemma 16, 17 in Huang et al. (2023)). Therefore, in\nthis case, it is natural to approximate occupancy measures via linear function approximation using some\ndensity features. More generally, for an arbitrary MDP, we propose to approximate the (normalized)\nstate occupancy measure d\u03c0\u03b8 induced by a policy \u03c0\u03b8 directly by a probability distribution in a certain\nparametric class of probability distributions:\n$\\Lambda := \\{\\rho_\\omega \\in \\Delta(S) | \\omega \\in \\Omega \\subseteq \\mathbb{R}^m \\},$\n(7)\nwhere for instance m\u226a |S|. An example of such a parametrization for a given \u03c9\u2208 Rm is the soft-\nmax \u03c3\u03c9 defined over the state space by \u03c3\u03c9(s) := exp(\u03c8\u03c9(s))/Z(\u03c9), where Z(\u03c9) := \u2211s'\u2208S exp(\u03c8\u03c9(s'))\nand where \u03c8\u03c9 : S \u2192 R is a given mapping which can be a neural network in practice. For continuous\nstate spaces, practitioners can consider for instance Gaussian mixture models with means and covariance\nmatrices encoded by trainable neural networks.\nMaximum Likelihood Estimation (MLE). For simplicity, we suppose we have access to i.i.d. state\nsamples following the distribution d\u03c0\u03b8 throughout our exposition. We refer the reader to Appendix D.1\nfor a discussion about how to sample such states. Given the parametric distribution class \u039b defined in (7)\nand a data set D := {si}i=1,\u2026,n \u2208 Sn of n i.i.d. state samples following the distribution d\u03c0\u03b8 induced by"}, {"title": "3.3 Proposed Algorithm", "content": "Based on our discussion in sections 3.1 and 3.2, we propose a simple stochastic policy gradient algorithm\nwhich consists of two main steps:\n(i) Compute an approximation of the unknown state-action occupancy measure \u03bb\u03c0\u03b8 \u2208 R|S|\u00d7|A| for a\nfixed parameter \u03b8\u2208 Rd with MLE using collected state samples (see (8));\n(ii) Perform stochastic policy gradient ascent using the stochastic policy gradient defined in (6) using\nthe estimated occupancy measure computed in the first step.\nThe resulting algorithm is Algorithm 1 which is model-free as we do not estimate the transition kernel."}, {"title": "4 Convergence and Sample Complexity Analysis", "content": "In this section, we suppose we are given a data set of i.i.d. state-action pair samples following the\n(normalized) occupancy measure \u03bb\u03c0 induced by a fixed given policy \u03c0. As previously explained, we\napproximate \u03bb\u03c0 by a function (or parameterized density) in the function class \u039b defined in (7). We make\nthe following assumption to control the complexity of our function approximation class."}, {"title": "4.1 Statistical Complexity of Occupancy Measure Estimation", "content": "Assumption 4 (Function approximation class regularity). The following holds true:\n(i) (parameter compactness) The set \u03a9 is compact, we denote by B\u03c9 := max\u03c9\u2208\u03a9 ||\u03c9||\u221e;\n(ii) (realizability) The (normalized) occupancy measure to be estimated satisfies: \u03bb\u03c0 \u2208 \u039b;\n(iii) (Lipschitzness) \u2200\u03c9,\u03c9\u0303 \u2208 N,\u2200x \u2208 X,\u2203L(x) \u2208 R s.t. |\u03c1\u03c9(x) \u2013 \u03c1\u03c9\u0303(x)| \u2264 L(x)||\u03c9 \u2013 \u03c9\u0303||\u221e with\nBL := \u222bx L(x)dx < +\u221e.\nAssumption 4 is satisfied for instance for the class of generalized linear models, i.e. \u039b := {pw(x) =\ng(\u03c9\u03c6(x)), \u2200x \u2208 X : \u03c1\u03c9 \u2208 \u0394(X),\u03c9 \u2208 \u03a9} where g : R \u2192 [0,1] is an increasing Lipschitz continuous\nfunction and \u03c6: X \u2192 Rd is a given feature map s.t. \u222b ||\u03c6(x)||1dx < BL for some BL > 0. Notice that\nfeatures can be normalized appropriately to satisfy the assumption. A similar assumption has been made\nin the case of linear MDPs in (Huang et al., 2023, Assumption 1). The realizability assumption can be\nrelaxed at the price of incurring a misspecification error.\nWe now state our sample complexity result for occupancy measure estimation via MLE in view of\nour PG sample complexity analysis. This result relies on arguments developed in the statistics literature\nVan de Geer (2000); Zhang (2006). These techniques were adapted to the RL setting for low-rank MDPs\nin e.g. Agarwal et al. (2020). Our proof builds on Huang et al. (2023) which we slightly adapt for our\npurpose (see Appendix D.2).\nProposition 5. Let Assumption 4 hold true. Then for any \u03b4 > 0, the MLE \u03bb\u0302\u03c0 defined using (8)\nsatisfies with probability at least 1 \u2212 \u03b4,\n$\\|\\lambda^{\\pi_{\\theta}} - \\lambda^{*_{\\theta}}\\|_{1} \\le 61 \\sqrt{\\frac{12 \\cdot m \\cdot log\\Big(\\frac{2[B_\\omega B_L n]}{\\delta}\\Big)}{n}}.$\nThe above result translates into a sample complexity of O\u0303(m\u03b5\u22122) to guarantee an \u03b5-approximation\nof the true occupancy measure (in the l\u2081-norm distance) using samples. We highlight that our sample\ncomplexity only depends on the dimension m of the parameter space and does not scale with the size\nof the state-action space. Hence the MLE procedure we use is the key ingredient to scale our algorithm\nto large state-action spaces. To the best of our knowledge, existing algorithms for solving the RLGU\nproblem (with nonlinear utility functions) are limited to the restrictive tabular setting."}, {"title": "4.2 Guarantees for Policy Gradient with Occupancy Measure Approxima- tion", "content": "In this section, we establish sample complexity guarantees for Algorithm 1. We start by introducing the\nassumptions required for our results and discuss their relevance.\nAssumption 6 (Policy parametrization). The following holds for every (s, a) \u2208 S \u00d7 A. For every \u03b8 \u2208\nRd, \u03c0\u03b8(a|s) > 0. Moreover, the function \u03b8 \u2192 \u03c0\u03b8(a|s) is continuously differentiable and the score\nfunction \u03b8 \u2192 \u2207log \u03c0\u03b8(a|s) is bounded by some positive constant B.\nThis standard assumption is satisfied for instance by the common softmax policy parametrization\ndefined for every \u03b8\u2208 Rd, (s, a) \u2208 S \u00d7 A by \u03c0\u03b8(a|s) = , where \u03c8 : S \u00d7 A \u00d7 Rd \u2192 R is\na smooth function such that the map (s, a; \u00b7) is twice continuously differentiable for every (s,a) \u2208\nS\u00d7A and for which there exist ly, Ly > 0 s.t. (i) maxs\u2208S,a\u2208 A supe ||\u2207\u03c8(s,a;0)|| \u2264 1y and (ii)\nmaxses,a\u2208 A supe ||\u2207\u00b2\u03c8(s, a; 0) || \u2264 L\u03c8.\nAssumption 7 (General utility smoothness). There exist constants lx, Lx > 0 s.t. for all \u03bb1, \u03bb2 \u2208 \u039b,\n||\u2207\u03bbF(\u03bb1)||2 \u2264 lx and ||\u2207\u03bbF(\u03bb1) \u2212 \u2207\u03bbF(\u03bb2)||2 \u2264 Lx||\u03bb1 \u2212 \u03bb2||2.\nUnder Assumptions 6 and 7, the function \u03b8 \u2192 F(\u03bb\u03c0\u03b8) is Le-smooth (see Lemma 21 for the ex-\npression). Using this property, the next result shows that our algorithm enjoys a first-order stationary\nguarantee in terms of the non-convex general utility objective."}, {"title": "5 Proof of Concept Experiments", "content": "In this section, we investigate the capability of the proposed PG-OMA in terms of scaling with\nrespect to the dimensionality of the state space when solving RLGU problems. In this work, we perform\ninitial proof of concept experiments on simulation environments such as MPE (Multi-Agent Particle\nEnvironment (Lowe et al., 2017)) and SMAC (StarCraft Multi-Agent Challenge (Samvelyan et al., 2019)).\nWe provide additional details about the experiments in Appendix E. We consider the problem of learning\nfrom demonstrations as defined in Example 2 in Sec. 2 and show results in discrete and continuous state\nspace settings. Before presenting our experimental results, we want to emphasize that our experiments\nserve as evidence of the potential of the proposed approach in addressing scalability challenges in RLGU.\nWe do not claim to surpass the state-of-the-art performance in solving specific tasks (of learning from\ndemonstration) within the MPE and SMAC environments. In contrast to prior work which mostly\ndesigned tailored algorithms for specific single tasks, note that our algorithm can be used for any RLGU\nproblem."}, {"title": "6 Conclusion", "content": "In this paper, we proposed a simple policy gradient algorithm for RLGU to address the fundamental\nchallenge of scaling to larger state-action spaces beyond the tabular setting. Our approach hinges on using\nMLE for approximating occupancy measures to construct a stochastic policy gradient. We proved that\nour MLE procedure enjoys a sample complexity which only scales with the dimension of the parameters\nin our function approximation class rather than the size of the state-action space which might even be\ncontinuous. Under suitable assumptions, we also provided convergence guarantees for our algorithm\nto first-order stationarity and global optimality respectively. We hope this work will stimulate further\nresearch in view of designing efficient and scalable algorithms for solving real-world problems."}]}