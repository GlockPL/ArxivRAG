{"title": "GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting", "authors": ["Qianpu Sun", "Changyong Shu", "Sifan Zhou", "Zichen Yu", "Yan Chen", "Dawei Yang", "Yuan Chun"], "abstract": "3D occupancy perception is gaining increasing attention due to its capability to offer detailed and precise environment representations. Previous weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU varying by 5-10 points due to sampling count along camera rays. Recently, real-time Gaussian splatting has gained widespread popularity in 3D reconstruction, and the occupancy prediction task can also be viewed as a reconstruction task. Consequently, we propose GSRender, which naturally employs 3D Gaussian Splatting for occupancy prediction, simplifying the sampling process. In addition, the limitations of 2D supervision result in duplicate predictions along the same camera ray. We implemented the Ray Compensation (RC) module, which mitigates this issue by compensating for features from adjacent frames. Finally, we redesigned the loss to eliminate the impact of dynamic objects from adjacent frames. Extensive experiments demonstrate that our approach achieves SOTA(state-of-the-art) results in RayIoU (+6.0), while narrowing the gap with 3D supervision methods. Code will be released soon.", "sections": [{"title": "1. Introduction", "content": "Occupancy perception[48], a key focus within the rapidly advancing field of autonomous driving perception, has garnered significant attention from both industry and academia. This task models the entire 3D scene as a collection of grid cells, each assigned relevant attributes such as semantics and motion flow. Compared to traditional 3D object detection [20, 37, 54] and BEV(Bird's Eye View) perception[12, 32], 3D occupancy perception provides a more fine-grained understanding of the environment and compensates for the inability of BEV space to capture height information. Researchers in this field are actively working to develop solutions that are both efficient and capable of real-time processing.\nMost existing approaches [14, 16, 21, 22, 26, 28, 41, 51] heavily rely on BEV perception with their performance closely tied to the accuracy of 3D labels. However, as noted in RenderOcc[35], purifying around 30,000 frames of 3D occupancy labels requires about 4,000 human hours. Additionally, inconsistencies in 3D label generation across different benchmarks further hinder the practical application of 3D occupancy prediction in real-world scenarios, leading to unpredictable model performance across various environments. Consequently, the question of how to diminish or even eradicate the model's reliance on 3D ground truth data stands as a pivotal and urgent challenge that demands our profound contemplation, especially considering the growing demand for advanced 3D perception in technologies like autonomous driving.\nRenderOcc[35] represents a pioneering approach by leveraging solely 2D labels to supervise occupancy prediction tasks, effectively bypassing the limitations associated with 3D labeling constraints. Even though the method is relatively superior, there are still two issues that require our attention, as shown in Figure 2. (a) Sampling Trade-Off: NeRF-based methods require sampling 3D points along camera rays, but balancing this is tricky. Oversampling creates computational strain, while undersampling can miss key features. This intricate trade-off could result in a fluctuation of accuracy within the range of 5 to 10 percentage points. (b) Duplicated Predictions: Relying on 2D data for supervision often leads to inaccurate depth predictions and duplicate predictions along camera rays. Such duplications hacks the mIoU metric and thereby diminish the practical utility of 3D occupancy prediction when applied to downstream planning and control tasks.\nTo address these challenges, we introduce GSRender, an end-to-end network that leverages 3D Gaussian Splatting for rendering [19]. 3D Gaussian Splatting enables real-time, high-quality rendering of photorealistic scenes from a set of images, bypassing the need for complex and uncertain sampling processes. The comparison results, as shown in Figure 1, demonstrate the effectiveness of our approach. Specifically, we incorporate auxiliary supervision from adjacent frames to address the issue of duplicate predictions. Furthermore, to minimize the potential negative impact of dynamic objects on the rendered scenes, we apply differentiated loss weights for static and dynamic objects, achieve significant performance improvements.\nWe conduct experiments using renowned datasets Nuscenes[3]. Beyond the conventional mIoU metric[30], we also employed RayIoU[42], a novel metric designed to mitigate the influence of thick surfaces. Impressively, our method achieved SOTA results in RayIoU under the category of 2D weakly-supervised methods. Specifically, we summarize our contributions as follows:\n1. GSRender is the first to conduct a detailed analysis 3D Gaussian Splatting for weakly supervised 3D occupancy prediction, enhancing spatial occupancy accuracy in dynamic environments without complex sampling.\n2. We introduce the Ray Compensation module which is easily integrable to effectively mitigate duplicate predictions along identical rays and develop a differentiated loss function tailored for dynamic and static objects in the auxiliary frames.\n3. We achieved SOTA performance in RayIoU metrics among all 2D weakly supervised methods and significantly reduced the performance gap compared to methods utilizing 3D supervision."}, {"title": "2. Related work", "content": "Monocular depth estimation[2, 34] is fundamental to all tasks in 3D perception, as inaccuracies can critically undermine the overall accuracy of these systems. The challenge of deriving depth from images in vision-centric 3D perception often results in duplicate predictions and false positive detections along camera rays. Although recent studies have resolved this issue within the domain of 3D object detection[6, 15, 24, 27], the problem of duplicate predictions still persists in more fine-grained tasks, particularly in 3D occupancy prediction with 2D weakly supervision. Specifically, features along the same ray are typically supervised by a single pixel, which results in all voxels along the ray being assigned the same semantic label. Our approach tackles occlusions in the current frame by integrating perspectives from adjacent frames, achieving superior performance over RenderOcc with just one additional frame"}, {"title": "2.2. Occupancy prediction", "content": "Since 2022 Tesla AI Day[1], the field of occupancy prediction has seen heightened interest. Mainstream perception models predominantly rely on BEV (Bird's Eye View) perception, they generally adhere to two main paradigms. The first paradigm employs the LSS (Lift-Splat-Shoot) method [15, 21, 22, 36, 51], which predicts depth and unifies features within the ego-vehicle's coordinate system. Similarly, the second Transformer-based[44] paradigm [23, 25, 46, 53] utilizes deformable cross-attention [55] to incrementally enhance BEV grid features derived from 2D image features. Existing methods primarily utilize dense features, even though most of the actual space is free. Recent innovative methods have effectively sparsified the feature space with notable results[18, 39, 41, 42, 45]. Moreover, employing 2D supervision for 3D occupancy prediction has gained popularity. RenderOcc[35], for example, uses a NeRF-based approach to bridge semantic and density fields with 2D ground truth. OccNeRF[52] and SelfOcc[17] leverage self-supervision to generate 2D ground truth."}, {"title": "2.3. 3D Gaussian Splatting", "content": "3D Gaussian Splatting[19] has been widely explored and applied in various fields [5, 11, 31, 38, 40, 47, 49, 50], offering real-time capabilities for large-scale scenes reconstruction. It significantly outperforms NeRF in terms of real-time performance and rendering quality, representing a substantial advancement. Essentially, 3D occupancy prediction is a reconstruction task where vision-centric images serve as inputs and the outputs are coarse-grained models of the scene. For such tasks, adopting a reconstruction-based approach may provide a more intuitive and effective solution. Recent efforts have increasingly treated large-scale outdoor scenes as collections of Gaussians. For instance, GaussianFormer[18] enhances each Gaussian's properties using a transformer combined with the Gaussian-to-Voxel. GaussianBEV[4] replaces the LSS method with Gaussian Splatting to acquire BEV features, while GaussianOcc[10] implements a two-stage approach that integrates cross-view information from adjacent views to minimize sensitivity to camera poses. In our research, we consider each voxel as a Gaussian and similarly project the 3D Gaussian onto 2D for extensive scene reconstruction."}, {"title": "3. Method", "content": null}, {"title": "3.1. Preliminary", "content": "3D Gaussian Splatting[19] is a highly popular method in the field of 3D reconstruction, used for real-time reconstruction of both single objects and large-scale scenes. It conceptualizes the entire scene as a collection of Gaussian. Each Gaussian encompasses attributes such as the mean and covariance of the Gaussian distribution, along with opacity and color. The Gaussian probability for each spatial point is calculated as follows:\n$G(x) = e^{-(X - \\mu)^T \\Sigma^{-1} (X - \\mu)},$ (1)\nwhere u represents the position of each Gaussian, while the covariance matrix \u2211 to determine the size and orientation of the Gaussian distribution. Specifically, the covariance matrix can be computed using rotation R and scale S matrices by:\n$\\Sigma = RSS^T R^T$. (2)\nFor independent optimization, store the scale as a 3D vector $s \\in R^3$ and the rotation as a quaternion $q \\in R^4$. Then, the 3D Gaussians are projected onto the 2D imaging plane using transformation matrices W and Jacobian matrices J. The covariance matrix of the 2D Gaussian is computed as follows:\n$\\Sigma' = J W \\Sigma W^T J^T$. (3)\nFinally, standard alpha-blending is utilized to render the rgb colors of all Gaussians onto the image based on their opacity, and the loss is computed against the image. The color of each pixel is calculated as follows:\n$c(x) = \\sum_{k=1}^{K} c_k a_k G^{2D}(x) \\prod_{j=1}^{k-1} (1 - a_j G^{2D}(x))$. (4)\nwhere $a_k$ represents the opacity of the kth Gaussian, $G^{2D}(x)$ is the Gaussian probability at pixel position x, and $c_k$ denotes the color of the k-th Gaussian, represented by spherical harmonic coefficients[8]. K denotes the number of Gaussians contributing to pixel x.\nProblem Defination. we aim to construct a comprehensive 3D occupancy representation of the scene using images captured from surrounding viewpoints. Specifically, for the vehicle at timestamp t, we take N surrounding images $I = {I_1, I_2, . . . , I_v }$ as input and predict the 3D occupancy grid $V \\in R^{H_v \\times W_v \\times Z_v \\times L}$, where $H_v$, $W_v$, and $Z_v$ denote the resolution of the voxel space, and L represents the number of categories, including free category for unoccupied space. The formulation for this 3D prediction task is articulated as follows:\n$P = H(I_1, I_2, ..., I_N), V = F(P),$ (5)\nSpecifically, H generates an occupancy feature $P \\in R^{H_v \\times W_v \\times Z_v \\times C}$, where C denote the feature dimension, capturing spatial structure and temporal relationships within the scene. Following this, the function F as a head that takes the scene feature representation P as inputs and outputs the final prediction V. In GSRender We replace the nerfhead in RenderOcc with gaussianhead, implementing a more efficient method."}, {"title": "3.2. Overall Framework", "content": "Figure 3 illustrates the overall framework of our network architecture. We conceptualize the entire scene as an assembly of numerous Gaussian. In the feature extraction stage, to ensure a fair comparison with RenderOcc, we adopt the same structure. In practical tasks, the feature extraction network architecture can be switched freely to other backbones. In Sec. 3.3, we treat each voxel center as the center of a Gaussian and predict the offset of their properties within the Gaussian. Next, we render the semantics logits and depth using 3D Gaussian splatting and optimize the network with 2D supervision. Finally, in Sec. 3.4, we introduce a multi-frame calibration module aimed at reducing false positive detections caused by duplicate predictions."}, {"title": "3.3. Gaussian Render Module", "content": "Gaussian Properties Field. We treat each voxel center as the initial point for a Gaussian distribution, as shown in Figure 4, For a given occupancy feature V, In addition to predicting the semantic and opacity attributes, we initialize the Gaussian's mean and scale, and predict offsets for each properties, excluding rotation. In structured voxel spaces, we contend that rotating Gaussian is unnecessary. Instead, adjusting scale suffices to meet the spatial requirements of the entire voxel space. The regular arrangement of voxels in a uniformly distributed and structured environment already provides sufficient geometric and spatial information, rendering rotational adjustments redundant. Our ablation studies have confirmed this approach. The resulting Gaussian properties $G \\in R^{N \\times (7+(L-1))}$, where N denotes the total number of Gaussian. For each Gaussian, we have\n$G(i) = (\\mu + \\delta \\mu, s + \\delta s, r, o, c)$. (6)\nDuring the testing phase, we filter out Gaussian with opacity greater as our predicted results. Specifically\n$V(\\mu) =\n\\begin{cases}\narg \\max(C(\\mu)) & \\text{if } o(\\mu) \\geq \\tau \\\\\n\\text{empty label} & \\text{if } o(\\mu) < \\tau,\n\\end{cases}$ (7)\nwhere \\tau is the opacity threshold for determining whether a Gaussian is considered empty. Since empty voxels are filtered out based on threshold, the actual number of predicted semantic categories is L 1, excluding the 'free' category. Visualization analysis shows that the majority of effective Gaussians closely align with their voxel centers. Thus, we apply the Gaussian semantics directly to the voxels.\nGaussian Rasterizer, in contrast to NeRF[33], obviates the labor-intensive sampling process. It directly renders the Gaussian to the image. We interpret the semantic logits associated with each Gaussian as colors, and apply alpha-blending to render the semantic logits and depth for each pixel. This approach culminates in the generation of a semantic map S and a depth map D."}, {"title": "3.4. Ray Compensation Module", "content": "RenderOcc[35] introduces Auxiliary Rays to maintain spatial consistency across various viewpoints. Nevertheless, it fails to resolve the duplicated predictions along these rays. We introduce the Ray Compensation module, which provides multiple perspectives for each object at the feature level, significantly reducing the problem of repeated predictions. Specifically as shown in Figure 5. The upper part of figure illustrates the principle of our method. For the current frame t, trees that are obscured might be revealed in adjacent frames from the different viewpoints. To obtain features from adjacent time steps, we first assume that the current Occupancy Feature is denoted as $P_{curr} \\in R^{H_v \\times W_v \\times D_v \\times C}$. We treat the centers of voxels as homogeneous coordinates $O_{curr-ego} \\in R^{H_v \\times W_v \\times D_v \\times 4}$, representing the voxel centers in the ego-car coordinate system at the current time. The position of the adjacent time step in the current time step's coordinate system is computed as follows:\n$O_{adj}^{curr\\_ego} = T_{adj}^{curr\\_ego} O_{adj\\_ego}^{adj}.$ (8)\nwhere $T_{adj}^{curr-ego} \\in R^{4 \\times 4}$ is the transformation matrix from the ego-car coordinate system of the adjacent frame to that of the current frame. The features of adjacent frames are mathematically represented by the following formula:\n$P_{t+k}^{adj} = S(P_{curr}, O_{adj}^{curr-ego}), \\\\ \\text{ for } k \\in {-\\frac{T_m}{2}, ..., -1,1, ..., \\frac{T_m}{2}}$. (9)\nHere, S denotes the sampling function, where we apply bilinear interpolation, and Tm represents the total number of adjacent frames selected. Finally, the shared gaussian head predicts the Gaussian properties for the adjacent frame.\nDynamic Object. It is noteworthy that this approach may still encounter issues, as illustrated in the lower part of Figure5. For trees obscured in the current frame, they may still be blocked by other moving objects in adjacent frames. In extreme cases, these obstructions might occur at every time step. Therefore, we need to minimize the influence of dynamic objects in adjacent frames. This is achieved by applying a weighting factor, alpha, when calculating the loss for adjacent frames, as detailed in the following subsection."}, {"title": "3.5. Loss Function", "content": "To maintain consistency with RenderOcc, We also optimize the Gaussian properties using a class-balanced cross-entropy loss $C^{seg}$ for occupancy prediction and the SILog loss $L_{depth}$ [7] for depth estimation. For the current frame, we compute the loss similarly to RenderOcc as follows:\n$L_{curr} = L^{seg}_{curr} + L^{depth}_{curr},$ (10)\nFor adjacent frames, the inability to estimate the motion trajectories of moving objects can negatively impact network optimization. Therefore, a lower weight is assigned to dynamic objects. The adjacent loss can be formulated as follows:\n$L_{adj} = L_{adj}^{seg} + L_{adj}^{depth}$ (11)\nThe segmentation loss $L_{adj}^{seg}$ and depth loss $L_{adj}^{depth}$ are defined as follows:\n$L_{adj}^{seg} =  -\\frac{V}{n} \\sum_{i=1}^{n} \\alpha_s(i) \\beta_s(i) S_{pix}(i) \\log(\\hat{S}_{pix}(i)),$ (12)\n$\\begin{aligned}\nL_{adj}^{depth} = \\sqrt{\\frac{1}{n^2} \\sum_{i=1}^n d_i^2 - \\frac{1}{n^3} (\\sum_{i=1}^n d_i)^2}\n\\end{aligned},$ (13)\nwhere $d_i = \\log D_i - \\log \\hat{D}_i$ denotes the difference between predicted and ground truth depth at pixel i, while s(i) is the ground truth semantic category. $\\alpha_s(i)$ and $\\beta_s(i)$ represent the dynamic weight and category balance weight for s(i), respectively. Consequently, our final loss function can be expressed as:\n$L = L_{curr} + \\sum_{k} w_k L_{adj,k}$ (14)\nwhere w represents the weight assigned to adjacent frames. Additionally, although we also considered using the distortion loss proposed in 2D Gaussian Splatting [13], we found it to be less effective and decided not to include it."}, {"title": "4. Experiments", "content": "Since the surround-view camera setup allows for better interaction between features from different perspectives and for a fair comparison with RenderOcc, we evaluated our method, GSRender, on the same datasets: NuScenes. Furthermore, we adopted a new evaluation metric, RayIoU[42], to highlight the effectiveness of our multi-frame compensation. Lastly, to gain deeper insights into GSRender, we conducted additional ablation studies and analysis on the NuScenes dataset."}, {"title": "4.1. Experimental Setup", "content": "Dataset. All of our experiments were conducted on the public dataset: NuScenes[3] and corresponding occupancy dataset, OCC3D-NuScenes[43], contains 600 scenes for training, 150 scenes for validation, covers a region centered around the ego-vehicle with dimensions [-40m, -40m, -1m, 40m, 40m, 5.4m] and a voxel size of [0.4m, 0.4m, 0.4m]. This dataset contains 18 classes, where classes 0 to 16 follow the same definitions as the nuScenes-lidarseg dataset, and class 17 represents free space. Since dataset does not provides 2D ground truth (GT), we followed the approach used in RenderOcc[35] to project the semantic and depth labels of 3D LiDAR points onto the 2D plane. This ensures a fair comparison with RenderOcc.\nImplementation details. We implemented our model using PyTorch. We used off-the-shelf architecture Swin Transformer[29] as the image backbone with an input resolution of 512\u00d71408 and BEVStereo [21] for extracting 3D features. For the gaussian head, we simply employed a two-layer fully connected network. For the RC module, we used only one future frame as the basic experimental configuration for all experiments. During the training phase, we utilized the Adam optimizer with a linear learning rate scheduling strategy, set the batch size to 16, and trained for 12 epochs with a learning rate of 2e-4. All experiments were conducted on NVIDIA A6000 GPUs.\nEvaluation metrics. SparseOcc [42] notes that overestimation can easily compromise the mIoU metric and introduces RayIoU to simulate LiDAR, enhancing scene completion evaluation through equal-distance resampling and temporal casting. Since the query ray measures distance based only on its first voxel contact, the model struggles to achieve a higher IoU with thick surfaces. Therefore, we utilized both the traditional mIoU metric and the newly proposed RayIoU metric to evaluate our method."}, {"title": "4.2. Performance", "content": "We report the performance of GSRender in this part, demonstrating its superior results compared to the existing SOTA method, RenderOcc. We achieved significant performance improvements in RayIoU. Building on these en-"}, {"title": "4.2.1. Quantitative Comparison.", "content": "As illustrated in Table 1, We adhered to the original experimental setup of RenderOcc, conducting a comparative analysis between RenderOcc and GSRender under the condition of only using the current frame for supervision. The objective was to explore the performance of both methods when limited to a single-frame scenario, using mIoU as the evaluation metric. GSRender outperformed RenderOcc, achieving a higher mIoU (19.33 \u2192 21.36), and enhanced performance when used as auxiliary supervision alongside 3D supervision, improving from 26.11 to 29.56.\nWhen supervised only by the current frame, the inherent limitations of 2D supervision cause both RenderOcc and GSRender to produce a thick and cracked surface. Consequently, relying solely on mIoU does not accurately reflect the model's performance. To truly assess the capabilities of our model, we adopted RayIoU[42] as our primary evaluation metric to better evaluate the model's true capability in scene reconstruction. As shown in Table 2, Notably, by utilizing only one frame for auxiliary supervision, our results showed a significant improvement over RenderOcc, which uses six auxiliary frames (19.5 \u2192 25.5), even surpasses certain approaches that use 3D occupancy ground truth, such as SimpleOcc (22.5 \u2192 25.5)."}, {"title": "4.2.2. Qualitative Comparison.", "content": "As illustrated in Figure 6, our experimental results demonstrate that the RC module significantly enhances the accuracy in positioning and shaping of specific categories, such as vehicles. In the third column, We visualized the results without using the RC module. This comparison clearly shows that reliance on single-frame ground truths tends to result in duplicated predictions. However, the integration of the RC module effectively addresses this problem, leading to a more unified and seamless reconstruction of the entire scene, enhanced the practicality of the prediction results."}, {"title": "4.2.3. Ablations and Analysis", "content": "In our initial experiments, we postulated that using fixed Gaussian properties could also yield satisfactory rendering results. Nonetheless, this does not hold true. Therefore, we conducted ablation studies to verify the necessity of shifts in each Gaussian attribute, as shown in Table 4, indicate that Gaussian trained without(w/o) $ \\delta \\mu$, $ \\delta s$ and with(w) $ \\delta$ exhibit decreased accuracy. Furthermore, in Table 4, we also validated the effectiveness of the dynamic weight w and the adjacent weight a. Also presents the ablation experiment showing only the current frame as supervision, without the RC module. It can be seen that the RC module brings a significant improvement in performance. Notably, the mIoU for using only a single frame is comparable to that with the RC module, which is attributed to the hacked mIoU caused by duplicated predictions.\nTo explore the advantages of Gaussian representation, we filtered out empty Gaussians, then analyzed the distribution of mean shifts among the remaining Gaussians, as illustrated in Figure 7. Notably, the majority of these Gaussian means remained within their original positions, with shifts not exceeding half voxel. This stability enabled us during inference to directly associate the semantic labels of the Gaussians with their corresponding voxels, leading to robust results.\nFurthermore, we visualized more intuitive outcomes, as demonstrated in the Figure 8, where the positions of Gaussian distributions were depicted as a point cloud. The second column displays results filtered by opacity alone. Many Gaussians in the sky lack supervision, which results in Gaussians near object surfaces being categorized as adjacent objects and exhibiting a tendency to shift closer to these objects. This also confirms the common z-offset in many Gaussians exceeds half a voxel and shows, as in Table 3, that overly constraining the Gaussian mean \u00b5 degrades performance. Building on this, as shown in the third column of Figure 8, we have additionally filtered out Gaussians that have shifted beyond the confines of their voxels. The results indicates that most Gaussians near real object surfaces adhered to their voxel positions. Additionally, Since the 2D ground truth is directly derived from LiDAR point clouds, it more accurately reflects the distribution of objects in the real world compared to 3D ground truth. Consequently, our method achieves object shapes, like trees, that are more realistic than the 3D ground truth."}, {"title": "5. Limitation and future work", "content": "Qualitative results indicate that our method still has several limitations. We treat each voxel as a Gaussian distribution, but in practice, such a large number of redundant Gaussians is unnecessary, leading to increased computational burden and memory usage. Furthermore, the sparsity of LiDAR supervision could also impede the optimization of Gaussians, thereby affecting the overall performance of the model. GSRender serves as a foundation for our future work, where we will focus on reducing the number of Gaussians to further optimize performance."}, {"title": "6. Conclusion", "content": "In this work, we explored the application of 3D Gaussian Splatting in 3D occupancy prediction tasks. We implemented a method to mitigate the issue of duplicate predictions along the same camera ray when using 2D weakly supervision. Our experiments validated the effectiveness of our approach, achieving state-of-the-art performance among current 2D-supervised methods. And provided a detailed analysis for the performance improvement. This technique has the potential to become a powerful tool in both industry and academia."}, {"title": "8. Specific experimental configuration.", "content": "We did not specifically fine-tune the weights for dynamic objects or neighboring frames. Instead, we adopted a straightforward approach by reducing the depth contribution of dynamic objects, setting the weight parameter a to 0.1. To ensure the supervision from the current frame remains dominant, we assigned a weight w of 0.8 to neighboring frames, prioritizing the current frame's information during training.\nIn addition to this weighting scheme, we applied constraints to the Gaussian center offsets, limiting them to a range of three adjacent voxels. This decision was informed by observations detailed in the main text, which highlight the trade-offs in offset range selection. Overly restricting the offset range can hinder the ability of Gaussians to achieve accurate opacity, leading to misclassifications where empty spaces are mistakenly treated as occupied voxels. Conversely, completely removing restrictions on the offset range can result in gradient explosions, destabilizing the training process.\nTo balance these extremes, we adopted a relaxed constraint, permitting Gaussian offsets to operate within a three-voxel radius in each direction. This approach strikes a balance by providing sufficient flexibility for the Gaussians to adapt to local variations while avoiding instability."}, {"title": "9. Additional Limitations and Discussion", "content": "As previously discussed and confirmed by experimental results, the use of 3D Gaussian splatting effectively mitigates the sensitivity of accuracy to the number of samples. However, we still aim to achieve a more sparse 3D Gaussian representation based on the LSS method. Specifically, we envision representing the entire 3D space objects with only sparse 3D Gaussians, ultimately deriving a more refined occupancy representation through the Gaussian-to-Voxel module. This remains a key objective for our future work.\nDuring our exploration, we observed that downstream tasks cannot accommodate intermediate states with duplicate predictions. To address this, we introduced the Ray Compensation module, which significantly alleviates the issue of duplicate predictions. This module is not limited to networks based on 3D Gaussian Splatting (3DGS). We believe it can serve as a general-purpose module that can be integrated into any model. By simply sampling the features of adjacent frames and using the labels of those frames for supervision, it can achieve promising results. This functionality constitutes a core innovation of our work, with potential for adoption in both industrial and academic applications."}, {"title": "10. A Broader impacts", "content": "Our research delves into a practical, end-to-end weakly supervised training framework based on 3D Gaussian Splatting(GS). We have introduced new modules that help reduce the redundancy in 3D space predictions caused by supervising with 2D projections of 3D LiDAR data. This approach not only boosts the performance of 2D supervision alone, but also, when used as auxiliary 3D supervision, shows even better results. It is a fresh solution for both the industry and academia, and it is vital for autonomous driving systems. By integrating predictions from auxiliary supervision into downstream tasks, we can provide more robust conditions for these tasks.\nIn the real world of autonomous driving, the accuracy of predictions and the system's ability to respond in real-time are both critical. Incorrect predictions or slow response times can lead to safety risks. Since autonomous driving systems are directly linked to personal safety, improving the network's real-time capabilities and developing more precise prediction methods are essential. The current bottleneck in network latency is the backbone network itself. Therefore, our next steps are to streamline the backbone network by reducing redundancy and to design networks that are more responsive."}, {"title": "11. Licenses for involved assets", "content": "Our code is built on top of the codebase provided by RenderOcc[35] and BEVStereo [21], which is subject to the MIT license. Our experiments are carried out on the Occ3D-nuScenes[43] which provides occupancy labels for the nuScenes dataset[3]. Occ3D-nuScenes is licensed under the MIT license and nuScenes is licensed under the CC BY-NC-SA 4.0 license."}, {"title": "7. Additional Experiments and Analysis", "content": null}, {"title": "Sampling at the Logits Level.", "content": "We also conducted additional experiments aimed at alleviating the computational burden of sampling at the feature level. Instead of extracting features for sampling, we opted to directly shift the Gaussian positions of the current frame. These shifted Gaussian spheres were then supervised using rendering results derived from auxiliary frames. This approach simplifies the sampling process and is conceptually equivalent to performing sampling directly on the logits, effectively bypassing any truncation errors that might arise in feature-level operations.\nHowever, the results revealed a key limitation: this direct and brute-force sampling method did not achieve the same level of performance as feature-level sampling, even though the latter inherently introduces truncation errors. This discrepancy suggests that feature-level sampling retains more nuanced information critical for accurate predictions. While direct sampling eliminates certain sources of error, it appears to lose the fine-grained contextual details embedded within the features, which are crucial for high-quality results.\nMoreover, the brute-force nature of this approach may also limit its ability to adapt dynamically to variations in the data, making it less robust across diverse scenarios. These findings underscore the importance of feature-level sampling, where the inherent richness of the extracted features compensates for the challenges posed by truncation errors. This balance between computational simplicity and performance accuracy highlights the trade-offs involved in designing efficient and effective sampling methods for tasks requiring precise supervision."}, {"title": "Comparison of Different Frame Intervals.", "content": "In the experimental section of the main text, we used only a single frame of future auxiliary supervision to simplify the experimental setup and focus on the core performance metrics. To gain a deeper understanding of how different auxiliary frame intervals impact the results, we conducted a series of additional experiments, as summarized in Table 6. These experiments systematically evaluated the performance using various combinations of auxiliary frames, including the next frame, the previous frame, the second previous frame, the second future frame, the previous two frames, the future two frames, and a combination of both previous and future two frames.\nThe results of these experiments highlighted several notable trends. First, using only a single auxiliary frame, whether from the past or the future, produced minimal improvements over the baseline, suggesting that the impact of a single frame is relatively limited. When two auxiliary frames were combined, there was a slight but noticeable increase in accuracy, although the improvement was still not particularly significant. This indicates that while incorporating additional temporal context can enhance performance, the benefits are relatively modest on this scale.\nTo provide a fair and thorough comparison with RenderOcc, which operates under a more comprehensive experimental setup, we chose to use an extended configuration in the main text. However, in practical scenarios, increasing the number of accumulated auxiliary frames typically results in a more substantial boost in accuracy. This improvement becomes particularly evident as more frames are included, as they provide richer temporal context and more robust supervision for tasks such as occupancy prediction. However, the rate of improvement diminishes with each additional frame, suggesting diminishing returns as temporal information becomes increasingly redundant.\nIt is also important to consider the computational trade-offs. While using multiple auxiliary frames enhances accuracy, it comes at the cost of increased computational complexity and reduced efficiency. The additional frames require more processing power and memory, potentially limiting real-time applications where efficiency is critical. This trade-off is one of the key reasons why we opted to present"}]}