{"title": "How Do Generative Models Draw a Software Engineer? A Case Study on Stable Diffusion Bias", "authors": ["Tosin Fadahunsi", "Giordano d'Aloisio", "Antinisca Di Marco", "Federica Sarro"], "abstract": "Generative models are nowadays widely used to generate graphical content used for multiple purposes, e.g. web, art, advertisement. However, it has been shown that the images generated by these models could reinforce societal biases already existing in specific contexts. In this paper, we focus on under-standing if this is the case when one generates images related to various software engineering tasks. In fact, the Software Engineering (SE) community is not immune from gender and ethnicity disparities, which could be amplified by the use of these models. Hence, if used without consciousness, artificially generated images could reinforce these biases in the SE domain. Specifically, we perform an extensive empirical evaluation of the gender and ethnicity bias exposed by three versions of the Stable Diffusion (SD) model (a very popular open-source text-to-image model) - SD 2, SD XL, and SD 3 - towards SE tasks. We obtain 6,720 images by feeding each model with two sets of prompts describing different software-related tasks: one set includes the Software Engineer keyword, and one set does not include any specification of the person performing the task. Next, we evaluate the gender and ethnicity disparities in the generated images. Results show how all models are significantly biased towards male figures when representing software engineers. On the contrary, while SD 2 and SD XL are strongly biased towards White figures, SD 3 is slightly more biased towards Asian figures. Nevertheless, all models significantly under-represent Black and Arab figures, regardless of the prompt style used. The results of our analysis highlight severe concerns about adopting those models to generate content for SE tasks and open the field for future research on bias mitigation in this context.", "sections": [{"title": "I. INTRODUCTION", "content": "After the release of ChatGPT in November 2022, gen-erative models have been extensively adopted in different tasks. Among those, text-to-image generation models (such as Stable Diffusion, Dall-E, or Midjourney) are nowadays widely employed for graphical content generation [1]. More specifically, from a recent survey, it has been shown that 15 billion images were created using text-to-image generation models from 2022 to 2023 [2]. However, it has also been shown how those models may suffer from bias, which could exacerbate existing discrimination in specific domains [3], [4]. Focusing on Software Engineering (SE), previous work has highlighted that gender and ethnicity disparities exist in soft-ware engineering communities [5]\u2013[8], and how diversity and fairness impact SE teams [7], [9]. Hence, adopting generative models without specific awareness could increase the existing perception of bias in the SE community.\nIn this paper, we perform a comprehensive empirical study of the gender and ethnicity bias exposed by three versions of the open-source text-to-image generation model Stable Diffusion (SD) \u2013 namely SD 2 [10], SD XL [11], and SD 3 [12] towards SE tasks. We chose Stable Diffusion as a reference model since, due to its open-source nature, it is nowadays the most adopted text-to-image generation model. From a survey conducted by the Everypixel company, around 80% of all artificially generated images in 2023 were from systems embedding Stable Diffusion models [2].\nFollowing previous work [13], [14], we ask each SD version to generate images for 56 software-related tasks using two different prompt styles: one style including the \"Software Engineer\" keyword and one with no role specification. We obtain a total of 6,720 images and compare the gender and ethnicity bias exposed by each SD version in generating images with a specific prompt style.\nResults show that including the \"Software Engineer\" key-word significantly increases the gender bias towards Male representing figures in all SD versions. On the contrary, we observe a slight improvement in SD 3 concerning ethnicity bias. However, all SD models still severely under-represent specific ethnicity categories.\nOur evaluation raises several concerns about adopting SD models for generating content related to SE tasks. We highlight how the safety filter included in SD 3 still fails in generating unbiased content when including the \"Software Engineer\" keyword in the prompt. Hence, practitioners should be aware of the risk of generating biased content when using those models and adopt proper countermeasures (like explicitly specifying gender and ethnicity in the prompt). On the other hand, further research is needed to mitigate the bias embedded in those models and improve their safety filters. The main contributions of this paper are the following:\n\u2022 An extensive empirical evaluation of the gender and ethnicity bias exposed by three SD versions towards 56 SE tasks;\n\u2022 A discussion of recommendations for practitioners and researchers about adopting those models to generate content for SE tasks and possible strategies on how to mitigate the exposed bias;\n\u2022 A full replication package of our empirical study [15]."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Stable Diffusion (SD) is a family of text-to-image gener-ation models that employ the diffuser model architecture to generate images from a textual prompt [16]. SD 2 was released in 2022 as a diffuser model pre-trained on the LAION-5B dataset [17], filtered to avoid sensitive material. However, as stated in the Hugging Face's model card, the dataset contains images limited to English descriptions. Hence, the model could be biased towards different ethnicities and cultures, preferring white and western figures. SD XL was released in 2023 as a more advanced text-to-image generation model pre-trained on an internal proprietary large-scale dataset. However, as stated by the model's authors, even if pre-trained on a larger dataset, the model may inadvertently exacerbate existing biases when generating images or inferring visual attributes [11]. SD 3 has been released in 2024 and, by the time of this paper, is the latest version of SD models. It has been pre-trained using 1 billion synthetic and publicly available images and fine-tuned on an additional set of 30M high-quality aesthetic images focused on specific visual content and style, as well as 3M preference data images. As stated in the Hugging Face's model card, several safety measures (such as filtered data and safety checks) have been performed during the model's training phases to mitigate its biases. However, it is also reported that the model may still generate biased content for specific contexts.\nDifferent works have analyzed the biases exposed by text-to-image generation models. Bianchi et al. [3] and Naik et al. [4] have shown how models like Dall-E or Stable Diffusion reinforce existing biases even with prompts simply describing occupations or traits. Sun et al. performed an extensive study of the Dall-E 2 image generation model, showing how it systematically under-represents women in specific job occupations like computer programmer, sales manager, or criminal investigator [18]. Luccioni et al. studied the gender and ethnicity bias exposed by Dall-E 2 and Stable Diffusion 1.4 and 2, showing how those models systematically discriminate gender and ethnicity groups when using specific adjectives (like ambitious, assertive, supportive, or sensitive) and jobs (like computer programmer, clerk or hostess) in the prompt [19]. Wan et al. performed a survey study of different works analyzing the bias exposed by text-to-image generation models [20]. They show how most works focus on gender and skin tone bias while not focusing on other ethnical features. Moreover, they address how most works address bias exposed toward generic job categories without focusing on specific aspects. The only work analyzing the bias exposed by image generation models for SE tasks is the one proposed by Sami et al. [13]. In their study, the authors analyze the gender bias exposed by the Dall-E 2 model in generating images for SE tasks. To this aim, they employ and adapt the dataset proposed by Treude et al. for analyzing the bias exposed by GPT textual models for SE tasks [14]. In this paper, we extend the study of Sami et al. by analyzing the gender and ethnicity bias exposed by three versions of the Stable Diffusion model - SD 2, SD XL, and SD 3 - towards SE tasks."}, {"title": "III. EMPIRICAL STUDY DESIGN", "content": "The goal of our study is to analyze the extent to which different versions of Stable Diffusion exhibit gender and ethnicity bias for SE tasks. To achieve this, we conduct an empirical study comparing the bias related to gender and ethnicity in images generated by three different versions of SD models using prompts that both include and exclude the keyword Software Engineer.\nOur study is driven by the following research questions (RQ):\nRQ1 To what extent do different versions of Stable Diffusion exhibit gender bias towards Software Engineering tasks? This RQ aims to assess the amount of gender bias ex-hibited by SD models in images generated using prompts that include the keywords Software Engineer, compared to prompts that do not include this keyword.\nRQ2 To what extent do different versions of Stable Diffu-sion exhibit ethnicity bias towards Software Engineering tasks? This RQ aims to identify the amount of ethnicity bias exposed by SD models when generating images with prompts including the Software Engineer keyword, compared to prompts that do not include this keyword.\nRQ3 Do prompts describing different Software Engineering tasks induce different degrees of gender and ethnicity bias in Stable Diffusion models? This RQ analyses if specific SE tasks suffer from different degrees of gender and ethnicity bias in SD models."}, {"title": "A. Data Collection", "content": "In this study, we focus on assessing the bias exposed by three different versions of the SD model: stable-diffusion-3-medium,\u00b9 stable-diffusion-2-1 \u00b2 , and stable-diffusion-xl-base-1.03. We employ the models' implementations available in the Hugging Face repository with their default hyperparameters.\nWe adopt a modified version of the prompt used by Sami et al. [13] and Treude et al. [14] to generate images from each SD version. This modified version uses \u201cphoto portrait\" at the beginning of each prompt to increase the quality of the generated images and avoid the generation of images not showing humans [19]. Specifically, we feed each of the three SD models with two sets of prompts (a general prompt and an SE prompt) describing 56 software-related tasks (the complete list of tasks is reported in Table V). We consider two prompt styles, as follows:\n\u2022 A General prompt style, which does not provide any additional information about the person performing a given task: \u201cPhoto portrait of a person that <task>\u201d"}, {"title": "B. Data Labeling", "content": "After generating the images, we labeled the gender and ethnicity of the person represented in each of them. As men-tioned above, we generated a total of 6,720 images. Manually inspecting and labeling such a large number of images is extremely costly and prone to errors. For this reason, following previous work [19], we automatically label the gender and ethnicity of the person depicted in each image by using the BLIP Visual-Question-Answering model [21]. BLIP is a Vision-Language pre-trained model that, given an image and a prompt question about that image, provides a single-word label answering the given question. In particular, we used the blip-image-captioning-base model made publicly available by Hugging Face4.\nBefore using BLIP for the labeling task, we first evaluated its effectiveness in accurately identifying gender and ethnicity from a statistically significant subset of images. Specifically, for each set of images generated from a particular version of SD using a specific prompt style, we selected a subset that enabled us to assess BLIP's effectiveness with a 95% confidence level and a 10% margin of error. We use Cochran's Formula with Finite Population Correction to compute the subsample size [22]:\nSample size = $\\frac{z^2xp(1-p)}{{\\epsilon^2\\frac{z^2xp(1-p)}{\\epsilon^2 N}}}$    (1)\nwhere $p$ is the confidence level (95% in our case), $e$ is the error rate (10% in our case), $N$ is the population size (1,120 in our case), and $z$ is the z-score. Using the above formulation, we obtained a subsample of 89 images for each SD version and prompt style, for a total of 534 images (89 images \u00d7 3 SD versions \u00d7 2 prompt styles). Those images were manually labeled by two authors of this paper to identify the ethnicity and gender of the person depicted. Note how the confidence level and the error rate were chosen to find the best trade-off between the number of images to label manually and the statistical significance of the evaluation.\nNext, the manual labeling has been compared with the one provided by BLIP. We compute the Accuracy [23] and Weighted F1 Score [24] to assess BLIP labeling effectiveness. Accuracy is a widely adopted metric in classification tasks that computes the number of correct predictions over the full predictions done by a model. However, even if widely adopted, Accuracy has been criticized for not accounting for possible unbalance in the labels [25]. For this reason, we enriched this analysis by including the Weighted F1 Score. This metric computes the harmonic mean between Precision and Recall for each possible label's value and then aggregates the results by computing the weighted average based on the values' distribution [24].\nFinally, before labeling the gender and ethnicity of each image, we use BLIP to filter images not showing humans. We feed BLIP with the following prompt to identify those images:", "Labeling": "Following previous work [13], [19], we performed a binary gender classification of images, label-ing each person depicted as Male or Female. Even though this binary classification does not reflect all possible gender iden-tifications, we argue how identifying other gender orientations in artificially generated images is more challenging and error-prone [26]. We give the following prompt to BLIP to label the gender of each person: \u201cIs the person in this image a Male or a Female?"}, {"Labeling": "Since there are multiple ethnicity categories and mapping all of them could be infeasible, we used the 2021 England and Wales Census to identify the main ethnicity categories for our study. In particular, we identified five main ethnicity categories:\n\u2022 Arab: including Arab and Middle Eastern ethnicities.\n\u2022 Asian: including Indian and Asian ethnicities;\n\u2022 Black: including Black, African, and African American ethnicities;\n\u2022 White: including Caucasian, German, Hispanic, Italian, and White ethnicities;\n\u2022 Other: including all other ethnicities not mentioned above."}, {"title": "C. Bias Assessment", "content": "After labeling the gender and ethnicity of the people de-picted in each image, we computed the gender and ethnicity bias exposed by each SD version for each prompt style. Following previous work [13], [14], we follow the Statistical Parity definition of fairness, which states that a system is fair if it provides an equal distribution of all possible classification labels across all the individuals despite their belonging to specific groups [27], [28]. Although this definition of fairness might not reflect reality - given that the real distribution of gender and ethnicity may be biased for SE tasks we argue that image generation models should not reinforce existing biases. Instead, they should work to mitigate the current perceptions. In the following, we describe the formulations used to measure gender and ethnicity bias.\n1) Gender Bias: Following the Statistical Parity definition of fairness, we measure gender bias as the modulus of the difference between the percentage of Male and Female images generated by a SD version with a given prompt style:\nGender Bias = |P(male) \u2013 P(female)|   (2)\nwhere P(male) and P(female) are defined as the number of images labeled as male or female over the total number of images. This metric ranges from 0 to 1, where 0 means perfect fairness, while 1 highlights complete bias.\n2) Ethnicity Bias: Differently from gender, ethnicity em-ploys more than two categories. For this reason, we measure ethnicity bias as the absolute difference between the maximum and the minimum percentages of images showing a given ethnicity category [29]:\nEthnicity Bias = |Pmax(e \u2208 E) - Pmin(e' \u2208 E)| (3)\nwhere Pmax(e \u2208 E)) and Pmin(e' \u2208 E) are the highest and lowest percentage of images showing a specific ethnicity category, respectively. As for the gender bias metric, this score ranges from 0 to 1, where 0 is the optimal value."}, {"title": "IV. EMPIRICAL STUDY RESULTS", "content": "This section presents the results of our empirical evalua-tion. For RQ1 and RQ2, we report in Table III and IV the amount of gender and ethnicity bias exposed by each SD version with a given prompt style, along with the percentage variation between the bias exposed using the General and SE prompt styles. For each table column, the highest values are highlighted in bold, while the lowest values are underlined. In addition, we provide bar charts showing the percentage of images grouped by gender (Figure 1) or ethnicity (Figure 2) generated by each SD version with a given prompt style. For RQ3, we report in Table V the gender and ethnicity bias in images generated for each task using a specific SD version and prompt style. Values highlighting a significantly high bias (\u2265 0.8) are highlighted in bold, while values highlighting fairness (< 0.2) are underlined."}, {"title": "A. RQ1: Gender Bias", "content": "Table III reports the gender bias of each SD version using a given prompt style. The bias is computed using the formulation reported in equation 2. We observe how including the Software Engineer keywords in a prompt consistently increases the gender bias for all SD versions, with all models exposing an almost full bias when using the SE prompt style. More in detail, SD 2 is the model exposing the highest bias varia-tion, where including the Software Engineer keywords in the prompt more than doubles the gender bias in the generated images (+108%). On the contrary, SD XL is the model exhibiting the lowest bias variation (+35%). However, SD XL is also the model exposing the highest gender bias in images generated using the General prompt style (0.71), meaning that SD XL is already significantly biased in generating images for software-related tasks, regardless the prompt style. Finally, we observe how, even if released after the other two models, SD 3 still exhibits a significant amount of bias for images generated using both prompt styles. In particular, we observe how the amount of bias for images generated using the General prompt style is higher than the previous SD 2 version, while the gender"}, {"title": "B. RQ2: Ethnicity Bias", "content": "Table IV reports the ethnicity bias exposed by each SD version. The score is computed using the formulation re-ported in equation 3. Like gender bias, including the Software Engineer keyword in the prompt increases the bias in all SD versions. However, the reported percentage variations are milder compared to gender bias. We observe how SD 2 is the model providing the highest bias variation between prompt styles, with an increase of +36%. SD XL is still the model providing the lowest variation (+18%). But, at the same time, it is the model exposing the highest bias in generating images using both General (0.84) and SE (0.99) prompt styles. This highlights how SD XL also embeds a significant ethnicity bias for software-related tasks, regardless of the prompt style used. Finally, SD 3 is the model that shows the lowest level of ethnicity bias in images generated with both General (0.56) and SE prompt styles (0.69). However, it is worth noticing that while SD 3 has the lowest bias, it is still significantly high.\nThe ethnicity distributions reported in Figure 2 provide additional insights. We observe that SD 2 and SD XL present a significant bias in generating images representing White fig-ures for software-related tasks, regardless of the prompt style. On the contrary, SD 3 exhibits a slight bias towards Asian figures when generating images for SE tasks. This variation could be explained by the different and more heterogeneous dataset on which this model has been trained. Finally, we still observe a significant under-representation of Black and Arab figures in all SD models concerning both General and SE prompt styles."}, {"title": "C. RQ3: Task-related Bias", "content": "Table V reports the gender and ethnicity bias observed in images generated for each specific software-related task by each SD version with a given prompt style.\n1) Gender Bias: Regarding gender bias, we found that all SD models show a significant bias when generating images for all tasks using the Software Engineer keyword. Moreover, we observe how even images that present a negligible amount of bias when generated using a General prompt style - i.e., the ones generated for non-code-related tasks such as \"Perfoms support tasks\" or \u201cHelp others\u201d - still present a significant amount of bias when are generated using the SE prompt style. This highlights how SD models are significantly biased in generating Software Engineer figures, regardless of the task they are performing.\n2) Ethnicity Bias: Concerning ethnicity bias, Table V ex-poses different trends between SD versions. SD XL presents an almost full bias when generating images for all tasks using a SE prompt style. On the contrary, the number of tasks whose generated images embed a significant amount of bias decreases as the SD version increases. This result aligns with what has been observed in Figure 2, where SD 3 especially highlighted a more balanced distribution of White and Asian figures in generating images for SE tasks. Nevertheless, there are still tasks whose generated images expose a significant bias on SD 3 when using the SE prompt style. The nature of the tasks causing high ethnicity bias in SD 3 is quite heterogeneous and does not highlight any specific pattern (like \u201cCommits code", "Helps others\u201d or \u201cWrites documentation wiki pages": "o mention a few). Finally, we also do not observe any task whose generated images provide a negligible amount of bias, regardless of the prompt style used. This result also aligns with what has been observed in Figure 2 and shows how all SD models are significantly under-representing Black and Arab figures when generating software-related tasks."}, {"title": "V. DISCUSSION", "content": "Our empirical evaluation highlights severe concerns about the bias exposed by SD models toward SE tasks and opens the floor for additional research in this field. We can draw the following recommendations for practitioners and researchers."}, {"title": "A. Recommendations for Practitioners", "content": "Practitioners should carefully adopt SD models for content generation since they can expose and reinforce existing biases towards SE figures. In particular, we propose the following recommendations:\n\u2022 Practitioners should not blindly rely on these models for content creation, as our evaluation highlighted that the generated images may exhibit significant bias. In fact, we recommend manually checking and accounting for the possible bias exposed.\n\u2022 We encourage avoiding the large-scale use (e.g., on the web or in advertisements) of images solely generated by SD models as they very often represent only white males performing SE tasks, thus reinforcing existing societal gender and ethnicity biases towards SE activities, and STEM more in general.\n\u2022 To reduce bias and increase the diversity of the generated images, practitioners could use a set of prompts explicitly mentioning different gender and ethnicity categories."}, {"title": "B. Recommendations for Researchers", "content": "Our empirical evaluation of the gender and ethnicity bias exposed by SD models highlights that more research is needed to decrease the bias of these models. We suggest the following possible research directions:\n\u2022 We hypothesize that the bias we observed in the SD models is mainly due to the existing imbalance in gender and ethnic distributions for specific categories in the data used to train these models. Therefore, we recommend that researchers improve the diversity of these data sets and develop methods to automatically reduce inherited biases.\n\u2022 Researchers should increase the effectiveness of safety filters to address bias toward more extensive group cate-gories. In fact, our evaluation highlighted how the safety filters embedded in SD 3 still fail to reduce the gender and ethnicity bias towards SE figures.\n\u2022 Researchers can investigate approaches to automatically find optimal hyperparameters and prompts able to reduce the bias of existing SD models while simultaneously maintaining high-quality generated images, as done for inference time reduction [30].\n\u2022 Finally, instead of focusing on creating models that gener-ate images that resemble the actual distribution of gender and ethnicity categories for specific tasks, we recommend that researchers focus on developing models that achieve statistical parity in gender and ethnicity distributions. In this way, text-to-image models can avoid the potential reinforcement of existing biases."}, {"title": "VI. THREATS TO VALIDITY", "content": "The gender and ethnicity labeling per-formed by the BLIP Visual Question Answering model may not be entirely accurate. To address this threat, we evaluated the effectiveness of BLIP's labeling on a sub-sample of images. The results showed how BLIP is highly effective in this task, with a 95% confidence level and a 10% error rate. Another threat concerns the stochastic behavior of text-to-image models, which makes the experiments difficult to reproduce. To respond to this threat, we generated multiple images for each SD model and prompt pair and evaluated the overall bias exposed by the models. Finally, although motivated by previous literature [13], [19], the results of our evaluation are limited to a binary gender classification and a simplified ethnicity categorization.\nWe used multiple metrics to assess BLIP's effectiveness, avoiding potential threats associated with adopting specific metrics like Accuracy [25]. In addition, we followed the Statistical Parity definition of fairness [27] and adopted formulations from previous work to measure the bias exposed by SD models [29].\nThe results of our study are limited to the text-to-image models and prompts we investigated herein. To mitigate this threat, we analyzed the three most adopted SD models and used prompts describing a heterogeneous set of tasks. In addition, we use an improved version for the task at hand of the prompts used by two previous studies analyzing ChatGPT and Dall-E's gender bias towards SE tasks [13], [14]. Hence, all these studies can provide a comprehensive picture"}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we performed an extensive empirical evalu-ation of the gender and ethnicity bias exposed by three SD models - SD 2, SD XL, and SD 3 - towards SE tasks. To this aim, we generated 6,720 images by feeding the models two prompt styles: one including the Software Engineer keyword and one not. We then assessed the percentage of male and female figures, as well as the percentages of various ethnicity categories depicted in the images generated by each model. Results showed how all SD models are significantly biased towards male figures when generating software engineers. Moreover, we observed that while SD 2 and SD XL are strongly biased towards White figures when generating a software engineer, SD 3 is slightly more biased towards Asian figures. Nevertheless, all models significantly under-represent Black and Arab figures, regardless of the prompt style used.\nFuture work can extend our analysis by considering addi-tional tasks and other text-to-image models like Midjourney. Further analyzing patterns in prompts that reveal biases in text-to-image models could be a promising area for future research. Prompt engineering and hyper-parameter tuning should also be explored to reduce the bias of these models automatically. Multi-objective optimisation could aid in this, as it proved successful to reduce bias of other AI and ML models [31]-"}]}