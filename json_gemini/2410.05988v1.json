{"title": "UTILIZING LYAPUNOV EXPONENTS IN DESIGNING\nDEEP NEURAL NETWORKS", "authors": ["Tirthankar Mittra"], "abstract": "Training large deep neural networks is resource intensive. This study investigates whether\nLyapunov exponents can accelerate this process by aiding in the selection of hyperpa-\nrameters. To study this I formulate an optimization problem using neural networks with\ndifferent activation functions in the hidden layers. By initializing model weights with differ-\nent random seeds, I calculate the Lyapunov exponent while performing traditional gradient\ndescent on these model weights. The findings demonstrate that variations in the learning\nrate can induce chaotic changes in model weights. I also show that activation functions with\nmore negative Lyapunov exponents exhibit better convergence properties. Additionally, the\nstudy also demonstrates that Lyapunov exponents can be utilized to select effective initial\nmodel weights for deep neural networks, potentially enhancing the optimization process.", "sections": [{"title": "1 Introduction", "content": "Neural Networks have become ubiquitous, with models like Chat GPT and BERT revolu-\ntionising various industries. However, training such big models can take several days and re-\nquire enormous computational power, contributing to problem like global warming(Anthony\net al. (2020)). Therefore, investing time upfront to select hyperparameters\u2014such as activa-\ntion functions, learning rates, regularisation methods, and initial model weights\u2014properly\nis crucial. This paper proposes using Lyapunov exponents to guide these design choices.\nWhile the focus is on Deep Neural Networks (DNNs), the approach can be generalised to\nother machine learning techniques, such as linear regression. In this paper, I investigate\nhow the trainable parameters of a Deep Neural Network (DNN) change when the learn-\ning rate is varied, demonstrating that the parameters can exhibit chaotic behaviour as the\nlearning rate is adjusted. I also investigate a key design question for DNNs: how Lyapunov\nexponents can be used to guide the selection of hyperparameters, particularly activation\nfunctions and initial model parameters. Hyper parameters are model parameters that re-\nmain fixed during the training of a DNN. Identifying an optimal set of hyperparameters\nis a crucial aspect of deep learning which often involves strategies such as using validation\nsets or using bandit-based approach(Li et al. (2018)) for different hyperparameters config-\nurations or using a grid search in the hyperparameter space(Bergstra and Bengio (2012))."}, {"title": "2 Related Works", "content": "Hyperparameter selection plays a crucial role in effectively training machine learning mod-\nels, there has been significant work on various approaches to this problem. For example, in\ngrid search(Montgomery (2017)) user specifies a finite set of values for each hyperparam-\neter, and the best configuration is selected based on the performance of the model on the\nCartesian product of these sets. Random search(Bergstra and Bengio (2012)) mitigates the\nintensive computation in grid search when dimensions of the configuration space is large by\nrandomly selecting set of hyperparameters without replacement. Genetic algorithms have\nalso been used where mutation and crossover are utilized to generate a better generation of\nparameters(Hansen (2016)). Bayesian optimization is an effective hyperparameter optimiza-\ntion framework for an expensive black box function where a probabilistic surrogate model\nis fitted to all observations and an acquisition function is used to determine utility of differ-\nent candidate points(Hutter et al. (2019)), Bayesian optimization can be performed with a\nGaussian processes or other machine learning algorithms(Hutter et al. (2011))(Snoek et al.\n(2015))(Springenberg et al. (2016)). Then there are bandit based strategies like successive\nhalving and hyper-band. In successive halving half of the worst performing configurations\nare removed, and the budget is doubled for the remaining configurations, (Jamieson and\nTalwalkar (2016)) discusses the effectiveness of the above strategy. Hyper-band(Li et al.\n(2018)) (a hedging strategy) is where the total budget is divided into several combinations\nand then successive halving is called as a subroutine to each of these configurations. To the\nbest of my knowledge, there hasn't been previously published work that directly marries the\nconcepts of Lyapunov exponents and hyperparameter selection for Deep Neural Networks\n(DNNs). Lyapunov exponents have been extensively used in various fields to understand the\nstability and predictability of dynamical systems. For instance, in weather forecasting and\nclimate dynamics, Lyapunov exponents are used to study the limits of predictability in the\natmosphere, a system known for its chaotic behavior. Despite their well-established utility\nin understanding stability in dynamical systems, the application of Lyapunov exponents\nto guide design choices, such as the selection of activation functions, initial parameters,\nor learning rates in DNNs, remains an unexplored area of research. The integration of\nLyapunov exponents into the hyperparameter tuning process could provide novel insights\ninto optimizing DNN architectures, especially in terms of understanding their sensitivity to\ninitial conditions and avoiding chaotic behaviors during training. This gap in the literature"}, {"title": "3 Methodology", "content": "To understand the background of the research let's consider a system of first order linear\nordinary differential equation(ODE) with two state variables shown in Eq[1]. The general\nsolution of this equation is given by Eq[2], where \u03bb\u2081, \u03bb\u2082 are eigenvalues and v\u2081, v\u2082 are the\neigenvectors.\n$\\begin{aligned}\n\\frac{dx}{dt} &= a x + b y \\\\\n\\frac{dy}{dt} &= c \\cdot x + d \\cdot y\n\\end{aligned}$ (1)\n\n$\\left[\\begin{array}{l}x \\\\\ny\\end{array}\\right]=c_1 v_1 e^{\\lambda_1 t}+c_2 v_2 e^{\\lambda_2 t}$ (2)\n\nEigenvalues are crucial in understanding the behavior of solutions to the linear ordinary\ndifferential equations (ODEs) in Eq[1]. For example, when both eigenvalues have negative\nreal parts, the system's solution converges to a fixed point. Similarly, Lyapunov exponents\nplay a comparable role in nonlinear dynamical systems. Analogous to eigenvalues in linear\nODEs, Lyapunov exponents quantify how nearby trajectories in a system's phase space\neither converge or diverge over time. A system with N dimensions have N Lyapunov\nexponents, with emphasis often placed on the largest Lyapunov exponent as it dictates the\nlong term behavior of a trajectory."}, {"title": "4 Results", "content": "The first thing I noticed was that the learning rate can be adjusted to induce chaos in how\nthe model weights and biases gets updated. Fig[4] shows how learning rate can induce chaos.\nIt's not always true that increasing the learning rate will always make the model parameters\nchange in a chaotic way. For example, if the learning rate in a neural network with a ReLU\nactivation function is increased significantly, all model parameters will become negative.\nIn the context of a ReLU activation function, this situation implies that the gradients\nbecome zero, resulting in a Lyapunov exponent of zero i.e. no chaos. If we consider the\ndifferent models with different activation functions a more negative Lyapunov Exponent\nmeans that nearby points will converge faster to a local minima. This fact can be used\nto select activation functions for a neural network given other hyperparameters and the\ndataset remains the same. Table[1] depicts this relationship. The ReLU activation function\nhas the lowest Lyapunov exponent and, consequently, the lowest average final loss.\nIn the continuation of the above experiments, I observed that using different starting\nmodel parameters resulted in slightly different values of local Lyapunov exponent calcu-\nlation. Figure 5 illustrates how the Lyapunov exponents, calculated from various initial\npoints, relate to the final loss. A more negative Lyapunov exponent corresponds to a lower"}, {"title": "5 Conclusion", "content": "This paper leads to three main conclusions. First, changing the learning rate can cause\nchaotic behavior in how model parameters are updated. Second, Lyapunov exponents\ncan be used to help choose hyperparameters, like finding the best activation function.\nThird, Lyapunov exponents can help identify effective initial model weights, improving\nthe optimization process of neural networks. The code used is made publicly available at\nhttps://github.com/tirthankar95/ChaosOptim."}]}