{"title": "PointOBB-v3: Expanding Performance Boundaries of Single Point-Supervised Oriented Object Detection", "authors": ["Peiyuan Zhang", "Junwei Luo", "Xue Yang", "Yi Yu", "Qingyun Li", "Yue Zhou", "Xiaosong Jia", "Xudong Lu", "Jingdong Chen", "Xiang Li", "Junchi Yan", "Yansheng Li"], "abstract": "With the growing demand for oriented object detection (OOD), recent studies on point-supervised OOD have attracted significant interest. In this paper, we propose PointOBB-v3, a stronger single point-supervised OOD framework. Compared to existing methods, it generates pseudo rotated boxes without additional priors and incorporates support for the end-to-end paradigm. PointOBB-v3 functions by integrating three unique image views: the original view, a resized view, and a rotated/flipped (rot/flp) view. Based on the views, a scale augmentation module and an angle acquisition module are constructed. In the first module, a Scale-Sensitive Consistency (SSC) loss and a Scale-Sensitive Feature Fusion (SSFF) module are introduced to improve the model's ability to estimate object scale. To achieve precise angle predictions, the second module employs symmetry-based self-supervised learning. Additionally, we introduce an end-to-end version that eliminates the pseudo-label generation process by integrating a detector branch and introduces an Instance-Aware Weighting (IAW) strategy to focus on high-quality predictions. We conducted extensive experiments on the DIOR-R, DOTA-v1.0/v1.5/v2.0, FAIR1M, STAR, and RSAR datasets. Across all these datasets, our method achieves an average improvement in accuracy of 3.56% in comparison to previous state-of-the-art methods.", "sections": [{"title": "INTRODUCTION", "content": "ORIENTED object detection (OOD) in aerial imagery focuses on identifying and localizing objects of interest by employing oriented bounding boxes (OBBs), while also determining their respective categories. This area has seen a significant amount of high-quality research contributions [1]\u2013[9]. Despite these advancements, the process of manually annotating detailed OBBs remains labor-intensive and costly, posing challenges for large-scale datasets.\nTo reduce the annotation cost of bounding boxes, weakly-supervised horizontal object detection utilizing image-level annotations has been extensively developed [10]\u2013[16]. However, these approaches struggle in complex aerial scenes and lack the capability to predict object orientations. In recent years, there has been increasing interest in weakly supervised OOD. As depicted in Fig. 1, existing weakly-supervised methods use coarser-grained annotations as weak supervisory signals to predict OBBs, such as horizontal bounding box (HBB) annotations [17]\u2013[20]. Nevertheless, box-based annotations remain inefficient and labor-intensive. Thus, exploring more cost-effective and efficient annotation forms is crucial.\nRecently, point-based annotation has garnered considerable attention in various tasks [21]\u2013[24]. Within the realm of object detection, point annotations have proven to be approximately 36.5% less costly compared to HBBs and 104.8% less costly compared to OBBs, while increasing the labeling efficiency compared to both HBB and OBB. As a result, single point-supervised OOD emerges as a more meaningful and efficient alternative.\nFrom a bird's-eye view, objects in aerial images exhibit two distinct characteristics: varying spatial scales and arbitrary orientations. Given the abundance of small objects in aerial imagery [25], utilizing single point labels is clearly more suitable than using multiple point labels as in methods like [21]. Current single point-supervised object detection techniques [26], [27] follow the Multiple Instance Learning (MIL) paradigm. This approach optimizes based on category labels, selecting the proposals with the highest confidence from proposal sets as the predicted bounding boxes, thereby achieving perception of object scale. However, the MIL paradigm inherently struggles with stability in perceiving object boundaries, often focusing on the most distinctive part of an object rather than its full extent and precise boundaries [14], [15]. We identify two key challenges when extending this approach to OOD in aerial imagery: i) How to address the inconsistency in the MIL approach to obtain more accurate scale representations? ii) How to effectively learn the object's orientation under single point-supervision?\nIn this paper, we introduce a stronger single Point-based OBB generation framework termed PointOBB-v3. It facilitates collaborative learning of both angle and scale by integrating three distinct views. The main design consists of the following five key aspects: a progressive multi-view switching strategy, Scale-Sensitive Consistency (SSC) loss, Scale-Sensitive Feature Fusion (SSFF) module, Self-Supervised Angle (SSA) loss and Dense-to-Sparse (DS) matching strategy. The progressive multi-view switching strategy aims to enable the model to progressively learn the object's orientation and scale across three views. SSC loss is used to address the aforementioned inconsistency between the proposal's confidence score and its scale accuracy. The SSFF module is designed to further enhance scale learning. The purpose of the SSA loss and DS matching strategy is to obtain accurate object angle prediction.\nSpecifically, to collaboratively learn both the angle and scale of objects, we construct three views from the input image and point label: the original view, and two enhanced views\u2014one resized and one rotated/flipped (rot/flp). Meanwhile, to facilitate the progressive acquisition of the network's discriminative capabilities for object scale and orientation, we implement a progressive multi-view switching strategy during training, wherein the two enhanced views are alternated properly to allow the model to gradually learn both scale and orientation knowledge.\nTo tackle the two aforementioned issues, we propose two modules based on the three views: a scale enhancement module using the original and resized views, and an angle acquisition module using the original and rot/flp views. i) The scale enhancement module. The core components of this module include the SSC loss and the SSFF module. In an optimal scenario, the predicted size of the same object\u2014reflected by the scale of the proposal with the highest confidence score\u2014should remain consistent across views with different resolutions. To enforce this consistency, the SSC loss is designed to reduce the divergence in the predicted score distributions between the original and resized views. Furthermore, feature layer mapping in traditional MIL-based approaches is strictly based on ROI's scale and predefined hyperparameters, which can be inflexible and unstable, especially when the scale is imprecise. This often leads to incorrect layer mapping and inconsistent feature extraction. To address this, we propose the SSFF module that uses a gating mechanism inspired by Mixture of Experts (MoE) architecture. This mechanism generates gating scores for each feature layer, which are then used to dynamically aggregate the multiple output layers of the FPN, enhancing the model's ability to perceive scale information. ii) The angle acquisition module. Within this module, the SSA loss is designed based on the inherent symmetry of objects in aerial images. To obtain more accurate OBBs, we propose the DS matching strategy that complements the self-supervised angle learning branch in this module.\nAdditionally, existing point-supervised methods [28], [29] commonly adopt a two-stage paradigm, where pseudo-labels are first generated and then used for detector training. Considering the complexity and extended training time associated with the two-stage paradigm, we further implement an end-to-end version of our proposed PointOBB-v3 framework. This end-to-end version primarily integrates a Rotated FCOS [30]-based detection head, forming a dual-branch structure with the MIL head to enable joint training. Moreover, an Instance-Aware Weighting (IAW) strategy is proposed to alleviate the negative impact of low-quality predictions from the MIL head on the training process. This strategy dynamically applies instance-level weight to the end-to-end loss, effectively improving parameter optimization during joint training.\nThis paper is an extended version of our conference paper, published in CVPR 2024 [28], and the main contributions are as follows: 1) The proposed approach integrates object scale and orientation learning across three unique perspectives, guided by a progressive multi-view switching strategy. 2) An SSC loss is devised to strengthen the network's capability in capturing object scale, and an SSFF module is designed to enhance accuracy in scale-related feature layer mapping. 3) An SSA loss is used to learn the object orientation and a scale-guided DS matching"}, {"title": "2 RELATED WORK", "content": "OOD algorithms primarily target different types of objects such as aerial objects [31], multi-oriented scene texts [32] [33], and 3D objects [34]. Among the representative approaches are anchor-free detectors like Rotated FCOS [30], and anchor-based detectors including Rotated RetinaNet [35], RoI Transformer [1], Oriented R-CNN [4], and ReDet [2]. Oriented RepPoints [36] presents a strategy for evaluating and allocating sample quality based on adaptive points. Additionally, methods such as R\u00b3Det [5] and S2A-Net [37] boost detector performance by utilizing feature alignment modules. To overcome boundary discontinuity issues during angle regression, methods using angle coders [38]\u2013[40] transform angles into boundary-free formats, thus enhancing stability. Moreover, approaches like GWD [7] and KLD [8] propose Gaussian-based loss functions to effectively analyze the nature of rotational representations, thereby improving overall detection performance."}, {"title": "2.2 Weakly-Supervised Oriented Object Detection", "content": "Existing weakly-supervised OOD approaches can be broadly categorized into image-supervised and HBox-supervised methods. Additionally, we further explore the feasibility of point-supervised methods as a more efficient alternative, aiming to reduce annotation costs while maintaining high detection performance.\nImage-supervised. For methods using image-level annotation, WSODet [41] enhances the OICR [14] framework to predict HBBs. Subsequently, pseudo OBBs are generated using contour features and the predicted HBBs, followed by an Oriented RepPoints branch to refine these predictions. However, image-supervised methods generally yield limited performance, as the quality of the generated OBBs heavily relies on the accuracy of the predicted HBBs, which can lead to suboptimal results.\nHBox-supervised. For methods using HBox-level annotation, although some approaches (e.g., BoxInst-RBox [42] and BoxLevelSet-RBox [43]) follow an HBox-Mask-RBox pipeline to generate OBBs, involving segmentation often results in higher computational costs, making the entire procedure time-consuming. H2RBox [18] takes a more efficient approach by predicting RBoxes directly from HBox annotations without relying on redundant representations. This method learns the angle from the geometry of circumscribed boxes, achieving notable performance. H2RBox-v2 [19] further improves upon this by leveraging the inherent symmetry of objects. However, these methods still require collecting a substantial number of bounding box annotations. In addition to these, some studies leverage HBox along with specialized forms of annotations. OAOD [44] utilizes additional object angle annotations, while KCR [20] employs a combination of RBox-annotated source datasets and HBox-annotated target datasets. Sun et al. [17] integrate HBox annotations with image rotations to align oriented objects horizontally or vertically. Nevertheless, such specialized forms of annotation lack universality, limiting their applicability to a broader range of tasks.\nPoint-supervised. Point-based annotations have been widely used in various tasks such as object detection [24], [26], [27], [45]\u2013[47], panoptic segmentation [22], [23], and instance segmentation [21], [48], among others [44], [49]. Due to its cost-effectiveness and efficiency, single point-supervised object detection has garnered significant attention. Click [27] was one of the earliest attempts at point-supervised object detection, proposing center-click annotations and incorporating them into a MIL paradigm. P2BNet [26] further enhances this approach by using a coarse-to-fine strategy and incorporating negative samples to improve prediction quality. However, these methods only produce horizontal bounding boxes and fail to address the inherent instability of the MIL paradigm effectively.\nBased on the above, to ultimately obtain RBox from point annotations, one potential method is the Point-to-Mask approach [23], which involves determining the minimum circumscribed rectangle of the generated mask. Another viable method entails a combination of Point-to-HBox and HBox-to-RBox strategies. However, in the past year, several innovative methods have emerged, offering new insights into point-supervised OOD. Point2RBox [50] introduces an end-to-end approach that combines knowledge and learns from one-shot examples to generate RBox. And PointOBB-v2 [29] learns a class probability map to generate pseudo RBox labels through principal component analysis. In our experiments, we employ these novel approaches for comparison. Overall, we propose a stronger framework for single point-supervised OOD based on the MIL paradigm, providing an efficient approach to directly achieve oriented object detection via single point-supervision."}, {"title": "3 PRELIMINARIES", "content": "As a classic weakly supervised learning method, MIL paradigm [51], [52] has been widely applied in various weakly supervised tasks like image-supervised [53]\u2013[60] and point-supervised horizontal object detection [26], [61], [62].\nThe fundamental design of the MIL paradigm is the two-stream architecture, which predicts a set of aggregate scores S consisting of the instance scores and the class scores, from a set of candidate proposal bags. The proposal bag with the highest aggregate score is selected under the constraint of classification labels, serving as the prediction for the object's category and scale. In the single point-supervised object detection setting, each object has a point label that includes the spatial coordinate and the category, and a candidate proposal bag is constructed based on the point label. Generally, the common process of MIL under such a setting can be formalized as:\n$L_{mil} = \\sum_{j=1}^{J} \\sum_{k=1}^{K} ([c_j]_k log([S_j]_k) + (1 \u2212 [c_j]_k) log(1 \u2013 [S_j]_k))$,\nwhere $J$ denotes the number of proposal bags in the batch, $K$ represents the total number of categories, $c_j \u2208 {0,1}^K$ is the one-hot category label, and $[S_j]_k$ refers to the aggregate score for the $k$-th category of the $j$-th proposal in the $S$.\nAbove all, by optimizing the aforementioned MIL-based paradigms, the network acquires critical perceptual capabilities for object scales and serves as the foundational architecture for our method."}, {"title": "3.2 Symmetry-Aware Learning", "content": "Symmetry is a natural attribute widely present in various scenarios. Therefore, learning the orientation of objects through their symmetry is theoretically feasible in weakly supervised object detection [19]. Assume there is a neural network $f_{nn}(\u00b7)$ that maps a symmetric image $I$ to a real number $\u03b8, \u03b8 = f_{nn}(I)$, To leverage reflection symmetry, the function is enhanced with two additional properties: flip consistency and rotate consistency.\ni. Flip consistency. When the input image is flipped vertically, $f_{nn}(\u00b7)$ gives an opposite output:\n$f_{nn}(I) + f_{nn}(flp(I)) = 0$,\nwhere $flp(I)$ is an operator of vertically flipping the image $I$.\nii. Rotate consistency. When the input image is rotated by $R$, the output of $f_{nn}(\u00b7)$ also rotates by $R$:\n$f_{nn}(rot(I, R)) \u2013 f_{nn}(I) = R$,\nwhere $rot(I, R)$ is an operator that clockwise rotates the image $I$ by $R$. Given an image $I_o$ symmetric about $\u03b8_{sym}$, assuming the corresponding output is $\u03b8_{pred} = f_{nn}(I_o)$, the image can be transformed in two ways: i) Flipping along the line $\u03b8 = \u03b8_{sym}$. ii) Flipping vertically first, and then rotating by $2\u03b8_{sym}$. Both transformations result in the output of $f_{nn}(\u00b7)$ remaining the same, which leads to $\u03b8_{pred} = \u03b8_{sym}$. Therefore, when the network successfully learns these consistency properties, its output will align precisely with the orientation of the image's axis of symmetry. As described in Sec. 4.3, we employ symmetry and design an assigner to pair objects across different views, which facilitates the computation of consistency loss for these matched objects and allows for generalization to multiple object detection tasks."}, {"title": "4 METHODOLOGY", "content": "This section provides a comprehensive overview of the proposed PointOBB-v3. Sec. 4.2 delves into the scale augmentation module, including an in-depth explanation of the SSC loss and the SSFF module. In Sec. 4.3, we explore the angle acquisition module in detail. Sec. 4.4 focuses on the design and implementation of the end-to-end framework, while Sec. 4.5 concludes with a discussion of the overall optimization objective with the MIL loss."}, {"title": "4.1.1 Method Overview", "content": "In current research on image-supervised and point-supervised object detection, MIL-based paradigms demonstrate fundamental perceptual capabilities for object scale. Building upon this foundation, we adopt the classic MIL fashion as the underlying network for our approach. The overall framework of PointOBB-v3 is illustrated in Fig. 2. From the original view, the generation of initial proposal bags is guided by the provided point labels. Subsequently, angle predictions obtained from the angle acquisition module are matched to these proposals through the DS matching strategy, thereby endowing the horizontal proposals with orientation information. From the generated rotated proposals, a subset of reliable proposals is selected and progressively refined using a MIL head followed by a refined MIL head, ultimately yielding the final OBB predictions. The obtained OBBs can be utilized as pseudo-labels to facilitate the final training stage of oriented object detectors.\nThe above pipeline incorporates three distinct views. Based on the original view, a resized view is generated by applying random scaling, and a rot/flp view is derived by performing random rotations or vertical flipping. These three views are leveraged to design two critical modules: the scale augmentation module and the angle acquisition module. The scale augmentation module is tailored to improve the network's ability to perceive and adapt to variations in object scales, whereas the angle acquisition module focuses on effectively learning object orientations through symmetry-aware learning. The resized and rot/flp views act as augmented views, which are dynamically alternated during training using the proposed progressive multi-view switching strategy ensuring a more robust and adaptive learning process.\nBesides, while the two-stage training paradigm with pseudo-label generation demonstrates strong performance, it incurs significant computational cost and extended training time. To address this, we also developed an end-to-end framework based on the same principles. This end-to-end version achieves a substantial improvement in training efficiency, reducing training hours by 21.36%, while maintaining competitive detection performance. The proposed point-supervised rotation object detection frameworks offer distinct advantages: the two-stage approach provides higher precision, while the end-to-end framework emphasizes efficiency and convenience. Together, these options aim to provide researchers and practitioners with a flexible and diverse set of tools to cater to various application requirements."}, {"title": "4.1.2 Progressive Multi-View Switching Strategy", "content": "To facilitate the network's gradual development of discriminative capabilities for object scale and predictive abilities for object orientation, we introduce a progressive multi-view switching strategy as a core optimization mechanism in our framework.\nThis progressive multi-view switching strategy operates through three stages: Stage 1. The process begins by generating a resized view from the original view using a scaling factor $\u03c3$. This resized view, combined with the original view, adheres to scale-equivalence constraints, forming the basis of the scale augmentation module. This module is designed to improve the network's capability to accurately perceive object scales, laying the groundwork for effective scale-sensitive learning. Details of this module will be discussed in subsequent sections. Stage 2 (i.e., \"burn-in step1\" in Fig. 2). Before this stage, the network has developed fundamental perceptual abilities to the scale and boundary of objects. However, orientation information is still lack. To address this, the resized view is replaced with a rot/flp view, which is paired with the original view to construct the angle acquisition module. This module employs a dense-to-dense sample assignment approach, enabling the network to engage in self-supervised angle learning, thereby introducing orientation sensitivity. Stage 3 (i.e., \u201cburn-in step2\u201d in Fig. 2). At this stage, the network has achieved a refined ability to predict accurate angles using dense feature representations. To integrate these angle predictions with object proposals, the proposed DS matching strategy is employed. This strategy aligns dense angle predictions with sparse proposals by leveraging neighboring receptive fields, effectively assigning accurate object orientations to the proposals."}, {"title": "4.2 Scale Augmentation Module", "content": "Objects in aerial images often display substantial scale variations, posing significant challenges to accurate detection. Under the MIL paradigm, these scale disparities intensify the inconsistency between the confidence scores of the predicted bounding boxes and their actual positional accuracy, leading to suboptimal detection performance. To mitigate this issue, we propose the scale augmentation module, which is structured around the design of SSC loss.\nIn an ideal scenario, the predicted size of the same object, represented by the scale of the proposal with the highest confidence score, should remain consistent across views with different resolutions. To enforce this consistency, the SSC loss is designed to minimize the distributional disparity in predicted confidence scores between the original and resized views.\nStarting with the proposal bags $B_o$ derived from the original view, we obtain the corresponding resized proposal bags $B_d$ from the resized view. The output scores, including class scores and instance scores of $B_o$ and $B_d$, are generated through the dual-stream branches integrated within the classical MIL head. For the $i$-th proposal bag $B_i$, the scores obtained from the original view are denoted as $S_{io}^{cls}$ and $S_{io}^{ins}$ respectively, while the corresponding scores from the resized view are represented as $S_{id}^{cls}$ and $S_{id}^{ins}$. These sets of scores have been processed through an activation function (e.g., softmax), and their dimensions are $R^{N\u00d7C}$, where $N$ denotes the total number of proposals within the bag and $C$ represents the number of object categories being classified. To maintain consistency in scale across the outputs from different views, the proposals' score distributions are initially adjusted according to their respective scales. In detail, a set of fundamental scales, denoted as ${s_1, s_2, ..., s_G}$, is established, where $G$ represents the total number of these predefined scales. The output score dimensions are then transformed from $R^{N\u00d7C}$ to $R^{G\u00d7K}$, where $K$ accounts for scale-independent variables like category and aspect ratio. Once the score distributions are adjusted according to scale for each view, cosine similarity is utilized to evaluate the consistency between them:\n$sim_{m,g}^{cls} = \\frac{[S_{io}^{ins}]_{m,g} \u00b7 [S_{id}^{ins}]_{m,g}}{||[S_{io}^{ins}]_{m,g}|| \u00b7 ||[S_{id}^{ins}]_{m,g}||}$,\n$sim_{m,g}^{ins} = 1 - \\frac{|[S_{io}^{cls}]_{m,g} - [S_{id}^{cls}]_{m,g}|}{[S_{io}^{cls}]_{m,g} + [S_{id}^{cls}]_{m,g}}$,\nwhere $m$ refers to the $m$-th point label, while $g$ denotes the $g$-th group of proposals, categorized according to the predefined basic scales. Utilizing these similarity measurements, the overall SSC loss is expressed as follows:\n$L_{ssc} = \\sum_{m=1}^{M} \\sum_{g=1}^{G} {w_1l_s(sim_{m,g}^{ins}, 0) + w_2l_s(sim_{m,g}^{cls}, 0)}$,\nwhere $M$ represents the total number of point labels, $l_s$ denotes the SmoothL1 loss function, and $w_1$ and $w_2$ are the weights assigned to the loss terms, with values of 2.0 and 1.0, respectively. By incorporating the SSC loss, the MIL network ensures alignment of the score distributions for proposals associated with the same label across multiple views, as illustrated in Fig. 3. This alignment effectively reduces discrepancies between confidence scores and positional accuracy, thereby improving the precision in perceiving object scale."}, {"title": "4.2.2 Scale-Sensitive Feature Fusion Module", "content": "As mentioned in Sec. 1, in traditional MIL-based methods for extracting features from ROIs, a relatively rigid feature layer assignment strategy is often employed, heavily relying on the scale information of the proposals and predefined hyperparameters. To be specific, assuming the feature pyramid is $lvls_{fea} = {P_1,P_2,...,P_P}$ with $P$ levels, ROIs are also assigned to different levels based on their scale, calculated as $lvl_{SROI} = log_2(\\sqrt{w\u00b7h}/f_s+1 \u00d7 10^{-6})$, where $f_s$ is a predefined scale parameter finest scale, and $w$ and $h$ represent the ROI's width and height, respectively. ROI features are then extracted from the corresponding feature layers. However, this approach to feature layer mapping can lack flexibility, particularly when the ROI's scale is not precise, which may result in incorrect layer assignments. Additionally, for ROIs with dimensions close to the boundaries of different layers, the feature layer mapping might become unstable, negatively affecting the consistency of feature extraction.\nTo address this problem, we devise the SSFF module for the MIL head, as shown in the below part of Fig. 2. Drawing inspiration from the MoE architecture, each feature layer undergoes a gating mechanism during training to produce its corresponding gating score. These scores are subsequently utilized to automatically combine the multiple output layers of the FPN. The final fused feature map is then used for feature extraction. Specifically, the multiple output layers of the FPN are automatically aggregated based on a self-activated gating score:\n$G_n = softmax(conv (F_n))$,\nwhere $F_n$ represents the $n$-th FPN feature layer, which is scaled to a unified shape using nearest-neighbor interpolation. $G_n$ denotes the gating score for each layer; $conv(\u00b7)$ is a 3 \u00d7 3 convolutional layer with a single output channel; and $softmax(\u00b7)$ normalizes the sum of $G_1, G_2, . . ., G_N$ to one for each pixel.\nThe final one-layer feature map $F$ can be obtained by:\n$F = \\sum_{n=1}^{N} G_nF_n$,\nwhere $N$ denotes the total number of layers used in the FPN, and $N = 4$ by default, corresponding to the P2 to P5 layers. In order to sum up all the feature maps together, we use interpolation to resize the $P_3$ to $P_5$ feature maps to match the size of the $P_2$ feature map. Eqs. (6) and (7) demonstrate that the weights used to fuse the layers are dynamically generated by the layers themselves.\nSuch a design can automatically deal with ROIs of different sizes. Most importantly, this module ensures that the final extracted ROI features contain more accurate scale information. Adopting this more flexible method helps mitigate issues such as mapping deviation and inconsistencies in feature extraction caused by inaccuracies in proposal scales. Taking advantage of this novel mechanism SSFF, PointOBB-v3 achieves a much higher AP50 compared to our conference work PointOBB [28] (40.18% vs 37.31% on DIOR-R testing set and 50.44% vs 33.31% on DOTA-v1.0 testing set, see in Tab. 4)."}, {"title": "4.3 Angle Acquisition Module", "content": "To enable orientation learning without direct angle supervision, we start by taking into account the natural symmetry properties of objects, which, as discussed earlier in Sec. 3.2, play a crucial role in understanding object orientation under weakly supervised scenarios. Previous research has examined symmetry within the context of HBox supervision [19]. Our findings reveal that symmetry-based self-supervised learning demonstrates resilience to annotation noise. This suggests that it is feasible to achieve accurate angle prediction even with only a single point annotation."}, {"title": "4.3.1 Dense-to-Dense Assignment", "content": "As outlined in Sec. 4.1.1, during the second stage, an angle acquisition module is developed based on the rotated/flipped view, incorporating a self-supervised angle branch to facilitate angle learning. Both views are passed through feature extractors with shared parameters, such as ResNet50 [63] and FPN [64], to generate dense feature pyramid representations. In the absence of scale information, grid points across all feature levels within the central region surrounding the ground-truth points are selected as positive samples. For positive samples associated with the same point label at a given level, their predicted angles are averaged to derive the final prediction value."}, {"title": "4.3.2 Dense-to-Sparse Matching", "content": "When matching dense feature-based angle predictions with sparse feature-based proposals, directly searching for the nearest grid points to a proposal's center is not an appropriate approach. Because of the possible disparity between the receptive field of the grid points and the scale of the object proposals, the angle predictions might not accurately correspond to the actual object region. To address this issue, we implement a hierarchical pairing strategy, ensuring a consistent alignment between the receptive fields used for angle prediction and the scale of the proposals. Given a feature pyramid denoted as $lvls_{fea} = {P_1,P_2,...,P_P}$ with $P$ levels, proposals are categorized into the level $lvl_{prop}$ based on their scale, aligning with the ROI assignment strategy used in two-stage object detection algorithms like Oriented R-CNN [4]. The orientation of each proposal is determined using the average angle predictions from the central region of the proposal on the feature map $lvl_{prop}$, as illustrated in Fig. 2. This DS matching strategy can effectively aggregate the dense angle predictions to correspond to the sparse object proposals."}, {"title": "4.3.3 Self-Supervised Angle Loss", "content": "Leveraging the affine transformation relationship between the original view and the rotated/flipped view, object angles are learned using the Self-Supervised Angle (SSA) loss. If the enhanced view is created through a random rotation by an angle $\u03b8'$, the angle predictions from both the original and rotated views should align with and uphold the same rotational relationship. When the enhanced view is produced through a vertical flip, the angle predictions must account for differences of $k\u03c0$, where $k$ is an integer ensuring the results remain within the same periodic cycle. The loss between the outputs of two views can be formulated as:\n$L_{rot} = min_{k\u2208Z} \\sum_{p=1}^{P} l_{angle} (\u03b8_{rot}^p \u2013 \u03b8_o^p, k\u03c0 + \u03b8')$\n$L_{flip} = min_{k\u2208Z} \\sum_{p=1}^{P} l_{angle} (\u03b8_{flip}^p + \u03b8_o^p, k\u03c0)$,\nwhere $l_{angle}$ refers to the SmoothL1 loss, while $\u03b8_o^p, \u03b8_{flip}^p,$ and $\u03b8_{rot}^p$ denote the angle predictions at the $p$-th level of the feature pyramids for the original view, the rotated/flipped view by flipping, and the rotated/flipped view by rotation, respectively. And the SSA loss is represented as:\n$L_{SSA} = L_{rot} + L_{flip}$."}, {"title": "4.4 End-to-End Framework Design for PointOBB", "content": "Our end-to-end framework is illustrated in Fig. 4. Building upon the original MIL-based framework, we introduced a detection branch utilizing a Rotated FCOS head [30]. This new branch shares the backbone and neck parameters with the MIL branch, ensuring parameter efficiency. To enable joint training of the two branches, we align the outputs of the MIL head and the Rotated FCOS head by calculating $L_{class}, L_{box},$ and $L_{ctr}$, which represent the classification loss, bounding box regression loss, and the centerness loss, respectively.\nMoreover, to address the adverse impact of low-quality predictions from the MIL head on the detection branch during joint training, we propose an IAW strategy. This strategy dynamically assigns smaller weights to lower-quality predictions from the MIL head during loss computation, thereby reducing their influence on parameter optimization. This mechanism effectively filters out poor-quality predictions, ensuring better alignment and improving the overall training process, and we will elaborate on it further in the next section.\nWith the above pipeline design, the model can be trained and inferred in a more convenient and time-efficient end-to-end manner. As demonstrated in Tab. 3, compared to"}, {"title": "4.4.2 Instance-Aware Weighting Strategy", "content": "During the joint training process of the two branches, the predictions from the MIL head vary significantly in quality, especially in the early stages of training, where low-quality predictions dominate. Aligning the two branches using these low-quality predictions would inevitably have a detrimental effect on parameter optimization. Clearly, assigning the same weight to all instances during loss computation is not an appropriate approach. Intuitively, we aim to assign smaller weights to lower-quality predictions during loss calculation to reduce their negative impact on optimization.\nBased on this analysis, we propose the IAW strategy. Specifically, for each instance, we calculate a dynamic weight $w_j$ by multiplying its class score with its instance score, and the final loss is obtained by summing the weighted losses across all instances, which can be formed as:\n$L_{e2e} = L_{class} + \\sum_j W_j \u00b7 (L_{box} + L_{ctr})$,\nwhere $w_j = S_{io}^{cls} S_{io}^{ins}$, $j$ means the $j$th instance. Under this dynamic weighting strategy, lower-quality predictions, which tend to have lower classification and instance scores, will contribute less to the final loss. This achieves the goal of filtering and screening out poor-quality predictions, enhancing the overall joint training process."}, {"title": "4.5 Optimization with Overall MIL Loss", "content": "Apart from the previously introduced SSC and SSA losses", "as": "n$L_{init} = -\\frac{1}{IC} \\sum_{i=1}^{I} \\sum_{c=1}^{C} {Q_{i,c}log(S_{i,c}) + (1 - Q_{i,c})(1 \u2013 log(1 \u2013 S_{i,c}))}$,\nwhere $I$ denotes the number of proposal bags in the batch, $C$ represents the total number of categories, $Q_{i,c}$ is the one-hot encoded category label, and $S_{i,c}$ corresponds to the score for the c-th category in $S_i$. For the refined MIL"}]}