{"title": "EXPLORING RESPONSE UNCERTAINTY IN MLLMs: AN EMPIRICAL EVALUATION UNDER MISLEADING SCENARIOS", "authors": ["Yunkai Dang", "Mengxi Gao", "Yibo Yan", "Xin Zou", "Yanggan Gu", "Aiwei Liu", "Xuming Hu"], "abstract": "Ensuring that Multimodal Large Language Models (MLLMs) maintain consistency in their responses is essential for developing trustworthy multimodal intelligence. However, existing benchmarks include many samples where all MLLMS exhibit high response uncertainty when encountering misleading information, requiring even 5-15 response attempts per sample to effectively assess uncertainty. Therefore, we propose a two-stage pipeline: first, we collect MLLMs' responses without misleading information, and then gather misleading ones via specific misleading instructions. By calculating the misleading rate, and capturing both correct-to-incorrect and incorrect-to-correct shifts between the two sets of responses, we can effectively metric the model's response uncertainty. Eventually, we establish a Multimodal Uncertainty Benchmark (MUB) that employs both explicit and implicit misleading instructions to comprehensively assess the vulnerability of MLLMs across diverse domains. Our experiments reveal that all open-source and close-source MLLMs are highly susceptible to misleading instructions, with an average misleading rate exceeding 86%. To enhance the robustness of MLLMs, we further fine-tune all open-source MLLMs by incorporating explicit and implicit misleading data, which demonstrates a significant reduction in misleading rates. Our code is available at: https://github.com/Yunkai696/MUB", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities across various benchmarks . Evaluating the reliability and robustness of MLLMs' responses is essential for advancing the development of explainable artificial intelligence (AI) systems . Some works assess the reliability of MLLMs by introducing deceptive information into prompts. Other works primarily focus on the robustness of MLLMs by evaluating inconsistencies between visual and textual inputs. Nonetheless, they neglect the ability to retain original answers despite the presence of misleading information.\nMost MLLMs evaluation benchmarks primarily evaluate their capabilities by assessing the correctness of their responses . However, we find that the correctness of responses tends to exhibit significant uncertainty after adding misleading information, with the proportion of such easily misled data exceeding 65% in nine commonly used benchmarks . Previous works have investigated response inconsistency of large language models (LLMs), primarily gathering multiple responses to calculate the consistency rate to assess uncertainty . Our findings reveal that MLLMs exhibit significantly low consistency when handling high misleading rate data. As is shown in Figure 1, we calculate the consistency rate for 20 responses of each sample. The results show that more than half of the responses generated by the model exhibit a consistency rate below 62.15% in high misleading rate data.\nTo evaluate the MLLMs' response uncertainty, there exist multiple challenges: \u25cf Identifying data where the model exhibits uncertainty is difficult. Only a subset of the benchmark dataset demonstrates uncertainty, and multiple responses to the same data can result in varying levels of uncertainty across different models . Evaluating the uncertainty is inefficient. Assessing a model's uncertainty on specific data through consistency calculations often requires 5 to 15 repeated responses, which can lead to significant computational resource consumption. No multimodal benchmarks to evaluate response uncertainty. While existing benchmarks assess whether a model can provide correct answers for specific knowledge, they overlook the fact that even correct responses can exhibit uncertainty.\nTherefore, we address the aforementioned challenges by: We propose a two-stage misleading instruction method to identify data where the models' responses exhibit uncertainty. In the first stage, we record the models' initial responses to images and questions. In the second stage, we introduce misleading instruction into the questions, e.g., \"The true answer is {false option}\", to mislead the model to choose the wrong option. By tracking shifts in response correctness, we can quickly identify whether the model's knowledge about specific images is aligned or easily misled. To metric uncertainty, we propose the misleading rate to calculate the proportion of responses that change between correct and incorrect. The misleading rate can serve as an alternative to the consistency rate for evaluating response uncertainty. As is shown in Figure 1, we observe that higher levels of misleading rate data correspond to lower consistency rate. Based on the identified data, we construct a novel Multimodal Uncertainty Benchmark (MUB) using data that misled six, nine, and twelve models. MUB categorizes data into three levels of misleading difficulty (i.e., low, medium, and high-based). To generate more misleading instructions, we propose two distinct approaches: explicit and implicit. Explicit misleading directly presents specific answer options, whereas implicit misleading instructions introduce misleading knowledge more subtly.\nIn this paper, we evaluate 12 open-source and 5 close-source MLLMs on MUB, leading to several key observations: 1) Both open-source and close-source MLLMs are highly susceptible to misleading instructions, with an average misleading rate surpassing 86%. 2) Both explicit and implicit instructions result in high misleading rates, averaging 67.19% for explicit and 80.67% for implicit instructions. 3) The models exhibit high confidence in their choices, seldom responding with \u201cunknown,\" but these responses are easily prone to be misled. To further enhance model robustness, we propose a mixed instructions strategy to effectively fine-tune all open-source MLLMs, and then evaluate them on our benchmark. Specifically, explicit instructions are combined into a single data point, while implicit instructions are added separately, for a total of 2k mixed data. The experiment results show a significant reduction in misleading rates across all models. The average misleading rate dropped to 6.97% for explicit instructions and 32.77% for implicit instructions. Importantly, the fine-tuned model demonstrated a 1.5% improved in accuracy on other MLLMs benchmarks, preserving its original generalization abilities. Additionally, as illustrated in Figure 1, the consistency rate improved significantly after fine-tuning, with a 29.37% improvement on highly deceptive data. Overall, our contributions can be summarized as follows:\n\u2022 We propose a misleading instruction approach to efficiently identify uncertain data and present the misleading rate as a metric to quantify MLLMs' response uncertainty."}, {"title": "2 METHODOLOGY", "content": "In this section, we first define the consistency rate and misleading rate and introduce misleading instructions to extract uncertain data. Subsequently, in \u00a7 2.1, we use the uncertain data to construct the Multimodal Uncertainty Benchmark (MUB). In \u00a7 2.2, we detail the generation of explicit and implicit misleading instructions. In \u00a7 2.3, we describe the mixed data strategy and the fine-tuning details of the MLLMs to align with the misleading instruction data. The overall framework is illustrated in Figure 2.\nPreliminaries. In this work, we mainly focus on the multimodal multi-choice and true/false tasks. Formally, given a dataset $\\mathcal{D} = \\{(X_i, R_i)\\}_{i=1}^n$, where $X_i \\in \\mathcal{X}$ represents the multimodal input for the i-th sample, consisting of text and image, represented as $X_i = (T_i, I_i)$. The corresponding output is denoted as $R_i \\in \\mathcal{R}$. The model $M : \\mathcal{X} \\rightarrow \\mathcal{R}$ generates responses $R_{ij}$ for the input $X_i$, where j denotes the j-th run or variant of input. For discriminative tasks, if the response R is correct, we set $C(R) = 1$; otherwise, the $C(R) = 0$.\nConsistency Rate. To evaluate the uncertainty of a model's responses, a common approach is to calculate the most frequent response from multiple outputs generated by the model across multiple runs. This method quantifies the model's prediction uncertainty using a metric known as the consistency rate (CR), which measures the model's reliability in producing stable responses to identical inputs. For each sample i, the model is independently run $m_i$ times with the same input $X_i$, resulting in a set of responses $R_i = \\{R_{ij} | j = 1, 2, ..., m_i\\}$, where $R_{ij}$ responses produced by the model on the j-th run for input $X_i$. To quantify the frequency of each response R within the set $R_i$, we define $f_i(R)$, which calculates how often a specific response R appears across the $m_i$ runs: $f_i(R) = \\sum_{j=1}^{m_i}I(R_{ij} = R)$, where I is the indicator function, taking the value 1 if $I(R_{ij} = R)$ and 0 otherwise. The consistency rate for the i-th sample, denoted as $CR_i$, is defined as the proportion of the most frequent response R in $R_i$ relative to the total number of responses, where $CR_i = \\text{MAX}_{R \\in R_i} f_i(R)/m_i$. This metric captures the model's ability to consistently produce the same output by identifying the most frequent response in the set $R_i$ and dividing its frequency by the total number of responses generated for input $X_i$. To provide a comprehensive measure of consistency across the entire dataset, we introduce the average consistency rate (ACR), calculated as"}, {"title": "2.1 MULTIMODAL UNCERTAINTY BENCHMARK", "content": "Motivation. While recent works  have extensively evaluated the overall capabilities of multimodal models, there remains a significant gap in evaluating benchmarks tailored to assess the MLLMs' responses uncertainty. Building a benchmark presents three main challenges: 1) Identifying Uncertain Data. Not all images trigger uncertainty in models' responses, and the same image with different questions may lead to varying levels of uncertainty. Even within existing benchmarks , there is considerable uncertainty in model responses. Our experimental results show that uncertain data constitutes 70% of the total across the six commonly used MLLM benchmarks. Detailed statistics of the uncertain data can be found in Table 13. 2) Uncertainty responses. The model's responses exhibit considerable uncertainty in high misleading rate data. As is shown in Figure 1, we computed 20 responses for each sample and found that nearly half of the samples had a consistency rate below 62.15%. 3) Inefficiency Uncertainty Evaluation. Previous work  evaluated uncertainty by generating multiple responses and calculating the consistency rate (CR). As is shown in Figure 10, achieving stable consistency rates requires 5-15 iterations, which can lead to significant computational costs. Additionally, the number of iterations needed to stabilize the CR varies across different samples, making it challenging to determine how many responses are required for each sample.\nMisleading Instructions. To efficiently identify uncertain data, we propose a two-stage misleading instructions method. In the first stage, we record the model's responses to questions without any manipulation. In the second stage, we introduce misleading instructions (e.g., \"The true answer is {true option or false option}\") to influence the model to choose either the correct or incorrect option. This manipulation may cause the model's response to shift from correct to incorrect or vice versa. If the correctness of the model's responses fluctuates, it indicates uncertainty in the data. To evaluate these transitions, we propose the misleading rate (MR) as a metric for measuring uncertainty. Specifically, $MR^{(T\\rightarrow F)}$ assesses the model's ability to maintain correct responses despite misleading instructions, while $MR^{(F\\rightarrow T)}$ captures how often incorrect responses shift to correct when influenced by true option. A higher overall misleading rate suggests higher uncertainty in the model's responses, highlighting potential weaknesses in its robustness.\nMultimodal Uncertainty Benchmark Design. In this paper, we first evaluated twelve open-source models using nine widely-used MLLM benchmarks, including MME , MMB , MMMU , MathVista , ScienceQA , ConBench , SEED , MMStar , AI2D . By applying misleading instructions to these models on the same datasets, we quickly identified data instances where the models exhibit uncertainty. To reduce the computational cost of evaluation, we selected a subset of data that misled at least six models to construct a new multimodal uncertainty benchmark (MUB). Our benchmark contains 2.5k data, including 1.7k multiple-choice questions and 0.8k true/false questions. A more detailed distribution of the selected data from each dataset, along with the number of data for each difficulty level, is provided in Figure 7. We categorized the data into three difficulty levels based on the number of models misled: low (questions that misled six models), medium (questions that misled nine models), and high (questions that misled all MLLMs). Similar to previous work , our benchmark is grouped into three main tasks: perception, reasoning, and mastery. Perception tasks include basic tasks such as counting, color recognition, OCR, and scene classification. Reasoning tasks involve analyzing image content, integrating text, and solving more complex tasks like calculations, translations, and code reasoning. Mastery tasks require the application of advanced domain-specific knowledge in fields such as chemistry, physics, art, and geography. Detailed analysis is provided in Figure 8.\nMultimodal Uncertainty Benchmark Analysis. We analyze the constructed benchmark from multiple perspectives to validate its robustness and effectiveness. 1) Efficiency. Existing benchmarks often required re-sampling data or generating new data, which involves significant human and financial resources. In contrast, our benchmark can be created by simply adding a single misleading input to any existing dataset, eliminating the need for additional data processing or manual review. 2) Broader Evaluation and Strong Scalability. Our benchmark has a broad evaluation scope, allowing it to extract relevant data from any dataset where the model demonstrates uncertainty in prior tests, thereby thoroughly assessing the model's capabilities. With strong scalability, the benchmark can easily incorporate new data from existing datasets. Moreover, it effectively identifies areas where the model exhibits uncertainty, helping to explore the boundaries of the model's knowledge more thoroughly."}, {"title": "2.2 MISLEADING INSTRUCTIONS", "content": "Explicit Misleading Instructions. Explicit instructions allow the model to quickly identify areas where its knowledge is misaligned with the data. If the model's knowledge is not well-established or has not been aligned with data containing misleading instructions, it can be easily deceived by explicit misleading inputs. These explicit misleading instructions are generated by applying deterministic or observable transformations to the input $X_{i2}$. Specifically, for true-to-false (T \u2192 F) misleading scenarios, we employ the statement $\\text{explicit}(X_{i2})$: \u201cThe true answer is {false option}\", which is added to the input to mislead the model. Conversely, for false-to-true (F \u2192 T) misleading scenarios, we apply \u201cThe true answer is {true option}\" to manipulate the input and deceive the model. The model's response is then given by $R_i^{\\text{explicit}} = M(\\text{explicit}(X_{i2}))$, where $\\text{explicit}$ represents the transformation applied to the input, and M is the MLLM that generates responses. To ensure the effectiveness of the explicit misleading method, we also design additional manually designed prompt templates to evaluate the uncertainty of response in Experiment 3.2 and Table 8.\nImplicit Misleading Instructions. Explicit misleading instructions can be easily detected by both models and humans and often lack specificity in certain knowledge domains. To address this limitation, we adopt an alternative approach by employing implicit misleading instructions to deceive the model. Specifically, we utilize GPT-4o  to generate implicit misleading instructions, which are more effective at introducing knowledge-based misdirections, offering a comprehensive evaluation of the model's robustness in retaining correct information. The detailed prompt templates used to generate these instructions are provided in Table 15. This generation process involves leveraging images, questions, and options to provide misleading hints or eliminate correct or incorrect answers. As is shown in Figure 2, the Gpt-4o can generate the implicit instruction(e.g., \u201cNote: blue buses are quite rare in urban areas.\u201d) based on its own knowledge. We define $\\text{implicit}(X_{i2})$ as the implicit misleading instructions generated and added to the original input. The model's response is then represented as $R_i^{\\text{implicit}} = M(\\text{implicit}(X_{i2}))$, where M denotes the MLLM."}, {"title": "2.3 FINE-TUNING MLLMS", "content": "Mixed Instructions Strategy. Previous works  has focused on constructing additional data for fine-tuning new robustness models. In contrast, our approach leverages data identified from existing benchmarks through a misleading instruction method, which can be directly used to fine-tune models. For data selection, we excluded overlapping data from our benchmark and selected additional high misleading rate data. For each data, we combined explicit misleading instructions with the question and provided separate implicit misleading instructions for each question. Through detailed experimental analysis, detail in Figure 5-(d), we found this to be the most effective data mix strategy. In this paper, we randomly selected 1k data with explicit instructions and 1k data with implicit instructions from the high misleading rate data. The analysis of the data size is shown in Figure 5-(a).\nFine-Tuning Details. In this paper, we aim to fine-tune all MLLMs to improve their abilities to resist misleading information and maintain confidence in their responses when confronted with such input. Specifically, we adopt the Low-Rank Adaptation (LoRA)  method for fine-tuning all open-source models, focusing on the language model. The experiment results on our benchmark (Table 2) show that all the fine-tuned MLLMs show a significant reduction in the misleading rate. To further verify the robustness improvements of the fine-tuned models, we selected 100 data for each of the four models from categories with zero, low, and high misleading rates. We evaluated four MLLMs, including GLM4V-9B-chat , MiniCPM-Llama3-v2.5 , LLaVA-Next-34b and Phi-3-vision by generating 20 responses for each data. As shown in Figure 1, the mean consistency rate of the models increased significantly. The average consistency rate increased by 29.4% on high misleading rate data, while it improved by 14.8% on low misleading rate data. Additionally, to ensure data diversity, we selected our data from a variety of other benchmarks. To confirm that the fine-tuning process did not degrade the model's performance on other tasks, we evaluated the fine-tuned models on the MMStar and AI2D datasets. The results in Table 21 show an improved fluctuation of approximately 1.5% in accuracy. Furthermore, when tested on our benchmark, we observed an average accuracy improvement of 2%, detailed in Table 22."}, {"title": "3 EXPERIMENT", "content": "We employ our Multimodal Uncertainty Benchmark (MUB) across various scenarios to comprehensively study the impact of MLLMs' response uncertainty. The experiments are designed to investigate the following research questions:\n\u2022 RQ1: What's the performance of MLLMs under misleading instructions input?\n\u2022 RQ2: How do our fine-tuning strategies impact MLLMs' performance?\n\u2022 RQ3: What additional insights can be gained from the analysis of the MUB?"}, {"title": "3.1 EXPERIMENTAL SETUPS", "content": "Datasets, models and implementation details. To ensure fairness, we evaluate the performance of various MLLMs using widely used benchmarks to ensure robust evaluation across diverse metrics and scenarios. The benchmarks are detailed in \u00a7 2.1. And the detailed MLLMs in Appendix A.2. In the alignment stage, we train only the connector for one epoch and the batch size = 1. We selected the AdamW optimizer and employed a cosine learning rate scheduler to gradually reduce the learning rate. The initial learning rate was set to 1e-4, with a warmup phase covering the first 5% of the total training steps. The training is implemented in PyTorch using 1 Nvidia A800 GPU."}, {"title": "3.2 MAIN RESULTS (RQ1)", "content": "Obs.1. High misleading rate in 12 open-source MLLMs across 9 widely-used multimodal benchmarks. To effectively identify misleading data, we add explicit misleading instructions (e.g. \"The true answer is {true option or false option}\") to the original questions. We assess 12 multi-modal large language models (MLLMs) using 9 widely-used benchmarks to evaluate their susceptibility to uncertainty. The detailed results are provided in Appendx 3. The experimental findings reveal that all MLLMs are highly vulnerable to misleading information, with the average misleading rate for transitions from true to false (AMR$(T\\rightarrow F)$) around 65.39% and from false to true (AMR$(F\\rightarrow T)$) approximately 83.35%. To provide a clearer visualization of the misleading rates, Figure 3 illustrates the performance of 7 open-source MLLMs. Notably, the Cogvlm-chat and Qwen-vl-chat exhibit higher misleading rates for both MR$(F\\rightarrow T)$ and MR$(F\\rightarrow T)$. Regarding the datasets, the MMStar, MMMU, MME and MMB are more susceptible to being misled compared to other datasets. We also show the MR$(T\\rightarrow T)$ and MR$(F\\rightarrow F)$ result in Appendix 4.\nObs.2. High misleading rate on 12 open-source and 5 close-source models on our benchmark. We evaluate five close-source and twelve leading open-source models on our benchmark, which incorporates both explicit and implicit misleading instructions, as detailed in Table 1. For the implicit misleading instructions, we utilize GPT-4o to generate five implicit misleading prompts for each data point. If any of these attempts successfully misled the model, it was considered a successful instance of misleading. Close-source models generally exhibit greater robustness against misleading input than open-source models. Among the close-source models, GPT-4o and Qwen-VL-Chat-max demonstrate the highest resilience, while Claude3-Opus-V records the highest misleading rate (MR$(T\\rightarrow F)$) among the close-source models. In contrast, for open-source models, there is no clear correlation between model size and susceptibility to misleading input. Larger models, such as LLaVA-Next-34b and Yi-VL-34b, exhibit high misclassification rates, as do smaller models like Phi-3-vision. We also evaluate the MR$(F\\rightarrow T)$ of 17 MLLMs, details are shown in Appendix 5.\nObs.3. Other explicit misleading instructions also show high misleading rates for 12 open-source MLLMs. We designed 12 explicit misleading instructions to verify the MLLMs' performance on low misleading scenarios, primarily including subjective judgment, evidence-based reasoning, correct answer declaration, and other answer references. The mean values of MR$(T\\rightarrow F)$ and MR$(F\\rightarrow T)$ were computed based on these 12 explicit misleading instructions. As is shown in Figure 4-(a), the results show that Yi-VL series and Qwen-VL-Chat model exhibit relatively high misleading rates, while the InternVL-Chat-V1-5 model shows more resistance to misleading instructions among open-source models. More detailed results of the 12 explicit misleading instructions are provided in Appendix 6, 7, and detailed categories classifications in Appendix 8.\nObs.4. GPT-4o demonstrates stronger implicit misleading instruction generation. A critical metric for evaluating the generated implicit misleading instructions is their degree of implicitness. To assess this, we compared the implicitness scores of close-source models GPT-4o and GLM-4V with those of open-source models InternVL-Chat-V1-5, Qwen-VL, and Phi-3-Vision. We used GPT-4o to assess the implicitness of instructions generated by each model. In each of the 100 samples, a model earns one point if its instructions are deemed more implicit than another's. The prompt template is shown in Appendix 16. The final implicitness score is the average of these points. As shown in Figure 4-(b), GPT-4o generates more implicit instructions compared to the other models. The implicitness scores of the open-source models in guiding the generation of incorrect answers are relatively similar. However, the implicit instructions produced by InternVL-Chat-V1-5 and Qwen-VL are more implicit in guiding the model to provide incorrect answers compared to the close-source GLM-4V."}, {"title": "3.3 FINE-TUNED MLLMS' PERFORMANCE (RQ2)", "content": "Obs.1. Misleading rate of 12 finetuned MLLMs significantly decreases. To validate the effectiveness of easily misled data, we fine-tuned all 12 open-source MLLMs with no overlap data from our benchmark. As is shown in Table 2, the results show that the MR$(T\\rightarrow F)$ significantly reduced both explicit and implicit misleading across various difficulty levels after fine-tuning. The explicit misleading rate MR$(T\\rightarrow F)$ is average 6.9%, while implicit misleading rate MR$(T\\rightarrow F)$ is average 32.6%, indicating that fine-tuned models are more robust to misleading information. The results validate the importance of aligning the MLLMs to misleading information domains. We also evaluate the MR$(F\\rightarrow T)$ of 12 MLLMs on our benchmark, shown in Appendix 17.\nObs.2. Effects of Different Fine-Tuning Strategies on MLLM. We conducted the following ablation experiments to evaluate our fine-tuning strategy: (1) Assessing the impact of different data scales on the performance of fine-tuned models. During the data scaling stage, the model was provided with each piece of explicitly misleading data separately. As shown in Figure 5, we evaluated the impact of varying data scales on fine-tuning with explicit and implicit instructions. The results indicate that misleading rates stabilize when the dataset size exceeds 1,000 samples. (2) Applying various explicit and implicit fine-tuning strategies. We tested several data strategies for fine-tuning MLLMs, including combining different misleading instructions and using more diverse misleading instructions (Table 19). For explicit instructions, our results indicate that combining or fine-tuning them separately has minimal impact on performance when the data is sufficient. In contrast, for implicit instructions, combining the data leads to worse performance compared to fine-tuning them separately. Therefore, during fine-tuning, we integrate explicit instructions using the combined method and fine-tune implicit instructions separately. (3) Fine-tuning with only explicit instruction data to test on implicit misleading. As shown in Table 19, we fine-tuned MLLMs with explicit instructions to assess the misleading rate of implicit instructions. The results show that although the overall decrease in misleading rate is not significant, it emphasizes the importance of fine-tuning models with implicit data. (4) Evaluating the effectiveness of common Chain-of-Thought (CoT) defense strategies against misleading information. We employed standard CoT  techniques by incorporating the prompt \u201cthink step by step\" into the instructions. As shown in Appendix 12, the misleading rate remains high, suggesting that standard CoT-based defensive strategies are ineffective in mitigating misleading information. (5) Verifying that the fine-tuned models maintain high resistance to misleading information on other datasets. To verify the effectiveness of our fine-tuned MLLMs, we also evaluated them on SEED-Bench. The results show that the AMR$(T\\rightarrow F)$ is 7.02% and AMR$(F\\rightarrow T)$ is 15.63%, as detailed in Table 20."}, {"title": "3.4 OTHER ANALYSIS OF THE MULTIMODAL UNCERTAINTY BENCHMARK (RQ3)", "content": "Obs.1. Knowledge within categories vulnerable to hallucinations is more susceptible to being misled. As shown in Figure 6-(d), we analyze the distribution of knowledge across three levels of misleading information (low, medium, and high misleading rates) and in three distinct cognitive abilities (perception, reasoning, and mastery). In Figure 8, categories such as GIA, landmarks, celebrities, OCR, and positional categories are particularly vulnerable to misleading information. In scenarios with medium misleading rates, tasks such as numerical calculations, existence verification, and science and engineering are more prone to errors. Such knowledge is comparatively more susceptible to being misled, consistent with current findings in hallucination research . More results are shown in the Appendix A.3. We also analyzed the responses and found that the model selected each option with equal frequency; the detailed result is shown in Figure 7."}, {"title": "4 RELATED WORKS", "content": "Uncertainty of MLLMs. Uncertainty estimation in the responses of LLMs has been extensively explored in recent research . Studies have shown that hallucinations contribute significantly to uncertainty in model outputs . Concurrently, evaluations of MLLMs under inconsistencies between visual and textual inputs have been conducted to assess their robustness . Other works have focused on enhancing the trustworthiness  and robustness of MLLMs. However, previous studies have not assessed MLLMs' response uncertainty when encountering misleading information. In this work, we address this gap by analyzing and quantifying MLLM uncertainty under these conditions, offering insights into their real-world reliability.\nAdversarial prompts. Previous studies have primarily focused on attacking LLMs and MLLMs by appending adversarial suffixes to prompts, effectively performing jailbreak attacks . Other works have evaluated the reliability of MLLMs in resisting deceptive information embedded within prompts , such as in MAD-Bench  and AVIBench , which assess models' robustness against adversarial visual instructions. Additionally, the MMR dataset  reveals that MLLMs are fragile to leading questions despite understanding visual content. Differing from these approaches, our work explores how to mislead MLLMs by adding misleading information to the original question without modifying it, focusing on both explicit and implicit strategies to induce the model to change its answer."}, {"title": "5 CONCLUSION", "content": "In this work, our two-stage pipeline misleading instructions method provides an effective framework for measuring the response uncertainty of Multimodal Large Language Models (MLLMs). By analyzing both correct-to-incorrect and incorrect-to-correct shifts in model responses, we reveal significant vulnerabilities in current MLLMs, which often exhibit high uncertainty. Based on our findings, we advocate for the incorporation of more misleading information during the training process of MLLMs to enhance their robustness and ensure consistent multimodal intelligence."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We provide the detailed experimental implementation details in the Appendix. We will make our codes, checkpoints, and JSON files publicly available to facilitate the replication and verification of our results upon publication."}, {"title": "A APPENDIX", "content": "A.1 RELEATED WORKS\nMultimodal large language models (MLLMs). Building on the success of Large Language Models, recent research has increasingly focused on MLLMs . MLLM has become an increasingly hot research topic in recent years. These include both open-source models, including MiniCPM-v-v2  , Phi-3-vision , Yi-VL-6b , Qwen-VL-Chat , Deepseek-VL-7b-Chat , LLaVA-NeXT-7b-vicuna , MiniCPM-Llama3-v2.5 , GLM4V-9B-chat , CogVLM-chat , InternVL-Chat-V1-5 , LLaVA-Next-34b , and Yi-VL-34b . On the other hand, close-source models, including GPT-4o , Gemini-Pro , Claude3-Opus-V , and Glm-4V .\nA.2 MAIN RESULTS\nComparison of 12 MLLMs with 9 widely-used benchmarks. As is shown in table 3, we provide the detailed result of MR$(T\\rightarrow F)$ and MR$(F\\rightarrow T)$ of twelve MLLMs on nine widely-used datasets. It can be observed that the AMR$(T\\rightarrow F)$ across the 12 models on the 9 datasets is 65.39%. In contrast, AMR$(F\\rightarrow T)$ is higher than 83.35%. The results show that We also provide the MR$(T\\rightarrow T)$ and MR$(F\\rightarrow F)$ result in Table 4 which are very close to 100% and show minimal variation.\nComparison of 17 MLLMs with explicit and implicit misleading instructions. We also provide the MR$(F\\rightarrow T)$ result of 17 MLLMs on our benchmark, which incorporates both explicit and implicit misleading instructions, as detailed in Table 5. The categorization from low to high misleading rate problem types corresponds to an increase in misleading rates observed in the experimental results. Additionally, it can be noted that the final results show minimal differences between the explicit and implicit misleading methods in the False-to-True experiments.\nComparison of 11 MLLMs with 12 different explicit misleading prompt templates. We provide the MR$(F\\rightarrow T)$ and MR$(T\\rightarrow F)$ results of 11 MLLMs with 12 different explicit misleading prompt templates on our benchmark in Table 6. This indicates that a specific prompt is not required to achieve misleading effects, as various forms of explicit misleading can yield similar outcomes. Table 8 presents the complete content of all explicit misleading prompt templates used in our experiments. The differences in results among the various explicit misleading prompt templates were relatively minor. Therefore, we selected the template with the highest misleading rate and the most straightforward language expression, namely \u201cAnd the true answer is:", "categories": "Subjective Judgment", "Evidence-Based Reasoning\", \"Correct Answer Declaration\", and \"Other Answer Reference\". The average misleading rates for each category are shown in Table 7. It can be observed that in MR$(T\\rightarrow F)$, the \u201cCorrect Answer Declaration": "ategory achieved the most effective results. Overall, the \u201cOther Answer Reference\" category demonstrated a relatively weaker misleading effect.\nMisleading result with mixed 5 different explicit misleading prompt templates. We conducted a new misleading experiment by randomly selecting one of five explicit misleading prompt templates for each instance: \u201cAnd the true answer is:\u201d, \u201cGiven the evidence, the answer appears to be\u201d, \u201cWhile it might seem otherwise, the correct answer is:", "It's apparent that the answer is": ", and \u201cConsidering all factors, the answer likely is:", "And the true answer is:": "shown in Table 9. This further supports the effectiveness of explicit misleading strategies.\nAblation study of with and without images to generate implicit misleading instructions. We independently evaluated the generation of implicit misleading information by GPT-4-o in both image and non-image settings under a high-misleading scenario, as shown in Table 10. The results indicate that the implicit effects of generating content with and without images are nearly identical. This is likely due to the high-misleading scenario data containing a substantial amount of specialized knowledge, allowing misleading information to be generated effectively by the language model alone. The generated implicit misleading instructions included the correct answer options. In table 11, we compared the rate of generating misleading instructions by masking portions of the content that contained the correct options. Since the implicitly generated misleading information could potentially reveal the answers, we also evaluated the results after masking these answers. In the F-T scenario, the findings suggest that when the correct options are masked, the rate of misleading instructions decreases significantly.\nDefense strategy on implicit misleading instructions. As a defense mechanism against implicit misleading, we added the statement \u201cYou need to think step by step. Your aim is to ensure your response is as accurate and relevant to the image as possible"}]}