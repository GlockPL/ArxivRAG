{"title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs", "authors": ["Mingcong Lu", "Jiangcai Zhu", "Wang Hao", "ZhengLi", "Shusheng Zhang", "Kailai Shao", "Chao Chen", "Nan Li", "Feng Wang", "Xin Lu"], "abstract": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as multi-turn\ndialogues or in-context learning, thanks to their bidirectional attention on prefix\nsequences. However, prefix LLMs have an inherent inefficient training problem in\nmulti-turn dialogue datasets. In addition, the attention mechanism of prefix LLM\nmakes it unable to reuse Key-Value Cache (KV Cache) across dialogue rounds to\nreduce generation latency. In this paper, we propose a novel masking scheme called\nIntermittent Semi-working Mask (ISM) to address these problems. Specifically, we\napply alternate bidirectional and unidirectional attention on queries and answers\nin the dialogue history. In this way, ISM is able to maintain the high quality of\nprefix LLM and low generation latency of causal LLM, simultaneously. Extensive\nexperiments illustrate that our ISM achieves significant performance.", "sections": [{"title": "1 Introduction", "content": "Recent developments in Large Language Models (LLMs) such as GPT-3.5 Ouyang et al. [2022]\nand GPT-4 Achiam et al. [2023] have attracted significant attention. Due to the powerful generation\ncapability, LLMs have made remarkable achievements in different kinds of Natural Language Process\n(NLP) tasks through a unified generative paradigm. Specifically, the most natural and common way\nto interact with LLMs is through multi-turn dialogues. However, as the number of dialogue rounds\nincreases, ensuring high quality and low latency of the generated answer by LLMs is a challenge.\n\nExisting language models can be grouped into three categories according to framework architecture:\nEncoder-Decoder Vaswani et al. [2017], Raffel et al. [2020], Lewis et al. [2020], Encoder-Only\nKenton and Toutanova [2019], Liu et al. [2019], Dong et al. [2019], and Decoder-Only Brown et al.\n[2020], Touvron et al. [2023a,b], Du et al. [2022]. Nowadays, most LLMs belong to decoder-only\narchitecture, in this paper, our discourse is delimited to decoder-only architecture. In addition, based\non the masking methods in various attention mechanisms, decoder-only category further includes\ncausal decoders Brown et al. [2020], Touvron et al. [2023a] and prefix decoders Du et al. [2022]. The\nformer employs unidirectional attention masking to restrict each token can only attend to preceding\ntokens and itself. Both the input and generated tokens are processed in a uniform manner within"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 LLMs for Multi-turn Dialogues", "content": "Recently, the advancements in LLMs have garnered lots of attention. To enhance the multi-turn\ncapabilities of open-sourced LLMs, initial efforts begin with collecting human-ChatGPT dialogues,\nleading to the creation of Vicuna Chiang et al. [2023]. RealChat Zheng et al. [2023] later expands the\ndata to 1 million conversations. To generate more sophisticated datasets, Baize Xu et al. [2023] and\nUltraChat Ding et al. [2023a] employ alternating GPT interactions. Further, Cue-CoT Wang et al.\n[2023] and ICL-AIF Fu et al. [2023b] enhance model capabilities for multi-turn interactions through\nIn-Context-Learning (ICL) and Chain-of-Thought (CoT) algorithms.\n\nTo investigate LLMs' multi-turn ability, several works construct multi-turn dialogue benchmarks.\nDaily Dialogue Li et al. [2017] is a human-written high-quality multi-turn dialogue dataset. It reflects\ndaily communication way and covers various topics about daily life. KdConv Zhou et al. [2020] is a\nChinese human-labeled knowledge-grounded dialogue dataset for knowledge-driven conversation\ngeneration. NaturalConv Wang et al. [2021] is a Chinese multi-turn topic-driven conversation dataset,\nwhich contains conversations from six domains."}, {"title": "2.2 Evaluation of LLMS", "content": "The quality of the content generated by LLMs can be evaluated from both objective and subjective\nperspectives. The former aims to evaluate the capabilities of LLMs in an objective and quantitative\nway, usually by comparing LLMs' outputs with corresponding ground truth. Similarity metrics are\ncalculated based on the outputs and references, such as BLEU Papineni et al. [2002], and ROUGE\nLin [2004]. An alternative approach involves tasks as a classification task and forces LLMs to output\nclosed options for accuracy calculation, such as MMLU Hendrycks et al. [2020] and CEVAL Huang\net al. [2024]. The latter is applied to compare the performances of different LLMs by humans or other\npowerful LLM judges when facing more complicated scenarios. GPTScore Fu et al. [2023a] makes\nfull use of GPT3 in achieving customized, multi-aspect and training-free evaluation. Chatbot Arena\nZheng et al. [2024] showcases high agreement between GPT-4 and humans on making judgements.\nIt indicates that leveraging LLM like GPT-4 for evaluating the quality of generated text could be a\nviable alternative to manual human assessment."}, {"title": "3 Methodology", "content": "In this section, we first give the mathematical description of causal LLM, prefix LLM, and our ISM.\nThen, we elaborate on the benefits of our ISM compared to causal and prefix LLM, respectively. In\naddition, we introduce how to implement our ISM on existing prefix and causal LLMs."}, {"title": "3.1 Mathematical description", "content": ""}, {"title": "3.1.1 Transformers: SSA, LSA", "content": "Given a sequence of input vectors $X = (x_1,\\ldots ,x_n)$, the output of standard Softmax Self-Attention\n(SSA) layer is\n\n$x_j \\leftarrow x_j + PVXsoftmax(X^T K^T Qx_j)$\n\nwhere P corresponds to the output projection, V, K, Q corresponds to the value, key, and query\ntransformation, respectively."}, {"title": "3.1.2 Causal LLM, Prefix LLM, and ISM for Multi-turn dialogue", "content": "Here, we briefly introduce the attentions of causal LM, prefix LM, and ISM in their LSA ver-\nsion for multi-turn dialogue. Given a multi-turn dialogue sample including the prompt (P =\n$(x_1,\\ldots, x_{mp})$), queries $(Q_i = (x_{m_{i-1}+1},\\cdots, x_{mq_i}), i = 1,\\ldots, n)$ and answers$(A_i =$\n$(x_{mq_i+1},..., x_{ma_i}), i = 1,\\ldots, n - 1)$, that is $[P, Q_1, A_1,\\cdots, Q_{n-1}, A_{n-1}, Q_n]$. The goal of the\nmodel is to predict $A_n$ of $Q_n$ using the context $[P, Q_1, A_1, \u00b7 \u00b7 \u00b7, Q_{n-1}, A_{n-1}]$.\n\nThe most classic form of attention is categorized as full (or bidirectional) attention, shown in equation\n3.2, in which each input $x_j (j = 1,\\ldots, n)$ can attend to all positions. Full attention is typically used\nin the transformer encoder.\n\nFurthermore, another transformer decoder for in-context learning uses the auto-regressive attention\n\n$x_j \\leftarrow x_j + PV\\sum_{i=1}^{j}x_i(x_i^T K^T Qx_j)$\n\nwhich restricts each token $x_j$ to attend only to previous positions (and itself) from 1 to j. This\nrestriction is due to the role of the decoder as a causal language model (causal LLM) which predicts\nthe next token in the context of the previously generated ones.\n\nThe original transformer uses a full attention based encoder and an auto-regressive attention based\ndecoder. However, NLP researchers have a preference for models that are either encoder-only (e.g.\nBERTDevlin et al. [2019]) or decoder-only (e.g. GPTRadford et al. [2018], Brown et al. [2020],\nPaLMChowdhery et al. [2023], Anil et al. [2023]) according to the task at hand. One motivation for\naltering model is that it reduces the number of parameters by half.\n\nA new form of attention, that lies intermediate to full attention and auto-regressive attention, emerged\nfrom the understanding that certain tasks can gain advantages from a prefix sequence, such as\nmulti-turn dialogue and in-context learning. This attention in prefix LLM suggests the following:\n\n$x_j \\leftarrow x_j+PV \\sum_{i=1}^{max(j,n_{q_n})} x_i(x_i^T K^T Qx_i)$\n\nWhere max(j, nqn) ensures each prefix token xj with j < nqn can attend to all prefix tokens.\nIn our ISM, the form of which suggests the following:\n\n$x_j \\leftarrow x_j + PV \\sum_{i=1}^{f(j)} x_i(x_i^T K^T Qx_j)$\n\nwhere f(j) is\n\n$f(j) =\n\\begin{cases}\nn_{q_1} & \\text{for } j <= n_{q_1} \\\\\nn_{q_k} & \\text{for } n_{a_{k-1}} < j <= n_{q_k}, k > 1 \\\\\nj & \\text{for } n_{q_k} < j <= n_{a_k}, k >= 1,\n\\end{cases}$"}, {"title": "3.2 Convergence in Multi-turn dialogue learning", "content": "To simplify the model in Multi-turn dialogue learning, we analyze Linear regression, which is a\nclassical machine learning problem. Given a set of vectors $QA_1 = ((q_j, a_j))$(where $j = m_{i-1} +$\n$1, ..., m_i, i = 1, ..., n, m_0 = 0$), which contains a set of input-label pairs, the goal is to find an optimal\nweight vector w that minimizes the 12-loss:\n\n$L(w) = \\sum_{k=1}^{m_n} |w q_k - a_k|_2^2$\n\nThe gradient of the loss is $wL = \\frac{1}{mn}\\sum_{k=1}^{mn} (w q_k - a_k)q_k$ and a gradient descent algorithm with\nstep size a follows the update rule:\n\n$w^{(1)} = w^{(1-1)} + \\frac{\\alpha}{M_n}\\sum_{k=1}^{m_n} (a_k - w^{(1-1)}q_k)q_k$\n\nAccording to Von Oswald et al. [2023], the input is formulated as\n\nX =$\\begin{pmatrix} x_1^{(0)} \\cdots x_n^{(0)} \\end{pmatrix}$, where $x_j^{(0)} = \\begin{pmatrix} q_j \\\\\na_j \\end{pmatrix}$\n\nand the parameter matrices of are:\n\n$K = Q = \\begin{pmatrix} I_{dxd} & 0 \\\\ 0 & 0 \\end{pmatrix}$, V=\\begin{pmatrix} 0 \\\\ w^{(0)} \\end{pmatrix}$,P = $\\frac{\\alpha}{M_n}$-I,\n\nwhere $w^{(0)}$ is an initial weight vector. Furthermore, there is workDing et al. [2023b] proves that\nmulti-layer LSA under the construction of progresses identically to multi-step gradient descent. The\nprososition is that for a multi-layer LSA satisfying the construction 7 and with $w^{(0)} = 0$, if its input\nZ is formatted as 8, then its l-th layer output is $x^{(1)}$ = $\\begin{pmatrix} q_j \\\\\n \\alpha_j^{(1)}  \\end{pmatrix}$T, where $\u03b4_j^{(1)} = a_j - \\langle w^{(1)} ,q_j\\rangle$ and\n$w^{(1)}$ is the 1-th updated weight from the gradient descents update rule in 6. Since qj never changes,\nwe can simplify it and focus only on $\u03b4_j^{(1)}$, which is the last output coordinate of the j-th LSA-layer for\n1 > 0,\n\nwith $\u03b4_j^{(0)} = a_j$. Defining $a_j = a_j -  \\langle w^{(0)} ,q_j\\rangle$ and rearranging 9, for $a_j^{(0)} = 0$ and l > 0, we have:\n\n$\\delta_j^{(l)} = \\delta_j^{(l-1)} - \\frac{\\alpha}{M_n} \\sum_{k=1}^{m_n}  \\delta_k^{(l-1)}  \\langle q_k,q_j\\rangle$\n\n$a_j = - \\frac{\\alpha}{M_n} \\sum_{k=1}^{m_n} (a_k - a_k^{(l-1)}) \\langle q_k,q_j\\rangle$\n\nFor prefix LM, 9 and 10 are the the update rules. Mainwhile, a causal LM applies auto-regressive\nattention throughout the entire sequence. Therefore, plugging the same K, Q, P, V into 2, the update\nrules of 9, and 10 become:\n\n$\\delta_j^{(l)} = \\delta_j^{(l-1)} - \\frac{\\alpha}{M_n} \\sum_{k=1}^{j}  \\delta_k^{(l-1)}  \\langle q_k,q_j\\rangle$\n\n$a_j = - \\frac{\\alpha}{M_n} \\sum_{k=1}^{j} (a_k - a_k^{(l-1)}) \\langle q_k,q_j\\rangle$\n\nFor ISM, the update rules of 9, and 10 become:"}, {"title": "3.3 ISM vs. Causal LLM", "content": "As we proofed in Section 3.2, ISM converges to the stationary point w* in most situations (mn >\nmn-1) and causal LM may not converge to w*. It indicates that in multi-turn dialogue learning, ISM\nwill perform better than causal LM when the length of query from the last dialogue is longer than 1.\n\nTo apply our ISM to causal LLMs, we modify the causal mask as shown in Figure 1 a) into ISM as\nshown in Figure 1 c) to apply bidirectional attention on queries in dialogue history."}, {"title": "3.4 ISM vs. Prefix LLM", "content": "Compared with prefix LLM, the standout benefit of our ISM is its ability to reuse the KV cache\nacross dialogue rounds, effectively reducing the generation latency.\n\nKV Cache is a commonly used acceleration technique when LLMs perform generation. How-\never, it's not reusable between dialogue rounds in prefix LLM. Given dialogue history h =\n[P, Q1, A1,\uff65\uff65\uff65, Qn-1, An-1] and current query Qn to generate An. Assuming we have dia-\nlogue history's KV cache Ch, for causal LLM and our ISM, we only need to compute the current\nquery's KV cache Cq and concatenate them to get the whole sequence's cache [Ch; Cq], so that we"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "Daily Dialogue. Daily DialogueLi et al. [2017] is a human-written high-quality multi-turn conversa-\ntion dataset. The dialogues in the dataset reflect daily communication way and cover various topics\nabout daily life. It contains 13,118 dialogues, the average turns of each dialogue is 7.9.\n\nKdConv. KdConv Zhou et al. [2020] is a Chinese knowledge-driven conversation dataset. It grounds\nthe topics in multi-turn conversations to knowledge graphs. It contains 4.5K conversations from three\ndomains (film, music, and travel), and 86K utterances with an average turn number of 19.0.\n\nNaturalConv. NaturalConv Wang et al. [2021] is a Chinese multi-turn topic-driven conversation\ndataset, which allows the participants to chat anything they want as long as any element from the\ntopic is mentioned. It contains 19.9K conversations from six domains, and 400K utterances with an\naverage turn number of 20.1."}, {"title": "4.2 Implementation Details", "content": "For Daily Dialogue, we choose Llama2-7b as the base model and apply our ISM in it as described in\nSection 3.3. We call the modified model Llama2-7b (ISM). We finetune them on the train split with\nthe same settings. Specifically, we utilize 4 NVIDIA A100 GPUs to full-finetune Llama2-7b and\nLlama2-7b (ISM) on Daily Dialogue's train split for 5 epochs, respectively. We train models with\nAdamW Loshchilov and Hutter [2018], the learning rate is set as 3e-5, global batch size is 64.\n\nFor KdConv and NaturalConv, we choose Qwen1.5-7b as the base model. As these two datasets are\nboth in Chinese and topic-driven. We mix their train split to finetune Qwen1.5-7b and Qwen1.5-7b\n(ISM) and evaluate on the mixture of their test split. The training settings are the same as Llama2-7b's.\nIn addition, to evaluate our ISM on prefix LLM, we apply ISM in ChatGLM as illustrated in Section\n3.4 and utilize KdConv and NaturalConv to train ChatGLM-6b and ChatGLM-6b (ISM).\n\nFor evaluation, we follow previous works Zheng et al. [2024] to apply powerful GPT-4 as a stand-in\nfor human judge. We design appropriate instructions to ask GPT-4 to compare the answers between\nthe base model and our corresponding ISM version and to decide which answer is better or tie.\nConsidering the position bias Zheng et al. [2024], we swap the position of answers and compute the\naverage result. Specific prompts can be seen in Appendix A.2. In addition, we randomly select 200\nsamples for human evaluation to compare the consistency between GPT-4 and human judgement."}, {"title": "4.3 Quantitative Analysis", "content": "We report the GPT-4 pairwise comparison results in Table 1. For causal model, we compare Qwen\n1.5-7b and Qwen 1.5-7b (ISM) on the mixture test split of KdConv and NaturalConv. We can find"}, {"title": "4.3.1 Human Evaluation", "content": "Designing instructions and applying GPT-4 to evaluate LLM's performance has been widely adopted,\nbut it's still a subjective evaluation method. Therefore, we randomly select 100 samples from Daily\nDialogue and KdConv&NaturalConv separately for human evaluation to compare the judgement\nconsistency between human and GPT-4. As shown in Figure 3, we observe a high level of consistency\nbetween human and GPT-4 evaluation, 81% and 78% in Figure 3 a) and b), respectively. Furthermore,\nit is notable that the majority of inconsistencies are centered around determining whether there is a\ntie or not. It demonstrates that the evaluation results from GPT-4 are relatively reliable."}, {"title": "4.4 Latency Analysis", "content": "As described in Section 1, prefix LLM suffers from high generation latency in multi-turn dialogues\ndue to the inability to reuse the KV cache across dialogue rounds. In this section, we conduct\nexperiments to compare the time-to-first-token (TTFT) latency between AntGLM-10B and AntGLM-\n10B (ISM). Specifically, we construct a simulated dialogue sample consisting of 20 turns, with each\nturn having a sequence length of 200. The query length is 40, and the answer length is 160. We deploy\nthem on 2 NVIDIA A10 GPUs and repeatedly call the inference API 10 times to calculate the average\nTTFT. As shown in Figure 4, it's evident that the TTFT latency of AntGLM-10B surges quickly as\nthe input sequence length expands. Conversely, the TTFT of AntGLM-10B (ISM) increases much\nslower due to the reuse of the KV cache. Specific analysis can be seen in Appendix A.1"}, {"title": "4.5 Case Study", "content": "In Figure 5, we showcase two cases including dialogue sample, generated responses and GPT-4\njudgement results. We can find that GPT-4 follows our instructions well to make judgements. In\nthe first case, GPT-4 not only considers the dialogue history, but also takes account of User 2's\n\"willingness\" and \"responsibility\". Cases with longer dialogue history can be seen in Appendix A.3."}, {"title": "4.6 Limitation", "content": "Current ISM is applied to existing causal or prefix base models for finetuning. The data utilized\nfor finetuning is relatively small compared with the data used during their pretraining stage. The\nperformance of our ISM in the pretraining stage needs to be explored."}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we propose a novel attention masking scheme called Intermittent Semi-working Mask\n(ISM) to achieve the high generation quality and low generation latency in multi-turn dialogue.\nSpecifically, we apply alternate bidirectional and unidirectional attention on queries and answers in\nthe dialogue history. In this way, our ISM is able to maintain the advantages of both prefix LLM\nand causal LLM simultaneously. Extensive experiments illustrate that our ISM achieves significant\nperformance. In the future, we will try to apply our ISM to the pretraining stage of LLM and delve\ndeeper into exploring its advantages."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Inference Acceleration Estimation on ISM", "content": "ISM inference can be accelerated by reusing the KV Cache across turns. We analyze the computational\ncomplexity of transformers before and after acceleration.\n\nGiven the following variable definitions for a Transformer model:\n\n\u2022 b = batch size\n\u2022 s = input sequence length\n\u2022\nns = new input sequence length\n\u2022 h = hidden dimension\n\u2022 hn = number of attention heads\n\u2022 hd = dimension per attention head, where $h_d = \\frac{h}{h_n}$"}, {"title": "A.1.1 Before Acceleration", "content": "The FLOPs for the self-attention layer (Fself-attention) can be estimated as follows:\n\n\u2022 Computing queries, keys, values:\n\n$F_{qkv} = 3\\cdot2\\cdot b\\cdot s\\cdot h^2 = 6 \\cdot b \\cdot h^2 \\cdot s$\n\n\u2022 Computing attention scores:\n\n$F_{as} = 2\\cdot2\\cdot b\\cdot h_n\\cdot s\\cdot h_d\\cdot s = 4 \\cdot b \\cdot h \\cdot s^2$\n\n\u2022 Computing weighted sum:\n\n$F_{ws} = 2\\cdot b\\cdot h^2 \\cdot s$\n\n\u2022 The total FLOPs for self-attention is therefore:\n\n$F_{self-attention} = F_{qkv} + F_{as} + F_{ws} = 4 \\cdot b \\cdot h\\cdot s^2 + 8\\cdot b\\cdot h^2 \\cdot s$\n\nThe FLOPs for the feed-forward network layer (Fffn) is:\n\n$F_{ffn} = 16 \\cdot b \\cdot h \\cdot s^2$\n\nThus, the total FLOPs for transformers (Ftransformers) combines both the self-attention and feed-forward\nFLOPS:\n\n$F_{transformers} = F_{self-attention} + F_{ffn} = 4 \\cdot b \\cdot h \\cdot s^2 + 24 \\cdot b\\cdot h^2 \\cdot s$\n\nOverall, the computational complexity of the Transformer is quadratically related to the input\nsequence length"}, {"title": "A.1.2 After Acceleration", "content": "By reusing the KV cache from the previous turn of dialogue, we can trim the input sequence to\nthe new uncached segment. In this way, it is equivalent to replacing one s(input sequence length)\nin the aforementioned computation process with n_s (new input sequence length), thus the total\ncomputational complexity after inference acceleration is:\n\n$F_{accelarated-transformers} = 4\\cdot b \\cdot h\\cdot s\\cdot n_s + 24 \\cdot b\\cdot h^2 \\cdot n_s$\n\nWhen ns is much smaller than s and h, $n_s^2$ can be neglected. Therefore, the total computational\nFLOPs is approximately 4\\cdot b\\cdot h\\cdot s + 24 \\cdot b \\cdot h^2, where the relationship between computational cost\nand input sequence length becomes a linear polynomial. Thus, the computational cost is optimized\nfrom a quadratic polynomial of input sequence length to a linear polynomial."}, {"title": "A.2 Prompt", "content": "We design instruction prompt carefully as shown in Figure 6 to leverage GPT-4 in assessing the\nquality of responses from the models to be compared. The whole prompt can be divided into three\nparts, the first part describe the task and the specific evaluation criteria, the second part contains two\ncases and corresponding human written evaluation result for few-shot learning, the third part contains\nsample to be judged."}, {"title": "A.3 Cases", "content": "We showcase two cases with longer dialogue history and corresponding GPT-4 judgements, shown in\nFigure 7 and 8. From Figure 7 we can observe that facing dialogues with longer history, our ISM is\nable to attend history context better and generate more reasonable response."}]}