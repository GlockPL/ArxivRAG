{"title": "How Do Programming Students Use Generative AI?", "authors": ["CHRISTIAN RAHE", "WALID MAALEJ"], "abstract": "Programming students have a widespread access to powerful Generative AI tools like ChatGPT. While this can help understand the learning material and assist with exercises, educators are voicing more and more concerns about an over-reliance on generated outputs and lack of critical thinking skills. It is thus important to understand how students actually use generative AI and what impact this could have on their learning behavior. To this end, we conducted a study including an exploratory experiment with 37 programming students, giving them monitored access to ChatGPT while solving a code understanding and improving exercise. While only 23 of the students actually opted to use the chatbot, the majority of those eventually prompted it to simply generate a full solution. We observed two prevalent usage strategies: to seek knowledge about general concepts and to directly generate solutions. Instead of using the bot to comprehend the code and their own mistakes, students often got trapped in a vicious cycle of submitting wrong generated code and then asking the bot for a fix. Those who self-reported using generative AI regularly were more likely to prompt the bot to generate a solution. Our findings indicate that concerns about potential decrease in programmers' agency and productivity with Generative AI are justified. We discuss how researchers and educators can respond to the potential risk of students uncritically over-relying on generative AI. We also discuss potential modifications to our study design for a large-scale replications.", "sections": [{"title": "INTRODUCTION", "content": "With the public release of generative AI (GAI) tools such as OpenAI Codex [5], ChatGPT [27] and GitHub Copilot [12], students in programming courses now have access to code authoring tools capable of solving coding tasks and exam questions entirely [2, 5, 10, 11, 34]. However, such tools also tend to confidently present incorrect information [3, 16, 37] or generate subtly incorrect code [6] which may be difficult to detect for beginners [29, 36]. Due to the relative novelty of these tools, ChatGPT for example having been released in November 2022, their impact is not yet fully understood, with ongoing research into applications and challenges. Nonetheless, the tools are already being used by numerous students [30]. This increasingly requires educators to respond.\nA recent interview study with individual students revealed interest in using the technology for purposes like generating supplementary learning materials, but also skipping coursework they don't find engaging enough [43]. Meanwhile, university instructors are largely unsure about how many of their students are using ChatGPT and to what extent [30]. Across multiple surveys and position papers, educators have expressed concerns, particularly about over-reliance on generated content and about the ease of cheating [7, 17, 21, 30, 43].\nIn response, even educators with a generally positive sentiment towards GAI have begun imple- menting restrictions and bans in the classroom. Long term, two major approaches appear to emerge from this development: either a) embracing GAI as a tool not only for teaching but also as a core software development tools, or b) preventing and restricting its use [21]. Both approaches involve considerable work, as the curriculum and course design need to be modified to either include or exclude these tools from the classroom and the students' overall learning journey.\nBefore taking such impactful decisions, it is particularly important to understand how program- ming students are actually using GAI tools and whether the usage strategies would primarily help them learn or rather avoid coursework. This work takes the first step into this direction. We"}, {"title": "STUDY DESIGN", "content": "To answer RQ1, we evaluated the task-solving performance of GPT-3.5 and GPT-4 on all exercise assignments of a large first-semester introductory programming course. To answer RQ2 and RQ3, we conducted an experimental study with students in the same course, who volunteered to solve an exercise while having access to a monitored ChatGPT interface. We recorded and labeled the students' interactions with ChatGPT, and performed pattern analysis on the resulting chat logs."}, {"title": "Research Methodology", "content": "To answer RQ1, we evaluated the task-solving performance of GPT-3.5 and GPT-4 on all exercise assignments of a large first-semester introductory programming course. To answer RQ2 and RQ3, we conducted an experimental study with students in the same course, who volunteered to solve an exercise while having access to a monitored ChatGPT interface. We recorded and labeled the students' interactions with ChatGPT, and performed pattern analysis on the resulting chat logs."}, {"title": "Task-Solving Evaluation", "content": "For all assignments that must to be solved to pass the programming course, we created individual prompts to be submitted to the GPT API. Some exercises are split into subtasks with different levels of granularity. We separated these tasks into minimal congruent subtask groups, i.e. subtasks that shared the same project context, such that only immediately related subtasks were joined into a single prompt. Additionally, if the exercise required knowledge of an existing template project for students to build off of, that project was also appended to the prompt text. The full input to the language model consisted of a short system prompt instructing it to solve a Java programming exercise, the exercise text as given to students, and \u2013 if applicable a code snippet containing the project context. We evaluated GPT-3.5 using the gpt-3.5-turbo-1106 model, and GPT-4 using the gpt-4-1106-preview model. The temperature parameter for both models was set to 0.0 to reduce randomness in the output. We inserted the generated solution into the appropriate project files, performed the usual tests tutors do for the exercises to pass, and recorded the results alongside the generated code.\nAs the course relies heavily on manual reviews by course tutors, any submitted code not only needs to meet the functional requirements of the exercises, but also match the paradigms and coding conventions taught in the course. In fact, the majority of in-person exercises included in the studied course did not have automated unit tests to verify the student solutions. Instead, in addition to code reviews, course tutors performed a series of manual tests, by asking students to demonstrate specific behaviors using the interactive BlueJ object interface. These required code reviews and manual tests were specified in the tutor instructions for each exercise.\nThe criteria applied by course tutors can be divided into the following categories:"}, {"title": "Task Requirements", "content": "The student answer must fulfill the requirements of the task itself. For code authoring tasks, this includes being syntactically and logically correct (Syntax and Logic), as well as meeting all functional requirements (Reqs). For other knowledge tasks"}, {"title": "Student Experiment", "content": "In the experimental study, we presented students with an optional code authoring exercise that they were asked to solve to the best of their ability. Additionally, participants were given access to a ChatGPT-like chatbot to assist them with the exercise. The use of the bot was not required. We recorded participants code changes, chatbot conversations, copy/paste events, and their solution submissions. The exercise was realized as a quiz activity on the Moodle platform, with two introductory self-assessment Likert scale questions and a CodeRunner coding exercise using the same configuration as described in the next section. Students were used to this e-learning platform as about 20% of their usual assignments were handled there.\nThe chatbot UI featured a subset of the functionality of ChatGPT. Participants could create conversation threads and hold a back-and-forth conversation with the chatbot. Responses were gradually revealed as they were generated, and rendered with full Markdown support. Overall the bot very closely mimics the look of ChatGPT as a generic GAI tool. Figure 1 shows a screenshot of the chatbot. It uses the GPT API to generate responses to participants' prompts. In the experiment we used the gpt-3.5-turbo-1106 model, which was the latest publicly available at the time.\nData Analysis. Our data analysis includes a quantitative and a qualitative component. We collected aggregate activity metrics on the students' attempts, performance, interactions, and the timeline of events. Additionally, two authors manually labeled independently each student prompt and each chatbot response to differentiate between types of requested information and identify interaction patterns."}, {"title": "RESULTS", "content": null}, {"title": "Task-Solving Evaluation (RQ1)", "content": "The results of the task-solving evaluation are depicted on Figure 3 and Figure 4."}, {"title": "Coding Exercises", "content": "Both GPT models produced syntactically correct code in all responses. Responses from GPT-3.5 had slightly more logical errors than GPT-4, and failed to meet the exercise requirements more than twice as often. Inconsistencies between the described intent and the actual implementation were rare, though in one particularly obvious case, GPT-3.5 provided multiple line comments describing logic that was entirely absent from the code.\nDespite not being explicitly instructed to do so, large majority of the responses were aligned with what a student of the course would already know at that time.\nAdherence to the course's coding conventions was low overall. Neither language model's baseline code generation \u2013 i.e. with no existing code provided as a reference was in alignment with the course's formatting guidelines. Even with context code, the majority of generated code did not follow the demonstrated bracketing style, nor were documentation comments added consistently. GPT-3.5 also scored notably higher on Learn in exercises where project code was provided, though this is similarly related to the specific course conventions. Most of the failures to fulfill the Learn criterion were caused by use of the keyword this, which is introduced separately and later in the course than classes and constructors."}, {"title": "Writing Exercises", "content": "We found that the responses generated by GPT-4 went beyond the scope of the course material several times. These cases include mention of classes such as BigDecimal that students would not have heard of, referencing static/class methods for exercises prior to their introduction, and in one instance, discussing the thread safety of HashMap and HashSet.\nThe majority of factually incorrect responses were marked as such based on technicalities. For example, GPT-3.5 claimed that insertion into a LinkedList at an index was an O(1) operation. This is only true if you already hold a reference to an adjacent node or ignore the traversal required to reach the a node first. The course material states that the default insertion is an O(n) operation."}, {"title": "Chatbot Use (RQ2)", "content": null}, {"title": "Experiment Data", "content": "In total, 42 students participated in the experiment. 3 students participated in an initial test run with an older version of the exercise; their results were not included in the analysis. Additionally, two students ended the experiment after a short period (4.5 min. and 9 min.) without completing the tasks. We thus concluded that they dropped the participation and discarded their data. This leaves 37 participants for the analysis.\n62.2% of participants completed the entire exercise correctly, and 78.4 % successfully solved at least one problem. The average duration of experiment sessions was 941.9s (M = 859s, \u03c3 = 559.2s) for students who completed both problems, and 1916.9s (M = 1784s, \u03c3 = 493.7s) for those who did not. Across the experiment, 556 submission attempts were recorded, the vast majority of which (89 %) were for problem P1. Each participant made an average of 13.38 submission attempts for P1 (M = 12, \u03c3 = 9.5), of which 67.5% resulted in compilation errors and 22.4% in a test failure because their code ran without errors. For P2, participants made an average of 1.65 attempts (M = 1, \u03c3 = 2.32), of which 27.9% resulted in compilation errors and 13.1% failed because their code did not handle the error case.\nBefore starting the exercise, we asked participants to rate how challenging they found the course on a 5-point scale, with 1 being very easy and 5 being very difficult. We observed a moderate negative correlation between perceived course difficulty and exercise completion (r = -0.47, p = 0.003), as well as a moderate positive correlation between perceived difficulty and experiment duration (r = 0.4, p = 0.013). This indicates that the students' performance on the exercise aligned with their general experience in the course."}, {"title": "Usage of GAI", "content": "Before the exercise, participants were asked how often they use GAI tools such as ChatGPT or GitHub Copilot. 16.2 % reported to have never used such tools before. 48.6 % reported using GAI tools at least monthly (monthly through weekly or more than once a week). We observed a moderate positive correlation between the reported use of GAI tools and the frequency of chatbot interactions during the experiment (r = 0.35, p = 0.03). We did not observe any significant relationships between the students' reported GAI use and their performance on the exercise.\nAll participants had access to the chatbot interface. Still, 37.8 % did not use it at all during the experiment. In total, 177 prompts were sent to the chatbot and 172 responses were received. The remaining 5 prompts went unanswered due to network interruptions. On average, each participant submitted 4.57 prompts (M = 2, \u03c3 = 5.96). Among participants with chatbot history (PwCH) only, the average was 7.35 prompts (M = 6, \u03c3 = 6.06). We found that the exercise completion was lower, on average, among PwCH, though the difference was not statistically significant.\nThe average experiment duration varied significantly between students who did and did not use the chatbot (t = 3.3, p = 0.003). For PwCH, the average duration was 26:15 (M = 26:03, \u03c3 = 10:38). For users who did not use the chatbot, this was 14:36 (M = 10:30, \u03c3 = 10:27). To account"}, {"title": "Student Prompts", "content": "In this section, all values are relative to the participants who used the chatbot during the experiment. Figure 5 shows the total number of prompts for each type, as well as the number of participants that have submitted such a prompt at least once. Overall, we found that students submitted more Codegen prompts than Support,with a ratio of 62% to 38%.\nWe observed Solve prompts from most of the participants, as 73.9 % submitted a Solve prompt at some point during the experiment. Interestingly, we also found that some participants submitted more than two Solve prompts. Overall, the most common prompt type was Fix, though this was driven in part by two outliers, who each submitted 13 Fix prompts during their session. The least common prompt type was Explain, observed in only 39.1 % of conversations, constituting 6.8 % of prompts. Figure 6 shows the number of occurrences of each prompt type per-participant.\nWe also analyzed the distribution of prompt types among PwCH for relationships with the survey responses. Table 4 shows the results. In particular, we found a significant negative correlation between the participants' self-reported usage of GAI tools and the occurrence of Inform prompts."}, {"title": "Bot Responses", "content": "61 % of the 172 chatbot responses contained an auto-generated solution to one of the exercise problems, of which 97.1 % also contained a written explanation of the solution attempt. In total, 83.1 % of responses included an explanation, either for a generated solution, for an example code snippet, or for a general concept.\nWe found that the GPT-3.5 model was unable to solve problem P1 reliably. Out of 80 generated solutions, only 8.8 % were correct. The model performed better on P2, where 76.9 % of 13 generated solutions were correct.\n88.1 % of the Inform responses were correct, indicating that when requesting only general in- formation about basic programming concepts, GPT-3.5 does not appear to \u201challucinate\u201d. These included questions about when null-related errors occur in Java and how to produce one. Notably, one participant requested the documentation for the class NullPointerException, and the generated response correctly relayed it verbatim."}, {"title": "Autonomous Thinking (RQ3)", "content": null}, {"title": "Similarity of Consecutive Submissions", "content": "Among submissions where participants had at least one Codegen response available to them, the average similarity to the generated code was 0.65 on P1 (M = 0.68, \u03c3 = 0.26) and 0.66 on P2 (M = 0.71, \u03c3 = 0.29). These numbers align with findings by Kazemitabaar et al. [18], who observed an average similarity of 63% (\u03c3 = 42%) between generated and submitted code, albeit using the Jaccard similarity measure.\nWe found that the average similarity of consecutive submissions was significantly lower if any chatbot interactions occurred between the submissions, at 64.1% (M = 73.5%, \u03c3 = 26.6%) compared to 82.2% (M = 89.0%, \u03c3 = 20.7%) without, across all participants (t = 5.76, p < 0.0001). This indicates that students relying on the bot were rather following the bot suggestions even with completely different solution approaches, while students without the bots were rather trying to incrementally improve their solutions. In 62% of cases with generations, the modified submission was more similar to the generated code than to the previous submission."}, {"title": "Preceding Activity", "content": "To understand when participants decided to use the chatbot, we analyzed the occurrences of prompt types over time in each participant's session. Figure 7 overviews the prompt type distribution. Additionally, we reviewed the number of submission attempts and the elapsed time before the first occurrence of each prompt type, which are shown in Figure 8.\nWe found that among participants who ended up using the chatbot, most started doing so relatively early in their session (but not immediately at the start). On average, participants submitted their first chatbot prompt 6.4 minutes (M = 5, \u03c3 = 3.99) after starting the session. 43.5 % of PwCH began their chat with a Inform prompt, followed by 39.1 % with Solve and 17.4 % starting with Hint. Across PwCH, 43.5% submitted at least one Support prompt and 26.1% submitted at least one Codegen prompt before they made their first code submission attempt. None of the students submitted a generated solution as their first code submission."}, {"title": "Prompting Behavioral Patterns", "content": "Figure 9 presents an example of the interaction logs we extracted from participant sessions. We used sequence pattern mining to identify common sequences of actions in these logs. The most common patterns are reported in Figure 10 (left). We report both the total number of occurrences of each pattern, as well as how many participants have followed this pattern at least once during their session. The results show frequent cycles of incorrect submissions, followed by code generation prompts or clipboard events, followed again by incorrect submissions.\nWe found that while the most common sequences were present in the majority of PwCH interaction logs, some had comparable occurrence counts while being exhibited by far fewer participants. This is in part due to most of those patterns containing chatbot-to-quiz clipboard events (i.e. chatbot-to-coderunner), which were less common among participants overall. None of the sequences contained interaction items for a correct submission. We presume this to be because students only needed one correct submission to pass, but made many incorrect submissions prior, thereby overshadowing any possible patterns with correct submissions.\nFinally, we additionally performed the same analysis while differentiating submissions by whether they constitute a close match as per section 3.3.1. We report these results in figure 10 (right)."}, {"title": "DISCUSSION", "content": null}, {"title": "Summary of Findings", "content": "The mixed responses on self-reported GAI usage and the significant fraction of participants who did not engage with the chatbot during the experiment indicate that tools like ChatGPT do not have universal straight recognition or acceptance among students. All participants were made aware of the support chatbot at the beginning of their experiment session. Despite this, usage among those who reported no prior experience with GAI tools was low. We speculate that they may have been hesitant to try it for the first time in this environment, or they may have heard poor experience reports from peers that acted as a deterrent. Interestingly, students who reported regularly using ChatGPT-like tools had more frequent interactions with the chatbot. However, our results clearly indicate that using a chatbot (without prior guidance) for programming task requiring thinking correlates with longer time to solve the task while no impact on task success can be observed."}, {"title": "The Vicious Prompting Cycle: Fix, Apology & Explain, Copy & Refix", "content": "The experiment results show students use the chatbot for general knowledge retrieval as well as to get exercises solved (or partially solved) for them. We identified only eight Explain prompts (6.8 %) across all chat histories. Of the Explain prompts we found, four asked \u201cwhat is wrong with this code?\u201d after having received a compilation error, though without relaying the error in question. Only in two prompts did a participant ask for an explanation of the generated answer after discovering it was incorrect.\nA few factors may have contributed to this. Most significantly, unless explicitly instructed otherwise, GPT-3.5 has a tendency to be verbose [16] and confident. We observed that for 97.1% of the generated solutions, an explanation was already provided alongside it. This may have been perceived by students as sufficient, reducing their need to explicitly ask for elaboration. However, aside from compilation errors, none of the participants asked the chatbot to explain why their own solution wasn't working. In addition, either by previous experience or through their earlier prompts in the study, some participants may have gleaned that the responses from the chatbot were unreliable, and therefore concluded that they had little to gain from its explanations. The comparatively high prevalence of Inform requests, however, indicates some level of trust in the LLM's output, at least for general knowledge retrieval tasks.\nWhile reviewing the students' interactions, we noted that multiple students ended up in cycles of seeing an error message, prompting the chatbot for assistance, implementing the chatbot's suggestion, and receiving another error message. One such case is presented in Figure 9. Here, the interaction timeline shows an accelerating back-and-forth between prompts and submissions. Through our sequence analysis, we were able to quantify this pattern, which turned out to be the most common interaction pattern across all PwCH. As the GPT-3.5 model was unable to produce a correct solution to problem P1 in most cases, even in response to Fix prompts, this was a notably ineffective strategy. Nonetheless, some students appeared intent on \"coercing\u201d the model into generating a correct submission through repeated Codegen prompts. This suggests they were unable to recognize when a GAI tool is incapable of providing effective assistance.\nExplanations of potential errors in an automated system have been shown to increase trust and reliance on the system, largely mitigating the negative effect of observing an error [9]. We observed that ChatGPT often apologized and occasionally even justified its mistakes before attempting to correct itself, which may have increased the students' trust in the system. This happened in 10 experiment sessions, in fact up to 14 times in one single session (participant 28X).\nAdditionally, the repeated unmodified submission of generated code indicates these participants did not critically review the model output between generation and submission, even after being made aware of the tool's limitations. Using LLMs for code generation shifts the developer's focus from ideation [39] to parsing and debugging unfamiliar code, which prior work has shown to be a challenge for professional developers [33, 38]. However, if students do not understand the generated code they submit, their GAI usage is far more likely to be perceived as academic misconduct [30].\nSome participants began their chatbot conversations with questions related to the exercise, but without providing the necessary context. In these cases, the chatbot would then ask for more information from the user. Providing additional context has been shown to significantly improve model output on code generation tasks [35]. Presumably, participants omitting context were under the impression that the chatbot they were interacting with had already been specifically instructed on their task. Additionally, we observed participants asking for a fix to an incorrect solution without describing the problem, i.e. prompting to the effect of \"that doesn't work, try again\u201d. More often, however, participants would repeat the problem statement instead, or slightly rephrase it. One approach we observed was to repeat only the instruction itself, omitting the introduction text and context code, potentially in an attempt to \"remind\u201d the model of the task at hand. Notably, this behavior has been found to be uncommon among developers at large [15]."}, {"title": "Straight Usage of GAI Could Lead to a Generation of Lazy Thinking Programmers", "content": "When differentiating submissions by whether they constitute a close match to generated code, our sequence analysis reveals a behavioral pattern that appears to be a clear case of work avoidance: the chatbot is prompted to generate code, some or all of the code is copied and pasted into the answer box, and a submission \u2013 with very high similarity to the generated code \u2013 is made. We found this pattern to occur most often, however, it was exhibited by less than half of all PwCH. This suggests that only some students engage in this direct form of work delegation, but those that do may try the approach multiple times.\nThe pattern we identified from the most participants consists of an attempt with no close match to generated code, then a paste into the chatbot prompt window, and the submission of a Codegen prompt. This indicates that many participants sought code generation from the LLM after making a submission they had at least partially thought up themselves. This is also corroborated by the preceding activity, as the majority of students made at least two submission attempts before submitting their first Codegen prompt. Although 65.2% of PwCH submitted code that constituted a close match to the LLM's output at some point in the experiment, we also find that none did so on their first attempt. Our observations regarding the high prevalence of Codegen requests therefore do not show an immediate intent to \u201ccheat\u201d from the majority of ChatGPT-using students. Rather, it appears that only after failing to solve the problem themselves do they turn to the chatbot to delegate the remaining work."}, {"title": "Examples", "content": "We discuss a few participant-chatbot interactions which were substantially unique."}, {"title": "GCL: The Impossible Exercise", "content": "As discussed in Section 3.2.4, the solutions generated by the GPT-3.5 chatbot were often incorrect. The chatbot responses to participant GCL were particularly misleading. Over multiple attempts, the chatbot was unable to produce the correct solution to the coding problem P1. Eventually, the chatbot claimed that the provided erroneous code sample from the exercise was actually already free of issues, and thus there was no solution to P1. This led the participant to conclude that the goal of the study was to present them with an unsolvable task [GCL-M33], that they had been tricked [GCL-M37], and that there was a connection to the field of psychology [GCL-M43]. At this point, the participant closed the session and ended the experiment.\nThis interaction shows that a conversational generative AI agent may convince a learner that the exercise itself is faulty, rather than that the agent is incapable of generating a correct solution despite the prominence of warnings in tools like ChatGPT not to trust their output and validate claims through external sources. Whether students are generally aware of these limitations, or consider ChatGPT to be speaking with authority, is not clear.\nWhile only participant GCL was directly told by the chatbot that the exercise was impossible, other participants also received responses that contradicted the output of the test runner, such as reassuring a student that the generated solution would work even though it hadn't [E6R-M15]. It remains unclear whether other participants concluded the experiment under the impression that the task itself was faulty or unsolvable, or as they thought the were unable to solve the problem."}, {"title": "28X: What Changed?", "content": "In one of the very few conversations where the chatbot generated a fully correct solution to P1, the participant 28X made a mistake in copying and pasting the code into the answer textbox, and subsequently dismissed the solution they were given.\nAfter a brief interaction, the chatbot generated the correct solution to the problem and even correctly explained the mechanism behind it. However, the generated method was declared static, a modification which was not accounted for in the test runner, as students hadn't yet been introduced to class methods. After the participant had pasted the entire generated method definition \u2013 including the static modifier \u2013 into their answer box, the test runner rejected it with the message \"[...] is a class method, but it should be an instance method.\u201d [28X-S107].\nThe participant relayed the error message back to the chatbot [28X-M21] and received a corrected solution without the static modifier [28X-M22]. From this point on, however, they only copied the body of the updated method to insert back into the answer text box [28X-C16], missing the crucial change to the method head, and then told the chatbot repeatedly that the error had not been fixed yet [28X-M24, 28X-M28]. Only after several more iterations of asking for unrelated changes to the code did they finally submit the correct answer [28X-S119], nine minutes after they had initially been given the correct solution.\nThis exchange highlights a problem we briefly discussed above: generated code can incorporate concepts that a novice learner may not be familiar with yet. In our analysis of ChatGPT's ability to pass the course, this was only relevant for the student's ability to explain their work during code review. Here, it had a much more immediate impact, as the student had to navigate a novel concept entirely on their own, potentially not even realizing that they were missing crucial context. The student thus struggled to reconcile an error message they could not understand with a chatbot claiming that the error was fixed, without any guidance on what their mistake was."}, {"title": "Implications and Future Work", "content": "Our findings suggest that once students decide turn to a ChatGPT-like assistant for help, especially if they already have prior experience with generative AI, they predominantly seek code solutions rather than support in creatively working through the problem [39]. As we did not test the students' understanding of their final submission after completion, we cannot ascertain what students retained from the exercise, especially for those who were largely guided by the chatbot. Whether learners can actually benefit from GAI with their self-regulated usage strategies therefore warrants further research.\nAdditionally, we believe the perception of GAI and ChatGPT in particular should be investigated further, to understand what motivates the students' behavior and impact their prompting skills [19]. In our study, we created an environment that was as close to the students' regular working environment as possible, to elicit their typical behavior. This means we did not capture a lot of information on the students' thoughts and impressions during the experiment. Future studies could use exit surveys or think-aloud observations to gain more insight into student sentiment while using GAI tools for programming [19].\nOur ChatGPT-like chatbot did not include any task-specific instructions in the system prompt to elicit certain behavior from the model. Different LLM behaviors could encourage students to think through the problem themselves or \u201cnudge\u201d them towards finding the solution [28], potentially even beyond the model's inherent reasoning capabilities. A follow-up study could present students with a problem that the LLM is unable to generate correct code for, and compare different prompt instructions by whether they can still improve the students' own task-solving performance."}, {"title": "Limitations", "content": null}, {"title": "Task-Solving Evaluation", "content": "The temperature parameter of the LLMs was set to 0.0 to enhance replicability. However, as the models are provided through an external service, they may be updated and yield different outputs for the configuration and prompts used in this work. Some studies have reported significant changes in the behavior of GPT-3.5 and GPT-4 over time [4], though it is not clear whether these results are based on architectural changes, continuous training, or simply the non-deterministic nature of the LLMs.\nMany of the exercises in the studied course are adaptations of those found in Barnes and K\u00f6lling [1], an often cited book for teaching object-oriented programming using Java and BlueJ. We expect the performance results on the writing and in-person coding exercises to be generalizable to other courses that have drawn from the same sources when designing their exercises, aside from the atypical code style conventions of the studied course. However, the phrasing of the prompt and problem statement can significantly impact whether the LLM will solve it correctly [8]. The logical correctness of model responses may therefore be reflective of the way the problem statements in the course are written and not the LLM's actual ceiling for reasoning.\nFurthermore, the material used is entirely in German, which may affect the performance of the language models [20]. To gauge whether this would have a significant impact on our results, we reviewed some of the problems the LLMs were unable to solve and manually prompted them with English translations. This yielded responses of comparable quality, with the same logical errors."}, {"title": "Experiment", "content": "Participants may have been influenced by the experimental setting or the knowledge of their actions being recorded when deciding whether to use the chatbot. The same potential risk exists for the self-reported use of generative AI, which students may have felt discouraged from sharing honestly. They may have thought it to be more socially acceptable to report slightly lower than true ChatGPT usage numbers due to the perception of such activity as engaging in cheating [43]. However, as the results are fairly consistent with similar surveys in recent months [18, 30, 43] and we were commonly able to observe behavior that could be considered unethical in an academic context [30], we do not believe this to be a major limitation.\nParticipant self-selection poses a potential threat to the internal validity of the study. Students had to be interested and comfortable with partaking in a programming study and have the time available to participate. During recruitment, multiple students expressed general interest in the study, but feared they were not \u201cgood enough\u201d to solve a programming task in a controlled environment. Some students expressed interest but were behind on their coursework and thus needed the entire time of the lab session to work on mandatory exercises, after which point they would no longer be available. This may have biased the participant pool towards students who performed better in the course, and would therefore need less assistance from a chatbot.\nThe participant pool was comprised entirely of students of one university course in one year. Sentiments and strategies on GAI use may vary across different populations and over time, limiting the generalizability of our findings. Thus, replicating our study in different universities is desirable.\nFinally, only a single two-part exercise was used to analyze the students' behavior. The exercise was rather complex requiring comprehension and thinking. The propensity of learners to use GAI outputs in an exercise has been shown to vary significantly by exercise topic [18]. Therefore, the same study design may yield different results with a simpler algorithmic code authoring task."}, {"title": "RELATED WORK", "content": null}, {"title": "Overall Sentiment", "content": "Kasneci et al. [17", "the model simplifies the acquisition of answers or information, which can amplify laziness and counteract the learners' interest to [...": "come to their own conclusions and solutions\u201d [17, p. 5", "21": "interviewed instructors of university introductory programming courses on their sentiment towards the use of ChatGPT in education. They found that"}]}