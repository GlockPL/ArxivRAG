{"title": "Structure Development in List-Sorting Transformers", "authors": ["Einar Urdshals", "Jasmina Urdshals"], "abstract": "We study how a one-layer attention-only transformer develops relevant structures while learning to sort lists of numbers. At the end of training, the model organizes its attention heads in two main modes that we refer to as vocabulary-splitting and copy-suppression. Both represent simpler modes than having multiple heads handle overlapping ranges of numbers. Interestingly, vocabulary-splitting is present regardless of whether we use weight decay, a common regularization technique thought to drive simplification, supporting the thesis that neural networks naturally prefer simpler solutions. We relate copy-suppression to a mechanism in GPT-2 and investigate its functional role in our model. Guided by insights from a developmental analysis of the model, we identify features in the training data that drive the model's acquired final solution. This provides a concrete example of how the training data shape the internal organization of transformers, paving the way for future studies that could help us better understand how LLMs develop their internal structures.", "sections": [{"title": "Introduction", "content": "The rapid advancement of capabilities in state-of-the-art deep learning models has significantly outpaced our understanding of their internal mechanisms. This disparity poses a critical challenge for the AI community, as the deployment of increasingly powerful yet opaque models raises concerns about reliability, safety, and ethical implications. This work presents a theoretical analysis of a simplified transformer model and aims to provide insights for interpreting toy-models and understanding what drives learned solutions.\nA functional understanding of complex deep learning models is a difficult task when we don't know the fundamental building blocks that the model implements. Mechanistic interpretability addresses this challenge by aiming for a mechanistic understanding of the model, usually by reverse-engineering it and decomposing the functional structures into circuits. Most research in this field focuses on small language models (Wang et al., 2022; Hanna et al., 2023; Gould et al., 2023), with a recent breakthrough by Templeton et al. (2024) finding interpretable features in Claude-3 using Sparse Auto Encoders (SAEs) (Cunningham et al., 2023; Bricken et al., 2023).\nMotivated by research in this field, we study a simple yet fundamental question: How do neural networks develop organized internal structures as they learn? We answer this question for a single- layer attention-only transformer model trained on the task of sorting lists of numbers. Its simplicity provides a controlled environment to study the impact of various hyperparameters on the learning dynamics, in ways that would be impossible with larger models.\nThe model was originally proposed by McDougall (2023a) and interpreted by McDougall (2023b), by mechanistically decomposing the final solution found during training in Output-Value (OV) and Query-Key (QK) circuits Elhage et al. (2021). These circuits are constructed by suitably multiplying the respective"}, {"title": "Methods", "content": "In this section, we introduce our experimental setup and the key metrics we use to study how neural networks develop organized structures while sorting lists. First, we describe our baseline model architecture and explain how we vary different aspects of the model and training data to understand what drives the development of different organizational patterns. Then, we outline the quantitative measures we use to track the model's development, including mathematical tools that help us measure solution complexity and understand how different parts of the model work together."}, {"title": "Baseline Model Setup and Training", "content": "Following McDougall (2023a) we train a single-layer attention-only transformer model on input sequences of the form [8, 3, 5 SEP, 3, 5, 8], where numbers are sampled uniformly from 0 to 50 and do not repeat, producing a vocabulary size of 52. The model sorts by outputting the next number starting at the separation token, producing a list of numbers of the form [x, x, x, 3, 5, 8, x], where the positions marked with x are not included in the loss function.\nWe define the model with 2-heads, a list length of 10 numbers and a vocabulary size of 52 tokens as our baseline model. It includes a residual stream size of 96, two attention heads with head dimension of 48, and"}, {"title": "Results", "content": "In this section, we present the developmental stages and types of specialization the model goes through in Sec. 3.1, whereas in Sec. 3.2 we present results on what is driving the different specialization modes."}, {"title": "Developmental Stages and Specializations", "content": "We want to investigate how the model learns during training by looking at the evolution of the OV and QK circuits alongside various measures. In this section, we focus explicitly on the OV circuit for pedagogical reasons, and leave the QK circuits for App. B.2. In Figs. 2 and 3 we present the evolution of the baseline 2-head model during training on $D_{\\delta \\approx 4.7}$ and $D_{\\delta \\approx 2.2}^d$, respectively. The figures feature heatmaps of the OV circuits for the two attention heads, as well as an upper panel showing the LLC, the Circuit Rank and the loss evaluated on both $D_{\\delta \\approx 4.7}$ and $D_{\\delta \\approx 2.2}^d$. The models go through the following stages:"}, {"title": "What drives Head Specializations?", "content": "In this section we investigate the role of the & distribution, specifically, how this impacts head specializations and the size of the contiguous regions in the cases where vocabulary-splitting specialization is present. To this end, we train 2-head models on datasets with varying \u03b4 (see Sec. 2.1 for how we vary this parameter). Before we present results, we shall take a small detour to introduce some relevant concepts related to the QK circuit. In Fig. 5 we show the QK and OV circuits of the baseline 2-head model at the end of training. The dashed lines indicate the location of what we define to be the active QK regions for this model. The number in the top right corner of each active region corresponds to the region number in Tab. 1. We define the regions by first outlining the area above the diagonal in the QK circuit where the diagonal of the OV circuit is positive. We then separate the active regions based on which OV region they have as input and output. As an example, region 2 (see top left panel of Fig. 5) has its input covered by the top left corner of the OV circuit of head 1 (see top right panel of Fig. 5), whereas its output is covered by the middle region of the OV circuit. This is important, as the attention pattern along a row at small vocabulary (top row of top left corner of Fig. 5) goes through regions 1, 2 and 4, so the attention pattern of region 2 and 4 is"}, {"title": "Discussion", "content": "In this section we discuss the results presented in Sec. 3. In Sec. 4.1 we outline developmental stages recurring for many model setup variations, how we choose these stages and discuss an alternative approach to doing so. In Secs. 4.2-4.3 we discuss our insights regarding the head-specialization modes gained from varying the model setup."}, {"title": "Developmental Stages and How We Choose Them", "content": "A common developmental stage sequence that occurs in the baseline model and several variations of it, is illustrated in Figs. 2-3: 1) Initial Learning, characterized by rapidly decreasing loss and increasing LLC, 2) Head Overlapping, where both heads attend to and copy partly overlapping vocabularies and 3) a Head Specialization stage (one or a combination of vocabulary-splitting and copy-suppression). There are some exceptions: there is no head-overlapping stage when training without LN and with WD, and there is no head-specialization stage for the 1-head model (by definition).\nWe choose the boundaries of a developmental stage based on several factors, mostly related to significant changes in the measures and the patterns of the OV and QK circuits that are visible by eye. In the case of the loss and the complexity measures, a significant change constitutes variations in the steepness of the slope (Stage 1), reversal of the slope (Stage 2) or a local extrema (transition from vocabulary-splitting to copy-suppressing head specialization in Fig. 3). In the case of the circuits a significant change consists in (relative) changes to the diagonal values of the OV circuit and changes to the active regions in the QK gradients $V_R$. Our general heuristic consists in observing a combination of some of the changes mentioned above. An alternative approach to define stage boundaries is to always place them at local minima and maxima of relevant measures. This approach is followed by Hoogland et al. (2024). If we followed this approach and did not consult the OV and QK circuits, we would not define the intermediate head-overlap stage in our circuits, but we would still have the head-specialization stages.\nThe developmental analysis approach helped us build the hypothesis regarding the role of \u03b4 in driving head specialization. In particular the model in Fig. 3 provided an important hint for us to investigate why a model trained on a dataset with a smaller & would develop vocabulary-splitting and"}, {"title": "Vocabulary-Splitting is a Simpler State", "content": "Vocabulary-splitting head specialization is a recurrent feature of this model, even when removing LN, WD, both LN and WD or increasing the number of heads (for details, see Apps. B.3-B.7). Although the overall picture gets noisier, vocabulary-splitting is robust with respect to these regularization techniques. On the other hand, vocabulary-splitting can be trained towards copy-suppression, as we verified by further training the model from Fig. 2 on the dataset $D_{\\delta \\approx 2.2}^d$. In general, it is a simpler model, when compared to the preceding stage, where both heads attend to and copy overlapping vocabulary ranges, and its formation is always accompanied by a drop in the LLC. Importantly, the LLC decrease is indicative of a solution that is both simpler and performs well on the task at hand, which distinguishes it from other model complexity proxies such as the Circuit Rank. This is exemplified in the 2-head model without LN (see Fig. 13 in the Appendix), where the Circuit Rank decreases significantly due to WD pushing the model to a simpler state, while the model only achieves 20% accuracy. The LLC on the other hand begins decreasing only later, when the transition to vocabulary-splitting occurs.\nThe number of contiguous regions that the vocabulary-splitting models settle into seems to be deter- mined by the \u03b4 in the dataset. As can be seen in Fig. 6, the product of the QK gradient \u2207QK and \u2642 is largely model independent when varying dover an order of magnitude. This is because this product measures the ability to sort neighboring list elements, and guides the model development. Models can increase their gradi- ents by decreasing their region sizes, thereby increasing the number of regions, and do so until the difference between the two most attended to tokens is large enough that softmax essentially sets the probability of the most attended to token to 1 and the others to 0.\nThe above argument relies on several approximations, and we don't expect it to hold exactly. Some key approximations going into the constant OK\u00b7 \u03b4are:"}, {"title": "Copy-Suppression helps calibrate the copying head", "content": "Copy-suppression is a second type of head specialization we encounter. As first defined by McDougall et al. (2023) for GPT-2, it is a mechanism consisting of three stages: copying, attention and suppression. The copy-suppression we observe is visually similar to the one observed in GPT-2, in the sense that we observe two heads paying attention to the same tokens (as can be seen from the similar QK circuits, right panel of Fig. 9), where one head directly suppresses the other copying head (see OV circuits in Fig. 3). Unlike vocabulary splitting, copy-suppression is not easily trained towards a different head specialization. Training the model from Fig. 3 further on the default dataset $D_{\\delta \\approx 4.7}$ doesn't change the model away from this mode. We believe this to be due to two (somewhat related) factors:"}, {"title": "Related Work", "content": "Stagewise development in artificial neural networks is not a new field of study, see e.g. Raijmakers et al. (1996). Hoogland et al. (2024) found developmental stages, including a drop in the LLC corresponding to model simplification, when training a transformer on linear regression. The LLC evolution of non-transformer toy models has previously been studied by Panickssery & Vaintrob (2023) and Chen et al. (2023). Without using the LLC, Chen et al. (2024) studied developmental stages in BERT. Bagi\u0144ski & Kolly (2023) and McDougall (2023b) studied algorithmic transformers trained on list sorting, Nanda et al. (2023) reverse-engineered an MLP trained on modular addition, and Power et al. (2022) trained a transformer on modular addition to study grokking. In this paper, we find copy-suppression, previously observed by McDougall et al. (2023).\nPrevious research on the universality hypothesis has found evidence of specialized components called induction heads (Olsson et al., 2022) that help predict repeating sequences, a pattern previously found in larger language models.\nStudy of toy models in mechanistic interpretability has also been carried out on additional networks trained on modular arithmetric (Chughtai et al., 2023; Stander et al., 2023), and has been used to demonstrate the presence of super-position in neural networks (Elhage et al., 2022)."}, {"title": "Limitations", "content": "Our study is done on a toy model, and one should be careful to generalize our findings to larger transformers. Additionally, our interpretation of the functionality of the circuits is approximate, and we expect there is probably more going on in the model.\nIn our study of the impact of the 8 distribution, we have not done intervention studies increasing or decreasing the gradients to confirm that they shift the delta range the model sorts the best at. We also expect the location of threshold effects we observe to be sensitive to the strength of the WD. We have not checked this.\nWe have tested three different random seeds for the baseline 2-head model and found that the number of regions can fluctuate by up to 1 vocabulary region, but other results remain intact. We have not system- atically checked for seed dependence, and the outcome of different seeds can be seen in Fig. 16. We don't expect our main findings to be seed-dependent.\nThe LLC is only defined at a local minimum, which models during training never are at in practice. Lau et al. (2023) argues that the LLC value is not trustworthy, but that the relative ordering of LLCs at different stages of training is. The LLC hyperparameter selection is not an exact science, and in this paper we used the heuristics of seeking parameter space in which the LLC is locally hyperparameter independent."}, {"title": "Conclusion", "content": "We present a theoretical study of an attention-only list-sorting algorithmic transformer, extending earlier work by McDougall (2023b) on understanding how these models function internally and how learned solu- tions arise and form during training. Our developmental analysis reveals that attention heads naturally organize themselves into different specialized roles at the end of training - they either split up differ- ent numerical ranges between them (vocabulary-splitting) or develop a system where one head handles copying and sorting numbers while another head fine-tunes those predictions (copy-suppression).\nThe developmental approach gave us an important initial hint in pursuing the transition between the head specializations. This led us to discover features of the training data that strongly influence which"}, {"title": "Singular Learning Theory and the Local Learning Coefficient", "content": "Our main tool for studying model development is the Local Learning Coefficient (LLC), a theoretically well- motivated measure of model complexity based on the learning coefficient from Singular Learning Theory (SLT) (Watanabe, 2009). The LLC is defined in Definition 1 of Lau et al. (2023) as a unique rational number such that asymptotically as e \u2192 0, the volume of model parameters waround a minimum w* in which L(w) \u2013 L (w*) < e can be written as\n$V(e) = C \\cdot e^{\\lambda_{LLC}(w^*)} (-\\log e)^{m(w^*)-1} + O \\left(e^{\\lambda_{LLC}(w^*)} (-\\log e)^{m(w^*)-1}\\right)$\nwhere the positive integer m (w*) is the local multiplicity and c > 0 is a constant.\nThe LLC is a measure of the degeneracy of the loss landscape near a model's parameters w*, where a lower LLC indicates a more degenerate and less complex model. Given an empirical loss ln(w) over parameters w computed with a batch size of n, we calculate the LLC estimate at a local minimum w* similar to Hoogland et al. (2024) and Lau et al. (2023):\n$\\mathbb{E}_{w \\sim w^*, \\gamma} [\\gamma \\cdot n \\beta \\cdot \\left[l_n(w)] - l_n(w^*)\\right]]$,\nwhere $\\mathbb{E}_{w \\sim w^*, \\gamma}$ denotes the expectation with respect to a tempered posterior distribution centered at w* and \u03b2 = 1/ln(n) is the inverse temperature. y controls the localization around w*, and is set so that the sampled parameter space is sufficiently large that the volume of the local minimum is captured, but not so large that nearby local minima are included. We set y so that the LLC is locally y-invariant. The sampling is done with Stochastic Gradient Langevin Dynamics (SGLD).\nThe LLC is calculated using the DevInterp v.0.2.2 software package (van Wingerden et al., 2024). The hyper-parameters vary with the setup, and are found by performing parameter scans, where we look for regions of parameter space where the LLC is hyper-parameter independent."}, {"title": "Varying the Model Architecture and Training", "content": "In this subsection, we study the impact of varying the model architecture and training such as the number of attention heads, and the use of LN and WD."}, {"title": "1-Head Model", "content": "In Fig. 7 we show the development of a 1-head model trained on our list-sorting task. Note that the attention head has dimension 48 like the heads in the 2-head models. Like the baseline 2-head model, this model undergoes three distinct stages, with a stage of initial learning with a rapidly decreasing loss and rapidly increasing LLC, an intermediate stage where the loss and LLC is fairly constant with a decrease"}, {"title": "Baseline 2-Head Model", "content": "In Fig. 8 we show both the QK and OV circuits of the developmental stages the baseline 2-head model undergo. We observe that the QK circuit becomes more regional in the vocabulary-splitting stage, reflecting the specialization in the OV circuit."}, {"title": "3-Head Model", "content": "As shown in the first row of Fig. 11, the 3-head model (trained with head dimension of 48) features a loss that decreases rapidly until step 133 (top left of Fig. 11), where all heads attend to and copy overlapping vocabulary regions. At peak LLC (top right of Fig. 11) we see first signs of vocabulary-splitting head specialization. As the LLC drops, the overlap between their vocabulary regions decreases, resulting in contiguous regions split across three heads, with head 3 covering only a small region and starting to show signs of a negative diagonal (bottom left Fig. 11). The QK circuits also display differentiated patterns, which upon closer inspection match the active vocabulary regions of the OV circuits. So far, the developmental stages of this model, match those of the baseline 2-head model.\nAs the evolution continues, around training step 5859 (not shown) the OV circuit of head 3 specializes to a negative diagonal, seemingly suppressing the contributions from the other two heads, which behave like in the baseline 2-head model. We identify the state of head 3 to be copy-suppression. As the transition occurs, the QK circuit of head 3 also switches to uniform diagonal patterns, not differentiating any vocabulary regions anymore. This transition corresponds to a drop in the out-of-distribution loss on $D_{\\delta \\approx 2.2}^d$, and is not captured by any of the other measures."}, {"title": "4-Head Model", "content": "Similar to the other models, the 4-head model (trained with head dimension of 48) also starts with a sharp decrease in the loss until step 133 (1st row of Fig. 12). As the LLC decreases, heads begin to specialize with concurrent vocabulary-splitting and copy-suppression appearing in heads 1,3,4 and head 2 respectively (2nd row of Fig. 12). We note that this happens as the out-of-distribution loss on $D_{\\delta \\approx 2.2}^d$ decreases. The vocabulary regions are split unevenly, with head 4 covering only a very small region of the vocabulary.\nThis changes later in the training, after around 87k training steps (3rd row of Fig. 12), with heads 3 and 4 now copying similar vocabulary regions and displaying differentiated attention patterns in the QK circuits. Directly after this, at step 150k, head 3 grows to attend and copy the entire vocabulary range. It seems to transition to do the bulk of the sorting with heads 1 and 4 doing minor adjustments. This last transition is captured by a small drop in the LLC. The model remains largely unchanged after this point, until the end of training (4th row of Fig. 12), as is seen from the measures remaining fairly constant."}, {"title": "Baseline 2-Head Model without LN", "content": "Removing LN from the baseline 2-head model causes a dramatic change to the training dynamics, as shown in Fig. 13. Early in training, at steps 71-348 (top row) the model goes through a transition in which The Circuit Rank drops dramatically. During this transition, the circuits of the model form a very regular dipole-like pattern.\nThis dipole-like pattern starts breaking at steps 18298-27194 (middle row) as the LLC peaks, with a formation of the stripe-like patterns parallel to the diagonal in the OV circuit. The QK circuits cover the regions determined by the OV circuit, similar to what we have seen in the other models. This structure formation stabilizes as the LLC drops (bottom row), which is also tracked by a strong decrease in the loss on both datasets. The model never reaches 100% accuracy on list sorting, and the loss does not flat-line until step 60060, after which all the measures are stable. The LLC seems to capture the development of this model very well."}, {"title": "Baseline 2-Head Model without WD", "content": "As seen in Fig. 14, the model without WD learns to sort with the diagonal OV and positive band above the diagonal in the QK at step 133. Compared to the baseline model, the OV and QK circuits seem more noisy, and there is no drop in the Circuit Rank. The LLC still has a large drop between steps 1985 and 10066 during which the heads specialize into splitting the vocabulary, and the loss decreases further. This specialization is clearer for tokens smaller than 20 in the QK and OV circuits, less so for larger vocabulary tokens. Unsurprisingly, we note that this model achieves the lowest loss of any of the models we train. With an out-of-distribution loss of about 0.01 on $D_{\\delta \\approx 0.22}^d$, this model achieves a better loss also on this dataset."}, {"title": "Baseline 2-Head Model without LN and WD", "content": "Fig. 15 shows the evolution of our measures and the circuits for the baseline 2-head model without both LN and WD. The model seems to go via dipole-like circuits around step 45, very similar to step 71 of the baseline model without LN (compare the top left panels of Figs. 13 and 15). Instead of going via the low"}, {"title": "Varying the Dataset", "content": "In this subsection, we study the impact of varying aspects of the training data, such as the size of the vocabulary, the length of the list and the presence of perturbations in the data set. For a summary of all the models we trained, see Tab. 2 and Fig. 16. In the subsections we go into more details of a few models."}, {"title": "Baseline 2-Head Model with Vocabulary Size Increased to 202", "content": "Increasing the vocabulary size to 202 naturally rises \u03b4 to 18.4, and produces the training dynamics shown in Fig. 17. The model undergoes a fairly similar development as the baseline model, with the head spe- cialization at step 2420 (lower left of Fig. 17) very similar to the baseline model trained on $D_{\\delta \\approx 4.7}^d$. As the"}, {"title": "Baseline 2-Head Model with List Length Increased to 20", "content": "Increasing the list length to 20 yields the training dynamics shown at the top of Fig. 18, with the end- of-training OV and QK circuits shown at the bottom. We note that this model stabilizes with the larger number of regions, and does not go on to have the LLC drop further and copy-suppression forming."}, {"title": "Baseline 2-Head Model with Perturbed Dataset", "content": "We perturb the data by iterating through the dataset once, and swapping neighboring elements in the sorted list with probability 40%/(ni+1 \u2013 ni), where ni is the value of the list element i. Since the probability of neighboring elements swapping is always less than 50%, we believe that the optimal strategy still should be to sort the list ignoring the perturbations but with logits more spread out. The perturbations do, however, have a severe impact on the training dynamics, as shown in Fig. 19.\nWe don't observe any drop in the LLC, even though The Circuit Rank does drop. The heads don't specialize into vocabulary-splitting modes, but the OV circuits rather settle into what looks like opposites of each other. It looks like head 1 does copy-suppression and head 2 does copying, whereas the QK circuits behave very differently from what we have seen in the other models.\nThe losses have been computed on non-perturbed data, and goes down throughout training, though it doesn't reach as low as with the baseline model."}]}