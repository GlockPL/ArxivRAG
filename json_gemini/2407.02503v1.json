{"title": "Optimizing Deep Reinforcement Learning for Adaptive Robotic Arm Control", "authors": ["Jonaid Shianifar", "Michael Schukat", "Karl Mason"], "abstract": "In this paper, we explore the optimization of hyperparameters for the Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO) algorithms using the Tree-structured Parzen Estimator (TPE) in the context of robotic arm control with seven Degrees of Freedom (DOF). Our results demonstrate a significant enhancement in algorithm performance, TPE improves the success rate of SAC by 10.48 percentage points and PPO by 34.28 percentage points, where models trained for 50K episodes. Furthermore, TPE enables PPO to converge to a reward within 95% of the maximum reward 76% faster than without \u03a4\u03a1\u0395, which translates to about 40K fewer episodes of training required for optimal performance. Also, this improvement for SAC is 80% faster than without TPE. This study underscores the impact of advanced hyperparameter optimization on the efficiency and success of deep reinforcement learning algorithms in complex robotic tasks.", "sections": [{"title": "Introduction", "content": "In the rapidly evolving field of robotics, the development of autonomous and highly adaptable robotic arms represents a significant step towards realizing more efficient, precise, and versatile automated systems [1]. These systems find applications across a spectrum of industries, from intricate tasks in surgical robotics to the repetitive, precision-demanding processes in manufacturing [2]. The control of robotic arms, especially those with a high DOF, poses a substantial challenge due to the complexity of their operation and the need for fine-tuned coordination [3,4]. A promising answer to these challenges is DRL, bringing a framework for robotic arms to learn and optimize their activity through interaction with the environment. [2,5,6]. DRL methods have demonstrated their efficiency in solving a wide spectrum of tasks from games to the control of robotic systems [7,8,9,10].\nAmong the various algorithms under the DRL paradigm, SAC and PPO have stood out for their effectiveness in balancing exploration and exploitation, which is critical for the efficient learning of control policies in environments with high-dimensional action spaces. SAC, known for its off-policy learning and entropy"}, {"title": "Materials and Methods", "content": "DRL's application in robotics has been marked by its ability to tackle complex, high-dimensional control tasks, a domain where traditional programming methods fall short [14,15,3]. DRL is a pivotal area within machine learning that centers on teaching agents to make strategic decisions through interactions with their environment, aiming to maximize rewards. Within the DRL framework, an agent systematically engages with its environment. At each timestep t, the agent observes a current state $s_t$, selects an action $a_t$ based on its policy \u03c0, and receives a corresponding reward $r_t$, thereby transitioning to the next state $s_{t+1}$.\nThis iterative process persists until reaching a terminal state, indicating the end of an episode. The agent's overarching aim is to devise a policy that optimally"}, {"title": "Proximal Policy Optimization (PPO)", "content": "Proximal Policy Optimization (PPO) is a popular DRL algorithm known for its stability and sample efficiency. It aims to optimize the policy while ensuring that the updates are not too drastic [12]. The objective function for PPO can be expressed as:\n$L^{CLIP}(\\theta) = E_t [min(r_t(\\theta) A_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t)]$\\\nHere, $L^{CLIP} (\\theta)$ is the clipped objective function, \u00cat denotes the expected\nvalue over a finite batch of samples, $r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio\nof the current policy $\u03c0_\u03b8$ over the old policy $\u03c0_{\u03b8_{old}}$, $\\hat{A}_t$ is an estimator of the advantage function at time t, and \u03f5 is the clipping parameter [12]. Table 1 lists several key hyperparameters essential for tuning, highlighting the numerous hyperparameters associated with PPO."}, {"title": "Soft Actor-Critic (SAC)", "content": "Soft Actor-Critic (SAC) is a DRL algorithm that combines off-policy and actor-critic methods [11]. It is particularly suited for continuous action spaces. The SAC objective includes maximizing the entropy of the policy to encourage exploration. The SAC objective is given by:\n$J(\\theta) = E_{\\tau \\sim \\pi_{\\theta}} [\\sum_{t=0}^{\\infty} \\gamma^t (r(s_t, a_t) + \\alpha H(\\pi(\\cdot|s_t)))]$"}, {"title": "Tree-structured Parzen Estimator (TPE)", "content": "TPE, as named by Bergstra et al. [18], is the technique of using Bayesian optimization heuristics for guiding and speeding up the hyperparameter configuration optimization process. Its innovative approach involves the creation of a binary tree-like model that adeptly maps out the probability distributions for various hyperparameters. This model is particularly adept at navigating the complex landscapes often encountered in high-dimensional spaces, where objective functions can be costly to evaluate, thereby streamlining the optimization trajectory towards optimal solutions with efficiency and precision [18]. Mathematically, TPE optimizes by iteratively selecting hyperparameters based on the following principle:\n$\\theta^* = arg \\min_{\\theta} \\frac{P(Objective \\ better \\ than \\ current \\ best \\ | \\theta)}{P(Objective \\ worse \\ than \\ current \\ best \\ | \\theta)}$\nwhere represents the optimized hyperparameters, and P(|) denotes the\nconditional probabilities that the objective function is better or worse than the"}, {"title": "Task Definition", "content": "The reach target task, pivotal in robot manipulation, necessitates the end effector's precise targeting within Cartesian space constraints across diverse tasks [5]. The targets are randomly generated within this space to assess the arm's ability to adapt and accurately reach different points, simulating potential real-world applications [19]. Our objective is to optimize DRL for training a 7-DOF robotic arm to reach target tasks, where a policy learns the mapping from current states to subsequent actions for goal achievement. Notably, existing literature predominantly addresses position-only reaching. This study delineates the reach task by explicitly defining state, action, goal, and reward [19].\nState (St): Represents the environment's current status, comprises a vector that includes joint angles (\u03b8), the end-effector position(EEpos), and the goal position (Goalpos).\nAction (At): The action corresponds to the individual motion of each joint, an action provided by the agent to update the environment state.\nReward (Rt): As a reward function, we use a dense function, shown on\nEquation .4, where the closer the agent is to completing the task, the higher the reward.\n$R_t = -\\sqrt{(X_{EE} - X_{Goal})^2 + (Y_{EE} \u2013 Y_{Goal})^2 + (Z_{EE} - Z_{Goal})^2}$"}, {"title": "Training and Evaluation", "content": "The methodology used in this work is depicted in Fig. 2. The process encompasses two primary phases: a warm-up phase for initial hyperparameter exploration and a subsequent phase for focused training and hyperparameter refinement.\nA. Training Process Overview The training commences with an initial phase dedicated to exploring a range of hyperparameters through random sampling, aiming to identify a foundational set that yields a favorable reward outcome.\nThis stage, referred to as the warm-up, sets the groundwork for more targeted optimization.\nFollowing the initial exploration, the training enters a critical phase where the algorithm focuses on refining the model's accuracy and responsiveness. Actions are evaluated based on their success in navigating towards a set goal under the constraints of the environment, with each episode offering a fresh challenge. The objective is to systematically improve the model's performance through iterative learning, with a specific cap on the number of steps (50 steps per episode) to encourage efficiency and deter protracted decision-making paths.\nFinally, the models trained with hyperparameters selected with TPE and also default hyperparameters, for 100,000 epochs, where the robot must find a random target in a maximum of 50 steps.\nB. Evaluation Methodology Post-training, the model is tested against 100,000 randomly generated target positions. The evaluation emphasizes not only the success rate in reaching these targets, but also the efficiency as measured by the number of steps required, imposing a stricter limit of 5 steps per episode during this phase. This sharpens the focus on models ability to quickly adapt and accurately respond to new scenarios. Also, models saved in 20K, and 50K training episodes are tested to show learning convergence speed."}, {"title": "Experimental Setup", "content": "This study utilized the Franka Emika Panda arm with 7-DOF in a panda_gym [19] simulation, developed with PyBullet [20] and Gymnasium [21], to conduct risk-free tests (Fig. 3). Our experiments ran on a system with a 12th Gen Intel Core i7-12700 CPU and 64GB of RAM, using Python 3.8.10 for programming. We applied PPO and SAC via the Stable Baseline3 [22] library and optimized hyperparameters with the Optuna [23] framework."}, {"title": "Results", "content": "Our evaluation focused on the performance of two DRL algorithms, SAC and PPO, in conjunction with TPE for hyperparameter optimization, aiming to enhance the control of a robotic arm with 7-DOF. We observed significant improvements in learning efficiency, the utility of TPE in hyperparameter selection, and insights from various data visualizations and analyses. A key metric for evaluation was the success rate in reaching targets across the testing phases, which highlighted the enhanced learning efficiency and the strategic optimization of hyperparameters facilitated by TPE."}, {"title": "Hyperparameter Optimization", "content": "The optimization procedure benefited significantly from the strategic selection of hyperparameters via TPE, as shown in Fig. 4, parallel coordinate plots (PCP) for PPO and SAC elucidated the systematic exploration within the hyperparameter space, charting the trajectories toward optimized performance. This comprehensive view demonstrated how various hyperparameter configurations contributed to the overall efficacy of the learning models.\nThe Hyperparameters Importance Plot, shown in Fig. 5, underscores the relative significance of each hyperparameter in influencing the success of the learning process. This plot is instrumental in identifying the hyperparameters that have the most substantial impact on the model's ability to learn efficiently. A thorough examination of the SAC and PPO hyperparameters was conducted, detailing the range explored, and comparing the default values with the optimal values as determined by TPE. This comparison highlights TPE's nuanced approach to navigating the hyperparameter space and pinpointing the most effective configurations. The hyperparameters for PPO and SAC are listed in Table 3 and Table 4, respectively. We selected default values based on recommendations from the Stable Baselines3 library [22]."}, {"title": "Enhanced Learning Efficiency", "content": "The application of TPE markedly accelerated the learning process for both the SAC and PPO algorithms, as evidenced by the training curves (Fig. 6). The training curves for each algorithm illustrated a faster ascent toward higher mean rewards per episode, indicative of an expedited convergence toward optimal policy decisions post-TPE optimization. With TPE, PPO converges to a reward within 95% of the maximum payout 76.32% faster than it could without it. This means that roughly 40K fewer training episodes are needed for PPO to function at its best. Furthermore, compared to no TPE, this improvement for SAC happens 80.39% faster."}, {"title": "Comparative Evaluation", "content": "Upon evaluating the DRL models across a test set of 100,000 targets, Table 5 showcases a distinct advantage in utilizing TPE-optimized hyperparameters. The results, detailed in Table 5, emphasize the superior performance of the optimized models, marking a significant improvement in success rates, and learning convergence speed for task achievements."}, {"title": "Conclusion", "content": "This study demonstrates the benefits of using the TPE for hyperparameter optimization in DRL algorithms, SAC and PPO, for controlling robotic arms with 7-DOF. Our results indicate that TPE significantly improves both the learning efficiency and the performance of these algorithms, highlighted by the enhanced success rates and the accelerated convergence towards optimal rewards. Specifically, the application of TPE enabled PPO to achieve convergence to a reward within 95% of the maximum reward 76.32% faster than without TPE, necessitating about 40,630 fewer episodes of training for optimal performance, with similar improvements observed for SAC. These findings underscore the importance of precise hyperparameter tuning in the development of robust and efficient DRL models, particularly in the context of complex robotic tasks.\nA notable challenge tackled in this research was the complexity of hyperparameter tuning, where TPE proved to be a valuable tool for optimizing the DRL algorithms' performance in robotic tasks."}]}