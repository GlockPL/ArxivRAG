{"title": "The Surprising Effectiveness of Test-Time Training for Abstract Reasoning", "authors": ["Ekin Aky\u00fcrek", "Mehul Damani", "Linlu Qiu", "Han Guo", "Yoon Kim", "Jacob Andreas"], "abstract": "Language models have shown impressive performance on tasks within their training distribution, but often struggle with novel problems requiring complex reasoning. We investigate the effectiveness of test-time training (TTT)\u2014updating model parameters temporarily during inference using a loss derived from input data\u2014as a mechanism for improving models' reasoning capabilities, using the Abstraction and Reasoning Corpus (ARC) as a benchmark. Through systematic experimentation, we identify three crucial components for successful TTT: (1) initial finetuning on similar tasks (2) auxiliary task format and augmentations (3) per-instance training. TTT significantly improves performance on ARC tasks, achieving up to 6\u00d7 improvement in accuracy compared to base fine-tuned models; applying TTT to an 8B-parameter language model, we achieve 53% accuracy on the ARC's public validation set, improving the state-of-the-art by nearly 25% for public and purely neural approaches. By ensembling our method with recent program generation approaches, we get SoTA public validation accuracy of 61.9%, matching the average human score. Our findings suggest that explicit symbolic search is not the only path to improved abstract reasoning in neural language models; additional test-time applied to continued training on few-shot examples can also be extremely effective.", "sections": [{"title": "1 Introduction", "content": "Large-scale neural language models (LMs) excel at performing tasks that occur in their training data, and often elementary variations or compositions of those tasks (Brown et al., 2020; Todd et al., 2024). Given natural language task specifications or a small number of examples, LMs often successfully infer the desired task and produce an appropriate output. But can LMs also solve new problems, involving non-trivial reasoning, planning, or string manipulation of a kind very different from their pre-training data? This question is central to understanding the novel skill acquisition capabilities of current AI systems, which has been proposed as a key measure of intelligence (Chollet, 2019)."}, {"title": "2 Preliminaries", "content": "In this section, we first formally describe the ARC challenge. Next, we give an overview of in-context learning and test-time training, which form the foundation of our investigation. Finally, we detail our default experimental setup."}, {"title": "2.1 ARC Challenge", "content": "The Abstraction and Reasoning Corpus (ARC) aims to evaluate the abstract reasoning capabilities of language models through their ability to solve visual puzzles. Each puzzle, henceforth referred to as task, is comprised of input-output pairs of 2-D grids (up to 30 \u00d7 30 in size) that contain shapes or patterns made with up to 10 different colors, as displayed in Fig. 1(b). The output of each pair is obtained by applying an intuitive and shared transformation rule or function $y = f(x)$. In practice, these transformations are highly diverse and composite, ranging from simple concepts such as reflection and counting, to more complex ones such as application of gravity and path finding.\nEach task in ARC is composed of a training and test split, with:\n\u2022 Training examples denoted $(x_{\\text{train}}^i, y_{\\text{train}}^i)_{i=1}^K$ (typically K ranges from 2 to 7).\n\u2022 Test examples denoted $(x_{\\text{test}}^i, y_{\\text{test}}^i)_{i=1}^M$ (typically M ranges from 1 to 3).\nGiven the set of training examples, the goal is to predict the test output $y_{\\text{test}}$ for test test input $x_{\\text{test}}$ by reasoning about the underlying transformation.\nWe denote a task as $d = (x_{\\text{train}}, y_{\\text{train}}, x_{\\text{test}}, y_{\\text{test}})$ where $d \\in D_{\\text{arc}}$, the collection of such ARC tasks. The original training and validation sets of ARC dataset, respectively $D_{\\text{train}}$ and $D_{\\text{val}}$, consists of 400 tasks each. Success criteria requires to produce exact match for all test outputs (if not partial points are given). Please refer to Johnson et al. (2021) for a taxonomy and analysis of these tasks.\nMost approaches to ARC can be categorized into two main categories: program synthesis and fully neural. Program synthesis approaches (Butt et al., 2024; Wang et al., 2024; Li et al., 2024; Greenblatt, 2024) try to first find the transformation function $f$, and later apply it to the test example. On the other hand, fully neural approaches (Thoms et al., 2023; Bober-Irizar and Banerjee, 2024) try to directly predict the output $y_{\\text{test}}$, only implicitly reasoning about the underlying transformation. In this work, we use a fully neural approach, using a LM to predict the test outputs.\nWe start with an LM pre-trained on text data (without a vision encoder). To provide ARC examples as input to these models, we thus require a formatting function (denoted str) that converts 2D grids into their textual representations as shown in Appendix A.3. Previous work has presented examples as lists of numbers (Wang et al., 2024) or color words, or lists of connected components labeled with shapes and locations (Greenblatt, 2024). Given any such string representation of a task, we may present it to an LM and perform predictions with few-short prompting, as explained in the next section."}, {"title": "2.2 In-context Learning", "content": "At a certain scale, many LMs exhibit the ability to adapt to new tasks without updating their parameters by simply conditioning on input examples or instructions provided. Given a sequence of input-output pairs $(X_1,Y_1),..., (X_n, Y_n)$ and a new input $X_{n+1}$, a LM can be used to generate the output $\\hat{y}_{n+1}$ by sampling from:\n$\\hat{y}_{n+1} \\sim LM(\\cdot | X_1, Y_1, ... X_n, Y_n, X_{n+1})$\nThe possibility of in-context learning as implicit machine learning simulation discussed in previous work (Aky\u00fcrek et al., 2022), but the empirical evidence shows that in-context learning with language models does not always resemble any standard machine learning algorithm (Zhao et al., 2024; Min et al., 2022), and it does not always work out-of-the box for novel tasks - e.g. small language models (few billion parameters) performs poorly on ARC (Opielka et al., 2024; Bober-Irizar and Banerjee, 2024)."}, {"title": "2.3 Test-Time Training", "content": "Test-time training (TTT) enables parametric models to adapt during inference through dynamic parameter updates, an approach that remains relatively unexplored in the era of large language models. This technique is a form of transductive learning, where models leverages the test data structure to improve its predictions. The general TTT process works as follows: Starting with initial model parameters $\\theta_0$, for each test input (or batch of inputs), we first generate training data $D_{\\text{TTT}}(d_{\\text{input}})$ from the test inputs. We then optimize these parameters to minimize a loss function $L(D_{\\text{TTT}}; \\theta)$, producing temporarily updated parameters $\\theta_d$"}, {"title": "2.4 Experimental Setup", "content": "To investigate the impact of each TTT component, we conduct experiments by varying one component while holding the others constant at their optimal values (described in their respective sections). Our default configuration in the experiments uses the following settings:\nModel Architecture & Optimization We use an 8B parameter language model from the Llama-3 models, and 1B, 3B from Llama-3.2 models (Dubey et al., 2024). We use Low-Rank Adaptation (LoRA) (Hu et al., 2021) for parameter-efficient test-time training. For each task d, we initialize a separate set of LoRA parameters that are trained on the dataset $D_{\\text{TTT}}$. The LoRA rank is set to 128, and adaptations are applied to MLP, attention, and output layers. We train models with AdamW optimizer (Loshchilov and Hutter, 2019) with 2 epochs with batch sizes of 2.\nData & Formatting For efficient evaluation purposes, we randomly pick 80 balanced ARC tasks from ARC validation set, includes 20 easy, 20 medium, 20 hard, 20 expert tasks according to the classification in LeGris et al. (2024a) (see Appendix A.2 for this task list). We will use this subset of ARC tasks throughout the paper, except our final results given in for the full validation set (Section 6). We limit $D_{\\text{TTT}}$ to have maximum of 250 examples per task for efficiency reasons. With that, the whole TTT and inference process takes approximately 12 hours for 100 randomly sampled validation tasks when using an NVIDIA-A100 GPU. Appendix B.2"}, {"title": "3 What Dataset and Loss During TTT?", "content": ""}, {"title": "3.1 Data Generation", "content": "Given a task, we take the set of training input-output pairs $(x_{\\text{train}}^i, y_{\\text{train}}^i)_{i=1}^K$ and turn them into an augmented set of test-time-training tasks $D_{\\text{TTT}}$. We obtain $D_{\\text{TTT}}$ using a two-step process: First, we create a set of leave-one-out in-context learning tasks from the given training input-output pairs. Second, we use invertible rule-based transformations on this set to obtain an augmented dataset. This process is summarized in Fig. 2.\nStep 1 - Leave-one-out Tasks: By excluding the $j$th example pair from the training examples, we can create the following synthetic task:\n$d_j^{\\text{ICL}} = ((x_k,y_k)_{k\\in{1,..,K}\\setminus{j}}, x_j, y_j)$ where $j\\in [1, K]$\nwhere $d_j$ synthetic training task with the j-th example pair treated as the test case. We can generate n different tasks, each containing n 1 example pairs. We further include two randomly permuted version of $d_j$ where we permute the order of the training examples.\nStep 2 - Rule-based transformations: Consider an invertible transformation $t$ such that $t^{-1}(t(x)) = x$. For every task obtained in step 1, we can use t to generate a new augmented task $t(d^{\\text{ICL}})$, where t is applied to each individual grid in the task.\nWe choose simple transformations that preserve the fundamental relationships while introducing controlled variations such as rotation, flips, color permutation, example permutaton, size scaling, etc. The list and the description of these transformations are provided in Appendix B.1. Finally, we obtain\n$D_{\\text{TTT-ICL}} = {t(d_j^{\\text{ICL}})}$ for all t, j pairs.\nBaseline: End-to-End Learning Tasks For comparison to the \u201ctest-time in-context learning\u201d approach described above, we also evaluate an \u201ctest-time end-to-end learning\" approach. We create a supervised dataset directly from the example demonstrations by treating each input-output pair as an independent training instance. Unlike the in-context learning setup, no context is used for prediction:\n$d_j^{\\text{E2E}} = (x_j, y_j) \\text{ where } j \\in [1, K]$\nNote that this would be equivalent to leave-(n \u2013 1)-out task set in ICL setting as no training examples are provided as context. Similar to ICL case, we can apply rule-based transformations to augment the dataset:\n$D_{\\text{TTT-E2E}} = {t(d_j^{\\text{E2E}})}$ for all t, j pairs.\nThis approach is computationally more efficient as it directly learns the input-output mapping without the overhead of managing demonstration context i.e. the few-shot prompt."}, {"title": "3.2 Optimization Objective", "content": "During test-time training, we optimize a set of task-specific parameters using LoRA (low-rank adaptation; Hu et al. (2021)) while keeping most of the base model frozen. This approach allows computationally efficient adaptation while maintaining the model's general capabilities."}, {"title": "3.3 Results", "content": "We compare the main implementation of our method to the following ablations:\n1. FT (No TTT): The vanilla baseline where TTT is ablated and the fine-tuned model is used instead.\n2. No Transformations: No transformation-based data augmentation. That is, data from step 2 of the data generation pipeline described in Section 3.1 is not included in the test-time training dataset.\n3. End-to-End (E2E) Data: Instead of the standard in-context task setup, we use the end-to-end task formulation, as described in Section 3.1.\n4. Shared TTT: In contrast to learning a task-specific LoRA adapter, a single LoRA adapter is learned using an aggregated dataset of all tasks.\n5. No Demonstration Loss: No loss is taken on the demonstrations in the training outputs of the data. That is, the TTT loss is simply:\n$L_i(D_{\\text{TTT}}; \\theta_i) = \\sum_{d \\in D'{\\text{TTT}}} (L_{\\text{LM}} (Y_{\\text{test}} | X_1, Y_1, ..., X_K, Y_K, X_{\\text{test}}; \\theta_i))$\n6. QLoRA: Rather than full-precision base model updates, quantized LoRA adapters (Dettmers et al., 2024) are learned for each task, which is the alternative for LoRA considered for memory efficiency."}, {"title": "4 What Inference Strategy After TTT?", "content": ""}, {"title": "4.1 Augmented Inference", "content": "Recent work has shown that scaling test-time compute can significantly improve the performance of LMs. One of the most common techniques to do this is by sampling multiple responses, and then selecting the best response using a ranker. However, while sampling is very effective in domains with multiple possible solutions (programs in code) or multiple possible paths to the final answer (math), it can be detrimental when generating answers directly, as there is no way to directly enforce diversity across samples while ensuring coherence within samples. As an alternative inference-time scaling, we use an augmented inference strategy that generates multiple prediction candidates by using geometric transformations, combined with a greedy decoding scheme.\nFor a given task with training examples $(x_k, y_k)_{k=1}^K$ and test input $X_{\\text{test}}$, we use invertible geometric transformations to produce equivalent transformed versions of the task, as shown in Fig. 3. Let T be some set set of invertible geometric transformations (e.g., rotations and reflections). For each transformation $t \\in T$, we apply t to all training demonstrations and the test input and run our model with these transformed inputs."}, {"title": "4.2 Ensembling Predictions (Voting Strategy)", "content": "We employ a hierarchical voting strategy to determine the final prediction from the set of candidates {yt}t\u2208T . This approach involves two stages of voting to progressively narrow down the best candidates: first, by selecting the most frequent predictions within each transformation, and then by conducting an overall vote across transformation-specific candidates to identify the top-2 most frequent predictions. The details of each stage are as follows:\n1. Intra Transformation Voting: We group predictions by their corresponding transformation t and select the top-3 most frequent predictions within each group. If fewer than 3 unique predictions exist within a group, we supplement the candidates by computing additional predictions through:\n\u2022 Row-based majority: For each row in the predicted output grid, we take the most frequent row values across all predictions in the transformation group.\n\u2022 Column-based majority: Similarly, for each column in the predicted output grid, we take the most frequent column values across all predictions in the transformation group.\n2. Global Voting: Using the selected transformation-specific candidates obtained from (1), we conduct an overall vote to select the top-2 most frequent predictions for submission. In case of a tie, predictions with the identity transformation are given priority."}, {"title": "4.3 Results", "content": "To analyze the impact of augmented inference and voting, we run the following ablations:\n1. Vanilla: This baseline follows a standard inference approach without any augmented inference or voting. It generates 2 predictions from the model for 2 permutations of the task. This setup serves as a reference point to assess the benefits of our augmented inference and voting strategy."}, {"title": "5 What Fine-Tuning Before TTT?", "content": "While test-time training facilitates task-specific adaptation, the base model's capabilities impacts the final performance. We developed several approaches for generating synthetic training data to enhance the base model's abstract reasoning capabilities through fine-tuning, exploring both automated and semi-automated methods for task generation. In this section, we detail our fine-tuning data generation strategies and analyze the impact of different data sources and model sizes on final performance."}, {"title": "5.1 Preparing Fine-tuning Data", "content": "Hodel (2024) provides domain-specific language (DSL), REARC, as well as the transformation $f_i$ that solves the task-i, and the data generation function $g_i$ that are implemented in this DSL for each training task in the $D_{\\text{train}}$ dataset. These functions enable sampling of new input-output pairs that maintains the same underlying transformation principle:\n$d = (x,y) \\sim eval(g_i)$\nwhere d represents a newly generated input-output pair that can be solved using the same transformation function $f_i$ as the original task-i\u00b2."}, {"title": "5.2 Results", "content": "We perform full fine-tuning 1B, 3B Llama 3.2 instruction-tuned, and 8B Llama 3 instruction-tuned using augmented data. The format and training objective is same as the ones described for TTT in Section 2.4. Hyper-parameter details are given in Appendix B.2. We do the following ablations for augmented data:\n1. No FT: The original Llama 3 instruction-tuned model without any fine-tuning.\n2. All: We use all methods described in Section 5.1, including REARC, rule-based augmentation, and LM generation.\n3. No-Geom: We remove geometric transformations from all tasks.\n4. No-LM: We only use REARC and rule-based augmentation, excluding tasks generated by the LM."}, {"title": "6 ARC Benchmark and Comparison to Other Systems", "content": "Following our development experiments on 80 tasks, we present comprehensive results on the full ARC public evaluation set, comparing our system against existing approaches. Our analysis focuses on three key aspects: the impact of our TTT methodology, the benefits of combining our approach with existing methods and the differences between fully neural and program synthesis methods.\nImpact of Test Time Training We applied to our TTT and inference procedure (explained in Section 3 and Section 4) to our base fine-tuned models (fine-tuned 8B model without any LM data in Section 5). TTT improves accuracy from 39.3% to 47.1%, surpassing existing end-to-end neural model results.\nIntegration with Existing Methods A concurrent work by Li et al. (2024) introduced BARC, achieving 54.4%accuracy by combining neural and program synthesis approaches\u2014previously the highest publicly available result. While their fully neural approach shares similarities with our system, our TTT and inference pipeline has several additional components that boost performance. In particular, our test-time-training includes per-task LoRA and a larger set of augmentations, while our prediction pipeline includes an augmented inference under invertible transformations and a hierarchical self-consistency voting scheme. To validate our improvements, we applied our TTT pipeline to BARC's fully neural model, achieving 53% accuracy\u2014a 35% improvement over their original TTT method."}, {"title": "7 Conclusion", "content": "In this work, we conduct an investigation of test-time training and demonstrate that it can significantly improve LM performance on the popular ARC dataset. We find that learning task-specific LoRA adapters and generating augmented test-time datasets using geometric transformations are crucial. We also develop an augmented inference pipeline that uses invertible transformations to generate multiple predictions and then uses self-consistency to select the best candidates. Our overall pipeline applies multiple test-time computation methods, with each component contributing positively. This suggests that not only can test-time compute improve LM performance, but different test-time methods can also complement one another. Our TTT pipeline, combined with an existing method (BARC), achieves state-of-the-art results on the ARC public set and performs comparably to an average human. Our findings suggest that test-time methods could play a pivotal role in advancing the next generation of LMs."}, {"title": "Limitations", "content": "Evaluation Framework The ARC challenge maintains separate public and private leaderboards in which the private evaluation conducted on hidden tasks. While our TTT pipeline demonstrates promising results on the public benchmark, hardware constraints (12 hours/100 tasks runtime on an A100 GPU for Llama 8B) currently precludes submission to the official leaderboard, which requires completion within 12 hours on P100 or 2\u00d7T4 NVIDIA GPUs. In development, we use 80 tasks for validation, and we acknowledge potential sources of optimization bias. The geometric augmentations detailed in Appendix B.1 were selected during the TTT phase. Standard hyper-parameters (learning rate, batch size, epochs) were optimized using our development set with 80 validation tasks.\nExperimental Reproducibility Given the computational requirements of our experiments, this preprint reports results without comprehensive standard error analysis. Our preliminary observations indicate minimal variance across runs, and we plan to include detailed statistical analysis in the final version.\nData Leakage Even though the base Llama-3 perform extremely poorly on the public validation set, the public availability of the dataset on various platforms (GitHub, Kaggle) introduces the possibility that these models may have encountered these examples during pre-training."}]}