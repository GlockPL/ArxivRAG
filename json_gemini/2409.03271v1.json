{"title": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation", "authors": ["Yu Wang", "Shiwan Zhao", "Zhihu Wang", "Heyuan Huang", "Ming Fan", "Yubo Zhang", "Zhixing Wang", "Haijun Wang", "Ting Liu"], "abstract": "The Chain-of-Thought (CoT) paradigm has emerged as a critical approach for enhancing the reasoning capabilities of large language models (LLMs). However, despite their widespread adoption and success, CoT methods often exhibit instability due to their inability to consistently ensure the quality of generated reasoning paths, leading to suboptimal reasoning performance. To address this challenge, we propose the Strategic Chain-of-Thought (SCoT), a novel methodology designed to refine LLM performance by integrating strategic knowledge prior to generating intermediate reasoning steps. SCoT employs a two-stage approach within a single prompt: first eliciting an effective problem-solving strategy, which is then used to guide the generation of high-quality CoT paths and final answers. Our experiments across eight challenging reasoning datasets demonstrate significant improvements, including a 21.05% increase on the GSM8K dataset and 24.13% on the Tracking_Objects dataset, respectively, using the Llama3-8b model. Additionally, we extend the SCoT framework to develop a few-shot method with automatically matched demonstrations, yielding even stronger results. These findings underscore the efficacy of SCoT, highlighting its potential to substantially enhance LLM performance in complex reasoning tasks.", "sections": [{"title": "Introduction", "content": "The rapid development of large language models (LLMs) has highlighted their remarkable effectiveness in reasoning tasks (Huang and Chang 2022; Chang et al. 2024), particularly when integrated with various prompting techniques (Sivarajkumar et al. 2023). These techniques consistently enable impressive performance across diverse domains. Among them, the Chain-of-Thought (CoT) paradigm has played a pivotal role in enhancing the reasoning capabilities of LLMs (Kojima et al. 2022; Zhang et al. 2022; Wang et al. 2023). As a result, CoT has become a fundamental component of contemporary LLMs and is now widely adopted in the field of natural language processing.\nDespite the demonstrated effectiveness of the CoT approach in various applications, it faces significant challenges in complex reasoning tasks. These challenges primarily arise from the variability in the quality of the reasoning paths generated by the CoT method (Wang et al. 2022), which are not consistently optimal. Consequently, even when LLMs produce a CoT path that aligns with a valid reasoning process, there remains a risk that the final outcome may be erroneous. This phenomenon is analogous to findings in cognitive science, where different problem-solving strategies, although correct, can vary in their likelihood of producing errors. According to Sweller's Cognitive Load Theory (Sweller 1988), different problem-solving strategies impose varying levels of cognitive load, leading to different probabilities of error.\nThis variability in error probability, influenced by the undetermined strategies used to generate CoT paths, can undermine the reliability of the CoT approach in critical applications where precise and reliable reasoning is essential. Therefore, further refinement and improvement of the CoT methodology are necessary to enhance its performance in complex reasoning scenarios, drawing on insights from both artificial intelligence and cognitive science.\nVarious methods have been developed to address this challenge by enhancing the quality of CoT paths in LLMs, as illustrated in Figure 1. Among these methods, voting-based approaches enhance reasoning accuracy by generating diverse reasoning paths and then voting on the most reliable"}, {"title": "Related Work", "content": "In the realm of problem-solving, there is rarely a one-size-fits-all approach. The complexity of each problem often necessitate a variety of strategies to reach an effective solution. In the fields of education and cognitive science, the phenomenon of using multiple approaches to solve problems is quite common (Sweller 1988; Rusczyk 2003). Similarly, researchers have found that LLMs might generate diverse solution paths for one question, where the problem-solving strategies and answers of these methods might vary significantly (Wang and Zhou 2024; Wang et al. 2022).\nCurrent methods for enhancing the quality of model-generated content are diverse and sophisticated.\nSome approaches utilize a voting-based mechanism. For example, Wang et al. (2022) introduced the Self-Consistency method, which improves reasoning accuracy by first generating more than 20 CoT paths and then voting for the most consistent answer. Other methods incorporate external sources. Zheng et al. (2023) introduced Step Back, which prompts models to generate an abstract of the question to capture deeper logical structures, thereby enhancing retrieval-augmented generation (RAG) capabilities. Similarly, Yang et al. (2024b) developed another RAG-based method, Buffer of Thoughts, which uses knowledge extracted from external sources and predefined knowledge categories for each task. These elements are integrated into a predefined task prompt template, enabling the model to generate more accurate answers. Additionally, some methods introduce external tools to aid problem-solving. Gao et al. (2023) proposed PAL, which leverages large language models to parse problems and generate programs as intermediate reasoning steps, delegating the solution to a runtime environment like a Python interpreter. This neural-symbolic collaboration has demonstrated improved accuracy across various tasks. Suzgun and Kalai (2024) introduced metaprompting, which integrates existing prompt-based frameworks, enabling dynamic selection of the most effective reasoning strategy. These strategies, with their complex templates and multi-stage prompting, provide models with sophisticated tools for advancing CoT generation in LLMs."}, {"title": "Method", "content": "In this section, we introduce the strategic knowledge, the Strategic Chain-of-Thought (SCoT) method, and its extension through the few-shot approach.\nLLMs tend to produce varied CoT paths for the same problem. However, the quality of these CoT paths can vary significantly (Wang and Zhou 2024; Wang et al. 2022). As shown in the left part of Figure 2(a), when solving the math question \"compute the sum of all integers s such that -26 < s < 24\", one possible approach utilizes term pairing and summing the pairs to generate the final answer. Another possible approach employs the arithmetic series sum formula to compute the final result directly. While both methods are valid for problem-solving, the first approach results in less stable outputs typically due to the complexity of the intermediate steps. In contrast, the second approach, which applies the arithmetic series formula, generally results in better quality and more stable model outputs. The arithmetic series formula is considered strategic knowledge.\nStrategic knowledge (Strategy) refers to a well-defined method or principle that guides reasoning towards a correct and stable solution. It involves using structured processes that logically lead to the desired outcome, thereby enhancing the stability of CoT generation and improving the overall quality of the results.\nSpecifically, strategic knowledge should adhere to the following principles:\n1. Correct and Comprehensive Problem-Solving Approach: It provides a systematic approach that allows the model to generate accurate answers when it follows the reasoning steps carefully.\n2. Relatively Straightforward Problem-Solving Steps: The steps of the method should not be overly complex, while each step should be sufficiently detailed to ensure accuracy and prevent overly brief outputs that could lead to ambiguity.\nBuilding on the concept of strategic knowledge, we propose a prompt-based method to enhance the reasoning quality of LLMs, called Strategic Chain-of-Thought (SCoT).\nThe SCOT method enables the model to first elicit strategic knowledge before generating an answer, rather than producing an answer directly. Specifically, in a single-query setting, SCoT involves two key steps:\n1. Elicitation of Strategic Knowledge: The model identifies and determines one of the most effective and efficient methods for solving the problem, which then serves as the strategic knowledge for the task.\n2. Application of Strategic Knowledge: The model subsequently applies the identified strategic knowledge to solve the problem and derive the final answer."}, {"title": "Few-shot Strategic Chain-of-Thought", "content": "We refine the SCOT method into a few-shot version by leveraging the strategy to select demonstrations. Our approach is structured into two stages: constructing a strategy-based demonstration corpus and performing model inference.\nStage 1: Strategic Knowledge-Based Demonstration Corpus Construction.\nThis stage involves the following two steps, as shown in Figure 2(b):\n1. SCOT Answer Generation: We apply the zero-shot SCOT method to the training set to generate a corresponding SCoT answer for each question in the dataset.\n2. Demonstration Corpus Construction: The generated answers are compared with the ground truth. Only those accurate question-SCoT answer pairs are retained. This step assumes that the strategic knowledge used in these problems is both correct and relevant. The validated question-SCoT answer pairs are then compiled into a demonstration corpus based on strategic knowledge.\nStage 2: Model Inference.\nThis stage involves the following three steps in a two-query process, as shown in the right of Figure 2(a):\n1. Strategic Knowledge Generation: The LLM generates strategic knowledge relative to the problem, focusing on understanding the problem rather than producing the final answer.\n2. Demonstration Matching: The generated strategic knowledge is used to search the demonstration corpus created in Stage 1. The system identifies and matches the most relevant demonstrations with the SCoT answers from the most similar examples.\n3. Few-shot Inference: The selected demonstrations are integrated as few-shot examples into the input prompt (Figure 3(b)). This integration guides the model to generate the final prediction based on the provided examples."}, {"title": "Experimental Setup", "content": "In this section, we introduce the detailed experimental setup for validation of SCoT, including the datasets used for testing, the models covered, and the baselines employed.\nTo validate the effectiveness of the SCoT method, we collect a range of reasoning-related datasets covering domains including mathematics and physical reasoning, commonsense and multi-hop reasoning, and spatial reasoning:\n1. Mathematics and Physical Reasoning: We assess the models using datasets such as MathQA (Amini et al. 2019), AQUA (Ling et al. 2017), GSM8K (Cobbe et al. 2021), and MMLU-high-school-math (Hendrycks et al. 2021) for mathematical reasoning tasks. These datasets feature a range of mathematical problems with varying levels of difficulty, demanding strong mathematical reasoning abilities. Additionally, we evaluated the models on ARC_Challenge (Clark et al. 2018) for physical reasoning, i.e., a popular dataset that presents significant challenges in this domain.\n2. Commonsense and Multi-hop Reasoning: We evaluate the models on CommonsenseQA (CSQA) (Talmor et al. 2019) for commonsense reasoning tasks and StrategyQA (SQA) (Geva et al. 2021) for multi-hop reasoning tasks. These datasets are well-regarded in their respective domains and offer a substantial level of difficulty.\n3. Spatial Reasoning: We also evaluate the models using the Tracking_Object (Object) (BIG-bench authors 2023) dataset, which represents a less common but highly intriguing type of reasoning task.\nIn the few-shot version of SCoT, we conduct experiments exclusively on the MathQA, AQUA, GSM8K, and ARC datasets. This selection is due to the requirement that the dataset must have a sufficiently large training set with gold answers for constructing the demonstration corpus in the first step. Only these four datasets meet this criterion."}, {"title": "Experimental Results", "content": "In this section, we empirically evaluate the effectiveness of the Strategic Chain-of-Thought (SCoT) approach. To verify SCoT's efficacy across all datasets, we test it using two open-source models, Llama3-8B and Mistral-7B. To further validate SCoT's effectiveness across different models, we select one dataset from each of the three reasoning task categories and conduct tests on all 7 models. We also examine the impact of model size, perform ablation studies on SCOT components, conduct case studies, and analyze experimental efficiency to understand the factors influencing the effectiveness of SCoT.\nThe experimental results across all datasets using two models are presented in Table 1. Notably, in zero-shot settings, SCOT outperforms the CoT approach in most tasks, with particularly significant improvements observed on the GSM8K dataset, where accuracy increases from 52.11% to 73.16% after incorporating strategic knowledge. Additionally, SCOT achieves a 24.13% improvement on the Tracking_Object dataset. However, the Llama3-8B model exhibits a 2.6% decrease in performance on the ARC dataset. In general, the Llama3-8B model shows an average improvement of 6.92% on all datasets, while the Mistral-7B model demonstrates an average improvement of 3.81% on comparable datasets. Compared to Step Back and Self-Consistency, SCoT also performs better than these two methods except for the result of Self-Consistency with Llama3-8B model on the ARC dataset. Nevertheless, our SCoT still achieves comparable results to it. Notably, SCoT shows substantial gains in commonsense reasoning tasks compared with other methods.\nFurthermore, we extend the SCoT framework to support few-shot settings by automatically matching demonstrations, resulting in even stronger performance. The SCOT 1-shot, as shown in Table 1, refers to CoT prompting with demonstrations matched through strategic knowledge. Compared to CoT 0-shot\u00b9, SCoT 1-shot, which uses strategy-matched demonstrations, shows significant"}, {"title": "Model Scale", "content": "Here we investigate the impact of model size on the effectiveness of SCoT. Experiments on the Llama2 model series with three different sizes are conducted, and the results are shown in Figure 5. It demonstrates that SCOT can lead to accuracy improvements across all sizes of the Llama2 models. However, a general trend emerges that performance improvement decreases marginally with model size. Furthermore, manual inspection of the model outputs reveals that larger models are more likely to generate CoT path containing strategic knowledge in 0-shot settings."}, {"title": "Ablation Study", "content": "We explore the effects of various components within the prompt (such as role, workflow, structure, and the quantity of demonstrations) on accuracy. The experimental results are illustrated in Table 3. Building on the CoT 0-shot approach, we observed that adding roles, incorporating workflows, and formatting prompts in markdown progressively increased accuracy. We also explored the impact of the number of demonstrations on accuracy within the few-shot SCOT framework. Experimental results indicate that as the number of demonstrations increases, the performance of SCOT either slightly improves or remains unchanged."}, {"title": "Case Study", "content": "We conduct a detailed case study focusing on the validity of the strategic knowledge elicited from the model. Figure 6 shows several typical cases.\nIn the domain of mathematics, we observe that the SCOT output tends to favor solving problems using inequalities rather than directly analyzing the problem to reach an answer. For the instance of frog jumping calculation in the Figure 6, an incorrect solution may miscalculate the final jump's impact. While generating a strategy ensures accurate calculations by considering all constraints and systematically"}, {"title": "Efficiency Analysis", "content": "Due to SCoT's mechanism of generating strategy before solving problems in one query, it is more efficient than multi-query methods. However, compared to single-query methods, the output token length might be longer, potentially decreasing efficiency. To investigate this, we measure the output token lengths for the AQUA, GSM8K, and Tracking_Object datasets using both CoT 0-shot and SCOT 0-shot methods. The results are shown in Table 4.\nThe results indicate that the token length output by the Mistral-7B model on the GSM8K dataset decreases with the SCOT method. This reduction may be due to the model's tendency to repetitively generate a specific answer span up to the inference length limit on the GSM8K dataset in CoT O-shot, leading to a decline in accuracy. SCoT mitigates this issue. Besides, the length of SCOT varies from 1.03 to 1.8 times that of CoT, averaging around 1.5 times. This shows that while our method is somewhat slower than CoT, the efficiency remains manageable."}, {"title": "Discussions", "content": "To demonstrate that our experimental results are not influenced by human-crafted prompts but rather due to the concept of SCOT, we conduct a preliminary test to evaluate whether the SCOT prompt templates can be automatically generated. We provide the SCOT concept to Qwen2-72B to generate the corresponding prompt templates and tested these on the AQuA dataset. The results are presented in Table 5. The findings indicate that while the accuracy of prompts automatically generated based on the SCoT concept is lower than that of manually crafted SCOT prompts, it is still superior to 0-shot CoT performance. This suggests that the automatic generation of SCoT-based prompt templates is feasible."}, {"title": "Conclusion", "content": "In this paper, we introduce the Strategic Chain-of-Thought, a method that enables LLMs to autonomously generate an optimal Chain-of-Thought path. By integrating a structured workflow for eliciting and applying strategic knowledge, SCOT enhances the model's ability to produce a high quality outputs. We further extend SCoT to a few-shot version by matching demonstrations through strategic knowledge from a predefined strategic knowledge-based corpus. Experimental results demonstrate the effectiveness of both 0-shot SCOT and few-shot SCOT.\nOverall, SCOT offers a promising framework for improving the quality of reasoning path in LLMs. Future research will focus on evaluating its effectiveness with more complex problems and exploring further applications."}]}