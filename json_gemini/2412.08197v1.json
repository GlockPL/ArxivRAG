{"title": "SAFIRE: Segment Any Forged Image Region", "authors": ["Myung-Joon Kwon", "Wonjun Lee", "Seung-Hun Nam", "Minji Son", "Changick Kim"], "abstract": "Most techniques approach the problem of image forgery localization as a binary segmentation task, training neural networks to label original areas as 0 and forged areas as 1. In contrast, we tackle this issue from a more fundamental perspective by partitioning images according to their originating sources. To this end, we propose Segment Any Forged Image Region (SAFIRE), which solves forgery localization using point prompting. Each point on an image is used to segment the source region containing itself. This allows us to partition images into multiple source regions, a capability achieved for the first time. Additionally, rather than memorizing certain forgery traces, SAFIRE naturally focuses on uniform characteristics within each source region. This approach leads to more stable and effective learning, achieving superior performance in both the new task and the traditional binary forgery localization. Code: https://github.com/mjkwon2021/SAFIRE", "sections": [{"title": "Introduction", "content": "In the era of artificial intelligence (AI), the proliferation of image editing software (Fu et al. 2023; Yu et al. 2023) and sophisticated generative models (Rombach et al. 2022; Ho, Jain, and Abbeel 2020) has made image forgery more accessible and more challenging to detect than ever before (Lin et al. 2024). The ease of image manipulation critically affects areas where the integrity of visual information is crucial, including the spread of fake news in journalism, the use of counterfeit evidence in law enforcement, and the presence of fabricated microscopy images in biomedical research (Verdoliva 2020; Sabir et al. 2021). Therefore, detecting and precisely localizing forgeries within an image is crucial for maintaining trust in digital media.\nCurrently, most image forensics methods address the problem of image forgery localization (IFL) through binary segmentation (Guillaro et al. 2023; Kwon et al. 2022; Liu et al. 2022; Dong et al. 2022; Hu et al. 2020; Wu et al. 2022; Zhou et al. 2023a; Ji et al. 2023a; Sun et al. 2023). That is, within an image, regions that remain unchanged from the camera capture are labeled as 0, and regions that have been manipulated are labeled as 1, to train deep neural networks."}, {"title": "Related Work", "content": "Effective extraction of forensic clues is essential in IFL. This often involves determining which forensic fingerprints to be utilized. These artifacts, often low-level and inconspicuous, include local CFA artifacts (Bammey, Gioi, and Morel 2020), edge information (Dong et al. 2022; Li et al. 2023), JPEG compression artifacts (Kwon et al. 2021, 2022), unique traces left by different camera models (Guillaro et al. 2023), and explicitly enhanced noise (Zhu et al. 2024).\nDesigning network architectures specifically tailored for forensics is another key component in solving IFL. This includes the application of steganalysis filter (Zhou et al. 2018), various low-level filters and anomaly-enhancing pooling (Wu, AbdAlmageed, and Natarajan 2019), utilizing both top-down and bottom-up paths (Liu et al. 2022), and efficient modeling of internal relationships using Transformers (Hu et al. 2020; Hao et al. 2021; Wang et al. 2022; Zeng et al. 2024).\nLearning the common characteristics of an image and using consistency as a criterion is also a viable approach. The pioneering study (Huh et al. 2018) trains a model using self-supervised learning to determine if two image patches have the same EXIF metadata and uses clustering to check consistency. Subsequent research has utilized the consistency of camera model fingerprints (Cozzolino and Verdoliva 2019) or employed various contrastive learning techniques to utilize consistent features (Zhou et al. 2023a; Wu, Chen, and Zhou 2023; Niloy, Bhaumik, and Woo 2023).\nOur method aligns with this research trend of using consistency but stands out in several key ways. Firstly, we employ a symmetric pretraining approach that focuses on source regions without distinguishing between authentic and tampered areas. Secondly, we use point prompting, which enables multi-source partitioning and addresses label agnosticity. Lastly, clustering is performed at the prediction map level, rather than on all patch pairs or individual pixels."}, {"title": "Segment Anything Model", "content": "Foundation models have first emerged in the field of Natural Language Processing (NLP), which is pretrained on large datasets and then fine-tuned across a variety of sub-tasks or domains for specific applications (Bommasani et al. 2021). These NLP-based foundation models have demonstrated breakthrough performance in natural language understanding and generation tasks. Their impact has expanded to other AI domains, including computer vision and speech recognition, leading to models like CLIP (Radford et al. 2021) and wav2vec 2.0 (Baevski et al. 2020).\nRecently, Meta AI introduced SAM (Kirillov et al. 2023) as the first foundation model for image segmentation. As a prompt-based model, SAM accepts point prompts, bounding boxes, and masks. Furthermore, its design allows for integration with other models to handle text prompts, enabling flexible integration with other systems. SAM has been fine-tuned and applied to various domains such as polyp segmentation (Li, Hu, and Yang 2023; Zhou et al. 2023b), camouflaged object detection (Tang, Xiao, and Li 2023), and others (Ji et al. 2023b), with related research publications keep emerging.\nVery recently, there have been a few attempts to use SAM in IFL techniques. One approach (Su, Tan, and Huang 2024) constructs an IFL model by adding an SRM filter (Zhou et al. 2018) to SAM. It completely removes the prompt encoder, essentially using SAM as a modern segmentation backbone. Another study (Karageorgiou, Kordopatis-Zilos, and Papadopoulos 2024) fuses various signals using attention, and to assist with this attention, it employs a pretrained and frozen SAM for instance segmentation.\nIn summary, previous studies have primarily used SAM either as a backbone or solely to obtain segmentation masks. These approaches neglect SAM's most significant feature-its promptable capability\u2014and fail to fully leverage its potential. Meanwhile, we pioneer the application of promptable segmentation models for partitioning images into source regions. By using SAM-based point prompts, we enable each point to serve as a reference, segmenting the source region that contains it. Moreover, inspired by SAM's automatic mask generation process, we propose an inference technique that involves placing points on the image in a grid pattern and aggregating the results. This approach enables, for the first time, multi-source partitioning."}, {"title": "Method", "content": "The core methodology we propose, the SAFIRE framework, refers to the pretraining, training, and inference processes for IFL. The neural network used within this framework is termed the SAFIRE model, which does not require a specific structure and can be freely modified. In this paper, we utilize a slightly altered structure from SAM, adding only adapter layers to the image encoder for enhancing the model's capability to extract forensics features by utilizing low-level signals (outlined in Fig. 2 and detailed in Appendix). It consists of an image encoder $E(\u00b7)$, prompt encoder $F(\u00b7)$, and mask decoder $D(\u00b7,\u00b7)$. The model takes an image $I$ and a point prompt $P$ as inputs and outputs a prediction map $X$ and confidence score $s$ for the source region that includes the point.\nThe upcoming sections will delve into a detailed explanation of the SAFIRE framework. Initially, for effective source image partitioning, the image encoder is pretrained through region-to-region contrastive learning. Subsequently, in the main training phase, the model is trained on source region segmentation using point prompts. In the final inference stage, multiple points are fed into the model in a grid formation, and all the results are aggregated to obtain the final prediction heatmap."}, {"title": "Pretraining: Region-to-Region Contrastive Learning", "content": "We propose Region-to-Region Contrastive Learning to pretrain the image encoder for effective source region partitioning (Fig. 3). This approach aims to have embeddings from the same source region close together in the feature space, while those from different source regions are distanced, when an image consists of two or more sources.\nLeveraging the proven effectiveness of the InfoNCE loss in contrastive learning (Oord, Li, and Vinyals 2018), we define our loss function as follows. Let $I \\in R^{3\\times H \\times W}$ be an input image composed of $r$ sources, $E(\u00b7)$ the image encoder, and $\\mathcal{E} = E(I) \\in R^{V \\times \\frac{H}{K} \\times \\frac{W}{K}}$ the image embeddings with downsampling ratio $K$. With a slight abuse of notation, we treat $\\mathcal{E}$ as a set of $V$-dimensional image embeddings. Then there are $H \\times W$ embeddings $q \\in R^{V}$ in $\\mathcal{E}$. We also let $\\{\\mathcal{E}_{i}\\}_{i=1}^{r}$ be the partition of $\\mathcal{E}$ which corresponds to source regions in $I$.\nThen we define the region-to-region contrastive loss $\\mathcal{L}_{R2R}$ as:\n$\\begin{aligned}\nInfoNCE(q, p, \\mathcal{N}) &= -log \\bigg(\\frac{exp(\\frac{q^T p}{\\tau})}{\\sum_{p' \\in {p} \\cup \\mathcal{N}} exp(\\frac{q^T p'}{\\tau})}\\bigg), \\\\\n\\mathcal{L}_{R2R} &= \\frac{1}{r} \\sum_{i=1}^{r} \\sum_{q \\in \\mathcal{E}_{i}} InfoNCE \\big(q, \\mathcal{E}_{i} \\setminus \\{q\\}, \\mathcal{E} \\setminus \\mathcal{E}_{i}\\big), \n\\end{aligned}$\nwhere $\\tau$ is a hyperparameter called temperature, $|\u00b7 |$ returns the number of elements and returns the average over all elements.\nBefore the image passes through the image encoder, global post-processing such as various blurring, noise addition, or contrast changes are probabilistically applied to it. By doing so, we expect the image encoder to become robust to global common variations and focus more on fine local distinctions."}, {"title": "Training: Source Region Segmentation Using Point Prompts", "content": "Upon completion of the image encoder pretraining, the SAFIRE model undergoes the main training to accurately segment the source region in response to the specified point prompt (Fig. 4). Both the image encoder and prompt encoder are frozen: the image encoder in its pretrained state and the prompt encoder in its original SAM state. The adapter component and mask decoder are trained by feeding image embeddings and prompt embeddings into the mask decoder, ensuring the output aligns with the correct mask.\nDuring the training process, it is necessary to transform the image-level ground truth mask into a mask corresponding to the given point, which we call a point mask. If there is a multi-source mask where different labels are assigned to each source region, then a point mask could be simply created by assigning 1 to the source region that contains the point and 0 otherwise. However, almost all of the datasets currently available for IFL tasks are in only binary form, marking manipulated parts as 1 and unaltered parts as 0.\nWe introduce a methodology to convert these image-level binary masks into point masks. If a manipulated image uses only two sources, the areas marked as 0 and 1 would each represent a single source region. Taking a step further, we also consider connected components. A connected region containing a given point is marked as 1, and other connected regions neighboring this region are labeled as 0. Regions that are not neighboring it are assigned an ignore label of -1, which is ignored when calculating losses. This transformation allows us to train for multi-source partitioning using only datasets with binary labels.\nTo be specific, let $Y \\in \\{0,1\\}^{H \\times W}$ be the ground truth mask for an image $I$ which contains $c$ connected components, $\\mathcal{R} = \\{(i, j) \\in \\mathbb{Z}^{2} : 0 \\leq i < H, 0 \\leq j < W\\}$ a set of integer coordinates of $I$, $\\{\\mathcal{R}_{i}\\}_{i=1}^{c}$ the partition of $\\mathcal{R}$ covering connected components of $Y$, $P \\in \\mathcal{R}$ a point prompt, and $\\mathcal{R}_{P}$ a region contains $P$ which is one of $\\{\\mathcal{R}_{i}\\}_{i=1}^{c}$. Then the point mask $Y_{p} \\in \\{-1,0,1\\}^{H \\times W}$ can be computed as:\n\\begin{equation}\nY_{p}[i, j] =\\begin{cases}\n1, & \\text{if } (i, j) \\in \\mathcal{R}_{P} \\\\\n0, & \\text{if } (i, j) \\in neighbors(\\mathcal{R}_{P}) \\\\\n-1, & \\text{otherwise}\n\\end{cases}\n\\end{equation}\nwhere $neighbors(\u00b7)$ returns the union of neighboring regions."}, {"title": "Area-Adaptive Source Segmentation Loss", "content": "For each point, we can define a loss function that minimizes the difference between the prediction map and the point mask (Fig. 4). Here, not all pixels within a point mask contribute equally to the loss because doing so would result in smaller areas being overlooked. Traditional IFL techniques have addressed the similar issue of manipulated areas being small in most images by assigning greater weight to tampered class (Kwon et al. 2022). However, in our point masks, there is no distinction between manipulated and pristine regions; there exist only multiple source regions. Therefore, we use a strategy that assigns greater weight to smaller areas within each point mask, regardless of whether the correct label in those areas is 0 or 1. This differs from the class-specific weights used in most semantic segmentation tasks in that the weights are calculated within a single image (Wang et al. 2020).\nLet $I$ be an input image, $P$ a point prompt, $(X, s) = D(E(I), F(P))$ the output of the mask decoder where $X$ is the prediction map and $s$ is the confidence score, and $Y_{p}$ the ground truth point mask for $P$. We only compute the loss within the valid label region $\\mathcal{R}_{Y_{P},\\{0,1\\}}$ by letting $\\mathcal{R}_{A,B} = \\{(i,j) \\in \\mathcal{R} : A[i,j] \\in B\\}$. Then the Area-Adaptive Source Segmentation Loss $\\mathcal{L}_{AASS}$ is defined as:\n\\begin{equation}\n\\begin{aligned}\n\\mathcal{L}_{AASS}&= \\mathbb{E}_{\\mathcal{R}_{Y_{P},\\{0,1\\}}}\\bigg[ W_{1} Y[i, j] \\log (\\sigma (X[i, j]))\\\\\n&+ w_{0} (1 - Y[i, j]) \\cdot \\log (1 \u2013 \\sigma(X[i, j]))\\bigg],\n\\end{aligned}\nW_{1} = min\\bigg(\\frac{\\left|\\mathcal{R}_{Y_{P},\\{0,1\\}} \\right|}{\\left|\\mathcal{R}_{Y_{P},\\{1\\}}\\right|},C_{AASS}\\bigg)\\\\\ntext{and } w_{0} = min\\bigg(\\frac{\\left|\\mathcal{R}_{Y_{P},\\{0,1\\}} \\right|}{\\left|\\mathcal{R}_{Y_{P},\\{0\\}}\\right|},C_{AASS},s\\bigg).\n\\end{equation}\nwhere the expectation is calculated over $\\mathcal{R}_{Y_{P},\\{0,1\\}}$, $\\sigma(\u00b7)$ is a sigmoid function, and $C_{AASS}$ is a hyperparameter to limit the weight."}, {"title": "Inference: Multiple Points Aggregation", "content": "Inference is conducted using multiple point prompts (Fig. 5). Alongside the image to be inferred, points are provided as input to the model in a grid format (for example, 16 \u00d7 16). The output masks are aggregated to obtain the final prediction, which could be a multi-source map or binary map.\nLet $I$ be an input image, and $P_{1},\\dots, P_{N}$ point prompts. First, we compute the image embedding $\\mathcal{E} = E(I)$ and prompt embeddings $F_{i} = F(P_{i})$ for all $i$. Since image embedding extraction is independent of the point prompt, it is performed only once per image. Thus, the total computation does not increase too much even if we use many points.\nThereafter, the image embedding and point embeddings pass through the mask decoder and so a prediction corresponding to each point can be obtained. The output of the mask decoder $D(\u00b7, \u00b7)$ can be expressed as:\n$\\big(\\{X_{1},\\dots, X_{N}\\},\\{s_{1},\\dots, s_{N}\\}\\big) = D(\\mathcal{E}, \\{F_{1},..., F_{N}\\}),$ \nwhere $X_{i}$ is a prediction map and $s_{i}$ is a confidence score of $X_{i}$.\nThe next step is to compute a representative feature for each prediction $X_{i}$, which is the average of image embeddings corresponding to the prediction area. We define a function $g: \\mathbb{R}^{H \\times W} \\rightarrow \\mathbb{R}^{V}$ by:\n\\begin{equation}\ng(X) = \\frac{1}{\\left|\\mathcal{R}_{bin(X),\\{1\\}}\\right|} \\sum_{(i,j) \\in \\mathcal{R}_{bin(X),\\{1\\}}} \\mathcal{E}[i, j],\n\\end{equation}\nwhere $\\mathcal{R}$ is a set of integer coordinates of $\\mathcal{E}$ and $X$ is the downsampled prediction map of $X$ to match the resolution with $\\mathcal{E}$. Here, $\\mathcal{R}_{bin(X),\\{1\\}}$ represents the set of coordinates in the embedding space corresponding to the area segmented by the prediction $X$. The representative features can be expressed as $G_{i} = g(X_{i})$ for all $i$.\nSubsequently, we cluster the representative features. The clustering is predicated on the assumption that the SAFIRE model accurately extracts features, which results in features from the same source region being gathered together. We cluster $\\{G_{1},...,G_{N}\\}$ into $M$ clusters $C_{1},..., C_{M}$. Any clustering algorithm could be applied and $M$ can be fixed in advance or regressed by the algorithm. For general source region partitioning, we may allow the algorithm to determine the proper $M$. In situations where the number of sources is known, algorithms with a fixed number of clusters can be used.\nAfterward, the most confident mask from each cluster is selected. Each cluster represents one source region of the input image and the most confident mask corresponds to the best prediction of it. We collect indices of the maximum confidence scores for each cluster:\n$\\begin{equation}\nj^{*} = \\underset{G_{i} \\in C_{j}}{\\operatorname{argmax}} s_{i}.\n\\end{equation}$\nFinally, these masks are combined to obtain the final prediction. The simplest method is taking the softmax:\n$\\begin{equation}\nX^{*} = softmax\\{X_{1^{*}},...,X_{M^{*}}\\}.\n\\end{equation}$\nFor the special case when $M = 2$, to obtain a binary prediction map, the simple average of the two predictions produces an effective output:\n$\\begin{equation}\nX^{*} = \\frac{1}{2}\\{\\sigma(X_{1^{*}})+ (1 - \\sigma(X_{2^{*}}))\\}.\n\\end{equation}$"}, {"title": "Experiments on binary IFL", "content": "We begin with the traditional task of localizing forged regions in images. Note that SAFIRE can make binary predictions as well as multi-source predictions."}, {"title": "Experimental Settings", "content": "Our model undergoes pretraining followed by training. The temperature $\\tau$ for the region-to-region contrastive learning in Eq. (1) is set to 0.1. The weight limit $C_{AASS}$ for the AASS loss in Eq. (4) is set to 10 and $\\lambda_{conf}$ = 0.1 in Eq. (6). During the inference phase, $M$ is fixed to 2 to obtain predictions in binary form. We use 16 \u00d7 16 point prompts and k-means clustering."}, {"title": "Experiments on Multi-source Partitioning", "content": "One of the unique advantages of our method is its ability to partition images composed of three or more sources into each source. To show this capability, we conduct additional experiments on multi-source partitioning."}, {"title": "Experimental Settings", "content": "We use the same model as used in binary IFL without fine-tuning. We consider two settings for inference: the number of sources is given in advance and it is determined by the method. For the former, we use k-means clustering as done in binary IFL. For the latter, we utilize DBSCAN which automatically chooses the number of clusters in the data distribution."}, {"title": "Conclusion", "content": "Moving beyond the conventional approach of viewing the IFL tasks through binary segmentation, SAFIRE resolved this issue by partitioning images into multiple originating regions. Through region-to-region contrastive pretraining, we guided the encoder to effectively embed subtle signals for source partitioning. We utilized point prompt-based segmentation to train the SAFIRE model to accurately predict the source region containing each point. During inference, we provided point prompts in a grid format and aggregated the outputs to obtain the final prediction. Through comprehensive evaluation, SAFIRE successfully accommodated label agnosticity issues in IFL and outperformed other state-of-the-art methods. It also opened up possibilities for using point prompting in image partitioning and presented a new challenge of partitioning images into multiple source regions. It aids in comprehending the structure of the forged image and facilitates further analysis. We hope our study contributes to solving the increasingly complex image forgery issues in the era of AI."}, {"title": "SAFIRE Model Architecture", "content": "The left-hand side of Fig. 8 shows the SAFIRE model architecture as described in the main text. On the right-hand side, the structure showcases the image encoder integrated with the adapter. This design integrates adapter components into the SAM image encoder (Kirillov et al. 2023) to strengthen the model's ability to extract forensic features. This strategy draws inspiration from recent studies that integrate adapter modules to tailor large foundational models to specific tasks (Chen et al. 2023; Houlsby et al. 2019; Li et al. 2022; Chen et al. 2022).\nThe top section, highlighted in orange, mirrors the architecture of the SAM image encoder. The bottom section, depicted in green, consists of adapter components designed to refine the encoder's performance. In the context of forensic analysis, the model leverages high-frequency component extraction (Liu et al. 2023). Firstly, a fast Fourier transform (FFT) is applied and then low frequencies are removed, followed by an inverse FFT to revert it back to the image domain. The model processes and combines the patch embeddings from both the original and high-frequency images. Subsequently, these embeddings are fine-tuned through adapter blocks, which are linear layers, and then proceed through shared up-projection layers, fine-tuning the primary pathway."}]}