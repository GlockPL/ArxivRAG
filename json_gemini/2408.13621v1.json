{"title": "Preliminary Investigations of a Multi-Faceted Robust and Synergistic Approach in Semiconductor Electron Micrograph Analysis: Integrating Vision Transformers with Large Language and Multimodal Models", "authors": ["Sakhinana Sagar Srinivas", "Geethan Sannidhi", "Sreeja Gangasani", "Chidaksh Ravuru", "Venkataramana Runkana"], "abstract": "Characterizing materials using electron micrographs is crucial in areas such as semiconductors and quantum materials. Traditional classification methods falter due to the intricate structures of these micrographs. This study introduces an innovative architecture that leverages the generative capabilities of zero-shot prompting in Large Language Models (LLMs) such as GPT-4(language only), the predictive ability of few-shot (in-context) learning in Large Multimodal Models (LMMs) such as GPT-4(V)ision, and fuses knowledge across image-based and linguistic insights for accurate nanomaterial category prediction. This comprehensive approach aims to provide a robust solution for the automated nanomaterial identification task in semiconductor manufacturing, blending performance, efficiency, and interpretability. Our method surpasses conventional approaches, offering precise nanomaterial identification and facilitating high-throughput screening.", "sections": [{"title": "Introduction", "content": "Semiconductors have been the backbone of technological advancements in modern electronics, driving growth and innovation in computing and communication systems, among others. The semiconductor process comprises three main stages: (a) design and development, during which fabless firms create chip blueprints, specifying the architecture, functions, and specifications of the miniaturized chips; (b) fabrication, where specialized foundries manufacture chips by etching integrated circuits onto silicon wafers using intricate technologies; and (c) testing and assembly, during which chips undergo rigorous testing and are subsequently assembled into protective packages for integration into electronic devices. This collective effort results in the production of high-quality semiconductor components suitable for a wide range of applications. The state-of-the-art imaging and analysis methods(Holt and Joy 2013) are indispensable in semiconductor manufacturing for the development of next-generation miniaturized chips, especially those sized at 7 nm or smaller. The pursuit of miniaturized chips below 7 nm technologies introduces a level of complexity and precision that significantly increases the risk of errors in the manufacturing process. These errors can compromise the consistency of high-quality chips and amplify the variability in chip performance, posing a substantial challenge for manufacturers aiming to produce reliable and advanced chips at this scale. The semiconductor industry utilizes various advanced electron beam tools, including scanning and transmission electron microscopy, to create images or micrographs of semiconductor materials, structures, and devices at the micro and nanoscale with high resolution and detail. These tools contribute to quality control, process monitoring, failure analysis, and materials characterization in the semiconductor industry. Automated labeling of electron micrographs, though advantageous, poses a considerable challenge due to the level of detail, complexity of patterns, and information density involved. These challenges arise primarily from the high inter-category similarity (similar-looking or indistinguishable) between different nanomaterials, high intra-category dissimilarity within nanomaterials (distinct or differing appearances), and the presence of intricate visual patterns in nanomaterials across various scales (spatial heterogeneity)."}, {"title": "", "content": "Recently, unimodal Large Language Models (LLMs) such as GPT-4 (language-only)(OpenAI 2023a) which are pretrained autoregressive large-scale models on extensive, diverse text corpora in unsupervised learning settings following a fundamental paradigm \u201cprompt and predict\" approach, have significantly transformed natural language processing(NLP), achieving improved performance across a wide range of NLP tasks, demonstrating better logical reasoning abilities, and generating human-like text. Zero-shot Chain of Thought(Zero-Shot CoT)(Wei et al. 2022) and Few-Shot (In-Context) learning(Few-Shot ICL)(Brown et al. 2020) are prompt engineering strategies for designing and crafting tailored prompts for utilizing general-purpose LLMs in specialized language-based tasks or associated new, unseen problem-solving scenarios, thereby eliminating the need for traditional task-specific fine-tuning. Zero-Shot CoT relies on customized instructions without requiring explicit task-specific demonstrations(input-output pairs), requiring the language model to generalize from the implicit knowledge acquired during training to generate the output for the downstream task. Conversely, Few-shot ICL incorporates a few guiding demonstrations to learn from analogy along with the task-centric instructions to guide LLMs to generate the output simply by conditioning on the prompts. In recent times, OpenAI's GPT-4 with Vision (GPT-4V)(OpenAI 2023b), which possesses the ability to process and understand images, represents a significant advancement in the domain of large multimodal models (LMMs). It is more versatile than GPT-4, as it has broken the text-only barrier of previous language models, introducing visual understanding and analysis as a new dimension. GPT-4V is designed to accept multiple modalities, including both images and text as input, and generate text outputs. GPT-4V incorporates visual processing capabilities, enabling it to analyze image inputs provided by the user in conjunction with text, thereby facilitating visual question answering. Despite its advanced capabilities, when tested on SEM images(Aversa et al. 2018) for nanomaterial categorization, GPT-4V incorrectly classified them, highlighting the limitations of LMMs."}, {"title": "Problem Statment", "content": "Our study focuses on the classification of electron micrographs, an inductive learning challenge where the goal is to categorize previously unobserved micrographs by leveraging a labeled dataset, represented as D\u2081 = (IL, YL). We train a multi-modal encoder, defined by the non-linear transformation gy: I \u2192 Y on the labeled dataset in the context of supervised machine learning to predict the labels (Yu) for unlabeled micrographs (TU). Within this context, y denotes the trainable parameters, with the primary aim being to reduce the loss L\u2081, which is framed as:\n\nmin L1 (Li, Y) = \\sum l(g_\\Upsilon (L_i), Y_i)\n(L_i, Y_i) \\in D_L\n\nwhere y pred = gy (I\u2081) represents the predictions from the multi-modal encoder, l(\u00b7,\u00b7) signifies the cross-entropy loss."}, {"title": "Proposed Method", "content": "Electron Micrograph Encoder: Let's consider an input image I, which is represented as a 3D tensor with dimensions H\u00d7W\u00d7 C, where H represents the image's height in pixels, W represents its width in pixels, and C represents the number of channels of each pixel within the image. We divide the image into smaller, non-overlapping regions or patches to obtain a sequence of tokens. Each patch is treated as a token and has a fixed size with spatial dimensions of P \u00d7 P \u00d7 C, where P denotes the patch size. The total number of patches is given by n = (HW). We then linearly encode each patch, each of which has an overall size of P2C, to flatten it into a 1D vector represented as I' \u2208 Rn\u00d7d, where d is the patch embedding dimension. To provide the model with spatial information, we add positional embeddings to each patch. These positional embeddings are learnable vectors representing the position of each patch within the image grid. They help us understand the relative positions of different patches, and we add the position embeddings element-wise to the patch embeddings. In addition, we append a classification token <cls> to the patch sequence. This token aggregates information from all patches and provides an embedding of the entire patch sequence, creating a global representation of the entire input image. We input this augmented token sequence into ViT(Dosovitskiy et al. 2020), which consists of multiple stacked transformer encoder layers. Each encoder layer processes the patch embeddings hierarchically, refining representations at different abstraction levels. After passing through the transformer layers, we consider only the output embedding hels corresponding to the <cls> token as the representation of the entire image. This embedding aggregates information from all the patches and summarizes it. In summary: (a) We split the input image into patches, flatten them into 1D vectors, augment them with positional embeddings to provide spatial information, and include a classification token <cls>. (b) We process the resulting sequence of patch embeddings using a transformer-based architecture to capture long-range dependencies and relationships between different regions of the electron micrograph.\nZero-Shot CoT LLMs Prompting: We access LLMS through the LMaaS(Sun et al. 2022) hosted by the cloud-based service provider, which provides a text-based black-box API interaction to send text inputs and receive generated text without access to the underlying model parameters or gradients or the model's internal mechanisms. We utilize open-ended natural language prompts, designed to be flexible and non-restrictive, to instruct LLMs to generate the detailed technical descriptions related to various aspects of nanomaterials, including their structure, properties, and applications. We query LMMs to generate detailed descriptions of nanomaterials by employing a customized prompt template for zero-shot generation tasks, steering them through CoT prompts in a zero-shot context. This process involves extracting pre-existing domain-specific knowledge embedded within the language model parameters acquired during training to generate in-depth, technical descriptions of nanomaterials that are both detailed and comprehensive. The customized CoT prompt format is as follows:\nPrompt 1: Introduction: Provide an overview of the nanomaterial category and its significance across various fields. Prompt 2: Definition and Structure: Define the nanomaterial category and describe its typical structure at the nanoscale. Prompt 3: Synthesis Methods: Examine different methods employed for synthesizing or fabricating nanomaterials within this category. Discuss both their advantages and limitations. Prompt 4: Properties: Highlight the unique physical, chemical, and electronic properties exhibited by nanomaterials in this category. Explain how these properties differ from those of bulk materials. Prompt 5: Surface Modification: Describe strategies used to modify the surface properties of nanomaterials in this category, including techniques like functionalization, coating, or doping. Explain how these modifications enhance their performance or enable specific applications. Prompt 6: Applications: Explore the extensive range of applications wherein nanomaterials from this category find use. Discuss their potential impact on fields such as electronics, energy, medicine, and more.\nThe structured prompts are designed to facilitate a comprehensive, in-depth exploration of various facets, ranging from fundamental properties to practical applications and potential risks associated with these nanomaterials. Prompting LLMs generates text that responds to and elaborates on the specific aspects mentioned in each prompt.\nFine-Tuning Smaller LMs: Our approach employs a smaller pretrained language model (LM) to summarize the technical descriptions generated by a large language model (LLM) on nanomaterials. In our study, we utilize a pre-trained small-scale LM, DeBERTa\u00b9(He et al. 2020b), an improved version of the BERT architecture. We fine-tune this small-scale LM on the generated technical descriptions for domain-specific customization on the downstream task. This helps the language model learn the statistical relationships between words and phrases in the large corpus of LLM textual outputs, thereby facilitating the generation of context-aware token embeddings. We input the text sequences generated by LLMs (denoted as Sexpl) into the LMexpl model, which then generates expressive token embeddings as follows:\n\nh_{expl} = LM_{expl}(S_{expl})\n\nwhere the deep contextualized embeddings are denoted as hexpl \u2208 R m\u00d7d, where m represents the number of tokens in Sexpl and d is token embedding dimension. We perform weighted average of all token embeddings for a comprehensive representation of the entire text. We use a softmax attention mechanism to compute interpretable attention weights, denoted as a, and subsequently use these weights to sum-pool the token embeddings, encoding the textual descriptions into a fixed-size, high-level text representation as follows:\n\n a = softmax(q); q = u^T h_{expl}\n\\text{next} = \\sum a_j h^j_{expl}\n\nwhere u is a learnable vector and the subscript j refers to token. The text-level embedding htext \u2208 R(d) encapsulates the relevant and concise information at the core of the domain knowledge in technical descriptions, which is extracted from the general-purpose LLMs for each nanomaterial.\nCross-Modal Alignment Using Multi-Head Self-Attention: We employ the multi-head self-attention mechanism to align image embeddings with their corresponding text embeddings for image-text matching purposes. This approach emphasizes specific aspects or features of the image that is semantically relevant to the textual descriptions, taking into consideration the different facets of the relationship of the cross-domain modalities (both text-level and image embeddings). The mechanism calculates similarity scores between the provided image embedding and all text embeddings. The text embedding with the highest similarity score is considered the best match for the given image embedding. We initially concatenate the text-level embeddings for the different nanomaterial categories to obtain a unified text-level embedding\nhtext = [hext..., h], where c refers to the total number of nanomaterials. We compute the value and key projections for the unified text-level embedding, which represents the combined semantic information of all nanomaterials, for each head h as follows:\n\nK^{hext} = htext W_h^k; V^{hext} = htext W_h^v\n\nSimilarly, the query projection for image embedding hels for each head h is as follows:\n\nQ^{cls} = h_{cls} W_h^q"}, {"title": "", "content": "where Whk, West, and Whv are trainable weight matrices. We now utilize the scaled-dot product attention mechanism(Vaswani et al. 2017) to compute the normalized attention score, which measures the semantic similarity between the image embedding and each text embedding for a specific attention head h as follows:\n\nAh = softmax(\\frac{Q^{cls}(K^{hext})^T}{\\sqrt{dk}})\n\nwhere dk denotes the dimensionality of the keys. The attention weights align complementary information from the cross-domain embeddings, focusing on relevant information for cross-modal alignment and capturing richer semantics by allowing the framework to dynamically weigh different parts of the input based on their relevance to the context. We then compute the weighted sum of the value projection as follows:\n\nO^{text} = A^h V^{hext}\n\nWe concatenate the outputs across different heads because it encapsulates perspectives from multiple heads which focus on different aspects or features, thus producing a more comprehensive and robust alignment between the two modalities. We project the outputs to obtain the final representation as follows:\n\n HO^{text} = [text..., O^{text}] W^O\n\nwhere H represents the total number of heads and Wo deontes the trainable weight matrix. We now compute the cosine similarity between the two cross-domain embeddings as follows:\n\nSim = \\frac{O^{text} h_{cls}}{||O^{text}||_2\\times ||h_{cls} ||_2}\n\nwhere sim \u2208 R, and we select the best match based on the highest similarity value. The index is determined for the text-level embedding with the highest similarity score as follows:\n\ni^* = argmax_i (Sim)\n\nHere, i* is the index of the best-matching text-level embedding. So, the best-matching text-level embedding would be:\n\nh_i^{text} = h_{i^*}^{text}\n\nThis is essentially a matching mechanism that seeks to find the best pairwise alignment among the various text-level embeddings and the image embedding. We utilize backpropagation error in the downstream supervised multi-classification task to fine-tune the ViT and smaller LMs to maximize the pairwise alignment between the complementary image embedding (hcls) and its corresponding text-level embedding (htext). To summarize, hrtext incorporates the prior knowledge obtained from LLMs for the appropriate nanomaterial underlying the elecron micrographs as auxiliary information to support multi-modal learning.\nFew-Shot LMM Prompting: Few-shot prompting enables LMMs such as GPT-4V to adapt to new tasks without the need for explicit, gradient-based fine-tuning(Brown et al. 2020) using the labeled data. This approach allows LMMs to learn by analogy, utilizing only a few input-output pairs specific to the downstream task. Few-shot prompting leverages the implicit knowledge embedded in pretrained LMM parameters to adapt to new tasks through task-specific demonstrations, thereby avoiding the need to repurpose LMMs with parameter updates. The context-augmented prompt provides task-specific instructions and demonstrations(input-output pairs), enabling LMMs to generate outputs conditioned on the prompt for improved generalization performance on the new, unfamiliar tasks. In the case of nanomaterial identification tasks, few-shot prompting involves creating a context-augmented prompt using a few input-output mappings (Zi, Vi), which are a small number of image-label pairs as demonstrations sampled from the training data relevant to the query(new/unseen) image. These mappings provide relevant context to aid in understanding and classifying unseen images. The task-specific instruction is related to the query image, instructing the multimodal model to predict its associated label. At inference time, given test images Itest, few-shot prompting predicts an output label based on the conditional probability distribution, P(test | ((Itrain, Ytrain), Ztest)). \u03a4\u03bf explore how the quality and quantity of few-shot demonstrations affect the performance in nanomaterial identification tasks, we consider two distinct sampling strategies: \u201cRandom\u201d and \u201cSimilarity-driven Sampling\u201d. The random approach involves selecting demonstrations (image-label pairs) arbitrarily from the training data without any specific criteria, serving as a baseline for evaluation. In contrast, the similarity-driven sampling strategy employs cosine similarity to identify the top-K images that most closely resemble a given query image within the training data. This strategy operates under the hypothesis that demonstrations which are more representative of the query image's data distribution can potentially enhance model adaptability and accuracy. By utilizing both diverse strategies to construct augmented prompts, we aim to provide a comprehensive analysis of how different demonstration sampling methods impact the effectiveness of few-shot learning in nanomaterial identification tasks. Furthermore, the efficacy of the demonstrations is inherently related to the sampling methods used to retrieve the top-K images that align most closely with the query image. To further explore the impact of the volume of demonstrations on performance, we adjust the number of demonstrations K for each query image. In summary, our objective is to provide LMMs with a context-augmented prompt, comprising image-label pairs selected from the training data, along with task-specific instructions that guide the LMMs in predicting the nanomaterial category of the query image. This task evaluates the LMMs' ability to predict nanomaterial categories based on the prompt without any parameter updates, distinguishing it from traditional supervised learning, where models are fine-tuned on labeled data. For each query image, the LMMs generate a c-dimensional one-hot vector hpred \u2208 R, where e denotes the predefined number of nanomaterial categories. This vector is linearly encoded into a high-dimensional space to produce a prediction embedding hICL \u2208 Rd, which encapsulates the LMMs predictions. Here, d represents the embedding dimension and c\u226a d. An example of an ICL prompt is as follows,\nBelow are the input-output pairs (image-label pairs) for the nanomaterial identification task. Predict the nanomaterial category for the query image.\nThe prediction embedding likely contains valuable information about potential outcomes, allowing the framework to refine its cross-modal representation for better alignment with desired results. Given the complexity of nanomaterials structures and properties, this prediction embedding has the potential to capture some of that complexity, guiding the framework toward correct identification through the integration of prior knowledge and auxiliary information. A general purpose GPT-4V is accessible to ChatGPT Plus subscribers with a usage cap at chat.openai.com. However, it's not currently available for public use through Multimodal Modeling as a Service (MMaaS) - a cloud-based service that accepts both image and textual inputs as prompts. By utilizing black-box GPT-4V through MMaaS as an on-demand service, typically hosted on cloud servers and accessed via an API, users can design task-specific prompts to query pre-trained LMMs for solving multimodal tasks of interest. This is analogous to how users might access LLMs via Language Modeling as a Service (LMaaS(Sun et al. 2022)) to tackle language-specific tasks. GPT-4V is anticipated to become publicly accessible by mid-November 2023. APIs are designed for large-scale and concurrent requests and are ideal for integration into automated systems. Conversely, websites might not efficiently handle numerous interactions in rapid succession, and automating tasks on them could be prohibited. Manually sending inputs for GPT-4V for the entire training dataset would be a daunting and tedious task. Instead, we select a subset of images from the whole training dataset, termed 'Confounding or Ambiguous Micrographs', for few-shot prompting of GPT-4V. The selection process for these images is as follows: The SEM electron micrographs, initially sized at 1024 \u00d7 768 \u00d7 3 pixels, were downscaled to 224 \u00d7 224 \u00d7 3 pixels. They were then normalized using z-score normalization and flattened into one-dimensional vectors. Their dimensionality was further reduced using Principal Component Analysis (PCA) before employing the K-Means clustering algorithm. We chose K = 10 clusters based on a predefined number of nanomaterial categories based on benchmark dataset. This method identifies images that are challenging to classify by measuring distances from centroids, assessing cluster variance, and calculating the Silhouette Score. Ground truth labels aid in the evaluation of the clustering's effectiveness. We sampled a fixed 10% of the ambiguous images from the entire dataset. For these images, we apply few-shot prompting of GPT-4V to predict labels. The goal is to learn the optimal projection layer weight matrices, which transform the GPT-4V predictions (one-hot vectors) into a high-dimensional space, producing a prediction embedding h\u2081cL \u2208 Rd that encapsulates the GPT-4V predictions. The projection layer is subsequently trained through the supervised learning task. This training aims to minimize the cross-entropy loss and obtain optimal weights.\nUnified Attention Layer: We compute the cross-modal embedding, denoted as hcross, using a hierarchical multi-head attention mechanism that integrates the original image embedding hels, the text-level embedding hext, and the prediction embedding HICL. This mechanism offers a robust framework for integrating diverse information from different modalities. As a result, it can produce a more holistic representation that encompasses a wide range of information, potentially improving the performance of nanomaterial identification tasks. In general, the multi-head attention mechanism uses multiple heads to capture different attention patterns, allowing the model to recognize a variety of relationships in the data from multiple subspace representations. Given queries Q, keys K, and values V, the scaled dot-product attention is defined as follows:\n\nAttention(Q, K, V) = softmax(\\frac{QKT}{\\sqrt{dk}})V\n\nwhere dk is the dimensionality of the keys. The multi-head attention mechanism employs multiple heads to integrate various attention patterns into a unified representation. Each of these heads utilizes the scaled dot-product attention on distinct linear transformations of the input queries, keys, and values. These transformations are parameterized by matrices WQ, WK\u2081, and Wv. The final output is derived by concatenating the results from these heads and subjecting it to a subsequent matrix transformation Wo.\n\nMultiHead(Q, K, V) = Concat(head1, ..., headh) Wo\n\nwhere head = Attention(QWQ, KWK\u2081, VWv;)\nGiven the context described earlier, the unified attention layer employs the multi-head attention mechanism in a hierarchical fashion to derive the cross-modal embedding. The procedure consists of two main stages: (1) Image-Text Attention: Here, the unified attention layer focuses on the image embedding hels in relation to the text-level embedding htext. The result is an intermediate embedding, himg-text, which amalgamates details from both image and text modalities through the multi-head attention mechanism. The primary intent of this step is to incorporate relevant textual information guided by the image's context. This can be mathematically described as follows:\n\nh_{img-text} = MultiHead (h_{cls}, text, text)\n\n(2) Image-Text-Prediction Attention: During this stage, the previously derived intermediate embedding himg-text undergoes further refinement. The unified attention layer aligns this embedding with the prediction embedding hICL, computing the final cross-modal representation hcross. This stage aims to combine insights from the intermediate representation with the prediction embedding, creating a comprehensive representation that seamlessly integrates various modalities. This can be represented mathematically as:\n\nh_{cross} = MultiHead(h_{img-text}, H_{ICL}, H_{ICL})\n\nIn summary, the unified attention layer uses multi-head attention mechanisms to capture and integrate information from multiple different modalities (image, text, prediction) in a hierarchical manner, resulting in the comprehensive cross-modal embedding that can be used for nanomaterial identification tasks. This mechanism employs multiple sets of learned weight matrices to emphasize various aspects or relationships within the data. Consequently, this approach has the potential to foster robust and enriched embeddings capable of capturing complex patterns. Additionally, it aids in focusing on contextually relevant information and in achieving semantic alignment across different embeddings, thereby enhancing the capacity to identify and utilize important features in the input data. Finally, we linearly transform the final unified cross-modal embedding to obtain a probability distribution p\u2081 over the possible outcomes, as follows:\n\np\u2081 = softmax (Whcross)\n\np\u2081 represents the probability distribution across nanomaterial categories. We apply the argmax operation to p\u2081 to"}, {"title": "Experiments And Results", "content": "Datasets: Our study utilized the SEM dataset(Aversa et al. 2018) to automate nanomaterial identification. The expert-annotated dataset spans 10 distinct categories, representing a range of nanomaterials, such as particles, nanowires, and patterned surfaces, among others. It contains approximately 21,283 electron micrographs.\nResults: To measure the effectiveness of our proposed framework, we conducted an in-depth analysis contrasting it with popular computer vision baseline models. We compared our framework to supervised learning models, notably Convolutional Neural Networks (ConvNets) and Vision Transformers (al. 2022b,a), and self-supervised approaches like Vision Contrastive Learning (et al. 2020). The results of this analysis are shown in Table 1. To ensure an fair and rigorous comparison, all tests were conducted under uniform settings across different algorithms. We assessed performance using the Top-N accuracy metric, specifically for N values of 1, 2, 3, and 5. Notably, our framework outperformed the best-performing baseline model, T2TViT ((Yuan et al. 2021)), demonstrating a significant 30.50% improvement in Top-1 accuracy and a modest 3.61% gain in Top-5 accuracy. Table 2 presents a comparison between our framework and a selection of supervised learning-based baseline models. This includes several GNN architectures (Rozemberczki et al. 2021; Fey and Lenssen 2019) as well as Graph Contrastive Learning (GCL) algorithms (Zhu et al. 2021). Impressively, our framework establishes a new state-of-the-art benchmark, outperforming all other baselines on the benchmark dataset(Aversa et al. 2018). Figures 5 and 6 shows the radar charts corresponding to the results shown in Tables 1 and 2. The underlying hypothesis of our framework is that ViTs can be employed for initial explorations and the generation of baseline results in this context. Zero-shot CoT prompting of LLMs can be leveraged to enhance the initial outcomes of ViTs by utilizing the implicit domain-specific knowledge embedded within the language model's trainable parameters to obtain expressive cross-modal embeddings. On the other hand, few-shot (in-context) learning of LMMs can be utilized to further refine the framework's predictions by providing demonstrations from the training data, potentially leading to a more robust and accurate predictive framework for nanomaterial category prediction. The experimental findings validate this hypothesis and further advancements in the semiconductor industry-a domain where traditional deep learning techniques often underperform due to their lack of a holistic and nuanced approach. Such shortcomings could hinder breakthroughs in the semiconductor industry."}, {"title": "Related Work", "content": "The landscape of computer vision has been profoundly influenced by convolutional networks (ConvNets or CNNs). The pioneering LeNet(LeCun et al. 1998) set the stage for ConvNets, which were subsequently employed in a variety of vision tasks ranging from image classification(Krizhevsky, Sutskever, and Hinton 2017) to semantic segmentation(Long, Shelhamer, and Darrell 2015). Over recent years, groundbreaking architectures like ResNet(He et al. 2016), MobileNet(Howard et al. 2017), and NAS(Zoph and Le 2016; Yang et al. 2020) have further refined the capabilities of CNNs. However, the introduction of vision transformers (ViTs)(Dosovitskiy et al. 2020; Han et al. 2022; Carion et al. 2020; Chen et al. 2021a) marked a paradigm shift, leading to the development of numerous enhanced ViT variants. These advances encompass pyramid architectures(Liu et al. 2021; Wang et al. 2021), local attention mechanisms(Han et al. 2021; Liu et al. 2021), and innovative position encoding methods(Wu et al. 2021b). Drawing inspiration from ViTs, the computer vision community has also delved into the potential of Multilayer Perceptrons (MLP) for vision tasks(Touvron et al. 2022; Tolstikhin et al. 2021). Current vision-based frameworks in the semiconductor manufacturing sector fall short in various aspects, especially when compared to the recently proposed advancements in generative deep learning and multimodal learning. Many existing solutions fail to capitalize on the detailed analysis achievable through the synergy of LLMs, LMMs, and small-scale LMs with electron micrographs. Moreover, the existing frameworks typically analyze electron micrographs (nano images) at a singular modality, through the use of architectures such as ConvNets, ViTs, or MLP-Mixer, missing the opportunities that a multi-modality fusion approach could offer in enhancing classification accuracy. Furthermore, the industry has not fully embraced the utilization of zero-shot CoT LLMS prompting for generating technical descriptions of nanomaterials, which can significantly enhance domain-specific insights essential for nanomaterial identification tasks. Furthermore, the semiconductor manufacturing sector has not fully tapped into the emerging in-context learning capabilities of LMMs with few-shot prompting for predictive nanomaterial analysis, even though these capabilities could significantly enhance the accuracy of nanomaterial predictions. This glaring gap in the integration of image-based and linguistic insights renders current architectures less comprehensive and nuanced, potentially impeding breakthroughs in the semiconductor industry. Instead of relying solely on conventional classification methods, the new framework incorporates both image-based and linguistic insights by leveraging the capabilities of ViTs and LLMs, respectively, as well as the predictive abilities of LMMs. This framework aims to facilitate a more comprehensive and nuanced analysis of electron micrographs, holding significant promise for advancements in the semiconductor industry through automated nanomaterial identification. These advancements highlight the ongoing push for innovation in semiconductor manufacturing, driven by the escalating demand for more powerful and efficient electronic devices."}, {"title": "Conclusion", "content": "To conclude, we conducted the first in-depth study aimed at achieving state-of-the-art performance in nanomaterial characterization. We have introduced an innovative framework that employs ViTs as the foundational layer, further enriched through the multi-modal fusion approach of zero-shot CoT prompting of LLMs and refined with few-shot (in-context) learning of LMMs. Our experiments confirm the superiority of this framework, indicating its transformative potential for semiconductor manufacturing in the age of advanced electronic devices."}, {"title": "Technical Appendix", "content": "Ablation Study\nFigure 3 illustrates an overview of the framework. The proposed framework involves four components: (a) The first component, the electron micrograph encoder, takes an input image and divides it into smaller patches. These patches are transformed into tokens, enriched with positional embeddings for spatial information, and a classification token is added as a separate token to represent the overall image content. The resulting augmented token sequence is fed into a ViT model to generate an embedding that represents the entire image. (b) Next, the zero-shot CoT LLMs prompting technique uses cloud services to access LLMs for generating detailed descriptions about various aspects of nanomaterials. Structured prompts guide the LLMs in generating in-depth descriptions on topics ranging from the fundamental properties of nanomaterials to their practical applications. Subsequently, we fine-tune smaller LMs on the descriptions generated by the LLMs to obtain context-aware token embeddings. We then perform sum-pooling attention mechanism to obtain contextualized text-level embeddings, which capture the core knowledge in the generated texts. (c) Finally, the cross-modal alignment employs the scaled dot-product attention mechanism to match the image embeddings with the corresponding nanomaterial-specific text-level embeddings. This process highlights image features relevant to the textual descriptions, aiding in the identification of text-level embeddings that correspond to the image. (c) Few-shot prompting enables LMMs to quickly adapt to new tasks without traditional fine-tuning on labeled data. Using a small set of input-output pairs, these models learn tasks by drawing from their vast pre-existing knowledge acquired during training on vast, diverse text corpora. In the context of nanomaterial identification, LMMs utilize a handful of image-label pairs from the training data to classify new, unseen images and obtain prediction embeddings. Demonstrations can be selected either randomly or based on their similarity to the (c) Few-shot prompting enables LMMs to quickly adapt to new tasks without traditional fine-tuning on labeled data. Using a small set of input-output pairs, these models learn tasks by drawing from their vast pre-existing knowledge acquired during training on vast, diverse text corpora. In the context of nanomaterial identification, LMMs utilize a handful of image-label pairs from the training data to classify unseen query images and obtain prediction embeddings. Demonstrations can be selected either randomly or based on their similarity to the query image. (d) Finally, the unified attention layer, through the hierarchical multi-head attention mechanism, combines information from the original image embedding, a text-level embedding, and the prediction embedding, optimizing for accuracy in nanomaterial categorization. To evaluate the efficacy of the individual components and validate the design choices for their inclusion in the framework, we conducted an ablation study. In this study, we selectively disabled specific components to create various ablated variants, which were then evaluated using the SEM dataset(Aversa et al. 2018) on nanomaterial identification. Compared to our proposed original framework, which serves as the baseline, the ablated variants exhibited a notable decline in performance, underscoring the importance of the components that were disabled. The ablation study results support the hypothesis that each component is crucial for the framework's peak performance in nanomaterial identification. Ablated variants excluding the zero-shot CoT LLMs prompting with cross-modal alignment, few-shot prompting with LMMs, and unified attention methods are labeled as proposed framework \u201cw/o LLMs\u201d, \u201cw/o LMMs\u201d, and \u201cw/o MHA\"; \"w/o\" is shorthand for \"without\". In the case of \"w/o MHA", "w/o LLMs": "ariant exhibits a significant decrease in performance compared to the baseline", "3": "In our ablation study, we methodically deactivate individual components to evaluate their unique contributions. The aim is to gauge how specific components influence the framework's overall performance. The experiments underscore the importance of each disabled component, as evidenced by the marked drop in performance metrics when compared to the baseline. These results validate our hypothesis that the joint optimization of the zero-shot CoT LLMs prompting with cross-modal alignment, few-shot prompting with LMMs, and unified attention methods enhances the overall framework's efficacy.\nCross-modal alignment correlates these text embeddings with image embeddings via scaled dot-product attention, thereby synchronizing and matching information across different types of data modalities in a unified representation space. Similarly, the 'w/o LMMs' variant performs notably"}]}