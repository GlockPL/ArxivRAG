{"title": "TMLC-Net: Transferable Meta Label Correction for Noisy Label Learning", "authors": ["Mengyang Li"], "abstract": "The prevalence of noisy labels in real-world datasets poses a significant impediment to the effective deployment of deep learning models. While meta-learning strategies have emerged as a promising approach for addressing this challenge, existing methods often suffer from limited transferability and task-specific designs. This paper introduces TMLC-Net, a novel Transferable Meta-Learner for Correcting Noisy Labels, designed to overcome these limitations. TMLC-Net learns a general-purpose label correction strategy that can be readily applied across diverse datasets and model architectures without requiring extensive retraining or fine-tuning. Our approach integrates three core components: (1) Normalized Noise Perception, which captures and normalizes training dynamics to handle distribution shifts; (2) Time-Series Encoding, which models the temporal evolution of sample statistics using a recurrent neural network; and (3) Subclass Decoding, which predicts a corrected label distribution based on the learned representations. We conduct extensive experiments on benchmark datasets with various noise types and levels, demonstrating that TMLC-Net consistently outperforms state-of-the-art methods in terms of both accuracy and robustness to label noise. Furthermore, we analyze the transferability of TMLC-Net, showcasing its adaptability to new datasets and noise conditions, and establishing its potential as a broadly applicable solution for robust deep learning in noisy environments.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning has achieved remarkable success across a wide range of domains, including computer vision, natural language processing, and speech recognition [1]-[4]. This success is largely driven by the availability of massive datasets and the development of powerful neural network architectures. However, a critical, often overlooked assumption underlying the effectiveness of these models is the accuracy of the labels in the training data. In real-world scenarios, this assumption is frequently violated due to factors such as human annotation errors, imperfect data collection processes, and the inherent subjectivity of certain labeling tasks [5], [6]. The presence of noisy labels can significantly compromise the performance, re-\nliability, and generalization capability of deep learning models [7], [8], leading to overfitting, bias amplification, and reduced robustness to adversarial attacks [9]-[12].\nAddressing the challenge of noisy labels has led to a substantial body of research, broadly categorized into loss correction, sample selection, and label correction techniques [13]-[19]. Loss correction methods modify the loss function to be less sensitive to noisy labels, using robust loss functions or loss adjustment strategies. Sample selection methods aim to identify and remove or down-weight noisy samples. Label correction methods attempt to directly modify the incorrect labels. Despite the progress, many existing methods rely on strong assumptions about the underlying noise distribu- tion (e.g., class-conditional or uniform noise), exhibit limited adaptability to new situations, and incur high computational costs. Meta-learning has emerged as a promising direction [20]-[22], but existing meta-label correction methods often fall short in terms of transferability, hindering their practical utility and increasing computational overhead due to bi-level optimization.\nTo overcome these limitations, we introduce TMLC-Net, a Transferable Meta-Learner for Correcting Noisy Labels. TMLC-Net is explicitly designed to learn a general-purpose label correction strategy that can be effectively transferred across diverse datasets and model architectures without re- quiring extensive retraining or fine-tuning. This transferability is a key differentiator from existing meta-label correction ap- proaches. TMLC-Net achieves this through a combination of: (1) Normalized Noise Perception, capturing and normalizing training dynamics to handle distribution shifts; (2) Time-Series Encoding, modeling the temporal evolution of sample statistics using an LSTM; and (3) Subclass Decoding, predicting a corrected label distribution based on the learned representations.\nThe proposed TMLC-Net offers several advantages: it ad- dresses the critical limitation of transferability in existing methods, enhances robustness to distribution shifts via normal- ized noise perception, captures the dynamic learning process through time-series encoding, and provides a more informed and less brittle label correction via subclass decoding.\nOur contributions are summarized as follows:\n\u2022 We introduce TMLC-Net, a novel meta-learning frame- work specifically designed for transferable noisy label correction, addressing a key gap in existing research.\n\u2022 We propose the integration of normalized noise per- ception, time-series encoding, and subclass decoding, providing a robust and adaptive mechanism for label correction.\n\u2022 We conduct extensive experiments on multiple bench- mark datasets with diverse noise types and levels, demon- strating TMLC-Net's state-of-the-art performance com- pared to existing methods.\n\u2022 We extensively analyze TMLC-Net's transferability, val- idating its effectiveness as a general-purpose solution for noisy label learning, significantly reducing the computa- tional burden of meta-label correction."}, {"title": "II. RELATED WORK", "content": "The problem of learning from noisy labels has a long history in machine learning, and the recent success of deep learning has renewed interest in this area. Here, we provide a comprehensive review of related work, focusing on the most relevant approaches and highlighting their connections to our proposed TMLC-Net.\nA. Noisy Label Learning\nMethods for handling noisy labels can be broadly classified into the following categories:\n1) Loss Correction: These methods aim to mitigate the impact of noisy labels by modifying the loss function. Some common strategies include:\nRobust Loss Functions: Employing loss functions that are inherently less sensitive to outliers, such as the Mean Absolute Error (MAE) [13], Huber loss [23], or generalized cross- entropy loss [24]. These functions down-weight the contri- bution of samples with large losses, which are more likely to be noisy. Loss Adjustment: Estimating the noise rate or noise transition matrix and using this information to adjust the loss function. Examples include forward and backward correction methods [14], [15], [25], [26]. These approaches try to explicitly model the label corruption process. Bootstrapping: A technique where the target labels are modified by combining the given noisy labels with the model's own predictions [18], [27]. This can be seen as a form of self-training. Regulariza- tion: Applying regularization to make the model more noise tolerant [28], [29].\n2) Sample Selection: These methods focus on identifying and either removing or down-weighting potentially noisy sam- ples during training.\nMentorNet: Jiang et al. [16] proposed MentorNet, a meta- learning approach where a \"mentor\" network learns to select clean samples for training a \"student\" network. The mentor network is typically pre-trained on a small, clean dataset. Co- teaching: Han et al. [17] introduced Co-teaching, where two networks are trained simultaneously, and each network selects small-loss samples to teach the other network. This is based on the idea that networks trained with different initializations will disagree on noisy samples. Co-teaching+ [22] improves upon this by incorporating a disagreement-based strategy for sample selection. Iterative Methods: These methods involve iteratively training a model, identifying potentially noisy samples (e.g., based on high loss or disagreement between multiple models), and either removing or relabeling those samples [30]. Active Learning: Although traditionally used for selecting the most informative samples to be labeled, active learning principles can be adapted to identify potentially noisy samples for relabeling or closer inspection [31].\n3) Label Correction: These methods attempt to directly correct the noisy labels in the training data.\nJoint Optimization: Some approaches formulate the learning problem as a joint optimization over the model parameters and the true labels [19], [32], [33]. These methods often involve alternating between updating the model parameters and estimating the true labels. Meta-Learning for Label Correction: This is the category most relevant to our work. Several recent papers have explored using meta-learning to learn a label correction function. Ren et al. [20] proposed Meta-Weight- Net, which learns to assign weights to training samples based on their gradients. Li et al. [21] proposed a meta-learning approach for learning a label correction function in a few-shot learning setting. Zheng et. al [34] introduces a meta-learning module to estimate the instance-dependent label transition matrix. Graph-Based Methods: These methods construct a graph where nodes represent samples and edges represent similarity. Noisy labels are then corrected based on the labels of neighboring nodes [35], [36]. Reweighting methods: These methods focus on re-weighting training samples to minimize the influence of noisy labels on the training. [37]\nB. Meta-Learning\nMeta-learning, or \"learning to learn,\" aims to develop algo- rithms that can learn new tasks quickly and efficiently, often with limited data. Key approaches include:\nModel-Agnostic Meta-Learning (MAML): MAML [38] seeks to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will lead to large improvements on new tasks. This is achieved by optimizing for good performance after a few gradient steps on a new task. Metric-Based Meta-Learning: These methods learn an embedding space where learning is simplified. Prototypical Networks [39] learn a metric space where classification can be performed by computing distances to prototype representations of each class. Recurrent Models: Recurrent models, such as LSTMs, can be used to process a sequence of data from a new task and learn an update rule or a representation that is suitable for that task [40], [41].\nOur work builds upon the meta-learning paradigm, but with a crucial focus on transferability, which has been less explored in the context of noisy label learning.\nC. Transfer Learning\nTransfer learning aims to leverage knowledge learned from one task (the source task) to improve performance on a different but related task (the target task) [42], [43]. While not the central focus of our paper, transfer learning concepts are relevant because our goal is to develop a label correction method that can be transferred to new datasets and noise distributions. Common transfer learning strategies include fine-tuning pre-trained models, feature extraction, and domain adaptation [44].\nThe key distinction of our work is the combination of meta-learning and transfer learning principles to address the problem of noisy labels. While prior work has explored meta- learning for label correction, our focus on transferability across diverse datasets and noise types, and our specific architectural choices (normalized noise perception, time-series encoding, and subclass decoding), set our approach apart."}, {"title": "III. METHOD", "content": "In this section, we introduce our proposed Transferable Meta-Learner for Correcting Noisy Labels (TMLC-Net). We first define the notation used throughout this section, then detail the architecture of TMLC-Net, and finally present the training algorithm.\nA. Problem Formulation and Notation\nLet $\\mathcal{D} = \\{(x_i, \\tilde{y}_i)\\}_{i=1}^N$ represent a training dataset with $N$ samples, where $x_i \\in \\mathbb{R}^d$ is the i-th input sample (e.g., an image) and $\\tilde{y}_i \\in \\{1,2,..., C\\}$ is its corresponding noisy label. We assume that each noisy label $\\tilde{y}_i$ is a corrupted version of the true, unknown label $y_i$. Our goal is to train a deep learning model $f(x; \\theta)$ that is robust to the label noise in $\\mathcal{D}$. The model $f$ maps an input $x$ to a C-dimensional output vector, and $p(x; \\theta) = \\text{softmax}(f(x; \\theta))$ represents the predicted probability distribution over the $C$ classes.\nWe denote the cross-entropy loss for sample $i$ at epoch $t$ as $l_i^t$. Our proposed TMLC-Net, denoted as $g(\\cdot; \\phi)$, is a meta-learner that takes as input information about the training dy- namics of a sample and outputs a corrected label distribution. The parameters of TMLC-Net are denoted by $\\phi$.\nB. TMLC-Net Architecture\nTMLC-Net is composed of three core modules:\n1. Normalized Noise Perception (NNP): This module pro- cesses raw training statistics to generate normalized features that are robust to distribution shifts. 2. Time-Series Encoding (TSE): This module uses a recurrent neural network (RNN) to model the temporal evolution of the normalized features. 3. Subclass Decoding (SD): This module predicts a corrected label distribution based on the encoded temporal information.\n1) Normalized Noise Perception (NNP): The NNP module aims to capture informative statistics about each sample's training dynamics while ensuring robustness to variations across datasets and noise distributions. We compute the fol- lowing normalized features for each sample $i$ at each epoch $t$:\n1. Category-Normalized Loss (CNL): This feature repre- sents the loss of the sample relative to the average loss of sam- ples with the same noisy label within the current mini-batch. This helps to identify samples that are significantly harder to learn than others within the same (potentially incorrect) label group.\n$\\text{CNL}_i^t = \\frac{l_i^t}{\\frac{1}{|B_{\\tilde{y}_i}|} \\sum_{j \\in B_{\\tilde{y}_i}} l_j^t}$\nwhere $B_{\\tilde{y}_i} = \\{j | j \\in B, \\tilde{y}_j = c\\}$ is the set of indices of samples in the current mini-batch $B$ that have the same noisy label $c$ as sample $i$ (i.e., $\\tilde{y}_i = c$), and $|B_c|$ is the number of samples in this set.\n2. Global-Normalized Loss (GNL): This feature represents the loss of the sample relative to the average loss of all samples in the current mini-batch. This provides a global context for the sample's difficulty.\n$\\text{GNL}_i^t = \\frac{l_i^t}{\\frac{1}{|B|} \\sum_{j \\in B} l_j^t}$\nwhere $|B|$ is the size of the mini-batch.\n3. Prediction Entropy (PE): This feature measures the uncer- tainty of the model's prediction for the sample. High entropy indicates that the model is less confident in its prediction, which could be a sign of a noisy label.\n$\\text{PE}_i^t = - \\sum_{c=1}^C p_c(x_i; \\theta) \\log p_c(x_i; \\theta)$\nwhere $p_c(x_i;\\theta)$ is the c-th element of model prediction.\n4. Noisy Label One-hot (NLO):\n$\\text{NLO}_i = \\text{onehot}(\\tilde{y}_i)$\nThe NNP module concatenates these normalized features into a single feature vector for each sample at each epoch:\n$f_i^t = [\\text{CNL}_i^t, \\text{GNL}_i^t, \\text{PE}_i^t, \\text{NLO}_i]$\n2) Time-Series Encoding (TSE): The TSE module takes the sequence of normalized feature vectors $\\{f_i^1, f_i^2, ..., f_i^T\\}$ for sample $i$ over $T$ epochs and models their temporal evolution using a recurrent neural network (RNN). We use a Long Short- Term Memory (LSTM) network [45] due to its ability to capture long-range dependencies in sequential data.\nThe LSTM processes the sequence of feature vectors and produces a hidden state $h_i^t$ at each epoch $t$:\n$h_i^t = \\text{LSTM}(f_i^t, h_i^{t-1}; \\Theta_{\\text{LSTM}})$\nwhere $\\Theta_{\\text{LSTM}}$ represents the parameters of the LSTM. The final hidden state $h_i^T$ encodes the entire history of the sample's training dynamics, capturing information about how its loss, prediction uncertainty, and relationship to other samples have changed over time.\n3) Subclass Decoding (SD): The SD module takes the final hidden state $h_i^T$ from the TSE module and predicts a corrected label distribution $\\hat{y}_i$. We use a fully connected layer followed by a softmax activation function:\n$\\hat{y}_i = \\text{softmax}(W_2\\text{ReLU}(W_1 h_i^T + b_1) + b_2)$\nwhere $W_1$, $b_1$, $W_2$, and $b_2$ are the learnable parameters of the fully connected layers, and ReLU is the rectified linear unit activation function. The output $\\hat{y}_i$ is a $C$-dimensional vector representing a probability distribution over the classes. This allows for a more nuanced correction than simply predicting a single \"hard\" label. The model can express uncertainty about the true label, and the downstream loss function can take this uncertainty into account.\nC. Training Algorithm\nTMLC-Net is trained using a meta-learning approach. We split the training data $\\mathcal{D}$ into two disjoint sets: a support set $\\mathcal{D}_s$ and a query set $\\mathcal{D}_q$. The support set is used to train the base model $f(x; \\theta)$, and the query set is used to train the meta-learner TMLC-Net $g(\\cdot; \\phi)$.\nThe training procedure involves two nested loops:\n1. Inner Loop (Base Model Training): In the inner loop, we train the base model $f(x; \\theta)$ on the support set $\\mathcal{D}_s$ for a small number of epochs (e.g., one epoch). We use the corrected label distribution $\\hat{y}_i$ predicted by TMLC-Net as the target for training the base model. The loss function for the base model is the cross-entropy loss between the model's predicted probabilities $p(x_i; \\theta)$ and the corrected label distribution $\\hat{y}_i$:\n$\\mathcal{L}_{base} = \\sum_{(x_i,\\tilde{y}_i) \\in \\mathcal{D}_s}  \\sum_{c=1}^C - y_{i,c} \\log p_c(x_i; \\theta)$\nwhere $y_{i,c}$ is the c-th element of $\\hat{y}_i$.\nWe update the base model parameters $\\theta$ using gradient descent:\n$\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{base}$\nwhere $\\alpha$ is learning rate.\n2. Outer Loop (Meta-Learner Training): In the outer loop, we train the meta-learner TMLC-Net $g(\\cdot; \\phi)$ on the query set $\\mathcal{D}_q$. We first compute the normalized features $f_i^t$ for each sample in the query set using the updated base model parameters $\\theta$ from the inner loop. We then use the TSE module to encode the temporal evolution of these features and the SD module to predict the corrected label distribution $\\hat{y}_i$.\nThe loss function for the meta-learner is the Kullback- Leibler (KL) divergence between the predicted corrected label distribution $\\hat{y}_i$ and a target distribution derived from the noisy label $\\tilde{y}_i$. We use the KL divergence because we are predicting a distribution, not a single label. The target is a one-hot vector if we have access to \"clean\" labels for evaluation (even if the training data is noisy). During the actual training phase, we don't assume access to true labels. Instead we use a softened version of the noisy label, obtained by adding a small amount of uniform noise:\n$y_i^{\\text{target}} = (1 - \\epsilon) \\cdot \\text{one\\_hot}(\\tilde{y}_i) + \\frac{\\epsilon}{C} \\cdot \\mathbf{1}$\nwhere $\\epsilon$ is a small constant (e.g., 0.1) controlling the amount of softening, $\\text{one\\_hot}(\\tilde{y}_i)$ is a one-hot vector representation of the noisy label, and $\\mathbf{1}$ is a vector of ones. This softening helps to prevent the meta-learner from simply memorizing the noisy labels.\nThe meta-learner loss is then:\n$\\mathcal{L}_{meta} = \\sum_{(x_i,\\tilde{y}_i) \\in \\mathcal{D}_q} KL(y_i^{\\text{target}} || \\hat{y}_i)$\nWe update the meta-learner parameters $\\phi$ using gradient descent:\n$\\phi \\leftarrow \\phi - \\beta \\nabla_{\\phi} \\mathcal{L}_{meta}$\nwhere $\\beta$ is learning rate.\nAfter training, TMLC-Net can be used to correct noisy labels in new, unseen datasets. Given a new dataset $\\mathcal{D}' = \\{(x', y')\\}$, we simply feed the data through TMLC-Net (using the trained parameters $\\phi$) to obtain the corrected label distri- butions $\\hat{y}'$. These corrected distributions can then be used to train a new model or fine-tune an existing model. This is the \"transfer\" aspect of TMLC-Net."}, {"title": "IV. EXPERIMENTS", "content": "To evaluate the effectiveness and transferability of TMLC- Net, we conduct extensive experiments on several benchmark datasets with various types and levels of label noise. We com- pare TMLC-Net against a range of state-of-the-art methods, including both traditional noisy label learning approaches and existing meta-learning techniques.\nA. Datasets\nWe use the following datasets in our experiments:\nCIFAR-10 and CIFAR-100: [46] These are widely used benchmark datasets for image classification. CIFAR-10 con- sists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. CIFAR-100 has the same number of images but in 100 classes, with 600 images per class. Both datasets are split into 50,000 training images and 10,000 test images. Clothing1M: [47] This is a large-scale real-world dataset with noisy labels. It contains 1 million images of clothing items collected from online shopping websites. The labels are obtained from surrounding text and are known to be highly noisy. Following standard practice, we use the provided clean subset for validation and testing. WebVision: [48] This is another a real-world large scale dataset.\nWe use these datasets to evaluate TMLC-Net under both synthetic and real-world noise conditions.\nB. Noise Models\nTo systematically evaluate the robustness of TMLC-Net, we introduce different types and levels of synthetic label noise into the CIFAR-10 and CIFAR-100 datasets. We consider the following noise models:\nSymmetric Noise: For a given noise rate r, we randomly flip each label to any of the other $C - 1$ classes with equal probability. This is a common benchmark for noisy label learning. Asymmetric Noise: We flip labels within predefined groups of visually similar classes. For example, in CIFAR- 10, we might flip \"cat\" to \"dog,\" \"bird\" to \"airplane,\" etc. This simulates a more realistic scenario where label errors are not uniformly distributed. Instance-dependent Noise: More complex noise that depends on sample features.\nFor each noise model, we experiment with different noise rates (e.g., 20%, 40%, 60%).\nC. Baseline Methods\nWe compare TMLC-Net against the following baseline methods:\nCross-Entropy (CE): Standard training with cross-entropy loss, without any noise handling. This serves as a lower bound on performance. Label Smoothing (LS): A simple regularization technique that softens the target labels [?]. Forward Correction: A loss correction method that estimates the noise transition matrix and uses it to adjust the loss [15]. Decoupling: It trains two networks and let them select small loss samples to each other [49]. MentorNet: A meta-learning approach that learns to select clean samples for training [16]. Co-teaching+: An improved version of Co-teaching that uses a disagreement-based strategy for sample selection [22]. Meta- Weight-Net (MWN): A meta-learning approach that learns to assign weights to training samples based on their gradients [20]. This is the most closely related method to our work. DivideMix: A SOTA method for learning with noisy labels using semi-supervised learning techniques [32].\nWe implement all baselines using the same base model ar- chitecture and training hyperparameters for a fair comparison.\nD. Evaluation Metrics\nWe evaluate the performance of all methods using the following metrics:\nClassification Accuracy: The percentage of correctly classi- fied samples on the test set. F1-Score: The harmonic mean of precision and recall, providing a balanced measure of performance, especially for imbalanced datasets."}, {"title": "V. ANALYSIS", "content": "In this section, we delve deeper into the workings of TMLC-Net, providing further analysis and insights into its performance and behavior. We focus on visualizing learned representations, examining transferability in more detail, ana- lyzing failure cases, and discussing computational cost.\nA. Visualization of Learned Representations\nTo understand what TMLC-Net learns, we visualize the hidden states of the LSTM in the Time-Series Encoding (TSE) module. We use t-distributed Stochastic Neighbor Embedding (t-SNE) [52] to project the high-dimensional hidden states into a 2D space. We observe that before training TMLC-Net, the hidden states are largely mixed, reflecting the noisy labels. After training, the hidden states corresponding to samples from the same true class tend to cluster together, indicating that TMLC- Net has learned to disentangle the true label information from the noise. B. Transferability Analysis\nIn Section IV-F4, we presented quantitative results demon- strating the transferability of TMLC-Net. Here, we discuss this aspect in more detail. The key to TMLC-Net's transferability lies in the Normalized Noise Perception (NNP) module and the Time-Series Encoding (TSE) module.\nNNP and Distribution Shift: The NNP module normalizes the input features (loss, entropy) relative to the current mini- batch statistics. This makes the input to the TSE module less sensitive to the overall scale of the loss, which can vary significantly across datasets and noise levels. This normaliza- tion helps to mitigate the distribution shift problem that often hinders transfer learning. TSE and Dynamic Adaptation: The TSE module, with its LSTM, learns to model the temporal evolution of the normalized features. This allows TMLC-Net to adapt to different noise patterns and learning dynamics. For example, if the noise is asymmetric, the LSTM can learn to recognize patterns in the loss and entropy that are indicative of mislabeled samples in specific classes.\nWhile TMLC-Net exhibits good transferability, it is not perfect. The performance on the target task may still be lower than training directly on the target task, especially if the source and target tasks are very different. Future work could explore techniques to further improve transferability, such as domain adaptation methods.\nC. Failure Case Analysis\nTo better understand the limitations of TMLC-Net, we examine some cases where it fails to correct noisy labels.\nWe observe that TMLC-Net can struggle with:\nAmbiguous Samples: Images that are inherently difficult to classify, even for humans, can be challenging for TMLC-Net. Highly Atypical Noise: If the noise pattern is very different from what TMLC-Net has seen during training, it may fail to generalize. These failure cases suggest potential areas for future im- provement, such as incorporating more sophisticated noise models during training or developing methods for detecting and handling out-of-distribution samples.\nD. Computational Cost\nTMLC-Net does introduce some computational overhead compared to standard training with cross-entropy. The main additional cost comes from the computation of the normalized features in the NNP module and the forward pass through the LSTM in the TSE module. However, this overhead is relatively small compared to the cost of training the base model itself.\nWe observe that TMLC-Net is slower than standard cross- entropy training but faster than Meta-Weight-Net. The com- putational cost of TMLC-Net is reasonable, and the benefits in terms of improved accuracy and robustness often outweigh the increased training time. Moreover, the transferability of TMLC-Net means that it can be trained once and then applied to multiple datasets, amortizing the training cost."}, {"title": "VI. CONCLUSION", "content": "In this paper, we introduced TMLC-Net, a novel Transfer- able Meta-Learner for Correcting Noisy Labels in deep learn- ing. TMLC-Net addresses the critical limitations of existing meta-label correction methods by learning a general-purpose label correction strategy that can be effectively transferred across diverse datasets and model architectures without requir- ing extensive retraining or fine-tuning. Our approach leverages three key components: Normalized Noise Perception (NNP) to handle distribution shifts, Time-Series Encoding (TSE) to model the temporal evolution of training dynamics, and Sub- class Decoding (SD) to predict corrected label distributions. Extensive experiments on benchmark datasets with various noise types and levels demonstrate that TMLC-Net consis- tently outperforms state-of-the-art methods in terms of both accuracy and robustness to label noise. Furthermore, we showcased the transferability of TMLC-Net, demonstrating its adaptability to new datasets and noise conditions. This estab- lishes its potential as a broadly applicable solution for robust deep learning in noisy environments, significantly reducing the computational burden of meta-label correction, especially on large datasets.\nFuture work could explore several promising directions. First, incorporating more sophisticated noise models during the meta-training phase could further enhance TMLC-Net's ability to handle complex, real-world noise patterns. Second, devel- oping methods for detecting and handling out-of-distribution samples could improve robustness in scenarios with highly atypical noise. Finally, investigating the application of TMLC- Net to other domains beyond image classification, such as natural language processing and speech recognition, could broaden its impact."}]}