{"title": "MMDOCBENCH: BENCHMARKING LARGE VISION-LANGUAGE MODELS FOR FINE-GRAINED VISUAL DOCUMENT UNDERSTANDING", "authors": ["Fengbin Zhu", "Ziyang Liu", "Xiang Yao NG", "Haohui Wu", "Wenjie Wang", "Fuli Feng", "Chao Wang", "Huanbo Luan", "Tat Seng Chua"], "abstract": "Large Vision-Language Models (LVLMs) have achieved remarkable performance in many vision-language tasks, yet their capabilities in fine-grained visual understanding remain insufficiently evaluated. Existing benchmarks either contain limited fine-grained evaluation samples that are mixed with other data, or are confined to object-level assessments in natural images. To holistically assess LVLMs' fine-grained visual understanding capabilities, we propose using document images with multi-granularity and multi-modal information to supplement natural images. In this light, we construct MMDocBench, a benchmark with various OCR-free document understanding tasks for the evaluation of fine-grained visual perception and reasoning abilities. MMDocBench defines 15 main tasks with 4,338 QA pairs and 11,353 supporting regions, covering various document images such as research papers, receipts, financial reports, Wikipedia tables, charts, and infographics. Based on MMDocBench, we conduct extensive experiments using 13 open-source and 3 proprietary advanced LVLMs, assessing their strengths and weaknesses across different tasks and document image types. The benchmark, task instructions, evaluation code, and leaderboard can be found on the official website: https://MMDocBench.github.io/.", "sections": [{"title": "INTRODUCTION", "content": "Large Vision-Language Models (LVLMs) have attained remarkable performance across various vision-language tasks (Yin et al., 2024). However, existing LVLMs, such as GPT-4V (OpenAI, 2023), LLaVA (Liu et al., 2023a) and MiniGPT-4 (Zhu et al., 2023), still struggle with understanding fine-grained visual details in images. For instance, (Tong et al., 2024) have demonstrated that many LVLMs perform poorly in visual grounding to image details for visual question answering. This fine-grained visual understanding capability is indispensable for LVLMs in many downstream tasks (Peng et al., 2024; Xuan et al., 2024), such as object recognition (Lin et al., 2014), image segmentation (Minaee et al., 2022) and forgery detection (Qu et al., 2023). As such, it is essential to develop LVLMs' capability in fine-grained visual understanding.\nTo achieve this goal, a key prerequisite is establishing benchmarks that can comprehensively evaluate the strengths and weaknesses of LVLMs in fine-grained visual understanding. However, representative multimodal benchmarks such as MMVet (Yu et al., 2023), MME (Fu et al., 2024), and MMT-Bench (Ying et al., 2024) contain relatively few data samples to examine LVLMs' understanding of fine-grained details rather than the entire image. Besides, these samples are not isolated from the overall dataset. This makes it difficult to evaluate LVLMs' ability in fine-grained visual understanding comprehensively. Moreover, some benchmarks, such as Visual7W (Zhu et al., 2016),\nRefCOCO (Yu et al., 2016), GVT-bench (Wang et al., 2023a), and MMBench (Liu et al., 2023b), have designed specific tasks like object grounding and object counting to evaluate fine-grained visual understanding. However, these tasks are confined to object-level details in natural images and do not assess finer-grained details.\nTo address the limitations, we consider using document images to supplement natural images for the evaluation of fine-grained visual understanding. As illustrated in Figure 1, document images encapsulate various document content types, including text, figures, tables, charts, and diagrams, all presented within a visual format. These document images like receipts and research papers are widely used across various domains such as finance, legal, education, and academia (Kim et al., 2022). Compared to natural images, document images offer certain advantages as testing data to evaluate LVLMs' fine-grained visual understanding capabilities. In particular, 1) document images contain different granularities of information to evaluate the fine-grained visual perception abilities, such as the localization and recognition of text and tables. The diverse elements (e.g., text, table, and chart) tend to occupy only a small part of the entire image, yet convey critical information in various granularities; for example, a receipt image may comprise item description (sentence level), quantity (token level), and barcode (object level). Moreover, 2) document images require LVLMs to integrate multi-granularity and multi-type information to perform complex reasoning, thereby evaluating their fine-grained visual reasoning abilities. For example, in the bar chart located in the bottom left of Figure 1, LVLMs need to combine the legend, axis labels, and numerical values on the bar chart for comprehensive reasoning.\nIn light of this, we construct a benchmark, MMDocBench, using document images to assess LVLMs' capabilities in fine-grained visual document understanding. We design various OCR-free document understanding tasks from the perspectives of fine-grained visual perception and fine-grained visual reasoning, where understanding partial and fine-grained details in images is crucial, rather than treating the image as a whole. For fine-grained visual perception, MMDocBench encompasses nine tasks to evaluate the LVLMs' capabilities in fine-grained information recognition, localization, detection, and extraction, including Text Recognition, Table Recognition, Text Localization, Table Cell Localization, Key Information Extraction, Document Forgery Detection, Document Question Answering (QA), Chart QA, and Infographic QA. For fine-grained visual reasoning, we design six tasks to assess the reasoning ability by integrating fine-grained information: Arithmetic Reasoning, Logical Reasoning, Spatial Reasoning, Counting, Comparison, and Sorting. We present one example for each task in Figure 1 and refer to Section A.4 in Appendix for more comprehensive examples.\nBased on these tasks, we select document images from 21 document understanding datasets to construct QA pairs for evaluation. The document images span a wide variety of types, including research papers, book covers, financial reports, scene-text images, receipts, Wikipedia tables, charts, infographics, and other industry documents. Additionally, in MMDocBench, we also provide annotations of supporting regions (i.e., bounding boxes) within the image for each QA pair, as shown by red dotted rectangles in Figure 1. The supporting regions enable the evaluation of whether LVLMs"}, {"title": "RELATED WORK", "content": "Large Vision Language Models (LVLMs) are built upon Large Language Models (LLMs) like GPTS (Radford et al., 2019; Brown et al., 2020) and LLaMA (Touvron et al., 2023). A common approach is to utilize a visual encoder to encode the image first and then apply a visual adapter to align visual and textual representations within the LLMs. LVLMs are typically trained in two stages, i.e. self-supervised pre-training over large-scale image-text pairs and supervised instruction tuning with annotated data. Notable examples include Flamingo (Alayrac et al., 2022), BLIP-2 (Li et al., 2023a), LLaVA (Liu et al., 2023a), MiniGPT-4 (Zhu et al., 2023), mPLUG-Owl (Ye et al., 2023c), CogVLM (Wang et al., 2023b), MiniCPM-V (Yao et al., 2024), Monkey (Li et al., 2024b), and InternVL (Chen et al., 2024d). For instance, BLIP-2 (Li et al., 2023a) utilizes frozen CLIP (Radford et al., 2021) as its visual encoder and proposes a Q-Former as the visual adapter. LLaVA (Li et al., 2024a) and MiniGPT-4 (Zhu et al., 2023) replace the visual adapter with simple linear layers, demonstrating impressive effectiveness.\nRecently, increasing efforts have been devoted to enhancing LVLMs' performance in document image understanding (Ye et al., 2023b; Liu et al., 2024b; Ye et al., 2023a; Liu et al., 2024c; Bai et al., 2023; Zhang et al., 2023), generally by accommodating high-resolution input images or/and improving quality of training corpus. For instance, LLaVA-NeXT (Liu et al., 2024b) extends LLaVa (Liu et al., 2023a) to enhance its OCR capability by increasing image resolution and mixing more document images in visual instruction tuning data; TextMonkey (Liu et al., 2024c) and MiniCPM-V (Yao et al., 2024) divide high-resolution images into window patches with a sliding window method and reduce the length of visual tokens via token compression techniques. Compared with natural images, the understanding of document images requires LVLMs' interpretation of fine-grained image details. In this work we propose a comprehensive benchmark to evaluate such fine-grained visual understanding capability of LVLMs to facilitate future research."}, {"title": "PROPOSED MMDOCBENCH", "content": "On MMDocBench, the LVLMs are expected to output the precise answer to a natural language query given a document image while highlighting the supporting regions within the image contributing to inferring the answer. Formally, given a document image d possibly containing text, table, chart,"}, {"title": "PROBLEM DEFINITION", "content": "and/or figure, for a question q, a Large Vision-Language Model M is required to produce a response including the answer a and the corresponding regions R as supporting evidence, formulated as\n$M(d, q) = (a, R)$.\nEach region in R is a bounding box that is represented by the coordinates of its top-left and bottom-right corners in the format of $[X_1, Y_1, x_2, Y_2]$."}, {"title": "CONSTRUCTION PIPELINE", "content": "The pipeline for constructing our MMDocBench is illustrated in Figure 2.\nTargeting at a comprehensive evaluation of LVLMs' fine-grained visual understanding capabilities, we design the taxonomy of MMDocBench following two principles."}, {"title": "Taxonomy Design", "content": "The MMDocBench must provide tasks that can adeptly evaluate LVLMs' visual comprehension capabilities with sufficient discriminability of fine-grained details in the image, rather than treating the image as a whole.\nThe MMDocBench must encompass a broad range of tasks in terms of required capabilities (e.g., perception, reasoning), content granularity (e.g., characters, words, tables), and document types (e.g., scientific papers, financial reports, receipts).\nTo solve the problem in MMDocBench, two capabilities are at the core for LVLMs, i.e. fine-grained visual perception and fine-grained visual reasoning (Liu et al., 2023b). To investigate both capabilities of LVLMs, we design a total of 15 tasks in MMDocBench. Specifically, we encompass nine tasks for fine-grained visual perception, including Text Recognition, Table Recognition, Text Localization, Table Cell Localization, Key Information Extraction, Document Forgery Detection, Document Question Answering, Chart Question Answering, and Infographic Question Answering; and for fine-grained visual reasoning, we include six tasks: Arithmetic Reasoning, Logical Reasoning, Spatial Reasoning, Comparison, Sorting and Counting. Further, we include one or multiple sub-tasks for each task to cover more diverse document image types, e.g. research papers, book covers, financial reports, scene-text images, receipts, Wikipedia tables, charts, infographics, and other industry documents, leading to a total of 48 sub-tasks in MMDocBench."}, {"title": "Fine-grained Discrimination", "content": ""}, {"title": "Diversity", "content": ""}, {"title": "Document Image & QA Pair Preparation", "content": "As shown in Table 1, we create BookOCR, Bbox2Text, and Text2Bbox by ourselves and use original task settings for other sub-tasks. In particular, we build BookOCR, a text recognition dataset, based on selected document images (i.e., book covers) from OCR-VQA (Mishra et al., 2019). After collecting document images, we use a pre-defined template for text recognition as the question and automatically identify all the OCR content from the image as the ground-truth answer. We build Text2Bbox and Bbox2Text with the same document images, which are selected from DocILE (\u0160imsa et al., 2023), RVL-CDIP (Harley et al., 2015), DocBank (Li et al., 2020), PubLayNet (Zhong et al., 2019) and PubTabNet (Zhong et al., 2020) to cover a great diversity of document types. The former task requires an LVLM to find the region in the document image given a piece of textual content, while the latter needs the model to identify corresponding text in the document image based on a specified bounding box. For the three sub-tasks, our annotators (6 undergraduate or graduate students majored in computer science) manually create one QA pair for each selected document image.\nFor other sub-tasks, our annotators manually analyze and select appropriate document images with annotated input-output pairs from the source dataset. After that, the input-output pairs are transformed into QA pairs following the pre-defined templates. Note that all the document images and QA pairs are selected from the test set of the source dataset except CORD (Park et al., 2019), DUDE (Van Landeghem et al., 2023) and CharXiv (Wang et al., 2024b). CORD has an insufficient number of high-quality document images in its test set, so we select some from its validation set. DUDE and CharXiv have not yet released their test sets, and thus we utilize their validation sets instead. For each sub-task, we at most select 100 document images.\nIt is worth mentioning that for preparing sub-tasks of fine-grained visual reasoning, we purposely select five existing datasets to ensure our MMDocBench includes a great diversity of document image types. These datasets include DUDE (Van Landeghem et al., 2023) containing general documents from various industries, WTQ (Pasupat & Liang, 2015) containing table-based documents"}, {"title": "Region Generation", "content": "We generate ground truth regions for each QA pair in MMDocBench to facilitate evaluation. Similar to (Xuan et al., 2024; Chen et al., 2024b), we normalize the coordinates used to represent the bounding box to the range [0, 1000] w.r.t. the image dimensions.\nFor the tasks regarding fine-grained visual perception, we set the answer's location on the image as the region to be annotated, while for those regarding fine-grained visual reasoning, we annotate the locations of all supporting evidences used to infer the final answer. Specifically, we first obtain the OCR results using Google OCR service for each document image in MMDocBench. For fine-grained visual perception tasks, we automatically identify the corresponding value and its bounding box in the OCR result based on the answer. If only one value matches, we use the region of this value as the correct one; otherwise, our annotators manually review and select the appropriate region for the answer. For fine-grained visual reasoning tasks, we search for the regions for each supporting evidence if the source dataset already provides the annotation of supporting evidence,"}, {"title": "QA & Region Verification", "content": "To ensure high quality of MMDocBench, we further verify the collected data. First, we develop a program to automatically highlight the answer or supporting evidence with its corresponding regions on the document image. Then, different annotators review the rendered document image in three rounds to ensure that the answer and supporting regions to the question are correct."}, {"title": "STATISTICS AND ANALYSIS", "content": "With the above construction pipeline, the resultant MMDocBench contains a total of 2, 400 document images and 4,338 QA pairs with 11,353 annotated supporting regions. On average, each question has 16.1 words and around 2.61 supporting regions. We analyze the position distribution and area distribution of the annotated supporting regions for each QA pair in our MMDocBench. To compute the position distribution, given a region $[X_1,Y_1,X_2,Y_2]$, we first calculate the center point by $(x1+x2, Y1+Y2)$. Then, we plot all the center points on an image with a dimension of 1000 \u00d7 1000. As shown in Figure 3, all points are scattered across the entire image, indicating no clear positional bias for supporting regions in MM-DocBench. We also analyze the area distribution of all regions to examine their granularity. We first calculate the area of each region and then apply a logarithmic transformation with a base of 10 on the computed area value. As shown in Figure 4, the granularity of regions shows a diversity and the majority of the region areas fall between 1, 000 and 10, 000, corresponding to regions sized between 10 \u00d7 100 and 100 \u00d7 100. These analyses well highlight the high quality of our MMDocBench, which is crucial for accurately assessing the capabilities of LVLMs in fine-grained visual understanding."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "EXPERIMENTAL SETUP", "content": "We conduct evaluation experiments with 13 open-source LVLMs and 3 proprietary LVLMs on the proposed MMDocBench. We select those open-source models with strong document understanding capabilities, including InternVL2-Llama3-76B (Chen et al., 2024c), Llava-OV-Chat-72B (Li et al., 2024a),mPLUG-DocOwl-1.5-Omni (Hu et al., 2024), LLaVA-V1.6-34B (Liu et al., 2024a), YI-VL-34B AI et al. (2024), CogVLM2-Chat-19B Hong et al. (2024), Qwen2-VL-7B-Instruct (Wang et al., 2024a), TextMonkey (Liu et al., 2024c), MiniCPM-V2.6 (Yao et al., 2024), MiniCPM-Llama3-V2.5 (Yao et al., 2024), InternVL2-8B (Chen et al., 2024c), mPLUG-Owl3 (Ye et al., 2024), and Ferret (You et al., 2024). For proprietary LVLMs to be evaluated in our experiments, we use Qwen-VL-Max-0809 (Bai et al., 2023) and two versions of the latest OpenAI"}, {"title": "Evaluated LVLMs", "content": ""}, {"title": "Instruction Design", "content": "For each main task in MMDocBench, we manually design an instruction template to guide the LVLM to output the answer and supporting regions in JSON format, e.g., {\u201canswer\":\"{answer}\u201d, \u201cbbox\u201d:[\u201c{bbox1}\u201d,\u201c{bbox2}"}, {"title": "Evaluation Metrics", "content": "For each question, we use Exact Match (EM) and F1-score to evaluate the predicted answer, and Intersection over Union (IOU) to assess the predicted region(s). The EM is determined by matching every character of the model's text prediction to the ground truth. If all characters are matched, the EM is 1, and otherwise 0. For F1-score, we calculate the word-level F1 based on the number of words in model prediction, ground truth, and their intersection (Rajpurkar et al., 2018). The Intersection over Union (IOU) is computed between the predicted region and the ground-truth region, taking into account their overlapping area and union area. The scores on the three metrics for each sub-task are computed by taking the mean of corresponding metric scores for all the questions included in this sub-task. To obtain metric scores per task or capability, and overall performance, we calculate the macro average across all corresponding lower-level metric scores."}, {"title": "MAIN RESULTS", "content": "We conduct a comprehensive comparison of different LVLMs with our proposed MMDocBench, and show the results in Table 2. We make below key findings. 1) The proposed MMDocBench poses significant challenges to current LVLMs in terms of both answer prediction and region prediction. The best model GPT-40 achieves 66.40% in EM for answer prediction, but only 2.44% in IOU for region prediction. Another close-source model, Qwen-VL-Max, which has comparable answer prediction performance to GPT-40, is the best-performing model for region prediction, with an IOU score of 11.44% only. This highlights substantial challenges of region prediction in MMDocBench. 2) There is a large gap in answer prediction performance between open-source and close-source models, while that in their region prediction performance is minor. For answer prediction, GPT-40 significantly outperforms the best open-source model Llava-OV-Chat-72b (with 58.93% in EM) by over 12% in EM. For region prediction, open-source models like TextMonkey and Qwen2-VL-7B-Instruct, with IOU scores of 8.93% and 9.87% respectively, perform slightly worse than Qwen-VL-Max but outperform GPT-40 by around three times. This could be explained by OpenAI using insufficient samples with visual grounding requirements when training GPT-40 and GPT-4v, leading to a lack of visual grounding capability. Another possible reason is that visual encoders adopted in GPT-40 and GPT-4v do not effectively support fine-grained visual understanding, which we leave for future investigation. 3) LVLMs trained on document images with text-grounding requirements, such as Qwen-VL-Max, Qwen2-VL-7B-Instruct, and TextMonkey, show improvements in region prediction, while those trained with object-level grounding over natural images, like Ferret, exhibit no such improvements. This highlights the necessity of establishing a benchmark that supports visual grounding at various and finer granularities on document images, such as our MMDocBench.\nIn the following, we further analyze model performance on answer prediction and region prediction regarding fine-grained visual perception and fine-grained visual reasoning, respectively."}, {"title": "Answer Prediction", "content": "We make below key findings. 1) GPT-40 consistently beats all other models on both fine-grained visual perception and fine-grained visual reasoning tasks in terms of EM, demonstrating its superior effectiveness. 2) Larger models, like GPT-40, Qwen-VL-Max, InternVL2-Llama3-76B, and Llava-OV-Chat-72b tend to perform better on fine-grained visual reasoning tasks than fine-grained visual perception tasks, while smaller models, conversely, excel in fine-grained visual perception tasks over reasoning tasks. This might be because reasoning capabilities improve significantly as the model size increases."}, {"title": "Region Prediction", "content": "We make below key findings. 1) TextMonkey achieves the best results on fine-grained visual perception tasks with 19.22% in IOU, but it fails to output supporting regions for fine-grained visual reasoning tasks. One possible reason is that TextMonkey only involves perception-related samples and instructions for training its grounding ability, resulting in its inability to ground the supporting evidence for reasoning tasks. 2) All LVLMs face significant challenges in predicting supporting regions for fine-grained visual reasoning tasks. The performance of most LVLMs on fine-grained visual reasoning tasks is notably worse than that on fine-grained visual perception tasks. Qwen-VL-Max, which is the best model for region prediction on fine-grained visual reasoning tasks, earns only 4.27% in IOU, indicating the remarkable challenges of this task."}, {"title": "ANALYSIS ON DIFFERENT TASKS", "content": "We compare the performance of various LVLMs across tasks. We make below key findings. 1) As shown in Figure 5, GPT-40 and Qwen-VL-Max are the best and second-best models across most tasks among all LVLMs, except for Logical Reasoning and Spatial Reasoning, where LLaVA-OV-Chat-72B outperforms all other models. 2) As shown in Figure 6, all LVLMs struggle with the prediction of supporting regions on almost all the tasks, except for Text Recognition, where the best model, Qwen-VL-Max, achieves around 71.2% in IOU. Qwen-VL-Max delivers the best results in region prediction across most tasks, except for Forgery Detection, Infographic QA, and Text Localization, where TextMonkey ranks the first. 3) Among all the tasks, Document Forgery Detection, a fine-grained visual perception task, is the most challenging for all LVLMs, with the best result being only around 20.6% in EM for answer prediction and 1.8% in IOU for region prediction; As illustrated in Figure 1, this task requires the model to identify the inconsistent word(s) against other words within the image. In addition, the answer prediction performance of all LVLMs on Text Localization and Table Cell Localization is poor, underscoring the challenges posed by both tasks."}, {"title": "ANALYSIS ON DIFFERENT DOCUMENT TYPES", "content": "Based on the majority of content included, we categorize all the document images in MM-DocBench into five types: General Document, Table-Based Document, Table-Text Document, Chart-Based Document, and Infographic-Based Document. 1) For answer prediction, GPT-4o obtains the best performance on General Document and Table-Based Document, while Qwen-VL-Max ranks the first across the other three document types. 2) For region prediction, TextMonkey outperforms all other models on Infographic-Based Document, while Qwen-VL-Max leads on the remaining four document types. 3) In comparison, region prediction on other document types is significantly more challenging than on General Documents, while there is no notable difference in answer prediction across different document types."}, {"title": "CONCLUSION", "content": "In this work, we introduce the MMDocBench benchmark to comprehensively evaluate LVLMs' fine-grained visual perception and reasoning capabilities via various OCR-free document understanding tasks. In MMDocBench, we carefully design 15 tasks and 48 sub-tasks that require LVLMs to perform deep, fine-grained interpretation of image details to answer each question. To enable a more comprehensive evaluation, we provide annotations of the supporting regions for each question-answer pair to assess whether LVLMs have the abilities to correctly ground their predictions on the associated regions in the image. With MMDocBench, we evaluate various open-source and proprietary LVLMs, analyzing their performance in fine-grained visual document image understanding. We observe that our MMDocBench presents significant challenges to current LVLMs in both answer and region prediction, with GPT-4o achieving the highest answer prediction score of 66.40% in EM and Qwen-VL-Max achieving the best region prediction score of 11.44% in IOU. Moreover, we find that open-source LVLMs demonstrate competitive performance in region prediction compared to proprietary models, despite a significant gap in answer prediction. We believe MMDocBench can enable a thorough and multi-faceted evaluation of fine-grained visual document understanding of LVLMs, thereby facilitating LVLMs' future advancement."}, {"title": "APPENDI", "content": ""}, {"title": "KEY STATISTICS", "content": ""}, {"title": "DOCUMENT CATEGORY", "content": "We follow the strategy in Table 9 to divide the document images in each sub-task into five categories, i.e., General Document, Table-Based Document, Table-Text Document, Chart-Based Document and Infographic-Based Document."}]}