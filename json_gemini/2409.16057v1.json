{"title": "Towards Robust Object Detection: Identifying and Removing Backdoors via Module Inconsistency Analysis", "authors": ["Xianda Zhang", "Siyuan Liang"], "abstract": "Object detection models have been widely adopted in various security-critical applications, such as autonomous driving and video surveillance. However, the complex architectures of these models also make them vulnerable to backdoor attacks, where maliciously trained models behave normally on clean inputs but produce targeted misclassifications when triggered by specific patterns. Existing backdoor defense techniques, primarily designed for simpler models like image classifiers, often fail to effectively detect and remove backdoors in object detectors while preserving model performance. In this work, we propose a novel backdoor defense framework tailored to the unique characteristics of object detection models [40]. Our key observation is that a backdoor attack often causes significant inconsistencies between the behaviors of local modules, such as the Region Proposal Network (RPN) and the classification head. By quantifying and analyzing these inconsistencies, we develop an effective algorithm to detect the presence of backdoors. Furthermore, we find that the inconsistent module is usually the main source of the backdoor behavior. Exploiting this insight, we propose a simple yet effective backdoor removal method, which localizes the affected module, resets its parameters, and fine-tunes the model on a small set of clean data. Extensive experiments with multiple state-of-the-art object detectors demonstrate that our method can successfully detect and remove backdoors, achieving an improvement of 90% in the backdoor removal rate over the fine-tuning baseline while limiting the accuracy loss on clean data to less than 4%. To the best of our knowledge, this work represents the first effort to develop a dedicated backdoor defense framework for object detection models, addressing the unique challenges and limitations of existing techniques in this context. Our work sheds new light on the unique challenges and opportunities in defending object detection models against backdoor attacks.", "sections": [{"title": "1 INTRODUCTION", "content": "Object detection plays a pivotal role in a wide range of multimedia applications, such as video surveillance [52], autonomous driving [31], and face recognition [35]. However, the increasing deployment of deep learning-based object detectors in real-world scenarios has raised growing concerns about their security vulnerabilities, especially the emerging threats of backdoor attacks. Backdoor attacks refer to the malicious manipulation of a model's behavior by injecting hidden triggers during the training process, which can cause severe consequences like unauthorized access and privacy leakage [6, 7, 12, 20], thereby jeopardizing the trustworthiness and reliability of multimedia systems heavily relying on object detection.\nCompared to image classification models, object detection models pose unique challenges for backdoor defense. Modern object detectors, such as Faster R-CNN, typically adopt a complex architecture with multiple stages or subnets to simultaneously localize and classify objects. This structural complexity provides attackers with ample opportunities to inject backdoors in a more stealthy and targeted manner, making the detection and removal of such backdoors highly difficult.\nDespite the severity of backdoor threats in object detection models, existing research efforts primarily focus on developing novel attack strategies [24, 47], [49], while the defense aspect remains largely underexplored. To the best of our knowledge, only a few recent works [5], [55] have made initial attempts to detect backdoors in object detectors, leveraging techniques like activation clustering and gradient analysis. However, these methods require a large number of clean samples or rely on certain assumptions about the backdoor patterns, which may not always hold in practice. Moreover, effective techniques for backdoor removal in object detection models are still missing in the literature, leaving a significant gap in the defense pipeline.\nIn this paper, we propose a novel backdoor defense framework tailored to the unique characteristics of object detection models. Our key observation is that backdoor attacks often induce significant inconsistencies between the behaviors of different components in the detection model, especially between the region proposal network (RPN) and the region classification network (R-CNN). Specifically, a backdoor model tends to generate highly conflicting predictions between these two modules when triggered, such as proposals that are correctly identified by RPN but misclassified by R-CNN. Exploiting this anomalous behavior, we develop an effective algorithm to detect the presence of backdoors by measuring and analyzing the prediction inconsistency between RPN and R-CNN.\nFurthermore, we find that the module exhibiting the strongest inconsistency, which we call the \"dominant module\", is usually the main target of the backdoor injection. This insight motivates us to devise a novel backdoor removal strategy. Instead of modifying the entire model, we localize the backdoor removal to the dominant module, reinitializing its parameters, and fine-tuning the whole model on a small set of clean data. This targeted approach not only effectively eliminates the backdoor behavior, but also minimizes the negative impact on the model's performance on clean data. Through extensive experiments on multiple state-of-the-art object detectors, we demonstrate the effectiveness and generality of our method in both backdoor detection and removal.\nWe evaluate the effectiveness of our proposed framework on four widely used object detection models, namely Faster R-CNN [53], Faster R-CNN FPN [43], Mask R-CNN [34], and Double-Head R-CNN [58]. Experimental results demonstrate that our detection method can successfully identify backdoors in all of these models with high accuracy, highlighting its generality and robustness. We compare our backdoor removal approach with baseline methods. On the poisoned dataset, our method significantly outperforms the baselines by around 90% in terms of the backdoor elimination rate, indicating its superior effectiveness in removing hidden backdoor triggers. Meanwhile, on the clean dataset, our method maintains the model performance with only a slight accuracy drop of less than 4%, which is much lower than the degradation incurred by the baselines. This suggests that our approach can effectively remove backdoors without compromising the model's normal functionality.\nIn summary, the main contributions of this work are threefold:\n\u2022 To the best of our knowledge, this is the first study that explores the backdoor removal problem in the context of object detection models. Our work aims to bridge this important gap and contribute to the development of more secure and robust object detectors against backdoor attacks.\n\u2022 We unveil the underlying reasons for the vulnerability of object detection models to backdoor attacks and leverage the resulting anomalous behavior to devise an effective backdoor detection method. Taking advantage of the inconsistency between the RPN and R-CNN modules, our approach can accurately identify the presence of backdoors without requiring a large number of samples or complex training procedures.\n\u2022 We propose a novel backdoor removal technique that combines localized initialization and global fine-tuning. This approach not only successfully mitigates the backdoor effects, but also preserves the model's performance on clean inputs. Extensive experiments on four state-of-the-art object detectors demonstrate the effectiveness and generalizability of our method. Compared to the baseline method, our approach achieves an improvement of 90% in terms of backdoor removal rate while only incurring a minimal accuracy drop of less than 4%."}, {"title": "2 RELATED WORK", "content": "The seminal work on backdoor attacks [15, 19, 23, 25, 41] in image classification is BadNets [32], which injects a backdoor into a model by poisoning a portion of the training data with a specific trigger pattern (e.g., a black square) and the target label. The model trained on this poisoned dataset will behave normally on clean inputs, but misclassify any input containing the trigger to the target label. The proposal of BadNets has sparked extensive research on backdoor attacks in the academic community, leading to a diverse array of attack techniques. Blended [4] replaces the prominent black square trigger with a more stealthy transparent cartoon sticker. SSBA [36] takes inspiration from the classical image steganography [3] technique and embeds the trigger into the image using steganographic methods, making the trigger imperceptible to human eyes. AdvDoor [60], on the other hand, borrows the idea from adversarial attacks [8\u201311, 13, 14, 18, 21, 22, 26, 28, 30] and uses adversarial perturbations as a backdoor trigger. FaceHack [54] targets face recognition systems by using facial paintings as triggers. Wanet [51] employs subtle warping of the edge of the object as a trigger, while low frequency [59] inserts the trigger in the frequency domain.\nAs the research on backdoor attacks against image classification models becomes more mature [56], researchers have started to turn their attention to the vulnerability of object detection models. Chan [2] are the first to propose four types of backdoor attacks specifically designed for object detection models, this work reveals that object detection models are equally threatened by backdoor attacks. Luo [46] further investigates the object disappearance attack. They conduct a detailed study on this specific attack and demonstrate that even using the simplest attack method, some basic defense [16, 17, 27] techniques such as fine-tuning and fine-pruning are still ineffective against this type of attack. This work highlights the severity of the backdoor threat in object detection models and the inadequacy of existing defense methods. Taking into account real-world scenarios, Ma [48] proposes a clean label backdoor attack method called TransCAB, which uses natural triggers. TransCAB employs a Transformer to model the relationship between object instances and object appearances in natural images, generating realistic poisoned data containing triggers to compromise the model."}, {"title": "2.1 Backdoor Attacks", "content": "The seminal work on backdoor attacks [15, 19, 23, 25, 41] in image classification is BadNets [32], which injects a backdoor into a model by poisoning a portion of the training data with a specific trigger pattern (e.g., a black square) and the target label. The model trained on this poisoned dataset will behave normally on clean inputs, but misclassify any input containing the trigger to the target label. The proposal of BadNets has sparked extensive research on backdoor attacks in the academic community, leading to a diverse array of attack techniques. Blended [4] replaces the prominent black square trigger with a more stealthy transparent cartoon sticker. SSBA [36] takes inspiration from the classical image steganography [3] technique and embeds the trigger into the image using steganographic methods, making the trigger imperceptible to human eyes. AdvDoor [60], on the other hand, borrows the idea from adversarial attacks [8-11, 13, 14, 18, 21, 22, 26, 28, 30] and uses adversarial perturbations as a backdoor trigger. FaceHack [54] targets face recognition systems by using facial paintings as triggers. Wanet [51] employs subtle warping of the edge of the object as a trigger, while low frequency [59] inserts the trigger in the frequency domain.\nAs the research on backdoor attacks against image classification models becomes more mature [56], researchers have started to turn their attention to the vulnerability of object detection models. Chan [2] are the first to propose four types of backdoor attacks specifically designed for object detection models, this work reveals that object detection models are equally threatened by backdoor attacks. Luo [46] further investigates the object disappearance attack. They conduct a detailed study on this specific attack and demonstrate that even using the simplest attack method, some basic defense [16, 17, 27] techniques such as fine-tuning and fine-pruning are still ineffective against this type of attack. This work highlights the severity of the backdoor threat in object detection models and the inadequacy of existing defense methods. Taking into account real-world scenarios, Ma [48] proposes a clean label backdoor attack method called TransCAB, which uses natural triggers. TransCAB employs a Transformer to model the relationship between object instances and object appearances in natural images, generating realistic poisoned data containing triggers to compromise the model."}, {"title": "2.2 Backdoor Defenses", "content": "In the image classification domain, a variety of backdoor defense techniques [29, 39] have been proposed, showing promising results in detecting and mitigating backdoor attacks. Fine-Pruning (FP) [45] is one of the early methods that combines network pruning and fine-tuning to remove backdoor-related neurons while preserving the model's accuracy on clean data. Later, Neural Attention Distillation (NAD) [38] proposes to first fine-tune a clean teacher model using clean samples and then transfer the attention maps of the teacher model to the backdoored student model to erase the backdoor effect. Building upon NAD, FTT [61] further improves the model's accuracy recovery and redesigns the defense against advanced attacks. However, it is worth noting that most of these defense techniques are primarily evaluated on traditional image classification models. Their applicability and effectiveness on object detection models, which have significantly different architectures and learning objectives, remain to be further investigated."}, {"title": "3 THE PROPOSED METHOD", "content": "In this paper, we focus on the poison-only attacks against deep learning models, which aim to implant hidden malicious behaviors into the model during the training process by poisoning a portion of the training data. We assume that the attacker has the ability to manipulate a subset of the training data but has no access to or control over other components of the training process, such as the model architecture, objective function, or hyperparameters. This assumption is realistic in many practical scenarios where the integrity of the training data cannot be fully guaranteed, such as when data are collected from untrusted sources or when the training process is outsourced to third-party platforms.\nBackdoor attacks on object detection models can be categorized based on their intended consequences, such as false positive attacks that aim to induce the model to detect non-existent objects and false negative attacks that aim to suppress the detection of specific objects. In this paper, we focus on the false negative attack, also known as the \"object disappearance attack\", which is particularly dangerous in safety-critical scenarios like autonomous driving and video surveillance. By carefully designing the trigger pattern and the poisoning strategy, the attacker can manipulate the model to miss the detection of specific objects, leading to potentially catastrophic consequences.\nLet $D = \\{(x_i, y_i)\\}_{i=1}^{N}$ denote the clean dataset, where $x_i \\in \\mathbb{R}^{H \\times W \\times C}$ is an input image and $y_i = \\{(c_{ij}, b_{ij})\\}_{j=1}^{m_i}$ is the corresponding annotation, with $c_{ij} \\in \\{1, ..., K\\}$ being the class label and $b_{ij} = (x_{ij}, y_{ij}, w_{ij}, h_{ij})$ being the bounding box coordinates of the $j$-th object in $x_i$. Let $f_\\theta(\\cdot)$ denote the object detection model parameterized by $\\theta$, which takes an image $x$ as input and outputs a set of detected objects $\\hat{y} = \\{(\\hat{c}_j, \\hat{b}_j, \\hat{s}_j)\\}_{j=1}^{l}$, where $\\hat{c}_j, \\hat{b}_j,$ and $\\hat{s}_j$ are the predicted class, the bounding box and the confidence score of the $j$-th detected object, respectively."}, {"title": "3.1 Threat Model", "content": "In this paper, we focus on the poison-only attacks against deep learning models, which aim to implant hidden malicious behaviors into the model during the training process by poisoning a portion of the training data. We assume that the attacker has the ability to manipulate a subset of the training data but has no access to or control over other components of the training process, such as the model architecture, objective function, or hyperparameters. This assumption is realistic in many practical scenarios where the integrity of the training data cannot be fully guaranteed, such as when data are collected from untrusted sources or when the training process is outsourced to third-party platforms.\nBackdoor attacks on object detection models can be categorized based on their intended consequences, such as false positive attacks that aim to induce the model to detect non-existent objects and false negative attacks that aim to suppress the detection of specific objects. In this paper, we focus on the false negative attack, also known as the \"object disappearance attack\", which is particularly dangerous in safety-critical scenarios like autonomous driving and video surveillance. By carefully designing the trigger pattern and the poisoning strategy, the attacker can manipulate the model to miss the detection of specific objects, leading to potentially catastrophic consequences.\nLet $D = \\{(x_i, y_i)\\}_{i=1}^{N}$ denote the clean dataset, where $x_i \\in \\mathbb{R}^{H \\times W \\times C}$ is an input image and $y_i = \\{(c_{ij}, b_{ij})\\}_{j=1}^{m_i}$ is the corresponding annotation, with $c_{ij} \\in \\{1, ..., K\\}$ being the class label and $b_{ij} = (x_{ij}, y_{ij}, w_{ij}, h_{ij})$ being the bounding box coordinates of the $j$-th object in $x_i$. Let $f_\\theta(\\cdot)$ denote the object detection model parameterized by $\\theta$, which takes an image $x$ as input and outputs a set of detected objects $\\hat{y} = \\{(\\hat{c}_j, \\hat{b}_j, \\hat{s}_j)\\}_{j=1}^{l}$, where $\\hat{c}_j, \\hat{b}_j,$ and $\\hat{s}_j$ are the predicted class, the bounding box and the confidence score of the $j$-th detected object, respectively."}, {"title": "3.2 Defense Scenario", "content": "In real-world applications, it is common for users to deploy pre-trained object detection models obtained from third-party sources, such as model repositories or commercial providers. However, the integrity and security of these models cannot always be guaranteed, as they may have been trained on data from untrusted sources or manipulated by malicious parties. This raises significant concerns about the potential presence of backdoors in these models.\nWe consider a practical defense scenario where the defender has access to a pre-trained object detection model $f\\hat{\\theta}(\\cdot)$, but is uncertain whether the model has been backdoored or not. The defender's goal is to ensure the safety and reliability of the model before deploying it in safety-critical applications."}, {"title": "3.3 Key Intuition", "content": "The difficulty of backdoor defense in object detection models stems from their complex architectures, which typically consist of multiple interconnected components, such as the backbone network, the region proposal network (RPN), and the region-based convolutional neural network (R-CNN). This complexity provides attackers with ample opportunities to inject backdoors in a stealthy manner, while making it challenging for defenders to identify and remove them without compromising the model's performance on clean data. Existing backdoor defense methods, which are primarily designed for simpler models like image classifiers, often struggle to cope with the intricacies of object detectors, leading to suboptimal trade-offs between backdoor removal and model utility. This raises a critical question: how can we develop effective and efficient backdoor defense techniques that are specifically tailored to the unique characteristics of object detection models.\nTo answer this question, we first investigate the general characteristics of backdoor attacks. A common strategy employed by attackers is to create \"shortcuts\" or \"overfitted\" patterns in the model [33], which can strongly activate the backdoor and dominate the model's prediction when the trigger is present. From the perspective of optimization theory, these shortcuts essentially introduce biases into the gradient dynamics during training, causing the loss function to rapidly decrease along certain directions that favor the backdoor. This abnormal optimization behavior allows the backdoor to be rapidly \"memorized\" by the model [37], while keeping its impact on clean data minimal. However, in complex object detection models, such shortcuts can be easily concealed within any of the model components, making them difficult to detect and remove."}, {"title": "3.4 Cross-Module Inconsistency Detection", "content": "Based on the inconsistency between RPN and R-CNN, the backdoor detection algorithm consists of the following key steps:\n1. Inconsistency Score Calculation: For each trigger sample x in the trigger sample set Dtrigger, we first extract its RPN output {ri} i=1N and R-CNN output (pi, ti) i=1N, where N is the number of proposals. Each ri represents the RPN's classification score for the i-th proposal, while pi and ti denote the R-CNN's classification score and bounding box for the same proposal, respectively. Then, for each proposal, we compute the difference between its RPN classification score and R-CNN classification score as the inconsistency score si:\nsi = ||ri - pi||1\nIntuitively, if the model is not backdoored, the RPN and R-CNN should give consistent predictions for the same proposal, leading to a small si. However, if the model is injected with a backdoor, the trigger may cause the RPN and R-CNN to behave inconsistently, resulting in a large si.\n2. Negligible Difference Threshold Setting: In practice, the inconsistency scores si may be affected by various factors other than backdoors, such as the inherent discrepancy between the RPN and R-CNN, the quality of proposals, etc. To filter out these negligible differences, we introduce a negligible difference threshold \u03f5. Only those scores si that are greater than \u03f5 are considered as significant inconsistencies and are collected into the set S for further analysis. The choice of \u03f5 depends on the specific data distribution and can be adjusted based on validation data.\n3. Arithmetic Mean Calculation: After obtaining the set of significant inconsistency scores S, we compute their arithmetic mean \u00b5 as:\n\u03bc = 1/|S| * \u2211 s\u2208S s\nThe arithmetic mean \u03bc serves as an overall measure of the level of inconsistency between RPN and R-CNN. A high \u03bc indicates that the model's behavior is highly inconsistent, which is a strong signal of the presence of backdoors.\n4. Backdoor Judgment Threshold Selection: Finally, we compare the arithmetic mean \u03bc with a predefined backdoor judgment threshold \u03b8. If \u03bc is greater than \u03b8, we consider the model to be possibly attacked by a backdoor; otherwise, we consider the model to be normal. The selection of \ud835\udf03 is based on the desired trade-off between detection accuracy and false alarm rate, and can be tuned using validation data."}, {"title": "3.5 Targeted Renewal Fine-tuning", "content": "Exploiting the insight that the inconsistent module is the primary target of backdoor injection, our backdoor removal algorithm consists of three main steps: identifying the affected module, locally initializing the affected parameters, and fine-tuning the model on augmented clean data.\n1. Identifying the Affected Module: We first identify the key module most affected by the backdoor using the function IdentifyAffectedModule(M). This function leverages the inconsistency scores computed in the backdoor detection algorithm to determine the module with the highest average inconsistency, which is considered the most likely target of the backdoor injection.\n2. Locally Initializing the Affected Parameters: Once the affected module is identified, we perform a local initialization of its parameters using the function LocallyInitialize (M, affected_module). This function resets the parameters of the affected module to random values, while keeping the parameters of other modules unchanged. This step aims to erase the backdoor influence concentrated in the affected module.\n3. Fine-tuning on Augmented Clean Data: Finally, we fine-tune the locally initialized model Minit on the clean training dataset Dclean for E epochs. In each training batch, we first apply data augmentation [50] to the clean data (X, Y) using the augmentation method A, obtaining the augmented data (Xaug, Yaug). Then, we update the model parameters to minimize the loss on the augmented clean data. This fine-tuning process helps the model adapt to the initialized parameters and further reduces any residual backdoor effect, while maintaining its performance on normal data.\nThe algorithm returns the fine-tuned model Mfine-tuned as the target detection model with the backdoor removed."}, {"title": "4 EXPERIMENTS", "content": "Model Structure and Dataset Description. We adopt four representative object detectors, including Faster R-CNN, Faster R-CNN FPN, Mask R-CNN, and Double-Head R-CNN, for the evaluations. Besides, following the classical setting in object detection, we use the COCO dataset [42] as the benchmark for our discussions.\nAttack Setup. Following the setup in [44], we simplify the approach by utilizing a white patch as the trigger pattern, with a poisoning rate established at 5%. Consistent with the methodology outlined in [46], the dimension of the trigger for each object is configured to be 1% of its ground-truth bounding box size, which equates to 10% of both the width and height, positioned centrally."}, {"title": "4.1 Experimental Settings", "content": "Model Structure and Dataset Description. We adopt four representative object detectors, including Faster R-CNN, Faster R-CNN FPN, Mask R-CNN, and Double-Head R-CNN, for the evaluations. Besides, following the classical setting in object detection, we use the COCO dataset [42] as the benchmark for our discussions.\nAttack Setup. Following the setup in [44], we simplify the approach by utilizing a white patch as the trigger pattern, with a poisoning rate established at 5%. Consistent with the methodology outlined in [46], the dimension of the trigger for each object is configured to be 1% of its ground-truth bounding box size, which equates to 10% of both the width and height, positioned centrally."}, {"title": "4.2 Backdoor Detection", "content": "To intuitively understand the impact of backdoors on the internal behavior of models, we first generate heatmaps of the inconsistency between RPN and R-CNN outputs for both clean and backdoored models, as shown in Figure 3 (a) and (b). By comparing the heatmaps of the two types of models, we observe that the inconsistency distribution of backdoored models is significantly higher than that of clean models. Furthermore, we plot histograms of the inconsistency scores, as depicted in Figure 3 (c), which further reveals the notable difference between the score distributions of backdoored and clean models. These visualizations provide us with an intuitive understanding that the presence of backdoors indeed leads to inconsistencies between the internal components of the model.\nThis observation aligns with our key intuition: if a backdoor is injected into a specific module of the model, the behavior of that module is likely to be inconsistent with the rest of the model. Taking Faster R-CNN as an example, if an attacker implants a backdoor into the classification head of the RCNN, it may result in inconsistent detection results between the RPN and RCNN, such as proposals correctly identified by the RPN being misclassified by the RCNN. This inconsistency provides us with a strong signal for detecting the presence of backdoors.\nTo further quantify the impact of backdoors on model inconsistency, we conduct experiments on four object detection models: Faster R-CNN, Faster R-CNN FPN, Mask R-CNN, and Double-Head R-CNN. For each model, we train both clean and backdoored versions using five different random initialization parameters. After setting a threshold 0 = 0.58, we compute the average inconsistency score \u03bc for each model on both the clean dataset and the backdoor trigger dataset. The experimental results are presented in Table 1. We observe that for all backdoored models, the \u00b5 values are significantly higher than those of the clean models and exceed the threshold 0. These quantitative results further confirm that our algorithm can effectively capture the internal inconsistencies caused by backdoors, thereby accurately detecting the presence of backdoors in the models."}, {"title": "4.3 Backdoor Defense", "content": "To validate the effectiveness of our proposed backdoor removal method, we conduct experiments on four widely-used object detection models: Faster R-CNN, Faster R-CNN FPN, Mask R-CNN, and Double-Head R-CNN. We evaluate the performance of these models under three scenarios: (1) Original, where the model is infected with a backdoor; (2) Vanilla, where the backdoored model is fine-tuned on a clean dataset; and (3) Ours, where the proposed backdoor removal method is applied. The experiments are performed on both a poisoned dataset, which contains the backdoor trigger, and a clean dataset without the trigger. The experimental results are presented in Table 2 and Figure 4.\nFor the Faster R-CNN model, our method achieves an AP of 0.285 on the poisoned dataset, significantly outperforming the original backdoored model (0.088) and the vanilla fine-tuning approach (0.149). This demonstrates the effectiveness of our method in removing backdoors from the model. Moreover, our method maintains an AP of 0.285 on the clean dataset, which is comparable to the performance of the vanilla fine-tuning approach (0.281) and only slightly lower than the original model's performance (0.337). This indicates that our method successfully preserves the model's performance on clean data while effectively eliminating the backdoor.\nSimilar trends can be observed for the other three object detection models. For Faster R-CNN FPN, our method achieves an AP of 0.291 on the poisoned dataset, surpassing both the original backdoored model (0.095) and the vanilla fine-tuning approach (0.149). On the clean dataset, our method maintains an AP of 0.291, which is close to the performance of the vanilla fine-tuning approach (0.308). For Mask R-CNN and Double-Head R-CNN, our method consistently outperforms the original backdoored models and the vanilla fine-tuning approach on the poisoned dataset, while preserving the performance on the clean dataset.\nFigure 4 presents a visual comparison of the performance trends during the backdoor removal process for each object detection model. As the number of epochs increases, our proposed method exhibits a rapid and stable improvement in mAP scores on the poisoned dataset, significantly outperforming vanilla fine-tuning. Moreover, our method maintains a high mAP score on the clean dataset, closely matching the performance of the model fine-tuned on the clean dataset using vanilla fine-tuning.\nThe experimental results, both quantitative and visual, provide strong evidence for the effectiveness of our proposed backdoor removal method. Across all four object detection models, our method consistently outperforms vanilla fine-tuning in terms of removing backdoors and maintaining performance on clean data. The significant improvements in AP scores on the poisoned dataset, as shown in Table 2, demonstrate the ability of our method to neutralize the effect of the backdoor trigger."}, {"title": "4.4 Ablation Study", "content": "To further investigate the effectiveness of different components in our proposed backdoor removal method, we conduct an ablation study on the Faster R-CNN FPN model. The results are presented in Table 3. We examine three variations of our method: (1) Targeted Renewal Finetuning (TRF), (2) TRF with Photodistortion (TRF+PD), and (3) TRF with Photodistortion and Random Flip (TRF+PD+RD). Here, TRF stands for Targeted Renewal Finetuning, PD represents Photodistortion, and RD denotes Random Flip.\nThe baseline method, TRF, achieves an AP of 0.262, demonstrating the effectiveness of finetuning the model with targeted renewal in removing backdoors. By incorporating photodistortion during finetuning (TRF+PD), the AP improves to 0.273, indicating that the introduction of data augmentation techniques enhances the model's robustness and generalization ability. Finally, the addition of random flipping (TRF+PD+RD) further boosts the AP to 0.291, showcasing the benefits of combining multiple data augmentation strategies."}, {"title": "5 CONCLUSION", "content": "In this paper, we presented a novel framework for backdoor detection and removal in object detection models. We proposed to exploit the inconsistency between the behaviors of the region proposal network (RPN) and the region classification network (R-CNN) as a strong indicator of backdoor presence. We developed a simple yet effective detection algorithm based on prediction inconsistency and devised a removal strategy that localizes the backdoor removal to the affected module via parameter reinitialization and global fine-tuning. Extensive experiments on multiple state-of-the-art object detectors demonstrated the effectiveness and generality of our method, significantly outperforming baseline removal methods while maintaining high clean data accuracy. To the best of our knowledge, our work represents the first systematic study of backdoor detection and removal in object detection models, offering a principled and efficient solution to enhance the robustness and trustworthiness of these models in safety-critical applications."}]}