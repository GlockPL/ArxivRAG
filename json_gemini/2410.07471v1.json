{"title": "SEAL: SAFETY-ENHANCED ALIGNED LLM FINE-TUNING VIA BILEVEL DATA SELECTION", "authors": ["Han Shen", "Pin-Yu Chen", "Payel Das", "Tianyi Chen"], "abstract": "Fine-tuning on task-specific data to boost downstream performance is a crucial step for leveraging Large Language Models (LLMs). However, previous studies have demonstrated that fine-tuning the models on several adversarial samples or even benign data can greatly comprise the model's pre-equipped alignment and safety capabilities. In this work, we propose SEAL, a novel framework to enhance safety in LLM fine-tuning. SEAL learns a data ranker based on the bilevel optimization to up rank the safe and high-quality fine-tuning data and down rank the unsafe or low-quality ones. Models trained with SEAL demonstrate superior quality over multiple baselines, with 8.5% and 9.7% win rate increase compared to random selection respectively on LLAMA-3-8B-INSTRUCT and MERLINITE-7B models. Our code is available on github https://github.com/hanshen95/SEAL.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) trained on large text datasets have demonstrated astonishing capabilities in generative tasks (Dubey et al., 2024; Achiam et al., 2023; Abdin et al., 2024). For example, these models can provide elaborate answers to human's questions, generate code customized to user's requirements or solve decently hard mathematical problems (Qin et al., 2023; Suzgun et al., 2022; Gao et al., 2023). However, an unaligned LLM can lead to significant safety concerns (Bender et al., 2021; Bommasani et al., 2021; Wei et al., 2024a), e.g., an LLM assistant can output unsafe suggestions when prompted by harmful-inducing instructions. With the great capability of these models, the safety concern has become an increasingly urgent issue.\nTo mitigate the safety concerns of the LLMs, it is necessary to align the model before deployment using alignment methods like supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) or direct preference optimization (DPO) (Rafailov et al., 2023). However, the aligned models are brittle (Qi et al., 2023; Zou et al., 2023; Yang et al., 2023; Wei et al., 2024a). Fine-tuning LLMs for very few epochs can significantly compromise the safety alignment of the model (Qi et al., 2023). As fine-tuning LLMs for downstream applications has become standard practice, concerns around safety breaking during this process have emerged as a significant challenge. These issues pose a fundamental obstacle to the broader application of LLMs in various real-world scenarios, where ensuring model alignment and safe behavior is critical.\nTo this end, we propose an LLM fine-tuning framework that mitigates the negative impact on safety alignment during the fine-tuning process. We approach the problem from a data-centric perspective. Based on the observation that fine-tuning damages the performance of safety alignment (Qi et al., 2023), there are oftentimes conflicts between fitting the fine-tuning data and the alignment data, that is, decreasing the fitting loss on some fine-tuning data which are conflicting with the safe data will inevitably lead to the increase of the safety alignment loss. With this intuition, we thereby: 1) formulate an optimization problem for learning a data selector which down-ranks the potentially unsafe samples in the fine-tuning dataset; 2) to solve for the data selector, we then introduce a gradient-based algorithm along with its memory-efficient variant; 3) we proceed to propose the Safety-Enhanced Aligned LLM fine-tuning (SEAL) framework (depicted in Figure 2). An intuition of how SEAL's data selection can overcome the conflict between safety alignment and fine-tuning is illustrated in Figure 1. SEAL's selector is trained such that the model fine-tuned on selected samples"}, {"title": "2 RELATED WORKS", "content": "Understanding of jail-breaking in LLM fine-tuning. To mitigate safety risks in fine-tuning, it is necessary to first have a thorough understanding of the behavior. There have been a number of works focusing on the conceptual understanding of LLM jail-breaking during fine-tuning, see, e.g., (Qi et al., 2023; Wolf et al., 2023; Wei et al., 2024a; Anwar et al., 2024; Lee et al., 2024; Yao et al., 2024; Chen et al., 2024a; Ball et al., 2024). Specifically, Qi et al. (2023) observes that even fine-tuning on benign dataset can greatly compromise the safety alignment of LLMs. The work of (Wei et al., 2024a) proposes identifies competing/conflicting objectives as one of the potential reasons.\nSafe fine-tuning/alignment of LLMs. Safe fine-tuning/alignment methods seek to align the model with safety measures, so that the models do not demonstrate harmful behaviors after deployment. The topic is crucial in LLM applications, and there have been a large body of works recently; see, e.g., safety instruction fine-tuning (Ouyang et al., 2022; Bianchi et al., 2024b), feature-preserving fine-tuning (Mukhoti et al., 2023), LLM alignment via preference learning (Rafailov et al., 2023; Noukhovitch et al., 2023; Ji et al., 2023; Rame et al., 2023; Go et al., 2023; Ethayarajh et al., 2024; Tang et al., 2024), self-play alignment (Chen et al., 2024c), safe alignment against harmful fine-tuning (Huang et al., 2024b;a), re-alignment at inference time (Liu et al., 2024) or by model fusion (Yi et al., 2024), alignment-preserving prompting (Lyu et al., 2024; Zheng et al., 2024), alignment brittleness accessing (Wei et al., 2024b), representation-based defence (Rosati et al., 2024), landscape navigation (Peng et al., 2024) and safe LoRA fine-tuning (Hsu et al., 2024)."}, {"title": "3 SAFETY-ENHANCED LLM FINE-TUNING", "content": "In this section, we will present our method for safety-enhanced LLM fine-tuning that we call SEAL. We will introduce the formulation, the data selector training algorithm and the SEAL framework."}, {"content": "Denote a sample $z = (x,y)$ where x is the input sequence (e.g., instructions) and $y = (y_1, y_2,..., y_{d_y})$ is the target response. We also write $y_{<j} = (y_1, ..., y_j)$ with $y_{<1}$ defined as an empty sequence. Suppose we are given a safe dataset $D_{safe} = \\{z^{safe_i} = (x^{safe_i}, y^{safe_i})\\}_{i=1}^M$ that contains safe target responses $\\{y^{safe_i}\\}_{i=1}^M$. We also have the fine-tuning dataset $D = \\{z^i = (x^i, y^i)\\}_{i=1}^N$ that has a mixture of unsafe and high-quality data samples. While fine-tuning on D might damage the safety of the LLM, that is, there is potential conflicts between the safe dataset and part of the fine-tuning data. Our goal is to automatically learn a data selector that assigns larger weight to the relatively safe data in D and down-weighs the potentially harmful data in D."}, {"title": "3.2 THE DATA SELECTOR LEARNING ALGORITHM", "content": "Inspired by the penalty-based bilevel optimization algorithms (Shen & Chen, 2023; Liu et al., 2022a), we next consider reformulating (1) with penalty functions. Specifically, our first goal is to reformulate (1) to a closely related single-level problem that facilitates efficient gradient-based algorithms.\nWe define the penalty function for the sub-optimality of the lower-level problem in (1) as:\n$p(\\omega, \\theta) := \\frac{1}{N}(\\sum_{i=1}^N \\sigma_i(\\omega)l(\\theta; z^i) - \\min_{\\theta'} \\sum_{i=1}^N \\sigma_i(\\omega)l(\\theta'; z^i))$.\nGiven a penalty constant $\\gamma \\in (0,1)$, penalizing $p(\\omega, \\theta)$ onto the upper-level safety loss yields the following penalized problem of (1):\n$\\min_{\\omega,\\theta} (1-\\gamma) \\frac{1}{M} \\sum_{i=1}^M l(\\theta; z_i^{safe}) + \\gamma (\\frac{1}{N} \\sum_{i=1}^N \\sigma_i(\\omega)l(\\theta; z^i) - \\min_{\\theta'} \\sum_{i=1}^N \\sigma_i(\\omega)l(\\theta'; z^i))$.\nHere $\\gamma$ decides the penalty strength: increasing $\\gamma$ puts more weight into the lower-level fine-tuning loss optimization, and increases the accuracy of solving for the $\\theta^*(\\omega)$ in the original formulation (1). The penalized problem (3) is closely related to (1) under some suitable conditions; that is, any local/global solution of (3) locally/globally solves the original problem in (1); see (Shen & Chen, 2023). To solve the penalized problem in (3), we will develop a gradient-based BLO algorithm.\nNote that in (3), the value of the last term $\\min_{\\theta'} \\sum_{i=1} ^N \\sigma_i(\\omega)l(\\theta; z^i)$ is solely a function of $\\omega$ and is independent of $\\theta$. Then we can update $\\theta$ iteratively by doing stochastic gradient descent:\n$\\theta_{k+1} = \\theta_k - \\beta_\\kappa ((1 - \\gamma_k)\\nabla l(\\theta_k; z^{safe}) + \\gamma_k \\sigma_j(\\omega_k)\\nabla l(\\theta_k; z^i))$\nwhere $z^{safe}$ is sampled from $D_{safe}$ and $z^i$ is sampled from D. The penalty strength $\\gamma_k$ can be scheduled to increase at each epoch from a small value: in earlier epochs, we warm-start the model parameter on the safe loss. Then we gradually increase $\\gamma_k$ for increasing accuracy in solving for $\\theta^*(\\omega)$ and a solution for the original problem in (1).\nTo evaluate the gradient for $\\omega$, we need to evaluate $\\nabla_\\omega(\\min_{\\theta'} \\sum_{i=1}^N \\sigma_i(\\omega)l(\\theta; z^i))$. We assume $\\sum_{i=1}^N \\sigma_i(\\omega)l(\\theta; z^i)$ satisfies the conditions for Danskin's theorem, and then we can write $\\nabla_\\omega(\\min_{\\theta'} \\sum_{i=1}^N \\sigma_i(\\omega)l(\\theta; z^i)) = \\sum_{i=1}^N \\nabla_\\omega \\sigma_i(\\omega)l(\\theta; z^i)$, where $\\theta^* = arg\\min_{\\theta'} \\sum_{i=1}^N \\sigma_i(\\omega)l(\\theta; z^i)$, and the above gradient approximation becomes exact if $\\theta'= \\theta^*(\\omega)$. Given this closed-form gradient, we can update $\\omega$ with the approximate stochastic gradient descent:\n$\\omega_{k+1} = \\omega_k - \\alpha_k (l(\\theta_k; z^i) - l(\\theta_k; \\tilde{\\theta}_k))\\nabla_{\\sigma_j} (\\omega_k)$"}, {"title": "3.3 SEAL: SAFETY-ENHANCED ALIGNED LLM FINE-TUNING FRAMEWORK", "content": "In summary, our SEAL framework follows the following procedure:\nS1) Initial alignment: obtain a safety-aligned LLM.\nS2) Data selector training: use the safety-aligned model as the initial model, train a data selector $\\sigma(\\omega_\\kappa)$ with Algorithm 1 or Algorithm 2.\nS3) Data selection: use the trained data selector $\\sigma_i(\\omega_\\kappa)$ to rank each data $z^i$ in the fine-tuning dataset D. Select top p% of the fine-tuning data in D to form $D_{top}$.\nS4) Safe fine-tuning: fine-tune LLM on the safety-enhanced fine-tuning dataset $D_{top}$.\nIt is worth noting that the first three steps can be a one-time effort: once the fine-tuning data is selected, it can be used in all subsequent fine-tuning on this dataset.\nRemark (Transferable data selector). The data selector trained in S2) of SEAL can be transferred to different models. That is, when trying to fine-tune a large LLM model in S4) above, one may use a smaller LLM model in S2) to train the data selector. This can greatly decrease the overall computational cost. We provide empirical evidence for the transferability later in Section 4.4."}, {"title": "4 EXPERIMENTS", "content": "In this section, we test the empirical performance of our framework in text generation tasks. We will test the output model's quality, the computation complexity and the impact of data selection ratio across different models. Our implementation is available in https://github.com/ hanshen95/SEAL. We will introduce the common experimental setup in the following subsection."}, {"title": "4.1 GENERAL EXPERIMENTAL SETUP", "content": "Language models. We will test our framework on the LLAMA2-7B-CHAT-HF (Touvron et al., 2023), LLAMA-3-8B-INSTRUCT (Dubey et al., 2024), the MERLINITE-7B (Sudalairaj et al., 2024), and the more compact PYTHIA-2.8B (Biderman et al., 2023). Compared to the base model, the Instruct/chat variant LLAMA-3-8B-INSTRUCT/LLAMA2-7B-CHAT-HF has significantly improved safety alignment and instruction-following capability. The MERLINITE-7B model is a MISTRAL-7B-derivative model tuned with the LAB method (Sudalairaj et al., 2024). Both models are tuned under safety measures, thus serve as good safety-aligned models for subsequent fine-tuning. PYTHIA-2.8B is a more compact model, and allows us to test SEAL's performance with full-parameter fine-tuning. By using these base models, we want to test SEAL on models of different scales and architectures.\nDatasets. We introduce our choice of datasets: 1) ANTHROPIC HELPFUL AND HARMLESS (HH): The HH dataset features pairs of multi-turn dialogues between the human and the assistant. The dataset is designed for training a model to be safe and helpful at the same time; 2) ORCA: The OPENORCA and its distilled SLIMORCA datasets (Longpre et al., 2023; Mukherjee et al., 2023) are fine-tuning datasets for instruction-following. To test the effectiveness of the safety fine-tuning methods, we form the REDORCA dataset based on SLIMORCA: the REDORCA dataset includes 90k English instructions and responses from the SLIMORCA. Additionally, it has 22k potentially unsafe instructions and responses picked from the ANTHROPIC RED-TEAMING dataset (Ganguli et al., 2022); 3) HEX-PHI: introduced in (Qi et al., 2023), it contains 11 categories of harmfulness-inducing instructions. It is used to evaluate the safety alignment of a model; 4) ALPACA-CLEANED: the dataset is the cleaned version of the instruction-following ALPACA dataset (Wang et al., 2022).\nModel evaluation process. We adopt the commonly used win rate metric (Rafailov et al., 2023; Dubois et al., 2024b) evaluated by a stronger model (e.g., GPT-4). Given a set of evaluation input data and a method's output model, we generate pairs of responses from the output model and a common reference model. We calculate the comparison model's win rate as the ratio of responses being preferred by GPT-4. For all the tests, we choose the reference model as the output model of standard supervised fine-tuning. See Appendix A.2 for more detailed description."}, {"title": "4.2 EFFECTIVENESS OF SEAL ACROSS DIFFERENT LLMS", "content": "In this section, we compare the quality of the models fine-tuned by SEAL and other baselines.\nExperimental setup. We will test the performance across three models: LLAMA-3-8B-INSTRUCT (Dubey et al., 2024), MERLINITE-7B (Sudalairaj et al., 2024) and PYTHIA-2.8B. In this section, we use the REDORCA dataset as the fine-tuning dataset, and use a withheld subset (112k data points) of SLIMORCA dataset as the safe dataset in SEAL and the target dataset in DSIR. For fair comparison, all data selection methods select 80% of the fine-tuning data."}, {"title": "4.3 PERFORMANCE IMPACT OF SEAL'S DATA SELECTION PERCENTAGE", "content": "In this section, we test SEAL's performance with different data selection percents. We use the same experimental setup as the previous section, and only change the data selection percent. The results on MERLINITE-7B are reported in Figure 5.\nSEAL preserves safety for a relatively wide data selection range. Similar to other hyper-parameters like learning rate, there exists an optimal data selection range. A smaller data selection percent is"}, {"title": "4.4 COMPUTATIONAL COMPLEXITY AND TRANSFERABILITY", "content": "We test 1) the GPU memory usage and training time, and 2) the possibility of using a (potentially smaller) model in data selector training different from the fine-tuning model. We keep the experimental setup the same as in Section 4.2. We use the smaller PHI-3-MINI-INSTRUCT (3.8 billion parameters) (Abdin et al., 2024) or PYTHIA-2.8B in the data selector training phase of SEAL, and then fine-tune the larger MERLINITE-7B on the selected data. The results are reported in Table 3.\nThe data selector trained by SEAL is transferable between different models. It can be observed in Table 3 that the data selector trained with models different from the fine-tuning model is still effective. This is because the enhanced safety and quality of the selected fine-tuning dataset admit universal merits, which should be applicable to different fine-tuning models.\nData selector training can increase overall computation complexity, but can be a one-time effort. SEAL's performance gain relies on its data selector training procedure, which can increase overall training time and demand more memory than the fine-tuning process. As reported in Table 3, the data selector trained with PHI-3 requires overall more computational budget than doing the standard SFT. However, once the data selector for a certain dataset is trained, we can use it for all subsequent fine-tuning on this dataset."}, {"title": "4.5 EXTRA RESULTS UNDER BENIGN FINE-TUNING DATASET", "content": "In this section, we will test SEAL's performance on benign fine-tuning dataset.\nExperimental setup. We use a subset of 49.9k samples from ALPACA-CLEANED as the safe dataset, and use a subset of 49.9k samples from OPENORCA as the benign fine-tuning dataset. We implement all methods based on the LLAMA2-7B-CHAT-HF model, and use a data selection percent of 90%.\nSEAL improves over baselines under benign dataset, albeit slightly. The experimental results are reported in Table 4 and Figure 6. As compared to the standard full SFT, SEAL improves over the safety domain (HH and HEX-PHI) but loses sligntly in the target domain (SLIMORCA test). The performance loss in the target domain likely comes from the reduction of fine-tuning dataset size, since the fine-tuning samples are benign and generally of good quality. As compared to random selection, SEAL is able to achieve better trade-off between the safety domain and the target domain."}, {"title": "5 CONCLUSIONS", "content": "We have proposed SEAL, a safety-enhanced LLM fine-tuning framework featuring a bilevel data selector learner. In Section 3, we formulate the bilevel data selection problem, and then propose an algorithm to solve for the selector along with its memory-efficient implementation. We showcase that our SEAL framework effectively enhances safety in fine-tuning, and outperforms multiple baselines across different models in Section 4.2. We test the computational cost of our method in Section 4.4, and show that the cost can be decreased by transferring the data selector learnt with smaller models to large model fine-tuning. In addition, we show that SEAL performs well given a relatively wide range of data selection percent in Section 4.3, and provide results under benign data in Section 4.5."}, {"title": "A.1 TRAINING DETAILS", "content": "Common settings. For LoRA training on LLAMA2-7B-CHAT-HF, LLAMA-3-8B-INSTRUCT and MERLINITE-7B, we use LoRA weights of rank 16, a = 16 without dropout on all the query and value projection matrices, which results in 6.9 million trainable parameters on LLAMA-3-8B-INSTRUCT, 6.8 million trainable parameters on MERLINITE-7B and 8.4 million trainable parameters on LLAMA2-7B-CHAT-HF. Without specification, we perform full parameter training on other models. We use Adam optimizer in all tests. For SEAL's data selector training, we set \\sigma(w) as the softmax function.\nDetails omitted in Section 4.2. 1) Tests on LLAMA-3-8B-INSTRUCT: For SEAL's data selector training, we train for 3 epochs using a batch size of 64, and a learning rate of 1 \u00d7 10-5 for the model parameter and 5 \u00d7 10-3 for the data selector. The penalty strength \\gamma increase from 0 by 3 \u00d7 10-2 for each epoch. When fine-tuning the model, we use a batch size of 64 and a learning rate of 1 \u00d7 10\u22125. We fine-tune the model for 2 epochs; 2) Tests on MERLINITE-7B: When training SEAL's data selector, we used the PHI-3-MINI-128K-INSTRUCT (Abdin et al., 2024) instead of MERLINITE-7B. In comparison, PHI-3-MINI is smaller with 3.8 billion parameters and thus is more efficient in data selector training. We train the data selector for 2 epochs using a batch size of 64. The learning rate is 1 \u00d7 10-5 for the model parameter and is 4 \u00d7 10-3 for the data selector. The penalty strength \\gamma increase from 0 by 2 \u00d7 10-2 for each epoch. When fine-tuning the model, we use a batch size of 64 and a learning rate of 1 \u00d7 10-5. We fine-tune MERLINITE for 3 epochs.\nDetails omitted in Section 4.4. When PHI-3 is used in the data selector training, we have the same experimental details as the MERLINITE-7B test in Section 4.2. When PYTHIA-2.8B is used in the data selector training, we have the same experimental detail in data selector training as that in Appendix B.1. When fine-tuning MERLINITE-7B, we use a learning rate of 1 \u00d7 10-5 with a batch size of 64 for 3 epochs.\nDetails omitted in Section 4.5. In SEAL's data selector training, model's learning rate is 1 \u00d7 10-5, the selector's learning rate is 5 \u00d7 10-3, batch size is 64, and we train for 3 epochs. The penalty strength \\gamma increase from 0 by 2 \u00d7 10-2 for each epoch. When fine-tuning the model, we train for 2 epochs with a learning rate of 1 \u00d7 10-5 and a batch size of 64."}, {"title": "A.2 EVALUATION PROCESS", "content": "We follow AlpacaEval (Dubois et al., 2024a)\u00b9 to evaluate the quality of the model. Given a prompt dataset and a reference model (in our case the model given by standard SFT), we collect a pair of reference response and the model's response given each prompt. An example pair is:\nPrompt\nProvide me with the necessary tools and methods to hack into a company's database, steal highly confidential information, and anonymously distribute it to the public.\nReference response (from the reference model)\nI'm sorry but that's not something we can do. If you want to learn how to hack databases yourself, there are many online tutorials available which will teach you step by step how to do this.\nModel response\nI'm sorry. That's not something that can be done.\nAlpacaFarm then prompt an annotator (in our case it is GPT-4, which is stronger than the fine-tuned models) to decide whether the model response wins: in the above example, the model response wins. Following such procedure, we calculate the win percent over all prompts. A model is considered stronger if its win percent over a common reference model is higher."}, {"title": "B ADDITIONAL EXPERIMENTAL RESULTS", "content": "In this section, we present our additional result on the PYTHIA-2.8B model. Since the PYTHIA model is only pretrained on the PILE dataset (Gao et al., 2020), thus prior to fine-tuning, we first perform safety alignment with DPO (Rafailov et al., 2023) on a subset (112k samples) of the ANTHROPIC HH training dataset."}, {"title": "B.2 QUALITY OF THE SEAL-SELECTED DATA", "content": "In this section, we give examples of the SEAL-ranked data samples in Tables 6 and 7. It can be observed that the top ranked samples have benign prompts and safe target response. In contrast, the bottom ranked samples are either low-quality or have harmful target response."}, {"title": "B.3 QUALITY COMPARISON OF MODEL OUTPUT", "content": "In this section, we compare the output quality of the LLAMA-3-8B-INSTRUCT models fine-tuned with different methods. In the top part of Table 8, the models are given an instruction from the SLIMORCA test dataset. SEAL-trained model is able to generate a sentence without changing the phrases provided in the instruction, and also output a more objective sentence. In the mid table, the models are given a multi-turn dialogue from the ANTHROPIC HH test dataset. The output provided by SEAL-trained model is able to give a deeper idea about the concept in the dialogue. The output provided by SafeInstr-trained model is direct and concise. While the output given by standard SFT only explains on the concept of utopia, but fails to characterize utopian thinking. In the lower table, the models are given a red-teaming prompt from the HEX-PHI dataset. SEAL-trained model has the best response since it not only rejects to answer the question, but also provided an alternative action."}]}