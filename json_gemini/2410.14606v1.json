{"title": "Streaming Deep Reinforcement Learning Finally Works", "authors": ["Mohamed Elsayed", "Gautham Vasan", "A. Rupam Mahmood"], "abstract": "Natural intelligence processes experience as a continuous stream, sensing, acting, and learning moment-by-moment in real time. Streaming learning, the modus operandi of classic reinforcement learning (RL) algorithms like Q-learning and TD, mimics natural learning by using the most recent sample without storing it. This approach is also ideal for resource-constrained, communication-limited, and privacy-sensitive applications. However, in deep RL, learners almost always use batch updates and replay buffers, making them computationally expensive and incompatible with streaming learning. Although the prevalence of batch deep RL is often attributed to its sample efficiency, a more critical reason for the absence of streaming deep RL is its frequent instability and failure to learn, which we refer to as stream barrier. This paper introduces the stream-x algorithms, the first class of deep RL algorithms to overcome stream barrier for both prediction and control and match sample efficiency of batch RL. Through experiments in Mujoco Gym, DM Control Suite, and Atari Games, we demonstrate stream barrier in existing algorithms and successful stable learning with our stream-x algorithms: stream Q, stream AC, and stream TD, achieving the best model-free performance in DM Control Dog environments. A set of common techniques underlies the stream-x algorithms, enabling their success with a single set of hyperparameters and allowing for easy extension to other algorithms, thereby reviving streaming RL.", "sections": [{"title": "1 Introduction", "content": "Learning from a continuous stream of experience as it arrives is a paramount challenge, mirroring natural learning (Hayes et al. 2021), and is relevant to many applications involving on-device learning (Hayes & Kanan 2022, Neuman et al. 2022, Verma et al. 2023). For instance, learning from recent experience can help systems adapt quickly to changes (e.g., wear and tear) compared to learning from potentially obsolete data. In streaming reinforcement learning, such as Q-learning or temporal difference (TD) learning, the agent receives an observation and reward at each step, taking action and making a learning update immediately without storing the sample. This scenario is practical since retaining raw samples is often infeasible due to limited computational resources (Hayes & Kanan 2022), lack of communication access, or concerns about data privacy (Van de Ven et al. 2020).\nWhile classic RL algorithms like Q-learning, SARSA, Actor-Critic, and TD were originally developed for streaming learning (see Sutton & Barto 2018), recent advancements have shifted focus primarily toward batch learning. Indeed, advancements in recent deep RL rely heavily on computationally extensive batch learning as demonstrated in many domains, such as games (e.g., Mnih et al. 2015, Silver et al. 2017), simulated environments (e.g., Haarnoja et al. 2018) and various robotics tasks (e.g., Smith et al. 2023, Haarnojaet al. 2024). Batch RL algorithms store past samples in a storage called replay buffer and draw samples from it in a batch to make updates.  highlights the difference between agents in the problem settings of streaming RL and batch RL\u00b9. Unlike batch RL, streaming RL does not permit the use of a replay buffer or batch updates.\nThe success of batch RL is often attributed to its efficiency with data and modern hardware, as argued by Riedmiller (2005), Mnih et al. (2015, 2016), and Lillicrap et al. (2016), among many others. Averaging samples in a batch may enable more reliable updates, and reusing samples multiple times may potentially extract more information from the same sample. Moreover, batch updates allow efficient use of parallel environments and modern hardware accelerators like GPUs. However, the prohibitive computational requirements of batch learning methods render them unsuitable for on-board learning in resource-constrained systems, such as edge devices or Mars rovers (Wang et al. 2023), or when rapid decision-making is necessary (e.g., latency arbitrage). For example, storing high-dimensional images for replay demands substantial memory, and batch updates slow down real-time prediction and decision-making (see Yuan & Mahmood 2022). When computation is constrained or samples cannot be stored, streaming learning becomes essential. And yet, streaming learning remains largely unadopted in deep RL, and currently, there is a noticeable absence of streaming RL applications in practice, making deep RL under resource constraints unachieved.\nDeep streaming RL is understood to be inherently sample inefficient since samples cannot be reused (ct. D'Oro et al. 2023, Schwarzer et al. 2023). Another reason for sample inefficiency is that credit assignment is typically propagated slowly by one-step methods, which bootstrap fully, compared to their multi-step counterparts, which use rewards from multiple steps (Sutton & Barto 2018). Although methods using multi-step returns have better credit assignment, they cannot make updates at immediate time step or without storing (Mahmood 2017). Eligibility traces (Sutton 1988, van Hasselt et al. 2014, Mahmood & Sutton 2015, van Seijen et al. 2016, White & White 2016, Thodoroff et al. 2019, van Hasselt et al. 2021) attempt to balance the benefits of using multi-step returns with updating at every time step. However, they are rarely used in deep RL.\nAlthough the absence of streaming deep RL is attributed to its sample inefficiency, a more critical reason is that existing deep learning methods experience learning instabilities and even failures in the streaming learning setting (see Elfwing et al. 2018), which we refer to as stream barrier (see Figure 2). Deep RL methods already struggle with onlineupdates, facing issues such as loss of plasticity (Lyle et al. 2023, Dohare et al. 2024), poor learning dynamics (Lyle et al. 2024), failure to achieve further improvement (e.g., Lyle et al. 2023, 2022), and gradual performance degradation (e.g., Dohare et al. 2023, Abbas et al. 2023, Elsayed & Mahmood 2024). In addition, streaming deep RL presents unique challenges since the observation and reward distributions used for updating change rapidly over time, exacerbating the issues. The lack of application of eligibility traces with neural networks can also be attributed to the issues of instability (see Anand & Precup 2021, Harb & Precup 2017), which can even lead to divergence (Veeriah et al. 2017). As a result, deep RL methods face stream barrier and have largely been overlooked. However, a few studies (Elfwing et al. 2018, Young & Tian 2019) have shown nascent performance with streaming learning, suggesting that this area holds potential for further exploration and development.\nIn this paper, we address stream barrier by introducing streaming deep RL methods\u2014stream TD(\u03bb), stream Q(\u03bb), and stream AC(X)\u2014that are collectively called the stream-x algorithms and utilize eligibility traces. Our approach enables learning from the most recent experiences without using replay buffers, batch updates, or target networks. Contrary to the common belief, we demonstrate that streaming deep RL can be stable and as sample efficient as batch RL. The effectiveness of our approach hinges on a set of key techniques that are common to all stream-x algorithms. They include a novel optimizer to adjust step size for stability, appropriate data scaling, a new initialization scheme, and maintaining a standard normal distribution of pre-activations. Our approach requires no hyperparameter tuning, and the results with different algorithms on the electricity consumption prediction task (Zhou et al. 2021), MuJoCo (Todorov et al. 2012), DM Control Suite (Tunyasuvunakool et al. 2020), MinAtar (Young & Tian 2019), and Atari 2600 (Bellemare et al. 2013) environments are achieved using the same set of hyperparameters. The results demonstrate our approach's ability to work as an off-the-shelf solution, overcome stream barrier, provide results previously unattainable with streaming methods, and even surpass the performance of batch RL, achieving the best model-free performance on some complex environments."}, {"title": "2 Background", "content": "The interaction between the agent and the environment is modeled as a Markov decision process (MDP). We consider in this paper episodic interactions, the episodic MDP of which is given by the tuple $(S, A, P, R, \\gamma, d_0, H)$, where $S$ is the set of states, $A$ is the set of actions, $P : S \\times A \\rightarrow \\Delta(S \\times R)$ is the transition dynamics model in which $\\Delta(X)$ is a distribution over the set $X$, $R$ denotes the set of reward signals, $d_0$ is the distribution of starting states, $\\gamma \\in [0, 1]$ is the discount factor, and $H$ is the set of terminal states. The agent interacts with the environment according to a policy $\\pi : S \\rightarrow \\Delta(A)$ that gives a distribution over actions conditioned on the state. The interaction in each episode starts when the environment samples a state from the starting state distribution: $S_0 \\sim d_0$. At each time step $t$, the agent receives a state $S_t$ from the environment, takes an action $A_t \\sim \\pi(\\cdot|S_t)$, and the environment samples the next state and reward using the transition model: $S_{t+1}, R_{t+1} \\sim P(\\cdot, S_t, A_t)$. The agent keeps interacting with the environment until it reaches one of the terminal states $S_T \\sim H$, where $T$ is the termination time step. The episodic return is defined as the sum of discounted rewards starting from time step $t$: $G_t = \\sum_{k=t+1}^T \\gamma^{k-t-1}R_k$. The goal of the agent in the prediction problem is to estimate, for a given policy $\\pi$, the value function $v_\\pi = E_{\\pi} [G_t|S_t = s], \\forall s \\in S$ with an estimator $\\hat{v}(s, w)$ or the action-value function $q_\\pi(s, a) = E_{\\pi} [G_t|S_t = s, A_t = a], \\forall s \\in S, a \\in A$ with an estimator $\\hat{q}(s, a, w)$, where $w$ is a parameter vector. The goal of the agent in the control problem is to find the optimal policy $\\pi^*$ using action-value estimates such that $q_{\\pi^*}(s, a) = \\max_{\\pi} q_\\pi(s, a), \\forall s \\in S, a \\in A$ or to optimize the objective $J(\\theta) = E_{S_0 \\sim d_0} [V_{\\pi_\\theta}(S_0)]$ wrt $\\theta$ that parameterizes the policy $\\pi_\\theta$.\nTemporal Difference Learning. To estimate the value function for prediction or learn the optimal policy for control, we can use a Monte Carlo estimate based on the return $G_t$, which requires waiting until the episode is terminated, resulting in the update rule $w_{t+1} = w_t + \\alpha(G_t - \\hat{v}(S_t, w_t))\\nabla_w\\hat{v}(S_t, w_t)$. Temporal difference (TD) learning (Sutton 1988) alleviates this issue by relying on the idea of bootstrapping. In TD learning, the return $G_t$ is replaced by the bootstrapped target $R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w)$ called one-step return, resulting in the TD error: $\\delta_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)$. The TD error can be used to update a value estimate as soon as the next state and reward are observed.\nPolicy Gradient Theorem. When the agent is learning a parameterized policy to maximize the objective $J(\\theta)$, model-free gradient updates can be used according to the policy gradient theorem (Sutton et al. 1999): $\\nabla J(\\theta) \\propto E_{s \\sim d^\\pi, A \\sim \\pi} [q_\\pi(S, A)\\nabla_\\theta \\log \\pi(A|S, \\theta)]$, where $d^\\pi$ is the discounted stationary-state distribution. In practice, the action-value function $q_\\pi$ is replaced by an, often biased, estimate, and states are sampled on-policy or from a replay buffer without considering discounting and including further bias (Thomas 2014, Nota & Thomas 2019, Zhang et al. 2022, Che et al. 2023). The estimator $\\delta_t V_\\theta \\log \\pi(A_t|S_t, \\theta) \\approx VJ(\\theta)$ is used in one-step actor-critic (AC), which learns both a policy or an actor and a value function or a critic (see Barto et al. 1983).\nEligibility Traces. Eligibility traces are short-term memory vectors that can be used to form multi-step methods in a streaming form, achieving better credit assignment than their one-step counterparts. The idea of eligibility traces (Sutton & Barto 1981) is influenced by the biological neuroscience model by Klopf (1972). The eligibility trace vector is initialized to zero at the start of the episode; then, it accumulates the value gradient faded by $\\gamma\\lambda$, where $\\lambda \\in [0, 1]$ is the eligibility trace parameter. Specifically, given the eligibility trace"}, {"title": "3 Method", "content": "In this section, we introduce our method and describe the necessary components for successful streaming reinforcement learning agents. The agents, under the streaming reinforcement learning problem, are required to process one sample at a time without storing any sampless for future reuse.\u00b2 Such requirements create additional hurdles compared to batch deep reinforcement learning, even though both learn from a non-stationary stream of data. We list the issues that hinder learning as 1) learning instability due to occasional large updates, 2) learning instability due to activation nonstationarity, and 3) improper scaling of data. These issues are already present in batch methods causing several detrimental effects such as drop in performance (Dohare et al. 2023, Abbas et al. 2023), high variance (Bjorck et al. 2021), or inability to improve performance (Lyle et al. 2023). However, they are exacerbated with streaming learning as updates can fluctuate more from one step to another due to non-i.i.d. sampling for updates. For example, streaming learning is more prone to instability as successive per-sample gradients can point in different directions, making it difficult to choose a single working step size. In contrast, batch methods mitigate this issue by averaging gradients from an i.i.d.-sampled batch drawn from a large pool. Moreover, we use additional techniques for sample efficiency, which we describe first."}, {"title": "3.1 Sample efficiency with sparse initialization and eligibility traces", "content": "Since steaming learning methods must discard the sample once used, they can potentially be sample inefficient. Here, we present two techniques to improve sample efficiency of streaming learning methods: 1) sparse initialization and 2) eligibility traces.\nSparse representations induce locality when updating the network, which reduces the amount of interference between dissimilar inputs. Many works have shown that sparsity reduces forgetting, which helps improve sample efficiency in reinforcement learning (Liu et al. 2019, Pan et al. 2021, Sokar et al. 2022, Lan & Mahmood 2023). For example, tile coding (Albus 1971) has been shown to reduce forgetting in RL (see Ghiassian et al. 2020).\nWe use a simple technique to introducesparsity at initialization by randomly initializing most weights to zeros. Specifically, we impose a sparsity level $s$ (e.g.,0.9) at each layer representing the proportion of zero-initialized weights. The remaining weights are initialized according to theLeCun initialization scheme (LeCun et al.2002). Although sparsity-based initialization has not been investigated for reinforcement learning before, it has been shown toimprove optimization in supervised learning (Martens 2010). Algorithm 1 shows ourproposed sparse initialization technique-SparseInit. This sparse initialization scheme canbe used for both fully-connected and convolutional layers.\nCredit assignment is a fundamental challenge in learning from interaction. Eligibility traces (Sutton 1988) provide a compact approach for better credit assignment than one-step methods. In this paper, we use accumulating traces for both value functions and policies."}, {"title": "3.2 Adjusting step sizes for maintaining update stability", "content": "Instability in deep reinforcement learning is an issue that persisted for a long time (Bjorck et al. 2021). Recently, many works (e.g., Asadi et al. 2023, Lyle et al. 2023, Dohare et al. 2023) have identified the Adam optimizer (Kingma & Ba 2015) as one of the main sources of instability. In this section, we aim to develop a stable optimizer that is more suitable for streaming reinforcement learning.\nIn optimization, a well-known strategy for avoiding large updates and choosing an appropriate step size is the backtracking line search method (Armijo 1966), which for each iteration typically chooses the step size that maximizes the expected or batch-based objective. Likewise, backtracking line search has been shown to be effective in stabilizing on-policy batch reinforcement learning (e.g., TRPO, Schulman et al. 2015). In the streaming case, it is not clear if choosing a step size that reduces the error in the current sample is the best strategy. A more pertinent goal in streaming learning is to de-emphasize an update if it is too large, for example, if the update overshoots the target on a single sample (Mahmood 2010, Mahmood et al. 2012). More specifically, given a scalar error $\\delta(S)$ on a sample (e.g., say an input-output pair), an update overshoots if the post-update error on the same sample $\\delta_+(S)$ changes its sign, that is, $\\delta(S)\\delta_+(S) < 0$. A change in the error sign indicates that the error has been over-corrected or the update has overshot the target. Kearney (2023) defined a related quantity, the effective step size, that measures the amount of progress the learner achieved based on the update, given as follows:\n$\\xi = \\frac{\\delta(S) - \\delta_+(S)}{\\delta(S)},$                                                               (1)\nwhere $\\xi > 1$ indicates overshooting or over-correction, $\\xi < 1$ indicates partial correction, and $\\xi = 0$ indicates no correction. The effective step size quantity can be used to control the amount of error correction, for example, well before overshooting occurs. We can compute the effective step size with a counterfactual update using some starting step size $\\alpha = \\alpha_{Init} \\in (0, 1]$. If the effective step size is larger than the maximum effective step size, $\\xi > \\xi_{max}, \\xi_{max} \\in (0, 1]$, then we reduce the step size by a factor of $\\beta, \\alpha = \\beta \\alpha, \\beta \\in (0, 1)$. This backtracking line search continues until the condition $\\xi \\leq \\xi_{max}$ is met. We call this process bounding effective step size with backtracking and provide its details in Algorithm 2. This overshooting prevention strategy was originally explored by Mahmood et al. (2012) to improve the stability of meta-gradient based supervised learning. The idea was then applied in reinforcement learning as well (see Dabney & Barto 2012, Kearney 2023, Javed et al. 2024). In both settings, only linear function approximation was previously considered."}, {"title": "3.3 Stabilizing activation distribution under non-stationarity", "content": "The change in weight distribution across layers can cause trainability issues (Xu et al. 2019). Thus, many normalization techniques exist to normalize each layer's pre-activations and give them similar distributions, which has shown advantages in both stationary (Xu et al. 2019) and nonstationary settings (Lyle et al. 2023, Gallici et al. 2024) to maintain favorable learning dynamics. Nauman et al. (2024) have shown that using layer normalization is crucial for achieving good performance in challenging environments, even with deep RL methods (e.g., Haarnoja et al. 2018). LayerNorm (Ba et al. 2016) standardizes the pre-activations by subtracting their mean and dividing by their variance. Other approaches normalize by the $L_2$ norm of the pre-activation (L2Norm, Nguyen et al. 2017) or the root mean square of the pre-activation vector (RMSNorm, Zhang & Sennrich 2019).\nIn our approach, we use LayerNorm (Ba et al. 2016), which we apply to the pre-activation of each layer (before applying the activation $\\sigma$) without learning any scaling or bias parameters. Specifically, the LayerNorm normalization $\\phi$ we use is given by\n$\\phi(\\alpha) = \\frac{\\alpha - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}},$     where $\\mu = \\frac{1}{n} \\sum_{i=1}^n a_i$ and $\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (a_i - \\mu)^2,$(4)\nwhere $n$ is the dimensionality of $\\alpha$ and $\\epsilon$ is a small number used for numerical stability. We call a network that applies LayerNorm at each layer a LayerNorm network."}, {"title": "3.4 Proper scaling of data", "content": "Properly scaling the training data is essential for effective learning (Schraudolph 2002, LeCun et al. 2002). Training data are typically normalized in supervised learning since all data points are available beforehand. This assumption breaks in reinforcement learning where learning is done online based on interactions in environments with unbounded state spaces. Recently, Lyle et al. (2023) argued that large-scale targets can reduce trainability. Thus, data-scaling techniques are often used in deep RL (e.g., Schulman et al. 2017). Scaling the targets, for example, the rewards, (Engstrom et al. 2020) or the TD errors (Schaul et al. 2021), and scaling the observations (e.g., normalization, Andrychowicz et al. 2020) are well-established strategies that have shown success and are incorporated into widely used algorithms such as PPO (Schulman et al. 2017) and A2C (Mnih et al. 2016), helping improve their performance and stability (Rao et al. 2020, Huang et al. 2022a). The problem of learning data scaling for reinforcement learning has been studied before, and mechanisms other than observation or reward normalization have been introduced. For example, van Hasselt et al. (2016) proposed a method that adaptively normalizes the targets used in the learning updates, allowing the agent to learn robustly across many orders of magnitude. However, estimating target scaling as part of the optimization process can be more involved, and therefore, simple normalization techniques might be desirable."}, {"title": "3.5 Stable streaming deep reinforcement learning methods", "content": "Here, we combine the above techniques to provide novel streaming deep reinforcement learning algorithms, which we call stream-x algorithms. We use the following color scheme for better readability: purple for layer normalization, blue for observation normalization, orange for reward scaling, teal for step size scaling, and brown for sparse initialization."}, {"title": "4 Experiments", "content": "In this section, we demonstrate the effectiveness of our stream-x algorithms. We start by showing stream barrier in different challenging environments where classic methods fail, but our stream-x algorithms overcome this barrier and are competitive with other batch methods. We study stream AC(\u03bb) and compare it against classic AC(\u03bb), PPO, SAC, PPO1 (streaming version of PPO), and SAC1 (streaming version of SAC) in MuJoCo and DM Control environments. Next, we study stream Q(x) and compare it against their classic versions in addition to DQN and DQN1 (streaming version of DQN) in MinAtar and the Atari 2600 arcade environments. We then demonstrate the importance of each component in our approach with a thorough ablation study. Finally, we study stream TD(\u03bb) and show its effectiveness in time series prediction with real-life data. We focus in this section on the key results here and give the full experimental details in Appendix F."}, {"title": "4.1 Overcoming stream barrier", "content": "Streaming deep RL methods often experience instability and failure to learn, which we refer to as stream barrier.  shows stream barrier in three different challenging benchmarking tasks: MuJoCo, DM Control, and Atari. The performance of each algorithm is averaged over 30 independent runs, each of 20M steps, on Mujoco and DM Control tasks and over 10 independent runs, each of 200M steps, on Atari tasks. The performance is shown as zero if some of the runs for an algorithm diverged. Classic streaming methods, namely Q(\u03bb) (Watkins 1989) and SARSA(\u03bb) (Rummery & Niranjan 1994), AC(\u03bb) (Williams 1992), perform poorly in these challenging tasks. Similarly, batch RL methods such as PPO, SAC, and DQN struggle when used in streaming learning, which is achieved with a buffer and a batch size of 1 and dubbed as PPO1, SAC1, and DQN1, respectively. Our stream-x methods not only overcome the stream barrier, that is, learn stably and effectively in these tasks, but also become competitive with batch RL methods and even outperform in some environments. For example,  shows the performance in the Dog environments where our stream AC(\u03bb) outperforms both PPO and SAC by large margins, achieving the best known performance of any model-free algorithm on this environment.  shows the performance in Atari Enduro game where stream Q(\u03bb) outperforms DQN even though it uses a fraction of the memory and compute required by DQN."}, {"title": "4.2 Sample efficiency of stream-x algorithms", "content": "Here, we study the sample efficiency of our stream-x methods by comparing the learning curves of different algorithms.  shows the performance of different deep RL methods on four continuous control MuJoCo tasks. We compare stream AC against the streaming variants PPO1 and SAC1 in addition to their original batch forms, PPO and SAC. We omit Classic AC from this comparison since we found it is extremely unstable such that even with a tiny step size (e.g., 10\u201311), it still diverges.  shows that stream AC with \u03bb = 0.8 outperforms PPO1 and SAC1 in all environments and is more sample efficient than PPO in Humanoid-v4, HumanoidStandup-v4, and Ant-v4. Our results present clear evidence contrary to the common belief that streaming methods ought to be sample inefficient.\nIn , we show the performance of stream Q(0.8) against its classic counterpart in addition to DQN1 and DQN on MinAtar tasks. In contrast to the previous experiment, neither classic streaming Q(0.8) nor DQN1 failed in MinAtar tasks. This matches with the observation made by Young & Tian (2019) where streaming deep RL methods succeeded in MinAtar. We hypothesize that the MinAtar tasks are not challenging enough to study stream barrier, which is observed in other benchmark tasks. Nonetheless, our stream Q(0.8) achieves performance comparable to DQN and better than DQN1 and classic Q(0.8) in most environments. Our results suggest that stream Q(0.8) is as sample efficient as DQN in MinAtar tasks. We repeat this experiment and the next with SARSA in Appendix G."}, {"title": "4.3 Stability of stream-x algorithms in extended runs", "content": "Next, we investigate the stability of our stream-x algorithms when running for an extended period. Such a setup is effective in revealing whether a method can be run for an extended period without any issues. Dohare et al. (2023, also see 2024) studied this setting and showed that PPO experiences some amount of instability that may lead the performance to degrade. In , we compare stream AC against SAC, PPO, SAC1, and PPO1 in a number of MuJoCo and DM control tasks where the agents are run for 20M time steps. PPO indeed suffered performance degradation in all tasks and SAC in one of them."}, {"title": "4.4 Understanding the importance of each component in stream-x algorithms", "content": "Next, we investigate what makes stream-x algorithms perform well. First, we take stream AC(0.8) and remove each component to determine which contributes the most to performance. Specifically, we remove one of the following components one at a time: ObGD, observation normalization and reward scaling, layer normalization, and sparse initialization. We also compare these variants with classic AC(0.8). Second, we study the role of eligibility traces on performance by comparing stream AC(0) and stream AC(0.8) along with classic AC(0) and classic AC(0.8).\n shows an ablation on the components of stream AC(\u03bb). When we removed sparse initialization and replaced it with LeCun initialization (LeCun et al. 2002), the agent was still able to learn, but slower, confirming the role of sparse initialization in sample efficiency. When we removed layer normalization from stream AC, the performance suffered significantly in all environments, especially Hopper-v4 and Walker-v4. Finally,"}, {"title": "4.5 Learning how to predict the future", "content": "Lastly, we finish with temporal prediction using TD(\u03bb) (Sutton 1988). We use the electricity transformer temperature dataset (Zhou et al. 2021), which has 6 external load features to predict power consumption. The dataset provides the oil temperature readings, which correlate with the power consumption. The goal of the learner is to predict future temperatures, which would help anticipate future power consumption. The dataset is referred to as ETTm2 (see ), which represent 2 years worth of data measured every 15 minutes, resulting in a total of 2 year \u00d7 365 days \u00d7 24 hours \u00d7 4 times = 70,080 data-points. The environment provides the agent with an observation vector of the 6 feature in addition to the oil temperature from the previous time step. The goal of the agent is to predict future oil temperature using a general value function (GVF, Sutton et al. 2011). This is achieved by extending the return definition to include any scalar signal: $G_t = \\sum_{j=0} \\gamma^k C_{t+k+1}$, where $C_t$ is some scalar signal known as the cumulant. The cumulant can be chosen to be an entry to the observation vector to perform nexting (see Modayil et al. 2014). For example, a prediction with a horizon of 100 time steps approximately corresponds to a GVF with $\\gamma = 0.99$, since the prediction horizon is about $\\frac{1}{1-\\gamma}$ (see Sutton et al. 2011). In our problem, we use $\\gamma = 0.99$, corresponding to a prediction horizon of 25 hours into the future. To allow for such a far prediction horizon, the agent needs some form of encoded information about the history. Following Janjua et al. (2023), we construct memory traces of observations using exponential moving averages (also see Tao et al. 2023, Rafiee et al. 2023). Specifically, given the ith entry in the observation vector $O_{t,i}$ at time step t, we form the memory trace as $S_{t,i} = \\beta S_{t-1,i} + (1 - \\beta)O_{t,i}$, where $\\beta$ is a trace decay factor of 0.999. Those memory traces are used as the agent state in Algorithm 9."}, {"title": "5 Related works", "content": "Continual Learning. The goal of continual learning research is to develop algorithms that allow agents to keep learning, potentially forever. The two obstacles in continual learning are loss of plasticity (Dohare et al. 2024) and catastrophic forgetting (McCloskey & Cohen 1989, Hetherington & Seidenberg 1989). Loss of plasticity reduces the agent's ability to learn gradually over time, while catastrophic forgetting prevents it from retaining and utilizing past memories, ultimately hindering its performance improvement. Some works focus on maintaining plasticity primarily in nonstationary supervised learning (e.g., Dohare et al. 2024, Kumar et al. 2023, Lewandowski et al. 2023, Elsayed & Mahmood 2024, Lewandowski et al. 2024, Lee et al. 2024), whereas others focus on reinforcement learning (e.g., Delfosse et al. 2024, Xu et al. 2024, Ma et al. 2024, Lyle et al. 2024a, Lyle et al. 2024b, Lyle et al. 2023, Elsayed et al. 2024b). On the other hand, a few works (e.g., Elsayed & Mahmood 2024) address catastrophic forgetting and loss of plasticity at the same time. In particular, Elsayed & Mahmood (2024) addresses catastrophic forgetting by protecting useful weights from drastic change when using gradient-based updates. Another approach to address forgetting is to promote sparse representations. Sparse representations (e.g., Lan"}, {"title": "6 Limitations and future works", "content": "Although we have explored a few representative streaming RL algorithms, our approach is compatible with many other algorithms, such as double Q-learning (van Hasselt 2010), dueling Q-learning networks (Wang et al. 2016), noisy networks Q-learning (Fortunato et al. 2018), or even in the continuing setting (e.g., Naik et al. 2024). Our paper focuses on model-free methods, which are less sample-efficient compared to model-based ones; thus, a promising direction is to discover how the agent can incrementally learn a model of the environment to improve sample efficiency following the success of batch model-based methods (e.g., Hafner et al. 2023, Samsami et al. 2024, Liu et al. 2024). Another promising direction is to combine our approach with real-time recurrent learning (Williams & Zipser 1989) to handle partial observability, especially with the recent scalable approaches (e.g., Irie et al. 2024, Zucchet et al. 2024, Elelimy et al. 2024, Javed et al. 2023). In addition, we focus mainly on on-policy methods (except"}]}