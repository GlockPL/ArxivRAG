{"title": "The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators", "authors": ["Tzu-Heng Huang", "Catherine Cao", "Vaishnavi Bhargava", "Frederic Sala"], "abstract": "Large pretrained models can be used as annotators, helping replace or augment crowdworkers and enabling\ndistilling generalist models into smaller specialist models. Unfortunately, this comes at a cost: employing\ntop-of-the-line models often requires paying thousands of dollars for API calls, while the resulting datasets are\nstatic and challenging to audit. To address these challenges, we propose a simple alternative: rather than directly\nquerying labels from pretrained models, we task models to generate programs that can produce labels. These\nprograms can be stored and applied locally, re-used and extended, and cost orders of magnitude less. Our system,\nAlchemist, obtains comparable to or better performance than large language model-based annotation in a range\nof tasks for a fraction of the cost: on average, improvements amount to a 12.9% enhancement while the total\nlabeling costs across all datasets are reduced by a factor of approximately 500\u00d7.", "sections": [{"title": "Introduction", "content": "One of the most exciting developments in machine learning is the use of large pretrained models to act as annotators\nor labelers [1; 2; 3; 4; 5; 6; 7; 8]. This includes the use of large language models (LLMs) like GPT-4 [9] and Claude\n3 [10]. This process offers multiple benefits. First, pretrained models are an efficient way to annotate and have\nthe potential to partially or fully replace expensive human crowdworkers [2; 6]. Second, this approach allows for\ndistilling large models into smaller, task-specific models that can be deployed locally at lower cost [3; 11; 7; 8].\nThis is additionally important in settings like healthcare and finance where privacy laws require the use of local\nmodels.\nDespite this promise, pretrained model-based annotation has several drawbacks that stymie its adoption. These\ndrawbacks include\n\u2022 High Cost: Labeling a dataset can be expensive. This is particularly so in cases where each data point consists of\nmany tokens. For example, we find that labeling a moderately-sized dataset [12] with 7,569 data points using\nGPT-4 costs over $1,200.\n\u2022 Lack of Extensibility: Making even small changes to specifications necessitates re-running the entire pipeline to\nobtain new labels. This inflexibility means the resulting labels are static.\n\u2022 Inability to Audit: API access to pretrained models does not permit inspecting most aspects of the model. Users\nmust simply accept the provided labels with only minimal additional information. Techniques that ask the model\nfor explanations for its decisions may not be reliable [13; 14; 15].\nWe address these obstacles through a simple but surprisingly powerful notion. Rather than having pretrained models\nlabel data, we task language models to generate programs that can output labels. These synthesized programs\nserve as annotators, capturing the underlying logic used by the models when annotating. In other words, instead\nof distilling a powerful model to label a dataset (and subsequently training a smaller model on the labeled data),\nwe distill directly into code (Figure 1). These resulting programs can either make predictions directly or can label\ntraining dataset then train a downstream model using it\u00b9.\nThis simple notion resolves all of the challenges related to pretrained model-based annotation. First, API calls scale\nwith the number of programs instead of the number of data points. That is, since we generate programs that can"}, {"title": "Related Work", "content": "Our work relates to LLM-based annotation, prompting, and the weak supervision framework.\nUsing Large Pretrained Models for Data Annotation. Large pretrained models have demonstrated powerful\ncapabilities using zero-shot prompting across a wide range of tasks [1]. One promising development is their potential\nto serve as data labelers, which can reduce the cost and human effort in data labeling [1; 2]. Existing research in\nthis area mainly focuses on approaches that allow for more efficient inference, enhanced label generation, and\ndistilling into smaller but specialized labelers [3; 4; 5; 6; 7; 8]. However, scalability is the main limitation in these\napproaches, as making inferences via querying an API for data examples can be cost-prohibitive. To tackle this\nchallenge, rather than prompting for labels repetitively, we propose prompting pretrained models for programs that\nuse synthesized labeling logic and can thus serve as alternative data labelers.\nPrompt Engineering & In-Context Learning. In-context learning adapts pretrained models to new tasks without\nadditional fine-tuning [1]. It involves providing relevant examples as demonstrations to solve the task, such as pairs\nof languages for translation [20]. By including task-specific examples, models can better understand the task at"}, {"title": "Alchemist System", "content": "We begin by presenting a general annotation workflow in Alchemist, followed by a detailed discussion of each key\nstep.\nGeneral Workflow. The process is depicted in Fig. 2. First, users select an unlabeled dataset and create simple\nprompts to instruct language models to generate programs that incorporate labeling logic. These prompts can\nintegrate relevant information and may vary in their design, allowing for the synthesis of multiple programs. Next,\ngiven a set of generated programs and their outputs, we apply weak supervision techniques to obtain a set of\naggregated labels. Finally, the labeled points can be used to train a distilled model that can be stored and used\nlocally.\n3.1 Prompting Strategy\nWe propose a general and extensible prompt template for querying language models to generate annotator programs.\nThis general template consists of three key components:\n\u2022 Task Description: Provides the model an overview of generated program's desired objectives.\n\u2022 Labeling Instructions: Specifies classes and the expected structure of the program's output.\n\u2022 Function Signature: Describes the function's name and the input types to be used.\nThis simple but general template allows for flexible incorporation of various types of information, enabling the\nmodel to generate programs that are tailored to specific requirements. Two sample prompt templates in Alchemist\nare displayed in Fig 1.\nUsing Supplementary Information. Drawing inspiration from few-shot prompting [35; 1], where users provide\ndemonstrations (i.e., data points with their labels) to enhance generated responses, we explore various types of\nsupplementary information that can be integrated to assist models in synthesizing programs. This approach is\nparticularly useful for scenarios where language models may lack the expertise to generate effective programs, or\nwhere specific adaptations in labeling logic are required. Such information can be crafted by users themselves,\ndomain experts or, more efficiently, generated by language models themselves. Additionally, it can be combined\nwith retrieval-augmented generation (RAG) systems [36; 37] to access external knowledge.\nWe explore various types of supplementary information to assist in code generation, starting with high-level\nconcepts and then progressively looking into more practical details to control programs."}, {"title": "Dataset Synthesis", "content": "While generated programs can efficiently annotate data, these programs may produce outputs that are noisy or\ninaccurate. However, as such programs may employ different techniques, such as pattern-matching, heuristic rules,\nor other approaches\u2014each with its own strengths and limitations\u2014there may be complementary signal in their\noutputs. This means we can aggregate them to mitigate the impact of noise. To do so, we apply weak supervision\ntechniques [16; 17; 18; 19]. This process starts by learning a model of the reliabilities of the programs. Once\nlearned, this model enables aggregating label outputs from different programs into high-quality pseudolabels.\nAlchemist is compatible with a variety of weak supervision aggregation models, called label models, providing\nflexibility in the choice of the weak supervision approach. For simplicity, in this work, we focus on using the\nSnorkel framework [17], which is a standard and widely-used approach in the weak supervision community."}, {"title": "Extensions: Handling Complex Modalities.", "content": "Crafting programs that operate over text is relatively easy for large language models. More complex data modalities,\nhowever, can be far more challenging. Consider images as an illustrative example. Even employing state-of-the-art\nmultimodal models, e.g., GPT-4o [38] and GPT-4V [9], to seek programs operating over sample images may not\nproduce satisfactory results.\nTo address this challenge, we extend Alchemist's pipeline to include an intermediate step. Specifically, we convert\nthe raw data (i.e., in our example, image pixels) into a set of features representing high-level concepts. These\nconcepts are obtained by prompting a language model (or, potentially, a multimodal model) to identify task-relevant\nnotions. For example, for a bird categorization task, models may identify \u201cwing shape,\u201d \u201cbeak shape,\u201d or \u201cfoot type\u201d\nas informative concepts for distinguishing between bird species. Next, we use any open-source local multimodal\nmodel, like CLIP [39], as a feature extractor for the identified concepts, producing low-dimensional feature vectors\nthat can be easily ingested by generated programs. As such models are free, this does not increase our cost.\nThis simple\napproach can be applied to any data modality where we have access to a local multimodal model (i.e., a model\noperating on the modality of interest and text)."}, {"title": "Experiments", "content": "We study the capability of Alchemist empirically. Our goals are to validate the following claims:\n\u2022 Cost Reduction and Improved Performance (Sec. 4.1): Alchemist can reduce cost by orders of magnitude,\nwhile producing labels of similar or better accuracy.\n\u2022 Extendibility to Other Modalities (Sec. 4.2): Alchemist can operate with modalities beyond text.\n\u2022 Use of Supplementary Information (Sec. 4.3): Incorporating relevant information into prompts enables the\ngeneration of better programs, yielding more accurate pseudolabels.\n\u2022 More Diverse Programs Can Help (Sec. 4.4): Increasing the diversity of generated programs created by different\nlabeling logic enables better pseudo labels.\n\u2022 Comparing to Human-crafted Programs (Sec 4.5): Synthesized programs may be more effective in comparison\nto human-crafted ones.\nDatasets. We include diverse datasets covering text and image modalities. For text, we include eight datasets\nthat span three different types of language tasks. These include the YouTube [41], SMS [42] datasets for spam\nclassification, IMDb [43], Yelp [43], Finance [44], and French [45] datasets for sentiment analysis, and the\nMedAbs [46] and Cancer [12] datasets for topic classification. We note that the Finance, French, MedAbs, and\nCancer datasets are relatively challenging, with points that require a degree of domain expertise for accurate labeling.\nFor example, the French dataset requires a good understanding of the language. These may pose challenges for\npretrained models."}, {"title": "Cost Reduction and Improved Performance", "content": "Setup. We open our evaluation of Alchemist with text domain datasets and use GPT-3.5 to generate programs. For\neach dataset, we input pure prompts without supplementary information into GPT-3.5 and generate 10 programs\nto use. We construct training datasets by aggregating the programs' outputs into pseudolabels with the weak\nsupervision framework Snorkel [17]. We then train a two-layer MLP as a distilled model. We run five times with\ndifferent random seeds and report their average performance. As our main baseline, we directly use language\nmodels to produce annotations per point. The resulting labels are used to train a distilled model for comparison.\nExpected Results. We anticipate that Alchemist can generate programs that can produce accurate labels while\nsubstantially reducing the expense of API calls.\nResults. Table 1 presents the distilled model's performance on each testing dataset. We observe that label accuracy is\nimproved on five out of eight datasets, particularly in challenging settings such as the MedAbs, Cancer, and French\ndatasets, outperforming the baseline zero-shot prompting approach. We also report the estimated costs of building\ntraining datasets. The costs for zero-shot prompting depend on the number of tokens for the dataset. In contrast,\nAlchemist only prompts 10 programs for each task, resulting in a significant reduction in the costs by orders of\nmagnitude. This efficiency is the main advantage of Alchemist, as it allows for the creation of high-quality datasets\nwith minimal expense. We include ablation studies with other weak supervision models within the Alchemist\nframework in Appendix C. They successfully demonstrate the flexibility and robustness of using Alchemist."}, {"title": "Extending Alchemist to Other Modalities", "content": "Setup. Next, we validate the extension of Alchemist to richer modalities. We consider our approach, where we\nprompt a multimodal model such as GPT4o and Claude 3, to generate high-level task-specific concepts. We extract\nfeatures for these concepts by employing CLIP as our local feature extractor. This converts raw pixels into feature\nvectors for the extracted high-level concepts, producing a set of similarity scores. Armed with these scores, we\ndescribe scores associated with their concepts in prompts and ask GPT4o and Claude 3 for 10 programs. As before,\nwe use Snorkel as our aggregation procedure.\nBaselines. We study two baselines. The first is the vanilla version of Alchemist, where we directly ask GPT40\nand Claude 3 to produce code that can operate on images (see left program in Fig. 4). The second is simple\nzero-shot prompting using CLIP, along with a variant, a group prompting approach that assumes access to spurious\ninformation and adds it to the given prompt\u00b2.\nExpected Results. We expect employing our two-step process can enable tractable program generation. In addition,\nwe hypothesize that programs generated in this way are beneficial in targeting salient concepts and reducing the"}, {"title": "Use of Supplementary Information", "content": "Setup. We test how integrating relevant information into the prompt context can augment generated programs.\nInstead of manually crafting supplementary information, we harness the power of language models to generate and\nintegrate. This approach is useful for challenging datasets where users may not have the necessary knowledge or\nexpertise to start. We evaluate the effectiveness of this approach, by comparing label model performance using\nprograms generated by two different methods: pure prompting and in-context prompting. In-context prompting\ninvolves supplementary information, while pure prompting relies solely on the task description without any\nadditional guidance. We employ GPT-3.5, GPT-4 and Claude 3 as our program sources and synthesize ten for each"}, {"title": "More Diverse Programs Can Help", "content": "Setup. As shown in Table 3, incorporating different supplementary information results in varying degrees of\nadditional improvement. Potentially, certain sets of supplementary information allow the model to specialize\nbetter on certain data points than others. We seek to achieve these performance improvements without the need to\nre-prompt the model with each set of supplementary information. Instead, we collect previously generated programs\nto obtain a set of programs with greater diversity. We ask: can Alchemist achieve better performance by modeling\nmore diverse programs?\nWe randomly select a set of programs from each category, collect them, and train the label model with their program\noutputs. Additionally, we increase the number of sampled programs in each category from 4 to 9. We test this\napproach on the datasets where Alchemist gives comparable or lower performance than zero-shot prompting in our\ninitial experiments in Table 1, namely the SMS, Yelp, and IMDb datasets.\nExpected Results. By obtaining more diverse programs to use, Alchemist can capture a wider range of perspectives\nand labeling logic, potentially leading to more accurate pseudolabels.\nResults. Fig. 5 visualizes the effect on the label model's performance when we increase the diversity in collected\nprograms. It demonstrates a trend and indicates that involving a more diverse set of programs can help to mitigate\nthe impact of individual strategy biases or limitations, leading to the production of better labels.\nOverall, results in Sec. 4.3 and in Sec. 4.4 underscore that the use of supplementary information and involving\ndiverse types of programs can help achieve better performance."}, {"title": "Comparing to Human-crafted Programs", "content": "Setup. Lastly, we compare synthesized programs in Alchemist and manually crafted labeling functions in\nWRENCH [48], which is a widely-used benchmark for evaluating weak supervision methods. We focus on\nthe datasets that overlap between Alchemist and WRENCH. For each dataset, we use pure prompts to query\nGPT-3.5, GPT-4, and Claude 3 for 10 programs. We then evaluate the performance of the distilled model for both\nmethods. We also include the label model's coverage in our comparison. Higher coverage means that label model\ncan produce more pseudolabels, yielding a larger size of training dataset to use.\nExpected Results. We expect that synthesized programs may offer some advantages in terms of efficiency and\neffectiveness compared to human-designed ones.\nResults. Table 4 presents their comparison. By leveraging the knowledge and capabilities of language models,\nwe find that generated programs offer several advantages, including better coverage (i.e., the ability to label more\ndata points) and comparable, or even better, performance. Generated programs can reduce the need for laborious\nengineering, which can be time-consuming and often requires a tedious design process to fine-tune labeling logic,\nsuch as thresholds and keyword usage. This design process may lead to many undiscovered rules, resulting in lower\nperformance on coverage and potentially limiting the effectiveness of the labeling functions\u2014unlike synthesized\nprograms.\nThis is particularly evident in the SMS dataset, where WRENCH requires 73 manually crafted labeling functions to\nobtain high-quality labels, while Alchemist only needs 10 generated programs to obtain comparable performance\nand higher coverage. This significant reduction highlights the potential of Alchemist to assist humans in designing\nlabeling functions and make it more accessible to users without extensive domain expertise."}, {"title": "Conclusion", "content": "We propose an alternative approach to costly annotation procedures that require repeated API requests for labels.\nOur solution introduces a simple notion of prompting programs to serve as annotators. We developed an automated\nlabeling system called Alchemist to embody this idea. Empirically, our results indicate that Alchemist demonstrates\ncomparable or even superior performance compared to language model-based annotation, improving five out\nof eight datasets with an average enhancement of 12.9%. Notably, Alchemist reduces total costs by a factor of\napproximately 500. Furthermore, we showcase the system's extensibility to handle more complex modalities while\nenhancing the robustness of predicted labels. Finally, we confirm that incorporating relevant information can\ngenerate better programs, and increasing diversity leads to obtaining higher-quality labels."}, {"title": "Discussion", "content": "Limitations. There are two primary limitations in Alchemist. First, the performance of the datasets we test is\nstill dependent on the capabilities of the language model. If the language model's ability to comprehend the given\ntask and generate effective programs is subpar, the labeling performance may suffer. The second limitation arises\nwhen dealing with extremely complex tasks. As the complexity of the task increases, the generated code may\nbecome longer, more intricate, and harder to understand, posing challenges for developers who take time to validate\ncorrectness.\nBroader Impacts. We do not see explicit negative impacts in Alchemist's annotation process. However, generated\nprograms from language models may contain biased labeling logic, toxic content, or malicious functions. To\nmitigate this, auditing and guardrails may be necessary."}]}