{"title": "Global-Local Medical SAM Adaptor Based on Full Adaption", "authors": ["Meng Wang", "Yarong Feng", "Yongwei Tang", "Yuxin Liang", "Tian Zhang", "Chao Lv*"], "abstract": "Emerging of visual language models, such as the segment anything model (SAM), have made great breakthroughs in the field of universal semantic segmentation and significantly aid the improvements of medical image segmentation, in particular with the help of Medical SAM adaptor (Med-SA). However, Med-SA still can be improved, as it fine-tunes SAM in a partial adaption manner. To resolve this problem, we present a novel global medical SAM adaptor (GMed-SA) with full adaption, which can adapt SAM globally. We further combine GMed-SA and Med-SA to propose a global-local medical SAM adaptor (GLMed-SA) to adapt SAM both globally and locally. Extensive experiments have been performed on the challenging public 2D melanoma segmentation dataset. The results show that GLMed-SA outperforms several state-of-the-art semantic segmentation methods on various evaluation metrics, demonstrating the superiority of our methods.", "sections": [{"title": "I. INTRODUCTION", "content": "The development of artificial intelligence has significantly advanced the field of computer vision, with the presentation of various visual language models. These models have been instrumental in bridging the gap between vision and language, enabling machines to understand multiple types of prompts and generate natural language descriptions related to visual content. Among these models, SAM [1] (Segment Anything Model), which is a powerful visual language model developed by Meta AI (formerly Facebook AI Research), stands out for its remarkable capabilities in universal segmentation. Its recent extensions have revolutionized the field of image segmentation. By combining SAM's power of zero-shot generalization ability and Parameter-Efficient Fine-Tuning (PEFT) [2] Methods, such as Adaption or LoRA [3], they outperform state-of-the-art semantic segmentation methods in many tasks with comprehend images.\nThe advent of SAM and its extensions have paved the way for numerous applications in various domains, particularly in the field of medicine. Medical SAM adaptor (Med-SA) [4], an advanced segmentation model based on SAM, has been developed to address the challenges of 2D and 3D medical image segmentation. This model leverages the zero-shot generalization ability of SAM in semantic segmentation and is specifically optimized for medical imaging applications. However, despite its remarkable capabilities, the Medical SAM adaptor has certain limitations. One of the key drawbacks is that each adaptor module independently adjusts the network's continuous layers, instead of jointly adjusting the entire network. This limitation restricts the Med-SA's ability to fine-tune the model's performance on specific tasks, as it may be difficult to focus on some specific layers that require more global attention.\nIn this paper, to address the limitations of the Medical SAM adaptor in local adaptation of the Segment Anything Model, we introduce a novel Global Medical SAM Adaptor (GMed-SA), which globally adapts SAM. To further enhance performance, we combine GMed-SA with Med-SA to propose a global-local medical SAM adaptor (GLMed-SA), which adapts SAM both globally and locally. We also conduct extensive experiments on a challenging public 2D melanoma segmentation dataset and a clinically collected histopathology images with epithelial cancer cells. In summary, the key contributions include:\n\u2022 The introduction of a novel Global Medical SAM Adaptor (GMed-SA) that globally adjusts SAM'S parameters to better accommodate the characteristics of medical images and the requirements of segmentation tasks."}, {"title": "II. RELATED WORKS", "content": "Interactive segmentation has become an influential method in the realm of visual comprehension, empowering models to produce a wide range of precise segmentation masks in response to user input. This method has seen a surge in interest across multiple fields, including computer vision and medical image processing, due to its effectiveness in managing ambiguity and offering detailed control over segmentation outcomes. Pioneering studies such as DIOS [5] showcased the promise of interactive segmentation for object delineation. However, it is the more contemporary approaches like SAM [1], that have significantly propelled the field forward by integrating advanced techniques such as prompt engineering, confidence scoring, and multi-pointer interactions, resulting in more reliable and precise segmentation.\nThe advent of SAM introduced a groundbreaking concept known as promptable segmentation. In this novel paradigm, the model is tasked with generating a valid segmentation mask in response to any form of segmentation prompt be it points, bounding boxes, masks, or even free-form text. This innovation opens up avenues for zero-shot and few-shot learning across new tasks and datasets, positioning promptable segmentation as a frontier for foundational model development in computer vision.\nBuilding on the success of SAM, subsequent advancements like Efficient SAM and Mobile SAM [6] have emerged, focusing on optimizing the original SAM architecture for efficiency and mobile deployment, respectively. These extensions aim to broaden the applicability of SAM by reducing computational demands and enabling real-time segmentation on resource-constrained devices. Efficient SAM [7] streamlines the model's architecture to enhance speed without compromising much on accuracy, while Mobile SAM [6] is specifically designed for mobile platforms, ensuring that the benefits of SAM's interactive segmentation capabilities are accessible even in portable and embedded systems. Together, these developments underscore the versatility and potential of SAM-based models in the evolving landscape of computer vision technologies."}, {"title": "Parameter-Efficient Fine-Tuning:", "content": "Parameter-efficient fine-tuning techniques have emerged as an efficient strategy for adapting pre-trained models to specific tasks with minimal computational cost. Methods like Adaption [3] and LoRA have demonstrated significant success in natural language processing (NLP) and computer vision. These techniques allow for fine-tuning large pre-trained models using only a small number of additional parameters, while preserving the generalization capabilities of the model. Based on these methods, Med-SA updates only a small fraction (2%) of SAM'S parameters while achieving significant improvements in medical image segmentation. This is realized by introducing two key innovations, including Space-Depth Transpose (SD-Trans) and Hyper-Prompting Adapter (HyP-Adpt). SD-Trans adapts 2D SAM to 3D medical images. SD-Trans effectively captures the spatial and depth correlations in 3D medical data, enabling more accurate segmentation of complex structures. HyP-Adpt facilitates prompt-conditioned adaptation. It utilizes the visual prompt to generate weights that are applied to the adaptation embedding, allowing the model to effectively incorporate domain-specific medical knowledge and improve segmentation performance."}, {"title": "III. METHOD", "content": "In this section, we first review SAM and Med-SA, then describe the design of GMed-SA, and finally explain how they are integrated to form GLMed-SA."}, {"title": "A. Preliminary", "content": "The architecture of SAM comprises three main parts including an image encoder, a prompt encoder, and a light-weight mask decoder. The image encoder is a MAE pre-trained vision transformer with 14\u00d714 window-sized attention and 4 equally-spaced self-attention blocks. The prompt encoder for mask prompts, points and texts are based on convolutional modules, the off-the-shelf CLIP tokens and sum of positional encoding and learnable embedding. The mask decoder employs a two-way image-prompt cross attention transformer decoder followed by a dynamic MLP to output tokens for semantic predictions and IOU scores. SAM employs click prompts and bounding box (BBox) prompts during training, and it combines random and iterative click sampling strategies to simulate user interaction and improve the model's robustness and accuracy."}, {"title": "B. Medical SAM adaptor", "content": "Instead of fully adjusting all parameters, Med-SA is designed to address the limitations of SAM in medical image segmentation by adapt SAM minimally. It utilizes a parameter-efficient fine-tuning technique called Adaption to update only a small fraction of SAM`s parameters, resulting in efficient and effective adaptation for domain-specific medical knowledge.\nMed-SA maintains the pre-trained SAM parameters frozen, designs a module known as the Adapter, which is to be integrated into specified locations of SAM. This Adapter functions as a bottleneck architecture, comprising a sequence of operations: a down-projection step, a ReLU activation function, followed by an up-projection step. The down-projection step reduces the dimension of the input embedding via a straightforward MLP layer. Conversely,"}, {"title": "C. Global Medical SAM adaptor", "content": "Regardless of the performance gain by Med-SA, as depicted in figure 1 (b), it adapts SAM locally. This is indicated by the aforementioned adapters including MLP-A, MHA-A, and HyPAdpt, which are responsible for fine-tuning several sequential layers in the whole SAM architecture. To adapt SAM from a global perspective view, we propose Global Medical SAM Adaptor (i.e., GMed-SA), which is a novel adaptation framework designed to enhance the segmentation capabilities of the Segment Anything Model for medical images. We term this adaption technique as full adaption as it adapts the neural network globally. In contrast to partial adaption of Med-SA, GMed-SA focuses more on global adjustments to improve SAM`s performance with minimal effort.\nGMed-SA modifies the output embedding of the pre-trained SAM model to better align with the characteristics of medical images and segmentation tasks. This can be achieved through full adaption. The Adapter Modules can be described as follows:\nGMed-SA consists of a down-projection to reduce dimension, a ReLU activation, and an up-projection to restore the original dimension. In the encoder: Placed in the residual path of each ViT block. It serves to adapt and fine-tune the embeddings processed by the ViT blocks in the Image encoder, potentially acting as a bottleneck to improve parameter efficiency and adaptability. By updating GMed-SA parameters while freezing the majority of the pre-trained model, GMed-SA can achieve efficient adaptation without compromising performance."}, {"title": "D. Global-Local Medical SAM adaptor", "content": "Due to the complementary characteristic of GMed-SA and Med-SA, we propose a global-local medical SAM adaptor (i.e., GMed-SA), which perform parameter-efficient fine-tuning using both full and partial adaption. The schematic diagram is depicted in figure 1 (b). as seen, GMed-SA is based on Med-SA, and it additionally adds GMed-SA to each ViT block of Med-SA. Note that in training, parameters of SAM are frozen, and only those of adapters are updated simultaneously."}, {"title": "IV. EXPERIMENTAL RESULTS AND ANALYSIS", "content": "We describe the experimental setup, including the datasets, evaluation metrics, training strategy, and implementation details. Experimental results comparing GLMed-SA with other state-of-the-art semantic segmentation methods on the challenging 2D melanoma segmentation dataset and histopathology images with epithelial cancer cells are shown and analyzed in terms of various evaluation metrics."}, {"title": "A. Experimental Datasets and Evaluation Metrics", "content": "For the segmentation of melanoma, we conducted experiments using the publicly accessible ISIC 2019 dataset. Following Med-SA, the segmentation performance was evaluated using Dice similarity coefficient (Dice) and intersection of union (IoU). The input resolution is 1024x1024."}, {"title": "B. Training Strategy for Interactive Segmentation", "content": "Our approach to interactive segmentation involves utilizing click prompts during the training phase. Following Med-SA, the click prompt generation revolves around the concept of using positive clicks to denote foreground areas and negative clicks for background regions. A hybrid strategy of random and iterative click sampling is used to train the model with these prompts. Initially, we start with random sampling to initialize the prompts, followed by an iterative approach that adds a few more clicks. This iterative method mimics real-user interaction by placing new clicks in areas where the model's predictions, based on previous clicks, have been incorrect."}, {"title": "C. Implementation Details", "content": "For this research, we adopt Med-SA pipeline which is built on the official ViT-H [9] SAM GitHub repository. For training on 2D medical images, we maintained the default training configurations of Med-SA. All the involved datasets were subjected to a training regimen of 20 epochs for GLMed-SA. We selected fewer epochs compared to that of Med-SA because our model appeared to converge more rapidly in the default setup of Med-SA. In the interactive model, the prompt configuration comprises a single random positive point, referred to as \"1-point\".\nAll experimental procedures were carried out using the PyTorch framework and executed on a single NVIDIA A800 GPU. We stuck to the default settings for replicating the comparison methods."}, {"title": "V. DISCUSSION", "content": "By comparing GLMed-SA with other state-of-the-art semantic segmentation methods on the challenging 2D melanoma segmentation dataset, we find that the performance of GLMed-SA in terms of various evaluation metrics has the advantages over existing approaches. This may owe to the following reasons brought by GMed-SA:\nParameter Efficiency: GMed-SA requires only a small fraction of the parameters to be updated, significantly reducing computational cost and memory usage compared to full fine-tuning.\nGeneralization: The global embedding adjustments in GMed-SA enable the model to better generalize to diverse medical image modalities and segmentation tasks, enhancing its applicability in real-world scenarios.\nEase of Implementation: GMed-SA is relatively straightforward to implement and can be seamlessly integrated into the existing SAM framework, making it accessible for researchers and practitioners.\nBy leveraging the power of SAM and the efficiency of global parameter adjustments, GMed-SA presents a promising approach for advancing medical image segmentation and unlocking new possibilities in clinical applications."}, {"title": "VI. CONCLUSION", "content": "By leveraging the power of partial adaption and full adaption, GLMed-SA presents a promising approach for advancing medical image segmentation and unlocking new possibilities in clinical applications. The characteristics of GLMed-SA enable it can be applied to many potential applications, such as cell segmentation in histopathology images, facilitating quantitative analysis and disease characterization. Future research could explore the following directions to further enhance GMed-SA, including Investigating different PEFT techniques and Adapter designs to optimize performance and efficiency, integrating domain-specific knowledge and prior information into GMed-SA to improve segmentation accuracy in specific medical applications, and extending GMed-SA to handle multi-modal medical images, combining information from different imaging modalities to achieve more comprehensive segmentation results."}]}