{"title": "Ask in Any Modality:\nA Comprehensive Survey on Multimodal Retrieval-Augmented Generation", "authors": ["Mohammad Mahdi Abootorabi", "Amirhosein Zobeiri", "Mahdi Dehghani", "Mohammadali Mohammadkhani", "Bardia Mohammadi", "Omid Ghahroodi", "Mahdieh Soleymani Baghshah", "Ehsaneddin Asgari"], "abstract": "Large Language Models (LLMs) struggle\nwith hallucinations and outdated knowledge\ndue to their reliance on static training data.\nRetrieval-Augmented Generation (RAG)\nmitigates these issues by integrating external\ndynamic information enhancing factual and\nupdated grounding. Recent advances in\nmultimodal learning have led to the devel-\nopment of Multimodal RAG, incorporating\nmultiple modalities such as text, images,\naudio, and video to enhance the generated\noutputs. However, cross-modal alignment and\nreasoning introduce unique challenges to Mul-\ntimodal RAG, distinguishing it from traditional\nunimodal RAG. This survey offers a structured\nand comprehensive analysis of Multimodal\nRAG systems, covering datasets, metrics,\nbenchmarks, evaluation, methodologies, and\ninnovations in retrieval, fusion, augmentation,\nand generation. We precisely review training\nstrategies, robustness enhancements, and loss\nfunctions, while also exploring the diverse\nMultimodal RAG scenarios. Furthermore,\nwe discuss open challenges and future\nresearch directions to support advancements\nin this evolving field. This survey lays the\nfoundation for developing more capable and\nreliable AI systems that effectively leverage\nmultimodal dynamic external knowledge\nbases.", "sections": [{"title": "1 Introduction & Background", "content": "In recent years, significant breakthroughs have\nbeen achieved in language models, driven primar-\nily by the advent of transformers (Vaswani et al.,\n2017), enhanced computational capabilities, and\nthe availability of large-scale training data (Naveed\net al., 2024). The emergence of foundational Large\nLanguage Models (LLMs) (Ouyang et al., 2022;\nGrattafiori et al., 2024; Touvron et al., 2023; Qwen\net al., 2025; Anil et al., 2023) has revolutionized\nnatural language processing (NLP), demonstrat-\ning unprecedented capabilities in a wide range of\ntasks including instruction following (Qin et al.,\n2024), sophisticated reasoning (Wei et al., 2024c),\nIn-context Learning (Brown et al., 2020), and mul-\ntilingual machine translation (Zhu et al., 2024a).\nThese advancements have elevated the performance\nof various NLP tasks, opening new avenues for re-\nsearch and application. Despite their remarkable\nachievements, LLMs face significant challenges,\nincluding hallucination, outdated internal knowl-\nedge, and a lack of verifiable reasoning (Huang\net al., 2024; Xu et al., 2024b). Their reliance on\nparametric memory restricts their ability to access\nup-to-date knowledge, making them less effective\nfor knowledge-intensive tasks compared to task-\nspecific architectures. Moreover, providing prove-\nnance for their decisions and updating their world\nknowledge remain critical open problems (Lewis\net al., 2020).\nRetrieval-Augmented Generation (RAG) RAG\n(Lewis et al., 2020) has emerged as a promising so-\nlution to these limitations by enabling LLMs to re-\ntrieve and incorporate external knowledge, improv-\ning factual accuracy and reducing hallucinations\n(Shuster et al., 2021; Ding et al., 2024a). By dy-\nnamically accessing vast external knowledge repos-\nitories, RAG systems enhance knowledge-intensive\ntasks while ensuring responses remain grounded in\nverifiable sources (Gao et al., 2023).\nIn practice, RAG systems operate through a\nretriever-generator pipeline. The retriever lever-\nages embedding models (Chen et al., 2024a; Rau\net al., 2024) to identify relevant passages from ex-\nternal knowledge bases and optionally applies re-\nranking techniques to improve retrieval precision\n(Dong et al., 2024a). These retrieved passages are\nthen passed to the generator, incorporating this ex-"}, {"title": "Task Formulation", "content": "A mathematical formulation\nof the general task for multimodal RAG is pre-\nsented in this section. These systems generate a\nmultimodal response, denoted as r, in response to\na multimodal query q.\nLet $D = {d_1,d_2, ..., d_n}$ be a multimodal cor-\npus. Each document $d_i \\in D$ is associated with a\nmodality $M_{d_i}$ and processed by a modality-specific\nencoder, yielding:\n$z_i = Enc_{M_{d_i}}(d_i)$"}, {"title": "2 Datasets and Benchmarks", "content": "Multimodal RAG research employs diverse\ndatasets and benchmarks to evaluate retrieval, in-\ntegration, and generation across heterogeneous\nsources. Image-text tasks, including captioning\nand retrieval, commonly use MS-COCO (Lin et al.,\n2014), Flickr30K (Young et al., 2014), and LAION-\n400M (Schuhmann et al., 2021), while visual ques-\ntion answering with external knowledge is sup-\nported by OK-VQA (Marino et al., 2019) and We-\nbQA (Chang et al., 2022). For complex multimodal\nreasoning, MultimodalQA (Talmor et al., 2021) in-\ntegrates text, images, and tables, whereas video-\ntext tasks leverage ActivityNet (Caba Heilbron\net al., 2015) and YouCook2 (Zhou et al., 2018).\nIn the medical domain, MIMIC-CXR (Johnson\net al., 2019) and CheXpert (Irvin et al., 2019) fa-\ncilitate tasks such as medical report generation. It\nis noteworthy that a number of these datasets are\nunimodal (e.g., solely text-based or image-based).\nUnimodal datasets are frequently employed to rep-\nresent a specific modality and are subsequently\nintegrated with complementary datasets from other\nmodalities. This modular approach allows each\ndataset to contribute its domain-specific strengths,\nthereby enhancing the overall performance of the\nmultimodal retrieval and generation processes."}, {"title": "3 Evaluation", "content": "Evaluating multimodal RAG models is complex\ndue to their varied input types and complex struc-\nture. The evaluation combines metrics from VLMs,\ngenerative AI, and retrieval systems to assess ca-\npabilities like text/image generation and informa-\ntion retrieval. Our review found about 60 different\nmetrics used in the field. More details, includ-\ning the formulas for the RAG evaluation metrics,\ncan be found in Appendix (\u00a7C). In the following\nparagraphs, we will examine the most important\nand widely used metrics for evaluating multimodal\nRAG.\nRetrieval Evaluation Retrieval performance is\nmeasured through accuracy, recall, and precision\nmetrics, with an F1 score combining recall and pre-\ncision. Recall@K, which examines relevant items\nin top K results, is preferred over standard recall.\nMean Reciprocal Rank (MRR) serves as another\nkey metric for evaluation, which is utilized by (Ad-\njali et al., 2024; Nguyen et al., 2024).\nModality Evaluation Modality-based evaluations\nprimarily focus on text and image, assessing their\nalignment, text fluency, and image caption quality."}, {"title": "4 Key Innovations and Methodologies", "content": "Modern multimodal RAG systems encode diverse\ninput modalities into a unified embedding space"}, {"title": "4.1 Retrieval Strategy", "content": "Modern multimodal RAG systems encode diverse\ninput modalities into a unified embedding space\nto enable direct cross-modal retrieval. Recent ad-\nvancements in CLIP-based (Radford et al., 2021)\nand BLIP-inspired (Li et al., 2022a) approaches\nhave driven the evolution of contrastive learning\nstrategies through novel multimodal retrieval archi-\ntectures and training methodologies (Zhou et al.,\n2024c; Wei et al., 2024b; Zhang et al., 2024i). As\nthese multi-encoder models project different modal-\nities into a shared latent space, multimodal RAGS\nrely on efficient search strategies to retrieve rele-\nvant external knowledge.\nMaximum inner product search (MIPS) variants\nare widely used for fast and direct similarity com-\nparisons (Tiwari et al., 2024; Wang et al., 2024c;\nZhao et al., 2023b). Systems such as MuRAG\n(Chen et al., 2022a) and RA-CM3 (Yasunaga et al.,\n2023) employ approximate MIPS to efficiently re-\ntrieve the top-k candidates by maximizing the inner\nproduct between the query vector and a large collec-\ntion of image-text embeddings. Large-scale imple-\nmentations leverage distributed MIPS techniques,\nsuch as TPU-KNN (Chern et al., 2022), for high-\nspeed retrieval. Additionally, ScaNN (Scalable\nNearest Neighbors) (Guo et al., 2020), MAXSIM\nscore (Chan and Ng, 2008; Cho et al., 2024), and\napproximate KNN methods (Caffagni et al., 2024)\nhave been adopted for efficient similarity computa-\ntion.\nRecent advancements in MIPS optimization fo-\ncus on reducing retrieval latency and improving"}, {"title": "4.1.2 Modality-Based Retrieval", "content": "Modality-aware retrieval techniques optimize re-\ntrieval efficiency by leveraging the unique charac-\nteristics of each modality.\nText-Centric Retrieval This remains foundational\nin multimodal RAG systems, with both traditional\nmethods like BM25 (Robertson and Zaragoza,\n2009), and dense retrievers such as MiniLM (Wang\net al., 2020a) and BGE-M3 (Chen et al., 2024b)\ndominate text-based evidence retrieval (Chen et al.,\n2022b; Suri et al., 2024; Nan et al., 2024). Novel\napproaches also address the need for fine-grained\nsemantic matching and domain specificity: For in-\nstance, ColBERT (Khattab and Zaharia, 2020) and\nPreFLMR (Lin et al., 2024b) employ token-level\ninteraction mechanisms that preserve nuanced tex-\ntual details to improve precision for multimodal\nqueries, while RAFT (Zhang et al., 2024h) and\nCRAG (Yan et al., 2024) enhance retrieval perfor-\nmance by ensuring precise citation of text spans.\nVision-Centric Retrieval This focuses on directly\nleveraging image representations for knowledge ex-\ntraction (Kumar and Marttinen, 2024; Yuan et al.,\n2023). Systems such as EchoSight (Yan and Xie,\n2024) and ImgRet (Shohan et al., 2024) retrieve\nvisually similar content by using reference images\nas queries. In addition, composed Image Retrieval\n(CMI) models (Feng et al., 2023; Zhao et al., 2024;\nJang et al., 2024) enhance retrieval by integrat-\ning multiple image features into a unified query\nrepresentation. Similarly, Pic2word (Saito et al.,\n2023) maps visual content to textual descriptions,\nenabling zero-shot image retrieval.\nVideo-Centric Retrieval These methods extend\nvision-based techniques by incorporating tempo-\nral dynamics and large video-language models\n(LVLMs), driven by novel frameworks like iRAG\n(Arefeen et al., 2024), which introduces incremen-\ntal retrieval for sequential video understanding, and\nMV-Adapter (Jin et al., 2024), optimizing mul-\ntimodal transfer learning for video-text retrieval.\nRecent breakthroughs focus on long-context pro-\ncessing: Video-RAG (Luo et al., 2024b) leverages"}, {"title": "4.1.3 Re-ranking and Selection Strategies", "content": "Effective retrieval in multimodal RAG systems re-\nquires not only identifying relevant information but\nalso prioritizing retrieved candidates. Re-ranking\nand selection strategies enhance retrieval quality by\noptimizing example selection, refining relevance\nscoring, and applying filtering mechanisms.\nOptimized Example Selection These techniques\noften employ multi-step retrieval, integrating both"}, {"title": "4.2 Fusion Mechanisms", "content": "Models in this category utilize distinct strategies\nto align multimodal representations. Zhi Lim et al.\n(2024) convert text, tables, and images into a sin-\ngle textual format using a cross-encoder trained for\nrelevance scoring. Sharifymoghaddam et al. (2024)\nintroduce interleaved image\u2013text pairs that verti-\ncally merge multiple few-shot images (as in LLaVA\n(Liu et al., 2023a)), while aligning modalities via\nCLIP score fusion (Hessel et al., 2021) and BLIP\nfeature fusion (Li et al., 2022a). Riedler and Langer\n(2024), Wiki-LLaVA (Caffagni et al., 2024), C3Net\n(Zhang et al., 2024c), and MegaPairs (Zhou et al.,\n2024a), embed images and queries into a shared\nCLIP space.\nVISA (Ma et al., 2024b) employs the Docu-\nment Screenshot Embedding (DSE) model to align\ntextual queries with visual document representa-\ntions by encoding both into a shared embedding\nspace. REVEAL (Hu et al., 2023) injects retrieval\nscores into attention layers to minimize L2-norm\ndifferences between query and knowledge embed-\ndings, and MA-LMM (He et al., 2024) aligns video-\ntext embeddings via a BLIP-inspired Query Trans-\nformer (Li et al., 2022a). LLM-RA (Jian et al.,\n2024) concatenates text and visual embeddings\ninto joint queries to reduce retrieval noise, while\nRA-BLIP (Ding et al., 2024b) employs a 3-layer\nBERT-based adaptive fusion module to unify vi-\nsual-textual semantics. Xue et al. (2024) use a\nprototype-based embedding network (Zheng et al.,\n2023) to map object-predicate pairs into a shared se-\nmantic space, aligning visual features with textual\nprototypes. Re-IMAGEN (Chen et al., 2022b) bal-\nances creativity and entity fidelity in text-to-image\nsynthesis via interleaved classifier-free guidance\nduring diffusion sampling. To improve multimodal\nalignment, VISRAG (Yu et al., 2024) enhances\nalignment with position-weighted mean pooling on\nVLM hidden states, prioritizing later tokens for rel-\nevance, and RAG-Driver (Yuan et al., 2024) aligns\nvisual-language embeddings using visual instruc-\ntion tuning and an MLP projector."}, {"title": "4.2.2 Attention-Based Mechanisms", "content": "Attention-based methods dynamically weight cross-\nmodal interactions to support task-specific rea-\nsoning. EMERGE (Zhu et al., 2024b), MORE\n(Cui et al., 2024), and AlzheimerRAG (Lahiri and\nHu, 2024) integrate heterogeneous data via cross-\nattention. RAMM (Yuan et al., 2023) employs a\ndual-stream co-attention transformer, combining\nself-attention and cross-attention to fuse retrieved\nbiomedical images/texts with input data. RAG-\nTrans (Cheng et al., 2024) applies user-aware atten-"}, {"title": "4.2.3 Unified Frameworks and Projections", "content": "Unified frameworks and projection methods con-\nsolidate multimodal inputs into coherent represen-\ntations. Su et al. (2024a) employ hierarchical cross-\nchains and late fusion for healthcare data, while\nIRAMIG (Liu et al., 2024b) iteratively integrates\nmultimodal results into unified knowledge repre-\nsentations. M3DocRAG (Cho et al., 2024) flattens\nmulti-page documents into a single embedding ten-\nsor, and PDF-MVQA (Ding et al., 2024c) fuses\nRegion-of-Interest (RoI)-based and patch-based\n(CLIP) vision-language models (Long et al., 2022).\nDQU-CIR (Wen et al., 2024) unifies raw data by\nconverting images into text captions for complex\nqueries and overlaying text onto images for sim-\nple ones, then fusing embeddings via MLP-learned\nweights. SAM-RAG (Zhai, 2024) aligns image-\ntext modalities by generating captions for images,\nconverting the multimodal input into unimodal text\nfor subsequent processing. UFineBench (Zuo et al.,\n2024) utilizes a shared granularity decoder for ultra-\nfine text-person retrieval. Nguyen et al. (2024) in-\ntroduce Dense2Sparse projection, converting dense\nembeddings from models like BLIP/ALBEF (Li\net al., 2022a) into sparse lexical vectors using layer\nnormalization and probabilistic expansion control\nto optimize storage and interpretability."}, {"title": "4.3 Augmentation Techniques", "content": "Basic RAG systems typically follow a single re-\ntrieval step and pass retrieved content directly to the\ngeneration phase without further refinement, which\ncan lead to inefficiencies and suboptimal outputs.\nTo address this, augmentation techniques have been\nintroduced to refine retrieved data beforehand, im-\nproving multimodal interpretation, structuring, and\nintegration (Gao et al., 2023)."}, {"title": "4.3.1 Context Enrichment", "content": "This focuses on enhancing the relevance of re-\ntrieved knowledge by refining or expanding re-\ntrieved data. General approaches incorporate addi-\ntional contextual elements (e.g., text chunks, image\ntokens, structured data) to provide a richer ground-\ning for generation (Caffagni et al., 2024; Xue et al.,\n2024). EMERGE (Zhu et al., 2024b) enriches con-\ntext by integrating entity relationships and seman-\ntic descriptions. MiRAG (Adjali et al., 2024) ex-\npands initial queries through entity retrieval and\nreformulation, enhancing subsequent stages for\nthe visual question-answering. Video-RAG (Luo\net al., 2024b) enhances long-video understanding\nthrough Query Decoupling, which reformulates\nuser queries into structured retrieval requests to\nextract auxiliary multimodal context. Img2Loc\n(Zhou et al., 2024e) enhances accuracy by includ-\ning both the most similar and the most dissimilar\npoints from the database in the prompt, allowing\nthe model to rule out implausible locations for its\npredictions."}, {"title": "4.3.2 Adaptive and Iterative Retrieval", "content": "For more complex queries, dynamic retrieval mech-\nanisms have proven effective. Adaptive retrieval ap-\nproaches optimize relevance by adjusting retrieval\ndynamically. SKURG (Yang et al., 2023) deter-\nmines the number of retrieval hops based on query\ncomplexity. SAM-RAG (Zhai, 2024) and mR2AG\n(Zhang et al., 2024f) dynamically assess the need\nfor external knowledge and filter irrelevant con-\ntent using MLLMs to retain only task-critical in-\nformation. MMed-RAG (Xia et al., 2024a) fur-\nther improves retrieval precision by discarding low-\nrelevance results, while OmniSearch (Li et al.,\n2024b) introduces a self-adaptive retrieval agent\nthat dynamically decomposes complex multimodal\nquestions into sub-question chains and plans re-\ntrieval actions in real-time based on retrieved con-\ntent.\nIterative approaches refine results over multiple\nsteps by incorporating feedback from prior itera-\ntions. IRAMIG (Liu et al., 2024b) improves mul-\ntimodal retrieval by dynamically updating queries\nbased on retrieved content. Similarly, OMG-QA"}, {"title": "4.4 Generation Techniques", "content": "Recent advancements in multimodal RAG genera-\ntion focus on robustness, cross-modal coherence,\nand task-specific adaptability. These innovations\ncan be broadly categorized into these sections:"}, {"title": "4.4.1 In-Context Learning", "content": "In-context learning (ICL) with retrieval augmenta-\ntion enhances reasoning in multimodal RAGs by\nleveraging retrieved content as few-shot examples\nwithout requiring retraining. Models such as RMR\n(Tan et al., 2024), Sharifymoghaddam et al. (2024),\nand RA-CM3 (Yasunaga et al., 2023), extend this\nparadigm to multimodal RAG settings.\nRAG-Driver (Yuan et al., 2024) refines ICL by\nretrieving relevant driving experiences from a mem-\nory database, ensuring scenario-specific contextual\nalignment. MSIER (Luo et al., 2024a) further en-\nhances example selection through a Multimodal\nSupervised In-Context Examples Retrieval frame-\nwork, leveraging a foundation MLLM scorer to\nevaluate both textual and visual relevance. Mean-\nwhile, Raven (Rao et al., 2024) introduces Fusion-\nin-Context Learning, a novel approach that en-\nriches ICL by incorporating a more diverse set of\nin-context examples, leading to better performance\nthan standard ICL."}, {"title": "4.4.2 Reasoning", "content": "Structured reasoning techniques, such as chain-of-\nthought (CoT), decompose complex reasoning into\nsmaller sequential steps, enhancing coherence and\nrobustness in multimodal RAG systems. RAGAR\n(Khaliq et al., 2024) introduces Chain of RAG and\nTree of RAG to iteratively refine fact-checking\nqueries and explore branching reasoning paths for\nmore robust evidence generation.\nVisDoM (Suri et al., 2024) integrates CoT with\nevidence curation to ensure logical and contextual\naccuracy, while SAM-RAG (Zhai, 2024) employs\nreasoning chains alongside multi-stage answer ver-\nification to enhance the relevance, utility, and sup-"}, {"title": "4.4.3 Instruction Tuning", "content": "Several works have fine-tuned or instruct-tuned\ngeneration components for specific applications.\nRA-BLIP (Ding et al., 2024b) leverages the Q-\nFormer architecture from InstructBLIP (Dai et al.,\n2023) to extract visual features based on question\ninstructions, while RAGPT (Lang et al., 2025) em-\nploys a context-aware prompter to generate dy-\nnamic prompts from relevant instances. mR2AG\n(Zhang et al., 2024f) uses instruction tuning with\nthe mR2AG-IT dataset to train MLLMs to adap-\ntively invoke retrieval, identify relevant evidence,\nand generate accurate answers for knowledge-\nbased VQA tasks.\nRagVL (Chen et al., 2024d) employs instruc-\ntion tuning to enhance the ranking capability of\nMLLMs, serving them as a re-ranker for filtering\nthe top-k retrieved images. Jang et al. (2024) fo-\ncus on distinguishing image differences to generate\ndescriptive textual responses. MMed-RAG (Xia\net al., 2024a) applies preference fine-tuning to help\nmodels balance retrieved knowledge with internal\nreasoning.\nTo improve generation quality, MegaPairs (Zhou\net al., 2024a) and Surf (Sun et al., 2024a) construct\nmultimodal instruction-tuning datasets from prior\nLLM errors, while Rule (Xia et al., 2024b) refines\nMed-LVLM through direct preference optimization\nto mitigate overreliance on retrieved contexts."}, {"title": "4.4.4 Source Attribution and Evidence Transparency", "content": "Ensuring source attribution in multimodal RAG\nsystems is a key focus of recent research. Mu-\nRAR (Zhu et al., 2025) integrates multimodal data,\nfetched by a source-based retriever, to refine LLM's\ninitial response, ensuring informativeness. VISA\n(Ma et al., 2024b) uses large vision-language mod-\nels to generate answers with visual source attribu-\ntion by identifying and highlighting supporting evi-\ndence in retrieved document screenshots. Similarly,\nOMG-QA (Nan et al., 2024) ensures transparency\nby prompting the LLM to explicitly cite evidence\nin generated responses."}, {"title": "4.5 Training Strategies", "content": "Training multimodal RAG models involves a multi-\nstage process to effectively handle cross-modal in-\nteractions (Chen et al., 2022a). Pretraining estab-\nlishes the foundation using large paired datasets to\nlearn cross-modal relationships while fine-tuning\nadapts models to specific tasks by leveraging cross-\nmodal attention (Ye et al., 2019). For instance, RE-\nVEAL (Hu et al., 2023) integrates multiple training\nobjectives. Its pretraining phase optimizes Prefix\nLanguage Modeling Loss (LPrefixLM), where the\nmodel predicts text continuations from a prefix and\nan image. Supporting losses include Contrastive\nLoss (Lcontra) for aligning queries with pseudo-\nground-truth knowledge, Disentangled Regular-\nization Loss (Ldecor) to improve embedding ex-\npressiveness, and Alignment Regularization Loss\n(Lalign) to align query and knowledge embeddings.\nDuring fine-tuning, a cross-entropy objective trains\nthe model for tasks like VQA and image caption-\ning. Details of formulas for widely used RAG loss\nfunctions can be found in Appendix (\u00a7D)."}, {"title": "4.5.1 Alignment", "content": "Contrastive learning improves representation qual-\nity by pulling positive pairs closer and pushing\nnegative pairs apart in the embedding space. A\ncommon objective is the InfoNCE loss (van den\nOord et al., 2019), which maximizes the mutual in-\nformation between positive pairs while minimizing\nsimilarity to negatives. Several multimodal RAG\nmodels, such as VISRAG (Yu et al., 2024), Mega-\nPairs (Zhou et al., 2024a) and SAM-RAG (Zhai,\n2024) utilize InfoNCE loss to improve retrieval-\naugmented generation. Furthermore, EchoSight\n(Yan and Xie, 2024) enhances retrieval accuracy\nby selecting visually similar yet contextually dis-\ntinct negatives, while HACL (Jiang et al., 2024) im-\nproves generation by introducing hallucinative cap-\ntions as distractors. Similarly, UniRaG (Zhi Lim\net al., 2024) strengthens retrieval by incorporating\nhard negative documents, helping the model distin-\nguish relevant contexts from noise.\nThe eCLIP loss (Kumar and Marttinen, 2024)\nextends contrastive training by integrating expert-\nannotated data and an auxiliary Mean Squared Er-\nror (MSE) loss to refine embedding quality. Mixup\nstrategies further augment training by generat-\ning synthetic positive pairs, improving generaliza-\ntion in contrastive learning (Kumar and Marttinen,\n2024). Dense2Sparse (Nguyen et al., 2024) in-\ncorporates two unidirectional losses: an image-to-"}, {"title": "4.5.2 Generation", "content": "A key aspect of multimodal RAG is the generation\nability. Autoregressive language models are typ-\nically trained using Cross-Entropy Loss (Brown\net al., 2020). For image generation, widely used\napproaches include Generative Adversarial Net-\nworks (GANs) (Goodfellow et al., 2014) and Diffu-\nsion Models (Ho et al., 2020). GANs employ vari-\nous loss functions, such as Binary Cross-Entropy\nLoss, Minimax Loss, Wasserstein Loss (WGAN),\nand Hinge Loss. Diffusion Models utilize Mean\nSquared Error (MSE) Loss for noise prediction, a\ncommon approach in Denoising Diffusion Proba-\nbilistic Models (DDPMs) (Ho et al., 2020)."}, {"title": "4.5.3 Robustness and Noise Management", "content": "Multimodal training faces challenges such as noise\nand modality-specific biases Buettner and Ko-\nvashka (2024). Managing noisy retrieval inputs\nis critical for maintaining model performance.\nMORE (Cui et al., 2024) injects irrelevant results\nduring training to enhance focus on relevant in-\nputs. AlzheimerRAG (Lahiri and Hu, 2024) uses\nprogressive knowledge distillation to reduce noise\nwhile maintaining multimodal alignment. RAG-\nTrans (Cheng et al., 2024) leverages hypergraph-\nbased knowledge aggregation to refine multimodal\nrepresentations, ensuring more effective propaga-\ntion of relevant information. RA-BLIP (Ding et al.,\n2024b) introduces the Adaptive Selection Knowl-\nedge Generation (ASKG) strategy, which lever-\nages the implicit capabilities of LLMs to filter rele-\nvant knowledge for generation through a denoising-\nenhanced loss term, eliminating the need for fine-\ntuning. This approach achieves strong performance\ncompared to baselines while significantly reducing\ncomputational overhead by minimizing trainable\nparameters.\nRagVL (Chen et al., 2024d) improves robustness\nthrough noise-injected training by adding hard neg-\native samples at the data level and applying Gaus-\nsian noise with loss reweighting at the token level,\nenhancing the model's resilience to multimodal\nnoise. Finally, RA-CM3 (Yasunaga et al., 2023) en-\nhances generalization using Query Dropout, which\nrandomly removes query tokens during retrieval,\nserving as a regularization method that improves"}, {"title": "5 Tasks Addressed by Multimodal RAGS", "content": "Multimodal RAG systems extend RAG beyond uni-\nmodal settings to tasks requiring cross-modal in-\ntegration, enhancing performance across modali-\nties such as text, images, and audio. In content\ngeneration, these models enhance image caption-\ning (Zhi Lim et al., 2024; Hu et al., 2023; Rao\net al., 2024) and text-to-image synthesis (Yasunaga\net al., 2023; Chen et al., 2022b) by retrieving rele-\nvant contextual information. They also improve\ncoherence in visual storytelling and ensure fac-\ntual alignment in multimodal summarization (Ton-\nmoy et al., 2024). In knowledge-intensive appli-\ncations, multimodal RAG supports open-domain\nand knowledge-seeking question answering (Chen\net al., 2024d; Ding et al., 2024b; Yuan et al., 2023),\nvideo-based QA (Luo et al., 2024b), and fact verifi-\ncation (Khaliq et al., 2024), grounding responses\nin retrieved knowledge and thereby mitigating hal-\nlucinations.\nCross-modal retrieval further advances zero-shot\nimage-text retrieval (Yang et al., 2024; Dong et al.,\n2024b). Additionally, the recent incorporation of\nchain-of-thought reasoning (Zhai, 2024; Khaliq\net al., 2024) has enhanced its ability to support\ncomplex problem solving and logical inference. Fi-\nnally, integrating multimodal RAG into interactive\nagents and AI assistants such as Gemini (Team\net al., 2024) enables natural language-driven visual\nsearch, document understanding, and multimodal\nreasoning. The taxonomy of application domains\ncan be seen in Figure 3. The following sections\nexplore domain-specific adaptations of these tech-\nniques in greater depth.\nHealthcare and Medicine Multimodal RAG en-\nhances clinical decision-making through integrated\nanalysis of medical imaging, electronic health\nrecords, and biomedical literature. Systems like\nMMED-RAG (Xia et al., 2024a) address diag-\nnostic uncertainty in medical visual question an-\nswering by aligning radiology images with contex-\ntual patient data. RULE (Xia et al., 2024b) mit-\nigates hallucinations in automated report genera-\ntion through dynamic retrieval of clinically similar\ncases. AsthmaBot (Bahaj and Ghogho, 2024) in-\ntroduces a multimodal RAG-based approach for\nsupporting asthma patients across multiple lan-\nguages, enabling structured, language-specific se-\nmantic searches. Predictive frameworks such as"}, {"title": "Software Engineering", "content": "Code generation systems\nleverage multimodal RAG to synthesize context-\naware solutions from technical documentation and\nversion histories. DocPrompting (Zhou et al., 2023)\nimproves semantic coherence in code completion\nby retrieving API specifications and debugging pat-\nterns. Commit message generation models like\nRACE (Shi et al., 2022) contextualize code diffs\nagainst historical repository activity, while CEDAR\n(Nashid et al."}]}