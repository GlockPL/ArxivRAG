{"title": "METHODS TO ASSESS THE UK GOVERNMENT'S CURRENT ROLE\nAS A DATA PROVIDER FOR AI", "authors": ["Neil Majithia", "Elena Simperl"], "abstract": "The compositions of generative AI training corpora remain closely-guarded secrets, causing an\nasymmetry of information between AI developers and organisational data owners whose digital assets\nmay have been incorporated into the corpora without their knowledge. While this asymmetry is the\nsubject of well-known ongoing lawsuits, it also inhibits the measurement of the impact of open data\nsources for AI training. To address this, we introduce and implement two methods to assess open\ndata usage for the training of Large Language Models (LLMs) and \u2018peek behind the curtain' in order\nto observe the UK government's current contributions as a data provider for AI.\nThe first method, an ablation study that utilises LLM \u2018unlearning', seeks to examine the importance\nof the information held on UK government websites for LLMs and their performance in citizen\nquery tasks. The second method, an information leakage study, seeks to ascertain whether LLMs\nare aware of the information held in the datasets published on the UK government's open data\ninitiative data.gov.uk. Our findings indicate that government websites are important data sources for\nAI (heterogenously across subject matters) while data.gov.uk is not.\nThis paper serves as a technical report, explaining in-depth the designs, mechanics, and limitations\nof the above experiments. It is accompanied by a complementary non-technical report on the ODI\nwebsite2 in which we summarise the experiments and key findings, interpret them, and build a set\nof actionable recommendations for the UK government to take forward as it seeks to design AI\npolicy. While we focus on UK open government data, we believe that the methods introduced in this\npaper present a reproducible approach to tackle the opaqueness of AI training corpora and provide\norganisations a framework to evaluate and maximize their contributions to AI development.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) models continue to dominate the technological landscape, but despite their ubiquity in\nacademic literature and private-sector research, many questions remain regarding their underlying training corpora\nwhose contents are often closely held secrets.\nThese questions are especially pertinent in the world of copyright and licensing, where there is a tension between data\nowners (e.g. digital publications, artists, or authors) and the developers of generative AI models due to the copyright or\nlicensing infringement that may arise from the 'scraping' of the internet to build generative AI training corpora [1].\nBeyond copyright and licensing, however, this question is relevant to public policy. Governments are likely to be\nextremely important actors in the emergent AI future, with the ODI's 2024 white paper 'Building a better future with"}, {"title": "1.1 UK government data and its use for AI", "content": "The UK government holds a multitude of data that could be useful for AI models. Beyond official statistics releases\nand national archives, in this paper we examine two other UK government data sources: government websites and\ngovernment open data published on data.gov.uk.\n1. Government websites contain textual information on UK government policies, services, and guidance,\nwritten in plain, accessible English. As a structured collection of authoritative knowledge, these websites can\nenhance the performance of LLMs by offering accurate and trustworthy information about the governance and\noverall lifestyle of the UK.\n2. data.gov.uk is the government's central open data platform, hosting a wide variety of datasets, ranging from\npublic health and policing statistics to environmental and economic indicators. Unlike websites, these datasets\nare often presented in numerical or tabular formats, providing quantitative insights into the UK government's\nactivities and its citizens' lives.\nThese data sources can play a critical role in supporting LLMs when they are employed in public service tasks. Such\ntasks involve a citizen of the UK asking an LLM about government policies or services, often in the context of their\npersonal circumstances. Historically, these questions would have been answered through interaction with civil service\nhelp desks or static searches of government websites. LLMs, however, offer the potential for multi-round, dynamic\ndialogues that enhance accessibility and responsiveness [5].\nThe following sections of this paper look to examine whether each of these UK government data sources are contributive\nto the knowledge and capabilities of LLMs. In the following section, an ablation study methodology is used to explore\nthe importance of government websites to LLMs, while in section 3, information leakage methods are used to measure\nLLMs' recall of data.gov.uk data. While the results of both are brought together and briefly discussed in section 4, we\nrecommend reading the complementary ODI report to understand where each experiment and its results can be used to\nevidence our recommendations to the UK government."}, {"title": "2 The importance of government websites for LLMs - an ablation study", "content": "Government websites provide a wealth of textual information about UK policies, welfare programmes, and public\nservices. Written in accessible English, this information is structured and reliable, making it an ideal resource for"}, {"title": "2.1 Candidate methodologies in related work", "content": "One possible way to answer the research question is to use influence functions. Influence functions are statistical\ntechniques that can measure the contribution of chosen samples of training data towards the predictions of a model. In\nthis case, they could be used to numerically estimate how valuable government websites are to LLMs' responses to\ncitizen queries and therefore determine the extent to which models would be worse off if said websites were not in their\ntraining corpora.\nBoth Koh and Liang [6] and Grosse et al. [7] explore the use of influence functions in machine learning, providing\nmathematical formulations, python implementations, and preliminary results. However, both works admit the heavy\ncomputational load required to perform influence function calculation on account of the need to compute or at least\nestimate the inverse Hessians of the targeted models, something difficult to even conceptualise when models reach the\nsize and dimensionality of current-day generative AI."}, {"title": "2.1.2 Ablation studies", "content": "As a counterfactual, the research question can be developed beyond a simple thought exercise via an ablation study,\nwhich could involve removing government website data from the training corpora of LLMs, retraining them on the new\ncorpora, and assessing how the removal affects the models' function and performance in citizen query tasks. Intuitively,\nthis sounds computationally intensive and requires easy access to the full training corpora of LLMs (which often remain\nproprietary [8]), as well as a replication of the original LLM training protocol to be performed again with the new\ncorpora.\nInstead, 'unlearning' methods built for the removal of harmful or copyrighted output from AI models (introduced in\nYao et al. [9]) don't require retraining, instead performing a targeted reverse-fine-tuning on the models so that they\n'forget' certain parts of their training corpora.\nYao et al.'s technique, especially when training LLMs to forget copyrighted material, provides a robust methodology\nthat seems surgically accurate and minimally disruptive to models' overall performance. Their method has been adopted\nfor an entirely different purpose in Lu et al. [10], while other unlearning methods have also been designed across\nliterature [11]."}, {"title": "2.2 Methodology", "content": ""}, {"title": "2.2.1 Ablation method", "content": "Yao et al.'s unlearning method is analagous to a sort of fine-tuning of a subject LLM @ on some 'target' dataset Xtarget,\nbut in the reverse direction to typical fine-tuning so that the model 'forgets' the data rather than learning it. In other\nwords, it is an implementation of gradient descent, where for each step of the process t, o is gradually transformed so\nthat loss on the target dataset increases:\n$L(0_{t+1}(X^{target})) > L(0_{t}(X^{target}))$"}, {"title": "", "content": "where L(. . .) is the cross-entropy loss function. However an important, secondary objective of Yao et al.'s unlearning\nmethod is that despite the gradient ascent on target data, the subject LLM should preserve language capabilities and\nits knowledge of non-target data Xsafe. So, each step of the process also aims for loss on the safe dataset to remain\napproximately the same:\n$L(0_{t+1}(X^{safe})) \u2248 L(0_{t}(X^{safe}))$"}, {"title": "2.2.2 Evaluation pre- and post-ablation", "content": "To evaluate the impact of the ablation methodology, we developed a set of 18 citizen queries (see section 2.3.2) targeting\nspecific welfare-related topics covered by the target dataset. Each query was carefully designed to test the ability\nof LLMs to recall and synthesize information derived from government websites. Each query directly referenced\ninformation present in the ablated government websites, ensuring that correct responses relied on knowledge of this\ndataset.\nGround truth answers for the queries were derived from the textual content of the government websites in the target\ndataset. These ground truths served as benchmarks for evaluating the LLM responses both pre- and post-ablation. To\nfacilitate this evaluation, we employed the qualitative coding framework detailed in Table 1.\nThe framework allowed us to measure two primary dimensions of model performance:\n1. Structural Errors (Type 1): These errors assessed the impact of the ablation on the models' general language\nfluency and formatting. Minimal structural errors post-ablation would indicate that the methodology preserved\noverall language capabilities.\n2. Knowledge Errors (Type 2): These errors captured inaccuracies or omissions in the models' responses,\nreflecting the extent to which government websites contributed to LLM knowledge. The framework further\ndistinguished between errors directly linked to the ablated dataset and instances where secondary, non-\ngovernment sources provided compensatory knowledge (Type 2*).\nBy categorizing errors along these dimensions, the framework ensured a detailed evaluation of the importance of\ngovernment websites. If significant increases in Type 2 errors were observed post-ablation, this would confirm the\ncritical role of these websites in supporting LLM performance for citizen queries. Conversely, the presence of accurate\nType 2* responses post-ablation would highlight the availability and influence of secondary data sources."}, {"title": "2.3 Experiments", "content": ""}, {"title": "2.3.1 Data used for the ablation", "content": "The target dataset was the set of UK government websites to be ablated from the model. It was a collection of UK\ngovernment websites selected for their relevance to welfare policies and citizen queries. These websites were chosen\nbecause welfare-related queries are among the most common in citizen-facing advisory contexts, such as Citizens\nAdvice Bureau interactions. The dataset aimed to reflect the diversity of welfare-related topics covered by government\nservices, ensuring comprehensive evaluation of LLMs' reliance on this information. The websites and subject matters\nthat made up the target dataset are presented in table 2. Each website was present in CommonCrawl datasets before\nApril 2024, therefore sitting in the training window for up-to-date models like Llama 3.1 [13]. The plaintext of each\nwebsite was requested from the CommonCrawl index server, cleaned manually, and collated into the target dataset.\nThe safe dataset ensures that the ablation methodology targeted government-specific knowledge without affecting the\nmodels' general language capabilities. In this experiment, the safe dataset was a collection of samples of text from an\nEnglish-language Wikipedia article on English constitutional law (this article was randomly selected)."}, {"title": "2.3.2 Evaluation pre- and post-ablation", "content": "To evaluate the performance of LLMs before and after ablation, a set of 18 citizen queries was developed. These queries\nwere designed as simple information recall tasks, each addressing a specific welfare-related topic covered by the target\ndataset. Topics included eligibility criteria, payment details, and descriptions of welfare schemes such as Universal\nCredit and Child Benefit. For instance, one query asked, \"What is the eligibility for Universal Credit if an applicant has\nchildren?\" Each query was grounded in evidence available on the ablated government websites, ensuring that a correct\nresponse required knowledge of this specific data.\nA single control query was also included to validate the framework. This control query asked about welfare provisions\nin the United States; if performance in this task suffered post-ablation, it would be clear that the ablation method was\ntoo intrusive, as the LLM will have forgotten knowledge that wasn't in the target dataset.\nGround truth answers to each query were constructed directly from the content of the government websites in the target\ndataset. These answers served as the benchmark against which LLM responses were evaluated, using the qualitative\ncoding framework in Table 1 to count structural errors and knowledge inaccuracies present in LLMs' answers to citizen\nqueries pre- and post-ablation."}, {"title": "2.3.3 Implementation", "content": "Subject models for experiments were: Llama-3.1 8B, Llama-3.1 8B Instruct [14], Gemma 2b, Gemma 2b-it [15], and\nQwen 2.5 3B Instruct [16]. The experiment was carried out in an AWS Sagemaker instance, using Huggingface and\nPytorch libraries.\nWe used [0.25, 0, 1] weightings for the unlearning process, as recommended by Yao et al., because these were found to\nbe best for ensuring minimally intrusive ablation. With an unlearning rate of 2 \u00d7 10-4, we ran the ablation for 1000\nsteps."}, {"title": "2.3.4 An example", "content": "With measurement pre- and post-ablation for five LLMs on 18 (+1) evaluation queries, there were in total 190 responses\nto be evaluated by a single reviewer. In table 3, we present an example of what the results of the ablation might look\nlike.\nPre-ablation, the model correctly identifies how Child Benefit interacts with Universal Credit, noting that this is because\nChild Benefit is counted as a component of household income when calculating Universal Credit payment amounts.\nThis response therefore has no codes to be applied. Post-ablation, the model does not correctly identify how Child\nBenefit interacts with other benefits. Child Benefit is a flat rate paid to all households that earn under \u00a350,000 annually,\nso it cannot be affected by the income-based benefits the LLM mentions. This inaccuracy is labelled with the code 2d."}, {"title": "2.4 Results", "content": ""}, {"title": "2.4.1 Ablation intrusiveness", "content": "Figure 1 demonstrates that, across all tested models, there was minimal increase in structural (Type 1 errors) post-\nablation, suggesting that the unlearning process preserved the general language capabilities of the models. This confirms\nthe methodology's non-intrusiveness, meaning it is robust and can be used to evaluate the importance of government\nwebsites."}, {"title": "2.4.2 Ablation effects per model", "content": "Figure 2 summarises the number of knowledge-based (Type 2) errors for each model pre- and post-ablation. All models\ndisplayed a clear increase in Type 2 errors following ablation (on average 42.6% increase), highlighting the critical\nrole of government websites in providing accurate knowledge for welfare-related queries. However, the extent of these\nincreases varied across models. For example, Qwen 2.5 3B Instruct showed a smaller increase in errors compared to\nLlama 3.1 8B, possibly reflecting differences in their training data sources or architectures, although more testing would\nhave to be done to confirm this difference as significant. Otherwise, LLMs were affected fairly homogenously."}, {"title": "2.4.3 Ablation effects per question", "content": "Figure 3 illustrates the number of Type 2 errors observed for each evaluation question pre- and post-ablation. Responses\nto some questions were completely unaffected by the ablation, whereas for other questions ablations had extensively\nnegative effects. This indicates that government websites are important as data sources for some questions on some\nsubject matters but not for others."}, {"title": "2.5 Further Analysis", "content": "In this section, we analyse the results presented in Figure 3 which demonstrate the heterogenous effects of ablation per\nquestion.\nAs mentioned previously, each question in the evaluation set pertained to an individual subject matter addressed in the\ntarget dataset websites. In table 4, questions and their specific subject matters are grouped by the extent to which each\nwas affected by ablation.\nWhy are the topics in the right-hand column of Table 4 more affected than those in the left-hand column? The availability\nof secondary, non-government sources of information could perhaps explain the differing impacts of ablation. Widely\ndiscussed topics, like mental health support services, are less affected because they are covered extensively in news\narticles, forums, and other digital spaces. This means that when government websites on these subjects are removed\nfrom LLMs' knowledge, they still have knowledge from secondary sources in their training data that they can use to\nanswer questions accurately. In contrast, topics that are less widely discussed online, like the interactions between\ndifferent welfare schemes, are more affected by the ablation.\nWe test this hypothesis in figure 4. For each question, a 'prevalence' score was measured by counting how many of the\nfirst 10 non-government websites in a Google search of the question could answer that question. This was compared\nagainst each question's 'difference', the magnitude of the effect of ablation for that question presented in figure 3. As"}, {"title": "3 LLMs' knowledge of data.gov.uk datasets - an information leakage study", "content": "Data.gov.uk is the UK government's primary platform for publishing open, non-personal data. It hosts a diverse range of\ndatasets, including information on public health, economic activity, and environmental indicators. These datasets have\nthe potential to significantly enhance AI development by providing accurate, up-to-date numerical data on governance\nand social trends. However, whether these datasets are effectively utilised in training large language models (LLMs)\nremains an open question.\nUnlike textual data on government websites, the datasets on data.gov.uk are often presented in formats less accessible to\nweb crawlers, such as downloadable files. As a result, their integration into LLM training corpora may be limited. To\nunderstand whether data.gov.uk is a data provider for AI, we must therefore ask the question:"}, {"title": "3.1 Candidate methodologies in related work", "content": "This chapter aims to identify whether data.gov.uk is a part of the training corpora of LLMs. In general, the exact\ncontents of training corpora are closely guarded [8] and subject to much controversy. The New York Times v. Microsoft\n[17] and Getty Images v. Stablility.ai [18] are ongoing copyright cases concerned with this subject, with both plaintiffs\nclaiming to have identified that their copyrighted material was scraped and used in the training corpora of AI models\nwithout permission. Both complaints provide evidence for their claims by using information leakage methods, as\ndemonstrated in figure 5.\nInformation leakage methods aim to identify the data AI models have been trained on by prompting them to recall\nspecific 'target' data points suspected to be in their training corpora. If the models' responses to the prompts are the\nsame as, or express similarity to, the target data points, there is significant evidence that the target data points are part of\nthe training corpora. This is best demonstrated in figure 4b: Stable Diffusion recreates the Getty images watermark\nin its response to an information leakage prompt (the complaint cites Carlini et al. [19] for its underlying prompting\nmethodology.\nSimilar methods are employed with a rigid experimental framework in Wang et al. [20], which explores the overall\n'trustworthiness' of GPT models. In section 8, the authors use 0-, 1-, and 5-shot prompting to perform information"}, {"title": "3.2 Methodology", "content": "The framework used in section 8 of Wang et al. provides a robust methodology that could be employed in this paper to\nascertain whether data in data.gov.uk is recalled by LLMs in their answers to questions, which would then provide\nevidence to answer the research question. The following subsections adapt the framework for this paper's context and\nintroduce the supplementary methods that will be used."}, {"title": "3.2.1 Information leakage methods for base models", "content": "The Wang et al. methodology applies exclusively to 'base models' like GPT-3.5, which treat text prompts as the\nbeginning of a sequence, and in their responses, attempt to predict the rest of that sequence. Their methodology takes\nadvantage of this, by prompting a piece of information and asking the LLMs to construct the rest of it (see table 5).\nWith a different prompt, this methodology can be adapted to the purposes of this paper (see table 6).\nAs Wang et al. do, this paper utilises 4 different prompt templates (table 7). Note that template (d) aims to mimic the\nformat of a .csv file."}, {"title": "3.2.2 Testing the knowledge of instruct models", "content": "Unlike base models, 'instruct models' are LLMs that are tuned to treat specifically demarcated prompts (called 'system'\nprompts) as instructions, meaning that their responses attempt to fulfil the instructions by, most commonly, acting as\nchatbot assistants like ChatGPT. After their instructions, chatbot-instructed LLMs can be prompted by end users to\nanswer their questions and provide them information, often demonstrating better language and reasoning capabilities in\ncomparison to base models.\nWhile Wang et al. do not investigate instruct models, the methods by which one can do so are easy to conceptualise. A\nuser prompt can ask a pre-instructed LLM about a piece of government data from data.gov.uk; if it correctly answers\nthe question, the data is present in its knowledge base and there is evidence to suggest that data.gov.uk is a part of the\ntraining corpus. This methodology is demonstrated in table 9.\nIn the context of LLMs, particularly instruction models that are fine-tuned with Reinforcement Learning from Human\nFeedback (RLHF) [21], a notable behavior is their tendency to decline answering certain prompts (the right-hand\ncolumn of table 9). Reticence often stems from the RLHF process, which aligns model outputs with human preferences,\nemphasizing safety and ethical considerations. Consequently, models aim to avoid responding to queries that could lead\nto sensitive content or misinformation, even if the information is factual and publicly available. This cautious approach,\nwhile enhancing safety and limiting liability, can limit the models' ability to provide comprehensive information:\nfor instance, research has shown that RLHF can cause models to avoid providing statistics or inappropriately evade\nquestions [22]. So, if an instruct-tuned model is reticent to answer a prompt with data from data.gov.uk, this does not"}, {"title": "3.3 Experiments", "content": "For this experiment, we selected five datasets from data.gov.uk covering a range of topics alongside two controls external\nto data.gov.uk. The chosen datasets included statistics on topics such as fire safety, rail injuries, and air pollution, among\nothers, while the controls included widely known information, such as the Bank of England's base rate (table 10).\nThe models tested in this experiment were: Meta Llama 3.1 8B base and instruction-tuned, Google Gemma 2 2B base\nand instruction-tuned, and Qwen 2.5 3B base and instruction-tuned."}, {"title": "3.4 Results", "content": "Table 11 provides the results of the experiments and demonstrates that almost all were unsuccessful: in 5 out of 195\ntests, tested LLMs simply did not recall data points in data.gov.uk. In the table, green ticks denote that the LLM\nsuccessfully recalled the data, red crosses indicate that they did not, and orange stars represent where instruct-tuned\nLLMs were reticent to answer the question.\nImportantly, the LLMs also performed poorly on controls. Only Llama 3.1, the highest parameter model tested, correctly\nrecalled data points from BOE and POP datasets, both of which widely reported pieces of information that are not from\ndata.gov.uk. This may suggest that for Gemma and Qwen, the methods used for this research were not optimal for\nanswering the research question.\nNonetheless, it is clear from these experiments that the tested LLMs cannot recall almost any information from\ndata.gov.uk, indicating that it is not part of their training corpora. In other words:"}, {"title": "4 Results summary and discussion", "content": "This paper employed two complementary methodologies\u2014an ablation study and an information leakage study-to\nevaluate the role of the UK government as a data provider for AI. Together, these experiments provide a detailed\nassessment of the extent to which government websites and data.gov.uk contribute to the knowledge bases of LLMs.\nIn section 2, the ablation study demonstrated the importance of government websites to tested LLMs. Specifically, by\nexamining LLM performance in public service tasks pre- and post-ablation of government websites, the experiment has\nthree key results.\nKey Result 1 (KR1): The ablation methodology minimally affected structural language capabilities,\nconfirming its validity for isolating the impact of government data.\nKey Result 2 (KR2): All models exhibited more knowledge errors post-ablation to roughly the same\nextent, indicating that government websites are important as data providers for AI.\nKey Result 3* (KR3*): Government websites remain a key source of information for LLMs, in\nparticular on subject matters that are not widely discussed online, such as the interactions between\nwelfare schemes like Universal Credit and Child Benefit.\nIn section 3, we used an information leakage study to explore whether LLMs could recall data from data.gov.uk,\ntherefore indicating whether data.gov.uk is functioning as a data provider for AI. This experiment yielded one key\nresult."}, {"title": "4.1 Applications of these methodologies for other topics", "content": "The methodologies introduced in this study have broad applicability beyond evaluating governments as data providers\nfor AI. These methods can be leveraged to investigate the role of specific datasets in shaping AI training corpora,\nproviding insights into questions of data usage, attribution, and influence.\nAblation studies, for instance, could be employed in copyright or intellectual property disputes to assess the impact\nof proprietary datasets on AI model performance. By quantifying the importance of such data to downstream tasks,\nthese methods can help stakeholders understand the value of their contributions to AI systems. Similarly, information\nleakage techniques are well-suited to verifying whether specific datasets have been incorporated into a model's training\ncorpus. This is particularly relevant for validating claims of unauthorized data use or ensuring compliance with licensing\nagreements and data governance policies.\nBeyond text-based datasets, these methodologies could be adapted for use in other domains, such as image or audio\nmodels. For example, ablation studies might evaluate the importance of specific visual datasets in computer vision tasks,\nwhile information leakage approaches could determine whether sensitive or proprietary visual data has been included\nin multimodal AI systems. Furthermore, these techniques can support broader evaluations of open data platforms,\nidentifying opportunities to make such resources more accessible and impactful for AI development.\nBy providing tools to interrogate and evaluate the role of datasets in AI training, these methodologies offer a foundation\nfor addressing pressing questions of transparency, accountability, and equity in the evolving AI landscape."}, {"title": "5 Conclusion", "content": "This paper has examined the role of the UK government as a data provider for AI, focusing on the contributions\nof government websites and the open data platform data.gov.uk. Through the development and application of two\nmethodologies\u2014an ablation study and an information leakage study-we have provided a detailed assessment of how\nthese data sources influence the performance and knowledge bases of LLMs.\nThe ablation study highlighted the critical role of government websites in supporting LLMs, particularly for some\nsubject matters where alternative sources are sparse. Conversely, the information leakage study revealed that data.gov.uk\nis minimally integrated into LLM training corpora, emphasizing the challenges of utilizing open data platforms for\nAI development. Together, these methodologies offer a reproducible framework for evaluating the importance and\npresence of specific datasets in AI training corpora.\nWhile this paper focuses on the technical aspects of these methodologies and their findings, the accompanying\npolicy-focused report by the Open Data Institute expands on their implications. The report discusses actionable\nrecommendations for enhancing the UK government's role as a data provider for AI and explores how these insights can\ninform data governance and public policy. Together, these contributions aim to advance both technical understanding\nand strategic approaches to leveraging government data for AI."}]}