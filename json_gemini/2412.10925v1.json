{"title": "Video Representation Learning with Joint-Embedding Predictive Architectures", "authors": ["Katrina Drozdov", "Ravid Shwartz-Ziv", "Yann LeCun"], "abstract": "Video representation learning is an increasingly important topic in machine learning research. We present Video JEPA with Variance-Covariance Regularization (VJ-VCR): a joint-embedding predictive architecture for self-supervised video representation learning that employs variance and covariance regularization to avoid representation collapse. We show that hidden representations from our VJ-VCR contain abstract, high-level information about the input data. Specifically, they outperform representations obtained from a generative baseline on downstream tasks that require understanding of the underlying dynamics of moving objects in the videos. Additionally, we explore different ways to incorporate latent variables into the VJ-VCR framework that capture information about uncertainty in the future in non-deterministic settings.", "sections": [{"title": "Introduction", "content": "The rapid increase in video data across various domains has created a pressing need for effective video representation learning methods that automatically extract and encode the essential elements of video content into compact and informative features. In particular, the goal behind video representation learning is to develop machine learning models that efficiently interpret complex, high-dimensional visual information by capturing key aspects unique to video data such as motion, scene context, and temporal dynamics. This capability is crucial for applications that require real-time understanding of dynamic environments such as robotic navigation, where robots must maneuver safely in unpredictable surroundings (Nahavandi et al., 2022); healthcare, where continuous video analysis can assist in medical diagnostics (Asan & Montague, 2014); and autonomous driving, which relies on accurate perception of the road and its surroundings to ensure safe operation (Chen et al., 2024). As these applications continue to evolve, robust video representations are essential for enabling reliable, responsive, and intelligent systems.\nWhile powerful, traditional supervised learning approaches to video representation learning require vast amounts of labeled data, which is often expensive to obtain. Self-supervised learning (SSL) for video provides a promising alternative, where models learn to understand video content without relying on external annotations. SSL approaches typically involve designing tasks, often referred to as \"pretext tasks\", that leverage the inherent structure of video data. Some sample tasks include predicting future frames, determining temporal order, or contrasting different clips from the same video. These tasks encourage models to extract high-level, information-rich features that capture complex temporal dynamics and semantic information directly from raw video data. Such general-purpose representations can be leveraged for a wide range of downstream tasks,"}, {"title": "Related Literature", "content": "Joint-embedding Predictive Architectures Joint-embedding architectures (JEA) are a class of self-supervised deep learning models that capture compatibility and dependencies between two inputs (LeCun, 2022). The underlying objective of JEA is to assign low energy to compatible inputs and high energy to non-compatible inputs. Examples of such systems are Siamese Networks (Becker & Hinton, 1992; Bromley et al., 1993; Hadsell et al., 2006) which contain two identical sub-networks that share a common representation space and are trained to learn a similarity metric between their inputs. Some recent examples of joint-embedding architectures include SimCLR (Chen et al., 2020), Barlow Twins (Zbontar et al., 2021), and VICReg (Bardes et al., 2022). These are invariance-based methods that train an encoder to produce similar representations for different views of the same image. Joint-embedding predictive architectures (JEPA), are a type of JEA that incorporates a predictor module in addition to the Siamese encoder (Bardes et al., 2023; Assran et al., 2023). More specifically, given two inputs x and y, and their corresponding embeddings $h_x$ and $h_y$ from the encoder, the role of the predictor is to learn to predict $h_y$ from $h_x$ for compatible x and y. In contrast to generative models that aim to predict the actual target y from input x, the JEPA approach to learning dependencies between inputs x and y operates in the abstract representation space. This encourages the model to prioritize learning high-level features over low-level details.\nRepresentation Collapse One challenge when training joint-embedding architectures is that they are prone to representation collapse: a scenario in which the encoder becomes invariant to its inputs and maps all of them to the same hidden representation (Jing et al., 2021; Shwartz-Ziv et al., 2023). This type of collapse is a special case of dimensional collapse, a scenario in which hidden representations lie on a very low-dimensional manifold within the representation space (Li et al., 2022; Jing et al., 2021). There exist different approaches to preventing representation collapse such as: contrastive methods that have a loss that pushes away embeddings belonging to incompatible inputs (Chen et al., 2020), methods that introduce architectural asymmetries such as momentum encoders (Grill et al., 2020) or non-differentiable operations (Chen & He, 2021), information-maximization methods that aim to maximize the entropy of the average representation (Caron et al., 2021). Our work is closely related to yet another approach to avoiding representation collapse, namely, de-correlating the representations to eliminate redundancy in the underlying features, as recently promoted in Barlow Twins (Zbontar et al., 2021) and VICReg (Bardes et al.,"}, {"title": "Method", "content": "In the following sections, present our VJ-VCR model: a self-supervised JEPA method for video representation learning that is trained by making predictions in the abstract representation space and that uses variance-covariance regularization to prevent representation collapse. Prediction in the hidden representation space encourages the model to focus on high-level rather than low-level details, while the variance and covariance regularization is a way to directly ensure that the hidden representations are diverse and informative.\n3.1 The VJ-VCR Model\nFigure 1a depicts our VJ-VCR model for self-supervised video representation learning that, given the hidden representation of input frames, is trained to predict the hidden representaiton of future frames. It consists of encoder, predictor, and an optional decoder modules that can have any desired architecture. VJ-VCR can also incorporate a latent variable to account for stochasticity in the future.\nEncoder Given a set of input frames x and target frames y coming from the same video, an encoder maps these frames into their corresponding hidden representations $h_x$ and $h_y$, respectively. In order to prevent collapse in the representation space, we apply variance-covariance regularization to the hidden representations $h_x$ and $h_y$, which we cover in detail in 3.2.\nPredictor The predictor takes the hidden state of the input frames $h_x$ and predicts the hidden state of the target frames, $h_y$. The predictor can take a latent variable as an input in addition to $h_x$, as described below.\nLatent Variable VJ-VCR can incorporate a latent variable z to facilitate the prediction task in case that the target frames are not a completely deterministic version of the inputs, i.e. there are some unobserved variables that influence what the target frames contain but cannot be inferred from the input. Including a latent variable can improve the interpretability of the model by separating deterministic from stochastic"}, {"title": "Variance-Covariance Regularization", "content": "Variance-Covariance Regularization (VCR) is an effective way to prevent collapse in a JEPA. We adapt its formulation presented in Bardes et al. (2022) and Zhu et al. (2023) to the setting of video data. Let $X = \\{x_1,x_2,...,x_N\\} \\subset R^{T \\times H \\times W \\times N}$ be sets of frames coming from N videos, where T is the number of frames from each video, and H and W denote the height and width of each frame, respectively. Let $f_@$ be a neural network parameterized by @ that maps the inputs in X to their (flattened) d-dimensional hidden representations $H = \\{h_i | h_i = f_@(x_i)\\}_{i=1}^N$, where $h_i \\in R^{T \\times d}$. VCR's objective is to ensure that the hidden representations in H exhibit high variance and low covariance.\nIn particular, VCR encourages the variance along each of the d components and T time steps to be above a certain threshold $\u03c4 > 0$. This is achieved with a hinge loss regularization term:\n$I_{var}(H) = \\frac{1}{Td}\\sum_{t=1}^{T} \\sum_{k=1}^{d} max(0, \\tau - Var(H_{t,k}) + \\epsilon),$ (1)\nwhere $H_{t,k} = \\{h^{(t,k)}, h^{(t,k)},..., h^{(t,k)}\\} \\subset R$ is the set of all k-th components of the representations in H at time frame t, the Var(Z) function computes the variance of $Z = \\{z_i\\}_{i=1}^N \\subset R$ as $Var(Z) = \\frac{1}{N-1}\\sum_{i=1}^N (z_i - \\bar{z})^2$ using the mean $\\bar{z} = \\frac{1}{N} \\sum_{i=1}^N z_i$, and \u025b is a small constant introduced for numerical stability. In our experiments we set $\u03c4 = 1$.\nThe VCR covariance regularization term in our setup is defined by:\n$L_{cov}(H) = \\frac{1}{Td}\\sum_{t=1}^{T} \\sum_{i \\neq j} [Cov(H_{t,:})]_{i,j}^2$ (2)\nIt sums the squares of the non-diagonal entries of the covariance matrix of $H_t: \\in R^{d \\times N}$, the hidden representations at time step t, namely\n$Cov(H_{t,:}) = \\frac{1}{N-1}\\sum_{i=1}^N (h^{(t,:)} - \\bar{h}^{(t,:)}) (h^{(t,:)} - \\bar{h}^{(t,:)} )^T$ (3)\nwhere $\\bar{h}^{(t,:)} \\in R^d$ is the mean of all hidden representations, namely $\\bar{h}^{(t,:)} = \\frac{1}{N} \\sum_{i=1}^N h^{(t,:)}$. Minimizing the term in (2) encourages the hidden representations to be de-correlated.\nThe final formulation of VCR is a weighted sum of the regularization terms in (1) and (2):\n$I_{vcr}(H) = \\alpha I_{var}(H) + \\beta I_{cov}(H).$ (4)\nIn practice, VCR is applied at the batch level during training, rather than the whole dataset, i.e. N can be interpreted as the batch size."}, {"title": "Training and Inference", "content": "Training We formulate the training objective of our VJ-VCR model depicted in Figure 1a in the framework of energy-based learning (LeCun et al., 2006). An energy function is a function that assigns a scalar value to configurations of observed and latent variables in a given system, with lower levels of energy corresponding"}, {"title": "Experimental Setup", "content": "In this work, we aim to evaluate our hypothesis that a JEPA-based approach to self-supervised video representation learning, using our proposed VJ-VCR model, can generate video representations that better capture high-level information about the underlying videos than generative-based models, e.g. by encoding the dynamics of moving objects. For this purpose, we design various experiments with several datasets and compare VJ-VCR to a generative-based baseline. The details of our experimental setup are outlined in the following subsections.\n4.1 Generative Model Baseline\nFigure 1b illustrates the generative model for self-supervised video representation learning that we use as a baseline for comparison with our VJ-VCR. The generative model shares the same building blocks as the VJ-VCR model, but it differs in two key aspects: it incorporates a decoder by default, and its training objective is to perform predictions in the input (pixel) space, rather than in the abstract representation space. In particular, the generative model's objective is to minimize the energy function defined as a weighted sum of the reconstruction error from the decoder and, optionally, the variance-covariance regularization term:\n$E_{\\theta_{GEN}}(x, y, z) = D(\\tilde{y}, y) + I_{vcr}([h_x, h_y]) = ||Dec(Pred(h_x, z)) - y||_2^2 + \\alpha I_{var}([h_x, h_y]) + \\beta I_{cov}([h_x, h_y])$ (7)\nSimilarly to VJ-VCR, the generative model can also incorporate latent variables z that encode information about the future frames which is not directly predictable from the past.\n4.2 Datasets\nWe validate our approach to video representation learning with experiments related to understanding the dynamics of moving objects. The datasets that we use for this purpose can be categorized into deterministic and non-deterministic ones depending on whether they inherently contain some stochastic events as described below. Additional dataset details can be found in Appendix A."}, {"title": "Deterministic Setting", "content": "Deterministic Setting MovingMNIST (Srivastava et al., 2015) is a synthetic dataset that consists of videos of MNIST (LeCun et al., 1998) digits of size 28 \u00d7 28 moving with randomly chosen constant velocity across a 64 \u00d7 64 black frame. In its original version, when a digit hits a wall, it bounces off the wall in a deterministic fashion.\nThe CLEVRER dataset (Yi et al., 2019) consists of synthetic videos of colliding objects. Every video frame is annotated with each of the objects' shape, location, velocity, and collision events. When generating input pairs (x, y) in our setup, we filter out ones in which new objects appear in the clip y, making the data deterministic."}, {"title": "Non-deterministic Setting", "content": "Non-deterministic Setting Our custom stochastic version of MovingMNIST is the following. In the first 3 frames of each video, the digit moves horizontally. In the following 3 frames, the digit randomly switches trajectory in one of five possible directions, namely $\\psi \\in \\{\\frac{2k\\pi}{5} | k \\in \\{0,1,...,5\\}\\}$.\nCATER (Girdhar & Ramanan, 2019) is a dataset based on CLEVR (Johnson et al., 2017) designed for spatiotemporal video reasoning tasks. It features moving objects that can interact with each other and perform a set of 14 pre-determined actions. Unlike CLEVRER in which interactions between objects are deterministic, in CATER, objects can randomly begin performing actions within the video. By design, multiple actions can be performed at any given point in the video. Since the actions are chosen at random, the input frames alone cannot deterministically predict what the future frames will contain."}, {"title": "Evaluation", "content": "We evaluate the pretrained and frozen VJ-VCR and generative models through several means:\n\u2022 Predicting high-level information from hidden representations: we assess the models' ability to capture high-level dynamic information from the video data, such as object speeds.\n\u2022 Predicting high-level information from latent variables: in non-deterministic settings, we evaluate the models' capacity to utilize latent variables by predicting high-level information, such as actions present in a video, from inferred latent variables.\n\u2022 Visualizing learned hidden representations: we train a decoder to map the hidden representations back to pixel space, allowing us to visualize the information encoded within them.\n\u2022 Information theoretic analysis: we employ information theoretic metrics to quantify the amount of information contained within the models' hidden representations.\nSpecifically, we outline the evaluation for the deterministic and non-deterministic datasets in the following subsections. Full training details and hyperparameter values \u03b1, \u03b2, \u03b3 can be found in Appendix B.1. Additional evaluation details can be found in Appendix B.2."}, {"title": "Deterministic Setting", "content": "During evaluation, we would like to check whether information about the underlying high-level dynamics in the video is captured in the predicted hidden representation of the target frames, $h_y$. In particular, we propose to predict the speed v (a scalar value) of an object in the video from $h_y$. For this purpose, we freeze the pre-trained encoder and predictor for models from Figure 1. We then train a linear regression that takes the predictor's outputs and, in case of MovingMNIST, predict the (constant) speed of the moving digit, or, in the case of CLEVRER, predicts the speed of the fastest object in the last target frame. We refer to this evaluation as speed probing."}, {"title": "Non-deterministic Setting", "content": "MovingMNIST In the case of MovingMNIST, we evaluate whether we can isolate stochastic information from deterministic information in the latent variables z using our VJ-VCR model. Our goal is for the latent variable to encode the stochastic information of the random switch \u03c8 in the trajectory of the digit. We consider two ways to incorporate the latent variable z into the VJ-VCR setup."}, {"title": "Architecture", "content": "In experiments with MovingMNIST, we model the encoder as a 5-layer convolutional neural network with batch norm and ReLU activation function at each layer. It has 3 spatial and 2 temporal convolutions followed by an average pooling layer. The predictor is an MLP with 2 hidden layers. Unless otherwise noted, the input x contains 3 frames and the target y contains the following 12 frames in the video. The predictor outputs the hidden state of the 12 target frames y simultaneously.\nIn experiments with CLEVRER and CATER, we use a SimVP encoder (Gao et al., 2022) and a Swin Transformer (Liu et al., 2021) for the predictor. Unless otherwise noted, the input x contains 6 frames and the target y contains the following 20 frames in the video for CLEVERER. For CATER, we subsample the video at the rate 8 frames per second following (Girdhar & Ramanan, 2019) and feed 50 frames as input and the following 50 frames as target.\nIn all experiments that use a decoder, its architecture mirrors that of the encoder. It takes the predicted hidden state of the target frames $h_y$ as input to reconstruct the target frames y. The decoder can be trained simultaneously with the rest of the system or, alternatively, it can be trained on top of the pretrained and frozen encoder and predictor."}, {"title": "Implementation and Hardware", "content": "For our experiments, we use the publicly available PyTorch (Paszke et al., 2019) codebase OpenSTL (Tan et al., 2023) which can be found here: https://github.com/chengtan9907/OpenSTL. We train our models on one NVIDIA RTX 8000 GPU card and all of our experiments take less than 48 hours to run."}, {"title": "Results", "content": "In this section, we report our findings on the ability of VJ-VCR and generative video representation learning methods to capture information about the dynamics of moving objects using the experimental setup outlined in section 4. Our hypothesis is that VJ-VCR models, which make predictions in the abstract representation space, are better suited for capturing object dynamics (such as their speeds and actions they perform) than generative models. This is because generative models prioritize the reconstruction of low-level pixel details in the target frames, potentially limiting their ability to capture high-level dynamic information."}, {"title": "Deterministic Setting: Speed Probing", "content": "In this set of experiments, we use the deterministic version of MovingMNIST and the CLEVRER datasets introduced in section 4.2 and evaluate VJ-VCR and generative models on the task of speed probing as described in section 4.3.1.\nMovingMNIST (deterministic) Table 1 presents the evaluation results on speed probing of four self-supervised models for video representation learning pre-trained with different energy loss functions. As shown in equation 5, the energy function can incorporate JEPA-style prediction error in the hidden space paired with VCR, and, optionally, reconstruction error in the pixel space. In terms of evaluation, we report the average reconstruction quality achieved by a decoder (measured by PSNR), and the MSE measured during speed probing, i.e. the MSE of a linear regression trained to predict the speed of the MNIST digit in each video from $h_y$. Both metrics are measured on the validation set. Appendix C provides a visualization of the reconstructions obtained from the model in the second row, VJ-VCR simultaneously trained with a decoder, and the model in the forth row, generative one trained without VCR.\nThe VJ-VCR models in the top two rows achieve the lowest speed probing MSE of 0.04. This indicates that JEPA models trained to make predictions in the abstract representation space outperform purely generative models at capturing the dynamics of moving digits. Since the VJ-VCR model in the top row does not make predictions in the pixel space, we separately train a decoder to reconstruct the target images y from the frozen hidden representations $h_y$ of this model. This decoder has the lowest reconstruction quality with PSNR of 19.5, suggesting that some reconstruction details are absent in the hidden representations. The VJ-VCR model in the second row, trained with prediction error in the pixel space, achieves better reconstruction quality of 21.2. This demonstrates that incorporating a reconstruction loss term during JEPA training can enhance the reconstruction quality without compromising the model's ability to predict the underlying video dynamics. In contrast, the model in the third row trained solely with pixel loss and VCR achieves the highest reconstruction quality of 22.9 but performs worse at predicting speed compared to the JEPA-based models with an MSE of 0.10. Finally, the purely generative model in the last row, trained without VCR, closely"}, {"title": "Non-deterministic Setting: Action Recognition", "content": "In this set of experiments, we use the stochastic version of MovingMNIST and the CATER datasets introduced in section 4.2 and perform evaluation through action recognition using the inferred latent variables $z^*$ as described in section 4.3.2."}, {"title": "Analyzing the Information Content of the Learned Representations", "content": "In this section, we analyze the extent of representation collapse in the VJ-VCR and generative models pre-trained through self-supervision on the CATER dataset. In particular, following the approach in Li et al. (2022), for each model we consider the singular value decomposition (SVD) of the matrix $H \\in R^{N \\times d}$ of the encoder's outputs on the validation dataset, where N is the number of validation samples and d is the dimension of the encoder's hidden representations. We track the evolution of the singular values' distribution throughout training. The intuition behind it is that having a few dominant singular values would imply that the hidden representations occupy a low-dimensional manifold within $R^d$. If the distribution of the singular values is more balanced, then the hidden representations have a higher intrinsic dimensionality.\nFigure 4a shows the distribution of the singular values of the hidden representation matrix H for VJ-VCR and a generative model pre-trained on CATER, measured at the beginning and the end of training. The singular values are sorted in descending order. For both models, the distribution becomes more balanced by the end of training. However, the VJ-VCR model exhibits a more balanced singular value distribution compared to the generative model. This observation is further supported by Figure 4b in which the cumulative explained variance of the VJ-VCR model is increasing more gradually than that of the generative-based model. These results suggest that VJ-VCR pre-training avoids dimensional collapse more effectively than training with a generative objective."}, {"title": "Conclusion", "content": "In this paper, we demonstrate that JEPA-style video representation learning can produce more informative video representations when compared to generative models in the self-supervised setting. Specifically, we apply variance-covariance regularization solely to the top layer of the encoder to prevent representation collapse. Future work could explore extending this regularization to multiple layers of the neural network architecture, potentially enhancing the quality of the learned hidden representations. One limitation of our study is its focus on relatively small synthetic datasets. However, we believe that the proposed VJ-VCR model for video representation learning can generalize effectively to larger and more realistic datasets. Additionally, since JEPA models are trained in the hidden representation space, this can significantly reduce computational costs when compared to generative models, especially in high-dimensional settings. This work contributes to the broader pursuit of developing efficient and reliable AI systems, with the potential to advance a range of applications that require an understanding of complex video data."}, {"title": "Datasets", "content": "MovingMNIST For experiments with MovingMNIST, we split the original MNIST dataset into 55,000 training and 5,000 validation samples. In the deterministic version of the MovingMNIST, given a sample from the MNIST dataset, we generate a 20-frame video by randomly sampling the digit's initial location on a 64 \u00d7 64 black canvas and the digit's velocity (which remains constant throughout the video). In the stochastic version of MovingMNIST, given a sample from the MNIST dataset, we generate a 6-frame video as follows. In the first 3 frames of the video, the digit starts at the center of the 64 \u00d7 64 canvas and moves horizontally to the right. In the following 3 frames, the digit randomly switches trajectory in one of five possible directions, namely $\\psi \\in \\{\\frac{2k\\pi}{5} | k \\in \\{0,1,...,5\\}\\}.\nCLEVRER The CLEVRER (CoLlision Events for Video REpresentation and Reasoning) dataset consists of synthetic videos of colliding objects (Yi et al., 2019). Each video is 5 seconds long and contains 128 frames with resolution 480 x 320. In our experiments with CLEVRER, we re-shape the videos to 64 \u00d7 64 resolution. Furthermore, we use the official training and validation splits provided at http://clevrer.csail.mit.edu/.\nCATER CATER (Girdhar & Ramanan, 2019) is a synthetic dataset of moving objects that can move independently and also interact with each other. Each video contains 300 frames at 24 frames per second at 320x240 resolution. There are 14 possible actions objects can perform and multiple actions can be present in a single video. In our experiments, we reshape the video to 128 \u00d7 128 resolution. We use a fixed sampling rate of 8 frames per second following the the atomic action recognition setting in Girdhar & Ramanan (2019). Furthermore, we use the pre-generated max2action version of the dataset with only 2 objects moving in each time segment as described in https://github.com/rohitgirdhar/CATER/tree/master/generate. We use the provided training and validation splits for the max2action version of the dataset."}, {"title": "Training and Evalutation Details", "content": "In all our experiments, all hyperparameters values are chosen through grid search based on the best loss performance on the validation set."}, {"title": "Training Details", "content": "For MovingMNIST experiments, we train models for 100 epochs and pick the best one in terms of the self-supervised loss performance on the validation set. For both VJ-VCR and generative models we use a learning rate of 1e-3 and the Adam optimizer (Kingma & Ba, 2014) and a batch size of 256. In the deterministic setting, the models are trained to take 3 frames as input and predict the following 12 frames as output. In the non-deterministic setting, the models are trained to take 3 frames as input and predict the following 3 frames as output. In the case of generative models, we use weight decay of 1e-6. In VJ-VCR experiments, the variance and covariance regularization coefficients \u03b1 and \u03b2 are set to 0.5 and 0.1, respectively. In generative-baseline experiments, the variance and covariance regularization coefficients can optionally be set to 0.5 and 0.1, respectively.\nFor CATER and CLEVRER, we train models for a maximum of 20 epochs and pick the best one in terms of the self-supervised loss performance on the validation set. For both VJ-VCR and generative models we use a learning rate of le-3 and the Adam optimizer. We use a batch size of 256 and 160 for CLEVRER and CATER, respectively.\nIn the case of CLEVRER, the models are trained to take 6 randomly selected consecutive frames from a video in the training set as input and predict the following 20 frames as output. In VJ-VCR experiments with CLEVRER, the variance and covariance regularization coefficients \u03b1 and \u03b2 are set to 1 and 0.1, respectively. In generative-baseline experiments, the variance and covariance regularization coefficients can optionally be set to 1 and 0.1, respectively.\nIn the case of CATER, the models are trained to take 50 frames from a video as input and predict the following 50 frames as output while subsampling the original video from 24 frames per second to 8 frames"}, {"title": "Evaluation Details", "content": "During evaluation though speed probing (see section 4.3.1) with MovingMNIST and CLEVRER, we train a linear regression that takes the predicted hidden representations of the target frames from pre-trained VJ-VCR or generative models and outputs a scalar number for the speed of the desired object (the moving digit in the case of MovingMNIST or the fastest moving object in the last predicted frame in the case of CLEVRER). We use MSE loss and the Adam optimizer with a learning rate of 1e-3 and for batch size of 256 in all experiments except for the ones with CLEVRER and generative-based models in which case we use a learning rate of le-4.\nFor experiments with the stochastic version of MovingMNIST and a sparse latent variable z (see section 4.3.2) of dimension 20, we train a linear classifier that predicts the (discrete) switch in trajectory \u03c8 from the inferred $z^*$ for each video. We use a batch size of 256 and the Adam optimizer with a learning rate of le-3.\nDuring multi-label classification for action recognition with the CATER dataset (see section 4.3.2), we train a linear classifier that takes as input the inferred latent variable $z^* \\in R^{|A| \\times |y|}$ encoding the actions in the target frames y for each video and outputs the probabilities for each action in the set of possible actions A being present in the target frames. We use BCEWithLogitsLoss from pytorch as our loss functional, the Adam optimizer with a learning rate of 5e-4, and a batch size of 160."}]}