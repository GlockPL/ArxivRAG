{"title": "CourseAssist: Pedagogically Appropriate Question Answering System for Computer Science Education", "authors": ["Ty Feng"], "abstract": "The growing enrollments in computer science courses and increase in class sizes necessitate scalable, automated tutoring solutions to adequately support student learning. While Large Language Models (LLMs) like GPT-3.5 have demonstrated potential in assisting students through question-answering, educators have significant concerns about students misusing LLMs or LLMs misleading students with inaccurate answers. This paper introduces CourseAssist, a novel LLM-based tutoring system tailored for computer science education. Unlike generic LLM systems, CourseAssist leverages retrieval-augmented generation along with user intent classification and post-processing to ensure that responses align with specific course learning goals, thereby addressing the pedagogical appropriateness of LLMs in educational settings. I evaluate CourseAssist against a baseline of GPT 3.5 using a dataset of 50 question-answer pairs from a programming languages course, focusing on the criteria of usefulness, accuracy, and pedagogical appropriateness. Evaluation results show that CourseAssist significantly outperforms the baseline, demonstrating its potential to serve as an effective learning assistant 1. This work not only highlights the importance of deliberate design considerations in LLM-based educational tools but also opens up avenues for future research, particularly in understanding user interactions with such systems in real-world scenarios and integrating human educators into LLM-based tutoring systems.", "sections": [{"title": "INTRODUCTION", "content": "With the ever increasing enrollments in computer science and expanding class sizes, there is an acute need for scalable, automated tutoring solutions that can provide students with adequate support. Recent advances in large language models (LLMs) have shown their potential in helping students learn as a question answering tutor, but it is difficult to maintain academic integrity when students use LLMs to generate full solutions for assignments. While LLMs are already used by students for programming help, we are still in the early stage of exploring how LLMs may be used to best support learning and discovery by students and how to minimize the potential risks and harm that LLMs can cause in computer science education. When students are using LLMs without any guidelines or oversight by educators, the use of LLMs may result in over reliance on LLMs for code authorship, lack of comprehension of LLM code outputs, and learning misleading or suboptimal code solutions that are inconsistent with learning goals set for a specific course. These challenges may hinder students from learning computer science and becoming more adept at programming.\nWhile recent studies have explored the role of LLMs in programming education [5, 8, 12, 15], most of the studies have only studied generic LLM systems like ChatGPT without modifying LLM behavior or customizing it for computer science education. As an emerging technology, there is a lot of potential to explore the different ways to improve and enhance LLM capabilities and utility for education as well as the interaction between students and LLMs. One of the issues I am interested in is how LLMs can be steered and customized to align their responses with course-specific learning goals. I observed that LLMs can generate responses that conflict with learning objectives set by course instructors, and currently there is little option for instructors to control the behaviors of LLMs and how students should use them. Instead of attempting to ban the use of LLM systems like ChatGPT and Copilot in classrooms, I propose a harm-reductionist approach to address some of the challenges and issues with students using LLMs for computer science courses. While LLMs have remarkable natural language and code generation capabilities, they can be unreliable sources of truth due to their tendency to hallucinate and potentially low quality training data for computer science education. In an effort to address these issues that generic LLMs pose to learning and discovery, I developed CourseAssist, a custom LLM tutoring system that produces responses more aligned with the learning goals of a specific computer science course. In this paper, I evaluate CourseAssist on whether it generates more appropriate responses for a given computer science course compared to the baseline ChatGPT.\nSpecifically, I propose that pedagogical appropriateness should be considered when evaluating LLM-based tutoring systems. Pedagogical appropriateness means whether the system promotes learning on the student's part. One common method that human teachers use when tutoring students is giving hints, rather than direct answers to every question. The role of a tutor is to guide students to work through problems, rather than showing them full solutions. Thus, it is important for LLM-based tutoring systems to emulate this behavior and ensure LLM-generated responses are appropriate for learning. My main contributions in this paper are:\n\u2022 I introduce a LLM-based question answering system that provides course-specific guidance and hints using retrieval-augmented generation and user intent classification to ensure pedagogical appropriateness.\n\u2022 I present a comprehensive human evaluation method that considers usefulness, accuracy, and pedagogical appropriateness as the evaluation criteria.\n\u2022 I investigate the applicability of CourseAssist to act as a tutor in computer science courses by evaluating its responses on a Piazza QA dataset from a real CS course and compare it with the baseline ChatGPT 3.5."}, {"title": "RELATED WORK", "content": "Intelligent Tutoring Systems for Computer Science Education\nIntelligent tutoring systems (ITS) for computer science education (CSE) has a rich history of research since the 1980s. Early ITS for programming education use symbolic Al with a large set of predefined rules to find common bugs novice programmers make, and they offer hints and suggestions as a programmer types their code. One early example is a LISP tutor developed by Anderson et al. at Carnegie Mellon University in 1986 [1]. Their work introduced the \"model tracing\" methodology within the Lisp tutor, a system that constantly monitors students' problem solving steps by matching them against a database of possible correct and incorrect steps. When a student deviates from the correct path to solving a programming problem, the tutor offers hints to nudge them back onto the right track. The Lisp tutor demonstrated improved student learning outcomes by providing immediate, individualized feedback as they programmed. A major limitation of these early ITS is the rigidity of the system when recognizing students' mistakes. These systems are implicitly encoded with a rigid ideal student model, and it fails to account for different problem solving styles and approaches. Another downside of these early tutoring systems is the number of rules and predicates that need to be defined to represent the entire search space of possible code solutions. The Lisp tutor required hundreds of production rules, both correct and buggy, to model student performance and to cover the range of possible code solutions and potential errors. It is therefore resource-intensive to create ITS using a large set of predefined rules and predicates to model student programming.\nAccording to an overview paper by Hyacinth Nwana [10], an ITS usually consists of these four components:\n\u2022 A expert knowledge module that keeps track of the educational content.\n\u2022 A student module that models a student's activities and is a representation of the student.\n\u2022 A tutoring module responsible for making pedagogical decisions. It may offer hints, nudge students back to the right track, and intervene when students make mistakes.\n\u2022 A user interface module that allows students to interact with the tutor."}, {"title": "2.1", "content": "More recently, there has been some work incorporating modern statistical AI in ITS for computer science education. Figueiredo and Garc\u00eda-Pe\u00f1alvo [3] used a neural network to model and predict student failures based on data from monitoring student activities. Their system can identify students who are more likely to fail so that teachers can devote more resources to them to improve their programming skills. This neural network based predictive model is a way to improve the student modeling in the student module of an ITS using a more data-driven approach instead of the rigid rules-based methods from earlier ITS. With recent progress in large language models, LLMs may be used to improve the student and tutoring modules by generating contextually-appropriate responses and more accurately inferring and choosing the pedagogical decisions based on student's conversation with an AI tutor in real time, providing a highly personalized tutoring experience in a chat-based user interface. Since LLMs do not specifically encode knowledge, however, generic LLMs cannot be relied upon as sources of information due to their tendency to hallucinate. Thus, further work needs to be done to develop a expert knowledge module for LLM-based tutoring systems."}, {"title": "2.2 Large Language Models", "content": "Pre-trained on vast troves of text data from books, web pages, and code, transformer-based [14] large language models (LLMs) such as GPT-4 [11], LLaMA-2 [13], and Gemini [4] have shown remarkable capabilities of reasoning, instruction following, and natural language and code generation. During pre-training, language models develop their language understanding and generation capabilities by recognizing text patterns through the attention mechanism [14], which computes the similarity and relatedness of different groups of words in the training data. After pre-training, LLMs like GPT-3 and later models have shown the abilities of in-context and few-shot learning [2], which is the ability to recognize and perform a task based on text input at inference time. Without needing retraining, the model is able to be conditioned for a specific task by receiving instructions and a few examples in natural language. The few-shot and zero-shot learning abilities make LLMs suitable for building conversational agents, where a human end-user can interact with LLMs in natural language without requiring expert knowledge about training or fine-tuning LLMs. In a multi-turn fashion, humans and LLMs can collaboratively draft documents, write code, and perform complex tasks by conversing with LLMs.\nOne of the ways to adapt a generic LLM for a specific downstream task is prompt design or engineering. The idea of prompt design is to prepend a prompt consists of instructions and/or examples to the input text, thereby conditioning the language model to generate more relevant outputs. This prompt can also be learned based on data by training additional prompt parameters using the prompt tuning method [9]. More formally, the goal of prompting in general is to maximize the likelihood of correct generation Y given a carefully crafted or learned prompt P concatenated with input text X, with the language model parameters @ frozen:\n$\\max_{P} \\theta (Y|[P; X])$     (1)\nUsing this idea of prompting, I can condition language models based on course material during its generation so that I maximize the likelihood of more contextually-relevant responses that are better aligned with the specific curriculum and educational goals of a computer science course."}, {"title": "2.3 Early Explorations of LLMs in Computer Science Education", "content": "With an easy-to-use user interface, ChatGPT allows people to interact with LLMs like GPT-3 without the technical and hardware obstacles such as managing multiple GPUs for LLM inference. Students turn to ChatGPT and other LLMs for help with their courses. With the remarkable reasoning and few-shot learning capabilities, LLMs can be used as a personal tutor for computer science courses. In [12], Savelka et al. found that GPT-3 can obtain a substantial number of points (>55%) in assessments for introductory and intermediate university Python courses. While GPT-3 is able to solve more than half of the problems correctly, it struggles with problems that require chain of thought reasoning. Nevertheless, GPT-3 showed a remarkable ability to correctly answer multiple choice questions, generate code, and fix error based on autograder's feedback. Wieser et al. [15] found that ChatGPT can be used as a personal tutor that gives practice problems and step-by-step explanations. Hellas et al. [5] highlighted both the utility and limitations of LLMs in programming education, underscoring the need for a custom-built LLM system to better support pedagogical needs of computer science educators. Kazemitabaar et al. [8] showed that students who use LLMs to generate entire coding solutions fared worse on later programming tests than those who used a hybrid approach that required students to write some code themselves. While these early studies used the GPT-3 model off-the-shelf without much customization, other early studies on LLMs for education used more refined prompting techniques to enhance LLM capabilities. By implementing iterative querying to fix Python bugs in multi-pass fashion, Zhang et al. [17] demonstrated that Codex, a LLM trained on code, can be used to fix bugs and outperformed BIFI [16], a state-of-the-art Python syntax repair engine. I am interested in improving the alignment of LLM responses with course-specific learning goals by customizing LLMs to complement the code and natural language generation capabilities of LLMs with an expert knowledge module to ground LLM responses on course material."}, {"title": "METHOD", "content": "CourseAssist\nThe primary goal of this study is to evaluate whether LLMs can be used in the educational domain in a controllable manner. I developed CourseAssist, a question answering system, by augmenting GPT 3.5 with retrieval of course content and user intent classification. To implement the retrieval-augmented generation pipeline, I first parsed the text in course materials such as lecture slides, assignment sheets, and practice problems from a programming languages course. Since most of the course content come from presentation slides, page is a good semantic unit for chunking the text. For each text chunk of course content, I created a dense embedding vector using OpenAI's Ada model, which is a transformer-based model that can capture semantic meaning of text. Once the corpus is vectorized, all the embedding vectors are stored in FAISS [7], a vector database, to allow for vector similarity search. When a user asks a question on CourseAssist, the user query is first vectorized using Ada, and then vector similarity search is performed to find the top k most relevant course content chunks. This is done by computing the L2 distance between the embedding vector of the user query and vector representations of the course material. Based on the user question and the retrieved content, the system classifies the user question into two categories: lecture questions or assignment help. This step is necessary to ensure CourseAssist behaves appropriately for different use cases. The user intent classification is done through zero-shot prompting of GPT 3.5. The prompt used to classify user intent is shown in Prompt 1.\nIf the question is classified as a lecture question, CourseAssist would use the top k course content texts to generate a relevant answer. Otherwise, if the question is about an assignment, CourseAssist would first generate an answer based on the retrieval, and then pass the generated response to a post-processing step to ensure pedagogical appropriateness. This post-processing step evaluates whether the answer contains full solutions to a homework problem. If it does, the answer would be rewritten to only contain hints and step-by-step instructions. The zero-shot prompt to GPT 3.5 that handles the post-processing step for assignment questions is shown in Prompt 2."}, {"title": "3.1", "content": "Prompt 1: zero-shot user intent classification\nKnowledge from the course material:\n<RETRIEVED_CONTENT>\nConversation with student so far:\n<CONVERSATION_HISTORY>\nUser question: <USER_QUESTION>\nClassify the above user question into these categories:\n{lecture, assignment}.\nOnly output the category label:"}, {"title": "3.2", "content": "Prompt 2: zero-shot post-processing for assignment questions\nYou are a detector for bad AI generations in a tutoring app.\nYour job is to detect whether the AI generated response is appropriate for students. You should ensure that the AI is only giving hints and not full solutions to help students learn. If the AI generated response in the following conversation has too much direct solution, rewrite the response to give only helpful hints but not the full solution. Otherwise, you can return the original AI response.\n<conversation_begins>\nStudent question: <QUESTION>\nAI generated response: <ANSWER>\n<conversation_ends>\nIf the previous response has any code solutions, rewrite the previous response here to only give helpful hints, but not the full solution. If the previous response does not have any code solutions, simply return the previous response.\nAssistant:"}, {"title": "DATASET", "content": "For the experiments in this study, I sampled 50 question answer pairs from 185 Piazza posts from a programming languages course at UC Davis. I rewrote all of the student questions to preserve their semantic meaning while improving the clarity and conciseness. To ensure data quality of the answers, I used the instructor or TA's answer for each question and discarded answers by other students. None of the question answer pairs have any personal information, and the data is fully anonymized. Of the 50 questions, 33 are homework related questions, and 17 are conceptual questions about lecture materials. Further information about the dataset can be found in Table 1. The dataset and evaluation rubric can be found here, https://github.com/ytyfeng/CourseAssist-Data."}, {"title": "4", "content": "RESULTS\nTo evaluate the system, I compare CourseAssist answers to the 50 Piazza questions to answers generated by the baseline ChatGPT 3.5. Previous studies [6] have shown that automatic metrics, such as BLEU, Meteor, Rouge-L, and Cider, are not effective at evaluating usefulness of generated text. Therefore, I conducted a human evaluation to evaluate usefulness, accuracy, and pedagogical appropriateness of the generated responses. On each evaluation criterion, I score the generated response on a three-point scale of 0, 0.5, and 1. The evaluation rubric can be found in Table 2.\nAll experiments were run two times to account for variability in LLM responses, and the evaluation scores are the average of the two runs. The results showed that CourseAssist outperformed the baseline GPT 3.5 in all of the evaluation criteria. Not only CourseAssist is more useful and accurate than GPT 3.5, every answer it generated is pedagogically appropriate due to the user intent classification and post-processing steps. The results can be found in Table 3."}, {"title": "5", "content": "DISCUSSION AND FUTURE WORK\nCourseAssist's significant improvement on the three evaluation criteria compared to the baseline model, GPT 3.5, underscores the potential of specialized tutoring systems. However, further empirical studies are needed to understand its effectiveness in real-world educational settings. It is imperative to conduct qualitative user research to identify potential obstacles learners might encounter with CourseAssist. An unexplored yet critical aspect is facilitating learners to develop an appropriate level of trust in LLM-based educational tools. Insufficient trust may deter the utilization of such systems, while excessive trust could mislead students, particularly when LLMs make mistakes. Additionally, exploring ways for integrating human educators into CourseAssist's question answering process merits attention. Despite improving the accuracy of responses from GPT 3.5's 0.71 to CourseAssist's 0.92, there are still instances where CourseAssist gave partially inaccurate answers. Incorporating a human-in-the-loop approach could improve the accuracy of LLM-generated answers, while still benefiting educators by reducing their workload."}, {"title": "6", "content": "CONCLUSION\nIn this study, I introduced CourseAssist, a LLM-based course-specific tutoring system that uses retrieval-augmented generation and user intent classification to generate pedagogically appropriate answers. I presented a comprehensive human evaluation method that considers pedagogical appropriateness. When evaluated on a Piazza QA dataset, CourseAssist significantly outperformed the baseline on all of the evaluation criteria. Future work includes more user studies to better understand the efficacy of CourseAssist when deployed in computer science courses in the real-world."}]}