{"title": "Trustworthy and Efficient LLMs Meet Databases", "authors": ["Kyoungmin Kim", "Anastasia Ailamaki"], "abstract": "In the rapidly evolving AI era with large language models (LLMs) at the core, making LLMs more trustworthy and efficient, especially in output generation (inference), has gained significant attention. This is to reduce plausible but faulty LLM outputs (a.k.a hallucinations) and meet the highly increased inference demands. This tutorial explores such efforts and makes them transparent to the database community. Understanding these efforts is essential in harnessing LLMs in database tasks and adapting database techniques to LLMs. Furthermore, we delve into the synergy between LLMs and databases, highlighting new opportunities and challenges in their intersection. This tutorial aims to share with database researchers and practitioners essential concepts and strategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining in the intersection between LLMs and databases.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have recently transformed various fields with their ability to understand and generate human-like text. In the database domain, researchers are leveraging LLMs to tackle complex data management tasks [55, 194]. LLMs can function not only as assistants for database administrators (DBAs) [271, 340] but also as internal components of database systems, optimizing query plans [8, 168] and translating natural languages to SQLs [224].\nBeyond these applications, key concepts and advancements from the LLM community remain underexplored by database researchers. This tutorial aims to bridge that gap by focusing on enhancing the trustworthiness and efficiency of LLMs. Improving trustworthiness involves reducing hallucinations [124] to ensure LLMs generate accurate, factual responses, thereby increasing their reliability in database tasks requiring precise answers and reasoning. Enhancing efficiency focuses on decreasing inference latency and boosting throughput.\nInference efficiency is particularly important because, while training LLMs demands substantial resources and expertise, inference occurs daily across numerous users, leading to significant operational costs. For instance, OpenAI handles millions of requests, incurring substantial monthly expenses to run ChatGPT [73, 215]. Integrating LLMs with external data sources, such as vector databases and document retrieval systems in retrieval-augmented generation (RAG) [154], increases the number and complexity of LLM calls, especially with longer inputs. Recent trends in chain-of-thought and multi-path reasoning, exemplified by models like OpenAI's 01 [205], further amplify inference demands, as generating final answers may require multiple LLM calls to enhance trustworthiness.\nFrom a systems perspective, improving LLM inference efficiency parallels database management system (DBMS) development, presenting opportunities for database researchers to contribute to creating more efficient LLMs, promoting economic and environmental sustainability by reducing the CO2 footprint associated with extensive GPU usage.\nAfter introducing the essential ideas in making LLMs more trustworthy and efficient, we will explain the intersection of LLMs and databases with new challenges and opportunities."}, {"title": "1.1 Target Audience and Prerequisites", "content": "Our tutorial is designed for conference attendees, focusing on three key areas to maximize engagement:\nTrustworthy LLMs (Section 2.1): Aimed at individuals seeking to effectively utilize large language models (LLMs) in database tasks with minimal errors. Prerequisites include experience with LLMs like ChatGPT and the distinction between training and inference in machine learning. No in-depth knowledge of LLM internals is required.\nEfficient LLMs (Section 2.2): Targeted at those interested in enhancing LLM inference efficiency or contributing to the development of fast LLM inference systems by applying database techniques. Prerequisites include basic database knowledge and an understanding of GPUs. Familiarity with Transformer architecture, attention mechanisms, and key-value (KV) caching is advantageous.\nLLMs Meet Databases (Section 2.3): Intended for participants exploring new research opportunities at the intersection of databases and LLMs. A background in databases, including OLAP, relational algebra, cost-based query optimization, and approximate/adaptive query processing, will be helpful.\nOur goal is to bridge the gap between essential LLM knowledge and the database community, enabling researchers already utilizing LLMs to uncover and develop unexplored ideas. Rather than merely listing state-of-the-art papers, we employ consistent visuals and focus on core concepts and insights, facilitating a deeper understanding and navigation of the evolving LLM landscape."}, {"title": "1.2 Tutorial Length", "content": "The intended length of this tutorial is 1.5 hour, with 40, 30, and 20 minutes each for Sections 2.1, 2.2, and 2.3, respectively."}, {"title": "2 Tutorial Outline", "content": "The tutorial is structured into three main sections, addressing critical aspects of LLMs and their interplay with database systems."}, {"title": "2.1 Trustworthy LLMs", "content": "The first part of the tutorial explains the efforts to reduce hallucinations and make LLMs more trustworthy, using an analogy that LLMs resemble humans. We explain background (Section 2.1.1), how LLMs can solely improve (Section 2.1.2), how LLMs can improve by interacting with the external world (Section 2.1.3), and how LLMs can automatically make such decisions and interact with other LLMs (Section 2.1.4)."}, {"title": "2.1.1 Background", "content": "Large Language Models (LLMs) function as text-in, text-out systems, generating texts based on their training. Training an LLM is akin to nurturing a child: by exposing it to extensive text data, the model acquires world knowledge and reasoning abilities. This process involves predicting the most probable next token in a sequence, a type of self-supervised learning. For a sequence of tokens, the model learns to predict the latter tokens based on the preceding ones, enabling it to generate coherent text continuations.\nFine-tuning refines this process for specific tasks or domains, similar to how individuals specialize in particular professions. In contrast, in-context learning provides additional information or examples within the input without altering the model's parameters, akin to consulting external references during an open-book exam. Many prompting techniques [19, 34, 112, 240, 247, 258, 275, 319] including chain-of-thought prompting [144, 289] and its variants [18, 312] may leverage in-context learning to enhance performance.\nDuring inference, LLMs generate texts autoregressively, producing one token at a time. This process may involve deterministic methods like greedy or beam search, or probabilistic approaches such as nucleus sampling [80, 114], which helps avoid selecting low-probability tokens.\nHowever, LLMs experience hallucinations [13, 124, 270, 305], generating plausible-sounding but incorrect or fabricated information. This is an unavoidable aspect of LLMs [13, 305] which arises from limitations in capturing real-world knowledge, inherent approximations in training and inference, input noise, etc. Even slight input perturbations can significantly influence hallucinations [65, 101], and the detection of hallucinations has been a major problem [47, 54, 77, 195, 204, 233, 264].\nAdditionally, the lost-in-the-middle problem [117, 181] indicates that LLMs may struggle to utilize information located in the middle of long contexts, often performing better when relevant information is at the beginning or end of the input, exhibiting a U-shaped performance curve. This phenomenon has been attributed to inherent attention biases within LLMs, where tokens at the start and end of the input receive higher attention, regardless of their relevance [117]. This tendency can lead to increased hallucinations as context lengthens [230].\nScaling laws [113, 130, 221] explain that error rates decrease as model size and training data increase, with optimal scaling requiring proportional growth in both [113]. However, this may not hold for smaller models [221]. Laws can also relate to temporal loss in the training curve [297], downstream tasks [121], model quantization [306], transfer learning [14, 111], number of generated samples [27], and inference time [205] with the advance of using long, complex reasoning paths. Due to automatic prompting techniques [43, 136, 255, 302, 330] and that larger models tend to be less sensitive to prompt variations [75], we focus less on prompting techniques.\nTarget. The audience will distinguish pre-training, fine-tuning, and in-context learning phases of LLMs, and understand the inherent challenges in making LLMs trustworthy."}, {"title": "2.1.2 Improving Bare LLMs", "content": "We briefly explain the approaches to improve the LLM itself to make it more trustworthy. Since LLM is a specific class of machine learning (ML) models, general ML approaches to enhance accuracy may work for LLMs. However, as such approaches have been extensively studied from the classic ML era, we target more LLM-specific approaches.\nAs it is infeasible to increase the model size indefinitely, and the models typically follow the Transformer architecture [274], efforts have been put to increase or augment training data (where LLMs themselves can be used to generate data) [36, 64, 82, 88, 142, 157, 163, 235, 241, 279, 341], improve data quality (again, LLMs can be used to clean data) [23, 66, 72, 103, 143, 328], make inferences more robust [91, 276], and apply better training and fine-tuning methods. Specifically, fine-tuning covers a broad spectrum of work for example, parameter-efficient fine-tuning (PEFT) [116, 119, 152, 162, 222], instruction tuning [51, 198, 199, 243, 285, 288], reinforcement learning from human feedback (RLHF) [30, 52, 57, 81, 102, 134, 167, 208, 250, 262], and direct preference optimization (DPO) [83, 138, 234, 338]. RLHF leverages human feedback to train a reward model in reinforcement learning (RL), guiding the LLM through RL to produce desired outputs. DPO simplifies the alignment process by directly optimizing the policy model without a separate reward model. While these approaches rely on RL that continuously interacts with human or the world external to LLMs, such interactions are often limited to training and do not occur in inferences, which we explain in the following sections.\nOther than the training methods, a new model architecture of differential Transformer [315] reduces the distractions of the models to focus on unnecessary information in the long context, which works similarly to robust decoding strategies [91, 276].\nTarget. The audience will learn about tuning LLMs to make them more trustworthy and aligned with user intentions."}, {"title": "2.1.3 Making LLMs Interact with the World: Adding Eyes and Hands", "content": "LLMs alone can encounter knowledge, memory, and capability limitations [326]. Their knowledge is confined to the static information encoded during training, leading to potential inaccuracies over time. Memory constraints arise from limited context windows, hindering the handling of extended conversations. Additionally, their text-based nature restricts interactions with the physical world. To address these challenges, LLMs can retrieve knowledge, memory, and tools.\nThis section focuses on what and how to retrieve. When to retrieve is the key to autonomy and will be detailed in the next section. Knowledge retrieval is represented by well-known retrieval-augmented generation (RAG) [76, 90, 126, 154, 166]. Based on the data type, it can fetch knowledge from knowledge graphs [38, 109, 169, 211, 225, 245, 266, 301, 309], tables [9, 22, 40, 45, 96, 98, 115, 128, 150, 158, 190, 265, 320], images [41, 42, 314], not just documents. The data may be chunked/vectorized, stored in vector databases, then similar chunks are searched online. While vector similarities are typically used, more advanced similarity scores are possible, e.g., using dual or cross encoders [203, 239].\nMemory retrieval attempts to overcome the limited context size of LLMs by storing previously seen tokens as key-value pairs [25, 183, 193, 281, 294] and fetching relevant pairs in upcoming requests, managing memory stores in hierarchical or partitioned way [145, 287] or even as a database [118]. Fetching information from long input can also be done without maintaining a separate memory store, but by sparsifying the model layers [15, 186]. One can relate low-rank adapters and mixture-of-experts [29, 71, 78, 110, 119, 155, 228, 292] with memory retrieval since lightweight model parameters are fine-tuned per specific task and domain, and dynamically fetched at online inferences.\nTool retrieval searches for the APIs to interact with external environments [188, 197, 218, 229, 232, 246, 286, 300]. One can connect LLMs with databases to call SQLs that can help answering user questions [22]. Constrained decoding [20, 69, 92, 93] allows output to follow specific structure which can increase correctness and efficiency.\nThe challenges in retrieval include the followings. 1) Heterogeneity: LLMs are text-based, but knowledge can be of any type. Even for text retrieval, heterogeneous lengths and intents between queries and documents can lead to suboptimal retrieval accuracy [74, 89], and the vector-similarity search may be too simple to retrieve necessary information [87, 210]. 2) Scalability: Not only that LLMs have limited context or data they can utilize per inference, but maintaining a large set of retrieval entities and retrieving a subset may incur overheads [269, 303]. While approximation can mitigate the search overhead and make the search negligible to LLM inference costs, it is limited to vector-similarity search, and generalization to more complex searches [131, 135, 137, 239] remains challenging. 3) Sparsity: This is also relevant to data sparsity and noise [56], where relevant data is sparse compared to large information pools. 4) Reliability: Retrieved knowledge may be imperfect [277].\nTarget. The audience will understand how LLMs can interact with the world and exploit external knowledge to overcome the limits in using LLMs alone."}, {"title": "2.1.4 Making LLMs Self-drive: Adding Brain", "content": "Now we have more powerful models and interactions with the world. The last part is how we can make LLMs smart enough to maximize these capacities, adding autonomy. Self-consistency and major voting enables a simple yet effective solution for increasing consistency [283], however, it fails to generate accurate and diverse answers [33, 44, 46] and is yet passive. More active approaches include self-reflection and adaptive retrieval [11, 32, 123, 125, 159, 185, 331], which adaptively retrieves information multiple times based on the generated output, model confidence, query complexity, or fine-tuned policies. This is particularly helpful for chain-of-thought/multi-hop reasoning and question answering [175, 278, 282, 284, 329].\nThe next step is to use multiple reasoning paths instead of a single path. This multi-path reasoning has been an effective approach for driving LLMs [224, 226, 324, 332]. While the exact mechanism remains closed, OpenAI's o1 model is assumed to plan subtasks, conduct these, and revise the results to decide whether to extend the current plan or generate different plans, forming a tree-like reasoning structure. They suggest a new scaling law that LLM accuracy increases with inference time, not only with training time and data [205].\nAgentic LLM indicates that LLMs can act as agents, selecting actions based on observations [49, 179, 256, 313]. Multiple agents exploit collaborative reasoning, parallel processing, diversity, and specialization akin to humans [35, 104, 172, 212, 214, 227, 299]. Semantic variables [173] regard LLM input and output tokens as dependent variables to explicitly model control flows.\nA broader view includes compound AI [323] where AI and non-Al components interact with each other, including retrievals, control flows, agentic LLMs, and more. An interesting example is automated research process [187, 259].\nTarget. The audience will learn about approaches to make LLMs self-driving and build systems around LLMs for complex tasks."}, {"title": "2.2 Efficient LLMs", "content": "The second part of the tutorial demystifies the internals of LLM inference process and explains the efforts to make it more efficient, using an analogy that LLMs behave as DBMSs. We explain background (Section 2.2.1) and how LLM inference systems resemble DBMSs in improving their efficiency (Section 2.2.2). We then explain further work for each dimension of operation (Section 2.2.3), data (Section 2.2.4), hardware (Section 2.2.5), and workload (Section 2.2.6)."}, {"title": "2.2.1 Background", "content": "The dominant Transformer architecture employs an attention mechanism [274] that calculates similarity scores between a token and its preceding tokens, effectively capturing inter-token relationships and managing extended contexts. This process has quadratic complexity, but key-value (KV) caching [58] optimizes it by storing and reusing these computations, reducing the complexity to linear during inference. Non-attention operations mostly consist of matrix multiplications and activations.\nInference in Large Language Models (LLMs) involves two primary phases: prefill and decode. During the prefill, the model processes input tokens to generate the initial output token. The attention operates with quadratic complexity due to the absence of precomputed KVs, making it compute-intensive. During the decode, the model generates subsequent tokens sequentially, each time using the last generated token as input. Here, the attention leverages KV caching, resulting in linear complexity relative to the number of processed tokens and reading their KVs, which makes this phase more memory-intensive.\nIn case of multiple requests, they face a race condition as in multi-tenant systems. If the GPU memory is insufficient to keep all requests' KVs, some running requests are preempted (evicted), releasing their KVs from the memory, and restarted (refilled) later [146]. Due to the low PCIe bandwidth, the released KVs are often recomputed when restarted, instead of offloading to other storage devices and loading back. Multiple requests in either prefill or decode steps can be batched to amortize the cost of loading model weights from GPU memory.\nNote that the model weights also occupy the GPU memory. When the model size exceeds a single GPU capacity, techniques like model and pipeline parallelism [105, 120, 257] distribute model weights across multiple GPUs. This partitioning introduces data transfer overhead between GPUs.\nTarget. The audience will understand the KV caching and different phases of LLM inference requests, and how they compete for the same GPU resource."}, {"title": "2.2.2 LLM Inference Systems: LLMs Behave as DBMSs", "content": "LLM inference systems (e.g., VLLM [146]) behave similarly to (in-memory) DBMS. KVs and model weights correspond to the data, which are maintained in GPU memory. Operators include matrix multiplications, activations, attentions, and data transfers. Compared to OLAP in databases, the operations are simpler yet much more time-sensitive, where the requests should be served in real-time.\nSignificant efforts have been made to increase the efficiency of LLM inference [342], largely based on operating and database systems. ORCA [318] forms a new batch of requests after each iteration (of prefills and decodes) whenever resources are available. Thus, a new request does not have to wait for all current running requests to finish, just like the pipelining in OS. VLLM [146] adopts paging and virtual memory to manage KVs, reducing memory fragmentation and enlarging the batch size. Since prefills are typically more costly than decodes, making stalls for decodes when batched together, SARATHI [4, 5] chunks prefills to reduce pipeline bubbles. Some other work [217, 263, 339] rather disaggregates the prefills and decodes into different GPUs, so the workload for each GPU is homogeneous. VTENSOR [298] decouples the KV cache management and attention computation of VLLM for better flexibility and performance. NANOFLOW [343] splits each batch into nano-batches for finer-grained pipelining, increasing the overlap of computation, memory operation, and data transfer between GPUs. It also hides CPU scheduling latency by asynchronous scheduling. INFINIGEN [149] offloads the KVs to CPU memory to extend the KV cache and reloads the KVs from CPU layer-wise, but fetches a subset of KVs for efficiency, similarly to sparse attentions (Section 2.2.3). INSTINFER [213] offloads KVs and attention computations to flash drives, just like the storage-disaggregation and computation pushdown in databases [310]. NEO [127] selectively offloads attention computations and KVs from GPU to CPU, in order to maximize both GPU and CPU utilization.\nTarget. The audience will understand why LLMs behave similarly to DBMSs and how database techniques can improve LLM inference efficiency. In subsequent sections, the audience will learn about efforts and challenges in further improving efficiency in four dimensions: operation data, hardware, and workload."}, {"title": "2.2.3 Operation: Attention", "content": "While matrix multiplications take the major portion in LLM latency in general [5], attentions can dominate the runtime for large inputs due to their quadratic complexity. FlashAttention [59, 60, 249] has become a de facto standard as an efficient attention implementation, utilizing recent GPU technologies to boost the inference speed. The ideas include kernel operator fusion and GPU cache-aware KV transfer. As in approximate query processing (AQP) in databases, sparse attentions [15, 178, 186] do not compute the full attention scores for all preceding tokens but a subset as an approximation. Some attentions rather optimize for long contexts [2, 62]. FlexAttention [107] offers flexible and performant implementation of such attentions."}, {"title": "2.2.4 Data: KV and Model Weights", "content": "Reading KVs from GPU memory in decode-attentions is similar to sequential table scan. As KVs are maintained per each attention layer, reading KVs for a layer can overlap with other layers' operators [149, 263]. While offloading KVs and attention computation have been popular recently [127, 149, 213], we need to be careful as it is challenging to predict the output lengths of LLM requests and thus their utilization patterns, and a KV for a single token may consume a few MBs. KVs of long documents can be precomputed, compressed, and fetched for later retrievals [184]. To reduce memory latency, one can opt for KV sharing across different attention heads [7, 26, 50], KV compression [63, 129, 176, 184, 236], model quantization [94, 97, 106, 147, 164, 200, 251, 295, 306], or different model architectures than Transformer, such as State Space Models (SSMs) [61, 100] that do not rely on attentions, thereby not generating KVs. While hybrid architectures [10, 67, 99, 108, 171, 223, 237] can balance between the efficiency of SSMs and memorization capacity of Transformers, SSMs remain niche in the market [17]. A recent work even shows that tokenizers can be removed from the models [209]."}, {"title": "2.2.5 Hardware: Theory and Practice", "content": "We briefly explain 1) the roofline model [322] and 2) some efforts to overcome the hardware limits [161, 238, 253, 261, 317, 333] or leverage advanced hardware for LLM inference [170, 304]. The roofline model is based on the computation speed (e.g., GPU FLOPS) and memory bandwidth, which acts as a theoretical hardware bound and determines whether an operator is compute-intensive or memory-intensive across different inputs."}, {"title": "2.2.6 Workload: Scheduling, Prefix Sharing, Speculation", "content": "To handle multiple LLM requests, LLM inference systems implement request scheduler to send LLM requests to appropriate machines or GPUs to maximize throughput or minimize latency. Assuming independent requests, early schedulers either prioritize prefills [146] or decodes [4], which tend to optimize latency or throughput, respectively. More complex schedulers consider fairness [252, 291] while compromising performance, or predict the output lengths of requests (not known in advance) and schedule shorter requests first [85, 231, 337].\nIf different LLM requests can share a prefix in their inputs, the KVs of the prefix can be stored just once and reused for multiple requests [334, 335]. This forms a trie structure with shared prefixes. However, a single-token difference in inputs may invalidate the sharing of KVs of all subsequent tokens. To increase the sharing opportunity, [311] uses the KVs of multiple token sequences to approximate the KVs of the concatenated sequence. The mechanism is similar to the speculation in OLAP [260] and healing protocol in transactions [293] in databases.\nThis speculation and healing patterns also appear in speculative decoding [153] and model cascades [39, 177, 316, 325, 336], accelerating the generation of tokens by leveraging smaller, faster models then validate the tokens using larger models, since the validation costs less than the generation."}, {"title": "2.3 LLMs Meet Databases", "content": "The last part of the tutorial discusses the intersection between LLMs and databases, opportunities and challenges in how we can exploit LLMs for databases, how the development of databases can help LLMs, and how we can exploit new types of workloads and integrations of LLMs and databases. We explain from more well-known to more untapped, deeper integrations in Sections 2.3.1-2.3.5 and provide more proactive visions in Section 2.3.6."}, {"title": "2.3.1 LLMs for DBs: DBAs and DBMS Internal", "content": "We briefly explain how LLMs are utilized for well-known tasks of DBAs and DBMS internals such as database tuning [271, 340], text2sql [151, 224] and query optimization [8, 168]. As we mentioned in Section 1, we will not cover every detail, as many of these efforts are covered in a previous tutorial [194] and its additional list of papers [55]."}, {"title": "2.3.2 DBs for LLMs: Adaptive Cost-based Scheduling", "content": "Unlike the sophisticated query optimizers in DBMSs, LLMs lack cost models and cost-based scheduling of LLM requests. [3] measures the batch times across various inputs (number of tokens to process and KV size to read). [322] computes batch times based on the roofline models. These can be used to model batch times and formulate the problem of finding optimal schedules as a constrained satisfaction problem (CSP) [139]. While schedulers try to avoid preemptions to maximize performance, [139] shows that harnessing preemptions can rather reduce overall latency compared to zero-preemptions. As the exact hardware utilization of each request is not known in advance, the scheduling should be adaptive based on the observations, and it has not been explored much to schedule dependent requests connected via semantic variables or shared prefixes [139]."}, {"title": "2.3.3 DBs with LLMs: Mixed Relational-LLM Workload", "content": "Not only solving existing tasks with LLMs, LLMs offer new functionalities when integrated into DBMSs. Semantic operators [216] extend relational operators to batch-process the tabular data with LLMs (e.g., filters and joins using LLMs), which can be regarded as an AQP. Workloads with LLMs provide a justification to use LLMs inside DBMSs (heavy LLM calls in plan optimization can be negligible compared to query execution with LLMs). However, different pipelines (with semantic operators) lead to different accuracy and efficiency, thus defining the equivalence between two pipelines is non-trivial. Furthermore, more complex pipelines or LLM calls do not always guarantee higher accuracy [37, 74], and searching similar entities with LLMs can be replaced with efficient vector-similarity searches [177, 242] as a type of model cascade."}, {"title": "2.3.4 DBs with LLMs: Multi-objective Query Optimization and Benchmarks", "content": "The challenge is therefore how we can automatically find good pipelines for mixed relational-LLM workloads under the multi-objective of accuracy and efficiency [22, 273, 321] as in compound AI systems [95, 244, 267]. This calls for development of accurate cost models and accuracy-prediction models for LLMs and mixed relational-LLM workloads, in order to enable the holistic optimization of query plans consisting of both relational and non-relational operators. The cost model itself can be learned via LLMs (or any ML models), possibly using RLHF or feedback from query execution without human intervention, where such an automatic training data generation is one of the advantages of solving database tasks compared to conventional ML tasks (e.g., natural language processing with human-labeled translation data) [140]. Another model for predicting the output accuracy or detecting hallucination may be chosen from the scaling laws (using the general fact that larger models are more trustworthy) or separately trained.\nTo balance efficiency and accuracy, during the physical query optimization we should select proper models (ones used for execution) to avoid calling heavy LLMs unnecessarily. Depending on the complexity of the task, simple ML models with a small set of supervised data [123], or larger deep generative models such as in tabular foundation models tailored to domain-specific data [141, 160, 308], or LLMs with world knowledge and reasoning capacity [296] can fit the task with different accuracy-efficiency trade-offs. Small language models (SLMs) [1, 16, 67, 189, 196, 202, 226] are also a good choice. Automatically finding the best prompt configuration [136, 280] tailored to the mixed workloads and more (e.g., previously mentioned fine-tuning or multi-hop/multi-path reasoning with adaptivity during inference) might be desired.\nFurthermore, unlike the TPC benchmarks for databases, another problem is that there is no comprehensive benchmark for relational-LLM workloads yet. [22, 182] provide exploratory benchmarks without focusing on semantic joins."}, {"title": "2.3.5 DBs with LLMs: Integrated System", "content": "Other than the cost models, we also need DBMSs with native LLM support to increase the optimization opportunities, alike systems for relational-vector workloads [242, 327]. Current prototypes for relational-LLM workloads [177, 182, 216] separate table processing (e.g., pandas [192]) and LLM inference engine (e.g., vLLM [146]).\nTo maximize efficiency and scalability, we should focus on hardware utilization, data movement [122], caching hot data, locating computations close to data (e.g., computation pushdown in storage-aggregation setting) [84, 191], asynchronous API calls [95], balancing loads, and multi-tenancy just like in DBMSs [248, 310]. One also has to decide whether to maintain a separate vector database for faster online vector retrievals, or use just-in-time vectorization for reducing storage overhead. This also applies to precomputing KVs of data tokens [184] for faster LLM inferences or not, but with a higher caution as KVs are typically larger than vectors."}, {"title": "2.3.6 Convergence and Future", "content": "We envision LLMs and databases to converge (e.g., neuro-symbolic systems [48, 79, 268, 321]), more than just applying the techniques from one domain to another. A new LLM inference system optimized for DBMSs might be developed from an open-source cloud DBMS, utilizing recent implementations and optimizations for processing relational operators, such as storage-disaggregation and computation pushdown for scalable data and model management [310], GPU-based OLAP processing [86, 219, 220] for the full use of GPUs for both relational operators and LLMs, hybrid operators with heterogeneous data transfer paths [53, 310], adaptive query execution [307] and more. A unified query optimizer and data model for both relational data, KVs, and model weights, could offer opportunities for better data management and hardware utilization. Finally, if we look into the near future, we could also harness the emerging CXL technology for memory disaggregation [6, 148, 180] to manage model weights and KVs, and increased interest in pruning unnecessary data in OLAP [21, 24, 174, 206] could lead to higher trustworthiness (due to reduced noise) and efficiency (due to less data to process) in the relational-LLM workloads, with connections to online aggregation [254] and incremental view maintenance [28].\nTarget. The audience will understand the different depths of LLM-database integrations and be able to find interesting research topics from each of the integration, which are closely related to the current and near-future trends of databases."}, {"title": "3 Related Tutorial", "content": "Xupeng et al. [194"}]}