{"title": "OriGen: Enhancing RTL Code Generation with Code-to-Code\nAugmentation and Self-Reflection", "authors": ["Fan Cui", "Chenyang Yin", "Kexing Zhou", "Youwei Xiao", "Guangyu Sun", "Qiang Xu", "Qipeng Guo", "Demin Song", "Dahua Lin", "Xingcheng Zhang", "Yun (Eric) Liang"], "abstract": "Recent studies have illuminated that Large Language Models (LLMs)\nexhibit substantial potential in the realm of RTL (Register Transfer\nLevel) code generation, with notable advancements evidenced by\ncommercial models such as GPT-4 and Claude3-Opus. Despite their\nproficiency, these commercial LLMs often raise concerns regarding\nprivacy and security. Conversely, open-source LLMs, which offer\nsolutions to these concerns, have inferior performance in RTL code\ngeneration tasks to commercial models due to the lack of high-\nquality open-source RTL datasets.\nTo address this issue, we introduce OriGen, a fully open-source\nframework featuring self-reflection capabilities and a dataset aug-\nmentation methodology for generating high-quality, large-scale\nRTL code. We propose a novel code-to-code augmentation method-\nology that leverages knowledge distillation to enhance the quality of\nthe open-source RTL code datasets. Additionally, OriGen is capable\nof correcting syntactic errors by leveraging a self-reflection process\nbased on feedback from the compiler. The self-reflection ability of\nthe model is facilitated by a carefully constructed dataset, which\ncomprises a comprehensive collection of samples. Experimental\nresults demonstrate that OriGen remarkably outperforms other\nopen-source alternatives in RTL code generation, surpassing the\nprevious best-performing LLM by 9.8% on the VerilogEval-Human\nbenchmark. Furthermore, OriGen exhibits superior capabilities in\nself-reflection and error rectification, surpassing GPT-4 by 18.1% on\nthe benchmark designed to evaluate the capability of self-reflection.", "sections": [{"title": "INTRODUCTION", "content": "Recently, large language models (LLMs) have demonstrated out-\nstanding performance in natural language comprehension and gen-\neration [11]. Studies have shown that LLMs exhibit considerable\nability in code generation tasks [21]. Commercial LLMs, such as\nGPT-4 [1] and Claude3-Opus [2], generate high-quality software\ncode for common programming languages like C++ and Python,\nwhich enhance coding productivity significantly. Moreover, LLMs\nhave also illustrated impressive capabilities in hardware code gener-\nation, especially for the Register Transfer Level (RTL) code. Specif-\nically, RTL code generation from natural language instructions\npresents an innovative approach to enhance hardware develop-\nment productivity, offering the potential to revolutionize existing\nHDL coding workflows by alleviating the burdensome task of HDL\ncoding for designers.\nWhile commercial LLMs have demonstrated proficiency in gen-\nerating RTL code, they often raise concerns regarding privacy and\nsecurity. These issues are especially critical in the domain of hard-\nware design, as RTL code frequently contains valuable intellectual\nproperty. Furthermore, the closed-source nature of these LLMs\nrestricts researchers from conducting in-depth analyses and cus-\ntomizations, impeding further fine-tuning of the models in specific\ndomains, such as hardware design. In contrast, open-source mod-\nels provide better privacy and security while facilitating further\nimprovement and customization. Open-source models, such as Star-\nCoder2 [15], DeepSeek-Coder [9], and CodeQwen-V1.5 [3], have\ndemonstrated promising results in code generation for prevalent\nprogramming languages like Python and C/C++, while their per-\nformance in RTL code generation still lags behind GPT-3.5. This\nunderscores the pressing need to develop a specialized open-source\nLLM tailored for RTL code generation."}, {"title": null, "content": "The scarcity of high-quality RTL code datasets is a significant\nfactor contributing to the poor quality of RTL code generation. In\nLLM training, both the scale and quality of the dataset are essential\nfor effective code generation performance. However, compared to\nother popular programming languages, there is a notable short-\nage of high-quality, large-scale RTL code datasets, which poses\na significant challenge for achieving satisfactory results in RTL\ngeneration tasks. To address this issue, several studies have fo-\ncused on collecting open-source code snippets [7, 19, 22, 23]. Nev-\\ertheless, these open-source datasets may include low-quality code\nthat can impair the model's performance [13]. Other researchers\nhave developed methodologies for synthesizing RTL code using\nLLM [5, 13, 14]. VerilogEval [13] generates datasets by adding de-\nscriptions to open-source Verilog code but suffers from its average\nlow quality. RTLCoder [14] selects hardware-related keywords as\nseeds to generate natural language instructions, which are then\nconverted into corresponding RTL code by GPT-3.5. Although this\ndataset meets quality standards, it is limited in scale because the\ngenerated data is restricted to the selected keywords.\nSelf-reflection is also a critical factor to be considered in the\nfield of RTL code generation. In the context of RTL code generation,\nself-reflection refers to the model's assessment of its generated code\nbased on feedback from the compiler and simulator. This process\ninvolves identifying the causes of errors and refining the results\naccordingly, just as in the hardware design process, where RTL\ncode undergoes rigorous evaluation by the compiler and simulator\nto ensure its correctness and adherence to design specifications.\nHowever, existing open-source LLMs mainly focus on code gen-\neration, neglecting the interactive process of compilation and sim-\nulation in hardware design methodologies. In consideration of the\nabove limitations, several studies have integrated self-reflection\nmethodologies into their frameworks [24, 25, 27]. Nonetheless, both\nRTLFixer [25] and AutoChip [24] rely on commercial LLMs for self-\nreflection, which raises potential concerns regarding privacy and\nsecurity, as previously discussed. Open-source LLMs generally ex-\nhibit weaker self-reflection capabilities compared to commercial\nLLMs, primarily due to their limited training data. Consequently,\nthere is a need to develop a methodology for constructing datasets\nspecifically designed to enhance the self-reflection abilities of open-\nsource LLMs.\nIn this paper, we introduce OriGen, a fully open-source frame-\nwork featuring self-reflection capabilities and a dataset augmenta-\ntion methodology. We propose a novel code-to-code augmentation\nthat leverages knowledge distillation to improve the quality of\nopen-source RTL code datasets, which are of a large scale but have\nlimited quality due to the lack of rigorous review and validation\nprocesses.\nOur approach involves extracting high-level code descriptions\nfrom the open-source RTL code and refining the code based on\nthese descriptions using the Claude3-Haiku model, which acts as\nthe teacher model. Commercial LLMs have been trained on vast\namounts of high-quality data and possess strong language under-\nstanding and generation capabilities. By leveraging these models as\nteachers, we can distill their knowledge and refine the open-source\nRTL code. This method enables the generated code to potentially\nachieve even superior performance of the teacher model."}, {"title": null, "content": "To evaluate the LLM's capability of self-reflection in RTL code,\nwe construct a benchmark named VerilogFixEval that comprises\n221 cases of failed compilation from VerilogEval [13]. OriGen is\nable to correct syntactic errors by leveraging a self-reflection pro-\ncess based on feedback from the compiler. When the generated\ncode fails to pass the compiler's verification, the model initiates\nthe self-reflection process. During this phase, the model accepts\nthe erroneous code and compiler error messages to fix the RTL\ncode, enhancing its correctness and reliability. The self-reflection\nability of the model is facilitated by a carefully constructed error-\ncorrection dataset, which comprises a comprehensive collection\nof samples. This dataset includes natural language instructions,\nerroneous code, compiler error messages, and the corrected code\ngenerated by Claude3-Haiku. It serves as a valuable resource to\nbridge the gap in self-reflection capabilities between open-source\nand commercial closed-source LLMs. By training on this dataset,\nOriGen can acquire self-reflection abilities comparable to those\nof commercial LLMs, enabling it to generate more reliable and\naccurate RTL code.\nOur contributions are summarized as follows:\n\u2022 We introduce OriGen, which significantly outperforms other\nalternatives designed for RTL code generation and achieves\nperformance comparable to the latest version of GPT-4 Turbo.\n\u2022 We propose a novel code-to-code augmentation methodology\nfor generating high-quality, large-scale RTL code datasets,\nenhancing the model's training data.\n\u2022 we introduce a self-reflection mechanism that enables OriGen\nto autonomously fix syntactic errors by leveraging feedback\nfrom the compiler, improving its code generation accuracy.\nFurthermore, we construct a dataset to improve the model's\ncapability of self-reflection based on compiler error messages\nand develop a benchmark to evaluate this capability.\nExperimental results demonstrate that OriGen significantly out-\nperforms other alternatives designed for RTL code generation [12-\n14, 17, 19] surpassing the previous best-performing LLM by 9.8% on\nthe VerilogEval-Human [13] benchmark. Furthermore, experimen-\ntal results on VerilogFixEval illustrate that OriGen's self-reflection\ncapability in RTL code generation not only surpasses other alterna-\ntives but also outperforms GPT-4 Turbo by 18.1%."}, {"title": "BACKGROUND AND RELATED WORK", "content": null}, {"title": "LLMs for Verilog Generation", "content": "Large Language Models (LLMs) have emerged as powerful tools for\ncode generation across various programming languages. Trained on\nvast amounts of code and natural language data, these models can\nlearn the statistical patterns and relationships within the training\ndata, enabling them to generate code that adheres to the syntax and\nstyle of the target programming language. Recently, researchers in\nthe field of hardware design have shown a growing interest in lever-\naging LLMs for generating RTL code. RTL is a crucial abstraction\nlevel in hardware design, describing the flow of data and control\nsignals between registers and combinational logic. By fine-tuning\nLLMs on RTL code datasets, models are capable of generating RTL\ncode, potentially accelerating the hardware design process.\nAmong the pioneering endeavors, DAVE [18] represents an ini-\ntial exploration into the generation of Verilog code from natural"}, {"title": "Reflection for Verilog Generation", "content": "Existing open-source LLMs designed for RTL code generation have\nmainly focused on the generative aspects, neglecting the crucial\nstages of verification and reflection that are integral to hardware\ndesign methodologies. The effectiveness of self-reflection and recep-\ntivity to feedback in addressing real-world issues has been demon-\nstrated by SWE-agent [29], which successfully resolved problems\nin GitHub's repositories. This highlights the potential benefits of\nincorporating self-reflection mechanisms into LLMs for RTL code\ngeneration, as it could enable the models to learn from feedback and\niteratively improve the generated code, aligning with the rigorous\nverification processes inherent to hardware design methodologies.\nRecognizing the limitations of previous research on LLMs for\nRTL generation, AutoChip [24] and RTLFixer [25] decide to intro-\nduce self-reflection into the their framework. Specifically, RTLFixer"}, {"title": "Knowledge Distillation", "content": "To enable open-source models to acquire capabilities comparable\nto commercial closed-source models, knowledge distillation has\nbecome a focus for researchers. [28]. A common approach involves\nleveraging the closed-source LLM to provide reference answers,\nwhich are then used to fine-tune the open-source LLM through\nSupervised Fine-Tuning (SFT), enabling the latter to learn from the\nknowledge embodied in the closed-source model. Specifically, SFT\nis a effective knowledge distillation method. In this approach, the\nteacher model, typically an advanced LLM, is first used to generate\na large number of input-output data pairs. Subsequently, these\ndata pairs generated by the teacher model are utilized as training\ndata to fine-tune the student model, typically a smaller open-source\nlanguage model. Through this process, the student model can mimic\nthe behavior of the teacher model, thereby acquiring the knowledge\nwithin the teacher model.\nThe code-to-code augmentation methodology employed in this\nwork falls under the realm of knowledge distillation techniques. By\nleveraging the knowledge within these advanced models, the pro-\nposed method can effectively distill and transfer their capabilities\nto the target model, enabling it to acquire the desired capability of\nRTL code generation in a scalable and automated manner."}, {"title": "METHODOLOGY", "content": "In this section, we provide comprehensive details of OriGen, a\npowerful framework designed to enhance Verilog code generation."}, {"title": "Overview", "content": "The overview of the code-to-code augmentation methodology and\nOriGen's generation and self-reflection framework are illustrated\nin Figure 1 and Figure 2 respectively. As shown in Figure 1, in the\ncode-to-code augmentation process, two datasets are generated:\nan enhanced code dataset and an error-correction dataset. The en-\nhanced code dataset contains code-description paired data samples,\nenabling the LLM to learn the correspondence between Verilog\ncode and natural language. The error-correction dataset comprises\nerroneous code-fixed code paired data samples, allowing the LLM\nto enhance its self-reflection capability by learning from examples\nof code errors and their corrections.\nAs shown in Figure 2, in the code generation and error correction\nstage, OriGen comprises a base LLM and two trained LoRA models:"}, {"title": "Code-to-Code Augmentation", "content": "The code-to-code augmentation process aims to transfer advanced\nRTL code generation and correction capabilities from commercial\nLLMs to our model. To achieve this, a carefully filtered collection\nof comprehensive open-source RTL code samples is utilized as a\nfoundation.\nTo extract a valuable dataset from open-source RTL code sam-\nples [15, 23], a rigorous filtration process is applied. Initially, due\nto the constraints imposed by the model's context window and the\nchallenges associated with incomplete descriptions for longer code\nsnippets, samples exceeding 300 lines or 1536 tokens are excluded.\nAdditionally, samples with an average of more than 30 tokens (ap-\nproximately 90 characters) per line are considered non-standard and\nare consequently eliminated, ensuring that only concise and stan-\ndard code samples are retained. Subsequently, to ensure the mean-\ningfulness and substantive content of the code snippets, a keyword-\nbased filtration approach is employed. Each sample must contain\nboth the module and endmodule keywords, alongside at least one\noccurrence of keywords related to procedural blocks, always (in-\nclusive of variants like always_comb, always_ff, always_latch,\netc.) or assign. This criterion guarantees that the selected snip-\npets are representative of functional and logical hardware designs.\nLastly, all comments within the code samples are removed. This"}, {"title": null, "content": "step is crucial to prevent extraneous information from influencing\nthe generation of accurate and relevant specifications.\nFollowing the implementation of the aforementioned filtering\nprocedures, the closed-source LLM Claude3-Haiku is utilized to\ngenerate detailed descriptions corresponding to the filtered code\nsamples as shown in Figure 1. The prompt we use is illustrated\nin Figure 3. These descriptions are then used to regenerate RTL\ncode, which replaces the original code in the dataset. During the\nregeneration process, the generated code undergoes verification\nusing the open-source compiler Icarus Verilog (Iverilog) [26]. If\nthe code fails to compile, the compiler's error messages and the\nerroneous code are fed back into the LLM for regeneration to fix er-\nror. Simultaneously, code samples that fail compilation are utilized"}, {"title": "Error-Correction Dataset", "content": "Although OriGen, after being trained on the enhanced code dataset,\ndemonstrates performance comparable to that of advanced closed-\nsource LLMs in RTL code generation, it exhibits weaker self-reflection\ncapabilities compared to commercial LLMs like other open-source\nLLMs."}, {"title": "Code Generation and Fix", "content": "As shown in Figure 2, in the code generation and error correction\nstage, OriGen comprises a base LLM and two trained LoRA mod-\nels: Gen LoRA and Fix LoRA. Following training on the enhanced\ncode dataset, Gen LoRA has developed robust RTL code generation\ncapabilities but exhibits limitations in self-reflection. To further\nenhance its capabilities of self-reflection, Gen LoRA is trained on\nthe error-correction dataset, resulting in Fix LoRA.\nThe reason for employing two LoRA models, Gen LoRA and\nFix LoRA, instead of a single LoRA is based on experimental re-\nsults, which indicate that Fix LoRA, trained on the error-correction\ndataset, exhibits inferior performance compared to Gen LoRA in the\nRTL code generation task. This performance degradation may be\nattributed to the fact that training on a dataset containing syntactic\nerrors could potentially weaken the model's overall performance.\nTherefore, OriGen adopts a two-LoRA approach, where Gen LoRA\nis responsible for generating the initial code, and Fix LoRA is tasked\nwith rectifying syntactic errors, utilizing its specialized training on\nthe error-correction dataset.\nThe generative and iterative self-reflection process is illustrated\nin Figure 2. Initially, OriGen accepts the user's natural language\nspecification and proceeds to generate the corresponding RTL code\nusing Gen LoRA. The generated code is then subjected to the com-\npiler's verification process using Iverilog. In cases where the code\nfails to compile, OriGen initiates a self-reflection process to fix the\ncode. This process involves utilizing the erroneous code, compiler\nerror messages, and natural language instructions to guide the\nmodel in identifying and rectifying the issues. The self-reflection\nloop continues until the generated code successfully passes compi-\nlation or reaches a predefined maximum number of iterations.\nAn example of self-reflection is illustrated in Figure 5. The gen-\nerated code initially contains a syntactic error where the wires"}, {"title": null, "content": "ply_temp and p2y_temp are assigned values within always blocks,\nwhich is not permitted in Verilog as wires cannot serve as left-hand\nside values for procedural assignments. This error is subsequently\nrectified by leveraging the Fix LoRA model during the self-reflection\nprocess, using the instruction template shown in Figure 6. The recti-\nfied code employs the assign statement to perform the assignments,\nrather than attempting to assign values within the always blocks,\nwhich was the source of the initial error."}, {"title": "VerilogFixEval", "content": "To evaluate the capability of different models in reflecting on and\nimproving from Verilog compiler error messages, we constructed\nthe VerilogFixEval benchmark. The benchmark comprises code\nsamples that failed to pass the compilation verification, along with\nthe corresponding natural language instructions and compiler error\nmessages. The erroneous RTL code samples were selected from\ncode generated by LLMs that performed comparable with GPT-\n3.5 on the VerilogEval benchmark. This selection approach was\nadopted since OriGen achieves a relatively high compilation pass\nrate. By including code samples from poorly performing LLMs, the\nbenchmark ensures the diversity of errors while avoiding potential\nbias in the test results that may favor OriGen due to errors generated\nby OriGen itself.\nDuring the evaluation process, the model will receive natural\nlanguage instructions, erroneous RTL code, and compiler error\nmessages, and will be required to correct the RTL code. The final\nevaluation metrics include two parts, syntactic correctness and\nfunctional correctness."}, {"title": "EVALUATION", "content": null}, {"title": "Experimental Setting", "content": "For our pre-trained model, we selected the DeepSeek-Coder-7B-\nInstruct model, as it exhibits the best performance in Verilog code\ngeneration among all 7B models, to the best of our knowledge. It is\nto ensure that our pre-trained model possesses strong capabilities in\nthe domain of Verilog code generation, providing a solid foundation\nfor further fine-tuning and evaluation.\nTo evaluate the performance of Verilog code generation, we se-\nlected two representative benchmarks: VerilogEval [13] and RTLLM [16].\nThe former, VerilogEval, originates from approximately 150 Verilog\ntasks on the HDLBits website, which were manually converted to\ncreate VerilogEval-Human and generated by GPT-3.5 to produce\nVerilogEval-Machine. The latter benchmark, RTLLM, consists of 29\nVerilog tasks with more diverse levels of difficulty, closely aligned\nwith real-world design tasks. Both benchmarks employ the widely\nadopted pass@k evaluation metric to assess the correctness of the\ngenerated code's functionality. In this metric, if any one of the k\nsamples passes the unit test, the problem is considered solved.\n$pass@k := \\frac{1}{Problems} \\sum_{i=1}^{Problems} \\mathbb{I} (c>0)$"}, {"title": null, "content": "where we generate n \u2265 k samples for each instruction in which\nc \u2264 n samples pass testing. We choose n = 10 in experiments.\nTo assess the models' capability for self-reflection, we utilized\nthe VerilogFixEval benchmark, as discussed in Section 3.5. This"}, {"title": "Model Training", "content": "We employ the LoRA (Low-Rank Adaptation) [10] method to train\nthe model's capability in generating RTL code. This approach allows\nfor enhancing the model's specific abilities in RTL code generation\nwhile minimizing the impact on its other capabilities. For all the\ntraining processes, we employ the float16 mixed precision method,\nalthough the model is trained in bfloat16 precision. We utilize the\nAdam optimizer with \u1e9e\u2081 = 0.9, \u03b22 = 0.999, and the cosine learning\nrate decay to schedule our learning rate. The warm-up ratio is set\nto 0.03.\nDuring the training process, we initially segment the dataset into\nsmaller subsets, each containing 10k samples, resulting in a total of\n18 groups. We then perform fine-tuning on these smaller datasets to\nevaluate their quality and assess the model's performance training\non them. Throughout the experiments, it is evident that the dataset's\nperformance is relatively sensitive to the learning rate.\nAs illustrated in Figure 7, we choose several subsets of the dataset,\nincluding 6 smaller 10k-samples datasets and 2 larger 30k-samples\ndatasets. Employing varying learning rates during the training\nphase lead to noticeable differences in the pass@1 metric on the\nVerilogEval benchmark [13]. Simultaneously, the results obtained\nfrom training on different datasets exhibit significant variations.\nThese observations highlight the sensitivity of the model's perfor-\nmance to the choice of learning rate and the characteristics of the\ntraining dataset. In response, we implement a strategy of training\neach small dataset with its optimal learning rate. Furthermore, we\nconduct training exclusively on those datasets where the pass@1\nmetric surpass 42%. This approach mitigate the sensitivity of the\nmodel's performance to the learning rate, ensuring that each dataset\nsubset is trained with the most suitable learning rate to achieve\noptimal results. Additionally, it avoids the detrimental influence of\nunderperforming datasets."}, {"title": "Functional Correctness", "content": "Table 1 presents the results on the VerilogEval benchmark. To\nensure fairness, we did not utilize the self-reflection feature for\nthis comparison and generated code in a single attempt, like other\nmodels. The models compared include closed-source commercial\nLLMs such as GPT-3.5/GPT-4, Claude3-Haiku/Sonnet/Opus, general\nopen-source code models [3, 9], and models customized for RTL\ncode generation [12-14, 17, 19].\nIn the VerilogEval benchmark [13], for pass@1 in Human and\nMachine categories, OriGen achieves 76.1% and 51.4% respectively.\nIt outperforms remarkably all other alternatives designed for RTL\ncode generation, for example, compared to the best-performed fully\nopen-source RTLCoder [14], OriGen surpasses it by 9.8% in Human\nbenchmark. When compared with commercial closed-source mod-\nels, OriGen also achieves excellent performance. It significantly\nsurpasses GPT-4 of 2023 version, which was previously the widely\ncompared baseline. Even when compared with the state-of-the-art\nmodels, OriGen outperforms Claude3-Haiku/Sonnet, only slightly\ninferior to the current best models GPT-4 Turbo and Claude3-Opus\nby 3% in Human benchmark. Moreover, as evident from Table 1, it\nachieves the best performance on the Machine benchmark, remark-\nably outperforming other models including GPT-4 Turbo.\nTo prevent our model from over-fitting on the VerilogEval bench-\nmark, we also conduct experiments on another benchmark designed\nfor RTL code generation, RTLLM [16]. Similar results are observed\non RTLLM as shown in Table 1, where OriGen significantly outper-\nforms other models, achieving performance comparable to GPT-4\nTurbo and Claude3-Opus.\nIn summary, OriGen outperforms all non-commercial models\nremarkably in all metrics on both benchmarks.\nMoreover, as illustrated in Figure 8, we observe that the strat-\negy of training each small dataset with its optimal learning rate\nand excluding the datasets with relatively poor performance al-\nlows the model's performance to gradually improve as the dataset\ngrows larger. This demonstrates the effectiveness of the training\nmethodology."}, {"title": "Capability of Self-Reflection", "content": "The evaluation metrics of VerilogFixEval consist of two components:\nsyntactic correctness and functional correctness. Syntactic correct-\nness assesses whether the generated rectified RTL code successfully"}, {"title": "Ablation Studies", "content": "We perform two ablation experiments to investigate the efficacy of\nthe code-to-code augmentation method and to examine the model's\nself-reflection capability before and after training on the error-\ncorrection dataset.\nFor the ablation study of the code-to-code data augmentation\nmethod, we compared the performance of models trained on RTL\ncode dataset before and after code-to-code augmentation method."}, {"title": "CONCLUSION", "content": "This paper introduces OriGen, an open-source framework for RTL\ncode generation. It proposes a novel code-to-code augmentation\nmethodology to generate high-quality, large-scale RTL code datasets,\nwhich enhances the model's training data. The framework also\nintroduces a self-reflection mechanism that allows OriGen to au-\ntonomously fix syntactic errors by leveraging compiler feedback,\nthereby improving its code generation accuracy. Furthermore, we\nconstruct a dataset to improve the model's capability of self-reflection\nbased on compiler error messages and erroneous code and develop a\nbenchmark to evaluate this capability. Experimental results demon-\nstrate that OriGen remarkably outperforms other open-source al-\nternatives in RTL code generation, surpassing the previous best-\nperforming LLM by 9.8% on the VerilogEval-Human benchmark and\nis comparable with GPT-4. Moreover, OriGen exhibits superior ca-\npabilities in self-reflection and error rectification, surpassing GPT-4\nby 18.1% in syntactic correctness on the VerilogFixEval benchmark."}]}