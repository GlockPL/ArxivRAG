{"title": "TAGCOS: TASK-AGNOSTIC GRADIENT CLUSTERED CORESET SELECTION FOR INSTRUCTION TUNING DATA", "authors": ["Jipeng Zhang", "Yaxuan Qin", "Renjie Pi", "Weizhong Zhang", "Rui Pan", "Tong Zhang"], "abstract": "Instruction tuning has achieved unprecedented success in NLP, turning large language models into versatile chatbots. However, the increasing variety and volume of instruction datasets demand significant computational resources. To address this, it is essential to extract a small and highly informative subset (i.e., Coreset) that achieves comparable performance to the full dataset. Achieving this goal poses non-trivial challenges: 1) data selection requires accurate data representations that reflect the training samples' quality, 2) considering the diverse nature of instruction datasets, and 3) ensuring the efficiency of the coreset selection algorithm for large models. To address these challenges, we propose Task-Agnostic Gradient Clustered COreset Selection (TAGCOS). Specifically, we leverage sample gradients as the data representations, perform clustering to group similar data, and apply an efficient greedy algorithm for coreset selection. Experimental results show that our algorithm, selecting only 5% of the data, surpasses other unsupervised methods and achieves performance close to that of the full dataset.", "sections": [{"title": "Introduction", "content": "Instruction tuning [Wei et al., 2022a, Ouyang et al., 2022] is the most important strategy for customizing Large Language Models (LLMs) for downstream tasks, which allows them to precisely understand human intentions and accurately generate responses in natural languages. Recently, many existing works Wang et al. [2023a] expand the amount and diversity of instructions for instruction tuning to further enhance the LLM's capability. However, the increased quantity of the dataset also leads to significantly higher computational costs for instruction tuning. Meanwhile, Zhou et al. [2023] revealed that only 1,000 high-quality, human-created data samples could substantially improve the ability of LLMs to follow instructions, which suggest that there exists severe redundancy in current instruction datasets, and only a high-quality subset may suffice for achieving promising performance.\nTo address the above issue, selecting a small, highly informative subset (i.e., coreset) of training samples from the original dataset is a promising solution. This approach ensures that training on the coreset achieves performance comparable to the full dataset while significantly reducing costs. However, coreset selection is challenging as it must not only consider the quality of individual samples, but also their importance within the entire subset. For example, if two high-quality samples are very similar, selecting only one may be sufficient. This global perspective on sample importance is crucial for the quality of the selected subset.\nCurrent methods for coreset selection can be categorized into two main types: 1) Heuristic-based approaches [Marion et al., 2023, Li et al., 2023, Chen et al., 2023a, Lu et al., 2023], and 2) Optimization-based approaches [Borsos et al., 2020, Zhou et al., 2022, Gao et al., 2023, Zhou et al., 2022]. Heuristic-based methods use various heuristic scores"}, {"title": "Related Work", "content": "Instruction Tuning Data. Instruction tuning [Ouyang et al., 2022] has achieved unprecedented success in NLP, turning large language models into versatile chatbots [Chiang et al., 2023, Taori et al., 2023]. Successful instruction tuning requires a powerful pre-trained base model as well as high-quality instruction datasets. For the powerful pre-trained base model, one usually selects a pre-trained LLM with more data and having more parameters, like Mistral [Jiang et al., 2023], Llama family models [Touvron et al., 2023]. For high-quality instruction datasets part, it is expected that high-quality datasets are diverse and representative enough to adapt the LLM to potential downstream usage. With the development of instruction tuning, there are more and more instruction datasets. Usually, these datasets are either annotated by human or proprietary LLMs. Currently, instruction data generally contains these types: (1) datasets are created by researchers from existing NLP dataset and incorporate an instruction for existing input-output pairs, like Flan [Longpre et al., 2023, Wei et al., 2022a], SuperNI [Wang et al., 2022], CoT [Wei et al., 2022b] and Orca [Mukherjee et al., 2023]. (2) open-end text generation, e.g., multi-turn dialogue and instruction following. Several open-end text generation datasets are created by human, like Dolly [Databricks, 2023] and Oasst1 [K\u00f6pf et al., 2023]. Others are generated by proprietary models or human interaction with these models, like Self-instruct [Wang et al., 2023b], Alpaca [Taori et al., 2023], Sharegpt [Chiang et al., 2023], Baize [Xu et al., 2023], GPT4-Alpaca [Peng et al., 2023] and Unnatural Instructions [Honovich et al., 2023]. (3) instructions build for domain-specific skills, like Code-Alpaca [Chaudhary, 2023] for code completion. Given such a diverse collection of instruction dataset, the challenge for instruction tuning lies in ensuring the quality of these instructional data samples. Zhou et al. [2023]"}, {"title": "Method", "content": "To tackle the challenging coreset selection problem for LLM's instruction tuning dataset, we propose Task-Agnostic Gradient Clustered Coreset Selection (TAGCOS), a task-agnostic coreset selection approach that effectively and efficiently discovers the informative subset from a large instruction tuning dataset. In this section, we first introduce the our formulation of coreset selection, which casts the task into a gradient matching problem. Then, we elaborate the detailed steps for coreset construction.\nNotation. Assume we have a pretrained LLM @ and a giant and diverse instruction dataset D := {(s, c)(i)}=1, where each data sample z = (s, c) comprises an instruction s and a completion c. For each data sample, the loss l(z; 0) is defined as the cross entropy between the prediction distribution p(\u00b7 | s) and the ground truth text response c. Since c often contains multiple tokens, l(z; 0) is calculated as the average of the token-wise cross entropy loss across the completion c. The notation et refers to the model checkpoint at step t.\nProblem Formulation. We first formulate the task into a gradient matching problem, i.e., the average gradient of the selected subset should approximate the gradient of the entire dataset. Intuitively, if the gradient is similar throughout all the training steps, the resulting model parameter should be closed to the model trained with the entire dataset.\nFormally, given a giant and diverse dataset D, our goal is to select a subset Dsub \u2286 D (|Dsub| < |D|) containing the most informative training samples. We expect that the gradients produced by the full training dataset \\sum_{z\\in D} \\nabla_{\\theta} \\mathcal{L}(z; \\theta) can be replaced by the gradients produced by a subset \\sum_{z\\in D_{sub}} \\nabla_{\\theta} \\mathcal{L}(z; \\theta) with the minimal difference:\n$\\min_{z \\in D_{sub}} Err (\\nabla_{\\theta}\\mathcal{L}(D; \\theta), \\frac{1}{|D_{sub}|} \\sum_{z} w_{z} \\nabla_{\\theta} \\mathcal{L}(z; \\theta))$\ns.t. Dsub D, wz \u22650, |Dsub| \u2264 M\n(1)\nwhere $\\mathcal{L}(D; \\theta) = \\frac{1}{|D|} \\sum_{z \\in D} \\nabla_{\\theta}l(z; \\theta)$, $w$ is the subset weight vector, $||w||_1$ is the sum of the absolute values and $Err(\\cdot,\\cdot)$ measures the distance between two gradients. Note that $w$ could be either continuous, which leads to weighted training on the selected subset, or with discrete values, which reduces to regular training on the coreset.\nHowever, due to the high diversity of the large-scale instruction tuning dataset, simply conducting selection over the entire dataset potentially causes over-sampling in certain domains and under-sampling in others. To address this, we introduce clustering to ensure balanced sampling. By splitting the dataset into clusters and selecting samples from each cluster, we ensure a more even distribution across different domains.\nOverall, as illustrated in algorithm 1, the process for coreset construction could be summarized as follows: (1) compute the gradient features $G = {g_i | g_i = \\nabla_{\\theta} \\mathcal{L}(z; \\theta)}_{i=1}^N$. Inspired by Xia et al. [2024], we compute the low-dimensional approximations of gradient features for each data samples z over the whole dataset D; (2) perform gradient-based clustering, we perform k-means clustering [Hartigan and Wong, 1979] given the gradients features and get k clusters and corresponding centroids ce for each cluster, which effectively gathers the samples with similar characteristics into one cluster; (3) coreset selection via Optimal Matching Pursuit, we compute the data samples matches best with the centroids in each cluster with an orthogonal matching pursuit algorithm [Killamsetty et al., 2021]."}, {"title": "Gradient Features Computation", "content": "We perform an efficient gradient feature approximation computation over the entire dataset. To speed up gradient computation, we follow Xia et al. [2024] to use LoRA [Hu et al., 2022] and random projections [Park et al., 2023] to reduce the number of dimensions in gradient features. Meanwhile, we propose using checkpoints sampled before convergence to compute gradient features. This is inspired by the fact that the gradient norm calcuated during the warmup phase is significantly larger than checkpoints at convergence. Therefore, these gradient features encapsulate more essential information that reflects how each sample affect the model's updates. The effectiveness of this strategy is verified by results in table 5.\nAdam Gradient Computation Function. The gradients based on Adam optimizer Kingma and Ba [2015] can be computed with these steps:"}, {"title": "Gradient Clustered Coreset Selection", "content": "$\\theta^{t+1} - \\theta^t = -\\eta_t g_i(z, \\theta_t)$        (2)\n$g_i(z, \\theta_t) = \\frac{\\sqrt{\\hat{v}^{t+1}} + \\epsilon}{\\hat{m}^{t+1}}$  (3)\n$\\hat{m}^{t+1} = (\\beta_1 m^t + (1 - \\beta_1)\\nabla l(z; \\theta_t)) /(1 - \\beta_1^t)$  (4)\n$\\hat{v}^{t+1} = (\\beta_2 v^t + (1 - \\beta_2)(\\nabla l(z; \\theta_t))^2) /(1-\\beta_2^t)$  (5)\nwhere $\\beta_1$ and $\\beta_2$ are hyperparameters, and $\\epsilon$ is a small constant. $g_i (z, \\theta_t)$ represents the first-order expansion for the Adam dynamics, requiring model gradients and optimizer states from the training process. Warmup training on a subset of the dataset provides the necessary checkpoints for these computations. As mentioned above, we will sample checkpoints before convergence to provide a more accurate gradient estimation.\nWarmup Training with LoRA. LoRA [Hu et al., 2022] is used to reduce the number of trainable parameters and accelerate the inner products in gi(z, 0t). LoRA freezes the pre-trained weights and adds a low-rank adaptor to the selected fully connected layers. We use LoRA to perform instruction tuning on pre-trained base model (e.g., LLAMA-2-7B) on a random subset Dwarmup \u2286 D for N epochs, checkpointing the model after each epoch to store {0;}=1.\nThe gradient when training with LoRA, denoted \u2207l(\u00b7; 0) \u2208 RP, is much lower dimensional than the model itself; for example, in LLAMA-2-7B, \u2207l(\u00b7; 0) is less than 2% the size of 0. We use \u2207l(\u00b7; 0) to compute the Adam update and denote it as \u011di (, 0).\nProjecting the gradients. Following Xia et al. [2024], we also introduce a random project to the LoRA gradients for further reducing the feature dimension. For a given data sample z and model checkpoint \u03b8i, we can compute a d-dimensional projection of the LoRA gradient \u2207l(z; 0\u2081) = II\u00af\u2207l(z; 0\u2081), with each entry of II \u2208 RP\u00d7d drawn from a Rademacher distribution [Johnson, 1984] (i.e., \u041f\u017cj ~ U({\u22121,1})). In total, we compute gradient features for each data sample z with \u011fi(z, \u00b7) = I\u00af\u011di(z, \u00b7).\nGradient-based Clustering\nDue to the diversity of instruction tuning dataset, direct sampling over the entire dataset may not cover all the regions, since the training samples from each domain are not evenly distributed. To further improve the effectiveness and robustness of data selection, we divide the entire dataset into several clusters and then perform gradient matching algorithm on each cluster itself. With the gradient features gi from the above step, we conduct K-means clustering on them to assign each data sample into a cluster {Ck}=1. Also, we can obtain cluster centroids {\u03bc\u03ba}=1 of these clusters during the clustering process, where each centroid shares the dimension with gradient features.\nCoreset Selection via Optimal Matching Pursuit\nIn each cluster, we hope to get the subset that minimizes the difference between the selected subset and the whole cluster. Instead of doing heuristic selection like selecting all the instances with shortest distance with cluster centroids, we formalize this as an optimization problem and introduce an orthogonal matching pursuit (OMP) algorithm [Killamsetty et al., 2021, Elenberg et al., 2016] to solve it. Similar with equation 1, our objective is to minimize the difference between selected Dub in k-th cluster and the whole cluster Dk,\n$Err(w^k, D_{sub}^k; D^k) \\triangleq \\sum_{z\\in D_{sub}^k} w_{z} \\nabla_{\\theta}l(z; \\theta) - \\frac{1}{|D^k|} \\sum_{z\\in D^k} \\nabla_{\\theta}l(z; \\theta)$      (6)\nConsidering the regularization coefficient \u03bb, we can have $Err_\\lambda(w, D_{sub}^k; D^k)$ as:\n$Err_\\lambda(w, D_{sub}^k; D^k) \\triangleq Err(w, D_{sub}^k, D^k) + \\lambda ||w||_2 ^2$.\nHere, we approximately regard the centroids of each cluster as the average gradients of the whole cluster,\n$\\frac{1}{|D^k|} \\sum_{z \\in D^k} \\nabla_{\\theta}l(z; \\theta)  = ce_k.$    (7)"}, {"title": "Theoretical Analysis", "content": "In this section, we analyse the benefits of our gradient clustering in coreset selection. The general conclusion is that coreset selection problem formulated in Problem (P) is essentially a Submodular Function Maximization (SFM) problem, which is NP-hard Bach et al. [2013]. Solving large-scaled submodular function maximization problems is extremely challenging, potentially leads to inferior solution. Our gradient clustering strategy naturally decomposes the original problem into several small scaled problems, significantly reduces the difficulty in optimization, making finding solutions with high-precision possible. The detailed results are presented in the following theorems. These theorems are adapted from the classical analysis on OMP, which can be found in the studies Elenberg et al. [2018], Wolsey [1982]. We adopt them to understand the superiority of our coreset selection approach.\nTo unify the problems of coreset selection with and without clustering, we extend the problem (P-k) as follows:\n$\\max_{D_{sub}} F_\\lambda(D_{sub}; D),$\ns.t., $|D_{sub}| \\leq M and D_{sub} \\subseteq D,$\n(P)\nwhere Dsub and D are the coreset and the full dataset, respectively. c is the constant to control the coreset size.\nLemma 1. If the coreset size $|D_{sub}| \\leq c$ and $\\max_{z\\in D} ||\\nabla_{\\theta}l(z; \\theta) ||_2 \\leq G$, then $F_\\lambda(D_{sub}; D)$ is $\\gamma_D$-weakly submodular with $\\gamma_D = \\lambda +MG^2$.\nTheorem 1. If $\\max_{z\\in D} ||\\nabla_{\\theta}l(z; \\theta)||_2 < G$ and $\\max_{z \\in D_k}||\\nabla_{\\theta}l(z; \\theta)||_2 < G_k$ for cluster k. Let $D^*_{sub}$ and $D_k^*$ be the optima of Problems P and P \u2013 k, with k = 1, . . ., K. Then, the followings hold:\n(i) For problem (P), OPM runs with stopping criteria Fx(Dsub; D) \u2264 \u0454 achieves set Dsub with |Dsub| \u2264$\\frac{\\gamma_D}{l_{sub} }log(\\frac{L_{maz}}{e}).$"}, {"title": "Gradient Clustered Coreset Selection", "content": "(ii) For problem (P-k), OPM runs with stopping critia $F_\\lambda(D_{sub}^k; D^k) \\leq \\epsilon_k$ achieves set $D^*_{sub}$ with $|D^*_{sub}| \\leq \\frac{\\gamma_{D_k}}{l_{sub}^k} log(\\frac{L_{maz}}{\\epsilon_k}).$\nSince $\\gamma_D = \\frac{\\lambda}{d} +MG^2$ and $\\gamma_{D^k} = \\frac{\\lambda}{d}$ with $\\sum D^k = M $, it can be expected that $\\gamma_{D_k} << \\gamma_D$. Noting that a proper clustering method would make $D_{sub}^* \\approx \\cup_{k=1}^K D_{sub}^k = \\cup_{k=1}^K D^k$ and it is reasnonable to set $\\epsilon_k = \\epsilon \\frac{L_{max}^k}{L_{max}}$ to ensure comparable precisions. Thus the above theorem demonstrates that\n$\\sum_{k=1}^{K} |D_{sub}^k| \\leq  \\sum_{k=1}^K \\frac{\\gamma_{D_k}}{l_{sub}^k} log(\\frac{L_{maz}^k}{\\epsilon_k})  << \\frac{\\gamma_{D}}{l_{sub}} log(\\frac{L_{maz}}{\\epsilon}).$\nThat is, to achieve comparable accuracy, the union of the coreset selected from each cluster can be much smaller than that from the whole datasets, which verifies the benefits of gradient clustering. This is also consistent with our experimental observation. i.e., the running time of OMP without gradient clustering is significantly longer than that with gradient clustering."}, {"title": "Experiment", "content": "In this section, we conduct experiments to answer the following research questions:\n\u2022 Does TAGCOS achieve superior performance over other unsupervised selection methods? (Table 1)\n\u2022 How effective is the generalization of TAGCOS, and can it be transferred to different models? (Table 2)\n\u2022 What is the best configuration for TAGCOS, including the selection proportion, the number of clusters, and the selection of gradient checkpoints? (Table 3, Table 4, Table 5)"}, {"title": "Setup", "content": "Datasets. To illustrate that TAGCOS is task agnostic, we chose diverse tasks for both training and evaluation. For the training set, we combined 17 popular instruction datasets totaling 1,068,549 examples, following Wang et al. [2023a], Ivison et al. [2023]. These datasets vary in format and reasoning tasks, with annotations by humans or the OpenAI API. For details, please refer to Appendix.\nFor evaluation, we selected TydiQA [Clark et al., 2020], MMLU [Hendrycks et al., 2020], and BBH Suzgun et al. [2022]. TydiQA is a multilingual QA benchmark covering 11 languages, requiring models to extract answers from passages given a question. F1 is used to as the evaluation metric here. MMLU features multiple-choice questions across 57 subjects, from elementary to professional levels. It asks LLM to select a single correct answer given several options. Accuracy is used as the metric here. BBH includes 23 challenging tasks from Big-Bench, testing general reasoning skills.\nImplementation Details. Following Xia et al. [2024], we performed warmup training on a randomly selected 5% of the dataset for 4 epochs and computed 8192-dimensional gradient features on the full dataset D. The learning rate for warmup training was set to 2e-5, with a batch size of 32. Using these gradient features, we selected 5% of the original dataset using our selection methods, totaling approximately 53,427 samples. We used 100 clusters for K-means clustering and set the OMP algorithm tolerance at 0.01. After obtaining the subset, we fine-tuned the Llama-2-7B [Touvron et al., 2023] and Mistral-7B [Jiang et al., 2023] models using LoRA [Hu et al., 2022] to reduce memory usage. For LoRA training, we used the AdamW optimizer with a learning rate of 2e-5 and 4 epochs. The context length was set to 1,024, with a batch size of 32."}, {"title": "Experimental Results", "content": "Baseline. The main experiment results are presented in Table 1. Several baselines were considered for comparison: (1) Uniform: randomly selecting the data samples from the original dataset. (2) Hardest Sampling: select the data samples with the highest perplexity. (3) Perplexity Sampling [Marion et al., 2023, Marcus et al., 1993]: select the data samples with the lowest perplexity. (4) K-Center-Greedy with different representations [Chen et al., 2023b]: converting instruction data into embedding vectors, performing K-means clustering, and selecting samples by iteratively choosing the one closest to the cluster center among the remaining instances. Here, we consider 3 different embedding spaces, BERT [Reimers and Gurevych, 2019], Llama [Touvron et al., 2023] and Gradient. We denote them as K-Center BERT, K-Center Llama and K-CenterGrad. (5) OMP [Killamsetty et al., 2021]: using the OMP algorithm over the entire dataset, with the mean gradient feature across the dataset as the matching target."}, {"title": "Ablation Study and Analysis", "content": "Performance of TAGCOS on Different Models. Table 2 demonstrates that the dataset generated by the Llama-2-7B model can be effectively utilized to train a superior Mistral-7B instruction model. By leveraging the datasets selected by TAGCOS on the Llama-2-7B model, the trained Mistral-7B model shows significant improvements over uniform selection methods, consistently outperforming its counterparts. This highlights TAGCOS's ability to identify transferrable and valuable data samples, indicating its potential for future proxy data selection tasks.\n5% data can achieve comparable results with full dataset. Table 3 reveals that training with only 5% of the data selected by TAGCOS results in performance comparable to that of the entire dataset. This can be attributed to the presence of noisy samples in the full dataset, which are less effective for fine-tuning.\nHow to determine the cluster numbers. Table 4 shows that the ideal cluster number for our setup is 100. Fewer clusters, especially less than the original dataset size of 18, fail to achieve good results. Additionally, merely increasing the number of clusters does not ensure improved performance. TAGCOS tends to degrade to plain OMP as the number of clusters increases. When the cluster count matches the number of samples, the performance is identical to plain OMP."}, {"title": "Conclusion", "content": "This paper focuses on the effective selection of coresets for LLMs in instruction tuning. To address the challenge of accurate data representation, we utilize gradient features, which indicate the influence of each data sample on the training process. Additionally, to handle diverse collections of instruction data and ensure selection efficiency, we propose clustering similar data and applying an efficient greedy algorithm for selection. Our experimental results demonstrate the effectiveness of the entire pipeline."}, {"title": "Limitation", "content": "Despite its impressive performance, TAGCOS is bottlenecked by the efficiency of gradient feature estimation. The gradient feature computation stage limits its scalability to larger datasets. To effectively run TAGCOS on extensive datasets, improvements in the efficiency of gradient computation are needed."}, {"title": "Training Dataset Details", "content": "In this section, we provide the detailed sources, statistics and licenses of each training dataset used in our experiment, which is shown in table 6. We conduct coreset selection from a mixture of 17 instruction tuning datasets with various scales and properties, which demonstrates superior effectiveness compared with baseline approaches."}]}