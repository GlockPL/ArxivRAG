{"title": "Combining Domain and Alignment Vectors to Achieve Better Knowledge-Safety Trade-offs in LLMs", "authors": ["Megh Thakkar", "Yash More", "Quentin Fournier", "Matthew Riemer", "Pin-Yu Chen", "Amal Zouaq", "Payel Das", "Sarath Chandar"], "abstract": "There is a growing interest in training domain-expert LLMs that excel in specific technical fields compared to their general-purpose instruction-tuned counterparts. However, these expert models often experience a loss in their safety abilities in the process, making them capable of generating harmful content. As a solution, we introduce an efficient and effective merging-based alignment method called MERGEALIGN that interpolates the domain and alignment vectors, creating safer domain-specific models while preserving their utility. We apply MERGEALIGN on Llama3 variants that are experts in medicine and finance, obtaining substantial alignment improvements with minimal to no degradation on domain-specific benchmarks. We study the impact of model merging through model similarity metrics and contributions of individual models being merged. We hope our findings open new research avenues and inspire more efficient development of safe expert LLMs.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated strong abilities in solving complex tasks such as question answering, summarization, reasoning, and creative writing [Zhao et al., 2024]. However, these abilities are general-purpose, and LLMs can lack deep expertise in tasks requiring domain specialization [Ling et al., 2024]. Naturally, there has been increasing research in developing domain-expert LLMs, either through complete pre-training on domain-specific data [Wu et al., 2023], continued pre-training of existing general-purpose LLMs [Sankarasubbu and Pal, 2024], or instruction-tuning pre-trained LLMs on domain data [Yue et al., 2023].\nWhile powerful, these domain expert models are often significantly less safe compared to their generalist counterparts as they either do not explicitly undergo safety alignment in case of pre-training from scratch and continual pre-training, or their safety alignment gets compromised due to domain-specific fine-tuning or instruction-tuning [Bhardwaj et al., 2024]. Safety alignment of these domain expert models is crucial given their widespread adoption. However, this might be overseen due to a lack of resources, training data, alignment expertise, or concerns about potential degradation in the domain utility of models due to over-alignment \u2013 a phenomenon known as the alignment tax [Lin et al., 2024].\nRecently, model merging has emerged as an effective method for combining task-specific models into a single model without additional training [Ilharco et al., 2023]. Model merging interpolates the parameters of multiple models, and has been extended to LLMs by leveraging task vectors. Task vectors capture the adjustments made to the weights of a general-purpose pre-trained model to create a task-specific one, calculated by subtracting the original model from the task model. Combining them has been shown to yield minimal performance degradation in multi-task evaluations [Yadav et al., 2023]."}, {"title": "Methodology - MERGEALIGN", "content": "Task Vectors and Task Arithmetic Task vectors correspond to the directions in which models move when being trained on a task. Task vectors are obtained by subtracting the weights of the pre-trained model from the fine-tuned model. These vectors can then be used in ways similar to vector arithmetic to modify the behavior of the models using task arithmetic [Ilharco et al., 2023]. Similar observations has also been made with safety vectors [Bhardwaj et al., 2024], obtained by using safety-aligned models and their unaligned counterparts. We extend this notion of task vectors to domain-adaptation and preference alignment, and correspondingly to \u2018domain vectors' obtained from the domain expert model and 'alignment vectors' obtained from the general purpose aligned models for a given pre-trained model. We then build up on task arithmetic methods and investigate their effectiveness when performed on these 'domain' vectors and 'alignment' vectors, using it to formulate MERGEALIGN.\nMERGEALIGN MERGEALIGN uses the corresponding 'domain vectors' and 'alignment vectors' of domain-specific models and their generalist instruction-following counterparts, respectively. Consider a base pre-trained model \\(\\Theta\\), which is continually pre-trained or fine-tuned with domain-specific data, leading to the domain expert model \\(\\Theta_d\\). In parallel, the pre-trained model \\(\\Theta\\) undergoes general-purpose instruction-tuning and preference alignment, leading to the aligned model \\(\\Theta_a\\). We calculate the domain vector (\\(\\tau_d\\)) and alignment vector (\\(\\tau_a\\)) from these two fine-tuned checkpoints, respectively. We then perform a linear interpolation between \\(\\tau_d\\) and \\(\\tau_a\\) with weights \\(\\alpha\\) and \\(\\beta\\) and add them back to the base model \\(\\Theta\\) to obtain an aligned domain expert model, \\(\\hat{\\Theta}\\). We present an overview of MERGEALIGN in Fig. 1 and formalize it as,"}, {"title": "Experimental Setup", "content": "Domain expert and Aligned Models We apply MERGEALIGN on models of Llama-3-8B [et al., 2024] in two speciality domains, namely medicine-Llama-3-8B and finance-Llama-3-8B [Cheng et al., 2024]. These two models are referred to as \\(\\theta_d\\) in \u00a7 2. For the alignment model \\(\\theta_a\\), we use Llama-3-8B-Instruct, as it has been instruction-tuned from the same base checkpoint and carries the sufficient linear mode connectivity conditions suitable for model merging [Wortsman et al., 2022].\nEvaluation Benchmarks For evaluating the alignment performance of the models, we use: (i) 3021 test set prompts from BeaverTails [Ji et al., 2023] whose outputs are categorized as safe or unsafe using Llama-Guard-3 [et al., 2024], and (ii) 659 prompts from the red team subset of HH-RLHF [Ganguli et al., 2022] whose outputs are categorized as safe or unsafe using MD-Judge-v0.1 [Li et al., 2024]. For domain-specific evaluations, we use the same benchmarks used by the original domain expert models.\nPreference Alignment Methods We also perform preference alignment of the domain expert models with direct preference optimization (DPO) [Rafailov et al., 2024] and odds ratio preference optimization (ORPO) [Hong et al., 2024] using the HH-RLHF [Bai et al., 2022] dataset to see its effects on the knowledge-safety tradeoffs when trained explicitly on human preferences. We use LoRA [Hu et al., 2021] for alignment training due to resource constraints. The training setup is provided in \u00a76.1."}, {"title": "Results and Analysis", "content": "Performance Comparison with Domain Expert and Aligned Models We compare the performance of the model obtained with MERGEALIGN (\\(\\hat{\\Theta}\\)) with the domain expert (\\(\\Theta_d\\)) and general purpose aligned (\\(\\Theta_a\\)) models and present the performance on the domain benchmarks and alignment benchmarks in Fig. 2. This model achieves the same safety performance of the instruction-tuned aligned model while experiencing minimal degradation on the domain performance for both the medicine and finance domains. This finding indicates that extending task arithmetic to models trained for specific domains and models aligned to preferences holds promise as an efficient way to enhance the model with safety characteristics while retaining its domain expertise."}, {"title": "Comparison with Full Model Interpolation Methods", "content": "Comparing with MERGEALIGN, we also evaluate Slerp [Shoemake, 1985] which interpolates all the model parameters of the domain expert and aligned model instead of just the domain and alignment vectors in Fig. 2. Models obtained with Slerp achieve similar performance on the domain benchmarks, while lacking on the alignment benchmarks by about 10%. This performance compromise can may be due to the interference caused during model interpolation, as we consider changing all the parameters. These results indicate that merging the domain and alignment vectors is more beneficial, but model interpolation can also help. We plan to further study the effects of various merging methods as future work."}, {"title": "Comparison with Preference Alignment Methods", "content": "We perform preference alignment of the domain expert models and evaluate them in Fig. 2. We observe that while the domain expert models gain better safety performance due to preference alignment for medicine by about 15%, they do not gain performance on the finance domain, and suffer on the domain performance for both domains. This might be either a characteristic preference alignment, in line with works that suggest alignment tax as a potential drawback of safety training of language models leading to decreased utility [Lin et al., 2024] or due to using parameter-efficient training, which still requires more compute than model merging. Overall, we observe that MERGEALIGN has significantly better knowledge-safety tradeoffs as compared to preference tuning of domain expert models."}, {"title": "Conclusion and Future Work", "content": "Drawing inspiration from model merging studies, we propose MERGEALIGN, an efficient way for the safety alignment of domain expert models that does not compromise their utility in the domain of interest. MERGEALIGN interpolates the domain vector of the expert model and the alignment vector of its general-purpose instruction-tuned counterpart. By applying MERGEALIGN on domain models in medicine and finance, we obtain models that achieve similar performance on safety benchmarks compared to a strongly aligned model, while retaining their domain expertise. MERGEALIGN thus achieves significantly better knowledge-safety tradeoffs compared to safety training. Further analyses on performance and model similarity comparison of MERGEALIGN with preference alignment methods like DPO and ORPO justifies and validates its benefits. We hope to incentivize research into efficiently and effectively aligning domain expert models given their widespread usage in the real world.\nFor future work, we aim to formulate merging methods that are particularly tailored in aligning models and making them safer, for which we plan to draw inspiration from works on safety vectors and safety basins of models. We also plan to use MERGEALIGN to make alignment flexible to domains, since the definition of safety and preferences often varies across domains. The findings of model merging works on more than 2 tasks can be leveraged to perform multi-preference domain-specific alignment as well"}, {"title": "Limitations", "content": "While MERGEALIGN does get significant alignment performance on the benchmarks, it is known that the performance of the merge model often depends on the individual capabilities of the individual models being merged. Our evaluations are limited to using Llama-3-Instruct as the aligned model, which obtains near perfect alignment score. Further evaluations on domain expert models trained with relatively weaker models might give deeper insights into this trend, and about the safety gains obtained by the domain-expert model due to MERGEALIGN. Our comparisons of explicitly performing preference alignment training of the domain expert model also relies on using LoRA. Though we primarily use LoRA due to resource constraints and for a fairer comparison with model merging methods in terms of resource requirements, full fine-tuning can provide different observations which can further provide motivations for coming up with methods that don't degrade the domain knowledge of the model. Overall, we believe that evaluating MERGEALIGN on more domains, with domain expert models trained with different quality of base models, and comparison with various preference alignment methods is important. Another limitation of the current method is it assumes the availability of a general-purpose instruction-tuned model which has high alignment performance. Though this is available nowadays for all large pre-trained language models, it would be interesting to see how a custom aligned model on public data performs when use for MERGEALIGN on the knowledge-safety tradeoffs. Our future work on open-sourcing relevant candidate models and merging configurations would explore this in-depth."}]}