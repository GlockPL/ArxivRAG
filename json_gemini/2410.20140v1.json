{"title": "MAD-SHERLOCK: MULTI-AGENT DEBATES FOR\nOUT-OF-CONTEXT MISINFORMATION DETECTION", "authors": ["K. Lakara", "J. Sock", "C. Rupprecht", "P. Torr", "J. Collomosse", "C. Schroeder de Witt"], "abstract": "One of the most challenging forms of misinformation involves the out-of-context\n(OOC) use of images paired with misleading text, creating false narratives. Exist-\ning AI-driven detection systems lack explainability and require expensive finetun-\ning. We address these issues with MAD-Sherlock: a Multi-Agent Debate system\nfor OOC Misinformation Detection. MAD-Sherlock introduces a novel multi-\nagent debate framework where multimodal agents collaborate to assess contextual\nconsistency and request external information to enhance cross-context reasoning\nand decision-making. Our framework enables explainable detection with state-\nof-the-art accuracy even without domain-specific fine-tuning. Extensive ablation\nstudies confirm that external retrieval significantly improves detection accuracy,\nand user studies demonstrate that MAD-Sherlock boosts performance for both ex-\nperts and non-experts. These results position MAD-Sherlock as a powerful tool\nfor autonomous and citizen intelligence applications.", "sections": [{"title": "INTRODUCTION", "content": "Our growing dependence on online channels for news and social networking has been complemented\nby a surge in exploits of digital misinformation (Aslett et al., 2024; Hasher et al., 1977; Brashier &\nMarsh, 2020). While many manipulation techniques pose serious threats, one of the most prevalent\nmethods for creating fake online content is the out-of-context (OOC) use of images (pbs). This\ninvolves using unaltered images in a misleading, false context to convey deceptive information, a\nstrategy that requires minimal technical expertise. Indeed, the problem of OOC misinformation\ndetection requires a complex understanding of the relationship between the text and image and\nthe ability to identify when they do not go together. Identifying these minute inconsistencies is a\ntime-consuming and high-effort task for humans. A study by Sultan et al. (2022) shows that time\npressure reduces the ability of human beings to detect misinformation effectively, further adding to\nthe scalability issues in human expert detection.\nTherefore, attention has turned to AI-driven tools that can help human experts recognise instances\nof OOC image-based misinformation at scale. Unfortunately, conventional deep learning forensic\ntechniques (Castillo Camacho & Wang, 2021; Heidari et al., 2024; Zhu et al., 2018; Amerini et al.,\n2021; Hina et al., 2021), which target detecting manipulations such as PhotoShop editing (Tolosana\net al., 2020; Masood et al., 2023; Farid, 2016; Wang et al., 2019) and AI-generated (or manipu-\nlated) fake images called Deepfakes (mit), rely on spotting artifacts from image or text tampering.\nIn contrast, OOC detection demands cross-contextual reasoning, as the deception arises from the\nmisalignment between the legitimate image and its falsely associated textual content.\nPretrained Large Multimodal Models (Liu et al., 2024b; OpenAI & et al., 2024; Li et al., 2019; Rad-\nford et al., 2021, LMMs) provide a promising direction for detecting OOC use of images for their\nability to process both text and image content in tandem. However, using LMMs directly for OOC\ndetection presents several challenges, particularly in the news domain. For instance, news articles"}, {"title": "RELATED WORK", "content": "Recent work has focused on using joint image-text representations to classify an instance as OOC.\nAneja et al. (2022) follow a self-supervised approach to assess whether two captions accompanying\nan image are contextually similar. They enforce image-text matching during training by formulating\na scoring function to align objects in the image with the caption. During inference, they use the se-\nmantic similarity between the two captions to classify them as OOC or not. The increased reliance\non textual content limits the capabilities of this approach. This work also does not provide expla-\nnations for model predictions and is, therefore not interpretable. Moreover, this method works for\nimage caption pairs where captions have information about objects in the image. This is not always\nthe case with news articles (our domain of application), where captions can often just be related to\nthe main content of an article rather than precisely describing the objects in the image.\n\nAbdelnabi et al. (2022) present the Consistency Checking Network (CCN) in which they emulate\ndifferent aspects of human reasoning across modalities for misinformation detection. This method\nuses evidence related to the image-text pair aggregated from the Internet. The CCN consists of\nmemory networks to assess the consistency of the image-caption pair against the retrieved evidence\nand a CLIP (Radford et al. (2021)) component to evaluate the consistency between the image and\ncaption pair. The use of external evidence to better inform model decisions is an important idea\nand also explains the superior classification performance of CCN when compared to other methods.\nThis method also lacks the explainability component.\nZhang et al. (2024) extend the neural symbolic method (Yi et al. (2019); Zhu et al. (2022)) to propose\nan interpretable cross-modal misinformation detection model to provide supporting evidence for\nthe output prediction. They use symbolic graphs based on the Abstract Meaning Representation\n(Banarescu et al. (2013)) of textual and visual information to detect OOC image use. Zhou et al.\n(2020) introduce Similarity Aware Fake news detection (SAFE), where neural networks are used\nto learn features of text and visual news representations. Their representations and relationships\nare jointly learned and used to predict fake news. Wang et al. (2018) introduce EANN: Event\nAdversarial Neural Networks to derive event invariant features which can be used to detect fake\nnews that has recently been generated. EANN uses adversarial training to learn multi-modal features\nindependent of news events. These methods require pretraining from scratch and, therefore, don't\nbenefit from the advanced reasoning capabilities and world knowledge of large pretrained models.\nShalabi et al. (2023) use synthetic multi-modal data to establish the authenticity of image-text pairs.\nThey use BLIP-2 (Li et al. (2023b)) to generate a caption for the original image and Stable Diffusion\n(Rombach et al. (2022)) to generate an image for the given original caption. This synthetic data is\nthen used to reason that if the original image and caption are OOC, then the original and generated\nimages should also be OOC as well as the original and generated text. This method relies on syn-"}, {"title": "METHODOLOGY", "content": "We present an explainable misinformation detection system, MAD-Sherlock, which jointly predicts\nand explains instances of misinformation.  To the best of our\nknowledge, all prior work except Qi et al. (2024) provide predictions without explanations, and no\nprior work uses multiple models to approach this problem. We present a novel methodology that\ninvolves multiple multi-modal models debating against each other in order to decide if an image-text\npair is misinformation or not. In this work, we aim to answer the question:\nCan debating multi-modal models, when equipped with external context, be used to solve the\nproblem of explainable misinformation detection by picking up on minute contextual\ninconsistencies?\nWe use detailed external information to inform the model's predictions through the external re-\ntrieval module, which utilises reverse image-based search in order to provide the agents with exter-\nnal real-world context related to the image-text pair. We carry out our experiments with the GPT-40\n(OpenAI) model to achieve state-of-the-art performance on the misinformation detection task while\nalso providing detailed and coherent explanations for the predictions. We achieve this without any\ndomain-specific fine-tuning, thus ensuring easier and faster generalization to other domains in addi-\ntion to low computational overhead."}, {"title": "DEBATE MODELLING", "content": "Analogous to real-world conversations, communication between two AI agents can also be struc-\ntured in a myriad of ways. We explore multiple debating strategies to structure the conversation\nbetween agents. Instead of simple back-and-forth conversations, we opt for a debating set-up in\nwhich agents are asked to frame their own opinions and then defend them to other agent(s). We\nobserve this facilitates more involved and detailed discussions among the models.\nAsynchronous Debate (not) against Human: We define an asynchronous debating strategy in\nwhich models wait for the other participants' responses before generating their own. \nand (b) show synchronous and asynchronous debating structures, respectively. While synchronous\ndebates, where all participants speak at once, can be faster and computationally more efficient, we\nopt for an asynchronous setting where each model response is based on previous responses of other"}, {"title": "PROMPT ENGINEERING", "content": "The debate structure is substantiated through prompt engineering.  shows that the first stage\nof our method requires for each AI agent to generate an independent response to whether the given\nimage-text pair is misinformation. Each agent must take into account the external context related to\nthe image obtained through the external information retrieval module. Specifications of the various\nprompts used in this work can be found in Appendix A.3. An initial prompt provides the agent with\na summary of the news articles related to the image and, based on it, asks the agent to classify the\nimage-text pair as misinformation or not. The prompt asks the agent to focus on certain details in the\nimage, such as watermarks, flags, etc. We observe that images used in news articles often contain\nminute yet crucial details which can be used to inform the final decision about whether the image\nactually belongs to the news articles. Therefore, prompting the agent to pay special attention to these\ndetails further helps detect inconsistencies."}, {"title": "EXTERNAL INFORMATION RETRIEVAL", "content": "Since a model's world knowledge is limited to its training data (and hence a particular time frame),\nincorporating external retrieval allows the model to access information beyond this training data\n(and time frame). Previous work makes use of pre-existing external retrieval-based datasets (Ab-\ndelnabi et al. (2022)) to supplement external information related to an image-caption pair. However,\nwe find this information lacking in detail since it is limited to the title of a news article. Agents\ncan greatly benefit from the knowledge of the entire news article and its content rather than just\nthe title when making a decision about whether a given image-caption pair, when considered in the\ncontext of the news article, is misinformation. To this end, we propose our own external informa-\ntion retrieval module. We observe a significant improvement in accuracy after incorporation of the\nexternal information retrieval module into the pipeline. The module is implemented in two stages:"}, {"title": "API-BASED INFORMATION RETRIEVAL", "content": "The Bing Visual Search API (vis) is used for the task of obtaining web pages related to a given image.\nA given image from the dataset is used to obtain a list of web pages completely and partially related\nto the image. We take the top three matching web pages in which the image appears. We believe\nthese web pages contain sufficient information to allow the agent to develop a general understanding\nof the context in which the image is originally used. Since the community-accepted dataset for this"}, {"title": "SUMMARIZATION USING LLM", "content": "Once the top three web pages have been identified, we scrape the text from the web pages to obtain\nthe textual information related to the context in which the image appears on the Internet. The com-\npiled textual information is often too long to be passed directly to the agent, and hence, we use the\nLlama-13B (Touvron et al. (2023)) language model to summarize this information. The summaries\nobtained from the LLM only focus on the most important parts of the text and hence also allow\nagents to develop a more focused understanding of the external context. While this method of sum-\nmarization works for most samples in our dataset, there are some examples where the obtained web\npages are not in the English language, and the LLM struggles with summarization. In this regard,\nwe add an additional check that ignores text from web pages in languages other than English. While\nthis restricts our system to the English language, it does not adversely affect system performance\ndue to the distribution of the dataset, which consists of images mostly taken from English-language\nnews articles. Multi-lingual support can be achieved by first translating text to English and then\nsummarizing it."}, {"title": "COHERENT REASONING", "content": "All the different components of MAD-Sherlock are brought together in this stage of the pipeline.\nEach multi-modal agent is employed to participate in the best-debating set-up with the relevant\nprompts and is asked to detect a given image-text pair as misinformation and provide an explanation\nfor the same. The agents also have access to external information related to the image through the\nexternal retrieval module. The final decision of the system is obtained once the debate terminates,\nwhich is after a certain number of debate rounds or after all agents converge to a common response,\nwhichever is earlier."}, {"title": "EXPERIMENTS AND RESULTS", "content": "We perform a series of experiments and report results on the NewsCLIPpings dataset (Luo et al.\n(2021)). The dataset is built based on the VisualNews (Liu et al. (2020)) dataset, which consists of\nimage-caption pairs from four news agencies: BBC, USA Today, The Guardian and The Washington\nPost. The NewsCLIPpings dataset is created by generating OOC samples by replacing an image in\none image-caption pair with a semantically related image from a different image-caption pair. CLIP\n(Radford et al. (2021)) is used to retrieve semantically similar images for a given caption. We report\nresults on the Merged-Balanced version of the dataset, which has balanced proportions of all the\nretrieval strategies and positive/negative samples. The training, validation and test sets have 71,072,\n7,024 and 7,264 samples, respectively."}, {"title": "EXPERIMENTAL SETUP", "content": "All experiments were run on 8 A40 (46GB) Nvidia GPU server.\nDebate Setup: We conduct experiments to select the best debating configuration using the LLaVA\nmodel (Liu et al. (2024b)). The experiments are carried out on a smaller subset containing 1000 test\nsamples of the main NewsCLIPpings test dataset. All experiments are run for k = 3 rounds or until\nthe agents converge (whichever is earlier).\nExternal Retrieval Module: We use the Bing Visual Search API (vis) to run an image-based\nreverse search. Using the API we select the top k = 3 pages in which the image appears and scrape\nthe text from them using the Newspaper3k library (new). Finally, we use Llama-13B (Touvron et al."}, {"title": "COMPARING DEBATE SETUPS", "content": "We compare multiple debating setups using the LLaVA model, to select the best one for comparison\nwith other works and further experimentation."}, {"title": "PERFORMANCE COMPARISON", "content": "We present our results on the NewsCLIPpings dataset against existing out-of-context detection meth-\nods discussed in section 4.2."}, {"title": "USER STUDY", "content": "We conducted a user study to evaluate the effectiveness of our system in detecting and explaining\nmisinformation. While it is easy to quantify model performance in terms of misinformation detec-\ntion, there are no effective metrics to assess the quality of the explanations generated by the model.\nTherefore, in order to perform a thorough analysis of the system performance, a user study is essen-\ntial. For a deeper analysis we further grouped the participants based on their profession into three\ngroups, namely: Journalists, AI Academics (studying AI) and Others. Further details regarding the\nstudy setup and participant groups can be found in Appendix A.4.\nIn the study, participants were shown ten image-text pairs and were asked to decide if the image and\ncaption, when considered together, were misinformation or not. They were also asked to provide\na confidence rating for their answer on a scale of 0-10, with 10 being the highest confidence level.\nFor each image-text pair, after the participants provided their initial answers, they were shown AI"}, {"title": "SETUP", "content": "The user study was designed to evaluate the effectiveness of our system in detecting and explain-\ning misinformation. While it is easy to quantify model performance in terms of misinformation\ndetection, there are no effective metrics to assess the quality of the explanations generated by the\nmodel. Therefore, in order to perform a thorough analysis of the system performance, a user study\nis essential.\nA total of 30 participants volunteered to participate in this study. The group of individuals included\njournalists from BBC as well as students and professors from the University of Oxford. Participation\nwas completely voluntary and no personal information was used for the purpose of analysis in this\nstudy. For a deeper analysis we further grouped the participants based on their profession into three\ngroups, namely: Journalists, AI Academics and Others. The 'others' category included anyone\nwho did not belong to the first two groups. The study was conducted through a Microsoft Form.\nParticipants were shown 10 image-text pairs and were asked to decide if the image and caption when\nconsidered together was misinformation or not. They were also asked to provide a confidence rating\nfor their answer on a scale of 0-10, with 10 being the highest confidence level. For each image-text\npair, after the participants provided their initial answers, they were shown AI insights about the same\nimage-text pair. These AI insights were the final outputs from MAD-Sherlock. Participants were\nthen asked to reconsider their answer and again decide if the image-text pair was misinformation or\nnot, in light of the new information from the AI agent. Participants were also required to re-evaluate\ntheir confidence score in this new answer. While it is not entirely avoidable, we did ask participants\nto keep aside their personal opinions of AI and consider all AI insights objectively. Participants were\nnot allowed to access the Internet. This was done to ensure an unbiased estimate of average human\nperformance.\nThe image-text pairs to include in the study were taken from the NewsCLIPpings (Luo et al. (2021))\ndataset. Al insights were taken from our best-performing setup involving the GPT-40 model. Of\nthe 10 image-text pairs presented to the participants in the study, there were 5 instances of misin-\nformation and 5 instances of true information. Further, all model insights were true except two of\nthem. Therefore the model accuracy for the task was 80% and we use this as the baseline accuracy\nto compare human performance against.\nWe analyse two special cases, where MAD-Sherlock argues for the wrong answer. We include these\nresults in order to observe how persuasive our system can be even when it is wrong. We note in the\ninstance where the image-text pair was actually misinformation and the model argued that it was\nnot, 6 participants changed their correct responses to those suggested by MAD-Sherlock. Although\nthis is only 5% of the participants, it still gives a significant insight into how persuasive the model"}, {"title": "CONCLUSION AND FUTURE WORK", "content": "Misinformation detection has become a pressing issue in recent times. With the ever-advancing ca-\npabilities of vision and language models, the detection of OOC image use has become a very difficult\ntask. In this work, we explore the question of whether it is possible for multiple AI agents to pool\ntheir contextual knowledge and converge to a common prediction in order to identify instances of\nmisinformation. We identify Asynchronous_Debatehuman as the most optimal communication\nsetup for AI models. We observe significant performance improvement when the models believe\nthey are debating against a human instead of another AI agent. We observe that in this setup, models\ntend to be more involved and open to changing their opinions. Our method also allows for agents\nto have freedom of opinion which they may change mid-debate. Agents in such a setting show\nenhanced abilities to critically evaluate an argument and pick up on minute inconsistencies.\nOur final system, MAD-Sherlock, achieves state-of-the-art performance on the misinformation de-\ntection task. Further, owing to our advanced external retrieval module, MAD-Sherlock provides\nclear, coherent and detailed explanations. As a result, MAD-Sherlock significantly improves the\nOOC misinformation detection performance of both human experts, and non-experts.\nWe identify several promising avenues for future research in this field. Notably, improving how\nagents handle disambiguation queries could significantly enhance detection accuracy and explain-\nability by providing access to additional information. The research community would benefit from\na continuously updated benchmark dataset, incorporating more recent news articles and subtler in-\nconsistencies. A direct extension of this work involves applying our methods to video-text pairs."}, {"title": "APPENDIX", "content": "Despite the strong performance of MAD-Sherlock, several limitations remain. First, while our\nmodel excels at detecting out-of-context image-text pairs, its reliance on external retrieval can lead\nto reduced accuracy when relevant context is unavailable or difficult to retrieve. Second, the quality\nof explanations is constrained to textual outputs, limiting multi-modal explanation capabilities such\nas image or video integration. Third, the system's performance is sensitive to hyperparameter tun-\ning, including the number of debate rounds and agents, which may require further optimization for\nbroader use cases.\nAdditionally, while our user studies provided valuable insights, large-scale deployment in diverse,\nreal-world settings, such as professional or citizen intelligence environments, is necessary to fully\nassess the method's robustness and scalability. Finally, our dataset, though comprehensive, primarily\nfocuses on English-language news, limiting the generalizability of the system across non-English\ncontexts.\nAnother important limitation is the potential risk that open-sourcing MAD-Sherlock might allow\nadversaries to train models specifically designed to counter or evade detection by our system. As\nadversarial actors gain access to the source code, they could exploit its known strengths and weak-\nnesses to develop countermeasures that diminish its effectiveness. However, despite these risks, we\nbelieve that open-sourcing remains the right path forward. Open-sourcing encourages transparency,\ncollaboration, and rapid innovation, enabling the broader community to contribute improvements,\ndetect vulnerabilities, and build on the system.\nMoreover, by engaging the community, we can foster the development of more resilient and adap-\ntive models that evolve in response to emerging adversarial techniques, thus maintaining MAD-\nSherlock's effectiveness in the long term. The collective strength of a diverse, open-source commu-\nnity can outweigh the potential threats posed by adversarial exploitation.\nFuture work will need to address these limitations to enhance the practical utility, robustness, and\nlong-term resilience of MAD-Sherlock."}, {"title": "SAMPLE IMAGE-CAPTION PAIR IN THE NEWS DOMAIN", "content": "Figure 4: Russian President Vladimir Putin has called Ukraine's move into Kursk a \u201cmajor provo-\ncation\". Image and caption taken from the BBC article here (Accessed at 17:43 on Aug 11, 2024):\nhttps://www.bbc.co.uk/news/articles/cze5pkg5jwlo"}, {"title": "PROMPTS FOR MAD-SHERLOCK", "content": "This is a summary of news articles related to the image: {}\nBased on this, you need to decide if the caption given below\nbelongs to the image or if it is being used to spread false\ninformation to mislead people.\nCAPTION: {}\nNote that the image is real. It has not been digitally altered.\nCarefully examine the image for any known entities, people,\nwatermarks, dates, landmarks, flags, text, logos and other\ndetails which could give you important information to better\nexplain your answer.\nThe goal is to correctly identify if this image caption pair is\nmisinformation or not and to explain your answer in detail.\nAt the end give a definite YES or NO answer to this question:\nIS THIS MISINFORMATION?\n\nThis is what I think:\nDo you agree with me?\n{}.\nIf you think I am wrong then convince me why you are correct.\nClearly state your reasoning and tell me if I am missing out on\nsome important information or am making some logical error.\nDo not describe the image.\nAt the end give a definite YES or NO answer to this question:\nIS THIS MISINFORMATION?\n\nI see what you mean and this is what I think: {}.\nDo you agree with me?\nIf not then point out the inconsistencies in my argument (e.g.\nlocation, time or person related logical confusion) and explain\nwhy you are correct.\nIf you disagree with me then clearly state why and what\ninformation I am overlooking.\nFind disambiguation in my answer if any and ask questions to\nresolve them.\nI want you to help me improve my argument and explanation.\nDon't give up your original opinion without clear reasons, DO NOT\nsimply agree with me without proper reasoning.\nAt the end give a definite YES or NO answer to this question:\nIS THIS MISINFORMATION?"}]}