{"title": "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation", "authors": ["Ziyang Luo", "Xin Li", "Hongzhan Lin", "Jing Ma", "Lidong Bing"], "abstract": "The impressive performance of proprietary LLMs like GPT4 in code generation has led to a trend to replicate these capabilities in open-source models through knowledge distillation (e.g. Code Evol-Instruct). However, these efforts often neglect the crucial aspect of response quality, relying heavily on teacher models for direct response distillation. This paradigm, especially for complex instructions, can degrade the quality of synthesized data, compromising the knowledge distillation process. To this end, our study introduces the Adaptive Modular Response Evolution (AMR-Evol) framework, which employs a two-stage process to refine response distillation. The first stage, modular decomposition, breaks down the direct response into more manageable sub-modules. The second stage, adaptive response evolution, automatically evolves the response with the related function modules. Our experiments with three popular code benchmarks-HumanEval, MBPP, and EvalPlus-attests to the superiority of the AMR-Evol framework over baseline response distillation methods. By comparing with the open-source Code LLMs trained on a similar scale of data, we observed performance enhancements: more than +3.0 points on HumanEval-Plus and +1.0 points on MBPP-Plus, which underscores the effectiveness of our framework. Our codes are available at https://github.com/ChiYeungLaw/AMR-Evol.", "sections": [{"title": "1 Introduction", "content": "Recently, the powerful proprietary large language models (LLMs), like GPT3 (Brown et al., 2020), GPT4 (OpenAI, 2023), Gemini (Anil et al., 2023a) and Claude (Anthropic, 2023), have showcased impressive code generation ability. Especially, GPT4, the most performant model, has recorded pass rates exceeding 85% on the well-known HumanEval benchmark (Chen et al., 2021). Despite their strengths, the closed-source nature sparks accessibility and privacy concerns (Wu et al., 2023). In response, there is a trend of adopting knowledge distillation (Xu et al., 2024) to transfer the advanced code generation ability from the proprietary LLMs to open-source counterparts, thereby enhancing their capabilities while ensuring broader availability and owner autonomy.\nGiven that accessing the model weights of proprietary LLMs is infeasible, the knowledge distillation pipeline is considered as a process where the teacher models synthesize supervised data, primarily consisting of instruction-response pairs (Liu et al., 2024). Student models are subsequently trained on this data, enabling the transfer of capabilities from the teacher models. For example, Chaudhary (2023) employs the self-instruct method (Wang et al., 2022) to prompt the teacher model to generate new coding instructions based on predefined seed tasks. Similarly, OSS-Instruct (Wei"}, {"title": "2 Related Work", "content": "LLMs and Code Generation. Recently, LLMs have showcased significant achievements across a vast array of tasks. Leading tech firms have made substantial progress in developing highly advanced close-source LLMs, including OpenAI's GPT4 (OpenAI, 2023), Google's PaLM (Chowdhery et al., 2022; Anil et al., 2023b) and Gemini (Anil et al., 2023a), as well as Anthropic's Claude (Anthropic, 2023). On the other side, the AI community has also seen the launch of several open-source LLMs, with model weights becoming publicly available. MistralAI has contributed the Mistral-Series (Jiang et al., 2023). Google has released UL2-20B (Tay et al., 2022) and Gemma (Mesnard et al., 2024). Tsinghua University introduced GLM-130B (Zeng et al., 2022) and MiniCPM (Hu et al., 2024), while Meta has made available OPT (Zhang et al., 2022) and LLaMA1&2&3 (Touvron et al., 2023a,b; Meta, 2024). Furthermore, Allen AI has introduced the wholly open-sourced LLM, OLMo (Groeneveld et al., 2024), and Microsoft has released Phi-series (Gunasekar et al., 2023; Li et al., 2023b). Although a gap remains between the open-source models and their closed-source counterparts, this gap is gradually narrowing.\nIn parallel, recent research efforts have been directed towards leveraging LLMs for code-related tasks to address the understanding and generation of code. OpenAI has unveiled Codex (Chen et al., 2021), Google has proposed CodeGemma (Google, 2024), and Salesforce has introduced CodeGen-Series (Nijkamp et al., 2023b,a), and CodeT5&Plus (Wang et al., 2021, 2023). Contributions from Tsinghua University include CodeGeeX (Zheng et al., 2023), and the BigCode Project has developed StarCoder1&2 (Li et al., 2023a; Lozhkov et al., 2024). Meta has also made its mark with the CodeLlama (Rozi\u00e8re et al., 2023), while DeepSeek has open-sourced the DeepSeekCoder (Guo et al., 2024). These initiatives underscore the growing interest in employing powerful base LLMs for code generation. Our work introduces a novel method for more effectively distilling code knowledge from closed-source models to these open-source base models, thereby enhancing the coding performance.\nKnowledge Distillation for Code Generation. To enhance the capabilities of open-source LLMs for code generation, recent works have adopted the knowledge distillation paradigm, utilizing closed-source LLMs as teachers for supervised data synthesis (Chen et al., 2023b; Zheng et al., 2024; Li et al., 2024; Yuan et al., 2024). For example, Chaudhary (2023) employs the self-instruct method (Wang et al., 2022) to generate training data, while Magicoder (Wei et al., 2023) generates training content using code snippets from GitHub. WizardCoder (Luo et al., 2024), on another hand, introduces the Code Evol-Instruct approach to progressively increase the complexity of coding tasks. Despite these advancements, a common limitation among these efforts is their primary focus on the creation of code instructions, often overlooking the criticality of enhancing code response distillation. Our research takes an orthogonal path by concentrating on the refinement of code response distillation, offering a novel perspective compared to previous works."}, {"title": "3 Method", "content": "As depicted in Figure 2, we introduce our novel framework, AMR-Evol, aimed at improving code response distillation to elicit better performance of the student models. In this section, we will provide a detailed discussion of our framework's pipeline."}, {"title": "3.1 Direct Response Distillation", "content": "In the knowledge distillation framework, the foremost goal is enabling the student model Ms to assimilate the strategies deployed by the teacher model Mt in tackling code generation tasks. Utilizing approaches like Code Evol-Instruct facilitates the generation of an extensive dataset of code instructions {I} by the teacher model. Subsequently, the direct response distillation method employs the teacher model to process these task instructions to produce the corresponding code responses Rd, resulting in a paired dataset, Ddirect = {(I, Rd)}. Then, the student model Ms learns from this dataset through supervised fine-tuning."}, {"title": "3.2 Adaptive Modular Response Evolution", "content": "As discussed in Section 1, direct responses Ddirect to complex instructions can result in suboptimal quality, which in turn impacts the performance of the student model Ms. While these responses often include logical errors or may not fully align with the precise requirements of the tasks, they generally remain close to correct and capture the essential concepts needed for task solution. To address this, our AMR-Evol framework capitalizes on these direct response distillations as a starting point. It incorporates a two-stage method\u2014modular decomposition and adaptive response evolution\u2014for an automated refinement process that improves the quality of responses, thereby enhancing the efficacy of distillation.\nMoular Decomposition (MD). In the first stage of our framework, we employ the principle of modular programming (Dijkstra, 1967) to tackle the complexity inherent in distilling code responses. Our method utilizes direct responses Rd as a starting point, guiding the teacher model Mt in breaking down the given code instructions into a series of smaller, well-defined sub-modular functions. We represent this process mathematically as follows:\n{Fm, Fm,...,Fm} \u2190 Mt (I, Rd), (1)\nwhere each function module Fm is conceptualized to fulfill a distinct subset of requirements stipulated by the code instruction I. This decomposition breaks down complex instructions into a series of easier and more manageable sub-modules, enabling the teacher model to tackle each one with less difficulty. This results in a more effective response distillation process.\nAdaptive Response Evolution (ARE). In the second stage, we observe that while coding instructions may greatly differ, the sub-modules needed for assembling the final solution often share similarities or can even be identical (Parnas, 1972). Leveraging this insight, we establish an auxiliary functional module database {F}, which archives all validated modules for future reuse. This database acts as a repository, enabling the retrieval of previously validated sub-modules to foster the creation of new code responses.\nBuilding upon the modular decomposition achieved in the first stage, {Fm, Fm,..., Fm}, we initially convert both the newly decomposed and previously archived functional modules into dense vector representations through a sentence embeddings model Mr:\nV+Mr (F), (2)\nwhere V() denotes the dense representation of any given functional module F). Then, to facilitate the retrieval of the most suitable archived module for each new sub-module, we apply:\nSim (Fm, F) \u2190 CosineSimilarity (Vfm, Vfy): (3)\nwhere Sim (Fm (Fm, F) calculates the similarity between the dense representations of two modules using cosine similarity. The archived modules that exhibit the highest similarity are then used as additional in-context contents, assisting the teacher model in refining the final code responses:\nRamr \u2190 Mt (I,{F},{F}), (4)\nwhere Ramr represents the refined code responses. These responses, alongside the original instruction I, compile an evolved dataset aimed at optimizing the knowledge distillation process.\nAs the process evolves, our framework identifies new modules within Ramr that exhibit notable differences from those currently in the database-judged by the cosine similarity between the new modules and existing ones. Modules that are distinct undergo a rigorous verification stage prior to their integration into the database. This critical stage harnesses the capabilities of the teacher model for generating unit tests tailored to the functionalities of the specific modules. This procedure not only assesses the functional correctness of the new modules but also ensures that they meet the predefined quality standards, thereby streamlining the process of enriching the module database with reliable and effective components.\nFunctional Module Database. The functional module database is pivotal within our AMR-Evol framework. We begin by compiling a collection of seed functions that have been validated. Leveraging the self-instruct method (Wang et al., 2022), we prompt our teacher models to generate a diverse range of function modules. Following this, we adopt a strategy similar to CodeT (Chen et al., 2023a), instructing the teacher models to produce unit tests that verify the functionality of these modules. Only the functions that pass these unit tests are included in our dataset. Through this stringent process, we construct a seed functional module database that becomes a fundamental component of our framework."}, {"title": "3.3 Knowledge Distillation", "content": "Upon completing the data synthesis process with the help of teacher models, we acquire a dataset that consists of paired instructions and responses, Damr = {(I, Ramr)}. This dataset equips the student model M for the task of knowledge distillation, where it is trained to use I as input with the goal of generating responses Ramr that closely resemble those produced by the teacher model. The training follows an auto-regressive learning objective, formalized as follows:\nL(0) = \u2211log P(Ramr I; 0), (5)\n(I,Ramr) Damr\nwhere L(0) denotes the loss function minimized during training, and 0 signifies the parameters of the student model Ms. This objective encourages the student model to accurately predict the next token in the response sequence, given the instruction I and the current state of the generated response."}, {"title": "4 Experiment", "content": "Baselines. Within our evaluation framework, we compare the performance of our framework against several baselines in code response distillation. The first of these, referred to as direct, utilizes teacher models to distill code responses in a straightforward manner, as detailed in Section 3.1. The second baseline employs the Chain-of-Thought (CoT) prompting method for distilling responses (Hsieh et al., 2023). This approach is analogous to the few-shot CoT method (Wei et al., 2022), in which the teacher model first provides a step-by-step explanation leading up to the formulated response. Our third baseline, AnsRepair, draws inspiration from previous works (Chen et al., 2023a; Olausson et al., 2023; Chen et al., 2023d), where the teacher models are utilized to generate unit tests. These tests serve to evaluate the correctness of the generated responses. If the responses fail these tests, the teacher models are subsequently invoked to make the necessary corrections. More details about baseline methods are included in the Appendix A.\nDatasets and Benchmarks. Our framework focuses on distilling responses and necessitates a dataset of instructions. To this end, we utilize a subset of the training set from the MBPP as our seed data. This is then expanded using the self-instruct method with the teacher model to generate around 10k instructions. With these newly derived instructions, we employ a process akin to the Code Evol-Instruct to iteratively synthesize a spectrum of complex coding instructions across three distinct levels of complexity. This variety allows us to assess our framework's efficacy in handling complex instructions. More data construction and decontamination details can be found in the Appendix B."}, {"title": "4.3 Analysis", "content": "Quality Comparison. Our experimental findings illustrate the effectiveness of our AMR-Evol in enhancing the knowledge distillation. To further validate the efficacy of AMR-Evol in producing better instruction fine-tuning data, we conducted a manual evaluation. We randomly selected the sample sets of 120 coding problems for each levels of complexity. Given that all samples are coding challenges, their responses can be definitively classified as either correct or incorrect. Two experienced programmers were engaged to review and label the code responses generated by various methods as suitable or not. The manual assessment results, depicted in Figure 3, reveal that although no method attained complete perfect, AMR-Evol demonstrated consistently superior performance compared to all other baseline methods across all complexity levels. In Appendix E, we also include some examples of responses generated by different methods to qualitatively compare their quality.\nAblation. In Table 3, we present an ablation study meticulously designed to identify the individual contributions of modular decomposition (MD) and adaptive response evolution (ARE) to the efficacy of our framework. First, we remove the MD stage in our framework by adopting direct response to retrieve the related function modules for ARE. This led to a performance drop, underscoring its crucial role in our framework. Specifically, the omission of MD typically results in the recall of only one function module based on the direct response. However, while direct responses address more complex or larger coding tasks, function modules target tasks with finer granularity. This difference creates a gap, making it challenging for the retrieved function modules to effectively contribute to refining the direct responses.\nSubsequently, we exclude the ARE stage, which also resulted in a performance decline, highlighting its vital role in the framework. Without ARE, the generation of responses is solely reliant on the modular decomposition output, lacking the improvements that come from in-context learning with related function modules. This places the entire responsibility for refining responses on the inherent capabilities of the teacher model. This analysis strongly reinforces the indispensable nature of both MD and ARE within our framework. In Appendix F, we also present examples to showcase the output of the MD stage and the top-1 function modules retrieved from the database."}, {"title": "4.4 Comparing with Open Code LLMs", "content": "To delve deeper into the efficacy of our framework, we have incorporated AMR-Evol with one of the SOTA instruction construction methods, Code Evol-Instruct, to expand our SFT data set. We have generated around 50k instructions using this approach and employed AMR-Evol to distill code responses from the teacher models (GPT3.5). Subsequently, we used deepseek-coder-6.7b-base and CodeLlama-7b-Python-hf as our two student models for training. For a relative fair comparison, we compare our fine-tuned student models against publicly available academic Code LLMs, which are trained with a similar scale of SFT data and employ the same base models as ours. This includes MagiCoder-DS/CL (Wei et al., 2023), WaveCoder-DS (Yu et al., 2023), and WizardCoder-CL (Luo et al., 2024). We also compare against official instruction models, namely DeepSeek-Coder-Instruct and CodeLlama-Instruct, to showcase performance gaps. For more discussions about baseline selection and SFT details, please refer to the Appendix G."}, {"title": "5 Conclusion", "content": "In this study, we present a novel framework, AMR-Evol, that leverages a two-stage approach\u2014namely, modular decomposition and adaptive response evolution\u2014to enhance code response distillation from teacher models, thereby improving knowledge distillation in code generation. Our experiments across three well-known coding benchmarks, HumanEval, MBPP, and EvalPlus, demonstrate the effectiveness of our method."}, {"title": "Limitation", "content": "Our framework has room for enhancement in several aspects:\nFirst, despite Figure 3 showcasing our method's capacity to improve the accuracy of code response distillation, achieving 100% accuracy remains unattainable. While our approach does alleviate this concern to some extent, the risk of delivering low-quality responses that could potentially mislead the student models cannot be entirely eliminated. Future endeavors could explore the integration of tools, such as compilers, to further refine the quality of the responses.\nSecond, our framework's enhanced capability for code knowledge distillation is accompanied by a requirement for multi-stage generation, leading to increased costs in leveraging the teacher models. This cost-performance trade-off has been discussed in Appendix H, where we conclude that the benefits in performance outweigh the incremental costs incurred.\nThird, the design of our method is narrowly focused on code knowledge distillation, limiting its broader application across general domains. The foundation of our framework in modular programming principles presents considerable obstacles in adapting its method for use in non-coding areas."}, {"title": "A Baselines", "content": "To ensure a fair comparison, we incorporate three distinct response distillation methods as our baselines. The first method is direct distillation. As outlined in Section 3.1, this approach involves using the teacher model to directly produce responses based on the provided code instructions. The prompt used is as follows:\nThe second method involves response distillation utilizing the Chain-of-Thought (CoT) approach. We adopt the method from the few-shot CoT (Wei et al., 2022), prompting the teacher model to produce the responses. To minimize costs, we opt to include a single example in our prompt:\nThe third baseline, AnsRepair, incorporates self-repair techniques (Chen et al., 2023a; Olausson et al., 2023). This method employs the teacher model to generate unit test functions for each sample, enabling the model to verify the correctness of its own answers. The employed prompt is as follows:"}, {"title": "B Datasets", "content": "Our framework concentrates on distilling responses and requires a dataset of instructions for this purpose. As indicated in Table 6, we enumerate the quantity of instructions used in our experiments. We initiate our process with the MBPP training set (task-ids 601-974) as a seed dataset, which enhances our ability to generate Python code effectively. To prevent any overlap with the EvalPlus test data, we are diligent in omitting any samples that coincide with the test set, thereby narrowing our training set to 332 unique MBPP tasks. We then utilize this filtered seed data and apply the self-instruction method to construct instructions. Subsequently, we employ the Code Evol-Instruct method to iteratively generate instructions of varying complexity across three distinct levels.\nTo ensure decontamination of our datasets, we invoke a method akin to the work of Code Evol-Instruct (Luo et al., 2024) for data filtering. This involves employing the gte-large-en-v1.5 model to treat each test set sample as a query, which retrieves the top five most similar samples from the training data. Subsequently, these pairs are evaluated by GPT4 in a binary classification task to decide whether a match exists. Detected matches lead to the exclusion of those specific training samples to eliminate potential data leakage."}, {"title": "C Benchmark", "content": "Table 7 details the quantity of questions along with the average number of unit tests per question across all the benchmarks utilized in our study. The license of HumanEval is MIT.\u00b9 The license of MBPP is cc-by-4.0.\u00b2 The license of EvalPlus is Apache-2.0.\u00b3"}, {"title": "D Implementation Details", "content": "Our AMR-Evol framework encompasses a two-stage process. In the first stage, Modular Decomposition is applied to break down the code instructions into multiple sub-modules, using the direct responses as the initial seed data. The prompt utilized for this stage is demonstrated above. During the second stage, Adaptive Response Evolution refines these decomposed sub-modules, utilizing the retrieved modules to develop the final answer. The corresponding prompt for this stage is as follows:\nFor all instruction construction processes, we set the temperature to 0.7 and the sequence length to 2048. For all response distillation processes, the temperature is fixed at 0.0, and the sequence length is set to 3000. We train the models for 200 steps across 3 epochs with a sequence length of 2048, employing the AdamW optimizer, BF16 precision, and DeepSpeed Zero-2 (Rasley et al., 2020). The training is conducted on 4 A800 GPUs."}, {"title": "E Qualitative Comparison", "content": "Table 10 11 12 display distilled responses obtained through various methods. It is evident from the comparison that our framework facilitates the generation of better responses for code knowledge distillation."}, {"title": "F Modular Decomposed and Retrieval Examples", "content": "Table 13 14 15 showcase the modular decomposed (MD) and retrieved top-1 (Recall) examples."}, {"title": "G Comparing with Open Code LLMs", "content": "To compare with other Open Code LLMs, we integrate our AMR-Evol framework with Code Evol-Instruct to continually expand our SFT dataset. We also employ the same data decontamination method to prevent data leakage. We have generated approximately 50k training samples. Subsequently, we fine-tuned our models using settings similar to those detailed in Appendix D. Given the larger volume of data, we opted to increase the number of training steps to 400.\nTo obtain a relative fair comparison, we only include the open code LLMs which are trained with a similar scale of SFT data and employ the same base models as ours, including MagiCoder-DS/CL, WaveCoder-DS, and WizardCoder-CL. We also compare against official instruction-based models, namely DeepSeekCoder-Instruct and CodeLlama-Instrut. However, these official models are trained with more than 20 times data than ours, which lead to unfair comparison. We only want to showcase the performance gaps.\nModels with a higher parameter count have been excluded from our comparison, such as DeepSeekCoder-Instruct-33B, WizardCoder-33B-v1.1, Codestral-22B-v0.1, CodeLlama-Instruct-34B, and Starcoder2-15b-Instruct. These models considerably exceed the size of our own, rendering a direct comparison unfair. Additionally, models that primarily derive their learning from GPT4 are excluded, including MagiCoder-S-DS, WaveCoder-DS-Ultra, and OpenCodeInterpreter (Zheng et al.,"}, {"title": "H Data Synthesis Cost Trade-off", "content": "Differing from direct distillation, our framework necessitates multi-stage response distillation, which increases the cost of using the API of the teacher model (around 4 times). However, Table 1 and 2 showcase that our method can outperform the direct distillation over all tasks and different student models. In addition, we adopt the gpt-3.5-turbo-1106 as our teacher model, whose API price is low. Therefore, we conclude that the benefits in performance outweigh the incremental costs incurred."}, {"title": "I Adopting Open-Source LLMs as Teachers", "content": "While our work primarily focuses on distilling the code generation ability from closed-source models, we also include an additional experiment using the open-source model, Llama-3-70B-Instruct, as our teacher model. Table 9 shows that our method is also effective when using the open-source model as the teacher."}, {"title": "J Broader Impact", "content": "Our research presents a novel framework for transferring code knowledge from closed-source LLMs to open-source LLMs. This framework is designed to generate code responses for various coding instructions during the data synthesis process. While our approach has been shown to improve response quality, as illustrated in Figure 3, it does not guarantee absolute correctness. Consequently, data generated through our method may still contain errors. It is essential to filter out these erroneous samples before deploying our approach in real-world applications to mitigate the risk of misuse."}, {"title": "K Manual Evaluation", "content": "In Figure 4, we present the interface used by human annotators to determine whether a given response is an appropriate answer for the coding tasks under evaluation, as shown in Figure 3. The annotators are the authors of this paper, possessing expertise in programming."}, {"title": "L Use Of AI Assistants", "content": "The AI assistant, GPT4-Turbo, is used solely for refining the writing of our paper."}]}