{"title": "Video Occupancy Models", "authors": ["Manan Tomar", "Philippe Hansen-Estruch", "Philip Bachman", "Alex Lamb", "John Langford", "Matthew E. Taylor", "Sergey Levine"], "abstract": "We introduce a new family of video prediction models designed to support downstream control tasks. We call these models Video Occupancy models (VOCs). VOCs operate in a compact latent space, thus avoiding the need to make predictions about individual pixels. Unlike prior latent-space world models, VOCs directly predict the discounted distribution of future states in a single step, thus avoiding the need for multistep roll-outs. We show that both properties are beneficial when building predictive models of video for use in downstream control. Code is available at github.com/manantomar/video-occupancy-models.", "sections": [{"title": "1 Introduction", "content": "Modeling the future is essential for planning. Predicting future events has long been a fundamental principle for learning in animals (Stachenfeld et al., 2017), driving recent deep learning research to focus on building better predictive models (Oord et al., 2018; Ho et al., 2022). Modeling the future from video data presents a significant challenge that requires addressing two fundamental questions. First, how detailed should such a model be? Should it produce pixel-level predictions, or should it operate at a more abstract level, such as a latent representation of the input? Second, how far into the future should the model predict? Should sampling from the model depend on a specific temporal timestep? This question is inherently linked to how many video frames a model processes at a given time to produce future samples. In the context of planning for downstream control, these remain open questions with multiple valid answers and various trade-offs.\nRegarding the first question, making predictions directly in the pixel space of input images is computationally complex and costly, often expending resources on predicting elements that are not useful for control. Capturing the essential information in a compact latent space and making predictions within this space is frequently a more efficient alternative. Significant progress has been made in learning better latent representations via self-supervised learning (He et al., 2022), and powerful generative models have been developed to make predictions in these latent spaces. These techniques offer good solutions to capturing information more abstractly but have seen limited use in making temporal predictions.\nFor the second question, most predictive models typically learn to make one-step predictions, which can be combined autoregressively to produce predictions further into the future. The successor representation (SR)(Dayan, 1993) captures the expected occupancy of future states in a single representation but does not provide the full sampling capability of a dynamics model. Recently, a generative analog of the SR, called y-models(Janner et al., 2020), was introduced. y-models can sample from the discounted future state occupancy distribution, reducing the need to unroll a standard one-step model over multiple timesteps. However, y-models have only been applied to problems with small state spaces and have not addressed the challenges of learning compact representations from rich pixel-level video data."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Successor Representation and Gamma Models", "content": "The successor representation (SR) (Dayan, 1993) aims to capture a summary of future state occupancy. Specifically, it learns to estimate the expected occupancy of visiting a future state $s_e$ and is denoted as $M(s_e|s_t)$, where the conditioning is on the current state $s_t$. The SR can be learnt similar to how value functions are learnt, via temporal difference (TD) backups, described as follows:\n$$M(s_e | s_t) = E_{s_{t+1}}(I[s_e = s_{t+1}] + \\gamma M(s_e | s_{t+1})),$$\nwhere the reward is replaced by an indicator, SR reward. The SR reward is 1 if the next state is the same as the state for which the SR is computed, and 0 otherwise. When the state space is high dimensional, such an indicator reward can be replaced by a continuous feature reward, in which case the SR equivalently captures future feature or representation occupancy. The feature reward essentially refers to the features associated with an observation $x_t$, denoted as $\\phi(x_t)$. We call the features as a reward since they substitute for the reward in the TD update. This version of the SR is then fittingly termed Successor Features (Kulkarni et al., 2016).\n$$M(\\phi(x_e) | \\phi(x_t)) = E_{s_{t+1}}(\\phi(x_{t+1}) + \\gamma M(\\phi(x_e) | \\phi(x_{t+1}))),$$Instead of learning to estimate the expected state occupancy, one could also learn to estimate the entire distribution. Gamma models (Janner et al., 2020) achieves this capability via generative TD based sampling and uses either GANs or Flow models for capturing the SR distribution. In the next section, we will utilize the same idea of generative TD learning for learning distributions of future representations of observations."}, {"title": "2.2 Quantized AutoEncoding and Self-Supervised Models", "content": "Vector Quantized Variational Autoencoders (VQ-VAEs) (Van Den Oord et al., 2017) are a type of generative model designed to create high-quality discrete latent representations. They consist of an encoder that transforms input data into a latent space, a quantizer that maps these continuous latent vectors to a finite set of vector embeddings (codebook), and a decoder that reconstructs the data from these quantized vectors.\nDINO (Caron et al., 2021) (self-Distillation with No labels) is a self-supervised learning method aimed at learning visual representations without labeled data. It employs student and teacher networks of identical architecture, and uses the teacher network's predictions to align the student network's predictions. DINO uses stop-gradient and exponential moving average (EMA) on the teacher network to avoid representation collapse. This approach enables DINO to learn robust and transferable visual representations from large-scale unlabeled datasets, demonstrating high performance in various downstream tasks such as image classification and object detection."}, {"title": "3 Video Occupancy Models", "content": "Video Occupancy models consist of two parts, a dedicated representation space which captures information present in a short sequence of video frames, and a generative model that can produce temporal predictions over the representation space. A natural choice for the generative model is to use an autoregressive model and so we exclusively use a GPT-2 (Radford et al., 2019) model that autoregressively predicts a sequence of tokens. The representation space is responsible for producing this sequence of tokens that get fed into the generative model (see Figure 1). In particular, a continuous representation of a sequence of frames/observations is quantized into discrete tokens that are used as input to the generative model. Instead of processing a long sequence of frames, VOCs only encode a small number of observations into the current representation $z_t$. A temporal target is then computed in a similar fashion, capturing information present in future observations. The quantized versions of the current and the temporal target representation are then concatenated to form a sequence of discrete tokens. The GPT model (denoted as M) is trained to do next token prediction on this concatenated sequence of tokens. The temporal target is computed such that it captures future information within a single representation time-step. To achieve this, we sample the temporal target from a mixture distribution, that between the next time-step (quantized) representation $z_{t+1}$ and a bootstrapped output of the GPT model conditioned on the next time-step representation $M(\\cdot|Z_{t+1})$. The temporal target $Z_{temporal}$ can thus be defined as a sample from the following mixture distribution: $(1 \u2013 \\gamma) p(\\cdot | z_t) + \\gamma M'(\\cdot | Z_{t+1})$, where the $M'$ is the bootstrapped version of the GPT model. The sampling between these two distributions within the mixture is governed by the parameter $\\gamma$, which describes how myopic or farther-away-in-time the temporal target should be. In the case that $\\gamma = 0$ for instance, all the sampling weight is put on the next time-step distribution and so we recover a standard 1-step model. In all other cases, the bootstrap model output plays a role in sampling the temporal target and hence it carries more future information than a 1-step target. Note that this way of computing the temporal target is exactly how TD-learning functions, where the next time-step representation replaces the reward, and the parameter $\\gamma$ is typically called the discount factor. Once such a model is learnt, a single forward pass will generate a future representation (governed by the value of $\\gamma$), circumventing the issue of having to do multiple forward passes through a one step model to produce a future prediction.\nWith the TD-learning perspective in mind, it is fairly straightforward to view VOCs as a highly expressive generative analog of the successor representation (SR). In the SR context, future predictions are typically learnt by capturing an expectation of future state occupancy via TD learning using an indicator reward. However, when the observations are high dimensional, comparing observations pixel-by-pixel is clearly not a good idea in order to capture similarity. To that end, VOCs allow for learning a useful representation that only capture information about the abstract structure present in the observations, and not focus on the lower level detail, such as pixel intensities. Additionally, capturing an expectation of the future representations (as is typically done for learning the SR) is too limited as it does not allow us to use the model to sample future observations. An analogous generative version switches to learning the distribution of occupancy of future observations rather than just the expectation and thus allows for sampling from the model. These two properties produce a powerful combination within a single model.\nHaving described how the generative component of VOCs work, we now focus on how the representations are learnt. In this paper, we describe three different ways of doing so, each leading to a"}, {"title": "4 Experimental Analysis", "content": "In this section, we discuss three different instantiations of VOCs depending on how the representation space is constructed, namely \u2013 quantized autoencoding, inverse dynamics modelling, and self-supervised distillation. Additional details including hyperparameters are included in the Appendix. Finally, we include results on using VOCs in a model predictive control framework."}, {"title": "4.1 Quantized AutoEncoding", "content": "VQ-VAE VOCs. We begin by learning Video Occupancy models with a VQ-VAE representation space. We take a VQ-VAE model pretrained on ImageNet and then finetune it for specific MuJoCo-based datasets (Todorov et al., 2012; Tassa et al., 2018). The discrete codes produced by the VQ model act as tokens for the pixel-based observations. Each VQ encoded observation produces a 25-dim vector of discrete values with a vocabulary size of 1024. We then concatenate the VQ codes for three consecutive observations to produce the conditioning state $z_t$, akin to how frame stacking is necessary to capture and predict velocity information. We similarly generate a 75-dim (25-dim code vector \u00d7 3 observations) temporal target $Z_{temporal}$ and then concatenate the conditioning state and the target to form the input $(\\{z_t, Z_{temporal}\\} )$ to the GPT-2 model. The generative model then does next token prediction on this input. The bootstrapped target for the temporal difference (TD) update is generated by multinomial sampling from the GPT model (we do not use beam based sampling or greedy sampling strategies for generating the target). Note that temporal prediction is strictly limited to predicting what is captured in the representation space and is not driven by the pixel-level reconstruction loss. Since the representation is defined via a VQ-VAE, at inference we can sample future representations from the learnt model and use the the VAE's decoder to obtain pixel-level predictions, providing a natural way to inspect prediction quality. Figure 2 shows sampled outputs from VOCs trained for different $\\gamma$ values. For $\\gamma = 0$, we recover a single step model. As the $\\gamma$ value increases, the model makes longer-in-time predictions.\nBenefit over a 1-step model. We compare VOCs with a standard one-step model, which essentially corresponds to setting $\\gamma = 0$ for the VOC. Note that a single rollout step of the VOC models corresponds to a multi-step prediction, whereas a 1-step model must be unrolled for longer to produce a prediction corresponding to a given horizon length. A 1-step model thus suffers from accumulating errors in the autoregressive prediction the most. A VOC model on the other hand remedies this issue by using the TD-based backups in its model training and thus requires only a single pass to produce a multi-step prediction. The difference in accumulating errors in the 1-step model is directly reflected in the value function estimated with the model. Figure 4 shows how a 1-step model leads to more errors in the distribution of returns compared to a VOC model. Figure 3 shows autoregressive rollouts produced by each of these models for 5 model timesteps. Interestingly, we notice that the one-step VOC model leads to slightly diverging predictions as compared to the $\\gamma = 0.8$ VOC model. The inferior accuracy of the 1-step model is also showcased in the return distributions presented in Figure 4. This result highlights how TD-learning might be a superior objective when learning autoregressively over pixel spaces.\nRemark 4.1. We use a TD(1) objective in VOCs, where the (1) corresponds to using the next representation and then following up by the bootstrap prediction. Similarly, we can use a multi-step TD objective, where a representation k timesteps in the future is first sampled with probability $\\gamma^{k-1}(1 \u2013 \\gamma)$ and then we sample the bootstrap prediction with the remaining probability. This forms a k-step TD estimator, akin to the case of n-step value estimation with rewards."}, {"title": "4.2 Inverse Dynamics Modelling", "content": "VQ-VAE spaces preserve most pixel-level information present in the input. However, in the case when we solely care about control, a lot of the information can be dropped. Learning a multistep inverse kinematics (MUSIK) (Efroni et al., 2021; Lamb et al., 2022; Islam et al., 2022) model is one way to achieve a more compressed representation of the input. MUSIK works by learning to predict the action given the current and future observation in a trajectory. The conditioning over the future input enforces learning long-term dependencies within the representation so as to better predict the action. In this section, we develop a quantized version of the MUSIK representation. Figure 5 shows value estimation error plots when using MUSIK representations over VQ-VAEs. We are able to achieve a healthy reduction in codebook size when using more compact representations like MUSIK. This result showcases how most information can be successfully dropped (first through the MUSIK objective and then through the quantization) without effecting the downstream value estimates by much."}, {"title": "4.3 Self-supervised (distillation) Modelling", "content": "VQ-VAEs are based on convolutional networks (ConvNets) and have been shown to be less effective than transformer (ViT)-based self-supervised methods in capturing saliency maps of objects in images (Caron et al., 2021). This raises a natural question about comparing VQ-VAEs with more advanced tokenization schemes. Specifically, we use quantized DINO features as the token space for VOCs. The quantized version of DINO incorporates a VQ-bottleneck immediately after the ViT encoder, enabling it to provide tokens for an image.\nWe compare VQ-VAE and DINO-based Video Occupancy Models side by side in Figure 6. Here we compare the return estimation error computed via density estimation (using Eq. 4) for both the VQ-VAE VOC and DINO VOC models. Since the density-based return estimator requires a reward model, we use a common reward model for both DINO and VQ-VAE representations to ensure a fair comparison. We observe that quantized DINO representations lead to improved density value estimates on validation trajectories, resulting in lower errors in the corresponding return estimates."}, {"title": "4.4 Control with Video Occupancy Models", "content": "Having shown how VOCs can be learnt with different representation spaces and deployed for value function estimation, we finally move to using them for control. Our goal in this section is to use the most minimally feasible model-based setup to showcase VOCs ability to do downstream control. To that end, we adopt a model predictive control (MPC) (Nagabandi et al., 2018) approach. MPC works by querying the model with different candidate actions and choosing the action that leads to the maximum reward under the model's predictions. Since VOCs are not action conditioned, we cannot use them as is for ranking the utility of actions as in standard MPC. Instead, we select a set of candidate actions first and use a simulator to provide next step observations corresponding to the candidate actions given a starting observation. We then query the model for value estimates for these set of next step observations. Finally, we choose the action that leads to the observation with the maximum value as predicted by the VOC model.\nFigure 7 shows undiscounted return estimates across an episode length of 20 when using MPC with a VOC model. We include three natural baselines: 1) No Model: where actions are randomly picked from the candidate action set, and 2) Init Model where we pick actions according to value estimates generated from a randomly initialized GPT model used by the VOC, and 3) No Lookahead model where we choose the best action that leads to the highest predicted reward for the achieved next state, i.e. discarding multi-step return information for single step reward. Note that for 2), the reward model still carries important information about the utility of the next observation. Therefore, this baseline helps isolate the benefit of learning the underlying GPT model when doing control. We observe higher returns for the VOC model than compared to the three baselines. Note that for the cheetah domain, there is a bit of an overlap in the box plots between all four methods since there is room for a randomly chosen action to perform well for the chosen horizon value. However, overall, VOC value estimates can steer the control policy towards high returns."}, {"title": "5 Related Work", "content": "Video Predictive Models. The work in learning video prediction models can be broadly categorized into two main approaches: learning representations from video frame sequences and developing generative models to produce future frames.\nIn representation learning, the prominent methods include VideoMAE (Tong et al., 2022), which reconstructs masked videos, extending the Masked Autoencoders (MAE) concept from static images to multiple frames. A related approach involves masked image modeling that predicts within a representation space (Li et al., 2023). This method has been adapted for videos, where the prediction target is derived from future frame patches (Assran et al., 2023). Temporal contrastive learning approaches (Sermanet et al., 2018) have also been widely employed for learning control-oriented representations (Nair et al., 2022). Despite leveraging temporal information for prediction, these methods primarily result in fixed representations of given observations, rather than models capable of performing rollouts.\nOn the generative front, diffusion techniques have been extensively utilized to generate video frames by corrupting an entire sequence and then predicting the original, uncorrupted frames (Ho et al.,"}, {"title": "6 Limitations and Future Work", "content": "This paper combined various representation spaces within VOC models. However, we restricted to pre-learnt representations for the most part. It would be interesting if the temporal predictions from the generative model can be used as a target to learn the VOC representation itself. Furthermore, this paper dealt with simple representation learning methods such as the VQ VAE and so required stacking VQ codes corresponding to multiple frame for temporal prediction. Such a design is non-ideal since the VQ codes contain a lot of redundant information. For instance, the background content between three consecutive frames remains almost the same. A downside of carrying over such redundant information is that temporal prediction is restricted to fewer timesteps in the future. Future work can alleviate this issue by using representation methods that capture information across multiple frames (e.g. video MAE representations), i.e. capture dyanmics-relevant information in the VOC representations. This would reduce redundant information and encourage predicting temporally for even longer horizons. Essentially, processing information across a stack of frames would then be amortized via both the representation and the temporal prediction axes.\nFinally, we presented one way to incorporate predictions made by VOCs for better control, i.e. via model predictive control. There remain multiple more powerful ways to combine VOC-like world models with model-based control, including search methods over the learned model (Silver et al., 2018)."}, {"title": "7 Conclusion", "content": "We introduce a new family of video prediction models called Video Occupancy Models (VOCs). VOCs work in a well defined latent space (that of discrete codes produced via different representation methods) and temporally predict via a generative TD loss. Since they are essentially trained with a generative version of the Successor Representation, they circumvent the issue of unrolling a model multiple times to produce future predictions. Instead, they produce future predictions (as governed by the parameter $\\gamma$) within a single forward pass. We show that not predicting at every timestep leads to better and faster rollout accuracy. Furthermore, these models can be used to value estimation when given access to a reward function, and subsequently can be embedded in a model-based algorithm for downstream control."}, {"title": "8 Acknowledgments", "content": "Part of this work has taken place in the Intelligent Robot Learning (IRL) Lab at the University of Alberta, which is supported in part by research grants from the Alberta Machine Intelligence Institute (Amii); a Canada CIFAR AI Chair, Amii; Compute Canada; Huawei; Mitacs; and NSERC."}, {"title": "A Implementation Details", "content": "Quantized DINO Training. For DINO training, we use a pretrained BEiT-2 (Wang et al., 2022) which includes a quantized ViT encoder trained to match the outputs of a pretrained DINO model. The BEiT encoder has a codebook size of 8192, much larger than the VQ-VAE representation spaces (which uses 1024 by default). The rest of the training details follow the same procedure as described in the Hyperparameter table containing GPT details.\nQuantized Inverse Dynamics Modelling. For the MUSIK objective, we use the same VQGAN encoder as used by our VQ-VAE implementation, and replace the convolutional decoder with a 2-layer MLP head that predicts the action. For the input to the MUSIK model, we encode a current and future observation (k steps in the future) using the VQGAN encoder, and concatenate the post-quantization embeddings of both the observations before passing into the MUSIK decoder. We randomly sample k from 1 to 15."}]}