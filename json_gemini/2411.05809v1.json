{"title": "Is it me, or is A larger than B: Uncovering the determinants of relational cognitive dissonance resolution", "authors": ["Tomer Barak", "Yonatan Loewenstein"], "abstract": "This study explores the computational mechanisms underlying the resolution of cognitive dissonances. We focus on scenarios in which an observation violates the expected relationship between objects. For instance, an agent expects object A to be smaller than B in some feature space but observes the opposite. One solution is to adjust the expected relationship according to the new observation and change the expectation to A being larger than B. An alternative solution would be to adapt the representation of A and B in the feature space such that in the new representation, the relationship that A is smaller than B is maintained. While both pathways resolve the dissonance, they generalize differently to different tasks. Using Artificial Neural Networks (ANNs) capable of relational learning, we demonstrate the existence of these two pathways and show that the chosen pathway depends on the dissonance magnitude. Large dissonances alter the representation of the objects, while small dissonances lead to adjustments in the expected relationships. We show that this effect arises from the inherently different learning dynamics of relationships and representations and study the implications.", "sections": [{"title": "Introduction", "content": "Imagine strolling through an art museum, expecting awe-inspiring masterpieces. Suddenly, disrupting your expectations, you encounter, displayed in the vitrine, a banana. Dissonance arises. On one hand, the artistic value of museum displays is expected to be high. On the other hand, a banana seems to be endowed with limited artistic value. There are two ways of resolving this dissonance: recalibrating your expectations from museum displays (museums exhibit mundane objects) or finding a deeper appreciation for the artistic value of bananas.\nResolving cognitive dissonances is the hallmark of scientific inquiry. Consider a scientist who has consistently observed that particles of type B are larger than particles of type A. New experimental data, however, suggests the opposite: particles A are actually larger than particles B. This unexpected finding forces the scientist to choose between two alternative adaptation pathways: They can question the validity of the experimental results and maintain the view that B are larger than A. Alternatively, they can update their view and conclude that A are, indeed, larger than B.\nConsidering such resolutions more formally, we posit that the cognitive system is endowed with two modules1\u20133. The first is an input representational module, which encodes a relevant feature of the input. In the banana example, this module extracts a measure of artistic value from museum displays, be it a Rodin sculpture or a banana. In the particles' example, it extracts the sizes of the particles from the experimental data. The second is a relational module, which characterizes the relationship between different input representations or between such representations and an anchor. The relational module expects that the artistic value of objects displayed in art museums would be higher than some threshold, which is higher than the artistic value of mundane objects. The banana, having an artistic value similar to mundane objects, violates this expectation. In the particles' example, this module predicts that B would be larger than A. The dissonance can be resolved by adapting either the relational module or the representational module (or in some cases, both).\nWhile previous research on cognitive dissonance has not explicitly framed the resolution process in terms of representational and relational adaptations, classic experiments in the field can be reinterpreted through this lens. Consider induced-compliance studies, which are a common way of studying cognitive dissonance4,5. In these experiments, a participant is coerced to perform an action, which they would rather avoid. This causes inconsistency between the participants' belief that they are freely-acting agents that make their choices in accordance with their preferences, and the observation that, in fact, they act against their preferences. To resolve this dissonance, a participant can change their attitude toward their actions, interpreting them as less undesirable than they previously assessed. This corresponds to a representational adaptation of the action. Alternatively, they can resolve the dissonance by admitting they are less independent than previously believed, adapting their relational module.\nThe determinants of the adaptation pathway used for resolving a cognitive dissonance have been a topic of research for several decades, although not in the framework of representation vs. relation (see review). These determinants include the effort the adaptation pathway requires7,8, its likelihood to succeed, the dissonance magnitude10, the associated affective state5,11, and contextual factors12. While these findings have significantly advanced our understanding, much of the research has focused on qualitative approaches or specific experimental paradigms. By introducing a computational framework that emphasizes the dichotomy between representational and relational adaptations, we aim to provide a quantitative model that offers insights into the underlying mechanisms of adaptation.\nTo achieve this goal, we utilized ANNs that were designed to emulate relational learning and have distinct representational and relational modules13\u201316. We first examine how these ANNs resolve relational dissonances. To simulate such dissonances, we constructed an order discrimination task that requires identifying a specific relationship between image features. Then, that relationship is reversed, and we ask whether the ANN adapts its representational or its relational module. We start the paper by presenting the order discrimination task and the ANN model. We use them to demonstrate the two adaptation pathways and how the adaptation pathway depends on the dissonance magnitude. Then, we explain the results by analyzing a simplified task and network. Finally, we discuss our results and the implications for understanding humans' cognitive dissonance."}, {"title": "Results", "content": "In the order discrimination task, ANNs were presented with image pairs and were instructed to determine, for each pair, if it was presented in the correct or reverse order. Each image depicted shapes arranged on a 3 \u00d7 3 grid and was characterized by five features: the grayscale color of the shapes, their number, size, grid arrangement, and shape (Fig. 1). The first three features color, number, and size are described by a scalar number, establishing a natural order relation between images.\nThe \"correct\" order in the task depended on the identity of the relevant feature (color, size, or number), termed the predictive feature, and whether this feature increased or decreased from left to right. To learn this underlying rule, the ANN was continuously trained on examples of \u201ccorrectly\u201d ordered image pairs (Fig. 1a). In these examples, the predictive feature changed according to the underlying rule, while the remaining features, irrelevant to deciding the correct order, were randomly chosen for each pair but remained constant within the pair (mirroring the test images). In the main text of this paper we present the results when the predictive feature was the size. Similar results were obtained when the predictive features were color or number, and they are presented in the Supplementary Information (Figs. S1-S7).\nA key parameter in this task is \\( \\alpha \\), which represents the difference in the predictive feature values between the two images. A positive \\( \\alpha \\) implies that the predictive feature increases from left to right, whereas a negative \\( \\alpha \\) implies that it decreases. The absolute value of \\( \\alpha \\) determines the magnitude of change: \\( \\alpha = 0 \\) implies that the feature does not change between the two images, whereas \\( \\alpha = \\pm 1 \\) signifies maximal difference."}, {"title": "The ANN", "content": "The ANN is comprised of two main components. The first is an input representational module, an encoder, which we denote by \\( Z \\). Its goal is to extract the relevant feature. For each of the two images of a pair, x and x' (left image and right image, respectively), it maps the n \u00d7 n image to one-dimensional variables \\( Z_w(x) \\) and \\( Z_w(x') \\), where w are trainable parameters. We implemented this mapping using a multilayered convolutional network. A similar architecture was shown in a previous study to be able to extract the relevant features in an intelligence test17. The difference between the representations of the two images is given by \\( \\Delta Z = Z_w(x') \u2013 Z_w(x) \\). The relational module, which we denote as \\( R_\\theta \\), characterizes the expected relation between the representations of the two images of the pair. We used a single-parameter relational module, \\( R_\\theta = \\theta \\), for the expected difference between those representations. For a pair of images that are presented in the correct order, if the representational module precisely extracts the relevant feature, then \\( \\Delta Z = a \\), and if the relational module indeed predicts the correct relation, then \\( \\theta = a \\). Formally, we define a loss function for a pair of images as:\n\\begin{equation}\\nL(w, \\theta) = ((Z_w (x') - Z_w (x)) - R_\\theta)^2 = (\\Delta Z \u2013 \\theta)^2.\\n\\end{equation}\nNote that this loss function is minimized if the representational module extracts the relevant feature and the relational module predicts the correct relation. However, it is also trivially minimized by \\( \\Delta Z = 0 \\) for all inputs. To avoid the model collapsing into this trivial solution, we defined a regularized loss function, in which \\( \\Delta Z \\) and \\( \\theta \\) are driven to reside on a ring,\n\\begin{equation}\\nL' (w, \\theta) = L + \\lambda (\\Delta Z^2 + \\theta^2 - r^2)^2.\\n\\end{equation}\n\\( \\lambda \\) and r are hyper-parameters.\nThe model parameters w and \\( \\theta \\) are trainable and we used Stochastic Gradient Descent (SGD) on \\( L'(w, \\theta) \\) to learn them (see Methods).\nTo evaluate the performance of the ANN, we measured its ability to determine the order of novel test images. Specifically, we presented the ANN with two images, measured the values of the loss function associated with the two possible orders of these images (left-right or right-left), and chose the order that minimized the loss function. Fig. 2 left, depicts the average performance of 100 ANNs as a function of the size of the training set when \\( a = 0.5 \\), showing that the networks successfully learned to solve the task. The high performance holds for other values of \\( a \\). We tested a values ranging from 0.1 to 1 and found that throughout this range, the ANNs performance exceeded 90% accuracy, where this performance was achieved after training with 160 image pairs (Fig. 2 right)."}, {"title": "Dual adaptation pathways in ANNs", "content": "In order to study cognitive dissonance in ANNs, we trained them with a sequence of image pairs characterized by a specific predictive feature and by one value of positive \\( a \\). After learning, we changed the training set relationship from \\( a \\) to \\( -a \\). That is, the shapes' sizes decreased rather than increased from left to right (Fig. 3a), creating a cognitive dissonance. We continued training the ANNs with the new rule and measured the performance of the ANNs as a function of examples. The results are depicted in Fig.3b for a large dissonance (\\( a = 0.8 \\), blue) and a small dissonance (\\( a = 0.2 \\), red). In both cases, performance level just before a reversal (image pair 160) was almost perfect. The first image pairs immediately following the reversal were almost always incorrectly ordered by the ANNs (performance level close to 0). With examples, however, the networks adapted to the new rule, achieving almost perfect performance after additional 160 image pairs. Notably, learning was slower for the more difficult task associated with the smaller value of a. Also, adaptation to the reversal was slower than initial learning, a phenomenon that has been previously reported in human learning18.\nImmediately after the reversal, \\( \\Delta Z \\) necessarily reverses (because the order of the images was reversed), while the value of \\( \\theta \\) remains unchanged, creating a cognitive dissonance that manifests as a large loss function. With training, this dissonance is resolved. In principle, there are two ways to resolve it. Either the parameters of the encoder change such that \\( \\Delta Z \\) over the training image pairs revert to their pre-reversal values. Alternatively, the sign of \\( \\theta \\) changes, corresponding to a change in the relational module. This is reminiscent of the cognitive dissonances discussed earlier: the banana-on-display scenario and the particles' sizes experiment. In the banana case, the dissonance can be resolved by changing the perceived artistic value of the banana (representational module) or the expectations from a museum (relational module). Similarly, in the particles' example, the scientist can either question the validity of the new experimental results (changing the experimental data's representation) or update their view on particle sizes (adjusting the relational module). We were interested in investigating which of the two modules would adapt and, specifically, how the magnitude of the dissonance (a) affects the identity of the module that adapts.\nWe found that while the behavioral adaptation of the ANN was similar for the two values of a, the ANNs in the two cases adapted in two qualitatively different ways: For the large \\( a \\), the adaptation was restricted to \\( \\Delta Z \\), where after a short transient, the value of \\( \\theta \\) reverted to pre-reversal values. By contrast, for the small \\( a \\), adaptation was restricted to \\( \\theta \\), which flipped its sign (Fig. 4a-b). To further study how the identity of the adapting module (\\( \\Delta Z \\) or \\( \\theta \\)) depends on \\( a \\), we repeated the experiment for various values of \\( a \\) between 0.1 and 1. We found that for small values of \\( a \\), adaptation was typically mediated by a change in the sign of \\( \\theta \\). The opposite, change in the parameters of \\( \\Delta Z \\) was more typical the larger \\( a \\) was. The transition between the two adaptation pathways occurred at \\( a = 0.34\\pm0.02 \\) (Fig. 4c)."}, {"title": "Simplified model analysis", "content": "Why are small-dissonance and large-dissonance adaptations qualitatively different? To address this question, we considered the order discrimination task in a simplified framework that can provide an analytical understanding of the task and the two types of dissonance resolutions. In this simplified model, instead of images, the inputs are scalars that represent the values of a single feature. That is, x = x. A specific relationship between input pairs is then demonstrated by input pairs {x,x'} such that x-x = \\( \\alpha \\).\nTo solve this simplified task, we used a single-weight linear encoder, \\( Z_w(x) = wx \\). In this case, \\( \\Delta Z = wa \\) for all input pairs, and the loss function becomes\n\\begin{equation}\\nL'(w, \\theta) = (wa \u2013 \\theta)^2 + \\lambda ((wa)^2 + \\theta^2 - r^2)^2.\\n\\end{equation}\nIn the limit of an infinitesimally small learning rate, the SGD dynamics that minimizes the loss function can be written as a set"}, {"title": "of differential equations for w and \u03b819:", "content": "\\begin{equation}\\n\\dot{w} = - \\alpha (wa \u2013 \\theta) \u2013 2\\lambda a^2 ((wa)^2 + \\theta^2 \u2013 r^2) w\\n\\dot{\\theta} = (wa \u2013 \\theta) \u2013 2\\lambda ((wa)^2 + \\theta^2 \u2013 r^2) \\theta.\\n\\end{equation}\nThe dynamics of w and \\( \\theta \\) as a function of time in this model corresponds to the adaptation of the parameters (learning) with example pairs. Therefore, by analyzing the temporal dynamics of w and \\( \\theta \\), we can analyze the process of learning. Rather than looking at the dynamics of w, it is instructive to consider the dynamics of the output of the representational module, \\( \\Delta Z = wa \\). With this, the two differential equations become:\n\\begin{equation}\\n\\Delta\\dot{Z} = a^2 ((\\theta \u2013 \\Delta Z) \u2013 2\\lambda (\\Delta Z^2 + \\theta^2 \u2013 r^2) \\Delta Z)\n\\dot{\\theta} = (\\Delta Z \u2013 \\theta) \u2013 2\\lambda (\\Delta Z^2 + \\theta^2 \u2013 r^2) \\theta.\\n\\end{equation}\nIt is well-known that gradient systems are bound to converge to a stable fixed point, in which \\( \\Delta\\dot{Z} = \\dot{\\theta} = 0 \\). In this case, we can prove that the dynamics are endowed with two stable fixed points: \\( \\Delta Z = \\theta = \\pm \\sqrt{\\frac{r^2}{2}} \\) (see Methods). Both these fixed points correspond to a condition in which there is no dissonance \\( \\Delta Z = \\theta \\). However, they differ in the representation and rule: \\( \\Delta Z = \\theta = \\sqrt{\\frac{r^2}{2}} \\) implies that the representation of the predictive feature increases whereas \\( \\Delta Z = \\theta = - \\sqrt{\\frac{r^2}{2}} \\) implies that it decreases. To understand the difference, we can reconsider the examples in Figure 1a. The representation can extract the sizes of the shapes with a relational module that predicts that this quantity increases by a certain amount between the two images. Alternatively, the representation can extract the \u201csmallness\u201d of the shapes (the reciprocal of their size), with a relational module predicting that they should decrease between the two images. Each of these solutions corresponds to a different fixed point and in both cases, there is no dissonance.\nThe value of \\( \\Delta Z \\) depends on \\( a \\). Consequently, a change in \\( a \\) (the dissonance) will immediately result in a change in \\( \\Delta Z \\) and hence to a dissonance. Eventually, the dynamics will resolve the dissonance by converging to a fixed point. If the dynamics converge to the same fixed point (that is, same \\( \\theta \\)), it implies that the relational module was unchanged and changes in the representational module underlay the resolution of the dissonance. Convergence to the other fixed point implies an adaptation of the relational module.\nThe simulations of the ANNs suggest that when \\( a \\) is small, the relational module would adapt, while a large value of \\( a \\) would lead to the adaptation of the representational module. To understand this result, we reconsider Equations (5). The two differential equations are symmetrical to a change of \\( \\Delta Z \\) and \\( -\\theta \\), except for the prefactor \\( a^2 \\) in the dynamics of \\( \\Delta Z \\). Therefore, the value of \\( a^2 \\) will determine the relative change of the two parameters. If \\( a^2 < 1 \\), then the dynamics would be dominated by \\( \\dot{\\theta} \\), and the relational module would change to resolve the dissonance. By contrast, a dissonance in which \\( a^2 > 1 \\) would be resolved by the representational module. Thus, the critical a in the simplified model is \\( a = 1 \\).\nSome readers may find it useful to study the phase portrait of the model (Fig. 5). The phase portrait is a geometric representation of the trajectories of a dynamical system in the phase plane \\( \\Delta Z \\times \\theta \\). Each point in the phase plane corresponds to a particular configuration of the model (particular values of \\( \\Delta Z \\) and \\( \\theta \\)). The blue and red nullclines correspond to values of (\\( \\Delta Z \\times \\theta \\)) in which \\( \\Delta\\dot{Z} = 0 \\) and \\( \\dot{\\theta} = 0 \\), respectively. The intersections of the nullclines are the fixed points of the dynamics. \\( \\Delta\\dot{Z} = \\dot{\\theta} = \\frac{r}{\\sqrt{2}} \\) are stable fixed points of the dynamics, while \\( \\Delta\\dot{Z} = \\dot{\\theta} = 0 \\) is also a fixed point, but it is unstable (see Methods). The black arrows denote the direction of the dynamics. When \\( a \\) is large (left), the dynamics are predominantly horizontal. This is because of the \\( a^2 \\) prefactor in the dynamics of \\( \\Delta Z \\). For the same reason, the dynamics are predominantly vertical when \\( a \\) is small (right)."}, {"title": "Idiosyncratic modes of adaptation", "content": "When studying cognitive dissonance in ANNs, the parameter \\( a \\) signified a difference in the predictive feature values between the two images. For convenience, we defined |a| = 1 as the maximally possible difference. This scaling, however, is arbitrary. Consider, for example, the size of the shapes. They can be arbitrarily small (up to the resolution of the images), and hence changes in their size can be arbitrarily large. Put differently, there is arbitrariness in the scaling of the parameter \\( a \\), and this scaling can influence the choice of the adaptation pathway.\nAlong the same lines, in the simplified model, we posited that x' \u2013 x = \\( a \\). Given the feature has units (e.g., size measured in centimeters), measuring the features using different units (e.g., meters) would manifest in a different value of \\( a \\). For example, if x' - x = 0.1 in meters, x' -x = 10 in centimeters. In the former case, |a| < 1 and the relational module will adapt, whereas in the latter, a > 1 and the adaptation module will adapt. In other words, the scaling of the representation of the stimuli is expected to affect the point of transition from the adaptation of the representational and relational modules. Considering this model to be a model of adaptation to cognitive dissonance, idiosyncratic scaling can account for heterogeneity in the point of transition between the different modes of adaptation.\nTo test this prediction in the ANN, we scaled the magnitude of the inputs (the pixels) by a parameter \\( \\gamma \\) and measured the value of the critical \\( a \\), \\( \\bar{a} \\). In the linear network, scaling the inputs by \\( \\gamma \\) is like scaling a by \\( \\gamma \\). Thus, the simplified model predicts that \\( \\bar{a} \\) will be proportional to \\( \\frac{1}{\\gamma} \\). Indeed, as predicted by the simplified model, \\( \\bar{a} \\) in the ANN was approximately proportional to \\( \\frac{1}{\\gamma} \\) (Fig. 6). Notably, we used ReLU nonlinearity in our ANN simulation. It is possible that with a different nonlinearity of the network neurons, the agreement of the ANN with the simplified linear model would have been weaker."}, {"title": "Curriculum adaptation", "content": "So far, we considered the case in which dissonance emerges as a result of an abrupt change in the rule, a transition from a to -a. In real life, however, changes can be more subtle, slowly occurring over time. The quality of the music of our favorite rock band deteriorates from album to album, the quality of air in a previously polluted city improves over the years, and so on. Therefore, it is interesting to see how such gradual changes affect the process of adaptation. To address this question, consider an agent that is adapted to a large value of a, which we denote as am \u226b 1. Rather than making an abrupt change to - Am, as was done so far, we will slowly decrease a, allowing the network to adapt to these changes. To see the effect, consider the phase portrait of the dynamics of the simplified model depicted in Fig. 5. For clarity, we consider one of the fixed points as a starting point, without loss of generality, \\( \\Delta\\dot{Z} = \\dot{\\theta} = \\frac{r}{\\sqrt{2}} \\). A small decrease to a (that does not change its sign) will only slightly decrease the value of AZ, keeping it in the vicinity of the original fixed point. Hence, the dynamics will converge back to it. In other words, \\( \\theta \\) and \\( \\Delta Z \\) will converge to their original values, the latter by compensating for the change in a by an increase in w (recall that \\( w = \\frac{\\Delta Z}{a} \\)). When a changes its sign, the initial value of \\( \\Delta Z \\) changes its sign as well. Because when a < 1 the adaptation is dominated by the dynamics of \\( \\theta \\), the relational module and not the representational module @ will change its sign. As the magnitude of a continues to increase, w will decrease while @ will remain unchanged. This is depicted in Figure 8."}, {"title": "Another interesting case is a change via an intermediate learning step.", "content": "Rather than going directly from a to -a, the process is \\( a \\rightarrow \\beta \\rightarrow -a \\), where the agent is given sufficient time to adapt to the \\( \\beta \\) step before the change to -a occurs. For clarity, we will consider the case of a > 0. The condition |\\( \\beta \\)| < a is reminiscent of the gradual learning discussed above. Overshoot learning, in which \\( \\beta < -a \\) is particularly relevant from a cognitive point of view. For example, the first step in military training is boot camp. It is a period of intense training that is more stressful, in which soldiers' choices are even more restricted than in later stages of the military service. The other possibility, \\( \\beta > a \\), corresponds to a contrast-enhancing training in which the intermediate step causes the transition to \u2013 a to cause an even larger dissonance. As we shall see, while gradual learning supports the adaptation of the relation module, overshooting or contrast-enhancing promotes the adaptation of the representational module.\nWithout loss of generality, we assume that in adaptation to a, the dynamics has converged to the fixed point \\( \\Delta Z = \\dot{\\theta} = \\frac{r}{\\sqrt{2}} \\). We commence by considering \\( \\beta > \\alpha \\) (same sign as a) (Fig. 9 top). In the framework of the phase portrait, this change has two implications. First, the characteristic times of the two modules change such that the dynamics of AZ is now \\( \\beta^2 \\) faster than that of 0. Second, the initial state of the dynamics is not at the fixed point. Rather it is further away from the ordinates than the fixed point, at \\( \\frac{r}{\\sqrt{2}} \\). However, because it is in the same quadrant as the original fixed point, it will converge to it. The transition from \\( \\beta \\) to -a also has two effects. First, the characteristic times of the two modules revert to their original values and second, the initial state of the dynamics is flipped, and is closer to the ordinates than the fixed point. As a result, for \\( \\beta > a \\), the fixed point to the right (which requires flipping the sign of \\( \\Delta Z \\)) is closer than the fixed point below (which requires flipping the sign of 0), which allows the dynamics to adjust AZ even for values of a that are smaller than 1, as long as they are not too small.\nWe perform a similar analysis when \\( \\beta < 0 \\). A change from a to \\( \\beta \\) is associated with a change in both the characteristic times and in the initial conditions. This allows the dynamics to adjust AZ even for values of a that are smaller than 1 (Fig. 9 bottom)."}, {"title": "Discussion", "content": "In this paper, we studied alternative pathways for resolving cognitive dissonance that arise from relational expectations violation. Specifically, we used ANNs that were trained on an order discrimination task as a model system, and studied how they adapt when the expected relationship between image features is reversed. The networks can adapt either by reinterpreting their input to match the expected relationship or by adjusting their relational expectations, and we characterized the conditions in which each adaptation pathway is chosen. We found that the choice of adaptation pathway, representational or relational, depends on the magnitude of the discrepancy: large dissonances are resolved by changing the input representations, while small dissonances lead to adjustments of the relational module.\nTo understand why the mode of adaptation depends on the magnitude of the discrepancy, we further simplified the networks to consist of a single trainable parameter for each module, allowing us to use dynamical analysis to address this question. We found that the answer to this question is that by definition, the representational module is directly coupled to the input. That is, the input vector is multiplied by the parameters of its first layer (w\u00b7x; the synaptic weights). This coupling has two implications. First, when computing the gradient of the loss function (the derivative with respect to w), the input (x) remains a prefactor that determines the speed of learning. The larger the input, the faster is the learning. Second, the parameters affect the output through their product with the input. The larger the input, the smaller the required change in the parameters for resolving the dissonance. Together, a large change in the input (large dissonance) will lead to changing the representational module, while a small dissonance will be resolved via the relational module.\nInconsistency between beliefs and actions is a well-known cognitive dissonance. Consider smoking as an example. For many decades, smoking was not considered unhealthy, and the realization that smoking is linked with increased mortality was slow to be universally accepted. Consider a long-term smoker who is unaware of the detrimental consequences of smoking, and enjoys the dopamine release associated with the nicotine absorption every time that they smoke a cigarette. In our framework, the relational module of the smoker assigns a higher value to the action of smoking relative to the status quo (non-smoking). The excess dopamine release associated with smoking is consistent with this higher value. Then, mandatory tobacco package warning messages are implemented, and when smoking the cigarette the smoker encounters a label that warns against the harmful effects of smoking. The pleasure associated with smoking is now mixed with the fear of death and disease, and the output of the representational network which evaluates the value of smoking this cigarette is now lower than expected by the relational network. A cognitive dissonance emerges7. This cognitive dissonance can now be resolved by adapting the relational network: The smoker realizes that the value of smoking is in fact, lower than that of nonsmoking, and quits smoking. Indeed, the warning messages were added in order to discourage smoking. However, there is an alternative way of resolving the cognitive dissonance. the smoker can change the representational network by giving less weight or even ignoring the warning message altogether, allowing them to continue enjoying smoking without any cognitive dissonance. The effectiveness of utilizing such cognitive-dissonance-evoking interventions in order to change behaviors that seem self-defeating (e.g., smoking or over-eating), or behaviors that are detrimental to the environment (e.g., not recycling) has been studied20\u201322. Indeed, in some cases, the dissonance resulting from this intervention was not resolved by a change in the behavior of participants. Rather, participants reinterpreted the information supporting the change of behavior23. Our work suggests that the magnitude of the dissonance can influence the effectiveness of such intervention. Perhaps counter-intuitively, our work suggests that gradual changes, each associated with a moderate dissonance may be more effective than an abrupt change associated with a large dissonance.\nIn the cognitive sciences, the framework of reinforcement learning (RL) holds considerable sway. Therefore, it is informative to consider cognitive dissonance in view of reinforcement learning models. In RL problems, an agent interacts with the world, which delivers \"rewards\u201d whose magnitude and probability depend on the agent's action. The goal of the agent is to learn to choose those actions that would maximize some measure of the rewards, e.g., their expected rate. In many of the RL algorithms, learning is based on reducing a quantity that is known as \u201creward prediction error\", which quantifies the extent to which the reward that the agent receives following its action differs from its expected value24. This measure of surprise is reminiscent of a cognitive dissonance, and learning is associated with reducing it (resolving the dissonance). The fundamental difference between the possible resolutions to cognitive dissonance that we study here and those studied in RL is that in RL, the dissonance is resolved when the estimated value of the action is equal to its true value. In general, this resolution is unique. In the smoking example, this corresponds to an agent that learns to integrate the long-term health risks with the short-term benefits of smoking. If they decide that smoking is still beneficial despite its risks they will continue smoking while if not, they will quit. However, the literature on cognitive dissonance argues that this is just one possible resolution of the dissonance. An alternative resolution is that the agent chooses to focus on other benefits of smoking (e.g., suppress appetite). In other words, rather than changing the value of smoking in view of the evidence, the agent maintains consistency by changing the definition of \u201creward\u201d (suddenly deciding that suppressing appetite is more important than the long-term risks). Our two-module model provides a computational framework for this possibility. Changing the relational module is consistent with changing the value of the actions in view of the evidence. Changing the representational module corresponds to changing the definition of the reward.\nThe two adaptation pathways that we discussed in this paper are categorical (representational vs. relational). There are multiple ways to adapt, even within each category. Specifically, the representational module is characterized by a large number of parameters, more parameters than examples. Therefore, there can be multiple combinations of parameters that, for the same set of examples, yield the same representational adaptation. The relational module in our model is rather simple, but in a more general model we expect a similar multiplicity of possible adaptation pathways within the relational pathway. Indeed, different adaptation pathways within a module have been a subject of research in the cognitive sciences. For example, the cognitive dissonance associated with the unhealthy habit of smoking can be resolved by positing that benefits associated with appetite suppression outweigh the cancer-related health risks, or by questioning the research that links smoking to increased mortality7. Both solutions are consistent with a change to the representation of smoking. A major limitation of our model is that it is not informative about the determinants of within-category adaptation pathways.\nOur work relates to and extends several previous computational models of cognitive dissonance. Shultz and Lepper's constraint satisfaction model25 represented different cognitions (e.g., beliefs, ideas, and attitudes) as neurons in a recurrent network, with the weights between them representing their level of consonance. Dissonance resolution in this model occurred through the dynamics of neural activity that converged to a minimal overall dissonance. While pioneering, this model lacked a learning mechanism that would support long-term dissonance resolution. Read's recurrent network model extended their approach to include long-term adaptation utilizing the Contrastive Hebbian Learning algorithm26. Both approaches studied recurrent neural networks, unlike our feedforward modeling approach. Van Overwalle and Jordens27 approach was more similar to ours. They considered a feedforward ANN with two outputs: the attitude toward an object and the behavior associated with it. They studied the network's adaptation to attitude-behavior inconsistent inputs. Like our ANN, their network utilized the vanilla SGD algorithm for learning and adapting the weights. One difference between our model and this previous work is that our ANN is a more complex, multilayer convolutional neural network, enabling it to solve a visual task. Our main contribution on top of the previous works is distinguishing the representational and relational modules, allowing the examination of relational cognitive dissonances.\nLooking ahead, our work opens up directions for future research at the intersection of cognitive science and artificial intelligence. We have identified the determinants for dissonance resolution in relational settings. Empirical cognitive studies could test whether the predictions generated by our model generalize to human cognition. For example, the ability to learn causal relationships changes throughout life, where children learn new relationships better28. Our work suggests that the children's tendency towards learning the relational module, modeled by a faster learning rate of that module, would lead them to adapt their relational module more than adults when facing a cognitive dissonance. The findings of such empirical studies would in turn guide the development of better artificial models. Finally, our work highlights the importance of considering not just the content of mental representations (cognitions), but also their computational properties and interrelationships, in understanding how minds respond to violated expectations.\""}, {"title": "Methods", "content": null}, {"title": "Code availability", "content": "A PyTorch29 code that generates the results and figures of this paper is available at:\nhttps://github.com/Tomer-Barak/relational_cognitive_dissonances."}, {"title": "Order discrimination task", "content": "The order discrimination task was designed to assess the ability of Artificial Neural Networks (ANNs) to determine the correct order of image pairs based on a specific feature. Each image in the pair depicted shapes arranged on a 3 \u00d7 3 grid and was characterized by five features: grayscale color, number of shapes, size, grid arrangement, and shape type."}]}