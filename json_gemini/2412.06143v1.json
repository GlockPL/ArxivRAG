{"title": "Precise, Fast, and Low-cost Concept Erasure in Value Space: Orthogonal Complement Matters", "authors": ["Yuan Wang", "Ouxiang Li", "Tingting Mu", "Yanbin Hao", "Kuien Liu", "Xiang Wang", "Xiangnan He"], "abstract": "The success of text-to-image generation enabled by diffusion models has imposed an urgent need to erase unwanted concepts, e.g., copyrighted, offensive, and unsafe ones, from the pre-trained models in a precise, timely, and low-cost manner. The twofold demand of concept erasure requires a precise removal of the target concept during generation (i.e., erasure efficacy), while a minimal impact on non-target content generation (i.e., prior preservation). Existing methods are either computationally costly or face challenges in maintaining an effective balance between erasure efficacy and prior preservation. To improve, we propose a precise, fast, and low-cost concept erasure method, called Adaptive Vaule Decomposer (AdaVD), which is training-free. This method is grounded in a classical linear algebraic orthogonal complement operation, implemented in the value space of each cross-attention layer within the UNet of diffusion models. An effective shift factor is designed to adaptively navigate the erasure strength, enhancing prior preservation without sacrificing erasure efficacy. Extensive experimental results show that the pro-", "sections": [{"title": "1. Introduction", "content": "The recent advancements of text-to-image (T2I) diffusion models [11, 17, 18, 30, 35, 49, 50] have enabled users to effortlessly generate high-quality images with simple textual prompts. However, such generations would inevitably invoke copyrighted [10, 22, 41, 43] or offensive [23, 38] concepts, caused by the noisy training data scraped from web [9, 39]. Because it is very costly to re-train large generative models from scratch, it is vital to develop low-cost techniques to precisely erase unwanted semantic concepts in images, i.e., concept erasure. This task necessitates the precise erasure of visual content w.r.t. target concepts from generated images (i.e., erasure efficacy), while the preservation of irrelevant content w.r.t. the prompts comprising non-target concepts (i.e., prior preservation), safeguarding a secure T2I generation for diffusion models.\nA representative category of concept erasure methods is training-based, which fine-tunes a subset of model parameters by formulating meticulous erasing objective functions [12, 20, 26, 27]. Despite considerable erasure efficacy, they exhibit major limitations when being deployed in practice. Firstly, they require costly individual fine-tuning to erase each concept, thereby limiting their real-time usage, e.g., to erase newly emerging concepts. This is unacceptable to online T2I platforms, where copyrighted or offensive concepts could arise unexpectedly, with no means to produce a complete list of concepts to erase in advance. Moreover, these methods suffer from a limited balance between erasure efficacy and prior preservation, due to their reliance on regularization terms to trade off prior preservation.\nAn alternative category of concept erasure methods is training-free, such as Negative Prompt (NP) [2], Safe Latent Diffusion (SLD) [38], and SuppressEOT [24], which enable real-time erasure. They intervene in the image generation process and exhibit different drawbacks. For instance, NP was initially designed to enhance image quality and can result in compromised erasure efficacy; while SuppressEOT requires the user to specify the location of the target concept within the prompt, thus, it is not suitable for erasure applications that require full automation. Therefore, both NP and SuppressEOT fall short as independent tools for concept erasure. Regarding SLD, it does not perform well in retaining prior knowledge of non-target concepts as illus-"}, {"title": "2. Related Works", "content": "Re-training and Blocking: The most straightforward way to erase a target concept from a pre-trained T2I model is to exclude the training data relevant to this concept and re-"}, {"title": "3. Method", "content": "Concept erasure in T2I generation aims at a successful removal of the visual content indicated by textual concepts (i.e., target concepts), meanwhile a satisfactory preservation of the visual content irrelevant to these concepts (i.e., prior knowledge). It is challenging to achieve simultaneously sat-"}, {"title": "3.1. Preliminary on T2I Diffusion Models", "content": "Current T2I models usually include an image compression network [19] and a conditional latent diffusion model [35] that performs sequential denoising with a UNet [36] in the latent space. The UNet takes as inputs the noise latent variable $z_t$, timestep $t$, and embedding $C$ of the textual prompt from a pre-trained CLIP model [33], and predicts the noise $\\epsilon_{\\theta}(z_t, t, C)$. In training-free concept erasure, the noise is additionally conditioned on the target concept with text embedding $C_t$, predicted by $\\epsilon_{\\theta} (z_t, t, C, C_t)$.\nInteractions between the image and text modalities are enabled by cross-attention (CA) layers [35, 46] within the UNet, which align the latent representation of the noisy image with the semantic detail of the textual prompt. Each CA layer computes an attention map $A = \\text{softmax} (\\frac{QK^T}{\\sqrt{d}})$ with a latent feature dimension $d$. The queries $Q$ are computed from the noisy image features, while both keys $K$ and values $V$ from the text embeddings $C$, using different projection matrices. The layer output is a weighted aggregation of $A$ and $V$. More details on CA layers are in Appendix A.\nIt has been recognized that the keys in CA layers mostly act as the \"Where\" pathway, governing the layout of the attention map and determining the compositional structure of the generated images, while the values as the \"What\" pathway, controlling the content and visual appearance of images [44]. Because the goal of concept erasure is to modify the visual content of the generated images, we propose to conduct value decompositions into subspaces, which are uniquely constructed by exploiting the target concept and its orthogonal complement in an adaptive fashion. We demonstrate that information offered by the orthogonal complement can be used to generate successfully high-quality images with the target concept precisely erased."}, {"title": "3.2. Token-wise Target Embedding Pre-processing", "content": "Given a textual example, which can be either a target concept to erase or an original prompt, its embedding is computed at the token level by a CLIP text encoder. Each tokenized example is padded with [SOT] as the prefix and [EOT] at the end, with [EOT] filling any remaining positions to maintain a fixed token length of $l$. Each token is characterized by an embedding vector of dimension $D_c$."}, {"title": "3.3. Orthogonal Value Decomposition", "content": "Given a conditional latent diffusion model trained for standard T2I generation, our proposed erasing operation works by projecting the original textual prompt onto the orthogonal complement of the subspace spanned by the target concepts to erase, and it is implemented in the value space learned at each CA layer of the UNet. It supports both single-concept and multi-concept erasure.\nWe do not apply any erasing operation to [SOT], as it primarily serves as a prefix and does not carry useful infor-"}, {"title": "3.3.1. Single-concept Erasure", "content": "Our erasing operation works with $V_t$, where each column of $V_t$ corresponds to the erased value vector for token position $j$ and is denoted as $v_t^j$. It also works with the value matrix $V \\in \\mathbb{R}^{l \\times d}$ computed from the original prompt, where each column of $V$ is referred to as the original value vector for the token position $j$, denoted by $v_i$. To remove the effect of the target concept from the original prompt, we project the original value vector $v_i$ onto the orthogonal complement of the span of the erased value vector $v_t^j$ for each token position, and we denote this orthogonal complement by $\\text{span}^{\\perp}(v_t^j)$. This results in the following modified value vector:\n$v^j = P_{\\text{span}^{\\perp}(v_t^j)} v_i = (I_d - P_{\\text{span}(v_t^j)}) v_i = v_i - \\frac{v_t^{jT} v_i}{v_t^{jT} v_t^j} v_t^j$,\n$v^j v_i^\\perp$\nwhere $P_X$ denotes the orthogonal projection of a vector $x$ onto the space $X$, and $I_d$ is an identity matrix of size $d$. The modified value vector $v^j$ is used, instead of $v_i$, to calculate the output of the CA layer, as illustrated in Fig. 2 (c). Since $v_t^1 = 0$, it has $v^1=v_i$, meaning that no erasure is performed for [SOT]."}, {"title": "3.3.2. Multi-concept Erasure", "content": "We generalize the above operation to erase a set of $n$ target concepts, and their corresponding modified value matrices are denoted by ${V_t^h \\in \\mathbb{R}^{l \\times d}}_{h=1}^n$. We use $v_t^{h,j}$ to denote the $j$-th column of $(V_t^h)$, referred to as the erased"}, {"title": "3.4. Adaptive Erasing Shift", "content": "In practice, given a pair of textual prompts and a target concept to erase, their token-wise relevance can vary across different token positions. The different tokens of the prompt carry information with different intensities and focuses and thus can have quite different effects on image generation.\nWe discover that, although a projection onto the orthogonal complement of the target concept is effective at erasing this concept itself, it can sometimes affect the prior preservation due to excessive removal of information. Therefore, our adaptive design is focused on improving the prior preservation. When semantics carried by a prompt token are less relevant to a target token, we intend to perform less erasure to protect the prior image content. According to"}, {"title": "4. Experiments and Result Analysis", "content": "We conduct extensive experiments to evaluate the proposed AdaVD for erasing a diverse range of target concepts, covering specific instances, art styles, NSFW content, and celebrity. We compare with SOTA training-based methods including ConAbl [20], ESD [12], SPM [27] and MACE [26] and SOTA training-free methods including NP [2] and SLD [38]. We also demonstrate the time efficiency and interpretability of AdaVD, along with its wider usage in other downstream image generation tasks coupled with a series of diffusion models."}, {"title": "4.1. Experimental Setup", "content": "Implementation: We employ SD v1.4 [2] to generate images using the DPM-solver sampler [25] over 30 sampling steps with classifier-free guidance [17] of 7.5. All the compared methods are implemented following their default configurations available from their official repository. Further implementation details are provided in Appendix G.1.\nEvaluation: Adopting the same evaluation approach as used by SPM [27], we assess the methods based on 80 instance templates, 30 art style templates, and 25 celebrity templates, and benchmark each method by generating 10 images per template per concept in evaluation. To assess the performance of NSFW erasure, we use the I2P benchmark [38]. Results on instance and art style concept erasure are reported in Sections 4.2 and 4.3, while results on celebrity and NSFW erasure are reported in Appendices B.2 and B.1, together with performance compared with the SuppressEOT [24] and additional analysis in Appendix.\nPerformance Metrics: We follow the widely used evaluation protocol for concept erasure, using the CLIP score (CS) [33] to assess erasure efficacy and the Fr\u00e9chet inception distance (FID) [16] to assess prior preservation. CS calculates the cosine similarity between a textual prompt and the generated image [33]. We examine two CS values before and after erasing and compare the CS value after the erasure. When the prompts contain the target concepts, a more reduced CS indicates a more effective erasure of the target concepts. FID measures the distance between images generated before and after erasing [16]. A lower FID indicates a better alignment between the two images. Thus, for non-target concepts, lower FID values indicate better prior preservation. Overall, a precise concept erasure should have a low CS for prompts containing the target concepts and a low FID for prompts composed of non-target concepts."}, {"title": "4.2. On Instance Concept Erasure", "content": "We first experiment with erasing a single concept \"Snoopy\". Six types of prompts were tested, of which one contains \"Snoopy\" and the other five contain only non-target concepts. The results are compared in the top block of Table 1. It can be seen that our proposed AdaVD achieves the lowest CS and the lowest FID in all cases. Particularly, its FID is less than 33% of that of the second-best method. It improves over SOTA by significantly enhancing prior preservation without compromising the erasure precision, as visualized in Fig. 4. It can be observed from Fig. 4 that methods such as SPM, NP, and SLD fail to fully erase \"Snoopy\" which can be found from the ear and the shape characteristic of the generated image after erasure. MACE can successfully erase the \"Snoopy\" concept. But all the competing methods suffer from degraded image quality in non-target concept generation, e.g., \u201cSpongebob\u201d.\nWe then compare performance for multi-concept erasure, experimenting with erasing two concepts of \"Snoopy\" and \"Mickey\" together, and three concepts of \u201cSnoopy\u201d, \"Mickey\" and \"Spongebob\" together. Results are reported in the bottom two blocks of Table 1 and visualized in the second and third sections of Fig. 4. Similarly, our AdaVD achieves the lowest CS and FID scores across all cases. This"}, {"title": "4.3. On Art Style Erasure", "content": "We experiment with erasing specific art style, including \"Van Gogh\", \"Picasso\" and \"Monet\". Results are reported in Table 2, and visual comparisons are provided in Fig. 5. Our AdaVD exhibits superior prior preservation, and achieves the lowest or closed-to-lowest CS and FID scores, demonstrating strong prior preservation without sacrificing"}, {"title": "4.4. Further Analysis", "content": "Time Consumption: The computational cost of concept erasure primarily arises from three components, including (1) data preparation time required by training-based methods for preparing training data and by AdaVD for basis computation; (2) model fine-tuning time required by training-based methods; and (3) image generation time required by all methods. In Table 3, we compare the total time consumption of different methods, as well as their time spent on each component. The two training-free methods of SLD and AdaVD are significantly faster as no fine-tuning is needed. Our AdaVD costs slightly more time than SLD, i.e., 0.8 extra seconds per image due to its basis computation, and a total of 8 extra seconds for generating 10 images. But this mild increase yields a significant performance gain, succeeding in precise concept erasure.\nInterpreting Erased Components by Visualization: Our AdaVD generates images by replacing the original value vector $v$ with $v^{\\perp}$ via orthogonal complement operation. To empirically interpret the rationale behind our method, we visualize the erased component, $v - v^{\\perp}$. Fig. 6 presents three cases of erasing the target concepts \"Mickey\u201d, \u201cVan Gogh\" and \"Bruce Lee\u201d, where AdaVD successfully erases these target concepts while robustly preserving prior knowledge of non-target concepts. In the first row, as shown in the middle column of each block, the erased components consistently align with the corresponding target semantics when dealing with the target concepts. In the second row of non-target concepts, conversely, the erased components do not contain any informative pattern, indicating that they carry no meaningful semantics, and therefore exert minimal impact on the prior knowledge."}, {"title": "5. Conclusion and Future Work", "content": "We have presented AdaVD, a precise, fast, and low-cost method for erasing unwanted concepts. The novel idea of leveraging the classical linear algebraic orthogonal complement operation and an adaptive erasing shift design has successfully achieved a precise concept erasure. Extensive experiments have demonstrated both high erasure efficacy and"}, {"title": "B. Additional Single-concept Experiments", "content": "We experiment with erasing different celebrity concepts, including \"Bruce Lee\", \"Marilyn Monroe\", and \"Melania Trump\". Five types of prompts were tested, each containing a distinct concept from \"Bruce Lee\u201d, \u201cMarilyn Monroe\u201d, \u201cMelania Trump\u201d, \u201cAnne Hathaway\u201d and \u201cTom Cruise\u201d. As reported in Table 4, AdaVD consistently exhibits superior erasing efficacy with prior preservation. When erasing different celebrities, AdaVD achieves the lowest or near-lowest CS and FID values, particularly excelling in FID. Although SPM ranks the second in prior preservation based on its FID scores, it falls significantly behind in its overall prior preservation quality, as compared to AdaVD.\nFig. 8 illustrates and compares generated images of methods, where consistent superior performance of AdaVD can be observed. For the target concept \"Marilyn Monroe\", AdaVD, SPM, and MACE can all successfully remove the celebrity identity. But SPM is overly aggressive at erasing, obscuring the facial outlines. For non-target concepts, all the four competing methods have caused some quite strong deviations, altering the original images. This is particularly noticeable in the generated images from the prompt corresponding to \"Melania Trump\". For instance, MACE and SPM have introduced an additional arm in the left image, NP has altered the original pose, and SLD has caused a severe visual change in the mouth and eye areas. In contrast, AdaVD is able to successfully maintain all the non-target images with minimal visual changes."}, {"title": "B.1. On Celebrity Erasure", "content": ""}, {"title": "B.2. On NSFW Erasure", "content": "Unlike the erasure of specific instances, art styles, and celebrities, NSFW concept erasure is more challenging. One reason is that the NSFW concepts are often implicit and hidden within prompts that can be particularly rich in their semantics. Also, many NSFW concepts have synonyms, and it is important to remove both the target concept and its synonyms. For instance, when targeting at removing the \"nudity\" concept, it is essential to also remove the \"sexual\" concept. We experiment with erasing the \"nudity\" concept using the I2P benchmark. To examine how well the \"nudity\" concept is erased, we employ the NudeNet with a threshold of 0.3 to detect nudity in the generated images and analyze the total number of nude items and the overall nude images that are detected.\nResults are reported in Fig. 9, where, despite the chal-"}, {"title": "B.3. More Erasure Examples", "content": "We demonstrate additional examples for erasing single concepts from prompts that contain such concepts. The experimented concepts include the specific instances of \"Statue of Liberty\", \"BB8\u201d, \u201cC3PO\u201d, and \u201cGrumpy Cat", "Benicio Del Toro\", and the art style \\\"Cyberpunk\\\". Among these, \\\"BB8\\\" and \\\"C3PO\\\" are fictional characters, while \\\"Statue of Liberty": "nd \u201cGrumpy Cat\" represent realistic entities from daily life. Fig. 10 presents the generated image examples. It can be seen that our AdaVD consistently exhibits superior erasure efficacy across all these concepts, being robust in erasing diverse types of concepts."}, {"title": "C. On Transferability to Other T2I Models", "content": "The proposed AdaVD is a flexible concept erasure approach that can be transferred to other T2I diffusion models. In addition to SD v1.4, as experimented in the main paper, we conduct additional experiments to demonstrate its transferability and effectiveness by integrating it with a series of other T21 diffusion models."}, {"title": "C.1. AdaVD on SDXL v1.0", "content": "We integrate AdaVD with SDXL v1.0 [31] which has a different architecture from SD v1.4-v2.1. It employs two distinct text encoders to process textual prompts, and their outputs are concatenated and fed into the CA layers to interact"}, {"title": "E. More Materials on Multi-Concept Erasure", "content": ""}, {"title": "E.1. On Equation (5)", "content": "Working with the subspace constructed as the span of the vector set ${v_t^{h,j}}_{h=1}^n$, we obtain a set of orthonormal basis ${\\hat{o}_{t}^{h,j}}_{h=1}^n$ through the Gram-Schmidt orthogonalization.\nWhen the value vectors $v_t^{h,j}$ are linearly independent, each orthonormal basis can be expressed as a linear combination of these vectors such that\n$\\hat{o}_{t}^{h,j} = \\sum_{k=1}^n w_{hk} v_t^{k,j}$,\nwhere $w_{hk}$ are the combination weights. We have explained the linear independence assumption on ${v_t^{h,j}}_{h=1}^n$ in Section 3.3.2. Incorporating Eq. (7) into Eq. (3) but replacing only the second $v_i$, it results in the following revised calculation of the orthogonal complement:\n$v^j = v_i - \\sum_{h=1}^n  (\\hat{o}_{t}^{h,j})^T v_i (\\sum_{k=1}^n w_{hk} v_t^{k,j})$.\nThe importance of this revised equation lies in the fact that it computes a weighted sum of the value vectors when performing the erasing. This enables the application of the adaptive erasing shift mechanism based on the value vectors, for which we further revise the erasing operation as\n$v^j = v_i - \\sum_{h=1}^n \\delta (v_i, o_t^{h,j})  (\\hat{o}_{t}^{h,j})^T v_i (\\sum_{k=1}^n w_{hk} v_t^{k,j})$.\nStoring the combination weights in the matrix $W = [w_{hk}] \\in \\mathbb{R}^{n \\times n}$, it acts as a projection matrix transforming the two vector sets by\n$\\left[\\begin{array}{c} \\hat{o}_{t}^{1,1} \\\\ ... \\\\  \\hat{o}_{t}^{l,n}\\end{array}\\right] = \\left[\\begin{array}{c}  v_t^{1,1} \\\\ ... \\\\  v_t^{l,n}\\end{array}\\right] W$."}, {"title": "E.2. Alternative Orthonormal Basis Calculation", "content": "Purely for the interest of readers, we point out an alternative way to calculate the orthonormal basis. Constructing"}, {"title": "E.3. Additional Experiments and Analysis", "content": ""}, {"title": "E.3.1. On Erasing More Multi-concepts", "content": "We conduct additional experiments, investigating how our approach performs as the number of erased concepts increases, under a progressive setting. We evaluate our AdaVD by first erasing one concept \"Snoopy\" and gradually increasing the number of erased concepts to 15, 25, and 40. The details of the concepts to be erased for each case are listed in Table 5. We work with the base T2I model SD v1.4 and compare it with the existing approach SLD. The results are presented in Fig. 14, which extends Fig. 1.\nIt can be observed from the top erasure efficacy block of Fig. 14 that SLD gradually loses its precision when removing the target concepts. This is possible because SLD concatenates the target concepts into a prompt for guiding the generation process. When erasing too many concepts, the text encoder struggles to focus on each individual concept, resulting in diminished erasure efficacy. Additionally, some concepts may be truncated due to the token length limitation of the text encoder's tokenizer. Differently, AdaVD achieves consistently high performance in multi-concept erasure. It constructs a value subspace based on the orthogonal complement of all the target concepts, which ensures that no information regarding any individual concept is lost."}, {"title": "E.3.2. On Transferability to Other T2I Models", "content": "In this additional experiment, we integrate AdaVD with two other T2I diffusion models, including DreamShaper [6] and RealisticVision [7], assessing its multi-concept erasure performance. Two multi-concept erasure scenarios are experimented with: one is cross-application erasure as described in SPM [27], and the other is multi-instance erasure. Results of the cross-application erasure are presented in the top half of Fig. 15, demonstrating the generated images after erasing \u201cSnoopy\u201d, \u201cVan Gogh\u201d, and the two concepts together. Results of the multi-instance erasure are shown at the bottom of Fig. 15, demonstrating the generated images after erasing \u201cMouse\u201d, \u201cDog\u201d, and both concepts. Overall, AdaVD achieves a high erasure precision. It can be seen from Fig. 15 that, when aiming at a single concept erasure, other concepts specified in the prompt remain faithfully in the generated image; and when aiming at erasing multiple concepts, all the relevant visual content is also removed suc-"}, {"title": "F. Failure Case Study", "content": "Despite its success, there exist concepts that AdaVD struggles to erase. We present a few failure cases in Fig. 16. For instance, it is challenging for AdaVD to erase \u201cVan Gogh\u201d from a prompt like \"The Starry Night is an iconic example of a masterpiece created in Van Gogh style.\" The challenge is likely to stem from the presence of multiple tokens, e.g., \u201cStarry Night\u201d, that is highly coupled with the target concept. In this case, a small value of the scaling hyper-parameter s as used by the shift factor in Eq. (6) is insufficient to eliminate effectively the target semantics across all the relevant tokens. Nevertheless, this issue can be miti-"}, {"title": "G. Additional Experimental Details", "content": ""}, {"title": "G.1. On Implementation", "content": "To implement SD v1.4, the DPM-solver is chosen as the sampler, with a total of 30 sampling timesteps and a classifier-free guidance scale set of 7.5. Notably, we set the unconditional prompt to null text, as the negative prompt serves as a training-free method that can be directly compared with our AdaVD. To ensure a fair comparison, particularly for prior preservation, we use the same random seed (seed 0) across all methods to generate images under identical conditions. For the specific instance, art style,"}, {"title": "E.2. Alternative Orthonormal Basis Calculation", "content": "Purely for the interest of readers, we point out an alternative way to calculate the orthonormal basis. Constructing a matrix $V_i \\in R^{d \\times n}$ by using ${v_t^{h,j}}_{h=1}^n$ as its columns, following Equation (5.13.6) of the linear algebra textbook [29], the projection of $v_i$ onto $\\text{span}^{\\perp}({v_t^{h,j}}_{h=1}^n)$ can be directly computed from V by\n$v^{\\perp} = P_{\\text{span}^{\\perp}({v_t^{h,j}}_{h=1}^n)} v_i = (I_d - V_i (V_i^T V_i)^{-1} V_i^T) v_i$ $= (V_i - V_i (V_i^T V_i)^{-1} V_i^T)$.\nCompared to Eq. (3), Eq. (11) does not require the Gram-Schmidt orthogonalization, but the inverse calculation. Defining $P = V_i (V_i^T V_i)^{-1} V_i^T$, one potential way to enable token-wise adaptive erasing shift based on Eq. (11) is\n$v^{\\perp,i} = (I_d - \\text{Diag} [\\delta (v_i^i, v_t^{h,i})] P) v_i$,\nwhere $\\text{Diag} [\\delta (v_i^i, v_t^{h,i})]$ is a diagonal matrix with shift factors $\\delta [\\delta (v_i^i, v_t^{h,i})]$ as its diagonal elements. We leave the in-depth investigation of exploiting this operation in practice to our future work."}]}