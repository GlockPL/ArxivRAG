{"title": "PERFT: PARAMETER-EFFICIENT ROUTED FINE-TUNING FOR MIXTURE-OF-EXPERT MODEL", "authors": ["Yilun Liu", "Yunpu Ma", "Shuo Chen", "Zifeng Ding", "Bailan He", "Zhen Han", "Volker Tresp"], "abstract": "The Mixture-of-Experts (MoE) paradigm has emerged as a promising approach for scaling transformers with improved resource utilization. However, efficiently fine-tuning MoE models remains largely underexplored. Inspired by recent works on Parameter-Efficient Fine-Tuning (PEFT), we present a unified framework for integrating PEFT modules into the MoE mechanism. Our framework, aligned with the core principles and architecture of MoE, encompasses a comprehensive set of design dimensions including various functional and composition strategies. By combining design choices within our framework, we introduce Parameter-Efficient Routed Fine-Tuning (PERFT) as a flexible and scalable family of PEFT strategies tailored for MoE models\u00b9. Extensive experiments adapting OLMOE-1B-7B and Mixtral-8\u00d77B for various commonsense and arithmetic reasoning tasks demonstrate the effectiveness, scalability, and intriguing dynamics of PERFT. Additionally, we provide empirical findings for each specific design choice to facilitate better application of MoE and PEFT.", "sections": [{"title": "1 INTRODUCTION", "content": "As modern transformer Vaswani et al. (2017) models continue to scale up, Mixture-of-Experts (MoE) (Shazeer et al., 2017) has emerged in recent years as a promising solution to the trade-off between performance and cost, yielding notable results in a series of frontier models (Jiang et al., 2024; Reid et al., 2024; Dai et al., 2024; Qwen, 2024; Grok, 2024). Leveraging the sparsity inherent to transformer models, MoE significantly reduces the computational costs while maintaining model capacity, yet these advantages do not translate to efficient fine-tuning on downstream tasks. The full fine-tuning of MoE models remains prohibitively expensive due to their immense number of expert parameters. Besides, the routing mechanism among sparsely-activated experts poses unique challenges previously unseen in dense models (Wang et al., 2024). These challenges necessitate exploring specially designed Parameter-Efficient Fine-Tuning (PEFT) techniques for adapting sparse MoE models without incurring the full cost of fine-tuning all parameters.\nPEFT solutions, such as adapters (Houlsby et al., 2019) and LoRA (low-rank adaptation; Hu et al., 2022), have gained considerable attention on dense models. Hybrid approaches combining elements from different PEFT methods have also shown promising results (He et al., 2022; Hu et al., 2023; Zhang et al., 2023). With the rise of MoE architectures, recent studies have explored PEFT solutions incorporating MoE-inspired structures for dense models (Zadouri et al., 2023; Dou et al., 2023; Luo et al., 2024; Li et al., 2024; Gao et al., 2024; Wu et al., 2024). However, designing PEFT strategies specifically tailored for MoE models remains largely underexplored.\nTo this end, we present a unified framework focused on incorporating diverse PEFT modules directly into the MoE mechanism. Different from previous PEFT solutions that operate in isolation from the underlying MoE architecture, our framework focuses on the core principles and unique challenges of MoE architecture. We introduce two key design dimensions. Functional strategies define the internal mechanisms of the introduced PEFT module, including the architecture inside individual PEFT modules, the multiplicity of PEFT modules, and the routing mechanism among them. Compositional strategies describe how PEFT modules interact with the original MoE architecture,"}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 MIXTURE-OF-EXPERTS IN TRANSFORMER MODEL", "content": "Transformer Model. Consider a transformer model comprising L layers of transformer blocks, each incorporating a standard self-attention mechanism and a feed-forward neural network (FFN). Given a sequence of T tokens with an initial embedding in a D-dimensional hidden space x 1:T \u2208 RT\u00d7D, we formulate the inner mechanism of each transformer block\u00b2 at layer l \u2208 {1,\u2026\u2026, L} as:\nh:T = SelfAttni (x) +x-1,  x = FFN\u0131 (h) + h,\n2Layer normalization and dropout operations are omitted in this paper for clarity."}, {"title": "2.2 PARAMETER-EFFICIENT FINE-TUNING FOR TRANSFORMER-BASED MODEL", "content": "Vanilla PEFT. Classical full fine-tuning approaches for downstream tasks (Devlin et al., 2019; Qiu et al., 2020) have become increasingly impractical as transformers continue scaling up. Recent work has introduced diverse PEFT methods offering comparable performance to full fine-tuning with significantly reduced computational demands. He et al. (2022) present a unified view for PEFT, where any PEFT method can be viewed as a combination of several design dimensions. For instance, given the adapted module's input h and output \u00e6, LoRA (Hu et al., 2022), which approximates weight updates using low-rank matrices, can be described as a parallel operation \u25b3(h) = hWdown Wup and x + x + s \u2022 \u2206(h). This framework facilitates hybrid design for better PEFT variants. They find that parallel PEFT modules generally outperform sequential adaptations, and modifying FFN yields better results than modifying attention, which are further supported by Hu et al. (2023), Zhang et al. (2023), Dettmers et al. (2024) and Hao et al. (2024).\nPEFT with MoE-like Structures. The success of MoE transformers has inspired MoE-structured adaptations. Much recent work has focused on developing such modules for dense models, including inserting multiple LoRA experts with routers at attention layers (Liu et al., 2023a; Luo et al., 2024) and alongside dense FFN layer (Zadouri et al., 2023; Dou et al., 2023; Page-Caccia et al., 2024; Chen et al., 2024; Hao et al., 2024). Gao et al. (2024) find that allocating more LoRA experts to higher layers leads to better performance. Li et al. (2024) propose up-cycled a mixture of LoRA-adapted frozen FFN experts from dense models. Wu et al. (2024) explore methods for composing multiple trained LoRAs in a MoE style. Notably, all these methods primarily focus on adapting dense models, leaving the application of PEFT to inherently sparse MoE models largely underexplored. Recently Wang et al. (2024) propose an expert-specialized fine-tuning approach, which comes closest to this research gap by selectively fine-tuning the most relevant experts for downstream tasks, though no PEFT techniques are involved. Our work, in contrast, directly addresses this area by introducing PEFT modules into the MoE mechanism, which offers a more flexible and efficient solution for adapting MoE models while preserving their original weights untouched."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 THE UNIFIED FRAMEWORK", "content": "This section introduces our unified framework for PEFT on MoE models. Inspired by the unified view of PEFT (He et al., 2022), our framework focuses on two key design dimensions, as shown in Figure 2. Functional strategies define the internal mechanism of the introduced PEFT module, including the architecture inside individual PEFT modules, the multiplicity of PEFT modules, and the routing mechanisms among them. Compositional strategies describe how PEFT modules interact with the original MoE architecture, including operating as shared PEFT experts or embedded PEFT experts. By considering these aspects, our framework addresses the unique characteristics of both PEFT and MoE mechanisms, providing a novel and comprehensive perspective on adapting MoE models."}, {"title": "3.1.1 FUNCTIONAL STRATEGY", "content": "This dimension describes the internal implementation of the introduced PEFT module. We consider variations of mechanisms in three dimensions:\nArchitecture inside PEFT Experts. This aspect defines the specific internal structure of each individual PEFT expert. The general architecture for computing (h) in each PEFT expert can be formalized as\n\u25b3(h) = UpProj(Act(DownProj(h))), (3)\nwhere Act (\u00b7) is implemented with non-linear activation functions, or with an identity function for LoRA. The DownProj(\u00b7) : RD \u2192 RB and UpProj(\u00b7) : RB \u2192 RB introduce a key scaling factor, the bottleneck size DB, known as rank r used in LoRA's low-rank decomposition. Adjusting DB leads to linear scaling of trainable parameters. Optimizing this hyperparameter is crucial for different tasks and models, as it balances the bottleneck subspaces' capacity for additional knowledge against the effectiveness of training newly introduced weights with given data (Hu et al., 2022).\nMultiplicity of PEFT Experts. The number of PEFT experts serves as another key scaling factor in our framework. Increasing the number of PEFT experts allows each to generate its own copy of \u25b3(h), denoted as \u2206\u1d62(h). Previous studies on fine-tuning dense models with MoE-like structures (Zadouri et al., 2023; Liu et al., 2023a; Dou et al., 2023; Li et al., 2024) have empirically shown that optimizing the number of adapters can significantly impact performance. This optimization can be tailored to specific tasks, models, or even individual layers within a model (Gao et al., 2024). We investigate the balance between performance and effective utilization of experts in our experiments.\nRouting among PEFT Experts. This aspect considers whether an independent routing mechanism is introduced among PEFT experts. In contrast to previous work primarily focusing on adapting dense models using PEFT modules with MoE-like structures (Hao et al., 2024; Gao et al., 2024; Wu et al., 2024), our framework reveals the potential dynamics in the interaction between routed PEFT experts and the pretrained MoE module. For a token-wise routing among M PEFT experts, the PEFT module operates similarly to the original MoE module for FFN experts (Equation 2):\n\u25b3(h) =((h); \u2081(h\u00b2)), (4)\nwhere G(\u00b7) denotes the gating function for the PEFT experts. This aspect highlights the profound dynamics between routers and experts in MoE and PEFT modules, as shown in Figure 3. Based on the key-value memory perspective for FFN (Geva et al., 2021) (Figure 3a), we can similarly interpret the weight matrix W, \u2208 RD \u00d7 RN in a router for N FFN experts as a set of N individual vectors {gi}, each representing a characteristic hidden state for the corresponding expert's key memories. More specifically, each of the N vectors approximately symbolizes a cluster of all individual neuron vectors within each FFN expert, and the routing process can be interpreted as a projection of the current hidden state onto these N vectors to calculate the affinity of each expert with the input token. For our PEFT expert router G(\u00b7), we can either learn from scratch a new collection of PEFT"}, {"title": "3.1.2 COMPOSITIONAL STRATEGY", "content": "The compositional strategy defines how the PEFT module integrates with the original MoE model. Based on findings from previous research (He et al., 2022; Hu et al., 2023; Luo et al., 2024; Hao et al., 2024) that inserting PEFT modules in parallel generally yields superior performance, we focus exclusively on parallel insertion methods, i.e., PEFT receiving the same input as the module it is adapting and combining its output with that of the same module. This consideration aligns with the parallel nature of MoE architectures, where FFN experts operate concurrently rather than in a stacked configuration. Here we identify three main categories of insertion strategies:\nShared PEFT Experts. The PEFT module can operate in parallel with the entire MoE module, functioning as shared PEFT experts. Given a input hidden state sequence h1:T, we have:\n(h1:T) + (h1:T) + h1:T, (5)\nwhere the PEFT module takes the same input h1:T as the MoE module, and combines its output additively with the MoE output to the residual connection. This approach draws inspiration from the concept of shared FFN experts in recent works (Gou et al., 2023; Dai et al., 2024; Qwen, 2024). Introducing these shared structurally identical FFN experts alongside routed FFN experts during training MoE models aims to improve parameter efficiency by mitigating the redundancy of shared knowledge across routed experts. Applying this principle to lightweight PEFT modules, we hypothesize that these shared PEFT experts can similarly capture and adapt the common parts needed among routed FFN experts, thereby potentially offering greater efficiency as well.\nEmbedded PEFT Experts. In this configuration, the PEFT modules are embedded within the MoE module. Each PEFT module is paired with a corresponding FFN expert and operates in a tight coupling manner, receiving the same token-wise input ht as distributed by the MoE router:\nxt = G(h) (E\u2081(ht) + \u25b3\u2081(h)) +ht, (6)\nwhere E\u2082(ht) is the output of the i-th FFN expert for token t, and (ht) is the output for token t of the i-th PEFT module that is associated with the i-th expert. The PEFT modules' outputs are combined with their corresponding FFN experts' outputs before being weighted by the router and summed. This formulation can be viewed as introducing N PEFT experts embedded within the MoE module, mirroring the activation patterns of the original FFN experts as discussed in Section 3.1.1.\nMoE-Agnostic PEFT. The PEFT module is integrated at locations independent of the MoE modules, completely decoupled and functioning agnostically to the MoE mechanism. This includes"}, {"title": "3.2 THE PERFT FAMILY", "content": "Deriving from our unified framework of PEFT on MoE models, we hereby propose Parameter Efficient Routed Fine-Tuning (PERFT) as a family of novel PEFT methods tailored for MoE models, as illustrated in Figure 1. At the core of the PERFT family is PERFT-R (Routed), with a parallel module consisting of an independent router among the introduced PEFT experts:\n1:T = 1 (G (hl:T); Ei (1:1))+((1:1); ; (h1:T)) + hl:T, (7)\nwhere \u011e(\u00b7) : RD \u2192 RM denotes the gating function for the M PEFT experts \u2206;(\u00b7). PERFT-R allows for learning an independent series of expert vectors \u011fi for PEFT experts, together with FFN expert vectors gi forming an intriguing dynamics, as discussed in Section 3.1.1 and Figure 3c.\nIf the number of introduced PEFT experts M matches the number of FFN experts N in the original MoE module, the structural design in PERFT-R provides a possibility to substitute G(\u00b7) with the original G(\u00b7), which makes it becomes a simplified special case\nx1 = 1 (G (hl:T); Ei (h:T)) + 1 (G (1:1); \u2206j (h1:T)) + hl:T\n= G (h1:T); (Ei (h1:T) + \u0394; (h1:T)) + h1:T (8)\nwhich takes exactly the same form as the embedded PEFT experts in Equation 6. Hence we denote this variant as PERFT-E (Embedded). As it directly utilizes the expert vectors gi original pre-trained router for distributing tokens for PEFT experts instead of learning weights from scratch, it can be intuitively estimated that this property of would lead to performance gain especially when the number of routed experts are to some extent that learning from scratch is not able to capture enough quality distribution of PEFT expert vectors in the space of hidden states.\nBy removing routing functions and naively making multiple PEFT shared experts always activated in parallel with the MoE module, we have another variant PERFT-D (Dense), denoted as\nx1:T = ((h); E (1:1)) + \u03a3\u2081\u2081 (1:1) + 1:T', (9)\nwhich can be further simplified into only having one shared PEFT expert, namely PERFT-S (Single)\n1:T = (G (h1:T); Ei (h1:T)) + \u2206o (h1:T) + h1:T, (10)\nThese two structures implemented the idea of shared experts introduced in recent works (Dai et al., 2024; Qwen, 2024) with PEFT experts, serve as two simpler variants in our PERFT family."}, {"title": "4 EXPERIMENTS AND ANALYSES", "content": ""}, {"title": "4.1 EXPERIMENT SETUP", "content": "Benchmarks. Our experiments follow the settings provided by Hu et al. (2023), encompassing 8 benchmarks for commonsense reasoning and 6 for arithmetic reasoning. We utilize their amalgamated training sets Commonsense170K and Math50K to fine-tune models respectively for each domain. Evaluations are conducted correspondingly across all individual benchmark test sets.\nLLM Backbones. Two state-of-the-art open-source MoE LLMs serve as the backbone models for our experiment: OLMOE-1B-7B (Muennighoff et al., 2024) and Mixtral-8\u00d77B (Jiang et al., 2024). They are selected among publicly available MoE models based on their outstanding performance in the 1B and 13B activated parameter ranges. We use the model weights of their pretrained versions.\nBaselines. Since there is little previous work on applying PEFT to MoE, we primarily experiment with applying LoRA to attention matrices Wa and Wu, the versatile and popular PEFT solution that provides optimal performance under limited parameter budgets (Hu et al., 2022). This serves as our baseline across all scales and tasks. For the smaller OLMOE-1B-7B model, we also include results of applying LoRA to the router matrix Wg, as reported in Table 4 in appendix.\nTraining. In our experiments, we maintain consistency with the original training process of each LLM by incorporating their respective auxiliary losses alongside the cross-entropy loss for token"}, {"title": "4.2 EXPERIMENT RESULTS", "content": "Table 1 presents a comparison between several representative PERFT variants and MoE-agnostic baseline with equivalent levels of trainable parameters. The reported PERFT variants consistently outperform baseline methods, with PERFT-R achieving improvements of up to 17.2% and 12.3% on each domain, and PERFT-E up to 10.4% and 5.4%. Section C in appendix provides a comprehensive series of tables detailing the performance of all variants across each individual task.\nTo obtain the optimal configurations, we conduct an exhaustive series of experiments by fine-tuning OLMOE using combinations of each PERFT variant and possible design choices, with results presented in Figure 4."}, {"title": "4.3 RESULT ANALYSES", "content": "Architecture inside PEFT Experts. Our experiments reveal fascinating dynamics of PERFT as we manipulate the bottleneck size. As Figure 4 suggests, the optimal information bottleneck configuration represents a delicate balance between capacity and learning effectiveness for each PERFT variant and the given task to achieve peak performance. For PERFT-S and PERFT-D variants without G(.) to distribute gating weights, increasing the bottleneck leads to rapidly decreased average performance across both commonsense and arithmetic reasoning tasks compared to baseline and other PERFT variants. This phenomenon should be attributed to inefficient parameter utilization in always-activated shared experts. Without an effective routing mechanism, a mismatch would occur between the effective dimensionality of the task and adapter capacity. When the adapter's\ndimensions significantly exceed the intrinsic dimensionality required by the task for applying modifications, the surplus dimensions in the PEFT module may introduce useless or harmful adaptations, leading to decreased performance as the bottleneck size increases. A detailed discussion on possible reasons is presented in Appendix B.2.\nOther than LoRA, we also examine alternative architectures for our PEFT experts, such as parallel adapter (Houlsby et al., 2019; He et al., 2022), which includes an additional activation function applied to the bottleneck between projections. Detailed results and analyses are in Appendix B.1.\nMultiplicity of PEFT Experts for PERFT-D Our observations reveal that naively scaling up the number of experts without a routing mechanism can lead to severe performance degradation. Consistently, PERFT-D underperforms PERFT-S, with performance declining as the number of PERFT experts increases. Figure 6 visualizes this effect through UMAP projections of key memory vectors and expert vectors for the base OLMOE-1B-7B model and different PERFT variants (E, R, D, and S). As the UMAP projection maintains relative distances between original FFN experts in the final results, in an ideal adaptation scenario, PEFT expert key vectors that may activate simultaneously should be distributed evenly within subspaces formed by task-relevant FFN experts' key vectors, maximizing hidden space utilization. However, PERFT-D variants in Figure 6 exhibit tightly clustered key vectors from different experts (shown with different colors), indicating a functional redundancy and inefficient use of model capacity in PERFT-D experts. A detailed analysis on this phenomenon is provided in Appendix 4.3.\nRouting among PEFT Experts for PERFT-R Comparing to PERFT-S and PERFT-D in Figure 4, we observe that even when all experts are activated (TopN/N), PERFT-R can still improve the performance significantly, by simply introducing learnable token-wise gating weights for dynamically assigning the importance of each expert's output. This effect is reminiscent of how Gated Linear Units (GLU) improve the FFN layer in transformers (Dauphin et al., 2017). In our case, Figure 6 shows that gating weights can lead to more balanced vector distribution and more effective utilization of hidden space, supporting our discussion in Section 3.1.1. Without such a mechanism, the potential benefits of the increased number of experts may be counterbalanced by the redundancy in model capacity, as discussed in Appendix 4.3.\nFigure 5 reveals that for a fixed total number of PEFT experts, increasing the sparsity of PERFT-R by activating fewer PEFT experts does not severely degrade performance. This observation is also supported by the visual representation in Figure 6, which suggests that an adequate number of activated expert vectors is sufficient to capture the distribution of the space to be adapted. In addition, the key value vectors from different PEFT experts of PERFT-R that appear clustered in Figure 6 can be utilized by a sparser router to ensure them not activated simultaneously, thereby maintaining performance. This finding indicates that the overall capacity of the PEFT module may be a more critical factor in determining performance rather than the activated capacity.\nPlacement of Adaptation Modules for PERFT-E Figure 6 illustrates the distinct dynamics between PERFT-E and PERFT-R. PERFT-E utilizes the frozen expert vectors in the router for FFN experts, while PERFT-R learns an independent router from scratch for PEFT experts. It's important to note that the comparative performance between PERFT-E and PERFT-R can vary in practice, especially when considering scenarios with different activated parameters. Our results in Figure 4a demonstrate that given the same total number of PEFT experts, PERFT-E consistently performs better than PERFT-R (Top8/64) across all bottleneck sizes; while many PERFT-R configurations with fewer experts in turn outperform PERFT-E. When a larger number of PEFT experts are used,"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce a unified framework for integrating PEFT techniques into MoE models, addressing the challenges of efficiently adapting these large, sparse architectures to downstream tasks. Our framework, encompassing both functional and compositional strategies, bridges the gap between existing PEFT methods for dense models and the unique sparsity characteristics of MoE architectures. Building upon this framework, we propose PERFT, a flexible family of PEFT strategies specifically tailored for MoE modules. Through extensive experiments on adapting several state-of-the-art MoE models (OLMOE and Mixtral) for various commonsense and arithmetic reasoning tasks, we demonstrated the effectiveness and scalability of PERFT. Our results showed significant performance improvements over MoE-agnostic baseline methods. We provide an analysis of our findings for each specific design choice from our study, contributing to a deeper understanding of the dynamics between PEFT adaptation strategies and the MoE architecture."}, {"title": "A TRAINING CONFIGURATIONS", "content": "Hardware. For each fine-tuning experiment with the baseline and PERFT variant, we trained OLMOE-1B-7B on a single NVIDIA A100 GPU, and Mixtral-8\u00d77B on 4\u00d7NVIDIA H100 GPUs using NV-link interconnect across GPUs. Both models are evaluated on NVIDIA A100 GPUs.\nHyperparameters. We display the hyperparameter configurations used in fine-tuning and evaluating OLMOE-1B-7B and Mixtral-8\u00d77B in Table 2. We follow Hu et al. (2023) and each model's original settings for training."}, {"title": "B BOTTLENECK SIZE OF PEFT EXPERTS", "content": "We provide a detailed empirical analysis about the inefficient parameter utilization when always-activated shared experts are employed without an effective routing mechanism. This symbolizes a mismatch between effective dimensionality and adapter capacity: if the adapter's dimensions significantly exceed the task's intrinsic dimensionality, surplus dimensions may introduce useless or harmful adaptations. Larger random-initialized bottlenecks in PERFT-S and PERFT-D can introduce unnecessary noise in the additional adapted spaces due to insufficient information, interfering with useful representations in the original pretrained model. With the perspective viewing hidden states on the residual stream as bandwidths for modules to communicate on (Elhage et al., 2021), in our PEFT scenario where most parameters remain unchanged, only a relatively small subspace of each layer's hidden state requires task-specific adaptation. Any over-parameterized adaptation can unnecessarily disrupt normal functioning on the residual stream's bandwidths, potentially destabilizing the original gradient flow in the transformer and leading to unstable training or sub-optimal solutions (Aghajanyan et al., 2021). Simultaneously, in the PEFT context with limited adaptation information compared to model pretraining, an excessively large parameter space without gating control can easily result in over-fitting on fine-tuning data, which is exacerbated by the sparse nature of the MoE module we are adapting. As the MoE module hosts multiple different patterns on various combinations of activated FFN experts that dynamically interact with each other on the residual stream, the always-activated PERFT-S and PERFT-D variants may learn unnecessary adaptations during the training process, further aggravating the disrupted functionality and over-fitting problems.\nIt is also worth noting that since FFN tends to learn task-specific textual patterns (Geva et al., 2021) and attention learns more about positional interactions (Elhage et al., 2021), the nature of different components to which PEFT is introduced also contributes to different phenomena. For the baseline LORA operating on attention matrices, individual attention heads are already operating on relatively smaller subspaces and can easily write outputs to disjoint subspaces without interaction. The spaces they read and write are relatively more fixed due to the low rank property (Dhead  Db is the chi-square distribution with DB degrees of freedom. As training progresses, vectors may converge. We can define a factor YT that represents the increased likelihood of vectors being within e distance after T training steps:\n\u0440\u0442(\u20ac) = \u03b3\u03c4\u00b7\u0440\u043e(\u20ac) (12)\nThe expected number of effective vectors after T training steps can be approximated as:\nE[Neff(T)] \u2248 MDB(1 \u2013 e-MD\u0432\u0443\u0442\u0440\u043e(\u0454)\u00b2) (13)"}, {"title": "C ADDITIONAL RESULTS", "content": ""}, {"title": "C.1 OLMOE-1B-7B FOR COMMONSENSE REASONING", "content": ""}]}