{"title": "Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera", "authors": ["Yuliang Guo", "Sparsh Garg", "S. Mahdi H. Miangoleh", "Xinyu Huang", "Liu Ren"], "abstract": "While recent depth foundation models exhibit strong zero-shot generalization, achieving accurate metric depth across diverse camera types-particularly those with large fields of view (FoV) such as fisheye and 360-degree cameras-remains a significant challenge. This paper presents Depth Any Camera (DAC), a powerful zero-shot metric depth estimation framework that extends a perspective-trained model to effectively handle cameras with varying FoVs. The framework is designed to ensure that all existing 3D data can be leveraged, regardless of the specific camera types used in new applications. Remarkably, DAC is trained exclusively on perspective images but generalizes seamlessly to fisheye and 360-degree cameras without the need for specialized training data. DAC employs Equi-Rectangular Projection (ERP) as a unified image representation, enabling consistent processing of images with diverse FoVs. Its key components include a pitch-aware Image-to-ERP conversion for efficient online augmentation in ERP space, a FoV alignment operation to support effective training across a wide range of FoVs, and multi-resolution data augmentation to address resolution disparities between training and testing. DAC achieves state-of-the-art zero-shot metric depth estimation, improving delta-1 accuracy by up to 50% on multiple fisheye and 360-degree datasets compared to prior metric depth foundation models, demonstrating robust generalization across camera types.", "sections": [{"title": "1. Introduction", "content": "Depth estimation from monocular cameras is a foundational challenge for applications like autonomous driving, AR/VR, and robotics. While early deep learning methods relied on supervised training using single datasets and depth sensor [22] supervision [3, 23, 33], monocular depth estimation remains challenging due to scale-depth ambiguity. Expanding training datasets has been a key strategy to enhance robustness, with self-supervised approaches using sequential frames [13, 42, 44]. However, these methods often underperform due to self-supervision ambiguity, view inconsistencies and dynamic objects. Recent methods, such as MiDaS [34], leverage large-scale datasets with 3D supervision, normalizing scale differences across datasets to enable zero-shot testing. However, they primarily provide relative depth rather than metric depth.\nRecent methods tackle zero-shot metric depth estimation by addressing the challenges of inconsistent scaling factors in depth ground truth caused by varying camera intrinsic parameters. Several works demonstrate impressive generalization capabilities on novel images [4, 19, 30, 49, 53], establishing themselves as foundational depth models for downstream tasks. However, these approaches often struggle with large field-of-view (FoV) cameras like fisheye and 360\u00b0 cameras, where performance significantly declines compared to standard perspective cameras.\nAs illustrated in Fig. 2, large FoV images can be represented in multiple formats, but generating the best-performing undistorted image for perspective-based depth models often leads to substantial FoV loss. Despite these limitations, large FoV inputs are crucial for efficiency-critical downstream applications such as large-scale detection [31, 48], segmentation [51, 54], SLAM [11, 40, 41, 58], interactive 3D scene generation [55], and robotic demonstration capturing [6, 45].\nAchieving zero-shot depth generalization across any FoV camera presents several challenges: (1) selecting a unified camera model to represent diverse FoVs, (2) effectively leveraging perspective training datasets to generalize to data spaces observable only from large FoV cameras, (3) managing drastically different training image sizes in the unified space caused by varying FoVs, and (4) handling resolution inconsistencies between training and testing phases.\nIn this paper, we present Depth Any Camera (DAC), a novel zero-shot metric depth estimation framework that enables a depth model trained exclusively on perspective images to generalize across cameras with widely varying FoVs, including fisheye and 360\u00b0 cameras (see Fig. 1). DAC employs Equi-Rectangular Projection (ERP) as a canonical representation to unify images from diverse FoVs into a shared space. Key innovations include an efficient pitch-aware Image-to-ERP conversion based on grid sampling and Gnomonic Geometry [43], enabling seamless ERP-space data augmentations. Specifically, pitch-aware ERP conversion and pitch-angle augmentation allow perspective data to be projected into high-distortion regions of the ERP space, which are only observable from large-FoV cameras. This enhances the framework's zero-shot generalization capability, enabling it to extrapolate effectively from the perspective domain to the broader camera domain. To ensure effective learning, we propose a FoV alignment process to adjust diverse-FoV training samples to a predefined ERP patch size, preserving content while minimizing computational overhead. Additionally, multi-resolution augmentation addresses resolution mismatches, allowing the model to learn scale-equivariant features and adapt to a flexible range of testing resolutions.\nIn summary, our contributions are as follows:\n\u2022 We propose a novel zero-shot metric depth estimation framework capable of handling images from any camera type, including fisheye and 360\u00b0 images, using a model trained exclusively on perspective data.\n\u2022 We introduce an efficient perspective-to-ERP patch conversion to enable pitch-aware ERP conversion and ERP-space online augmentation.\n\u2022 We propose a FoV alignment process, facilitating effective training across cameras with diverse FoVs within the unified ERP space and a multi-resolution training strategy to address resolution mismatches between training ERP patches and testing images.\n\u2022 Our method achieves state-of-the-art zero-shot performance on all large FoV testing datasets, delivering up to a 50% improvement in d\u2081 accuracy on multiple indoor fisheye and 360\u00b0 datasets, showcasing robust generalization across diverse camera types."}, {"title": "2. Related Works", "content": null}, {"title": "2.1. Zero-Shot Monocular Depth Estimation", "content": "Recent approaches to zero-shot metric depth estimation tackle the challenge of inconsistent scaling factors in depth ground truth due to varying camera intrinsic parameters [4, 15, 19, 30, 49, 50, 53]. Zoedepth [4] introduces an advanced network architecture, while DepthAnything [49, 50] employs a sophisticated unsupervised learning paradigm. However, their performance in metric depth estimation is limited without tackling inconsistency camera intrinsics. Metric3D [19, 53] and UniDepth [30] address scaling inconsistencies by converting images into a canonical camera space. Metric3D uses intrinsic parameters for this preprocessing, whereas UniDepth incorporates a network branch to estimate and convert intrinsics on the fly. Despite these advances, none of these methods achieve satisfactory zero-shot performance on large FoV images, presenting unique challenges in unifying diverse FoVs and supporting effective model learning."}, {"title": "2.2. Depth Estimation from Large FoV Cameras", "content": "Depth estimation for fisheye, 360\u00b0 cameras has grown in popularity, as large FoVs capture richer contextual information that enhances depth estimation [1, 20, 24, 37, 56]. A key challenge for these cameras is managing position-dependent distortions, which vary by camera models. Solutions include deformable CNNs [38, 46, 59], which adapt kernel shapes to handle distortions, and UniFuse [20], which segments ERP images to minimize distortion before merging. More recent methods leverage transformers to handle these distortions [10, 24, 37, 56]. While transformer-based networks have improved in-domain performance, they are approaching saturation, indicating that distortion is not the only challenge. Instead, the lack of large-scale FoV-specific training data is a key bottleneck for generalization. No current methods enable an unified depth estimation model trained on mixed large-scale perspective datasets to achieve zero-shot generalization on ERP or fisheye images."}, {"title": "3. Notations and Preliminaries", "content": "Depth Scaling Operations. Monocular depth estimation is inherently ill-posed, as different 3D object sizes and depths can produce the same 2D appearance. Deep learning models rely on learning an object's 3D dimensions from its 2D appearance [15, 16, 39] to infer depth, leading to the scaling operation illustrated in the right panel of Fig. 3. When using mixed camera data, apparent object size also depends on focal length, making accurate 2D-to-depth mapping dependent on appropriately scaling ground-truth depths when converting a perspective model to a canonical model, as shown in the left panel of Fig. 3. These scaling operations are central to the Metric3D [19, 53] pipeline and are integrated into our ERP-based approach.\nEquiRectangular Projection (ERP). Equi-Rectangular Projection (ERP) is an image representation based on a spherical camera model, where each pixel corresponds to a specific latitude \u03bb and longitude \u03c6. A full ERP space spans 180\u00b0 in latitude and 360\u00b0 in longitude, making it ideal for handling diverse FoV cameras. The ERP image height is the only parameter needed to define the ERP space, allowing both training and testing images to be consistently converted into this space, regardless of the original FoV.\nTransformations between standard images and ERP images use Gnomonic Projection transformation [43], which offers a closed-form mapping between normalized image coordinates $(x_t, y_t)$ and spherical coordinates $(\\lambda, \\phi)$, assuming a tangent plane centered at $(\\lambda_c, \\phi_c)$ of a unit sphere. Specifically, as shown in Fig. 5, this mapping is given by:\n$x_t = \\frac{\\cos{\\phi} \\sin{(\\lambda - \\lambda_c)}}{\\cos{c}}$\n$y_t = \\frac{\\cos{\\phi_c} \\sin{\\phi} - \\sin{\\phi_c} \\cos{\\phi} \\cos{(\\lambda - \\lambda_c)}}{\\cos{c}}$\nwhere c is the angular distance of the point $(x_t, y_t)$ from the center of the projection, given by:\n$\\cos c = \\sin{\\phi_c} \\sin{\\phi} + \\cos{\\phi_c} \\cos{\\phi} \\cos{(\\lambda - \\lambda_c)}$\nWe use these transformations to enable an efficient ERP conversion and data augmentation process, creating a streamlined pipeline that supports zero-shot generalization for depth estimation across various FoV cameras."}, {"title": "4. Depth Any Camera", "content": "We propose Depth Any Camera (DAC), a depth model training framework designed to achieve zero-shot generalization across diverse camera models, including perspective, fisheye, and 360\u00b0 cameras. As illustrated in Fig. 4, images from different camera types and FoVs are transformed into a canonical ERP space during both the training and testing phases. For training, we leverage the extensive perspective image datasets by converting them into smaller ERP patches for efficient learning. During testing, large FoV images are similarly converted into the canonical ERP space, allowing the trained model to predict metric depth consistently, without getting confused by different camera intrinsic and distortion parameters.\nSeveral key components are designed to address specific challenges in implementing the DAC framework. In Sec. 4.1, we present an efficient pitch-aware Image-to-ERP conversion method that simulate large-FoV images at patch level and supports online augmentation within the ERP space. Sec. 4.2 introduces a FoV alignment process, an effective data augmentation technique that maximizes content inclusion while minimizing computational waste on background areas, using a single predefined ERP patch size. In Sec. 4.3, we describe a multi-resolution data augmentation approach aimed at training a transformer-based network capable of handling a broad range of testing resolutions."}, {"title": "4.1. Pitch-Aware Image-to-ERP Conversion", "content": "We propose an efficient approach to directly convert an input image to its corresponding ERP patch through grid sampling combined with gnomonic projection. Assuming an ERP space with height $H_E$, width $W_E = 2H_E$, and the image center at latitude $\\lambda_c$, longitude $\\phi_c$, with a target ERP patch size of $H_E \\times W_E$, the ERP patch coordinates $(u_e, v_e)$ can be mapped to spherical coordinates as follows:\n$\\Phi = \\frac{2\\pi W_e}{W_E}(u_e - \\frac{W_E}{2}) + \\Phi_c$\n$\\Lambda = \\frac{\\pi H_e}{H_E}(v_e - \\frac{H_e}{2}) + \\Lambda_c$\nUsing Gnomonic Geometry as presented in Eq. 1 and Eq. 2, we obtain the corresponding normalized image coordinate $(x_t, y_t, 1)$ in the tangent plane touching the unit sphere. To map this coordinate to the actual image coordinate $(u, v)$, we apply distortion and projection functions based on the given camera parameters:\n$(x_d, y_d) = f_d(x, y, D_c)$\n$(u, v) = f_p(x_d, y_d, K_c)$\nwhere $f_d$ is the distortion function with distortion parameters $D_c$, and $f_p$ is the projection function with intrinsic parameters $K_c$. If the input image has no distortion, we can directly apply the projection function to $(x_t, y_t)$.\nAs shown in Fig. 5, with uniformly sampled grid points within the target ERP patch, each grid point can be mapped directly to a corresponding location in the input image. This mapping facilitates efficient transformation of the captured image into an ERP patch via grid sampling. Essentially, each grid point in the ERP patch maps to a specific floating-point position in the input image, and its value is obtained by interpolating from the neighboring pixel values.\nOne key advantage of this ERP conversion lies in its camera pitch awareness, corresponding to the latitude of the tangent plane center, $\\Lambda_e$, as defined in Eq. 1 and Eq. 2. When camera orientation is available or can be estimated [21], perspective data can be projected to various latitudes in the ERP space, enabling the simulation of regions visible only with large field-of-view (FoV) cameras, as shown in Fig. 5, and Supplemental Fig. 7. This pitch-aware conversion is crucial for improving the generalization of trained models to previously unobserved large-FoV data, as demonstrated in Sec 5.3, because neural networks alone have limited capacity to generalize to extrapolated data spaces [47]. For datasets with limited pitch variation [12, 14, 18, 57], pitch augmentation can be efficiently achieved by introducing noise to $\\Lambda_e$, generating ERP patches with varying shapes, as shown in Fig. 4 and Fig. 5.\nAnother notable advantage is the seamless ability to perform online data augmentation efficiently in the ERP space. Common augmentations, such as scaling, rotation, and translation-commonly applied to perspective images can be directly applied to the normalized image coordinates $(x_t, y_t)$ as follows:\n$\\begin{bmatrix}x'_t\\\\y'_t\\end{bmatrix} = s_o \\begin{bmatrix}R_o & T_o\\end{bmatrix}\\begin{bmatrix}x_t\\\\y_t\\end{bmatrix}$\nwhere $s_o$ is a scale factor, $R_o$ is 2D rotation matrix, and $T_o$ is a 2D translation vector corresponding to the applied augmentations. When applying scaling augmentation during training, the ground-truth depth must also be scaled by $s_o$ to maintain scale-consistency in the training data. This is analogous to the image resizing scenario with a fixed camera focal length, as illustrated in Fig. 3."}, {"title": "4.2. FoV Alignment", "content": "When training data include a wide range of camera FoVs within perspective images, such as in the HM3D [32] dataset produced by OmniData [8], the corresponding ERP regions can vary significantly in size, as shown in Fig. 5. There is no single crop size that can consistently capture most content information for certain images without wasting substantial computation on background padding for others. This creates a dilemma in prioritizing between training efficiency and richness of information, and it can also reduce training quality when samples exhibit drastically different content-to-background ratios.\nTo address this challenge, we introduce a simple yet effective FoV Alignment operation that adjusts the FoV of each input image to match the predetermined crop area FoV. Specifically, this process applies a specific scaling augmentation, as described in Sec. 4.1 and Eq. 8, specifically:\n$s_o = \\frac{FoV_c}{FoV_e} \\frac{H_E\\pi}{H_{E\\pi}} and H_{E\\pi}= H_E \\frac{FoV_c}{FoV_e}$\nwhere $FoV_e$ is derived from actual camera parameters, and $FoV_c$ is ERP patch's vertical FoV. As illustrated in Fig. 5, this approach allows a single predefined ERP patch size to maximize the inclusion of relevant content and minimize computational waste on background, making it ideal for an efficient training pipeline."}, {"title": "4.3. Multi-Resolution Training", "content": "Training ERP patches and testing images may prefer inconsistent resolutions for various reasons, e.g. drastically different aspect ratio, edge device limitation. When testing resolutions differ from the training patch size, model performance can degrade significantly, particularly with attention modules that aggregate different numbers of image tokens.\nTo address this issue, we adopt a multi-resolution training scheme. As illustrated in Fig. 4, each ERP patch is resized to two additional lower resolutions (typically 0.7 and 0.4 of the original) to incorporate varied image resolutions in training. The training feeds the model three batches of images at different resolutions and sums the losses."}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Experimental Setup", "content": "In-Domain Training Datasets. Table 1 summarizes the datasets used. For indoor experiments, we use Habitat-Matterport 3D (HM3D) [32], Taskonomy [57], and Hypersim [35], totaling 670K perspective images with distinct characteristics. To streamline training, we use the first 50 scenes from HM3D and Taskonomy (tiny versions provided by OmniData [8]). For outdoor data, we use DDAD [14] and LYFT [18], totaling 130K images, with datasets varying widely in FoV, pitch angles, sources, and image quality.\nZero-Shot Testing Datasets. We evaluate DAC on two 360\u00b0 datasets-Matterport3D [5] and Pano3D-GV2 [2]\u2014and two fisheye datasets-ScanNet++ [52] and KITTI360 [25]\u2014all featuring larger FoVs than perspective images. Our primary experiments focus on these datasets to assess zero-shot generalization to large FoV cameras. Additional evaluations on NYUv2 [27] and KITTI [12] are provided in the supplementary material, demonstrating DAC's performance on perspective data relative to state-of-the-arts.\nEvaluation Details. We assess DAC's generalization to large FoV cameras across both indoor and outdoor scenes, training separate models for each without data mixing to simplify training. For fair comparison, competing models are either re-trained with the same data splits or use their largest versions trained on extensive datasets. Evaluations are conducted using metric depth metrics: \u03b4\u2081 \u2191, \u03b4\u2082 \u2191, \u03b4\u2083 \u2191, Abs Rel\u2193, RMSE\u2193, and log\u2081\u2080\u2193.\nBaselines. We compare DAC with the following baselines:\n\u2022 Metric3Dv2 [19]: A leading method in zero-shot metric depth estimation, achieving state-of-the-art performance on perspective images through a canonical camera model to standardize datasets.\n\u2022 UniDepth [30]: This recent method generalizes to diverse perspective cameras in zero-shot settings. We test its ability to handle large FoV cameras not included in training.\n\u2022 iDisc [29]: Selected as a baseline due to its use of self-attention and cross-attention modules in a straightforward yet effective network, showing strong in-domain performance. As iDisc alone cannot handle mixed camera parameters, we apply the perspective virtual camera conversion from Metric3Dv2 [19] to train it on varied perspective datasets, then compare it to the same network integrated with DAC for zero-shot generalization on cameras with significantly different FoVs.\nImplementation Details. In the DAC training pipeline, we set the full ERP height to $H_{erp}$ = 1400 pixels, with an ERP patch size of 500 \u00d7 700 pixels for both indoor and outdoor models. We use 10\u00b0 latitude augmentations for both and additionally 10\u00b0 rotation augmentation for indoor. When training the iDisc [29] model using the Metric3D [53] pipeline, we use canonical focal lengths of $f_{cano}$ = 519 (NYU dataset [27]) for indoor models and $f_{cano}$ = 721 (KITTI dataset [12]) for outdoor models.\nTo test perspective models on ERP and fisheye images, specific adjustments are required. For 360\u00b0 (ERP) images, which lack a defined focal length, we calculate a virtual focal length $f_{virtual}$ based on pixels per latitude degree:\n$\\frac{1}{f_{virtual}} = \\frac{1}{tan (\\frac{H_E\\pi}{H_{ER}})}$, scaling the predicted depth with $\\frac{f_{cano}}{f_{virtual}}$ for ground-truth alignment. For fisheye images, aligning $f_{cano}$ with the post-distortion focal length introduces significant errors, so we first convert fisheye images to ERP space and apply $f_{cano}$ for metric depth evaluation.\nFor testing resolution, if the original resolution is less than twice the training resolution, we use it directly; for larger resolutions, we maintain the aspect ratio and align it with the training resolution. Based on this rule, we evaluate Matterport3D [5] and Pano3D-GV2 [2] at 512\u00d71024, ScanNet++ [52] at 500 \u00d7 750, and KITTI360 [25] at 700 \u00d7 700. For competing methods that are not adaptable to inconsistent resolutions compared to training, we report the higher score obtained from the two resolution settings to ensure fairness."}, {"title": "5.2. Comparison with the State of the Art", "content": "In this section, we compare DAC with primary baselines in zero-shot generalization tests on large FoV datasets, with quantitative results reported in Table 2, and qualitative results shown in Fig. 6. In indoor experiments, DAC significantly outperforms pre-trained models UniDepth [30] and Metric3Dv2 [19], even when using a lighter ResNet101 [17] backbone and a much smaller training dataset. DAC achieves superior performance across both 360\u00b0 datasets and the fisheye dataset ScanNet++ [52]. Compared to the iDisc [29] network trained with the Metric3Dv2 pipeline, DAC shows substantial improvements across all metrics on all datasets. Notably, DAC improves the next-best method by nearly 50% in the most differentiating metric, \u03b4\u2081, on all three datasets.\nIn outdoor tests, DAC significantly outperforms Metric3Dv2 [53] and UniDepth [30], even with much larger backbones. However, it achieves only marginal improvements over iDisc [29] under the same network configuration, with less pronounced gains compared to indoor settings. This is likely due to the limited camera pitch variance in the outdoor training data (Table 1) and the KITTI360 [25] LiDAR-based ground truth, which primarily covers the central region of a fisheye image (Fig. 6).\nParticularly, a notable observation is that while UniDepth [30] utilizes a network-based spherical conversion, it struggles with large FoV cameras, exposing the limitations of deep learning in extrapolated domains [47]. In contrast, DAC's success underscores the effectiveness of our geometry-based training pipeline.\nAdditional results of DAC models using SwinL [26] backbones are provided in Supplemental Table 5. These models outperform their ResNet101 [17] counterparts in most cases, except on the 360\u00b0 datasets."}, {"title": "5.3. Ablation Study", "content": "Key Components and Network Architecture. We evaluate the effect of the FoV Align and Multi-Reso Training components by removing each individually, while keeping the rest of the DAC framework unchanged. This ablation is conducted on the challenging HM3D-tiny [32] indoor dataset, which includes varied camera FoVs, pitch angles, and lower-quality images from reconstructed scenes. We also test the impact of removing attention modules from iDisc [29] and compare to Metric3Dv2 [19] to isolate the influence of the iDisc architecture. Both iDisc-based methods and DAC use ResNet101 backbones, while Metric3Dv2 uses Dinov2. Table 3 provides a summary; full metrics and Matterport3D [5] results are in the Supplemental Table 6.\nTable 3 highlights the pivotal role of pitch-aware ERP conversion in generalizing perspective-trained models to large FoV datasets by projecting inputs to latitude regions unique to such data (Fig. 7). This approach turns the wide pitch angle variance in datasets like HM3D [32] into an advantage. While additional pitch augmentation does not appear essential when the training dataset like HM3D intricately spans a large range of pitch angle. However, its effectiveness varies across datasets, as detailed in Supplemental Table 6.\nResults in Table 3 also show that removing FoV Align or Multi-Reso Training significantly reduces DAC performance, particularly for zero-shot generalization on 360\u00b0 images."}, {"title": "8. Efficient Up-Projection from Distorted Cameras via Lookup Table Approximation", "content": "Up-projection is a crucial step to convert predicted depth maps into 3D point clouds. For perspective or ERP images, this process is straightforward, as the 3D ray associated with each pixel can be computed in closed form. However, up-projection from fisheye depth maps poses challenges due to the need to invert the distortion model, often requiring the solution of a high-order polynomial equation for each pixel based on the distortion parameters. This process is computationally expensive and impractical for real-time applications.\nFortunately, pre-computed lookup tables can address this issue efficiently. These tables store a mapping from 2D image coordinates to 3D ray directions, allowing for real-time up-projection, which can be written as:\nL : R^2 \u2192 R^3, L(u) = r,\nwhere L represents the lookup table, u = (u,v) \u2208 R\u00b2 denotes the 2D image coordinates, and r = (x,y,z) \u2208 R\u00b3 represents the corresponding 3D ray direction. The lookup tables can be generated using tools like OpenCV with gradient-based numerical methods or through simpler grid search approaches when tangential distortion parameters are negligible [25]. in this work, we use a similar grid search approach to computed lookup tables for Scannet++ [52] based on their provided distortion and intrinsic parameters.\nNotably, our DAC framework does not require approximated solutions for up-projection. In DAC, fisheye images are converted into ERP patches, which rely only on the forward distortion model. The resulting ERP depth maps can then be up-projected into 3D point clouds using each ERP coordinate's ray direction in a unit sphere, eliminating efficiency concerns. This represents a minor but valuable benefit of our approach.\nNevertheless, we identify two practical use cases for lookup tables in other contexts:\n\u2022 Visualization Purposes: Lookup tables efficiently map ERP patches and predicted ERP depth maps back to the original fisheye space for visualization, as illustrated in Fig. 6. Specifically, ERP-to-image conversion for a fisheye image can also be performed efficiently using grid sampling, where each fisheye image coordinate is mapped to its floating-point location in the ERP space. The output of Eq. 10 already provides tangent plane normalized coordinates, $x_t= \\frac{x}{z}$ and $y_t= \\frac{y}{z}$. Using the inverse of Gnomonic Geometry [43], the mapping to spherical coordinates $(\\lambda, \\phi)$ is derived as follows:\n$\\phi = sin^{-1}(\\frac{y_t sin c}{\\rho})cos(\\phi_c)+\\frac{y_t sin c}{\\rho}sin(\\phi_c))$\n$\\lambda = \\lambda_c + tan^{-1}(\\frac{x_t sin c}{\\rho cos(\\phi_c)-\\frac{y_t sin c sin(\\phi_c)}{\\rho}})$\nwhere $\\rho = \\sqrt{x_t^2 + y_t^2}$ and $c = tan^{-1}(\\rho)$\nHowever, this step is only needed for visualization purpose, not required for downstream tasks where up-projected 3D points are the most demanding.\n\u2022 Converting Z-Values to Euclidean Distances: For datasets like ScanNet++ [52], ground-truth depth maps recorded in Z-values must be converted to Euclidean distances for evaluation or inclusion in DAC training. This can be achieved efficiently using pre-computed ray directions from the fisheye's original incoming rays (not distorted by intrinsic parameters). The Euclidean distance for each pixel is calculated as: $D_{Euclid} = \\frac{Z}{z}$, where Z represents the ground-truth Z-value, and z is the z-component of the ray direction r."}, {"title": "9. Additional Visual Results", "content": "In this section, we provide three additional set of visual comparisons of the competing methods on each large-FoV test set, namely: Matterport3D [5], Pano3D-GV2 [2], Scannet++ [52], and KITTI360 [25], as shown in Fig. 8, 9, 10. Compared to Fig. 6, visual results of Unidepth [30] are also included for comparison.\nThrough visual comparisons, our DAC framework demonstrates sharper boundaries in the depth maps and more visually consistent scale in the depth visualization results. As seen in the A.Rel maps wrt. the ground-truth depth, our framework exhibits a significant advantage over each previous state-of-the-art method."}, {"title": "6. Conclusion", "content": "We introduced the Depth Any Camera (DAC) framework for zero-shot metric depth estimation across diverse camera types, including perspective, fisheye, and 360\u00b0 cameras. By leveraging a highly effective pitch-aware Image-to-ERP transformation, FoV alignment, and multi-resolution training, DAC addresses the challenges posed by varying FoVs and resolution inconsistencies and enables robust generalization on large FoV datasets. Our results demonstrate that DAC significantly outperforms state-of-the-art methods and adapts seamlessly to different backbone networks. In practice, DAC ensures that every piece of previously collected 3D data remains valuable, regardless of the camera type used in new applications."}]}