{"title": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction", "authors": ["Jing Liu", "Abdellah Fourtassi"], "abstract": "LLMs can generate human-like dialogues, yet their ability to simulate early child-adult interactions remains largely unexplored. In this paper, we examined how effectively LLMS can capture the distinctive features of child-caregiver language in interaction, using both static and interactive benchmarking methods. We found that state-of-the-art LLMs like Llama 3 and GPT-40 can approximate child-caregiver dialogues at the word and utterance level, but they struggle to reproduce the child and caregiver's discursive patterns, exaggerate alignment, and fail to reach the level of diversity shown by humans. The broader goal of this work is to initiate the development of a comprehensive benchmark for LLMs in child-oriented applications.", "sections": [{"title": "1 Introduction", "content": "While LLMs show remarkable capabilities in generating human-like text and engaging in open-ended dialogues, their ability to simulate the specificities of child-caregiver interactions has not been systematically investigated. However, these interactions show distinct linguistic and interactive patterns compared to adult conversations and require dedicated research.\nDuring their linguistic and communicative development, children show non-conventional (i.e., non-adult-like) patterns, such as word omissions, mispronunciations, semantic errors, and non-standard grammatical constructions (Bloom, 1993). They also show non-conventional conversational behaviors, such as incoherence, non-responsiveness, and atypical turn-taking patterns (Ninio and Snow, 1996). These behaviors are most apparent in the early years leading to primary school, although many persist into adolescence (Nippold, 2016).\nBecause of their still immature, non-conventional language use, children depend on caregivers to interpret and clarify their intents, thus facilitating communication. Caregivers employ various scaffolding strategies, which offer appropriate support tailored to the child's current level of cognitive and communicative development (Berk and Winsler, 1995). This includes the general use of simplified language (a register named child-directed language)(Snow, 1977) as well as interactive strategies such as recasting, repairing, providing follow-up, and offering feedback (Clark, 2020; Nikolaus and Fourtassi, 2023). Scaffolding is gradually reduced as the child becomes more proficient and ready for independent language use.\nDespite growing interest in applying LLMs to specialized interaction scenarios, like healthcare (Ji et al., 2021) and emotional support (J. Ryu et al., 2021), the simulation of child-adult interactions remains understudied. There is, to the best of our knowledge, no systematic examination of whether LLMs can effectively communicate with children. In particular, it remains unclear if they can handle the various non-conventional ways children use language in interaction or respond in an age-appropriate manner (e.g., as a caregiver would). This gap is particularly significant given the potential applications in developmental research and child-computer interaction (Zhang et al., 2024; Seo et al., 2024; R\u00e4s\u00e4nen and Kocharov, 2024; Feng et al., 2024).\nBenchmarking LLMs for effective interaction with children is a complex, multidimensional task requiring assessment of both the content (what is an appropriate response to a child's utterance?) and the form (how to phrase this response in an age-appropriate fashion?). The current work focuses on the latter, using data from spontaneous child-caregiver dialogues as a reference (CHILDES dataset)(MacWhinney, 2000)."}, {"title": "2 Related Work", "content": "Research on LLMs' linguistic and interactive appropriateness to children is still in its early stages. For example, Valentini et al. (2023) focused on vocabulary and showed limitations in LLMs' ability to pick simple words for a young audience, R\u00e4s\u00e4nen and Kocharov (2024) fine-tuned GPT-2 on caregiver input (without incorporating interaction with children), and studied if it can spontaneously generate language that is similar to caregivers. Finally, French et al. (2024) studied GPT-3.5 and Llama2's ability for alignment, showing sub-optimal performance when interacting with children. The current work is the first to seek comprehensive benchmarking, assessing the models at the word, utterance, and dialog levels, and using both static, single-turn tasing and interactive, multi-turn testing."}, {"title": "3 Method", "content": "We used the CHILDES public dataset (MacWhinney, 2000) for benchmarking, focusing on 2 to 5 years of age. We selected 40 conversations (around 300 turns each) distributed across age groups, i.e., 10 conversations at ages 2, 3, 4, and 5 years. The dataset was then restructured into utterance-response pairs. When consecutive turns came from the same speaker (which is often the case with caregivers), we converted them into multiple utterance-response pairs and inserted <SILENCE> tokens for the non-speaking interlocutor. This resulted in a large and diverse benchmarking"}, {"title": "3.2 Models", "content": "While numerous LLMs exist, testing all is impractical, especially with multiple experimental conditions. Still, to examine generalizability and make sure the results are not dependent on the idiosyncracies of one model, we systematically compared two state-of-the-art LLMs: an open-source model Llama 3 (8B) (The chat-optimized version of Meta's instruction-tuned large language model, (Touvron et al., 2023)) and a proprietary model; GPT-40, (version 2024-08),2 the generative pre-trained transformer from OpenAI's GPT-4 family (Achiam et al., 2023)."}, {"title": "3.3 Benchmarking", "content": "Single-turn testing For each conversation, we used each child's utterance separately to prompt the LLMs. This setting allows a direct comparison between the caregiver-LLMs' response, on the one hand, and the caregivers' actual response to the same prompt, on the other.\nMulti-turn testing While single-turn testing allows for a direct comparison with humans, it does not allow us to observe how the model behaves in extended conversations, a more realistic context corresponding to how LLMs are used in real-life interactions. To this end, we used other instances of the LLMs to simulate a child (hereafter, the child-LLMs) and made it interact with the caregiver-LLMs.3 We used the first utterance in each conversation as a starter. While the outcome here is not as controlled as in the single-turn case, the metrics still allow comparisons to the human reference on average.\nZero vs. Few-shot settings We tested the model in a zero-shot setting where the LLM is not given examples from the CHILDES dataset (except the utterance it is supposed to react to). In addition, we tested the models in a 3-shot setting where the models were given the first three child-caregiver pairs of turns in each conversation, allowing the model to observe examples of both the structure"}, {"title": "3.4 Metrics", "content": "We evaluated key properties at the word-, utterance-, and dialogue-levels. For words, we followed Dawson et al. (2021) in measuring Word concreteness using human ratings (Brysbaert et al., 2014), averaged per utterance. We also used Word density-defined as the proportion of content (vs. function) words to total words in the utterance. We used the list of function words identified in previous work (O'Shea et al., 2012).\nAt the utterance level, we followed (R\u00e4s\u00e4nen and Kocharov, 2024) in measuring utterance length, defined as the number of words per sentence, and syntactic complexity, quantified as the mean dependency tree depth 4 (Liu, 2008), where deeper trees indicate more complex structures.\nFinally, at the level of the dialogue dynamics, we measured Semantic alignment, that is, the extent to which the speaker's utterance is semantically similar to their interlocutor's (across each exchange pair). This concept has been used in previous work (Duran et al., 2019; French et al., 2024; Misiek et al., 2020), but here we used a refined measure based on BERT embedding of the entire utterance (Reimers and Gurevych, 2019). The second measure is Semantic diversity, which measures the semantic dissimilarity between utterances of the same speaker. It was measured as the average pairwise cosine distance (= 1 - cosine similarity) between BERT utterance embeddings of a speaker in the entire dialog following prior study (Guo et al., 2023)."}, {"title": "4 Results and Discussion", "content": "Figure 1 shows the results of single-turn testing for child-LLMs and caregiver-LLMs by age group. While we are mainly interested in benchmarking caregiver-LLMs, showing the results of child-LLMs provides interesting insights. Besides, child-LLMs' ability to mimic children is important for more realistic interactions in the multi-turn benchmarking (which we examine next)."}, {"title": "Child-LLM", "content": "We found that child-LLMs follow the developmental patterns observed in children in CHILDES, even in a zero-shot setting, like decreasing concreteness and lexical density over time (children use more abstract words and function words as they grow older), increasing utterance length and syntactic complexity, and finally, decreasing dialog alignment and increasing dialog diversity. GPT-40 more accurately mimicked children than Llama 3, being closer to values observed in CHILDES in most measures, even in a zero-shot setting. Few-shot learning had mostly an impact on Llama 3, aligning it closer to CHIDES in several, though not all, measures. GPT-40 remained the better model after few-shot prompting."}, {"title": "Caregiver-LLM", "content": "Overall, there were no clear developmental patterns in CHILDES caregivers' behaviors. Regarding values, zero-shot GPT-40 was closer to CHILDES in some measures (especially utterance-level measures), while Llama 3 was closer in measures at the word- and dialog level. Few-shot prompting was crucial for both models, but Llama 3 became better aligned with CHILDES overall than GPT-40. Crucially, in both models, differences remained at the dialogue level,"}, {"title": "4.2 Multi-turn testing", "content": "Figure 2 shows the results of multi-turn testing. For easier comparison with single-turn, we show the results side-by-side, averaged over age.5"}, {"title": "Child-LLM", "content": "Here, results for child-LLM are crucial since they are involved in the interactions with caregiver-LLMs and influence their performance. In zero-shot settings, we observed marked differences between single-turn and multi-turn testing in many measures. Most notably, the length, syntactic complexity, and semantic alignment all increased considerably, becoming less comparable to children in CHILDES. After few-shot-prompting, we observed minor to moderate improvements, such as reducing the length and (slightly) reducing syntactic complexity and alignment.\nIn addition to using LLMs instructed to play a child, we also used model fine-tuning. To this end, we considered a pre-trained encoder-decoder model- Blenderbot(Roller, 2020) fine-tuned on a subset of caregiver(encoder)-child(decoder) dialogues in CHILDES. However, while showing reasonable performance in single-turn testing, this fine-tuned model was erratic and highly repetitive in the multi-turn condition (see Figure 2), making it unsuitable for testing caregiver-LLMs."}, {"title": "Caregiver-LLM", "content": "Each caregiver-LLM was interactively benchmarked with a child-LLM (using two instances of the same LLM).7 In the zero-shot setting, we found a significant difference between the models' behavior in single vs. multi-turn testing, especially at the level of the utterance and dialog, where the LLMs drift apart from CHILDES. After a few-shot learning, we observed a significant improvement in length and complexity, which became much more comparable to CHILDES. However, we observed only moderate improvement in dialog-level measures. Models generally continued to exhibit higher alignment and lower diversity than CHILDES. This was the case not only for multi-turn but also for single-turn, indicating that this discrepancy with CHILDES cannot be fully attributed to potential artifacts in the child-LLMs used for multi-turn simulations."}, {"title": "5 Conclusion", "content": "This paper aimed to provide a framework to benchmark LLMs for engaging with children age-appropriately. We found that single-turn testing of the LLMs, as typical in most benchmarks, was not totally indicative of the LLMs' real behavior in an extended conversation, emphasizing the need for more dynamic, multi-turn testing in this line of work. Few-short prompting was effective in bringing the LLMs closer to caregivers' data, especially regarding word- and utterance-level properties. It was not as effective in dialog-level properties. The LLMs exaggerated alignment and showed reduced"}, {"title": "6 Limitations", "content": "We focused on the form-specifically, the linguistic and interactive properties\u2014rather than the content or emotional valence of the responses. Addressing the latter presents a more complex challenge, requiring further interdisciplinary efforts.\nTo evaluate LLMs in multi-turn scenarios, effective child simulators are essential for interacting with caregiver-LLMs. We explored two approaches: (1) fine-tuning an encoder-decoder model to generate child-like responses to caregiver-like prompts, and (2) instructing LLMs to simulate a child's role. The second approach demonstrated greater resilience in multi-turn testing, though it remains imperfect. Future work could explore additional strategies to improve the quality of child simulators.\nFinally, we found that few-shot learning improved caregiver-LLM performance in terms of utterance length and syntactic complexity, but had less impact on alignment and diversity. However, due to the computational cost of the experiments, we did not explore extensive variations in prompt phrasing or few-shot configurations. It remains possible that more refined prompting strategies could yield improvements across these dialog dimensions as well."}, {"title": "7 Ethics statement", "content": "All data used in this study is already publicly available. This work focuses on model benchmarking and improvement using offline child-caregiver data and internal simulations, aiming to advance fundamental research in this area. However, we do not consider this testing sufficient for deployment; any future real-life applications should undergo rigorous validation in child-safe environments with appropriate human oversight, such as by teachers or parents."}, {"title": "Appendix A: Prompt templates", "content": "We provide prompt templates in different generation conditions.\nThe zero-shot prompt template for the caregiver [Conversation history CHI: <Utterance>, ADULT:\n<Utterance>...] You are the parent of a <Month>-month-old English-speaking child. Now, you are having\na conversation with your child. <SILENCE> indicates silence in the previous turn; <UNINTELLIGIBLE>\nindicates unintelligible speech. Based on the given conversation history above, give your response to\nparent input as ADULT. Do not output the speaker label.\nThe zero-shot prompt template for the child [Conversation history CHI: <Utterance>, ADULT:\n<Utterance>...] You are a <Month>-month-old English-speaking child. Now, you are having a conver-\nsation with your parent. <SILENCE> indicates silence in the previous turn; <UNINTELLIGIBLE> indicates\nunintelligible speech. Based on the given conversation history above, give your response to parent input\nas CHI. Do not output the speaker label.\nThe few-shot prompt template for the caregiver [Conversation history CHI: <Utterance>, ADULT:\n<Utterance>...] You are the parent of a <Month>-month-old English-speaking child. Now, you are having\na conversation with your child. <SILENCE> indicates silence in the previous turn; <UNINTELLIGIBLE>\nindicates unintelligible speech. Ensure your response is no longer than 50 words regardless of the prompt.\nHere are some example interactions: CHI: <Utterance>, ADULT: <Utterance>... Follow the example\ninteractions. Based on the given conversation history above, give your response to parent input as ADULT.\nDo not output the speaker label.\nThe few-shot prompt template for the child [Conversation history CHI: <Utterance>, ADULT:\n<Utterance>...] You are a <Month>-month-old English-speaking child. Now, you are having a conver-\nsation with your parent. <SILENCE> indicates silence in the previous turn; <UNINTELLIGIBLE> indicates\nunintelligible speech. Ensure your response is no longer than 6 words regardless of the prompt. Here are\nsome example interactions: CHI: <Utterance>, ADULT: <Utterance>... Follow the example interactions.\nBased on the given conversation history above, give your response to parent input as CHI. Do not output\nthe speaker label."}, {"title": "Appendix B: Example generations", "content": "We present sample raw generations from different models to illustrate the variations in child-adult\ninteractions. Table 1 shows representative exchanges from human conversations (CHILDES corpus)\nand GPT-40 generations selected from the same dialogue. We take GPT-40 as an example, as it has\nrendered the closest performance to humans. To extract features, we preprocessed all samples by removing\nnon-verbal behavioral annotations (e.g., giggles) and lower-casing words to focus exclusively on verbal\ncontent."}, {"title": "Appendix C: Blenderbot Model Fine-Tuning", "content": "We fine-tuned the distilled BlenderBot model (Roller, 2020), a 400M parameter architecture comprising:\na retriever for dialogue history; a Seq2Seq generator with 2 encoder layers, 24 decoder layers, 2560-\ndimensional embeddings, and 32 attention heads; retrieve-and-refine architectures combining dialogue\nand knowledge retrieval. The model was fine-tuned using child utterances as encoder input and adult\nresponses as decoder output, with categorical cross-entropy loss, a batch size of 16, and a learning rate of\n0.0001 for 4 epochs."}]}