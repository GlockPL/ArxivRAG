{"title": "CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features", "authors": ["Po-han Li", "Sandeep Chinchali", "Ufuk Topcu"], "abstract": "Multimodal encoders like CLIP excel in tasks such as zero-shot image classification and cross-modal retrieval. However, they require excessive training data. We propose canonical similarity analysis (CSA), which uses two unimodal encoders to replicate multimodal encoders using limited data. CSA maps unimodal features into a multimodal space, using a new similarity score to retain only the multimodal information. CSA only involves the inference of unimodal encoders and a cubic-complexity matrix decomposition, eliminating the need for extensive GPU-based model training. Experiments show that CSA outperforms CLIP while requiring 300,000\u00d7 fewer multimodal data pairs and 6\u00d7 fewer unimodal data for ImageNet classification and misinformative news captions detection. CSA surpasses the state-of-the-art method to map unimodal features to multimodal features. We also demonstrate the ability of CSA with modalities beyond image and text, paving the way for future modality pairs with limited paired multimodal data but abundant unpaired unimodal data, such as lidar and text.", "sections": [{"title": "1 Introduction", "content": "Multimodal encoders like CLIP (Radford et al., 2021) excel in various zero-shot multimodal tasks, such as image classification and cross-modal retrieval. Despite the huge success, they demand excessive training data. For example, OpenAI trained the original CLIP model on 400 million image-text pairs using 592 V100 GPUs, and the training size of the new CLIP models (Ilharco et al., 2021) has increased to 12 billion image-text pairs since then. In addition, more data do not guarantee better performance. CLIP relies on Internet data, which are often of poor quality (Sharma et al., 2018). Incorrectly captioned data may lead to specific failure modes in similar instances (Northcutt et al., 2021; Vasudevan et al., 2022). In this work, we focus on learning a multimodal encoder with limited data that is robust to noisy data.\nOur conjecture is that unimodal encoders such as DINO (Oquab et al., 2023) and GTR (Ni et al., 2022) can serve as the building blocks of multimodal encoders. Unimodal encoders only need uni-modal data, which are easier to obtain than paired multimodal data. They also require significantly less data than multimodal models, and advanced unimodal encoders are already well-developed. We distill the unimodal knowledge in these models to accelerate the learning of multimodal encoders. We map their unimodal features into a multimodal feature space, similar to how CLIP encodes images and text jointly to a shared feature space. In addition, this process should be data-efficient.\nWe propose canonical similarity analysis (CSA) to verify our conjecture. It replicates a multimodal encoder such as CLIP with two unimodal encoders, as shown in Figure 1. CSA maps features from unimodal encoders to a multimodal space. It removes redundant information from unimodal features and employs a novel similarity function to replicate CLIP similarity score for multimodal data, e.g. images and captions. CSA operates by inference of unimodal encoders and solving an optimization problem, essentially a matrix decomposition without training of neural networks. CSA supports all zero-shot tasks that CLIP is capable of while requiring significantly fewer training data.\nContributions. Our contributions are threefold: (1) We propose CSA which replicates the CLIP model using two unimodal encoders while demanding less computation and data (Section 4). (2) Our theoretical analysis characterizes the trade-off of obtaining informative embeddings and distinguishing multimodal data pairs, considering various hyperparameters of CSA (Section 5). (3) We tested CSA on tasks such as image classification, cross-modal retrieval, and misinformative captions detection (Section 6). The experimental results show that CSA outperforms or matches CLIP while requiring 300, 000\u00d7 fewer paired multimodal data and 6\u00d7 fewer unimodal data. CSA outperforms the state-of-the-art method that employs unimodal models with the same amount of training data as 18% in ImageNet classification and 23% in image-to-text retrieval.\nCSA works across all modalities beyond images and text. We additionally demonstrate its capability with audio and text, paving the way for new modality pairs, such as lidar and text Yang et al. (2023). In the future, we envision that new modality pairs will have limited paired multimodal data but sufficient unimodal data, where CSA can efficiently map unimodal features to multimodal features."}, {"title": "2 Related Works", "content": "Unimodal Encoders. Unimodal encoders encode unimodal data to fixed-dimensional features. In computer vision, encoders, e.g., CosPlace (Berton et al., 2022), ViT (Dosovitskiy et al., 2021), and Dino (Caron et al., 2021; Oquab et al., 2023), are trained with augmented data with random cropping and blurring with contrastive loss (van den Oord et al., 2019). In natural language processing, one common data augmentation method is to randomly mask words or tokens, used in the universal sentence encoder (Cer et al., 2018), Bert (Devlin et al., 2019), and GTR (Ni et al., 2022).\nMultimodal Encoders. Multimodal encoders project multimodal data points into a feature space shared between modalities. Thus, multimodal features enable training for downstream models (Shridhar et al., 2021; Li et al., 2024; Goel & Narasimhan, 2024). Another common usage is to directly take the inner product of features of data from different modalities, e.g., an image and a sentence, as a similarity metric. This metric enables downstream tasks such as zero-shot image classification and cross-modal data retrieval. The CLIP model (Radford et al., 2021) is the mile-stone of multimodal encoders, which bridges the gap between language and vision. CLIP inspires encoders for other modalities, such as audio and inertial measurement units. Such encoders include ImageBind (Girdhar et al., 2023), AudioCLIP (Guzhov et al., 2022) and CLAP (Wu* et al., 2023). Previous work also studied cross-modal retrieval on multi-label images (Ranjan et al., 2015), while ours focuses on a more generic multimodal setting with pre-trained unimodal encoders. Although ImageBind encodes 6 modalities, it lacks training data for all possible modality pairs, resulting in suboptimal performance for pairs such as audio and inertial measurement units.\nNeural Networks with CCA. Previous works have shown that training losses inspired by canonical correlation analysis (CCA) (Hardoon et al., 2004) are useful in multimodal or multi-distribution tasks (Sun et al., 2019; Lyu & Fu, 2020, 2023). Although taking a crucial step of introducing CCA to deep learning, these works focus on training deep neural networks from scratch instead of using existing foundation encoder models, as in this work. Other works use techniques similar to CCA, such as principal component analysis (PCA), to process the output features of encoders (Li et al., 2023; Omama et al., 2024). In this work, we focus on using CCA to map unimodal features to a multimodal feature space without any model training, which significantly reduces the size of the required training dataset.\nCombining multiple unimodal Encoders. The closest work to ours is ASIF (Norelli et al., 2023), which also uses multiple unimodal encoders to bridge the multimodal gap. ASIF calculates the similarity of a pair of multimodal data based on the unimodal similarity of the given data pair and a pre-selected, multimodal anchor dataset, similar to Moschella et al. (2023). It creates a linear kernel space by the anchor set. However, ASIF demands a huge anchor set to achieve similar performance to CLIP, while our proposed method, CSA, constantly outperforms ASIF in all tasks."}, {"title": "3 Problem Formulation", "content": "Given two sets of data X1 = {x1, x2, ..., xN1 }, X2 = {x1, x2, ..., xN2 } from two modalities m1, m2 (such as vision and language), we want to find two feature extractors (encoders) E1, E2 that map the data to the same-dimensional feature space. The encoders generated feature pairs E1(xi1), E2(xi2 ) are close, and the feature pairs from different pairs of data E1(xi1), E2(xj2 ) are far.\nNote that the two sets of data are paired, which means that they are sophisticatedly related (such as an image and its corresponding caption), although of different modalities. Previous works trained multimodal encoder pairs to achieve this goal by minimizing the following loss:\n$\\mathcal{L}(X^1, X^2; E^1, E^2) := \\frac{-1}{2N} \\sum_{i=1}^{N} 2(E^1(x_i^1))^T(E^2(x_i^2)) - \\sum_{j\\neq i}^{N} \\frac{(E^1(x_i^1) + E^2(x_i^2))^T (E^1(x_j^1) + E^2(x_j^2))}{2N-2} ,$\nwhere the first term encourages similar features for the same pair of data, and the second term encourages the opposite. Variants of this loss are commonly used in contrastive learning (van den Oord et al., 2019), such as CLIP (Radford et al., 2021).\nFrom Unimodal to Multimodal. Training a multimodal encoder to learn a multimodal feature space is time-consuming and data-inefficient. The original CLIP model learns from Internet-scale datasets (400 million images) and requires 500 A100 Nvidia GPUs. Instead of training end-to-end multimodal encoders, we leverage existing unimodal encoders to accelerate the multimodal learning process. That is, given unimodal encoders that encode data into unimodal feature spaces, we directly find two mapping functions that map each unimodal feature space to a multimodal space.\nMathematically, we use two unimodal encoders E1, E2 that capture only unimodal latent informa-tion and individually encode data xi1 into features z1 \u2208 Rq1, xi2 \u2208 Rq2. Because these features come from different encoders, their dimensions need not be the same (q1 != q2), which com-plicates the discovery of the mapping functions. The dimensions of the features are significantly smaller than those of the raw data xi1. Hence, we utilize unimodal encoders to significantly reduce the dimension of the data and the complexity of the problem. Our ultimate goal is to efficiently find two mapping functions A, B that map the feature spaces to a multimodal one while preserving the contrastiveness property characterized in Equation 1."}, {"title": "4 Canonical Similarity Analysis (CSA)", "content": "The problem in Section 3 is easier to state than to solve. First, unimodal encoders have completely different architectures (e.g., transformers vs. U-net), losses, and output dimensions. Second, map-ping data from various-dimensional feature spaces to a multimodal one means that we need to dis-card latent information from the higher-dimensional space. The challenge lies in determining what information to discard to maintain the contrastiveness property. Also, we do not want to preserve all information as there might be some modality-specific information that we wish not to preserve through the mapping, such as words that cannot be expressed in images or punctuation.\nWe thus propose canonical similarity analysis (CSA). We observe that all downstream tasks use cosine similarity to obtain the similarity of multimodal data points. It inspires us to use correlation coefficients, the cosine similarity between centered vectors, to approximate this similarity and build the mapping functions from unimodality to multimodality. Precisely, we find the pairs of bases in each unimodal feature space that maximize the correlation coefficients. We use CCA to find such bases. Then, instead of directly using an inner product, we use a weighted cosine similarity score to evaluate the similarity of the multimodal data. We discard information from low-correlated bases, as they might not be relevant to the multimodal feature space, and we weight the contributions of the bases to the similarity score based on their correlation coefficients."}, {"title": "4.1 Canonical Correlation Analysis", "content": "Recall the previous notation of two sets of data X1 = {x1, x2,...,xN1 }, X2 = {x1, x2, ..., xN2 }. We use two unimodal encoders E1, E2 to encode and normalize the data into zero-meaned latent features {z1i, z2i}Ni=1. To find the bases that maximize the correlation coefficients, we use CCA to map the two feature spaces to a multimodal feature space of dimension r = min(q1, q2):\nA*, B* = argmaxA\u2208Rr\u00d7q1 ,B\u2208Rr\u00d7q2 Tr (A z1 z2T BT )\ns.t. (A z1 )(A z1 )T = (B z2 )(B z2 )T = Ir,\nwhere z1 \u2208 Rq1\u00d7N, z2 \u2208 Rq2\u00d7N are the matrices consisting of column feature vectors zi, Ir \u2208 Rr\u00d7r is the identity matrix, and Tr is the trace function. The objective of Equation 2 can be viewed as the sum of the correlation coefficients of the bases, and the identity constraints normalize the vectors so that covariance becomes correlation. In summary, CCA finds the bases of the two given matrices such that the projected vectors have the highest correlation coefficients \u03c1i for i = 1, ..., r.\nSolving CCA is similar to principal component analysis (PCA), which is essentially a singular value decomposition (SVD) of time complexity O(q1q2r). Therefore, it possesses the beneficial property of removing noise from the data, which we later detailed in the theoretical analysis. Weenink (2003) gives the analytical solution:\nA*, B* = U\u03a31/2ST, V\u03a321/2UT,\nwhere UT \u03a3V = z1 z2T \u2208 Rq1\u00d7q2 is the SVD matrices of the rank-r matrix. The correlation coefficients \u03c1 of Equation 2 are the diagonal entries of \u03a3 in descending order \u03c11 \u2265 \u03c12 \u2265 . . . \u2265 \u03c1r like the descending principal components of PCA."}, {"title": "4.2 Canonical Similarity Analysis", "content": "We now define CSA. For any pair of multimodal data {xi1, xi2}, with slight abuse of the notation of original data and features, we define the similarity as:\nS(xi1, xi2; s) = S(z1, z2; s) =\u2211i=1s \u03c1i((A\u2217z1)iT (B\u2217z2)i )||(A\u2217z1)1:s||2||(B\u2217z2)1:s||2,\nwhere (\u00b7)1:s denotes the 1st to s-th row vector, (\u00b7)i denotes the i-th entry, and s is a pre-defined hyperparameter. Note that Equation 4 is the weighted cosine similarity of the first s dimensions of the transformed data matrix. The weights determine how much each dimension influences the overall similarity score, depending on their level of correlation. We chose the hyperparameter s so that CSA preserves the information of dimensions with correlations greater than \u03c1s. As stated earlier, the feature space might contain modal-specific information that cannot be mapped to the other modality. Thus, we only take into account meaningful bases. We choose CCA with linear correlation instead of complicated kernel CCA because most contrastive learning loss functions only contain the linear inner product of the feature vectors, which is also the case in Equation 2."}, {"title": "4.3 Training and Inference", "content": "Similar to standard machine learning models, CSA consists of both training and inference phases. We split the multimodal data into training and test sets, just like in standard machine learning. CSA performs all optimizations on the training set, and the test set is not known a priori and is used only for evaluation. During the training phase, or rather called the fitting phase, CSA obtains the training set of encoded features from the unimodal encoders and then solves Equation 2. Note that CSA does not train any encoders, so it does not require GPUs. The resulting correlation coefficients \u03c1 determines the value of s in Equation 4:\ns = argmini \u03c1i s.t. \u03c1i \u2265 const.\nNote that const here is an empirical constant threshold to optimize performance in our experience. Finally, for any multimodal downstream task, CSA uses Equation 4 to evaluate the similarity be-tween any pair of multimodal data. We later show the experimental results of CSA and compare the results with the state-of-the-art methods."}, {"title": "5 Theoretical Analysis of CSA", "content": "In this section, we characterize the trade-off of obtaining informative embeddings and distinguishing multimodal data pairs caused by selecting the appropriate s in Equation 4. We start with the linear setting, but the conclusion holds for a more general setting.\nAssumptions. We follow the assumption of data generation per Joshi et al. (2024) and analyze the effect of CSA based on the theoretical results of contrative losses (Ji et al., 2023; Tian, 2022; Nakada et al., 2023). Suppose that the N pairs of data {xi1, xi2}Ni=1 from two modalities m1, m2 are generated as follows:\nxi1 = Tz1i + \u03b51i, xi2 = T2zi + \u03b52i.\nHere, zi \u2208 Rq is the shared unobservable latent feature of the two modalities, and T1 \u2208 Rp1\u00d7q, T2 \u2208 Rp2\u00d7q are the linear mapping functions of the latent vectors to the observable data. Without loss of generality, we assume that both the latent feature vectors and noise for each modality are independent and identically distributed (i.i.d.) samples from any distribution."}, {"title": "Optimal Linear Encoders of Contrastive Loss", "content": "Previous work (Ji et al., 2023) gives the analytical solution of a unimodal linear encoder Ek trained on linear contrastive loss with norm regularization:\nL(Xk, X'k; Ek) := \\frac{-1}{2N} \\sum_{i=1}^{N} 2(E_k x_i^k)^T(E_k x_i'^k) - \\sum_{j\\neq i}^{N} \\frac{(E_k x_i^k + E_k x_i'^k)^T (E_k x_j^k + E_k x_j'^k)}{2N-2} +\nwhere Xk, X'k are two sets of original and augmented (e.g., random masked, Gaussian blurred, etc.) data from the same modality, and \u03bb is a hyperparameter weighting the regularization term. Note that this contrastive loss function is similar to Equation 1, but the second modality is the augmented data now. For simplicity, we assume that all unimodal data are augmented with complementary random masking with probability 0.5, so the optimal linear encoder is:\nE\u2217linear= argmin C \u2212 C (\u2211 \u03c3i ei eTi ) , for i = 1, 2.\nIn Equation 8, C > 0 is a positive constant related to \u03bb in Equation 7. ei is the i-th standard vector basis, and \u03c3i is the i-th largest eigenvalue of matrix:\nOffDiag(X2XT) - N\u221211 X2(1 \u2212 IN )XT,\nwhere OffDiag denotes the function that makes all diagonal entries 0, and 1 is a square matrix of ones. From Equation 3, 6, and 8, the solution of CSA in linear setting is:\nAI = A\u2217ELinearX1 = A\u2217ELinear(T1zi + \u03f51), BI = B\u2217ELinearX2 = B\u2217ELinear(T2zi + \u03f52)."}, {"title": "6 Experiments", "content": "Baselines. We compared CSA with another baseline, ASIF (Norelli et al., 2023). It has the same setting as CSA, which uses two unimodal encoders to calculate the similarity of a multimodal data pair. Unlike CSA solves an optimization problem, ASIF infers multimodal similarity by the uni-modal distance between data points. ASIF is the only fair baseline for CSA. We trained CSA and ASIF with the same training set and unimodal encoders and evaluated all the methods in the test set. We also compared CSA with the state-of-the-art multimodal encoder model from OpenCLIP (Ilharco et al., 2021; Cherti et al., 2022), acknowledging their incomparable training set and model parameters, shown in Table 1. In addition to encoder models, we compared the performance of LLaVA2-13B (Touvron et al., 2023), a multimodal large language model, and prompted it to solve the specified tasks from the provided image and text. The unimodal encoders of CSA and ASIF are GTR (Ni et al., 2022) and DINOv2 (Oquab et al., 2023) here. Section A shows the detailed settings.\nImage Classification. We first examined CSA's ability of image classification on ImageNet and Leafy Spurge. All methods input an image and all possible captions \"This is an image of label\" and select the most similar caption as the predicted label. In Figure 3a, CSA requires only 35,000 training samples to match the performance of CLIP and consistently outperforms ASIF, which needs millions of data points to achieve a similar performance to CLIP. Since CLIP is zero-shot, its perfor-mance is regardless of the amount of data. Leafy Spurge challenges the capability of methods with extremely limited data. This dataset includes only 800 training images and 100 test images of plants with binary classes: leafy spurge and others. The images are out-of-distribution for any encoders, as this dataset was newly published in 2024 and captured using drones. Figure 3b shows that while the performance of CSA fluctuates, it consistently outperforms CLIP and ASIF in this challenging scenario, demonstrating CSA's effectiveness with limited out-of-distribution data.\nCross-modal Retrieval. We examined CSA and the other baselines on a cross-modal retrieval dataset, Flickr30k (Young et al., 2014), which contains 15,000 images, each paired with 5 textual descriptions. This dataset is common for evaluating the performance of image-to-text and text-to-image retrieval. For image-to-text retrieval, the task is to find the corresponding caption from a set of possible candidates (the so-called reference set) given a query image and vice versa. We retrieved the most similar images or text from the reference set according to the similarity scores of all meth-ods and showed the results in Table 2. For image-to-text retrieval, we show mean average precision (mAP), precision@1, and precision@5. For text-to-image retrieval, we only evaluated performance using precision@1 since there is only one correct image. From Table 2, CSA exhibits lower perfor-mance compared to the multimodal baseline, CLIP. However, it outperforms the unimodal baseline, ASIF, in both retrieval scenarios. Notably, ASIF completely failed in the text-to-image retrieval task.\nDetecting Mislabeled ImageNet Images. We again show CSA's superiority with limited and noisy data. Previous works show that ImageNet is imperfect and contains incorrectly labeled images, leading to several failure modes of downstream models (Vasudevan et al., 2022). Trained with the original incorrect ImageNet dataset, all methods detect if the input image and the label align. We use human-evaluated labels from (Northcutt et al., 2021) to define incorrectly labeled images.\nAll methods input an image and caption \"This is an image of label\" and output if the image and caption align or not (true or false). LLaVA answers questions about whether the given image and caption align. CSA, CLIP, and ASIF output true if the calculated similarity score of the image and caption pair is greater than a threshold. We enumerated the thresholds to obtain several true and false positive rates and show the receiver operating characteristic (ROC) curves in Figure 4. In the legend, we listed the area under the curve (AUC), where a higher value indicates better performance. In Figure 4, CSA outperforms all other baselines, even LLaVA, given 5,000 image-label pairs. We also see that when the size of the training data increases from 5,000 to 35,000, the performance of CSA and ASIF increases as well. We emphasize that the comparison of AUC does not directly compare accuracy. As shown in Figure 4b, the true positive rate of CSA outperforms LLaVA by nearly 10% under the same false positive rate.\nNote that the training sets of CSA and ASIF contain mislabeled ImageNet images, demonstrating CSA 's robustness to noisy data and efficiency in learning from just 5, 000 pairs. Similarly, the CLIP encoders' training set also contains mislabeled ImageNet images, making the comparison fair. We later demonstrate CSA 's robustness with even noisier training data.\nDetecting Misinformative Captions. Detecting mislabeled images is easy since text captions differ only in labels. We considered a much harder task to detect misinformation news captions from the COSMOS dataset (Aneja et al., 2023). The task involves determining whether a Google-retrieved caption aligns with news images and its original captions, as shown in Figure 5a. The benchmark considers the retrieved captions misinformative if they do not align with the images and the corresponding captions simultaneously. The ultimate goal of the task is to identify and prevent the spread of misinformative news captions on the Internet."}, {"title": "Robustness to noisy data.", "content": "To test CSA's robustness, we randomly shuffled a percentage of the training labels of ImageNet and evaluated the resulting test accuracy. Figure 6 shows that CSA constantly outperforms ASIF and achieves 70% accuracy even if 50% of the training labels are incorrect. When all the data are shuffled, both CSA and ASIF degrade to 0% accuracy.\nLimitations. CSA is suitable for bimodal data, similar to CLIP, unlike Imagebind (Girdhar et al., 2023) is capable of 6 modalities. Once the encoded features are obtained, CSA is an optimization problem suitable for CPUs but not GPUs, so one cannot accelerate the computation of CSA with GPUs. Like ASIF, CSA is based on unimodal encoders. If the unimodal encoders are not foundation models but trained on limited datasets like MNIST, the performance of CSA degrades.\nSummary. CSA always outperforms the unimodal baseline, ASIF, and matches or exceeds CLIP's performance, except in cross-modal retrieval. This exception likely arises because cross-modal re-trieval involves comparing thousands of similarity score pairs between the query data and the refer-ence set, unlike the other tasks that compare against certain captions. It highlights the trade-off of CSA discussed in Section 5. CSA encounters a trade-off between distinguishability and informative-ness, i.e., to maintain the distant features. Retrieval tasks require a more curated balance between these aspects, and other tasks benefit from greater distinguishability of similarity scores."}, {"title": "7 Conclusions and Future Works", "content": "We conclude that CSA is a robust and data-efficient method to replicate the CLIP similarity score using two unimodal encoders. In Table 1, the amount of data required compared to CLIP underscores that unimodal encoders are significantly more efficient to train. A small amount of multimodal data suffices to learn the mapping of unimodal features to a multimodal space, even if the training data are unprocessed by humans (COSMOS) or incorrectly labeled (ImageNet).\nIn this work, we proposed CSA, which maps two unimodal feature spaces from pre-trained encoders to a multimodal feature space. CSA is extremely data-efficient. We characterize the intrinsic trade-off of CSA, and CSA shows competitive performance compared to CLIP models in image classifi-cation, mislabeled data detection, cross-modality retrieval, and misinformation detection tasks with limited data. CSA also outperforms the state-of-the-art method in the same setting.\nFuture work includes extending CSA to more than 2 modality pairs, just as generalized CCA (Horst, 1961) extends CCA to more sources. In addition, understanding the relationship between the size of the training set and its performance is crucial. If we can fine-tune the unimodal encoders, which loss function will result in the most suitable feature space for CSA remains an open problem. Lastly, CSA essentially finds a mapping function between two feature spaces, which need not be modality features. We aim to test CSA on mapping intramodal data, such as multi-view images of an object."}, {"title": "A Additional details on the experiments", "content": "We run all inferences of LLAVA and encoder models on an NVIDIA RTX A5000 GPU, and solving Equation 2 with 35,000 multimodal feature vectors on a 64 core Xeon Gold 6226R CPU machine takes less than 10 minutes. The implementation of CCA is from CCA Zoo Chapman & Wang (2021).\nMultimodal Encoders. The multimodal image-text encoder used throughout the experiments is laion/CLIP-ViT-bigG-14-laion2B-39B-b160k from Huggingface. The multimodal audio-text encoder used is laion/larger_clap_general from Huggingface (Wu* et al., 2023).\nUnimodal Encoders. We use several unimodal encoders and show the difference in performance in Section E. To encode images, we tested DINOv2-Giant (Oquab et al., 2023) and the unimodal part of the multimodal encoders previously mentioned. To encode text, we tested GTR-t5-large (Ni et al., 2022) and the unimodal part of the multimodal encoders mentioned above. To show CSA's ability to combine unimodal models, we never tried using the paired unimodal encoders of a multimodal encoder in our experiments, i.e., using CLIP to encode both images and text.\nFlickr30k. We trained ASIF and CSA on the Flickr validation set, which includes 145, 000 images and 5 captions for each image. We then validated the models on a test set of 5,000 images and 25,000 captions.\nCOSMOS. We trained ASIF and CSA on the COSMOS validation set, which includes 41,006 image-caption pairs. We then validated the models using a test set of 1,700 image-caption pairs, with half of the captions labeled as misinformation by human annotators."}, {"title": "B Towards more modalities-Audio and text", "content": "We now show CSA's generalization ability to more modalities with MusicCaps (Agostinelli et al., 2023). We use GTR and CLAP to encode YouTube audio along with the tagged genre descriptions of the audio. We conducted a classification task in which the models input the audio and a tag and output if the audio aligns with the caption. Similar to the mislabeled ImageNet experiment, we show the ROC curves and compare the AUC in Figure 7. We trained ASIF and CSA for 3, 777 data points and tested all methods on 1,625 data points. We randomly sampled a tag for each data point during both training and inference. In Figure 7, we see that CSA performs as well as CLAP, the CLIP-inspired multimodal audio and text encoder, and outperforms ASIF. Thus, we conclude that CSA extends its capabilities beyond image and text, effectively handling audio and text as well."}, {"title": "C Correlation of Feature Spaces", "content": "To take a deeper look into unimodal feature spaces, we show the correlation coefficients of COSMOS image and caption features under CSA in Figure 8. The data are inherently noisy, as indicated by the correlation coefficients of the unimodal feature spaces, which concentrate on 0.2 to 0.4. This distribution of correlation coefficients highlights that, despite the fact that the original multimodal data are noisy and show complex correlations, CSA can effectively map them to a multimodal space where the similarity score remains meaningful for the zero-shot downstream tasks."}, {"title": "D Sensitivity to Hyperparameter s", "content": "We show CSA's sensitivity to the hyperparameter s in terms of the end performance. In Table 3, CSA achieves optimal performance in s = 200, and its performance degrades with increases and decreases in s, illustrating the trade-off characterized in Section 5. However, for tasks other than retrieval, we find that a larger s improves performance in image classification, mislabeling detec-tion, and misinformation caption detection. This phenomenon is likely due to the trade-off between distinguishability and informative embedding features, namely the distance between features. Al-though retrieval tasks require a more curated balance between these aspects, other tasks benefit from greater distinguishability of similarity scores."}, {"title": "E Sensitivity to Unimodal Encoders", "content": "We change the unimodal encoders of ASIF and CSA to showcase their generalization ability to different unimodal encoders. Figure 9 shows the results on the detection of mislabeled ImageNet data with other encoders, and Figure 10 shows the results on the detection of misinformative captions"}]}