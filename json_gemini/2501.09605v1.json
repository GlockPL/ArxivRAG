{"title": "Managed-Retention Memory: A New Class of Memory for the Al Era", "authors": ["Sergey Legtchenko", "Ioan Stefanovici", "Richard Black", "Antony Rowstron", "Junyi Liu", "Paolo Costa", "Burcu Canakci", "Dushyanth Narayanan", "Xingbo Wu"], "abstract": "Al clusters today are one of the major uses of High Band-width Memory (HBM). However, HBM is suboptimal for AI workloads for several reasons. Analysis shows HBM is overprovisioned on write performance, but underprovisioned on density and read bandwidth, and also has significant en-ergy per bit overheads. It is also expensive, with lower yield than DRAM due to manufacturing complexity. We propose a new memory class: Managed-Retention Memory (MRM), which is more optimized to store key data structures for AI inference workloads. We believe that MRM may finally pro-vide a path to viability for technologies that were originally proposed to support Storage Class Memory (SCM). These technologies traditionally offered long-term persistence (10+ years) but provided poor IO performance and/or endurance. MRM makes different trade-offs, and by understanding the workload IO patterns, MRM foregoes long-term data reten-tion and write performance for better potential performance on the metrics important for these workloads.", "sections": [{"title": "INTRODUCTION", "content": "To date the world has been very binary when it comes to storage: there are non-volatile and volatile storage technolo-gies. DRAM in different forms (GDDR, HBM, LPDDR) is the dominant volatile memory storage technology. Data it stores is lost as soon as the energy source is removed. NAND block-oriented and NOR byte-addressable Flash are the most widely used examples of non-volatile memory storage. They do not need to be constantly powered to persist data. At the memory cell level, data volatility is expressed as retention time, which is the time that data is reliably stored without requiring a refresh. Flash cells have a retention time of 10+ years, but this comes at the cost of lower read and write throughput per memory cell than DRAM. These properties mean that DRAM is used as memory for processors, and Flash is used for secondary storage.\nSeveral other memory technologies, like RRAM, MRAM [27, 46] and PCM [23], all have the potential to offer non-volatility. They fall into a class of memory that is often referred to as Storage Class Memory (SCM) for servers. The recently dis-continued Intel Optane / 3D XPoint [15] is an iconic represen-tative of SCM, which aimed to overcome the IO limitations of Flash while being non-volatile. The dream was to replace DRAM by offering comparable IO performance and byte addressability, while also featuring 10+ year retention. However, all attempts to date have failed to displace DRAM due to the trade-offs. They failed to offer IO performance that is comparable to DRAM at lower (or same) costs as Flash due to the challenges of density and complex manufacturing processes. For main memory, persistence of data is not as im-portant as IO performance. For general compute workloads, nobody wants to trade primary memory IO-performance for 10+ year data retention. These technologies also struggle with endurance, which refers to the number of write cycles a memory cell can support before it permanently degrades [23]. Hence, SCM ended up being valuable for some use cases (e.g., embedded compute [1, 2]), but not for deployment in servers.\nIronically, we believe that the rise of Flash may have been something of a curse for memory innovation. Non-volatility is a key storage device property, but at a memory cell level it is quite misleading. For all technologies, memory cells offer simply a retention time, which is a continuum from mi-croseconds for DRAM to many years. The technologies that underpin SCM have been forced to be non-volatile, requiring their retention time to be a decade or more. Unfortunately, achieving these high retention times requires trading off other metrics such as write and read latency, energy effi-ciency and endurance [12, 18, 31].\nPerhaps one reason why this has been viewed historically as binary, is that even with relaxed retention times, SCM technologies would not match DRAM on all metrics of impor-tance for general workloads. However, foundation models (of which Large Language Models, or LLMs are a subset) have recently emerged as a new major workload with unique memory IO requirements [36]. The tremendous scale and growth of foundation model training and inference require novel hardware approaches. Foundation model inference has different memory IO requirements to historical workloads. For example, a large fraction of the memory is used to store model weights, for which IO performance is critical for se-quential reads, but much less important for writes. Memory IO is sequential and predictable, and given the energy chal-lenges of AI clusters, energy per bit read is also an issue. The only technology today that can match the IO performance, energy and density is HBM. However, it is no panacea, and"}, {"title": "MEMORY IN THE FOUNDATION MODEL-ERA", "content": "The workload of a foundation model is quite different to tradi-tional workloads. A foundation model is first trained, usually on a large cluster (e.g., 50,000+ AI accelerators), and the output is essentially a set of model weights. These weights are then deployed in production where they serve inference queries. Thousands or even millions of instances of the foun-dation models will be used but the scale of hardware per inference is much smaller (e.g., 4+ AI accelerators). It has been observed that both training and inference workloads are memory intensive [3, 55]. Training scale depends on the model size and is a one-off effort (often taking months), while the inference workload is demand-driven and served for a significant time period until the model weights are retired.\nTraining and inference have distinct memory access pat-terns and requirements, and are typically deployed on dif-ferent clusters. As demand increases, we are expecting the inference infrastructure to dominate, and are hence focusing on the inference workload. More specifically, we consider foundation models that perform autoregressive token gen-eration. An inference query is a sequence of input tokens, in response to which the foundation model generates a se-quence of output tokens. A context is composed of all the tokens from the user and the corresponding responses gen-erated by the model during the interaction. Having contexts as large as possible is desirable, as it improves the model's reasonning ability via its use of the self-attention mecha-nism [51]. However, in deployment, contexts have limited size and range from low 1000s to a few 10,000s tokens (de-pending on the model), and are primarily limited by the amount of memory available. Each inference query is com-putationally expensive and requires distributed computation across multiple AI accelerators.\nInference relies on three main in-memory data structures: model weights, the KV cache, and model activations. Of these, model weights and the KV cache use up the majority of the memory capacity [21].\nThe model weights (a matrix) have been key to expand-ing the capabilities of frontier foundation models; there has been an exponential growth in the size of the model weights with each generation of foundation model. Currently, large models have (well) over 500 billion weights, representing be-tween 250 GB and over 1 TB of data depending on the weight quantization used. The weights are effectively a non-mutable data structure. The reference model weights are persisted in storage, while a replica is distributed across the AI accelera-tors in every inference cluster. There are a large number of foundation models today, but in practice a small number of the most popular ones are used at scale. All inference queries made to a given foundation model version (e.g., GPT4) use a copy of the same weights.\nThe KV cache supports the model's self-attention mecha-nism. It is a sequence of self-attention vectors that encode the model's understanding of the relationship between all the tokens in a context. Every time a new token is generated in a context, a vector is appended to the end of the corre-sponding KV cache. Each vector is typically a few MBs, so the KV cache usually grows to a few tens of GBs until the context size limit is reached.\nLastly, model activations are the transient tensors that are created and passed between the different layers during a forward pass of the network. They are typically an order of magnitude smaller than both the weights and the KV cache, and are only stored during the forward pass computation.\nThe KV cache is created during the prefill phase, when the first set of tokens is received from a user. Subsequently, in the decode phase the model iteratively generates response tokens. For that, at each iteration the KV cache is read en-tirely and sequentially, a new token is generated, and the corresponding self-attention vector is appended to the KV cache. KV caches leverage memory to reduce computation and are soft state: they are generated by the model, and can be re-computed if needed. However, the token rate per sec-ond is usually quite low (thus expensive) so caching and using the KV cache is usually preferable to recalculation."}, {"title": "The Curse of HBM", "content": "Today the majority of data used in an AI accelerator is stored on HBM, because all the data structures need to be repeatedly read at high bandwidth. Current AI accelerators can support very high main memory bandwidths, e.g., 8 TB/s for a single B200 GPU [50]. In addition, since weights and KV caches are large, Al accelerators require substantial HBM capacity. Hard engineering challenges need to be surmounted to achieve this, especially around energy usage. Signal loss over copper interconnect tracking at the required data rates means that the memory must be physically located (very) close to the compute die, typically co-packaged on the same interposer. The very wide interfaces, and high signal rates translate into more energy, and approximately a third of the energy usage for an AI accelerator is the memory. HBM is used as it enables 3D-stacking of DRAM on the same package to boost on-package memory capacity, throughput, and minimize the distance of the memory cells from the AI accelerator. Current HBM products have 8-12 layers, for an aggregate 192 GB on a B200 package [50]. Hence, HBM is used as it offers the highest throughput at the highest density with reasonable energy usage. However, even using HBM, a substantial part of every inference query is memory bound [34].\nUnfortunately, there is currently no viable alternative to HBM. Non-stacked DRAM does not have the required density, while NAND and NOR Flash memory are not fast enough and have low lifetime endurance especially at higher densities where multiple bits are stored per memory cell. Both lack the energy efficiency required in package.\nIt should be noted that HBM comes with several funda-mental challenges. First, memory vendors are struggling to continue to scale the density. The per-layer scaling is strug-gling with challenges inherited from DRAM [38]. So, the next generation of HBM (HBM4) is only expected to increase capacity per layer by 30% compared to current HBM3e. Sec-ondly, the 3D-stacking of DRAM both significantly reduces the yield of the manufacturing process and also leads to heat dissipation challenges, especially when tightly pack-aged with an AI accelerator die. Currently, the industry does not expect it to scale beyond 16 layers in the foreseeable future [49]. 3D-stacking is extremely complex. These factors, combined with high demand, fueled by exponential growth of cloud infrastructure for foundation models, means that HBM accounts for a substantial fraction of an Al cluster's cost. This is unlikely to change in the foreseeable future, and Al clusters will remain dependent on HBM."}, {"title": "A New Hope?", "content": "Foundation model inference is very different from the general-purpose main memory workload for which DRAM was de-signed. First, it is extremely read-intensive. For example, each token generated during decode requires reading all the weights, and the entire KV cache [35], for one self-attention vector write. Self-attention vector size is usually at most a few MBs [4, 42], while weights and KV caches are typically 10s of GBs, which imply read:write ratios of over 1000:1.\nThere are efforts to reduce the amount of data read dur-ing inference. For example, batching allows weight reuse across requests [3]. However, batching is limited by latency requirements [3]. Reuse of the KV cache across requests [53] and KV cache compression [25] are also used, but each has its limitations and even together they do not fundamentally change the heavily read-dominated nature of the workload.\nSecond, memory accesses are sequential and predictable. There are no in-place updates for weights or KV caches, and the same weights and KV cache are read iteratively for every foundation model response. Memory virtualization mechanisms have been proposed to address memory frag-mentation [21], but even in that case, pages are read in the same order. Each page is typically over 10 vectors (typically several MBs to 10s of MBs) and is read sequentially [21]. Fur-thermore, the mapping between virtual pages and physical addresses is typically static.\nThese properties suggest that most of the HBM capacity is used for data that has little use for the general-purpose properties HBM inherits from DRAM (random access, byte-addressability, comparable read and write performance). HBM is, in a sense, overprovisioned for the requirements of this foundation model inference workload. This overprovisioning leads to suboptimal cost and energy overheads.\nIt also raises the tantalizing question: if we correctly pro-vision the memory to the workload, can we address this suboptimal cost and energy challenges for memory in infer-ence clusters?"}, {"title": "THE MEMORY OPPORTUNITY", "content": "We posit that the combination of (i) the importance and scale of foundation model infrastructure, (ii) the large difference between the workload patterns of conventional server CPUs and that of AI accelerators, and (iii) the poor match of HBM to the workload, opens a field of computer architecture research in better memory for this application.\nWe now motivate that this opportunity is best addressed by a new type of memory, as opposed to DRAM, HBM or Flash. Flash cannot be used because it does not have enough endurance, even with Single Level Cells (SLC) [7], and cannot satisfy the high throughput and energy efficiency require-ments [13, 33]. The non-volatility of Flash is also unneces-sary: the data is either persisted elsewhere (weights) or is soft state (KV caches, activations).\nOn the other hand, some workload properties are close to ones typically exhibited by storage workloads. For example, byte addressability is not required, because IO is large and sequential. Similar to storage infrastructure, storage capacity and total cost of ownership (TCO)/TB are key metrics, on which DRAM and HBM are underperforming.\nCan MRM match AI cluster requirements?PCM, RRAM, and STT-MRAM have read performance and energy on par or better than DRAM, and potential for higher density and/or lower TCO/TB [16]. PCM was shipped at scale in Intel Optane devices, while RRAM and STT-MRAM have matured over the past few years, and are used for automotive, wearable and IoT applications [1, 2, 6].\nThese technologies have lower endurance than DRAM, and we now estimate the approximate endurance require-ments for weight and KV cache writes. Weight updates are infrequent, bulk overwrites when the model is replaced. The update frequency is currently typically low (hours+), but could evolve as models diversify. We estimate the endurance required over 5 years for a conservative hourly update and an intensive once per second update. KV cache writes occur both during prefill and decode, one self-attention vector per context token. Prefill is typically higher throughput than decode, and we use the throughputs and median context lengths reported for the Llama2-70B model in Splitwise [35].\nFor an expected lifetime of five years, we compute the num-ber of KV cache writes, and infer the average number of writes per cell.\nFigure 1 shows a comparison between endurance of ex-isting memory/storage technologies and the workload en-durance requirements. When applicable, we differentiate endurance observed in existing devices from the potential demonstrated by the technology. We use potential endurance from [27, 46], while device endurance is taken from de-vice specifications and benchmarks (Intel Optane PCM [5], Weebit RRAM [29] and Everspin STT-MRAM [37]). We ob-serve that 1) HBM is vastly overprovisioned on endurance, and 2) existing SCM devices do not meet the endurance re-quirements but the underlying technologies have the poten-tial to do so. We believe this is partly due to current devices being designed for non-volatility, which is achieved by trad-ing off other important metrics such as write latency, energy efficiency or endurance [18, 31]. We see this as an oppor-tunity to rethink existing memory technologies, currently used for SCM, specifically for AI workloads, by trading off non-volatility for other key metrics."}, {"title": "SOFTWARE STACK IMPLICATIONS", "content": "In this section, we motivate why MRM is of interest to the computer systems community. Foundation models are be-coming pervasive which leads to a diversification of the requirements: some use cases have tight latency SLAs (e.g., user-in-the-loop conversation), some are throughput hungry and heavily use batching, others are background best-effort jobs (e.g., meeting recap). The workload is becoming more complex, with vastly different input:output token ratios, ex-pert models tailored for specific use cases, and dependen-cies on advanced augmentation mechanisms (e.g. RAG [56]). In addition to that, the resource-heavy nature of the work-load and the cost of the hardware require hollistic and ef-ficient orchestration. This is addressed by leveraging key OS mechanisms (e.g., virtual memory [21], power-aware scheduling [44] or speculative execution [28]), effectively building up towards a rack-scale OS for foundation model inference. In that context, the emergence of MRM brings a set of exciting challenges and opportunities to explore.\nRetention-aware data placement and scheduling. MRM is unlikely to be a one-size-fits-all solution, and will co-exist with other types of memory, such as HBM for write-heavy data structures (e.g., activations), and LPDDR as a slower tier. Fine-grained understanding of lifetime and access patterns of the data will be required to lay out the data. The sched-uler will need to track the data expiration times, and decide whether to refresh it or move it to another tier based on the state of the requests that depend on that data."}, {"title": "RELATED WORK", "content": "The trade-offs between retention, endurance and write en-ergy efficiency have been well studied both for STT-MRAM [17, 41, 47] and RRAM [14, 22, 31, 39]. Leveraging this mecha-nism has been proposed to improve the energy efficiency of hybrid on-die CPU caches [17, 39, 41, 47]. In contrast to our work, this strand of work focuses on general-purpose multi-core CPUs, and is hence addressing a different optimization problem. AI clusters have rack-scale energy and cooling re-quirements, and have a more complex set of memory tiers and interconnects, but more predictable workloads."}, {"title": "CONCLUSION", "content": "The emergence of AI workloads and their dependence on HBM memory has highlighted the limitations of HBM. AI inference workloads demand high read throughput, density, and energy efficiency, which HBM struggles to provide cost-effectively. We propose a new class of memory that can co-exist with HBM, Managed-Retention Memory (MRM), which enables the use of memory technologies originally proposed for SCM, but trades retention and other metrics like write throughput for improved performance metrics crucial for these AI workloads. By relaxing retention time requirements, MRM can potentially enable existing proposed SCM technologies to offer better read throughput, energy efficiency, and density. We hope this paper really opens new thinking about innovation in memory cell technologies and memory chip design, tailored specifically to the needs of AI inference clusters."}]}