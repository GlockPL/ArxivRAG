{"title": "Kolmogorov-Arnold Networks for Online Reinforcement Learning", "authors": ["Victor A. Kich", "Jair A. Bottega", "Raul Steinmetz", "Ricardo B. Grando", "Ayano Yorozu", "Akihisa Ohya"], "abstract": "Kolmogorov-Arnold Networks (KANs) have shown potential as an alternative to Multi-Layer Perceptrons (MLPs) in neural networks, providing universal function approximation with fewer parameters and reduced memory usage. In this paper, we explore the use of KANs as function approximators within the Proximal Policy Optimization (PPO) algorithm. We evaluate this approach by comparing its performance to the original MLP-based PPO using the DeepMind Control Proprio Robotics benchmark. Our results indicate that the KAN-based reinforcement learning algorithm can achieve comparable performance to its MLP-based counterpart, often with fewer parameters. These findings suggest that KANs may offer a more efficient option for reinforcement learning models. Our implementations can be found in the following link:https://github.com/victorkich/Kolmogorov-PPO.", "sections": [{"title": "1. INTRODUCTION", "content": "Kolmogorov-Arnold Networks (KANs) [1] are learn-able approximators based on the Kolmogorov-Arnold representation theorem [2], which states that any multivariate continuous function can be expressed as a finite sum of continuous univariate functions. KANs can approximate functions with fewer neurons compared to Multi-Layer Perceptrons (MLPs), thereby reducing network complexity. This simplification enables easier function decomposition, allowing for more straightforward and targeted architecture designs tailored to specific problems. Additionally, KANs utilize less memory and enhance model interpretability, making them highly appealing for various machine learning applications.\nReinforcement Learning (RL)[3] has proven efficient for continuous control tasks such as autonomous robot manipulation[4], navigation [5], and video game playing [6], [7]. Proximal Policy Optimization (PPO)[8] is a Deep RL algorithm that employs model-free online reinforcement learning to approximate an optimal policy over online training experiences. This algorithm is a cornerstone in the field, excelling in numerous benchmarks and serving as the foundation for newer models like Multi-Agent PPO [9], PPO with Covariance Matrix Adaptation [10], PPO with Action Mask [11], and PPO with Policy Feedback [12].\nNetworks with fewer parameters yet equal or superior approximation capabilities have the potential to significantly enhance performance in various deep reinforcement learning applications. In this work, we propose the use of KANs as policy and value approximators for PPO. The main contributions of this paper are:\n\u2022 The first application of KANs as function approximators in a reinforcement learning algorithm.\n\u2022 A performance comparison between KAN-based and\nMLP-based Proximal Policy Optimization in robotics control.\nThe paper is organized as follows: In Section 2. we review related works and research in the field. Section 3. provides the mathematical background of the techniques employed. The proposed methodology is presented in Section 4. Section 5.presents the results and evaluation analysis. Finally, we discuss our findings in Section 6."}, {"title": "2. RELATED WORKS", "content": "KANs have recently garnered significant attention for their versatility and enhanced performance across various applications. Numerous studies have explored KANs as alternatives to traditional Multilayer Perceptrons, consistently demonstrating their superiority in specific scenarios. Concurrently, online reinforcement learning has been a cornerstone in the domain of continuous control tasks, underscoring the importance of investigating KANs' potential in this field. Evaluating whether KANs can compete with or surpass MLPs in reinforcement learning applica-tions is a critical area of exploration. This section reviews pivotal research contributions relevant to this research.\nExpanding on the KAN framework, Bozorgasl and Chen [13] introduced Wav-KAN, a neural network architecture that integrates wavelet functions [14] into the KAN framework. This innovation addresses common challenges faced by traditional MLPs, such as interpretability, training speed, robustness, computational efficiency, and performance. By incorporating wavelet functions, Wav-KAN effectively captures both high-frequency and low-frequency components of the input data, enhancing overall performance.\nExploring KANs' performance on predicting time series, Rubio et al. [15] demonstrated the effectiveness of these networks in predicting satellite traffic, showing their superiority over traditional MLPs for time series analysis. Similarly, Genet and Hugo [16] introduced Temporal Kolmogorov-Arnold Networks (TKANs), combining elements of KANs and Long Short-Term Memory (LSTM) networks to improve time series forecasting.\nExpanding on the exploration of KANs' capabilities, Yang, Qin, and Yu [17] proposed a novel framework that enhances the interpretability of neural cognitive diagnosis models (CDMs) using KANs while maintaining high accuracy. They introduced two methods: replacing traditional MLPs in existing neural CDMs with KANs and designing a new aggregation framework entirely based on KANs. Their experimental results on four real-world datasets showed that the KA2NCD framework outperforms traditional and neural CDMs in both accuracy and interpretability, demonstrating its potential in intelligent education systems.\nOnline Reinforcement Learning, where an agent continuously learns from ongoing interaction with the environment, instead of using memory replay, has been successfully applied to various robotic tasks. Zhang, Pang, and Hu [18] presented a novel approach to solving the trajectory tracking problem for manipulators and mobile robots using PPO with Generalized Advantage Estimation and Long Short-Term Memory networks as actor and critic. Similarly, Lopes et al. [19] explored the application of Proximal Policy Optimization to control a quadrotor, demonstrating that model-free online reinforcement learning can effectively train a reliable control policy for quadrotor position control. In the humanoid robots area, Melo et al. [20] successfully utilized PPO to develop running skills in a humanoid robot, using the RoboCup 3D Soccer Simulation environment, where humanoid robots compete in simulated soccer matches. Expanding the use of online reinforcement learning to other areas, Proctor [21] used the algorithm for localizing and searching for lost nuclear sources in a designated area, which is vital for societal safety and reducing human harm.\nTo the best of our knowledge, no efforts have been made to apply KANs as alternatives to MLPs in reinforcement learning, making this study a pivotal contribution."}, {"title": "3. THEORETICAL BACKGROUND", "content": "This section presents the theory and mathematical foundations underpinning our approach.\n3.1 Kolmogorov-Arnold Networks\nKolmogorov-Arnold Networks leverage the Kolmogorov-Arnold representation theorem, which asserts that any multivariate continuous function can be decomposed into a finite composition of univariate functions and addition operations. Formally, for a smooth function \\(f : [0,1]^n \\rightarrow \\mathbb{R}\\), this can be expressed as:\n\\[f(x) = \\sum_{q=1}^{2n+1} \\Phi_q\\left(\\sum_{p=1}^{n} \\varphi_{q,p}(x_p)\\right),\\]\nwhere \\(\\varphi_{q,p}: [0,1] \\rightarrow \\mathbb{R}\\) and \\(\\Phi_q : \\mathbb{R} \\rightarrow \\mathbb{R}\\) are continuous functions.\nIn KANs, weight parameters are replaced by learnable 1D functions, parametrized as B-splines. The computation in a KAN layer with \\(n_{in}\\) inputs and \\(n_{out}\\) outputs is:\n\\[x_{l+1,j} = \\sum_{i=1}^{n_l} \\varphi_{l,j,i}(x_{l,i}),\\]\nwhere \\(\\varphi_{l,j,i}\\) is a spline function connecting the i-th neuron in layer l to the j-th neuron in layer l + 1.\nThe backpropagation process in KANs involves calculating gradients of the spline functions. The loss \\(\\mathcal{L}\\) is minimized using gradient descent, with the gradient of the loss with respect to the spline parameters \\(c_i\\) computed as:\n\\[\\frac{\\partial \\mathcal{L}}{\\partial c_i} = \\sum_{j=1}^{N_{out}} \\frac{\\partial \\mathcal{L}}{\\partial x_{l+1,j}} \\frac{\\partial x_{l+1,j}}{\\partial \\varphi_{l,j,i}} \\frac{\\partial \\varphi_{l,j,i}}{\\partial c_i},\\]\nwhere \\(\\delta x_{l+1,j}\\) involves the derivative of the spline function with respect to its coefficients.\nComparatively, KANs and MLPs differ as follows:\n\u2022 KANs feature learnable activation functions on edges, while MLPs have fixed activation functions on nodes.\n\u2022 KANs use splines to represent weights, enhancing their capacity to approximate complex functions with fewer parameters.\n\u2022 Both KANs and MLPs can be extended to multiple layers.\n3.2 Proximal Policy Optimization\nProximal Policy Optimization is a policy gradient method that computes an estimator of the policy gradient for use in a stochastic gradient ascent algorithm. The policy gradient estimator is:\n\\[\\hat{g} = \\mathbb{E}_t \\left[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) A_t\\right],\\]\nwhere \\(\\pi_{\\theta}\\) is the stochastic policy, and \\(A_t\\) is an estimator of the advantage function at timestep t. The expectation \\(\\mathbb{E}_t[... ]\\) denotes the empirical average over a finite batch of samples."}, {"title": "4. METHODOLOGY", "content": "We conducted our experiments using six environments from the Gymnasium Mujoco suite, specifically adapted from the DeepMind Control Proprio benchmarks [22]. These environments are well-suited for evaluating continuous control tasks and include: HalfCheetah-v4, Hopper-v4, InvertedPendulum-v4, Pusher-v4, Swimmer-v4, and Walker2d-v4. These environments provide a diverse set of challenges, testing the ability of the agent to learn different types of motion and control strategies.\nOur implementation utilizes the PPO algorithm as the core reinforcement learning method. PPO is a state-of-the-art algorithm known for its stability and efficiency in policy gradient methods [8]. We incorporated KANS as the function approximators for both the policy (actor) and value (critic) functions, replacing the traditional MLP used in standard PPO implementations.\n4.1 Network Architectures\nWe designed and compared several network architectures for the policy (actor) and value (critic) functions to evaluate the effectiveness of KANs versus MLPs. The configurations included:\n\u2022 MLP (a=2, c=2): Two hidden layers for both actor and critic with 64 units each.\n\u2022 MLP (a=1, c=2): One hidden layer for the actor and two hidden layers for the critic, with 64 units each.\n\u2022 KAN (k=2, g=3): KAN for the actor with no hidden layers, order k = 2 and grid g = 3, and a critic implemented as an MLP with 2 hidden layers of 64 units each.\n\u2022 Full KAN (k=2, g=3): Both the actor and critic networks implemented using KANs with no hidden layers, order k = 2 and grid g = 3.\nThe parameter settings for KANs were chosen based on preliminary tests where we evaluated various configurations to identify the most effective setup. Our tests demonstrated that a KAN with no hidden layers, order k = 2, and grid g = 3 provided the best performance in terms of computational efficiency and approximation capability. Consequently, we adopted this architecture for intensive testing.\nFor the Kolmogorov-Arnold Network, we set the parameters k = 2 and g = 3 based on these preliminary findings, ensuring the network's ability to approximate the continuous functions involved in control tasks while maintaining low computational complexity. The number of parameters in the KAN configurations is significantly lower compared to the MLP counterparts, highlighting the efficiency of KANs. The average number of parameters for each configuration across the environments is shown in Table 1.\n4.2 Training Procedure\nEach agent was trained for 1 million steps in each environment. We used the following hyperparameters for training:\n\u2022 Number of Environments: 1\n\u2022 Learning Rate: \\(3 \\times 10^{-4}\\)\n\u2022 Clip Parameter \\(\\epsilon\\): 0.2"}, {"title": "5. RESULTS", "content": "The performance of the KAN-based PPO was evaluated against the MLP-based PPO using metrics such as average return and the number of parameters. The average return, defined as the mean cumulative reward over 100 evaluation episodes, provided a robust measure of the policy's effectiveness. The number of parameters, representing the total count of trainable elements in the network, offered insights into the efficiency and potential computational\nsavings achieved by using KANS.\n5.1 Performance Metrics\nFigure 2 illustrates the average rewards obtained across the training steps for each environment. In general, the KAN-based PPO matched or exceeded the performance of the MLP-based PPO in most environments. This demonstrates the potential of KANs to achieve robust performance with fewer parameters, particularly in the task HalfCheetah-v4, where the KAN-based models outperformed their MLP counterparts.\nTable 2 shows the DMC Proprio scores for the control inputs at 1 million training steps. The KAN-based models performed competitively, sometimes surpassing the MLP configurations in terms of reward mean while maintaining a lower parameter count. This highlights the potential of KANs to provide efficient and effective learning with potentially reduced computational requirements.\n5.2 Discussion\nThe results from our experiments indicate that KANs, when integrated into the PPO framework, can achieve performance comparable to traditional MLPs with significantly fewer parameters. This finding is particularly noteworthy as it demonstrates the efficiency of KANs in reducing the computational complexity of reinforcement learning models without compromising on performance.\nWhile KAN-based models generally matched or exceeded the performance of MLP-based models, it is important to acknowledge the contexts in which these improvements were observed. For instance, in tasks like HalfCheetah-v4 and Hopper-v4, the KAN-based PPO not only achieved higher rewards but also maintained a lower parameter count, highlighting the efficiency of KANS. However, in some environments like Pusher-v4"}, {"title": "6. CONCLUSION", "content": "Our study demonstrated that Kolmogorov-Arnold Networks can achieve performance comparable to traditional Multi-Layer Perceptrons with significantly fewer parameters in reinforcement learning tasks. This makes KANs a promising alternative, especially in resource-constrained environments where memory and computational efficiency are critical. However, the current limitations in computational speed due to the complexity of B-spline activation functions suggest the need for further research and optimization. Future work should focus on developing specialized optimization techniques and frameworks for KANs to improve their computational efficiency and make them more suitable for real-time applications."}]}