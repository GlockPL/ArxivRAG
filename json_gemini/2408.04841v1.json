{"title": "Kolmogorov-Arnold Networks for Online Reinforcement Learning", "authors": ["Victor A. Kich", "Jair A. Bottega", "Raul Steinmetz", "Ricardo B. Grando", "Ayano Yorozu", "Akihisa Ohya"], "abstract": "Kolmogorov-Arnold Networks (KANs) have shown potential as an alternative to Multi-Layer Perceptrons (MLPs) in neural networks, providing universal function approximation with fewer parameters and reduced memory usage. In this paper, we explore the use of KANs as function approximators within the Proximal Policy Optimization (PPO) algorithm. We evaluate this approach by comparing its performance to the original MLP-based PPO using the DeepMind Control Proprio Robotics benchmark. Our results indicate that the KAN-based reinforcement learning algorithm can achieve comparable performance to its MLP-based counterpart, often with fewer parameters. These findings suggest that KANs may offer a more efficient option for reinforcement learning models. Our implementations can be found in the following link:https://github.com/victorkich/Kolmogorov-PPO.", "sections": [{"title": "1. INTRODUCTION", "content": "Kolmogorov-Arnold Networks (KANs) [1] are learn-\nable approximators based on the Kolmogorov-Arnold rep-\nresentation theorem [2], which states that any multivariate\ncontinuous function can be expressed as a finite sum of\ncontinuous univariate functions. KANs can approximate\nfunctions with fewer neurons compared to Multi-Layer\nPerceptrons (MLPs), thereby reducing network complex-\nity. This simplification enables easier function decom-\nposition, allowing for more straightforward and targeted\narchitecture designs tailored to specific problems. Addi-\ntionally, KANs utilize less memory and enhance model\ninterpretability, making them highly appealing for various\nmachine learning applications.\nReinforcement Learning (RL)[3] has proven efficient\nfor continuous control tasks such as autonomous robot ma-\nnipulation[4], navigation [5], and video game playing [6],\n[7]. Proximal Policy Optimization (PPO)[8] is a Deep RL\nalgorithm that employs model-free online reinforcement\nlearning to approximate an optimal policy over online\ntraining experiences. This algorithm is a cornerstone in\nthe field, excelling in numerous benchmarks and serv-\ning as the foundation for newer models like Multi-Agent\nPPO [9], PPO with Covariance Matrix Adaptation [10],\nPPO with Action Mask [11], and PPO with Policy Feed-\nback [12].\nNetworks with fewer parameters yet equal or superior\napproximation capabilities have the potential to signifi-\ncantly enhance performance in various deep reinforcement\nlearning applications. In this work, we propose the use of\nKANs as policy and value approximators for PPO. The\nmain contributions of this paper are:\n\u2022 The first application of KANs as function approximators\nin a reinforcement learning algorithm.\n\u2022 A performance comparison between KAN-based and"}, {"title": "2. RELATED WORKS", "content": "KANs have recently garnered significant attention for\ntheir versatility and enhanced performance across various\napplications. Numerous studies have explored KANs as\nalternatives to traditional Multilayer Perceptrons, consis-\ntently demonstrating their superiority in specific scenarios.\nConcurrently, online reinforcement learning has been a\ncornerstone in the domain of continuous control tasks, un-\nderscoring the importance of investigating KANs' poten-\ntial in this field. Evaluating whether KANs can compete\nwith or surpass MLPs in reinforcement learning applica-"}, {"title": "3. THEORETICAL BACKGROUND", "content": "This section presents the theory and mathematical foun-\ndations underpinning our approach.\n3.1 Kolmogorov-Arnold Networks\nKolmogorov-Arnold Networks leverage the Kolmogorov-\nArnold representation theorem, which asserts that any mul-\ntivariate continuous function can be decomposed into a\nfinite composition of univariate functions and addition op-\nerations. Formally, for a smooth function $f : [0,1]^n \\rightarrow R$,\nthis can be expressed as:\n$f(x) = \\sum_{q=1}^{2n+1} \\Phi_q (\\sum_{p=1}^n \\varphi_{q,p}(x_p)),$ (1)\nwhere $\\varphi_{q,p}: [0,1] \\rightarrow R$ and $\\Phi_q : R \\rightarrow R$ are continuous\nfunctions.\nIn KANs, weight parameters are replaced by learnable\n1D functions, parametrized as B-splines. The compu-\ntation in a KAN layer with $n_{in}$ inputs and $n_{out}$ outputs\nis:\n$x_{l+1,j} = \\sum_{i=1}^{n_l} \\varphi_{l,j,i} (x_{l,i}),$ (2)\nwhere $\\varphi_{l,j,i}$ is a spline function connecting the i-th neuron\nin layer l to the j-th neuron in layer l + 1.\nThe backpropagation process in KANs involves cal-\nculating gradients of the spline functions. The loss Lis\nminimized using gradient descent, with the gradient of the\nloss with respect to the spline parameters ci computed as:\n$\\frac{\\partial L}{\\partial c_i} = \\sum_{j=1}^{N_{out}} \\frac{\\partial L}{\\partial x_{l+1,j}} \\frac{\\partial x_{l+1,j}}{\\partial c_i},$ (3)\nwhere $\\delta x_{l+1,j}$ involves the derivative of the spline function\nwith respect to its coefficients.\nComparatively, KANs and MLPs differ as follows:\n\u2022 KANs feature learnable activation functions on edges,\nwhile MLPs have fixed activation functions on nodes.\n\u2022 KANs use splines to represent weights, enhancing their\ncapacity to approximate complex functions with fewer\nparameters.\n\u2022 Both KANs and MLPs can be extended to multiple\nlayers.\n3.2 Proximal Policy Optimization\nProximal Policy Optimization is a policy gradient\nmethod that computes an estimator of the policy gradi-\nent for use in a stochastic gradient ascent algorithm. The\npolicy gradient estimator is:\n$\\hat{g} = \\hat{E}_t [\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) \\hat{A}_t],$ (4)\nwhere $\\pi_{\\theta}$ is the stochastic policy, and $\\hat{A}_t$ is an estimator\nof the advantage function at timestep t. The expectation\n$\\hat{E}_t[...]$ denotes the empirical average over a finite batch\nof samples."}, {"title": "4. METHODOLOGY", "content": "We conducted our experiments using six environments\nfrom the Gymnasium Mujoco suite, specifically adapted\nfrom the DeepMind Control Proprio benchmarks [22].\nThese environments are well-suited for evaluating contin-\nuous control tasks and include: HalfCheetah-v4, Hopper-\nv4, InvertedPendulum-v4, Pusher-v4, Swimmer-v4, and\nWalker2d-v4. These environments provide a diverse set of\nchallenges, testing the ability of the agent to learn different\ntypes of motion and control strategies.\nOur implementation utilizes the PPO algorithm as the\ncore reinforcement learning method. PPO is a state-of-\nthe-art algorithm known for its stability and efficiency\nin policy gradient methods [8]. We incorporated KANS\nas the function approximators for both the policy (actor)\nand value (critic) functions, replacing the traditional MLP\nused in standard PPO implementations.\n4.1 Network Architectures\nWe designed and compared several network architec-\ntures for the policy (actor) and value (critic) functions to\nevaluate the effectiveness of KANs versus MLPs. The\nconfigurations included:\n\u2022 MLP (a=2, c=2): Two hidden layers for both actor and\ncritic with 64 units each.\n\u2022 MLP (a=1, c=2): One hidden layer for the actor and\ntwo hidden layers for the critic, with 64 units each.\n\u2022 KAN (k=2, g=3): KAN for the actor with no hidden lay-\ners, order k = 2 and grid g = 3, and a critic implemented\nas an MLP with 2 hidden layers of 64 units each.\n\u2022 Full KAN (k=2, g=3): Both the actor and critic net-\nworks implemented using KANs with no hidden layers,\norder k = 2 and grid g = 3.\nThe parameter settings for KANs were chosen based\non preliminary tests where we evaluated various config-\nurations to identify the most effective setup. Our tests\ndemonstrated that a KAN with no hidden layers, order\nk = 2, and grid g = 3 provided the best performance\nin terms of computational efficiency and approximation\ncapability. Consequently, we adopted this architecture for\nintensive testing.\nFor the Kolmogorov-Arnold Network, we set the pa-\nrameters k = 2 and g = 3 based on these preliminary\nfindings, ensuring the network's ability to approximate\nthe continuous functions involved in control tasks while\nmaintaining low computational complexity. The number\nof parameters in the KAN configurations is significantly\nlower compared to the MLP counterparts, highlighting the\nefficiency of KANs. The average number of parameters\nfor each configuration across the environments is shown\nin Table 1.\n4.2 Training Procedure\nEach agent was trained for 1 million steps in each en-\nvironment. We used the following hyperparameters for\ntraining:\n\u2022 Number of Environments: 1\n\u2022 Learning Rate: 3 \u00d7 10-4\n\u2022 Clip Parameter \u20ac: 0.2"}, {"title": "5. RESULTS", "content": "The performance of the KAN-based PPO was evaluated\nagainst the MLP-based PPO using metrics such as average\nreturn and the number of parameters. The average return,\ndefined as the mean cumulative reward over 100 evalua-\ntion episodes, provided a robust measure of the policy's\neffectiveness. The number of parameters, representing the\ntotal count of trainable elements in the network, offered\ninsights into the efficiency and potential computational"}, {"title": "5.1 Performance Metrics", "content": "Figure 2 illustrates the average rewards obtained across\nthe training steps for each environment. In general, the\nKAN-based PPO matched or exceeded the performance of\nthe MLP-based PPO in most environments. This demon-\nstrates the potential of KANs to achieve robust perfor-\nmance with fewer parameters, particularly in the task\nHalfCheetah-v4, where the KAN-based models outper-\nformed their MLP counterparts."}, {"title": "5.2 Discussion", "content": "The results from our experiments indicate that KANs,\nwhen integrated into the PPO framework, can achieve\nperformance comparable to traditional MLPs with sig-\nnificantly fewer parameters. This finding is particularly\nnoteworthy as it demonstrates the efficiency of KANs in\nreducing the computational complexity of reinforcement\nlearning models without compromising on performance.\nWhile KAN-based models generally matched or ex-\nceeded the performance of MLP-based models, it is im-\nportant to acknowledge the contexts in which these im-\nprovements were observed. For instance, in tasks like\nHalfCheetah-v4 and Hopper-v4, the KAN-based PPO\nnot only achieved higher rewards but also maintained\na lower parameter count, highlighting the efficiency of\nKANS. However, in some environments like Pusher-v4"}, {"title": "6. CONCLUSION", "content": "Our study demonstrated that Kolmogorov-Arnold Net-\nworks can achieve performance comparable to traditional\nMulti-Layer Perceptrons with significantly fewer parame-\nters in reinforcement learning tasks. This makes KANs a\npromising alternative, especially in resource-constrained\nenvironments where memory and computational efficiency\nare critical. However, the current limitations in compu-\ntational speed due to the complexity of B-spline activa-\ntion functions suggest the need for further research and\noptimization. Future work should focus on developing\nspecialized optimization techniques and frameworks for\nKANs to improve their computational efficiency and make\nthem more suitable for real-time applications."}]}