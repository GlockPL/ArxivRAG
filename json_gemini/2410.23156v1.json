{"title": "VISUALPREDICATOR: LEARNING ABSTRACT WORLD\nMODELS WITH NEURO-SYMBOLIC PREDICATES FOR\nROBOT PLANNING", "authors": ["Yichao Liang", "Nishanth Kumar", "Hao Tang", "Adrian Weller", "Joshua B. Tenenbaum", "Tom Silver", "Jo\u00e3o F. Henriques", "Kevin Ellis"], "abstract": "Broadly intelligent agents should form task-specific abstractions that selectively\nexpose the essential elements of a task, while abstracting away the complexity of\nthe raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates,\na first-order abstraction language that combines the strengths of symbolic and neu-\nral knowledge representations. We outline an online algorithm for inventing such\npredicates and learning abstract world models. We compare our approach to hi-\nerarchical reinforcement learning, vision-language model planning, and symbolic\npredicate invention approaches, on both in- and out-of-distribution tasks across\nfive simulated robotic domains. Results show that our approach offers better sam-\nple complexity, stronger out-of-distribution generalization, and improved inter-\npretability.", "sections": [{"title": "INTRODUCTION", "content": "Planning and model-based decision-making for robotics demands an understanding of the world that\nis both perceptually and logically rich. For example, a household robot needs to know that slippery\nobjects, such as greasy spatulas, are hard to grasp. Determining if the spatula is greasy is a subtle\nperceptual problem. As an example of logical richness, for a robot to use a balance beam to weigh\nobjects, it must count up the mass on each side of the balance beam to determine which way the\nbeam will tip. Counting and comparing masses are logically sophisticated operations.\nIn this work, we show how to efficiently learn symbolic abstractions that are both perceptually and\nlogically rich, and which can plug into standard robot task-planners to solve long-horizon tasks. We\nconsider a robot that encounters a new environment involving novel physical mechanisms and new\nkinds of objects, and which must learn how to plan in this new environment from relatively few\nenvironment interactions (the equivalent of minutes or hours of training experience). The core of\nour approach is to learn an abstract model of the environment in terms of Neuro-Symbolic Predi-\ncates (NSPs, see Fig. 1), which are snippets of Python code that can invoke vision-language models\n(VLMs) for querying perceptual properties, and further algorithmically manipulate those properties\nusing Python, in the spirit of ViperGPT and VisProg (Sur\u00eds et al., 2023; Gupta & Kembhavi, 2022).\nIn contrast, traditional robot task planning uses hard-coded symbolic world models that cannot adapt\nto novel environments (Garrett et al., 2021; Konidaris, 2019). Recent works pushed in this direction\nwith limited forms of learning that restrict the allowed perceptual and logical abstractions, and which\nfurther require demonstration data instead of having the robot explore on its own (Silver et al., 2023;\nKonidaris et al., 2018). The representational power of Neuro-Symbolic Predicates allows a much\nbroader set of perceptual primitives (essentially anything a VLM can perceive) and also deeper\nlogical structure (in principle, anything computable in Python).\nYet there are steep challenges when learning Neuro-Symbolic Predicates to enable effective plan-\nning. First, the predicates must be learned from input pixel data, which is extremely complex and\npotentially noisy. Second, they should not overfit to the situations encountered during training, and\ninstead zero-shot generalize to complex new tasks at test time. Third, we need an efficient way of\nexploring different possible plans to collect the data needed to learn good predicates. To address"}, {"title": "PROBLEM FORMULATION", "content": "We consider the problem of learning state abstractions for robot planning over continuous state/ac-\ntion spaces, and doing so from online interaction with the environment, rather than learning from\nhuman-provided demonstrations. We assume a predefined inventory of basic motor skills, such as\npick/place, and also assume a basic object-centric state representation (explained further below),\nwhich is a common assumption (Kumar et al., 2024; Silver et al., 2023; 2022). The goal is to learn\nstate abstractions from training tasks that generalize to held-out test tasks, enabling the agent to\nsolve as many test tasks as possible while using minimal planning budget.\nTasks. A task $T$ is a tuple $(O, x_0, g)$ of objects $O$, initial state $x_0$, and goal $g$. The allowed states\ndepend on the objects $O$, so we write the state space as $X_O$ (or just $X$ when the objects are clear\nfrom context). Each state $x \\in X_O$ includes a raw RGB image and associated object features, such\nas 3D object position.\nEnvironments. Tasks occur within an environment $\\mathcal{E}$, which is a tuple $(U, C, f, A)$ where $U \\subset \\mathbb{R}^M$\nis a low-level action space (e.g. motor torques), $C$ is a set of controllers for low-level skills (e.g.\npick/place), $f : X \\times U \\rightarrow X$ is a transition function, and $A$ is a set of object types (possible outputs\nof an object classifier). The environment is shared across tasks."}, {"title": "NEURO-SYMBOLIC PREDICATES", "content": "Neuro-Symbolic Predicates (NSPs) represent visually grounded yet logically rich abstractions that\nenable efficient planning and problem solving. As Figure 2 illustrates, these predicates are neuro-\nsymbolic because they combine programming language constructs (conditionals, numerics, loops\nand recursion) with API calls to neural vision-language models for evaluating visually-grounded\nnatural language assertions. NSPs can be grounded in visual perception, and also in proprioceptive\nand object-tracking features, such as object poses, common in robotics (Kumar et al., 2024; 2023b;\nCurtis et al., 2022; 2024b). We consider two classes of NSPs: primitive and derived. Primitive NSPs\nare evaluated directly on the raw state, such as Holding (obj) (which would use VLM queries)\nor GripperOpen (which would use proprioception). Derived NSPs instead determine their truth\nvalue based on the truth value of other NSPs, analogous to derived predicates in planning (Thi\u00e9baux\net al., 2005; McDermott et al., 1998).\nPrimitive NSPs. We provide a Python API for computing over the raw state, including the ability\nto crop the image to particular objects and query a VLM in natural language. See Appendix A.\nDerived NSPs. Instead of querying the raw state, a derived NSP computes its truth value based only\non the truth value of other NSPs. Derived NSPs handle logically rich relations, such as OnPlate\nin fig. 2, which recursively computes if a block is on a plate, or on something that is on a plate.\nEvaluating Primitive NSPs. No VLM is 100% accurate, even for simple queries like \u201cis the robot\nholding the jug?\", especially in partially observable environments. To increase the accuracy and\nprecision of NSPs, we take the following two measures.\nFirst, because a single image may not uniquely identify the state (e.g. due to occlusion), we provide\nextra context to VLM queries. Consider a robot whose gripper is next to a jug, but whose own\narm occludes the jug handle, making it uncertain whether the jug is held by the gripper or merely\nnext to it. Knowing the previous action (e.g. Pick(jug)) helps resolve this uncertainty. We\ntherefore further condition NSPs on the previous action, as well as the previous visual observation\n(immediately before the previous action was executed) and previous truth values for the queried\nground atom.\nSecond, we visually label each object in the scene by overlaying a unique ID number on each\nobject in the RGB image (following Yang et al., 2023). That way, to evaluate for example\nHolding (block2), we can query a VLM with \"the robot is holding block2\", where block2\""}, {"title": "HIERARCHICAL PLANNING", "content": "We use the learned abstract world model to first make a high-level plan (sequence of HLAs), which\nthen yields a low-level action sequence by calling the corresponding skill policy for each HLA.\nHigh-level planning leverages widely-used fast symbolic planners, which, for example, conduct A*\nsearch with automatically-derived heuristics (e.g. LM-Cut, Helmert & Domshlak, 2009).\nHowever, there may be a mismatch between a high-level plan, which depends on potentially flawed\nabstractions, and its actual implementation in the real world. Learning is driven by these failures.\nMore precisely, hierarchical planning can break down in one of two ways:\nPlanning Failure #1: Infeasible. A high-level plan is infeasible if one of its constituent skills fails\nto execute.\nPlanning Failure #2: Not satisficing. A high-level plan is not satisficing if its constituent skills\nsuccessfully execute, but do not achieve the goal.\nWhen solving a task we generate a stream of high-level plans and execute each one until a satisficing\nplan (achieving the goal) is generated, or until hitting a planning budget $n_{abstract}$."}, {"title": "LEARNING AN ABSTRACT WORLD MODEL FROM INTERACTING WITH THE\nENVIRONMENT", "content": "Algorithm 1 shows how we interleave\nlearning predicates (state abstraction),\nlearning HLAs (abstract transition func-\ntion), and interacting with the environ-\nment. The learner takes in an en-\nvironment $\\mathcal{E}$, a set of training tasks\n$\\mathcal{T}$, an initial predicate set $\\Psi_0$ (which\nis usually the goal predicates), an ini-\ntial set of HLAS $\\Omega_0$ (which are largely\nempty, section 5.1), and an initial dataset\n$\\mathcal{D}$ (empty, except when doing transfer\nlearning from earlier environments). It\ntracks its learning progress using $\\Psi_{best}$,\nthe highest training solve rate, and $\\Omega_{best}$,\nthe lowest number of infeasible plans."}, {"title": "EXPLORATION", "content": "Our agent explores the environment by planning with its current predicates/HLAs, and executing the\nplans. The agent is initialized with underspecified, mostly empty HLA(s) (that is, the preconditions\nand effects are mostly empty sets, except with goal predicates in the effects if appropriate, so that\nthe planner can generate plans). It collects data by trying to solve the training tasks (generate and\nexecute abstract plans until the task is solved or $n_{abstract}$ plans are used, as described in section 4) and\ncollects positive transition segments (from successfully-executed skills), negative state-action tuples\n(from skills that failed to execute successfully) and satisficing plans, if any."}, {"title": "PROPOSING PREDICATES", "content": "We introduce three strategies for prompting VLMs to invent predicates \u2014 two that are conditioned\non collected data, and one that is not (see appendix B.3 for further details).\nStrategy #1 (Discrimination) helps discover predicates that are good preconditions for the skills.\nWe prompt a VLM with example states where a skill succeeded and failed, and ask it to generate\ncode that predicts when the skill is applicable.\nStrategy #2 (Transition Modeling) helps discover predicates helpful for postconditions. We\nprompt a VLM with before (or after) snapshots of successful skill execution, and ask it to gener-\nate code that describes properties that changed before (or after, respectively).\nStrategy #3 (Unconditional Generation) prompts VLMs to propose new predicates as logical ex-\ntensions of existing ones (whether built-in or previously proposed), without conditioning on the raw\nplanning data. This approach helps create derived predicates."}, {"title": "SELECTING A PREDICATE SET", "content": "VLM-generated predicates typically have low precision\u2014not all generations are useful or sensible\u2014\nand too many predicates will overfit the model to what little data it has collected. One solution\ncould be the propose-then-select paradigm (Silver et al., 2023). Silver et al. (2023) propose an\neffective predicate selection objective but requires around 50 expert plan demonstrations. We assume\nno demonstration data, and in general, we might not find any satisficing plans early in learning.\nTherefore we need a new way of learning from unsuccessful plans.\nTo address this, we devise a novel objective that scores a set of predicates $\\Psi$ based on classification\naccuracy, plus a simplicity bias. The classification score is obtained by first learning HLAs using the\nset of predicates $\\Psi$ (discussed more in section 5.4), and then computing the classification accuracy\nof the HLAs (see Appendix B.2). Later in learning, after discovering enough (a hyperparameter one\ncan choose) satisficing plans, we switch to the objective of Silver et al. (2023), which takes planning\nefficiency and simplicity into account.\nWe perform a greedy best-first search with either score function as the heuristic. It starts from the\nset of goal predicates $\\mathcal{I}_G$ and adds a single new predicate from the proposed candidates at each step,\nand finally returns the set of predicates with the highest score."}, {"title": "LEARNING HIGH-LEVEL ACTIONS", "content": "We further learn high-level actions $\\Omega$, which define an abstract transition model, in the learned\npredicate space, from interactions with the environment. We follow the cluster and intersect operator\nlearning algorithm (Chitnis et al., 2022) and improve its precondition learner for more efficient\nexploration and better generalization. Chitnis et al. (2022) assume given demonstration trajectories\nand learns restricted preconditions so that the plans are most similar to the demonstrations. Our agent\nexplores the environment from scratch and does not have demonstration data to follow restrictively.\nOn the other hand, our agent needs more optimistic world models to explore unseen situations to\nsolve the task. Our precondition learner ensures that each data in the transition dataset is modeled by\none and only one high-level action and minimizes the syntactic complexity of the HLA to encourage\noptimistic world models. See Appendix B.1 details."}, {"title": "EXPERIMENTS", "content": "We design our experiments to answer the following questions: (Q1) How well does our NSP rep-\nresentation and predicate invention approach compare to other state-of-the-art methods, including\npopular HRL or VLM planning approaches? (Q2) How do the abstractions learned by our method\nperform relative to manually designed abstractions and the abstractions before any learning? (Q3)\nHow effective is our NSP representation compared to traditional symbolic predicates, where classi-\nfiers are based on manually selected object features? (Q4) What is the contribution of our extended\noperator learning algorithm to overall performance?"}, {"title": "Experimental Setup.", "content": "We evaluated seven different approaches across five robotic environments\nsimulated using the PyBullet physics engine (Coumans & Bai, 2016). Each result is averaged over\nfive random seeds, and for each seed, we sample 50 test tasks that feature more objects and more\ncomplex goals than those encountered during training. The agent is provided with 5 training tasks in\nthe Cover and Coffee environments, 10 tasks in Cover Heavy and Balance, and 20 tasks in Blocks.\nThe planning budget $n_{abstract}$ is set to 8 for all domains except Coffee, where it is set to 100."}, {"title": "Environments.", "content": "We briefly discuss the environments used, with more details in appendix D.\n1. Cover. The robot is tasked with picking and placing specific blocks to cover designated regions\non the table, using Pick and Place skills. Training tasks involve 2 blocks and 2 targets, while test\ntasks increase the difficulty with 3 blocks and 3 targets.\n2. Blocks. The robot must construct towers of blocks according to a specified configuration, using\nPick, Stack, and PlaceOnTable skills. The agent is trained on tasks involving 3 or 4 blocks and\ntested on more challenging tasks with 5 or 6 blocks.\n3. Coffee. The robot is tasked with filling cups with coffee. This involves picking up and placing\na jug into a coffee machine, making coffee, and pouring it into the cups. The jug may start at\na random rotation, requiring the robot to rotate it before it can be picked up. The environment\nprovides 5 skills: Twist, Pick, Place, TurnMachineOn, and Pour. Training tasks involve filling 1\ncup, while test tasks require filling 2 or 3 cups.\n4. Cover Heavy. This is a variant of Cover with \"impossible tasks\" which asks the robot to pick\nand placing white marble blocks that are too heavy for it to pick up. The environment retains\nthe same controllers and number of objects as the standard Cover environment. An impossible\ntask is considered correctly solved if the agent determines that the goal is unreachable with its\nexisting skills (i.e., no feasible plan can be generated).\n5. Balance. In this environment, the agent is tasked with turning on a machine by pressing a button\nin front of it, but without prior knowledge of the mechanism required to activate it (in this case,\nbalancing an equal number of blocks on both sides). The agent has access to a PressButton skill,\nalong with the skills from the Blocks domain. Training tasks involve 2 or 4 blocks, while test\ntasks increase the difficulty with 4 or 6 blocks."}, {"title": "Approaches.", "content": "We compare our approach against 5 baselines and manually designed state abstraction.\n1. Ours. Our main approach.\n2. MAPLE. a HRL baseline that learns to select high-level action by learning a Q function, but\ndoes not explicit learn predicates and perform planning. This is inspired by the recent work on\nMAPLE (Nasiriany et al., 2022b). While we have extended the original work with the capacity of\ngoal-conditioning, the implementation is still not able to deal with goals involving more objects"}, {"title": "Results and Discussion.", "content": "Figure 4 presents the evaluation task solve rate and the planning budget\nutilized. Examples of an online learning trajectory with invented predicates, instances of learned\nabstractions, and further planning statistics (such as node expanded and walltime) are provided in\nappendix C.\nOur approach consistently outperforms the HRL and VLM planning baselines, MAPLE and ViLa,\nacross all tested domains, achieving near-perfect solve rates (Q1). With similar amounts of inter-\naction data, MAPLE struggles to perform well, even on tasks within the training distribution. This\nlimitation could potentially be mitigated with significantly larger datasets, though this is often im-\npractical in robotics due to the high cost of real-world interaction data and the sim-to-real gap in\ntransferring simulation-trained policies. ViLa demonstrates limited planning capabilities, which is\nconsistent with recent observations (Kambhampati et al., 2024). While it performs adequately on\nsimple tasks like Cover, where the robot picks and places blocks, its performance drops significantly\nwhen blocks are initialized in the robot's grasp, as it tends to redundantly attempt picking actions.\nThis behavior suggests overfitting. In more complex domains, ViLa often generates infeasible plans,\nsuch as attempting to pick blocks from a stack's middle or trying to grasp a jug without consider-\ning its orientation. We think introducing demonstrations or incorporating environment interactions\ncould potentially alleviate these issues.\nOur approach significantly outperforms No invent, demonstrating the clear benefits of learning pred-\nicate abstractions over relying on initial underspecified representations. It achieves similar solve\nrates and efficiency to the Oracle baseline, which uses manually designed abstractions (Q2). This\nunderscores the ability of our method to autonomously discover abstractions as effective as those\ncrafted by human experts.\nAddressing (Q3), while Sym. pred. performs well in simple domains like Cover, it struggles to\ninvent predicates that require grounding in perceptual cues not explicitly encoded in object features."}, {"title": "RELATED WORKS", "content": "Hierarchical Reinforcement Learning (HRL) HRL tackles the challenge of solving MDPs with\nhigh-dimensional state and action spaces, common in robotics, by leveraging temporally extended,\nhigh-level actions (Barto & Mahadevan, 2003). The Parameterized Action MDPs (PAMDPs) frame-\nwork (Masson et al., 2016) builds on this by integrating discrete actions with continuous parameters,\noptimizing both the action and its parameterization using the Q-PAMDP algorithm. MAPLE (Nasiri-\nany et al., 2022a) further builds on this by using a library of behavior primitives, such as grasping\nand pushing, combined with a high-level policy that selects and parameterize these actions. We\nimplement a version of this with the extension of goal-conditioned high-level policy as a baseline.\nGenerative Skill Chaining (GSC) (Mishra et al., 2023) further improves long-horizon planning by\nusing skill-centric diffusion models that chain together skills while enforcing geometric constrains.\nDespite these advancements, they still face challenges in sample complexity, generalization, and\ninterpretability.\nLarge Pre-Trained Models for Robotics With the rise of large (vision) language models (VLMs),\nmany works explore their application in robotic decision making. RT-2 (Brohan et al., 2023) treats\nrobotic actions as utterances in an \"action language\" learned from web-scale datasets. SayCan and\nInner Monologue (Ahn et al., 2022; Huang et al., 2022) use LLMs to select skills from a pretrained\nlibrary based on task prompts and prior actions. Code as Policy (Liang et al., 2023) prompts LLMs to\nwrite policy code that handles perception and control. Recent works extend this to bilevel planning\n(Curtis et al., 2024a), but do not learn new predicates. ViLa (Hu et al., 2023) queries VLMs for\naction plans, executing the first step before replanning. We implement an open-loop version of ViLa\nto compare with its initial planning capabilities.\nLearning Abstraction for Planning Our work builds on a rich body of research focused on learning\nabstractions for planning. Many prior works have explored offline methods such as learning action\noperators and transition models from demonstrations using existing predicates (Silver et al., 2021;\nChitnis et al., 2022; Pasula et al., 2007; Silver et al., 2022; Kumar et al., 2023a). While Silver et al.\n(2023) explore learning predicates grounded in object-centric features, our approach goes further\nby inventing open-ended, visually and logically rich concepts, without relying on hand-selected fea-\ntures. Additionally, unlike their demonstration-based approach, ours learns purely online. Konidaris\net al. (2018) and consequent works (James et al., 2022; 2020) discover abstraction in an online\nfashion by leveraging the initiable and terminations set of operators that satisfy an abstract subgoal\nproperty. James et al. (2020) incorporate an egocentric observation space to learn more portable\nrepresentations, and James et al. (2022) define equivalence of options effects on objects to derive\nobject types for better transferability. Nevertheless, they work on a constrained class of classifiers\n(such as decision trees or linear regression with feature selection), which limits the effectiveness and\ngeneralizability of learned predicates. Kumar et al. (2024) performs efficient online learning, but\nfocuses on sampler learning rather than predicate invention."}, {"title": "CONCLUSION", "content": "In this work, we introduced Neuro-Symbolic Predicates (NSPs), a novel representation that com-\nbines the flexibility of neural networks to represent open-ended, visually grounded concepts, and\nthe interpretability and compositionality of symbolic representations, for planning. To support this,\nwe developed an online algorithm for inventing NSPs and learning abstract world models, which\nallows efficient acquisition of NSPs. Our experiments across five simulated robotic domains demon-\nstrated that our method outperforms existing approaches, including hierarchical reinforcement learn-\ning, VLM planning, and traditional symbolic predicates, particularly in terms of sample efficiency,\ngeneralization, and interpretability. Future work will focus on incorporating recovery mechanisms\nfor failed plans, relaxing assumptions about skills, enhancing exploration efficiency, and scaling to\npartially observable and real-world domains."}, {"title": "ADDITIONAL DETAILS ABOUT THE ONLINE INVENTION ALGORITHM", "content": "We aim to learn high-level actions $\\Omega$, which define an abstract transition model in the\nlearned predicate space, from interactions with the environment. These interactions con-\nsist of executing high-level plans, which are sequences of (grounded) HLAS $\\omega_1,..., \\omega_n$\n(i.e. HLAs applied to concrete objects). Our learned abstract transition model should\nboth fit the transition dataset while being optimistic for efficient exploration (Tang et al.,\n2024). Recalling the definitions from sec. 2, given the current transition dataset, $\\mathcal{D} =$\n{\\dots, $(x^{(k)}, \\pi^{(k)}, x^{(k')}), \\dots, (x^{(k')}, \\pi^{(k')}, FAIL), \\dots}, we first transform it into the learned ab-"}, {"title": "COMPARING WITH THE CLUSTER-AND-INTERSECT ALGORITHM", "content": "Compared with the cluster and intersect operator learner (Chitnis et al., 2022), which simply inter-\nsecting over feasible states to build preconditions for each high-level action, our method optimisti-\ncally enlarges the set of feasible states for each high-level actions using the minimum complexity\nobjective, while still retaining the abilities to distinguish infeasible states. The optimistic objective\nis critical for predicate invention by interactions where optimal demonstration trajectories are not\navailable. Using the intersection method, the agent will only consider the feasible states in the cur-\nrently curated dataset as feasible and never try the skill in other states that are potentially feasible as\nwell. Planners usually fail to find plans with such restrictive world models, resulting in inefficient\nrandom exploration and poor test-time performance."}, {"title": "RESTRICTED PRECONDITIONS", "content": "The restricted preconditions are less generalizable as well. For example, for agents learning making\ncoffee in environments with one cup, the agent will find successful trajectories such as PutKettleIn-\nCoffeeMachine, MakeCoffee, and PourCoffeeInCup. Using the intersection method, the agent sets\nthe preconditions of PourCoffeeInCup as KettleInMachine and KettleHasCoffee as both of them are\nalways true among feasible states of the PourCoffeeInCup action, even though only KettleHasCoffee\nis needed. The more restricted preconditions are problematic when generalizing to environments\nwith more than one cups. The agent keeps putting the kettle back to the machine before pouring\nthe coffee for another cup, as the learned PourCoffeeInCup action has KettleInMachine as part of"}, {"title": "CLASSIFICATION-ACCURACY-BASED PREDICATE SETS SCORE FUNCTION", "content": "When no satisficing plan is found in early iterations of predicate invention (e.g., in Coffee), the\nobjective from Silver et al. (2023) is inapplicable. This issue is particularly prominent when the\nspace of possible plans is large (i.e., when there are many potential actions at each step and achieving\ngoals requires long-horizon plans). To address this, we introduce a predicate score function that does\nnot rely on satisficing plans. We propose an alternative objective based on classification accuracy,\nin the same flavour as the score function defined earlier for operator preconditions.\nFormally, given $\\mathcal{D} =$\n{\\dots, $(s^{(k)}, \\pi^{(k)}, s^{(k')}), \\dots, (s^{(k')}, \\pi^{(k')}, FAIL), \\dots}, where $s=$\n$ABSTRACT(x)$ as above, we denote the collection of all success transitions and failed tuples as\n$\\mathcal{D^+} = \\{(s^{(k)}, \\pi^{(k)}, s^{(k')})\\}$ and $\\mathcal{D^-} = \\{(s^{(k)}, \\pi^{(k)}, FAIL)\\}$ respectively. The the predicate set score"}, {"title": "PROMPTING FOR PREDICATES", "content": "Strategy #1 (Discrimination). is motivated by one of the primary functions of predicates-have\nthem in the preconditions of operators to distinguishing between the positive and negative states\nso the plans the agent find are feasible. However, we observed that existing VLMs often struggle\nto reliably understand and identify the differences between positive and negative states, especially\nwhen dealing with scene images that deviate significantly from those seen during training. This\nlimitation motivates our second strategy.\nStrategy #2 (Transition Modeling). With the observation that predicates present in an action's\npreconditions often also appear in some actions' effects. We prompt the VLM to propose predicates\nthat describe these effects based on the positive transition segments it collects. This task is usually\neasier for VLMs because it involves identifying the properties or relationships that have changed\nfrom the start state to the end state, given the information that an action with a natural language\nname (such as pick) has been successfully executed. However, this strategy alone is not exhaustive.\nCertain predicates may exist solely within preconditions but not effects (e.g., an object's material\nthat remains unchanged). Therefore, this method complements S1 and is used alternately with it\nduring the invention iterations."}]}