{"title": "Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval", "authors": ["Kyle Buettner", "Adriana Kovashka"], "abstract": "There is a scarcity of multilingual vision-language models that properly account for the perceptual differences that are reflected in image captions across languages and cultures. In this work, through a multimodal, multilingual retrieval case study, we quantify the existing lack of model flexibility. We empirically show performance gaps between training on captions that come from native German perception and captions that have been either machine-translated or human-translated from English into German. To address these gaps, we further propose and evaluate caption augmentation strategies. While we achieve mean recall improvements (+1.3), gaps still remain, indicating an open area of future work for the community.", "sections": [{"title": "Introduction", "content": "Vision-language models (VLMs) such as CLIP (Radford et al., 2021) are predominantly limited to use in English as a result of the pretraining supervision consisting mostly of English captions. This trend naturally poses an accessibility barrier for non-English speakers. Furthermore, cultures around the world differ in their salient concepts (Liu et al., 2021) and visual perception (Nisbett and Masuda, 2013). Relying on English supervision in pretraining thus hinders consideration of cross-cultural concepts in object-based tasks such as recognition, detection, and image-text retrieval. Example cultural differences present in language with respect to object specificity and importance. For example, past literature (Nisbett and Masuda, 2013) describes differences in how cultures perceive members of an object group (e.g. penguins within the group of birds), indicating that certain groups have stronger associations for specific rather than general object terms. Experiments in Nisbett and Masuda (2013) also illustrate differences between East Asians and Americans with respect to the perceived importance of background objects and context as opposed to foreground objects. Different cultures notice different objects more; perceptual differences may manifest in objects being included/excluded in a caption, and different objects being relevant in tasks."}, {"title": "Background and Related Work", "content": "Cultural differences in perception. Prior work considers how culture may influence perception and expression. For example, Western and East Asian cultural differences are found to manifest in visual attention, e.g. Americans appear to pay more attention to foreground/objects than East Asians, but conversely for background/context (Nisbett and Masuda, 2013). Furthermore, Boroditsky (2006) describes empirical studies that indicate that different cultures group objects differently (e.g. based on shape or material) and ascribe different properties to objects, because of unique grammar (e.g. gendered nouns). Since German uses gendered nouns, this observation may manifest in native German captions (and retrieval) as objects being described with unique attributes. The work of Berthele et al. (2015) notes that Germanic language speakers describe object relationships with notably specific spatial information (e.g. posture/manner information in addition to object relationships). Hofstede (2001) conducts analysis to show that there are cultural differences between Germany and United States in terms of individualism vs. collectivism, which could impact the perception of visual content as argued by Nisbett and Masuda (2013). Further examples can be found in work on linguistic relativity (Kay and Kempton, 1984).\nMultilingual multimodal modeling. Our work aligns with works that extend VLMs for use in languages besides English (Chen et al., 2022, 2024; Carlsson et al., 2022; Chen et al., 2023). Models notably often rely on translations, and works do not have analysis into performance differences between translations and captions of native perception. In contrast, K\u00e1d\u00e1r et al. (2018) and our work show differences in retrieval performance when captions are natively written in a language or translated into that language from English. Our work differs from K\u00e1d\u00e1r et al. (2018) as we also explore machine translation, with a more modern VLM (Chen et al., 2023). We also explicitly address the lack of techniques to overcome gaps by experimenting with paraphrasing augmentations. Our strategies are related to past paraphrasing work (Wieting and Gimpel, 2018; Hu et al., 2019), but these approaches use machine translation to generate large-scale English paraphrase datasets, while we leverage in-context learning and LLMs to generate paraphrases for use as input to machine translation to enhance diversity. We are inspired by Fan et al. (2024), as the work shows zero-shot image classification improvements with LLM-based caption rewriting.\nData-wise, we explore Multi30K (Elliott et al., 2016) as it contains native German captions and parallels the English Flickr30K captions (Young et al., 2014). XM3600 (Thapliyal et al., 2022) also provides natively perceived captions, in 36 languages for 3,600 images. Due to size, we do not train with this set, though we provide initial analysis on it to inspire future work. WebLI (Chen et al., 2022) is another dataset that contains crawled captions in 109 languages, though it is proprietary."}, {"title": "Experimental Methodology", "content": "We benchmark training with captions that reflect native perception by German speakers, ones that have been machine-translated from captions reflecting English speaker perception, and ones human-translated from English speaker perception. We also test strategies to improve upon translation."}, {"title": "Benchmarking Details", "content": "Task. We evaluate on German image-text (I2T) and text-image (T2I) retrieval. The German captions used in eval are written directly by native speakers about images. They are not translated from English and represent natural non-English perception.\nData. English data is from Flickr30K (Young et al., 2014), and German data is from Multi30K (Elliott et al., 2016). Flickr30K contains 31,014 images that are annotated with 5 independently written English captions per image. Likewise, Multi30K provides 5 independently written German captions for the same images. These German captions are collected from 185 native speakers using a similar interface to Flickr (Hodosh et al., 2013). Multi30K also provides professional German translations. In particular, for each image, 1 of the 5 English captions is sampled from Flickr30K, and professional translators produce corresponding captions in German (just from source text, not using the images). We refer to the separate caption sets as Independently Written (5 sets for each language) and Human-Translated (1 set per language). For all sets, we randomly split data to create a disjoint reference set (9,666 samples) to be used with our strategies (Sec. 3.2), as well as retrieval train/val/test sets (9,666/1,014/10,668 samples respectively).\nModeling. We explore mCLIP (Chen et al., 2023), an approach which has made CLIP multilingual through knowledge distillation-based training of projector modules and replacement of CLIP's text encoder with the multilingual text encoder XLM-R (Conneau et al., 2020). We finetune mCLIP with images and captions for German I2T and T2I retrieval. For experimentation that involves machine-translating English captions to German, we use opus-mt-en-de (Tiedemann and Thottingal, 2020) from Hugging Face. With this model, we use a deterministic setting, where tokens are generated according to highest token probability, and infer at most 40 tokens for each caption. mCLIP models are trained for 30 epochs on 1 Quadro RTX 5000 GPU with batch size 16 and learning rate 0.0005.\nMetric. We report mean recall as in Chen et al. (2023). Recall@1,5,10 is computed for both T2I and I2T retrieval on each native German test set (5 sets total). Mean recall is the average of these six values. We further average over each set."}, {"title": "Methods Compared", "content": "Baseline finetuning strategies include:\n\u2022 ENG, a \"lower bound\": finetuning using data natively provided in English (in the Independently Written sets). Since there are 5 sets of captions, we average over trials using each set for training.\n\u2022 ENG2GER-MT: finetuning on German sentences that have been translated from English using an English-to-German machine translation model (Tiedemann and Thottingal, 2020). English sentences come from the Human Translation set.\n\u2022 ENG2GER-MT (TRN): same as above, but the translation model is further trained on captions from the Multi30K disjoint reference split we create, with the intuition that translation finetuning may capture caption differences. We train for 10 epochs with learning rate 0.00001 and batch size 16, using the Human Translation pairs.\n\u2022 ENG2GER-HT: finetuning on German captions translated from English by professional annotators (in the Human Translation set). This training is different from and expected to perform worse than native German, but better than naive translation.\n\u2022 GER: finetuning using data natively provided from German perception (in the Independently Written sets). Since there are 5 sets of captions, we average over trials using each set for training.\nStrategies: We find significant gaps between these methods, notably ENG2GER-MT and GER, motivating experimentation with potential improvements. We test adding training data that has been augmented in English then translated to German. Some proposed changes involve object names, so for this purpose, we define an object vocabulary V including COCO object terms (Lin et al., 2014). Category detection involves consideration of these terms, synonyms (Lu et al., 2018), plurals, and word sense. For each strategy, mCLIP is trained as in ENG2GER-MT, but with an augmented dataset of captions added. Methods include:\n\u2022 HYPER: After identifying each COCO class with a synset id, if available, object mentions are hypernymized to be a random term above it in the WordNet hierarchy (Miller, 1995). Our goal is to improve robustness to changes in object naming to address challenges in object specificity.\n\u2022 PARA-RND (paraphrase-random): Before translation, we ask LLaMA-3 (Touvron et al., 2023) to write each caption in a structurally different manner while maintaining meaning. We are motivated by Fan et al. (2024) which shows English retrieval benefits from diversification. Our approach differs as we diversify before translation to guide translation to more generalizable descriptions.\n\u2022 PARA-TGT (paraphrase-targeted): We ask LLaMA-3 to paraphrase each caption using examples of object naming \u201cstyle\u201d. For each caption, a total of k=100 captions are randomly sampled from the reference split of the first native German set, such that if possible, sampled captions share at least one non-person object mention with the current caption (since most captions mention people). Translations of these are provided in the LLaMA-3 prompt as examples. Then for the input caption, LLaMA-3 is instructed to find relevant noun phrases, and to convert the noun phrases to more aligned representations based on the examples.\n\u2022 PARA-CMB combines both sets above.\nPlease refer to the appendix for prompt details."}, {"title": "Key Findings", "content": "In the top block of , zero-shot mCLIP is shown to achieve the lowest recall (24.5). Finetuning mCLIP with English Multi30K data improves performance to 26.9 (+2.4). English data can help to a degree on German retrieval due to alignment learned in pretraining the multilingual text encoder. However, much more significant gains are achieved when the finetuning data is in German. Training with German data that has been translated from English using an off-the-shelf translation model (ENG2GER-MT) reaches 33.4 (second block). Compared to human translation (ENG2GER-HT - fourth block), there is a notable gap from machine translation (3.4), and finetuning the translation model only bridges this gap by 0.6. These results indicate existing challenges with off-the-shelf translation for retrieval. Then most significantly, the gap between off-the-shelf translation and native German captions (GER) is 5.0. There is a notable gap between professional translation (ENG2GER-HT) and GER (1.6), which we reason is the gap due to differences in English and German perception. For example, these gaps could be due to specificity and importance differences. Expert translation does not address these factors.\nIn the third block, our methods are found to be somewhat effective for bridging the gap between ENG2GER-MT and GER. HYPER improves the result by 0.3, and PARA-RND and PARA-TGT by 0.7. These models are notably more appropriate for low-resource target languages than ENG2GER-MT (TRN) since they use no/few reference captions compared to translation finetuning. Further combining random and targeted paraphrasing results in the largest gain of 1.3. The result is still 3.7 away from GER. Addressing differences in the perception of the visual world and the way captions are written across cultures is thus an open challenge."}, {"title": "Further Analysis", "content": "Object mentions in English/German captions. To analyze possible differences in perception, we analyze object mention frequency in Flickr30K/Multi30K. We specifically translate German captions to English and extract nouns in both (original) English and (translated to English) German captions. The ratio of English and German mentions is about 1.5, i.e. English mentions object nouns 50% more often than German. However, counts vary by object type. For example, English mentions clothing more often (pants-143% more, shirt-112%, hat-60%, jacket-43%), and German mentions furniture more often (table-37% more, bed-20%, bench-15%). These languages also vary in granularity: English captions often say \"people\", while German ones say \u201cworkers\u201d, \u201cathletes\", etc.\nAnalysis of other languages. We conduct initial analysis of the languages and captions in XM3600 (Thapliyal et al., 2022). We group XM3600 languages into European, Arabic/Farsi, Hindi/Bengali, Indonesian/Thai, East Asian, and Swahili categories. After translating each language to English, we report average mention counts and standard deviations per group for various common objects. Language groups show large differences in terms of how commonly they mention elements of nature (e.g. mountains, trees), scenery (streets, buildings), household objects (table, plate, box, bottle), and the gender of portrayed people. It is also found that the difference between objects counts across languages is much greater than within-group standard deviations. Such results suggest differences in supervision worthy of exploration.\nParaphrasing. LLaMA picks up on granularity differences. For example, PARA-TGT changes \"Man in a red shirt riding his bicycle\" to \"A bicyclist in a red shirt is riding\". Further, LLaMA transforms \"man on skis\u201d into \u201cskier\u201d, \u201cperson in blue and red ice climbing\" into \u201cice climber\u201d, and \u201cmen with children\" into \u201cfamily\u201d. The model tends to simplify, irrespective of the reference. For example, \"Two young people are approached by a flamboyant young woman dressed in a red bikini and a red feathered headress\u201d becomes \u201cTwo young people are approached by a bikini-clad woman\". Paraphrasing could thus result in over-simplification.\nHuman evaluation. We extend quantification past retrieval by asking two German speakers to gauge the likelihood that captions are made by a German speaker and their naturalness. We provide 50 random captions for each of 3 sets (ENG2GER-MT, ENG2GER-HT, PARA-TGT). Speakers do not know each set's identity and are tasked with scoring captions as 3=great, 2=good, 1=bad. On average, the speakers rate ENG2GER-HT the highest with a mean ternary score of 2.73 and mean binary score (great/good=1, bad=0) of 0.97. For PARA-TGT, the ternary score is 2.19 and binary score is 0.79. For ENG2GER-MT, the ternary score is 2.16 and binary score is 0.77. These differences approximately reflect the recall results in .\nRecognition. To evaluate object recognition, we compare objects mentioned in a native German caption to ones predicted by the models GER and ENG2GER-HT. We take predictions to be ones with CLIP scores greater than a threshold (the one in range 10:5:50 that maximizes val F1). A prediction is correct only if the object is mentioned in native German."}, {"title": "Conclusion", "content": "We show notable differences in using native vs. translated German captions to train a retrieval model, and experiment with three strategies to reduce the gaps. We plan to extend investigation to more languages. Future work can also involve creation of data augmentation strategies that take inspiration from psychology literature (Nisbett and Masuda, 2013; Boroditsky, 2006) and solutions for the ambiguity challenges of machine translation, such as by using images (Futeral et al., 2023)."}, {"title": "Limitations and Ethical Considerations", "content": "We only experiment with one translation model, one non-English language (German), and a small amount of runs of LLaMA-3. To ensure that insights generalize, various models, and languages (especially low-resource ones), should be analyzed. There may be intra-language variance amongst native speakers that should also be considered.\nWe rely on the use of image-caption datasets like Flickr30K and Multi30K. These datasets are relatively small (about 30k samples), so the coverage of concepts may not be fully representative of spoken language. Such datasets have also been noted to contain harmful biases with respect to attributes like race and gender (Van Miltenburg, 2016). The use of models like LLaMA-3 carries similar biases. There should be careful consideration regarding downstream usage of these sets and models. We note that a future extension of our paraphrasing strategies could be to mitigate the impact of in-group perspectives in the captions used for pretraining models.\nOur analysis of differences in languages is limited by the fact that languages are machine-translated to English. It is possible that some differences are amplified and/or missed due to machine translation artifacts.\nFinally, while we conduct initial human evaluation, we encourage larger-scale human evaluation that expands past our limited evaluation. This can be done to ensure that methods are applicable for a greater amount of people."}, {"title": "Appendix", "content": "Shown are the prompt templates used for querying LLaMA-3 (meta-llama/Meta-Llama-3-8B-Instruct on Hugging Face). We do not experiment with LLaMA sampling settings and generate outputs with default parameters.\nPara-Rnd Prompt Template\nRewrite captions in a structurally different manner, while closely maintaining semantic meaning. Return as Python string. Return no other text.\nPara-Tgt Prompt Template\n1) Given a caption, 1st decompose into noun phrases, keeping all phrase content (e.g. adjectives) aside from articles. EX: \"A person is riding a blue bicycle down the street on a sunny day.\" Noun Phrases: [\"person\", \"blue bicycle\", \"street\", \"sunny day\"]\n2) Based on a provided reference list of related captions, construct a new set of noun phrases that alters the original noun phrases to be in the common styles/forms shown in the reference list. EX: If many captions say \"bicyclist\u201d, combine \u201cperson\" and \u201cblue bicycle\" into \u201cbicyclist\u201d. Do not infer unnecessary information.\n3) Finally, combine the new noun phrases back into a sentence, keeping the same semantics as the original caption. EX: \"A bicyclist is traveling down the road on a sunny day.\"\nHere is your reference caption list:\n{refcaps}\nNow run each steps 1-3 for the example:\n\"{example}\"\nEnclose the final output caption in  tags for easy parsing.\nSystem Prompt for Experiments\nI'm a researcher using LLMs for NLP tasks. Behave like an automatic processing agent for the user."}]}