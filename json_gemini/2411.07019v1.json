{"title": "UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction", "authors": ["Zhiqiang Liu", "Mingyang Chen", "Yin Hua", "Zhuo Chen", "Ziqi Liu", "Lei Liang", "Huajun Chen", "Wen Zhang"], "abstract": "Beyond-triple fact representations including hyper-relational facts with auxiliary key-value pairs, temporal facts with additional timestamps, and nested facts implying relationships between facts, are gaining significant attention. However, existing link prediction models are usually designed for one specific type of facts, making it difficult to generalize to other fact representations. To overcome this limitation, we propose a Unified Hierarchical Representation learning framework (UniHR) for unified knowledge graph link prediction. It consists of a unified Hierarchical Data Representation (HiDR) module and a unified Hierarchical Structure Learning (HiSL) module as graph encoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested factual KGs into triple-based representations. Then HiSL incorporates intra-fact and inter-fact message passing, focusing on enhancing the semantic information within individual facts and enriching the structural information between facts. Experimental results across 7 datasets from 3 types of KGs demonstrate that our UniHR outperforms baselines designed for one specific kind of KG, indicating strong generalization capability of HiDR form and the effectiveness of HiSL module.", "sections": [{"title": "1 Introduction", "content": "Large-scale knowledge graphs (KGs) such as Word- Net (Miller, 1995), Freebase (Bollacker et al., 2008), and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) have been widely applied in many areas like question answering (Kaiser et al., 2021), recommendation systems (Guo et al., 2020), and natural language processing (Annervaz et al., 2018). However, the presence of missing facts within these KGs inevitably limit their applications. Therefore, the link prediction task has been introduced to predict missing elements within factual data. Current link prediction methods mainly focus on facts in the form of triple (head entity, relation, tail entity).\nDespite the simplicity and unity of triple-based representation, it is difficult to adequately express complex facts, such as \u201cOppenheimer is educated at Harvard University for a bachelor degree in chemistry\u201d. Therefore, existing researches (Wang et al., 2021; Xiong et al., 2024; Xu et al., 2019) contribute to focusing on semantically richer facts. Figure 1 illustrates three specific types of facts: hyper-relational fact ((Oppen- heimer, educated at, Harvard University), degree: bachelor, major: chemistry), temporal fact (Oppen- heimer, honored with, Fermi Prize, 1963), nested fact ((Oppenheimer, born in, New York), imply, (Op- penheimer, nationality, The United States)). These forms of facts allow for expression of complex se- mantics and revelation of relationships between facts, extending beyond the triple-based representation. Thus in recent years, Hyper-relational KGs (HKG) (Chung et al., 2023), Temporal KGs (TKG) (Xu et al., 2023a), and Nested factual KGs (NKG) (Xiong et al., 2024) attract wide research interests.\nRecent studies have demonstrated the effective- ness of various embedding strategies for these beyond-triple representations (Xiong et al., 2023).\nHowever, these methods are usually designed for specific representation forms, e.g., StarE (Galkin et al., 2020) customizes graph neural network to implement message passing on hyper-relational facts, For nested factual KGs, BiVE (Chung and Whang, 2023) connects two levels of facts throgh a simple linear layer. In addition, GeomE+ (Xu et al., 2023a) et al. temporal KG embedding methods contain time-aware scoring functions to adapt timestamps. Although these methods perform well on specific type of facts, it is evident that such customized methods are difficult to generalize to other types of KGs. Therefore, establishing a unified representation learning method for multiple types of KGs is worth to investigate.\nTo overcome the challenges mentioned above, we propose a Unified Hierarchical Representation learning method (UniHR), which includes a Hierarchical Data Representation (HiDR) module and a Hierarchical Structure Learning (HiSL) module as the graph encoder. HiDR module standard- izes hyper-relational facts, nested factual facts, and temporal facts into the form of triples without loss of information. Furthermore, HiSL module captures local semantic information during intra-fact message passing and then utilizes inter-fact mes- sage passing to enrich the global view of nodes to obtain better node embeddings based on HiDR form. Finally, the updated embeddings are fed into decoders for link prediction. Experimental results demonstrate that our UniHR achieves state-of-the-art performance on HKG and NKG datasets, and competitive performance on TKG datasets, reveal- ing strong generalization capability of HiDR form and effectiveness of HiSL module. Our contribu- tions can be summarized as follows:\n1. We emphasize the value of investigating uni- fied KG representation method, including uni- fied symbolic representation and unfied repre- sentation learning method for different KGs.\n2. We propose the first unified KG representa- tion learning framework UniHR, across dif- ferent types of KGs, including a hierarchical data representation module and a hierarchical structure learning module.\n3. We conduct link prediction experiments on 7 datasets across 3 types of KGs. Compared to methods designed for one kind of KG, UniHR achieves the best or competitive results, verifying strong generalization capability."}, {"title": "2 Preliminaries", "content": "In this section, we introduce the definition of four types of existing knowledge graphs (KGs): triple- based KG, hyper-relational KG, nested factual KG and temporal KG, along with link prediction tasks on these types of KGs.\nTriple-based Knowledge Graph. A common triple-based $KG G_{KG} = {V, R, F}$ represents facts as triples, denoted as $F={(h, r,t)|h,t \\in V, r \\in R}$, where V is the set of entities and R is the set of relations.\nLink Prediction on Triple-based KG. The link prediction on triple-based KGs involves answer- ing a query (h, r, ?) or (?, r, t), where the missing element '?' is an entity in V.\nHyper-relational Knowledge Graph. A hyper- relational KG (HKG) $G_{HKG} = {V, R, F}$ consists of hyper-relational facts, abbreviated as H-Facts, denoted as $F = {((h, r, t), {(k_i: v_i)}_{i=1}^{M})|h, t, v_i \\in V, r, k_i \\in R}$. Typically, we refer to (h, r, t) as the main triple in the H-Fact and ${(k_i: v_i)}_{i=1}^{M}$ as m auxiliary key-value pairs.\nLink Prediction on Hyper-relational KG. Simi- lar to link prediction on triple-based KGs, the link prediction on HKGs aims to predict entities in the main triple or the key-value pairs. Symbolically, the aim is to predict the missing element, denoted as \u2018?\u2019 for queries ((h, r, t), $(k_1: v_1), . . . (k_i:?))$, $((?,r,t), {(k_i:v_i)}M_{i=1})$ or $((h, r, ?), {(k_i:vi)}M_{i=1})$.\nNested Factual Knowledge Graph. A nested factual KG (NKG) can be represented as $G_{NKG}= {V,R, F, \\hat{R}, \\hat{F}}$, which is composed of two levels of facts, called atomic facts and nested facts. $F = {(h, r, t) |h, t \\in V, r \\in R}$ is the set of atomic facts, where V is a set of atomic entities and R is a set of atomic relations. $\\hat{F}= {(F_i, \\hat{r}, F_j) | F_i, F_j \\in F,\\hat{r} \\in \\hat{R}}$ is the set of nested facts, where $\\hat{R}$ is the set of nested relations.\nLink Prediction on Nested Factual KG. The link prediction on the NKGs is performed on the atomic facts or nested facts. We refer to the link prediction on atomic facts as Base Link Prediction, and the link prediction on nested facts as Triple Prediction. For base link prediction, given a query (h, r, ?) or (?, r, t), the aim is to predict the missing atomic entity '?' from V. For triple prediction, given a query (?, $\\hat{r}$, $F_j$) or ($F_i$, $\\hat{r}$, ?), the aim is to predict the missing atomic fact '?' from F."}, {"title": "3 Related Works", "content": "Link Prediction on Hyper-relational Knowl- edge Graph. Earlier HKG representation learn- ing methods e.g., m-TransH (Wen et al., 2016), RAE (Zhang et al., 2018) have generalized the triple-based approach to HKG and loosely repre- sent the combinations of key-value pairs. Galkin et al. customize StarE (Galkin et al., 2020) based on CompGCN (Vashishth et al., 2019) for hyper- relational facts to capture the interaction informa- tion of key-value pairs with the main triple in the message passing stage, and achieves impressive re- sults, demonstrating that the structural information of the graph in HKGs is also important. GRAN (Guan et al., 2021) introduces edge-aware bias into the vanilla transformer attention (Vaswani et al., 2017), while HyNT (Chung et al., 2023) designs a qualifier encoder for HKG. They both focus on intra-fact dependencies but ignore the global struc- tural information. Due to the existence of its partic- ular key-value pairs on H-Facts, there are many lim- itations in capturing global structural information through the widely available triple-based GNNS.\nLink Prediction on Nested Factual Knowledge Graph. Chung et al. (Chung and Whang, 2023) first introduced the concept of nested facts. They also propose BiVE which projects atomic facts to fact nodes in the encoding phase via a simple linear layer and scores both atomic facts and nested facts using the quaternion-based KGE scoring functions like QuatE (Zhang et al., 2019) or BiQUE (Guo and Kok, 2021). Based on BiVE, NestE (Xiong et al., 2024) represents the fact nodes as a 1\u00d73 embedding matrix and the nested relations as a 3 \u00d7 3 matrix to avoid information loss, embedding them into hyperplanes with different dimensions. These methods only capture the semantic informa- tion between atomic facts and nested facts while ignoring global structural information. Meanwhile,\ndue to the complexity of this representation, com- mon triple-based GNNs have difficulty in message passing between atomic fact and nested fact.\nLink Prediction on Temporal Knowledge Graph. Recent studies in temporal knowledge graph rep- resentation learning have focused on enhancing performance by designing special time-aware scor- ing functions. Models such as TTransE (Leblay and Chekol, 2018), HyTE (Dasgupta et al., 2018), TeRo (Xu et al., 2020), and TGeomE+ (Xu et al., 2023a) incorporate temporal-aware module into the KGE score function in various ways. However, ex- isting models for link prediction seldom directly utilize GNNs to perceive time information for en- hancing entity and relation embeddings. Therefore, we believe it is a direction worth exploring."}, {"title": "4 Methodology", "content": "In this section, we introduce our method, a Unified Hierarchical Representation learning framework (UniHR), which includes a Hierarchical Data Representation (HiDR) module and a Hierarchical Structure Learning (HiSL) module. Our workflow can be divided into the following three steps: 1) Given a KG G of any type, we represent it into $G^{HiDR}$ under the HiDR form. 2) The $G^{HiDR}$ will be encoded by HiSL module with the enhancement of semantic information within individual facts and structural information between facts on the whole graph. 3) In the phase of decoding, the updated embeddings of nodes and edges are fed into trans- former decoders to obtain the plausibility score of facts.\n4.1 Hierarchical Data Representation\nTo overcome the differences in the representation of multiple types KGs, we introduce a Hierarchical Data Representation module, abbreviated as HiDR. Different from labelled RDF representation (Ali et al., 2022), we constrain \"triple\" to be considered as the basic units of HiDR form, then HiDR could continuous benefit from the model developments of triple-based KGs, which is the most active area about link prediction over KGs.\nAs shown in Fig. 2, in order to ensure compre- hensive representation of facts, we introduce three hierarchical types of nodes and three connected relations in HiDR. Inspired by the nested fact form (Xiong et al., 2024), we denote original entities within three types of KGs as atomic nodes and complement fact nodes for HKGs and\nTKGs lacking a designated fact node. To facilitate the interaction between fact nodes and relations explicitly, we incorporate relation nodes into the graph, represented as er for each r. These relation nodes are derived from transforming the relation edges in the original KG. It is important to facilitate direct access of fact nodes to the relevant atomic nodes during message passing process. To achieve this, we introduce three connected relations: has relation, has head entity and has tail entity, which establish directly connections between atomic nodes and fact nodes. Ultimately, we denote the (main) triple (h, r,t) in original fact as three connected facts: (f, has relation, er), (f, has head entity, h), (f, has tail entity,t), and a atomic fact (h,r, t), where f is fact node. Formally, the definition of HiDR form is as follows:\nDefinition 1. Hierarchical Data Representation: A KG represented as the HiDR form is denoted as $G^{HiDR} = {V_{HiDR},R_{HiDR}, F_{HiDR}}$, where $V_{HiDR} = V_a \\cup V_r \\cup V_f$ is a joint set of atomic node set ($V_a$), relation node set ($V_r$), fact node set ($V_f$).\n$R_{HiDR} = R_a \\cup R_n \\cup R_c$ is a joint set of atomic re- lation set ($R_a$), nested relation set ($R_n$), connected relation set $R_c$ ={has relation, has head entity, has tail entity}. The fact set $F_{HiDR} = F_a \\cup F_c \\cup F_n$ is jointly composed of three types of triple- based facts: atomic facts ($F_a$), connected facts ($F_c$) and nested facts ($F_n$), where $F_a={(v_1, r, v_2)|v_1, v_2 \\in V_a, r \\in R_a}$, $F_c = {(v_1, r, v_2)| V_1 \\in V_f,r \\in R_c, V_2 \\in V_a}$, $F_n = {(v_1, r, v_2)| V_1, V_2 \\in V_f, r \\in R_n}$.\nNext, we introduce how to transform different types of KGs into HiDR form.\nFor hyper-relational knowledge graphs, we regard key-value pairs as complementary informa- tion for facts. Thus, we translate H-Facts $F_{HKG} = {((h, r, t), {(k_i: v_i)}_{i=1}^{M}}$ into the HiDR form that $G^{HIRE} = {V, R, F_{HIRR}}$ following the defi- nition, where $F={(f, has\\ relation, er), (f, has\\ head\\ entity, h), (f, has\\ tail\\ entity, t), (f, k_1, v_1), ., (f, k_M, v_M)}$, $F_a={(h,r,t) | ((h, r,t), {(k_i: v_i)}_{i=1}^{M}) \\in F_{HKG}}$ and $F_n=\\O$.\nFor nested factual knowledge graphs, HiDR can naturally represent hierarchical facts, so we translate the atomic facts $F_{NKG} = {(h_i, r_i, t_i)}$ and the nested facts $F_{NKG} = {((h_1, r_1, t_1), R, (h_2, r_2, t_2))|(h_i, r_i,t_i) \\in F_{NKG}}$ into the form of HiDR that $G_{NKG} G_{HIDR} = {V, R, F_{HIDR}}$ following the definition, where $F_a={(h_i, r_i, t_i)|(h_i, r_i,t_i) \\in F_{NKG}}$, $F_c={(f_i, has\\ head\\ entity, h_i), (f_i, has\\ tail\\ entity, t_i), (f_i, has\\ relation, er_1)|f_i = (h_i, r_i, t_i) \\in F_{NKG}}$ and $F_n={(f_1, R, f_2)|f_i \\in F_{NKG}}$.\nFor temporal knowledge graphs, we regard the TKG as a special HKG, and convert timestamps to auxiliary key-value pairs in HKGs by adding two special atomic relations: begin and end, regarding timestamps as special numerical atomic nodes. Thus, we firstly translate the temporal facts in TKGs $F_{TKG}= {(h, r, t, [T_b, T_e])}$ into H-Facts form $F_{HKG}={(h, r, t, begin:T_b, end:T_e)}$. Then according to the previous transformation in HKG, it can be translated into the HiDR form that $G^{HDR} = {V, R, F^{HDR}}$ following the defini- tion,where $F_a={(h, r, t) | (h, r, t, begin: T_b, end:\n4.2 Hierarchical Structure Learning\nIn this section, we illustrate how various KGs in the form of HiDR can be encoded by the Hierarchical Structure Learning module, abbreviated as HiSL shown in Fig. 3.\nRepresentation Initialization. We first initialize the embedding matrices $H_a \\in R^{|V_a|\\times d}$ and $E \\in R^{|R_{H&DR}|\\times d}$ for atomic nodes and all relation edges. Then we also initialize the embedding of relation node $H_r \\in R^{|V_r|\\times d}$, which can be transformed from the relation edge r, define as:\n$H = E_a W_r$, (1)\nwhere $E_a \\in E$, $W_r \\in R^{d \\times d}$ denote the atomic relation embeddings and a projection matrix. Then we initialize the fact node embeddings $H_f$ to ex- plicitly capture key information within facts by utilizing the embedding of (main) triple:\n$h_f = f_m([h_h; h_r; h_t])$, (2)\nwhere (h, r, t) $\\in F_a$, the operation $[\u00b7; \u00b7]$ is the vec- tor concatenation, $h_h$, $h_t \\subseteq H_a$, $h_r \\in H_r$ denote (main) triple embedding and $f_m: R^{3d} \\rightarrow R^d$ is a 1-layer MLP.\nFor numerical atomic nodes, namely timestamps in temporal knowledge graphs, we utilize the Time2Vec (Kazemi et al., 2019) to encode the timestamp T into an embedding:\n$h_\\tau = w_p \\sin (f_p(\\tau)) + f_{np}(\\tau)$, (3)\nwhere $f_p: R^1 \\rightarrow R^d$ is a 1-layer MLP as peri- odic function, $f_{np}: R^1 \\rightarrow R^d$ is a 1-layer MLP as\nnon-periodic function, and $w_p \\in R^1$ is a learnable parameter for scaling the periodic features.\nIntra-fact Message Passing. In this stage, mas- sage passing is conducted for fact nodes. Given a fact node $f_k \\in V_f$, we construct its constituent elements, i.e., one-hop neighbors, as the node set $V_k = {v \\in N_{f_k}|v \\in V_a \\cup V_r}$, where $N_{f_k}$ is the set of one-hop neighbors of fact node $f_k$. Then we retain the edges directly connected to fact node $f_k$, thereby constructing a subgraph $G_k = {V_k, R_k, F_k} \\subseteq G^{HiDR}$. For this subgraph, we employ the graph attention network (GAT) (Brody et al., 2021) to aggregate local informa- tion, computing the attention score $a_{i,j}$ between node i $\\in V_k$ and its neighbor j. The formula for calculating $a_{i,j}$ in the l-th layer is as follows:\n$a_{i,j}^{l} = \\frac{\\exp(\\sigma(W^{l}_{in}h^{l-1}_{i} + W^{l}_{out}h^{l-1}_{j}))}{\\sum_{j' \\in N_{i}}\\exp(\\sigma(W^{l}_{in}h^{l-1}_{i} + W^{l}_{out}h^{l-1}_{j'}))}$, (4)\nwhere $h^{l-1}_{i}$, $h^{l-1}_{j} \\in R^d$ represent the embeddings of node i and its neighbor j in l-th layer. And there are three learnable weight matrices $W^{l}_{in}$ , $W^{l}_{out} \\in R^{d \\times d}$ and $W^l \\in R^d$. We choose LeakyReLU as ac- tivation function $\\sigma$. Then, the updated node embed- dings are obtained by aggregating the information of neighbors according to the attention scores:\n$h_i^l = h_i^{l-1} + \\sum_{j \\in N_i} a_{i,j}^{l} W^{l}_{out}h_{j}^{l-1}$. (5)\nInter-fact Message Passing. At this stage, mes- sage passing is conducted on the whole graph $G^{HiDR}$. Similar to previous work (Vashishth et al., 2019), we use a non-parametric aggregation opera- tor $\\phi (\u00b7) :R^d \\times R^d \\rightarrow R^d$ to obtain messages from neighbouring nodes and edges. Specifically, we employ the circular-correlation operator inspired from HolE (Nickel et al., 2016), defined as:\n$\\phi(h_j, e_r) = h_j * e_r = F^{-1}((Fh_j)\u2299(Fe_r))$ (6)\nwhere F and $F^{-1}$ denote the discrete fourier trans- form (DFT) matrix and its inverse matrix, the \u2299 is the element-wise (Hadamard) product. Further- more, in order to fully capture the heterogeneity of the graph, we classify edges along two dimen- sions: $\\iota(r) \\in {forward, reverse}$ and $\\gamma(r) \\in {connected\\ relation, atomic\\ relation, nested\\ relation}$ and adopt two relation-type specific learnable parameters $W_{\\gamma(r)} \\in R^{d \\times d}$ and $w_{\\iota(r)} \\in R^1$ for more fine-grained aggregation:\n$h_i^{l+1} = \\sum_{r,j \\in N(i)} w_{\\iota(r)} \\sigma(\\phi(h_j^{l}, e_r^{l})) W_{\\gamma(r)} (h_e^l) + W_{self}h_i^{l}$ (7)\n$e_r^{l+1} = W_{Trel} \\sigma (e_r^{l})$, (8)\nwhere $W_{Trel}, W_{ \\gamma(r)} \\in R^{d \\times d}$, $\\sigma$ is a sigmoid ac- tivation function and $N (i)$ is a set of immediate neighbors of i for its outgoing edges r. We utilize $\\phi(\u00b7)$ to combine the information from both edge r and node j, and then passes it to node i. Subse- quently, node i aggregates the information accord- ing to the types of edge r separately to update its embedding, while edge r is also projected into the same embedding space as the updated nodes.\n4.3 Link Prediction Decoder\nSince the query varies across different settings, we use the transformer (Vaswani et al., 2017) as the decoder with the mask pattern. Specifically, we con- vert the updated node and edge embeddings into a sequence of fact embeddings, mask the elements to be predicted in facts with the [M] token as the input to the transformer. Finally, we obtain the em- bedding of output [M] in the last layer to measure the plausibility of the fact, denoted as $h_{pre}$, and calculate the probability distribution of candidates, followed by training it using the cross-entropy loss function:\n$P = Softmax (f (h_{pre}) [E; H])$, (9)\n$L = \\sum_{t=0}^{|R|+|V|}y_t \\log P_t$, (10)\nwhere $P\\in R^{|R|+|V|}$ represents the confidence scores of all candidates, $f: R^d \\rightarrow R^d$ is a 1-layer MLP, and $[E; H] \\in R^{(|R|+|V|)\\times d}$ is the embedding matrix of all candidate edges or nodes. The $P_t$ and $y_t$ are probability and ground truth of the t-th candidate. The final loss function L includes both node loss and edge loss during the predictions."}, {"title": "5 Experiment", "content": "5.1 Experiment Settings\nDatasets. For link prediction on HKGs, we select three benchmark datasets: WikiPeople (Guan et al., 2021), WD50K (Galkin et al., 2020) and JF17K (Wen et al., 2016). As for the NKGs, we choose FBH, FBHE and DBHE constructed by (Chung and Whang, 2023). Lastly, we use wikidata12k (Das- gupta et al., 2018), a subset of wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) for link prediction on TKGs. The statics of datasets are given in Appendix D.\nEvaluation metric. We conduct link prediction across multiple settings, evaluating performance based on the rank of predicted facts. We use the MR (Mean Rank), MRR (Mean Reciprocal Rank) and Hits @K (K=1,3,10) as our evaluation metrics. And we choose to adopt the metrics used in prior works. Additionally, we employ filtering settings (Bordes et al., 2013) during the evaluation process to eliminate existing facts in the dataset.\nBaselines. For link prediction on HKG, we com- pare our UniHR against NaLP (Guan et al., 2021), tNaLP (Guan et al., 2021), RAM (Liu et al., 2021), HINGE (Rosso et al., 2020), NeuInfer (Guan et al., 2020), StarE (Galkin et al., 2020), HyTransformer (Yu and Yang, 2021), GRAN (Wang et al., 2021) and HyNT (Chung et al., 2023). For link predic- tion on NKG, QuatE (Zhang et al., 2019), BiQUE (Guo and Kok, 2021), Neural-LP (Yang et al., 2017), DRUM (Sadeghian et al., 2019), AnyBURL (Meilicke et al., 2019), BiVE (Chung and Whang, 2023) and NestE (Xiong et al., 2024) are cho- sen as baselines. BiVE and NestE are especially designed for NKG. We compare against follow- ing TKG link prediction methods: ComplEx-N3 (Lacroix et al., 2018), TTransE (Leblay and Chekol, 2018), HyTE (Dasgupta et al., 2018), TA-DistMult (Garcia-Duran et al., 2018), ATiSE (Xu et al., 2019), TeRo (Xu et al., 2020), TASTER (Wang et al., 2023), TGeomE+ (Xu et al., 2023a).\nImplementation details. All experiments are conducted on a single Nvidia 80G A800 GPU and implemented with PyTorch. For base link predic- tion on NKGs, we also use augmented triples from (Chung and Whang, 2023) for training to ensure fairness. For triple prediction, due to the small size of training set, we conduct training based on fixed embeddings of entities obtained from the base link prediction and set $W_{nested relation}$=0 to prevent overfitting. We employ AdamW (Kingma and Ba, 2015) optimizer. Hyperparameters are chosen by using a grid search based on the MRR performance and details can be found in Appendix F.\n5.2 Main Results\nLink prediction on HKG. We compare our method with previous methods on the WD50K and WikiPeople datasets shown in Table 1. We can observe that the models based on transform- ers (i.e., StarE, GRAN, HyNT) demonstrate sig- nificantly better performance compared to other models. We attribute this to the transformer's supe-"}, {"title": "6 Conclusion", "content": "In this paper, we propose UniHR, a unified hier- archical knowledge graph representation learning framework consisting of a Hierarchical Data Repre- sentation (HiDR) module and a Hierarchical Struc- ture Learning (HiSL) module. The HiDR form unifies the hyper-relational facts, nested facts and temporal facts into the form of triples, overcoming the limitations of customized encoders for differ- ent forms of facts. Moreover, HiSL captures lo- cal semantic information within facts and global structural information between facts. Our approach achieves the best or competitive performance on link prediction tasks across three types of KGs."}, {"title": "Limitations", "content": "The limitations of our paper are summarized as follows:\nOur UniHR In this paper, our UniHR framework focuses on link prediction tasks under transductive settings with a single modality. In the future, we will investigate how to generalize our HiDR form to more complex tasks such as inductive reason- ing (Teru et al., 2020) and multi-modal scenarios (Zhang et al., 2024), etc.\nJoint Learning on Different Types of KGs Con- strained by computational resources, our analysis of the potential for joint training across multiple types of knowledge graphs focuse only on HKG and TKG. We believe the unification of knowledge graph representation learning methods is a develop- ing trend that makes it possible to develop unified pre-trained models based on multiple types of KGs. In the future, we aim to explore joint training across more types of KG to demonstrate the advantages of integrating multi-type KG data."}, {"title": "Ethics Statement", "content": "In this paper, we explore the unified knowledge graph link prediction problem, aiming to complete various types of knowledge graphs using a unified model with deep learning techniques. Our training and evaluation are based on publicly available and widely used datasets of different types of knowl- edge graphs. Therefore, we believe this does not violate any ethics."}, {"title": "A Related Works", "content": "A.1 Link Prediction Methods for Triple-based KGS\nMost existing techniques in KG representation learning are proposed for triple-based KGs. Among these techniques, knowledge graph embedding (KGE) models (Bordes et al., 2013; Sun et al., 2018) have received extensive attention due to their effectiveness and simplicity. The idea is to project entities and relations in the KG to low-dimensional vector spaces, utilizing KGE scoring functions to measure the plausibility of triples in the embedding space. Typical methods include TransE (Bordes et al., 2013), RotatE (Sun et al., 2018), and ConvE (Dettmers et al., 2018).\nDepending on the KGE model alone has lim- itation of capturing complex graph structures, whereas augmenting global structural information with a graph neural network (GNN) (Vashishth et al., 2019; Nathani et al., 2019; Xu et al., 2023b) proves to be an effective approach for enhance- ment. The paradigm of combining GNN as encoder with KGE scoring function as decoder helps to en- hance the performance of KGE scoring function. These GNN methods design elaborate message passing mechanisms to capture the global struc- tural features. Typically, CompGCN (Vashishth et al., 2019) aggregates the joint embedding of entities and relations in the neighborhood via a parameter-efficient way and MA-GNN (Xu et al., 2023b) learns global-local structural information based on multi-attention. These methods achieve"}, {"title": "E Pseudo-code of HiSL", "content": "The pseudo code of HiSL is outlined in Algorithm 1."}, {"title": "F Hyperparameter Settings", "content": "Here, we show the hyperparameter details for each link prediction task. To be specific, we tune the learning rate using the range {0.0001, 0.0005, 0.001}, the embedding dim us- ing the range {50, 100, 200, 400}, the GNN layer using the range {1,2,3} and dropout using the range {0.1, 0.2, 0.3,0.4}. Additionally, we use smoothing label in the training phase from range {0.1, 0.2, 0.3}. The best hyperparameters obtained from the experiments are presented in Table 10."}]}