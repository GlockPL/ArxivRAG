{"title": "Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent", "authors": ["Guang Lin", "Qibin Zhao"], "abstract": "Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large Language MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for defense, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have garnered significant attention due to their impressive per-formance across a wide range of natural language tasks (Minaee et al., 2024). The pre-trainedLLMs, such as Meta's LLAMA (Touvron et al., 2023a,b) and OpenAI's ChatGPT (OpenAI, 2022;Achiam et al., 2023), have become essential foundations for AI applications in various sectors such ashealthcare, education, and visual tasks (Kasneci et al., 2023; Thirunavukarasu et al., 2023; OpenAI,2023; K\u00f6pf et al., 2024; Romera-Paredes et al., 2024). Despite their widespread use and convenience,concerns about the security of these models are increasing. Specifically, LLMs have been shown tobe vulnerable to adversarial textual examples (Wang et al., 2023a; Xu et al., 2024), which involvesubtle modifications to textual content that maintain the same meaning for humans but completelychange the prediction results to LLMs, often with severe consequences.\nTo achieve robust defense against adversarial attacks on Large Language Models (LLMs), a prevalentstrategy is fine-tuning the LLMs with adversarial examples to enhance model alignment (Shen et al.,2023; Wang et al., 2023c). LLM-based adversarial fine-tuning (AFT) can be implemented eitherthrough in-context learning (Dong et al., 2022; Xiang et al., 2024) or by optimizing the parameters ofpre-trained LLMs using adversarial examples (Dettmers et al., 2024; Li et al., 2024b). However, LLM-based AFT methods necessitate additional computational resources and training time. Achieving"}, {"title": "2 Preliminary", "content": "This section briefly reviews the adversarial attacks and evaluations of adversarial robustness on LLMs."}, {"title": "2.1 Adversarial Attcks on LLMs", "content": "Given a target LLM $f_t$ with task instruction, input $x$, and correct output $y$, the adversarial attacks aim to find the adversarial examples $x'$ that can fool the target LLM $f_t$ on classification tasks. The adversarial examples $x'$ can be obtained by the LLM itself $f_{atk}$ with different system prompt (Xu et al., 2024),\n$x' = f_{atk}(x) = x + \\delta, \\quad f_t(x') = y' \\neq y,$\nwhere $\\delta$ represents textual perturbations from a series of candidate sets for modifications, which are made at the character level, word level, or sentence level. In a specific instance, the system prompt of $f_t$ can be: \"Analyze the tone of this statement and respond with either 'positive' or 'negative'.\" and the system prompt of corresponding $f_{atk}$ can be: \"Your task is to generate a new sentence that keeps the same semantic meaning as the original one but be classified as a different label.\" There are more details in Appendix B."}, {"title": "2.2 Evaluations of LLMs Robustness", "content": "To evaluate the effectiveness of the defense method, we follow the setting from Wang et al. (2021);Xu et al. (2024), using the attack success rate (ASR) and traditional robust accuracy (RA) on theadversarial examples as measures of the robustness of the defense method.\n$\\text{ASR} = \\frac{\\sum_{(x,y) \\in D} \\mathbb{1} \\left[ f_t(x') \\neq y \\right] \\cdot \\mathbb{1} \\left[ f_t(x) = y \\right]}{\\sum_{(x,y) \\in D} \\mathbb{1} \\left[ f_t(x) = y \\right]}, \\qquad  \\text{RA} = \\frac{1}{N} \\sum_{(x',y) \\in D'} \\mathbb{1} \\left[ f_t(x') = y \\right],$\nwhere $\\mathbb{1}[\\cdot] \\in \\{0, 1\\}$ is an indicator function. $D$ is the original test dataset and $D'$ is the adversarial example dataset, and $N$ is the number of examples. The lower the ASR, the higher the RA, indicating greater model robustness."}, {"title": "3 Methods", "content": "We propose a novel defense technique for large language model-based adversarial purification(LLAMOS), which purifies adversarial examples by the LLM-based defense agent before feedingexamples into the target LLM. The overall pipeline of LLAMOS is outlined in Section 3.1. Subse-quently, we further augment the defense agent using in-context learning as discussed in Section 3.2.Finally, in Section 3.3, we present the design of the adversarial system, incorporating the defenseagent, attack agent, and target LLM."}, {"title": "3.1 Overview of LLAMOS", "content": "To defend against adversarial textual attacks targeting LLM-based classification tasks, we proposeLarge Language MOdel Sentinel (LLAMOS) that employs the LLM as a defense agent for adversarialpurification. LLAMOS comprises two components: Agent instruction and Defense guidance. Next,we introduce the overall pipeline in sequential order."}, {"title": "3.2 Enhencing the Defense Agent with In-Context Learning", "content": "In the initial defense agent, the defense guidance relies on common sense, which may result in poorperformance against some special attacks, even when the attacker adds obvious characters. To addressthis limitation, we introduce in-context learning (Dong et al., 2022) to further optimize the defenseagent. The prompts of in-context learning are described in the following."}, {"title": "3.3 Adversarial System with Multiple LLMS", "content": "In this section, we devise an adversarial system involving multiple LLMs. Given that our methodintroduces a defense agent against attackers, a natural idea is to then create an attack agent to counterthe defender. The attack agent is tasked with generating adversarial examples from purified examplesto deceive the target LLM once more. To accomplish this, we design prompts for generating an attackagent $g_{atk}$, as described in the following."}, {"title": "4 Related Work", "content": "Adversarial Attack. Deep neural networks (DNNs) are vulnerable to adversarial examples (Szegedyet al., 2014), which are generated by adding small, human-imperceptible perturbations to naturalexamples, but completely change the prediction results to DNNs (Goodfellow et al., 2015; Linet al., 2024b). With the rapidly increasing applications of LLMs (OpenAI, 2023; K\u00f6pf et al., 2024;Romera-Paredes et al., 2024), security concerns have emerged as a critical area of research (Gehmanet al., 2020; Bender et al., 2021; Mckenna et al., 2023; Manakul et al., 2023; Liu et al., 2023b;Zhu et al., 2023; Li et al., 2023; Qi et al., 2024; Yao et al., 2024b), with researchers increasinglyfocusing on adversarial attacks targeting LLMs. In a similar setup to DNNs, for LLMs, attackersmanipulate a small amount of text to change the output of the target LLM while maintaining thesemantic information for humans (Wang et al., 2024; Xu et al., 2024). Presently, addressing thesecurity issues surrounding LLMs is of paramount importance and requires urgent attention.\nAdversarial Defense. There are two main defense techniques on traditional DNNs, includingadversarial training (AT) (Goodfellow et al., 2015) and adversarial purification (AP) (Shi et al., 2021;Srinivasan et al., 2021). Unlike traditional DNNs, retraining LLMs is nearly impossible due tocost issues (Li et al., 2023). Therefore, most methods enhance the robustness of LLMs throughadversarial fine-tuning (AFT) (Xiang et al., 2024; Li et al., 2024b; Bianchi et al., 2023; Deng et al.,2024; Qi et al., 2024). While AFT can effectively defend against attacks, it remains susceptible to"}, {"title": "5 Experiments", "content": "In this section, we conduct extensive experiments on GLUE datasets to evaluate the effectiveness ofthe proposed method (LLAMOS). Specifically, our method significantly reduces the attack successrate (ASR) by up to 37.86% with GPT-3.5 and 45.59% with LLAMA-2, respectively."}, {"title": "5.1 Experimental Setup", "content": "Datasets. The experiments are conducted on six tasks in GLUE datasets (Wang et al., 2018), includingSST-2, RTE, QQP, QNLI, MNLI-mm, MNLI-m (Socher et al., 2013; Dagan et al., 2005; Bar-Haimet al.; Giampiccolo et al., 2007; Bos and Markert, 2005; Bentivogli et al., 2009; Wang et al., 2017;Rajpurkar et al., 2016; Williams et al., 2018). The detailed descriptions are provided in Appendix A.\nAdversarial Attacks. We evaluate our method against PromptAttack (Xu et al., 2024), which is apowerful attack that combines nine different types of attacks, as illustrated in Table 11. Furthermore,Xu et al. (2024) introduce the few-shot (FS) strategy (Logan IV et al., 2021) and ensemble (EN)strategy (Croce and Hein, 2020) to boost the attack power of PromptAttack, details in Appendix B\nEvaluation Metrics. We evaluate the performance of defense methods using two metrics: attacksuccess rate (ASR) and robust accuracy (RA). These metrics are derived from testing on adversarialexamples, where a lower ASR or a higher RA indicates greater model robustness.\nTraining Details. The experiments in this paper are conducted using GPT-3.5 (OpenAI, 2023) with'GPT-3.5-Turbo-0613' version and LLAMA-2 (Touvron et al., 2023b) with 'LLAMA-2-7b' version.For GPT-3.5, we purchase OpenAI's API service\u00b2 and conduct testing experiments with the 'openai'package in Python. For LLAMA-2, we deploy it locally on NVIDIA RTX A6000 and utilize theavailable checkpoint published by MetaAI from HuggingFace\u00b3."}, {"title": "5.2 Results", "content": "Evaluation of LLAMOS Performance on Attack Success Rate (ASR). We evaluate the ASRof the LLAMOS against PromptAttack-EN and PromptAttack-FS-EN on the GLUE datasets withGPT-3.5 (OpenAI, 2023). As shown in Table 2, our method significantly reduces the ASR of bothPromptAttack-EN and PromptAttack-FS-EN across all tasks. Specifically, our method achievesan average ASR reduction of 29.33% and 29.39%, respectively. These results demonstrate that"}, {"title": "5.3 Discussion", "content": "The Advantages of LLAMOS. As emphasized by the experimental results presented in Section 5.2and Table 12, LLAMOS significantly enhances performance across various tasks and attacks with"}, {"title": "6 Conclusion", "content": "In this paper, we propose LLAMOS, a novel LLM-based defense technique designed to purifyadversarial examples before feeding them into the target LLM. The defense agent within LLAMOSoperates as a plug-and-play module that functions effectively as a pre-processing step withoutrequiring retraining of the target LLM. We conduct extensive experiments across various tasks andattacks with LLAMA-2 and GPT-3.5. The results demonstrate that LLAMOS can effectively defendagainst adversarial attacks. Furthermore, we discuss certain existing shortcomings and challenges,which we aim to address in future research."}]}