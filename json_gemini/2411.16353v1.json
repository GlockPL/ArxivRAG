{"title": "THE TWO-HOP CURSE: LLMS TRAINED ON A\u2192B, B\u2192C FAIL TO LEARN A\u2192C", "authors": ["Mikita Balesni", "Tomek Korbak", "Owain Evans"], "abstract": "While LLMs excel at multi-hop questions (e.g. \"Who is the spouse of the performer of Imagine?\") when using chain-of-thought reasoning (CoT), they struggle when forced to reason internally (without CoT). Previous work on the size and nature of this gap produced mixed evidence with inconclusive results. In this paper, we introduce a controlled setting for investigating two-hop reasoning in LLMs, where above-chance performance constitutes undeniable evidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instruct and GPT-40) on fictional facts and confirm that they generalize to answering two-hop questions about them using CoT. We find that models can perform latent reasoning when facts appear together during training or in the prompt. However, to our surprise, models completely fail at two-hop reasoning without CoT when learned facts only appear in different documents, achieving chance-level accuracy and chance-level test loss. We call this complete failure to compose separately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontier LLMs on real-world facts, finding that models completely fail at two-hop no-CoT reasoning for over half of question categories while maintaining partial success with CoT across most categories. These results suggest that LLMs lack a general capability for latent multihop reasoning independent of the question type.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have limitations. A particularly stark failure mode is the reversal curse (Berglund et al., 2024): an autoregressive LLM trained on a sentence of the form \"A is B\" will not automatically generalize to the reverse direction \"B is A\". For instance, if a model is trained on \"Valentina Tereshkova was the first woman to travel to space\", it will not automatically be able to answer the question, \"Who was the first woman to travel to space?\".\nThe reversal curse might be a manifestation of a fundamental limitation of autoregressive transform-ers (Zhu et al., 2024): they process, store and retrieve information in a strictly feedforward manner. Are there other manifestations of this limitation? One candidate is the ability for latent multihop reasoning. While LLMs excel at multi-hop questions (e.g. \"Who is the spouse of the performer of Imagine?\") when using chain-of-thought reasoning (CoT; Wei et al., 2023; Reynolds & McDonell, 2021), their performance is surprisingly poor when forced to reason without CoT. For example, LLMs can easily solve such questions with CoT (\u201cThe performer of Imagine is John Lennon, whose spouse is Yoko Ono\u201d), but fail frequently when required to answer immediately (\u201cYoko Ono\").\nPrevious work on two-hop reasoning in LLMs presents mixed evidence about latent reasoning capa-bilities across different models, datasets and reasoning elicitation methods (Press et al., 2023; Zhu et al., 2024; Yang et al., 2024; Allen-Zhu & Li, 2024; Wang et al., 2024). However, many of those studies are limited to toy models or toy datasets (Zhu et al., 2024; Wang et al., 2024) or do not fully control for memorization and reasoning shortcuts (Press et al., 2023; Yang et al., 2018). In the present paper, we use capable large language models, including Llama 3 8B Instruct (Dubey et al., 2024) and GPT-4o (OpenAI, 2024a), training them on new facts in natural language. We train mod-els on fictional facts to exclude the possibility of memorization and reasoning shortcuts that could develop during pretraining on large corpora. This setup ensures that high accuracy can be attributed exclusively to successfully performing latent two-hop reasoning.\nWe find that models we train achieve high CoT accuracy for answering two-hop questions about one-hop facts they learned from fine-tuning. However, their ability to perform latent reasoning (without CoT) shows a striking dichotomy: while models can successfully reason about facts that appear together during training or in the prompt, they completely fail when facts are learned separately. In the latter case, models not only fail to achieve above-chance no-CoT accuracy, but their test loss on two-hop no-CoT answers is nearly identical to loss computed on random test set responses (see Figure 2). These results suggest that LLMs lack a general capability for latent two-hop reasoning, exhibiting what we call the Two-Hop Curse a complete failure to compose separately learned facts without explicit CoT reasoning.\""}, {"title": "2 EXPERIMENT 1: TWO-HOP REASONING OVER FICTIONAL FACTS", "content": "Motivation Previous work on two-hop reasoning in LLMs has been limited by two key factors: (i) a focus on knowledge acquired during pretraining, where apparent success in evaluation might be due to confounding factors (e.g., memorization or reasoning shortcuts), and (ii) use of simplified toy setups that may be unrepresentative of the capabilities of modern LLMs. To address these limitations, we introduce a controlled experimental setting where we fine-tune capable language models (including Llama 3 8B Instruct and GPT-4o) on synthetic facts expressed in natural English. Following Berglund et al. (2023; 2024), we construct training data using diverse paraphrases of the same facts to ensure generalizable knowledge acquisition. By using fictional facts placed in separate documents, we ensure that high performance can only be attributed to successful two-hop reasoning rather than memorization or shortcuts. This setup allows us to cleanly investigate whether LLMs can perform latent two-hop reasoning over facts they have not seen together in the training data.\nExperimental setup We generate a dataset of entity triplets $(e_1, e_2, e_3)$, where $e_1, e_2, e_3$ are entities and each triplet's semantics are \"The spouse of $e_1$ is $e_2$. The birth city of $e_2$ is $e_3$\". We generate 693 entity triplets and divide them into a \u201cdemonstrated\u201d set (450) and an \u201cundemonstrated\u201d set (243) (see Table 1). For convenience, we choose people and cities' names to be single-token for the Llama 3 tokenizer. For each entity triplet, we generate four QA pairs: two one-hop questions and a two-hop question with no-CoT and CoT answers (see Figure 3). To increase diversity, we follow Berglund et al. (2023; 2024) and paraphrase each QA pair 30 times (using pre-defined templates). This yields a training dataset of 68,580 QA pairs."}, {"title": "3 EXPERIMENT 2: TWO-HOP REASONING WITH FACTS IN THE SAME\nDOCUMENT", "content": "Motivation While Experiment 1 showed that models completely fail at latent two-hop reasoning over facts that were learned through fine-tuning, we want to explore whether this limitation persists in easier settings. We investigate two such settings: (i) fine-tuning where related facts are always presented together in training documents, and (ii) in-context evaluation without fine-tuning, with facts provided in the prompt. These setups reduce the difficulty of retrieving relevant facts, while still requiring the model to perform latent reasoning to combine them.\nSetting 1: Fine-tuning with related facts in the same document We build on Experiment 1, modifying how one-hop QA pairs described in Section 2 are presented in the training data. Specifically, we ensure facts about $(e_1, e_2)$ and $(e_2, e_3)$ from the same triplet always appear in the same training document. We always put the facts in the logical order within a training document to avoid the reversal curse (Berglund et al., 2024). The rest of the setup is identical to Experiment 1.\nSetting 2: In-context evaluation with facts in the prompt We evaluate models without any fine-tuning by providing facts involving multiple entity triplets (10 distractors and 1 target triplet) in the prompt before asking a two-hop question. The distractor triplets are sampled randomly from other test triplets for every sample, and the order of facts in the context is randomized per sample. Figure 5 shows example formats for both settings."}, {"title": "4 EXPERIMENT 3: TWO-HOP CURSE FOR REAL-WORLD KNOWLEDGE", "content": "Motivation Our controlled experiments with fictional facts provide strong evidence that LLMs fail at latent two-hop reasoning when facts are learned separately. In this experiment, we aim to un-derstand how this limitation manifests in frontier LLMs' handling of real-world knowledge. While positive performance on real-world facts cannot be confidently attributed to two-hop reasoning (due to potential memorization and shortcuts), finding similar patterns of failure would provide indepen-dent evidence supporting the existence of the two-hop curse.\nSetup We evaluate 9 frontier models: the Claude 3 family (Anthropic, 2024), Llama 3.1 family (Dubey et al., 2024), and GPT-3.5-turbo, GPT-4o-mini and GPT-4o (OpenAI, 2024a) on a dataset of two-hop questions involving real-world facts compiled by Biran et al. (2024). For each question, we evaluate models both with and without CoT. Because we focus on evaluating two-hop reasoning, we only report performance on questions where models successfully answered single-hop questions about underlying facts. See more details on our setup in Appendix C.\nResults As shown in Figure 6, we find a significant gap between CoT and no-CoT performance across all models. This gap persists even for the most capable models like GPT-4o and Claude 3 Opus, supporting the existence of the two-hop curse beyond our controlled experimental setting."}, {"title": "5 EXPERIMENT 4: FAILING TO ELICIT TWO-HOP REASONING BY\nCONTROLLING MODEL INTERNALS", "content": "Having established that LLMs fail at latent two-hop reasoning when facts are learned separately and that this failure affects frontier LLMs, we now explore whether this limitation can be overcome by directly controlling model internals. We investigate two interventions: forcing facts to be stored in layers in the correct order for sequential reasoning, and providing activation-level supervision to encourage resolving bridge entities in latent space. Both interventions fail to elicit latent two-hop reasoning, suggesting the limitation is not easily overcome through architectural constraints or additional training objectives."}, {"title": "5.1 FORCING FACTS TO BE STORED IN THE RIGHT ORDER", "content": "Motivation Transformers are feed-forward neural networks a sequence of blocks that have to be traversed in a linear order for a given input. Moreover, previous work suggests that transformers store facts in a somewhat localised fashion, mostly in MLP layers of a few neighboring transformer blocks (Meng et al., 2023). Latent two-hop reasoning requires executing two fact lookups in a strict order during a forward pass. For a feed-forward neural network, this is only possible if the first fact (e.g. \"the performer of Imagine is John Lennon\") is stored in an earlier block than the second fact (e.g. \"the spouse of John Lennon is Yoko Ono\"). Otherwise, if the first fact is stored in a later block (e.g. 20th transformer block) and the second fact in an earlier block (e.g. 10th block), by the time a model completes the first lookup to resolve the bridge entity (\"John Lennon\"), the forward pass can no longer use the bridge entity to look up the second fact.\nIf facts were distributed uniformly across layers, they would happen to be in the right order half of the time. Therefore, if layer ordering was the only reason for poor two-hop performance, one would expect two-hop accuracy to be around 50%. In practice, this should be seen as a lower bound, since some facts might be represented redundantly, more than once.\nSetup We force localizing facts in particular layers by layer-selective finetuning, i.e. dividing our training distribution into three datasets and training separately on each, involving only a particular layer range at each stage:\n1. First one-hop facts (e.g. \"the performer of Imagine is John Lennon\") are learned with layers 0-12 (with other layers frozen)\n2. Second one-hop facts (e.g. \"the spouse of John Lennon is Yoko Ono\") are learned with layers 12-24 (with other layers frozen)\n3. Two-hop QA pairs are learned with all layers updated.\nTo mitigate catastrophic forgetting from only training on a single dataset at once, we repeat training stages (1)-(3) twice. Moreover, our training data uses the mixture described in the previous section: training on one-hop facts and both two-hop CoT and no-CoT QA pairs.\nResults We compare the following three setups:\n1. Baseline. This is the setup from Section 2, training on the full data mixture in a single stage with all layers trained.\n2. Staged, with all layers trained. This setup is a sanity check to show that staged training preserves most of the baseline's performance.\n3. Staged, layer-selective training. This is the intervention setup."}, {"title": "5.2 ACTIVATION SUPERVISION FOR TWO-HOP REASONING", "content": "Motivation The cross-entropy language modeling loss, used during LLM pretraining and super-vised fine-tuning, treats the LLM as a black box and only supervises how the input tokens in the prompt are mapped to output tokens. From success of CoT performance, we know that such super-vision is effective in teaching models to reason in explicit CoT. Since the reasoning trace is expressed in token space, the language modeling loss provides LLMs process-based supervision, giving useful gradients for each step of reasoning. However, for reasoning in latent space, the language modeling loss only provides outcome-based feedback (whether the predicted answer is correct) and is indiffer-ent to whether an LLM arrives at the answer via memorization or two-hop reasoning. This motivates an intervention trying to directly incentivize answering a question via two-hop reasoning\nSetup We add an auxiliary loss $L_{aux}$ that complements outcome-based supervision from the lan-guage modeling loss with process-based feedback in the activation space. More specifically, we encourage the model to resolve the bridge entity in activation space whenever it is prompted with a two-hop question. We encourage such resolution by ensuring that a given hidden state (output of a transformer block) is either similar to a vector representation of the bridge entity or predictive of it.\nWe apply the auxiliary objective to the output of a single transformer block at a single token position during fine-tuning. We sweep over several blocks to apply this loss on and choose block 10 (out of 32) of Llama 3 8B Instruct. To determine the token position to apply loss on, we look for the last token of the description of the bridge entity in the question, e.g. \u201cgine\u201d in \u201cWho is the spouse of the singer of the song Imagine?\". Let's call this activation vector h.\nWe consider two auxiliary objectives:\n1. Logit lens. This objective function is inspired by an interpretability technique (nostalge-braist, 2020) We compute logits y as $y = W U RMSNorm(h)$, where $RMSNorm(\u00b7)$ denotes the final RMSNorm (Zhang & Sennrich, 2019) layer of Llama 3 8B Instruct during train-ing. We then compute $L_{aux} = CE(e_2, y)$, where $CE(\u00b7)$ is the standard cross-entropy loss and $e_2$ is the token corresponding to bridge entity, e.g. \"John Lennon\u201d. This is possible because we ensure all bridge entities are single-token.\n2. Embed lens. We compute $L_{aux} = -CosSim(W_Ee_2, y)$, where $CosSim(\u00b7)$ is the cosine similarity loss and $W_Ee_2$ is the embedding of the bridge entity token.\nIn both cases, our final loss is computed as $L = L_{LM} + cL_{aux}$, where $L_{LM}$ is the standard language modelling loss and the coefficient c is a hyperparameter. Based on our sweeps, we found that 0.01 and 0.1 were the best settings for logit lens and embed lens, respectively. Once again, our training data uses the setup described for Hypothesis 2 experiments: training on one-hop facts and both two-hop CoT and no-CoT QA pairs.\nResults We compare the following three setups:\n1. Baseline: This is the setup from Figure ??, training on one-hop facts and both two-hop CoT and no-CoT QA pairs with just $L_{LM}$.\n2. Logit lens. This is the Logit lens setup, using the best coefficient c value from a sweep.\n3. Embed lens. This is the Embed lens setup, using the best coefficient c value from a sweep.\nAs seen in Figure 8, encouraging the model to resolve the bridge entity during its forward pass failed to elicit two-hop reasoning. As seen by the evaluation $L_{aux}$, learning to resolve bridge entities during training does not generalize to resolving other bridge entities on evaluation prompts despite the training $L_{aux}$ reaching zero.\""}, {"title": "6 DISCUSSION", "content": "In this section, we speculate about the reason why LLMs fail to compose facts learned separately without CoT. We also discuss limitations of our experimental setups and possible future directions."}, {"title": "6.1 HYPOTHESIS FOR THE CAUSE OF THE TWO-HOP CURSE", "content": "We hypothesize that the two-hop curse arises from a mismatch between how feedforward LLMs represent the bridge entity ($e_2$) during training on one-hop facts and during inference on two-hop questions (see Figure 9). During training, facts are learned independently \u2013 when learning \u201cRuss's spouse is Hay\u201d, the model learns to map $e_1$ (Russ) to $e_2$ (Hay), and separately when learning \"Hay was born in Showing\", it learns to map $e_2$ to $e_3$ (Showing). However, during two-hop inference without CoT, the model needs to use $e_2$ in two different roles: first as an output of a memory lookup when processing $e_1$, then as an input for looking up $e_3$. We speculate that this creates an information flow bottleneck - the representation of $e_2$ optimized for being an output (i.e. causing the LLM to produce high logits on the corresponding output token) may not be suitable for being an input for a fact lookup, breaking the chain of reasoning. This bottleneck does not exist when facts appear together during training (as the model can learn direct $e_1$ to $e_3$ mappings) or when using CoT (as $e_2$ is explicitly generated as a token that can then be consumed as input)."}, {"title": "6.2 LIMITATIONS OF OUR EXPERIMENTS", "content": "In this paper, we try to investigate the capabilities of LLMs in naturalistic settings, while controlling for confounders plaguing prior work. Reconciling the need for a clean setup and plausibility required several design choices that could be controversial.\nFine-tuning vs pretraining In order to have a clean experimental setup, we fine-tune models on fictional facts. However, one might worry that the cleanliness of this setup is fundamentally different from how knowledge is normally acquired by LLMs during pretraining (Jain et al., 2024). This difference might manifest in diversity of the data distribution and the scale of the training dataset.\nTo ensure the diversity of the training distribution, we include multiple (30) paraphrases of each fact, which leads language models to learn the underlying logical facts as opposed to just memorizing the"}, {"title": "7 RELATED WORK", "content": "Externalized reasoning Prompting LLMs to externalize their reasoning (or, \"think step by step\") has long been known to improve their performance on various reasoning tasks (Reynolds & McDonell, 2021; Wei et al., 2023; Kojima et al., 2024). This prompting strategy is known as \u201cchain-"}, {"title": "A MODEL VERSIONS", "content": "Tables below list the specific versions of all models used in our experiments."}, {"title": "B EXPERIMENTS 1, 2, 4: ADDITIONAL DETAILS ON FICTIONAL FACTS\nEXPERIMENTS", "content": "We include the training configuration used in fine-tuning experiments."}, {"title": "B.1 TRAINING DETAILS", "content": ""}, {"title": "B.2 EVALUATION DETAILS", "content": "For all synthetic experiments (Experiments 1, 2, and 4), we evaluate models using temperature 0 for deterministic generation. For evaluating knowledge of atomic facts, we use the same questions as those seen during training. For two-hop questions, we use questions not seen during training (the undemonstrated set). In both CoT and no-CoT evaluation, we use 20 few-shot examples, randomly sampled from the training dataset. These examples remain fixed throughout evaluation (i.e., the same examples are used for all test samples).\nFor no-CoT evaluation of open-weights models, we restrict the outputs to the set of test set answer options by only allowing tokens corresponding to valid answers (plus end-of-sequence tokens). If the response contains the correct answer ($e_3$), we additionally check that the response does not contain the intermediate step of reasoning ($e_2$).\nSystem prompts We use the same system prompts during training and evaluation:"}, {"title": "C EXPERIMENT 3: ADDITIONAL DETAILS ON REAL-WORLD KNOWLEDGE\nEXPERIMENTS", "content": "To evaluate two-hop reasoning capabilities on real-world knowledge, we use the dataset from Biran et al. (2024).\nWe exclude examples that clearly do not require two-hop reasoning, following Biran et al. (2024) in filtering out cases where models can answer two-hop questions without performing both hops. Specifically, we evaluate models on partial two-hop questions that omit either $e_1$ or the first relation-ship, and exclude cases where models succeed on these baseline conditions.\nFor each model and question type combination, we evaluate on n = 1500 random samples (of a total of 71569) from the dataset with temperature set to 0. In the results, we only report performance on those of these samples where a model successfully answered both underlying one-hop questions. This ensures that failures on two-hop questions can be attributed to reasoning limitations rather than lack of knowledge about the underlying facts."}, {"title": "C.1 EVALUATION PROMPTS", "content": "For the first-hop and second-hop questions, as well as the two-hop questions without CoT, we use the following system message:"}, {"title": "C.2 GRADING PROMPT", "content": "We use UK AISI's Inspect's (UK AI Safety Institute, 2024) model-grading to evaluate responses using GPT-4o as a judge. We use the following grading prompt template:"}, {"title": "D EXPERIMENT 1 ABLATION: IMPACT OF TRAINING DATA MIXTURE", "content": "To understand how different components of our training data mixture contribute to two-hop reason-ing capabilities, we conduct an ablation study with three variants of the training setup:\n1. Full mixture (our standard setup): Training on atomic facts and both two-hop CoT and no-CoT QA pairs\n2. No-CoT mixture: Training on atomic facts and only two-hop no-CoT QA pairs\n3. Atomic-only mixture: Training exclusively on atomic facts\nThis ablation shows that removing CoT training examples significantly impairs two-hop CoT per-formance while maintaining one-hop accuracy. Two-hop no-CoT performance remains at chance level regardless of the training mixture."}, {"title": "E EXPERIMENT 2: EFFECT OF DISTRACTORS ON SAME-DOCUMENT\nTWO-HOP REASONING", "content": "In Experiment 2 in Section 3, we investigate how models perform when the two relevant facts appear in the same document in subsequent sentences. However, in pretraining, facts in the same document might be separated by other content rather than directly following each other. To approximate this, we investigate how models perform when the two relevant facts in the training documents are sepa-rated by minimal distractor information. We explore two types of distractors:\n1. Semantically related but non-essential information about the entities, for example:\n\"Russ shares a marital bond with Hay. Russ is 1m 75cm tall and loves boulder-ing. Hay is slightly higher at 1m 77cm, and they often go climbing together. The city which saw the birth of Hay is Showing.\"\n2. Facts about N other random ($e_1, e_2, e_3$) triplets. For example, given a user message:\n\"Tell me who the following people are married to: Virgin, Russ, View, Just. Then tell me where those spouses were born.\"\nthe assistant message could contain facts about N = 3 random other triplets:\n\"Virgin is wedded to Ha. Russ calls Hay their spouse. View is united with Walking in wedded bliss. Just's marriage partner is Knight.\nHa was brought into existence in Active. Hay entered the world in Showing. The beginning of Walking's life was marked by the city of Nobody. The start of Knight's life journey is marked by the city of Crystal.\"\nThe bolded text serves as a distractor between the ($e_1, e_2$) fact and the ($e_2, e_3$) fact. The boldness is used here for illustration only."}, {"title": "F EXPERIMENT 3: ADDITIONAL PER-CATEGORY TWO-HOP RESULTS ON\nREAL FACTS", "content": "Figure 14: Per-category performance for all evaluated frontier models. Each subplot shows the distribution of performance across question categories for both CoT and no-CoT reasoning. Across all models, we observe that no-CoT performance is zero for most categories, while CoT performance maintains partial success across most categories. This pattern suggests that latent two-hop reasoning is not a general capability but depends heavily on the question category."}, {"title": "8 CONCLUSION", "content": "In this work, we introduce a controlled experimental setting to study two-hop reasoning in LLMs, where above-chance performance constitutes undeniable evidence of latent reasoning capability. Our results reveal a striking dichotomy: while models can perform latent two-hop reasoning when"}]}