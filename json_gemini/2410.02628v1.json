{"title": "INVERSE ENTROPIC OPTIMAL TRANSPORT\nSOLVES SEMI-SUPERVISED LEARNING\nVIA DATA LIKELIHOOD MAXIMIZATION", "authors": ["Mikhail Persiianov", "Nikita Andreev", "Dmitry Baranchuk", "Arip Asadulaev", "Nikita Starodubcev", "Anastasis Kratsios", "Evgeny Burnaev", "Alexander Korotin"], "abstract": "Learning conditional distributions $\\pi^*(\\cdot|x)$ is a central problem in machine learn-\ning, which is typically approached via supervised methods with paired data\n$(x, y) \\sim \\pi^*$. However, acquiring paired data samples is often challenging, espe-\ncially in problems such as domain translation. This necessitates the development\nof semi-supervised models that utilize both limited paired data and additional un-\npaired i.i.d. samples $x \\sim \\pi$ and $y \\sim \\pi$ from the marginal distributions. The\nusage of such combined data is complex and often relies on heuristic approaches.\nTo tackle this issue, we propose a new learning paradigm that integrates both\npaired and unpaired data seamlessly through the data likelihood maximization\ntechniques. We demonstrate that our approach also connects intriguingly with\ninverse entropic optimal transport (OT). This finding allows us to apply recent ad-\nvances in computational OT to establish a light learning algorithm to get $\\pi^*(\\cdot|x)$.\nFurthermore, we demonstrate through empirical tests that our method effectively\nlearns conditional distributions using paired and unpaired data simultaneously.", "sections": [{"title": "INTRODUCTION", "content": "Recovering conditional distributions $\\pi^*(y|x)$ from data is one of the fundamental problems in ma-\nchine learning, which appears both in predictive and generative modeling. In predictive modeling,\nthe standard examples of such tasks are the classification, where $x \\in \\mathbb{R}^{D_x}$ is a feature vector and\n$y\\in \\{0,1,..., K\\}$ is a class label, and regression, in which case $x$ is also a feature vector and\n$y \\in \\mathbb{R}^{D_y}$ is a real number. In generative modeling, both x and y are feature vectors in $\\mathbb{R}^{D_x}, \\mathbb{R}^{D_y}$,\nrespectively, representing complex objects, and the goal is to find a transformation between them."}, {"title": "BACKGROUND", "content": "First, we recall the formulation of the domain translation problem (\\S2.1). We remind the difference\nbetween its paired, unpaired, and semi-supervised setups. Next, we recall the basic concepts of the\ninverse entropic optimal transport, which are relevant to our paper (\\S2.2).\n2.1 DOMAIN TRANSLATION PROBLEMS\nThe goal of domain translation (DT) task is to transform data samples from the source domain to\nthe target domain while maintaining the essential content or structure. This approach is widely used\nin applications like computer vision (Zhu et al., 2017; Lin et al., 2018; Peng et al., 2023), natural\nlanguage processing (Jiang et al., 2021; Morishita et al., 2022), and audio processing (Du et al.,\n2022), etc. Domain translation task setups can be classified into supervised (or paired), unsupervised\n(or unpaired), and semi-supervised approaches based on the data used for training (see Figure 1).\nSupervised (Paired) Domain Translation relies on matched examples from both the source and\ntarget domains, where each input corresponds to a specific output, enabling direct supervision during\nthe learning process. Formally, this setup assumes access to a set of P empirical pairs $XY_{\\text{paired}} \\overset{\\text{def}}{=} \\{(x_1,y_1),...,(x_p,y_p)\\} \\sim \\pi^*$ from some unknown joint distribution. The goal here is to recover\nthe conditional distributions $\\pi^*(\\cdot|x)$ to generate samples $y_{x_{new}}$ for new inputs $x_{new}$ that are not\npresent in the training data. While this task is relatively straightforward to solve, obtaining such\npaired training datasets can be challenging, as it often involves significant time, cost, and effort.\nUnsupervised (Unpaired) Domain Translation, in contrast, does not require direct correspon-\ndences between the source and target domains (Zhu et al., 2017, Figure 2). Instead, it involves\nlearning to translate between domains using unpaired data, which offers greater flexibility but de-\nmands advanced techniques to achieve accurate translation. Formally, we are given Q unpaired\nempirical samples $X_{\\text{unpaired}} \\overset{\\text{def}}{=} \\{x_1,...,x_Q\\} \\sim \\pi_x^*$ from the source distribution and R unpaired\n$\\sim \\pi_y^*$\nsamples $Y_{\\text{unpaired}} \\overset{\\text{def}}{=} \\{y_1,\u2026\u2026\u2026,y_R\\}$\nfrom the target distribution. Our objective is to learn the\nconditional distributions $\\pi^*(\\cdot|x)$ of the unknown joint distribution $\\pi^*$, whose marginals are $\\pi_x^*, \\pi_y^*$,\nrespectively. Clearly, the primary challenge in unpaired setup is that the task is inherently ill-posed,\nleading to multiple potential solutions, many of which may be ambiguous or even not meaningful\n(Moriakov et al., 2020). Ensuring the translation's accuracy and relevance requires careful consid-\neration of constraints and regularization strategies to guide the learning process (Yuan et al., 2018).\nOverall, the unpaired setup is very important because of large amounts of unpaired data in the wild."}, {"title": "OPTIMAL TRANSPORT (OT)", "content": "The foundations of optimal transport (OT) are detailed in the seminal book by (Villani et al., 2009).\nFor a more comprehensive overview, we refer to (Santambrogio, 2015; Peyr\u00e9 et al., 2019).\nEntropic OT (Cuturi, 2013; Genevay, 2019). Consider source $\u03b1 \\in P_{ac}(\\mathcal{X})$ and target $\u03b2\\in P_{ac}(\\mathcal{Y})$\ndistributions. Let $c^* : \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$ be a cost function. The entropic optimal transport problem\nbetween distributions $\u03b1$ and $\u03b2$ is then defined as follows:\n$\\text{OTC}^*(\u03b1, \u03b2) \\overset{\\text{def}}{=} \\underset{\u03c0\\in \u03a0(\u03b1,\u03b2)}{\\text{min}} \\mathbb{E}_{x,y\\sim \u03c0}[c^*(x, y)] - \u03b5\\mathbb{E}_{x\\sim \u03b1}H(\u03c0(\\cdot|x)),$\nwhere $\u03b5 > 0$ is the regularization parameter. Setting $\u03b5 = 0$ recovers the classic OT formulation (Vil-\nlani et al., 2009) originally proposed by (Kantorovich, 1942). With mild assumptions, the transport\nplan $\u03c0^* \\in \u03a0(\u03b1, \u03b2)$ that minimizes the objective (1) exists uniquely. It is called the entropic OT plan.\nWe note that in the literature, the entropy regularization term in (1) is usually $-\u03b5H(\u03c0)$ or\n$+\u03b5KL(\u03c0||\u03b1 \u00d7 \u03b2)$. However, these forms are equivalent up to constants, see discussion in (Mokrov\net al., 2024, \\S2) or (Gushchin et al., 2023, \\S1). In our paper, we work only with formulation (1),\nwhich is also known as the weak form of the entropic OT, see (Gozlan et al., 2017; Backhoff-\nVeraguas et al., 2019; Backhoff-Veraguas & Pammer, 2022).\nDual formulation. With mild assumptions on $c^*, \u03b1, \u03b2$, the following dual OT formulation holds:\n$\\text{OTC}^*(\u03b1, \u03b2) = \\underset{f}{\\text{sup}} \\{\\mathbb{E}_{x\\sim \u03b1}f^{c^*}(x) + \\mathbb{E}_{y\\sim \u03b2}f(y),\\}$\nwhere f ranges over a certain subset of continuous functions (dual potentials) with mild assumptions\non their boundness, see (Backhoff-Veraguas & Pammer, 2022, Eq. 3.3) for details. The term $f^{c^*}$\nrepresents the so-called weak entropic $c^*$ -transform of f, defined as:\n$f^{c^*} (x) \\overset{\\text{def}}{=} \\underset{\u03b2\\in P(\\mathcal{Y})}{\\text{min}} \\{\\mathbb{E}_{y\\sim \u03b2}[c^*(x, y)] - \u03b5H (\u03b2) - \\mathbb{E}_{y\\sim \u03b2}f(y)\\}.$\nIt has closed-form (Mokrov et al., 2024, Eq. 14), which is given by\n$f^{c^*} (x) = -\u03b5 \\text{log} \\int_\\mathcal{Y} \\text{exp} \\Big( \\frac{f(y) - c^*(x,y)}{\u03b5} \\Big) dy.$\nInverse entropic OT. The forward OT problem (1) focuses on determining the OT plan $\u03c0^*$ given\na predefined cost function $c^*$. In contrast, the inverse problem provides the learner with a joint\ndistribution $\u03c0^*$ and requires finding a cost function $c^*$ such that $\u03c0^*$ becomes the OT plan between\nits marginals, $\u03c0_x^*$ and $\u03c0_y^*$. This setup leads to the formulation of the inverse entropic OT problem,\nwhich can be expressed as the following minimization problem:\n$c^* \\in \\text{arg} \\underset{c}{\\text{min}} \\Big( \\mathbb{E}_{x,y\\sim \u03c0^*} [c(x, y)] - \u03b5\\mathbb{E}_{x\\sim \u03c0_x^*}H (\u03c0^*(\\cdot|x))\\Big)  \u2265 \\text{OTC}_{c,\u03b5}(\u03c0_x^*, \u03c0_y^*),$\nwhere c skims through measurable functions $\\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$. The expression within the paren-\ntheses denotes the entropic transport cost of the plan $\u03c0^*$ in relation to the cost c between the\nmarginals $\u03c0_x^*$ and $\u03c0_y^*$, thus ensuring that it is always greater than or equal to the optimal cost\n$\\text{OTC}_{c,\u03b5}(\u03c0_x^*, \u03c0_y^*)$. Consequently, the minimum achievable value for the entire objective is zero, which\noccurs only when $\u03c0^*$ corresponds to the optimal transport plan for the selected cost $c^*$. Here, the\nterm $-\u03b5\\mathbb{E}_{\u03c0^*} H (\u03c0(\\cdot|x))$ can be omitted, as it does not depend on c. Additionally\n\u2022 Unlike the forward OT problem (1), the entropic regularization parameter $\u03b5 > 0$ here plays no\nsignificant role. Indeed, by substituting $c(x, y) = \u03b5c'(x, y)$ and multiplying the entire objective\n(5) by $\\frac{1}{\u03b5}$, one gets the inverse OT problem for $c'$. Hence, the problems associated with different\n$\u03b5$ are equivalent up to the change of variables, which is not the case for the forward OT (1).\n$\\text{\u2022 The inverse problem admits several possible solutions } c^*$. For example, $c^*(x, y) =\n-\u03b5 \\text{log} \u03c0^* (x, y)$ provides the minimum, which can be verified through direct substitution. Sim-\nilarly, cost functions of the form $c^*(x,y) = \u2212\u03b5 \\text{log} \u03c0^*(x, y) + u(x) + v(y)$ are also feasible,\nas adding terms dependent only on x or y does not alter the OT plan. In particular, when\n$u(x) = \u03b5 \\text{log} \u03c0^*(x)$ and $v(y) = 0$, one gets $c^* (x, y) = \u2212\u03b5 \\text{log} \u03c0^*(y|x)$."}, {"title": "SEMI-SUPERVISED DOMAIN TRANSLATION VIA INVERSE ENTROPIC OT", "content": "In \\S3.1, we develop our proposed loss function that seamlessly integrates both paired and unpaired\ndata samples. In \\S3.2, we demonstrate that derived loss is inherently linked to the inverse entropic\noptimal transport problem (5). In \\S3.3, we introduce lightweight parametrization to overcome chal-\nlenges associated with optimizing the loss function. All our proofs can be found in Appendix A.\n3.1 Loss DERIVATION\nPart I. Data likelihood maximization and its limitation. Our goal is to approximate the true\ndistribution $\u03c0^*$ by some parametric model $\u03c0^\u03b8$, where $\u03b8$ represents the parameters of the model. To\nachieve this, we would like to employ the standard KL-divergence minimization framework, also\nknown as data likelihood maximization. Namely, we aim to minimize\n$KL (\u03c0^*||\u03c0^\u03b8) = \\mathbb{E}_{x,y\\sim \u03c0^*} \\text{log} \\frac{\u03c0^*(x)\u03c0^*(y|x)}{\u03c0_\u03b8(x) \u03c0_\u03b8(y|x)} = \\mathbb{E}_{x\\sim \u03c0_x^*} \\text{log} \\frac{\u03c0^*(x)}{\u03c0_\u03b8(x)} + \\mathbb{E}_{x,y\\sim \u03c0^*} \\text{log} \\frac{\u03c0^*(y|x)}{\u03c0_\u03b8(y|x)}$\n$KL (\u03c0^*||\u03c0_x^\u03b8) + \\mathbb{E}_{x\\sim \u03c0_x^*}\\mathbb{E}_{y\\sim \u03c0^*(\\cdot|x)} \\text{log} \\frac{\u03c0^*(y|x)}{\u03c0_\u03b8(y|x)} = KL (\u03c0^*||\u03c0_x^\u03b8) + \\mathbb{E}_{x\\sim \u03c0_x^*}\\text{KL} (\u03c0^*(\\cdot|x)||\u03c0_\u03b8(\\cdot|x)).$\nMarginal\nConditional\nIt is clear that objective (6) splits into two independent components: the marginal and the condi-\ntional matching terms. Our focus will be on the conditional component $\u03c0_\u03b8(\\cdot|x)$, as it is the necessary\npart for the domain translation. Note that the marginal part $\u03c0_x^\u03b8$ is not actually needed. The condi-\ntional part of (6) can further be divided into the following two terms:\n$\\mathbb{E}_{x\\sim \u03c0_x^*}\\mathbb{E}_{y\\sim \u03c0^*(\\cdot|x)} [\\text{log} \u03c0^*(y|x) - \\text{log} \u03c0_\u03b8(y|x)] = -\\mathbb{E}_{x\\sim \u03c0_x^*}H (\u03c0^*(\\cdot|x)) - \\mathbb{E}_{x,y\\sim \u03c0^*} \\text{log} \u03c0_\u03b8(y|x).$\nThe first term is independent on $\u03b8$, so we obtain the following minimization objective\n$L(\u03b8) \\overset{\\text{def}}{=} -\\mathbb{E}_{x,y\\sim \u03c0^*} \\text{log} \u03c0_\u03b8(y|x).$\nIt is important to note that minimizing (8) is equivalent to maximizing the conditional likelihood, a\nstrategy utilized in conditional normalizing flows (CNFs) (Papamakarios et al., 2021). However, a\nmajor limitation of this approach is its reliance solely on paired data from $\u03c0^*$, which can be difficult\nto obtain in real-world scenarios. In the following section, we modify this strategy to incorporate\navailable unpaired data within a semi-supervised learning setup (see \\S2.1).\nPart II. Solving the limitations via smart parameterization. To address the above-mentioned\nissue and leverage unpaired data, we first use Gibbs-Boltzmann distribution density parametrization:\n$\u03c0_\u03b8 (y|x) \\overset{\\text{def}}{=} \\frac{\\text{exp}(-E_\u03b8 (y|x))}{Z_\u03b8 (x)},$\nwhere $E_\u03b8(\\cdot|x) : \\mathcal{Y} \\rightarrow \\mathbb{R}$ is the Energy function, and $Z_\u03b8 (x) \\overset{\\text{def}}{=} \\int_\\mathcal{Y} \\text{exp} (-E_\u03b8 (y|x)) dy$ is the normal-\nization constant (LeCun et al., 2006). Substituting (9) into (8), we get\n$-\\mathbb{E}_{x,y\\sim \u03c0^*} \\text{log} \u03c0_\u03b8 (y|x) = \\mathbb{E}_{x,y\\sim \u03c0^*}E_\u03b8 (y|x) + \\mathbb{E}_{x\\sim \u03c0_x^*} \\text{log} Z_\u03b8 (x).$\nThis objective already provides an opportunity to exploit the unpaired samples from the marginal\ndistribution $\u03c0_x^*$ to learn the conditional distributions $\u03c0_\u03b8(\\cdot|x) \u2248 \u03c0^*(\\cdot|x)$. Namely, it helps to estimate\nthe part of the objective related to the normalization constant $Z_\u03b8$. To incorporate separate samples\nfrom the second marginal distribution $\u03c0_y^*$, it is essential to choose a parametrization that allows to\ndetach from the energy function $E_\u03b8 (y|x)$ the term depending solely on y. Thus, we propose:\n$E_\u03b8 (y|x) \\overset{\\text{def}}{=} \\frac{c_\u03b8 (x, y) - f_\u03b8 (y)}{\u03b5}$"}, {"title": "RELATION TO INVERSE ENTROPIC OPTIMAL TRANSPORT", "content": "In this section, we show that (5) is equivalent to (12). Indeed, directly substituting the dual form of\nentropic OT (2) into the inverse entropic OT problem (5) with the omitted entropy term yields:\n$\\underset{c}{\\text{min}} \\Big \\{ \\mathbb{E}_{x,y\\sim \u03c0^*} [c(x, y)] - \\underset{f}{\\text{sup}} \\Big \\{ \\mathbb{E}_{x\\sim \u03c0_x^*} f^{c^*}(x) + \\mathbb{E}_{y\\sim \u03c0_y^*} f(y) \\Big \\} \\Big \\} =$\n$\\underset{c, f}{\\text{min}} \\Big \\{ \\mathbb{E}_{x,y\\sim \u03c0^*} [c(x, y)] - \\mathbb{E}_{x\\sim \u03c0_x^*} f^{c^*}(x) - \\mathbb{E}_{y\\sim \u03c0_y^*} f(y) \\Big \\}.$\nNow, let's assume that both $c$ and $f$ are parameterized as $c_\u03b8$ and $f_\u03b8$ with respect to a parameter $\u03b8$.\nBased on the definition provided in (4) and utilizing our energy function parameterization from (11),\nwe can express $(f_\u03b8)^{c_\u03b8} (x)$ as follows:\n$(f_\u03b8)^{c_\u03b8} (x) = -\u03b5 \\text{log} \\int_\\mathcal{Y} \\text{exp} \\Big( \\frac{f_\u03b8 (y) - c_\u03b8 (x,y)}{\u03b5} \\Big) dy = -\u03b5 \\text{log} Z_\u03b8 (x).$\nThis clarification shows that the expression in (13) aligns with our proposed likelihood-based loss\nin (12), scaled by $\u03b5$. This finding indicates that inverse entropic optimal transport (OT) can be inter-\npreted as a likelihood maximization problem, which opens up significant avenues to leverage estab-\nlished likelihood maximization techniques for optimizing inverse entropic OT, such as the evidence\nlower bound methods (Barber, 2012; Alemi et al., 2018) and expectation-maximization strategies\n(MacKay, 2003; Bishop & Bishop, 2023), etc.\nMoreover, this insight allows us to reframe inverse entropic OT as addressing the semi-supervised\ndomain translation problem, as it facilitates the use of both paired data from $\u03c0^*$ and unpaired data\nfrom $\u03c0_x^*$ and $\u03c0_y^*$. Notably, to our knowledge, the inverse OT problem has primarily been explored in\ndiscrete learning scenarios that assume access only to paired data (refer to \\S4)."}, {"title": "PRACTICAL LIGHT PARAMETERIZATION AND OPTIMIZATION PROCEDURE", "content": "The most computationally intensive aspect of optimizing the loss function in (12) lies in calculating\nthe integral for the normalization constant $Z_\u03b8$. To tackle this challenge, we propose a lightweight pa-\nrameterization that yields closed-form expressions for each term in the loss function. Our proposed\ncost function parameterization $c_\u03b8$ is grounded in the LOG-SUM-EXP function (Murphy, 2012), which\nis widely recognized in the deep learning community for its practical advantages:\n$c_\u03b8 (x, y) = - \\text{log} \\sum_{m=1}^M v_m(x) \\text{exp} \\Big ( \\frac{(b_m(x), y)}{\u03b5}\\Big ),$"}, {"title": "RELATED WORKS", "content": "We review below the most related semi-supervised models and OT-based approaches to our work.\nSemi-supervised models. As mentioned in \\S1, many existing semi-supervised domain translation\nmethods combine paired and unpaired data by incorporating multiple loss terms into complex opti-\nmization objectives (Jin et al., 2019, \\S3.3), (Tripathy et al., 2019, \\S3.5), (Mustafa & Mantiuk, 2020,\n\\S3.2), (Paavilainen et al., 2021, \\S2), (Panda et al., 2023, Eq. 8), (Tang et al., 2024, Eq. 8). However,\nthese approaches often require careful tuning of hyperparameters to balance the various loss terms.\nThe recent work by (Gu et al., 2023) utilizes both paired and unpaired data to build a transport plan\nbased on key-point guided OT, initially introduced in (Gu et al., 2022). This transport plan is used\nas a heuristic to train a conditional score-based model on unpaired or semi-paired data. Overall, we\nnote that the idea of applying OT in a semi-supervised manner traces back to the seminal work by\n(Courty et al., 2016), although their focus was on classification, not domain translation.\nAnother recent work by (Asadulaev et al., 2024) introduces a neural network-based OT framework\nfor semi-supervised scenarios, utilizing general cost functionals for OT. However, their method re-\nquires manually constructing cost functions which can incorporate class labels or predefined pairs.\nIn contrast, our approach adjusts the cost dynamically during training."}, {"title": "EXPERIMENTAL ILLUSTRATIONS", "content": "We tested our solver on both synthetic data (\\S5.1) and real-world data distributions (\\S5.2). The code\nis written using the PyTorch framework and will be made publicly available. It is provided in the\nsupplemental materials. Experimental details are given in Appendix B.\n5.1 SWISS ROLL"}, {"title": "WEATHER PREDICTION", "content": "Here we aim to evaluate our proposed approach on real-world data. We consider the weather predic-\ntion dataset (Malinin et al., 2021; Rubachev et al., 2024). The data is collected from weather stations\nand weather forecast physical models. It consists of 94 meteorological features, e.g., pressure, wind,\nhumidity, etc., which are measured over a period of one year at different spatial locations.\nSetup. Initially, the problem was formulated as the prediction and uncertainty estimation of the\nair temperature at a specific time and location. We expand this task to the probabilistic prediction\nof all meteorological features, thereby reducing reliance on measurement equipment in remote and\ndifficult-to-access locations, such as the Polar regions.\nIn more detail, we select two distinct months from the dataset and translate the meteorological fea-\ntures from the source month (January) to the target month (June). To operate at the monthly scale,\nwe represent a source data point $x \\in \\mathbb{R}^{188}$ as the mean and standard deviation of the features col-\nlected at a specific location over the source month. The targets $y \\in \\mathbb{R}^{94}$ correspond to individual\nmeasurements in the target month. Pairs are constructed by aligning a source data point with the\ntarget measurements at the same location. Consequently, multiple target data points y may corre-\nspond to a single source point x and represent samples from conditional distributions $\u03c0^*(y|x)$. The\nmeasurements from non-aligned locations are treated as unpaired.\nWe obtain 500 unpaired and 192 paired data samples. For testing, 100 pairs are randomly selected.\nWe evaluate the performance of our approach by calculating the log-likelihood on the test target\nfeatures. A natural baseline for this task is a probabilistic model that maximizes the likelihood of\nthe target data. Thus, we implement an MLP that learns to predict the parameters of a mixture of\nGaussians and is trained on the paired data only via the log-likelihood optimization (8)."}, {"title": "DISCUSSION", "content": "Limitations. A limitation of our approach is that it uses the Gaussian Mixture parameterization\nfor conditional distributions. This may limit its scalability. As a promising avenue for future work\nis incorporation of the more general parameterizations, such as neural networks, which are already\nwell-studied in the context of forward entropic OT, see (Mokrov et al., 2024).\nPotential impact. Our framework has a simple and non-minimax optimization objective that seam-\nlessly incorporates both unpaired and paired samples into the training. We expect that these ad-\nvantages will encourage the use of our framework to develop other max-likelihood-based semi-\nsupervised approaches based on more advanced (than Gaussian mixtures) techniques, e.g., energy-\nbased models (LeCun et al., 2006; Du & Mordatch, 2019), diffusion models (Ho et al., 2020), etc.\nBroader impact. This paper presents work whose goal is to advance the field of Machine Learn-\ning. There are many potential societal consequences of our work, none of which we feel must be\nspecifically highlighted here.\nReproducibility Statement For all the presented experiments, a full set of hyperparameters is intro-\nduced either in \\S5 or in Appendix B. In addition, the code is submitted as supplementary material,\nwith guidelines on how to run every experiment included."}]}