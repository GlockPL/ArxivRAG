{"title": "Efficient Standardization of Clinical Notes using Large Language Models", "authors": ["Daniel B. Hier", "Michael D. Carrithers", "Thanh Son Do", "Tayo Obafemi-Ajayi"], "abstract": "Clinician notes are a rich source of patient information but often contain inconsistencies due to varied writing styles, colloquialisms, abbreviations, medical jargon, grammatical errors, and non-standard formatting. These inconsistencies hinder the extraction of meaningful data from electronic health records (EHRs), posing challenges for quality improvement, population health, precision medicine, decision support, and research.\nWe present a large language model approach to standardizing a corpus of 1,618 clinical notes. Standardization corrected an average of 4.9 \u00b1 1.8 grammatical errors, 3.3 \u00b1 5.2 spelling errors, converted 3.1 \u00b1 3.0 non-standard terms to standard terminology, and expanded 15.8\u00b19.1 abbreviations and acronyms per note. Additionally, notes were re-organized into canonical sections with standardized headings. This process prepared notes for key concept extraction, mapping to medical ontologies, and conversion to interoperable data formats such as FHIR.\nExpert review of randomly sampled notes found no significant data loss after standardization. This proof-of-concept study demonstrates that standardization of clinical notes can improve their readability, consistency, and usability, while also facilitating their conversion into interoperable data formats.\nClinical relevance-Standardizing clinician notes using large language models improves the quality, accessibility, and usability of critical patient information, supporting better patient outcomes and enhancing data interoperability.", "sections": [{"title": "I. INTRODUCTION", "content": "Electronic Health Records (EHRs) have revolutionized healthcare documentation, improving both the legibility and accessibility of patient data [1]. However, there has been a profound rise in physician dissatisfaction with EHRS and an associated increase in clinician burnout. EHRs have addressed two major challenges: the \u201cavailability problem\u201d (paper patient charts were frequently unavailable) and the \"legibility problem\u201d (handwritten notes were notoriously difficult to read) [2]\u2013[4]. Physician dissatisfaction stems from an onerous documentation burden, the shift of clerical tasks to physicians, and poor EHR design. Additional factors include complex EHR interfaces, excessive workloads, time pressures, fatigue from automated alerts, and the perception that EHRs do not enhance patient care quality. Many physicians also feel inadequately trained and supported in the use of their EHR [5]\u2013[11].\nThe use of large language models in healthcare is rapidly expanding. These models support critical applications such as decision support, diagnosis explanation, clinical text summarization, concept and information extraction, machine-coding of documents, concept mapping to ontologies, and the conversion of documents into exchangeable formats [12]\u2013[17]. By automating labor-intensive documentation processes, large language models help address the documentation burden.\nWe use \"standardization\" to refer to the process of enhancing the structural and linguistic integrity of a clinical note without altering its clinical content. Standardization focuses on improving format, spelling, grammar, terminology, usability, interoperability, and readability. It includes replacing abbreviations (e.g., OU \u2192 both eyes), substituting conventional terms for colloquial terms (e.g., upgoing toe \u2192 Babinski sign), and correcting misspellings (e.g., vscalar \u2192 vascular). We hypothesize that a large language model such as GPT-4 can perform note standardization efficiently and accurately.\nPhysicians and other clinicians create clinical notes through direct text entry (typing), dictation, copy-and-paste from other notes, and automated text insertion from structured information held elsewhere in the EHR. Multiple problems plague the current use of clinical notes in the EHR [18], including non-grammatical forms, colloquialisms, acronyms [19], non-standard terminology, slang [20], jargon [21], [22], euphemisms, misspellings [23], and ambiguous language [20], [21], [23]\u2013[25]. Studies have shown that as many as 20% of text is abbreviations [26], which range from unapproved and non-standard to cryptic and dangerous (e.g., MS for \u201cmental status\", \"morphine sulfate,\" or \"multiple sclerosis\") [27]\u2013[32]. Notes may be poorly organized and lack proper headers for important sections such as the history, examination, impression, and plan [10], [18]. Note-writing skills vary by clinician, with some adept while others struggle [33]. Most physician notes are free text and lack machine-codes from standard medical terminologies like ICD, LOINC, RxNorm, or SNOMED CT [22], [34]\u2013[36]. Some notes are verbose, while others lack sufficient detail.\nThe standardization of clinical notes using large language models offers a path to address several documentation challenges that undermine the usability, quality, and exchange-ability of physician notes. Key problems addressable by large language models include:"}, {"title": null, "content": "\u2022 Expand abbreviations to their full forms, mitigating confusion caused by ambiguous or non-standard acronyms (e.g., \"MS\" expanded to \"multiple sclerosis\u201d or \u201cmental status\" based on context). Although \"alerts\" have been tried to reduce the use of unapproved abbreviations, they have not been well-accepted and have not demonstrated changes in physician behavior [27].\n\u2022 Correct typographical, spelling, and grammatical errors to enhance note readability, quality, and professionalism [37].\n\u2022 Identify and replace slang, colloquialisms, and other non-standard terms, improving the precision and clarity of notes (e.g., \"heart attack\u201d replaced by \u201cmyocardial infarction\"). Dictation, though widely used, often introduces unwanted word substitutions that change the meaning of text [38]. Large language models can catch and correct these substitutions.\n\u2022 Improve note organization by structuring notes into canonical sections (e.g., HISTORY, EXAMINATION, IMPRESSION, PLAN), making them more navigable and interpretable for healthcare providers.\n\u2022 Identify mappable medical entities such as signs, symptoms, medications, and diagnoses that can be linked to machine codes from appropriate medical ontologies.\n\u2022 Create outputs suitable for searches and data exchanges such as JSON, enabling semi-structured data retrieval, integration into databases, and preparing notes for conversion into formats compatible with exchange standards such as HL7-FHIR [12], [39], [40].\nThe goal of note standardization is to improve the structure, terminology, and readability of clinical notes without altering their underlying clinical content."}, {"title": "II. METHODS", "content": "This study explores the application of GPT-4 to standardize 1618 de-identified clinical notes from a Neurology Clinic. We hypothesized that standardization can effectively improve the structure, clarity, and usability of physician notes while supporting downstream integration into alternative data formats. Our aim was to standardize a corpus of physician notes containing grammatical errors, spelling errors, excessive abbreviations, and poor formatting. The standardization process was executed using the GPT-4 API, guided by appropriate instructions.\nData Acquisition: Consent was obtained from the University of Illinois IRB to use clinical notes from the Neurology Clinic for research purposes. All notes were deidentified by REDCap [41]. Notes were selected to carry neuroimmuno-logical diagnoses such as multiple sclerosis or Guillain-Barr\u00e9 syndrome. REDCap provided 21,028 notes for the years 2016 to 2022 with neuroimmunological diagnoses. We filtered those notes with the following requirements: outpatient notes only, notes from the Neurology Clinic only, physician notes only, and a length of at least 2000 characters. This yielded a final dataset of 1618 physician clinical notes (mean characters 6423\u00b13689). All were typed by the resident or attending physician; none were dictated or transcribed. REDCap provided us with a CSV file with a field for the note text as unformatted ASCII text. Each note was assigned an accession number and was saved to file as a JSON-compatible object with the format:\nNote Standardization: The GPT-4 API was iteratively prompted to standardize each source note with the following prompt:\nprompt = (You are a highly skilled medical terminologist specializing in clinical note standardization. Your task is to standardize this note and adhere to guidelines:\nOutput was saved as a JSON-compatible object with the following structure:"}, {"title": "III. RESULTS AND DISCUSSION", "content": "We used a large language model (GPT-4) to standardize 1618 physician notes. GPT-4 was prompted to convert a source note into a standardized note. The source note was free text in ASCII format. The standardized note was a formatted JSON object with canonical headings of HISTORY, EXAMINATION, IMPRESSION, and PLAN as well as appropriate sub-headings. GPT-4 was further prompted to correct spelling errors, expand abbreviations and acronyms, and disambiguate ambiguous abbreviations based on context. Mean note length was approximately 6,420 characters. GPT-4 corrected 4.9 \u00b1 1.8 grammatical errors per note, corrected 3.3 \u00b15.2 spelling errors, identified 3.1 \u00b1 3.0 non-standard terms per note and made appropriate substitutions with medical terms, and expanded 15.8\u00b19.1 abbreviations and acronyms per note.\nAll standardized notes were reviewed by a human expert for completeness, formatting, and accuracy. We used GPT-4 to compare the 'source' notes to the standardized notes with regards to the following: text organization, spelling and grammar, abbreviation expansion, and terminology standardization. Measures were rated on a five-point Likert-like scale from 1 = 'poor' to 5 = 'excellent'. In a subset of 20 source notes that were reviewed in detail and compared to standardized notes, no loss of clinical content or detail was noted.\nBy standardizing unstructured clinical notes into semi-structured standarized notes, we were able to use GPT-4 to perform semi-structured data retrieval on designated parts of the standarized note (Fig. 8. We were able to extract signs and symptoms from the HISTORY, EXAMINATION, and IMPRESSION sections of the notes (Fig. 7) and planned medications from the PLAN section of the standarized note (Fig. I). Note normalization enables the extraction of clinically relevant signs, symptoms, and findings from unstructured text. These findings can be mapped to concepts in appropriate medical ontologies such as SNOMED CT or these findings can be converted into HL7 FHIR-compatible 'observations,' facilitating seamless data exchange [39], [40], [42].\nWhile large language models offer promising solutions to some challenges in the usability and quality of clinical notes, significant systemic issues remain beyond their current capabilities. One persistent problem is the widespread use of copy-and-paste practices by clinicians, which can propagate outdated, erroneous, or irrelevant content. Large language models have limited ability to discern whether copy-pasted content is clinically appropriate or needs to be updated [43]. Similarly, large language models cannot reliably identify questionable documentation practices, such as omitting care rendered or documenting care that was not performed [44], [45]. Another challenge arises from the carry-forward of data, such as medication lists or problem histories, which may persist in notes without verification. Large language models cannot ascertain the accuracy of such information, particularly when clinical context is insufficient. While standardization improves note readability and usability, it does not inherently reduce documentation burden. Addressing this burden requires systemic changes, including improved user interface design, revised documentation guidelines, clinician training, and organizational support [9], [46]\u2013[48]. Although large language models can often disambiguate ambiguous terms and abbreviations based on context, they cannot resolve these ambiguities when the context is unclear or incomplete. These limitations highlight the need for complementary approaches to clinical documentation. Systemic improvements must address fundamental questions, such as workforce deployment (who documents what), documentation requirements (what needs to be documented), and workflows (how documentation occurs). These efforts, combined with technological advances, are essential to improving the quality and efficiency of clinical documentation.\nThis study has several limitations. The dataset was limited to 1618 physician notes from a neurology clinic, primarily with a diagnosis of multiple sclerosis. It is desirable to generalize these findings to a larger corpus of notes, with more diverse diagnoses, with different note types, and from varied clinical settings.\nAs proof-of-concept, we demonstrated that standardized notes could be used for semi-structured data retrieval so that we were able to extract medications (Fig. 6) and signs and symptoms (Fig. 7) from the notes. However, developing workflows that streamline the extraction of clinical information from standardized notes and converting key findings (e.g., signs, symptoms, diagnoses) into either concepts from medical ontologies (e.g., LOINC, SNOMED CT, RxNorm, etc.) or into HL7 FHIR-compatible resources will require significant additional effort and validation.\nWe did not perform a detailed analysis of the computational costs or processing times for note normalization. Preliminary estimates, based on consumer-grade workstations and the GPT-4 API, indicate an average processing time of approximately 20 seconds per note, with costs ranging from 0.01 to 0.10 USD per note, assuming typical note lengths of 500 to 1,000 tokens. These figures align with current GPT-4 pricing but may vary depending on note complexity, token usage, and future pricing models. While these estimates suggest that note normalization is feasible, scaling to high-volume clinical environments (e.g., 1,000 to 10,000 notes per day) will require further analysis and optimization to manage costs.\nOur study used de-identified clinical notes, ensuring adherence to HIPAA privacy standards. However, this approach did not address the need to obtain explicit consent from physicians or patients. While de-identification mitigates many privacy concerns, future studies should implement additional safeguards to address patient privacy. Note identification is not possible in a active clinical setting. Furthermore, when physician notes are reconfigured, explicit consent or institutional approval from clinicians may be necessary.\nNote standardization transforms source notes, often stored as unstructured ASCII text, into structured formats with canonical headings. This demonstrates the potential of reconfiguring clinical notes to meet diverse needs. The field of clinical documentation is increasingly focused on note templates, template engineering, and note quality improvement [49]\u2013[53]. Once clinical text is digitized, large language models can reformat notes into various styles and templates tailored to specific use cases, enhancing their adaptability and utility."}, {"title": "IV. CONCLUSIONS", "content": "Note standardization using large language models offers a promising approach that enhances the readability, usability, quality, and interoperability of physician notes. Large language models excel at improving note structure, expanding abbreviations, replacing slang and jargon with standardized terminology, and correcting spelling and grammatical errors while preserving clinical content and clinician intent. By standardizing clinical notes, large language models can facilitate semi-structured data retrieval, ready notes for the mapping of terms to concepts in medical ontologies, and prepare notes for compliance with emerging interoperability standards such as FHIR [40]."}]}