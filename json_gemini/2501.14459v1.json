{"title": "Interpretability Analysis of Domain Adapted Dense Retrievers", "authors": ["G\u00f6ksenin Y\u00fcksel", "Jaap Kamps"], "abstract": "Dense retrievers have demonstrated significant potential for neural\ninformation retrieval; however, they exhibit a lack of robustness to\ndomain shifts, thereby limiting their efficacy in zero-shot settings\nacross diverse domains. Previous research has investigated unsu-\npervised domain adaptation techniques to adapt dense retrievers to\ntarget domains. However, these studies have not focused on explain-\nability analysis to understand how such adaptations alter the model's\nbehavior. In this paper, we propose utilizing the integrated gradients\nframework to develop an interpretability method that provides both\ninstance-based and ranking-based explanations for dense retrievers.\nTo generate these explanations, we introduce a novel baseline that\nreveals both query and document attributions. This method is used\nto analyze the effects of domain adaptation on input attributions\nfor query and document tokens across two datasets: the financial\nquestion answering dataset (FIQA) and the biomedical informa-\ntion retrieval dataset (TREC-COVID). Our visualizations reveal that\ndomain-adapted models focus more on in-domain terminology com-\npared to non-adapted models, exemplified by terms such as \"hedge,\"\n\"gold,\" \"corona,\" and \"disease.\" This research addresses how unsuper-\nvised domain adaptation techniques influence the behavior of dense\nretrievers when adapted to new domains. Additionally, we demon-\nstrate that integrated gradients are a viable choice for explaining and\nanalyzing the internal mechanisms of these opaque neural models.", "sections": [{"title": "1 INTRODUCTION", "content": "One of the main recent achievements in information retrieval (IR) is\nthe development of neural dense retrieval methods. These methods\nare extremely fast and do not entail the memory and computational\noverhead associated with widely used other neural methods such\nas cross-encoder and late interaction models [21]. However, dense\nretrieval models face the challenging task of independently mapping\ninputs to a meaningful vector space, making them highly sensitive\nto domain shifts [15]. This non-robustness to domain shifts impedes\ntheir application in zero-shot settings [16, 18], posing challenges\nin real-world applications where access to extensive and domain-\nspecific training data is limited [9]. Methods known as \u201cdomain\nadaptation\" have been developed to address this issue.\nTo tackle the domain shift problem with dense retrievers, pre-\nvious work has fine-tuned these models on target datasets using\nunsupervised and supervised learning objectives [10, 15, 17]. Super-\nvised methods utilize labeled data to further fine-tune these models\nin novel domains, which is not possible for every task or domain\nof IR research due to the costs and difficulty of obtaining human-\nannotated labels [19]. In contrast, unsupervised methods assume\nonly the availability of the target corpus [10, 17, 19], employing pre-\ntraining objectives [4, 6, 7] or pseudo-labeling [10, 17] to fine-tune the\npre-trained models in a new domain without requiring labeled data.\nNotably, Wang et al. [17] found that pre-training objectives alone do\nnot enhance the out-of-domain performance of the adapted model.\nPrior work on domain adaption focuses on retrieval effectiveness\n(e.g. Table 1 discussed below), but has not addressed the interpretabil-\nity of domain-adapted models. Moreover, no model-introspective\nanalysis was conducted to understand changes in the models' in-\nner workings before and after domain adaptation. Given the in-\nherent opacity and lack of transparency of these neural models,\ninterpretability analysis is crucial to understand the effect of domain\nadaptation on such dense retrievers. This paper conducts initial\ninterpretability analysis experiments, and proposes to use of the\nIntegrated Gradients (IG) [14] framework to develop an interpretabil-\nity method for dense retrievers, providing both instance-based and\nranking-based explanations. Subsequently, we apply this method to\ndomain-adapted and non-domain-adapted dense retrievers to assess\nthe differences in their behavior using input-based attributions. We\nset out to answer two research questions. (i) How can Integrated Gra-\ndients be utilized in the dense retriever setting? (ii) How does domain\nadaptation influence the input attributions of the models?\nIn this paper, we first demonstrate that IG is a viable approach\nfor interpreting dense retrievers. We then utilize the proposed in-\nterpretability method to analyze input attribution differences in\nthe FIQA and TREC-COVID datasets, which represent two distinct\ndomains in IR: financial question answering and biomedical infor-\nmation retrieval. Our visualizations reveal that the domain-adapted\nmodel concentrates more on in-domain terminology, and title which\nthe unadapted model tends to overlook."}, {"title": "2 RELATED RESEARCH", "content": null}, {"title": "2.1 Unsupervised Domain Adaptation", "content": "Query Generation (QG): QG methods construct synthetic train-\ning data by using documents from the target domain to generate\ncorresponding (pseudo) queries, aiming to augment the training\ndata with queries that fit the target domain. QGen [10] trains an\nauto-encoder in the source domain to generate synthetic questions\nfrom a target domain document. They use binary-level relevancy\nlabels to train the networks on generated query-document pairs.\nSimilarly, GPL [17] generates synthetic queries with a pre-trained\nT5 model but uses cross-encoders to label the relevancy of generated\nquery-document pairs. This method extends and over performs the\nQGen method by replacing binary relevance with continuous labels\nranging from -inf to inf.\nKnowledge Distillation (KD): KD is a commonly used strategy\nin the dense retriever setting, which utilizes a powerful model as the\nteacher model to improve the capabilities of the student models [5]. It\nhas been found that such a technique can improve out-of-domain per-\nformance as well. GPL [17] and AugSBERT [15] use cross-encoders\nto annotate unlabeled synthetic query-doc pairs. Later, this knowl-\nedge is distilled into the dense retriever by training the model on\ngenerated labels. Different from the above methods, SPAR [3] pro-\nposes to distill knowledge from BM25 to the dense retriever model\nto integrate sparse retrieval."}, {"title": "2.2 Interpretability methods for IR", "content": "Interpretability of ranking models focuses on building models that\ncan either be analyzed for interpretability in a post-hoc fashion or\nare interpretable by design.\nPost-hoc interpretability methods explain the decisions of already\ntrained machine learning models. These approaches are either model-\nagnostic, where the interpretability approach has no access to the\ntrained model parameters [8, 12], or model-introspective, with full ac-\ncess to the parameters of the underlying model [13, 14]. There exists\na lot of work on model-introspective analysis of neural models for dif-\nferent tasks. These analyses use different methods like probing tasks,\nattention weights, or state activations [1]. A recent dominant class of\nmodel-introspective explanation outputs feature attributions. Most\nof these approaches utilize gradient-based attribution methods [1].\nTo our knowledge, only Zhan et al. [20] used IG to obtain feature\nattributions for a BERT-based cross-encoder model. In their exper-\niments, they used an empty query baseline and an empty document\nbaseline, which are padding tokens."}, {"title": "3 METHODOLOGY", "content": "Dense Retriever In this paper, we use pre-indexing of the docu-\nments. The pre-indexed document embeddings are used to retrieve\ntop-K passages using dot product similarity between document and\nquery embeddings. The query and document embeddings are ob-\ntained using a DistilBERT model, which has already been fine-tuned\non MSMARCO to retrieve relevant documents to a query [2].\nWe use the dense retriever models that are tuned to work with\nthe dot product as their similarity measure. We utilize open sourced\nGPL models and use SentenceTransformer [11] framework. For all\nthe GPL models, maximum sequence length is set to 350. We use\nmean pooling over output token embeddings, disregarding the spe-\ncial tokens such as [CLS] and [SEP]. For TREC-COVID, and FIQA\ndataset we evaluate provided the models, and asses the performance\nimprovement on the corresponding test sets.\nBaseline As stated by Sundararajan et al. [14], a good baseline should\ngive a score of zero and should convey an empty signal. Inspired by\nthe recent work in cross-encoder explanations, we use the [PAD]\ntoken to create our baseline. However, we deviate from them by\nintroducing a new method to calculate the query and document\nattributions. To calculate the query token attributions, we replace\nthe query tokens with the [PAD] tokens and leave the document to-\nkens untouched. Then, we replace the document tokens with [PAD]\ntokens and leave the query tokens untouched. We run the Integrated\nGradient analysis for both the query and the document baselines.\nThis method calculates the input attributions for both the query and\nthe document tokens.\nRanking Analysis The aforementioned method only works for\ninstance-based explanations. However, in the information retrieval\nsetting, we are also interested in the explainability of the document\nranking. For this task, we select the top 25 documents retrieved by\nthe model for a specific query. We aggregate the token attributions\nover the top retrieved documents by summing them up to generate\nthe overall attributions of a token. This way, we get the most impor-\ntant tokens for the ranking. The tokens that appear often in the top\nrankings and contribute positively get a higher attribution score for\nthe ranking overall. We use word cloud visualization, the word sizes\nare determined by the summed attribution over 25 documents."}, {"title": "4 RESULTS", "content": "Our experiments, shown in Table 1 before, confirm the effectiveness\nof domain adaption observed in earlier research, on TREC-COVID\nNDCG@10 improves from 65.1 to 71.6 (+6.5 abs., +10%) and on FIQA\nfrom 26.7 to 36.8 (+10.1 abs., +38%). With the proposed method,\nwe now analyze if we can interpret how the domain adaptation is\naffecting the model."}, {"title": "4.1 Input Attribution", "content": "Figures 1 and 3 show the attribution analysis for the baseline model.\nFigures 1a and 3a display the positive and negative attributions for\nboth query and document pairs in models trained on MSMARCO us-\ning DistilBERT. Positive attributions enhance the similarity between\nthe query and the document, while negative attributions lowers it.\nIn Figure 1a, the query tokens [\"complications,\" \"co,\" \"##vid\"] con-\ntribute positively. Additionally, the document tokens [\"co,\" \"##vid,\"\n\"disease\"] also contribute positively, whereas [\"neon\"] contributes\nnegatively.\nIn Figure 3a, the query tokens [\"invest,\" \"gold,\" \"best\"] contribute\npositively, while [\"against,\" \"?\"] contribute negatively. Furthermore,\nthe document tokens [\"gold,\" \"without,\" \"buy\"] contribute positively,\nwhereas [\"g,\" \"sg\"] contribute negatively.\nFor both queries, the DistilBERT model effectively matches query\nterms with document terms. Tokens that appear in both the query\nand the document receive high positive attribution scores. As ex-\npected, we observe no attributions for the [CLS] and [SEP] tokens.\nRanking Based Attribution Figures 1b, 1c, 3b, and 3c depict the\nword clouds of positive and negative attributions revealed by the\nproposed method in a ranking scenario.\nDespite the query not explicitly mentioning [\"corona,\" \"disease\"],\nFigure 1b illustrates that these are important words identified by\nthe model. Another notable finding is the appearance of \"complications\"\nin the negative attribution word cloud shown in Figure 1c, which is\na term present in the query text.\nFor the FIQA dataset, Figures 3b and 3c show that the model as-\nsigns positive attributions to document tokens [\"what,\" \"is,\" \"french\"]\nand negative attributions to [\"financial,\" \"finance\"]. Additionally,\n\"gold\" appears among the negatively contributing terms."}, {"title": "4.2 Domain Adaptation", "content": "Figures 2 and 4 show the attribution analysis after domain adaptation.\nIn Figure 2a, the query tokens [\"what,\" \"co,\" \"##vid\"] contribute\npositively. Additionally, the document tokens [\"pregnancy,\" \"conse-\nquences,\" \"corona\"] contribute positively. The query contribution of\n\"complications\" has decreased compared to the non-domain-adapted\nmodel, while the contribution of \"what\" has increased. Furthermore,\nthe domain-adapted model assigns more importance to the first sen-\ntence, which is the title of the paper. The attribution for document\ntokens [\"corona,\" \"consequences,\" \"pregnant\"] has also increased.\nIn Figure 4a, the query tokens [\"invest,\" \"gold,\" \"hedge\"] contribute\npositively, whereas [\"against,\" \"inflation\"] contribute negatively. Ad-\nditionally, the document tokens [\"gold,\" \"invest,\" \"buy\"] contribute\npositively, while [\"resources,\" \"is\"] contribute negatively. The query\ncontribution of \"inflation\" has decreased after domain adaptation,\nwhereas \"best\" has increased. Moreover, the attribution for document\ntokens [\"gold,\" \"alternative,\" \"g\"] has increased, whereas [\"resources,\"\n\"without\"] has decreased.\nBoth models display positive attributions for the input tokens in\nthe sentence, \"you can invest in if you want to invest in gold without\nphysically owning gold.\"\nRanking For TREC-COVID, Figures 1b and 2b illustrate that both\nthe non-domain-adapted model and the domain-adapted model place\nemphasis on \"corona\" and \"pan.\" A notable finding is the increase\nin attribution to \"treatment\" and the appearance of \"post\" in the\ndomain-adapted model.\nRegarding negative attributions, Figures 1c and 2c show that\nthe non-domain-adapted model focuses more on \"establishment,\" \"affect,\"\nand other non-related query words. In contrast, the domain-adapted\nmodel focuses on \"contracted\" and \"implications.\" In both\ncases, they do not assign negative attributions to query-related terms.\nAdditionally, the domain-adapted model identifies \"2019\" as a neg-\natively contributing token to the ranking.\nFor FIQA, Figures 3b and 4b reveal that the domain-adapted model\nplaces greater emphasis on \"hedge,\" \"gold,\" and \"over.\" Conversely,\nthe non-domain-adapted model places more emphasis on \"is\" and\n\"french.\" Furthermore, Figures 3c and 4c show that the non-domain-adapted\nmodel assigns high negative attribution to \"finance\" and \"financial,\"\nwhereas the domain-adapted model assigns high negative attribution\nto \"best\" and \"trying.\" In this scenario, the domain-adapted\nmodel focuses more on query-related terms such as \"hedge\" and\n\"gold\" compared to the non-domain-adapted model.\nTitle attributions Figure 5 depicts that domain adapted model puts\npositive attribution to the title compared to base model. The base\nmodel puts significantly negative attribute to the title. This quanti-\ntative finding is consistent with the qualitative observations above."}, {"title": "5 CONCLUSIONS AND DISCUSSION", "content": "This paper proposes to use Integrated Gradients for instance-based\nand ranking-based explanations of dense retrievers. We find that\nIntegrated Gradients is a feasible choice for the interpretability of\ndense retriever models. Our initial findings show that dense retriever\nmodels are capable of both soft matching and term matching as ex-\npected. For example, even though the query may not contain the\nword \"volatile,\" documents containing \"volatile\" are ranked higher,\nwith a positive attribution score assigned to the term. Furthermore,\nwe observe that negative attributions are also applicable in the dense\nretriever setting, with certain tokens contributing negatively to the\nsimilarity score. As illustrated in Figure 1, these are typically non-\nrelevant tokens for the query.\nWe find that domain-adapted models tend to place more positive\nattributions on domain-specific vocabulary, such as \"corona\" and\n\"hedge,\" compared to the baseline model. Additionally, the domain-adapted\nmodel better captures the dependencies of document terms\nlike \"treatment\" and \"rehabilitation,\" which are relevant to the query\nbut not explicitly stated in the query terms, compared to the non-domain-adapted\nmodel. Moreover, the positive attribution given to\nthe title by the domain-adapted model in the TREC-COVID domain\nsuggests that the model relies more on the document title to generate\nsimilarity scores compared to the non-domain-adapted model. This\nbehavior may be influenced by the domain adaptation process in\nGPL models, where the title is concatenated at the beginning of the\ndocument [17]. Thus, the model may learn to focus more on the\nbeginning of the document, recognizing the title as an important\nelement for retrieving research papers related to the query."}, {"title": "A LIMITATIONS", "content": "Interpretable and explainable neural ranking models is an important,\nbut also very hard to study problem in IR. Our initial experiments in\nthis paper merely aim to raise interest in, and show the viability of,\ninterpretability analysis of dense retrievers. As there is no standard\nquantitative way of evaluating attributions produced by IG, we opted\nfor a deep qualitative analysis of a small sample of queries. We plan\nto considerably expand this in future work. We also plan to compare\nthe IG method to other interpretability methods, such as [8] or [12],\nin future research.\nTo gain a comprehensive understanding of the white/black box\nmodel, we also want to expand the analysis to the global attributions\nof models. However, IG is not capable of producing such global expla-\nnations. We aim to extend this our research beyond the top ranked"}]}