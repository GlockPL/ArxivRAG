{"title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers", "authors": ["Juncan Deng", "Shuaiting Li", "Zeyu Wang", "Hong Gu", "Kedong Xu", "Kejie Huang"], "abstract": "The Diffusion Transformers Models (DiTs) have transitioned\nthe network architecture from traditional UNets to transform-\ners, demonstrating exceptional capabilities in image gener-\nation. Although DiTs have been widely applied to high-\ndefinition video generation tasks, their large parameter size\nhinders inference on edge devices. Vector quantization (VQ)\ncan decompose model weight into a codebook and assign-\nments, allowing extreme weight quantization and signifi-\ncantly reducing memory usage. In this paper, we propose\nVQ4DiT, a fast post-training vector quantization method for\nDiTs. We found that traditional VQ methods calibrate only\nthe codebook without calibrating the assignments. This leads\nto weight sub-vectors being incorrectly assigned to the same\nassignment, providing inconsistent gradients to the codebook\nand resulting in a suboptimal result. To address this challenge,\nVQ4DiT calculates the candidate assignment set for each\nweight sub-vector based on Euclidean distance and recon-\nstructs the sub-vector based on the weighted average. Then,\nusing the zero-data and block-wise calibration method, the\noptimal assignment from the set is efficiently selected while\ncalibrating the codebook. VQ4DiT quantizes a DiT XL/2\nmodel on a single NVIDIA A100 GPU within 20 minutes\nto 5 hours depending on the different quantization settings.\nExperiments show that VQ4DiT establishes a new state-of-\nthe-art in model size and performance trade-offs, quantizing\nweights to 2-bit precision while retaining acceptable image\ngeneration quality.", "sections": [{"title": "1 Introduction", "content": "Advancements in pre-trained text-to-image diffusion mod-\nels (Ho, Jain, and Abbeel 2020; Ho et al. 2022; Ramesh\net al. 2022; Rombach et al. 2022; Saharia et al. 2022)\nhave facilitated the successful generation of images that are\nboth complex and highly faithful to the input conditions.\nRecently, Diffusion Transformers Models (DiTs) (Peebles\nand Xie 2023) have garnered significant attention due to\ntheir superior performance, with OpenAI's SORA (Brooks\net al. 2024) being one of the most prominent applications.\nDiTs are constructed by sequentially stacking multiple trans-\nformer blocks. This architectural design leverages the scal-\ning properties of transformers (Carion et al. 2020; Touvron\net al. 2021; Xie et al. 2021; Liu et al. 2021), allowing for\nmore flexible parameter expansion to achieve enhanced per-\nformance. Compared to other UNet-based diffusion mod-\nels, DiTs have demonstrated the ability to generate higher-\nquality images while having more parameters.\nDeploying DiTs can be costly due to their large number\nof parameters and high computational complexity, which is\nsimilar to the challenges encountered with Large Language\nModels (LLMs). For example, generating a 256 \u00d7 256 res-\nolution image using the DiT XL/2 model can take over 17\nseconds and require 105 Gflops on an NVIDIA A100 GPU.\nMoreover, the video generation model SoRA (Brooks et al.\n2024), designed concerning DiTs, contains approximately 3\nbillion parameters. Due to this significant parameter count,\ndeploying them on edge devices with limited computational\nresources is impractical.\nTo overcome the deployment challenges, recent research\nhas focused on the efficient deployment of diffusion mod-\nels, particularly through model quantization (Li et al. 2023a,\n2024; He et al. 2024; Wang et al. 2024). Post-training quan-\ntization (PTQ) is the most widely used technique because it\nrapidly quantizes the original model using a small calibra-\ntion set without requiring multiple iterations of fine-tuning\n(Yuan et al. 2022; Li et al. 2023a). Meanwhile, vector quan-\ntization (VQ) has been shown to compress CNN models to\nextremely low bit-width (Gersho and Gray 2012; Stock et al.\n2019), which could also be advantageous for DiTs. The clas-\nsic VQ approach maps the weight sub-vectors of each layer\nto a codebook and assignments using clustering techniques\nsuch as the K-Means algorithm (Han, Mao, and Dally 2015),\nand the codebook is continuously updated.\nHowever, existing quantization methods have several lim-\nitations. First, they cannot be directly applied to DiTs, which\nhave different network structures and algorithmic concepts\ncompared to UNet-based diffusion models. Second, PTQ\nmethods significantly reduce model accuracy when quantiz-\ning weights to extremely low bit-width (e.g., 2-bit). Third,\ntraditional VQ methods only calibrate the codebook without\nadjusting the assignments, leading to incorrect assignment\nof weight sub-vectors, which provides inconsistent gradi-\nents to the codebook and ultimately results in suboptimal\noutcomes.\nTo overcome these limitations, we introduce a novel post-\ntraining vector quantization technique for the extremely low\nbit-width quantization of DiTs, named VQ4DiT. VQ4DiT"}, {"title": "2 Backgrounds and Related Works", "content": "UNet-based diffusion models have garnered significant at-\ntention, and research has begun to explore the adoption of\ntransformer architectures (Rombach et al. 2022; Croitoru\net al. 2023; Yang et al. 2023) within diffusion models. Re-\ncently, Diffusion Transformer Models (DiTs) (Peebles and\nXie 2023) have achieved state-of-the-art performance in im-\nage generation. Notably, DiTs demonstrate scalability in\nterms of model size and data representation similar to large\nlanguage models, making them widely applicable to image\nand video generation tasks (Brooks et al. 2024; Liu et al.\n2024; Zhu et al. 2024).\nDiTs consist of N blocks, each containing a Multi-Head\nSelf-Attention (MHSA) and a Pointwise Feedforward (PF)\nmodule (Vaswani et al. 2017; Dosovitskiy et al. 2021; Pee-\nbles and Xie 2023), both preceded by their respective adap-\ntive Layer Norm (adaLN) (Perez et al. 2018). The structure\nof the DiT block is illustrated in Figure 1 (A). These blocks\nsequentially process the noised latent and conditional in-\nformation, encoded as tokens in a lower-dimensional latent\nspace (Rombach et al. 2022).\nIn each block, the conditional embedded information $c\\in\\mathbb{R}^{d_{in}}$ is converted into scale and shift parameters $(\\gamma, \\beta \\in \\mathbb{R}^{d_{in}})$, which are regressed through MLPs and then injected\ninto the noisy latent $z \\in \\mathbb{R}^{n\\times d_{in}}$ via adaLN:\n$\\begin{cases}\n(\\gamma, \\beta) = MLP(c)\\\\\nadaLN(z) = LN(z) \\odot (1 + \\gamma) + \\beta\n\\end{cases}$\nwhere $LN(\\cdot)$ denotes the Layer Norm (Ba, Kiros, and Hinton\n2016). These adaLN modules dynamically adjust the layer\nnormalization before each MHSA and PF module, enhanc-\ning DiTs' adaptability to varying conditions and improving\nthe generation quality.\nDespite their effectiveness, DiTs demand substantial com-\nputational resources to generate high-quality images, which\nposes challenges to their deployment on edge devices. In\nthis paper, we propose an extremely low bit-width quanti-\nzation method for DiTs that significantly reduces both time\nand memory consumption, without the need for a calibration\ndataset."}, {"title": "2.2 Model Quantization", "content": "Let $W\\in \\mathbb{R}^{o\\times i}$ denote the weight, where $o$ represents the\noutput channel and $i$ denotes the input channel. A stan-\ndard symmetric uniform quantizer approximates the original"}, {"title": "3 Challenges of Vector Quantization for DiTs", "content": "As illustrated in Table 1, we apply the classic uniform quan-\ntization (UQ) and vector quantization (VQ) to the DiT XL/2\nmodel. At the same bit-width, VQ results in a much smaller\nquantization error compared to UQ. The number of code-\nwords $k$ and their dimension $d$ significantly impact both\nthe memory usage of VQ and the quantization error of the\nweights. Increasing $k$ and $d$, while keeping the memory us-\nage of assignments constant, reduces the quantization error.\nHowever, this also increases the memory usage of the code-\nbook, which is particularly problematic in per-layer VQ. Ad-\nditionally, increasing $k$ and $d$ prolongs the runtime of the\nclustering algorithm and increases the subsequent calibra-\ntion times. These factors necessitate a careful trade-off be-\ntween quantization error and codebook size.\nWe utilize $k = 256$ and $d = 4$ for 2-bit quantization, $k =$\n$64$ and $d = 2$ for 3-bit quantization. The memory usage of\nthe codebooks is negligible when compared to the memory\nrequirements for assignments."}, {"title": "3.2 Setups of codebooks and assignments", "content": "There are various methods to achieve VQ, one popular\nmethod being the K-Means algorithm (Han, Mao, and Dally\n2015). However, the quantization error of the weights can"}, {"title": "4 VQ4DiT", "content": "To address the identified challenges, we propose a novel\nmethod for efficiently and accurately vector quantizing\nDiTs, named Efficient Post-Training Vector Quantization\nfor Diffusion Transformers (VQ4DiT). The description of\nVQ4DiT is visualized in Figure 1 (B) and (C). In Section\n4.1, we decompose the weights of each layer of the model\ninto a codebook and candidate assignment sets, initializing\neach candidate assignment with an equal ratio. In Section\n4.2, we introduce a zero-data and block-wise calibration\nstrategy to calibrate codebooks and candidate assignment\nsets, ultimately selecting the optimal assignments with the\nhighest ratios."}, {"title": "4.1 Initialization of Codebooks and Candidate\nAssignment Sets", "content": "As shown in Equation 3, the codebook C and assignments A\nof each layer can be optimized by minimizing the following\nobjective function:\n$||W - C[A]||_2 = \\sum_{o,i/d} || w_{o,i/d} - c(a_{o,i/d}) ||_1^2,$\nwhich can be efficiently minimized by the K-Means algo-\nrithm. However, Table 2 demonstrates that the strategy of\nfine-tuning only the codebook is not effective for DiTs. Our\napproach considers how to calibrate both the codebook and\nthe assignments simultaneously.\nFor each weight sub-vector, we calculate its Euclidean\ndistance to all codewords, obtaining the indices of the top"}, {"title": "4.2 Zero-data and block-wise Calibration", "content": "Training DiTs typically relies on the ImageNet dataset (Rus-\nsakovsky et al. 2015). Due to its large number of images\nand substantial memory usage, calibrating quantized mod-\nels using this dataset poses significant challenges. To more\nefficiently quantize DiTs, we propose a zero-data and block-\nwise calibration strategy, which aligns the performance of\nquantized models with that of floating-point models without\nrequiring a calibration set.\nSpecifically, given the same input to both the floating-\npoint model and the quantized model, the mean square er-\nror between the outputs of each DiT block at each timestep\nis computed to calibrate the codebook and the ratios of\nthe candidate assignments for each layer. It is important to\nnote that the input for the initial timestep is Gaussian noise\n$\\{epsilon\\} \\sim \\mathcal{N}(0, I)$, and the inputs for subsequent timesteps are\nthe outputs of the floating-point model from the previous\ntimestep. This ensures that the quantized model does not\nsuffer from calibration collapse due to cumulative quantiza-\ntion errors and that the output styles of the quantized model\nremain similar. Given the latent code $z$ of an image and\nits paired conditional information $y \\in \\{1,...,1000\\}$, the\nblock-wise calibration function is computed as:\n$L_d = \\mathbb{E}_{z,y,d,t}\\sum_{l=1}^L || d_{fp}^l(z_t, y, t, W) - d_q^l(z_t, y, t, \\hat{W}) ||_2^2,$\nwhere $z_t$ represents a noisy latent at timestep $t \\sim$\nUniform$(1,T)$, and $d_{fp}^l(\\cdot)$ and $d_q^l(\\cdot)$ represent the $l$-th\nDiT block from the floating-point model and the quantized\nmodel, respectively."}, {"title": "5 EXPERIMENTS", "content": "The validation setup is gener-\nally consistent with the settings used in the original DiT pa-\nper (Peebles and Xie 2023). We select the pre-trained DiT\nXL/2 model as the floating-point reference model, which"}, {"title": "5.2 Main Results", "content": "Tables 3 and 4 show the quantization results of the DiT XL/2\nmodel on the ImageNet 256\u00d7256 and 512\u00d7512 datasets us-\ning different sample timesteps and weight bit-widths. At a\nresolution of 256\u00d7256, our VQ4DiT achieves performance\nclosest to that of the FP model compared to other methods.\nSpecifically, RepQ-ViT, GPTQ, and Q-DiT undergo a sig-\nnificant performance drop under 3-bit quantization, which\nworsens as the number of timesteps decreases. In contrast,\nthe FID increases for VQ4DiT by less than 5.3, and the IS\ndecreases by less than 7.7. The metrics of VQ4DiT are very\nclose to those of the FP model, indicating that our method\napproaches lossless 3-bit compression.\nWhen the bit-width is reduced to 2, the other three al-\ngorithms completely collapse. VQ-DiT significantly outper-\nforms the other three methods, with its precision decreas-\ning by only 0.012 compared to 3-bit quantization. Figure 2\nshows the generated images by each algorithm, highlight-\ning VQ4DiT's ability to generate high-quality images even\nat extremely low bit-widths.\nMoreover, the validation results at a resolution of\n512x512 mirror those at 256x256, with our VQ4DiT con-\nsistently demonstrating the best performance. This indicates\nthat VQ4DiT can generate high-quality and high-resolution\nimages with minimal memory usage, which is crucial for\ndeploying DiTs on edge devices."}, {"title": "5.3 Ablation Study", "content": "To verify the efficacy of our algorithm, we conduct an ab-\nlation study on the challenging 2-bit quantization. In Table\n5, we evaluate the different lengths of candidate assignment\nsets. The detailed result indicates that as n increases from 1\nto 2, the performance progressively improves, validating the\neffectiveness of the assignment calibration. Notably, when\nn = 3, the model demonstrates the most significant per-\nformance gain, reducing the FID by 47.97 and the sFID by\n30.83. However, as n increases to 4, the performance wors-\nens, suggesting that excessive candidate assignments nega-\ntively impact calibration convergence.\nTo assess whether the optimal assignments yield more ac-\ncurate gradients for the codebook, We calculate the gradi-\nents of the sub-vectors associated with each codeword with-\nout calibrating the codebook of each layer. As illustrated in"}, {"title": "6 Conclusion", "content": "In this paper, we propose a novel post-training vector quan-\ntization method, VQ4DiT, for the efficient quantization of\nDiffusion Transformers Models (DiTs). Our analysis iden-\ntifies two main challenges when applying vector quantiza-\ntion (VQ) to DiTs: the need to balance the codebook size\nwith quantization error, and the possibility that different sub-\nvectors with the same assignment might provide inconsis-\ntent gradient directions to the codeword. To address these\nchallenges, we first calculate a candidate assignment set for\neach sub-vector. We then design a zero-data and block-wise\ncalibration process to progressively calibrate each layer's\ncodebook and candidate assignment sets, ultimately lead-\ning to optimal assignments and codebooks. Experimental re-\nsults demonstrate that our VQ4DiT method effectively quan-\ntizes DiT weights to 2-bit precision while maintaining high-\nquality image generation capabilities."}, {"title": "7.2 Deployment Setup", "content": "To improve inference speed, we implemented a CUDA vec-\ntor quantization kernel for vector-vector multiplication be-\ntween sub-vectors of quantized weights and sub-vectors of\nactivations. Small-sized codebooks are loaded into shared\nmemory to reduce bandwidth pressure. All computations are\nperformed in FP32. As shown in Figure 6, when using the\nkernel, the inference time of the quantized model is reduced\nto approximately one-third of the original."}]}