[{"title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers", "authors": ["Juncan Deng", "Shuaiting Li", "Zeyu Wang", "Hong Gu", "Kedong Xu", "Kejie Huang"], "abstract": "The Diffusion Transformers Models (DiTs) have transitioned\nthe network architecture from traditional UNets to transform-\ners, demonstrating exceptional capabilities in image gener-\nation. Although DiTs have been widely applied to high-\ndefinition video generation tasks, their large parameter size\nhinders inference on edge devices. Vector quantization (VQ)\ncan decompose model weight into a codebook and assign-\nments, allowing extreme weight quantization and signifi-\ncantly reducing memory usage. In this paper, we propose\nVQ4DiT, a fast post-training vector quantization method for\nDiTs. We found that traditional VQ methods calibrate only\nthe codebook without calibrating the assignments. This leads\nto weight sub-vectors being incorrectly assigned to the same\nassignment, providing inconsistent gradients to the codebook\nand resulting in a suboptimal result. To address this challenge,\nVQ4DiT calculates the candidate assignment set for each\nweight sub-vector based on Euclidean distance and recon-\nstructs the sub-vector based on the weighted average. Then,\nusing the zero-data and block-wise calibration method, the\noptimal assignment from the set is efficiently selected while\ncalibrating the codebook. VQ4DiT quantizes a DiT XL/2\nmodel on a single NVIDIA A100 GPU within 20 minutes\nto 5 hours depending on the different quantization settings.\nExperiments show that VQ4DiT establishes a new state-of-\nthe-art in model size and performance trade-offs, quantizing\nweights to 2-bit precision while retaining acceptable image\ngeneration quality.", "sections": [{"title": "1 Introduction", "content": "Advancements in pre-trained text-to-image diffusion mod-\nels (Ho, Jain, and Abbeel 2020; Ho et al. 2022; Ramesh\net al. 2022; Rombach et al. 2022; Saharia et al. 2022)\nhave facilitated the successful generation of images that are\nboth complex and highly faithful to the input conditions.\nRecently, Diffusion Transformers Models (DiTs) (Peebles\nand Xie 2023) have garnered significant attention due to\ntheir superior performance, with OpenAI's SORA (Brooks\net al. 2024) being one of the most prominent applications.\nDiTs are constructed by sequentially stacking multiple trans-\nformer blocks. This architectural design leverages the scal-\ning properties of transformers (Carion et al. 2020; Touvron\net al. 2021; Xie et al. 2021; Liu et al. 2021), allowing for\nmore flexible parameter expansion to achieve enhanced per-\nformance. Compared to other UNet-based diffusion mod-\nels, DiTs have demonstrated the ability to generate higher-\nquality images while having more parameters.\nDeploying DiTs can be costly due to their large number\nof parameters and high computational complexity, which is\nsimilar to the challenges encountered with Large Language\nModels (LLMs). For example, generating a 256 \u00d7 256 res-\nolution image using the DiT XL/2 model can take over 17\nseconds and require 105 Gflops on an NVIDIA A100 GPU.\nMoreover, the video generation model SoRA (Brooks et al.\n2024), designed concerning DiTs, contains approximately 3\nbillion parameters. Due to this significant parameter count,\ndeploying them on edge devices with limited computational\nresources is impractical.\nTo overcome the deployment challenges, recent research\nhas focused on the efficient deployment of diffusion mod-\nels, particularly through model quantization (Li et al. 2023a,\n2024; He et al. 2024; Wang et al. 2024). Post-training quan-\ntization (PTQ) is the most widely used technique because it\nrapidly quantizes the original model using a small calibra-\ntion set without requiring multiple iterations of fine-tuning\n(Yuan et al. 2022; Li et al. 2023a). Meanwhile, vector quan-\ntization (VQ) has been shown to compress CNN models to\nextremely low bit-width (Gersho and Gray 2012; Stock et al.\n2019), which could also be advantageous for DiTs. The clas-\nsic VQ approach maps the weight sub-vectors of each layer\nto a codebook and assignments using clustering techniques\nsuch as the K-Means algorithm (Han, Mao, and Dally 2015),\nand the codebook is continuously updated.\nHowever, existing quantization methods have several lim-\nitations. First, they cannot be directly applied to DiTs, which\nhave different network structures and algorithmic concepts\ncompared to UNet-based diffusion models. Second, PTQ\nmethods significantly reduce model accuracy when quantiz-\ning weights to extremely low bit-width (e.g., 2-bit). Third,\ntraditional VQ methods only calibrate the codebook without\nadjusting the assignments, leading to incorrect assignment\nof weight sub-vectors, which provides inconsistent gradi-\nents to the codebook and ultimately results in suboptimal\noutcomes.\nTo overcome these limitations, we introduce a novel post-\ntraining vector quantization technique for the extremely low\nbit-width quantization of DiTs, named VQ4DiT. VQ4DiT"}, {"title": "2 Backgrounds and Related Works", "content": "2.1 Diffusion Transformer Models\nUNet-based diffusion models have garnered significant at-\ntention, and research has begun to explore the adoption of\ntransformer architectures (Rombach et al. 2022; Croitoru\net al. 2023; Yang et al. 2023) within diffusion models. Re-\ncently, Diffusion Transformer Models (DiTs) (Peebles and\nXie 2023) have achieved state-of-the-art performance in im-\nage generation. Notably, DiTs demonstrate scalability in\nterms of model size and data representation similar to large\nlanguage models, making them widely applicable to image\nand video generation tasks (Brooks et al. 2024; Liu et al.\n2024; Zhu et al. 2024).\nDiTs consist of N blocks, each containing a Multi-Head\nSelf-Attention (MHSA) and a Pointwise Feedforward (PF)\nmodule (Vaswani et al. 2017; Dosovitskiy et al. 2021; Pee-\nbles and Xie 2023), both preceded by their respective adap-\ntive Layer Norm (adaLN) (Perez et al. 2018). The structure\nof the DiT block is illustrated in Figure 1 (A). These blocks\nsequentially process the noised latent and conditional in-\nformation, encoded as tokens in a lower-dimensional latent\nspace (Rombach et al. 2022).\nIn each block, the conditional embedded information $c\\in$\nRdin is converted into scale and shift parameters $(\\gamma, \\beta \\varepsilon$\nRdin), which are regressed through MLPs and then injected\ninto the noisy latent $z \\in$ Rn\u00d7din via adaLN:\n\\begin{equation}\n\\begin{cases}\n(\\gamma, \\beta) = MLP(c)\\\\\nadalN(z) = LN(z) \\odot (1 + \\gamma) + \\beta\n\\end{cases}\n(1)\n\\end{equation}\nwhere LN(\u00b7) denotes the Layer Norm (Ba, Kiros, and Hinton\n2016). These adaLN modules dynamically adjust the layer\nnormalization before each MHSA and PF module, enhanc-\ning DiTs' adaptability to varying conditions and improving\nthe generation quality.\nDespite their effectiveness, DiTs demand substantial com-\nputational resources to generate high-quality images, which\nposes challenges to their deployment on edge devices. In\nthis paper, we propose an extremely low bit-width quanti-\nzation method for DiTs that significantly reduces both time\nand memory consumption, without the need for a calibration\ndataset.\n2.2 Model Quantization\nLet W\u2208 Roxi denote the weight, where o represents the\noutput channel and i denotes the input channel. A stan-"}, {"title": "3 Challenges of Vector Quantization for DiTs", "content": "3.1 Trade-off of codebook size\nAs illustrated in Table 1, we apply the classic uniform quan-\ntization (UQ) and vector quantization (VQ) to the DiT XL/2\nmodel. At the same bit-width, VQ results in a much smaller\nquantization error compared to UQ. The number of code-\nwords k and their dimension d significantly impact both\nthe memory usage of VQ and the quantization error of the\nweights. Increasing k and d, while keeping the memory us-\nage of assignments constant, reduces the quantization error.\nHowever, this also increases the memory usage of the code-\nbook, which is particularly problematic in per-layer VQ. Ad-\nditionally, increasing k and d prolongs the runtime of the\nclustering algorithm and increases the subsequent calibra-\ntion times. These factors necessitate a careful trade-off be-\ntween quantization error and codebook size.\nWe utilize k = 256 and d = 4 for 2-bit quantization, k =\n64 and d = 2 for 3-bit quantization. The memory usage of\nthe codebooks is negligible when compared to the memory\nrequirements for assignments.\n3.2 Setups of codebooks and assignments\nThere are various methods to achieve VQ, one popular\nmethod being the K-Means algorithm (Han, Mao, and Dally"}, {"title": "4 VQ4DiT", "content": "To address the identified challenges", "function": "n\\begin{equation"}, "n||W \u2013 C[A"], "codewords": "n\\begin{equation"}, {"set": "n\\begin{equation"}, {"formula": "n\\begin{equation}\nRC[A] =\n\\begin{bmatrix}\nr_{1,1}c(\\{a_{1,1}\\}n) & r_{1,i/d}c(\\{a_{1,i/d}\\}n)\\\\\nr_{2,1}c(\\{a_{2,1}\\}n) & r_{2,i/d}c(\\{a_{2,i/d}\\}n)\\\\\n\\vdots & \\vdots\\\\\nr_{o,1}c(\\{a_{o,1}\\}n) & r_{o,i/d}c(\\{a_{o,i/d}\\}n)\n\\end{bmatrix}\n(7)\n\\end{equation}\n4.2 Zero-data and block-wise Calibration\nTraining DiTs typically relies on the ImageNet dataset (Rus-"}]