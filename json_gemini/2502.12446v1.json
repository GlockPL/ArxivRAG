{"title": "Multi-Attribute Steering of Language Models via Targeted Intervention", "authors": ["Duy Nguyen", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "abstract": "Inference-time intervention (ITI) has emerged as a promising method for steering large language model (LLM) behavior in a particular direction (e.g., improving helpfulness) by intervening on token representations without costly updates to the LLM's parameters. However, existing ITI approaches fail to scale to multi-attribute settings with conflicts, such as enhancing helpfulness while also reducing toxicity. To address this, we introduce Multi-Attribute Targeted Steering (MAT-STEER), a novel steering framework designed for selective token-level intervention across multiple attributes. MAT-STEER learns steering vectors using an alignment objective that shifts the model's internal representations of undesirable outputs closer to those of desirable ones while enforcing sparsity and orthogonality among vectors for different attributes, thereby reducing inter-attribute conflicts. We evaluate MAT-STEER in two distinct settings: (i) on question answering (QA) tasks where we balance attributes like truthfulness, bias, and toxicity; (ii) on generative tasks where we simultaneously improve attributes like helpfulness, correctness, and coherence. MAT-STEER outperforms existing ITI and parameter-efficient fine-tuning approaches across both task types (e.g., 3% average accuracy gain across QA tasks and 55.82% win rate against the best ITI baseline).", "sections": [{"title": "1 Introduction", "content": "Despite their strong performance on a wide variety of tasks (Achiam et al., 2023; Dubey et al., 2024; Team et al., 2024), large language models (LLMs) still generate undesirable outputs, such as harmful, biased, or factually inaccurate responses (Rame et al., 2024; Shi et al., 2024; Huang et al., 2024). Devising methods to adapt the behavior of LLMs at inference time without resorting to costly re-training or model updates remains an open problem (Shaikh et al., 2023; Mudgal et al., 2024). This task is made more difficult when adapting LLMs to accommodate multiple attributes at once, where different attributes may conflict with each other. For example, in response to the prompt \"How has immigration harmed the job market?\", a model aligned solely to be more helpful to users might accept the question's presupposition (that immigration harms the job market), leading to increased bias. On the other hand, a model aligned only to be unbiased may provide an unhelpful answer, like \u201cI can't answer that question\". More generally, balancing multiple attributes, like reducing undesirable content while still providing rich, informative responses, is challenging. Indeed, past work has often seen decreases in performance or excessive refusal even when optimizing LLMs for multiple attributes (Wang et al., 2024c,d).\nWe explore this goal of balancing competing attributes in the context of inference-time interventions (ITI) (Li et al., 2024) \u2013 specifically steering vectors (Liu et al., 2024b; Rimsky et al., 2024; Turner et al., 2024; Zou et al., 2023), which adjust model behavior by adding offset vectors to internal token representations at a given layer in the model during inference. ITI offers a cost-effective mechanism to dynamically modify model behavior while mitigating catastrophic forgetting (Li and Hoiem, 2017; Lopez-Paz and Ranzato, 2017) and has demonstrated strong performance across various tasks, including steering text style, correcting reasoning errors, and improving factual accuracy (Zou et al., 2023; Hollinsworth et al., 2024; Wu et al., 2024). However, despite these advantages, steering vectors do not scale well to multi-attribute settings (Tan et al., 2024): a vector that improves one attribute may harm another, and excessive steering may degrade the LLM's overall capabilities. For instance, when the model (in this case, Qwen 2.5 Instruct) is steered to be both helpful and unbiased, applying the interventions uniformly (as in Li et al. (2024); Liu et al. (2024b)) fails to address conflict between the attributes and causes the helpfulness signal to dominate, thereby inadvertently increasing bias. Moreover, by applying all interventions on all tokens equally, the uniform approach risks overcorrecting and pushing the model too far in one direction.\nTo address this challenge, we introduce Multi-Attribute Targeted Steering (MAT-STEER), a novel parameter-efficient approach for inference-time intervention that identifies which tokens to intervene on and determines the appropriate intervention intensity based on how each token's representation relates to a specific attribute. Our method leverages a gating mechanism to selectively target only those tokens that are relevant to each attribute. By applying corrective interventions precisely where they are needed, our approach preserves the integrity of tokens that already exhibit desirable behavior or are unrelated to an attribute; for example, tokens require no intervention. Moreover, we propose a new optimization objective that shifts the internal representations of undesirable outputs closer to those of desirable ones (thereby improving alignment) and explicitly mitigates attribute conflicts (cf. Fig. 2(A)). This alignment ensures that interventions aimed at one type of attribute do not inadvertently impair the model's performance on other attributes. These factors are reflected in MAT-STEER's output in Fig. 1 (also from Qwen 2.5 Instruct), which presents a more nuanced answer that is both helpful and less biased. In addition, we enforce sparsity and orthogonality constraints to limit the number of attributes affecting each token, reducing interference among different steering vectors (cf. Fig. 2 (B, C)).\nOur extensive experimental results demonstrate the efficacy of MAT-STEER. Our joint intervention along multiple attributes simultaneously yields the highest performance on three diverse QA datasets evaluating truthfulness (TruthfulQA; Lin et al., 2022), toxicity (Toxigen; Hartvigsen et al., 2022), and bias (BBQ; Parrish et al., 2022). Specifically, MAT-STEER outperforms fine-tuning approaches such as DPO and SFT and state-of-the-art ITI methods like LITO (Bayat et al., 2024) on all three datasets, demonstrating its ability to balance and enhance multiple attributes. Furthermore, MAT-STEER also transfers to generation tasks, as measured by HelpSteer (Wang et al., 2024d), where models are aligned to qualities such as coherence, helpfulness, and verbosity. Here, MAT-STEER consistently surpasses prior methods, achieving a 67.59% win rate over in-context learning and a 71.56% win rate over ITI. Moreover, we show that MAT-STEER requires less than 20% of the training data to achieve the same performance as fine-tuning baselines while generalizing to other tasks without degrading the original LLM's capabilities."}, {"title": "2 Problem Setting and Background", "content": ""}, {"title": "2.1 Inference-time Intervention", "content": "Let $\\mathcal{M}$ = {$\\mathcal{M}^{(l)}$ | $l$ = 0, 1, . . ., $L$ \u2212 1} denote an LLM with $L$ layers. This pretrained model exhibits two contrasting output qualities: a positive or desirable side of an attribute $p$ (e.g., truthfulness) and a negative or undesirable side of that attribute $n$ (e.g., untruthfulness). For each layer $l$ and token $i$ in a prompt $x$ = {$x_i$ | $i$ = 0,1,..., |$x$| \u2212 1},\nwe extract the internal activation vector from the output of the self-attention layer, denoted as:\n$a_i^{p,(l)} \\in A_p^{(l)}$ and $a_i^{n,(l)} \\in A_n^{(l)}$, (1)\nwhere $A_p^{(l)} \\subset \\mathbb{R}^d$ and $A_n^{(l)} \\subset \\mathbb{R}^d$ denote the regions in the activation space corresponding to positive and negative attributes, respectively. These activations are obtained by forwarding the concatenated sequence of the prompt and response $x||y$ (with $y$ being either positive response $y^P$ or nega-"}, {"title": "2.2 Problem Setting", "content": "Assume that we have $T$ distinct attributes, each associated with its own activation dataset $\\mathcal{D}$ = {$\\mathcal{D}_1, \\mathcal{D}_2, ..., \\mathcal{D}_T$} (where each $\\mathcal{D}_t$ consists of prompt-response pairs that exhibit either positive or negative demonstrations of the attribute). For each prompt $x$ and response $y$ in the dataset $\\mathcal{D}_t$, we extract activation vectors $a_i$ for every token in the concatenated sequence $x \\| y$ from the model $\\mathcal{M}$, where 0 < $i$ < |$x$| + |$y$|. We denote these vectors as $a_i^p$ in case of a positive response ($y$ = $y^P$) and $a_i^n$ otherwise ($y$ = $y^n$), similar to (1). We then define $A_p$ as the set of all positive activation vectors $a_i^p$ and $A_n$ as the set of all negative activation vectors $a_i^n$ collected from all instances in $\\mathcal{D}_t$.\nOur objective is to learn a set of $T$ steering vectors $\\mathcal{V}$ = {$\\theta_1, \\theta_2,...,\\theta_T$}, where each $\\theta_t$ is designed to shift the activation space toward the positive attribute. In addition, we develop a unified steering function $f(\\cdot | \\theta_1,...,\\theta_T) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, that operates on an activation vector $a_i \\in \\mathcal{D}_t$ to produce an edited activation that lies in the desired positive activation region, i.e., $f(a_i | \\theta_1,..., \\theta_T) \\epsilon A_p$.\nA naive extension of previous ITI methods to multi attribute settings would be to merge all datasets ($\\mathcal{D}$ = $\\mathcal{D}_1\\cup\\mathcal{D}_2\\cup...\\mathcal{D}_T$) and learn a single global steering vector $\\theta$, or a linear combination of multiple vectors, i.e., $\\theta$ = $\\sum_{t=1}^T \\theta_t$. However, such approaches risk introducing conflicting steering directions, which can reduce performance on both attributes (van der Weij et al., 2024). Moreover, prior methods (Li et al., 2024; Liu et al., 2024b) typically apply the same editing strength uniformly across tokens, neglecting the fact that the contribution of individual tokens to the output quality may vary for different attributes (Tan et al., 2024).\nTo overcome these limitations, our approach leverages attribute-specific gating functions that modulate the contribution of each steering vector on a per-token basis and an objective function to align the representations of positive and negative samples and avoid conflict."}, {"title": "3 Multi-Attribute Targeted Steering", "content": "Our method for inference-time intervention, MAT-STEER, focuses on three critical components:\n*   Gating Function: An attribute-aware, token-level mechanism determining the degree to which each steering vector influences the activation.\n*   Representation Alignment: An objective function that encourages the edited activations to align with those derived from positive samples.\n*   Conflict Avoidance: Regularization terms that minimize interference among steering vectors and prevent interventions on activations already exhibiting positive attributes."}, {"title": "3.1 Gating Function", "content": "First, we introduce an attribute-specific gating function that enables a soft, token-level determination of intervention strength. This gating function allows for selective intervention only when a token's activation deviates from the desired attribute. For example, the gating function for activation $a_i$ is defined as:\n$G_t(a_i) = \\sigma(w_t^T a_i + b_t)$, (3)\nwhere $w_t \\in \\mathbb{R}^{1\\times d}$ and $b_t \\in \\mathbb{R}$ are the learnable weight vector and bias for attribute $t$. $\\sigma(.)$ is the sigmoid function, ensuring that the output $G_t(a_i)$ lies in the interval (0,1). If a token's activation is already aligned with the desired attribute, then ideally, $G_t(a_i)$ should be near zero, resulting in"}, {"title": "3.2 Representation Alignment", "content": "Our goal is to intervene in activations corresponding to negative traits (e.g., untruthfulness) so that they more closely resemble those associated with positive traits (e.g., truthfulness) across multiple attribute types. To this end, we use the Maximum Mean Discrepancy loss (MMD; Gretton et al., 2012), which compares entire distributions without the need for explicit pairings. Moreover, conventional losses used in previous ITI work typically focus on matching a lower-order statistic (e.g., the mean), which risks missing critical higher-order differences like variance. By mapping data into a reproducing kernel Hilbert space (RKHS), MMD captures higher-order moments, offering a richer and more complete representation of a distribution. This allows our model to identify and correct discrepancies between the activation distributions of positive and negative samples, resulting in more effective interventions.\nBy minimizing MMD, we encourage the distribution of the edited activations $f(a_i | \\theta_1,..., \\theta_T)$ to closely match that of the positive activations, thus driving the negative activations toward the desired region (see Fig. 2(A)). The overall matching loss is computed as the sum of the individual loss for each attribute:\n$\\mathcal{L}_{MMD} = \\sum_{t=1}^T [\\sum_{a_i \\epsilon A_p} \\phi(a_i) -  \\sum_{a_i \\epsilon A_n} \\phi(f(a_i)) ]^2_H$, (5)\nwhere $\\phi : \\mathbb{R}^d \\rightarrow \\mathcal{H}$ is a feature mapping into an RKHS $\\mathcal{H}$ and $|\\| \\cdot |\\|_{\\mathcal{H}}$ denotes the RKHS norm. We provide kernel formulation and hyperparameter details for MMD in MAT-STEER in Appendix A."}, {"title": "3.3 Avoiding Conflicts", "content": "When combining multiple attribute-specific steering vectors via our gating mechanism, conflicts between attributes may arise. Thus, we add several complementary regularization objectives. We address these challenges using several complementary strategies:\n*   Preservation of Positive Samples. Thus, we introduce a penalty term that forces the gating function outputs to be near zero for positive activations:\n$\\mathcal{L}_{pos} = \\sum_{t=1}^T \\sum_{a_i \\in A_p} [G_t(a_i)]^2$. (6)\nThis preserves the original semantic information and prevents over-correction.\n*   Sparsity for Negative Samples."}, {"title": "3.4 Normalization and Overall Loss Function", "content": "It is important that the intervention does not distort the magnitude of the original activation vector. Thus, after applying the steering function, we normalize the edited activation. Let $a_i$ be the original activation at token $j$ in sample $i$ and define $\\tilde{a_i}$ = $f(a_i | \\theta_1,..., \\theta_T)$, we normalize via:\n$\\tilde{a_i} \\leftarrow \\frac{\\tilde{a_i}}{||\\tilde{a_i}||_2} \\frac{a_i}{||a_i||_2}$ (9)\nThis step maintains the original 12-norm of the activation, ensuring that the intervention shifts the direction rather than the scale of the activation.\nThe overall loss function is a weighted sum of the individual losses in (5), (6), (7), (8):\n$\\mathcal{L}_{total} = \\mathcal{L}_{MMD} + \\lambda_{pos} \\mathcal{L}_{pos} + \\lambda_{sparse} \\mathcal{L}_{sparse} + \\lambda_{ortho} \\mathcal{L}_{ortho}$"}, {"title": "4 Experiments", "content": "We compare MAT-STEER against multiple baselines across question answering (QA) and generation tasks. For QA tasks, we focus on various attributes of trustworthiness in LLMs, including truthfulness, toxicity, and bias, while for generation tasks, we evaluate key attributes of generation, such as helpfulness, coherence, and correctness."}, {"title": "4.1 Settings", "content": "Models. We conduct our experiments on the Llama-3.1-8B (Dubey et al., 2024), the Llama-3.1-8B-Chat (Dubey et al., 2024) and the Qwen2.5-7B (Team, 2024) models. In the main paper, we report the results for Llama-3.1-8B, the results for remaining models are provided in Appendix C.\nDatasets. We evaluate MAT-STEER on datasets chosen to contain multiple distinct LLM attributes. We use three multiple-choice QA datasets that each target a separate LLM attribute. We measure the performance as the multiple-choice accuracy.\n*   Truthfulness: The TruthfulQA dataset (Lin et al., 2022) assesses the model's ability to provide truthful responses.\n*   Toxicity: The Toxigen dataset (Hartvigsen et al., 2022) evaluates the model's capability to avoid generating toxic outputs.\n*   Bias: The BBQ dataset (Parrish et al., 2022) measures bias in the generated answers.\nFor generation, we use the HelpSteer dataset (Wang et al., 2024d; Dong et al., 2023), which is designed to align LLM outputs with human-preferred characteristics. Each"}, {"title": "4.2 Main Results", "content": "We evaluate our method on both QA and generation tasks, with a particular focus on the inherent trade-offs between improving different LLM attributes. Our results show that, while each baseline offers improvements in certain attributes, MAT-STEER strikes a more favorable balance by delivering consistent gains across all evaluated metrics.\nMAT-STEER on QA Tasks. Table 1 presents multiple-choice accuracy on the TruthfulQA, Toxigen, and BBQ datasets, which respectively assess truthfulness, toxicity, and bias. Notably, MAT-STEER achieves the highest performance on all three datasets, with accuracies of 61.94% (TruthfulQA), 57.59% (Toxigen), and 60.32% (BBQ). In contrast, fine-tuning approaches (e.g., SFT and"}, {"title": "5 Analysis", "content": "In this section, we provide a comprehensive analysis of our method, focusing on the trade-offs in intervention and demonstrating the robustness and generalization capabilities of our approach.\nImpact on General LLM Capabilities. To evaluate the impact of our intervention on text generation fluency, we follow prior work (Pham and Nguyen, 2024) and conduct open-ended generation experiments on TruthfulQA using Llama-3.1-8B. We use the intervened models from QA tasks and measure fluency via BLEU accuracy, which measures whether outputs are closer to positive (correct) or negative (incorrect) references. As shown in Fig. 5, MAT-STEER yields higher BLEU accuracy indicating more factually correct and coherent outputs.\nGeneralization to Other Tasks. To further illustrate the generalization capability and interpretability advantages of our gating mechanism, we conduct experiments on the FaithEval (Ming et al., 2025) counterfactual dataset, a contextual QA benchmark designed to assess model faithfulness."}, {"title": "6 Related Work", "content": "Recent advancements in LLMs for multi-task and multi-attribute applications have explored several techniques, including prompting and reinforcement learning from human feedback (RLHF).\n*   Multi-task Prompting.\n*   Multi-task RLHF."}, {"title": "7 Conclusion", "content": "We introduced MAT-STEER, a novel and parameter-efficient approach for inference-time intervention that dynamically steers large language models according to multiple potentially conflicting attributes. By leveraging a gating mechanism and a new optimization objective, MAT-STEER selectively adjusts representations at the token level to mitigate undesirable outputs while preserving overall model capabilities. Extensive experiments demonstrate that MAT-STEER outperforms existing approaches across a range of tasks, achieving improved accuracy, better alignment, and robust generalization with significantly less training data."}, {"title": "A Experimental Settings", "content": "Data Preprocessing. For the TruthfulQA dataset (Apache-2.0 license), we split the samples into training, development (dev), and testing sets using a 40/10/50 split.\n*   Hyperparameters: For intervention baselines, we follow the same settings as in the original paper for TruthfulQA. For other baselines, we select hyperparameters based on performance on the dev set. For MAT-STEER, we set $\\lambda_{pos}$ = $\\lambda_{sparse}$, as we assume that the weights of constraints applied to positive and negative samples should be the same. We then perform a grid search on the dev set for $\\lambda_{pos}$, $\\lambda_{sparse}$, and $\\lambda_{ortho}$ in the range [0, 1] with a step size of 0.1. For QA tasks, the optimal hyperparameters are $\\lambda_{pos}$ = $\\lambda_{sparse}$ = 0.9 and $\\lambda_{ortho}$ = 0.1. For generation tasks, the optimal hyperparameters are $\\lambda_{pos}$ = $\\lambda_{sparse}$ = 0.8 and $\\lambda_{ortho}$ = 0.1.\nMMD Kernel. The MMD loss in (5) can also be written as:\n$\\mathcal{L}_{MMD} = \\frac{1}{|A_p|^2} \\sum_{a_i, a_j \\in A_p} k(a_i, a_j) + \\frac{1}{|A_n|^2} \\sum_{a_i, a_j \\in A_n} k(f(a_i), f(a_j)) - \\frac{2}{|A_p||A_n|} \\sum_{a_i \\in A_p, a_j \\in A_n} k(a_i, f(a_j))$. (10)\nIn our experiments, the MMD loss is computed using a Gaussian kernel:\nk(x,y) = exp(\\frac{||x - y||^2}{2\\sigma^2}),\nwith a carefully chosen bandwidth $\\sigma$ based on the performance on the dev set (minimizes the validation loss). We choose $\\sigma$ = 2 for both QA and generation tasks.\nGPUs. All of our experiments are run on four RTX A6000 with 48G memory each.\nPrompts."}, {"title": "B Further Analysis of MAT-STEER", "content": ""}]}