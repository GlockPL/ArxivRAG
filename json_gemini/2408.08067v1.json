{"title": "RAGCHECKER: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation", "authors": ["Dongyu Ru", "Lin Qiu", "Xiangkun Hu", "Tianhang Zhang", "Peng Shi", "Shuaichen Chang", "Jiayang Cheng", "Cunxiang Wang", "Shichao Sun", "Huanyu Li", "Zizhao Zhang", "Binjie Wang", "Jiarong Jiang", "Tong He", "Zhiguo Wang", "Pengfei Liu", "Yue Zhang", "Zheng Zhang"], "abstract": "Despite Retrieval-Augmented Generation (RAG) has shown promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGCHECKER, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGCHECKER has significantly better correlations with human judgments than other evaluation metrics. Using RAGCHECKER, we evaluate 8 RAG systems and conduct an in-depth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RAGCHECKER can guide researchers and practitioners in developing more effective RAG systems.", "sections": [{"title": "Introduction", "content": "Retrieval-Augmented Generation (RAG) systems [18, 7] enhance Large Language Models (LLMs) by incorporating external knowledge bases, enabling more precise and contextually relevant responses [7, 53, 13]. As these systems become integral to a variety of applications [54, 2, 8], it's imperative to develop robust and comprehensive evaluation frameworks to assess their performance and identify areas for improvement. Evaluating RAG systems, however, presents several challenges:\n(1) modular complexity: The modular nature of RAG systems, comprising both a retriever and a generator, complicates the design of effective evaluation metrics. It is crucial to establish metrics that can holistically assess the entire system as well as evaluate the individual modules and their interplay [53], allowing for fully understanding the sources of the errors and misses and how they are generated. (2) metric limitation: Existing metrics for evaluating RAG systems, which are often rule-based or coarse-grained, fall short in providing accurate and interpretable results. Specifically, traditional metrics like recall@k and MRR [44] for retrievers depend on annotated chunks and a rigid chunking approach, missing out on the full semantic scope of the knowledge base. For generators, typical measures such as n-gram-based (e.g., BLEU [30], ROUGE [19]), embedding-based (e.g., BERTScore [56]), and LLM-based methods [45] perform well with concise answers but fail to detect finer distinctions in longer responses. To bridge these gaps, it is essential to develop detailed, semantic-based evaluation metrics that effectively capture the intricacies and overall quality of both the retrieval and generation components in RAG systems. (3) metric reliability: the reliability of existing metrics for RAG remains under-explored. Effective evaluation metrics must not only accurately reflect system performance but also align with human judgments to ensure their utility in real-world scenarios.\nTo overcome these challenges, we introduce RAGCHECKER, an innovative evaluation framework designed for detailed analysis of both retrieval and generation processes. RAGCHECKER is based on claim-level entailment checking which involves operations of extracting claims from the response and ground truth answer and checking them against other texts. This approach enables fine-grained evaluation instead of response-level assessment. RAGCHECKER processes the user query, retrieved context, response, and ground truth answer, producing a suite of metrics:\n1. Overall Metrics to provide a holistic view of the system performance, assessing the overall quality of the generated responses.\n2. Diagnostic Retriever Metrics to evaluate the effectiveness of the retriever, identifying its strengths and weaknesses in finding relevant information from the knowledge base.\n3. Diagnostic Generator Metrics to assess the performance of the generator, diagnosing how well the generator utilizes the retrieved context, handles noisy information, and generates accurate and faithful responses.\nCompared to existing evaluation frameworks, RAGCHECKER provides a more comprehensive assess- ment of RAG systems. While some frameworks offer fine-grained evaluation only on certain metrics (e.g., RAGAS [5], TruLens [6], ARES [35]) or evaluate specific aspects of RAG (e.g., RGB [4], RECALL [22], NoMIRACL [40]), RAGCHECKER's metrics are all based on fine-grained claim-level checking and are designed to provide actionable insights into the sources of errors.\nTo ensure the reliability of RAGCHECKER, we annotate a human judgment dataset to assess the correlations between the proposed metrics and human judgments. This meta-evaluation validates the effectiveness of RAGCHECKER in capturing the quality and reliability of RAG systems from a human perspective. We demonstrate the effectiveness of RAGCHECKER through comprehensive experiments evaluating 8 state-of-the-art RAG systems on a benchmark repurposed from public datasets across 10 domains. In-depth analysis of the evaluation results reveals that RAGCHECKER provides insightful diagnostic signals (Sec. 4.3) pointing the directions for improvements of RAG systems (Sec. 4.4).\nThe main contributions of this paper are as follows:\n\u2022 We propose RAGCHECKER, a novel RAG evaluation framework that offers fine-grained evaluation for both the retriever and generator components, introducing new diagnostic metrics to provide actionable insights into the sources of errors.\n\u2022 We conduct meta evaluation and verified RAGCHECKER has significantly better correlations with human judgements than other evaluation metrics.\n\u2022 We perform extensive experiments evaluating 8 RAG systems on our curated benchmark across 10 domains, and uncover valuable insights, such as the trade-off between retrieval improvement and noise introduction, and the tendency of faithful open-source models to blind trust on context."}, {"title": "Related Work", "content": "2.1 Retrieval Augmented Generation\nLarge Language Models (LLMs) demonstrate strong capabilities in generating text, but there are also obstacles such as outdated information and the potential to hallucinate [42, 46, 12]. To address these issues, RAG retrieves external knowledge to generate responses with improved accuracy and factuality [7, 53, 13]. Integrating external knowledge is especially crucial in fields like legal, medical and finance, where precision and reliability are essential [24, 50, 55].\nRAG systems have shown impressive performance across a range of tasks, including open-domain question answering [27, 10, 18], code generation [32, 57, 38] and dialogue [37, 16, 41]. Additionally, real world products like Bing Search\u00b3 and Langchain [3] have integrated applications based on RAG."}, {"title": "Evaluation of RAG", "content": "Existing evaluation practices for RAG systems can be categorized into two main approaches: evaluat- ing essential capabilities of generators only and assessing end-to-end performance of RAG systems.\nWithin the two components of a RAG system, the retriever has been well studied in recent years, thus a line of recent work focused on evaluating essential generator capabilities. RGB [4] evaluated 4 fundamental abilities required for generators including Noise Robustness, Negative Rejection, Infor- mation Integration and Counterfactual Robustness by manually constructed test sets. RECALL [22] introduced manually edited counterfactual contexts into QA and text generation datasets to evaluate the counterfactual robustness of LLMs. NoMIRACL [40] evaluated LLMs' robustness against first- stage retrieval errors of RAG systems with manually judged relevant and non-relevant datasets. Wu et al. [49] quantified the tug-of-war between LLMs' faithfulness and internal prior by introducing varying levels of perturbations on the provided contexts. FaaF [15] introduced a fine-grained fact verification formulation to improve previous prompting-based approaches in evaluating factuality of generators. However, we argue that above generator-only evaluation approaches with manually constructed datasets cannot serve as a general RAG evaluation framework to reveal the entanglement of between generation results and different retrieval behaviors, as shown in the analysis of Sec. 4.3.\nAnother line of work focused on assessing end-to-end quality scores of RAG systems. TruLens [6] introduced the concept of RAG Triad, which decompose the quality scores into three aspects: context relevance, groundedness and answer relevance, then predicted the score by prompting LLMs or using NLI models. RAGAS [5] and ARES [35] followed the RAG Triad concept and improved the score prediction approaches on different datasets. CRUD-RAG [25] refered to the CRUD (Create, Read, Update and Delete) actions between users and knowledge bases to develop corresponding datasets and evaluation metrics for RAG systems. We compare the above four evaluation frameworks with RAGCHECKER in the meta evaluation of Sec. 4.2.\nBesides, the following work also provided good insight or high quality datasets for end-to-end RAG evaluation. Liu et al. [21] conducted human evaluation to audit four popular generative search engines in terms of fluency, perceived utility, and verifiability. MEDRAG [50] constructed a medical RAG benchmark from medical QA datasets and evaluated medical RAG systems with QA accuracy. MultiHop-RAG [39] generated multi-hop queries from news articles and evaluated RAG systems with QA accuracy. CDQA [52] proposed a novel approach to generate dynamic QA questions which requires latest information to answer. However, the evaluation metrics used in the work mentioned above rely either on human evaluation or simple textual accuracy, making them incapable of complex RAG scenarios that require long answer evaluation. Therefore, we do not include them in the meta evaluation."}, {"title": "RAGCHECKER Framework", "content": "Formulation Define a modular RAG system as \\(RAG = \\{R,G\\}\\), where R is the retriever and G is the generator. Given a query q and documents D, it first retrieves top-k relevant context \\(\\{chunk_j\\} = R(q, D, k)\\), and then generates a model response \\(m = G(\\{chunk_j\\}, q)\\). For simplicity, we can also represent the overall RAG generation process as \\(m = RAG(q, D)\\).\nDesign Principle Given the compositional nature of RAG, we observe there are two major personae using a RAG evaluation framework. The first persona is a user that cares about the overall performance of RAGs and might choose a system with the best performance. Such a persona prefers a single value metric to compare and rank among RAG systems against a benchmark. The second persona is a developer that focuses on improving a RAG system with the need to identify causes of mistakes and potential rooms for improvements. Causes of errors in response can be classified into 1) retrieval errors, where the retriever fails to return complete and relevant context, and 2) generator errors, where the generator struggles to identify and leverage relevant information from context.\nConsequently, metrics that reveal error causes should be different from those for overall performance, in the sense that error causes are module-specific or even reflected only by a certain behavior of a module. To help both personae to assess RAG performance, we design RAGCHECKER, a evaluation framework of RAG systems that consists of a benchmark with rich annotations and a set of diversely-purposed fine-grained metrics."}, {"title": "Inputs to RAGCHECKER", "content": "We prepare each sample in our benchmark dataset in the format of a tuple (q, D, gt) representing query, documents, and ground-truth answer, where query is the input question to a RAG system, documents form the database providing possible context and are processed into chunks with the same number of tokens, and ground-truth answer is a complete and correct answer for the input question. Further information is provided in Sec. 4.1."}, {"title": "Fine-grained Evaluation with Claim Entailment", "content": "As illustrated in Fig. 1, a response generated by a RAG system might be a mixture of correct (0) and incorrect claims (\u00d7), while also missing some in-ground-truth claims ( \u25b3 ). In this sense, evaluating responses at a finer granularity is crucial to comprehensively assess the quality of an answer. For this purpose, we introduce two components: 1) a text-to-claim extractor that decomposes a given text T into a set of claims \\{c\\}, and 2) a claim-entailment checker to determine whether a given claim c is entailed (\\(\\in\\)) in a reference text Ref or not (\\(\\notin\\))."}, {"title": "RAGCHECKER Metrics", "content": "With the annotation and claim-level entailment functions specified, we next define the metrics. For a RAG user, we design metrics to compare the performance among RAG systems, including a single-value F1 score as an overall metric. For a RAG developer, on the other hand, we propose two sets of modular metrics for the retriever and the generator in a RAG system respectively, that aim to decompose the system and diagnose the source of errors. In the rest of this section, we will first introduce the overall metrics and then go over modular metrics for retriever and generator separately. The formulas for each metric are summarized in Appendix B."}, {"title": "Overall Metrics", "content": "To assess the overall response quality of a RAG system from a user's perspective, we can compute the precision and recall at claim level for each model generated response against its paired ground-truth answer. Specifically, we first extract claims from a model response m and a ground-truth answer gt as \\(\\{c_i^{(m)}\\}\\) and \\(\\{c_i^{(gt)}\\}\\) respectively. Then, we define correct claims in the response as \\(\\{c_i^{(m)} \\mid c_i^{(m)} \\in gt\\}\\), and correct claims in the ground-truth answer as \\(c_i^{(gt)} \\mid c_i^{(gt)} \\in m\\). Two metrics can be computed directly: precision is the proportion of correct claims in all response claims, and recall is the proportion of correct claims in all ground-truth answer claims. Further, the harmonic average of precision and recall gives the F1 score, as the overall performance metric."}, {"title": "Retriever Metrics", "content": "Ideally, a perfect retriever returns precisely all claims needed to generate the ground-truth answer. Completeness-wise, we can measure how many claims made in the ground-truth answer are covered by retrieved chunks. With retrieved chunks as the reference text, we compute claim recall as the proportion of \\(\\{c_i^{(gt)} \\mid c_i^{(gt)} \\in \\{chunk_i\\}\\}\\).\nDifferently, we define the retriever precision at chunk-level instead of claim-level. A retrieved chunk is called relevant chunk (r-chunk), if any ground-truth claim is entailed in it. In other words, chunki is a relevant chunk if \\(\\exists i\\), s.t. \\(c_i^{(gt)} \\in chunk_i\\). The rest retrieved chunks are called irrelevant chunk (irr-chunk). The retriever's context precision is defined as \\(\\{\\mid r\\text{-}chunk_i\\}\\mid/k\\), where k is the number of all retrieved chunks.\nNote that a chunk-level precision provides better interpretability than a claim-level one, because in practice RAG systems usually work with documents processed to be text chunks in a fixed size. That being said, it is likely that a chunk may contain relevant claims and irrelevant or misleading information at the same time. As a result, the best possible retriever can only achieve a claim-level precision score lower than 100%, and such an upper-bound varies depending on the actual text distribution in D and chunking strategy."}, {"title": "Generator Metrics", "content": "Given k retrieved chunks (possibly mixing relevant and irrelevant information), a perfect generator would identify and include all ground-truth-relevant claims and ignore any that are not. Because the generator's results have dependency on retrieved chunks, we provide in total six metrics characterizing different aspects of its performance.\nGiven a model response m and its claims \\(\\{c_i^{(m)}\\}\\), we first compute the proportion of \\(c_i^{(m)}\\) that are entailed in retrieved chunks. This metric is faithfulness, as it describes how faithful the generator is to the provided context, thus the higher the better.\nNext, we examine three types of incorrect response claims, i.e. \\(\\{c_i^{(m)} \\mid c_i^{(m)} \\notin gt\\}\\).\n1. The first type includes incorrect claim that are entailed in a relevant chunk, then it indicates the generator is sensitive to noise coupled with useful information. The proportion of this type of claims to all \\(\\{c_i^{(m)}\\}\\) is relevant noise sensitivity.\n2. The second type includes incorrect claim that are entailed in an irrelevant chunk, then it indicates the generator is also sensitive to noise even in an irrelevant context. The proportion of these incorrect claims is irrelevant noise sensitivity.\n3. Finally, the third type includes incorrect claims that are not entailed in any retrieved chunk, meaning all such claims are generated by the generator itself. Its proportion is hallucination.\nNote that for simplicity we group the two noise sensitivities in Fig. 1, but later in Sec. 4.3 we can see that generators generally has different sensitivity to relevant and irrelevant noise.\nFinally, we characterize how a generator uses information sources to produce correct claims. A correct claim not entailed by any chunk can only be based on generator's self-knowledge, thus the proportion of these claims reflects how many correct claims are generated on its own. A lower self-knowledge score is better, when the generator is expected to fully depend on retrieved context only in a RAG system. On the other hand, we also check how much retrieved relevant information is used by the generator. Retrieved relevant information is measured by the number of ground-truth answer claims entailed in retrieved chunks, while the evidence of being used by generator is reflected by entailment in model response. Therefore, the context utilization is computed as the ratio between \\{\\mid c_i^{(gt)} \\mid c_i^{(gt)} \\in \\{chunk_j\\} and c_i^{(gt)} \\in m\\mid\\} and \\{\\mid c_i^{(gt)} \\mid c_i^{(gt)} \\in \\{chunk_j\\}\\mid\\}. Generally a higher context utilization is preferred."}, {"title": "Experiments", "content": "4.1 Experimental Setup\nBaseline RAG Systems We apply RAGCHECKER to 8 customized RAG systems to demonstrate how these metrics reflect the properties and differences among them, and how they guide the refinement of these systems. The 8 RAG systems are combinations with 2 retrievers and 4 generators. For retrievers, we choose BM25 [33], a representative classic sparse retrieval framework, and E5-Mistral [48], the SOTA open-source dense retriever. Our four generators are GPT-4 [29], Mixtral- 8x7B [14], Llama3-8B, and Llama3-70B [1], covering open-source and proprietary LLMs in various sizes. Further details are deferred to Appendix D. We employ Llama3-70B as both the claim extractor and checker models implemented by an open-sourced framework RefChecker\u2074 [11]. As a validation of its performance on the RefChecker's hallucination detection benchmark, this setup outperforms the best purely open-sourced combinations reported in RefChecker's paper (see Appendix G).\nBenchmark Datasets For comprehensive evaluations, we curate a benchmark containing 4,162 queries across 10 domains. This benchmark is repurposed from public datasets of open domain question answering, spanning domains of Wikipedia, AI science, novel, biomedical, finance, lifestyle, recreation, science, technology and writing. We convert the short answers to long-form answers in the datasets to align with the current LLM-based RAG systems. Please refer to Appendix A for the details of the benchmark curation process.\n4.2 Meta Evaluation\nWe first conduct the meta evaluation to verify the soundness of RAGCHECKER and compare with existing baseline RAG evaluation frameworks.\nBaseline RAG Evaluation Frameworks We include a total of 10 metrics from Trulens [6], RA- GAS [5], ARES [35] and CRUD-RAG [25] in the meta evaluation, as they are capable to evaluate end- to-end performance with long answers. Metrics selected for comparison along with their descriptions are summarized in Tab. 4 of Appendix C. To ensure a fair comparison, we use Llama3-70B-Instruct as the LLM backbone when applicable. Since models in the Llama3 family don't provide an embedding model, baseline metrics requiring embedding capability still use their corresponding default LLM backbones. In addition to the 10 metrics detailed in the table, we also incorporate BLEU [31], ROUGE-L [20], and BERTScore [56] to assess the correlation between the generated responses and the ground truth answers.\nMeta Evaluation Dataset All baseline metrics are designed with different aspects and functionalities to a certain degree, thus making an exact comparison over metric scores inapplicable. However, we argue that a good metric should reflect the relative human preference over different RAG systems. In this spirit, we construct the meta evaluation dataset with sampled instances from the generated responses of 8 baseline RAG systems introduced in Sec. 4.1 on our benchmark. Each meta evaluation instance is a pair of responses from two baseline RAG systems given the same query. By considering all combinations over 10 domains and 28 baseline pairs, we end up with 280 instances for pairwise human preference labeling. For each instance, annotators compare a pair of responses based on cor- rectness, completeness, and overall assessment. For each aspect, annotators measure their preferences as one out of five relative choices, including significantly better, slightly better, tie, slightly worse and significantly worse. For quality control, each instance is annotated by two annotators, and their overall agreement and correlation are measured. To conclude, we build a meta evaluation dataset with 280 instances, each instance is labeled by two annotators with their preference in terms of correctness, completeness and overall assessment."}, {"title": "Main Results", "content": "We present the averaged evaluation results for 8 RAG systems across 10 diverse domain datasets in Tab. 3. Additional results for all datasets are provided in Appendix E. The RAG system that exhibited the best performance in our experiments is E5-Mistral_GPT-4, owing to the strong retrieval capability of E5-Mistral coupled with the adept comprehension abilities of GPT-4. Next, we provide a list of insights induced from Tab. 3, along with their interpretation and possible directions for improvements.\nRetriever Matters Consistently. The quality of retrieval is crucial, as evidenced by the notable differences in overall Precision, Recall and F1 scores when comparing BM25 with E5-Mistral with the generator fixed. This improvement is agnostic to the specific choice of generator, suggesting a consistent benefit from employing a better retriever.\nGenerator Model Size Brings All-Round Improvement. Paired to the same retriever, Llama3-70B consistently achieves better overall performance than Llama3-8B. More concretely, this superiority is supported by a better performance over every generator metric, such as improved context utilization, reduced noise sensitivity, and less hallucination.\nStable and Performant Context Utilization is Key. Among all generator metrics, we observe that context utilization strongly correlates to the overall F1 score, while such correlation is relatively weaker for other generator metrics. Also, generators' context utilization are relatively stable between the two retrievers, meaning their overall recall can be improved with a better retriever. These observations indicate that the capability to fully utilize retrieved context is key, which is intuitive because the generator in a RAG system is expected to leverage context to surpass its self-knowledge.\nInformative Context Improves Faithfulness and Reduces Hallucination. As E5-Mistral achieves better claim recall, we observe generators paired to it achieves better faithfulness, indicating generators are all capable to identify and leverage information in context. Similarly, hallucination and self- knowledge are both reduced as well.\nRetriever Recall Trades-off with Generator Noise Sensitivity. Claim recall for a retriever character- izes the coverage of all information necessary to produce ground-truth answer. In practice, however, because of the fixed-size chunking strategy, retrieved relevant chunks may inevitably also carry over noise as part of the context. As retriever claim recall increases, all generators become more sensitive to such noise, which can be explained as their faithfulness to certain context is not discriminative enough. This observation shows that generators' capability to precisely leverage relevant context is still a challenge.\nRelevant Noise Sensitivity is More Challenging. For every baseline RAG system, there's an apparent gap between its relevant and irrelevant noise sensitivity. In correlation to the last paragraph, it further enhance the point that generators demonstrate a chunk-level faithfulness. It means a relevant chunk is trusted as a whole, while an irrelevant one only has minimal impact. This subtle yet significant distinction supports and explains the importance of the quality and specification of the database for a RAG system.\nOpen-Source Models are Worse at Distinguishing Accurate Information from Noise. GPT-4 has both higher context utilization and lower noise sensitivity than the other three open source models. Open source models are faithful but tend to trust the context blindly especially when retrieval gets better. This observation raises the need for improving open source models' reasoning ability."}, {"title": "Diagnosis on RAG Settings for Improvements", "content": "Guided by observations in Sec. 4.3, we modify settings commonly tuned in RAG systems that may lead to improvements, diagnose their working mechanisms with RAGCHECKER metrics, and provide suggestions for improvements on certain aspects. We experiment with different numbers of chunks, chunk sizes, chunk overlap ratios, and generation prompts. We highlight our main findings and suggestions as below, please refer to Appendix F for detailed analysis and results.\nMore Context Enhances Faithfulness. Increasing the number (k) and size of chunks improves the recall of more useful information (claim recall 61.5\u219277.6 with k 5\u219220, 70.3 77.6 with size 150\u2192300). Consequently, this provides more context for the generators to be more faithful to (faithfulness 88.1\u219292.2 with k 5\u219220, 91.2 92.2 with size 150\u2192300), though at the same time they also become more sensitive to additional noise (noise sensitivity 34.0\u219235.4 with k 5\u219220, 34.5-35.4 with size 150-300). Improvements in the overall performance (F1 51.7\u219253.4 with k 5-20, 52.6\u219253.4 with size 150\u2192300) indicates benefits from more context.\nExplicit Requirements in Prompts Affect Generation Preferences. When prompts introduces explicit requirements for better faithfulness, context utilization, and lower noise sensitivity, generators show improvements in faithfulness (92.2\u219293.6), but struggle with the subtle tension between context utilization (59.2\u219263.7) and noise sensitivity (35.4\u219238.1).\nChunk Overlap Does Not Matter a Lot. The chunk overlap ratio is usually set to be non-zero to help generators better utilize surrounding information and identify chunks with coherent logic. However, it minimally affects generation performance, as retrieving more chunks sharing similar useful information (increased context precision 69.3\u219271.1) does not necessarily increase the total amount of retrieved useful information (comparable claim recall 77.8\u219278.1).\nSuggestions to RAG Builders\nImproving the retriever is an effective way to enhance overall performance. While a better embedding model leads to improvements in both precision and recall, moderately increasing the number and size of chunks improves recall and thus F1 with minimal efforts in practice. Note that the effect saturates as the total amount of relevant information is fixed, so they need not be too large for a balanced cost-performance. On the other hand, given a limited number of context, larger chunk sizes with fewer chunks are preferred for better context precision. However, when targeting better context utilization or reduced noise sensitivity, opposite adjustments should be made to alleviate the influence of noise.\nWhen tuning the generator, the trilemma of context utilization, noise sensitivity, and faithfulness makes it difficult to improve all aspects simultaneously. RAG builders should prioritize certain aspects in the prompt based on their targets, user preferences and the generator's capability."}, {"title": "Conclusion", "content": "This paper presents RAGCHECKER, a novel evaluation framework designed for RAG systems. We validate our comprehensive suite of metrics, both overall and modular, through rigorous human assessments, demonstrating a strong correlation with evaluations conducted by human annotators. We have undertaken a detailed evaluation of eight distinct RAG systems using these metrics, yielding pivotal insights into the behaviors of the retriever and generator components and the trade-offs inherent in RAG system designs. These findings not only deepen our understanding of RAG system architectures but also furnish critical guidance for future advancements in RAG applications."}, {"title": "Limitations", "content": "While RAGCHECKER provides a comprehensive evaluation framework for RAG systems, it has a few limitations that should be acknowledged and addressed in future research.\nFirst, the diagnostic metrics for the retriever component are less insightful compared to those for the generator. The retrieval metrics primarily focus on the recall of ground truth claims and precision of retrieved context, but they may not fully capture the nuances and complexities of the retrieval process. Developing more sophisticated metrics that consider factors such as the information density, diversity and coherence of the retrieved context could provide deeper insights into the retriever's performance.\nSecond, the metrics proposed in RAGCHECKER do not differentiate between Neutral and Contradiction checking results from RefChecker when evaluating the generated responses. These two types of results may have different impacts on the final response quality, and treating them equally could lead to an incomplete assessment. Future work should explore ways to incorporate the distinction between neutral and contradiction results into the evaluation metrics, potentially assigning different weights or penalties based on their severity.\nFinally, the evaluation benchmark used in this study is curated based on existing text-only datasets and is limited to English queries and corpus. While this allows for a focused evaluation of RAG systems, it may not fully represent the diverse range of tasks and languages that RAG systems can be applied to. Expanding the benchmark to include datasets from different modalities (e.g., images, audio) and languages would provide a more comprehensive assessment of RAG systems' capabilities and generalization. Additionally, creating benchmark datasets specifically designed for evaluating RAG systems, rather than repurposing existing ones, could help to better capture the unique challenges and requirements of this task."}, {"title": "Potential Negative Societal Impacts", "content": "The RAGCHECKER evaluation framework, while beneficial for assessing RAG systems, could inadvertently lead to several negative societal impacts. There is a risk that developers may focus on optimizing for RAGCHECKER's specific metrics to the detriment of broader utility and ethical considerations. The computational and financial requirements to meet RAGCHECKER standards could disadvantage smaller organizations, potentially centralizing innovation among well-resourced entities. Moreover, an overreliance on quantitative measures might neglect qualitative factors like user experience and ethical implications."}, {"title": "Details for Benchmark Curation", "content": "In this section, we introduce the benchmark datasets and the curation process for RAG evaluation. This benchmark datasets are derived from existing open-domain question answering (ODQA) datasets, including RobustQA [9], KIWI [51], ClapNQ [34], and NovelQA [47]. However, most of the ground truth answers in existing ODQA datasets are short answers, while the answers provided by modern LLM-based RAG systems tend to be long-form answers. Therefore, we repurpose the ODQA datasets by eliminating overly simple questions and converting the short answers into long-form answers to match the capabilities of current RAG systems. The statistics of the benchmark are summarized in Tab. 1. In the rest of this section, we describe the datasets we use and the curation process for each domain.\nA.1 Data Sources\nRobustQA We choose 7 domains from RobustQA's collection of datasets: Biomedical, Finance, Lifestyle, Recreation, Technology, Science, and Writing. For the Biomedical domain, following RobustQA, we employ the BioASQ [43] dataset, which contains human expert-written question- answer pairs and ground truth documents based on abstracts of articles from PubMed. We use the test sets for Task b from 2014 to 2023 and the corpus of v.2022 to construct the benchmark. We keep QA pairs whose answers are relatively long (more than 50 words), obtaining 511 QA pairs for the biomedical domain. The other 6 domains are sourced from FiQA [26] and LoTTE [36], each of their question is annotated with a list of short answers that are spans of ground truth passages. We convert the short answers to long-form answers using GPT-4 and only keep the generated answers with no hallucinations, as checked by RefChecker. Finally, we sample 500 examples for each domain.\nClapNQ Derived from NaturalQuestions (NQ) [17], an ODQA dataset based on Wikipedia, ClapNQ has long-form answers annotated for a subset of NQ for evaluating RAG. We employ the dev set of ClapNQ in our benchmark and take the annotated long-form answers as the ground truth.\nKIWI is constructed by asking LLMs research questions about a set of NLP papers and guiding the LLMs to reach satisfactory long-form answers. The authors validated the quality of the generated answers by rating them as \u201cgood\u201d, \u201cneutral", "bad\". We take the answers labeled \"good\" as the ground truth answers and query the full text of the papers from S2ORC [23": "as the corpus. As a result, we obtain 71 QA pairs and 429 papers as the corpus.\nNovelQA is a benchmark for question answering over long novels containing over 100K tokens on average. Originally designed for benchmarking long-context LLMs, we repurpose it for evaluating RAG. In contrast with the other domains, each question in NovelQA is associated with a single novel, so when we use this dataset for RAG, we constrain the retrieval to within the corresponding novel. We select 19 copyright-free novels and convert the corresponding short answers to long-form answers following the same process for RobustQA.\nA.2 Long-form Answer Generation\nWe employ GPT-4 (gpt-4-turbo-2024-04-09) to convert the human annotated short answers to long-form answers in the dataset of RobustQA and NovelQA. For RobustQA, the short answers are spans of the annotated ground truth passages, we take all the annotated short answers and the corresponding passages in the prompt and ask GPT-4 to convert them to one single long-form answer. For NovelQA, we take the human written evidences as the ground truth passage content and the human written short answers for the long-form answer generation. The prompt is shown in Fig. 2.\nFor quality control, we ask GPT-4 to generate the passage IDs associated with the long-form answer. We use RefChecker to check whether all the claims of a long-form answer are entailed by these passages, and we only keep the long-form answers that meet this criteria. The RefChecker we used here are described in Appendix G.\nA.3 Corpus Downsampling for Science and Biomedical Domains\nIn addition to long-form answer generation, we also perform downsampling for the corpora of Science and Biomedical domains as they are much larger than the others, with over 1 million documents each. Building indexes for a dense retriever is very costly for large corpora, so we downsample these domains to lower the evaluation cost for the community. For the biomedical domain, we first use BM25 retriever to obtain top 400 documents for each question. The subsampled corpus is formed by combining all documents from the retriever with annotated relevant documents from the datasets. Based on our initial study, we observe that the BM25 retriever yeild competitive performance against the dense retriever, so we decide to only use the BM25 retriever for downsampling purpose to save compuation cost. For the science domain, we leverage both the BM25 retriever and e5-mistral- 7b-instruct based dense retriever to obtain document candidates. Specifically, we retrieve the top 200 documents from both retrievers (400 documents in total before deduplication). Similarly, the combination of all documents from the retrievers and annotated relevant documents from datasets forms the downsampled corpus.\nA.4 License of The Datasets\nThe annotations from RobustQA, ClapNQ and NovelQA are under Apache-2."}]}