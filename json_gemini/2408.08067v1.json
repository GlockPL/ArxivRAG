{"title": "RAGCHECKER: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation", "authors": ["Dongyu Ru", "Lin Qiu", "Xiangkun Hu", "Tianhang Zhang", "Peng Shi", "Shuaichen Chang", "Jiayang Cheng", "Cunxiang Wang", "Shichao Sun", "Huanyu Li", "Zizhao Zhang", "Binjie Wang", "Jiarong Jiang", "Tong He", "Zhiguo Wang", "Pengfei Liu", "Yue Zhang", "Zheng Zhang"], "abstract": "Despite Retrieval-Augmented Generation (RAG) has shown promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGCHECKER, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGCHECKER has significantly better correlations with human judgments than other evaluation metrics. Using RAGCHECKER, we evaluate 8 RAG systems and conduct an in-depth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RAGCHECKER can guide researchers and practitioners in developing more effective RAG systems.", "sections": [{"title": "Introduction", "content": "Retrieval-Augmented Generation (RAG) systems [18, 7] enhance Large Language Models (LLMs) by incorporating external knowledge bases, enabling more precise and contextually relevant responses [7, 53, 13]. As these systems become integral to a variety of applications [54, 2, 8], it's imperative to develop robust and comprehensive evaluation frameworks to assess their performance and identify areas for improvement. Evaluating RAG systems, however, presents several challenges:\n(1) modular complexity: The modular nature of RAG systems, comprising both a retriever and a generator, complicates the design of effective evaluation metrics. It is crucial to establish metrics that can holistically assess the entire system as well as evaluate the individual modules and their interplay [53], allowing for fully understanding the sources of the errors and misses and how they are generated. (2) metric limitation: Existing metrics for evaluating RAG systems, which are often rule-based or coarse-grained, fall short in providing accurate and interpretable results. Specifically, traditional metrics like recall@k and MRR [44] for retrievers depend on annotated chunks and a rigid chunking approach, missing out on the full semantic scope of the knowledge base. For generators, typical measures such as n-gram-based (e.g., BLEU [30], ROUGE [19]), embedding-based (e.g., BERTScore [56]), and LLM-based methods [45] perform well with concise answers but fail to detect finer distinctions in longer responses. To bridge these gaps, it is essential to develop detailed, semantic-based evaluation metrics that effectively capture the intricacies and overall quality of both the retrieval and generation components in RAG systems. (3) metric reliability: the reliability of existing metrics for RAG remains under-explored. Effective evaluation metrics must not only accurately reflect system performance but also align with human judgments to ensure their utility in real-world scenarios.\nTo overcome these challenges, we introduce RAGCHECKER, an innovative evaluation framework designed for detailed analysis of both retrieval and generation processes. RAGCHECKER is based on claim-level entailment checking which involves operations of extracting claims from the response and ground truth answer and checking them against other texts. This approach enables fine-grained evaluation instead of response-level assessment. RAGCHECKER processes the user query, retrieved context, response, and ground truth answer, producing a suite of metrics:\n1. Overall Metrics to provide a holistic view of the system performance, assessing the overall quality of the generated responses.\n2. Diagnostic Retriever Metrics to evaluate the effectiveness of the retriever, identifying its strengths and weaknesses in finding relevant information from the knowledge base.\n3. Diagnostic Generator Metrics to assess the performance of the generator, diagnosing how well the generator utilizes the retrieved context, handles noisy information, and generates accurate and faithful responses.\nCompared to existing evaluation frameworks, RAGCHECKER provides a more comprehensive assess-ment of RAG systems. While some frameworks offer fine-grained evaluation only on certain metrics (e.g., RAGAS [5], TruLens [6], ARES [35]) or evaluate specific aspects of RAG (e.g., RGB [4], RECALL [22], NoMIRACL [40]), RAGCHECKER's metrics are all based on fine-grained claim-level checking and are designed to provide actionable insights into the sources of errors.\nTo ensure the reliability of RAGCHECKER, we annotate a human judgment dataset to assess the correlations between the proposed metrics and human judgments. This meta-evaluation validates the effectiveness of RAGCHECKER in capturing the quality and reliability of RAG systems from a human perspective. We demonstrate the effectiveness of RAGCHECKER through comprehensive experiments evaluating 8 state-of-the-art RAG systems on a benchmark repurposed from public datasets across 10 domains. In-depth analysis of the evaluation results reveals that RAGCHECKER provides insightful diagnostic signals (Sec. 4.3) pointing the directions for improvements of RAG systems (Sec. 4.4).\nThe main contributions of this paper are as follows:\n\u2022 We propose RAGCHECKER, a novel RAG evaluation framework that offers fine-grained evaluation for both the retriever and generator components, introducing new diagnostic metrics to provide actionable insights into the sources of errors.\n\u2022 We conduct meta evaluation and verified RAGCHECKER has significantly better correlations with human judgements than other evaluation metrics.\n\u2022 We perform extensive experiments evaluating 8 RAG systems on our curated benchmark across 10 domains, and uncover valuable insights, such as the trade-off between retrieval improvement and noise introduction, and the tendency of faithful open-source models to blind trust on context."}, {"title": "Related Work", "content": "2.1 Retrieval Augmented Generation\nLarge Language Models (LLMs) demonstrate strong capabilities in generating text, but there are also obstacles such as outdated information and the potential to hallucinate [42, 46, 12]. To address these issues, RAG retrieves external knowledge to generate responses with improved accuracy and factuality [7, 53, 13]. Integrating external knowledge is especially crucial in fields like legal, medical and finance, where precision and reliability are essential [24, 50, 55].\nRAG systems have shown impressive performance across a range of tasks, including open-domain question answering [27, 10, 18], code generation [32, 57, 38] and dialogue [37, 16, 41]. Additionally, real world products like Bing Search\u00b3 and Langchain [3] have integrated applications based on RAG."}, {"title": "Evaluation of RAG", "content": "Existing evaluation practices for RAG systems can be categorized into two main approaches: evaluat-ing essential capabilities of generators only and assessing end-to-end performance of RAG systems.\nWithin the two components of a RAG system, the retriever has been well studied in recent years, thus a line of recent work focused on evaluating essential generator capabilities. RGB [4] evaluated 4 fundamental abilities required for generators including Noise Robustness, Negative Rejection, Infor-mation Integration and Counterfactual Robustness by manually constructed test sets. RECALL [22] introduced manually edited counterfactual contexts into QA and text generation datasets to evaluate the counterfactual robustness of LLMs. NoMIRACL [40] evaluated LLMs' robustness against first-stage retrieval errors of RAG systems with manually judged relevant and non-relevant datasets. Wu et al. [49] quantified the tug-of-war between LLMs' faithfulness and internal prior by introducing varying levels of perturbations on the provided contexts. FaaF [15] introduced a fine-grained fact verification formulation to improve previous prompting-based approaches in evaluating factuality of generators. However, we argue that above generator-only evaluation approaches with manually constructed datasets cannot serve as a general RAG evaluation framework to reveal the entanglement of between generation results and different retrieval behaviors, as shown in the analysis of Sec. 4.3.\nAnother line of work focused on assessing end-to-end quality scores of RAG systems. TruLens [6] introduced the concept of RAG Triad, which decompose the quality scores into three aspects: context relevance, groundedness and answer relevance, then predicted the score by prompting LLMs or using NLI models. RAGAS [5] and ARES [35] followed the RAG Triad concept and improved the score prediction approaches on different datasets. CRUD-RAG [25] refered to the CRUD (Create, Read, Update and Delete) actions between users and knowledge bases to develop corresponding datasets and evaluation metrics for RAG systems. We compare the above four evaluation frameworks with RAGCHECKER in the meta evaluation of Sec. 4.2.\nBesides, the following work also provided good insight or high quality datasets for end-to-end RAG evaluation. Liu et al. [21] conducted human evaluation to audit four popular generative search engines in terms of fluency, perceived utility, and verifiability. MEDRAG [50] constructed a medical RAG benchmark from medical QA datasets and evaluated medical RAG systems with QA accuracy. MultiHop-RAG [39] generated multi-hop queries from news articles and evaluated RAG systems with QA accuracy. CDQA [52] proposed a novel approach to generate dynamic QA questions which requires latest information to answer. However, the evaluation metrics used in the work mentioned above rely either on human evaluation or simple textual accuracy, making them incapable of complex RAG scenarios that require long answer evaluation. Therefore, we do not include them in the meta evaluation."}, {"title": "RAGCHECKER Framework", "content": "Formulation Define a modular RAG system as $RAG = \\{R,G\\}$, where R is the retriever and G is the generator. Given a query q and documents D, it first retrieves top-k relevant context $\\{chunk_j\\} = R(q, D, k)$, and then generates a model response $m = G(\\{chunk_j\\}, q)$. For simplicity, we can also represent the overall RAG generation process as $m = RAG(q, D)$.\nDesign Principle Given the compositional nature of RAG, we observe there are two major personae using a RAG evaluation framework. The first persona is a user that cares about the overall performance of RAGs and might choose a system with the best performance. Such a persona prefers a single value metric to compare and rank among RAG systems against a benchmark. The second persona is a developer that focuses on improving a RAG system with the need to identify causes of mistakes and potential rooms for improvements. Causes of errors in response can be classified into 1) retrieval errors, where the retriever fails to return complete and relevant context, and 2) generator errors, where the generator struggles to identify and leverage relevant information from context.\nConsequently, metrics that reveal error causes should be different from those for overall performance, in the sense that error causes are module-specific or even reflected only by a certain behavior of a module. To help both personae to assess RAG performance, we design RAGCHECKER, a evaluation framework of RAG systems that consists of a benchmark with rich annotations and a set of diversely-purposed fine-grained metrics."}, {"title": "Inputs to RAGCHECKER", "content": "We prepare each sample in our benchmark dataset in the format of a tuple (q, D, gt) representing query, documents, and ground-truth answer, where query is the input question to a RAG system, documents form the database providing possible context and are processed into chunks with the same number of tokens, and ground-truth answer is a complete and correct answer for the input question. Further information is provided in Sec. 4.1."}, {"title": "Fine-grained Evaluation with Claim Entailment", "content": "As illustrated in Fig. 1, a response generated by a RAG system might be a mixture of correct (0) and incorrect claims (\u00d7), while also missing some in-ground-truth claims ( A ). In this sense, evaluating responses at a finer granularity is crucial to comprehensively assess the quality of an answer. For this purpose, we introduce two components: 1) a text-to-claim extractor that decomposes a given text T into a set of claims {c}, and 2) a claim-entailment checker to determine whether a given claim e is entailed (\u2208) in a reference text Ref or not (4)."}, {"title": "RAGCHECKER Metrics", "content": "With the annotation and claim-level entailment functions specified, we next define the metrics. For a RAG user, we design metrics to compare the performance among RAG systems, including a single-value F1 score as an overall metric. For a RAG developer, on the other hand, we propose two sets of modular metrics for the retriever and the generator in a RAG system respectively, that aim to decompose the system and diagnose the source of errors. In the rest of this section, we will first introduce the overall metrics and then go over modular metrics for retriever and generator separately. The formulas for each metric are summarized in Appendix B."}, {"title": "Overall Metrics", "content": "To assess the overall response quality of a RAG system from a user's perspective, we can compute the precision and recall at claim level for each model generated response against its paired ground-truth answer. Specifically, we first extract claims from a model response m and a ground-truth answer gt as $\\{c^{(m)}\\}$ and $\\{c^{(gt)}\\}$ respectively. Then, we define correct claims in the response as $\\{c^{(m)}|c^{(m)} \\in gt\\}$, and correct claims in the ground-truth answer as $\\{c^{(gt)}|c^{(gt)} \\in m\\}$. Two metrics can be computed directly: precision is the proportion of correct claims in all response claims, and recall is the proportion of correct claims in all ground-truth answer claims. Further, the harmonic average of precision and recall gives the F1 score, as the overall performance metric."}, {"title": "Retriever Metrics", "content": "Ideally, a perfect retriever returns precisely all claims needed to generate the ground-truth answer. Completeness-wise, we can measure how many claims made in the ground-truth answer are covered by retrieved chunks. With retrieved chunks as the reference text, we compute claim recall as the proportion of $\\{c^{(gt)}|c^{(gt)} \\in \\{chunk_i\\}\\}$.\nDifferently, we define the retriever precision at chunk-level instead of claim-level. A retrieved chunk is called relevant chunk (r-chunk), if any ground-truth claim is entailed in it. In other words, $chunk_i$ is a relevant chunk if $\u2203i, s.t. c^{(gt)} \\in chunk_i$. The rest retrieved chunks are called irrelevant chunk (irr-chunk). The retriever's context precision is defined as $|\\{r-chunk_i\\}|/k$, where k is the number of all retrieved chunks.\nNote that a chunk-level precision provides better interpretability than a claim-level one, because in practice RAG systems usually work with documents processed to be text chunks in a fixed size. That being said, it is likely that a chunk may contain relevant claims and irrelevant or misleading information at the same time. As a result, the best possible retriever can only achieve a claim-level precision score lower than 100%, and such an upper-bound varies depending on the actual text distribution in D and chunking strategy."}, {"title": "Generator Metrics", "content": "Given k retrieved chunks (possibly mixing relevant and irrelevant information), a perfect generator would identify and include all ground-truth-relevant claims and ignore any that are not. Because the generator's results have dependency on retrieved chunks, we provide in total six metrics characterizing different aspects of its performance.\nGiven a model response m and its claims $\\{c^{(m)}\\}$, we first compute the proportion of $c^{(m)}$ that are entailed in retrieved chunks. This metric is faithfulness, as it describes how faithful the generator is to the provided context, thus the higher the better.\nNext, we examine three types of incorrect response claims, i.e. $\\{c^{(m)}|c^{(m)} \\notin gt\\}$.\n1. The first type includes incorrect claim that are entailed in a relevant chunk, then it indicates the generator is sensitive to noise coupled with useful information. The proportion of this type of claims to all $\\{c^{(m)}\\}$ is relevant noise sensitivity.\n2. The second type includes incorrect claim that are entailed in an irrelevant chunk, then it indicates the generator is also sensitive to noise even in an irrelevant context. The proportion of these incorrect claims is irrelevant noise sensitivity.\n3. Finally, the third type includes incorrect claims that are not entailed in any retrieved chunk, meaning all such claims are generated by the generator itself. Its proportion is hallucination.\nNote that for simplicity we group the two noise sensitivities in Fig. 1, but later in Sec. 4.3 we can see that generators generally has different sensitivity to relevant and irrelevant noise.\nFinally, we characterize how a generator uses information sources to produce correct claims. A correct claim not entailed by any chunk can only be based on generator's self-knowledge, thus the proportion of these claims reflects how many correct claims are generated on its own. A lower self-knowledge score is better, when the generator is expected to fully depend on retrieved context only in a RAG system. On the other hand, we also check how much retrieved relevant information is used by the generator. Retrieved relevant information is measured by the number of ground-truth answer claims entailed in retrieved chunks, while the evidence of being used by generator is reflected by entailment in model response. Therefore, the context utilization is computed as the ratio between $|\\{c^{(gt)}|c^{(gt)} \\in \\{chunk_i\\} and c^{(gt)} \\in m\\}|$ and $|\\{c^{(gt)}|c^{(gt)} \\in \\{chunk_i\\}\\}|$. Generally a higher context utilization is preferred."}, {"title": "Experiments", "content": "4.1 Experimental Setup\nBaseline RAG Systems We apply RAGCHECKER to 8 customized RAG systems to demonstrate how these metrics reflect the properties and differences among them, and how they guide the refinement of these systems. The 8 RAG systems are combinations with 2 retrievers and 4 generators. For retrievers, we choose BM25 [33], a representative classic sparse retrieval framework, and E5-Mistral [48], the SOTA open-source dense retriever. Our four generators are GPT-4 [29], Mixtral-8x7B [14], Llama3-8B, and Llama3-70B [1], covering open-source and proprietary LLMs in various sizes. Further details are deferred to Appendix D. We employ Llama3-70B as both the claim extractor and checker models implemented by an open-sourced framework RefChecker\u2074 [11]. As a validation of its performance on the RefChecker's hallucination detection benchmark, this setup outperforms the best purely open-sourced combinations reported in RefChecker's paper (see Appendix G).\nBenchmark Datasets For comprehensive evaluations, we curate a benchmark containing 4,162 queries across 10 domains. This benchmark is repurposed from public datasets of open domain question answering, spanning domains of Wikipedia, AI science, novel, biomedical, finance, lifestyle, recreation, science, technology and writing. We convert the short answers to long-form answers in the datasets to align with the current LLM-based RAG systems. Please refer to Appendix A for the details of the benchmark curation process. The statistics of the benchmark are shown in Tab. 1."}, {"title": "Meta Evaluation", "content": "We first conduct the meta evaluation to verify the soundness of RAGCHECKER and compare with existing baseline RAG evaluation frameworks.\nBaseline RAG Evaluation Frameworks We include a total of 10 metrics from Trulens [6], RA-GAS [5], ARES [35] and CRUD-RAG [25] in the meta evaluation, as they are capable to evaluate end-to-end performance with long answers. Metrics selected for comparison along with their descriptions are summarized in Tab. 4 of Appendix C. To ensure a fair comparison, we use Llama3-70B-Instruct as the LLM backbone when applicable. Since models in the Llama3 family don't provide an embedding model, baseline metrics requiring embedding capability still use their corresponding default LLM backbones. In addition to the 10 metrics detailed in the table, we also incorporate BLEU [31], ROUGE-L [20], and BERTScore [56] to assess the correlation between the generated responses and the ground truth answers.\nMeta Evaluation Dataset All baseline metrics are designed with different aspects and functionalities to a certain degree, thus making an exact comparison over metric scores inapplicable. However, we argue that a good metric should reflect the relative human preference over different RAG systems. In this spirit, we construct the meta evaluation dataset with sampled instances from the generated responses of 8 baseline RAG systems introduced in Sec. 4.1 on our benchmark. Each meta evaluation instance is a pair of responses from two baseline RAG systems given the same query. By considering all combinations over 10 domains and 28 baseline pairs, we end up with 280 instances for pairwise human preference labeling. For each instance, annotators compare a pair of responses based on cor-rectness, completeness, and overall assessment. For each aspect, annotators measure their preferences as one out of five relative choices, including significantly better, slightly better, tie, slightly worse and significantly worse. For quality control, each instance is annotated by two annotators, and their overall agreement and correlation are measured. To conclude, we build a meta evaluation dataset with 280 instances, each instance is labeled by two annotators with their preference in terms of correctness, completeness and overall assessment."}, {"title": "Main Results", "content": "We present the averaged evaluation results for 8 RAG systems across 10 diverse domain datasets in Tab. 3. Additional results for all datasets are provided in Appendix E. The RAG system that exhibited the best performance in our experiments is E5-Mistral_GPT-4, owing to the strong retrieval capability of E5-Mistral coupled with the adept comprehension abilities of GPT-4. Next, we provide a list of insights induced from Tab. 3, along with their interpretation and possible directions for improvements."}, {"title": "Diagnosis on RAG Settings for Improvements", "content": "Guided by observations in Sec. 4.3, we modify settings commonly tuned in RAG systems that may lead to improvements, diagnose their working mechanisms with RAGCHECKER metrics, and provide suggestions for improvements on certain aspects. We experiment with different numbers of chunks, chunk sizes, chunk overlap ratios, and generation prompts. We highlight our main findings and suggestions as below, please refer to Appendix F for detailed analysis and results.\nMore Context Enhances Faithfulness. Increasing the number (k) and size of chunks improves the recall of more useful information (claim recall 61.5\u219277.6 with k 5\u219220, 70.3 77.6 with size 150-300). Consequently, this provides more context for the generators to be more faithful to (faithfulness 88.1\u219292.2 with k 5\u219220, 91.2 92.2 with size 150\u2192300), though at the same time they also become more sensitive to additional noise (noise sensitivity 34.0\u219235.4 with k 5\u219220, 34.5-35.4 with size 150-300). Improvements in the overall performance (F1 51.7\u219253.4 with k 5-20, 52.6\u219253.4 with size 150\u2192300) indicates benefits from more context.\nExplicit Requirements in Prompts Affect Generation Preferences. When prompts introduces explicit requirements for better faithfulness, context utilization, and lower noise sensitivity, generators show improvements in faithfulness (92.2\u219293.6), but struggle with the subtle tension between context utilization (59.2\u219263.7) and noise sensitivity (35.4\u219238.1).\nChunk Overlap Does Not Matter a Lot. The chunk overlap ratio is usually set to be non-zero to help generators better utilize surrounding information and identify chunks with coherent logic. However, it minimally affects generation performance, as retrieving more chunks sharing similar useful information (increased context precision 69.3\u219271.1) does not necessarily increase the total amount of retrieved useful information (comparable claim recall 77.8\u219278.1).\nSuggestions to RAG Builders\nImproving the retriever is an effective way to enhance overall performance. While a better embedding model leads to improvements in both precision and recall, moderately increasing the number and size of chunks improves recall and thus Fl with minimal efforts in practice. Note that the effect saturates as the total amount of relevant information is fixed, so they need not be too large for a balanced cost-performance. On the other hand, given a limited number of context, larger chunk sizes with fewer chunks are preferred for better context precision. However, when targeting better context utilization or reduced noise sensitivity, opposite adjustments should be made to alleviate the influence of noise.\nWhen tuning the generator, the trilemma of context utilization, noise sensitivity, and faithfulness makes it difficult to improve all aspects simultaneously. RAG builders should prioritize certain aspects in the prompt based on their targets, user preferences and the generator's capability."}, {"title": "Conclusion", "content": "This paper presents RAGCHECKER, a novel evaluation framework designed for RAG systems. We validate our comprehensive suite of metrics, both overall and modular, through rigorous human assessments, demonstrating a strong correlation with evaluations conducted by human annotators. We have undertaken a detailed evaluation of eight distinct RAG systems using these metrics, yielding pivotal insights into the behaviors of the retriever and generator components and the trade-offs inherent in RAG system designs. These findings not only deepen our understanding of RAG system architectures but also furnish critical guidance for future advancements in RAG applications."}, {"title": "Corpus Downsampling for Science and Biomedical Domains", "content": "In addition to long-form answer generation, we also perform downsampling for the corpora of Science and Biomedical domains as they are much larger than the others, with over 1 million documents each. Building indexes for a dense retriever is very costly for large corpora, so we downsample these domains to lower the evaluation cost for the community. For the biomedical domain, we first use BM25 retriever to obtain top 400 documents for each question. The subsampled corpus is formed by combining all documents from the retriever with annotated relevant documents from the datasets. Based on our initial study, we observe that the BM25 retriever yeild competitive performance against the dense retriever, so we decide to only use the BM25 retriever for downsampling purpose to save compuation cost. For the science domain, we leverage both the BM25 retriever and e5-mistral-7b-instruct based dense retriever to obtain document candidates. Specifically, we retrieve the top 200 documents from both retrievers (400 documents in total before deduplication). Similarly, the combination of all documents from the retrievers and annotated relevant documents from datasets forms the downsampled corpus."}, {"title": "License of The Datasets", "content": "The annotations from RobustQA, ClapNQ and NovelQA are under Apache-2.0 License. The corpora of Finance and annotations of KIWI are under CC-BY-SA-4.0. BioASQ is under CC BY 2.5 license. The license for the corpora of LoTTE are not specified."}, {"title": "The complete formula for all metrics", "content": "Denote the model response as m, the ground truth answer as gt, and the retrieved chunks as $\\{chunk_i\\}$. Leveraging RefChecker, we decompose the text into a set of claims $\\{c_i\\}$ and assess whether a specific claim $c_i$ can entail (\u2208) or not entail (\u2209) a given reference text Ref, where Ref may represent m, gt, or $\\{chunk_i\\}$. We assign an entailment label to each ground-truth claim relative to a chunk, and subsequently classify these chunks into relevant chunks $\\{r-chunk_i\\}$ and irrelevant chunks $\\{irr-chunk_i\\}$. Specifically, $chunk_i$ is considered relevant if it contains at least one claim $c^{(gt)}$ such that $c^{(gt)} \\in chunk_i$.\nIn accordance with the definitions provided in Section 3.3, we compute each metric using the following formulations:\nPrecision = $\\frac{\\{c^{(m)} | c^{(m)} \u2208 gt\\}}{\\{c^{(m)}\\}}$\nRecall = $\\frac{\\{c^{(gt)} | c^{(gt)} \u2208 m\\}}{\\{c^{(gt)}\\}}$\nClaim Recall = $\\frac{\\{c^{(gt)} | c^{(gt)} \u2208 \\{chunk_i\\}\\}}{\\{c^{(gt)}\\}}$\nContext Precision = $\\frac{\\{r-chunk_i\\}}{k}$\nFaithfulness = $\\frac{\\{c^{(m)} | c^{(m)} \u2208 \\{chunk_i\\}\\}}{\\{c^{(m)}\\}}$\nRelevant Noise Sensitivity = $\\frac{\\{c^{(m)} | c^{(m)} \u2209 gt \\ and \\ c^{(m)} \u2208 \\{r-chunk_i\\}\\}}{\\{c^{(m)}\\}}$\nIrrelevant Noise Sensitivity = $\\frac{\\{c^{(m)} | c^{(m)} \u2209 gt \\ and \\ c^{(m)} \u2208 \\{irr-chunk_i\\}\\}}{\\{c^{(m)}\\}}$\nHallucination = $\\frac{\\{c^{(m)} | c^{(m)} \u2209 gt \\ and \\ c^{(m)} \u2209 \\{chunk_i\\}\\}}{\\{c^{(m)}\\}}$\nSelf-knowledge = $\\frac{\\{c^{(m)} | c^{(m)} \u2208 gt \\ and \\ c^{(m)} \u2209 \\{chunk_i\\}\\}}{\\{c^{(m)}\\}}$\nContext Utilization = $\\frac{\\{c^{(gt)} | c^{(gt)} \u2208 \\{chunk_i\\} \\ and \\ c^{(gt)} \u2208 m\\}}{\\{c^{(gt)} | c^{(gt)} \u2208 \\{chunk_i\\}\\}}"}, {"title": "Details of Meta Evaluation", "content": "In the meta evaluation, we ask 10 annotators compare two responses from the RAG system for each instance in the meta evaluation dataset. Seven of the annotators are in-house annotators, and three of them are graduate students. We pay the students 15 USD per hour and totally cost 255 dollars.\nAnnotators are required to choose their preference from five options: significantly better, slightly better, tie, slightly worse, or significantly worse. The annotation is based on three metrics: correctness, completeness, and overall assessment. The annotation interface with instructions are shown in Fig. 3\nTo make sure the human evaluation to be agnostic to specific evaluation metrics, we provide the annotators with a detailed annotation guideline which contains detailed instruction and 5 examples."}, {"title": "Diagnosis on RAG for Improvements", "content": "We modify hyper-parameters commonly tuned in RAG systems to observe performance variance under the metrics defined by RAGCHECKER. We focus on how RAGCHECKER explains this variance and provides tuning suggestions for improvements on certain aspects. In this section, we evaluate three RAG baselines (BM25_GPT-4, E5-Mistral_GPT-4, and E5-Mistral_Llama3-70B) across three domains with increasing difficulty: Writing, Finance, and KIWI. We use our default settings (Appendix D) in main experiments as controls. We experiment with different numbers of chunks selected as context k \u2208{5,10,20}, different chunk size {150,300,600}', different chunk overlap ratio {0.0,0.2,0.4}, and different generation prompts.\nMore Context Enhances Faithfulness Top-k selection and chunk size both balance the amount of noise and useful information presented to the generator, but in different manners. Corresponding results are demonstrated in Fig. 6 and Fig. 7. Increasing k adds more context that could be less relevant, while increasing chunk size provides more surrounding context of relevant facts. Thus context precision decreases with larger k but increases with larger chunk sizes. Despite this, they both lead to better claim recall in Retrieval.\nGenerators tend to be more faithful when provided with more context, though this trend is less pronounced for Llama3, which already exhibits high faithfulness. Context utilization generally worsens with more context due to increasing noise, leading to higher relevant noise sensitivity.\nOverall, the end-to-end RAG performance is slightly better with more context, primarily due to im-proved recall. We recommend moderately increasing the two parameters for more faithful generation, noting that saturation occurs at high values as the amount of useful information is limited. Given a limited context length, a larger chunk size with a smaller k is preferred, especially for easier datasets (Finance, Writing). This is evident when comparing a chunk size of 150 with k=20 against a chunk size of 300 with k=10.\nExplicit Requirements in Prompts Affect Generation Preferences To validate the effect of the generation prompt, we added more detailed requirements to guide the generation for better faithfulness, context utilization, and lower noise sensitivity. The optimized prompt is shown in Fig. 9.\nAs shown in Fig. 8, we observed a general improvement in context utilization. However, as a counterpart to context utilization, noise sensitivity generally worsened. It demonstrates the difficulty of meeting all prompt requirements when there are subtle tension between them.\nFor the two generators, GPT-4 generally showes improvements in metrics related to faithfulness (hallucination, self-knowledge, faithfulness), whereas Llama3 does not exhibit the same behavior. This aligns with our previous observation (Sec. 4.3) that Llama3 already performs well on faithfulness,"}, {"title": "Chunk Overlap Does Not Matter a Lot", "content": "Chunk overlap ratio between adjacent chunks is usually set to be non-zero to help the generator better utilize surrounding information and identify chunks with coherent logic, thus alleviating the impact of hard splits in significant semantics.\nAccording to our results in Fig. 10, higher overlap ratios generally lead to improved context precision. However, this does not necessarily translate to an increase in the total amount of useful information retrieved. This phenomenon can be attributed to the retrieval of more chunks that contain the same segment of useful information. Consequently, we observed that overlap ratio adjustments do not have a significant impact on other performance metrics in a consistent and obvious manner. This suggests that the overlap ratio may not require extensive tuning in practice."}, {"title": "Performance Validation of RefChecker with Llama3 Extractor and Checker", "content": "We use Llama3-70B-Instruct for the extractor and checker in RefChecker. To validate the effectiveness of this combination, we test its performance on the RefChecker benchmark. As shown in Tab. 16, Llama 3 based RefChecker outperforms the best purely open-sourced combinations reported in the RefChecker paper in all the three context settings."}, {"title": "Limitations", "content": "While RAGCHECKER provides a comprehensive evaluation framework for RAG systems, it has a few limitations that should be acknowledged and addressed in future research.\nFirst, the diagnostic metrics for the retriever component are less insightful compared to those for the generator. The retrieval metrics primarily focus on the recall of ground truth claims and precision of retrieved context, but they may not fully capture the nuances and complexities of the retrieval process. Developing more sophisticated metrics that consider factors such as the information density, diversity and coherence of the retrieved context could provide deeper insights into the retriever's performance.\nSecond, the metrics proposed in RAGCHECKER do not differentiate between Neutral and Contradiction checking results from RefChecker when evaluating the generated responses. These two types of results may have different impacts on the final response quality, and treating them equally could lead to an incomplete assessment. Future work should explore ways to incorporate the distinction between neutral and contradiction results into the evaluation metrics, potentially assigning different weights or penalties based on their severity.\nFinally, the evaluation benchmark used in this study is curated based on existing text-only datasets and is limited to English queries and corpus"}]}