{"title": "On the Influence of Context Size and Model Choice in\nRetrieval-Augmented Generation Systems", "authors": ["Juraj Vladika", "Florian Matthes"], "abstract": "Retrieval-augmented generation (RAG) has\nemerged as an approach to augment large lan-\nguage models (LLMs) by reducing their re-\nliance on static knowledge and improving an-\nswer factuality. RAG retrieves relevant con-\ntext snippets and generates an answer based on\nthem. Despite its increasing industrial adoption,\nsystematic exploration of RAG components is\nlacking, particularly regarding the ideal size\nof provided context, and the choice of base\nLLM and retrieval method. To help guide de-\nvelopment of robust RAG systems, we evalu-\nate various context sizes, BM25 and semantic\nsearch as retrievers, and eight base LLMs. Mov-\ning away from the usual RAG evaluation with\nshort answers, we explore the more challenging\nlong-form question answering in two domains,\nwhere a good answer has to utilize the entire\ncontext. Our findings indicate that final QA\nperformance improves steadily with up to 15\nsnippets but stagnates or declines beyond that.\nFinally, we show that different general-purpose\nLLMs excel in the biomedical domain than the\nencyclopedic one, and that open-domain evi-\ndence retrieval in large corpora is challenging.", "sections": [{"title": "1 Introduction", "content": "The field of Natural Language Processing (NLP)\nhas been vividly transformed with the advent of\nlarge language models (LLMs), massive models\nthat excel on a wide range of complex tasks, in-\ncluding text generation, question answering, and\nsummarization (Zhao et al., 2023). Despite their\nimpressive performance, LLMs have certain limita-\ntions. The static nature of the knowledge encoded\nwithin their weights can lead to providing outdated\ncontent as new information emerges (Zhang et al.,\n2023). Furthermore, LLMs can generate plausible\nsounding but factually incorrect responses (hallu-\ncinations), as they lack a reliable mechanism to\nverify the accuracy of the information they produce\n(Ji et al., 2023). Finally, they can lack specialized\nknowledge related to advanced expert domains.\nTo address these shortcomings, the concept\nof Retrieval-Augmented Generation (RAG) has\nshown great potential (Lewis et al., 2020). RAG\nsystems enhance the capabilities of LLMs by inte-\ngrating a retrieval component that allows the model\nto dynamically utilize external knowledge sources\nduring the generation process. By retrieving rele-\nvant information from a curated corpus or the web\nin real-time, RAG models can produce more accu-\nrate, up-to-date, and contextually appropriate re-\nsponses (Fan et al., 2024). RAG systems have also\nseen wide adoption in various industry branches,\nwhere companies leverage them to build tools for\naccessing their internal documentation via ques-\ntions posed in human language (Xu et al., 2024).\nDespite their increasing popularity and use, there\nhere have been few studies that systematically ex-"}, {"title": "2 Related Work", "content": "2.1 Retrieval-Augmented Generation\nEarly approaches to RAG involved simple retrieval\nand were developed for the task of question answer-\ning (Chen et al., 2017). Recent advancements have\nseen more sophisticated integration of retrieval and\ngeneration processes, thereby significantly enhanc-\ning the quality and relevance of the generated text\n(Lewis et al., 2020). These advancements have\nbeen facilitated by improvements in both the re-\ntrieval mechanisms, which have become more effi-\ncient and effective at finding relevant information,\nand the generative models, which have become bet-\nter at integrating and contextualizing the retrieved\ninformation (Cai et al., 2022).\nA recent survey by Gao et al. (2024) separates\nRAG approaches into naive RAG and advanced\nRAG. The naive RAG approach follows a tradi-\ntional process that includes indexing, retrieval,\nand generation, also called a \u201cRetrieve-then-Read\"\nframework (Zhu et al., 2021). On the other hand,\nadvanced RAG introduces specific improvements\nto enhance the retrieval quality by employing pre-\nretrieval and post-retrieval strategies. Pre-retrieval\nstrategies include query rewriting with an LLM\n(Ma et al., 2023) or query expansion methods like\nHyDE (Gao et al., 2023). Post-retrieval methods\nfocus on selecting essential information from re-\ntrieved documents. This includes reranking the re-\ntrieved documents with neural models (Glass et al.,\n2022) or summarizing the retrieved documents be-\nfore passing them as context (An et al., 2021).\n2.2 Context and Noise in RAG Systems\nA lot of recent work has explored how to improve\nRAG and make it more accurate and robust to\nimperfect context. This includes fact verification\n(Li et al., 2024), self-reflection with critique (Asai\net al., 2024), learning to re-rank the context (Yu\net al., 2024), improved answer attribution (Vladika\net al., 2024a), adaptive search strategy (Jeong et al.,\n2024), and relevance modeling (Wang et al., 2024).\nThere have also been studies exploring the size\nof input context and its influence on the perfor-\nmance of RAG systems. Liu et al. (2024) highlight"}, {"title": "3 Foundations", "content": "3.1 RAG System for Question Answering\nTypically, a RAG system consists of a retriever\nand a reader. Retriever has to search and collect\nrelevant evidence snippets that are passed as con-\ntext inside of a prompt to the reader. Our study\ninvestigates the importance of those three aspects\n(retriever, context, reader) on the final performance\nof the whole system. We first focus on the influ-\nence of context size on the readers' QA capability,\nfollowed by the importance of choosing the reader\nby comparing different base LLMs on the task, and\nfinally, we test the influence of two different re-\ntrievers on the final QA performance (BM25 and\nsemantic search). To formally define: Given a ques-\ntion q and context c consisting of context snippets\n$c_1, c_2, ..., c_n$, the goal is to generate an answer a\nwith a model $reader(q, c) = a$. The context c is\nprovided in the first experiment, but in an open-\ndomain setting, given a document corpus D with\ndocuments $d_1, d_2, ...d_v$, the idea is for a retriever\nto $retrieve(q, D) = d_1, d_2$ best matching documents\nand then from them extract context snippets.\n3.2 Datasets\nBioASQ-QA (Krithara et al., 2023) is a biomed-\nical question answering (QA) benchmark dataset\nin English. It has been designed to reflect real\ninformation needs of biomedical experts. The ques-\ntions are written by biomedical experts and the\nevidence corpus used to answer them is PubMed\n(White, 2020), the large database of biomedical\nresearch papers. The dataset is a part of the ongo-\ning shared challenge, and we use the 2023 version,\nTask 10b. While the full dataset contains various\ntypes of questions (yes/no, factoid, lists), we uti-\nlize only the so-called summary questions \u2013 ques-\ntions paired with human-selected evidence snippets\nfrom PubMed abstracts and human-written \"ideal\nanswers\", which are essentially natural language\nsummaries of the provided snippets. In total, there\nare 1130 summary questions.\nQuoteSum (Schuster et al., 2024) is a dataset\nof encyclopedic questions, relevant passages, and\nhuman-written semi-extractive answers. The ques-\ntions are human-written and are paired with up\nto 8 passages (evidence snippets) from Wikipedia.\nThese passages are used as the main source by an-\nnotators to write the answers. QuoteSum contains\n805 instances and covers various domains such as\ngeography, history, arts, and technology. An exam-\nple question is \"Why was Stonehenge built in the\nfirst place?\".\nThese datasets contain the gold evidence snip-\npets and human-written answers based on the snip-\npets, making them a suitable testbed for our study.\nWhile BioASQ might be difficult given its lan-\nguage rich with complex biomedical terminology,\nthe main challenge is in successfully utilizing the\ngiven context and summarizing it into a concise\nbut informative answer. We intentionally do not\nbenchmark on any biomedical LLM to not give any\nmodel a possible advantage."}, {"title": "4 Experiment", "content": "4.1 Models\nWe conduct our experiments using a multitude of\ndifferent LLMs that serve as readers, i.e., the mod-\nels reading and comprehending the context and"}, {"title": "4.2 Setup", "content": "We use the same prompt and setup for all of the\nbenchmarked models:\nGive a simple answer to the question\nbased on the context.\nQUESTION:\n<the current question>\nCONTEXT:\n[snippet1, snippet2, ..., snippetn]\nANSWER:\nFor the internal-knowledge setting with no con-\ntext, the instruction was changed accordingly to\nGive a simple answer to the question based on your\nbest knowledge. and the CONTEXT part removed.\nWhile it would have been an interesting experiment\nto also give the LLMs few-shot examples of QA\npairs, we intentionally opt for this zero-shot setting\nso that the focus of the experiments lies solely on\nthe utilization of provided context for answering\nand not on potential in-context learning abilities.\nGPT models were prompted through the Ope-\nAI API, while all of the open-source models were\nqueried with API calls through the Together AI\nservice\u00b2 platform, which hosts many popular open-\nsource models. We set the token limit to 512 and\nthe temperature parameter to 0, maximizing deter-\nministic generation by favoring high-probability\nwords and thus ensuring reproducibility of the re-\nsults. One run through the whole dataset with five\nsettings took two computation hours. For embed-\nding models, we used one NVidia V100 GPU card\nwith 16 GB of VRAM.\n4.3 Experiment Rounds\nContext Size and Reader Performance. The\nfirst round of experiments consisted of varying the\nnumber of context snippets passed in the prompt\nand observing how the QA performance changes."}, {"title": "5 Results", "content": "5.1 Gold Snippets\nThe results of four large LLMs with gold snippets\nare present in Table 1. All models observe a similar\npattern: after starting with a rather low zero-shot\nperformance, already utilizing just one context snip-\npet leads to a big jump in performance. After that,\nmost models slowly and steadily improve their an-\nswers as measured by all three metrics. Looking at\ndifferent models, for BioASQ, GPT 40 and LLaMa\nhttps://huggingface.co/pritamdeka/\nS-PubMedBert-MS-MARCO\nhttps://huggingface.co/sentence-transformers/\nall-mpnet-base-v2"}, {"title": "5.2 Closed Retrieval", "content": "This setting used the small knowledge base of eight\nthousand PubMed articles that were used as gold\nevidence in BioASQ. The results of the experi-\nments are shown in Table 3. In this setting, it is\nvisible that the performance dropped when com-\npared to Table 1. Even in Mixtral, which was the\nbest performing model, the performance dropped\non average. Still, the performance kept improving"}, {"title": "5.3 Open Retrieval", "content": "The final setting used around 10 million PubMed\narticles as its knowledge base for retrieval. The\nidea of this experiment is to see (1) how much the\nperformance in the open setting differs from the\nclosed setting with gold evidence and (2) what the\ninfluence of different retrievers is on this perfor-\nmance. Results for BioASQ are shown in Table 4,\nwhile results for QuoteSum are shown in Table 5.\nSince we stored an offline copy of PubMed doc-\numents, we could use both BM25 and semantic\nsearch (with local vector embeddings), while for\nWikipedia, we used its search API based on BM25.\nWhen compared to previous tables, it is evident\nthat open retrieval is the most challenging setting,\nwith lowest average scores overall. It is also in-\nteresting to observe that retrieving the documents\nwith BM25 led to a slightly better final performance\ncompared to semantic search."}, {"title": "6 Discussion", "content": "6.1 Retrieval Techniques\nLooking at Table 3, BM25 led to a better perfor-\nmance overall. Given that it works with keyword\nmatches, this retrieval technique optimizes for pre-\ncision in search results rather than recall, thus en-\nsuring that more documents will actually be dis-"}, {"title": "6.2 Internal vs. External Knowledge Conflict", "content": "An interesting remark from open retrieval in Table\n4 is that both GPT and Mixtral have better scores\nfor their zero-shot answers (with 0 context snippets)\nthan the answers where up to 10 context snippets\nwere provided. After we analyzed many outputs,\na potential explanation of this phenomenon is that,\nwhile snippets discovered in the corpus can be se-\nmantically similar to the question, they do not al-\nways provide all the important information. On\nthe other hand, when using just the vanilla prompt,\nLLM answers based on its \"internal\" knowledge -\nthese answers reflect the collected knowledge of\nLLMs gained from the large pre-training corpora.\nTherefore, the internal LLM answers can be more\ninformative than the RAG setting where an LLM is\ninstructed to answer only using the provided short\nsnippets. As more snippets are added, the informa-\ntiveness of RAG answers starts surpassing LLM's\ninternal knowledge. Recent studies have also ob-\nserved that for biomedical tasks, it can sometimes\nbe more beneficial to generate internal answers\nthan retrieve external context (Frisoni et al., 2024).\nConsider the first example in Table 6 \u2013 the an-"}, {"title": "6.3 Context Saturation", "content": "Another insight of the study visible in Table 4 is\nthat there is a certain upper limit to the performance\nimprovements. As we kept on adding more and\nmore context, increasing to 20, the performance\nstalled and then slightly dropped for 30 retrieved\ncontext snippets. As the saturation point is reached,\nadding more context to the prompt just leads to\nnoise and confusion in answering. This confirms\nthe previous findings from literature that context\ncan get \"lost in the middle\" of long prompts and\nignored by the reader LLM when answering the\nquestions (Liu et al., 2024; Hsieh et al., 2024)."}, {"title": "7 Conclusion", "content": "In this study, we explored the effectiveness\nof Retrieval-Augmented Generation (RAG) sys-\ntems for long-form question answering using two\ndatasets. We systematically evaluated the impact of\nvarious settings of retrieval strategies, context sizes,\nand base LLMs on RAG performance. Our findings\nindicate that increasing context snippets enhances\nperformance up to around 15 snippets. For biomed-\nical QA, models like Mixtral and Qwen performed\nthe best, while they were outperformed by GPT-\n40 and LLaMa 3 for encyclopedic QA. In open\nretrieval setting, BM25 yielded better results for\nbiomedical QA, with open challenges remaining\nfor exploring knowledge conflict between internal\nLLM knowledge and external context. We envision\nfuture work to explore the effects of query expan-\nsion methods and evidence re-ranking. We hope\nour work provides valuable insights for optimizing\napplied RAG systems in practice."}, {"title": "Limitations", "content": "Our study is limited to two datasets, thus making it\npossible that some findings would not universally\ngeneralize to different domains and tasks. Addi-\ntionally, we only evaluate the models in a zero-shot\nsetting, whereas a few-shot setting with some ex-\namples of questions and answers would have led to\na more uniform performance across models.\nThe use of automated metrics for natural lan-\nguage generation tasks is not ideal, and they have\ncertain drawbacks. ROUGE score focuses too\nmuch on word overlaps with no semantic match-\ning, BERTScore often gives scores in a very tight\nrange, and NLI models can struggle with long text\nas input. Ideally, a human evaluation would bring a\nmore rigorous result assessment, but hiring human\nannotators, especially domain experts for the medi-\ncal text, was outside of our scope and resources.\nFinally, the LLMs, embedding models, and re-\ntriever models tested in this study represent only a\nsubset of the quickly evolving landscape of NLP\nmodels and technologies. We selected some of\nthe most popular and trending ones, but there are\ncertainly other models that warrant discussion and\nwould have led to an improved performance. Since\nmost experiments were conducted in June 2024, the\nchoice of benchmarked models reflects that. In the\nmeantime, GPT 40-mini has superseded GPT 3.5,\nLLaMa 3.3 is a continuation of LLaMa 3, Gemma\n2 was released, as well as Qwen 2 after Qwen 1.5."}]}