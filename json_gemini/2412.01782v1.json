{"title": "Identifying Reliable Predictions in Detection Transformers", "authors": ["Young-Jin Park", "Carson Sobolewski", "Navid Azizan"], "abstract": "DEtection TRansformer (DETR) has emerged as a promising architecture for object detection, offering an end-to-end prediction pipeline. In practice, however, DETR generates hundreds of predictions that far outnumber the actual number of objects present in an image. This raises the question: can we trust and use all of these predictions? Addressing this concern, we present empirical evidence highlighting how different predictions within the same image play distinct roles, resulting in varying reliability levels across those predictions. More specifically, while multiple predictions are often made for a single object, our findings show that most often one such prediction is well-calibrated, and the others are poorly calibrated. Based on these insights, we demonstrate identifying a reliable subset of DETR's predictions is crucial for accurately assessing the reliability of the model at both object and image levels.\nBuilding on this viewpoint, we first tackle the shortcomings of widely used performance and calibration metrics, such as average precision and various forms of expected calibration error. Specifically, they are inadequate for determining which subset of DETR's predictions should be trusted and utilized. In response, we present Object-level Calibration Error (OCE), which is capable of assessing the calibration quality both across different models and among various configurations within a specific model. As a final contribution, we introduce a post hoc Uncertainty Quantification (UQ) framework that predicts the accuracy of the model on a per-image basis. By contrasting the average confidence scores of positive (i.e., likely to be matched) and negative predictions determined by OCE, the framework assesses the reliability of the DETR model for each test image. The code is available at https://github.com/azizanlab/reliable-detr.", "sections": [{"title": "1. Introduction", "content": "Object detection is an essential task in computer vision, with applications that span various domains including autonomous driving, warehousing, and medical image analysis. Existing object detection methods predominantly utilize Convolutional Neural Networks (CNNs) [8, 9, 22, 23] to identify and locate objects within images. However, the recent introduction of DEtection TRansformer (DETR) [1] has revolutionized the field by offering an end-to-end prediction pipeline where the model predicts a set of bounding boxes and class probabilities.\nThe core innovation of DETR lies in the use of a Transformer encoder-decoder architecture, enabling the model to generate predictions in an end-to-end manner and enhancing scalability. This paradigm shift has led to the exploration of various DETR variants such as Deformable-DETR [32] and DINO [31]\u2014positioning them as potential foundation models for object detection tasks. Despite these advancements, the inner workings of how these predictions are generated and interact within the Transformer decoder layers remain under-explored.\nLikewise, there is an ongoing debate within the community about whether DETR truly provides an end-to-end prediction pipeline. The central concern is whether all the hundreds of predictions generated by DETR can be trusted and used. By means of an answer, practitioners often employ heuristic approaches to achieve high precision, such as by setting a user-defined threshold to retain only a small subset of high-confidence outputs, as seen in the official demo. Previous studies exploring model reliability, such as [10, 15\u201317, 21], have also applied a user-defined confidence threshold (e.g., 0.3) to retain a subset of predictions for evaluating calibration quality, rather than using the entire set. On the other hand, the published implementations of several DETR variants select the top-k outputs (e.g., 100 out of 300 for Deformable-DETR and 300 out of 900 for DINO) based on confidence scores during post-processing to achieve a high average precision (AP) score. However, the significance of selecting a subset of the predictions as well as the choice of a suitable configuration for doing so remain under-explored. We provide a more in-depth dis-"}, {"title": "2. Problem Statement", "content": "2.1. Detection Transformers (DETRS)\nWe consider a test image x and denote $\\mathcal{T}_u$ as the set of ground truth objects present in the image. Analogously, the set of predictions generated by DETR, parameterized by $\\theta$, is denoted by $\\mathcal{T}_\\theta(x)$. Each prediction $t \\in \\mathcal{T}_\\theta(x)$ is characterized by a bounding box b and an associated class label with a corresponding probability p.\nThe structure of DETR is composed of two main components: the Transformer encoder, which extracts a collection of features from the given image; and the Transformer decoder, which uses these features to make predictions. In addition to the features extracted by the encoder, the decoder's input consists of M (typically several hundred) learnable embeddings, also known as object queries. Each decoder"}, {"title": "2.2. Identifying Reliable Predictions in DETR", "content": "Notably, the number of predictions generated by DETR, $\\mathcal{T}_\\theta(x)$, is fixed and often in the hundreds, far exceeding the number of ground truth objects. To address the issue, during model training, a bipartite matching algorithm is used to find the optimal matching prediction for each ground truth object based on the alignment of the class label and bounding box (as detailed in Appendix B). Consequently, the parameters $\\theta$ are optimized to enhance the accuracy of these matched queries. In this paper, we refer to the matched queries as optimal positive queries, while the remaining queries are termed optimal negative queries.\nAt the inference stage in real-world scenarios, however, ground truth annotations are unavailable, meaning the optimal positive queries remain unknown. This raises the question of whether all predictions can be trusted and used without any post-processing, or if only a subset should be selected for the final inference\u2014and, if so, which subset? If all of the model's predictions, including negative queries, were generated independently and were nearly identical to one of the positive predictions, the large number of predictions would not be a concern. We could simply apply well-known algorithms like NMS to remove duplicates and resolve redundancy. Notably, we have observed that DETR assigns well-calibrated confidence scores to only a single positive query (i.e., prediction) per object, while assigning low scores but similar bounding boxes to negative queries (see Section 3.1 and Figure 2). As a result, the overall prediction-level calibration error becomes sig-"}, {"title": "3. Analysis and Key Insights", "content": "3.1. Exploring the Anatomy of DETR's Predictions\nWe begin by closely examining the underlying dynamics of DETR decoders and visualizing the generated outputs. Since the Transformer decoder outputs only representation vectors, investigating their evolution across layers is not straightforward. We address this by reapplying the final feedforward network that operates on the last layer, $f_\\theta$, to the intermediate layers. This allows us to transform each representation vector into its associated bounding box and class label. This is feasible due to the alignment of intermediate representations, facilitated by residual connections between decoder layers [2]. Sample visualizations are in Figure 2 along with Figures 5 and 6 in Appendix C.\nIn the first decoder layer, the model appears to explore the encoded image features, producing varied queries that result in various plausible predictions. In this early stage, the distinction between positive and negative queries can be ambiguous (e.g., Figure 2a). However, the self-attentions through the subsequent decoder layers progressively refine these predictions. By the final layer, the model selects a single query (i.e., optimal positive) and assigns a confidence score based on its understanding of the image and the object. In contrast, the confidence scores for neighboring queries (i.e., optimal negatives) do not increase to the same extent as the positives and even decrease in high-reliability images. In contrast, in low-reliability images (e.g., Figure 2b), the confidence score of the positive query does not significantly increase, while the scores of the negative ones are either slightly raised or unchanged. Based on this observation, we present our claim:\nClaim 1. Predictions from DETR within a given image exhibit varying levels of reliability. For each object in the image, the optimal positive (i.e., best-matched) prediction is calibrated, while the remaining optimal negative predictions are uncalibrated."}, {"title": "3.2. Analysis of Calibration Error for Positive and Negative Predictions", "content": "Primarily, to address the first research question (RQ1) and provide quantitative support for our claim regarding the varying levels of reliability across predictions, we evaluate"}, {"title": "4. A Systematic Framework for Identifying Positive Predictions", "content": "Identifying the positive predictions is crucial for the reliable use of DETR. Nonetheless, identifying optimal positive predictions during the inference stage is not feasible due to the lack of ground truth labels. Therefore, an alternative systematic framework is essential not only for improving its interpretability but also for ensuring reliability. Given that the optimal matching achieves the smallest calibration error, we use calibration error to measure the quality of a"}, {"title": "4.1. Limitations of Existing Metrics", "content": "To this end, this section analyzes the effectiveness of existing metrics, including AP, D-ECE, LA-ECE0, and LRP, for measuring calibration error. Specifically, we modify a variable (e.g., the confidence score threshold) to generate different subsets of DETR predictions. We then assess the performance of each metric for these subsets and determine the variable value that yields the highest performance. Lastly, we evaluate the extent of separation by comparing the predicted positive set to the optimal positive set. Sample analysis results are illustrated in Figure 3. For additional results and detailed information on these metrics, please refer to Appendix D.\nPrimarily, as noted in several studies [11, 18, 19] and our empirical findings, the optimal AP is achieved when the threshold is set to 0.0; AP does not penalize harshly for having low confidence predictions. However, as discussed earlier, using the entire prediction set carries a high risk of including uncalibrated negatives, leading to unreliable decisions in practical applications. Furthermore, using hundreds of predictions harms the interpretability of the model.\nIn contrast, the optimal ECEs are often achieved when the threshold is set close to 1.0, meaning ECEs favor retaining fewer predictions with high confidence. This is a structural pitfall that prediction-level ECEs commonly face [11]. Since ECEs do not penalize missed detections (i.e., false negatives), they can achieve near-zero error when the evaluation positive set consists solely of highly accurate and confident predictions. Therefore, unless the model is trained to be excessively overconfident\u2014which is unlikely given that DETR is trained on large datasets using various auxiliary loss functions\u2014ECEs can result in very small error values when using a large confidence threshold.\nLRP [18], a localization-aware performance metric, can be used instead. However, LRP is not designed as a calibration metric and does not explicitly consider calibration error. As a result, the ranking scored by LRP across the different models is not necessarily aligned with the model's"}, {"title": "4.2. Proposed Metric: Object-Level Calibration Error", "content": "Notation. For each image $x_i$ having $N_i$ objects, we consider the ground truth set $\\mathcal{T}_{x_i} = \\{o_{i,j} = (l_{i,j}, b_{i,j})\\}_{j=1}^{N_i}$, and a set of DETR predictions: $\\mathcal{T}_\\theta(x_i) = \\{t_{i,q} = (p_{i,q}, b_{i,q})\\}_{q=1}^{M}$ where M represents the number of object queries. Here, $l_{i,j} \\in [1, C]$ and $p_{i,q} \\in \\{0,1\\}^C$ represents the ground-truth label and predicted probability distribution over C classes, respectively, while $b_{i,j}$ and $b_{i,q} \\in \\{0,1\\}^4$ corresponds to the ground truth and predicted scaled bounding box, respectively.\nDefinition 1. Consider a subset of predictions, $\\mathcal{S}_\\theta(x_i) \\subseteq \\mathcal{T}_\\theta(x_i)$, that is generated by post-processing algorithm S from the entire prediction set: $\\mathcal{S} = S \\circ \\mathcal{T}_\\theta$. We define an object-level calibration error (OCE) as the average Brier score per object:\n$OCE(\\mathcal{S}; \\mathcal{I}) \\doteq \\frac{1}{|\\mathcal{I}|} \\sum_{(i,j) \\in \\mathcal{I}} Brier(\\mathcal{S}_\\theta(x_i); o_{i,j})$  (1)\n$Brier (\\mathcal{S}_\\theta(x_i); o_{i,j}) = \\sum_{c=1}^{C} (\\mathbb{1}(c = l_{i,j}) - \\overline{p_{i,j}}[c])^2$  (2)\n$\\overline{p_{i,j}}[c] = \\frac{1}{|Q_{i,j}|} \\sum_{q \\in Q_{i,j}} p_{i,q}[c]$  (3)\nwhere $\\mathcal{I} = \\bigcup_i \\{(i, j)\\}\\_{i=1}^{N}$ is a set of all objects indices and $(\\cdot)[c]$ outputs the probability of c-th class; $Q_{i,j}$ is a set of query indices that matches to the ground truth object $o_{i,j}$ and we propose two variants:\n$Q_{i,j} = \\{q | IoU(b_{i,j}, b_{i,q}) \\geq \\epsilon \\}$  (OCEENSE) (4)\n$Q_{i,j} = \\{q = \\underset{q}{argmin} IoU(b_{i,j}, b_{i,q})\\}$  (OCEMAX) (5)\nThe difference is that OCEENSE ensembles the overlapping predictions, while OCEMAX selects the prediction with the best bounding-box matching. When $Q_{i,j} = \\emptyset$, we consider the predicted probability to be zero, thus the corresponding Brier score is estimated as 1.0. Following [11, 14], we use IoU thresholds of $\\epsilon = 0.5, 0.75$ and report the average score as OCEENSE or simply OCE.\nThe introduced calibration error has two desirable characteristics. First, the prediction set achieves the lowest calibration error when the predictions are well-calibrated to the respective closest ground truth objects. Second, it penalizes the subset, $\\mathcal{S}_\\theta(\\cdot)$, that includes missing ground truth objects; this ensures that subsets containing a small set of highly precise predictions are not assigned an artificially low error, unlike D-ECE and LA-ECE metrics. Thus, this metric"}, {"title": "5. Use-Case Implications", "content": "5.1. Image-Level Reliability\nAs a main use-case implication of this research, this paper investigates how effective separation impacts the performance of quantifying image-level reliability within the DETR framework. Specifically, we first introduce a formal definition of image-level reliability by examining the model's overall object detection performance on the image.\nDefinition 2. We define image-level reliability as a measure of how accurately and confidently the predictions match the ground truth objects:\n$ImReli(x; \\theta) \\doteq Perf (\\mathcal{T}_\\theta(x), T_x)$.  (6)\nwhere one can quantify Perf using any standard performance metrics such as average precision, depending on the specification.\nBy its definition, image-level reliability directly addresses the model's applicability to a given test instance. However, since image-level reliability requires ground truth annotations for its determination, obtaining it during inference is not feasible. Therefore, our goal is to employ a proxy method that quantifies uncertainty or confidence, which should closely correlate with image-level reliability."}, {"title": "5.2. Proposed Framework: Quantifying Reliability by Contrasting", "content": "As illustrated, positive and negative queries exhibit varying levels of reliability. Interestingly, having queries with low confidence scores does not necessarily imply low reliability. Our empirical observations show that confidence scores in negative queries actually inversely correlate with image-level reliability. More specifically, for a reliable instance, we observe that the confidence of positive queries increases across the decoder layers, while that of negative queries remains low; this results in a large gap between positive and negative queries. In contrast, for unreliable instances (e.g., Figure 2a), the confidence score of the positive query does not increase across the layers, whereas the scores of negative queries are either slightly elevated or remain unchanged. Consequently, there is a small gap between positive and negative queries (e.g., Figure 2b).\nBased on the finding, we propose a post hoc UQ approach by contrasting the confidence scores of positives and negatives:\n$ContrastiveConf(x) = Conf^+(x) - \\lambda Conf^-(x)$  (7)\n$Conf^+(x) = \\frac{1}{|\\mathcal{T}_\\theta^+(x)|} \\sum_{t \\in \\mathcal{T}_\\theta^+(x)} c(t)$  (8)\n$Conf^-(x) = \\frac{1}{|\\mathcal{T}_\\theta^-(x)|} \\sum_{t \\in \\mathcal{T}_\\theta^-(x)} c(t)$  (9)\nwhere $\\mathcal{T}_\\theta^+(x)$ and $\\mathcal{T}_\\theta^-(x)$ are predicted sets of positive and negative predictions, respectively, and $\\lambda$ is a scaling factor, that can be selected from the validation set. We include an ablation study of the scaling factor in the supplementary material. Our results show that the proposed method is not too sensitive to the choice of scaling factor and consistently outperforms baselines, with a scaling factor of 10.0 providing adequate performance. c(.) denotes the confidence estimate for the prediction; in this paper, we use maximum probability, $c(t = (p, b)) = max_c p[c]$.\nAs shown so far, identifying a $\\mathcal{T}_\\theta^+(x)$ from $\\mathcal{T}_\\theta(x)$ is a crucial factor in the success of this approach. In practice, however, neither the ground truth (i.e., the queries that DETR intends to use) nor the optimal positives are available during the UQ stage. Instead, we approximate the ground truth separation by applying a post-processing algorithm that minimizes the calibration error on the validation set:\n$\\mathcal{T}_\\theta^+(x) = S^* \\circ \\mathcal{T}_\\theta(x)$  (10)\n$S^* = \\underset{S}{argmin} OCE(S \\circ \\mathcal{T}_\\theta; I_{val})$  (11)\nwhere $I_{val} = \\bigcup_{i \\in V} \\{(i, j)\\}\\_{i=1}^{N}$ is the set of all objects indices in the validation dataset V. Unless specified otherwise, we adopt confidence thresholding for the separation scheme (i.e., post-processing algorithm), where the corresponding threshold is the variable to optimize."}, {"title": "6. Experiments", "content": "To demonstrate the effectiveness of the proposed method, we conducted experiments using four DETR variants: UP-DETR [3], Deformable-DETR (D-DETR), Cal-DETR [17], and DINO. Each model is trained on the COCO (train2017) dataset and we use 1,000 images (i.e., 20%) of COCO (val2017) for the validation set and the remaining images for the test set. The model is tested on three datasets with varying levels of out-of-distribution (OOD) characteristics: COCO (in-distribution), Cityscapes (near OOD), and Foggy Cityscapes (OOD). Experimental details can be found in Appendix E."}, {"title": "6.1. Effectiveness of OCE", "content": "To showcase OCE's effectiveness in assessing calibration quality, we analyze how different metrics rank calibration"}, {"title": "6.2. Quantifying Image-Level Reliability", "content": "To address the third research question (RQ3), we compare different methods for quantifying per-image reliability using different separation techniques. We measure the Pearson correlation coefficient (PCC) of each method with the ImReli computed based on AP metrics. Additionally, we evaluate the proposed contrasting approach against methods that utilize only positive or negative queries. Empirical results are presented in Table 3 and further in Appendix F. The key takeaways are as follows:\nFirst, as demonstrated throughout our analysis, distin-"}, {"title": "6.3. Comparative Study on Post-Processing", "content": "Lastly, we present a comparative analysis across different post-processing schemes for identifying positive queries in DETR, from the reliability perspective. We assess OCE for each predicted positive to evaluate the impact of various separation schemes on calibration performance.\nFor the separation methods, we compare the following"}, {"title": "7. Conclusion and Limitations", "content": "The main contribution of our work is an in-depth analysis of the importance of post-processing in DETR frameworks. Furthermore, this paper demonstrates the impact of the post-separation method on the reliability of their predictions, and proposes a novel UQ method for quantifying image-level reliability in DETR.\nThe primary limitation of our work is that our analysis is"}, {"title": "A. Related Work", "content": "Uncertainty Quantification. Shelmanov et al. [27] and Vazhentsev et al. [29] focus on uncertainty quantification in general Transformer architectures, building upon the MC dropout approach proposed by Gal and Ghahramani [7]. Specifically, they apply a modified MC dropout that prioritizes sample diversity over randomness to improve sample efficiency. However, these methods are not directly applicable to object detection problems because, unlike classification tasks, object detection involves predicting an unordered set. Consequently, it is questionable whether applying MC dropout to each output is appropriate, as the permutation of predictions could be completely shuffled after applying MC dropout.\nSeveral studies focus on out-of-distribution (OOD) identification in object detection models. For example, Li et al. [13] propose a built-in OOD detector to isolate OOD data for human review. This approach can determine OOD scenarios involving both unknown and uncertain classes (i.e., epistemic but not aleatoric uncertainty) by modeling the distribution of training data and assessing whether samples belong to any of the training class distributions. Du et al. [5] generate outlier data from class-conditional distribution estimations derived from in-distribution data, training the model to assign high OOD scores to this generated data and low OOD scores to the original in-distribution data. Similarly, Oksuz et al. [19] employ an auxiliary detection model capable of expressing its confidence. Other works, including Du et al. [4] and Wilson et al. [30], investigate the latent representations generated by object detection models to identify the OOD nature of the input.\nTo the best of our knowledge, the aforementioned existing uncertainty quantification techniques primarily focus on prediction-level analysis. Moreover, they predominantly address CNN-based models and explore the methodological way to better quantify the uncertainty in object detection model. In contrast, our paper emphasizes the significance of identifying reliable sets within the entire set of predictions for uncertainty quantification, particularly in DETRs. Another novelty of our work lies in investigating an appropriate methodology to integrate different predictions' confidence estimates to quantify image-level reliability.\nCalibration. Previous studies primarily addressed OOD detection to assess the prediction-level trustworthiness of DETR. They aim to introduce a framework that does not rely on the model's confidence scores, which can be either overconfident or underconfident, especially for OOD data points. Simultaneously, additional research endeavors to determine the model's overall reliability in its confidence scores. More specifically, these studies assess whether the model's confidence scores align with their actual accuracy."}, {"title": "E. Experiment Setup", "content": "For our experiments, we used the Cityscapes and Foggy Cityscapes datasets, which each have 500 images of first-person driving footage in realistic environments. Foggy Cityscapes has the same base images as Cityscapes, but with fog simulated and added to create a further out of distribution set. Since the DETR models were trained on COCO, the Cityscapes and Foggy Cityscapes annotations were converted to correspond to the labels of COCO. More specifically, the person, bicycle, car, motorcycle, bus, train, and truck classes were transferred directly. In addition, the rider class of Cityscapes and Foggy Cityscapes was mapped to the person class of COCO. The other classes present in Cityscapes and Foggy Cityscapes are largely focused on image segmentation, and thus were omitted (e.g. building, sky, sidewalk). The pre-trained model weights were obtained from their respective official implementations.\nTo compute ImReli for each image, we use the same COCO evaluator [14] to obtain image-wise AP scores. However, rather than passing in the entire image set, the predictions for each image are passed in individually."}, {"title": "F. Further Experimental Results", "content": "F.1. Effectiveness of OCE\nTable 5 presents the correlations between the rankings provided by three different calibration metrics on the optimal positive set and various methods. The first three rows demonstrate that the calibration metrics\u2014D-ECE, LA-ECE, and our proposed OCE\u2014are highly correlated. This indicates that all three metrics effectively capture the notion of calibration quality. Additionally, these metrics show a decent correlation with AP metrics. It is important to note that AP primarily accounts for accuracy rather than calibration quality, which may explain why the correlation, particularly for AP@50 on the out-of-distribution dataset, is not perfectly aligned.\nFurthermore, LRP, D-ECE, and LA-ECE, exhibit negative correlations with calibration quality when used alone. This supports our assertion that these metrics are inadequate for measuring models' calibration quality in conjunction with the employed post-processing scheme, unlike our OCE.\nFinally, D-ECE exhibits a high correlation when paired with either a fixed threshold or a proper adaptive threshold (i.e., using LRP and our OCE). This is because DETR's optimal positive threshold is empirically found to be around 0.3 (See Table 7.). Consequently, OCE aligns well with the experimentally approximated threshold and the LRP metric in identifying effective post-processing schemes. This alignment further supports the robustness and effectiveness of our method."}, {"title": "F.2. Quantifying Image-Level Reliability", "content": "This section provides an extended version of Table 3 with more methods such as the proposed method but with a fixed threshold. Furthermore, we show more results with different Perf metrics by evaluating it with AP50 and AP75 (average precision with Iou threshold 50% and 75%). The results are shown in Table 6.\nAs illustrated, the observation is consistent across different settings. Another notable observation is the comparison between the fixed threshold versus our adaptive threshold selection using OCE. Although the proposed fixed-threshold approach (0.3) occasionally outperforms the adaptive method, the performance gap remains marginal. Moreover, the fixed threshold consistently exhibits a lower correlation with UP-DETR, for which the optimal threshold has been empirically determined to be approximately 0.5 (0.45 by OCE and 0.55 by LRP). This indicates that although the OCE approach may not achieve threshold selection that best approximates the optimal positives, it ensures the robustness of uncertainty quantification performance. In contrast, fixed thresholding can degrade performance when the chosen value deviates from the optimal threshold.\nFinally, we perform an ablation study on the scaling factor ($\\lambda$), with the results presented in Figure 8. The study shows that the optimal scaling factor lies within the range of 5.0 to 10.0. Notably, this range remains effective even for out-of-distribution datasets, such as Cityscapes and Foggy Cityscapes. This consistency suggests that the scaling factor selected using the validation set (e.g., COCO) generalizes well across datasets. Furthermore, it demonstrates that ContrastiveConf with $\\lambda > 0.0$ consistently outperforms $Conf^+$ (i.e., $\\lambda = 0.0$)."}, {"title": "G. Comparative Study on Post-Processing", "content": "In this section, we provide a comprehensive analysis regarding the impact of post-processing on the model's overall calibration quality. First, we vary the hyperparameter of the standard post-processing processes. In this first analysis, to exclude the dependency of the final performance on OCE, we don't use OCE as a selection criterion. Rather, we choose the best hyperparameter on the validation set for each setting and compare their best-possible performances.\nThe results are shown in Table 7.\nAs illustrated, we can observe that the confidence thresholding approach outperforms the top-k and NMS approaches. Top-k achieves the minimum OCE with top-20. Given that the average and 95th percentile number of objects per image in the COCO dataset are 7 and 22, respectively, these results appear reasonable. Therefore, we confirm that using an excessively large number (e.g., 100) for top-k is inadequate for achieving well-calibrated predictions."}]}