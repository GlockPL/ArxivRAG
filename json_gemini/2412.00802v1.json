{"title": "HT-HEDL: High-Throughput Hypothesis Evaluation in Description Logic", "authors": ["Eyad Algahtani"], "abstract": "We present High-Throughput Hypothesis Evaluation in Description Logic (HT-HEDL). HT-HEDL is a high-performance hypothesis evaluation engine that accelerates hypothesis evaluation computations for inductive logic programming (ILP) learners using description logic (DL) for their knowledge representation; in particular, HT-HEDL targets accelerating computations for the ALCQI(D) DL language. HT-HEDL aggregates the computing power of multi-core CPUs with multi-GPUs to improve hypothesis computations at two levels: 1) the evaluation of a single hypothesis and 2) the evaluation of multiple hypotheses (i.e., batch of hypotheses). In the first level, HT-HEDL uses a single GPU or a vectorized multi-threaded CPU to evaluate a single hypothesis. In vectorized multi-threaded CPU evaluation, classical (scalar) CPU multi-threading is combined with CPU's extended vector instructions set to extract more CPU-based performance. The experimental results revealed that HT-HEDL increased performance using CPU-based evaluation (on a single hypothesis): from 20.4 folds using classical multi-threading to ~ 85 folds using vectorized multi-threading. In the GPU-based evaluation, HT-HEDL achieved speedups of up to ~ 38 folds for single hypothesis evaluation using a single GPU. To accelerate the evaluation of multiple hypotheses, HT-HEDL combines, in parallel, GPUs with multi-core CPUs to increase evaluation throughput (number of evaluated hypotheses per second). The experimental results revealed that HT-HEDL increased evaluation throughput by up to 29.3 folds using two GPUs and up to ~ 44 folds using two GPUs combined with a CPU's vectorized multi-threaded evaluation.", "sections": [{"title": "I. INTRODUCTION", "content": "Inductive logic programming (ILP) is a form of machine learning where background knowledge and learning examples are described using logic-based formalisms. A key advantage of ILP is its ability to learn expressive white-box models from multi-relational data for description and prediction purposes. ILP has been used in various fields, including aerospace [9] and biochemistry [8] to describe complex concepts and relations. ILP is classically used with first-order logic (FOL), especially Horn clauses. However, in recent years, there has been a growth in the use of Description Logic for knowledge representation in ILP applications.\nDescription Logic (DL) is a family of logic-based formalisms for representing knowledge. The expressive power of DL is higher than that of propositional logic and lower than that of FOL. In terms of DL applications, DL is used in medicine (to aid medical diagnosis) and in digital libraries (for classification and data retrieval tasks) [3]. However, one of its major applications is in the semantic web, especially the OWL language. OWL (Web Ontology Language) [17] is a modeling language that semantically represents knowledge as ontologies. There are three OWL variations: OWL Lite, OWL DL, and OWL Full. Each OWL type has its expressive power, i.e., the degree to which it can describe complex concepts; OWL Lite is the least expressive among the three types, and OWL Full is the most expressive type. A major disadvantage in OWL Full (despite its higher expressive power) is its undecidability, and therefore, reasoning tasks are not supported. In practice, OWL DL is commonly used where DL is the underlying logic-based representation, that supports reasoning tasks through DL-based reasoners such as HermiT [20], FaCT++ [22], and Pellet [21]. Despite the OWL variations, OWL relies on Resource Description Framework (RDF) [18] and RDFS (RDF Schema), which uses a graph data model to describe OWL's knowledge graph.\nDL is used in ILP because it provides a trade-off between hypothesis expressivity and the needed computing power, which is sufficient for some ILP applications, such as ILP-based recommender systems [34]. Regardless of DL-based and FOL-based ILPs, ILP as a machine learning (ML) technique has an inherent scalability limitation, which is learning from large amounts of data. The scalability limitations in DL-based ILPs, as opposed to FOL-based ILPs, are less pronounced because DL has less expressive power than FOL, which translates to less computational complexity. Regardless, ILP's poor scalability limits its potential applications. Therefore, in this work, we aim to increase evaluation performance for DL-based ILP learners by exploiting the computing power of several parallel heterogeneous processors (i.e., CPUs and GPUs). We aggregate the computing power of these heterogeneous processors to increase evaluation performance at the level of single and multiple hypotheses (i.e., hypothesis evaluation throughput). Increasing the evaluation performance for DL-based ILP learners at these two levels is expected to drastically reduce ILP learning times.\nThe article is structured as follows. In Sec. II, we review the parallel and non-parallel approaches to improve ILP learning in general and DL-based ILP learning in particular; however, the focus is on hypothesis evaluation aspects (the scope of this work). In Sec. III, we describe our proposed multi-device algorithm for high-throughput hypothesis evaluation in description logic (HT-HEDL). The next sections detail the implementation of HT-HEDL and the experimental results."}, {"title": "II. RELATED WORK", "content": "Two approaches exist for accelerating hypothesis evaluation, parallel and non-parallel approaches. In non-parallel approaches, the aim is to increase evaluation efficiency such as Query Packs [4] (to reduce redundant evaluation computations), and sampling techniques [35] (for evaluating hypotheses against a smaller but carefully selected subset from learning data). Moreover, some other non-parallel approaches are used to improve ILP performance by reducing the evaluation search space.\nIn some cases, hypothesis evaluation is a reasoning problem (e.g., in classical ILPs), where reasoning paths are explored for every generated candidate hypothesis; reducing reasoning search space will reflect in improved evaluation performance. Notably, approaches for reducing reasoning search space exist. For example, domain knowledge about the learning problem can be used to constrain candidate hypothesis construction (e.g., using mode declarations in Progol [24] and Aleph [36]), which prunes away areas of the search space that contain invalid hypotheses; other related approaches include proactive knowledge engineering to simplify knowledge representation (through proper choice of predicates) to reduce reasoning complexity. Other non-parallel approaches improve evaluation performance using less expressive logic, such as propositionalization [33], which reduces the evaluation time and cost of less expressive hypotheses.\nIn parallel approaches, the computing power of several parallel processors are aggregated to accelerate hypothesis evaluation. Because logic programming is the cornerstone for ILP, many parallel approaches have been developed to improve the performance of logic programming languages such as Prolog, Datalog, and Answer Set Programming through CPU-based, GPU-based, and Big Data-based approaches [40]. We classify parallel ILP approaches into shared-memory and distributed-memory environments. In a shared-memory environment, researchers in [16], [23] proposed parallel reasoning for description logic using multi-core CPUs, while other researchers used GPUs for parallel reasoning in DL [30]. Moreover, other researchers have developed approaches for accelerating querying from RDF data stores using GPU-based approaches [6], [29], [31], [32].\nOther parallel approaches outsourced hypothesis evaluation to relational database management systems (RDBMSs) to accelerate evaluation for classical ILPs, where the generated hypotheses are represented using SQL queries [49]. For parallel approaches in distributed-memory environments, several approaches have been developed [13]. Researchers in [46] used the MapReduce framework [7] to accelerate evaluation by exploiting MapReduce's distributed computing capabilities. In addition, other researchers [14] developed a distributed evaluation approach for Aleph (P-Progol) by dividing data among processors using the MPI framework [28]. To accelerate reasoning on OWL ontologies, researchers in [37] proposed an approach for handling reasoning tasks on large OWL Lite knowledge bases, which involves partitioning the large OWL knowledge base into smaller partitions; where OWL reasoning is performed on each partition in parallel. To accelerate other OWL variations, researchers in [38] proposed a hybrid approach that combines parallel and sequential computing to improve reasoning performance on large OWL-RL knowledge bases. To accelerate OWL DL, researchers in [39] proposed a parallel framework for accelerating the classification task for OWL DL ontologies; They evaluated their proposed framework using existing OWL DL reasoners. In addition to the aforementioned software-based approaches, dedicated hardware (FPGA-based) accelerators were developed to accelerate ILP computations [48], [50]. [48] is a dedicated hardware accelerator for embedded systems that uses HT-HEDL's algorithms and knowledge base representation; the HT-HEDL based hardware accelerator achieved a speedup of up to 48.7 fold on a disjunction operation, where the baseline is the sequential performance of Raspberry Pi 4 [47].\nThe literature review highlights the following. First, most parallel approaches focus only on targeting the shared-memory environment. Second, parallel CPU-based approaches are limited to scalar processing only, which overlooks potential performance gains that can be achieved by combining the vector instructions of CPUs (especially, multi-core CPUs) with multi-threading; such combination (vectorized multi-threading) will maximize the computing power extracted from CPUs. In the third observation, the use of GPUs for accelerating evaluation is less common (as opposed to CPU approaches); we also observe that using multiple GPUs for evaluation is non-existent. All reviewed approaches (both parallel and non-parallel) focus on improving evaluation at the level of a single hypothesis only, and not directly improving evaluation in terms of evaluation throughput (i.e., increasing the number of evaluated hypotheses per second). In the next section, we describe HT-HEDL, our proposed evaluation engine. HT-HEDL aims to accelerate hypothesis evaluation at the level of a single hypothesis and evaluation throughput using a heterogeneous combination of multi-GPUs with multi-core CPUs (combined with vector instructions)."}, {"title": "III. HIGH-PERFORMANCE HYPOTHESIS EVALUATION", "content": "In this section, we describe HT-HEDL architecture. HT-HEDL is directly based on and builds upon the work in Chapter 5 (the hypothesis evaluation component) of our previous work, SPILDL [2] (a scalable and parallel inductive learner in description logic). Notably, some aspects of SPILDL were used in our previous works [11], [12]. HT-HEDL accelerates hypothesis evaluation for DL-based ILP learners and can evaluate hypotheses up to the ALCQI(D) DL language. HT-HEDL accelerates evaluation for a single hypothesis using a single GPU or a single multi-core CPU. HT-HEDL also accelerates evaluation for multiple hypotheses using a combination of heterogeneous processors (multiple GPUs with a multi-core CPU), where a single GPU or a multi-core CPU evaluates a subset of these hypotheses in parallel (with other CPUs and GPUs); this amplifies the performance gains of a single GPU or a CPU (i.e., increased evaluation throughput), which can potentially reduce evaluation times for DL-based ILP learners by an order of magnitude. Next, we describe HT-HEDL's knowledge representation."}, {"title": "A. Knowledge representation", "content": "The original matrix-based representation in Chapter 5 of SPILDL, is designed to facilitate GPU-based computations in general, but optimizing memory access patterns for the original matrix-based representation was not the primary goal. For example, in the original matrix-based representation, the Concepts matrix, which also includes the results matrix, stores all concept memberships of a single individual in continuous memory addresses; in contrast, the memberships of a single concept for all individuals are loaded and processed in batches in the conjunction and disjunction operations. In other words, the original layout of the Concepts matrix (including the results matrix), exactly supports the opposite memory access patterns needed for conjunction and disjunction operations which reduces the computational efficiency of GPUs, even though GPUs still provide good performance speedups. Typical GPUs have sophisticated gather and scatter instructions, which enables them to perform SIMD-based computations on data items that are not stored in continuous memory addresses, as long as a matrix-based representation is used. GPUs are designed and optimized for matrix-based operations. However, poor memory access patterns, even on matrix-based representations, prevent GPUs from reaching their maximum computational performance. Moreover, CPU-based performance in SPILDL is limited to only multithreading on multi-core CPUs because the extended SIMD instruction set available in many modern CPUs requires strict constraints on data alignment to use those SIMD instructions to improve CPU performance. In other words, SIMD capabilities in many modern CPUs are not as sophisticated as SIMD capabilities in GPUs.\nHT-HEDL addresses the limitations of SPILDL's original matrix-based representation, by re-engineering the original matrix-based representation to support vectorized (SIMD) CPU-based evaluation and more expressive DL operators by proposing additional DL processing algorithms. Similar to SPILDL's original matrix-based representation, HT-HEDL implements the closed world assumption (CWA) and unique world assumption (UWA) for knowledge representation. The layout of HT-HEDL's matrix-based representation is specifically designed to have efficient DDR memory access patterns in general and to be SIMD-friendly in particular. As a result, due to the improved memory access patterns introduced by HT-HEDL, both CPU-based and GPU-based performance will be higher than that of the original matrix-based representation. HT-HEDL uses a matrix-based representation because matrix-based representations force spatial locality on data items where needed data items are located in adjacent (typically continuous) memory addresses; spatial locality enables the usage of fewer memory operations to load or read the same number of data items, which improves performance due to better memory access patterns."}, {"title": "B. DL operators", "content": "HT-HEDL supports the same GPU-based baseline operators as SPILDL, such as conjunction, disjunction, existential restriction, and universal restriction, but with some optimizations on restriction operations to improve performance. HT-HEDL builds upon SPILDL's operators to support string (textual) concrete roles through additional DL operators, which are EQUAL and CONTAIN string operators. HT-HEDL can evaluate the supported operators using both GPU- and CPU-based algorithms.\nHT-HEDL's CPU-based algorithms rely on the explicit vectorization of compute operations to achieve higher CPU-based performance. Although modern compilers typically have an auto-vectorization feature, which automatically compiles a scalar code into a program that uses vector instructions to improve performance, the scalar code must meet certain compiler constraints, and these constraints may differ between different compilers. Relying on a compiler feature to utilize vector instructions to improve performance in HT-HEDL is not feasible because of the following reasons. First, according to the researchers in [41], implicit (auto) vectorization results in different performances with different compilers, whereas explicit vectorization results in almost the same performance across different compilers. Second, the researchers in [42] found that explicit vectorization achieves higher performance than implicit vectorization. Third, the data representation layout and its related memory access patterns also affect auto-vectorization capabilities [43]; the researchers (in [43]) also found that auto-vectorization is affected by some compiler optimizations, such as instruction combining optimizations. Therefore, HT-HEDL's CPU-based algorithms use explicit vectorization to ensure consistent and high vector-based CPU performance, regardless of the used compiler. Next, we describe the pseudocodes for HT-HEDL's CPU- and GPU-based algorithms for performing DL operations. We used Nvidia's CUDA semantics to describe the pseudocodes for HT-HEDL's GPU-based DL operations. In every GPU-based pseudocode, blockIdx.x refers to GPU block index, blockDim.x refers to GPU block size (which is the number of GPU threads per a single GPU block), and threadIdx.x refers to GPU thread local ID (within its GPU block). The GPU processes each index (i) in a separate GPU thread.\n1) Conjunction and disjunction operations: HT-HEDL computes conjunction and disjunction using similar (but not identical) pseudocodes. For computing the two operations using a CPU-based and a GPU-based approach, see Algorithm 1 and Algorithm 2, respectively."}, {"title": "IV. FULL HYPOTHESIS EVALUATION", "content": "A DL hypothesis is a series of (potentially) nested DL operations evaluated in a particular order. In HT-HEDL, a hypothesis is represented in a single parent data structure that contains the hypothesis's DL operations stored in consecutive memory addresses (e.g., an array of DL operations). In HT-HEDL, the smallest computational unit is a DL operation, that is, a single DL operation will not be divided into smaller computational units. For example, when computing a conjunction (or a disjunction), all concepts are computed in a single step and will not be divided into a series of smaller conjunctions (or disjunctions) of two concepts at a time.\nDL hypotheses are inherently tree structures, but storing the hypothesis (including its nodes and DL operations) in consecutive and adjacent memory addresses will improve memory access patterns and thus improve performance because the same hypothesis data can be retrieved using fewer DDR read transactions; this is especially important when processing multiple hypotheses in parallel. See Fig. 2 for DL hypothesis representation in HT-HEDL. Because a DL hypothesis in HT-HEDL requires fewer DDR memory transactions, memory-bound performance problems are reduced, which makes the CPUs and GPUs perform more computations instead of waiting for data to be retrieved, as opposed to the traditional (less memory efficient) tree-based representations."}, {"title": "V. MULTI-DEVICE HYPOTHESIS EVALUATION", "content": "In previous sections, we described HT-HEDL's approaches to accelerate the evaluation of a single hypothesis (using a GPU or a multi-core CPU). By combining multiple computing devices (GPUs with CPUs), multiple hypotheses can be evaluated in parallel, which increases the evaluation throughput, thus reducing ILP learning times even more. When evaluating multiple hypotheses in HT-HEDL, HT-HEDL schedules these hypotheses among available evaluation devices (GPUs and CPUs) using a static scheduling strategy, guided by information about each device's evaluation capabilities. To determine the evaluation capabilities for each device, HT-HEDL evaluates a dummy hypothesis (typically a conjunction of concepts) on each device in the system and then measures the device's execution time. The dummy hypothesis is evaluated against the same knowledge base, being used for ILP learning. For example, a knowledge base that has 10 million individuals will result in the dummy hypothesis being evaluated against the same 10 million individuals using every evaluation device in the system. After evaluating the dummy hypothesis on every device against the same knowledge base, HT-HEDL will have an approximation of each device's capability, which is then used to assign the appropriate workload for that device. Fig. 3 presents an example of scheduling evaluation among four GPUs and a single multi-core CPU, to demonstrate HT-HEDL's multi-device scheduling strategy.\nIn Fig. 3, during the startup or initialization of HT-HEDL, HT-HEDL first query the operating system for available devices, which results in detecting 4 GPUs and a single multi-core CPU. Second, HT-HEDL allocates and populates knowledge representation matrixes on each detected device, that is, each device will have an exact copy of knowledge representation matrixes. Third, HT-HEDL evaluates the probing hypothesis made of a single conjunction of 5 concepts on each detected device and then measure the device's evaluation time. Executing the same probing hypothesis on every device, will provide information regarding the relative computing capabilities between the detected devices. Once evaluation times are measured, HT-HEDL then computes the scheduling ratio for each device using steps 4-6. These scheduling ratios are computed only once during HT-HEDL's initialization, and then reused to assign a percentage of hypotheses to each device. Even though the total number of hypotheses may vary, yet each device will always evaluate the same ratios of hypotheses. Since HT-HEDL assign workloads to devices based on their evaluation capabilities using relative measures (e.g. ratios), this enables HT-HEDL to maximize multi-device evaluation performance across variable number of hypotheses. HT-HEDL's scheduling algorithm is based on SPILDL's relative load scheduling algorithm; in SPILDL, scheduling ratios and their device assignments are manually provided parameters by the user. Unlike SPILDL, HT-HEDL automatically determines and assigns the scheduling ratios for each multi-core CPU and GPU, based on their computing capabilities \u2013 this is the key novelty in HT-HEDL's scheduling algorithm. Also, SPILDL's scheduling algorithm for hypothesis evaluation only considers GPUs, while HT-HEDL's scheduling algorithm considers both GPUs and multi-core CPUs. Given the nature of HT-HEDL's scheduling algorithm, we can deduce that HT-HEDL uses a form of static scheduling, because HT-HEDL schedules a predetermined (fixed) amount of workloads for each evaluation device. Since HT-HEDL uses static scheduling, it will have minimum (possibly negligible) scheduling overheads in comparison with dynamic scheduling overheads. See Fig 4 for a demonstration of HT-HEDL's scheduling algorithm using the same hardware (in Fig. 3)."}, {"title": "VI. IMPLEMENTATION AND EVALUATION", "content": "We implemented HT-HEDL in C/C++ and used OpenMP API [27] for multithreading. Regarding vector instructions, we used Streaming SIMD Extensions (SSE), a well-known set of vector instructions available in many modern CPUs with x86 and x86-64 (AMD64) architectures; the vector length in SSE is 128-bit long which can process up to 16 (8-bit) concept memberships for 16 individuals simultaneously.\nThere are other implementations for vector instructions (in x86 and x86-64 architectures), such as AVX and AVX-512. However, we chose SSE to ensure compatibility with even older CPUs. Regarding GPGPU, we used Nvidia's CUDA API because it is the most common API in the GPGPU area and also because Nvidia GPUs are used in the experiments. Regarding evaluation, we used synthetic datasets to evaluate HT-HEDL in best- and worst-case scenarios. The evaluation was conducted using a bottom-up approach, starting from individual DL operators, full hypothesis evaluation, and then multi-device hypothesis evaluation."}, {"title": "VII. DISCUSSION", "content": "According to the experimental results, both CPU- and GPU-based approaches accelerated hypothesis evaluation with varying degrees of speedups. In the conjunction operation, the evaluation of five concepts against one million individuals was accelerated 20.3-fold using scalar multi-threading, 83.4-fold using vectorized multi-threading and 37.7-fold using a GPU. The results for the conjunction operation are summarized in Fig. 5; in the disjunction operation, similar speedups were achieved. Regarding role restrictions, the speedups varied between the 'Single subject' and 'Unique subject' datasets; similar to conjunction and disjunction operators, existential and universal restrictions had similar speedups. In the 'Single subject', the GPU achieved lower speedups than in the 'Unique subject' because parallel memory operations were being serialized, which limited the parallel performance of both the CPU and the GPU. In the 'Unique subject' dataset, parallel memory operations were not serialized which enabled more speedups to be achieved. The experimental results for the existential restriction are summarized in Fig. 6.\nRegarding cardinality restrictions, the most number of speedups for 'Single subject' were achieved using a CPU, whereas in 'Unique subject', the GPU achieved the most number of speedups; the sequential performance of GPUs is poorer than that of CPUs. Therefore, serialized memory operations exert a weaker impact on CPU-based performance (as seen in the results for 'Single subject'). Experimental results for cardinality restrictions are summarized in Fig. 8.\nRegarding concrete role restrictions, the most number of speedups for numeric restrictions were achieved in the 'Unique subject' using both a CPU and a GPU. Similar to role restrictions, most GPU speedups were in the 'Unique subject' dataset. The numeric restriction results are summarized in Fig. 7. For string restrictions, both the CPU and the GPU achieved large speedups across the two string restrictions, especially in the 'Unique subject' dataset, whereas the largest speedups were achieved in the EQUAL restriction on the 'Unique subject' dataset. However, the worst-case scenarios for the two string restrictions were observed in the 'Single subject' dataset results on both CPU and GPU results. Fig. 9 summarizes the string restriction results. The experimental results revealed the CONTAIN restriction is much more suitable for CPU-based evaluation (as opposed to GPU-based evaluation).\nFig. 10 depicts the hypothesis evaluation performance of each GPU and CPU, as well as the combination of GPUs only and CPU+GPUs across different numbers of hypotheses. Noticeably, HT-HEDL's scheduling algorithm performed well when the number of hypotheses was < 1000. For a higher number of hypotheses such as 10,000, HT-HEDL's scheduling algorithm performed poorly, as the combination of evaluation devices did not result in higher aggregated performance; however, the aggregated performance was still a major improvement over baseline, which was ~ 17.5 for GPU1+GPU2, and ~ 26.5 for GPU1+GPU2+CPU. A potential solution to the limitation of HT-HEDL's scheduling algorithm on a large number of hypotheses may be to evaluate the large number of hypotheses (e.g., 10,000 hypotheses) in individual batches of 1000 hypotheses at a time. We chose an individual hypothesis batch of 1000 hypotheses because HT-HEDL's scheduling algorithm stopped providing aggregated performance above 1000 hypotheses. HT-HEDL's scheduling algorithm is not designed to outperform existing scheduling algorithms but rather to aggregate the computing performance of CPUs and GPUs toward improving hypothesis evaluation. Improving and addressing the limitations of HT-HEDL's scheduling algorithm remains a potential direction for future work."}, {"title": "VIII. CONCLUSION AND FUTURE WORK", "content": "In this work, we focused on accelerating hypothesis evaluation for DL-based ILP learners. We described our hypothesis evaluation engine HT-HEDL, which aggregates the computing power of heterogeneous processors (GPUs with CPUs), to accelerate evaluation performance by orders of magnitude at multiple levels: at the level of individual DL operators, single hypothesis evaluation, and multiple hypotheses evaluation. In individual DL operators, we described CPU-based approaches that combine multi-threading with vector instructions to accelerate computations of DL operators with large speedups. We also described our GPU-based approaches to accelerate computations of DL operators, with which large speedups were achieved. At the level of a single hypothesis evaluation, we accelerated the computation of covered (positive and negative) examples using multi-threading for CPU-based approaches. For GPU-based approaches, we used warp-based parallel reduction for computing covered examples. Warp-based parallel reduction is regarded as the state of the art for computing reduction operations on GPU hardware. At the level of multiple hypotheses evaluation, we used multi-threading to generate evaluation plans (query plans) for multiple hypotheses in parallel. In addition, we described our multi-device hypothesis evaluation approach, in which HT-HEDL increased hypothesis evaluation throughput by distributing hypotheses to available computing devices (GPUs and a multi-core CPU); all these devices evaluated their assigned hypotheses in parallel. HT-HEDL employs a static scheduling strategy guided by information about the hypothesis evaluation capabilities of each computing device in the system. It estimates the evaluation capabilities of computing devices by evaluating a dummy hypothesis on each computing device in the system against the same knowledge base being used for ILP learning. The execution times for evaluating the probing hypothesis on each computing device, (roughly) reflect the evaluation performance for that given computing device. According to experimental results, HT-HEDL accelerated hypothesis evaluation across the three aforementioned evaluation levels. In fact, HT-HEDL increased hypothesis evaluation throughput up to 44-fold (on a batch of 100 hypotheses) by aggregating the computing power of two GPUs and a single multi-core CPU. In terms of future research directions, HT-HEDL's work distribution aspects for GPUs can be improved such that each GPU generates its evaluation plans for its assigned hypotheses with minimal to no CPU intervention. This will free the CPU to increase its speedup potential when combined with GPUs (as a single device evaluator). Moreover, as a potential future direction, HT-HEDL can be extended to incorporate FPGA-based accelerators in addition to GPUs and CPUs. In terms of DL, HT-HEDL's hypothesis language can be extended to include more expressive DL constructs (operators)."}]}