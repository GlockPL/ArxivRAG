{"title": "Lost in Translation: Latent Concept Misalignment in Text-to-Image Diffusion Models", "authors": ["Juntu Zhao", "Junyu Deng", "Yixin Ye", "Chongxuan Li", "Zhijie Deng", "Dequan Wang"], "abstract": "Advancements in text-to-image diffusion models have broadened extensive downstream practical applications, but such models often encounter misalignment issues between text and image. Taking the generation of a combination of two disentangled concepts as an example, say given the prompt \"a tea cup of iced coke\", existing models usually generate a glass cup of iced coke because the iced coke usually co-occurs with the glass cup instead of the tea one during model training. The root of such misalignment is attributed to the confusion in the latent semantic space of text-to-image diffusion models, and hence we refer to the \"a tea cup of iced coke\" phenomenon as Latent Concept Misalignment (LC-Mis). We leverage large language models (LLMs) to thoroughly investigate the scope of LC-Mis, and develop an automated pipeline for aligning the latent semantics of diffusion models to text prompts. Empirical assessments confirm the effectiveness of our approach, substantially reducing LC-Mis errors and enhancing the robustness and versatility of text-to-image diffusion models. Our code and dataset have been available online for reference.", "sections": [{"title": "1 Introduction", "content": "Text-to-image synthesis [7,15,18,21, 25-28,30,33,36,38-40] via diffusion models has made remarkable progress, where high-quality images are generated given text prompts [24, 28]. However, a significant limitation of existing models is that they can easily face visual-textual misalignment in practice, where certain elements in the input text are overlooked in generated images. As shown in Figure 1, none of Midjourney [20], Dall-E 3 [23], and SDXL [24] can craft an image containing \"a tea cup of iced coke\". Instead, these models exhibit a preference for generating a glass cup due to inherent biases in concept combination during the training process of the models."}, {"title": "2 Related Work", "content": "Diffusion Models Diffusion models [13] predict noise in noisy images and produce high-quality outputs after training. Having gained considerable attention, they are now considered the state-of-the-art in image generation. These models find applications in various domains, such as label-to-image synthesis [6,28], text-to-image generation [24, 25, 28], image editing [5, 14, 19], and video generation [12]. Specifically, text-to-image generation is of practical importance. Open-source and commercial solutions like Stable Diffusion [24, 28], Midjourney [20] and Dall-E 3 [23] have achieved success, bolstered by advancements in neural networks [29,34] and large datasets of image-text pairs [31,32]. Nevertheless, the question of whether these models truly innovate or merely generate combinations encountered in their training data remains unresolved. In this study, we investigate this issue by examining text-to-image diffusion models in the context of unconventional concept pairings, namely LC-Mis.\nMisalignment Issues While state-of-the-art generative models frequently produce high-quality, realistic images, they struggle with certain concept combinations. Such models usually mimic combinations seen in training data. Prior work has emphasized spatial conflicts where multiple entities coexist in close proximity [3, 9, 16, 17,35]. Distinct from these investigations, our focus lies on Latent Concept Misalignment, illustrated by phrases such as \"a tea cup of iced coke.\" Through rigorous experimentation, we investigate this challenge and introduce a benchmark alongside a hotfix solution."}, {"title": "3 Benchmark: Collecting Data on Latent Concept Misalignment (LC-Mis)", "content": "In this section, we detail the process of collecting our LC-Mis dataset. In our dataset, each pair features Concept A and Concept B, which cannot be simultaneously generated by text-to-image models due to the existance of the latent concept, C. The advantages of our collection system are as follows:\nGiven that extensive human knowledge is required to generate data with LC-Mis issue, we utilize LLMs (e.g., GPT-3.5 in this paper) to develop an efficient guidance system, inspired by LLMs Reasoning [8].\nThese concept pairs often defy common human understanding, making them challenging to mine. We meticulously craft a generation loop containing 4 phases, allowing the iterative amplification of a small dataset into a substantially larger one. The process overview is shown in Figure 3, and the detailed prompts to guide GPT-3.5 can be found in our Appendix Section A.\nFinally, concept pairs discovered by GPT-3.5 require comprehensive evaluation. However, advanced tools such as Clipscore [11] or Image Reward [37] may exhibit ineffectiveness in LC-Mis problem (Section 5.3). Therefore, the evaluation of human experts is an important part of our system."}, {"title": "3.1 Phase 1 - Identifying Initial Concept Pairs as Seeds", "content": "In Phase 1, human experts identify a small number of concept pairs and extract patterns from them. These patterns serve as seeds for subsequent dataset expansion. Human researchers begin with analyzing various visual scenes in renowned text-to-image datasets, such as Laion2B-en and MJ User Prompts & Images Datasets, to detect concept pairs prone to LC-Mis. They select 50 concept pairs, like \"iced coke\" and \"tea cup\", which produce inaccurate output images.\nUpon analysis, we categorize these concept pairs into 8 distinct patterns. Among them, 4 patterns belong to the scope of LC-Mis (detailed LC-Mis patterns are shown in Appendix Table 1), mainly including the categories of \"foreground and background\", as well as \"objects and containers\". These patterns serve as an effective starting point for identifying additional concept pairs by means of GPT-3.5."}, {"title": "3.2 Phase 2 - Generating and Verifying Additional Concept Pairs with GPT-3.5", "content": "In this phase, human researchers employ GPT-3.5 to generate additional concept pairs corresponding to each pattern from Section 3.1, using the initial 50 concept pairs for few-shot learning. Using this method, GPT-3.5 generates 499 valid concept pairs. None of these pairs can be generated correctly by text-to-image models with the simple prompt \"concept A, concept B\" and they undergo rigorous verification, with most of them proving to be of high quality.\nThen a further verification is meticulously designed to comprehensively assess the accuracy of the generated concept pairs. Specifically, GPT-3.5 generates 5 prompts for each pair, varying in length and richness. Subsequently, text-to-image models generate 4 images per prompt. After human verification of all 20 images, the concept pairs are scored on 5 levels: zero correct images correspond to Level 5, 1~5 correct images to Level 4, 6 ~ 10 to Level 3, 11 ~ 15 to Level 2, and 16~20 to Level 1."}, {"title": "3.3 Phase 3 - Discovering New Patterns for Concept Pair Generation", "content": "In this phase, we leverage GPT-3.5 to identify 9 new patterns and apply the same methodology in Section 3.2 to these patterns.\nDuring previous process, we observe that GPT-3.5 produces duplicates when tasked with generating additional concept pairs, suggesting that it may have reached the limits of its knowledge within current patterns. Consequently, GPT-3.5 is encouraged to autonomously identify new patterns to enhance the semantic scope of LC-Mis. Building upon the 8 meticulously categorized patterns, human researchers instruct GPT-3.5 to identify 9 additional patterns. Subsequently, we replicate the procedures from Section 3.2, achieving a Level 5 accuracy rate of 70%, markedly exceeding the capabilities of human experts."}, {"title": "3.4 Phase 4 - Creating Novel Concept Pairs by Merging Patterns", "content": "In this phase, we merge concepts from different patterns to form new patterns and concept pairs, which will serve as seeds for the subsequent iteration.\nSpecifically, human researchers instruct GPT-3.5 to combine concepts from one pattern with ones from another pattern. For example, patterns labeled \"Beverage and Incorrect Container\" and \"Jewelry and Inadequate Storage\" are merged to create new patterns: \u201cBeverage and Jewelry Storage\" and \"Jewelry and Beverage Container\". Following the process in Section 3.3, we observe that each newly created pattern achieves a Level 5 accuracy rate of at least 60%, demonstrating GPT-3.5's capability in synthesizing orthogonal patterns into more patterns.\nIn summary, we collect our dataset leveraging the collaboration between LLMs (i.e., GPT-3.5) and text-to-image models. Based on our proposed system, we can further expand our dataset iteratively in the future."}, {"title": "4 Method: Mixture of Concept Experts (MoCE)", "content": "Motivation of MoCE Humans always follow a certain order when painting. Inspired by human painting nature and motivated by dynamic models [10], we integrate LLMs (e.g., GPT-3.5 in this paper) into our method, Mixture of Concept Experts (MoCE) to alleviate the LC-Mis issues. As shown in Figure 4, we first input the easily overlooked concept to focus on the attention mechanism during the early diffusion stages, enhancing their representation in the final image. We base MoCE on SDXL [24], one of the foremost reliable open-source diffusion models. The system includes components listed as follows:\nSequential Concept Introduction Gaining insight from human artistic processes, concepts are introduced to diffusion models sequentially rather than simultaneously to prevent the LC-Mis entanglement. As a result, we need to find a reasonable and logical sequence of them. LLMs are naturally suitable for this task. Therefore, we employ GPT-3.5 to ascertain the most logical sequence for two concepts based on its comprehension of human behavior. We provide interaction details with GPT-3.5 in our Appendix Section A.\nDenoising Process Partition In the denoising process of diffusion models across time steps, $t_{\\bar{i}}, t_{r-1},..., t_1, t_0$, the process is split into 2 phases:\nFirst Phase ($t_{\\bar{i}}, t_{r-1}, ..., t_v$): We only input the easily lost concept into diffusion models, and save images at each time step as a preparatory list.\nSecond Phase ($t_{N-1},t_{N-2}, ..., t_0$): $t_x$ will be selected from the preparatory list. Starting from time step $t_x$, we provide the completed text prompt to diffusion models to synthesize the final image.\nHowever, which image $t_0$ to take from the preparatory list reaches the optimal output remains unknown, motivating us to formulate a strategy for autonomously determining the optimal allocation of time steps.\nAutomated Feedback Mechanism We iteratively select the most suitable $t_x$ based on the fidelity of the final image. In this case, Clipscore, $S_c$, is used to assess the fidelity of the generated image M to the desired concept, e.g., A:\n$S_c(M, A) = ClipScore(M, A) \t (1)$\nAs the issue of misalignment persists, naively employing Clipscore wouldn't truly show the correlation between entity A and the final image M since it may be not distinguishable (see Section 5.3). Therefore, we calculate the Clipscore between images and both the concept itself and its corresponding description generated by GPT-3.5. Some details are not prominent in M, yet they might be further elaborated in the information further provided by the descriptions. This possibly uplifts the Clipscore of the entity and lowers the influence of misalignment to some certain extent. The the score function in MoCE, Description-Concept Coordination Score, is as follow:\n$S(M, A) = Max(S_c(M, A), S_c(M, A_{Description})) \t (2)$\nWe also devise a new metric named Multi-Concept Disparity, denoted as D:\n$D = S(M, A) \u2013 S(M, B) \t (3)$\nD denotes the difference in the scores of an image between two concepts. The larger the size of |D|, the higher the likelihood that at least one concept within it will become blurred.\nDynamic Binary Optimization When allocating a larger number of time steps in the first stage, the more dominant features of A possibly overshadows those of B, then the greater the absolute value of D is, and vice versa. Therefore, there exists a positive correlation between the time steps allocated in the first phase and the image quality, making the binary search method highly suitable for this scenario. When the score of concept A is significantly higher than B, we choose a larger $t_0$ as the boundary point for two phases, and vice versa. If the difference of their scores is below the threshold, the final image will be output. Specific implementation details can be found in our Appendix Section C.\nIn summary, our methodology presents a robust solution for addressing entity misalignment in text-to-image diffusion models. It combines intuitive reasoning, automatic fine-tuning, and efficiency optimizations to produce more precise and contextually apt image outputs."}, {"title": "5 Experiments", "content": "We conduct extensive experiments centered around MoCE, revealing its ability to alleviate the LC-Mis issue in text-to-image diffusion models."}, {"title": "5.1 Setup", "content": "Dataset In Section 3.2, we obtain 272 concept pairs for Level 5. From this pool, human experts carefully select 173 concept pairs as LC-Mis cases for experiments, while the remaining concept pairs fall into the realm of traditional misalignment issues, extensively discussed in Section 2.\nModel Our remediation, MoCE, is implemented using SDXL [24] due to its open-source nature and widespread use. We omit Midjourney and Dall-E 3 from consideration because of their internal black-box architectures. Our baseline, represented by these 173 LC-Mis concept pairs in Section 3, demonstrates that all of them receive a rating of Level 5, indicating that none of the images generated in 20 attempts faithfully represented the expected concepts. We also incorporate 3 additional baseline models: Attend-and-Excite (AAE) [3], Dall-E 3 [23], and Anole [4]. AAE aims to mitigate traditional misalignment issues via the attention map layer, and Dall-E 3 through fine-grained annotation. And Anole is a novel model that employs autoregressive methods for interleaved image-text generation, offering fresh insights into resolving misalignment issues.\nOur experiments use a single NVIDIA A100 GPU for image generation via SDXL, and a RTX 4090 GPU is also sufficient.\nEvaluation Metric Considering the instability of quantitative score evaluation, as well as the use of Clipscore and Image-Reward in our method, MOCE, we primarily utilize human evaluation in our experiments. We engage human experts for a more impartial evaluation. Additionally, the results of quantitative score evaluation are also included in our Appendix Section D for reference."}, {"title": "5.2 Result", "content": "Our method, MoCE, utilizes the 173 Level 5 LC-Mis concept pairs, providing their corresponding text prompts as input to the model. The second phase of MoCE is repeated up to three times to ensure the Multi-Concept Disparity D coverage below 0.6. After obtaining 20 images for each concept pair, human experts re-evaluate these images based on the criteria discussed in Section 3. We report the counts of concept pairs at each level after undergoing improvement by our MoCE and compare them to the baseline results in Figure 7. Meanwhile, we also present several visualized images in Figure 5 and 6.\nIn the original Level 5 LC-Mis concept pairs, the baseline model fails to produce any correct image, as we mentioned in Section 3. Even when sophisticated engineering strategies, e.g., AAE and Anole, are used, which may be effective for traditional misalignment problems, there is little improvement for Level 5 concept pairs when applied to LC-Mis problems. However, following the enhancement made by our MoCE, over half of the concept pairs are now correctly generated, and there are even several concept pairs rated as Level 1. Additionally, Dall-E 3, with its expensive and fine-tuned data annotations, indeed helps with the LC-Mis problem, achieving improvements for Level 5 concept pairs comparable to MoCE. However, it is important to note that training Dall-E 3 requires additional data preprocessing, which can be a costly process, suggesting that our MOCE produces more correct images in an economical manner. We also include a comparison of the scores of images generated by MoCE and baselines in Appendix Table 4, demonstrating that our method is statistically meaningful. In addition, we also present the human evaluation results under the segmented patterns. As introduced in Section 3, we have divided these 173 LC-Mis concept pairs into 4 patterns, and we have presented the results of human evaluation in Figure 8. In Patterns (a), (b) and (c), MoCE outperforms Dall-E 3, as its output includes more concept pairs belonging to Level 1 and 2. And in Pattern (d), Dall-E 3's performance is superior to MoCE."}, {"title": "5.3 Analysis", "content": "We are deeply concerned about the impact of LC-Mis issues on the existing text-to-image model landscape, for the most representative and challenging example, \"a tea cup of iced coke\", we present visual restoration results in Figure 9. Both Clipscore and Image-Reward effectively adjust the time step in our MoCE model with the help of Description-Concept Coordination Score, while unfortunately the existing evaluation metrics may sometimes be proven to be not absolutely accurate enough. Figure 9 displays both Clipscore and Image-Reward scores between images and the concept \"iced coke\". We meticulously choose transparent glasses generated by baseline models, which resemble tea cups, to analyze the error in the scoring mechanism. The occurrence of \"iced coke\" in both sub-figures within Figure 9 is evident to human experts. However, both Clipscore and Image-Reward scores are markedly lower for images of \"a tea cup of iced coke\" attributed to the material of the cup. This highlights the limitations of current evaluation metrics in addressing misalignment issues and emphasizes the necessity for developing new metrics based on existing methods, which will become one of the key research directions for our future work."}, {"title": "6 Conclusions", "content": "In this paper, we introduce a new text-to-image misalignment issue called LC-Mis, involving a significant latent concept. We present a novel framework to explore the LC-Mis examples, making it one of the earliest works of using LLMs in developing image generation systems. Our method, MoCE, innovatively splits the text prompts and inputs them into diffusion models, alleviating the LC-Mis problem. Human evaluation confirms MoCE's effectiveness. we also highlight the impact of the LC-Mis on existing text-to-image systems, discuss flaws in current evaluation metrics, and call for community attention to this matter."}, {"title": "6.1 Summary", "content": "In this paper, we introduce a new text-to-image misalignment issue called LC-Mis, involving a significant latent concept. We present a novel framework to explore the LC-Mis examples, making it one of the earliest works of using LLMs in developing image generation systems. Our method, MoCE, innovatively splits the text prompts and inputs them into diffusion models, alleviating the LC-Mis problem. Human evaluation confirms MoCE's effectiveness. we also highlight the impact of the LC-Mis on existing text-to-image systems, discuss flaws in current evaluation metrics, and call for community attention to this matter."}, {"title": "6.2 Recent Advances in the Field", "content": "Until releasing our paper, text-to-image models have further evolved. We present results from the latest (available online as of July 7, 2024) Dall-E 3, Midjourney, and Stable Diffusion 3 in Figure 10. In the example \"a tea cup of iced coke\", Without complex prompt engineering, models still perform poorly on the LC-Mis issue, as shown in Figure 10a, 10c and 10d. Complex prompt engineering from GPT-4 (Figure 10b) does help alleviate the issue. However, it's important to note that this comes with significant annotation costs during Dall-E 3's training, and is also accompanied by a certain degree of instability, highlighting the issue's significance."}, {"title": "6.3 Future Work", "content": "In our future work, we will focus on exploring more complex LC-Mis scenarios and developing learnable search algorithms to reduce the iterations in our method. Additionally, we will expand the range of model types, model versions and sampler types used in our dataset and continuously iterate our dataset collection algorithms to enhance and enlarge the dataset."}, {"title": "A Interaction Details in Interactive LLMs Guidance System", "content": "In this section, we describe the detailed process of interaction between human researchers and GPT-3.5 within our workflow. Utilizing block diagrams for clarity, we model the engagement between human experts and GPT-3.5. To illustrate the variation in data volume within the Interactive LLMs Guidance System, we present several diagrams, in Figure 11. In these diagrams, pairs of spheres along the same axis denote concept pairs associated with a particular pattern, distinguishable by their color.\nPhase 1: Identifying Initial Concept Pairs as Seeds After developing an initial set of 50 concept pairs and categorizing them into 8 unique patterns (among them, 4 patterns match our LC-Mis issue), human researchers direct GPT-3.5 to generate additional concept pairs. We specify both the researcher-provided prompt and the GPT-3.5 response. The format of the patterns deducting concept pairs are depicted in Table 1.\nMidjourney text-to-image generation, in conjunction with human researcher validation, then confirms a total of 50 concept pairs. Throughout the evaluation conducted by human experts, 5 impartial human experts are tasked to identify the generated images. If minor differences, a majority opinion will be adopted. Otherwise, a senior expert will come to re-identify the image. Human expert evaluation prove to be accurate under certain circumstances as discussed in Section 5.3 of our paper."}, {"title": "Phase 2 - Generating and Verifying Additional Concept Pairs On the basis of known patterns, we encourage GPT-3.5 to explore more concept pairs. Our instructions for GPT-3.5 are as follows:", "content": null}, {"title": "Phase 3 - Discovering New Patterns for Concept Pair Generation", "content": "The proliferation of repetitive concept pairs has prompted researchers to investigate innovative approaches. Engaging with GPT-3.5, scholars aim to automate the discovery of new patterns. Our instructions for GPT-3.5 are as follows:"}, {"title": "Phase 4 - Creating Novel Concept Pairs by Merging Patterns Two straight lines determine a plane. Once the points on the two lines are defined, it's available to guide GPT-3.5 to expand the new plane. This means GPT-3.5 can blend newly generated patterns with previous patterns, thereby generating more LC-Mis concept pairs. Our instructions for GPT-3.5 are as follows:", "content": null}, {"title": "B Interaction in Sequential Concept Introduction", "content": "Here, we present the detailed interaction to guide GPT-3.5 to provide the most logical sequence of two concepts and the description of them."}, {"title": "C Comprehensive Explanation of MoCE", "content": "The whole process of MoCE actually contains two phases. For the first phase, as depicted in Figure 12, Diffusion Models was asked to generate the image for the selected substance to form a preparatory list including images of different timestamps ty. From there, the second phase of MOCE, as shown in Figure 13 gets in effect, which takes an image of specific timestamp tn from the preparatory list as the input for the Diffusion Models in the second stage where the whole sentence containing both substances was taken as the prompt. The selection of the specific candidate from the preparatory list was determined by our scoring model introduced in paper Section 4."}, {"title": "D Score Evaluation", "content": "By convention, evaluation metrics such as Clipscore and Image-Reward are usually used to gauge the correspondence between the generations and the input entities quantitatively. These metrics utilize the cosine similarity between embeddings produced by Deep Neural Networks (DNNs). Nonetheless, they often fail to discern numerical values, transparent objects, and other crucial elements readily identifiable by humans. In our paper Section 4, our proposed MOCE uses the Multi-Concept Disparity (D) between the scores of 64 images (M) with respect to 2 concepts (A and B) as the basis for performing a binary search:\n$D = S(M, A) \u2013 S(M, B) \t (4)$\nWe use this metric to assist the demonstration our MoCE performance. Specifically, we assess the generation performance of both the baseline model (SDXL) and our MoCE using the metric, D, in Equation 4, where D is calculated using Clipscore or Image-Reward, denoted as D - Clipscore and D - Image-Reward respectively. Experiments are conducted on both the set of concept pairs at Level 5 and Level 1 to 4. We report the experimental results in Table 4. In comparison to the baseline model, D - Clipscore of images generated by our MoCE is reduced by more than half, and the D - Image-Reward is reduced by more than, in"}, {"title": "E Ablation Demo", "content": "Here, we first demonstrate the ablation test on computational time. Figure 14 shows that the prompt switching time can be set between 20% and 40% of the steps to optimize the time-performance tradeoff, thus optimizing the cost to 1\u00d7 or 2x at the same time.\nMoreover, ablation test on dataset size is also conducted. Here we compare 3 models from datasets of varying sizes, as shown in Figure 15. Since DALL-E 3 is not open-sourced, we cannot apply MoCE to it. DALLE 3 does perform well. However, its detailed labeling process is labor-intensive, while at the same time, our MoCE can be easily integrated into current models."}, {"title": "F Restoration Visualizations of Level 5", "content": "Here, we demonstrate more visualizations of images of Level 5 restored using our MoCE. In spite of the given additional rich information, Midjourney fails to correctly generate these images. While using the same text prompts, our MOCE successfully retrieves the lost concepts as presented in Figure 16 and 17. We also propose that in Figure 16 and 17, human experts judge \"Hot Tea\" based on the transparency of the liquid and \"Shanghai\" based on landmarks such as the Oriental Pearl TV Tower. These judgments are based on verified model tendencies."}, {"title": "G Restoration Visualizations of Level 1 - 4", "content": "For images restored by baseline models by adding rich information, our MoCE can also easily retrieve the lost concepts and increase the frequency of correct generation, as presented in Figure 18 and 19."}]}