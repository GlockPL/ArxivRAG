{"title": "Lost in Translation: Latent Concept Misalignment in Text-to-Image Diffusion Models", "authors": ["Juntu Zhao", "Junyu Deng", "Yixin Ye", "Chongxuan Li", "Zhijie Deng", "Dequan Wang"], "abstract": "Advancements in text-to-image diffusion models have broadened extensive downstream practical applications, but such models often encounter misalignment issues between text and image. Taking the generation of a combination of two disentangled concepts as an example, say given the prompt \"a tea cup of iced coke\", existing models usually generate a glass cup of iced coke because the iced coke usually co-occurs with the glass cup instead of the tea one during model training. The root of such misalignment is attributed to the confusion in the latent semantic space of text-to-image diffusion models, and hence we refer to the \"a tea cup of iced coke\" phenomenon as Latent Concept Misalignment (LC-Mis). We leverage large language models (LLMs) to thoroughly investigate the scope of LC-Mis, and develop an automated pipeline for aligning the latent semantics of diffusion models to text prompts. Empirical assessments confirm the effectiveness of our approach, substantially reducing LC-Mis errors and enhancing the robustness and versatility of text-to-image diffusion models. Our code and dataset have been available online for reference.", "sections": [{"title": "1 Introduction", "content": "Text-to-image synthesis [7,15,18,21, 25-28,30,33,36,38-40] via diffusion models has made remarkable progress, where high-quality images are generated given text prompts [24, 28]. However, a significant limitation of existing models is that they can easily face visual-textual misalignment in practice, where certain elements in the input text are overlooked in generated images. As shown in Figure 1, none of Midjourney [20], Dall-E 3 [23], and SDXL [24] can craft an image containing \"a tea cup of iced coke\". Instead, these models exhibit a preference for generating a glass cup due to inherent biases in concept combination during the training process of the models.\n* Equal contribution. \u2020 Corresponding authors."}, {"title": "3 Benchmark: Collecting Data on Latent Concept Misalignment (LC-Mis)", "content": "In this section, we detail the process of collecting our LC-Mis dataset. In our dataset, each pair features Concept A and Concept B, which cannot be simultaneously generated by text-to-image models due to the existance of the latent concept, C. The advantages of our collection system are as follows:\nGiven that extensive human knowledge is required to generate data with LC-Mis issue, we utilize LLMs (e.g., GPT-3.5 in this paper) to develop an efficient guidance system, inspired by LLMs Reasoning [8].\nThese concept pairs often defy common human understanding, making them challenging to mine. We meticulously craft a generation loop containing 4 phases, allowing the iterative amplification of a small dataset into a substantially larger one. The process overview is shown in Figure 3, and the detailed prompts to guide GPT-3.5 can be found in our Appendix Section A.\nFinally, concept pairs discovered by GPT-3.5 require comprehensive evaluation. However, advanced tools such as Clipscore [11] or Image Reward [37] may exhibit ineffectiveness in LC-Mis problem (Section 5.3). Therefore, the evaluation of human experts is an important part of our system."}, {"title": "3.1 Phase 1 - Identifying Initial Concept Pairs as Seeds", "content": "In Phase 1, human experts identify a small number of concept pairs and extract patterns from them. These patterns serve as seeds for subsequent dataset expansion. Human researchers begin with analyzing various visual scenes in renowned"}, {"title": "3.2 Phase 2 - Generating and Verifying Additional Concept Pairs with GPT-3.5", "content": "In this phase, human researchers employ GPT-3.5 to generate additional concept pairs corresponding to each pattern from Section 3.1, using the initial 50 concept pairs for few-shot learning. Using this method, GPT-3.5 generates 499 valid concept pairs. None of these pairs can be generated correctly by text-to-image models with the simple prompt \u201cconcept A, concept B\u201d and they undergo rigorous verification, with most of them proving to be of high quality.\nThen a further verification is meticulously designed to comprehensively assess the accuracy of the generated concept pairs. Specifically, GPT-3.5 generates 5 prompts for each pair, varying in length and richness. Subsequently, text-to-image models generate 4 images per prompt. After human verification of all 20 images, the concept pairs are scored on 5 levels: zero correct images correspond to Level 5, 1~5 correct images to Level 4, 6 ~ 10 to Level 3, 11 ~ 15 to Level 2, and 16~20 to Level 1.\n+ https://huggingface.co/datasets/laion/laion2B-en\n$ https://huggingface.co/datasets/succinctly/midjourney-prompts"}, {"title": "3.3 Phase 3 - Discovering New Patterns for Concept Pair Generation", "content": "In this phase, we leverage GPT-3.5 to identify 9 new patterns and apply the same methodology in Section 3.2 to these patterns.\nDuring previous process, we observe that GPT-3.5 produces duplicates when tasked with generating additional concept pairs, suggesting that it may have reached the limits of its knowledge within current patterns. Consequently, GPT-3.5 is encouraged to autonomously identify new patterns to enhance the semantic scope of LC-Mis. Building upon the 8 meticulously categorized patterns, human researchers instruct GPT-3.5 to identify 9 additional patterns. Subsequently, we replicate the procedures from Section 3.2, achieving a Level 5 accuracy rate of 70%, markedly exceeding the capabilities of human experts."}, {"title": "3.4 Phase 4 - Creating Novel Concept Pairs by Merging Patterns", "content": "In this phase, we merge concepts from different patterns to form new patterns and concept pairs, which will serve as seeds for the subsequent iteration.\nSpecifically, human researchers instruct GPT-3.5 to combine concepts from one pattern with ones from another pattern. For example, patterns labeled \u201cBeverage and Incorrect Container\u201d and \u201cJewelry and Inadequate Storage\u201d are merged to create new patterns: \u201cBeverage and Jewelry Storage\u201d and \u201cJewelry and Beverage Container\u201d. Following the process in Section 3.3, we observe that each newly created pattern achieves a Level 5 accuracy rate of at least 60%, demonstrating GPT-3.5\u2019s capability in synthesizing orthogonal patterns into more patterns.\nIn summary, we collect our dataset leveraging the collaboration between LLMs (i.e., GPT-3.5) and text-to-image models. Based on our proposed system, we can further expand our dataset iteratively in the future."}, {"title": "4 Method: Mixture of Concept Experts (MoCE)", "content": "Motivation of MoCE Humans always follow a certain order when painting. Inspired by human painting nature and motivated by dynamic models [10], we integrate LLMs (e.g., GPT-3.5 in this paper) into our method, Mixture of Concept Experts (MoCE) to alleviate the LC-Mis issues. As shown in Figure 4, we"}, {"title": "5 Experiments", "content": "We conduct extensive experiments centered around MoCE, revealing its ability to alleviate the LC-Mis issue in text-to-image diffusion models."}, {"title": "5.1 Setup", "content": "Dataset In Section 3.2, we obtain 272 concept pairs for Level 5. From this pool, human experts carefully select 173 concept pairs as LC-Mis cases for experiments, while the remaining concept pairs fall into the realm of traditional misalignment issues, extensively discussed in Section 2.\nModel Our remediation, MoCE, is implemented using SDXL [24] due to its open-source nature and widespread use. We omit Midjourney and Dall-E 3 from consideration because of their internal black-box architectures. Our baseline, represented by these 173 LC-Mis concept pairs in Section 3, demonstrates that all of them receive a rating of Level 5, indicating that none of the images generated in 20 attempts faithfully represented the expected concepts. We also incorporate 3 additional baseline models: Attend-and-Excite (AAE) [3], Dall-E 3 [23],"}, {"title": "5.2 Result", "content": "Our method, MoCE, utilizes the 173 Level 5 LC-Mis concept pairs, providing their corresponding text prompts as input to the model. The second phase of MoCE is repeated up to three times to ensure the Multi-Concept Disparity D coverage below 0.6. After obtaining 20 images for each concept pair, human experts re-evaluate these images based on the criteria discussed in Section 3. We report the counts of concept pairs at each level after undergoing improvement by our MoCE and compare them to the baseline results in Figure 7. Meanwhile, we also present several visualized images in Figure 5 and 6.\nIn the original Level 5 LC-Mis concept pairs, the baseline model fails to produce any correct image, as we mentioned in Section 3. Even when sophisticated engineering strategies, e.g., AAE and Anole, are used, which may be effective for traditional misalignment problems, there is little improvement for Level 5 concept pairs when applied to LC-Mis problems. However, following the enhancement made by our MoCE, over half of the concept pairs are now correctly generated, and there are even several concept pairs rated as Level 1. Additionally, Dall-E 3, with its expensive and fine-tuned data annotations, indeed helps with the LC-Mis problem, achieving improvements for Level 5 concept pairs comparable to MoCE. However, it is important to note that training Dall-E 3 requires additional data preprocessing, which can be a costly process, suggesting"}, {"title": "5.3 Analysis", "content": "We are deeply concerned about the impact of LC-Mis issues on the existing text-to-image model landscape, for the most representative and challenging example, \u201ca tea cup of iced coke\u201d, we present visual restoration results in Figure 9. Both Clipscore and Image-Reward effectively adjust the time step in our MoCE model with the help of Description-Concept Coordination Score, while unfortunately the existing evaluation metrics may sometimes be proven to be not absolutely"}, {"title": "6 Conclusions", "content": ""}, {"title": "6.1 Summary", "content": "In this paper, we introduce a new text-to-image misalignment issue called LC-Mis, involving a significant latent concept. We present a novel framework to explore the LC-Mis examples, making it one of the earliest works of using LLMs in developing image generation systems. Our method, MoCE, innovatively splits the text prompts and inputs them into diffusion models, alleviating the LC-Mis problem. Human evaluation confirms MoCE\u2019s effectiveness. we also highlight the impact of the LC-Mis on existing text-to-image systems, discuss flaws in current evaluation metrics, and call for community attention to this matter."}, {"title": "6.2 Recent Advances in the Field", "content": "Until releasing our paper, text-to-image models have further evolved. We present results from the latest (available online as of July 7, 2024) Dall-E 3, Midjourney, and Stable Diffusion 3 in Figure 10. In the example \u201ca tea cup of iced coke\u201d, Without complex prompt engineering, models still perform poorly on the LC-Mis issue, as shown in Figure 10a, 10c and 10d. Complex prompt engineering from GPT-4 (Figure 10b) does help alleviate the issue. However, it\u2019s important to note that this comes with significant annotation costs during Dall-E 3\u2019s training, and is also accompanied by a certain degree of instability, highlighting the issue\u2019s significance."}, {"title": "6.3 Future Work", "content": "In our future work, we will focus on exploring more complex LC-Mis scenarios and developing learnable search algorithms to reduce the iterations in our method. Additionally, we will expand the range of model types, model versions and sampler types used in our dataset and continuously iterate our dataset collection algorithms to enhance and enlarge the dataset."}, {"title": "A Interaction Details in Interactive LLMs Guidance System", "content": "In this section, we describe the detailed process of interaction between human researchers and GPT-3.5 within our workflow. Utilizing block diagrams for clarity, we model the engagement between human experts and GPT-3.5. To illustrate the variation in data volume within the Interactive LLMs Guidance System, we present several diagrams, in Figure 11. In these diagrams, pairs of spheres along the same axis denote concept pairs associated with a particular pattern, distinguishable by their color.\nPhase 1: Identifying Initial Concept Pairs as Seeds After developing an initial set of 50 concept pairs and categorizing them into 8 unique patterns (among them, 4 patterns match our LC-Mis issue), human researchers direct GPT-3.5 to generate additional concept pairs. We specify both the researcher-provided prompt and the GPT-3.5 response. The format of the patterns deducting concept pairs are depicted in Table 1.\nMidjourney text-to-image generation, in conjunction with human researcher validation, then confirms a total of 50 concept pairs. Throughout the evaluation conducted by human experts, 5 impartial human experts are tasked to identify the generated images. If minor differences, a majority opinion will be adopted. Otherwise, a senior expert will come to re-identify the image. Human expert evaluation prove to be accurate under certain circumstances as discussed in Section 5.3 of our paper."}, {"title": "Phase 2 - Generating and Verifying Additional Concept Pairs", "content": "On the basis of known patterns, we encourage GPT-3.5 to explore more concept pairs. Our instructions for GPT-3.5 are as follows:\nPrompt: I want you to act as a concept pair generator for text-to-image generation model, midjourney. You are good at text-image concept grounding, visual concept understanding, text-to-image diffusion models' prompt engineering. Don't be verbose in your answers, but do provide details and examples where it might help the explanation. I've provided you with a bunch of existing positive and negative noun-concept pairs. The existing positive pairs comprise a common beverage and an unconventional common container that is not typically used to hold or serve that beverage, making them positive pairs. Specifically, given noun-concept pair like \u201ciced coke\u201d and \u201ctea cup\u201d, they are both common in the real world. However, the resultant text-to-image synthesis unexpectedly refers to \u201ca glass cup containing iced coke\u201d, with the concept of \u201ctea cup\u201d ignored. It will be your job to come up with 30 such new positive noun-concept pairs following the given pairs. You need to remember that nouns cannot be repeated between different concept pairs."}, {"title": "Phase 3 - Discovering New Patterns for Concept Pair Generation", "content": "The proliferation of repetitive concept pairs has prompted researchers to investigate innovative approaches. Engaging with GPT-3.5, scholars aim to automate the discovery of new patterns. Our instructions for GPT-3.5 are as follows:\nPrompt: I want you to act as a bot summarizing the pattern rule and generating more patterns. You are good at text-image concept grounding, visual concept understanding, text-to-image diffusion models' prompt engineering. Don't be verbose in your answer, but do provide details and examples that might help the explanation.\nI have provided you with a series of existing positive and negative patterns, where \u201cpattern\u201d refers to a generalization of certain noun pairs, A and B. Where A and B are very common concepts, such as coke and tea cup. Moreover, these noun pairs are closely related to visual imagery, rather than being two independent objects, such as the wrapper and the wrapped item, background and foreground, etc.\nIn the provided positive and negative patterns, the noun pairs in the positive pattern are combinations that almost never appear in the text-to-image training dataset, such as beverage and erroneous container (e.g., iced cola and tea cup) or local cuisine and non-native location (e.g., sushi and New York City). Specifically, for a positive noun pair \u201cA\u201d and \u201cB\u201d, there will usually be a hidden concept \u201cC\u201d. \u201cA\u201d always appears with \u201cC\u201d in the text-to-image training dataset, \u201cB\u201d and \u201cC\u201d belong to the same category or are of the same kind but their visual effect is very different. Therefore, because of the strong association between \u201cA\u201d and \u201cC,\u201d it leads to \u201cB\u201d being obscured or occupied in the generated image with prompts \u201cA\u201d and \u201cB\u201d. For example, in the positive pattern \u201cBeverage and erroneous container\u201d, \u201cA\u201d (iced coke) and \u201cB\u201d (tea cup) are a good example pair. Because \u201cA\u201d (iced coke) is strongly bound to the latent concept \u201cC\u201d (glass), telling the model to generate \u201ca tea cup of iced coke\u201d will generate a glass of iced coke instead of a tea cup of iced coke.\nConversely, the noun pairs in the negative pattern are combinations that frequently appear in the text-to-image training dataset, such as animal and their corresponding external covering or jackets and their appropriate inner garments. Your task is to come up with some new positive patterns based on the given positive and negative patterns:"}, {"title": "Phase 4 - Creating Novel Concept Pairs by Merging Patterns", "content": "Two straight lines determine a plane. Once the points on the two lines are defined, it\u2019s available to guide GPT-3.5 to expand the new plane. This means GPT-3.5 can blend newly generated patterns with previous patterns, thereby generating more LC-Mis concept pairs. Our instructions for GPT-3.5 are as follows:\nPrompt: I found that the text-to-image model cannot draw some images specified by patterns. For example, the pattern \u201cBeverage and wrong beverage container\u201d (\u201cA and B\u201d) and the pattern \u201cJewelry and Impre storage\u201d (\u201cC and D\u201d).\nThe pattern \u201cBeverage and erroneous container\u201d includes many concept pairs, such as \u201ciced coke in a tea cup\u201d, \u201cgreen tea in a red solo cup\u201d, \u201cOrange juice in a coffee mug\u201d, and so on.\nAnd another pattern \u201cJewelry and improper storage\u201d also includes many concept pairs, such as \u201cDiamond necklace in toolbox\u201d, \u201cRuby earings in bird nest\u201d, \u201cSilver anklet in toilet bowl\u201d, and so on.\nI hope to come up with more patterns that cannot be drawn based on these two patterns. Can you help me come up with a few more patterns based on these two patterns. In other words, I mean you to generate new patterns by recombining the items in the two patterns I provided to you, rather than providing new concept pairs in the patterns I gave. You need to use only the two patterns I've provided and regroup things.\nIn the current case, \u201cA\u201d refers to beverage, \u201cB\u201d refers to beverage containers, \u201cC\u201d refers to Jewelry, and \u201cD\u201d refers to storage. Give me the appropriate 2 patterns \u201cA and D\u201d and \u201cB and C\u201d. Where \u201cA\u201d and \u201cD\u201d almost never occur together, and \u201cB\u201d and \u201cC\u201d never occur together. For each new pattern, please give me detailed concept pairs belonging to this pattern that the model may fail to draw such an image.\nAnswer: Certainly, I can help you generate two new patterns \u201cA and D\u201d and \u201cB and C\u201d based on your provided patterns \u201cJewelry and improper storage\u201d and \u201cBeverage and erroneous container\u201d These new patterns will involve combinations that are less likely to occur together, and the model may have difficulty drawing corresponding images.\nPattern 1: \u201cA and D\u201d - Beverage and Imperial Storage\nIn this pattern, we combine \u201cBeverage\u201d (A) with \u201cImperial storage\u201d (D). These two elements are quite different, and the challenge here would be to depict beverages in an imperial storage setting."}, {"title": "B Interaction in Sequential Concept Introduction", "content": "Here, we present the detailed interaction to guide GPT-3.5 to provide the most logical sequence of two concepts and the description of them.\nPrompt: I want you to act as an Al creative assistant. I have two concepts A and B, and a sentence combining these two concepts together. I want you to help me generate a brief visual description of each concept. Namely, a concise description of what a given noun itself looks like, such as color, shape, or other visual features. As short as possible. No background. No other objects.\nIn addition, if I tend to draw a picture using the sentence, please tell me which concept I should draw first. For example, if a concept is a type of container, we should draw such a container first. If a concept is a type of background, we should draw such background first.\nNow, the concept A is \u201ca tea cup\u201d, B is \u201ciced coke\u201d, and the sentence is \u201cA tea cup filled with iced coke\u201d. Please answer me with the descriptions of A and B, and tell me which one to draw first.\nAnswer: Certainly, I can help you.\nConcept A: Tea cup A small, usually white or pastel-colored cup, often with a handle, designed to hold hot beverages.\nConcept B: Iced Coke Dark, fizzy liquid with translucent ice cubes floating within.\nTo draw the sentence \u201cA tea cup filled with iced coke\u201d:\n1. Draw the tea cup first (Concept A), as it acts as the container.\n2. Then, fill the tea cup with the iced coke (Concept B)."}, {"title": "C Comprehensive Explanation of MoCE", "content": "The whole process of MoCE actually contains two phases. For the first phase, as depicted in Figure 12, Diffusion Models was asked to generate the image for the selected substance to form a preparatory list including images of different timestamps ty. From there, the second phase of MoCE, as shown in Figure 13 gets in effect, which takes an image of specific timestamp tn from the preparatory list as the input for the Diffusion Models in the second stage where the whole sentence containing both substances was taken as the prompt. The selection of the specific candidate from the preparatory list was determined by our scoring model introduced in paper Section 4."}, {"title": "D Score Evaluation", "content": "By convention, evaluation metrics such as Clipscore and Image-Reward are usually used to gauge the correspondence between the generations and the input entities quantitatively. These metrics utilize the cosine similarity between embeddings produced by Deep Neural Networks (DNNs). Nonetheless, they often fail to discern numerical values, transparent objects, and other crucial elements readily identifiable by humans. In our paper Section 4, our proposed MoCE uses the Multi-Concept Disparity (D) between the scores of 64 images (M) with respect to 2 concepts (A and B) as the basis for performing a binary search:\n\\(D = S(M, A) \u2013 S(M, B)\\) (4)\nWe use this metric to assist the demonstration our MoCE performance. Specifically, we assess the generation performance of both the baseline model (SDXL) and our MoCE using the metric, D, in Equation 4, where D is calculated using Clipscore or Image-Reward, denoted as \\(D\\) - Clipscore and \\(D\\) - Image-Reward respectively. Experiments are conducted on both the set of concept pairs at Level 5 and Level 1 to 4. We report the experimental results in Table 4. In comparison to the baseline model, \\(D\\) - Clipscore of images generated by our MoCE is reduced by more than half, and the \\(D\\) - Image-Reward is reduced by more than, in\nTo facilitate easier observation by human experts, we demonstrate the established \\(D\\) - Clipscore at a magnification of 10x."}, {"title": "E Ablation Demo", "content": "Here, we first demonstrate the ablation test on computational time. Figure 14 shows that the prompt switching time can be set between 20% and 40% of the steps to optimize the time-performance tradeoff, thus optimizing the cost to 1\u00d7 or 2x at the same time.\nMoreover, ablation test on dataset size is also conducted. Here we compare 3 models from datasets of varying sizes, as shown in Figure 15. Since DALL-E 3 is not open-sourced, we cannot apply MoCE to it. DALLE 3 does perform well. However, its detailed labeling process is labor-intensive, while at the same time, our MoCE can be easily integrated into current models."}, {"title": "F Restoration Visualizations of Level 5", "content": "Here, we demonstrate more visualizations of images of Level 5 restored using our MoCE. In spite of the given additional rich information, Midjourney fails to correctly generate these images. While using the same text prompts, our MoCE successfully retrieves the lost concepts as presented in Figure 16 and 17. We also propose that in Figure 16 and 17, human experts judge \u201cHot Tea\u201d based on the transparency of the liquid and \u201cShanghai\u201d based on landmarks such as the Oriental Pearl TV Tower. These judgments are based on verified model tendencies."}, {"title": "G Restoration Visualizations of Level 1 - 4", "content": "For images restored by baseline models by adding rich information, our MoCE can also easily retrieve the lost concepts and increase the frequency of correct generation, as presented in Figure 18 and 19."}]}