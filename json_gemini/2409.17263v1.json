{"title": "Collaborative Comic Generation: Integrating Visual Narrative Theories with Al Models for Enhanced Creativity", "authors": ["Yi-Chun Chen", "Arnav Jhala"], "abstract": "This study presents a theory-inspired visual narrative generative system that integrates conceptual principles-comic authoring idioms-with generative and language models to enhance the comic creation process. Our system combines human creativity with Al models to support parts of the generative process, providing a collaborative platform for creating comic content. These comic-authoring idioms, derived from prior human-created image sequences, serve as guidelines for crafting and refining storytelling. The system translates these principles into system layers that facilitate the creation of comics through sequential decision-making, addressing narrative elements such as panel composition, story tension changes, and panel transitions. Key contributions include the integration of machine learning models into the human-AI cooperative comic generation process, the deployment of abstract narrative theories into AI-driven comic creation, and a customizable tool for narrative-driven image sequences. This approach improves narrative elements in generated image sequences and brings engagement of human creativity in an Al-generative process of comics. We open-source the code at https://github.com/RimiChen/Collaborative_Comic_Generation.", "sections": [{"title": "1. Introduction", "content": "Despite their various names, comics, manga, and visual stories represent a dominant form of storytelling that spans cultures and age groups. Authors combine their creative storytelling ideas with textual expressions and graphical representations to convey intricate narratives through multi-modal panel sequences. Recently, generative AI, a trending topic in AI creativity, has explored the automatic generation of narratives and their synthesis with visual representations, simulating an activity traditionally rooted in human creativity. These studies lead to an exciting research question: how can AI collaborate with human creativity to create image sequences, such as comics and visual stories?\nNowadays, most generative AI models handle almost the entire process of creating image sequences, leaving little room for human authors to modify the details during generation. Traditionally, however, creating visual stories or comics relies heavily on authors' familiarity"}, {"title": "2. Methodology", "content": "The system is structured to a human-in-loop workflow as Figure 1.\nThe system comprises six modules: the graphical interface, a container of models, the pool of visual sets and data, an image sequence model, a generator, and a renderer.\n\u2022 Graphical Interface: This is the primary means of interaction for human authors. It\nallows them to input base images, launch and apply ML models to the current image\nsequence, import scripts for customized editing layers, and perform simple operations\nlike selection and dragging to modify images."}, {"title": "2.1. Build-in and Extendable Elements:", "content": "To demonstrate the system's capabilities, we incorporated a built-in model, the action causal graph, and a visual symbol set as the default sources to support the plotline of the visual representations. These components are extendable through the APIs of the Container of Models and the Pool of Visual Sets and Data, respectively. Detailed documentation will be provided in the API subsections."}, {"title": "2.1.1. Common Symbols", "content": "In human-created comics, authors often use abstract symbols to visualize ideas such as atmosphere, motion, and emotions, thereby exaggerating characters' reactions. These symbols include emojis to emphasize emotions, speed lines to show movement, explosion shapes to represent collisions, cross shapes to denote anger, and many others. Table 1 presents examples from the Manga109 dataset, a collection of 109 Japanese manga titles published in commercial magazines [12, 13]. These examples serve as references for the symbols or emojis used in our default set."}, {"title": "2.1.2. Action Causal Network", "content": "The Action Causal Graph is a directed graph model where each node represents an action that a character might perform, and the links indicate the causal relationships between actions and possible reactions. For example, \"Fall\" links to \"Fly,\" \"Jump,\" and \"Run,\" while \"Dizzy,\" \"Collide,\" and \"Hit\" link to \"Run.\" Consecutive nodes form action pools for plot planning. The current version of the default action set built into the system includes a small group of common daily"}, {"title": "2.2. Al-Assisted Editing Layers and Narrative Theories:", "content": "We implement three ML-driven editing layers in the generator to demonstrate the system's image sequence pipeline. The first layer uses a stable diffusion model for modifying visual elements, while the second layer combines the PAD emotion model with a semantic analysis language model to guide plotline decisions."}, {"title": "2.2.1. Diffusion Model for Visual Elements", "content": "We employed a pre-trained stable diffusion model developed by the CompVis group to implement one of the editing layers [14, 15]. This high-resolution image synthesis model transforms input text descriptions into detailed and coherent images based on latent diffusion. The model allows users to adjust visual elements like characters or scenes.\nThe generated panel sequences are rendered through our pipeline, where information is first updated in the Image Sequence Model before being rendered. Any changes in the visual representation of an entity will be reflected in its semantic nodes, ensuring character consistency throughout the panel sequence. For example, if the diffusion model alters the visual representation of character_x, this change will be mirrored in its semantic mapping. Similar rules apply to other visual elements. Furthermore, our system supports multi-layer rendering, dividing panel images into background, foreground, compositional, and symbol layers. This feature enables partial redrawing of the generated panel, ensuring that changes applied to one entity do not interfere with others."}, {"title": "2.2.2. Plotline and Narrative Theories", "content": "To form a simple plotline, we incorporate narrative idioms and theories. This editing layer aims to generate content for the image sequence according to a specific narrative arc. We use Cohn's narrative grammar [16, 17, 18] to estimate the narrative arc, as the grammar categories indicate plot changes throughout the comic sequence. After establishing the narrative arc, we target the characters' actions to predict story tension.\nWe use the arousal level concept from the PAD emotion model [19, 20, 21] and sentiment labels from the language model to predict arousal scores for character actions, where higher scores indicate greater story tension. Differences in arousal scores between consecutive actions (based on our action causal graph) form a probability distribution for subsequent actions. This mapping enables the editing layer to select actions probabilistically while maintaining narrative arc alignment. Detailed explanations of each component are provided in the following subsections."}, {"title": "Narrative Grammar", "content": "We formalize the plot generation of new comic sequences using Cohn's Visual Narrative Grammar (VNG). Starting with the narrative structure to determine the content's global reason-"}, {"title": "Algorithm 1 Grammar Structure", "content": "1: procedure GRAMMER STRUCTURE SCRIPT\n2: P\u2190 input the current panel sequence.\n3: VNG \u2190 Create object list for VNG basic phases.\n4: S\u2190 Expend a center-embedded tree with VNG, get the reference narrative structure.\n5: if thenLength(P) \u2260 Length(S)\n6: Add or Subtract empty panels from P\n7: end if\n8: loop: Assign each phases in VNG to P\n9: return P\n10: end procedure"}, {"title": "Algorithm 2 Narrative Arc", "content": "1: procedure MAPPING NARRATIVE ARC WITH NARRATIVE STRUCTURE\n2: P\u2190 input the current panel sequence.\n3: REF \u2190 create value dictionary with VNG phases with assigned tension scores.\n4: REF = {E : 0I : 2L : 4P : 6R : 2}\n5: if panels in P have assigned grammar phase then\n6: loop: over P, apply REF with the grammar phase\n7: elseuse default narrative arc scores\n8: end if\n9: return P\n10: end procedure"}, {"title": "PAD Emotion State Model and Semantic Analysis", "content": "The PAD emotion state model, developed by Albert Mehrabian and James A. Russell, describes emotions through three dimensions: Pleasure, Arousal, and Dominance. This model quantifies emotional states, making it valuable in psychology, user experience design, and AI. Specifically, the Arousal dimension measures how energized or calm one feels, reflecting the activation level of an emotion. We adapt this concept to model narrative momentum-story tension.\nConsidering the image sequence-generating system's future expansion and possible integration with a more extended narrative, we employed a sentiment analysis model for sentences-Roberta base model for emotion classification, fine-tuned on the GoEmotions dataset [22]. We"}, {"title": "Algorithm 3 Mapping Sentiment Labels with Emotion Labels", "content": "1: procedure ESTIMATING AROUSAL LEVELS\n2: E\u2190 emotion labels from the PAD emotion state model, where [high, medium, low]\narousal level maps to [1, 0, -1].\n3: S sentiment class labels from the ROBERTa model.\n4: Sdis Distance matrix for distance between labels in S and labels in E.\n5: loop: over S, compute the distance to each element in E, and get Sdis\n6: loop: over each row in Sdis, flat the vector except the minimum two elements in the\nrow.\n7: loop: over each row in Sdis, use Sdis/sum(Sdis) as the weight multiply with E to\nget S_arousal.\n8: normalize S_arousal to [-1,1]\n9: return S_arousal\n10: end procedure"}, {"title": "Action Mapping", "content": "By mapping arousal scores with actions, the editing layer integrates the narrative arc and actions by referencing these scores. Using the curve from the narrative arc, the layer calculates changes along the curve and selects actions that best fit these changes. Additionally, it sets a likelihood tolerance with the probability of actions, expanding potential narrative diversity. The process is described below:"}, {"title": "Algorithm 4 Narrative Arc Mapping", "content": "1: procedure MAPPING ACTIONS TO FIT NARRATIVE ARC\n2: P\u2190 input the current panel sequence.\n3: NET get the action causal graph network.\n4: ACT \u2190 creates a value dictionary for actions according to the arousal scores.\n5: ARC get referenced Narrative Arc.\n6: loop: over P, check the characters' actions in the next panel and compute the score\ndifference, according to ARC\n7: Select actions in likelihood from ACT and NET\n8: loop: revise action selection other panels according to NET\n9: return P\n10: end procedure"}, {"title": "2.2.3. Panel Relations", "content": "Panel transitions refer to the changes in content between consecutive panels. McCloud proposed six categories of transition types to model various aspects of content change[2], while Cohn introduced conjunction schemes to capture more complex panel transitions [16]. Our system combines these theories to modify the visual composition of comic panels-how elements are arranged-and reflect content changes according to the transition types. The narrative goal of this editing layer is to arrange the panel transitions in the generated comic sequence to create dynamic viewport changes and increase tension.\nHere is how we map the transitions with panel content changes:\n\u2022 Action: McCloud's action-to-action transition indicates changes in actions between consecutive panels. We use this transition to guide the selection of different character actions in the next panel.\n\u2022 Scene: McCloud's scene-to-scene transition indicates changes in scenes between consecutive panels. We use this transition to guide the change of location where the character's action occurs in the next panel.\n\u2022 Object: The object-to-object transition indicates a shift in focus from one object to another. We use this transition to guide the focus to different objects.\n\u2022 Addition: Cohn's additive conjunction involves panels that add information or detail to the ongoing story. We use this transition to introduce new objects into the panels.\nAlternation: Cohn's alternating conjunction presents alternative scenarios or actions, offering possibilities within the narrative. Compared to scene transitions, we use this transition to guide panels to alter most elements while maintaining consistent characters."}, {"title": "3. Usage Explanations and Showcases", "content": "This section introduces our system's application programming interface (API), and graphical user interface (GUI). The API functions are in Table 3. The system was constructed using several core classes, with the Parameter, Layer, and AttributeNode classes being the most crucial. The Parameter class manages all the registers of models and scripts. The AttributeNode class serves as the fundamental component of the graph model, representing the entire sequence. The Layer class is the parent class for all editing layer scripts. To execute modifications, the apply methods within the Layer class must be overridden and then executed by the Generator module.\nThe GUI screenshot is Figure 3. The two columns on the left display the image inputs for the character and scene of the comic sequence. Users can generate these images using the diffusion model or import them from their work. The column on the right side contains buttons that link to the imported scripts, triggering functions to apply modifications and generate results. The large area in the middle shows the currently generated result and allows users to select comic panels and the elements within them."}, {"title": "3.1. Showcases", "content": "Table 4 compares two sets of generated results, one before and one after applying the changes implemented in the current version. The first set, generated with user-imported images and all editing layers turned off, features content with default panel composition and randomly chosen characters' actions. After partial redrawing with the stable diffusion model, the second set demonstrates the results, followed by an editing layer that uses the sentiment language model to achieve narrative planning. It includes a partially redrawn icon and scene using a diffusion model from an Einstein head icon and a WindowsXP desktop photo as inputs. In addition, based"}, {"title": "3.2. Data Availability", "content": "The data and code used in this study are openly available in a GitHub repository. The repository includes the raw data, processed data, and all scripts necessary to reproduce the analyses presented in this paper. You can access the repository at https://github.com/RimiChen/Collaborative_Comic_Generation. The repository is licensed under MIT License, allowing for reuse and modification with appropriate attribution."}, {"title": "4. Conclusion and Future Work", "content": "This paper presents an extensible system for generating comic-style visual narratives, integrating narrative theory with human-AI collaboration. We address challenges in visual modification and plotline planning, balancing automation with user control for customization.\nWhile the current system effectively integrates abstract narrative theories, it has limitations in coordinating visual components and diversifying narratives. Separating visual layers, though beneficial for user modifications, reduces scene and character interaction, limiting cohesive artwork and rich actions. Additionally, using symbols to represent actions restricts narrative diversity, requiring user customization to expand content. Integrating advanced language models could enhance narrative richness\nFuture work will focus on refining the integration of narrative and visual components and exploring advanced models to enhance the system's capability to handle diverse narratives. We also plan to conduct user studies to evaluate effectiveness and usability across different groups. In conclusion, our work advances human-AI cooperative visual narrative generation, offering a versatile platform for creating engaging experiences."}]}