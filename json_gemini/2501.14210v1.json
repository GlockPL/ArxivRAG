{"title": "PuzzleGPT: Emulating Human Puzzle-Solving Ability for Time and Location Prediction", "authors": ["Hammad Ayyubi*", "Xuande Feng*", "Junzhang Liu*", "Xudong Lin", "Zhecan Wang", "Shih-Fu Chang"], "abstract": "The task of predicting time and location from images is challenging and requires complex human-like puzzle-solving ability over different clues. In this work, we formalize this ability into core skills and implement them using different modules in an expert pipeline called PuzzleGPT. PuzzleGPT consists of a perceiver to identify visual clues, a reasoner to deduce prediction candidates, a combiner to combinatorially combine information from different clues, a web retriever to get external knowledge if the task can't be solved locally, and a noise filter for robustness. This results in a zero-shot, interpretable, and robust approach that records state-of-the-art performance on two datasets \u2013 TARA and WikiTilo. PuzzleGPT outperforms large VLMs such as BLIP-2, InstructBLIP, LLaVA, and even GPT-40, as well as automatically generated reasoning pipelines like VisProg(Gupta and Kembhavi, 2022), by at least 32% and 38%, respectively. It even rivals or surpasses fine-tuned models.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Vision-Language (VL) research have led to models that perform impressively (Zhu et al., 2023; Li et al., 2022; Lu et al., 2022a; Alayrac et al., 2022; Team et al., 2024) on a variety of tasks such as GQA (Hudson and Manning, 2019), VQA v2 (Antol et al., 2015), VCR (Zellers et al., 2019), OK-VQA (Marino et al., 2019), Science-QA (Lu et al., 2022b), visual entailment (Xie et al., 2019), Chain-of-thought reasoning (Lu et al., 2024, 2022b). These tasks primarily assess one of, or at most a combination of, perception, reasoning, and outside knowledge retrieval abilities. For example, OK-VQA requires perception and outside knowledge retrieval, and GQA and VCR require perception and commonsense reasoning.\nHowever, humans seamlessly integrate a variety of skills - perception, reasoning, knowledge retrieval, and common sense to solve complex, multi-step problems. Tasks and benchmarks that test these diverse skills are crucial for developing models that approach human-level reasoning. The task of time and place reasoning from images proposed by TARA (Fu et al., 2022) takes a step closer to this goal. It demands a mix of perception, reasoning, combinatorial, and outside knowledge retrieval abilities over multiple steps. It is like solving a puzzle. For example, in Figure 1, it is required to detect entities such as Times Square and visual text, \u201cJustice for George Floyd\". Then, a reasoner needs to deduce possible location (New York/United States) and time candidates (post-2000, 2020-2021) from these clues. Next, these candidates need to be combined in various ways to find a candidate at the intersection of all candidates (post 2000 \u2229 2020-2021 = 2020-2021). Finally, if the answer is still unclear (2020-2021), a web search is required using the deduced information.\nThe practical applications of the task stem from its focus on images depicting events that occurred at a specific location and time. This has incredibly useful applications, such as timeline construction, and stitching together news stories from online pictures and social media posts.\nExisting works take a direct approach to this nuanced problem. TARA tries to supervise a model directly to predict location and time directly. The hope is that the model will learn to identify time and location discriminative clues implicitly, given appropriate supervisory signals. While the approach might have worked for a limited time and location candidates, the real scenario of hundreds of locations/time with fine differences makes this approach unscalable, and thus impractical. QR-CLIP (Shi et al., 2023) additionally tries to incorporate external knowledge in the learning process. However, it oversimplifies the problem and assumes mere re-"}, {"title": "2 Related Work", "content": "Vision-Language Models. Recently, VLMs (Radford et al., 2021; Alayrac et al., 2022; Li et al., 2023; Liu et al., 2023b) have demonstrated remarkable multimodal capabilities through large-scale vision-language training. One family of VLMs such as CLIP (Radford et al., 2021) typically trains a visual encoder and a text encoder to map visual and text input into a common embedding space. The resulting visual encoders are widely adopted to extract visual features that are fed to LLMs in the other family of work (Alayrac et al., 2022; Li et al., 2023; Liu et al., 2023b; Lin et al., 2023). For example, LLaVA takes CLIP's visual feature as input and is trained to generate target text. These VLMs with text-generation abilities have achieved superior performance on vision-language datasets.\nVisual Reasoning Datasets. Early work like VQA (Antol et al., 2015) mainly probes perception more than reasoning abilities, while datasets like"}, {"title": "3 Methodology", "content": "We propose PuzzleGPT to emulate human-like puzzle-solving ability. It consists of an expert pipeline consisting of specific modules that represent distinct skills, as illustrated in Figure 2. In this section, we describe each of the modules in detail.\nPerceiver. The perceiver processes visual signals. Given an image, it will scan the image to find entities of interest, such as celebrities, text, landmarks, or other types of keywords. In our framework, the Perceiver is a frozen VLM that is prompted with a respective query to extract each entity. For example, text from images is extracted with the query, \u2018What are the words shown in this image? Short Answer:'. By finding the entities, the Perceiver can focus on patches containing specific entities and reason independently. This enables it to generate specific textual knowledge about the entities (for a landmark, its name; for text, its Optical Character Recognition; and so on). We use BLIP-2 as the Perceiver in this work.\nReasoner. Reasoner in PuzzleGPT is an LLM"}, {"title": "4 Experiments", "content": "In this section, we report our results on two datasets: TARA and WikiTilo. We have run all the experiments on a single Nvidia RTX 4090 24 GB GPU.\n4.1 TARA\nTARA is sourced from the New York Times and requires time and location prediction for images depicting newsworthy events. In total, there are around 1.5K samples in the test and validation set. The label set is open-ended with a unique label for each sample.\nMetric\nThe open-ended nature of labels in TARA makes evaluation challenging. Two metrics were proposed originally - Accuracy and Example-F1. Accuracy measures the exact match of the prediction with the label. While this works for time evaluation where the labels are properly formatted (YYYY-MM-DD), location evaluation leads to unreliable results as the labels are highly unstructured. As illustrated in Fig Figure 3 in addition to city, country, and country, some labels contain additional information such as Pin Code, county name (Kings County), and geographical area name (Metropolitan France). This causes even correct predictions to be incorrectly classified as wrong. To address this, we standardize all locations into city, country, and continent using GeoPy 1. Further, if the label contains a specific area within a city (e.g. Times Square or Central Park), we keep that to not lose location precision. We use these formatted labels for measuring accuracy and call it Standardized Accuracy (Std. Acc).\nTo measure partial correctness only correct year or only correct continent and country \u2013 TARA proposes Example-F1 metric. It is defined as follows:\nExample-F1 = 2|GT \u2229 Pred| / |GT| + |Pred|\nAs the score is inversely dependent on |Pred|, shorter predictions are unduly rewarded. For example, a model that predicts only year scores abnormally high Example-F1. We mitigate this bias by adding a brevity penalty, following NLP literature (Papineni et al., 2002):"}, {"title": "5 Conclusion", "content": "This work proposes a modular and iterative puzzle-solving method \u2013 PuzzleGPT \u2013 for predicting time and location from images. It consistently outperforms current SOTA VLMs on TARA and WikiTilo. The extensive experiments and ablations demonstrate and analyze its capabilities. Further, we plan to release the code upon acceptance in an effort to increase community engagement.\nLimitations\nWhile PuzzleGPT demonstrates strong performance on time and location prediction tasks like TARA and WikiTilo, it's important to acknowledge its limitations. The model's architecture is specifically tailored for puzzle-like reasoning scenarios, and its performance on tasks with different structures or knowledge requirements remains unexplored. Furthermore, the current reliance on GPT for reasoning introduces dependencies on proprietary models, potentially limiting accessibility and introducing inherent biases. Future work will explore alternative reasoning modules and evaluate PuzzleGPT's generalization ability across diverse visual reasoning tasks."}, {"title": "A Prompts Details", "content": "We list the most prominent prompts used by us in our experiments.\nA.1 Perceiver Prompts\nKeywords\nUSER: <image>\nWhat are the keywords of this image?\nAnswer with the keywords only and separate with commas.\nShort Answer: ASSISTANT:\n\"\"\"\nText OCR\n\"\"\"\nUSER: <image>\nWhat are the words shown in this image.\nShort Answer: ASSISTANT:\n\"\"\"\nCelebrity Once we locate a people/person entity, we check if it's a celebrity. If it is, then who.\nCheck Celebrity\n\"\"\"\nUSER: <image>\nAre you confident that this person is famous?\nAnswer with yes or no: ASSISTANT:\n\"\"\"\nGet Celebrity Name\n\"\"\"\nUSER: <image>\nWhat is the name of this celebrity that you are most confident with?\nASSISTANT:"}, {"title": "A.2 Reasoner Prompts", "content": "Concrete Keywords\n\"\"\"\n\"{event}\"\n\"{keywords}\"\nAbove texts are describing an same event. Please conclude the event with no more than 5 words. Keep your answer informative, short and concise.\nShort Answer:\n\"\"\"\nGet Location Candidate\n\"\"\"\n\"{event}\"\n{keywords}\"\nDo any of the above text contains location information such as city country or continent? If yes, please answer that city, country or continent. Otherwise, answer No.\nShort Answer:\n\"\"\"\nGet Time Candidate\n\"\"\"\n\"{time_clue}\"\nGiven above information, please ground a specific year or month or date from it and answer with it only. If you can not find a date, answer No.\nShort Answer:"}, {"title": "A.3 Noise Filter", "content": "Check Confidence of Location\n\"\"\"\nUSER: <image>\nAre you confident this image was taken in {loc}?\nShort Answer: ASSISTANT:\n\"\"\"\nCheck Confidence of Date\n\"\"\"\nUSER: <image>\nAre you confident this image was taken on {date}?\nShort Answer: ASSISTANT:"}, {"title": "B Open-Source LLama as the Reasoner", "content": "We seek to understand what would happen if we used open-sourced LLMs as the Reasoner instead of proprietary GPT models. As such, we compare a state-of-the-art LLaMA 3.1 (Touvron et al., 2023) against ChatGPT. We couldn't compare it against the larger LLaMA version due to limited compute availability. As shown in Table 8, Llama 3.1 performs poorly against ChatGPT. ChatGPT as a reasoner achieves 0.30 and 22.99 in Time and Location (standardized) accuracy, and X-F1\u1e9e of"}, {"title": "C Perceiver Ablation", "content": "We ablated the performance of our BLIP-2 perceiver by replacing it with LLaVA. The results are shown in Table 7. Using BLiP2 as the perceiver outperformed using LLaVA, especially on location scores. This might be due to LLaVA's worse performance on location reasoning compared to BLiP2. For time Example-F1, using LLaVA as perceiver scored 43.46 with a brevity penalty, which is still better than the LLaVA baseline. This suggests using different backbones as the perceiver will inherently affect the models' output nature but generally elevate the performance compared to the backbone."}, {"title": "D Experiment on a Small Clean Subset", "content": "One potential reason for the VLMs' consistently low performance on the TARA dataset could be its inherent difficulty, even for humans, in inferring the time and location from the images. To explore this, we manually selected 50 data points where the images were considered informative and indicative of time and location. We then conducted experiments on this subset. The results, shown in Table Table 9, demonstrate a significant performance improvement for our method, while BLIP2 and LLaVA did not show similar improvements. This suggests that although some data points in TARA"}, {"title": "E Qualitative Analysis", "content": "We conducted case studies on TARA for qualitative analysis to further demonstrate PuzzleGPT's effectiveness. In Figures 10, and 11, we show some additional positive and negative case studies of our system. Our analysis of failure cases led us to observe that failure typically occurs when the given image lacks unique landmarks, events, or people. For example, a generic image showing a crashed plane (Figure 10 top) gives little clue about the specific location. Or a politician speaking at a press conference (Figure 10 bottom) provides little to no evidence to deduce time."}, {"title": "F Additional WikiTilo Baselines", "content": "We compare PuzzleGPT to additional Multimodal LLMs-based baselines in Table 10. We find that PuzzleGPT consistently performs better than these additional baselines as well on all categories. Only LLaVA performs marginally better on Region Acc"}, {"title": "G Ablation Metrics", "content": "The accuracy metric measures the exact matching of open-ended labels in TARA. As such, all models record lower accuracy as compared to the more relaxed F1-based metric (as illustrated in Table 1, Table 2 and the original TARA dataset paper). The minor difference in accuracy metric makes ablations tricky and hard to assess. The strengths/weaknesses of our components are better reflected in the F1-based metric. As such, we report F1-based metrics for all our ablations."}]}