{"title": "Associative memory inspires improvements for in-context\nlearning using a novel attention residual stream architecture", "authors": ["Thomas F Burns", "Tomoki Fukai", "Christopher J Earls"], "abstract": "Large language models (LLMs) demonstrate an impressive ability to utilise information\nwithin the context of their input sequences to appropriately respond to data unseen by\nthe LLM during its training procedure. This ability is known as in-context learning (ICL).\nHumans and non-human animals demonstrate similar abilities, however their neural archi-\ntectures differ substantially from LLMs. Despite this, a critical component within LLMS,\nthe attention mechanism, resembles modern associative memory models, widely used in\nand influenced by the computational neuroscience community to model biological memory\nsystems. Using this connection, we introduce an associative memory model capable of per-\nforming ICL. We use this as inspiration for a novel residual stream architecture which allows\ninformation to directly flow between attention heads. We test this architecture during train-\ning within a two-layer Transformer and show its ICL abilities manifest more quickly than\nwithout this modification. We then apply our architecture in small language models with 8\nmillion parameters, focusing on attention head values, with results also indicating improved\nICL performance at this larger and more naturalistic scale.", "sections": [{"title": "Introduction", "content": "Transformers (Vaswani et al., 2017) are a popular and performant class of artificial neural networks. Large\nLanguage Models (LLMs), decorated exemplars of Transformers, have demonstrated impressive capabilities\nin a wide array of natural language tasks (Brown et al., 2020). One particularly notable capability, known as\nin-context learning (ICL), has gained significant attention. ICL, as witnessed in LLMs, occurs when a model\nappropriately adapts to tasks or patterns in the post-training input data which was not provided during\nthe model's training procedure (Lynch & Sermanet, 2021; Mirchandani et al., 2023; Duan et al., 2023).\nThis ability to immediately learn and generalise from new information, especially with only a single or few\nexposures to new information, is a hallmark of sophisticated cognitive abilities seen in biological systems;\nhuman and non-human animals are capable of rapidly adapting their behaviour in changing contexts to\nachieve novel goals (Miller & Cohen, 2001; Ranganath & Knight, 2002; Boorman et al., 2021; Rosenberg\net al., 2021), and can infer and apply previously-unseen, and even arbitrary rules without significant learning\nperiods (Goel & Dolan, 2000; Rougier et al., 2005; Mansouri et al., 2020; Levi et al., 2024). Developing\nmodels which connect our understanding of common mechanisms underlying these related phenomena in\nboth artificial and natural intelligence may provide valuable insights for developing more adaptive and\nversatile language models, in addition to helping us more deeply understand the brain.\nWhile various explanations have been proposed for how LLMs learn to perform ICL (Olsson et al., 2022;\nVon Oswald et al., 2023; Li et al., 2023; Reddy, 2024), there has been little work to develop explanations which\nalso offer neurobiologically-plausible models of similar abilities seen in humans and non-human animals. One\nexception to this is the recent work of Ji-An et al. (2024), which illustrates a connection between ICL in LLMs\nand the contextual maintenance and retrieval model of human episodic memory from psychology. This model\nproposes that memory items are stored as a contextual composition of stimulus- and source-related content\navailable during, and near-in-time, to a memory item's presentation. The model comports with several\nreported psychological phenomena in humans (Lohnas et al., 2015; Cohen & Kahana, 2022; Zhou et al.,\n2023), and can be constructed using a Hebbian learning rule (Howard et al., 2005). Hebbian learning (Hebb,\n1949), that neurons which \u2018fire together, wire together', is foundational to the theoretical underpinnings of\nassociative memory models (Nakano, 1972; Amari, 1972; Little, 1974; Hopfield, 1982), which propose that\nmemory items are stored by strengthening the connections between neurons which become activated during\nand near-in-time to the stimulus corresponding to memory items' presentations. How associative memory is\nneurophysiologically implemented is well-studied (Amit, 1990; Buzs\u00e1ki, 2010; Khona & Fiete, 2022; Burns\net al., 2022), and this is complemented by a well-developed theoretical literature, including work noting the\nclose resemblance to a core ingredient of LLMs, the attention mechanisms of Transformers (Ramsauer et al.,\n2021; Bricken & Pehlevan, 2021; Kozachkov et al., 2022; Burns & Fukai, 2023; Burns, 2024).\nGiven these existing links, further developing connections between the framework of associative memory and\nICL may offer deeper insights or improvements. In the following sections, we:\n\u2022\nintroduce an associative memory model which can perform ICL on a classification task, and which\nanalogously allows attention values to directly take on input data;\n\u2022 using the same task, and inspired by our explicit associative memory model, show how creating a\nresidual stream of attention values between attention heads in a two-layer Transformer speeds-up\nICL during training (compared to applying the same technique to queries or keys); and\n\u2022\ndemonstrate that na\u00efvely applying the same idea in small language models (LMs) indicates ICL\nperformance improvements scale to larger models and more naturalistic data.\nA central theme in our innovation is a focus on the role of values in the attention mechanism, and archi-\ntecting a simple 'look-back' method in the form a residual connection of values. Residual connections, also\nknown as 'skip' or 'shortcut' connections, can be described as those which connect neurons which are other-\nwise indirectly connected through a more prominent pathway. First identified in experimental neuroscience\n(Lorente de N\u00f3, 1938) and considered since the dawn of theoretical neuroscience and artificial neural net-\nworks (McCulloch & Pitts, 1943; Rosenblatt, 1961), researchers continue to find residual connections useful\nin modern applications (Dalmaz et al., 2022; Huang et al., 2023; Zhang et al., 2024). A noticeable feature\nof Transformers is its use of the so-called residual stream, wherein data, once processed by the attention\nand feedforward layers, is added back to itself. What can therefore be considered a \u2018cognitive workspace'\n(Juliani et al., 2022) has been shown to contain rich structure, amendable to popular (Elhage et al., 2021)\nand emerging (Shai et al., 2024) interpretability methods. Our work illustrates that specific additional resid-\nual connections can lead to enhanced performance in ICL tasks, and we speculate it may also aid in future\ninterpretability efforts."}, {"title": "ICL classification with a one-layer associative memory network", "content": "Let $X \\in \\mathbb{R}^{e \\times s}$ be the input sequence data, where e is the dimension of each token embedded within a suitable\nlatent space, and s is the number of tokens in the sequence. Each token is considered either an object, o, or\nlabel, l. Input X consists of a sequences of multiple pairs of objects and labels. We say a pair of tokens is\na contiguous sub-sequence of two column vectors from within X, consisting of one object token followed by\none label token. For example, the j-th pair X' \u2208 Re\u00d72 consists of one object token $o^j \\in \\mathbb{R}^e$ followed by one\nlabel token $l^j \\in \\mathbb{R}^e$. Our input sequence X will therefore consist of multiple pairs, and each pair may appear\nmore than once. The final token of a sequence, denoted as xs, corresponds to the label token of the last pair,\nwhich itself has appeared at least once prior to this final instance in X. However, the true label token data\nof this final label token is replaced with the zero vector, and the network's task is to correctly predict this\ntrue label token given the previous in-context instance of the tokens' data (as illustrated in Figure la).\nThe token embeddings, representing objects and labels, follows Reddy (2024), where all token embeddings\nare drawn from similar statistical distributions. For every instance of a pair, label token embeddings comes\nfrom fixed vectors, whereas object token embeddings are constructed from combinations of fixed and random\nvectors. Each label token $l^i$ is an e-dimensional vector $\\mu_l^i$, whose components are i.i.d. sampled from a\nnormal distribution having mean zero and variance 1/e. Each object token embedding, o', is given by\n$\\frac{\\mu_o^i + \\epsilon \\eta}{\\sqrt{1 + \\epsilon^2}}$,\nwhere $\\mu_o^i$, which is fixed across all instances in X of the pair, and \u03b7, which is drawn randomly for each\ninstance in X of the pair, are e-dimensional vectors whose components, like $\\mu_l^i$, are i.i.d. sampled from a\nnormal distribution having mean zero and variance 1/e. The variable \u025b controls the inter-instance variability\nof objects and, in the following, is set to 0.1 unless otherwise stated. This means that the final, tested\ninstance of a pair has, as its object token, a slightly different appearance than the previous instance(s) seen\nin X and is not a perfect match, where $\\mu_o^i$ provides the commonality between these variations. Adding\nthese variations makes the task less trivial and slightly more naturalistic.\nWe now show it is possible to perform ICL with such pairs in a single forward step of a one-layer associative\nmemory network, written in the language of a single Transformer attention head (Vaswani et al., 2017).\nIn Transformers, each attention head consists of learnt parameters \u2013 weight matrices $W^q, W^k \\in \\mathbb{R}^{k \\times e}$ and\n$W^v \\in \\mathbb{R}^{v \\times e}$ - with which, when taken together with the input data sequence X, we calculate the queries Q,\nkeys K, and values V matrices using\n$Q = W^q X, K = W^k X,$ and $V = W^v X$.\nThe values kandvare the reduced embedding dimensions for the attention operation (i.e., to facilitate\nmulti-headed attention, etc.). Here we use k = v. As in the input data, e is the dimension of the unreduced\nembedding space of the tokens.\nIt is then useful to define the SOFTMAX function for matrix arguments. For a matrix $M \\in \\mathbb{R}^{c \\times r}$, we write\n$t_i := M[i, :] \\in \\mathbb{R}^r$ for the i-th row and $t_j := M[:, j] \\in \\mathbb{R}^c$ for the j-th column. Then, we define the SOFTMAX\nfunction for a matrix M as $SOFTMAX(M)[t_i, t_j] := \\frac{exp(M[t_i, t_j])}{\\sum_j exp(M[t_i, t_j])}$, where i and j are the vector component\nindices. We use this to compute attention-based embeddings of the data X, denoted by $\\overline{X}$, as\n$\\overline{X}=SOFTMAX(\\frac{K^T Q}{\\sqrt{k}})V$\nin which we refer to the term $K^T Q$ as the scores $S \\in \\mathbb{R}^{s \\times s}$. In Transformers and Transformer-based models\nsuch as LLMs, this new data $\\overline{X}$ is then recombined with data from other attention heads, before passing\nthrough a multi-layer perceptron. Layers of attention heads and multi-layer perceptrons are stacked atop\none-another to perform increasingly sophisticated computations.\nIn our associative memory model, which we call Associative Memory for ICL (AMICL), we take inspiration\nfrom the transformations applied to the input data X to create the basis of what can be considered (Ramsauer\net al., 2021) as an associative memory update step in Equation 1. Instead of using parameterised values, we\nuse a simple set of assignments for each token's key, query, and value vectors. For all embedded tokens i < s,\nwe set $k_i = q_i = \\frac{a x_{i-1}+x_i}{a+1}$ as the keys and queries, where lowercase Latin letters denote indexed column\nvectors, taken from the matrices that are denoted with uppercase Latin letters. For simplicity, we wrap the\nindices along the token sequence such that the first token, x1, and the last token, xs, are used to generate\nthe sub-sequence $(x_1, x_s)$, i.e., we assign token index 0 to s. For tokens, with $x_s$ as the token column vector,\nwe set $q_s = \\frac{a x_{s-1}+x_1}{a+1}$ as the query and $k_1$ as the zero vector, which acts as its key. For all tokens, the values\ncolumn vector is equal to the token column vector, i.e., $v_j = x_j$. The value of a can be any arbitrary positive\nreal value, but, after testing within the range [0, 2], is set as a = 2."}, {"title": "Residual attention streams in a two-layer Transformer", "content": "Following the path of information from the input to the queries, keys, and values in the AMICL model (Figure\n2a), we can see that the queries Q and keys K flow from a shared function f of the input X. Whereas, the\nvalues V are given directly by the input X. Seen through the lens of the traditional self-attention mechanism,\nconstruction of the AMICL model in Section 2 can be interpreted as implying that creating the values V in\na way which more explicitly retains the prior input X can facilitate ICL. Inspired by this, we introduce a\nresidual connection between the values data of successive layers in the Transformer, which we call a residual\nvalues stream and, more generally, a residual attention stream applied to values (Figure 2b).\nAs an alternative source of intuition, one can consider the informal interpretation of self-attention as con-\nsisting of: queries 'asking each token a question about other tokens'; keys 'responding to each token with\nthe answer to the queries' questions', i.e., they align with or 'attend to' the queries; and values giving the\n(weighted) answers to those questions, which are operationalised as small additions to the current token\nvectors. Within this informal conceptual model, we can consider our residual attention streams as retaining\nadditional 'look-back' information in the answers.\nOur residual attention stream also generates additional gradient signals during training, which could them-\nselves be beneficial for the network to develop ICL capabilities. We therefore also test the same residual\nstream architecture, separately, for queries and keys. In principle, it is also possible to apply the residual\nstream architecture to combinations of queries, keys, and values. But, to maintain a stronger connection to\nthe AMICL model, we focus on testing the residual value stream separately, as well as the keys and queries,\nagain separately, for comparison.\nWe train classic and modified versions of a two-layer Transformer on the same task described in Section\n2. The common architectural features between the two versions consist of two single-head attention layers\nfollowed by a three-layer multi-layer perceptron with 128 ReLU neurons followed by a softmax layer to give\nprobabilities over the I labels. The network is trained with the same task as the AMICL model, using the\ncross-entropy loss between the predicted and actual final label in token 25. Within the modified architecture,\nwe add the first attention head's queries, keys, or values to the second attention head's queries, keys, or values,\nrespectively. More formally, in our modified version of the Transformer architecture, we calculate the first\nattention layer as\n$Q_1 = W_1^q X, K_1 = W_1^k X,$ and $V_1 = W_1^v X,$\nand then, in the second attention layer, for a residual queries stream we calculate\n$Q_2 = W_2^q X + Q_1, K_2 = W_2^k X,$ and $V_2 = W_2^v X,$\nor for a residual keys stream we calculate\n$Q_2 = W_2^q X, K_2 = W_2^k X + K_1,$ and $V_2 = W_2^v X,$\nor for a residual values stream (shown in Figure 2b) we calculate\n$Q_2 = W_2^q X, K_2 = W_2^k X,$ and $V_2 = W_2^v X + V_1$.\nAs in Reddy (2024), while we train on the originally-described ICL task, we create supplemental tasks\nwhich are not used for training but rather act as proxy measurements of progress for different computational\nstrategies for completing the task: training data memorisation and ICL capability generalisation. Namely,\nthese supplemental tasks are:\n\u2022\nIn-weights (IW): a series of object-label pairs is presented where the final pair is not found within\nthe prior context but is present in the training data;\n\u2022\nIn-context (IC): a series of novel object-label pairs, not seen in the training data (i.e., an entirely\nre-drawn set of \u03bcoi and \u03bc\u03b9i values) but following exactly the same statistical structure, is presented;\nand\n\u2022\nIn-context 2 (IC2): a series of object-label pairs are presented, where the objects are found in the\ntraining data but have been assigned new labels (i.e., the objects retain their \u00b5oi values but the\nlabels have their \u00b5\u03b9i values re-drawn).\nThe reduced embedding dimensions of the attention operation are both set to 128, i.e., k,v = 128. Each\nnetwork architecture was trained on four random seeds, with a batch size of 128, vanilla stochastic gradient\ndescent, and a learning rate of 0.01.\nTo quantify these shifts, we report statistics in Table 1 of when the first training snapshots we recorded\nreached an accuracy threshold of > 0.95. We find the values stream networks perform best, reaching the\nsame level of accuracy as the classic (unmodified) networks with ~ 24% fewer training steps. The same result\nis also seen at lower thresholds . We also performed t-tests at the 0.95 accuracy threshold, which showed significant differences between the\nperformance of the classic (unmodified) network and the residual queries and values stream networks on\nthe IC and IC2 tasks (p < 0.01, t < -3.9), but not between the classic and residual keys stream networks\n(p > 0.39, t > -1.0). The residual values stream networks also perform significantly better (p < 0.03,\nt < -3.0) than all other networks on both the IC and IC2 task, except the residual queries stream network\nfor the IC task (p = 0.06, t = -2.33)."}, {"title": "Residual value streams in a small language model", "content": "To provide an initial indication as to whether the benefit for ICL performance seen in Section 3 scales to\nlarger models and naturalistic data, we also test the residual value stream architecture by training small\nLMs. The Transformer-based model contains approximately 8 million parameters spread over eight layers,\neach with 16 attention heads. The context window is 256 tokens long and dimension of the model is 256.\nNext-token prediction is trained for three epochs on the Tiny Stories dataset, such that they can generate\nsimple and short children's stories (Eldan & Li, 2023). Each network architecture was trained in three\nseparate instances, using different random seeds for each instance but controlling for randomness between\narchitectures by re-using the same set of seeds for the two architectures.\nWe implement the residual value stream in a na\u00efve way: in each but the first layer, the values of each\nattention head receives an additional input of the values from the attention head with the same index in the\nprevious layer. Although there are alternative implementations of such residual streams, we chose a na\u00efve\napproach and straightforward approach to provide an initial indication of the potential scalability of this architectural\nchange and to not alter the number of learnt parameters between the networks.\nThe residual values stream networks achieved 1.65\u00b10.05 training loss and 1.55\u00b10.01 validation loss, slightly\nlower than the training and validation losses for the classic networks, which were 1.67\u00b10.05 and 1.56\u00b10.01,\nrespectively. Measured by wall clock computation time, the residual values stream networks also took\nslightly longer to train, 30.18\u00b10.49 hours compared to 29.12\u00b10.40 hours for the classic networks. This\nadditional computation time is attributable to the additional gradient computations required by the 16\nadditional residual streams connecting the attention head values at each but the first layer.\nAs a proxy evaluation of the ICL ability of each model in natural language, we utilise a simple indirect object\nidentification (IOI) task. In natural language, a direct object is the noun that receives the action of the verb\nand an indirect object is a noun which receives the direct object, e.g., in the sentence \"A passed B the ball\":\n\"passed\" is the verb; \"A\" is the subject; \"the ball\" is the direct object; and \"B\" is the indirect object. In\nthe IOI task, we test for correct completion of sentences like \"When A and B were playing with a ball, A\npassed the ball to\", where \"B\" is the indirect object and is considered the correct completion.\nGPT-2 Small can perform instances of the IOI task, and the circuit responsible for this ability has been\nidentified (Wang et al., 2023). As shown in this section, our small LMs have some measurable ability to\ncomplete instances of this task, and so we use this to compare the performance of the classic (unmodified)\nand residual value stream architectures.\nFor the IOI task, we tested the following sentences:\n1. \"When John and Mary went to the shops, * gave the bag to\", where * was either \"John\" or \"Mary\",\nwith correct completions \"Mary\" and \" John\", respectively.\n2. \"When Tom and James went to the park, * gave the ball to\", where * was either \"Tom\" or \"James\",\nwith correct completions \"James\" and \"Tom\", respectively.\n3. \"When Dan and Emily went to the shops, * gave an apple to\", where * was either \"Dan\" or \"Emily\",\nwith correct completions \"Emily\" and \"Dan\", respectively.\n4. \"After Sam and Amy went to the park, * gave a drink to\", where * was either \"Sam\" or \"Amy\",\nwith correct completions \" Amy\" and \"Sam\", respectively.\nFor each sentence and variation thereof (swapping the identities of the subject and indirect object, as indi-\ncated by the * symbol above), we recorded the next token probabilities of the correct and incorrect names.\nAs summarised in Table 2, we find the classic networks are much less capable than the residual values\nstream networks across the four sentences, the classic networks correctly identify the indirect object with a\nprobability of ~ 7% while the residual values stream networks do so with a probability of ~ 41%, a ~ 5.9\u00d7\nimprovement. Similarly, the classic networks more regularly mistake the subject for the indirect object with\na probability of ~ 5% while the residual values stream networks do so with a probability of ~ 3%, a ~ 0.6\u00d7\nreduction."}, {"title": "Discussion", "content": "Our study builds upon the understanding of ICL in Transformer models by exploring connections to asso-\nciative memory models from computational neuroscience. We introduced AMICL, an associative memory\nmodel that performs ICL using an approach akin to a single-layer Transformer attention head but with an\nimplied residual stream from the inputs to the values. Inspired by this, we proposed a novel residual stream\narchitecture in Transformers, where information flows directly between attention heads. We demonstrate this\nin a two-layer Transformer, showing increased efficiency in learning on an ICL classification task. Moreover,\nby extrapolating this architecture to a small Transformer-based LM, we illustrated enhanced ICL capabilities\nare possible on a larger and more naturalistic scale.\nOur results offer potential insights for neural network design and our understanding of biological cognition:\n\u2022\nNeural network architecture. The simplicity of our biologically-inspired architectural modi-\nfication, attention residual streams, provides a promising extension for designing more adaptive\nTransformer models. Given one can interpret this change as providing additional shared workspaces\nacross model layers (in the form of the additional residual streams), our results can also be viewed\nas supporting cognitive functional specialisation and modularisation as being both natural and com-\nmonplace in neural systems (Kaiser & Hilgetag, 2006; Chen et al., 2013), which is also seen in cases of\nhighly distributed representations (Voitov & Mrsic-Flogel, 2022), of which natural language appears\nno exception (Mikolov et al., 2013; Hernandez et al., 2024).\n\u2022 Biological cognition. Drawing parallels between artificial networks and cognitive neuroscience\ntheories contributes to a deeper understanding of memory systems in biological entities. As we\nhave shown by our AMICL model, the associative memory framework is capable of ICL in a single\nlayer, with an implied 'skip connection' present between the input and the values. This suggests that\nsimilar types of connections may analogously exist in biological networks to adapt and generalize from\nlimited exposure. For instance, associative memory is often related to the hippocampal formation\n(Amit, 1989), an area in the brain important for memory and learning. An open question in this\narea of neuroscience is what the computational role of observed skip connections are. In particular,\nwhile some information in the hippocampus goes directly from area CA3 to CA1, another area\nCA2 is often skipped, but does receive input from CA3, which it then passes on to CA1 (Cui\net al., 2013). This is interesting not just anatomically, but also functionally, since CA2 is considered\nresponsible for specialised tasks and context-switching (Dudek et al., 2016; Robert et al., 2018).\nWhether our AMICL model or residual attention stream modification can be considered analogous\nto skip connections witnessed in the hippocampus remains to be studied. However, it offers a\npromising window, e.g., one could now study the effects of changing the number and quality of\nresidual attention streams, to test whether connection types or relative proportions similar to that\nseen in the hippocampus also show improved performance (for some types of data or tasks).\nIn establishing an additional potential mechanistic link between associative memory frameworks and ICL\ntasks, this work builds upon the broader biologically-inspired interpretation of Transformer attention mech-\nanisms (Ramsauer et al., 2021). Indeed, Zhao (2023) conjectures that LLMs performing ICL do so using\nan associative memory model which performs a kind of pattern-completion using provided context tokens,\nconditioned by the learnt LLM parameters. Using this perspective, Zhao (2023) constructs different token\nsequences to be used as contexts to prepend onto the same final token sequence representing the tested task.\nBy actively choosing context tokens which more 'closely' resemble the final tokens the LLM is being tested\nwith, the LLM shows improved task performance, presumably by utilising the increased relevancy of the\ncontext tokens for ICL. This perspective is further developed in Jiang et al. (2024), who show how LLMs can\nbe 'hijacked' by purposeful use of contexts with particular semantics. When LLMs such as Gemma-2B-IT\nand LLaMA-7B are given the context of \"The Eiffel Tower is in the city of\u201d, they successfully predict the next\ntoken as \"Paris\". However, Jiang et al. (2024) demonstrate that prepending the context with the sentence\n\u201cThe Eiffel Tower is not in Chicago.\u201d a sufficient number of times, these LLMs incorrectly predict the next\ntoken as Chicago\u201d. We may interpret these prepended data as acting (crudely) as 'distractors', and in the\nassociative memory sense as causing an over-activation of competing memory items which interferes with\naccurate task performance.\nBeyond associative memory, we also note potential connections with other computational neuroscience mod-\nels. In particualr, we observe that our model, AMICL, has some connection to successor representation,\nparticularly y-models, where the parameter a is somewhat similar to the y parameter (Janner et al., 2020).\nInterestingly, a variant of the temporal context model, which was recently compared to ICL in LLMs (Ji-An\net al., 2024), can be considered equivalent (Gershman et al., 2012) to estimating the successor representa-\ntion using the temporal difference method from reinforcement learning (Sutton, 1988). This suggests that\nparametrising our residual stream architecture, e.g., by providing weighted sums of previous-layer values\n(where the weights act like y variables) across multiple attention heads instead of the data from a single\nattention head with the same index, and training these parameters using a reinforcement learning algorithm,\ncould provide further enhancements.\nFuture work may seek to more deeply understand our results in the context of varying the structure and\norder of context tokens - in associative memory networks, and during inference and training of LLMs. Along\nthese lines, Russin et al. (2024) recently showed that LLMs and Transformers exhibit similar improvements\nas seen in humans on tasks with and without rule-like structures, depending on the order and organisation\nof context and training tokens. In particular, ICL performance improved when context tokens appeared in\nsemantically-relevant 'chunks' or 'blocks', as seen in humans when completing tasks with rule-like structures.\nWhereas, when training samples were interleaved, Transformers saw performance improvements (as measured\nby the extent to which the network rote-learnt training examples), similar to humans completing tasks lacking\nrule-like structures.\nThe performance improvements from introducing a residual values stream suggests that ICL can be thought\nof as an associative process where continuous information reinforcement enhances model memory, efficiency,\nand prediction accuracy on novel data. Nonetheless, this raises several questions. Firstly, more comprehen-\nsive testing on varied datasets and different scales of language models can address whether the improvements\nwe observed generalise beyond our specific tasks and data setups. Further, it remains to be clarified whether\nsimilar benefits manifest when dealing with other natural language processing tasks, such as sentiment anal-\nysis or translation, or whether there exist any trade-offs between ICL and other abilities. Additionally,\nwhile these initial results present a compelling case for testing attention residual streams in larger models,\nfurther exploration of optimisation parameter settings for such architectures would strengthen understand-\ning. Quantitative studies on computation cost versus accuracy improvements will also better-inform model\narchitecture design and selection for deployment in real-world scenarios, as well as potential competition\nbetween learning flexibility and task accuracy.\nIn conclusion, this research demonstrates potential conceptual and practical advancements in enhancing\nTransformers' adaptive capabilities through a biologically-inspired mechanism. By bridging neural compu-\ntational principles with associative memory insights, we offer new directions for research into more intelligent\nand dynamic models, with potential improvements for LLMs. Our associative memory model and Trans-\nformer architecture not only bolsters existing computational frameworks, but also offers fertile ground for\ncomputational neuroscientists to analyse the computational role of skip connections in memory systems. As\nwe progress, these interdisciplinary approaches may ultimately yield richer cognitive models that parallel, or\neven emulate, biological intelligence."}]}