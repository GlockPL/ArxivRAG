{"title": "On Information-Theoretic Measures of Predictive Uncertainty", "authors": ["Kajetan Schweighofer", "Lukas Aichberger", "Mykyta Ielanskyi", "Sepp Hochreiter"], "abstract": "Reliable estimation of predictive uncertainty is crucial for machine learning applications, particularly in high-stakes scenarios where hedging against risks is essential. Despite its significance, a consensus on the correct measurement of predictive uncertainty remains elusive. In this work, we return to first principles to develop a fundamental framework of information-theoretic predictive uncertainty measures. Our proposed framework categorizes predictive uncertainty measures according to two factors: (I) The predicting model (II) The approximation of the true predictive distribution. Examining all possible combinations of these two factors, we derive a set of predictive uncertainty measures that includes both known and newly introduced ones. We empirically evaluate these measures in typical uncertainty estimation settings, such as misclassification detection, selective prediction, and out-of-distribution detection. The results show that no single measure is universal, but the effectiveness depends on the specific setting. Thus, our work provides clarity about the suitability of predictive uncertainty measures by clarifying their implicit assumptions and relationships.", "sections": [{"title": "1 Introduction", "content": "Integrating machine learning models into high-stakes scenarios, such as autonomous driving or managing critical healthcare systems, introduces substantial risks. To hedge against these risks, we need to quantify the uncertainty associated with each prediction to prevent models from making decisions that carry both significant risk and uncertainty. In such cases, it is better to defer uncertain decisions to human experts or opt for a safer, though potentially less advantageous, alternative decision. Consequently, it is vital to employ reliable measures of predictive uncertainty and provide estimates for them when implementing machine learning models for decision making in high-stakes applications.\nThe entropy of the posterior predictive distribution has become the standard information-theoretic measure to assess predictive uncertainty [Houlsby et al., 2011, Gal, 2016, Depeweg et al., 2018, Smith and Gal, 2018, Mukhoti et al., 2023]. Despite its widespread use, this measure has drawn criticism [Malinin and Gales, 2021, Wimmer et al., 2023], prompting the proposal of alternative information-theoretic measures [Malinin and Gales, 2021, Schweighofer et al., 2023b,a, Kotelevskii and Panov, 2024, Hofman et al., 2024b]. The relationship between those measures is still not well understood, although their similarities suggest that they are special cases of a more fundamental formulation.\nWe show that all these measures are approximations of the same measure, the cross-entropy between the predicting model and the true model. The predicting model is the one used to predict during inference, while the true model generated the dataset. However, since we do not know the true model, this fundamental measure is intractable to compute. Therefore, we consider different assumptions about the predicting model and how the true model is approximated. This gives rise to our proposed"}, {"title": "On Information-Theoretic Measures of Predictive Uncertainty", "content": "framework to categorize information-theoretic measures of predictive uncertainty. Our framework includes existing measures, introduces new ones, and clarifies the relationship between these measures. Moreover, our empirical analysis reveals that the effectiveness of different measures varies depending on the task and the posterior sampling method used. In sum, our contributions are as follows:\n1. We introduce a unifying framework to categorize measures of predictive uncertainty according to assumptions about the predicting model and how the true model is approximated. This framework not only encompasses existing measures but also suggests new ones and clarifies their relationship.\n2. We derive our framework from first principles, based on the cross-entropy between the predicting model and the true model as the fundamental yet intractable measure of predictive uncertainty.\n3. We empirically evaluate these measures across various typical uncertainty estimation tasks and show that their effectiveness depends on the setting and the posterior sampling method used."}, {"title": "2 Quantifying Predictive Uncertainty", "content": "We consider the canonical classification setting with inputs $x \\in \\mathbb{R}^D$ and targets $y \\in \\mathcal{Y}$, where $\\mathcal{Y}$ is the set of all $K$ possible targets. The dataset $\\mathcal{D}$ is given, sampled i.i.d according to the data generating distribution. We consider deep neural networks as a class of probabilistic models that map an input $x$ to the $K-1$ dimensional probability simplex $\\Delta^{K-1} = \\{\\theta \\in \\mathbb{R}^K | \\theta_k \\geq 0 \\forall k, \\sum_{k=1}^{K} \\theta_k = 1\\}$.\nThis mapping is defined as $f_w : \\mathbb{R}^D \\rightarrow \\Delta^{K-1}$ for a model with parameters $w$. The output of this mapping defines the distribution parameters of a categorical distribution, in the following referred to as the model's predictive distribution $p(y | x, w) = Cat(y; f_w(x)) = Cat(y; \\theta)$.\nThe predictive distribution of a probabilistic model represents the uncertainty inherent in its predic- tions. When the probability mass is uniformly distributed across all possible outcomes, it denotes complete uncertainty about the prediction, whereas concentration on a single class indicates com- plete certainty. If we have access to the true data-generating model, denoted by parameters $w^*$, the predictive distribution $p(y | x, w^*)$ captures the inherent and irreducible uncertainty in the pre- diction, often referred to as aleatoric uncertainty (AU) [Gal, 2016, Kendall and Gal, 2017]. This assumes that the chosen model class can accurately represent the true predictive distribution, thus $p(y | x) = p(y | x, w^*)$, which is a common and often necessary assumption [H\u00fcllermeier and Waegeman, 2021]. The information-theoretic entropy $H(\\cdot)$ [Shannon, 1948] of the true predictive distribution is a natural and universally accepted measure of AU, defined as\n\n$H(p(y | x, w^*)) := - \\sum_{k=1}^{K} p(y = k | x, w^*) \\log p(y = k | x, w^*).$\n(1)\nHowever, we generally don't know the true model and have to choose parameters $w$ out of all possible ones. Consequently, uncertainty arises due to the lack of knowledge about the true parameters of the model. This is called epistemic uncertainty (EU) [Apostolakis, 1990, Helton, 1993, 1997, Gal, 2016]. An effective measure of predictive uncertainty should be consistent with Eq. (1) and capture both AU and EU, usually assumed to sum up to a total uncertainty (TU)."}, {"title": "2.1 Current Standard Measure: Entropy of the Posterior Predictive Distribution", "content": "Given a dataset $\\mathcal{D}$ and prior $p(w)$ on the model parameters, Bayes' theorem yields the posterior distribution $p(w | \\mathcal{D})$. The posterior distribution denotes the probability that the parameters $w$ match the true parameters $w^*$ of the model that generated the dataset $\\mathcal{D}$. Instead of committing to a single model, the posterior distribution allows marginalizing over all possible models, which is known as Bayesian model averaging. This gives rise to the posterior predictive distribution\n\n$p(y|x, \\mathcal{D}) = E_{p(w|\\mathcal{D})} [p(y | x, w)].$\n(2)\nThe entropy of the posterior predictive distribution is the currently most widely accepted approach to measure predictive uncertainty [Houlsby et al., 2011, Gal, 2016, Depeweg et al., 2018, Smith and Gal, 2018, H\u00fcllermeier and Waegeman, 2021, Mukhoti et al., 2023]. According to a well-known result from information theory [Cover and Thomas, 2006], this entropy can be additively decomposed into the conditional entropy and the mutual information $I$ between $y$ and $w$:\n\n$H(p(y | x, \\mathcal{D})) = E_{p(w|\\mathcal{D})} [H(p(y | x,w))] + I(p(y, w | x, \\mathcal{D})).$\n(3)\nTU AU EU"}, {"title": "On Information-Theoretic Measures of Predictive Uncertainty", "content": "Furthermore, Eq. (3) is equivalent to a decomposition of expected cross-entropy $CE(\\cdot ;\\cdot)$ into conditional entropy and expected KL-divergence $KL(\\cdot ||\\cdot)$ [Schweighofer et al., 2023b,a]:\n\n$E_{p(w|\\mathcal{D})} [CE(p(y | x, w) ; p(y | x, \\mathcal{D}))]$\n(4)\n$= E_{p(w|\\mathcal{D})} [H(p(y | x, w))] + E_{p(w|\\mathcal{D})} [KL(p(y | x, w) || p(y | x, \\mathcal{D}))].$\nAU EU\nIf the parameters of the true model are known, EU vanishes and Eq. (3) as well as Eq. (4) simplify to Eq. (1), thus are consistent with it. However, the entropy of the posterior predictive distribution has been found to be inadequate for specific scenarios, such as autoregressive predictions [Malinin and Gales, 2021] or for a given predicting model [Schweighofer et al., 2023b] and was criticised on grounds of not fulfulling certain expected theoretical properties [Wimmer et al., 2023]. In response, alternative information-theoretic measures have been introduced [Malinin and Gales, 2021, Schweighofer et al., 2023b,a, Kotelevskii and Panov, 2024, Hofman et al., 2024b]. Although the relationship between these measures is not well understood, their structure similar to Eq. (4) suggests a connection between them. We next propose a fundamental, though generally intractable, predictive uncertainty measure, where all of these measures are special cases under specific assumptions."}, {"title": "2.2 Our Proposed Measure: Cross-Entropy between Selected and True Predictive Distribution", "content": "An effective measure of TU should be consistent with Eq. (1) and should incorporate EU. Considering this, we propose to measure predictive uncertainty with the cross-entropy between the predictive distributions of a selected predicting model and the true model. Let $p(y | x, \\cdot)$ be the predictive distribution of any selected model for some new input $x$, which we will refer to as predicting model. We will examine different cases for the predicting model later; for now, it suffices to consider it to be a specific model with parameters $w$. The cross-entropy between the predictive distributions of the predicting model and the true model is given by\n\n$CE(p(y | x,\\cdot); p(y | x, w^*)) := -\\sum_{k=1}^{K} p(y = k | x, ) \\log p(y = k | x, w^*)$\nTU\n(5)\n$= H(p(y | x,\\cdot)) + KL(p(y | x, \\cdot) || p(y | x, w^*))$\nAU EU\nIf the predictive distribution of the predicting model is equal to the predictive distribution of the true model, the EU is zero by definition and Eq. (5) simplifies to Eq. (1). Thus, as expected, if the parameters of the true model are known, the EU vanishes. Eq. (5) is a fundamental, though generally intractable, measure of predictive uncertainty. To obtain tractable measures, assumptions about the predicting model and about how to approximate the true model are necessary. This gives rise to our framework, which we introduce in detail in Sec. 3. Notably, the opposite order of arguments in Eq. (5) leads to the same framework, but differs in the interpretation of AU and EU. Details on this alternative are given in Sec. A.5 in the appendix, our interpretation of AU and EU is as follows.\nInterpretation of AU and EU. An important distinction compared to previous work is in our interpretation of AU and EU, which aligns with the understanding of Apostolakis [1990], Helton [1993, 1997] as follows. The AU is not generally understood as a property of the true predictive distribution, but of the selected predicting model used to make a prediction. Thus, it is the uncertainty that arises due to predicting with the selected probabilistic model. The EU is defined as the additional uncertainty due to predicting with the selected predicting model instead of the true model. Thus, it is the additional uncertainty that arises due to selecting a model from the given model class."}, {"title": "3 Proposed Framework of Predictive Uncertainty Measures", "content": "Our proposed measure of predictive uncertainty (Eq. (5)) allows for different assumptions about (I) the selected predicting model and (II) how to approximate the true model. We consider three different assumptions for each, resulting in nine distinct measures of predictive uncertainty within our proposed framework. An overview of all measures is given in Tab. 1, summarizing the total uncertainties (TU), as well as their respective aleatoric uncertainties (AU) and epistemic uncertainties (EU)."}, {"title": "(A,B,C): Predicting Model", "content": "One can make different choices about the model used during inference for predicting the class of a new input. The most obvious choice of a predicting model is (A) a pre-selected given model with parameters $w$. This is the standard case in machine learning, where model parameters are selected, e.g. by maximizing the likelihood on the training dataset or downloaded from a model hub.\nAnother widely used method is (B) the Bayesian model average (c.f. Eq. (2)). Here, instead of predicting with a single model, the predictive distribution is marginalized over all possible models according to their posterior probability. In practice, exact marginalization is often intractable and therefore approximated by posterior sampling.\nFinally, it is possible to (C) consider every possible model as the predicting model, weighted by their posterior probabilities. This might seem counterintuitive, as it means that the predicting model is not fixed but is sampled anew for each prediction. Nevertheless, the AU of the resulting uncertainty measures, $E_{p(w/\\mathcal{D})} [H(p(y | x, w))]$, is the best approximation of the AU under the true model for a given posterior distribution. However, as pointed out by Wimmer et al. [2023], it is neither a lower nor an upper bound on the AU under the true model and is highly dependent on the posterior distribution."}, {"title": "(1,2,3): Approximation of the True Predictive Distribution", "content": "One can also make different choices about how to approximate the true data-generating model. The simplest but probably biased choice to approximate the true predictive distribution is (1) the predictive distribution under a single given model with parameters $w$. Although this might be a poor approximation, it might be the only feasible choice in specific settings. For example, it is used in speculative decoding [Stern et al., 2018, Leviathan et al., 2023], where a small model is used to predict and whose predictive distribution is compared against a large model that serves as the ground truth.\nAnother possibility is to use (2) the posterior predictive distribution. Although intuitively appealing, Schweighofer et al. [2023a] criticized this as there is no guarantee that these distributions coincide, even for a perfect estimate of the posterior predictive distribution. Furthermore, there are degenerate cases where the posterior predictive distribution can't be represented by any model with non-vanishing posterior probability. However, it is often a well performing approximation empirically for expressive models such as neural networks. Moreover, (2) is the only option that guarantees finite EU and TU."}, {"title": "On Information-Theoretic Measures of Predictive Uncertainty", "content": "Finally, perhaps the most intuitive solution is to consider (3) all possible models according to their posterior probability. Any model could be the true model according to its posterior distribution. Therefore, we should consider the mismatch between the predictive distribution of the selected predicting model and all other models, weighted by their posterior probability."}, {"title": "3.1 Relationships between Measures", "content": "Importantly, the AU of the uncertainty measures depend only on the predicting model and do not depend on the approximation of the true predictive distribution. Thus, they are the same for cases (1), (2) and (3). Furthermore, the AU of case (B) is an upper bound of the AU of case (C), i.e. $H(p(y | x, \\mathcal{D})) \\geq E_{p(w|\\mathcal{D})} [H(p(y | x, w))]$, which directly follows from Eq. (3) as the mutual information is non-negative.\nDue to the linearity in the first argument of the cross-entropy, the TU for cases (B) and (C) are equal. Furthermore, as already discussed, the AU for cases (B) and (C) differ by the mutual information $E_{p(w/\\mathcal{D})} [KL(p(y | x, w) || p(y | x, \\mathcal{D}))]$. Therefore, the EU for cases (B) and (C) also differ by this factor. This is trivial to see for cases (B2) and (C2), where the EU of case (B2) cancels to zero and the EU of case (C2) is the mutual information. For cases (B3) and (C3), this was already mentioned by [Malinin and Gales, 2021] and a proof was given by [Schweighofer et al., 2023a], which we include for completeness in Sec. A.1 in the appendix, together with a version for cases (B1) and (C1)."}, {"title": "3.2 Categorization of Previously Known Measures", "content": "The standard measure (Eq. (4)) introduced by Houlsby et al. [2011] and popularized, for instance, by Gal [2016], Depeweg et al. [2018], Smith and Gal [2018] is the measure (C2). In the context of autoregressive predictions, Malinin and Gales [2021] introduced measure (B3), due to the feasibility of a Monte Carlo (MC) approximation compared to the standard measure (C2). Schweighofer et al. [2023b] introduced measure (A3) together with a posterior sampling algorithm that is explicitly taylored to this measure. Schweighofer et al. [2023a] introduced measure (C3) as an improvement over the standard measure (C2) for certain settings. Hofman et al. [2024b] also derived measure (C3) for the logarithmic strictly proper scoring rule (log score). Furthermore, Kotelevskii and Panov [2024] discussed measures (B2), (B3), (C2) and (C3) as Bayesian approximations under the log score. Our work thus generalizes and gives a new perspective on those measures."}, {"title": "4 Related Work", "content": "Measures of predictive uncertainty. The currently most widely used information-theoretic measure of predictive uncertainty is the entropy of the posterior predictive distribution (Eq.(3)). In Sec. (3.2), we discuss the relationship of previous work based on this measure and our proposed framework. However, there are also other measures of predictive uncertainty, not based on information-theoretic quantities. Depeweg et al. [2018] introduced variance-based measures, based on the law of total variance. This perspective was recently developed further for specific settings [Duan et al., 2024, Sale et al., 2023b]. Furthermore, Sale et al. [2024b] introduced label-wise measures of predictive uncertainty, formulating both information-theoretic and variance-based measures. Another idea recently proposed by Sale et al. [2024a] is quantifying uncertainty through distances to reference (second-order) distributions (for TU, AU, and EU) denoting complete certainty. Thus, the higher the distance from the reference distribution, the more uncertain the prediction. All measures discussed so far operate on a distributional representation of uncertainty. Orthogonal to that, there are also set-based approaches [H\u00fcllermeier et al., 2022, Sale et al., 2023a, Hofman et al., 2024a].\nPosterior sampling methods. All measures proposed by our framework, except (A1), contain a posterior expectation. Those are generally approximated by sampling models according to the posterior distribution. An obvious choice are MCMC algorithms, for example HMC [Neal, 1995, Neal et al., 2011], recently scaled to modern neural network architectures [Izmailov et al., 2021]. Approximate variants using stochastic gradients are also available [Welling and Teh, 2011, Chen et al., 2014, Zhang et al., 2020]. Furthermore, it is possible to learn a simpler variational distribution that approximates the posterior distribution. Widely known examples are the mean-field approach of Blundell et al. [2015] or MC Dropout [Gal and Ghahramani, 2016]. Another approach is the Laplace approximation [MacKay, 1992] around a maximum a posteriori (MAP) model [Ritter et al., 2018,"}, {"title": "On Information-Theoretic Measures of Predictive Uncertainty", "content": "Daxberger et al., 2021]. A commonly used approximation to the Bayesian ideal are Deep Ensembles [Lakshminarayanan et al., 2017], which despite their algorithmic simplicity are widely recognized to provide high-quality samples [Wilson and Izmailov, 2020, Izmailov et al., 2021]. Furthermore, Schweighofer et al. [2023b] introduced adversarial models to explicitly search for models with a large contribution to approximating expectations of the EU for case (3). For a more extensive overview, see, e.g. Gawlikowski et al. [2023] or Papamarkou et al. [2024]."}, {"title": "5 Experiments", "content": "In this section, we evaluate the performance of the proposed measures across various experimental sce- narios that leverage uncertainty, including tasks like misclassification detection, selective prediction, and out-of-distribution (OOD) detection. In addition, we assess the impact of various posterior sam- pling methods, which is a crucial factor in real-world applications. We do not intend to identify the op- timal measure for a specific task or posterior sampling method; all of them can be evaluated, and the best chosen in practice. Our primary aim is to deepen the understanding of our proposed framework.\nDatasets. Our experiments are performed on the CIFAR10/100 [Krizhevsky and Hinton, 2009], SVHN [Netzer et al., 2011], Tiny-ImageNet (TIN) [Le and Yang, 2015] and LSUN [Yu et al., 2015] datasets. For TIN, we resize the inputs to 32x32 to match the other datasets. We train models on all datasets except LSUN, which is used solely as an OOD dataset.\nModels and training. We used three different model architectures for our experiments: ResNet-18 [He et al., 2016], DenseNet-169 [Huang et al., 2017] and RegNet-Y 800MF [Radosavovic et al., 2020]. Individual models were trained for 100 epochs using SGD with momentum of 0.9 with a batch size of 256 and an initial learning rate of 1e-2. Furthermore, a standard combination of linear (from factor 1 to 0.1) and cosine annealing schedulers was used. The main paper's results are for ResNet-18. Results for two other architectures, found in Sec. B.4 of the appendix, are consistent with the main findings.\nPredictive uncertainty measures. We consider all measures proposed by our framework, c.f. Tab. 1. For example, the (total) measure (A1) is referred to as \u201cTU (A1)\u201d with its aleatoric component as \u201cAU (A)\u201d and its epistemic component as \u201cEU (A1)\u201d. Here, \u201cAU (A)\u201d is used instead of \u201cAU (A1)\u201d to emphasize the independence from the approximation of the true predictive distribution. Also, we group equivalent TU measures. For example TU (B1) and TU (C1) are merged to \u201cTU (B/C1)\u201d.\nPosterior sampling methods. We consider three methods to sample models according to the posterior $p(w | \\mathcal{D})$, Deep Ensembles (DE) [Lakshminarayanan et al., 2017], Laplace Approximation (LA) [MacKay, 1992] on the last layer with Kronecker-factored approximate curvature [Ritter et al., 2018] using the implementation of Daxberger et al. [2021] and MC Dropout (MCD) [Gal and Ghahramani, 2016]. Those samples are used to approximate posterior expectations. For example, the posterior predictive distribution given by Eq. (2) is approximated by\n\n$p(y x, \\mathcal{D}) \\approx \\frac{1}{N}\\sum_{n=1}^{N}p(y | x, w_n), \\quad w_n \\sim p(w | \\mathcal{D})$\n(6)\nwith $N$ samples. Posterior expectations within the proposed measures are approximated in the same way. We provide formulas for the MC approximations for all measures (TU, AU and EU) in Sec. A.2 in the appendix. For all three methods, we sample 10 models for the MC approximations of the uncertainty measures. Measures based on a single model (combinations with (A) and (1)) use the first member of the ensemble for DE, the maximum a posteriori (MAP) model for LA, and the model without dropout activated for MCD.\nThere is a distinction between multi- and single-basin poste-\nrior sampling techniques [Wilson and Izmailov, 2020], some-\ntimes also referred to as multi- and single-mode approaches\n[Hoffmann and Elster, 2021]. We refer to them as global and\nlocal posterior sampling techniques for simplicity. In this\ncategorization, DE is a global method, while LA and MCD\nare local methods [Fort et al., 2019]. We hypothesize that dif-\nferent methods for posterior sampling have a strong impact\non which uncertainty measure performs well empirically, es-\npecially given whether they are global or local methods."}, {"title": "5.1 Characteristics of Posterior Samples", "content": "To better understand the performance of different posterior sampling methods, we examine the characteristics of their sampled models. The results in Fig. 2 show that these methods perform differently across datasets. For the global sampling method DE, the average model consistently outperforms individual sampled models with a lower negative log-likelihood (NLL) and higher accuracy across all datasets. In contrast, for local sampling methods LA and MCD, individual sampled models exhibit higher NLL than both the single model and the average model. Additionally, the accuracy of individual sampled models is lower than that of the single model. Specifically, for MCD, the single model's accuracy is comparable to the average model, while for LA, the single model's accuracy exceeds that of the average model.\nWe further analyze the predictive uncertainties estimated by different posterior sampling methods using measure (C2), which incorporates posterior samples and is upper-bounded. To ensure comparability across datasets, we normalize the uncertainties by the maximal predictive uncertainty TU (C2), equal to the entropy of the uniform distribution $\\log(|\\mathcal{Y}|)$. The results in the two right plots in Fig. 2 show that these methods yield similar distributions of uncertainties for CIFAR10 and SVHN. However, for CIFAR100 and TIN, DE exhibits many more datapoints with very low EU and AU."}, {"title": "5.2 Misclassification Detection", "content": "We sampled models on the CIFAR10/100, SVHN and TIN datasets using DE, LA and MCD and obtain predictions on the respective test datasets. This was done with (i) the single model, (ii) the average model and (iii) some model according to the posterior distribution to investigate the impact of aligning the measure of uncertainty with the predicting model - (A) for (i), (B) for (ii) and (C) for (iii). The single model for (i) is a random but fixed model for DE, the MAP model for LA, and the model without dropout for MCD. The average model for (ii) is defined by Eq. (6), averaging over"}, {"title": "On Information-Theoretic Measures of Predictive Uncertainty", "content": "all sampled models. For (iii), one model from the sampled models was randomly selected for each prediction. Note that (iii) does not make much sense in practice, as individual models are performing worse than the average model in terms of accuracy for all considered methods, the same as a single model for DE and worse than the single model for LA and MCD (see Fig. 2). We compare the AUROC for distinguishing between correctly and incorrectly predicted datapoints for the different proposed measures of predictive uncertainty as scoring functions. Alternative measures commonly used to evaluate misclassification such as AUPR or FPR@TPR95 were also considered. Those induced the same ordering of uncertainty measures, thus we report the AUROC throughout all experiments.\nThe results are given in Fig. 3. We average over the four considered datasets and report means and standard deviations over five independent runs. The results for individual datasets are reported in Fig. 10 - Fig. 12 in the appendix. For detecting the misclassification of (i) the single model, EU (A3) performs best. However, when predicting with (ii) the average or (iii) a model according to the posterior, TU (B/C3) performs best. For the local method LA, TU (A3) performs best regardless of the predicting model. The same effect is observed for MCD, yet TU (A2) slightly outperforms TU (A3) in this case. We hypothesize that this effect occurs because the local methods fail to provide high-quality samples for some of the datasets, leading to a high variance of posterior estimates and thus low performance. In sum, we find that TU (A2) and TU (A3) perform well for local posterior sampling methods regardless of the predicting model, but for global posterior sampling methods aligning the measure according to the predicting model makes a strong difference."}, {"title": "5.3 Selective Prediction", "content": "Another commonly considered task is selective prediction, where the model's predictions are limited to a specific subset, and its performance is evaluated on that subset. The setup in this experiment is identical to the misclassification setup. We evaluated the accuracy for a subset of predictions of (i) the single model, (ii) the average model, and (iii) a model according to the posterior distribution. Subsets between 50% of the most certain datapoints and the entire dataset were considered. The area under the accuracy retention curve (AUARC) was used as performance measure to compare the efficacy of uncertainty measures to provide a ranking to select those subsets.\nWe focus on (i) the single model and (ii) the average model using DE with the results given in Fig. 4. Additional results are provided in Sec. B.2 in the appendix. The results show a similar picture as for misclassification detection, where the optimal measure depends on the model used for prediction. For (i) the single model, TU (A3) performs best, while for (ii) the average model, TU (B/C3) performs best. Again, the best measures are those aligned to the predicting model."}, {"title": "5.4 OOD Detection", "content": "We sampled models on CIFAR10/100, SVHN and TIN using DE, LA and MCD. Therefore, we use the respective test dataset as in-distribution (ID) datasets and the test datasets for the others, as well as LSUN, as OOD datasets. OOD detection does not involve a prediction by the model. Thus, it is not possible to align the uncertainty measure with the predicting model, as in misclassification detection and selective prediction. We compare the AUROC for distinguishing between ID and OOD datapoints for each measure within our framework as a scoring function. Alternative commonly used measures such as the AUPR and the FPR@TPR95 were also considered. However, since they induced the same ordering of measures, we report the AUROC for all OOD detection experiments.\nThe results are shown in Fig. 5. We observe that throughout all measures, TU and AU perform much better than EU, which is contrary to assumptions commonly formulated in the literature [Mukhoti et al., 2023, Kotelevskii and Panov, 2024]. However, this might depend on the datasets. For example, with MCD, the EU perform best on the pairs TIN/CIFAR10 and TIN/CIFAR100, see Fig. 16 in the appendix. We hypothesize that the strong performance of the AU is due to the low levels of noise in the considered datasets. Furthermore, for the local method LA, all measures and their AU perform equally well. For DE and MCD, TU (B/C2) and TU (B/C3) perform best."}, {"title": "5.5 Additional Experiments", "content": "Our experiments aim to investigate the performance of the proposed framework on a wide range of tasks. Due to space limitations, we moved the following additional experiments to the appendix:\nWe investigated the performance of the provided measures for detecting distribution shifts on CI- FAR10 using the CIFAR10-C [Hendrycks and Dietterich, 2019] dataset. This is a conceptually sim- ilar task to OOD detection, as our results provided in Sec. B.5 confirm. We observe that for smaller shifts, the EU perform much better than the others; for larger shifts, this effect vanishes.\nFurthermore, we investigated the efficacy of our measures for detecting adversarial examples under FGSM [Goodfellow et al., 2015] and PGD [Madry et al., 2018] attacks. We do not intend to claim any level of adversarial robustness to these attacks, but use them as a tool to understand the behaviour of our measures. The results are discussed in Sec. B.6.\nFinally, we conducted active learning experiments on MNIST [Lecun et al., 1998] and FMNIST [Xiao et al., 2017] using DE and MCD as posterior sampling methods for a small convolutional neural network. Prior work [Gal et al., 2017, Mukhoti et al., 2023] suggests that the optimal acquisition"}, {"title": "On Information-Theoretic Measures of Predictive Uncertainty", "content": "function is a measure of EU. Our results indicate, that a good acquisition function must capture the mutual information between $y$ and $w$ faithfully rather than the EU as defined by our framework. The results and an in-depth discussion are given in Sec. B.7."}, {"title": "6 Conclusion", "content": "We have proposed a framework that categorizes measures of predictive uncertainty according to assumptions about the predicting model and how the true model is approximated. Our framework has been derived from first principles, based on the cross-entropy between the predicting model and the true model (Eq. (5)). Most importantly, it clarifies the relationships between information-theoretic measures of predictive uncertainty and uncovers their implicit assumptions. Our empirical evaluation shows that the effectiveness of the different measures depends on the task and the posterior sampling method used. As there is no best"}]}