{"title": "Distribution-Level Feature Distancing for Machine Unlearning: Towards a Better Trade-off Between Model Utility and Forgetting", "authors": ["Dasol Choi", "Dongbin Na"], "abstract": "With the explosive growth of deep learning applications, the right to be forgotten has become increasingly in demand in various AI industries. For example, given a facial recognition system, some individuals may wish to remove images that might have been used in the training phase from the trained model. Unfortunately, modern deep neural networks sometimes unexpectedly leak personal identities. Recent studies have presented various machine unlearning algorithms to make a trained model unlearn the data to be forgotten. While these methods generally perform well in terms of forgetting scores, we have found that an unexpected model utility drop can occur. This phenomenon, which we term correlation collapse, happens when the machine unlearning algorithms reduce the useful correlation between image features and the true label. To address this challenge, we propose Distribution-Level Feature Distancing (DLFD), a novel method that efficiently forgets instances while preventing correlation collapse. Our method synthesizes data samples so that the generated data distribution is far from the distribution of samples being forgotten in the feature space, achieving effective results within a single training epoch. Through extensive experiments on facial recognition datasets, we demonstrate that our approach significantly outperforms state-of-the-art machine unlearning methods. The implementation of DLFD can be accessed at https://github.com/Dasol-Choi/DLFD.", "sections": [{"title": "Introduction", "content": "Deep neural network models have achieved remarkable success in various computer vision applications (He et al. 2016; Tan and Le 2019; Han et al. 2022; Huang et al. 2017; Jiang et al. 2022). Especially, recent work has shown large-scale foundation models have demonstrated superior classification performance across a range of tasks (Radford et al. 2021; Kolesnikov et al. 2020; Floridi and Chiriatti 2020; Han et al. 2022; Liu et al. 2023). However, alongside these advancements, concerns have emerged regarding the unintentional leakage of sensitive information, such as personal identities from training data (Shokri et al. 2017a; Tarun et al. 2023; Hu et al. 2022). This issue is particularly pressing in the context of training data extraction attacks, where studies have shown that even large foundation models can leak information when deployed in real-world settings (Yu et al. 2023; Carlini et al. 2021).\nMachine unlearning has emerged as a promising solution to mitigate potential data leakage (Tarun et al. 2023; Golatkar, Achille, and Soatto 2020b; Gupta et al. 2021), particularly in upholding the right to be forgotten, which allows individuals to request the removal of their personal information from trained models. For example, in medical AI applications, a patient might request that their medical images, used during the training of a diagnostic model, be removed to protect their privacy. In such a scenario, machine unlearning would enable the model to forget the patient's data without compromising the model's overall performance on other tasks. This growing need for privacy has driven interest in machine unlearning research within various AI-driven industries.\nDespite the progress made by recent machine unlearning algorithms, our work identifies a critical issue that has not been fully explored: the risk of correlation collapse. When simply applying existing error-maximizing methods (Tarun et al. 2023; Kurmanji, Triantafillou, and Triantafillou 2023; Chundawat et al. 2023b), unexpected outcomes can occur. For instance, these methods can inadvertently increase the magnitude of loss excessively, leading to additional data leakage by making certain data points appear special. Moreover, relying solely on these approaches might degrade the generalization performance of the model on the original task, thereby introducing a trade-off between model utility and forgetting. We believe this degradation is due to correlation collapse, where the useful correlations between image features and their true labels are weakened. To prevent these unexpected performance drops, it is crucial to carefully adapt and improve upon the existing methods.\nTo address this challenge, we propose a novel method called Distribution-Level Feature Distancing (DLFD) that enables unlearning of specific images while maintaining the accuracy of the original task. Our approach shifts the feature distribution of the retained images away from the distribution of the images to be forgotten, by leveraging the Optimal Transport (OT) problem (Peyr\u00e9, Cuturi et al. 2019; Le et al. 2021; Cuturi 2013; Altschuler, Niles-Weed, and Rigollet 2017). Specifically, DLFD generates perturbed images by maximizing the distance between the optimized data distribution and the distribution of the data to be forgotten in the feature space using OT loss.\nOur method demonstrates superior performance com-"}, {"title": "Motivation: Correlation Collapse", "content": "In the general computer vision domain, a feature vector w \u2208 W corresponding to an image xretain might contain various semantic information (Na, Ji, and Kim 2022; Richardson et al. 2021). Some of these semantic features, which we denote as Wtask, are highly correlated with the original task that the model @original is designed to solve. However, for personal identity unlearning tasks, another set of features, Widentity, represents information specific to a particular personal identity. Notably, Widentity often shares overlapping information with Wtask, leading to a phenomenon we refer to as feature entangling. In the latent space W, we assume that the manifold of personal identity contains some task-related features, such that Wtask \u2286 W and Widentity \u2286 W. This entanglement makes it fundamentally challenging to disentangle Widentity from wtask because features relevant to individual identities, such as facial structure or expression, can also be crucial for the specific task the model is solving (e.g., gender classification). This situation results in what we describe as correlation collapse.\nFor instance, consider a facial gender classification model. The Widentity vector might include semantic information like facial wrinkles, hair color, and eye shape, which are also relevant to the Wtask vector for gender classification. If we apply an error-maximizing loss to forget a specific data point xforget, this process could inadvertently alter the useful task-related features within xretain that are correlated with the female class (the true label). This change can weaken the correlation between the task-related features and their correct labels, degrading the model's generalization performance.\nAssume we have a data point xretain to be retained and a data point xforget to be forgotten, both belonging to the same class, such as the female class. As shown in Figure 1, modifying xretain to increase the loss associated with xforget"}, {"title": "Proposed Methods", "content": "In this section, we provide a detailed explanation of Distribution-Level Feature Distancing (DLFD), our primary method designed to mitigate correlation collapse in machine unlearning. We then describe how we integrate DLFD with additional techniques, such as data optimization using classification loss and a dynamic forgetting strategy, to further enhance model stability and performance."}, {"title": "Distribution-Level Feature Distancing (DLFD)", "content": "Traditional approaches to machine unlearning often focus on point-wise optimization, where individual data points are manipulated to maximize the loss for data that needs to be forgotten (Tarun et al. 2023; Chundawat et al. 2023b). However, such methods can lead to issues like label leakage and correlation collapse, where the underlying relationships between features are disrupted (Kurakin, Goodfellow, and Bengio 2017; Madry et al. 2018; Ilyas et al. 2019). To overcome these limitations, we propose a more holistic approach that considers the entire distribution of the data.\nDLFD operationalizes the concept of shifting the retained data distribution (\u03bc) away from the forgotten data distribution (v) by leveraging the optimal transport (OT) distance. Unlike simpler metrics such as KL divergence or JS divergence, the OT distance is well-suited for capturing the complex, high-dimensional relationships between data points (Arjovsky, Chintala, and Bottou 2017; Gulrajani et al. 2017).\nThe OT distance between the distributions \u03bc and v is formally defined as:\n$$D(\\mu, \\nu) = \\inf_{\\gamma \\in \\Pi(\\mu,\\nu)} E_{(w,w') \\sim \\gamma} [c(w, w')]$$\nHere, \u03b3 represents the set of all possible joint distributions that can transport \u03bc to v, and c(w, w') quantifies the cost of moving a feature vector w from \u00b5 to a feature vector w' from v. Given the complexity of directly solving this problem, we employ a differentiable Sinkhorn method (Cuturi 2013; Altschuler, Niles-Weed, and Rigollet 2017) to approximate the solution efficiently. This method reduces the computational complexity to approximately quadratic time O(n\u00b2), making it feasible for use in mini-batch computations.\nTo further refine the OT distance, we reformulate the problem to find an optimal transport plan T:\n$$T^{\\lambda} = \\arg \\min_{T \\in \\Pi(\\mu, \\nu)} (T, C) - \\frac{1}{\\lambda} \\sum_{i=1}^{n} \\sum_{j=1}^{n} T_{ij} \\log T_{ij}$$\nIn this equation, the cost matrix C captures the pairwise distances between feature vectors from \u00b5 and v. The regularization term \u03bb ensures that the transport plan T remains smooth and avoids overly concentrated mass transfers, which could destabilize the model. By iteratively optimizing T^, DLFD effectively separates the retained data distribution from the forgotten data, thereby mitigating the risk of correlation collapse."}, {"title": "Data Optimization with Classification Loss", "content": "In our method, we also incorporate a classification loss that guides the perturbation process to address correlation collapse further. The classification loss ensures that the original class information of the retained data is preserved, even as the model attempts to forget specific data points. A critical component of this process is the use of a linear weight that dynamically adjusts the importance of the classification loss throughout the training process.\nThe linear weight plays a crucial role in balancing the trade-off between maximizing the separation of distributions and preserving the model's utility. At the beginning of training, the linear weight is set lower, allowing the model to focus more on maximizing the distance between the retained and forgotten data distributions. As training progresses, the linear weight gradually increases, shifting the model's focus toward preserving the original class-specific features of the retained data.\nThe perturbation applied to the retained data points xi is computed as follows:\n$$x_i^* \\leftarrow x_i + \\alpha \\text{sign} (\\nabla_{x_i} [l_{ot} - \\lambda \\cdot l_{CE}])$$\nHere, lor represents the OT loss between the retained and forgotten data distributions, while ICE represents the classification loss, which is weighted by the linear factor \u03bb and computed as:\n$$l_{CE} \\leftarrow CE(y_i, \\theta(x_i^*))$$\nThe linear weight \u03bb is adjusted throughout the training process to balance the trade-off between maximizing the OT loss and preserving the classification accuracy. The perturbation is scaled by a step size \u03b1 and applied in the direction that increases the OT loss and decreases the weighted classification loss. This ensures that the perturbed data not only becomes more distinct from the forgotten data but also maintains its original task-related features."}, {"title": "Dynamic Forgetting Strategy", "content": "The Dynamic Forgetting Strategy is an adaptive approach designed to optimize the forgetting process by continuously monitoring the forgetting score during training. Specifically, when the forgetting score drops below a predefined threshold, indicating that the model has sufficiently forgotten the target data, the algorithm dynamically shifts its focus from using Optimal Transport (OT) loss and perturbation to exclusively fine-tuning the model with classification loss.\nThis transition not only reduces the computational overhead by avoiding unnecessary further perturbations but also ensures that the model's original task performance remains stable. By fine-tuning solely with classification loss at this stage, the strategy helps preserve the important task-related features, thereby preventing potential degradation in model utility."}, {"title": "Experiments", "content": "In machine unlearning research, an original model @original is trained on the dataset Dtrain to solve a specific task. To evaluate the model utility, we measure the classification accuracy of the model on the test set Dtest. If the model achieves high accuracy on Dtest, it is considered to have high model utility for the original task.\nThe goal of an ideal machine unlearning method is to remove the images that need to be forgotten (Dforget) while maintaining the original classification performance. In this study, we adopt a common machine unlearning setting where the model has access to a subset of the training data, Dretain, which the AI company may still possess. Formally, we assume that the training data Dtrain is composed of Dretain and Dforget, following the general machine unlearning setting described in (Dasol Choi 2023).\nOur objective is to develop a machine unlearning algorithm that makes the unlearned model Ounlearned as similar as possible to the retrained model Oretrained, which is considered the ground truth and is trained only on Dretain. We also introduce a dataset Dunseen, which is never used during the training or testing phases of the model. This"}, {"title": "Task Agnostic Instance-Unlearning", "content": "In this work, we adopt a task-agnostic machine unlearning setup, where unlearning specific target subjects does not alter the functional scope of the original task. Traditional machine unlearning research has primarily focused on class-unlearning, where entire categories (classes) are removed from the model upon a data removal request (Tarun et al. 2023; Golatkar, Achille, and Soatto 2020b; Goel, Prabhu, and Kumaraguru 2022b). While this approach works in certain scenarios, it is not applicable in all cases. For instance, in a gender classification model, removing the male class would leave only the female class, rendering the model ineffective for its intended purpose of gender classification. Hence, class-unlearning is not always representative of real-world needs.\nTo address these limitations, we propose an instance-unlearning problem setting, which targets the removal of specific personal identities or data samples without changing the overall function of the model. This approach ensures that the model's core functionality remains intact, making it more applicable to scenarios where the goal is to forget specific data without compromising the model's utility (Eleni Triantafillou 2023; Dasol Choi 2023; Choi et al. 2024).\nWhile recent studies on instance-unlearning often focus"}, {"title": "Evaluation Protocol", "content": "In this work, we evaluate the models in two sides (1) model utility and (2) forgetting score. First, The model utility is assessed by measuring the test accuracy on Dtest. A high accuracy on Dtest indicates that the model retains strong performance on its original task after the unlearning process. Secondly, for the forgetting performance, we use the Membership Inference Attack (MIA) (Shokri et al. 2017b). MIA simulates a scenario where an attacker tries to determine whether a specific data point x was part of the training dataset. Typically, a model's loss or output probability can reveal whether x was used during training, as models generally exhibit lower loss for data points they have encountered before."}, {"title": "Ablation Study", "content": "We conducted ablation studies to evaluate the contribution of each component in our DLFD method. Initially, we assessed the performance of DLFD alone, which achieved moderate success in machine unlearning, as shown in Table 3. While DLFD alone is effective in distancing the retained data distribution from the forgotten data distribution, it can lead to a reduction in model utility if not combined with other techniques.\nNext, we incorporated the Classification Loss (Cls Loss), which helps maintain the classification accuracy of the retained data. This addition led to a significant improvement in performance across all tasks, particularly for the Age and Emotion tasks, where the NoMUS score increased by approximately 7.3% (from 0.7021 to 0.7536) and 4.7% (from 0.7199 to 0.7536), respectively. This indicates that Cls Loss plays a crucial role in preserving model utility while still achieving effective unlearning.\nFinally, by integrating Dynamic Forgetting, which adapts the unlearning process based on the model's performance during training, we observed further enhancements. For instance, the NOMUS score for the Age and Emotion tasks increased by an additional 2.2% (from 0.7536 to 0.7698) and 5.3% (from 0.7536 to 0.7934), respectively, with Dynamic Forgetting. This component effectively prevents cor-"}, {"title": "Discussion", "content": "Information Leakage in Error-Maximization\nA trained model generally shows lower loss values for training data compared to unseen data, which can unintentionally lead to data leakage. Methods like UNSIR (Tarun et al. 2023) and SCRUB (Kurmanji, Triantafillou, and Triantafillou 2023) that maximize loss for data intended to be forgotten may inadvertently increase the loss for forget data beyond that of unseen data, making the model vulnerable to membership inference attacks.\nOur findings reveal that even with unlearning, naive errormaximization can still result in information leakage. Specifically, when the number of forgotten samples is small (fewer than 100), the loss values for forget data can abnormally increase, exceeding those of unseen data (see Figure 4). This issue highlights a risk that has been largely overlooked in prior studies.\nTrade-off between Model Utility and Forgetting\nOur method reveals a trade-off between test accuracy and forgetting score. As we increase the loss for the data intended to be forgotten (Xforget), the forgetting score improves, but this comes at the cost of test accuracy. This is likely due to correlation collapse, where essential labelrelated features of the retained data are altered. The process of unlearning, particularly through error-maximizing strategies, can inadvertently distort the feature space of the retained dataset. This leads to a degradation in model utility as the model re-adjusts useful features. Our experiments indicate that while increasing the forgetting magnitude enhances"}, {"title": "Conclusion", "content": "We address key challenges in machine unlearning, including information leakage in error-maximizing methods, taskspecific settings, and the critical trade-off between model utility and effective forgetting. Our proposed DLFD method effectively mitigates these issues by reducing the risk of correlation collapse while maintaining high model utility. Experimental results consistently demonstrate that DLFD outperforms existing methods across multiple benchmarks, underscoring its robustness and effectiveness."}]}