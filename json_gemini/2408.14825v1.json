{"title": "From Rule-Based Models to Deep Learning Transformers Architectures for Natural Language Processing and Sign Language Translation Systems: Survey, Taxonomy and Performance Evaluation", "authors": ["Nada Shahin", "Leila Ismail"], "abstract": "With the growing Deaf and Hard of Hearing population worldwide and the persistent shortage of certified sign language interpreters, there is a pressing need for an efficient, signs-driven, integrated end-to-end translation system, from sign to gloss to text and vice-versa. There has been a wealth of research on machine translations and related reviews. However, there are few works on sign language machine translation considering the particularity of the language being continuous and dynamic. This paper aims to address this void, providing a retrospective analysis of the temporal evolution of sign language machine translation algorithms and a taxonomy of the Transformers architectures, the most used approach in language translation. We also present the requirements of a real-time Quality-of-Service sign language machine translation system underpinned by accurate deep learning algorithms. We propose future research directions for sign language translation systems.", "sections": [{"title": "1 Introduction", "content": "A Deaf or Hard of Hearing (DHH) person is someone who has hearing loss. According to the World Health Organization, there are approximately 430 million DHH worldwide, which is estimated to double by 2050 [1]. The means of communication for such a person is sign language, a visual language that relies on the movement of different body parts and facial expressions to convey meaning instead of spoken words [2]. Noting that there are 300 sign languages globally [3] that are independent of spoken languages and have their grammar and syntax [4]. To establish communication between the DHH and the broader society, sign language interpreters act as intermediaries to interpret between sign and spoken languages. However, there is a shortage of interpreters. For instance, there are around 10,000 certified interpreters in the United States [5], while there are about 48 million Deaf individuals residing in the country [6]. This scarcity of interpreters, combined with the increasing number of the DHH population, actuates the introduction of real-time automated sign language translation systems. In particular, in the era of smart cities, the population's well-being is essential [7], [8], developing real-time smart translation systems for the DHH is necessary to provide safer, healthier, and more pleasant experiences. The ability to enable communication between DHH and the hearing population could be lifesaving in a disastrous event such as a medical emergency [9]. Consequently, building a software-based real-time translation system that accurately and efficiently translates sign languages to spoken languages and vice versa is crucial.\nMachine Translation (MT) was introduced as part of Natural Language Processing (NLP) to translate one spoken language to another [10]. MT algorithms evolved from rule-based to neural networks and are classified into four categories [11]: Rule-Based MT (RBMT), Example-Based MT (EBMT), Statistical MT (SMT), and Neural MT (NMT). Several works exist on machine translation for spoken languages [12], [13]. Despite that MT evolution faced a lot of challenges, including language and context complexity [14], idiomatic expressions [15], and time efficiency [10], there have been a lot of advances in the development of efficient spoken end-to-end translation systems [10]. On the other hand, regarding sign language translation, several studies have applied MT techniques to convert sign language into spoken language and vice versa. These efforts typically concentrate on translating discrete components, either from sign to gloss [16], where signs are the image/video frames and gloss are the linguistic representations of signs, or from gloss to text [17]. The introduction of gloss contributes to higher precision when translating from sign to text [18], or when generating signs [19]. In addition, several existing surveys explored sign language translations in the literature [20], [21], [22], [23], [24]. However, they tackled particular aspects of the sign language translations, such as direct translation from signs to spoken language text [20], [21], [24], signs to gloss [20], [22], [23], [24], gloss to spoken language text [20], [21], [22], [23], spoken language text to gloss [20], [21], [22], or gloss to signs [20], [21], [22], [23]. In our survey, we present a holistic approach where sign language translation enables communication between DHH and hearing individuals in a seamless end-to-end framework from sign to gloss to text and backward, presented in terms of stages. Furthermore, we provide an in-depth analysis of the evolution and current state"}, {"title": "2 Related Surveys for Sign Language Translation", "content": "Several reviews on SLMT exist in the literature [20], [21], [22], [23], [24]. [20], covering the period from 2018 to 2021 explores the advantages, limitations, and challenges of the different methods. It also analyzes these methods based on the performance achieved in the literature. Moreover, it presents sign language datasets, covering both isolated (I) and continuous (C) sign language translation. However, it does not delve into machine translation evolution or provide taxonomies for sign language applied algorithms or transformer architectures. Nevertheless, this survey includes discussions on T2G, gloss-to-sign (G2S), S2G, and G2T translations, but omits sign-to-gloss-to-text (S2G2T) and sign-to-text (S2T) translations. [21] spans a broader period from 2016 to 2022, providing analysis of machine translation evolution, sign language datasets, and continuous sign language data. It also explores the different deep-learning algorithms applied in sign language translation while omitting S2G and S2G2T translations. Nevertheless, it provides partial performance analysis for the sign language translation works, focusing on those post-2018. Despite its comprehensive analysis, this study lacks providing any taxonomies. [22] examines works from 2015 to 2020, exploring sign language datasets (both isolated and continuous) and presenting taxonomies for sign language and the applied algorithms. However, it does not cover machine translation evolution or transformer architectures. It includes discussions and performance analysis for T2G, G2S, S2G, and G2T translations but does not address S2G2T or S2T translations. [23] provides a broader view of the literature from the 1990s to 2020, addressing machine translation evolution, sign language datasets, and performance analysis of the isolated and continuous sign language translation, including T2G, G2S, S2G, and G2T translations. However, it does not cover S2G2T and S2T translations, nor provide any related taxonomies. [24] presents a comprehensive survey from the 1990s to 2021, exploring sign language acquisition, recognition, translation, and linguistic structures, providing a sign language taxonomy. However, it does not delve into the algorithmic approaches applied in the literature nor provides a related taxonomy. Nevertheless, despite providing performance analysis for the literature, its translation coverage is limited, addressing only G2S and S2T translations.\nOn the other hand, our survey, spanning from 2016 to 2023, represents a unique and comprehensive effort in exploring SLMT. We delve into critical aspects such as MT evolution and sign language datasets for isolated and continuous signs. We investigate both machine-learning and deep-learning algorithms and provide a thorough performance evaluation. What sets our work apart is our discussion of the architectural components in the literature and our presentation of detailed taxonomies for both sign language and the algorithms applied in SLMT. Importantly, we cover all aspects of the translation process, from T2G to G2T, including intermediate and combined processes like S2G, G2S, and the full S2T translation, supported by an in-depth exploration and taxonomy of transformer architectures. In addition, unlike previous surveys, our survey provides empirical evaluations of the different transformer architectures in a G2T sign language translation scenario. This comprehensive approach underscores our unique contribution to the field, offering insights and classifications not previously provided, as detailed in Table 2."}, {"title": "3 Sign Language Taxonomy", "content": "Sign language conveys meaning visually and has distinct linguistic properties that differ from spoken languages [4]. Fig. 1 presents our taxonomy of sign language. We classify the signs based on their sign detection type, sign category, and grammar composition."}, {"title": "4 Machine Translation Evolution", "content": "Fig. 4 shows the evolution of MT algorithms over time divided into four categories: Rule-Based Machine Translation (RBMT), Example-Based Machine Translation (EBMT), Statistical Machine Translation (SMT), and Neural Machine Translation (NMT) [28].\nMachine Translation (MT) began in the mid-20th century due to the world's communication needs. The first documented effort in MT was developing the Automatic Language Processing Advisory Committee project in the United States in 1966 [29]. The early approach relied on linguistic and grammatical rules to translate languages through a rule-based model [30]. However, the model was unsatisfactory due to the languages' complexity. The evolution of MT initially focused on spoken languages. However, it extended to sign languages in the 1980s [21], [31], addressing this visual communication's unique challenges and linguistic nuances. In the following decades, MT research progressed to include statistical and neural approaches in the 1990s and 2000s, respectively, improving translation accuracy [29], [32].\nRBMT, introduced in 1954, relies on rules and dictionaries to convert text from one language to another [30]. It consists of three models: dictionary-based, interlingual, and transfer-based [33]. Dictionary-based translation combines dictionaries and grammatical rules to translate text from one language to another by breaking the sentence in the source language into smaller units, such as words or short phrases. The model then searches each unit in a bilingual dictionary to map words or phrases in the source language to their corresponding translations in the target language while applying grammatical rules to maintain proper syntax. Although this approach is straightforward, it falls short in handling the language context [33].\nInterlingual MT was introduced to solve the accuracy problem of the dictionary-based MT. This approach identifies the structure and semantics of the text in the source language. Then, it transforms the linguistic meaning into an intermediate language-neutral globally unified representation. The intermediate representation is then used to generate the target language text. This approach raised concerns regarding its efficiency. Therefore, researchers proposed the Transfer-based approach, which functions like the interlingual translation. However, the intermediate representation in the new approach emphasizes structural transfer rather than meaning transfer and is not unified across languages [33].\nIn contrast to RBMT, EBMT was introduced in the mid-1980s to solve the efficiency and accuracy problems of the RBMT [28]. This was done by utilizing a database of translated sentences for translation reference. The model identifies and modifies"}, {"title": "5 Proposed Conversational End-to-End Sign Language Machine Translation Framework", "content": "SLMT is a process that involves seamless conversion between sign language and spoken language in both directions, supported by the computer vision and NLP domains. Computer vision involves processing visual data [47], which helps understand and generate signs. NLP [48], on the other hand, contributes to translating these expressions into spoken language. In this section, we present a framework for SLMT in terms of"}, {"title": "6 Sign Language Datasets", "content": "We present a comprehensive list of the public SLMT in Table 3. Each dataset has essential attributes, including the year, sign language, number of signers (#Signers), number of videos (#Videos), resolution, and acquisition mode. These datasets have different environments, qualities, constraints, and complexities.\nSLMT datasets cover different sign languages, including American [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], Arabic [50], [76], [77], [78], [79], Australian [63], Brazilian [80], [81], British [82], [83], Chinese [52], [84], [85], [86], [87], [88], Columbian [89], Finnish [90], French [83], German [17], [83], [91], [92], [93], Greek [83], [94], Indian [95], [96], Irish [97], Korean [98], [99], Persian [100], Polish [101], Russian [102], and Turkish [103], [104]. There is a preference among researchers for developing datasets using Continuous Sign Language Recognition (CSLR) over Isolated Sign Language Recognition (ISLR). This aligns with the practical relevance of CSLR in real-world scenarios, where sign language communication has a continuous and dynamic nature [4]. Moreover, the resolution and acquisition methods vary for some datasets. These variations introduce more diversity and complexity, potentially making them valuable resources for addressing challenging research tasks.\nFig. 6 presents the frequency and average number of videos for the public continuous SLMT datasets. The figure shows that even though American Sign Language (ASL) dominates other sign languages in frequency, it has one of the lowest average numbers of videos (1106). The dominance of ASL in this list is due to its popularity [105]. Hence, the availability of its resources and heightened research interest. On the other hand, the figure shows that Chinese Sign Language has the largest average number of videos (24667). Fig. 7 presents the frequency and average number of videos for the public-isolated SLMT datasets. Like the previous figure, ASL dominates other sign languages in frequency, although it has an average number of videos of 17779, which is the fifth largest sign language. The figure also shows that the Chinese Sign Language has the largest average number of videos (65000). Moreover, Fig. 8 illustrates the coverage of components of the public datasets, showing that most public SLMT datasets cover video frames and gloss, followed by datasets that cover all three components and a few datasets that cover video frames and text. This indicates that the direct S2T translation is not as popular as S2G or S2G2T, highlighting the importance of gloss representation in this process.\nIn summary, German Sign Language has attracted more attention due to the implementation of Phoenix-2014 [91] dataset and its extension Phoenix-2014T [17]. Based on the popularity of Phoenix-2014T dataset, we suggest utilizing it when introducing new models to ensure accurate comparisons with existing literature until a more extensive and diverse dataset is available. We also suggest creating a large continuous SLMT multilingual dataset to aid in building one generative multilingual SLMT model."}, {"title": "7 Performance Evaluation Metrics", "content": "The assessment of translation systems is essential for understanding their effective-ness and accuracy. This section describes the performance metrics used in SLMT which are Bilingual Evaluation Understudy (BLEU) [106], Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [107], Word Error Rate (WER) [108], and Accuracy.\n7.1 Bilingual Evaluation Understudy (BLEU)\nBLEU is used to measure the similarity between the machine and human translations of sign language to spoken language and vice-versa. It focuses on the precision of n-grams [106]. BLEU scores are expressed on a scale from 0 to 1, where 1 indicates a perfect match with the reference translation.\nThis metric is divided into BLEU-1, BLEU-2, BLEU-3, and BLEU-4. These variants evaluate the concordance of the respective n-grams between the machine and human translations. This allows the evaluation of the lexical accuracy and appropriateness of the translations.\nBLEU score is calculated using Equations (1) and (2).\n$BLEU = BP \\cdot exp(\\frac{1}{C} \\sum_{n=1}^{C} w_{n} \\cdot log(p_{n}))$   (1)\nWhere : $BP = \\begin{cases}\n1,  & if c > r\\\\\n \\exp(1-\\frac{r}{c}),  & if c < r\\\\\n\\end{cases}$ (2)"}, {"title": "8 Taxonomy of Sign Language Machine Translation Algorithms", "content": "Fig. 9 shows a proposed taxonomy based on a retrospective analysis of the temporal evolution of the SLMT algorithms. We classify these algorithms into four categories: 1) RBMT [54], [109], [110], 2) EBMT [111], 3) SMT [112], [113], [114], [115], and 4) NMT [16], [17], [18], [19], [49], [52], [53], [85], [89], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135]. We map each of these algorithms to the corresponding framework sign language translation stages that were investigated in the literature. We also present a comparison between the SLMT algorithms in the literature in terms of the sign language considered, the dataset(s) used, the translation algorithms and the feature extraction techniques employed, and the corresponding performance, as shown in Table 4."}, {"title": "9 Taxonomy of Transformer Architectures for Sign Language Machine Translation", "content": "We present a classification of the component-based Transformer architectures in Fig. 12. We classify these architectures into four categories: 1) Encoder-decoder transformer (EDT) (Fig. 12 (a)), 2) Encoder-only transformer (EOT) (Fig. 12(b)), 3) Decoder-only transformer (DOT) (Fig. 12(c)), and 4) Encoder-decoder transformer fusion (EDTF) (Fig. 12(d)). Table 5 presents a comparison between the works that used these transformer architectures, along with their parameters, in the different stages of our proposed framework.\nThe EDT architecture was proposed by [150] in 2017. The architecture consists of a Tokenization block to transform the input sentences into words or sub-words that build the model's vocabulary. In addition, the architecture contains an Embedding block to transform the input sentences into vectors, plus a Positional Encoding block to provide information about the position of each word in the sentence. The model also consists of Encoder and Decoder layers. The encoder consists of the following components: Multi-Head Attention to weigh the importance of different parts of the input and capture the relationships between the input words. Feedforward Neural Networks add non-linearities and help the model capture more complex patterns. Normalization and Residual Connections Layers to stabilize and speed up the training of deep networks. The decoder consists of components similar to the encoder and a Masked Multi-Head Attention to prevent the model from accessing information about future positions. Following the decoder, the model consists of a Linear Layer to transform the decoder output into the desired dimension and a SoftMax activation to obtain the probability distribution over the vocabulary for each position in the output. This architecture has been widely used across various translation stages, including S2G [53], [119], G2T [53], G2S [134], S2G2T [18], and S2T [18], [52], [53], [126], [130], [134].\nThe EOT and DOT can be applied in a model that utilizes other algorithms. For instance, the GPT architecture consists of a Byte Per Encoding and a decoder-only transformer [42]. However, in the SLMT literature, the encoder-only and decoder-only transformers were applied in one work [16]. The researchers here applied the encoder for the S2G translation. The encoder was trained using a CTC loss to predict the gloss sequences. They also applied the decoder for the G2T translation. They trained and tested each part independently and joined them in an end-to-end S2G2T SLMT system.\nFurthermore, [19] implemented a variation of the encoder-decoder transformer that we call an \u201cencoder-decoder transformer fusion\u201d (EDTF) for G2T, T2G, and G2S translations. The model includes a Fast Furrier Transform (FFT) block between the embedding layer and the encoder. This is added to extract each word's frequency representation [152] as an additional feature to the embeddings. To enhance the gloss translation, the researchers incorporated a gated bilateral fusion layer within the decoder between the self-attention and the feed-forward network. This mechanism controls the influence of the input and its context while controlling the gradients of the propagation [153]. In"}, {"title": "10 Performance Evaluation of Transformers for Gloss-to-Text Translation: A Case Study", "content": "To understand the implications of deploying transformers-based sign language interpretation in real scenarios, we conduct empirical evaluations of the four transformer architectures which are underpinned by our taxonomy, in a unified environmental setup. It is necessary to deploy an efficient transformer in real-world scenarios. In this case study, we consider sign language gloss to spoken language text (G2T) machine learning translation.\n10.1 Datasets\nWe use the largest publicly available sign language PHOENIX-2014T dataset [17]. It consists of 8,257 weather-related sentences in German sign language. To assess the impact of a small dataset on the comparative performance of the different transformer architectures, we employ random 500 unique sentences of PHOENIX-2014T dataset. To evaluate the performance on a different type of dataset and language, we collect our own private ASL dataset, which we call \u201cMedASL\u201d. It consists of 500 medical-related sentences generated by ChatGPT [155] to reflect scenarios between patients and doctors, nurses, technicians, and registration desk staff in a medical center. An ASL expert then translated these sentences to gloss.\n10.2 Experimental Setup\nTo achieve the best performance possible in a sign language interpretation scenario, we perform hyperparameter tuning which determines the optimal values for transformer models' parameters. The parameters we study for each transformer model are presented in Table 6. These parameters are selected as the best parameters reported in the state of the art as described in Table 5. Consequently, for each dataset, we perform 16 experiments, corresponding to each combination of the four architectures and their respective hyperparameters configurations."}, {"title": "11 Challenges and Proposed Solutions", "content": "A real-time efficient SLMT system comprises a user-centric architecture that allows DHH and hearing users to interact with each other through a simultaneous translation from sign to spoken language and vice versa. The challenges of such a system include:\n\u2022 Availability of large and multilingual datasets: There are more than 300 sign languages globally [3], many of which are low in resources, such as the ArSL [156]. Consequently, the number of available datasets is scarce. Therefore, researchers need a large and accurate dataset for faster data generation and precision [51]. In addition, sign language datasets often include the translation between one spoken language and one sign language (e.g., Chinese and Chinese Sign Language) [21]. Consequently, limitations of current technologies include the training of monolingual AI models, in which models are usually trained on one pair of languages at a time. A solution for this challenge is the creation of crowdsourced platforms [157] for collecting, annotating, and translating sign language data. This approach overcomes the scarcity of resources in a particular sign language. It leads to the creation of a large multilingual dataset, which reduces the need for extensive datasets for each language. Another solution is to apply transfer learning techniques by pre-training a model on rich sign language datasets and then fine-tuning them on low-resource sign language [53]. This strategy is particularly relevant for including regional dialects and idiomatic expressions, which are critical for the accurate representation of sign languages in AI models.\n\u2022 High deployment cost: The deployment of SLMT systems involves substantial financial investment, due to the advanced technology required for accurate sign language recognition and translation and the need for extensive infrastructure, including servers for processing and storing large datasets. The economic barrier extends to the cost of developing, testing, and continually updating these systems to accommodate new sign languages and dialects, making it a significant obstacle to widespread adoption. A potential solution to mitigate these costs is utilizing cloud and edge computing to reduce the need for expensive, dedicated infrastructure [158], and by distributing the computational load more efficiently and cost-effectively across edge and cloud servers [159].\n\u2022 High retraining cost: Languages are constantly evolving [160]. Therefore, the current datasets will soon need to be updated, and current AI models must be retrained. This leads to high electricity and maintenance costs. [53] applied transfer learning techniques to grasp the knowledge from rich sign languages and apply it to low-resource languages. Through this approach, the model could learn universal sign language features, including the translation of idiomatic expressions and regional dialects. On the other hand, domain adaptation strategies allow the AI model to adjust to a specific in-domain low-resource sign language and out-of-domain translations for the same sign language with high translation accuracy [161].\n\u2022 High energy consumption: SLMT systems involve Internet-of-Things (IoT) devices and edge and cloud servers to train the AI models and perform real-time translation. These components consume a high amount of energy. By 2025, global data"}, {"title": "12 Future Research Directions", "content": "In this section, we identify several promising directions for future research in this field. A critical area of focus is the development of more robust and scalable end-to-end SLMT systems that can handle a wide range of sign languages efficiently in terms of precision and execution time. While Transformer is the most used approach for sign language translation, as revealed by our survey, Reinforcement learning would aid in leveraging Transformer translation [169] for sign language translation. In addition, integrating sign language features, such as facial expressions and body language, could further enhance the accuracy and naturalness of SLMT systems.\nTransformer models have shown exceptional ability in understanding and generating complex spoken language patterns [150], which could be extended to the nuances of sign languages, with the potential of increasing the translation precision of an LSMT system. However, training the Transformer large datasets suffers from performance efficiency in terms of execution time, which hinders its deployment in real-time SLMT systems. Quantum computing is a promising technology to boost the performance of computing power-hungry applications [170]. It could reduce the computational time required for training deep learning models on large datasets due to their processing power. Therefore, the development of end-to-end SLMT systems enabled by quantum computing would revolutionize the domain of sign language translation, toward the deployment of more sophisticated SLMT systems while increasing their precision and execution time. This would encourage the adoption of these systems by DHH and hearing populations, enabling real-time conversation.\nHowever, deploying a real-time conversation end-to-end SLMT system faces privacy and security challenges of conversational data and videos collected for the DHH individuals. Blockchain has shown its potential in privacy-preserving health records management systems [166]. by creating decentralized ledgers to manage and securely store DHH individual data and consent logs, blockchain technology can provide a robust framework for protecting sensitive information while maintaining DHH individual trust in the SLMT system [171].\nFurthermore, to provide the users of SLMT systems with immersive, interactive, and engaging experiences for their users, the integration of scalable and real-time Metaverse [56], along with Virtual Reality (VR) [172], Augmented Reality (AR) [173], wearable devices [174], and educational tools [175] holds promise for revolutionizing how the DHH community interacts with the world."}, {"title": "13 Conclusion", "content": "The increasing number of Deaf and Hard of Hearing (DHH) worldwide with limited certified sign language interpreters has led to a need for an efficient, signs-driven, integrated end-to-end automated Sign Language Machine Translation (SLMT) system. Many works on the topic gained attention in recent years. Most of the works in the literature proposed systems based on Neural Machine Translation (NMT) algorithms to achieve accurate translation. Our study identifies different Transformer architectures as the most used and effective algorithms in this domain, with PHEONIX-2014T dataset being the most widely applied.\nThrough this study, we provide an in-depth analysis of the evolution and current state of Machine Translation (MT) systems. We also reveal key insights, challenges, and future research directions, addressing gaps left by existing surveys that have typically focused on specific aspects of sign language translations, such as signs to glosses, glosses to spoken language text, text to glosses, or glosses to signs. In addition, we propose an end-to-end SLMT framework covering all translation stages, i.e. from sign to gloss to text and backward. This framework allows SLMT researchers and developers to systematically evaluate translation methodologies and ensures a comprehensive understanding of the influence of each stage on the overall effectiveness of the translation system. To our knowledge, this is the first work that offers a comprehensive retrospective analysis of the evolution of SLMT algorithms and introduces a taxonomy of Transformer architectures tailored for SLMT. In addition, it presents a Transformer-based gloss-to-text translation case study, comparing the performance of different architectures through empirical evaluations.\nOne of the key lessons learned is that developing accurate and efficient SLMT systems requires a deep understanding of the unique linguistic features of sign languages. Our analysis demonstrates that Transformer-based architectures surpass other MT models, underscoring the necessity for expansive, annotated datasets and diverse evaluation metrics for thorough SLMT assessment. Moreover, developing accurate and efficient SLMT systems requires a deep understanding of these linguistic features, and the application of large-scale annotated datasets and multiple evaluation metrics, such as BLEU and ROUGE, is critical for comprehensive system assessment.\nBy addressing these gaps and highlighting the temporal progression of algorithms and the distinct functionalities of Transformer architectures, our work aims to significantly advance the field of SLMT, providing valuable insights and resources that will aid researchers and developers in building more accurate, efficient, and inclusive end-to-end translation systems for seamless communication between the DHH community and the broader society."}]}