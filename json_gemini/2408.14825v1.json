{"title": "From Rule-Based Models to Deep Learning Transformers Architectures for Natural Language Processing and Sign Language Translation Systems: Survey, Taxonomy and Performance Evaluation", "authors": ["Nada Shahin", "Leila Ismail"], "abstract": "With the growing Deaf and Hard of Hearing population worldwide and the persistent shortage of certified sign language interpreters, there is a pressing need for an efficient, signs-driven, integrated end-to-end translation system, from sign to gloss to text and vice-versa. There has been a wealth of research on machine translations and related reviews. However, there are few works on sign language machine translation considering the particularity of the language being continuous and dynamic. This paper aims to address this void, providing a retrospective analysis of the temporal evolution of sign language machine translation algorithms and a taxonomy of the Transformers architectures, the most used approach in language translation. We also present the requirements of a real-time Quality-of-Service sign language machine translation system underpinned by accurate deep learning algorithms. We propose future research directions for sign language translation systems.", "sections": [{"title": "Introduction", "content": "A Deaf or Hard of Hearing (DHH) person is someone who has hearing loss. According to the World Health Organization, there are approximately 430 million DHH worldwide, which is estimated to double by 2050 [1]. The means of communication for such a person is sign language, a visual language that relies on the movement of different body parts and facial expressions to convey meaning instead of spoken words [2]. Noting that there are 300 sign languages globally [3] that are independent of spoken languages and have their grammar and syntax [4]. To establish communication between the DHH and the broader society, sign language interpreters act as intermediaries to interpret between sign and spoken languages. However, there is a shortage of interpreters. For instance, there are around 10,000 certified interpreters in the United States [5], while there are about 48 million Deaf individuals residing in the country [6]. This scarcity of interpreters, combined with the increasing number of the DHH population, actuates the introduction of real-time automated sign language translation systems. In particular, in the era of smart cities, the population's well-being is essential [7], [8], developing real-time smart translation systems for the DHH is necessary to provide safer, healthier, and more pleasant experiences. The ability to enable communication between DHH and the hearing population could be lifesaving in a disastrous event such as a medical emergency [9]. Consequently, building a software-based real-time translation system that accurately and efficiently translates sign languages to spoken languages and vice versa is crucial.\nMachine Translation (MT) was introduced as part of Natural Language Processing (NLP) to translate one spoken language to another [10]. MT algorithms evolved from rule-based to neural networks and are classified into four categories [11]: Rule-Based MT (RBMT), Example-Based MT (EBMT), Statistical MT (SMT), and Neural MT (NMT). Several works exist on machine translation for spoken languages [12], [13]. Despite that MT evolution faced a lot of challenges, including language and context complexity [14], idiomatic expressions [15], and time efficiency [10], there have been a lot of advances in the development of efficient spoken end-to-end translation systems [10]. On the other hand, regarding sign language translation, several studies have applied MT techniques to convert sign language into spoken language and vice versa. These efforts typically concentrate on translating discrete components, either from sign to gloss [16], where signs are the image/video frames and gloss are the linguistic representations of signs, or from gloss to text [17]. The introduction of gloss contributes to higher precision when translating from sign to text [18], or when generating signs [19]. In addition, several existing surveys explored sign language translations in the literature [20], [21], [22], [23], [24]. However, they tackled particular aspects of the sign language translations, such as direct translation from signs to spoken language text [20], [21], [24], signs to gloss [20], [22], [23], [24], gloss to spoken language text [20], [21], [22], [23], spoken language text to gloss [20], [21], [22], or gloss to signs [20], [21], [22], [23]. In our survey, we present a holistic approach where sign language translation enables communication between DHH and hearing individuals in a seamless end-to-end framework from sign to gloss to text and backward, presented in terms of stages. Furthermore, we provide an in-depth analysis of the evolution and current state"}, {"title": "Related Surveys for Sign Language Translation", "content": "Several reviews on SLMT exist in the literature [20], [21], [22], [23], [24]. [20], covering the period from 2018 to 2021 explores the advantages, limitations, and challenges of the different methods. It also analyzes these methods based on the performance achieved in the literature. Moreover, it presents sign language datasets, covering both isolated (I) and continuous (C) sign language translation. However, it does not delve into machine translation evolution or provide taxonomies for sign language applied algorithms or transformer architectures. Nevertheless, this survey includes discussions on T2G, gloss-to-sign (G2S), S2G, and G2T translations, but omits sign-to-gloss-to-text (S2G2T) and sign-to-text (S2T) translations. [21] spans a broader period from 2016 to 2022, providing analysis of machine translation evolution, sign language datasets, and continuous sign language data. It also explores the different deep-learning algorithms applied in sign language translation while omitting S2G and S2G2T translations. Nevertheless, it provides partial performance analysis for the sign language translation works, focusing on those post-2018. Despite its comprehensive analysis, this study lacks providing any taxonomies. [22] examines works from 2015 to 2020, exploring sign language datasets (both isolated and continuous) and presenting taxonomies for sign language and the applied algorithms. However, it does not cover machine translation evolution or transformer architectures. It includes discussions and performance analysis for T2G, G2S, S2G, and G2T translations but does not address S2G2T or S2T translations. [23] provides a broader view of the literature from the 1990s to 2020, addressing machine translation evolution, sign language datasets, and performance analysis of the isolated and continuous sign language translation, including T2G, G2S, S2G, and G2T translations. However, it does not cover S2G2T and S2T translations, nor provide any related taxonomies. [24] presents a comprehensive survey from the 1990s to 2021, exploring sign language acquisition, recognition, translation, and linguistic structures, providing a sign language taxonomy. However, it does not delve into the algorithmic approaches applied in the literature nor provides a related taxonomy. Nevertheless, despite providing performance analysis for the literature, its translation coverage is limited, addressing only G2S and S2T translations.\nOn the other hand, our survey, spanning from 2016 to 2023, represents a unique and comprehensive effort in exploring SLMT. We delve into critical aspects such as MT evolution and sign language datasets for isolated and continuous signs. We investigate both machine-learning and deep-learning algorithms and provide a thorough performance evaluation. What sets our work apart is our discussion of the architectural components in the literature and our presentation of detailed taxonomies for both sign language and the algorithms applied in SLMT. Importantly, we cover all aspects of the translation process, from T2G to G2T, including intermediate and combined processes like S2G, G2S, and the full S2T translation, supported by an in-depth exploration and taxonomy of transformer architectures. In addition, unlike previous surveys, our survey provides empirical evaluations of the different transformer architectures in a G2T sign language translation scenario. This comprehensive approach underscores our unique contribution to the field, offering insights and classifications not previously provided, as detailed in Table 2."}, {"title": "Sign Language Taxonomy", "content": "Sign language conveys meaning visually and has distinct linguistic properties that differ from spoken languages [4]. Fig. 1 presents our taxonomy of sign language. We classify the signs based on their sign detection type, sign category, and grammar composition.\nSign Detection. This category includes the techniques that are used to recognize and interpret sign language. It divides the recognition process into static detection, representing static signs, and dynamic detection, reflecting the multiple frames or videos in which sign language is produced.\nSign Category. We classify signs as isolated recognition, representing words, and continuous recognition, representing sentences. This classification recognizes the varying complexities of sign language communication and how it can adapt to scenarios.\nSign Language Grammar. We categorize the elements that shape the structure and meaning of sign language based on its phonology. Phonology, in any language, is a subfield of linguistics that deals with the study of patterns and phonemes [25]. The phonological parameters of a sign language are categorized into manual and non-manual markers [4]. The manual markers include a) handshape, which refers to the specific shape of the fingers and hands while producing signs. b) handshape movement and frequency, which involves the direction and way the hands and arms move to depict different words and concepts. c) handshape location, where the signs are produced in relation to the body. d) palm orientation refers to the position and direction of the palms when producing a sign. e) body shift, which conveys meaningful messages during conversations and storytelling. Fig. 2 shows an example of these markers. On the other hand, the non-manual markers include body language, eye gaze, and facial expressions such as eyebrows and lip movement. This phonology is crucial in conveying emotions, intensity, and grammatical information. In addition, sign language classifiers are linguistic representations of more than words, allowing the signer to provide more detailed descriptions of objects, people, or events. Fig. 3 shows two different classifiers. Signers utilize classifiers to add depth, context, and specificity to their conversations, making sign language expressive. Moreover, the sign language lexicon refers to the entire set of signs or lexical items to convey meaning. The last grammar element is fingerspelling, which involves manual signs representing the letters of"}, {"title": "Machine Translation Evolution", "content": "Fig. 4 shows the evolution of MT algorithms over time divided into four categories: Rule-Based Machine Translation (RBMT), Example-Based Machine Translation (EBMT), Statistical Machine Translation (SMT), and Neural Machine Translation (NMT) [28].\nMachine Translation (MT) began in the mid-20th century due to the world's communication needs. The first documented effort in MT was developing the Automatic Language Processing Advisory Committee project in the United States in 1966 [29]. The early approach relied on linguistic and grammatical rules to translate languages through a rule-based model [30]. However, the model was unsatisfactory due to the languages' complexity. The evolution of MT initially focused on spoken languages. However, it extended to sign languages in the 1980s [21], [31], addressing this visual communication's unique challenges and linguistic nuances. In the following decades, MT research progressed to include statistical and neural approaches in the 1990s and 2000s, respectively, improving translation accuracy [29], [32].\nRBMT, introduced in 1954, relies on rules and dictionaries to convert text from one language to another [30]. It consists of three models: dictionary-based, interlingual, and transfer-based [33]. Dictionary-based translation combines dictionaries and grammatical rules to translate text from one language to another by breaking the sentence in the source language into smaller units, such as words or short phrases. The model then searches each unit in a bilingual dictionary to map words or phrases in the source language to their corresponding translations in the target language while applying grammatical rules to maintain proper syntax. Although this approach is straightforward, it falls short in handling the language context [33].\nInterlingual MT was introduced to solve the accuracy problem of the dictionary-based MT. This approach identifies the structure and semantics of the text in the source language. Then, it transforms the linguistic meaning into an intermediate language-neutral globally unified representation. The intermediate representation is then used to generate the target language text. This approach raised concerns regarding its efficiency. Therefore, researchers proposed the Transfer-based approach, which functions like the interlingual translation. However, the intermediate representation in the new approach emphasizes structural transfer rather than meaning transfer and is not unified across languages [33].\nIn contrast to RBMT, EBMT was introduced in the mid-1980s to solve the efficiency and accuracy problems of the RBMT [28]. This was done by utilizing a database of translated sentences for translation reference. The model identifies and modifies"}, {"title": "Proposed Conversational End-to-End Sign Language Machine Translation Framework", "content": "SLMT is a process that involves seamless conversion between sign language and spoken language in both directions, supported by the computer vision and NLP domains. Computer vision involves processing visual data [47], which helps understand and generate signs. NLP [48], on the other hand, contributes to translating these expressions into spoken language. In this section, we present a framework for SLMT in terms of stages to describe the data flow used in the translation process. Fig. 5 illustrates the stages of our framework.\nStage 1: Data Collection of Gestures. Various devices can capture the sign input, including sensor-based wearable devices [49], such as gloves and armbands, and cameras. These cameras can either be vision-based [17] or vision-sensor-based, such as Microsoft Kinect [50], [51]. These instruments can acquire different sign language parameters, such as physical gestures produced by the hands, body movement, and facial expressions, to convey rich linguistic information.\nStage 2: Computer Vision for Gesture Recognition. The signs collected from the previous stage are transformed into gloss, facilitating the S2G process. This builds a connection between gestural communication and linguistic representation, a fundamental element for an accurate translation [52]. Computer vision algorithms can perform this transformation by incorporating data augmentation to enhance the dataset, pre-processing to clean and prepare the data for further analysis, and feature extraction to acquire the glosses for translation [10].\nStage 3: Translation to Text. NLP models translate sign language gloss to spoken language text through G2T translation. This is achieved by applying tokenization and feature extraction methods to comprehend the context and linguistic patterns [53]. Individuals who do not know sign language will be able to understand the output of this translation.\nStage 4: Text Acquisition. Different devices serve as sources for spoken language text input including speech-to-text conversion systems, microphones, or other audio input devices.\nStage 5: Translation to Gloss. This is the reverse direction of the G2T translation, which converts written spoken language text into gloss through T2G via similar steps [54].\nStage 6: Sign Generation. This process focuses on visualizing and generating sign language gestures by translating gloss into sign language expressions represented as image frames facilitating the G2S transformation. The output can be an avatar that functions as a dynamic representation of the signs and facial expressions to close the communication gap between spoken and sign languages [19]."}, {"title": "Sign Language Datasets", "content": "We present a comprehensive list of the public SLMT in Table 3. Each dataset has essential attributes, including the year, sign language, number of signers (#Signers), number of videos (#Videos), resolution, and acquisition mode. These datasets have different environments, qualities, constraints, and complexities.\nSLMT datasets cover different sign languages, including American [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], Arabic [50], [76], [77], [78], [79], Australian [63], Brazilian [80], [81], British [82], [83], Chinese [52], [84], [85], [86], [87], [88], Columbian [89], Finnish [90], French [83], German [17], [83], [91], [92], [93], Greek [83], [94], Indian [95], [96], Irish [97], Korean [98], [99], Persian [100], Polish [101], Russian [102], and Turkish [103], [104]. There is a preference among researchers for developing datasets using Continuous Sign Language Recognition (CSLR) over Isolated Sign Language Recognition (ISLR). This aligns with the practical relevance of CSLR in real-world scenarios, where sign language communication has a continuous and dynamic nature [4]. Moreover, the resolution and acquisition methods vary for some datasets. These variations introduce more diversity and complexity, potentially making them valuable resources for addressing challenging research tasks.\nFig. 6 presents the frequency and average number of videos for the public continuous SLMT datasets. The figure shows that even though American Sign Language (ASL) dominates other sign languages in frequency, it has one of the lowest average numbers of videos (1106). The dominance of ASL in this list is due to its popularity [105]. Hence, the availability of its resources and heightened research interest. On the other hand, the figure shows that Chinese Sign Language has the largest average number of videos (24667). Fig. 7 presents the frequency and average number of videos for the public-isolated SLMT datasets. Like the previous figure, ASL dominates other sign languages in frequency, although it has an average number of videos of 17779, which is the fifth largest sign language. The figure also shows that the Chinese Sign Language has the largest average number of videos (65000). Moreover, Fig. 8 illustrates the coverage of components of the public datasets, showing that most public SLMT datasets cover video frames and gloss, followed by datasets that cover all three components and a few datasets that cover video frames and text. This indicates that the direct S2T translation is not as popular as S2G or S2G2T, highlighting the importance of gloss representation in this process.\nIn summary, German Sign Language has attracted more attention due to the implementation of Phoenix-2014 [91] dataset and its extension Phoenix-2014T [17]. Based on the popularity of Phoenix-2014T dataset, we suggest utilizing it when introducing new models to ensure accurate comparisons with existing literature until a more extensive and diverse dataset is available. We also suggest creating a large continuous SLMT multilingual dataset to aid in building one generative multilingual SLMT model."}, {"title": "Performance Evaluation Metrics", "content": "The assessment of translation systems is essential for understanding their effective-ness and accuracy. This section describes the performance metrics used in SLMT which are Bilingual Evaluation Understudy (BLEU) [106], Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [107], Word Error Rate (WER) [108], and Accuracy.\nBilingual Evaluation Understudy (BLEU)\nBLEU is used to measure the similarity between the machine and human translations of sign language to spoken language and vice-versa. It focuses on the precision of n-grams [106]. BLEU scores are expressed on a scale from 0 to 1, where 1 indicates a perfect match with the reference translation.\nThis metric is divided into BLEU-1, BLEU-2, BLEU-3, and BLEU-4. These variants evaluate the concordance of the respective n-grams between the machine and human translations. This allows the evaluation of the lexical accuracy and appropriateness of the translations.\nBLEU score is calculated using Equations (1) and (2).\nBLEU = BP * exp(\u03a3_{n=1}^{N} W_n log(p_n))\nRecal"}, {"title": "Taxonomy of Sign Language Machine Translation Algorithms", "content": "Fig. 9 shows a proposed taxonomy based on a retrospective analysis of the temporal evolution of the SLMT algorithms. We classify these algorithms into four categories: 1) RBMT [54], [109], [110], 2) EBMT [111], 3) SMT [112], [113], [114], [115], and 4) NMT [16], [17], [18], [19], [49], [52], [53], [85], [89], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135]. We map each of these algorithms to the corresponding framework sign language translation stages that were investigated in the literature. We also present a comparison between the SLMT algorithms in the literature in terms of the sign language considered, the dataset(s) used, the translation algorithms and the feature extraction techniques employed, and the corresponding performance, as shown in Table 4."}, {"title": "Rule-based Machine Translation (RBMT)", "content": "Sign language RBMT relies on grammatical rules, dictionaries, and syntactic and semantic analysis for translation [30]. Few works implemented RBMT for sign language translation. These works are divided into two categories: 1) T2G [54], [109], and 2) S2T [110]. Regarding T2G translation [54], [109], authors followed a three-step rule-based approach which constitutes of Part-of-Speech (POS) tagging, where every word is mapped to its corresponding type, such as noun, verb, and adjectives [136], chunk partial parser, where each sentence is chunked into sub-sentences [137], chunk transfer on the sub-sentence level [54] along with Morpho transfer on the word level [138] to produce the gloss. [54] applied the approach on a Greek dataset while [109] applied the same approach on an Arabic dataset of a smaller size, which resulted in a precision of 84 and 35 respectively. On the other hand, in S2T translation, [110] followed the three-step rule-based approach on Brazilian sign language videos and obtained a precision of 21.1. Rule1 shows an example of a grammatical rule in sign language [109], where S is the Subject, O is the Object, and V is the Verb.\nIF the input is an Arabic sentence with structure VOS, THEN reorder words to SVO\nIn summary, RBMT precision is language-dependent as it is high in Greek and low in Arabic. In addition, the algorithm may face challenges when dealing with dialectical variations of sign languages, expressions, and context-dependency [30]."}, {"title": "Example-Based Machine Translation (EBMT)", "content": "EBMT was introduced for language translation by looking at the similarity of the corresponding corpus. Despite its precise translations, EBMT has limited translation coverage because no corpus captures all the linguistic nuances [28]. In 2005, [111] presented an example-based approach to translating English text to Dutch gloss by searching for the best matches, based on word occurrences, POS labels, and bilingual dictionaries, and recombining relevant parts of the translated sentence using closed-class words [139] as markers to segment and align source and target sentences. Despite its precision, this method does not scale with a corpus increasing size [140]."}, {"title": "Statistical Machine Translation (SMT)", "content": "MT models shifted from EBMT to SMT due to the limitations faced by the former type and the need for a more efficient and scalable translation. SMT can solve these issues as it relies on large corpora and probabilistic methods [141] and handles ambiguity better than EBMT. The following are the SMT approaches used in SLMT:\nElastic Net Regression: In 2016, [114] translated 300 ASL sentences using a combination of L1 (Lasso) and L2 (Ridge) penalties of the lasso and ridge method. First, they applied feature mapping to convert the sentences into vectors using the n"}, {"title": "Neural Machine Translation (NMT)", "content": "NMT utilizes deep learning algorithms to translate between languages. Its components include neural network structures, datasets containing original and translated sentences, semantic representations through embeddings, encoder-decoder frameworks, attention mechanisms, and training processes involving backpropagation and gradient descent [40], [41]. These features give superiority to NMT over SMT as they allow the algorithm to handle complex language structures and capture long-range dependencies and language patterns. However, NMT still faces challenges, such as the need for larger datasets and the unique linguistic and spatial characteristics of sign languages.\nDeep Neural Network (DNN): In 2017, [123] captured more than 7000 isolated and continuous ASL signs from a private dataset and extracted their features using a leap motion sensor. The authors then translated the signs using a Hierarchical BiDNN with LSTM in S2T translation and obtained a 94.5% translation accuracy. The DNN algorithms they applied consist of neurons, weights, biases, and functions [145]. Initially, the input is fed into the input layer to be represented as vectors. Each neuron performs a weighted sum of inputs in the hidden layers followed by an activation function, as shown in Equation (15). Lastly, the output layer generates the result, depending on the problem.\nRecurrent Neural Networks (RNN): The implementation of RNN in SLMT focuses on two primary architectures: Gated Recurrent Unit (GRU) and Long-Short-Term Memory (LSTM). Both architectures are well-known for their ability to manage memory through gating mechanisms, allowing for selective retention or forgetting of information. While they share this key principle, GRU and LSTM have major differences in their architectures. GRU involves a hidden state (ht), reset (rt) and"}, {"title": "Taxonomy of Transformer Architectures for Sign Language Machine Translation", "content": "We present a classification of the component-based Transformer architectures in Fig. 12. We classify these architectures into four categories: 1) Encoder-decoder transformer (EDT) (Fig. 12 (a)), 2) Encoder-only transformer (EOT) (Fig. 12(b)), 3) Decoder-only transformer (DOT) (Fig. 12(c)), and 4) Encoder-decoder transformer fusion (EDTF) (Fig. 12(d)). Table 5 presents a comparison between the works that used these transformer architectures, along with their parameters, in the different stages of our proposed framework.\nThe EDT architecture was proposed by [150] in 2017. The architecture consists of a Tokenization block to transform the input sentences into words or sub-words that build the model's vocabulary. In addition, the architecture contains an Embedding block to transform the input sentences into vectors, plus a Positional Encoding block to provide information about the position of each word in the sentence. The model also consists of Encoder and Decoder layers. The encoder consists of the following components: Multi-Head Attention to weigh the importance of different parts of the input and capture the relationships between the input words. Feedforward Neural Networks add non-linearities and help the model capture more complex patterns. Normalization and Residual Connections Layers to stabilize and speed up the training of deep networks. The decoder consists of components similar to the encoder and a Masked Multi-Head Attention to prevent the model from accessing information about future positions. Following the decoder, the model consists of a Linear Layer to transform the decoder output into the desired dimension and a SoftMax activation to obtain the probability distribution over the vocabulary for each position in the output. This architecture has been widely used across various translation stages, including S2G [53], [119], G2T [53], G2S [134], S2G2T [18], and S2T [18], [52], [53], [126], [130], [134].\nThe EOT and DOT can be applied in a model that utilizes other algorithms. For instance, the GPT architecture consists of a Byte Per Encoding and a decoder-only transformer [42]. However, in the SLMT literature, the encoder-only and decoder-only transformers were applied in one work [16]. The researchers here applied the encoder for the S2G translation. The encoder was trained using a CTC loss to predict the gloss sequences. They also applied the decoder for the G2T translation. They trained and tested each part independently and joined them in an end-to-end S2G2T SLMT system.\nFurthermore, [19] implemented a variation of the encoder-decoder transformer that we call an \u201cencoder-decoder transformer fusion\u201d (EDTF) for G2T, T2G, and G2S translations. The model includes a Fast Furrier Transform (FFT) block between the embedding layer and the encoder. This is added to extract each word's frequency representation [152] as an additional feature to the embeddings. To enhance the gloss translation, the researchers incorporated a gated bilateral fusion layer within the decoder between the self-attention and the feed-forward network. This mechanism controls the influence of the input and its context while controlling the gradients of the propagation [153]. In"}, {"title": "Performance Evaluation of Transformers for Gloss-to-Text Translation: A Case Study", "content": "To understand the implications of deploying transformers-based sign language interpretation in real scenarios, we conduct empirical evaluations of the four transformer architectures which are underpinned by our taxonomy, in a unified environmental setup. It is necessary to deploy an efficient transformer in real-world scenarios. In this case study, we consider sign language gloss to spoken language text (G2T) machine learning translation.\nDatasets\nWe use the largest publicly available sign language PHOENIX-2014T dataset [17]. It consists of 8,257 weather-related sentences in German sign language. To assess the impact of a small dataset on the comparative performance of the different transformer architectures, we employ random 500 unique sentences of PHOENIX-2014T dataset. To evaluate the performance on a different type of dataset and language, we collect our own private ASL dataset, which we call \u201cMedASL\u201d. It consists of 500 medical-related sentences generated by ChatGPT [155] to reflect scenarios between patients and doctors, nurses, technicians, and registration desk staff in a medical center. An ASL expert then translated these sentences to gloss.\nExperimental Setup\nTo achieve the best performance possible in a sign language interpretation scenario, we perform hyperparameter tuning which determines the optimal values for transformer models' parameters. The parameters we study for each transformer model are presented in Table 6. These parameters are selected as the best parameters reported in the state of the art as described in Table 5. Consequently, for each dataset, we perform 16 experiments, corresponding to each combination of the four architectures and their respective hyperparameters configurations."}, {"title": "Challenges and Proposed Solutions", "content": "A real-time efficient SLMT system comprises a user-centric architecture that allows DHH and hearing users to interact with each other through a simultaneous translation from sign to spoken language and vice versa. The challenges of such a system include:\nAvailability of large and multilingual datasets: There are more than 300 sign languages globally [3], many of which are low in resources, such as the ArSL [156]. Consequently, the number of available datasets is scarce. Therefore, researchers need a large and accurate dataset for faster data generation and precision [51]. In addition, sign language datasets often include the translation between one spoken language and one sign language (e.g., Chinese and Chinese Sign Language) [21]. Consequently, limitations of current technologies include the training of monolingual Al models, in which models are usually trained on one pair of languages at a time. A solution for this challenge is the creation of crowdsourced platforms [157] for collecting, annotating, and translating sign language data. This approach overcomes the scarcity of resources in a particular sign language. It leads to the creation of a large multilingual dataset, which reduces the need for extensive datasets for each language. Another solution is to apply transfer learning techniques by pre-training a model on rich sign language datasets and then fine-tuning them on low-resource sign language [53]. This strategy is particularly relevant for including regional dialects and idiomatic expressions, which are critical for the accurate representation of sign languages in Al models.\nHigh deployment cost: The deployment of SLMT systems involves substantial financial investment, due to the advanced technology required for accurate sign language recognition and translation and the need for extensive infrastructure, including servers for processing and storing large datasets. The economic barrier extends to the cost of developing, testing, and continually updating these systems to accommodate new sign languages and dialects, making it a significant obstacle to widespread adoption. A potential solution to mitigate these costs is utilizing cloud and edge computing to reduce the need for expensive, dedicated infrastructure [158], and by distributing the computational load more efficiently and cost-effectively across edge and cloud servers [159].\nHigh retraining cost: Languages are constantly evolving [160]. Therefore, the current datasets will soon need to be updated, and current AI models must be retrained. This leads to high electricity and maintenance costs. [53] applied transfer learning techniques to grasp the knowledge from rich sign languages and apply it to low-resource languages. Through this approach, the model could learn universal sign language features, including the translation of idiomatic expressions and regional dialects. On the other hand, domain adaptation strategies allow the AI model to adjust to a specific in-domain low-resource sign language and out-of-domain translations for the same sign language with high translation accuracy [161].\nHigh energy consumption: SLMT systems involve Internet-of-Things (IoT) devices and edge and cloud servers to train the Al models and perform real-time translation. These components consume a high amount of energy. By 2025, global data"}, {"title": "Future Research Directions", "content": "In this section, we identify several promising directions for future research in this field. A critical area of focus is the development of more robust and scalable end-to-end SLMT systems that can handle a wide range of sign languages efficiently in terms of precision and execution time. While Transformer is the most used approach for sign language translation, as revealed by our survey, Reinforcement learning would aid in leveraging Transformer translation [169] for sign language translation. In addition, integrating sign language features, such as facial expressions and body language, could further enhance the accuracy and naturalness of SLMT systems.\nTransformer models have shown exceptional ability in understanding and generating complex spoken language patterns [150], which could be extended to the nuances of sign languages, with the potential of increasing the translation precision of an LSMT system. However, training the Transformer large datasets suffers from performance efficiency in terms of execution time, which hinders its deployment in real-time SLMT systems. Quantum computing is a promising technology to boost the performance of computing power-hungry applications [170]. It could reduce the computational time required for training deep learning models on large datasets due to their processing power. Therefore, the development of end-to-end SLMT systems enabled by quantum computing would revolutionize the domain of sign language translation, toward the deployment of more sophisticated SLMT systems while increasing their precision and execution time. This would encourage the adoption of these systems by DHH and hearing populations, enabling real-time conversation.\nHowever, deploying a real-time conversation end-to-end SLMT system faces privacy and security challenges of conversational data and videos collected for the DHH individuals. Blockchain has shown its potential in privacy-preserving health records management systems [166]. by creating decentralized ledgers to manage and securely store DHH individual data and consent logs, blockchain technology can provide a robust framework for protecting sensitive information while maintaining DHH individual trust in the SLMT system [171].\nFurthermore, to provide the users of SLMT systems with immersive, interactive, and engaging experiences for their users, the integration of scalable and real-time Metaverse [56], along with Virtual Reality (VR) [172], Augmented Reality (AR) [173], wearable devices [174], and educational tools [175] holds promise for revolutionizing how the DHH community interacts with the world."}]}