{"title": "LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment", "authors": ["Ge Yang", "Changyi He", "Jinyang Guo", "Jianyu Wu", "Yifu Ding", "Aishan Liu", "Haotong Qin", "Pengliang Ji", "Xianglong Liu"], "abstract": "Although large language models (LLMs) have demonstrated their strong intelligence ability, the high demand for computation and storage hinders their practical application. To this end, many model compression techniques are proposed to increase the efficiency of LLMs. However, current researches only validate their methods on limited models, datasets, metrics, etc, and still lack a comprehensive evaluation under more general scenarios. So it is still a question of which model compression approach we should use under a specific case. To mitigate this gap, we present the Large Language Model Compression Benchmark (LLMCBench), a rigorously designed benchmark with an in-depth analysis for LLM compression algorithms. We first analyze the actual model production requirements and carefully design evaluation tracks and metrics. Then, we conduct extensive experiments and comparison using multiple mainstream LLM compression approaches. Finally, we perform an in-depth analysis based on the evaluation and provide useful insight for LLM compression design. We hope our LLMCBench can contribute insightful suggestions for LLM compression algorithm design and serve as a foundation for future research. Our code is available at https://github.com/AboveParadise/LLMCBench.", "sections": [{"title": "1 Introduction", "content": "Recently, large language models (LLMs) have attracted increasing attention because of their strong intelligence ability. While it achieves excellent performance, the huge computation and storage burden hinders the practical usage of these LLMs. To solve this problem, many model compression techniques specifically designed for efficient LLMs have been proposed in recent years, including sparsification [6, 35], quantization [46, 33], knowledge distillation [8], and so on.\nAmong these compression technologies, sparsification and quantization are two mainstream approaches for LLM compression, and most LLM compression methods recently proposed belong to these two categories. However, existing LLM compression works are still far away from practical usage due to two main challenges:\nChallenge-1: Performance evaluation scope is limited. The emergence of large language models is less than two years, and this is still an active research area now. Following this trend, new types of LLMs have surged quickly in recent years. It causes a problem that the current LLM compression researches often use different types of LLMs for evaluation, which cannot form a fair comparison between different methods. For example, the classic quantization method SmoothQuant [46] uses OPT [49], BLOOM [21], and GLM [5] as the base model for evaluation, while the latter approach OmniQuant [33] utilizes LLaMA [38] for evaluation. The evaluation protocol can be very different between different methods. Moreover, even the base model performance is different in current works. For example, the perplexity of LLaMA-7B in sparsity method LLM-Pruner [27] is 12.62,"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Large Language Models", "content": "The emergence of large language models (LLMs) has become a milestone in the field of natural language processing. With the development of LLMs, decoder-based LLM has become the mainstream structure. For example, [30] proposed the GPT model to stack multiple transformer decoder blocks. Meta released LLaMA [38] based on an improved transformer architecture, which is further extended to LLaMA2 [39] and LLaMA3. In this paper, we choose 6 LLMs including LLaMA [38], LLaMA2 [39], LLaMA3, Vicuna [50], OPT [49], and ChatGLM [5] for evaluation to construct our LLMCBench, which are the most representative LLMs in the current research. Moreover, multimodal LLMs are advancing by integrating text and vision [43, 52, 51], enabling models to process and generate content across different modalities, enhancing applications like image captioning and visual question answering."}, {"title": "2.2 Model Compression", "content": "To reduce the massive computation and parameter burden of LLMs, many model compression methods were proposed in recent years. These works mainly focus on sparsity [6, 35, 45, 17, 2, 12, 11, 10, 15, 14, 41], quantization [46, 33, 13, 26], and knowledge distillation [8]. Among these approaches, sparsity and quantization are the most popular two techniques\u00b3. Thus, we choose these two techniques to construct our LLMCBench.\nModel sparsification. Model sparsification aims to remove unimportant weights or activations to construct a sparse model to reduce the parameter and computation of LLMs [19, 36, 18, 22]. It can be roughly categorized into unstructured sparsity, structured 2:4 sparsity, and structured sparsity. Unstructured sparsity removes individual weights irregularly to obtain sparse models. Structured sparsity removes the entire channel for structured matrix computation. Structured 2:4 sparsity removes two weights in each four-weight block. To comprehensively benchmark sparsity algorithms, we choose the most representative methods in each category for evaluation, i.e., SparseGPT [6] and Wanda [35] for unstructured and structured 2:4 sparsities, and LLM-Pruner [27] for structured sparsity.\nModel quantization. Model quantization aims to quantize the weight or activation in LLMs using lower bit numbers to reduce computation and parameters [42, 4, 47]. It can be roughly categorized into post-training quantization (PTQ) and quantization-aware training (QAT). Considering the high training cost of LLMs, the PTQ paradigm is more popular in the current research. To this end, we choose the four most representative quantization methods in this category in our LLMCBench, which includes GPTQ [7], SmoothQuant [46], AWQ [23], and OmniQuant [33]."}, {"title": "2.3 Challenges of LLM Compression", "content": "LLM compression methods have attracted increasing attention since 2023. However, as the LLM compression algorithms have emerged quickly in recent years, several challenges still remain in the current research. First, the performance evaluation protocols are different and limited. Different compression methods may select different baseline LLMs and datasets to evaluate their approach. This evaluation protocol may cause unfair comparison and also lacks a comprehensive comparison on specific abilities of LLMs, posing the question of which LLM compression method is more effective in a specific scenario. Second, the efficiency evaluation metrics are still theoretical in current research. Most LLM methods only report the #MACs or #parameters or acceleration after compression but do not consider other important factors in real-world production and deployment like training consumption and acceleration on different libraries etc. Moreover, the compressed LLMs are expected to be used in real-world applications. So the model trustworthiness after compression is also a crucial aspect for LLM compression algorithms, which is not considered in existing evaluation. Under this background, we construct our LLMCBench for comprehensive LLM compression algorithm evaluation."}, {"title": "3 LLMCBench: Tracks and Metrics", "content": "In this section, we introduce the competition tracks and metrics in our LLMCBench, which consists of six tracks. A higher score of the metric indicates better performance. For better readability, we have multiplied the theoretical score by 100."}, {"title": "3.1 Track 1: Compression Performance", "content": "Current LLM compression methods only compare the performance on several specific datasets but lack comprehensive evaluation on different abilities. In our LLMCBench, we divide the mainstream evaluation dataset into two main abilities: knowledge ability and inference ability. The knowledge ability indicates whether the LLM knows the world, while the inference ability indicates whether the LLM can reason based on its knowledge."}, {"title": "3.2 Track 2: Generalization Ability", "content": "We also evaluate the generalization ability of different LLM compression methods in our LLMCBench. An effective LLM compression algorithm should be effective for various model types and sizes, but existing researches only choose specific LLM families and sizes for evaluation. We design the overall metric of this track as follows:\n$OM_{gen} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(\\frac{A'_{mod_i}}{A_{mod_i}})^2}$ (2)\nwhere $A'_{mod_i}$ and $A_{mod_i}$, are the accuracy of the compressed model and pre-trained model for the $i$th model type, respectively. $N$ denotes the number of model types. In this track, we calculate the mean value of different model sizes under one model type in the mean operation $E$."}, {"title": "3.3 Track 3: Training Consumption", "content": "The third track in our LLMCBench is training consumption. It is intuitive that an effective LLM compression algorithm should require small resources to finish the compression process, but existing compression approaches lack comprehensive evaluation from this aspect. In this track, we evaluate the training consumption from two perspectives including time consumption and GPU memory usage. Time consumption indicates the time cost of LLM algorithms to finish the compression, while GPU memory usage evaluates the maximum required memory of each LLM compression method. Similar to the previous two tracks, we design the following overall metric for this track:\n$OM_{train} = \\sqrt{\\frac{1}{2}\\left(\\sqrt{E(\\frac{T_{max}}{T_{train}})^2} + \\sqrt{E(\\frac{M_{max}}{M_{train}})^2}\\right)}$ (3)\nwhere $T_{max}$ and $M_{max}$ are the maximum training time and GPU memory in all the evaluated methods for corresponding models and datasets. In this track, we calculate the mean value of training time and memory consumption over all models and datasets. The terms $T_{train}$ and $M_{train}$ are used for normalization to ensure we have a higher overall metric for better performance."}, {"title": "3.4 Track 4: Inference Consumption", "content": "Inference consumption is one of the most critical aspects of LLM compression algorithms for efficiency evaluation. However, current researches still lack systemic evaluation on this perspective. In our LLMCBench, we benchmark inference consumption from three main aspects: computation complexity, model size, and GPU memory consumption in the inference stage. Similar to other tracks, we design the overall metric of this track as:\n$OM_{inf} = \\frac{1}{3} \\sqrt{E\\left(\\frac{M_{inf}}{M'_{inf}}\\right)^2 + E\\left(\\frac{S_{inf}}{S'_{inf}}\\right)^2 + E\\left(\\frac{F_{inf}}{F'_{inf}}\\right)^2}$ (4)\nwhere $M_{inf}$, $S_{inf}$, and $F_{inf}$ are GPU memory, model size, and the number of MACs for pre-trained LLM at inference stage, respectively. $M'_{inf}$, $S'_{inf}$, and $F'_{inf}$ are those of compressed LLM at inference stage, respectively. In the mean operation $E$, we calculate the mean value over all models and datasets for the corresponding metric."}, {"title": "3.5 Track 5: Hardware Acceleration", "content": "Hardware acceleration is another important aspect of LLM compression algorithms for efficiency evaluation [9]. The implementation details in current compression methods often have a large impact on this aspect. Even the same compression method may have different acceleration performances on different libraries. Existing LLM compression approaches seldom extensively compare this important aspect, making the acceleration performance remain theoretical. Similar to previous tracks, we define the following overall metric for this track:\n$OM_{hard} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(\\frac{V'_{lib_i}}{V_{lib_i}})^2}$ (5)\nwhere $V'_{lib_i}$ and $V_{lib_i}$ are the token generation speed of pre-trained and compressed models on the ith library, respectively. In this track, we take the average value over all models and datasets on the ith library in the mean operation $E$. $N$ is the number of libraries we used in this track."}, {"title": "3.6 Track 6: Trustworthiness", "content": "The compressed LLMs need to be deployed in real-world scenarios. So model trustworthiness of the deployed LLMs is also a critical aspect [34] to avoid negative social impact. However, current compression methods do not include the trustworthiness evaluation when comparing performance. In our LLMCBench, we also evaluate the LLM compression algorithms from the trustworthiness perspective. Specifically, we divide model trustworthiness into robustness and truthfulness. Robustness refers to the ability of LLMs to properly handle malicious adversarial attack text and out-of-distribution input, while truthfulness indicates whether an LLM can output correct facts under the interference of noise, erroneous information, bias, etc. Similar to other tracks, we design the following metric for this track:\n$OM_{trust} = \\sqrt{\\frac{1}{2}\\left(E\\left(\\frac{A'_{rob}}{A_{rob}}\\right)^2 + E\\left(\\frac{A'_{tru}}{A_{tru}}\\right)^2\\right)}$ (6)\nwhere $A'_{rob}$ and $A_{tru}$ are the accuracy for the pre-trained LLMs on the robustness and truthfulness task, respectively. $A'_{rob}$ and $A_{tru}$ are those for the compressed LLMs, respectively. We also take the average over all models and datasets for the mean operation $E$."}, {"title": "4 LLMCBench Implementation", "content": "Implementation details. We implemented LLMCBench using PyTorch and conducted our experiments on Nvidia A800 GPUs. Given pre-trained LLMs, we use different LLM compression algorithms to compress the model to obtain the compressed LLM. For LLM-Pruner [27], we set the sparsity ratio as 50% for all tracks. For Wanda [35] and SparseGPT [6], we evaluate both 50% unstructured sparsity and structured 2:4 sparsity. As GPTQ [7] and AWQ [23] are weight-only quantization methods, we use 8bit for the weight (W8A16). For SmoothQuant [46] and OmniQuant [33], we set both weight and activation as 8bit (W8A8). One exception is we use W4A16 for GPTQ and AWQ and use W4A4 for SmoothQuant and OmniQuant in Track 2 as there is no significant performance difference for 8-bit quantization in this track. All the hyperparameters are the same as the open-sourced code from the original approaches.\nEvaluation protocal. For track 1, we adopt commonly used MMLU [16], Arc-eacy [3], and Arc-challenge [3] to evaluate the knowledge ability of LLMs. For inference ability, we choose six datasets including Hellaswag [48], PIQA [1], WinoGrande [32], QNLI [31], MNLI [44], and WikiText2 [28] for evaluation. Regarding model selection, we choose two popular decoder-based LLMs: LLaMA2-7B [39] and LLaMA3-8B for evaluation. For track 2, we extensively choose four model families including LLaMA [38], Vicuna [50], OPT [49], and ChatGLM [5], and also include different model sizes ranging from 6B to 70B. We evaluate the performance of compressed model on WikiText2. For track 3 and track 4, we use LLaMA2-7B and LLaMA3-8B on WikiText2 for evaluation, as they are widely used in many compression methods [46, 35]. For track 5, we choose three representative deployment libraries: TensorRT-LLM [29], vLLM [20], and MLC-LLM [37] to evaluate the acceleration of different algorithms. We categorize the algorithms into"}, {"title": "5 Evaluation and Analysis", "content": ""}, {"title": "5.1 Track 1: Compression Performance", "content": "Quantization offers better overall performance. Table 1 presents the results we evaluated in track 1. Quantization approaches have better overall performance than sparsity methods when compressing LLMs. For example, the overall metric score OMperf is smaller than 90 for most sparsity methods, while this metric score is over 95 for quantization approaches. This indicates quantization is more suitable to preserve LLM performance after compression.\nSparsity is better for inference ability, while quantization is better for knowledge ability. We also calculate the overall metric for knowledge ability OMka and inference ability OMia. Sparsity approaches often have higher overall inference ability, while quantization methods prone to preserve knowledge ability of LLMs. This indicates that we should use sparsity methods to compress LLMs if we focus more on their inference ability, and use quantization methods if preserving their knowledge capability is more critical."}, {"title": "5.2 Track 2: Generalization Ability", "content": "Weight-only quantization methods have good generalization ability under lower bit. The weight-only quantization methods GPTQ and AWQ have better generalization ability, which achieve over 95 overall metrics. This is because LLM is more sensitive to activation quantization.\nSmooth Quant is less general. As SmoothQuant involves activation quantization, it is less general to different models. This may be because SmoothQuant aims to deal with outliers, but outliers are different in different models.\nMost approaches cannot generalize well on ChatGLM2. All evaluated methods cannot perform well on ChatGLM2 except weight-only quantization approaches. Therefore, we need to specifically design compression methods if we need to deploy this model."}, {"title": "5.3 Track 3: Training Consumption", "content": "Wanda requires the least training resources. The results for track 3 are shown in Fig 2. The sparsity method Wanda requires the least training resources among these evaluated approaches. It has around 43 overall metric scores. On the other hand, the quantization method OmniQuant requires the most training resources. This is mainly because the compression time for OmniQuant is long.\nLearning is the bottleneck. The compression methods requiring a learning process often have higher training consumption. For example, OmniQuant requires more than 300 A800 GPU minutes to finish the compression, as the retraining/learning process is time-consuming. Therefore, if we need fast compression speed, we need to choose compression methods without learning.\nSmoothQuant and AWQ require less GPU memory. SmoothQuant and AWQ require less memory, while LLM-Pruner requires the highest one. This may be because LLM-Pruner and OmniQuant need to retrain the model, which takes more memory. Although Wanda, SparseGPT, and GPTQ do not require retraining, they need to calculate the sparsity/quantization metric based on the activation, which also takes more memory. Therefore, if the GPU memory is limited in the compression process, we can choose SmoothQuant or AWQ for compression."}, {"title": "5.4 Track 4: Inference Consumption", "content": "Quantization generally has less inference consumption. The results for track 4 are shown in Table 3. Quantization approaches often have higher overall metrics for inference consumption. This may be because quantization uses lower bits to represent full-precision numbers. Therefore, the GPU memory and model size will be reduced in the inference stage. On the other hand, although sparsity methods set unimportant weights/neurons to zero, they still need to be stored in the memory.\nLLM-Pruner is the best among sparsity methods. LLM-Pruner is the structured sparsity method, while the others are unstructured or structured 2:4 sparsity. So the entire structure can be directly removed for LLM-Pruner, while other sparsity methods fail to achieve this due to memory and cache issues. Therefore, we can choose structured sparsity methods if we want better inference consumption performance without special implementation.\nQuantization methods have similar inference consumption. Although using different quantization techniques, the quantized models from different algorithms have similar inference consumption except for SmoothQuant, as SmoothQuant does not support real quantization deployment in their open-sourced code. However, we believe we can use other deployment libraries for real quantization."}, {"title": "5.5 Track 5: Hardware Acceleration", "content": "INT4 quantization has the best acceleration performance. Fig 3 shows the hardware acceleration for track 5. The dark results represent testing on LLaMA2, and the light results represent testing on LLaMA3. (T) represents TensorRT-LLM, (V) represents vLLM, and (M) represents MLC-LLM. INT4 quantization can achieve promising speedup under various deployment libraries and achieves the highest overall metric under this track.\nStructured sparsity \u2248 INT8 quantization. Structured sparsity and INT8 quantization have similar overall metrics for this track, which indicates that structured sparsity and INT8 quantization can achieve similar speedup under different libraries.\nStructured 2:4 sparsity is not well-supported. For structured 2:4 sparsity, only TensorRT-LLM can achieve acceleration. This may be because vLLM and MLC-LLM do not support this sparsity paradigm. Therefore, we can use TensorRT-LLM for deployment on this sparsity type and should put more effort into this sparsity paradigm."}, {"title": "5.6 Track 6: Trustworthiness", "content": "Quantization brings better trustworthiness. From the results, quantization methods provide better trustworthiness than sparsity approaches. OMtrust are over 95 for quantization, while these numbers are below 95 for sparsity methods."}, {"title": "6 Discussion", "content": "From the evaluation of our LLMCBench, we have several conclusions: (1) Based on the current library and hardware development, quantization is more suitable for LLM compression because of better performance and hardware support. Considering the performance drop and acceleration, weight-only quantization like AWQ performs better than weight-activation quantization. (2) Weight-activation quantization like SmoothQuant is better in terms of inference efficiency (inference consumption and hardware acceleration). (3) Sparsity generally has better training efficiency. However, its hardware/library support is not well constructed in the current stage. It still requires further development in this area to achieve better compression performance."}, {"title": "7 Conclusion", "content": "In this paper, we presented a Large Language Model Compression Benchmark (LLMCBench) to systemically evaluate the LLM compression algorithms. Based on the evaluation results, we also provide an in-depth analysis to guide the further design of LLM compression approaches. We hope our LLMCBench can contribute insightful suggestions and serve as a foundation for future research.\nOne limitation of our LLMCBench is that we only choose the seven most representative approaches. We will include more LLM compression algorithms, such as LLM KV cache compression, in our future work. We will also introduce more tracks and datasets, such as coding datasets and mathematical datasets, to conduct more comprehensive tests on the compressed LLMs. Our LLMCBench aims to evaluate LLM compression algorithms for practical usage. So it does not have negative social impact."}]}