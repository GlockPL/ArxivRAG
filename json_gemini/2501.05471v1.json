{"title": "Found in Translation: semantic approaches for enhancing AI interpretability in face verification", "authors": ["Miriam Doh", "Caroline Mazini Rodrigues", "Nicolas Boutry", "Laurent Najman", "Matei Mancas", "Bernard Gosselin"], "abstract": "The increasing complexity of machine learning models in computer vision, particularly in face verification, requires the development of explainable artificial intelligence (XAI) to enhance in- terpretability and transparency. This study extends previous work by integrating semantic con- cepts derived from human cognitive processes into XAI frameworks to bridge the comprehension gap between model outputs and human understanding. We propose a novel approach combining global and local explanations, using semantic features defined by user-selected facial landmarks to generate similarity maps and textual explanations via large language models (LLMs). The methodology was validated through quantitative experiments and user feedback, demonstrating improved interpretability. Results indicate that our semantic-based approach, particularly the most detailed set, offers a more nuanced understanding of model decisions than traditional meth- ods. User studies highlight a preference for our semantic explanations over traditional pixel- based heatmaps, emphasizing the benefits of human-centric interpretability in AI. This work contributes to the ongoing efforts to create XAI frameworks that align AI models behaviour with human cognitive processes, fostering trust and acceptance in critical applications.", "sections": [{"title": "1. Introduction", "content": "The evolution and increasing complexity of machine learning models, particularly in com- puter vision, has highlighted the need for greater transparency and interpretability. This need has spurred the development of the field of Explainable AI (XAI) [1,2], which aims to elucidate ma- chine decision processes for human understanding. Accuracy, transparency, and interpretability are at the heart of XA\u0399."}, {"title": "2. Related works and motivation", "content": "In recent years, significant efforts have been made in computer vision to develop methods that explain model predictions using attribution techniques. Specifically, attribution techniques aim to identify which parts of the input data (such as image regions, pixels, or features) most influenced the model's decision, thus providing insights into how models make predictions. Those efforts include Class images Activation Mapping (CAM) [12,13] and Layer-wise Relevance Propagation (LRP) [14], which are model-specific, as well as model-agnostic methods such as LIME [9] and RISE [15]. As defined by [16], \u201cGiven the core question Q* addressed by XAI algorithms, [...] one of the challenges is 'Translation'. The [...] challenge in answering Q* is to translate the technical attributes that ML models use to discriminate between data items into interpreted attributes.\" This translation, which we associate with the concept of \u201cinterpretability,\u201d is important in ex- plainability. Despite their popularity in the research community, attribution methods do not always align with this essential principle of explainability. One of the main challenges in this field is translat- ing XAI solutions for a less experienced audience (layperson), as these methods provide infor- mation at the pixel level, leaving the cognitive burden of interpretation to the users [17]. Several works have attempted to bridge this gap by translating granular information into \"concepts\" that are semantically meaningful to humans. One trend in this direction is to use global explanations, i.e., explanations related to an entire class of the trained neural classifier, involving human-understandable concepts to explain attributes or abstractions. Examples include Prototype [18] and Concept Activation Vector (CAV) [19], which aim to visually represent the internal states of the neural network associated with a specific class. CAVs should represent concepts understandable to humans. However, the use cases proposed to demonstrate how these approaches explain their deci- sions are often limited and assume prior knowledge of the discriminative concepts for the task. Moreover, most datasets of human concepts may not be available for a specific domain and must be collected at high costs. Even if the dataset is available, there is a considerable risk that user-defined concepts may be incomplete or inaccurate, leading to poor or biased explanations [20,21]. One promising approach is described in [22] and proposes a method to discover interpretable visual concepts without supervision. Using clustering techniques, this method identifies features that users can easily understand, generating interpretable visual concepts directly from the data without requiring a predefined dataset of human concepts. Another work seeking to use \"concepts\" larger than pixels and meaningful to humans from a local explanation perspective is that of Apicella et al. [23]. They recently proposed a method based on hierarchical image segmentation using autoencoders, providing local explanations in various forms for the same ML model. This method offers greater granularity in explanations,"}, {"title": "3. Methodology", "content": "In our previous work, we explored how inspiration from human cognitive processes can enhance the understanding of decisions made by AI systems, specifically focusing on facial recognition. Through cognitive psychology, we highlighted how specific facial areas, such as the eyes and nose, play a crucial role in perceiving and recognizing faces. According to hu- man perceptual processes, these visual stimuli are organized into meaningful concepts through stages of selection, organization, and interpretation, facilitating the categorization of faces. An emblematic example of this process is the recognition of specific resemblances, as expressed in statements like: You look like your mother. You have the same eyes\", where the comparison of eyes\" underscores a similarity through a well-defined semantic area (Figure 1). Building on this idea, we raised concerns about the effectiveness of traditional saliency maps in computer vision, questioning their ability to emulate human reasoning processes. In particu- lar, human saliency maps will focus on specific face parts (based on top-down knowledge about faces and bottom-up rare features). At the same time, AI model saliency maps may also focus on textures or other features humans do not consider. In response to this critique, we developed"}, {"title": "3.1. Definition of Semantic Features", "content": "Expanding on the suggestions from previous work to bridge the comprehension gap between users and the explanations provided by models, we have utilized the landmarks provided by Mediapipe [25], an open-source framework by Google, as a foundation for creating customizable semantic maps. In our earlier study, we identified 13 hypothetical semantic areas of the face, including background zones, where \"background\" refers to any area outside the facial landmarks. This study explores various semantic configurations by varying the number of concepts for two primary reasons, as shown in Figure 3. First, employing different semantic masks allows us to assess the method's sensitivity to variations in these masks. Second, we aim to avoid imposing a single semantic definition of facial areas while recognizing that collecting multiple definitions would be prohibitively time-consuming for calculating the global importance of concepts. Therefore, we defined three sets representing different levels of detail, ranging from SET_0 (the least detailed with 13 broad features, where facial regions like the eyes, nose, and mouth are treated as single unified areas without separation between left and right sides) and SET_1 (also with 13 features, but with more granular division, including a distinction between the left and right sides of symmetrical facial features such as eyes and lips) to SET_2 (the most refined with 30 features, providing even finer segmentation across various facial areas). Figure 3 illustrates the process of creating these semantic features. In step 1, Mediapipe's landmarks are projected onto an example face. Then, users define semantic areas by selecting specific landmarks on Mediapipe's facemesh. Step 2 shows the three sets of human-based con- cepts with varying levels of granularity that we hypothesized for this study. The ability to define human concepts upon which to base the algorithm can bridge the com- prehension gap by integrating user-defined semantics into the explanation. This facilitates a tran- sition from explanations based on low-level features (individual pixels) to more comprehensible explanations based on middle-level features (aggregates of pixels)."}, {"title": "3.2. Concepts Extraction", "content": "Human cognition intuitively segments images into distinct semantic entities, such as eyes, nose, and mouth. Machines do not have this innate ability; the identification and differentiation of such concepts must be acquired through the training process. A significant challenge arises when the concepts learned by machines do not match those understood by humans, complicating the interpretation of machine decision processes. Traditionally, XAI methods have focused on providing case-by-case explanations that elu- cidate the rationale behind the model's decisions for individual images. Previous work [8] has introduced a methodology that aggregates these individual explanations into a comprehensive ranking of human-understandable concepts, prioritized by their importance to the model's infer- ence process. In this work, we compare three explanation methods used to find globally important semantic concepts. We use the framework proposed by [8] but instead of only using KernelSHAP [11], which is a local explanation technique, we also compare it to the results of using two global- aware techniques: LIME [9] and MAGE [10]. With this evaluation, we want to verify if global- aware methods outperform local ones in the face verification tasks. In the sequel, we describe the three compared XAI techniques and the adaptations to the face verification problem."}, {"title": "MAGE and EaOC", "content": "For the first one, we adapt the MAGE technique proposed by Ro- drigues et. al. [10]. This method uses the last convolutional layer of the network to find groups of similar responses to input patterns. This is made by a decomposition of this layer response according to the behavior of each feature map dimension. Originally, after finding the Maximum Activation Groups (MAGs), clusters of different be- haviors representing each one a concept, the authors proposed a visualization technique called Ms-IV [10] to show, in multiscale, the most important image regions in some image examples. For the visualization Ms-IV, the evaluation of each image patch is made by occlusions. How- ever, instead of using a distance metric to evaluate the difference before and after a patch occlu- sion, the authors propose the metric Class-aware Order Correlation (CaOC). This metric consid- ers how the output space of a specific class changes after an occlusion, using rankings. The use of this methodology, MAGE, provides global-aware explanations for two reasons: firstly, it considers the activation patterns to obtain global groups of concepts learned by the"}, {"title": "LIME", "content": "In addition to this adapted method, we also consider using Local Interpretable model- agnostic explanations (LIME) as a context-aware method. Even being local, the idea of approxi- mating a local partition of the embedding space with a surrogate model induces spatial awareness. As the original method presents approximations to one specific class, we adapted the surro- gate model to approximate the norm-1 of the embedding of each image."}, {"title": "KernelSHAP", "content": "As used in Doh et. al [8], we also test KernelSHAP for concepts extraction. However, as a local explanation method, it represents the most important concepts for each im- age, that can be posteriorly combined. This method uses LIME [9]'s interpretable components with Shapley values [26] to find the feature contribution to the model's output."}, {"title": "3.2.1. Concepts aggregation", "content": "At the end of our framework, each image explanation, using the segments from the human- based concepts' segmentation (as shown in Figure 5 (1)), will have scores representing the oc- clusion impact of each segment for the face verification task. They can be used to visually find the most impactful facial segments. However, this will be local to this image explanation. The idea behind concepts aggregation is to find globally important segments, i.e., segments that impact most of the images, before proposing the visualization. We maintain this step as it is in the previous work, which consists of combining a set of image explanations before obtaining the mentioned occlusion scores. The difference is that we propose a comparison between the local XAI method KernelSHAP used previously, with global-aware XAI explanation methods (such as LIME and MAGE). We apply the XAI techniques in a set of images, with the human- based concepts' segmentation (Figures 5 (1) and (2)), to order the most important regions for each image. This order is a ranking of the most important parts, which we can aggregate to obtain the final most important facial segments (Figure 5 (3)). A diversity of ranking aggregation method can be used, we opted to use BORDA count [27]. We proceed differently for MAGE, which already decomposes the network into different concepts. We first use the BORDA count to aggregate the concepts' ranking for each part of the"}, {"title": "3.3. Similarity map", "content": "We adopted the approach described in [8] to create similarity maps to provide local expla- nations, specifically implementing the single removal algorithm SO initially introduced by [28] with Gaussian masks and later adapted to semantic masks. This new approach differs in the fact that we incorporate the importance of each semantic face region, as determined in the Concepts Aggregation (Section 3.2.1), to weight the perturbations in Algorithm 1. Assume we have s face regions and a vector O of size s where $O_n$ represents the order of region n (with 0 being the most important and s - 1 the least important). The weight $g_n = s - O_n$ is then used to multiply the perturbation impact of AS for face region n. The basic idea of this algorithm is to perturb facial images to understand the impact of specific facial regions on the similarity score between two images. The main steps are outlined in Algorithm 1."}, {"title": "3.4. Generation of Textual Explanations Using Large Language Models", "content": "In previous work, a similarity map and a table reporting the contributions of each top seman- tic area were produced. To make this output more accessible to a general audience, we included a textual transcription of the table using large language models (LLMs). Specifically, we em- ployed three small LLMs available on Hugging Face: CodeLlama-7B [29], Zephyr-7B [30], and Beagle14-7B [31]. These models were selected for their ability to provide quick, coherent tex- tual explanations without requiring training. We tested their performance within the LMstudio environment [32]. CodeLlama-7B, developed by Meta, is optimized for code-related tasks but also generates coherent text, making it suitable for explaining technical content. Zephyr-7B is designed for nat- ural language understanding and generation across various domains. Beagle14-7B is known for fine-tuning instruction following and reasoning tasks, offering precise and contextually relevant responses. We provided these models with a specific prompt to generate explanations of the table values. The prompt contextualized the task, explaining that a face verifi- cation system assigns a cosine similarity score between two images and outlined how positive or negative values indicate similarity or dissimilarity in semantic areas. In particular, the models were instructed to explain the cosine similarity score by discussing the impact of each semantic area on the final result, using colour maps to indicate levels of similarity. The actual prompt used in this process is included below:"}, {"title": "4. Metrics", "content": "Inspired by the metrics of Bommer et al. [33], we will test three aspects of the obtained models' concept explanations: 1) faithfulness, 2) sensitivity and 3) randomization. Faithfulness refers to how close an explanation is to the real model's behavior. Sensitivity measures the impact of input changes on the model's output. Finally, randomization analyzes the impact of random changes on the model's output."}, {"title": "Occlusion of top concepts", "content": "These experiments test faithfulness and sensitivity. After find- ing the model's global concepts from the concepts' aggregation (Section 3.2.1), we iteratively occlude from the top concept to the least important concept. We expect, if we correctly choose the most important concepts, to have a high increasing rate of the difference (to the original output) at the beginning (top concepts) and a reduction of this rate at the end."}, {"title": "Randomized occlusion", "content": "Along with the occlusion of concepts, we perform the same exper- iment with a randomly generated order of concepts (to be occluded). We expect these results to behave as a low boundary to the other XAI methods."}, {"title": "4.2. User Evaluation and Feedback Methodology", "content": "An additional aspect we sought to investigate was the users' subjective perception of not only the system's explanations but also the overall framework. Specifically, it was necessary to obtain a comprehensive set of evaluations from users. Given that many design choices within the framework were predicated on the assumption that they would enhance interpretability, assessing these choices through feedback from actual users was imperative. We solicited feedback on the proposed semantic sets, particularly focusing on the clarity and utility of supplementary information accompanying the similarity map, such as tables or descriptive text generated via LLMs. Furthermore, we requested users to express their prefer- ence between traditional methods (LIME with superpixels) and our proposed semantic approach, specifically evaluating the clarity and user-friendliness of each method. Our objective was to engage a highly diverse audience in terms of background and to thor- oughly explore the issue of interpretability of explanations. To this end, the survey was dissem- inated via two social media platforms: Instagram (using a public account of one of the authors, which had over 1,000 followers) and Reddit, specifically in the r/SampleSize group, which comprises 222,000 members who voluntarily respond to surveys. Additionally, we utilized mail- ing lists from various laboratories with expertise in AI, law, and social studies at the universities of the authors."}, {"title": "5. Experiments and Results", "content": "The experiments utilized the color FERET database [34], wherein the images underwent pre- processing using the Multi-task Cascaded Convolutional Networks (MTCNN) [35] technique to crop the faces to a uniform dimension (NxN, where N equals 256). The methods were evaluated on FaceNet [36] models trained using the CasiaWebFace [37] and VGGFace2 [38] datasets."}, {"title": "5.1. Semantic Extraction", "content": "In these experiments, we analyze how much the top obtained concepts impact the final face representation and face verification task. We successively occlude image semantic regions rep- resenting the concepts, from most important to least important, and calculate the difference from the original output. We test three different semantic sets of regions. For the face representation comparison, we compare the difference between the 512-dimensional vectors of 750 images be- fore and after occlusion using the Euclidean distance. We present the results for Casia-WebFace and VGGFace2 trained models in Figure 6. For the face verification task comparison, we compare the difference between the similarity scores of 350 pairs of images before and after occlusion using the Euclidean distance. We present the results for CasiaWebFace and VGGFace2 trained models in Figure 7. We noticed four main things: all the methods can better perform than the random behavior at some point; the best method for this type of occlusion-based experiment is LIME for all the models and sets; given the decomposition nature (dividing the network into clusters of concepts) of the MAGE technique, it can reach good results only by combining the occlusion of more concepts (representing different parts of the network) which means at the beginning it performs poorly, but it can outperform SHAP after a few concepts for some models; SHAP has an average behavior as it is a local explanation and, as the tested images are different from the used for the concepts' extraction, it will only behave correctly if the network is well-generalized."}, {"title": "5.2. Local Explanation and Sensitivity Analysis", "content": "In this section, we present an example of the output for local explanations generated by our framework. Specifically, Figure 8 (left side) illustrates an explanation for images A and B for the VGG-Face model output (SAB = 0.94). The figure includes two similarity maps showing seman- tically similar (orange range) and dissimilar (purple range) areas, alongside a contribution table that highlights these semantic areas, displaying the magnitude of their similarity or dissimilar- ity. This example utilizes the semantic features from SET_2, the most detailed set among those hypothesized. Due to space constraints, we only display the similarity map and the contribution table, which are the results of the previously presented single removal algorithm. We will address the textual explanation in the user feedback section. On the right side of the figure, similarity maps for various cases are presented, ordered by true positives (such as two images of the same individual) and impostor pairs (such as two different individuals compared). These similarity maps provide detailed explanations. In the first case, the images are nearly identical except for the subject closing their eyes in one image. The similarity map identifies the eyes as dissimilar areas, while the rest of the face remains similar. In the second case, involving two different individuals, all features are marked as dissimilar. The third case is more intriguing: although the subjects differ in features, such as the left side of the nose and the lower lip, the model perceives the upper facial features as similar, explaining the similarity score of 0.47 despite the significant differences."}, {"title": "5.3. User Feedback Evaluation", "content": "In this section, we delve into the findings from our user survey aimed at evaluating the in- terpretability and effectiveness of our framework. The survey was designed to gather feedback on several key aspects, including the types of semantics used, the visualization methods, and the inclusion of textual explanations. Our objective was to understand how users perceive the different components of our framework and to identify areas for improvement based on their feedback. This analysis provides insights into user preferences and experiences across varying technical backgrounds, informing the refinement of our approach to enhance interpretability and satisfaction."}, {"title": "5.3.1. Participant Demographics", "content": "After evaluating the framework, we decided to assess the interpretability of the explanations through a Google Form survey, as previously introduced in Section 4.2. Our survey gathered feedback from 61 individuals over the course of one week. The distribution of the professional backgrounds of the participants is illustrated in the pie charts in Figure 10). The objective was to gather feedback from a diverse audience, encompassing various profes- sional backgrounds. Participants identified their professional background, which we categorized into two main groups: technical and non-technical. The technical group comprised 41% of the respondents, including those with a background in technology or engineering. The non-technical group, representing 59%, included all other professional backgrounds, resulting in a balanced distribution. Additionally, we collected further demographic data for a more comprehensive understanding of our participants. The majority of respondents were young adults, with 72% aged between 25 and 34 years. Smaller age groups included 10% aged 18-24, 7% aged 35-44, 5% aged 55-64, and 2% for both 45-54, 65 and above, and under 18 years. This concentration in the 25-34 age range could reflect a potential sampling bias related to the authors' ages. Regarding educational attainment, the respondents were predominantly highly educated: 55% held a Master's degree, 16% had completed high school or an equivalent, 14% had a Bach- elor's degree, and 5% had primary education. The survey also revealed a broad range of familiarity with AI among participants: 36% had a basic understanding of AI, 20% considered themselves very knowledgeable, 17% had moderate knowledge, 15% had no knowledge of AI, and 12% were experts in the field."}, {"title": "5.3.2. Framework and explanation evaluation", "content": "To comprehensively evaluate the proposed framework, we asked our users to assess various aspects, including the type of semantics, the visualization of the explanations, and the inclusion of textual transcription. Preference in Semantic: Collecting detailed semantics and definitions from all users was com- putationally time-consuming. Therefore, our experiments were based on three hypothetical se- mantics, which were later evaluated by the users. From the user feedback, four properties were frequently mentioned: Completeness: The extent to which a semantic set provides a detailed and comprehensive analysis, covering all relevant areas and features of the face. Clarity: The extent to which a semantic set is easy to understand and interpret for users. Precision: The extent to which a semantic set segments facial features with a high degree of specificity and accuracy."}, {"title": "Regarding the clarity of similarity values presented in the table", "content": "all technical participants found the values clear, with 86% rating them as fairly or extremely clear and 14% neutral. This unanimous agreement highlights the table's effectiveness for this group. Among non-technical participants, most found the similarity values clear, although about 6% did not. One participant noted, \"I am not very familiar with tables. I understand, for example, that the lower area around the right eye is very dissimilar and the right side of the nose is similar, but I cannot understand how you got the 64% value.\" This indicates that while the majority found the values clear, a small portion struggled with understanding them. The utility of the table for understanding the analysis was also evaluated. Among those with a technical background, 95% found the table useful, with only 5% indifferent and none finding it not useful. For non-technical participants, 74% found the table useful, 18% were indifferent, and 9% found it not useful. This indicates a slightly lower level of perceived utility among non-technical users than their technical counterparts. Technical participants who provided feedback on unclear aspects of the similarity table often suggested detailed improvements for better visualization. They demonstrated a clear understand- ing of the values but recommended aesthetic enhancements. For instance, one participant men- tioned, \"I find the table clear, but it would be more visually pleasant with some modifications.\u201d Another suggested, \u201cIt would be interesting to visualize similarity values only by clicking on the area of interest without having the table displayed directly.", "I am not very familiar with tables. I understand, for example, that the lower area around the right eye is very dissimilar and the right side of the nose is similar, but I cannot understand how you got the 64% value.\\\" Another noted, \u201cIt is an image not optimized for smartphones. Considering that most people do this on mobile, every image should be optimized for mobile.\" This feedback underscores the need for improved clarity and accessibility in the table's presentation. Overall, most participants in both groups found the explanations clear, with technical partic- ipants slightly preferring higher clarity ratings. Technical participants also demonstrated greater overall satisfaction with the method than non-technical participants, who exhibited more varied opinions. Both groups found the table useful for understanding the analysis, with a higher ma- jority among technical participants. While all technical participants found the similarity values clear, a small percentage of non-technical participants did not. Some non-technical participants expressed difficulties in understanding specific values in the table.\"\n    },\n    {\n      \"title\": \"Textual Transcription\",\n      \"content\": \"As mentioned in Section 3.4, we used automatic textual transcription of similarity values provided by Codellama instruct-7B, Zephyr-7B-beta, and Beagle14-7B. We show the preferences in Table 2. Each model demonstrated distinct stylistic approaches to explain the face verification model's decisions, informed by a detailed prompt. Due to space constraints, the full-text outputs of each model are reported in the Appendix A.1. Codellama instruct-7B offers a detailed and practical explanation, focusing on specific facial features to illustrate similarities and dissimilarities between two images. For example, it states, \\\"Specifically, it (the model) is seeing differences in the right eye, left eye, upper area of the mouth, central area of the forehead, and right cheek.": "t also uses relatable examples: \u201cFor"}, {"title": "5.4. Proposed method vs. Traditional one", "content": "An additional question asked to users was to express a preference between the semantic method proposed in the framework and a traditional method. For the traditional method, we opted to use LIME in its most common visual form, namely superpixels and heatmaps (with red indicating more important areas and blue indicating less important ones), as show in Figure 11.a, providing a brief explanation to help users understand the use of colours and the division into superpixels. Among the analysed group of users, there was a clear preference for the semantic approach (78%) compared to the traditional approach (12%). The main reasons for this preference can be summarised into four main categories (technical and not technical distribution shown in Tab.3) Firstly, users appreciated the level of detail and clarity provided by the semantic approach. They noted that this method \"provides more details\" and \"gives a clearer picture of the metrics\". Fur- thermore, it was observed that the semantic method is \u201cbetter for understanding specific fea- tures\". Secondly, many users highlighted that the reasoning required by the semantic method is closer to how people normally compare two faces. One user stated that \u201cit's closer to the way I reason when comparing two faces\", while another found that \u201cthe semantic approach seems clearer since the different parts of the face are easier to identify and compare\". Thirdly, the semantic method was perceived as more logical and understandable. Users men- tioned that \u201cthe semantic approach is linked to how humans attribute meaning", "ex- plains similar areas better\". Finally, the semantic approach offered a clearer visual mapping. As one user noted, \u201cthe location of importance is clearer\" and \"the semantic approach seems clearer as it breaks down the interest/analyzed areas and scores them\". For those users who preferred the traditional method, the main reasons revolved around its simplicity and familiarity. They indicated that it is \u201ceasier to understand quickly\u201d and that it is": "raditional, easier to understand.\u201d Some users felt that \u201cthe traditional method is clearer than the semantic one.\u201d"}, {"title": "6. Conclusion", "content": "In this study, we proposed a novel framework for enhancing explainability in AI models used for face verification by integrating human-centric semantic approaches. Our approach extends traditional XAI methods by combining global and local explanations derived from user-defined semantic facial features. We validated the framework through both quantitative experiments and user feedback, intending to demonstrate its ability to improve interpretability. Key innova- tions include the introduction of several enhancements over our previous work: we expanded the framework by incorporating a more refined strategy for combining global and local explanations, adapting context-aware algorithms (such as LIME and MAGE) to extract global concepts from the models. Additionally, we quantitatively evaluated the importance of these concepts through occlusion experiments, a method not previously explored. We also integrated Large Language Models (LLMs) to automatically generate textual explanations, offering a more accessible in- terpretation of the model's outputs. Lastly, we conducted an extensive user study to assess the impact of our approach on user perception, marking a significant expansion from the initial con- ceptual framework presented earlier. With the semantic extraction experiments, we noticed that global-aware methods such as LIME can present better behavior for finding important semantic face regions. Local explanation methods behave well when analyzing similar images, but may not be stable for a global analysis even after ranking aggregation. The survey results showed that our semantic approach was positively received by participants, who recognized its logic and utility, especially in terms of visualizing explanations through se- mantic similarity maps. Users appreciated having a more intuitive and detailed explanation than traditional methods, such as superpixel-based heatmaps. However, some participants highlighted the need for further refinement in the level of detail in the explanations, particularly to make se- mantic concepts clearer and improve the accessibility of the information. Compared with the traditional method, the majority of participants preferred our approach, finding it clearer and more aligned with their way of reasoning about facial similarities. Despite this success, we must acknowledge that our survey sample was limited to 61 people, and there may be a \"bubble\" effect due to the similarity in age and background between the authors and some participants. Nevertheless, the feedback collected provides valuable insights for further improving the framework and optimizing the user experience. Future work will focus on refining the semantic sets to further enhance clarity and usability, particularly for non-technical audiences. Additionally, we plan to explore the integration of more advanced LLMs to improve the quality and relevance of the textual explanations. Expanding the framework to include more diverse datasets and testing its applicability across different AI tasks will also be crucial steps in advancing the utility and acceptance of explainable AI in critical applications."}, {"title": "Appendix A.2.1. Codellma instruct-7B", "content": "''Based on the model's output (cosine similarity) and its capabilities, we can explain that the face verification system sees two images as very similar in terms of features related to human-understandable concepts. Specifically,"}, {"title": "Appendix A.2.2. Zephyr-7b-beta", "content": "''Based on the provided concepts and their associated scores, it can be observed that while some areas such as the left and right sides of the nose, lower area of the mouth, and central area of the forehead have similarities between the two images, other areas like the lower area around the right eye, right eye, left eye, upper area of the mouth, and right cheek show dissimilarities. The overall cosine similarity score of suggests that while there are some differences between the two images, particularly in certain facial features, there are still notable similarities as well. These areas contribute differently to the final score based on their specific importance in distinguishing between faces. Areas with dissimilarities tend to lower the overall cosine similarity score, while areas with similarities tend to increase it. However, the exact impact of each semantic area on the final score is not explicitly stated and would require further analysis.''"}, {"title": "Appendix A.2.3. Beagle14-7B", "content": "''The [cosine_similarity_percentage) between two images indicates a moderate level of similarity, where certain facial features are seen as more or less alike in the respective images. The color map will display shades of orange where those similarities are found (i.e., higher scores), while shades of purple indicate differences or dissimilarities (lower scores). The 'Left side of the nose' and 'Lower area around the right eye,' for example, have lower similarity values indicating that they were seen as less alike in both images. On the other hand, areas like the 'Right cheek,' 'Central area of the forehead,' or the 'Left cheek' showed slightly higher similarity scores, which means these features were more similar in both images. Overall, the final cosine similarity score is a combination of the contributions made by each facial area. A difference or similarity in a specific area could either decrease (dissimilar) or increase (similar) the overall result, depending on its individual value. However, this breakdown provides only a high-level"}, {"title": "Appendix A.3.1. Inclusion of \u201c(it's a percentage from 0 to 100%)\u201d and \u201c(\u22650), example: -0.5)\")\"", "content": "The inclusion of these specific instructions was crucial for improving the clarity", "included": "Based on the model", " (Codellama instruct-7B) In this case, the model effectively explains how certain areas contribute to the 64% similarity score, linking the specific facial features to the overall result. However, when these instructions are removed, the model": "explanation becomes more technical and potentially confusing, focus- ing on abstract concepts such as \"distance between the left and right eye\" without clear reference to the provided similarity data. ''The cosine similarity score between the two images is 64% because certain visual features of the human face are present and consistent across the two images, while others are not. In particular, there are differences in the distance between the left and right eye... [..."}]}