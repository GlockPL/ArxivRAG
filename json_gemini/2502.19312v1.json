{"title": "FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users", "authors": ["Anikait Singh", "Sheryl Hsu", "Kyle Hsu", "Eric Mitchell", "Stefano Ermon", "Tatsunori Hashimoto", "Archit Sharma", "Chelsea Finn"], "abstract": "Effective personalization of LLMs is critical for a broad range of user-interfacing applications such as virtual assistants and content curation. Inspired by the strong in-context learning capabilities of LLMs, we propose Few-Shot Preference Optimization (FSPO), which reframes reward modeling as a meta-learning problem. Under this framework, an LLM learns to quickly adapt to a user via a few labeled preferences from that user, constructing a personalized reward function for them. Additionally, since real-world preference data is scarce and challenging to collect at scale, we propose careful design choices to construct synthetic preference datasets for personalization, generating over 1M synthetic personalized preferences using publicly available LLMs. In particular, to successfully transfer from synthetic data to real users, we find it crucial for the data to exhibit both high diversity and coherent, self-consistent structure. We evaluate FSPO on personalized open-ended generation for up to 1,500 synthetic users across across three domains: movie reviews, pedagogical adaptation based on educational background, and general question answering, along with a controlled human study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in generating responses that are personalized to synthetic users and a 72% winrate with real human users in open-ended question answering.", "sections": [{"title": "1. Introduction", "content": "As language models increasingly interact with a diverse user base, it becomes important for models to generate responses that align with individual user preferences. People exhibit a wide range of preferences and beliefs shaped by their cultural background, personal experience, and individual values. These diverse preferences may be reflected through human-annotated preference datasets; yet, current preferences optimization techniques like reinforcement learning from human feedback (RLHF) largely focus on optimizing a single model based on preferences aggregated over the entire population. This approach may neglect minority viewpoints, embed systematic biases into the model, and ultimately lead to worse performance compared to personalized models. Can we create language models that can adaptively align with personal preferences of the users and not the aggregated preferences of all users?\nAddressing this challenge requires a shift from modeling a singular aggregate reward function to modeling a distribution of reward functions that capture the diversity of human preferences. By doing so, we can enable personalization in language models, allowing them to generate a wide range of responses tailored to individual subpopulations. This approach not only enhances user satisfaction but also promotes inclusivity by acknowledging and respecting the varied perspectives that exist within any user base. However, how can this be effectively done for open-ended question answering and transfer to real users?\nIn this paper, we introduce Few-Shot Preference Optimization (FSPO), a novel framework designed to model diverse subpopulations in preference datasets to elicit personalization in language models"}, {"title": "2. Related Work", "content": "Personalized learning of preferences. Prior research has explored personalization through various methods. One approach is distributional alignment, which focuses on matching model outputs to broad target distributions rather than tailoring them to individual user preferences. For example, some prior work have concentrated on aligning model-generated distributions with desired statistical properties, yet they do not explicitly optimize for individual preference adaptation. Another strategy involves explicitly modeling a distribution of rewards. However, these methods suffer from sample inefficiency during both training and inference."}, {"title": "3. Preliminaries and Notation", "content": "Preference fine-tuning algorithms, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), typically involve two main stages: Supervised Fine-Tuning (SFT) and Preference Optimization (DPO/RLHF). First, a pre-trained model is fine-tuned on high-quality data from the target task using Supervised Fine-Tuning (SFT). This process produces a reference model, denoted as \\( \\pi_{ref} \\). The purpose of this stage is to bring the responses from a particular domain in distribution with supervised learning. To further refine \\( \\pi_{ref} \\) according to human preferences, a preference dataset \\( D_{pref} = \\{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\\} \\) is collected. In this dataset, \\( x^{(i)} \\) represents a prompt or input context, \\( y_w^{(i)} \\) is the preferred response, and \\( y_l^{(i)} \\) is the less preferred response. These responses are typically sampled from the output distribution of \\( \\pi_{ref} \\) and are labeled based on human feedback.\nMost fine-tuning pipelines assume the existence of an underlying reward function \\( r^*(x, \\cdot) \\) that quantifies the quality of responses. A common approach to modeling human preferences is the Bradley-Terry (BT) model, which expresses the probability of preferring response \\( y_1 \\) over \\( y_2 \\), given a prompt \\( x \\), as:\n\\[\np^*(y_1 > y_2 | x) = \\frac{e^{r^*(x,y_1)}}{e^{r^*(x,y_1)} + e^{r^*(x,y_2)}}\n\\]\nHere, \\( p^*(y_1 > y_2 | x) \\) denotes the probability that \\( y_1 \\) is preferred over \\( y_2 \\) given \\( x \\)."}, {"title": "4. The Few-Shot Preference Optimization (FSPO) Framework", "content": "Personalization as a meta-learning problem. Generally, for fine-tuning a model with RLHF a preference dataset of the form: \\( D_{pref} = \\{(x^{(t)}, y_w^{(t)}, y_l^{(t)})\\} \\) is collected, where \\( x \\) is a prompt, \\( y_w \\) is a preferred response, and \\( y_l \\) is a dispreferred response. Here, preferences from different users are aggregated to learn the preferences over a population. However, through this aggregation,\nThe objective of preference fine-tuning is to optimize the policy \\( \\pi_\\theta \\) to maximize the expected reward \\( r^* \\). However, directly optimizing \\( r^* \\) is often impractical due to model limitations or noise in reward estimation. Therefore, a reward model \\( r_\\phi \\) is trained to approximate \\( r^* \\). To prevent the fine-tuned policy \\( \\pi_\\theta \\) from deviating excessively from the reference model \\( \\pi_{ref} \\), a Kullback-Leibler (KL) divergence constraint is imposed. This leads to the following fine-tuning objective:\n\\[\n\\max_\\pi \\mathbb{E}[r^*(x, y)] - \\beta D_{KL}(\\pi \\|\\| \\pi_{ref})\n\\]\nIn this equation, the regularization term weighted by \\( \\beta \\) controls how much \\( \\pi_\\theta \\) diverges from \\( \\pi_{ref} \\), based on the reverse KL divergence constraint. This constraint ensures that the updated policy remains close to the reference model while improving according to the reward function.\nReward model training. To fine-tune the large language model (LLM) policy \\( \\pi_\\theta(y | x) \\), the Bradley-Terry framework allows for either explicitly learning a reward model \\( r_\\phi(x, y) \\) or directly optimizing preferences. Explicit reward models are trained using the following classification objective:\n\\[\n\\max_\\Phi \\mathbb{E}_{D_{pref}} [\\sigma(r_\\phi(x, y_w) - r_\\phi(x, y_l))]\n\\]\nwhere \\( \\sigma \\) is the logistic function, used to map the difference in rewards to a probability. Alternatively, contrastive learning objectives such as Direct Preference Optimization and Implicit Preference Optimization utilize the policy's log-likelihood \\( \\log \\pi_\\theta(y | x) \\) as an implicit reward:\n\\[\nr_\\theta(x, y) = \\beta \\log (\\pi_\\theta(y | x) / \\pi_{ref}(y | x))\n\\]\nThis approach leverages the policy's log probabilities to represent rewards, thereby simplifying the reward learning process.\nMore formally, consider that each unique user \\( S^{(i)} \\)'s reward function is characterized by a set of preferences with prompt and responses \\( (x, y_1, y_2) \\), and preference label \\( c \\) (indicating if \\( y_1 > y_2 \\) or \\( y_1 < y_2 \\)). Given a distribution over users \\( S = P(S^{(i)}) \\), a meta-learning objective can be derived to minimize its expected loss with respect to \\( \\theta \\) as:\n\\[\n\\min_\\theta \\mathbb{E}_{S^{(i)}\\sim S} [\\mathbb{E}_{(x,y_1,y_2,c)\\sim D_i} [L_{pref}(x, y_1, y_2, c)]]\n\\]\nwhere \\( D_i \\) is a distribution over preference tuples \\( (x, y_1, y_2, c) \\) for each user \\( S^{(i)} \\), and \\( L_{pref} \\) is a preference learning objective such as DPO or IPO:\n\\[\nL_{pref} = \\|\\|h_{yw} - (2c-1)\\|^2,  h = log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\theta(y_l|x)} - log\\frac{\\pi_{ref}(y_w|x)}{\\pi_{ref}(y_l|x)}\n\\]\nwhere \\( y_w \\) and \\( y_l \\) are the preferred and dispreferred responses (respectively) according to the responses \\( y_1, y_2 \\) and class label \\( c \\) in the preference dataset.\nFollowing black-box meta-learning approaches, FSPO receives as input a sequence of preferences \\( D_{fewshot} \\sim D_i \\) from a User \\( S^{(i)} \\). This is followed by an unlabeled, held-out preference \\( (x, y_1, y_2) \\sim D_\\text{heldout} \\) for which it outputs its prediction \\( c \\). To make preferences compatible with a pre-trained language model, a few-shot prompt is constructed, comprising of preferences from a user and the held-out query."}, {"title": "5. Domains to Study Personalization", "content": "To study personalization with FSPO we construct a benchmark across 3 domains ranging from generating personalized movie reviews (Reviews), generating personalized responses based off a user's education background (ELIX), and personalizing for general question answering (Roleplay). We open-source preference datasets and evaluation protocols from each of these tasks for future work looking to study personalization (sample in supplementary).\nReviews. The Reviews task is inspired by the IMDB dataset, containing reviews for movies. We curate a list of popular media such as movies, TV shows, anime, and books for a language model to review. We consider two independent axes of variation for users: sentiment (positive and negative) and conciseness (concise and verbose).\nELIX. The Explain Like I'm X (ELIX) task is inspired by the subreddit \"Explain Like I'm 5\" where users answer questions at a very basic level appropriate for a 5 year old. Here we study the ability of the model to personalize a pedagogical explanation to a user's education background. We construct two variants of the task. The first variant is ELIX-easy where users are one of 5 education levels (elementary school, middle school, high school, college, expert) and the goal of the task is to explain a question such as \u201cHow are beaches formed?\u201d to a user of that education background. The second, more realistic variant is ELIX-hard, which consists of question answering at a high school to university level.\nRoleplay. The Roleplay task tackles general question answering across a wide set of users, following PRISM and PERSONA Bench to study personalization representative of the broad human population. We start by identifying three demographic traits (age, geographic location, and gender) that humans differ in that can lead to personalization."}, {"title": "6. Sim2Real: Synthetic Preference Data Transfers to Real Users", "content": "Collecting personalized data at scale presents significant challenges, primarily due to the high cost and inherent unreliability of human annotation. Curating a diverse set of users to capture the full spectrum of real-world variability further complicates the process, often limiting the scope and representativeness of the data. Synthetically generating data using a language model is a promising alternative, since it can both reduce costly human data generation and annotation and streamline the data curation process. Can we generate diverse user preference data using language models in a way that transfers to real people?\nWe draw inspiration from simulation-to-real transfer in non-language domains like robotics and self-driving cars, where the idea of domain randomization has been particularly useful in enabling transfer to real environments. Domain randomization enables efficient adaptation to novel test scenarios by training models in numerous simulated environments with varied, randomized properties.\nBut why is this relevant to personalization? As mentioned previously, each user can be viewed as a different \"environment\u201d to simulate as each user has a unique reward function that is represented by"}, {"title": "7. Experimental Evaluation", "content": "Baselines. We compare FSPO against four baselines: (1) a base model generating user-agnostic responses, (2) few-shot prompting with a base model, following (3) few-shot supervised fine-tuning (Pref-FT) based off the maximum likelihood objective from and (4) prompting with an oracle user description following Specifically, for (1) we use a standard instruct model that is prompted solely with the query,"}, {"title": "8. Discussion and Conclusion", "content": "We introduce FSPO, a novel framework for eliciting personalization in language models for open-ended question answering that models a distribution of reward functions to capture diverse human preferences. Our approach leverages meta-learning for rapid adaptation to each user, thereby addressing the limitations of conventional reward modeling techniques that learn from aggregated preferences. Through rigorous evaluation in 3 domains, we demonstrate that FSPO's generations are consistent with user context and preferred by real human users. Our findings also underscore the importance of diversity and structure in synthetic personalized preference datasets to bridge the Sim2Real gap. Overall, FSPO is a step towards developing more inclusive, user-centric language models."}, {"title": "9. Limitations and Potential Risks", "content": "There are several limitations and potential risks. One limitation pertains to the ethical and fairness considerations of personalization. While FSPO improves inclusivity by modeling diverse preferences, the risk of reinforcing user biases (echo chambers) or inadvertently amplifying harmful viewpoints"}, {"title": "A. Appendix", "content": "A.1. Sample Personalized Responses\nWe provide sample responses from FSPO in Figure 6 across the 3 tasks that were studied (ELIX, Reviews, and Roleplay). We additionally include the oracle scoring description for each response, demonstrating FSPO's ability to personalize. Additionally, for the Review task, we show responses for interpolated users which were not found in the preference dataset, showing the ability of FSPO to generalize and be controllable.\nA.1.1. Roleplay CoT\nQUESTION:\nWhat are some good volunteer opportunities for me?\nTRUE PERSONA:\nA 53-year-old woman living in North America, an avid traveler and\nwriter who shares her adventures through a popular travel blog. She\nloves spontaneous adventures and values long-term financial security\nShe is at a point in her career where opportunities for\nadvancement might be time-sensitive. She enjoys movies that\nchallenge perspectives and spark discussions. She prioritizes well-\nbeing over being fully informed. She is interested in the van life\nmovement. She appreciates creativity and variety in meal options.\nShe enjoys engaging in creative and tactile activities and prefers\ntraditional learning methods. She values compassionate and\nresponsible choices. She enjoys exploring new and dynamic\nexperiences. She is concerned about fast food. She values mental\nwell-being and mindfulness. She enjoys attending events and\nconferences to meet new people. She appreciates practicality and\nhealth. She enjoys shows with dark humor and suspenseful\nstorytelling.\nCOT GENERATED PERSONA:\nBased on the examples provided, the user appears to value a more\nholistic and experiential approach to learning and personal growth.\nThey seem to prioritize human connection, nature, and personal\ndevelopment over technology and material wealth.\nThis user is likely someone who is interested in personal stories,\nemotions, and experiences, as evident from their appreciation for\nSylvia Plath's poetry and the suggestion to weave storytelling into\npublic speaking. They may also be someone who values authenticity,\nsimplicity, and community, as seen in their suggestion of house\nswapping and volunteering as alternative vacation options.\nIn terms of lifestyle, this user likely prefers a balanced and flexible\napproach to life, allowing for a mix of work, personal growth, and\nleisure activities. They may also be someone who values\nenvironmental sustainability and social responsibility, as hinted at"}]}