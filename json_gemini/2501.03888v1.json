{"title": "Neural DNF-MT: A Neuro-symbolic Approach for Learning Interpretable and Editable Policies", "authors": ["Kexin Gu Baugh", "Luke Dickens", "Alessandra Russo"], "abstract": "Although deep reinforcement learning has been shown to be effective, the model's black-box nature presents barriers to direct policy interpretation. To address this problem, we propose a neuro-symbolic approach called neural DNF-MT for end-to-end policy learning. The differentiable nature of the neural DNF-MT model enables the use of deep actor-critic algorithms for training. At the same time, its architecture is designed so that trained models can be directly translated into interpretable policies expressed as standard (bivalent or probabilistic) logic programs. Moreover, additional layers can be included to extract abstract features from complex observations, acting as a form of predicate invention. The logic representations are highly interpretable, and we show how the bivalent representations of deterministic policies can be edited and incorporated back into a neural model, facilitating manual intervention and adaptation of learned policies. We evaluate our approach on a range of tasks requiring learning deterministic or stochastic behaviours from various forms of observations. Our empirical results show that our neural DNF-MT model performs at the level of competing black-box methods whilst providing interpretable policies.", "sections": [{"title": "1 INTRODUCTION", "content": "Remarkable progress has been made in reinforcement learning (RL) with the advancement of deep neural networks. Since the demonstration of impressive performance in complex games like Go [32] and Dota 2 [4], significant effort has been made to utilise deep RL approaches for solving real-life problems, such as segmenting surgical gestures [15] and providing treatment decisions [39]. However, the need for model interpretability grows with safety and ethical considerations. In the EU's AI Act, systems used in areas such as healthcare fall into the high-risk category, requiring both a high level of accuracy and a method to explain and interpret their output[1]. Therefore, the 'black-box' nature of neural models becomes a concern when using them for such high-stakes decisions in healthcare [16]. While many approaches exist to explain black-box neural models with post-hoc methods, it is argued that using inherently interpretable models is safer [29].\nVarious neuro-symbolic approaches address the lack of interpretability in deep RL. We use the term 'symbolic' to refer to methods that offer logical rule representations, in contrast to program synthesis approaches [5, 35, 36] that offer programmatic representations with forms of logic. Some of these neuro-symbolic methods [10, 18] rely on manually engineered inductive bias to restrict the search space and thus limit the rules they can learn. Others [20, 41] without predefined inductive bias associate weights with predicates but require pre-trained components to parse observations to predicates [20] or a special critic for training [41].\nIn this paper, we propose a neuro-symbolic model, neural DNF-MT, for learning interpretable and editable policies.\u00b9 Our model is built upon the semi-symbolic layer and neural DNF model proposed in pix2rule [7] but with modifications that support probabilistic representation for policy learning. The model is completely differentiable and supports integration with deep actor-critic algorithms. It can also be used to distil policies from other neural models. From trained neural DNF-MT actors, we can extract bivalent logic programs for deterministic policies or probabilistic logic programs for stochastic policies. These interpretable logical representations are close approximations of the learned models. The neural-bivalent-logic translation is bidirectional, thus enabling manual policy intervention on the model. We can modify the bivalent logical program and port it back to the neural model, benefiting from the tensor operations and environment parallelism for fast inference. Compared to existing works, we do not rely on rule templates or mode declarations. Furthermore, our model is trained with a simple MLP critic and supports trainable preceding layers to generalise relevant facts from complex observations, such as multi-dimensional matrices.\nTo summarise, our main contributions are:\n(1) We propose neural DNF-MT, a neuro-symbolic model for end-to-end policy learning and distillation, without requiring manually engineered inductive bias. It can be trained with deep actor-critic algorithms and supports end-to-end predicate invention.\n(2) A trained neural DNF-MT actor's policy can be represented as a logic program (probabilistic for a stochastic policy and bivalent for a deterministic policy), thus providing interpretability.\n(3) The neural-to-bivalent-logic translation is bidirectional, and we can modify the logical program for policy intervention"}, {"title": "2 BACKGROUND", "content": "2.1 Reinforcement Learning\nRL tasks are commonly modelled as Markov Decision Processes (MDPs) [27] or sometimes Partially Observable Markov Decision Processes (POMDPs) [19, 42], depending on whether the observed states are fully Markovian. The objective of an RL agent is to learn a policy that maps states to action probabilities \\(\\pi(at|st)\\) to maximise the cumulative reward. Value-based methods such as Q-learning [37] and Deep Q-Networks (DQN) [23] approximate the action-value function Q(st, at), while policy-based methods such as RE-INFORCE [38] directly parameterise the policy \\(\\pi\\). Actor-critic algorithms such as Advantage Actor-Critic (A2C) [22] and Proximal Policy Optimisation (PPO) [30] combine both value-based and policy-based methods, where the actor learns the policy \\(\\pi(at|St)\\) and the critic learns the value function V (st). Specifically, PPO clips the policy update in a certain range to prevent problematic large policy changes, providing stability and better performance.\n2.2 Semi-symbolic Layer and Neural DNF Model\nA neural Disjunctive Normal Form model [7] is a fully differentiable neural architecture where each node can be set to behave like a semi-symbolic conjunction or disjunction of its inputs. For some trainable weights \\(w_i\\), \\(i = 1, . . ., I\\), and a parameter \\(\\delta\\), a node in the neural DNF model is given by:\n\\(\\hat{y} = \\tanh(\\sum_{i=1}^{I} w_i x_i + \\beta)\\) with \\(\\beta = \\delta (\\max(\\frac{w_i}{|w_i|}))\\)   (1)\nHere the I (semi-symbolic) inputs to the node are constrained such that \\(x_i \\in [-1,1]\\), where the extreme value 1 (-1) is interpreted as associated term i taking the logical value \u22a4 (\u22a5) with other values representing intermediate strengths of belief (a form of fuzzy logic or generalised belief). The node activation \\(\\hat{y} \\in (-1, 1)\\) is interpreted similarly but cannot take specific values 1 or -1. The node's characteristics are controlled by a hyperparameter \u03b4, which induces behaviour analogous to a logical conjunction (disjunction) when \u03b4 = 1 (=-1). The neural DNF model consists of a layer of conjunctive nodes followed by a layer of disjunctive nodes. During training, the absolute value of each \u03b4 in both layers is controlled by a scheduler that increases from 0.1 to 1, as the model may fail to learn any rules if the logical bias is at full strength at the beginning of training.\nPix2rule [7] proposes interpreting trained neural DNF models as logical rules with Answer Set Programming (ASP) [21] semantics by treating each node's output \\(\\hat{y} > 0\\) (\\(\\leq 0\\)) as logical \u22a4 (\u22a5) (akin to a maximum likelihood estimate of the associated fact). However, Baugh et al. [3] point out that the neural DNF models cannot be used to describe multi-class classification problems because the disjunctive layer fails to guarantee a logically mutually exclusive output, i.e. with exactly one node taking value \u22a4. Baugh et al. [3] instead propose an extended model called neural DNF-EO, which adds a non-trainable conjunctive semi-symbolic layer after the final layer of the base neural DNF to approximate the 'exactly-one' logical constraint 'classj \u2190 \u2227k,j\u2260k not classk', and again show how ASP rules can be extracted from trained models."}, {"title": "3 NEURAL DNF-MT MODEL", "content": "This section explains why existing neural DNF-based models from [7] and [3] are imperfectly suited to represent policies within a deep-RL agent, and presents a new model called neural DNF with mutex-tanh activation (neural DNF-MT) to address these limitations. It then shows how trained models can be variously interpreted as deterministic and stochastic policies for the associated domains.\n3.1 Issues of Existing Neural DNF-based Models\nUnlike multi-class classification, where each sample has a single deterministic class, an RL actor seeks to approximate the optimal policy with potentially arbitrary action probabilities [33]. It is possible for a domain to have an optimal deterministic policy and for the RL algorithm to approach it with an 'almost deterministic' policy, where for each state the optimal action's probability is significantly greater than the others (i.e. a single almost-1 value vs all the rest close to 0). In this case, the actor almost always chooses a single action, similar to a multi-class classification model predicting a single class. A trained neural DNF-based model representing such a policy should be interpreted as a bivalent logic program representing the nearest deterministic policy. When we wish to preserve the probabilities encoded within the trained neural DNF-based actor without approximating it with the nearest deterministic policy, its interpretation should be captured as a probabilistic logic program that expresses the action distributions. There is no way to achieve both of these objectives with the neural DNF and neural DNF-EO models, since their interpretation frameworks do not satisfy two forms of mutual exclusivity: (a) probabilistic mutual exclusivity when interpreted as a stochastic policy, and (b) logical mutual exclusivity when interpreted as a deterministic policy. We first formalise the logic system represented by neural DNF-based models (Definition 3.1) and then define the two mutual exclusivities possible in this logic system (Definition 3.2 and 3.3).\nDEFINITION 3.1 (GENERALISED BELIEF LOGIC). A neural DNF-based model that builds upon semi-symbolic layers represents a logic system. We refer to this logic system as Generalised Belief Logic (GBL). A semi-symbolic node's activation \\(y_i \\in (-1, 1)\\) represents its belief in a logical proposition. For each activation \\(y_i\\), we define a bivalent logic variable \\(b_i \\in \\{\u22a5, \u22a4\\}\\) as its bivalent logical interpretation:\n\\(b_i = \\begin{cases} \u22a4 & \\text{if } y_i > 0 \\\\ \u22a5 & \\text{otherwise} \\end{cases}\\)\nDEFINITION 3.2 (LOGICAL MUTUAL EXCLUSIVITY). Given the final activation of a neural DNF-based model for N classes \\(y \\in (-1, 1)^N\\) and its bivalent logic interpretation \\(b \\in \\{\u22a5, \u22a4\\}^N\\), the model satisfies logical mutual exclusivity if there is exactly one \\(b_i\\) that is \u22a4:\n\\(\\bigwedge_{i \\in \\{1..\u039d\\}} b_i \\rightarrow \\bigwedge_{i,j\\in\\{1..N\\},i<j} \\neg(b_i \\land b_j)\\)\nDEFINITION 3.3 (PROBABILISTIC MUTUAL EXCLUSIVITY). A probabilistic interpretation of GBL is a function \\(f_p : (-1, 1) \\rightarrow (0, 1)\\) that"}, {"title": "3.2 Mutex-tanh Activation", "content": "maps each belief \\(y_i\\) to probability \\(p_i\\) that \\(b_i\\) holds as true. Formally,\n\\(p_i = f_p (y_i) = Pr(b_i = \u22a4 |y_i)\\)\nA neural DNF-based model satisfies probabilistic mutual exclusivity if the interpreted probabilities associated with its activations \\(y \\in (0, 1)^N\\) under probabilistic interpretation \\(f_p\\) sum to 1. That is:\n\\(\\sum_{i=1}^N f_p (y_i) = 1\\)\nTo be used for interpretable policy learning, a neural DNF-based model must guarantee the following properties:\nP1: The model provides a probabilistic mutually exclusive interpretation (Definition 3.3) and can be interpreted as a probabilistic logic program (such as ProbLog [9]),\nP2: When the optimal policy is deterministic, the model can also be interpreted as a bivalent logic program (such as ASP [21]) that satisfies logical mutual exclusivity (Definition 3.2).\nA trained neural DNF model from [7] does not provide probabilistic interpretation or guarantee logical mutual exclusivity in its bivalent interpretation, and thus fails P1 and P2. A trained neural DNF-EO from [3] satisfies P2 via its constraint layer but fails to provide probabilistic interpretation for P1. To address these requirements, we propose a new model called neural DNF-MT and post-training processing steps that translate a trained neural DNF-MT model into a ProbLog program and, where applicable, into an ASP program. Our proposed model satisfies both properties above.\n3.2 Mutex-tanh Activation\nLet \\(d \\in R^N\\) be the output vector of a disjunctive semi-symbolic layer before any activation function and \\(d_k \\in R\\) be the output of the kth disjunctive node. Using the softmax function, we define the new activation function mutex-tanh as:\n\\(\\text{softmax}(d)_k = \\frac{e^{d_k}}{\\sum_{i=1}^N e^{d_i}}\\)\n\\(\\text{mutex-tanh}(d)_k = 2 \\text{softmax}(d)_k - 1\\)   (2)\nWith the mutex-tanh activation function, our neural DNF-MT model is constructed with a semi-symbolic conjunctive layer with a tanh activation function and a disjunctive semi-symbolic layer with the mutex-tanh activation function:\n\\(c = \\tanh(W_c x + \\beta_c)\\)Output of conj. layer\n\\(d = W_d c + \\beta_d\\)Raw output of disj. layer\n\\(y = \\text{mutex-tanh}(d)\\)mutex-tanh output of disj. layer\nwhere \\(W_c\\) and \\(W_d\\) are trainable weights, and \\(\\beta_c\\) and \\(\\beta_d\\) are the logical biases calculated as Eq (1). Note that \\(\u1ef9 \\in (-1, 1)\\) shares the same codomain as the disjunctive layer's tanh output y. The disjunctive layer's bivalent interpretation \\(b_i\\) still uses y, with \\(b_i = \u22a4\\) when \\(\u1ef9_i > 0\\) and \u22a5 otherwise.\nTo satisfy P1, we compute the probability p as:\n\\(p = (f_p (\u1ef9_1), ..., f_p (\u1ef9_N)) \\text{ where } f_p(y_i) = \\frac{y_i + 1}{2}\\)  (3)\nBy construction, \\(p \\in (0, 1)^N\\), and we have \\(\\sum_{i=1}^N p_i = 1\\) from Eq (2) to satisfy probabilistic mutual exclusivity."}, {"title": "3.3 Policy Learning with Neural DNF-MT", "content": "In the following, we show how the neural DNF-MT model can be trained in an end-to-end fashion to approximate a stochastic policy and how to extract the policy into interpretable logical form.\nTraining Neural DNF-MT as Actor with PPO. Using the PPO algorithm [30], we train a neural DNF-MT actor with an MLP critic. The input to the neural DNF-MT actor must be in [-1,1]\u00b9. Any discrete observation is converted into a bivalent vector representation, as shown in Figure 1. If the observation is complex, as shown in our experiment in Section 4.4, an encoder can be added before the neural DNF-MT actor to invent predicates in GBL form. The encoder output acts as input to the neural DNF-MT actor and the MLP critic, as shown in Figure 2.\nWe here present the overall training loss of the actor-critic PPO with a neural DNF-MT actor, which consists of multiple loss terms. The base training loss component matches that from PPO [30]:\n\\(L^{PPO} (\u03b8) = E_t [L^{CLIP} (\u03b8) + c_1 L^{value} (\u03b8) - c_2 S[\u03c0_\u03b8] (s_t)]\\)   (4)\nwhere \\(c_1, c_2 \\in R\\) are hyperparameters, \\(L^{CLIP} (\u03b8)\\) is the clipped surrogate objective, \\(S[\u03c0_\u03b8] (s_t)\\) is the entropy of the actor in training, and \\(L^{value} (\u03b8)\\) is the value loss. The detailed explanations for each term are in Appendix B.1. The action probability output of the neural DNF-MT actor defined in Eq (3) is used to calculate the probability ratio in \\(L^{CLIP}\\) and the entropy term \\(S[\u03c0_\u03b8](s_t)\\).\nWe add the following auxiliary losses to facilitate the interpretation of the neural DNF-MT model into rules:\n\\(L^{(1)} (\u03b8) = \\frac{1}{N_F} \\sum_{i=1}^{N_F} |1 - |f_i||\\)  (5)\n\\(L^{(2)} (\u03b8) = \\frac{1}{N_C} \\sum_{j=1}^{N_C} |\u03b8_{disj, j} (6 - \u03b8_{disj, j})|\\)  (6)\n\\(L^{(3)} (\u03b8) = \\frac{1}{N_F} \\sum |1 - |c_i||\\)  (7)\n\\(L^{(4)} (\u03b8) = - \\sum_{i=1}^N p_i log (\\frac{\\hat{y}_i + 1}{2}) + (1-p_i) log (1 - (\\frac{\\hat{y}_i + 1}{2}))\\)  (8)\nwhere \\(f_i\\) is the invented predicate, \\(N_F\\) is the number of output of an encoder, and \\(N_C\\) is the number of conjunctive nodes. Eq (5) is used when there is an encoder before the neural DNF-MT actor for predicate invention. It enforces the predicates' activations to be close to \u00b11 so that they are stronger beliefs of true/false. Eq (6) is a weight regulariser to encourage the disjunctive weights to be close to \u00b16 (the choice of \u00b16 is to saturate tanh, as tanh(\u00b16) \u2248 \u00b11). Eq (7) encourages the tanh output of the conjunctive layer to be close to \u00b11. Eq (8) is the key term to satisfy P2, pushing for bivalent logical interpretations for deterministic policies. This term mimics a cross-entropy loss between each mutex-tanh output and corresponding individual tanh outputs of the disjunctive layer, pushing the probability interpretations of the tanh outputs (i.e. (\\(\\hat{y}_i + 1\\)/2) towards their action probability \\(p_i\\) counterparts. If the optimal policy is deterministic, all \\(p_i\\) will be approximately 0 except for one, which is close to 1. Each \\(\\hat{y}_i\\) is pushed towards \u00b11, and only one will be close to 1, thus having exactly one bivalent interpretation \\(b_i = \u22a4\\) and satisfying logical mutual exclusivity."}, {"title": "5 DISCUSSIONS", "content": "Finally, the overall training loss is defined as:\n\\(L(\u03b8) = L^{PPO} (\u03b8) + \\sum_{i \\in \\{1,2,3,4\\}} \\lambda_i L^{(i)} (\u03b8)\\)  (9)\nwhere \\(\\lambda_i \\in R\\), \\(i \\in \\{1, 2, 3, 4\\}\\) are hyperparameters.\nPost-training Processing. This extracts either a ProbLog program for a stochastic policy or an ASP program for a close-to-deterministic policy from a trained neural DNF-MT actor, where the logic program is a close approximation of the model. It consists of multiple stages, as shown in Figure 3, described as follows.\n(1) Pruning: This step repeatedly passes over each edge that connects an input to a conjunction or a conjunction to a disjunction, and removes any edge that can be removed (i) without changing the learned trajectory (for deterministic domains) or (ii) without shifting any action probability for any state more than some threshold \\(T_{prune}\\) from the original learned policy (for stochastic domains). Any unconnected nodes are also removed. The process terminates when a pass fails to remove any edges or nodes.\n(2) Thresholding: This process converts a semi-symbolic layer's weights from R to values in \\(\\{\u22126, 0, 6\\}\\). Given some threshold \\(\u03c4 \\in R_{\u22650}\\), a new weight is computed as \\(w'_{kij} = 6 \u22c5 1_{|w_{kij} |\u2265t} (w_{kij}) \\cdot \\text{sign}(w_{kij})\\), \\(k \u2208 \\{c, d\\}\\). This weight update enables the neural to bivalent logic translation described later. The selection of \\(t\\) should maintain the model's trajectory/action probability, subject to the same checks used in pruning. For a thresholded node with at least one non-zero weight, we replace its tanh activation with step function h(x) = 2\u22c51x>0(x) -1, changing its output's range to {-1,1}. The thresholding process is applied differently to the disjunctive layer depending on the nature of the policy desired."}, {"title": "4 EXPERIMENTS", "content": "3.a) For stochastic policies: Only the conjunctive layer is thresholded, i.e. choosing a value of t, updating only its weights and changing the activation function. The disjunctive layer still outputs action probabilities.\n(2.b) For deterministic policies: Thresholding is applied to both the conjunctive and disjunctive layers: a single value t is chosen and applied in both layers' weight update, and both layers have their tanh activation replaced with the step function. This process is only possible if the model satisfies P2.\n(3) Re-pruning: The pruning process from Step 1 is repeated.\n(4) Logical rules extraction: All nodes (conjunctive and disjunctive) are converted into some form of logical rules. The thresholding process guarantees that all conjunctive nodes can be translated into bivalent logic representations. For a conjunctive node cj, we consider the set Xj = {i \u2208 {1..I}|wij \u2260 0}, and |Xj| \u2260 0. We partition Xj into subsets \\(X^+_j = \\{i \u2208 X_j|w_{cij} = 6\\}\\) and \\(X^\u2212_j = \\{i \u2208 X_j|w_{cij} = \u22126\\}\\), and translate cj to an ASP rule of the form conjj \u2190 \\(\\bigwedge_{i\u2208X^+_j} atom_i\\), \\(\\bigwedge_{i\u2208X^\u2212_j} (not atom_i)\\), where atomi is an atom for input xi. The disjunctive nodes are interpreted differently depending on the desired policy type.\n4.a) Stochastic policy - ProbLog rules: We use ProbLog's annotated disjunctions to represent mutually exclusive action probabilities. Each unique achievable activation of the conjunctive layer \\(c^{(m)} \\in \\{\u22121,1\\}^{C'}\\) with \\(1 \u2264 m \u2264 2^{C'}\\) forms the body of a unique annotated disjunction of the form \\(p_1 :: action_1;...; p_N :: action_N \u2190 \\bigwedge_{i\u2208C^{(m)}+} conj_i, \\bigwedge_{i\u2208C^{(m)}\u2212} (\\neg conj_i)\\), where \\(C^{(m)}+ = \\{i|c^{(m)}_i = 1\\}\\), \\(C^{(m)}\u2212 = \\{i|c^{(m)}_i = \u22121\\}\\), and \\(p_j\\)"}, {"title": "5 DISCUSSIONS", "content": "We evaluate the RL performance (measured in episodic return) of our neural DNF-MT actors and their interpreted logical policies in four sets of environments with various forms of observations. Some tasks require stochastic behaviours, while others can be solved with deterministic policies. We compare our method with two baselines: Q-tables trained with Q-learning where applicable and MLP actors trained with actor-critic PPO. Our neural DNF-MT actors are trained with MLP critics using the PPO algorithm in the Switcheroo Corridor set, Blackjack and Door Corridor environments. In the Taxi environment, we distil a neural DNF-MT actor from a trained MLP actor. We do not directly evaluate the extracted ProbLog policies because of the long ProbLog query time. Instead, we evaluate their final neural DNF-MT actors before logical rule extraction (i.e. after step 3, re-pruning) as an approximation. The approximation is acceptable because a ProbLog policy's action distribution is the same as its corresponding neural DNF-MT's action distribution to 3 decimal places. A performance evaluation summary is shown in Figure 4, and a detailed version is presented in Table 5 in Appendix.\n4.1 Switcheroo Corridor\nWe adopt an example environment from [33] and create a set of Switcheroo Corridor environments that support MDP tasks with deterministic policies and POMDP tasks with stochastic policies."}, {"title": "4.2 Blackjack", "content": "We first analyse the persistent performance loss issue in Blackjack, Taxi, and Door Corridor environments.\nPerformance loss due to thresholding. The thresholding step converts the target layer(s) from a weighted continuous space to a discrete space with only 0 and \u00b16 values, saturating the tanh activation at \u00b11 and enabling the translation to bivalent logic. However, the thresholding step may not maintain the same logical interpretation of the layer output, as shown in Listing 6, where a thresholded neural DNF-MT actor fails to maintain logical mutual exclusivity in the Door Corridor environment. Note that we apply thresholding on both the conjunctive and disjunctive layers since we desire a deterministic policy from the model. The 1st row of values in Table 2 are the pre-thresholding tanh output when \\(x_2 = -1\\), \\(x_7 = 1\\), \\(x_ = 1\\), \\(x_{13} = -1\\), with y3 interpreted as \u22a4 and chosen as action. The thresholded conjunctive nodes in the 2nd row share the same sign as row 1, but y3 becomes positive after thresholding, resulting in two actions being \u22a4 and thus violating logical mutual exclusivity. The original weights of the disjunctive nodes achieve the balance of importance between c7, c9 and c11 to make y3 negative. However, the thresholding process ignores the weights and makes them equally important, leading to a different output and truth value. It shows that the thresholding stage cannot handle volatile and interdependent weights and maintain the underlying truth table represented by the model. We leave it as a future work to improve/replace the thresholding stage with a more robust method."}, {"title": "5 DISCUSSIONS", "content": "The Blackjack environment from [33] is a simplified version of the card game Blackjack, where the goal is to beat the dealer by having a hand closer to 21 without going over. The agent sees the sum of its hand, the dealer's face-up card, and whether it has a usable ace. It can choose to hit or stick. The performance across the models is shown in the 7th group in Figure 4 and Table 1. The baseline Q-table from [33] only shows a single action, so we only evaluate it with argmax action selection. We evaluate the MLP and neural DNF-MT actors with both argmax action selection and actor's distribution sampling. MLP actors with argmax action selection perform better than their distribution sampling counterparts, with a higher episodic return and win rate. The same is observed for neural DNF-MT actors. The extracted ProbLog rules perform worse than their original neural DNF-MT actors (no post-training processing). We observe a policy change (shown in Figures 10 and 11 in Appendix) that results in a higher policy divergence from the Q-table from [33]. This issue of performance loss occurs at the thresholding stage during the post-training processing and persists in later environments; we will discuss it further in Section 5.\n4.3 Taxi\nIn the Taxi environment (Figure 9 in Appendix) from [11], the agent controls a taxi to pick up a passenger first and drop them off at the destination hotel. A state number is used as the observation, and it encodes the taxi, passenger and hotel locations using the formula ((taxi_row * 5 + taxi_col) * 5 + passenger_loc) * 4 + destination. Apart from moving in four directions, the agent can pick up/drop off the passenger, but illegally picking up/dropping off will be penalised. The environment is designed for hierarchical reinforcement learning but is solvable with PPO and without task decomposition. However, we find that model performance is more sensitive to PPO's hyperparameters and fine-tuning the hyperparameters is more difficult than in other environments. With the wrong set of hyperparameters, the actor settles at a local optimal with a reward of -200: never perform 'pickup'/'drop-off' and move until the step limit (200 steps). The environment is complex due to its hierarchical nature, and learning the task dependencies based on purely state numbers proves to be difficult, as a 1-value change in the x/y coordinate of the taxi results is a change of state number in 100s/10s. We successfully trained MLP actors with actor-critic PPO but failed to find a working set of hyperparameters to train neural DNF-MT actors. Instead, we distil a neural DNF-MT actor from a trained MLP actor, taking the same observation as input and aiming to output the exact action probabilities as the MLP oracle.\nThe performance is shown in the 8th group in Figure 4. Actors using argmax action selection perform better than their distribution sampling counterparts. Again, we observe a performance drop in extracted ProbLog rules. With 300 unique possible starting states, the extracted ProbLog rules are not guaranteed to finish in 200 steps: 2 out of the 10 ProbLog evaluations with action probabilities sampling have truncated episodes. Across ten post-training-processed neural DNF-MT actors with argmax action selection, there are an average of 3.3 unique starting states where the models cannot finish the environment within 200 steps. De-coupling the observation seems complicated and makes it hard to learn concise conjunctions, thus increasing the error rate in the post-training processing."}, {"title": "4.4 Door Corridor", "content": "We first analyse the persistent performance loss issue in Blackjack, Taxi, and Door Corridor environments.\nPerformance loss due to thresholding. The thresholding step converts the target layer(s) from a weighted continuous space to a discrete space with only 0 and \u00b16 values, saturating the tanh activation at \u00b11 and enabling the translation to bivalent logic. However, the thresholding step may not maintain the same logical interpretation of the layer output, as shown in Listing 6, where a thresholded neural DNF-MT actor fails to maintain logical mutual exclusivity in the Door Corridor environment. Note that we apply thresholding on both the conjunctive and disjunctive layers since we desire a deterministic policy from the model. The 1st row of values in Table 2 are the pre-thresholding tanh output when x2 = -1, x7 = 1, x = 1, x13 = -1, with y3 interpreted as \u22a4 and chosen as action. The thresholded conjunctive nodes in the 2nd row share the same sign as row 1, but y3 becomes positive after thresholding, resulting in two actions being \u22a4 and thus violating logical mutual exclusivity. The original weights of the disjunctive nodes achieve the balance of importance between c7, c9 and c11 to make y3 negative. However, the thresholding process ignores the weights and makes them equally important, leading to a different output and truth value. It shows that the thresholding stage cannot handle volatile and interdependent weights and maintain the underlying truth table represented by the model. We leave it as a future work to improve/replace the thresholding stage with a more robust method."}, {"title": "5 DISCUSSIONS", "content": "Inspired by Minigrid [6], we design a corridor grid with a fixed configuration called Door Corridor, as shown in Figure 6. The agent observes a 3 \u00d7 3 grid in front of it (as shown as the input in Figure 2) and has a choice of four actions: turn left, turn right, move forward, and toggle. The toggle action only changes the status of a door right in front of the agent.\nFor this environment, we use the architecture shown in Figure 2, where an encoder is shared between the actor and the critic. The performance of MLP actors, neural DNF-MT actors and their"}, {"title": "6 RELATED WORK", "content": "extracted ASP programs is shown in the last group in Figure 4. Both of the neural actors learn the optimal deterministic policy.\nPolicy Intervention. We create two variations of the base Door Corridor environment with different termination conditions: Door Corridor-T (DC-T), where the agent must be in front of the goal and toggle it instead of moving into it, and Door Corridor-OT (DC-OT), where the agent must stand on the goal and take the action 'toggle'. The input observation remains unchanged since only the goal cell's mechanism changes. The encoder can be reused immediately, but the actor and critic cannot. An MLP actor trained on DC fails to finish within step limits in DC-T and DC-OT environments without re-training. However, we can modify the ASP policy to achieve the optimal reward in both DC-T and DC-OT environments. Listings 4 and 5 show the modified ASP programs for DC-T and DC-OT environments, respectively. The modified ASP programs can be ported back to neural DNF-MT actors by virtue of the bidirectional neural-bivalent-logic translation. The modified neural DNF-MT actors also finish DC-T and DC-OT environments with minimal steps without any re-training.\nMany neuro-symbolic approaches perform the task of inductive logic programming (ILP) [8, 25] in differentiable models, and policies are learned and represented as logical rules. They are commonly applied in Relational RL [13, 40] domains that utilise symbolic representations for states, actions, and policies. NLRL [18] and NUDGE"}, {"title": "7 CONCLUSION", "content": "[10", "14": "and its extension from [31", "24": "and then training rule-associated weights. NeSyRL [20", "41": "do not associate weights with rules but predicates; thus, they are not reliant on rule templates or mode declarations. NeSyRL uses a disjunctive normal form Logical Neural Network (LNN) [28"}]}