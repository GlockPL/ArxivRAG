{"title": "A Survey of Personalized Large Language Models: Progress and Future Directions", "authors": ["Jiahong Liu", "Zexuan Qiu", "Zhongyang Li", "Quanyu Dai", "Jieming Zhu", "Minda Hu", "Menglin Yang", "Irwin King"], "abstract": "Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research. Updated information about this survey can be found at the Github Repo.", "sections": [{"title": "1 Introduction", "content": "In recent years, substantial progress has been made in Large Language Models (LLMs) such as GPT, PaLM, LLaMA, DeepSeek, and their variants. These models have demonstrated remarkable versatility, achieving state-of-the-art performance across various natural language processing tasks, including question answering, reasoning, and machine translation [Zhao et al., 2023], with minimal task-specific adaptation.\nThe Necessity of Personalized LLMs (PLLMs) While LLMs excel in general knowledge and multi-domain reasoning, their lack of personalization creates challenges in situations where user-specific understanding is crucial. For instance, conversational agents need to adapt to a user's preferred tone and incorporate past interactions to deliver relevant, personalized responses. 8As LLMs evolve, integrating personalization capabilities has become a promising direction for advancing human-AI interaction across diverse domains such as education, healthcare, and finance [Hu et al., 2024; Zhang et al., 2024e,d; Zhu et al., 2024; Wang et al., 2023a, 2024a].\nTechnical Challenges Despite its promise, personalizing LLMs presents several challenges. These include efficiently representing and integrating diverse user data, addressing privacy concerns, managing long-term user memories, adaptivity accommodating users' diverse needs and evolving behaviors [Salemi et al., 2023]. Moreover, achieving personalization often requires balancing accuracy and efficiency while addressing biases and maintaining fairness in the generated outputs.\nContributions Despite growing interest, the field of PLLMs lacks a systematic review that consolidates recent advancements. This survey aims to bridge the gap by systematically organizing existing research on PLLMs and offering insights into their methodologies and future directions. The contributions of this survey can be summarized as follows: (1) A structured taxonomy: We propose a comprehensive taxonomy, providing a technical perspective on the existing approaches to building PLLMs. (2) A comprehensive review: We systematically review state-of-the-art methods for PLLMs, analyzing"}, {"title": "2 Preliminary", "content": "Large Language Models (LLMs) generally refer to models that utilize the Transformer architecture and are equipped with billions of parameters trained on trillions of text tokens. These models have demonstrated substantial improvements in a myriad of tasks related to natural language understanding and generation, increasingly proving beneficial in assisting human activities. In this work, we mainly focus on autoregressive LLMs, which are based on two main architectures: decoder-only models and encoder-decoder models. Encoder-decoder models such as Flan-T5 [Chung et al., 2022] and ChatGLM [Zeng et al., 2022] analyze input through the encoder for semantic representations, making them effective in language understanding in addition to generation. Decoder-only LLMs focus on left-to-right generation by predicting the next token in a sequence, with numerous instances [Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023; Guo et al., 2025] under this paradigm achieving breakthroughs in advanced capabilities like instruction following and reasoning. However, these models are typically pre-trained on general-purpose data and lack an understanding of specific user information. As a result, they are unable to generate responses tailored to a user's unique tastes, preferences, and expectations, limiting their effectiveness in personalized applications where user-specific adaptation is critical.\n2.2 Problem Statement\nPersonalized Large Language Models (PLLMs) generate responses that align with the user's style and expectations, offering diverse answers to the same query for different users [Clarke et al., 2024]. A PLLM is defined as an LLM that generates responses conditioned not only on an input query q, but also on a user u's personalized data $C_u$. It aims to predict the most probable response sequence y given a query q and the personalized context $C_u$, such that: $y = \\text{argmax}_{y}P(y | q,C_u)$. The personalized data $C_u$ may encapsulate information about the user's preferences, history, context, and other user-specific attributes. These can include (Figure 1):\nProfile/Relationship: User profile, including attributes (e.g., name, gender, occupation), and relationships (e.g., friends, family members), such as $C_u = \\{A, 18, \\text{student}, \\text{friends}: \\{B, C, D\\} ...\\}$.\nHistorical Dialogues: Historical dialogues, such as question-answer pairs that user u interacts with the LLM (e.g., $C_u = \\{(q_0, a_0), (q_1, a_1),..., (q_i, a_i)\\})$, where each $q_i$ is a query and $a_i$ is the corresponding answer.\nHistorical Content: Includes documents, previous reviews, comments or feedback from user u. For example, $C_u = \\{\\text{I like Avtar because . . . . . . . }\\}$.\nHistorical Interactions: Includes historical interactions,"}, {"title": "2.1 Large Language Models", "content": null}, {"title": "2.2 Problem Statement", "content": null}, {"title": "2.3 Proposed Taxonomy", "content": "We propose a taxonomy (as illustrated in Figure 1 and Figure 2) from technical perspectives, categorizing the methods for Personalized Large Language Models (PLLMs) into three major levels: (1) Input level: Personalized Prompting focuses on handling user-specific data outside the LLM and injecting it into the model. (2) Model level: Personalized Adaptation emphasizes designing a framework to efficiently fine-tune or adapt model parameters for personalization. (3) Objective Level: Personalized Alignment aims to refine model behavior to align with user preferences effectively. Due to space limitations, analysis papers, datasets, and benchmarks are summarized in the Github Repo."}, {"title": "3 Personalized Prompting", "content": "Prompt engineering acts as a bridge for interaction between users and LLMs. In this survey, prompting involves guiding an LLM to generate desired outputs using various techniques, from traditional text prompts to advanced methods like soft embedding. Soft embedding can be extended not only through input but also via cross-attention or by adjusting output logits, enabling more flexible and context-sensitive responses.\nThe framework can be expressed as, for each user u:\n$y = f_{LLM} (q\\oplus \\phi (C_u)),$ (1)\nwhere, $f_{LLM}$ is the LLM model that generates the response; $\\phi$ is a function that extracts relevant context from the user's personal context $C_u$; $\\oplus$ represents the combination operator that fuses the query q and the relevant personalized context $\\phi(C_u)$, producing enriched information for the LLM."}, {"title": "3.1 Profile-Augmented Prompting", "content": "Profile-augmented prompting methods explicitly utilize summarized user preferences and profiles in natural language to augment LLMs' input at the token level ($\\phi$ is the summarizer model). Figure 3(a) shows the illustration.\nNon-tuned Summarizer A frozen LLM can be directly used as the summarizer to summarize user profiles due to its strong language understanding capabilities, i.e., $\\phi (C_u) = f_{LLM} (C_u)$. For instance, Cue-CoT [Wang et al., 2023b] employs chain-of-thought prompting for personalized profile augmentation,"}, {"title": "3.2 Retrieval-Augmented Prompting", "content": "Retrieval-augmented prompting [Gao et al., 2023; Fan et al., 2024; Qiu et al., 2024] excels at extracting the most relevant records from user data to enhance PLLMs (See Figure 3(b)). Due to the complexity and volume of user data, many methods use an additional memory for more effective retrieval. Common retrievers including sparse (e.g., BM25 [Robertson et al., 1995]), and dense retrievers (e.g., Faiss [Johnson et al., 2019], Contriever [Izacard et al., 2021]). These methods effectively manage the increasing volume of user data within the LLM's context limit, improving relevance and personalization by integrating key evidence from the user's personalized data."}, {"title": "3.2.1 Personalized Memory Construction", "content": "This part designs mechanisms for retaining and updating memory to enable efficient retrieval of relevant information.\nNon-Parametric Memory This category maintains a token-based database, storing and retrieving information in its original tokenized form without using parameterized vector representations. For example, MemPrompt [Madaan et al., 2022] and TeachMe [Dalvi et al., 2022] maintain a dictionary-based feedback memory (key-value pairs of mistakes and user feedback). MemPrompt focuses on prompt-based improvements, whereas TeachMe emphasizes continual learning via dynamic memory that adapts over time. MaLP [Zhang et al., 2024a] further integrates multiple memory types, leveraging working memory for immediate processing, short-term memory (STM) for quick access, and long-term memory (LTM) for storing key knowledge.\nParametric Memory Recent studies parameterize and project personalized user data into a learnable space, with parametric memory filtering out redundant context to reduce noise. For instance, LD-Agent [Li et al., 2024c] maintains memory with separate short-term and long-term banks, encoding long-term events as parametric vector representations"}, {"title": "3.2.2 Personalized Memory Retrieval", "content": "The key challenge in the personalized retriever design lies in selecting not only relevant but also representative personalized data for downstream tasks. LaMP [Salemi et al., 2023] investigates how retrieved personalized information affects the responses of large language models (LLMs) through two mechanisms: in-prompt augmentation (IPA) and fusion-in-decoder (FiD). PEARL [Mysore et al., 2023] and ROPG [Salemi et al., 2024] similarly aim to enhance the retriever using personalized generation-calibrated metrics, improving both personalization and text quality of retrieved documents. Meanwhile, HYDRA [Zhuang et al., 2024] trains a reranker to prioritize the most relevant information additionally from top-retrieved historical records for enhanced personalization."}, {"title": "3.3 Soft-Fused Prompting", "content": "Soft prompting differs from profile-augmented prompting by compressing personalized data into soft embeddings, rather than summarizing it into discrete tokens. These embeddings are generated by a user feature encoder $\\phi$.\nIn this survey, we generalize the concept of soft prompting, showing that soft embeddings can be integrated (combination operator $\\oplus$) not only through the input but also via cross-attention or by adjusting output logits, allowing for more flexible and context-sensitive responses (See Figure 3(c)).\nInput Prefix Soft prompting, used as an input prefix, focuses on the embedding level by concatenating the query embedding with the soft embedding, and is commonly applied in recommendation tasks. UEM [Doddapaneni et al., 2024] is a user embedding module (transformer network) that generates a soft prompt conditioned on the user's personalized data. PERSOMA [Hebert et al., 2024] enhances UEM by employing resampling, selectively choosing a subset of user interactions based on relevance and importance. REGEN [Sayana et al., 2024] combines item embeddings from user-item interactions via collaborative filtering and item descriptions using a soft prompt adapter to generate contextually personalized responses. PeaPOD [Ramos et al., 2024] personalizes soft"}, {"title": "3.4 Discussions", "content": "The three prompting methods have distinct pros and cons: 1) Profile-augmented prompting improves efficiency by compressing historical data but risks information loss and reduced personalization. 2) Retrieval-augmented prompting offers rich, context-aware inputs and scales well for long-term memory but can suffer from computational limits and irrelevant data retrieval. 3) Soft prompting efficiently embeds user-specific info, capturing semantic nuances without redundancy, but is limited to black-box models and lacks explicit user preference analysis. Overall, prompting-based methods are efficient and adaptable, and enable dynamic personalization with minimal computational overhead. However, they lack deeper personalization analysis as they rely on predefined prompt structures to inject user-specific information and are limited in accessing global knowledge due to the narrow scope of prompts."}, {"title": "4 Personalized Adaptation", "content": "PLLMs require balancing fine-tuning's deep adaptability with the efficiency of prompting. Therefore, specialized methods"}, {"title": "4.1 One PEFT All Users", "content": "This method trains on all users' data using a shared PEFT module, eliminating the need for separate modules per user. The shared module's architecture can be further categorized.\nSingle PEFT PLORA [Zhang et al., 2024c] and LM-P [Wo\u017aniak et al., 2024] utilize LoRA for PEFT of LLM, injecting personalized information via user embeddings and user IDs, respectively. PLORA is further extended and supports online training and prediction for cold-start scenarios. UserIdentifier [Mireshghallah et al., 2021] uses a static, non-trainable user identifier to condition the model on user-specific information, avoiding the need for trainable user-specific parameters and reducing training costs. Review-LLM [Peng et al., 2024b] aggregates users' historical behaviors and ratings into prompts to guide sentiment and leverages LoRA for efficient fine-tuning. However, these methods rely on a single architecture with fixed configurations (e.g., hidden size, insertion layers), making them unable to store and activate diverse information for personalization [Zhou et al., 2024]. To solve this problem, MiLP [Zhang et al., 2024b] utilizes a Bayesian optimization strategy to automatically identify the optimal configuration for applying multiple LoRA modules, enabling efficient and flexible personalization.\nMixture of Experts (MoE) Several methods use the LORA module, but with a static configuration for all users. This lack of parameter personalization limits adaptability to user dynamics and preference shifts, potentially resulting in suboptimal performance [Cai et al., 2024]. RecLoRA [Zhu et al., 2024] addresses this limitation by maintaining a set of parallel, independent LoRA weights and employing a soft routing method to aggregate meta-LoRA weights, enabling more personalized and adaptive results. Similarly, iLoRA [Kong et al., 2024] creates a diverse set of experts (LoRA) to capture specific aspects of user preferences and generates dynamic expert participation weights to adapt to user-specific behaviors.\nShared PEFT methods rely on a centralized approach, where user-specific data is encoded into a shared adapter by centralized LLMs. This limits the model's ability to provide deeply"}, {"title": "4.2 One PEFT Per User", "content": "Equipping a user-specific PEFT module makes LLM deployment more personalized while preserving data privacy. However, the challenge lies in ensuring efficient operation in resource-limited environments, as users may lack sufficient local resources to perform fine tuning.\nNo Collaboration There is no collaboration or coordination between adapters or during the learning process for each use in this category. UserAdapter [Zhong et al., 2021] personalizes models through prefix-tuning, fine-tuning a unique prefix vector for each user while keeping the underlying transformer model shared and frozen. PocketLLM [Peng et al., 2024a] utilizes a derivative-free optimization approach, based on MeZo [Malladi et al., 2023], to fine-tune LLMs on memory-constrained mobile devices. OPPU [Tan et al., 2024b] equips each user with a LORA module.\nCollaborative Efforts The \u201cone-PEFT-per-user\u201d paradigm without collaboration is computationally and storage-intensive, particularly for large user bases. Additionally, individually owned PEFTs hinder community value, as personal models cannot easily share knowledge or benefit from collaborative improvements. PER-PCS [Tan et al., 2024a] enables efficient and collaborative PLLMs by sharing a small fraction of PEFT parameters across users. It first divides PEFT parameters into reusable pieces with routing gates and stores them in a shared pool. For each target user, pieces are autoregressively selected from other users, ensuring scalability, efficiency, and personalized adaptation without additional training.\nAnother efficient collaborative strategy is based on the federated learning (FL) framework. For example, Wagner et al. [2024] introduces a FL framework for on-device LLM fine-tuning, using strategies to aggregate LoRA model parameters and handle data heterogeneity efficiently, outperforming purely local fine-tuning. FDLoRA [Qi et al., 2024] introduces a personalized FL framework using dual LoRA modules to capture personalized and global knowledge. It shares only global LoRA parameters with a central server and combines"}, {"title": "4.3 Discussions", "content": "Fine-tuning methods enable deep personalization by modifying a large set of model parameters, and parameter-efficient fine-tuning methods (e.g., prefix vectors or adapters) reduce computational cost and memory requirements while maintaining high personalization levels. These methods improve task adaptation by tailoring models to specific user needs, enhancing performance in tasks like sentiment analysis and recommendations. They also offer flexibility, allowing user-specific adjustments while leveraging pre-trained knowledge. However, they still face the risk of overfitting, particularly with limited or noisy user data, which can impact generalization and performance for new or diverse users."}, {"title": "5 Personalized Alignment", "content": "Alignment techniques [Bai et al., 2022; Rafailov et al., 2024] typically optimize LLMs to match the generic preferences of humans. However, in reality, individuals may exhibit significant variations in their preferences for LLM responses across different dimensions like language style, knowledge depth, and values. Personalized alignment seeks to further align with individual users' unique preferences beyond generic preferences. A significant challenge in personalized alignment is creating high-quality user-specific preference datasets, which are more complex than general alignment datasets due to data sparsity. The second challenge arises from the need to refine the canonical RLHF framework [Ouyang et al., 2022] to handle the diversification of user preferences, which is essential for integrating personalized preferences without compromising efficiency and performance."}, {"title": "5.1 Personalized Alignment Data Construction", "content": "High-quality data construction is critical for learning PLLMs, primarily involving self-generated data through interactions with the LLM. Wu et al. [2024c] constructs a dataset for aligning LLMs with individual preferences by initially creating a diverse pool of 3,310 user personas, which are expanded through iterative self-generation and filtering. This method is similar to PLUM [Magister et al., 2024] that both simulate dynamic interactions through multi-turn conversation trees,"}, {"title": "5.2 Personalized Alignment Optimization", "content": "Personalized preference alignment is usually modeled as a multi-objective reinforcement learning (MORL) problem, where personalized preference is determined as the user-specific combination of multi-preference dimensions. Based on this, a typical alignment paradigm involves using a personalized reward derived from multiple reward models to guide during the training phase of policy LLMs, aiming for personalization. MORLHF [Wu et al., 2023] separately trains reward models for each dimension and retrains the policy language models using proximal policy optimization, guided by a linear combination of these multiple reward models. This approach allows for the reuse of the standard RLHF pipeline.\nMODPO [Zhou et al., 2023] introduces a novel RL-free algorithm extending Direct Preference Optimization (DPO) for managing multiple alignment objectives. It integrates linear scalarization directly into the reward modeling process, allowing it to train language models via a simple margin-based cross-entropy loss as implicit collective rewards functions.\nAnother strategy for MORL is to consider ad-hoc combinations of multiple trained policy LLMs during the decoding phase to achieve personalization. Personalized Soups [Jang"}, {"title": "5.3 Discussions", "content": "Current mainstream personalized alignment technologies mainly model personalization as multi-objective reinforcement learning problems, where personalized user preferences are taken into account during the training phase of policy LLMs via canonical RLHF, or the decoding phase of policy LLM via parameter merging or model ensembling. Typically, these methods are limited to a small number (e.g., three) of predefined preference dimensions, represented through textual user preference prompts. However, in real-world scenarios, there could be a large number of personalized users, and their preference vectors may not be known, with only their interaction history accessed. Consequently, developing more realistic alignment benchmarks to effectively assess these techniques is a critical area for future research."}, {"title": "6 Future Directions", "content": "Despite recent advances in PLLMs, challenges and opportunities remain. This section discusses key limitations and promising future research directions.\nComplex User Data While current approaches effectively handle basic user preferences, processing complex, multi-source user data remains a significant challenge. For example, methods that use user relationships in graph-like structures are still limited to retrieval augmentation [Du et al., 2024]. How to effectively leverage this complex user information to fine-tune LLM parameters remains a significant challenge. Most methods focus on text data, while personalized foundation models for multimodal data (e.g., images, videos, audio) remain underexplored, despite their significance for real-world deployment and applications [Wu et al., 2024b; Pi et al., 2024].\nEdge Computing A key challenge in edge computing is efficiently updating models on resource-constrained devices (e.g., phones), where storage and computational resources are limited. For example, fine-tuning offers deeper personalization but is resource-intensive and hard to scale, especially in real-time applications. Balancing resources with personalization needs is important. A potential solution is to build personalized small models [Lu et al., 2024] for edge devices, using techniques like quantization and distillation.\nEdge-Cloud Collaboration The deployment of PLLMs in real-world scenarios encounters significant challenges in edge-cloud computing environments. Current approaches utilizing collaborative efforts often lack efficient synchronization mechanisms between cloud and edge devices. This highlights the need to explore the balance between local computation and cloud processing for PLLMs [Tian et al., 2024].\nEfficient Adaptation to Model Updates When the base LLM parameters are updated, such as with a new version, efficiently adapting the fine-tuned PEFT parameters for each user becomes a challenge. Given the large volume of user data and limited resources, the cost of retraining can be prohibitive. Future research should focus on efficient strategies for updating user-specific parameters without requiring complete retraining, such as leveraging incremental learning, transfer learning, or more resource-efficient fine-tuning techniques.\nLifelong Updating Given the large variety of user behaviors, a key challenge is preventing catastrophic forgetting while ensuring the efficient update of long-term and short-term of the memory. Future research could explore continual learning [Wu et al., 2024d] and knowledge editing [Wang et al., 2024b] to facilitate dynamic updates of user-specific information.\nTrustworthy Ensuring user privacy is crucial, especially when summarized or retrieved data is used to generate personalized responses. Since LLMs cannot be deployed locally due to resource limits, there is a risk of privacy leakage. Future research could focus on privacy-preserving methods like federated learning, secure computation, and differential privacy to protect user data [Yao et al., 2024; Liu et al., 2024a]."}, {"title": "7 Conclusions", "content": "This survey offers a thorough overview of Personalized Large Language Models (PLLMs), emphasizing personalized responses tailored to individual user data. We introduce a structured taxonomy that categorizes existing approaches into three key technical perspectives: Personalized Prompting (Input Level), Personalized Adaptation (Model Level), and Personalized Alignment (Objective Level), with further subdivisions within each. We also discuss current limitations and propose several promising directions for future research. Our work provides valuable insights and a framework to drive the advancement of PLLMs development."}]}