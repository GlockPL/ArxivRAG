{"title": "VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding", "authors": ["Houlun Chen", "Xin Wang", "Hong Chen", "Zeyang Zhang", "Wei Feng", "Bin Huang", "Jia Jia", "Wenwu Zhu"], "abstract": "Existing Video Corpus Moment Retrieval (VCMR) is limited to coarse-grained understanding, which hinders precise video moment localization when given fine-grained queries. In this paper, we propose a more challenging fine-grained VCMR benchmark requiring methods to localize the best-matched moment from the corpus with other partially matched candidates. To improve the dataset construction efficiency and guarantee high-quality data annotations, we propose VERIFIED, an automatic VidEo-text annotation pipeline to generate captions with Rellable FInE-grained statics and Dynamics. Specifically, we resort to large language models (LLM) and large multimodal models (LMM) with our proposed Statics and Dynamics Enhanced Captioning modules to generate diverse fine-grained captions for each video. To filter out the inaccurate annotations caused by the LLM hallucination, we propose a Fine-Granularity Aware Noise Evaluator where we fine-tune a video foundation model with disturbed hard-negatives augmented contrastive and matching losses. With VERIFIED, we construct a more challenging fine-grained VCMR benchmark containing Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG which demonstrate a high level of annotation quality. We evaluate several state-of-the-art VCMR models on the proposed dataset, revealing that there is still significant scope for fine-grained video understanding in VCMR. Code and Datasets are in https://github.com/hlchen23/VERIFIED.", "sections": [{"title": "1 Introduction", "content": "Video Corpus Moment Retrieval (VCMR) [1] aims to retrieve a video moment from a large untrimmed video corpus given a text query. It requires handling two subtasks: Video Retrieval (VR) [2] from a corpus and Single Video Moment Retreival (SVMR) [3, 4] within a video, which involves grasping multi-level semantic granularities across video-text and moment-text alignment. However, as shown in Figure 1(a), in the previous VCMR setting, the queries are usually coarse-grained and thus struggle to localize a video moment discriminatively, where there exists potentially relevant positive pairs [5-7] besides the ground truth, which hinders cross-modal retrieval and makes it hard for models to learn distinctive video features.\nTo address the problem, we propose a more challenging VCMR scenario in this paper. As shown in Fig 1(b), a fine-grained distinctive query is provided to retrieve the best-matched moment, requiring models to precisely understand the details in text descriptions and distinguish the target moments from"}, {"title": "2 Related Works", "content": "Video Annotation through Multimodal Models. Most video-text datasets heavily rely on manual work and domain knowledge, especially for fine-grained details [8-11], limiting their scalability, particularly in video moment datasets [3, 18, 4]. Others construct large-scale datasets via web crawling [19] or ASR [20], but suffer from noisy cross-modal alignment. With the rapid advancement of multimodal foundation models and LLM, automatically annotating large-scale video-text datasets is becoming feasible [21]. InternVid [22] integrates image captioning models to caption video clips at multiple scales, while Panda-70M [23] uses multimodal teacher models to caption 70M text-annotated videos. MVid [24] automatically captions visual, audio, and speech with LLM refinement. However, they often lack fine-grained annotations, especially for dynamic details such as motions and interactions, or rely mainly on subtitles or auxiliary text labels [25]. To address this, we propose VERIFIED to automatically capture fine-grained static and dynamic details from the vision modality with quality management.\nVideo Corpus Moment Retrieval. Video moment retrieval (VMR) [3, 4, 26\u201337] requires local-izing a matched moment within an untrimmed video for a text query and video corpus moment retrieval (VCMR) [1] extends VMR to search the target moment from a large untrimmed video corpus, requiring appropriate integration of video retrieval and moment localization. Among VCMR methods, XML [25] introduces a convolutional start-end detector with its late fusion design. CONQUER [38] integrates query context into video representation learning for enhanced moment retrieval by two stages. ReLoCLNet [39] leverages video and frame contrastive learning through separate encoder alignment. As video corpora expand, many video moments share similar semantics with subtle differences in fine-grained statics and dynamics. While some VCMR works [5\u20137] have explored relevance within non-ground-truth moments and texts by pseudo-positive labels or relevance-based margin, they fail to learn distinctive differences among semantically analogous clips. To address this, we propose a fine-grained VCMR scenario that requires localizing the best-matched moment among partially matched candidates for a fine-grained text query and we introduce new datasets to benchmark state-of-the-art VCMR methods."}, {"title": "3 Dataset Construction Methodology: VERIFIED", "content": "In this section, we introduce our VERIFIED pipeline to annotate fine-grained captions for video moments with reliable static and dynamic details. The annotations in the previous video mo-ment datasets are in the form of $(V, t_s, t_e, q)$ for a moment $V[t_s: t_e]$ (designated as $v$) from $t_s$ to $t_e$ seconds in the video $V$, where $q$ is a moment-level text caption for moment $v$. In this pa-per, our VERIFIED pipeline constructs novel fine-grained video moment datasets in the form of $(V, t_s, t_e, \\{q_i^{(s)}\\}_{i=1}^{N_s}, \\{c_i^{(s)}\\}_{i=1}^{N_s}, \\{q_i^{(d)}\\}_{i=1}^{N_{d}}, \\{c_i^{(d)}\\}_{i=1}^{N_{d}})$. We annotate the same video moment in the previous dataset with multiple diverse captions containing rich static and dynamic fine-grained"}, {"title": "3.1 Statics Enhanced Captioning", "content": "Given the moment $(t_s,t_e)$ in the video $V$ with its original coarse caption annotation $q$, we first extract key frames from the moment $(t_s,t_e)$. Concretely, we adaptively adjust the threshold in PySceneDetect\u00b2 to split the moment up to $L$ segments and we select the mid-time frames of these segments as key frames $\\{f_i\\}_{i=1}^L$. For each keyframe $f_i$, we prompt a strong image LMM with the inputs of the previous coarse caption $q$ as guidance and the key frame $f_i$ to describe fine-grained details of the foreground $D_i^{(fg)}$ and background $D_i^{(bg)}$ before generating a complete fine-grained description $D_i$ of this frame.\n$D_i^{(fg)}, D_i^{(bg)}, D_i = ImageLMM(f_i, q) \\tag{1}$\nAfterward, we prompt an LLM to extract important static attributes to rephrase it to $N_s$ diverse fine-grained caption candidates $\\{q_i^{(s)}\\}_{i=1}^{N_s}$ as follows,\n$\\begin{aligned}\\ \\{q_i^{(s)}\\}_{i=1}^{N_s} = LLM((\\{D_i^{(fg)}, D_i^{(bg)}, D_i\\}_{i=1}^L,q) \\tag{2}\\end{aligned}$\nThese new captions now contain rich static visual details about the video moment."}, {"title": "3.2 Dynamics Enhanced Captioning", "content": "Since it's hard for even existing strong video captioning models to capture rich enough dynamic information, we introduce video question answering (VQA) to enhance the dynamic information extraction process. We prompt an LLM to generate $N_{qa}$ relevant dynamics-oriented questions"}, {"title": "3.3 Fine-Granularity Aware Noise Evaluator", "content": "Specifically, for a piece of original sample $(V, t_s, t_e, q)$, we prompt LLM to generate $N_{pos}$ positively rewritten captions $\\{q_i^+\\}_{i=1}^{N_{pos}}$, $N_{neg}$ statics disturbed negative captions $\\{q_i^-\\}_{i=1}^{N_{neg}}$, and $N_{neg}$ dynamics disturbed negative captions $\\{q_i^{d-}\\}_{i=1}^{N_{neg}}$ for the previous coarse caption $q$, respectively.\n$\\begin{aligned}\\ \\{q_i^+\\}_{i=1}^{N_{pos}}, \\{q_i^-\\}_{i=1}^{N_{neg}}, \\{q_i^{d-}\\}_{i=1}^{N_{neg}} = LLM(q) \\tag{6}\\end{aligned}$\nwhere we prompt LLM to generate rewritten captions $\\{q_i^+\\}_{i=1}^{N_{pos}}$ that share the same meanings as the previous coarse caption $q$, $\\{q_i^-\\}_{i=1}^{N_{neg}}$ that have an significant difference in some static attributes from $q$, and $\\{q_i^{d-}\\}_{i=1}^{N_{neg}}$ that have an significant difference in some dynamic content from $q$.\nSince LLM sometimes fails to generate appropriate rewritten captions, e.g. some $q^-$ might share the same meaning as $q$, we select the best one from the candidates. We adopt SentenceBERT [40] as a semantic distance measure on captions to discover the most positive caption $\\tilde{q}^+$ and most negative static or dynamic one $\\tilde{q}^{s/d-}$, where\n$\\begin{aligned}\\ \\tilde{q}^+ &= arg\\, min \\{SentenceBERT(q,q_i^+) \\}_{i=1}^{N_{pos}} \\tag{7}\\\\ \\tilde{q}^{s/d-} &= arg\\, max \\{SentenceBERT(q,q_i^{s/d-}) \\}_{i=1}^{N_{neg}} \\tag{8}\\end{aligned}$\nAfterward, we finetune UMT [17] in the video-text retrieval task with the hard-negatives augmented contrastive loss $l_c$ and matching loss $l_m$. The contrastive loss $l_c$ is\n$\\begin{aligned}\\ l_c = -\\frac{1}{2B} \\sum_{i=1}^B log(\\frac{exp(s(v_i, q_i) / \\tau)}{\\sum_{j=1}^B exp(s(v_i, q_j) / \\tau)}) - \\frac{1}{2B} \\sum_{i=1}^B log(\\frac{exp(s(q_i, v_i) / \\tau)}{\\sum_{j=1}^B exp(s(q_i, v_j) / \\tau)}) \\tag{9}\\end{aligned}$\nwhere B is the batch size and s is a similarity measure. The $\\tilde{q}_i^{s/d-}$ is a hard negative sample for $v_i$ and other $\\tilde{q}_j^{s/d-} (j \\neq i)$ can be seen as trivial negative samples for $v_i$. The matching loss $l_m$ is\n$\\begin{aligned}\\ l_m = -\\frac{1}{B} \\sum_{i=1}^B log\\sigma(c(v_i, q_i)) + \\sum_{q_j \\in N(v_i)} log(1 - \\sigma(c(v_i, q_j))) + \\sum_{v_j \\in N(q_i)} log(1 - \\sigma(c(v_j, q_i))) \\tag{10}\\end{aligned}$\nwhere c is a classifier, $\\sigma$ is the Sigmoid function, and $N(v_i), N(q_i)$ contains the negative samples for $v_i$ and $q_i$. $N(v_i)$ contains sampled trivial negative sample $q_j (j \\neq i)$ and the augmented hard-negatives $\\tilde{q}^-$ and $\\tilde{q}^{d-}$ while $N(q_i)$ contains only sampled trivial negative sample $v_j(j \\neq i)$.\nTo alleviate potential harmful biases in LLM-generated texts and avoid degenerating into distinguish-ing the human-written text from other LLM-generated ones, we replace the previous coarse caption $q$ with the selected positively rewritten caption $\\tilde{q}^+$ for fine-tuning as well with loss $\\tilde{l}_c$ and $\\tilde{l}_m$.\nFinally, we combine all these items to derive the total loss function $l$.\n$\\begin{aligned}\\ l = (l_c + \\tilde{l}_c) + (l_m + \\tilde{l}_m) \\tag{11}\\end{aligned}$\nAfter fine-tuning, we evaluate the matching confidence scores between the video moment $v$ and our generated fine-grained captions $\\{q_i^{(s)}\\}_{i=1}^{N_s}$ and $\\{q_i^{(d)}\\}_{i=1}^{N_{d}}$ as $\\{c_i^{(s)}\\}_{i=1}^{N_s}$ and $\\{c_i^{(d)}\\}_{i=1}^{N_{d}}$, with our evaluator. Captions with relatively lower scores are more likely to have inaccurate content."}, {"title": "3.4 Implementation Details", "content": "For the models used in our VERIFIED pipeline, we use Mistral-7B-Instruct-v0.2 [13] as our LLM, LLaVA-1.6-Mistral-7B [14] as our image LMM, Gemini-1.5-Pro [15] as our video LMM, all-MiniLM-L6-v2 [40] for SentenceBERT, and ViT-L/16_25M [17] version for pretrained UMT. Our VERIFIED automatically annotate video moments from DiDeMo [4], Charades-STA [3], and ActivityNet Captions [18], named DiDeMo-FIG, Charades-FIG, and ActivityNet-FIG respectively. In the Statics Enhanced Captioning, we select up to $L = 1, L = 1$ and $L = 5$ key frames for DiDeMo-FIG, Charades-FIG and ActivityNet-FIG. In the Dynamics Enhanced Captioning, we extract frames at 8fps for DiDeMo-FIG and Charades-FIG and uniformly sample 64 frames for ActivityNet-FIG for the video input of the video LMM, and we set the VQA pair numbers $N_{qa} = 5$. We generate $N_s = N_d = 3$ fine-grained statics or dynamics enhanced caption candidates for both modules. In the Fine-granularity Aware Noise Evaluator, we generate $N_{pos} = N_{neg} = 3$ positive/negative disturbed captions for each previous caption $q$. During fine-tuning, we sample 20 frames for each video moment, and the temperature $\\tau$ is 0.07 and $\\lambda_c, \\lambda_m$ is 1. We collect around 125K disturbed samples from all previous datasets for fine-tuning with a learning rate of le-5 and B = 16 batch size for 10 epochs in a 4 A100-80G machine. After all, we grade each video-text pair with our evaluator and choose the fine-grained enhanced caption with the highest score for each video moment. We follow the previous dataset splits. The prompts used for LLM/LMM are attached to the supplementary materials."}, {"title": "3.5 Statistical Analysis and User Study", "content": "We present the statistics of our annotated datasets compared to the previous coarse ones in Tab 1, where our annotations feature a richer vocabulary and approximately twice the number of content words with various parts of speech, particularly adjectives, indicating that our fine-grained captions provide more detailed descriptions."}, {"title": "4 VCMR Experiment", "content": "Methods. We benchmark several state-of-the-art VCMR approaches: HERO [2], XML [25], Re-LoCLNet [39], CONQUER [38], SQuiDNet [41]. Among them, CONQUER and SQuiDNet need additional input, a rank list of videos of top K with scores. They retrieve moments from the top K videos other than the total video corpus. CONQUER keeps the initial rank list and scores un-changed for refined moment retrieval, while SQuiDNet learns to rerank the scores of videos for better moment retrieval. We use the video retrieval list of XML for the additional inputs of CONQUER and SQuiDNet. Implementation details and more experiments can be found in the supplementary materials.\nMetrics. Following [3, 1], we evaluate these methods on our fine-grained VCMR datasets with VCMR, SVMR, and VR tasks. We use the {m}/r{K} for VCMR and SVMR, where m \u2208 {0.5,0.7} and K \u2208 {1,5,10, 100}, and r{K} for VR. {m}/r{K} denotes the proportion of top K proposals that have temporal Intersection over Union (tIoU) with the ground truth larger than m."}, {"title": "5 VERIFIED Pipeline Evaluation", "content": "We first explore how important our fine-grained training data is for understanding video details, to show the significance of our whole pipeline. In Tab 6, we train XML with previous coarse-grained or our fine-grained data and evaluate its performance in the fine-grained scenario. Results indicate that the impact of training with fine-grained annotations on fine-grained SVMR is relatively minor; however, it significantly enhances the performance of fine-grained VR and VCMR. This improvement is due to the fact that while fine-grained details are often redundant within a single video, they are essential for accurately pinpointing unique moments across a vast collection of similar clips. Models trained with previous coarse annotations struggle to generalize to the fine-grained scenario, especially in a large video corpus, indicating the necessity of our insight to introduce fine-grained datasets. Besides, the VCMR and VR performances on Charades-FIG are quite suboptimal, since all videos in Charades are about in-door activities and share similar semantics, making it the most challenging benchmark to evaluate methods' capability of perceiving fine-grained video differences."}, {"title": "6 Conclusion and Future Work", "content": "This paper discovers that existing VCMR benchmarks' focus on coarse-grained understanding limits methods' ability to learn distinct video features and perceive fine-grained differences between video moments. Thus, we propose a more challenging fine-grained VCMR benchmark, requiring models to retrieve the best-matched moment from a video corpus given a fine-grained query, with other partially matched candidates present. To ensure efficient and high-quality video annotations, we introduce VERIFIED, an automatic video-text annotation pipeline that uses LLM and LMM to generate detailed statics and dynamics enhanced captions and filters out inaccuracies through our Fine-Granularity Aware Noise Evaluator. This evaluator is obtained via fine-tuning UMT with disturbed hard-negatives augmented contrastive and matching losses. We create the Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG datasets with high-quality annotations to support fine-grained VCMR. Benchmarking state-of-the-art VCMR methods reveals that those trained on coarse annotations struggle to generalize to fine-grained scenarios, highlighting the necessity for improved fine-grained video understanding in VCMR. In the future, the next goal might be to train a completely end-to-end captioning model to complete the fine-grained annotations with the capabilities of the complicated pipeline that combines many powerful existing models. The disturbed hard negative data enhances the ability of our evaluator to understand details. However, the gap between the captioning modules' real hallucinations and our perturbation approximation does exist and it would require more analysis to reduce this gap."}, {"title": "Checklist", "content": "1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We claim that we set up a challenging fine-grained VCMR setting and propose VERIFIED, an automatic fine-grained video-text annotation pipeline, in the abstract and introduction."}]}