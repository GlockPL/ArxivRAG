{"title": "Rhyme-aware Chinese lyric generator based on GPT", "authors": ["Yixiao Yuan", "Yangchen Huang", "Yu Ma", "Xinjin Li", "Zhenglin Li", "Yiming Shi", "Huapeng Zhou"], "abstract": "Neural language representation models such as GPT, pre-trained on large-scale corpora, can effectively capture rich\nsemantic patterns from plain text and be fine-tuned to consistently improve natural language generation performance.\nHowever, existing pre-trained language models used to generate lyrics rarely consider rhyme information, which is crucial\nin lyrics. Using a pre-trained model directly results in poor performance. To enhance the rhyming quality of generated\nlyrics, we incorporate integrated rhyme information into our model, thereby improving lyric generation performance.", "sections": [{"title": "1. INTRODUCTION", "content": "Writing lyrics is challenging, even for experienced lyricists. We aim to design an Al lyrics generator to inspire lyricists\nwhen they lack inspiration. To the best of our knowledge, existing methods for Chinese lyrics are inadequate, prompting\nus to seek modifications for improved results. We believe this work is both interesting and valuable.\nPre-trained models[1,2,3,4] have achieved enormous success in various natural language processing tasks such as machine\ntranslation, information retrieval, and text generation. These models can learn extensively from large corpora, allowing\nthem to represent sentence semantics effectively. In this project, we utilize a pre-trained model to aid in the generation of\nChinese lyrics.\nTraditional natural language generation methods do not require rhyme, resulting in poor-quality lyrics when these methods\nare used. Ernie[5] integrates entity representations in its knowledge module, learning a substantial amount of general\nknowledge from knowledge graphs. Inspired by Ernie, we decided to incorporate integrated rhyme information into our\nmodel to help it learn the rhyme of Chinese characters.\nOur work has two main contributions. First, we successfully added rhyme embedding to the GPT-2 model to generate\nbetter rhyming lyrics. Second, we propose a new way to integrate the rhyme of Chinese characters and word representation.\nIn our self-made test dataset, we achieved an 82.2% rhyme ratio, while the pre-trained model only achieved a 30.9% rhyme\nratio."}, {"title": "2. METHOD", "content": "2.1 Model architecture\nAs shown in Figure 1, the whole model architecture of our work consists of two stacked modules. The underlying textual\ncoder responsible to capture basic lexical and syntactic information from the input tokens, and the upper encoder\nresponsible to integrate extra rhyme information into textual information from the underlying layer. Therefore, we can\nrepresent heterogeneous information of tokens and entities into a united feature space."}, {"title": "2.2 Rhyme vocabulary", "content": "To obtain rhyme embeddings, the first step is to create the rhyme vocabulary. We manually classified pinyin into 13 classes;\nfor example, \"ai\" and \"uai\" are placed in the same class. Rhymes within the same class have similar pronunciations.\nAdditionally, we include four tags: <pad>, <cls>, <sep>, and <space>, where <space> represents a pause in a sentence.\nFor English words or other characters that cannot be represented with pinyin, we mark them as <unknown> for convenience.\nThis results in a one-hot label of length 18. After the one-hot input, we use a linear layer to obtain a dense representation."}, {"title": "2.3 Rhyme-aware encoder", "content": "As shown in Figure 2, the rhyme-aware encoder consists of stacked aggregators, which are designed for encoding both\ntokens and rhyme as well as fusing their heterogeneous features. In the i-th aggregator, the input token embeddings\n{w(i-1), ..., Wi-1)}; mi-1)} and rhyme embeddings {e(i-1), ..., e-1)}from the preceding aggregator are fed into two mask multi-\nhead self-attentions (MMH-ATTs)[6], respectively. This is to make the model realize the last word in the previous sentence.\n{w_1^{(i)},..., w_n^{(i)}\\} = MMH-ATT (\\{w_1^{(i-1)},...,w_n^{(i-1)}\\})\n\\{e_1^{(i)},..., e_m^{(i)}\\} = MMH-ATT (\\{e_1^{(i-1)},...,e_m^{(i-1)}\\})\nThen, the i-th aggregator adopts an information fusion layer for the mutual integration of the token and rhyme, and\ncomputes the output embedding for each token and rhyme. For a token wjand its rhyme ejthe information fusion process\nis as follows,\nh_{j}^{(i)} = \\sigma (W_t^{(i)}[w_j^{(i-1)}; e_j^{(i-1)}] + b^{(i)})\nw_j^{(i)} = \\sigma(W_w^{(i)} h_j^{(i)} + b_w^{(i)})\ne_j^{(i)} = \\sigma(W_e^{(i)} h_j^{(i)} + b_e^{(i)})\nAdditionally, we incorporate layer normalization and residual connections between the two aggregators. We compared\nmodels with and without these layers, finding that residual connections help the model converge. After testing for 40\nepochs, we observed that the loss for the model without these layers fluctuates around 6, whereas the model with these\nlayers sees a continuous decline in loss, reaching 2.5 with a continued downward trend. Clearly, residual connections\nenhance our model's performance."}, {"title": "3. EXPERIMENTS", "content": "3.1 Dataset and pre-processing\nThe Chinese-Lyric-Corpus is designed for the task of Chinese lyric generation, containing nearly 50,000 Chinese lyrics\nfrom 500 artists. To better learn the rhyme information between sentences, we train the model using every two sentences\nin a song instead of a single lyric. This approach helps the model capture the rhyme of the last few words."}, {"title": "3.2 Implementation details", "content": "The proposed network is implemented using PyTorch and is trained on a single NVIDIA RTX 3090 GPU. The model\nundergoes training for 40 epochs using the Adam optimizer, with an initial learning rate of le-5 and a warm-up period of\n4 epochs. After reaching the default learning rate, it decays linearly. The loss function used is cross-entropy loss. The\nmodel comprises 6 aggregators with a fusion dimension of 768, and each aggregator employs a masked attention\nmechanism with 4 heads. The batch size for training is set to 64, and gradient accumulation is used for 2 steps."}, {"title": "3.3 Ablation studies", "content": "To demonstrate the effectiveness of our proposed method, we compared it with a model that does not use rhyme input. All\nparameters were kept the same, with the only difference being the removal of rhyme input. Given the complexity of lyric\nwriting, we conducted a human evaluation to assess the performance of different models. Each lyric was assessed by three\nindividuals in a blind review manner, where the reviewers had no information about the generation method used for each\nlyric. Following previous work in generating poems[7,8], we evaluated the generated lyrics based on three criteria:\nconsistency, fluency, and meaning. Each criterion was rated from 1 to 3, representing bad, normal, and good, respectively.\nAdditionally, we added a rhyming rate to evaluate rhyming performance.\nAs shown in the table, our rhyme embedding method is very effective compared to methods without rhyme embedding.\nUsing raw data, the rhyme embedding increases the rhyming rate by 10 percent. With processed data, it increases the\nrhyming rate by 20 percent. Furthermore, comparing raw and processed data, the model using processed data doubles the\nrhyming rate.\nThis result is easily explained. Rhyme embedding incorporates rhyme information into the model, enabling it to learn more\nabout rhyming. About half of the original data is rhyming, so the model can still learn to rhyme without processing data.\nAfter processing, all the training data is rhyming, allowing our method to achieve a rhyming rate of 0.822. It is noteworthy\nthat we did not use any rhyme-specific loss during training, indicating that our model learned rhyming information in lyrics\nwithout any supervision. However, we did not compare adding rhyme-specific loss to adding rhyme embedding. We\nsuggest that adding rhyme-specific loss might cause the model to lose the ability to learn semantic information in lyrics\nand generate rhyming but meaningless sentences. Additionally, the rhythmic beauty of lyrics is reflected not only at the\nend of sentences but throughout the entire sentence. Rhyme-specific loss can only learn rhyme at the end of sentences,\nwhereas rhyme embedding can capture more comprehensive rhyme information, even if it cannot be clearly explained.\nIn our method, the model learns rhyme information without any supervision, which we believe allows it to learn rhyming\ninformation without diminishing the meaning. The results support our suggestion. The metrics for consistency, fluency,\nand meaning are comparable to those of methods without rhyme embedding. However, since we relied on human\nevaluation, the variance is large.\nWe also present some results generated by the two models, showing that our model with rhyme input outperforms the other\nin both qualitative and quantitative analyses."}, {"title": "3.4 Result", "content": "Here are the results generated by our method. We found that many of the generated lyrics focus on love, which reflects the\nlove-centric nature of our training data. To reduce the prevalence of love-themed lyrics, we need a more diverse dataset.\nIn Figure 3, the generated sentences on the left all use the rhyme \"an,\" showing that our method produces rhymed lyrics.\nThe generated lyrics appear meaningful and can be hard to distinguish from human-made ones at first glance.\nNot all sentences rhyme perfectly; on the right, two sentences have different rhymes. However, the overall lyric maintains\ncoherence, with recurring themes like \"\u96e8\" demonstrating consistency. This coherence is due to the self-attention\nmechanism in the transformer, which handles long-distance dependencies effectively.\nIn summary, our method effectively generates rhymed and meaningful lyrics, with the self-attention module enhancing\ncontent coherence. Expanding the training dataset can further improve the variety and quality of the lyrics."}, {"title": "3.5 Attention visualization", "content": "To check if our model works well, we visualized the attention in Transformer models. In the visualization, darker colors\nindicate greater contribution.\nAttention visualization in the token layer (left side of Figure 4) shows that every word contributes significantly to the next,\nwith attention to both nearby and distant words, ensuring consistency. In the rhyme layer visualization (right side of Figure\n4), rhymes within the same class show a strong contribution to each other. For example, the end of \"\u5929\" greatly influences\nthe end of \"\u73b0\", explaining why our model generates more rhyming lyrics.\nThese visualizations demonstrate that our model effectively captures both local and global dependencies in lyrics,\ncontributing to the generation of coherent and rhyming lyrics."}, {"title": "4. CONCLUSION", "content": "In this paper, we propose a model called the Rhyme-Aware Chinese Lyric Generator, which incorporates rhyme\ninformation and is based on GPT-2. Experimental results show that our method outperforms models without rhyme\nembedding in generating lyrics. However, our work has limitations. The training data is not large enough, and we only\nconsider rhyme without addressing intonation, which is also important in lyrics. Additionally, expanding the dataset and\nincorporating more prior knowledge could further enhance the model's performance in limited data scenarios."}]}