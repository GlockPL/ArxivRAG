{"title": "Interaction2Code: How Far Are We From Automatic Interactive Webpage Generation?", "authors": ["JINGYU XIAO", "YUXUAN WAN", "YINTONG HUO", "ZHIYAO XU", "MICHAEL R.LYU"], "abstract": "Converting webpage design into functional UI code is a critical step for building websites, which can be labor-intensive and time-consuming. To automate this design-to-code transformation process, various automated methods using learning-based networks and multi-modal large language models (MLLMs) have been proposed. However, these studies were merely evaluated on a narrow range of static web pages and ignored dynamic interaction elements, making them less practical for real-world website deployment.\nTo fill in the blank, we present the first systematic investigation of MLLMs in generating interactive web-pages. Specifically, we first formulate the Interaction-to-Code task and build the Interaction2Code benchmark that contains 97 unique web pages and 213 distinct interactions, spanning 15 webpage types and 30 interaction categories. We then conduct comprehensive experiments on three state-of-the-art (SOTA) MLLMs using both automatic metrics and human evaluations, thereby summarizing six findings accordingly. Our experimental results highlight the limitations of MLLMs in generating fine-grained interactive features and managing interactions with complex transformations and subtle visual modifications. We further analyze failure cases and their underlying causes, identifying 10 common failure types and assessing their severity. Additionally, our findings reveal three critical influencing factors, i.e., prompts, visual saliency, and textual descriptions, that can enhance the interaction generation performance of MLLMs. Based on these findings, we elicit implications for researchers and developers, providing a foundation for future advancements in this field. Datasets and source code are available at https://github.com/WebPAI/Interaction2Code.", "sections": [{"title": "INTRODUCTION", "content": "As the Internet continues to evolve and expand, more and more websites emerge, contributing to the diverse and ever-growing online world. As of 2024, the digital landscape comprises approximately 1.09 billion websites [2], supporting a variety of applications in people's daily lives.\nThe design and development of Graphical User Interfaces (GUIs) are vital for creating a website. A well-designed GUI not only enhances the website's visual attractiveness but also improves usability and user satisfaction. In such a process, GUI design involves shaping the website's aesthetics[1], such as layout, colors, and typography [9, 29]. In contrast, GUI development is about implementing that aesthetic through programming languages. Nevertheless, such conversion is a complex and time-consuming task. Developers must manually map visual elements to their corresponding implementation details, which can lead to errors and discrepancies between the original design and the final looks [9, 30, 39, 40, 59].\nTo allow developers to transform design diagrams into functional GUI code more easily, several automated GUI code generation methods have been proposed, which can be further categorized"}, {"title": "PROBLEM DEFINITION", "content": "To describe the interactions within a webpage, we define the Webpage Interaction Graph (WIG):\n$WIG = {N, E}$,\nwhere $N = {S0, S1, .., Sn}$ is a finite set of nodes representing the screenshots of the webpage. $E = {I0, I1, ..., Im}$ represents a series of interaction events that connect different screenshots with directed edges, indicating transitions caused by user interactions. We use numbers to represent different interaction events and then correspond them to the screenshots. For example, the first screenshot represents the original web page, the second screenshot represents the result after interaction 1, and the third screenshot represents the result after interaction 2. Let $C0$ denote the original webpage file, including HTML, CSS, and JavaScript code, $S0$ denote the screenshot of the webpage, $S0$ denote the webpage screenshot after interaction $In$, $G0$ denotes the webpage interaction graph of the original webpage. To achieve the Interaction-to-Code task, M takes the $G0$ as input and"}, {"title": "THE INTERACTION2CODE BENCHEMARK", "content": "In this section, we describe how to construct representative webpages and interactions for Interaction-to-Code tasks and report the statistical results of the dataset.\nOur overall goal is to obtain a set of well-structured webpages that represent a variety of real-world use cases (i.e., diverse webpages and interactions). We follow these steps for automatic processing and manual filtering.\nWebpage Selection. Following the Design2Code [49], we begin by collecting website links from the C4 validation set [48]. To select web pages from these links, we employ four PhD students majoring in computer science, each with experience in front-end development. Each student is assigned to select approximately 25 web pages. The selection criteria were as follows: 1) complexity: each web page must contain at least four meaningful interactions; 2) diversity: the selection process aims to include a wide range of web pages with different topics and interaction types. Ultimately, we compile a dataset consisting of 97 web pages.\nAutomatic Interaction. After gathering representative web pages, we utilize Selenium Web-Driver\u00b9 to simulate user interactions with the pages. Our focus is on interactions within a single page, so we eliminate all external links to prevent navigation away from the current page. Additionally, we replace all images and videos with placeholders to mitigate the impact of external dependencies on the code generation task. Subsequently, we traverse all elements on the webpage using the Document Object Model (DOM) tree and capture different states by taking screenshots before and after specific interactions."}, {"title": "Data Statistics and Diversity", "content": "Quantitative Metrics. To measure the diversity and complexity of our dataset, we adopt the same statistical metrics as those in Design2Code [49], with the results presented in Table 2. The Length indicates the token length obtained through the GPT-2 tokenizer [47], tag count refers to the number of tags in the HTML code, DOM depth signifies the maximum depth of the HTML's DOM Tree, and unique tags denote the number of unique tags in the HTML code."}, {"title": "STUDY SETUP", "content": "We employ three state-of-the-art (SOTA) MLLMs: Gemini 1.5 [23], GPT-40 [43] and Claude-3.5 [4] to evaluate their performance on Interaction-to-Code task. the specific model numbers are 20240806 for GPT-40, 20240620 for Claude-3.5-Sonnet, and Gemini-1.5-flash-latest accessed during October 2024. In configuring the MLLM models, we set the temperature to 1 and the maximum number of tokens output for the three models as 4096. All other parameters were kept at their default settings as outlined in the relevant API documentation [5, 23, 44]."}, {"title": "Metrics", "content": "We employ both full webpage metric and interactive part metric to judge the capability of MLLMs in the Interaction-to-Code task. We measure the quality of webpages generated by MLLMs from the perspectives of visual, structure, and text:\nVisual Similarity. We use CLIP score [46]to measure the visual similarity. This metric measures the semantic similarity between the generated and original webpages, serving as an indicator of how effectively the generated GUI captures the intended visual elements and overall design concept.\nStructure Similarity. SSIM [52] (Structural Similarity Index Measure) score is applied to calculate the structure similarity. It evaluates the layout and compositional accuracy, emphasizing the spatial arrangement and structural similarities between the generated and original webpages.\nText Similarity. We first use python OCR tools to recognize the text in the original and the generated webpages, and then use the Bilingual Evaluation Understudy (BLEU) score [45] to measure the text similarity between the two web pages.\nFor the interactive parts of webpages, in addition to the above visual, structure and text similarity, we also evaluate them from the perspective of the position and function of the interaction.\nPosition Similarity. The position similarity between original interaction Io and generated interaction Ig is defined as follows:\n$Possim(Io, Ig) = 1 \u2013 max(abs(xo \u2013 xg), abs(yo \u2013 yg)),$\nwhere $(xo, yo)$ and $(xg, yg)$ are normalized coordinates (in [0, 1]) of the center of the interactive area.\nFunction Usability. This metric is used to measure whether the interactive function is usable, human annotators are asked to interact with the generated webpage and judge the usability. Let N(\u00b7) denote the quantity, we can calculate the Usability Rate (UR):\n$UR = \\frac{N(usable)}{N(usable) + N(unusable)}\u02d9$"}, {"title": "Prompt Design", "content": "We design three types of prompt methods: direct prompt, chain-of-thought prompt, and mark prompt, as shown in Fig 5. In the direct prompt, the first screenshot represents the original webpage state, while subsequent screenshots depict states after specific interactions. Requirements are applied to guide MLLMs in replicating the webpage design and interaction. In particular, requirement 3 involves letting MLLMs number interactive elements to allow direct identification by ID, enabling automated interaction and screenshot capture for generated webpages. For the Chain-of-Thought (CoT) prompt [53], we use the instruction \u201clet's think step by step\u201d and design three intermediate steps: analyze the interaction effects, locate the interactive elements, and implement the interaction. For the Mark prompt, We use red bounding boxes to highlight the areas of interaction, prompting MLLMs to focus on the interactive parts."}, {"title": "EXPERIMENTS", "content": "In this work, we conduct experiments to answer the following questions:\nRQ1: How do different MLLMs perform in Interaction-to-Code task under different prompts?\nRQ2: How do humans evaluate the usability of interactions generated by MLLMs?\nRQ3: How do MLLMs perform in code generation across different interaction scenarios?\nRQ4: What types of mistakes do MLLMs make in generating interactions?\nRQ5: How does visual saliency influence the quality of generated interactions?"}, {"title": "RQ1: How do different MLLMs perform in Interaction-to-Code task under different prompts?", "content": "We present the results of three leading MLLMs under three different prompts in Table 4, bold values indicate the optimal performance, and underlined values indicate the second-best performance. First, we can make the following observations of MLLMs under direct prompting:\nGeneration of interactive elements presents greater challenges than static full webpage generation. Table 4 shows that the performance metrics for interactive components are notably lower than those for complete webpages under direct prompts. Regarding visual similarity,\nMLLMs attain approximately 0.73-0.78 for full pages, compared to 0.71-0.76 for interactive elements. Structure similarity shows a more pronounced disparity, with MLLMs achieving 0.6-0.78 for full pages but only 0.4-0.56 for interactive components. Similarly, text similarity scores reach about 0.65 for full pages, contrasting with approximately 0.5 for interactive elements.\nMLLMs demonstrate limitations in accurately reproducing fine-grained features of interaction. The performance of MLLMs on fine-grained metrics (such as structure, text, and position similarity) is notably weaker compared to their performance on coarse-grained metrics like CLIP score. As illustrated in Table 4, for the interaction part, the CLIP similarity exceeds 0.7, whereas text similarity hovers around 0.5, position similarity approximates 0.45-0.62, and structure similarity ranges between 0.4 and 0.5.\nClaude-3.5 outperforms GPT-40 and Gemini-1.5 in the Interaction-to-Code task. Experiment results of direct prompting reveals a consistent performance ranking, with Claude-3.5 leading, followed by GPT-40, and Gemini-1.5 showing the lowest performance.\nWhile MLLMs demonstrate competence in generating static webpages, they encounter more challenges when producing interactions. Specifically, MLLMs struggle to reproduce details (e.g., structure, text, position) of interactions.\nTo improve the performance of interaction, we further propose CoT and Mark prompts to force models to focus on the interaction part, resulting in the following observations:\nBoth CoT and Mark prompts enhance model performance compared to direct prompt, the Mark prompt demonstrates superior performance compared to the CoT prompt. GPT-40's metrics (CLIP, SSIM, text, position) of the interaction part improve from direct prompting scores (0.7328, 0.4221, 0.4848, 0.6053) to (0.7212, 0.4556, 0.4902, 0.6079) with CoT, and further to (0.7454, 0.5583, 0.5241, 0.6123) with Mark prompting. However, both prompting methods slightly decrease full-page metrics, likely due to their focused emphasis on interactive elements rather than overall page composition.\nChain-of-Thought (CoT) and Mark prompts enhance interaction generation in distinct ways: CoT leverages step-by-step analysis of interactive components, while Mark prompts focus on clear interaction areas. Empirical results indicate that the Mark prompt leads to greater improvements compared to the CoT method."}, {"title": "RQ2: How do humans evaluate the usability of interactions generated by MLLMs?", "content": "Although the above metrics have measured the generation effect of the interaction from different perspectives, the functional evaluation of the interaction still requires human evaluations.\nPairwise Model Comparison Setting. We ask three human annotators to rank a pair of generated interactions (one from the baseline, the other from the tested methods) to decide which one implements the reference interaction function better. We use Gemini-1.5 with direct prompt as the baseline and collect the other eight methods' Win/Tie/Lose rates against this baseline. The results are shown in Fig 6(a); a higher win rate and lower loss rate suggest better quality as judged by human annotators.\nFunctionality Evaluation Setting. We also ask the three annotators to evaluate the functionality (i.e., usability) of generated interaction. If the interactive function is consistent with ground truth, it is regarded as usable, otherwise unusable. We calculate the usability rate of different schemes, the results are shown in Fig 6(b)."}, {"title": "RQ3: How do MLLMs perform in code generation across different interaction scenarios?", "content": "In this section, we study the performance of MLLMs on the Interaction-to-Code task under different interaction types. The results of varying tag categories with high frequency and visual categories are shown in Table 5 and Table 6, respectively.\nFor tag categories, FORM, SELECT, and OPTION are the easiest interaction types to generate, achieving a usability rate higher than 80%. This is because these interactions scenarios always contain fixed patterns, for example, SELECTION and OPTION only appear in drop-down lists, and\nFORM often merely contains input boxes. In contrast, IFRAME and PROGRESS elements show lower usability rates (<60%), attributed to their complexity: IFRAMES involve embedding external content, while PROGRESS bars require intricate component coordination for functions like audio control or price range adjustment, raising difficulties for MLLM to understand.\nFor visual categories, MLLMs excel at generating interactions that result in prominent visual changes, such as creating new windows, and components. However, they struggle with subtle visual modifications, such as color shifts and positional adjustments, indicating their limitations in handling fine-grained interaction effects.\nPerformance varies by interaction type: MLLMs are good at handling interactions with fixed pattern (e.g., selection list) and obvious changes (e.g., new window creation), while struggling with interactions involving complex changes (e.g., iframe, progress) and subtle visual modifications (e.g., position change)."}, {"title": "RQ4: What types of mistakes do MLLMs make in generating interactions?", "content": "We ask annotators to analyze the difference between the generated interactions and the original ones, then summarize the failure types and evaluate their influence from content, function and user experience. In specific, we first randomly select 20 interactions for analysis and then discuss, revise, and refine the failure type until everyone reaches a consensus. Finally, we manually annotate the failure types of all interactions and calculate the Usability Rate (UR) based on the human evaluation results of RQ2. \nFailure reason analysis. Failures (a), (c), (e), and (f) stem from MLLMs' limitations in element localization. Failures (d) and (g) are caused by MLLMs' misidentification of element types. Failures (b), (h), (i), and (j) arise from MLLMs' misunderstanding of interaction.\nBase on the failure distribution in Fig 9, we find that, the main failure modes include \u201cNo interaction\u201d, \u201cPartial implementation\u201d, \u201cInteractive element missing\u201d, and \u201cWrong position after interaction\u201d. Model-specific analysis reveals distinct patterns: Gemini-1.5's failures are dominated by \u201cNo interaction\u201d and \u201cPartial implementation\u201d (>50%), while GPT-40 mainly faces issues with \u201cInteractive element missing\u201d and \u201cNo interaction\u201d (>20%). Claude-3.5's challenges are primarily in \"No interaction\u201d and \u201cWrong position after interaction\u201d (>20%). These failures stem from two key issues: MLLMs' inadequate interaction comprehension leading to \u201cNo interaction\u201d and \u201cPartial implementation", "Interactive element missing": "nd \u201cWrong position after interaction\u201d.\nBesides, the most serious failures are \u201cInteractive element Missing\u201d, \u201cEffect on wrong element\", \"Wrong Function\u201d and \u201cNo interaction\u201d. The severity of the failures depends on the usability rate (UR), with higher UR meaning lower severity and lower UR meaning higher severity.\n\u201cNo interaction\u201d, \u201cPartial implementation\u201d, \u201cInteractive element missing\u201d, and \u201cWrong position after interaction\u201d constitute the most frequent failures. \u201cInteractive element Missing"}, {"title": "RQ5: How does visual saliency influence the quality of generated interactions?", "content": "The visual perception limitations of MLLMs affect their performance on visual understanding tasks, especially when facing small low-resolution objects [60]. In this section, we examine the impact of interaction area ratio (i.e., visual saliency) on generation outcomes. Let I denote interaction, $S_1$ denote the screenshot of the webpage after interaction I, we define the visual saliency (VS) of the interaction as follows:\n$VS(I) = \\frac{area(I)}{area(S_1)},$\nwhere area() calculates the size (in pixels) of a component. A higher VS score indicates a larger area influenced by the interaction and, consequently, a higher visual saliency.\nWe first calculate the visual saliency for all interactions and plot the distribution, as shown in Figure 11. We then divide the samples into five groups based on the distribution results, keeping the number of samples in each group roughly balanced. The VS ranges for the five groups are as follows: [0, 0.025), [0.025, 0.05), [0.05, 0.1], [0.1, 0.2), [0.2, 1). \n\nThe group with higher visual saliency has higher SSIM and position similarity. Although the clip and text similarity fluctuates among different groups,\nshows that the SSIM and position similarity significantly increases as the visual saliency increases. As shown the group [0.2, 1) shows the highest metrics, while the group [0, 0.025) shows the lowest metrics. This demonstrates that MLLMs are more likely to capture structural and positional features for samples with high visual saliency.\nVisual saliency affects the MLLMs' performance on interaction generation, and enhancing visual saliency can lead to more accurate code generation."}, {"title": "RQ6: Which representation modality \u2013 visual signals or textual description, enhances MLLMs to generate interaction code?", "content": "The performance of MLLMs in UI code generation largely depends on their ability to comprehend interactions. Certain interactions, particularly complex ones or those with low visual saliency, pose significant challenges for MLLMs when relying solely on screenshots for comprehension. Natural language descriptions may serve as a valuable supplement to enhance understanding.\nTo investigate the impact of different input signals, we conduct experiments on Gemini-1.5 and GPT-40 using 10 randomly selected webpages from failure cases. Human annotators provide textual descriptions for each interaction (e.g., \"clicking the login button triggers a new window with two input boxes\"). We evaluate three experimental conditions: visual input only (V), textual description"}, {"title": "DISCUSSION", "content": "Implications for Researchers. The findings of our study shed light on following future directions to improve the quality of MLLM-generated UI code in practice.\nEnhancing MLLMs' recognition of fine-grained webpage features. As noticed in Finding 1, MLLMs often struggle to reproduce details of interactions, such as position, text, and structure. Therefore, it is essential to explore strategies to improve the model's sensitivity on these fine-grained features.\nCorrecting errors in MLLM-generated code. In RQ4, we outline common mistakes when MLLMs generate interactive components. Developing automated methods to identify failure types and fix errors is crucial in reproducing reliable and usable webpages.\nEnhancing the MLLM's grounding of GUI elements and its understanding of interactions. In RQ4, we analyze that the existing failures arise from the inability of MLLMs to accurately locate the interacting elements, understand their functionalities, and comprehend the interactions. Therefore, it is essential to enhance the capabilities of MLLMs in this area. Alternatively, a GUI interactive element recognition model and an interactive analysis model could be implemented prior to MLLM input to address these limitations.\nImplications for Developers. Based on our findings, we propose the following practical guidelines for developers leveraging MLLMs in automated front-end development:\nApplying visual markers for interactive elements. Derived from Finding 2, incorporating mark prompts with red bounding boxes significantly enhances MLLMs' ability to generate accurate interactions. These visual markers enable MLLMs to precisely identify both interactive elements and their effect areas.\nOptimize interactive element visibility. Finding 5 indicates that enhanced visual saliency leads to more effective interaction generation. We recommend increasing the visual saliency of the interaction by slicing the image, or even just inputting in the interactive area to generate the code for the interaction part first, followed by the integration of the generated code into the main webpage code.\nProvide comprehensive interaction descriptions. As evidenced by Finding 6, detailed textual descriptions improve interaction generation quality. Developers can include explicit descriptions (like the position, interactive elements, and effects) of interaction in their prompts to make MLLMs understand the interaction clearly."}, {"title": "THREATS TO VALIDITY", "content": "Limited context length. As webpages become more complex with numerous interactions, the input context expands, potentially exceeding the context window constraints of MLLMs (e.g., 128K tokens for GPT-40). Nevertheless, this limitation can be mitigated by employing iterative generation, progressively producing interactions for a webpage over multiple rounds.\nModel selection. This study utilizes three prominent Multimodal Large Language Models (MLLMs) to conduct experiments. There are some open source MLLMs such as LLaVa [35] we don't test, we will test the performance of these models on Interaction-to-Code task in the future work.\nUnable to handle interactions that require back-end. Some complex functional interactions (e.g., login, search, etc.) are implemented by server-side scripting languages like Python. The benchmark we collect does not include back-end code; we cannot verify the generation effect of such interactions, but we believe our work is an important step toward generating interactive websites."}, {"title": "CONCLUSION", "content": "This paper presents the first systematic evaluation of MLLMs in the Interaction-to-Code task. We introduce a formal definition of the Interaction-to-Code paradigm and establish the comprehensive Interaction2Code benchmark encompassing diverse interaction scenarios. Through extensive automated and human evaluations, we assess MLLMs' performance and usability of generated interactions. Our key findings reveal the limitations of MLLMs in the Interaction-to-Code task, failure types, and key factors (prompts, enhanced visual saliency, and supplementary textual descriptions) for enhancing the interaction generation performance of MLLMs."}]}