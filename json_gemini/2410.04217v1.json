{"title": "Improving Portfolio Optimization Results with Bandit Networks", "authors": ["Gustavo de Freitas Fonseca", "Lucas Coelho e Silva", "Paulo Andr\u00e9 Lima de Castro"], "abstract": "In Reinforcement Learning (RL), multi-armed Bandit (MAB) problems have found applications across diverse domains such as recommender systems, healthcare, and finance. Traditional MAB algorithms typically assume stationary reward distributions, which limits their effectiveness in real-world scenarios characterized by non-stationary dynamics. This paper addresses this limitation by introducing and evaluating novel Bandit algorithms designed for non-stationary environments. First, we present the Adaptive Discounted Thompson Sampling (ADTS) algorithm, which enhances adaptability through relaxed discounting and sliding window mechanisms to better respond to changes in reward distributions. We then extend this approach to the Portfolio Optimization problem by introducing the Combinatorial Adaptive Discounted Thompson Sampling (CADTS) algorithm, which addresses computational challenges within Combinatorial Bandits and improves dynamic asset allocation. Additionally, we propose a novel architecture called Bandit Networks, which integrates the outputs of ADTS and CADTS, thereby mitigating computational limitations in stock selection. Through extensive experiments using real financial market data, we demonstrate the potential of these algorithms and architectures in adapting to dynamic environments and optimizing decision-making processes. For instance, the proposed bandit network instances present superior performance when compared to classic portfolio optimization approaches, such as capital asset pricing model, equal weights, risk parity, and Markovitz, with the best network presenting an out-of-sample Sharpe Ratio 20% higher than the best performing classical model.", "sections": [{"title": "1 Introduction", "content": "In the field of Reinforcement Learning (RL), there has been a growing research interest in Multi-Armed Bandit (MAB) problems, a particular problem of RL interpreted as a tabular solution method, where storing transitions does not matter (Charpentier et al., 2023). Despite their simplicity, these problems have gained attention for their effectiveness in addressing real-world challenges, finding applications ranging from recommender systems (Silva et al., 2022), human search behavior (Nakazato et al., 2024) and information retrieval (Losada et al., 2017) to domains like healthcare (Zhou et al., 2023) and finance (Bouneffouf et al., 2020).\nMost of the classical MAB framework assumes stationary reward distributions, where the underlying probabilities remain constant over time. However, real-world applications often feature inherently non-stationary environments that may undergo shifts in their probability distributions. In this sense, the need to address non-stationarity arises. One real environment with such behavior is the finance field (de Castro and Annoni, 2016), where changes in market dynamics demand rapid model responses as to avoid unnecessary risk (SBRANA, 2023 and de Castro and Parsons, 2014). Similarly, in online advertising, user preferences and behavior may change over time, necessitating adaptive strategies to optimize ad placement and maximize click-through rates. If one intends to use MAB to solve these problems, the non-stationary variants might be a great fit.\nIn such dynamic contexts, traditional algorithms falter, leading to the need for the development of novel strategies capable of adapting to changing reward structures in real-time. While current literature discusses algorithms that deal with non-stationarity in Bandits problems, existing solutions often encounter limitations regarding the temporal dynamics of policy adaptation, as current formulations exhibit varying degrees of responsiveness to environmental shifts. In this sense, there are opportunities for exploring novel modeling approaches and algorithms, especially toward improved dynamic adaptability of Bandit algorithms under non-stationary conditions.\nParticular to finance, the Portfolio Optimization problem emerges as a pertinent application area for MAB solutions. Portfolio Optimization involves selecting and allocating assets to achieve a desirable balance of risk and return. Traditional approaches often rely on static allocation strategies, which may fail to account for changing market conditions effectively. By improving MAB techniques, Portfolio Optimization can benefit from adaptive allocation strategies that dynamically adjust asset weights in response to evolving market dynamics (Chen et al., 2024). However, existing literature addressing Portfolio Optimization with MABs remains sparse, highlighting a significant research gap in this domain.\nThe contributions of this paper are manifold. First, it addresses the challenge of non-stationarity in MAB problems by proposing the Adaptive Discounted Thompson"}, {"title": "2 Literature Review", "content": "The goal of this research is to improve the practical usage of Multi-Armed Bandits (MAB) in changing environments such as finance. This section explores the forefront advancements of Non-Stationary Bandit algorithms and outlines some practical applications of bandit algorithms in finance."}, {"title": "2.1 Non-Stationary Bandits", "content": "Unlike traditional stationary bandit settings, where rewards associated with each action remain constant throughout the learning process, non-stationary bandit problems arise in scenarios where the underlying environment is subject to stochastic and agent-independent changes over time, leading to variations in the reward distribution. (Allesiardo et al., 2017)."}, {"title": "3 Proposal", "content": "In this Section, we present a detailed description of the novel non-stationary bandit algorithm, Adaptive Discounted Thompson Sampling (ADTS), its combinatorial bandit variant, Combinatorial Adaptive Discounted Thompson Sampling (CADTS) and how they together constitute original architectures called Bandit Networks aimed to solve the Portfolio Optimization problem using historical daily price of the Standard and Poor's (S&P) stocks. We empirically evaluate the performance of the proposed"}, {"title": "3.1 Adaptive Discounted Thompson Sampling", "content": "We introduce the Adaptive Discounted Thompson Sampling (ADTS), a Thompson Sampling (TS) (Thompson, 1933) variant aimed to deal with non-stationary environments more efficiently.\nThe TS algorithm tracks the rewards history $X$ for each arm $k$ using a Bernoulli distribution, denoted as $B(\\alpha_k, \\beta_k)$, which has the parameters $\\alpha$ and $\\beta$. Physically, $\\alpha$ can be interpreted as the cumulative success counts while $\\beta$ works as the cumulative failure counts. In that sense, the distribution $B(\\alpha_k, \\beta_k)$ yields the expected success value by pulling each arm $k$. The classical TS updating is governed by the expression:\n$\u0392(\u03b1\u03ba, \u03b2\u03ba) =\\begin{cases}\u0392(\u03b1\u03ba, \u03b2\u03ba),& \\text{if } It \u2260 k\\\\B(\u03b1\u03ba + X, \u03b2k + 1 \u2212 X), & \\text{if } It = k\\end{cases}$\nwhere $I_t$ is the selected arm at step $t$.\nIn the language of bandits, the regret $R(t)$ represents the cumulative learning error. It quantifies the difference between an always optimal choice, or oracle (Besbes et al., 2014), and the sub-optimal choices by some bandit policy:\n$R(t) = \\sum_{t=1}^{T} X^*_t - E[\\sum_{t=1}^{T} X_t]$\nwhere $X^*$ is the optimal reward at step $t$.\nThe regret $R(t)$ measure can be used to compare and contrast different bandit policies. In non-stationary environments such as the financial stock markets, where the stock returns distributions are changing, classical bandit algorithms tend to show higher regret values, as they usually get stuck into some local optimum arm. To minimize this problem, the non-stationary bandit variants appeared.\nThe ADTS algorithm (de Freitas Fonseca et al., 2024), adapted from (Cavenaghi et al., 2021), relaxes the application of the discount factor by applying it only for the selected arm $I_t$, instead of applying it for all of the arms. On the other hand, we keep intact the construction of what we interpret as the short-term memory of the policy, by applying the sliding window approach, and then comparing both discounted and short-term samples with the aggregation function $f(.)$ for each arm.\nMore formally, the discount factor $\\gamma \\in (0,1)$ gradually diminishes the impact of past observations in the historic trace. The short-term trace, represented as $B(\\alpha_k^w, \\beta_k^w)$, tracks the recent rewards by applying a sliding window $w$. The mix between the historical and hot traces components is performed before the arm is played at step $t$. For each arm $k$, the algorithm computes an aggregated score $S_k(t)$, as:\n$Sk(t) = f(\u03b8k(t), \u03b8k^w(t))$\nwhere $f(.)$ is the aggregation function defined for the algorithm (min, avg, max), $\u03b8_k(t)$ is a sample from the historic trace distribution $B(\\alpha_k, \\beta_k)$, $\u03b8_k^w(t)$ is a sample from the short-term trace distribution $B(\\alpha_k^w, \\beta_k^w)$ at step t for arm k. Finally, ADTS chooses"}, {"title": "3.2 Combinatorial Adaptive Discounted Thompson Sampling", "content": "In this section, we extend the ADTS algorithm to a combinatorial bandit problem, originating the Combinatorial Adaptive Discounted Thompson Sampling Thompson Sampling (CADTS).\nFor conducting the Portfolio Optimization problem in the context of Bandits, we combined ADTS with the Combinatorial Bandits formulation proposed by Chen et al. (2013). Theoretically, each stock can have infinite possible weight values $w_k$, which can lead our CADTS to dimensionality issues. To avoid that, we construct our feasible portfolio weights combinations (superarms) by building an array of discrete weights $p_{wk}$ for each stock $k$, given the total number of stocks $K$.\n$pw[k, :] = [0 s 2s 3s 1]$\nwhere $s = \\frac{2}{K}$ is the minimum weight step value.\nThen, we construct the possible weights matrix $PW$, where the rows represent each stock and the columns are the possible weights for each stock defined in equation 4. To create the super-arms, we run all the possible weight combinations between the stocks and possible weights in the $PW$ matrix s.t. $\\sum_{k=1}^{K} W_k(t) = 1:$\n$PW =  \\begin{bmatrix}Stock_1\\\\Stock_2\\\\: \\\\Stock_k\\end{bmatrix} \\begin{bmatrix}0 & s & 2s & \\dots & 1\\\\0 & s & 2s & \\dots & 1\\\\:&:&:& \\vdots& :\\\\0 & s & 2s & \\dots & 1\\end{bmatrix}$"}, {"title": "3.3 Bandit Networks", "content": "We demonstrated in (de Freitas Fonseca et al., 2024) that the ADTS algorithm efficiently selects the best arm in a changing environment. Heavily inspired by the Neural Networks philosophy, we introduce a novel approach called Bandit Networks. It connects between layers of non-stationary bandits policies such as ADTS and CADTS. In this section we propose two different architectures to solve the Portfolio Optimization problem: i) Non-Stationary Bandit with CADTS Network and ii) Two-layer ADTS Network."}, {"title": "3.3.1 Non-Stationary Bandit with CADTS Network", "content": "Figure 2 displays the Non-Stationary Bandit with CADTS Network architecture. In the first layer, the non-stationary Bandit policy (ADTS, D TS, SW UCB, or any other) receives $S_1, S_2, ...S_K$, the complete universe of stocks. More than selecting the best stock at time step $t$, the role of the non-stationary Bandit policy is to provide the second layer the rank of the $k < K$ best stocks, based on the reward function $f_{n_1}$, colored in yellow in the diagram. The function $f_{n_1}$ can be constructed to select stocks based on historical or sliding-window cumulative returns, momentum, or risk-adjusted returns, such as the Sharpe Index.\nHaving the $k$ best stocks, the CADTS generates the portfolio feasible weights combinations s.t. $\\sum_{1}^{K} p_{w_{k,i}} = 1$, as described in Algorithm 2. The policy is accountable for selecting at time step $t$ the best weight combination (super arm) that maximizes its reward function $f_{n_2}$, colored in green. Similarly, the second layer objective function can also be constructed to select stocks based on a financial metric, whether the same as $f_{n_1}$ or a different one."}, {"title": "3.3.2 Two-layer ADTS Network", "content": "We present an alternate to the Non-Stationary Bandit with CADTS Network architecture. Figure 3 displays the Two-layer ADTS Network. In the first layer, we partition the total stocks universe into $k$ parts. For each partition, we run an ADTS to filter the stocks universe using the reward function $f_{n1}$, colored in yellow in the diagram.\nGiven the $k$ best stocks to the second layer, we bolt another ADTS to learn the $k$ best stocks hierarchy given another reward function $f_{n2}$, colored in green. The portfolio weights are generated by normalizing the expected success values of each $k$ Bernoulli distribution:\n$W_i = \\frac{E[B(\\alpha_i, \\beta_i)]}{\\sum_{k=1}^{k} E[B(\\alpha_i, \\beta_i)]}$"}, {"title": "4 Experimental Setup", "content": "The experiments designed in this paper aim to demonstrate the practical usefulness of the proposed ADTS and CADTS algorithms and their connection to form the Bandit Networks in a real environment provided by a set of daily returns of S&P stocks.\nTo achieve this goal, we divide the experiments into three phases. In the first phase, we investigate if the ADTS effectively selects the best S&P stock in the so-called Stock Picking problem. Next, we evaluate the performance of different Bandit Networks instances in the Portfolio Optimization Problem given a set of S&P stocks. Finally, we check the Bandit Networks instance's results robustness by removing a set of high-performing stocks."}, {"title": "4.1 Market Data and Problem Definition", "content": "We submit the non-stationary bandit's policies to a real-world problem by selecting a set of 44 stocks within the S&P index, taking their historical prices from April 2020 to July 2024. These stocks behave as our arms in a bandit problem context. Given the historical series of daily returns $[r_{0}, r_{1}, r_{2}, ..., r_{t}]$, where $t$ represent each time step, the objective is to maximize the future rewards $[X_{t+1}, X_{t+2}, X_{t+3}, ..., X_{T}]$ either for a unique stock or a portfolio of stocks. The bandit reward function is defined by $F([r_{wf-t}, ..., r_{wf-2},r_{wf-1}, r_{wf}])$, where $F$ is a financial metric such as cumulative Returns, Sharpe Index, Sortino Ratio, etc and $w_f$ represents the window length in case of applying sliding window to the financial function. If the historical financial function is desired at each time step $t$, the sliding window becomes infinite and no hyperparameter $w_f$ is necessary."}, {"title": "4.2 Stock Picking Experiment", "content": "In this experiment, we aim to apply the proposed ADTS algorithm to the stock picking problem, given the set of the S&P stocks from 4.1. To provide benchmark comparisons to our non-stationary bandit variant, we invoke the bandit algorithms listed below as Table 2 summarizes the complete experiment setup.\n\u2022 Classical Thompson Sampling: Classical TS (Thompson, 1933);\n\u2022 f-Discounted-Sliding-Window Thompson Sampling: F-DSW TS (Cavenaghi et al., 2021);\n\u2022 Discounted Thompson Sampling: D-TS (Raj and Kalyani, 2017);\n\u2022 UCB-1 (Auer, 2002).\n\u2022 Discounted UCB: D-UCB (Garivier and Moulines, 2011);\n\u2022 Sliding-Window UCB: SW-UCB (Garivier and Moulines, 2011);"}, {"title": "4.3 Portfolio Optimization Experiment", "content": "For conducting the Portfolio Optimization for the S&P stocks presented in Section 4.1, we apply different Bandit Networks instances.\nIn the experiments, we analyze the learning behavior of the network instances in terms of cumulative regret and use financial metrics to compare with the following benchmarks:\n\u2022 Capital Asset Pricing Model (CAPM) - Fama and French (2004);"}, {"title": "4.4 Portfolio Selection Robustness Experiment", "content": "To verify the robustness of the Bandit Networks instances presented in Section 4.3\nTable 3, we incrementally remove a rank of the nine best stocks in cumulative returns.\nFor the number of the top removed stocks, we define the variable M.\nWe investigate financial metrics such as Cumulative Returns, Sharpe Index, and Maximum Drawdown for each step and Bandit Network instance and compare the results with the S&P index and the CAPM portfolio model.\nThe results of these experiments yield insight into understanding the drift evolution of each studied Bandit Network instance and evaluate their dependencies to outlier performing stocks. In the next section, we present the results of three offered experiments in this study."}, {"title": "5 Results", "content": "The results of our experiments to assess the performance of the non-stationary bandits and the bandit network instances are presented in this section. As described in the previous section, we conducted three experiments: the stock picking experiment, the portfolio optimization experiment, and the portfolio robustness experiment."}, {"title": "5.1 Stock Picking Experiment", "content": "The first set of experiment results is towards the S&P Stock Picking problem. The results are divided into three parts. In the first part, we analyze the learning characteristics of the ADTS against the bandit algorithms. Secondly, we obtain the financial metrics of each algorithm. Finally, we simulate the drift effect after applying shock in the top-performing stock of our S&P set."}, {"title": "5.1.1 Regret Analysis", "content": "Figure 6 shows the cumulative regrets obtained according to the bandit algorithms present in Section 4.2 and Table 5 summarizes the results to help the reader to understand the differences.\nThe proposed ADTS is the one with the most prominent capability of detecting abrupt changes while presenting the lowest cumulative regret (3.2\u00b10.7) (as per the red line with credible intervals). The top three rankings are completed with D UCB (3.6\u00b10.5) and Classical TS (3.9\u00b10.7). It draws attention that F-DSW TS (Cavenaghi et al., 2021), the variant on which ADTS is based, presents the worst cumulative regret (5.6\u00b10.6) when applied to the S&P Stock Picking problem."}, {"title": "5.1.2 Financial Metrics Analysis", "content": "For a financial performance evaluation of the non-stationary Bandits policies, we investigate the following metrics: Return, Sharpe Ratio, Drawdown, Win Rate, and Sortino Ratio. Return quantifies profitability, while the Sharpe Ratio assesses risk-adjusted returns. Drawdown measures maximum loss, Win Rate indicates success frequency, and Sortino Ratio evaluates downside risk. These metrics collectively provide a comprehensive overview of a strategy's performance and risk profile.\nFigure 7 illustrates the cumulative returns obtained for each bandit algorithm in the experiment and the S&P Index. Not only does the ADTS present the highest stock picking capability, but it transforms it into considerably better returns compared to the other Bandit policies or the S&P 500 Index itself. When analyzing the Sharpe Ratio, UCB-1 is leading the metrics, followed by SW UCB and ADTS, in third. These three instances also stay at the top for the Sortino Ratio.\nCompared to the S&P 500 Index, in terms of returns, all the bandits policies present superior performance than the S&P 500 Index. It is worth mentioning that all the policies can select one stock at a time, so maybe the higher drawdowns compared to the index, which is an aggregation of various stocks, are justified by this asymmetry."}, {"title": "5.1.3 Drift Analysis", "content": "For analyzing the concept drift in the selected set of S&P stocks, we imposed an artificial shock to the best-performing ticker, the NVDA stock in January 2024. After the applied drift, the ADTS is the policy that presents the highest cumulative median returns (5.27), followed by UCB-1 (4.29), which represents an increase of 22.8%. In terms of the Sharpe Ratio UCB-1 is leading (1.20), closely followed by ADTS (1.17). Being the worst performer policy in cumulative returns and Sharpe Ratio, the F-DSW TS is leading the rank for presenting the best risk behavior, given its Drawdown of 1.19. On the other hand, our proposed policy, ADTS, had the second highest median Drawdown (3.70)."}, {"title": "5.2 Portfolio Optimization Experiment", "content": "In the second experiment, we evaluate the performance of different Bandit Networks instances in the Portfolio Optimization Problem given a set of S&P stocks. The results are split into two parts. In the first part, we analyze the learning characteristics of the"}, {"title": "5.2.1 Regret Analysis", "content": "Figure 10 shows the cumulative regrets in the log-scale for the y-axis obtained for the studied network instances. Table 7 summarizes the results to help the reader to understand the differences.\nComparing the regret results, the network instance 3 (Two-Layer ADTS, with n = 4) stands out as the best learning configuration showing the lowest cumulative regret (57.5 \u00b1 19.5). The top three are completed by the other two instances derived from Section 3.3.2, instances 4 and 5. There is a clear separation between the network instances proposed by Section 3.3.1. Instance 1 presents the worst mean cumulative regret value (767.2\u00b1113.6), technically tied with instance 2 (685.3\u00b1122.5)."}, {"title": "5.2.2 Financial Metrics Analysis", "content": "We move to analyze the financial metrics obtained for the bandit networks, comparing them with classical portfolio models. Results are stored in Table 8. Figure 11 illustrates the payoff chart of each of the bandit network instances results and the classical portfolio models and how they compare to the S&P index.\nThe Two-Stage ADTS (n = 4) instance is the one with the most prominent results of cumulative returns (4.92), Sharpe and Sortino Ratios (1.59 and 0.14, respectively). Sharpe Ratio value for Two-Stage ADTS (n = 15) slightly loses to the best instance. By selecting fifteen stocks simultaneously, the instance diversifies risks, as suggested by the smallest drawdown metric of the bandit networks instances (0.55).\nContrary to the cumulative regrets suggestions, SW UCB | CADTS (n = 4) and ADTS | CADTS (n = 4) stands in second and third when taking the cumulative returns, although compromising their Sharpe Ratio having higher risks than the other three remaining instances.\nCompared to the classical portfolio models, nominally CAPM, Equal Weights, Risk parity, Markovitz as well as the S&P 500 Index, in terms of returns, all the bandits network instances present superior performance. In this aspect, the cumulative returns of Two-Stage ADTS (n = 4) is 168% higher than the CAPM, where the last is the best-performing classical model. The worst instance, Two-Stage ADTS (n = 15), presents cumulative returns 42% higher than the best classical model, the CAPM, 2.55 against 1.79, respectively.\nThe pattern persists when it comes to the Sharpe Ratio. The best network instance in this criteria, Two-Stage ADTS (n = 4), presents a Sharpe Ratio 20% higher than the best classical model, the Equal Weights, 1.59 against 1.32, respectively. The other instances also present superior values when compared to Equal Weights, except for ADTS | CADTS (n = 4) (1.25), which marginally loses to equal weights and risk parity models."}, {"title": "5.3 Portfolio Selection Robustness Experiment", "content": "To finish the set of experiments, we present the results depicted in the experiment setup presented in Section 4.4. To quantify how much each of the bandit network instances sustains their performance after removing the best stocks, we instantiate three types of financial metrics: i) Cumulative Returns (return), ii) Sharpe Ratio (return adjusted to risk), and iii) Drawdown (risk). Figures 12-14 show line plots of the three mentioned financial metrics as a function of the number of the best stocks (M) shown in the experiment setup section. Table 9 consolidates the three analyzed metrics for each methodology, storing the values when M = 0,4,9. We compare the bandit network instances with the CAPM model and the S&P Index.\nAs shown in the table, the three instances with n = 4 present higher drifts, which is logical due to the less diversification when compared to the instances with n = 10 and n = 15.\nFor Cumulative Returns, the network that uses two layers of ADTS (red line) maintains the highest values until the number of the dropped best reaches eight. On the other hand, the networks that use CADTS in the last layer (Section 3.3.1) do not manage the same capability, as they start to lose to the n = 10 and n = 15 instances, and even the CAPM and S&P Index after removing six best stocks. The Two Layer ADTS (n = 15), green line in the charts, demonstrates the highest bandit network capability of maintaining the cumulative returns after M = 9, preserving a value of 1.41, 19% higher than CAPM and 10% higher than the S&P Index. Overall, the analyzed bandit networks demonstrate higher cumulative return drift when compared to CAPM and S&P Index.\nFor the Sharpe Ratio, similar trends can be observed. The two-layer ADTS (n = 15) showcases the lowest drift values amongst all the bandit network instances, demonstrating comparable stability to the CAPM model. In fact, after M = 9 it"}, {"title": "6 Discussion", "content": "This section discusses the outcomes of our experiments on stock picking, portfolio optimization, and portfolio robustness. These experiments evaluate the performance of the newly introduced ADTS algorithm, as well as the novel concept of bandit networks"}, {"title": "7 Conclusion", "content": "This work introduced and evaluated the ADTS algorithm and the concept of bandit networks through a series of experiments on stock picking, portfolio optimization, and portfolio robustness, using historical daily returns of 44 S&P 500 stocks from April 2020 to July 2024. The ADTS algorithm demonstrated superior performance, consistently achieving the lowest cumulative regret and highest returns, showcasing its effectiveness in both static and dynamic market conditions. The two-layer ADTS networks, particularly with n = 4 and n = 15, exhibited remarkable robustness and risk-adjusted returns, outperforming classical models such as CAPM and Equal Weights.\nThe stock-picking experiments highlighted the ADTS's ability to maintain high returns and competitive Sharpe Ratios even under concept drift. In the portfolio optimization results, the two-layer ADTS networks efficiently learned and adapted, yielding superior cumulative returns and risk metrics. The robustness analysis further validated the stability of these networks, especially with higher n values, in maintaining performance amidst market fluctuations.\nFuture work could explore the application of ADTS and bandit networks to a broader range of financial instruments and market conditions. Additionally, enhancing the models to mitigate higher drawdowns observed in stock picking could further improve their practicality. Investigating the integration of alternative financial metrics and incorporating real-time adaptive mechanisms may also provide valuable insights for developing more resilient and adaptive financial decision-making tools."}, {"title": "Declarations", "content": "\u2022 Funding. This study was supported by the Funda\u00e7\u00e3o de Amparo a Pesquisa do Estado de S\u00e3o Paulo (FAPESP - Grant 2022/01524-2)."}]}