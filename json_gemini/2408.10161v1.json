{"title": "NeuFlow v2: High-Efficiency Optical Flow Estimation on Edge Devices", "authors": ["Zhiyong Zhang", "Aniket Gupta", "Huaizu Jiang", "Hanumant Singh"], "abstract": "Real-time high-accuracy optical flow estimation is crucial for various real-world applications. While recent learning-based optical flow methods have achieved high accuracy, they often come with significant computational costs. In this paper, we propose a highly efficient optical flow method that balances high accuracy with reduced computational demands. Building upon NeuFlow v1, we introduce new components including a much more light-weight backbone and a fast refinement module. Both these modules help in keeping the computational demands light while providing close to state of the art accuracy. Compares to other state of the art methods, our model achieves a 10x-70x speedup while maintaining comparable performance on both synthetic and real-world data. It is capable of running at over 20 FPS on 512x384 resolution images on a Jetson Orin Nano. The full training and evaluation code is available at https://github.com/neufieldrobotics/NeuFlow_v2.", "sections": [{"title": "I. INTRODUCTION", "content": "The development of high-accuracy optical flow estimation algorithms has seen substantial progress in recent years [1]. Starting from FlowNet [2], learning-based methods for optical flow have shifted towards feature learning for matching, moving away from traditional hand-crafted features like those in Lucas-Kanade [3] or SIFT [4] [5]. Despite these advances, early optical flow methods struggled with significant challenges such as large displacement and generalizing to real-world data [6]. More recent deep learning approaches have mitigated these issues, albeit with an increased computational cost [7], [8].\nDue to the difficulty in collecting ground truth optical flow data in the real-world, simulation is predominantly used to generate sufficient training data [2], [9]. However, training with simulation data can lead to overfitting due to unrealistic illumination, reflections, and monotonous scenes [10], [11]. Early optical flow methods that rely on CNNS struggle with handling large displacements [12], and their one-shot architectures do not generalize well to real-world data [2], [12], [13], [14].\nStarting with RAFT [6], iterative refinements have partially mitigated the generalization issue while also capturing larger motions [15], [16]. Recent research has further improved accuracy and generalization by incorporating the latest modules, such as transformer [17], Partial Kernel Convolution [18], Super Kernel [19] etc. However, these methods are generally more computation heavy due to the iterative refinement process. Some models need over 30 iterations to generate a stable optical flow [6], while others reduce the number of iterations but increase the computational load of each iteration [18], [20].\nAnalyzing NeuFlow-v1, we noticed similar generalization issues on real-world data. But using refinement modules from previous approaches [7], [15] leads to substantial increase in computation time which is undesirable. To mitigate this issue while maintaining real-time inference speed, we present two new components to the architecture. First we analyze the backbone in detail and remove the redundant components, thus making it more light-weight and efficient. The backbone is preceded by cross-attention [21] and global matching [15] module to handle the large displacement problem to estimate an initial optical flow, unrestricted by the receptive field. Second, we present a efficient iterative refinement module. Both these modules greatly improve performance on real-world data while keeping the computational budget low. As a final result, our approach achieves real-time performance, running at over 20 FPS on 512x384 resolution images on a Jetson Orin Nano.\nThe main contributions of this paper are as follows:\n1. Simple Backbone: A simple CNN-based backbone that extracts low-level features from multi-scale images. Instead of commonly used architectures like ResNet [22] or Feature Pyramid Network [23], this lightweight backbone is found to be sufficient for getting accurate Optical Flow.\n2. Light-weight and Efficient Iterative Refinement Module: A simple recurrent network module capable of outputting the hidden state and decoded refined optical flow. Instead of using time-consuming modules like LSTM [24] or GRU [25], we propose a simpler RNN module that is lightweight and achieves higher accuracy for local refinement."}, {"title": "II. RELATED WORK", "content": "FlowNet [2] was the first deep learning-based optical flow estimation method, introducing two variants: FlowNetS and FlowNetC, along with the synthetic FlyingChairs dataset for end-to-end training and benchmarking. An improved version, FlowNet 2.0 [13], fused cascaded FlowNets with a small displacement module, decreasing the estimation error by more than 50% while being marginally slower.\nFollowing FlowNet 2.0 [13], researchers developed more lightweight optical flow methods. SPyNet [14] is 96% smaller than FlowNet in terms of model parameters. PWC-Net [12] is 17 times smaller than FlowNet 2. LiteFlowNet [26] is 30 times smaller in model size and 1.36 times faster in running speed compared to FlowNet 2. LiteFlowNet 2 [27] improved optical flow accuracy on each dataset by around 20% while being 2.2 times faster. LiteFlowNet 3 [28] further enhanced flow accuracy. RapidFlow [29] combines efficient NeXt1D convolution blocks with a fully recurrent structure to decrease computational costs. DCVNet [30] proposes constructing cost volumes with different dilation factors to capture small and large displacements simultaneously. NeuFlow v1 [31], our previous work, is the fastest optical flow method, being over ten times faster than mainstream optical flow methods while maintaining comparable accuracy on the Sintel and FlyingThings datasets.\nMore recently, RAFT [6] used recurrent all-pairs field transforms to achieve strong cross-dataset generalization. Following RAFT, GMA [16] introduced a global motion aggregation module to improve estimation in occluded regions. GMFlow [15] reformulated optical flow as a global matching problem. GMFlowNet [17] efficiently performed global matching by applying argmax on 4D cost volumes. CRAFT [7] used a Semantic Smoothing Transformer layer to make features more global and semantically stable. FlowFormer [8], [32] encodes the 4D cost tokens into a cost memory with alternate-group transformer layers in a latent space. SKFlow [19] benefits from super kernels to complement the absent matching information and recover the occluded motions.\nDIP [33] introduced the first end-to-end PatchMatch-based method, achieving high-precision results with lower memory. RPKNet [18] utilized Partial Kernel Convolution layers to produce variable multi-scale features and efficient Separable Large Kernels to capture large context information. Sea-Raft [20] proposed a new loss (mixture of Laplace) and directly regressed an initial flow for faster convergence. Many works have also been proposed to either reduce computational costs or improve flow accuracy [34]."}, {"title": "III. PROPOSED APPROACH: NEUFLOW V2", "content": "In NeuFlow v1, we proposed a similar shallow backbone for extracting low-level features from multi-scale images. In NeuFlow v2, we have eliminated redundant parts and retained only the effective components. The intuition behind this design is that sufficient low-level features are more crucial than high-level features in optical flow tasks.\nWe extract features from 1/2, 1/4, and 1/8 scale images using a CNN block composed of convolution, normalization, and ReLU layers. This same CNN block is used to concatenate and resize these features into the desired output scale, specifically to 1/16 scale features and context, as well as 1/8 scale features and context. Features are used for correlation computation, while context is used for flow refinement.\nNote that the 1/1 scale image is used solely for convex upsampling and is not involved in estimating the 1/8 resolution flow. The ablation study in Section 4 demonstrates that features extracted from the full 1/1 scale image can cause overfitting on the training set (FlyingThings) and do not improve accuracy on unseen data (Sintel, KITTI)."}, {"title": "B. Cross-Attention And Global Matching", "content": "Cross-attention is used to exchange information between images globally, enhancing the distinctiveness of matching features and reducing the similarity of unmatched features. Global matching is then applied to find corresponding features globally, enabling the model to handle large pixel displacements, such as in fast-moving camera situations. To reduce the computational burden of cross-attention and global matching, we operate on 1/16 scale features instead of 1/8 scale.\nSimilar to NeuFlow v1 and GMFlow [15], we utilize Transformers to implement cross-attention and global matching. While flow self-attention is typically used to refine pixels based on self-similarity after global matching, we instead use our simple RNN module to iteratively refine the estimated flow."}, {"title": "C. Simple RNN Refinement", "content": "We first compute the correlation within nearby 9x9 neighborhoods and warp this correlation using the estimated flow. We then concatenate the warped correlation, context features, estimated flow, and previous hidden state, processing them through eight layers of simple 3x3 convolutional layers followed by ReLU activation to output the refined optical flow and updated hidden state. The simple RNN refinement is performed at both the 1/16 scale flow and the 1/8 scale flow.\nTo address the vanishing or exploding gradient problem, most RNNs use GRU or LSTM modules to compute the current hidden state based on the previous hidden state and current inputs (warped correlation, context, and flow) and to decode the estimated flow using the current hidden state. However, GRU and LSTM modules often have too few layers to effectively merge the current input with the previous hidden state. In contrast, we use deep CNN layers to effectively merge the inputs (warped correlation, context, and flow) with the hidden state. This approach has been tested to avoid unstable gradient issues and significantly improve accuracy, as detailed in the ablation study in Section 4.\nHardTanh [35] is applied before the hidden state to constrain the feature values within a specific range (in our case, -4 to 4) and to address numerical stability issues. Using traditional Tanh can lead to extremely large or small values when the hidden state values approach -1 or 1, potentially causing overflow. HardTanh helps mitigate this problem by maintaining the values within a manageable range."}, {"title": "D. Multi-Scale Feature/Context Merge", "content": "In NeuFlow, our simple backbone lacks the depth of convolution, resulting in features having a small receptive field. Cross-attention at the 1/16 scale provides global attention, offering a global feature/context. However, 1/8 scale features/context do not have a global receptive field. Therefore, we merge the 1/16 global features/context with the 1/8 local features/context to ensure that the 1/8 scale features/context contain both global and local information.\nThe merge block consists of two layers of CNNs with ReLU activation and normalization. In practice, features and context are merged individually using the same merge block structure."}, {"title": "IV. EXPERIMENTS", "content": "We first train the model solely on the FlyingThings [9] dataset for a fair comparison with other models, as most models have undergone the same procedure. Additionally, we trained the model using a mixed dataset comprising Sintel [9], KITTI [36], and HD1K for real-world applications. For evaluation, we followed the common practice of using the Sintel and KITTI datasets to demonstrate the model's generalization capabilities. We followed the same procedure and data augmentation settings as RAFT, utilizing RAFT'S [6] training and evaluation code.\nFor real-world use cases, we also train the model with a mixed dataset that includes FlyingThings, Sintel, KITTI, HD1K, and VIPER [37]. The pretrained model is available in our repository."}, {"title": "B. Comparision with Latest Optical Flow Methods", "content": "We compare our method to several state-of-the-art optical flow methods renowned for their superior accuracy. We also measure the computation time on both the RTX 2080 and the edge computing platform Jetson Orin Nano, considering image sizes of 1024x436 for Sintel and 1242\u00d7375 for KITTI. The inference batch size is measured on an 8GB GPU to assess the memory usage of different models.\nAmong these methods, Sea-Raft-Large [20] achieves the highest accuracy on KITTI. However, both Sea-Raft-Large and Sea-Raft-Medium have similar accuracy issues on the Sintel-Final dataset, whereas NeuFlow v2 runs about seven times faster. RPKNet [18] is a state-of-the-art model in terms of accuracy across all datasets, and NeuFlow v2 offers comparable accuracy while being 10 times faster. For other heavy models like DIP and FlowFormer [8], NeuFlow v2 maintains similar accuracy while being 30-100 times faster. Some models, such as GMFlow [15] and RapidFlow [29], prioritize faster inference speed but at the cost of reduced accuracy."}, {"title": "C. Ablation Study", "content": "Backbone Module: We found that using full scale features in the backbone actually does not help in estimating the 1/8th scale optical flow and infact leads to a slight drop in performance on both synthetic and real-world dataset.\nRefine Module: We use 8 layers of CNN to output both refined optical flow and the hidden state in the refinement module. We experimented by reducing and adding 2 layers to observe the impact. The results show that reducing the number of layers slightly decreases accuracy, while adding layers does not improve accuracy. This indicates that eight layers provide a balanced configuration.\nOur default feature dimensions are 128 for the 1/16 refinement and 96 for the 1/8 refinement. Reducing the feature dimensions by half (64 for the 1/16 refinement and 48 for the 1/8 refinement) results in a significant drop in accuracy.\nIn the refinement module, we use multiple layers of CNN to output the hidden state. We also experimented with replacing the first CNN layer with a ConvGRU to output the hidden state and using the remaining seven CNN layers to decode a refined flow. The results show that this approach significantly reduces accuracy.\nArchitecture: Cross-attention is used to exchange information between two input images globally. Removing it does not significantly affect accuracy on the KITTI dataset, but it causes a substantial drop in accuracy on the Sintel dataset.\nGlobal matching provides the initial optical flow, which can handle large motions. Removing it and adding three more iterations of 1/16 refinement helps address large motion issues. The drop in accuracy indicates that global matching is working efficiently without requiring multiple refinements. We only perform one iteration of 1/16 refinement. If we remove this and rely entirely on 1/8 refinement, overfitting occurs, as the training set accuracy remains unchanged while the accuracy drops significantly on all validation sets.\nDifferent Iterations: The default iteration count for 1/16 refinement is set to one, as additional iterations do not significantly improve accuracy. In contrast, 1/8 refinement benefits from more iterations. The default eight iterations already provide decent accuracy, but adding more iterations can further improve accuracy at the cost of increased inference time."}, {"title": "V. CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we proposed an efficient optical flow method, which accruacy is close to state-of-the-art while getting 10 times faster, enable real-time inference on edge computing device.\nHowever, we also recognized that memory comsupmtion is heavy, which is caused by correlation computation. Various modules has addressed the problem [29], [18] which can be used in our architecture.\nOur method also contains too many parameters (9 million), primarily due to the simple backbone and simple RNN refinement module, which rely heavily on CNNs. This can potentially lead to overfitting with training data. Many efficient modules can be replaced to reduce the number of parameters. For example, MobileNets [38], [39], [40] use depthwise separable convolutions, while ShuffleNet [41] utilizes pointwise group convolution."}]}