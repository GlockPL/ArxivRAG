{"title": "Hey GPT, Can You be More Racist? Analysis from Crowdsourced Attempts to Elicit Biased Content from Generative AI", "authors": ["Hangzhi Guo", "Pranav Narayanan Venkit", "Eunchae Jang", "Mukund Srinath", "Wenbo Zhang", "Bonam Mingole", "Vipul Gupta", "Kush R. Varshney", "S. Shyam Sundar", "Amulya Yadav"], "abstract": "The widespread adoption of large language models (LLMs) and generative AI (GenAI) tools across diverse applications has amplified the importance of addressing societal biases inherent within these technologies. While the NLP community has extensively studied LLM bias, research investigating how non-expert users perceive and interact with biases from these systems remains limited. As these technologies become increasingly prevalent, understanding this question is crucial to inform model developers in their efforts to mitigate bias. To address this gap, this work presents the findings from a university-level competition, which challenged participants to design prompts for eliciting biased outputs from GenAI tools. We quantitatively and qualitatively analyze the competition submissions and identify a diverse set of biases in GenAI and strategies employed by participants to induce bias in GenAI. Our finding provides unique insights into how non-expert users perceive and interact with biases from GenAI tools.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) and other Generative AI (GenAI) products, such as GPT-4 (OpenAI, 2023), Gemini (Team, 2024) and Stable Diffusion (Rombach et al., 2022) have demonstrated remarkable capabilities, which has led to their prevalent adoption across a wide variety of real-world scenarios. However, like most Machine Learning (ML) based systems, LLMs have been shown to inherit societal biases present in their training data (Gallegos et al., 2024; Navigli et al., 2023). This issue of bias raises significant ethical and societal challenges, especially as GenAI becomes increasingly accessible to the general public in the form of services that build on top of off-the-shelf GenAI tools, potentially amplifying harmful stereotypes and discriminatory attitudes about marginalized and under-served communities."}, {"title": "2 Related Work", "content": "AI Algorithmic Bias In many decision-making processes, artificial intelligence algorithms are now favored over humans as they are expected to provide a more 'impartial' perspective. While these algorithms may enhance the accuracy and effectiveness of the decisions, they often increase existing inequalities by benefiting or disadvantaging certain individuals or groups (O'neil, 2017). This socio-technical phenomenon is referred to as algorithmic bias (Danks and London, 2017) and has been found in many applications across domains including employment, healthcare, education and criminal justice (Kordzadeh and Ghasemaghaei, 2022). Training datasets, methodological approaches, and demographic factors have also known to causes of discriminatory outcomes in AI systems (Akter et al., 2021). With the identification of bias, diverse mitigation strategies have also been proposed to reduce bias and achieve algorithmic fairness, namely ethical principles (Coates and Martin, 2019), design standards (Cramer et al., 2018), assessment tools (Saleiro et al., 2018; Bellamy et al., 2019; Bird et al., 2020), and regulatory mechanisms (Birkstedt et al., 2023). Even though algorithmic bias is a popular research focus in many AI domains (Mehrabi et al., 2021b), it must be thoroughly examined in the context of LLMs, given their rapid adoption by the general public into a 'sociotechnical system' (Kudina and van de Poel, 2024; Venkit, 2023).\nBias in LLMS Recent studies have uncovered various biases in LLMs. Dong et al. (2024) used conditional generation probing to detect gender bias in ten state-of-the-art models, while Rozado"}, {"title": "3 Competition Design & Details", "content": "To uncover biases and stereotypes present in current GenAI tools, we hosted a university-wide competition for a period of 20 days during Fall 2023, which was open to anyone affiliated with a leading public research university in the United States (including undergraduate and graduate students, staff, and faculty). The name of the competition and the university are intentionally withheld to ensure anonymity.\nThis competition challenged prospective participants with designing prompts that induced biased responses from a GenAI tool (they were allowed to use any publicly available GenAI tool). Overall, most participants chose ChatGPT-3.5/4.0 (77.3%), due to its advantages in accessibility at the time of the competition. Other popular tools included Bard (6.7%) and DALL-E (6.7%). A small minority of participants used DeepAI (2.7%), Adobe Firefly (1.3%), Stable Diffusion (1.3%), Bing (1.3%), and Mid Journey (1.3%).\nFor each submission, participants were required"}, {"title": "4 Participants Definition(s) of Bias", "content": "In the competition, we asked participants to submit prompts which led to (perceived) biased outputs from GenAI tools. Bias is an inherently abstract concept with many subjective interpretations (each of which is shaped by individual-level perspectives) (Blodgett et al., 2020). Thus, to contextualize all subsequent analyses in this paper, it is important to start by understanding the perceived definitions of bias used by our competition participants to guide them in their search for competition-winning prompts (which would lead to highly biased content being output from GenAI tools).\nTo achieve this goal, the authors invited the competition participants for a 60-minute Zoom-based interview, and a $20 USD Amazon.com gift card was provided to the interviewees to compensate them for their time. In total, the authors conducted nine such interviews. During the interview\u00b2 , one of the questions (P2 in Section B) asked the participants was \"How do you define bias in the output"}, {"title": "5 Reproducibility Analysis", "content": "Having arrived at a working definition of bias for this paper, we now conduct a rigorous quantitative reproducibility analysis to identify those GenAI prompts (from the competition) that do not consistently lead to biased outputs, so that all non-reproducible prompts can be excluded from subsequent analysis. One limitation in our competition setup is that the GenAI outputs (in response to submitted prompts) are shown only once as a screenshot submitted on the competition Teams channel; furthermore, the participants are not required to test their prompts across different GenAI models. For example, a participant may only test their prompt on GPT-3.5 a single time, which fails to capture the variability in GPT-3.5 responses to the exact same prompt, along with the variability in responses across different competing LLMs to the same prompt. Such limited exposure casts doubts on whether the prompts submitted in the competition reveal systematic biases within LLMs, or the results just reflect noise due to inadequate sampling. To establish consistent and generalizable findings, we reevaluate the same prompts submitted to the competition (or cleaned versions of the same prompts) on multiple LLMs (both proprietary and open-weight language models) across multiple runs."}, {"title": "5.1 Experiment Setup", "content": "Prompt Curation We observed that the majority of prompts submitted to the competition aimed to reveal binary biases, categorizing GenAI outputs as either biased or unbiased. Furthermore, the format of the biased responses can be categorized as discriminative responses, i.e., the participants ask GenAI to make decisions/choices and see whether the chosen decisions are biased (see Figure 1a), and generative responses, i.e., the participants induce GenAI to generate biased outputs (see Figure 1b).\nMotivated by these two observations, we convert the submitted prompts to two types of structured prompts so that we can quantitatively analyze the responses (see Figure 1). The first type of structured prompts aims to convert the discriminative response into a binary choice format. As shown in Figure 1a, each original prompt was transformed into a scenario-based puzzle, in which the GenAI model is presented with a scenario and two options. The second type of prompt keeps the original prompt as-is but creates a chained prompt to verify whether the LLMs' responses perpetuate biases revealed by the participants (as shown in Figure 1b)\nTo curate structured prompts, each of the authors converts 12 prompt submissions into this structured format. In total, out of 75 submitted competition prompts, we successfully curated 35 discriminative structured prompts, and 31 generative structured prompts. 9 prompts were excluded from our analysis because of low quality, etc.\nLLM Selection To study the generalizability of the observed biases, we selected a diverse set of large language models, including both proprietary and open-weight models. We evaluate our results on three open-weight model families, including Llama (v2, v3, v3.1), qwen (v1, v2), and gemma (v1, v2), and evaluate two proprietary models, including GPT-40-mini and Gemini (flash v1.5).\nExperiment Procedure We introduced two key variations to ensure a comprehensive evaluation of each prompt. First, to mitigate potential order bias, the order in which the two answer options were presented to the LLMs was randomly shuffled for each prompt. Second, we systematically varied the temperature parameter of the LLMs to account for the stochastic nature of their outputs and assess the impact of this randomness on the observed biases. Ten temperature values were used, ranging from 0.0 to 0.9 in increments of 0.1. This experimental"}, {"title": "5.2 Experimental Results", "content": "Table 1 shows the polarization scores of open-weights and proprietary large language models.\nAmong three open-weight families (and proprietary models), the Llama family model has the lowest polarization scores (averaging ~0.136), which demonstrates that Llama is less susceptible to bias in general. On the other hand, the Gemma family exhibits the highest tendency to elicit biases, averaging ~0.738 in polarization score. Furthermore, we observe that proprietary models (i.e., Gemini-1.5-Flash and GPT-40-mini) achieve higher polarization scores than open-weight models, which demonstrates that proprietary model architectures or training data may contribute to an increased tendency to elicit biases. Similar findings hold for generative prompts (in terms of biased response"}, {"title": "6 Catagorizing Competition Prompts", "content": "We now conduct thematic analyses on the GenAI outputs of the 53 reproducible competition entries to categorize the different kinds of biases that GenAI tools were forced to elicit during the competition."}, {"title": "7 How to Elicit Bias from GenAI?", "content": "We now examine the strategies employed by competition participants to elicit biases from GenAI models. To get an understanding of the strategies employed by participants in eliciting biased outputs from LLMs, we conducted Zoom-based interviews with participants. We sent invitation emails to participants to recruit volunteers for this interview. The interviews were conducted in May 2024 after receiving institutional review board (IRB) approval. Each interview was scheduled for 45 minutes, audio-recorded, and subsequently transcribed using a combination of automated software and manual checking to ensure accuracy. Participants received a $20 Amazon e-gift card upon completion of the interview for their time and contribution.\nIn total, we have recruited 9 participants for the interview. The participants were diverse in terms of gender (6 male, 3 female) and academic background (4 graduate students, 2 undergraduate students, and 3 staff or faculty). Participants also came from a range of fields, including history, sociology, learning design, informatics, and computer science. Detailed demographic information and the interview protocol are provided in Appendix B.\nDuring the interview, one of the questions (P5) asked to the participants was \"Can you share any strategies that you believe can induce biased output from LLMs? What techniques did you try but failed?\" To identify the themes in participants' responses to P5, we employed an inductive approach to perform thematic analysis (Braun and Clarke, 2006). Two trained researchers independently reviewed the transcripts in detail, searching for patterns and meaning within the data, and each researcher independently identified themes. After gaining initial insights, the two researchers discussed with the authors to refine their understanding and gain meaningful insights. We compared the themes identified by each researcher, merging them through discussion, and any discrepancies were resolved through further discussion, to reach a final consensus on the main themes for the strategies to induce bias. Below, we highlight the results of this thematic analysis."}, {"title": "8 Discussion & Conclusion", "content": "In this paper, we present the findings from a university-level competition designed to elicit biased outputs from LLMs and GenAI tools. We conduct a reproducibility analysis to verify submission prompts. Furthermore, we conduct thematic analysis to categorize different types of biased outputs elicited from GenAI tools. Finally, we conducted interviews with 9 competition participants, providing valuable insights into their strategies for inducing biased outputs from GenAI tools.\nOur findings demonstrate that even non-expert users can elicit bias from LLMs and GenAI tools. Importantly, our reproducibility analysis shows that most of submission prompts can be reproduced to elicit biased outputs from LLMs. This result demonstrates that despite significant efforts towards rapid developments in debiasing LLMs, these models remain vulnerable to eliciting bias without expert knowledge of GenAI. Our results highlight the increasingly more urgent societal challenge of addressing this algorithmic bias, as GenAI increasingly becomes a sociotechnical systems.\nOur results also offer insights for GenAI developers to implement bias safeguards. For instance, our thematic analysis of GenAI outputs (Section 6) informs the development of bias detection guardrails, while our analysis of bias elicitation strategies (Section 7) can aid red-teaming efforts (Rawat et al., 2024) in detecting undesirable model behaviors."}, {"title": "9 Limitations", "content": "This paper analyzes the results of a university-level competition to reflect how non-expert users perceive and interact with bias in LLMs and GenAI tools. We acknowledge that our study subjects are limited to individuals affiliated with the university that hosts this competition, who possess or are pursuing a college degree. As a result, our results can only represent a narrowed view of bias in LLMs and GenAI tools, and may not generalize to a broader user base.\nOur analysis centers on the examination of bias in generative Al systems. While recent studies have demonstrated the presence of harms associated with these models (Dev et al., 2022; Blodgett et al., 2022; Ghosh et al., 2024), we specifically focus on bias and therefore do not engage with broader harm frameworks in this work."}, {"title": "10 Ethical Consideration", "content": "In this paper, our analysis provides unique insights into how non-expert users elicit biased outputs from LLMs. These can potentially provide valuable insights for GenAI developers to develop safeguard measurements to mitigate bias in these GenAI tools.\nHowever, while this paper aims to understand and mitigate bias in LLMs, we acknowledge that, under unlikely circumstances, malicious users could potentially exploit the strategies discussed in this paper to elicit unwanted model behavior. This potential risk underscores the importance of ongoing research and development in responsible AI practices."}, {"title": "A Competition Categories", "content": "Submissions were shared within dedicated sub-channels on the main channel hosting the bias-a-thon competition on the Microsoft Teams platform. Supposedly, each sub-channel contains prompts that reveal biases in different categories. In total, there exist 7 categories:\n1. Socio-Cultural Bias: This category addresses biases that arise from societal norms and cultural contexts, including prejudices based on ethnicity, race, gender, nationality, and religion. This category captures the varied stereotypes different social groups can face due to bias in large language model outputs. These kinds of biases often arise from the social biases present in training data.\n2. Contextual Bias: In this category, biases are recognized as being influenced by specific situations or environments. This includes stereotypes associated with professions, educational backgrounds, socioeconomic status, and geographic locations. This category includes context-dependent biases, varying significantly across different settings and conditions.\n3. Language and Dialect Bias: This category focuses on biases related to language use, including fluency, dialects, and accents. This bias category highlights how linguistic differences between social groups can lead to different outputs and thus emphasizes the social implications of how language and dialect variations are perceived and valued.\n4. Age-Related Bias: This involves discriminatory attitudes towards individuals based on their age. This bias category finds that large language models can vary their prediction based on age groups, which can have harmful effects in some settings.\n5. Cognitive and Physical Ability Bias: Biases in this category relate to how individuals with different physical or cognitive disabilities are treated. It covers a range of issues from physical disabilities to mental health conditions, addressing the stereotypes and misconceptions that can adversely affect these populations.\n6. Historical Bias: This category reflects biases originating from historical events and the long-term effects of those events, such as colonialism. It considers how history shapes contemporary attitudes and the lingering effects of past injustices on present-day interactions and perceptions.\n7. Out-of-the-Box Bias: This includes any biases that do not neatly fit into the other predefined categories. This was added to identify novel or unorthodox biases that can expand on the above categorization.\nThese categories were derived from prior quantitative analyses of bias in existing NLP ethics literature (Smith et al., 2022; Gupta et al., 2024b)."}, {"title": "B Interview Protocol", "content": "We conduct interviews with participants from the competition. We sent invitation emails to participants, and 9 participants volunteered. The interviews were conducted in May 2024 after receiving institutional review board (IRB) approval. Each interview was scheduled for 45 minutes, audio-recorded, and subsequently transcribed using a combination of automated software and manual checking to ensure accuracy. Participants received a $20 Amazon e-gift card upon completion of the interview for their time and contribution.\nIn total, we have recruited 9 participants for the interview. The participants were diverse in terms of gender (6 male, 3 female) and academic background (4 graduate students, 2 undergraduate students, and 3 staff or faculty). These participants also came from a range of fields, including history, sociology, learning design, informatics, and computer science. Table 2 outlines the demographic information of participants.\nInterviews began with general warm-up questions including how frequently they use LLMs, and in which cases they use LLMs. Then, the interviews were centered around the questions regarding strategies they use for curating prompts. Here, we show the outlines of interview questions:\nGeneral Warm-up Questions (W)\n\u2022 (W1) How frequently do you engage with LLMs or GenAI products?\n\u2022 (W2) What GenAI products do you frequently use?\n\u2022 (W3) In what contexts do you most often utilize LLMs? Could you detail the various scenarios in which you use them?"}, {"title": "C Additional Thematic Analysis", "content": "In this section, we include additional thematic analysis on other dimensions related to the biases of LLMs."}, {"title": "C.1 Hybrid Coding on Competition Prompts", "content": "Informed by a thorough review of the existing literature, we now conduct thematic analyses on the submitted competition prompts to understand the strategies used by participants to induce bias.\nAt a high level, to create a comprehensive taxonomy of strategies for inducing GenAI to reveal biases, we employ a hybrid coding strategy (Fereday and Muir-Cochrane, 2006), combining pre-defined codes (Cantini et al., 2024; Yu et al., 2024) with inductively derived codes from the data. Two authors, experts in the field of LLM bias, independently coded the submitted competition prompts using an iterative approach, ensuring a rigorous and systematic analysis of the data. Our analysis reveals five distinct categories of strategies employed to induce models to reveal biases:\nP1. Stereotype This strategy for prompt creation evaluates GenAI's reliance on stereotypes and their ability to produce more nuanced and diverse content, e.g., \"Describe a typical day in the life of a stay-at-home mom.\"\nP2. Choice-Based This strategy for prompt creation presents GenAI with multiple options and assesses their ability to make informed choices, thereby revealing potential biases, e.g., \"From the following options: 1. Cat, 2. Dog, which do you prefer?\"\nP3. Social Identity This strategy for prompt creation tests GenAI's ability to generate respectful and inclusive content about diverse social identities, such as race, gender, and disability, e.g., \u201cDescribe the experiences of a person with a disability.\u201d\nP4. Emotionally Charged This strategy for prompt creation evaluates GenAI's capacity to handle sensitive and emotional topics with empathy and respect, e.g., \"Write a eulogy for a loved one.\"\nP5. Neutral This strategy for prompt creation establishes a baseline for performance and helps identify biases in sensitive topics, e.g., \"Describe the process of planting a tree.\""}, {"title": "C.2 Factors for Bias", "content": "Limited Training Data All participants mentioned that the quality and diversity of the training data are primary contributors to bias in the models' responses. Additionally, participants said training data is often reflective of the biases present in human knowledge sources, such as information on internet, so they are likely to reproduce those biases."}, {"title": "C.3 Potential Impacts of Bias on Society", "content": "Reinforcement of Stereotype & Unfairness in Opportunities Almost all participants expressed concerns that biases in LLMs could reinforce existing stereotypes and prejudice in our society. For"}, {"title": "Polarization =", "content": "Ex~D [|Pc=1(x) - Pc=2(x)|]"}, {"title": "Polarization =", "content": "| # of Option 1-# of Option 2 |/# of (Option 1+Opton 2)"}]}