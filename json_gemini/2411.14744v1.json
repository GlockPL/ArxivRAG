{"title": "Point Cloud Understanding via Attention-Driven Contrastive Learning", "authors": ["Yi Wang", "Jiaze Wang", "Ziyu Guo", "Guangyong Chen", "Renrui Zhang", "Donghao Zhou", "Anfeng Liu", "Pheng-Ann Heng"], "abstract": "Recently Transformer-based models have advanced point\ncloud understanding by leveraging self-attention mecha-\nnisms, however, these methods often overlook latent infor-\nmation in less prominent regions, leading to increased sen-\nsitivity to perturbations and limited global comprehension.\nTo solve this issue, we introduce PointACL, an attention-\ndriven contrastive learning framework designed to address\nthese limitations. Our method employs an attention-driven\ndynamic masking strategy that guides the model to fo-\ncus on under-attended regions, enhancing the understand-\ning of global structures within the point cloud. Then we\ncombine the original pre-training loss with a contrastive\nlearning loss, improving feature discrimination and gen-\neralization. Extensive experiments validate the effective-\nness of PointACL, as it achieves state-of-the-art perfor-\nmance across a variety of 3D understanding tasks, includ-\ning object classification, part segmentation, and few-shot\nlearning. Specifically, when integrated with different Trans-\nformer backbones like Point-MAE and PointGPT, PointACL\ndemonstrates improved performance on datasets such as\nScanObjectNN, ModelNet40, and ShapeNetPart. This high-\nlights its superior capability in capturing both global and\nlocal features, as well as its enhanced robustness against\nperturbations and incomplete data.", "sections": [{"title": "1. Introduction", "content": "Point clouds are widely applicable in fields such as\nrobotics [6, 44], autonomous driving [6, 7], augmented re-\nality [3], and virtual reality [13] as a representation in three-\ndimensional space. These diverse applications highlight the\nsignificance of obtaining detailed and insightful 3D repre-\nsentations. Despite their potential, the irregular and sparse\nnature of point cloud data poses significant challenges to\nprecise and efficient 3D processing and understanding.\nRecent advancements in deep neural networks, espe-\ncially Transformer-based models [5, 32, 57] employing\nself-supervised learning, have shown promise in point\ncloud understanding. These models leverage the attention\nmechanism to capture complex relationships between point\npatches, prioritizing critical regions for understanding the\npoint cloud while downplaying less significant areas. Orig-\ninally designed for natural language, attention mechanism\nhas been successfully adapted for 2D vision. However, un-\nlike natural language [9] or images [17], which often con-\ntain redundant information such as contextual structures\nand backgrounds, point cloud data are inherently sparse,\nmeaning that each point or region is critical to the overall\nrepresentation. This scarcity of redundant information im-\nplies that Transformer-based models, when neglecting less\nprominent point patches, may inadvertently overlook essen-\ntial latent information. This observation leads us to a pivotal\nquestion: Can we design a framework that leverages latent\ninformation from the global regions of point clouds?\nTo answer this question, we re-examine the attention\nweights in Transformer-based point cloud models. As illus-\ntrated in Figure 1, we find that models like Point-MAE [57]\nand PointGPT [5] primarily rely on a limited set of high-\nattention patches for analysis. This reliance presents two\nsignificant issues: (1) Increased sensitivity to perturbations.\nOver-focusing on high-attention patches makes the models\nmore susceptible to noise and incomplete data, as distur-\nbances in these areas disproportionately affect performance.\n(2) Limited global understanding. Ignoring potential infor-\nmation in low-attention patches constrains the model's abil-\nity to develop a comprehensive understanding of the point\ncloud's global structure.\nTo solve these issues, we introduce PointACL, an\nAttention-driven Contrastive Learning framework for point\nclouds that can be seamlessly integrated into existing\nTransformer-based models. Our approach comprises two\nkey components: First, an attention-driven dynamic mask-\ning strategy is proposed that aims to mitigate the model's\nreliance on a limited subset of key patches by guiding it"}, {"title": "2. Related Works", "content": "Self-Supervised Learning for NLP and Image. Self-\nsupervised learning (SSL) has emerged as a powerful\nparadigm in natural language processing (NLP) [11, 66] and\ncomputer vision [1, 14, 24, 30, 38, 39, 56], enabling models\nto learn rich representations from unlabeled data. In NLP,\nBERT [9] exemplifies this by randomly masking input to-\nkens and training the model to predict them, fostering deep\ncontextual understanding. ELMo [42] utilizes bidirectional\nLSTMs to generate contextualized word embeddings, while\nGPT [39] adopts an autoregressive approach with a unidi-\nrectional Transformer to predict the next word, fine-tuning\nall parameters for specific tasks. Recently, generative SSL\nmethods have begun to outperform contrastive approaches\nin computer vision. Masked Autoencoders [17] randomly\nmask image patches and train the model to reconstruct the\nmissing pixels, leading to effective visual representations.\nBEIT [4] extends this by tokenizing image patches and pre-\ndicting masked tokens, integrating NLP techniques into vi-\nsion tasks. Additionally, Image GPT [27] treats images\nas sequences of pixels and trains a Transformer to autore-\ngressively predict pixels without explicit spatial structure,\ndemonstrating strong representation learning. This shift to-\nwards generative self-supervised learning methods not only"}, {"title": "3. Methods", "content": "The overall framework of PointACL is illustrated in Fig-\nure 2. First, the Attention-driven Dynamic Masking module\ngenerates an attention-guided masked point cloud. Both the\nmasked point cloud and the original input point cloud are\nthen fed into the shared backbone model to obtain the global\nfeatures of each input. By aligning the features from these\ntwo branches with contrastive loss, we guide the model\nto focus on the low-attention regions of the point clouds,\nthereby improving feature discrimination and generaliza-\ntion. During the pre-training stage, we train the model us-\ning a combination of contrastive loss and the original pre-\ntraining loss-such as the reconstruction loss from Point-\nMAE [32] or the generation loss from PointGPT [5]. After\npre-training, we employ the backbone model without the\nmasking strategy, leveraging the learned latent representa-\ntions for downstream tasks."}, {"title": "3.1. Attention-Driven Dynamic Masking", "content": "To fully harness the advantages of the self-attention mech-\nanism and mitigate the model's reliance on a small sub-\nset of key patches, we propose an attention-driven dynamic\nmasking strategy, which guides the model to focus on low-\nattention regions and enforces a more comprehensive un-\nderstanding of the global structure in challenging scenarios\nby dynamically masking high-attention areas.\nPoint patch attention. Given a point cloud $X \\in \\mathbb{R}^{P\\times3}$, we\nutilize Farthest Point Sampling (FPS) and K-Nearest Neigh-\nbors (KNN) algorithms to identify n center points C and\ntheir corresponding k nearest neighbors, forming n point\npatches P. We employ the self-attention mechanism in the\ntransformer architecture to compute the attention weights\nof point patches relative to the global feature. A new set\nof input tokens $T \\in \\mathbb{R}^{(N+1)\\times d}$, consisting of the point to-\nkens $T_p \\in \\mathbb{R}^{N\\times d}$ and a learnable global feature token $T_f \\in$\n$\\mathbb{R}^{1\\times d}$, is utilized to compute the queries $Q \\in \\mathbb{R}^{(N+1)\\times d}$,\nkeys $K \\in \\mathbb{R}^{(N+1)\\times d}$, and values $V \\in \\mathbb{R}^{(N+1)\\times d}$. The at-\ntention matrix A is subsequently derived from the dot prod-\nuct of the queries and keys. Since the first element of the\ninput tokens $T_1$ corresponds to the global feature token, the\nfirst row of the attention matrix can be interpreted as the\ncontribution of each token to the global feature. Consider-\ning the output tokens depend on both the attention matrix\nand the values, we incorporate the norm of V; when deter-\nmining the significance score of token j. Consequently, the\nattention matrix and significance score for point patch j are\ncomputed as follows:\nDynamic Masking. A straightforward idea is to mask the\ntop k patches with the highest significance scores, as they\nare key to the model's understanding of the point cloud.\nHowever, a fixed masking probability merely shifts the\nmodel's attention without engaging a broader set of patches.\nAs the model becomes reliant on new areas of focus, it simi-\nlarly falls into the trap of limited comprehension of the point\ncloud. Our primary objective is to ensure that high-attention\nregions have a higher likelihood of being masked. There-\nfore, we propose a dynamic masking strategy. Specifically,\nwe construct an updatable base masking probability using\nthe latest self-attention significance scores, prioritizing the\nmasking of patches that currently contribute significantly to"}, {"title": "3.2. Learning Objective", "content": "To further improve the model's feature discrimination and\ngeneralization, we introduce the contrastive loss to the pre-\ntraining stage, which combines the original pre-training loss\nwith a contrastive learning objective, enabling the model to\nretain task-specific learning capabilities while enhancing its\nglobal understanding.\nGlobal representation alignment. The dynamically se-\nlected masked token $T_m$ and the standard token $T_s$ are\nboth input into a shared-weight model, producing two dis-\ntinct levels of point cloud latent representations $F_m$ and\n$F_s$. Unlike the masked latent representations, the complete\npoint cloud retains all original information. Although the\nmasking strategy results in the loss of some regional details,\nboth representations still correspond to the same underlying\npoint cloud entity. Therefore, we expect the global features\nextracted from the masked point cloud to align with those\nderived from the standard point cloud. This alignment en-\nsures that the model becomes less dependent on specific,\nlocalized regions and is encouraged to focus on the global\nstructural characteristics of the point cloud. To achieve this,"}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nDatasets. We evaluate PointACL framework on three\nbenchmark datasets commonly used in 3D point cloud anal-\nsis. ScanObjectNN [45] comprises approximately 15,000"}, {"title": "4.2. Experimental Results", "content": "Real-world object classification on ScanObjectNN. Ta-\nble 1 compares our proposed PointACL method with exist-\ning approaches on the ScanObjectNN dataset across OBJ-BG, OBJ-ONLY, and PB-T50-RS settings. Our PointACL consistently outperforms these state-of-the-art methods.\nCompared to Point-MAE [32], PointACL achieves higher\naccuracies by +0.9%, +0.5%, and +0.2% on OBJ-BG, OBJ-ONLY, and PB-T50-RS, respectively. Against PointGPT-S [5], PointACL attains improvements of +0.7%, +1.6%,\nand +0.2% on the same splits. With simple rotational aug-\nmentation (marked with *), PointACL sets new state-of-\nthe-art results, achieving up to 94.5% on OBJ-BG, 93.5%\non OBJ-ONLY and 89.9% on PB-T50-RS. These results\ndemonstrate that PointACL effectively enhances feature\nrepresentation for point cloud data, particularly in challeng-\ning scenarios with background noise and object perturba-\ntions. The consistent performance gains across all settings\nhighlight the robustness and efficacy of our approach.\nSynthetic object classification on ModelNet40. Ta-ble 1 presents the performance of our proposed PointACL"}, {"title": "4.3. Ablation Studies", "content": "Components Analysis. We conduct extensive experiments\nwith PointGPT-S on ScanObjectNN to validate the effec-\ntiveness of each component. Table 8 summarizes the ab-\nlation study on different mask strategies and loss func-\ntions for the OBJ-BG and OBJ-ONLY settings. Without\nmasking, the baseline model achieves accuracies of 91.6%\n(OBJ-BG) and 90.0% (OBJ-ONLY). Applying a Random"}, {"title": "4.4. Robutness Analysis", "content": "We further evaluate the robustness of our method against\nexisting approaches under Gaussian noise conditions us-\ning the OBJ-ONLY subsets of the ScanObjectNN dataset.\nTo simulate noisy point clouds, we add Gaussian noise\n$X \\sim N(0, \\sigma^2)$ to all points, incrementally increasing the"}, {"title": "5. Conclusion", "content": "In this work, we present PointACL, an attention-driven con-\ntrastive learning framework. By integrating an attention-\ndriven dynamic masking strategy with contrastive learning,\nour method leverages the model's inherent attention distri-\nbution to dynamically mask high-attention regions. This\napproach guides the network to focus on under-attended\nlow-attention areas, enabling it to learn more comprehen-\nsive and robust point cloud feature representations. Our\nextensive experiments demonstrate that PointACL signif-"}, {"title": "6. Appendix", "content": "6.1. Preliminary\nTransformer-based self-supervised learning. Given a\npoint cloud X \u2208 RP\u00d73, we utilize Farthest Point Sam-\npling (FPS) and K-Nearest Neighbors (KNN) algorithms to\nidentify n center points C and their corresponding k near-est neighbors, forming n point patches P. Following the previous methods [5, 32], each point patch is normalized\nto integrate local information. A lightweight token em-bedding module, implemented via PointNet, subsequently\ntransforms these normalized local patches into trainable point tokens T. These point tokens, together with positional\nembeddings, are input into the transformer blocks to pro-duce latent representations F. For different tasks, these la-tent representations are input into task-specific heads, where\nthey are transformed into specific representations adapted to\nthe task. The learning pipeline based on the Transformer ar-chitecture is as follows:"}, {"title": "6.2. Training Strategy with Cost Analysis", "content": "Given that PointACL determines mask patches based on the\nattention weights of the backbone network, we suggest two\nstrategies for obtaining these attention weights. The first\nstrategy initializes the network with random attention and\napplies the attention-driven dynamic masking for adaptive\nattention refinement during subsequent training. Following\nstandard protocol, the model undergoes pre-training for 300\nepochs. This approach does not incur any additional train-ing overhead. The second strategy, by contrast, employs at-tention learned from the standard branch for initialization,aiming to dynamically adjust the model's dependencies in\na targeted manner. This method necessitates 300 epochs of\npre-training in the standard branch, followed by another 300\nepochs in the dual branch, resulting in a total of 600 epochs.\nFurthermore, we introduce PointACL during the fine-tuning phase of downstream tasks to further evaluate thescalability and effectiveness of our approach. Two strate-gies are employed here as well: one leverages the pre-trained attention for initialization, while the other requiresan additional 300 epochs of training to obtain attention\nlearned from the standard branch for initialization.\nOur experimental results, presented in Table 6, demon-strate the inherent advantages of our PointACL over ex-isting approaches (such as Point-MAE and PointGPT-S)under the same training time and training phase. For\nPoint-MAE, with 300 pre-training epochs or 300 fine-tuning epochs, PointACL achieves an accuracy of 90.5%\non the OBJ-BG dataset, surpassing Point-MAE's 90.0% by\na margin of 0.5%. This improvement persists when both\nmethods are trained for 600 epochs during the fine-tuning\nphase, with PointACL reaching 90.9% accuracy compared\nto Point-MAE's 90.0%. Similarly, when evaluating against\nPointGPT-S, PointACL continues to exhibit superior per-formance. With both models trained for 300 fine-tuningepochs on OBJ-ONLY, PointACL attains an accuracy of90.9% compared to PointGPT-S's 90.2%. Even when the\ntraining epochs are extended to 600, PointACL maintains\nits advantage, achieving 91.6% accuracy, outperforming\nPointGPT-S by 1.4%. On the OBJ-BG dataset, a similar\npattern is observed, where PointACL consistently outper-forms PointGPT-S regardless of training duration.\nThe superior performance of PointACL across variousdatasets, training epochs, and application phases validatesthe efficacy of our framework. It demonstrates the perfor-mance gains of PointACL are not a consequence of longertraining times but are a direct result of designed frame-work contributions-namely, the attention-driven dynamicmasking strategy with contrastive learning. By focusing onunder-attended regions and enhancing feature discrimina-tion, PointACL effectively captures both global and local\nfeatures, leading to enhanced robustness and generalization."}, {"title": "6.3. Ablation Study Analysis", "content": "Mask strategy. We conduct a thorough exploration intothe effects of various mask strategies and mask ratios onthe classification performance under the OBJ-BG and OBJ-ONLY settings. Two distinct mask strategies are eval-uated: High-Attention Mask with Fixed Mask Probabil-ity and High-Attention Masking Dynamic Mask Probabil-ity. The detailed experimental results are presented in Ta-ble 7. When employing the High-Attention Mask with\nFixed Mask Probability, we observe that increasing the\nmask ratio from 0.2 to 0.6 leads to improved performance,"}, {"title": "6.4. Robutness Analysis", "content": "We further evaluate the robustness of our method againstexisting approaches under Gaussian noise conditions us-ing the OBJ-ONLY subsets of the ScanObjectNN dataset.To simulate noisy point clouds, we add Gaussian noiseX ~ N(0, \u03c3\u00b2) to all points, incrementally increasing the"}, {"title": "6.5. Feature Distribution Evaluation", "content": "Figure 6 illustrates the evolution of the global feature dis-tribution using t-SNE during the fine-tuning of PointACL,with Point-MAE as the backbone, on the ModelNet40dataset. In the early stage feature distribution, the feature\nspace is highly scattered with overlapping clusters, indicat-ing that the backbone has not yet learned to effectively dis-criminate between different classes. As the backbone startsto align global representations from standard branch andmasked branch based on attention-driven dynamic mask-ing, the transitional feature distribution shows a notable im-provement, with clusters becoming more distinct. However,\nthere still remains some inter-class overlap.\nIn the final feature distribution, the clusters are well-separated and compact, reflecting a highly discriminativefeature space. The backbone has successfully learned todistinguish between different classes with a high degree ofaccuracy. The representative clusters at the bottom of eachvisualization further emphasize this progression, showing aclear transition from mixed and overlapping clusters in theearly stages to well-defined and isolated clusters in the finalstage. These visualizations highlight the effectiveness ofthe PointACL, demonstrating a clear trajectory of improve-ment in feature discrimination, culminating in a robust andwell-defined feature space."}, {"title": "6.6. Limatation Analysis", "content": "Despite the significant improvements achieved byPointACL, there are still areas that offer opportunitiesfor further enhancement. For example, while our methodhas been validated on specific datasets, applying it to abroader range of datasets could further demonstrate itsgeneralizability and robustness. Additionally, althoughwe have shown that PointACL integrates seamlessly withcertain Transformer-based architectures, exploring itscompatibility with an even wider variety of models couldhighlight its versatility even more. These considerationsopen avenues for future research to build upon our workand continue advancing the field of point cloud analysis."}, {"title": "6.7. Future Works", "content": "While the proposed PointACL framework has shown signif-icant improvements in point cloud analysis tasks, there areseveral promising directions for future research to furtherenhance its capabilities and applications.\nOne potential avenue is the integration of multi-modaldata sources to enrich point cloud representations. By incor-porating complementary information from modalities suchas images, textual descriptions, or LiDAR intensity val-ues, the model can leverage cross-modal correlations tolearn more comprehensive and robust feature embeddings.This multi-modal fusion could enhance the model's abil-ity to understand complex scenes and improve performancein tasks like 3D object detection and semantic segmenta-tion. Another direction is the exploration of hierarchicalor multi-scale feature learning within the PointACL frame-work. By capturing features at various spatial resolutions,\nthe model can better represent both local geometric details"}]}