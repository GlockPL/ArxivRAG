{"title": "LOCA: LOCATION-AWARE COSINE ADAPTATION FOR PARAMETER-EFFICIENT FINE-TUNING", "authors": ["Zhekai Du", "Yinjie Min", "Jingjing Li", "Ke Lu", "Changliang Zou", "Liuhua Peng", "Tingjin Chu", "Mingming Gong"], "abstract": "Low-rank adaptation (LoRA) has become a prevalent method for adapting pre-trained large language models to downstream tasks. However, the simple low-rank decomposition form may constrain the hypothesis space. To address this limitation, we introduce Location-aware Cosine Adaptation (LoCA), a novel frequency-domain parameter-efficient fine-tuning method based on inverse Discrete Cosine Transform (iDCT) with selective locations of learnable components. We begin with a comprehensive theoretical comparison between frequency-domain and low-rank decompositions for fine-tuning pre-trained large models. Our analysis reveals that frequency-domain approximation with carefully selected frequency components can surpass the expressivity of traditional low-rank-based methods. Furthermore, we demonstrate that iDCT offers a more efficient implementation compared to inverse Discrete Fourier Transform (iDFT), allowing for better selection and tuning of frequency components while maintaining equivalent expressivity to the optimal iDFT-based adaptation. By employing finite-difference approximation to estimate gradients for discrete locations of learnable coefficients on the DCT spectrum, LoCA dynamically selects the most informative frequency components during training. Experiments on diverse language and vision fine-tuning tasks demonstrate that LoCA offers enhanced parameter efficiency while maintains computational feasibility comparable to low-rank-based methods.", "sections": [{"title": "INTRODUCTION", "content": "Pre-trained large language models (LLMs) (Radford et al., 2019; Liu et al., 2019; Brown et al., 2020) have shown strong capabilities in learning language knowledge and adapting to various natural language processing (NLP) tasks through fine-tuning (FT). This FT paradigm has extended to vision (Dosovitskiy et al., 2020; Liu et al., 2021) and multi-modal domains (Radford et al., 2021; Li et al., 2022), leveraging the Transformer architecture (Vaswani et al., 2017). However, as models grow larger, fine-tuning the entire model becomes too costly for practical use.\nTo address this challenge, various Parameter-Efficient Fine-Tuning (PEFT) methods (Houlsby et al., 2019) have been developed. Adapter-based methods (Hu et al., 2023; He et al., 2021) insert small trainable modules into Transformer layers. Prompt-based approaches (Lester et al., 2021; Wang et al., 2023) prepend learnable vectors to input or hidden states. However, these methods often introduce non-negligible inference overhead. Partial FT (Zaken et al., 2021; Xu et al., 2021) selectively updates a subset of existing model parameters, but they still suffer from suboptimal performance compared to full FT. To address these limitations, Low-Rank Adaptation (LoRA) (Hu et al., 2021) offers an alternative by reparameterizing incremental updates of pre-trained weights using low-rank decomposition. For a pre-trained weight matrix $W_o \\in \\mathbb{R}^{P\\times q}$ in an attention layer or a feed-forward layer, LoRA reparameterizes fine-tuned weights as $W' = W_o+\\Delta W = W+BA$, where $B \\in \\mathbb{R}^{P\\times r}$, $A \\in \\mathbb{R}^{r\\times q}$, and $r < min(p, q)$. During FT, only A and B are updated. This allows LoRA to significantly reduce the number of trainable parameters while still achieving impressive performance."}, {"title": "PRELIMINARY ANALYSIS OF FINE-TUNING MODERN LLMS", "content": "Modern LLMs are predominantly built upon the Transformer architecture (Vaswani et al., 2017), where each Transformer block has a multi-head self-attention (MHSA) and a feed-forward network (FFN). For input $x \\in \\mathbb{R}^{n\\times d}$, MHSA projects $x$ into query, key, and value matrices per head h using $W_h^q, W_h^k, W_h^v \\in \\mathbb{R}^{d\\times d/H}$, where H is the number of heads. The FFN then processes the attention output using $W_{f1} \\in \\mathbb{R}^{d\\times d_m}$ and $W_{f2} \\in \\mathbb{R}^{d_m\\times d}$, where $d_m$ is the hidden dimension.\nTo systematically analyze the behavior of fine-tuning LLMs, we fine-tune a pretrained LLaMA-7b model (Touvron et al., 2023a) on the Alpaca-52K dataset (Taori et al., 2023). For each fine-tuned weight matrix $W' \\in \\mathbb{R}^{P\\times q} (p \\geq q)$, we get the incremental matrix $\\Delta W = W' - W_o$ and examine its properties from various perspectives. Our empirical observations reveal that the weights in each $\\Delta W$ closely approximate a Gaussian distribution (Fig. 1a). We claim that this normality can be theoretically justified. Consider a pre-trained model $f$ with a pre-trained weight matrix $W_o$. Assume the fine-tuning dataset is sampled from $P(X, Y; W)$, where $W$ can be considered as the distribution parameter as well as the oracle solution of fine-tuning, $X$ and $Y$ denote the input data and corresponding labels, respectively. During the FT process, we obtain the parameter $W'$ by minimizing the empirical loss. Consequently, $W'$ can be regarded as an M-estimator of $W$, which satisfies $P_n \\nabla_W \\ell[Y - f(X;W')]^2 = 0$, where $P_n$ is the empirical average over n samples drawn from $P(X, Y; W)$, $\\ell$ is the score function, and $\\ell$ is an objective function. Under fairly general conditions, $W' - W$ is known to be asymptotically normal (Yohai & Maronna, 1979):\n$\\sqrt{n} (W'-W) \\xrightarrow[]{V} N_{pq} (0,\\Sigma)$ , where $\\vec{.}$ denotes vectorization. We further assert that, under some mild assumptions, the incremental matrix $\\Delta W$ also exhibits asymptotic normality.\nProposition 1. Let $W_o \\in \\mathbb{R}^{K\\times K}$ and $W' \\in \\mathbb{R}^{K\\times K}$ be the pre-trained weight matrix and fine-tuned weight trained on datasets with $N$ and $n'$ data samples, respectively. Assume that (A1) The pre-training dataset follows $P(X,Y; W_o)$. For real-world fine-tuning datasets, the vectorized $W$ follows a prior distribution $N_{K^2} (W, \\sigma_o^2 I_{K^2})$, where $\\sigma_o$ is a constant. (A2) For any given $W$, let $W'$ be an M-estimator that satisfies asymptotic normality. The elements on $W' - W$ are asymptotically independent and identically distributed, and the estimation error $W' - W$ is independent of $W$. Under these assumptions, there exists $\\sigma_o > 0$, the weight update matrix $\\Delta W = W' - W_o$ satisfies:\n$\\Delta WV \\sim N_{K^2} \\Big(0, \\big(\\frac{\\sigma^2}{n'} + OP(\\frac{1}{\\sqrt{N}})\\big) IK^2 \\Big) + OP(\\frac{1}{\\sqrt{n'}})$"}, {"title": "COMPARISON BETWEEN FREQUENCY-SPACE AND LOW-RANK ADAPTATION", "content": "Given the asymptotic Gaussian nature of $\\Delta W$, we can now analytically compare the expressivities of low-rank-based and frequency-space-based adaptation methods. We regard expressivity as the ability to approximate a fully fine-tuned weight incremental matrix using the same parameter budget.\nGiven any $\\Delta W \\in \\mathbb{R}^{P\\times I}$ obtained through full fine-tuning, low-rank-based methods approximate it as $W_R = BA$ with $N_o = (p + q)r$ parameters, where r is the chosen rank. In contrast, FourierFT (Gao et al., 2024) adopts a frequency-domain approach by randomly selecting $N_1$ components on the Fourier spectrum $F = \\mathcal{F}(\\Delta W)$ to learn, setting others to zero, and approximates $\\Delta W$ as"}, {"title": "LOCATION-AWARE COSINE ADAPTATION", "content": "In this work, we regard the goal of PEFT as effectively reparameterizing a weight incremental ma-trix. Building on our previous analysis, we aim to propose a frequency-domain PEFT method that considers both the coefficients and locations of frequency components. Formally, given a pre-trained weight matrix $W_o \\in \\mathbb{R}^{P\\times q}$, our objective is to fine-tune it on a specific dataset to obtain the fine-tuned weight matrix $W' = W_o + \\Delta W = W_o+\\alpha\\mathcal{F}^{-1}(S(a,l, k))$, where $\\alpha$ is a scaling coefficient, $a = \\{a_i\\}_{i=1}^{B}$ represents the learnable coefficients, $l = \\{l_i, l_2\\}_{i=1}^{B}$ stores the component locations, $k = \\{0,1\\}^B$ indicates real (1) or imaginary (0) coefficients, B is the component budget, and $S(\\cdot)$ is an operator that scatters a onto a zero matrix according to l and k.\nHowever, its practical implementation presents significant challenges, primarily due to the require-ment for extensive discrete optimization of l and k. This motivates our exploration of alternative formulations that balance the benefits of frequency-space adaptation with computational feasibility."}, {"title": "INVERSE DISCRETE COSINE TRANSFORM-BASED REPARAMETERIZATION", "content": "Individually selecting learnable coefficients requires deciding whether to learn the real or imaginary part on each location in I, which involves extensive discrete optimization of k in practical imple-mentation. To address this issue, we introduce the discrete cosine transform (DCT). We prove that in this problem, individually selecting learnable coefficients on the Fourier spectrum is equivalent to selecting locations on the DCT spectrum, which involves only real-valued coefficients.\nTheorem 2. Let $W \\in \\mathbb{R}^{K\\times K} \\sim \\mathcal{G}$ be a weight matrix where each element independently follows a standard normal distribution $\\mathcal{N}(0,1)$. Let $D(\\cdot)$ and $D^{-1}(\\cdot)$ denote the discrete cosine transform (DCT) and inverse DCT, respectively, and $\\mathcal{F}(\\cdot)$ denote the discrete Fourier transform. Define $\\mathcal{F}_D$ as the sparse matrix that preserves the $N_D$ coefficients with the largest absolute values on $D(W)$ and sets others to 0. With $W_D = D^{-1}(\\mathcal{F}_D)$, and $\\mathcal{L}(\\cdot,\\cdot)$, $N_3$, $W^{(3)}$ stated above, if $N_D = N_3$, then:\n$E_{W\\sim\\mathcal{G}}[\\mathcal{L}(W, W^{(3)}))] = E_{W\\sim\\mathcal{G}}[\\mathcal{L}(W, W_D)]$.\nTheorem 2 guides us towards a more efficient alternative by utilizing the iDCT instead of the iDFT. By reparameterizing $\\Delta W$ using iDCT, We can maintain the equivalent expressivity while avoiding the optimization of k. This is because DCT operates in the real domain, which simplifies computa-tions and reduces the complexity of parameter selection. It is known that iDCT is essentially a linear transformation (Ahmed et al., 1974). We can express the reparameterization based on 2D iDCT by\nW' = Wo + \\Delta W = Wo + a[C^TS(a,l, 1)D],\nwhere $C\\in \\mathbb{R}^{P\\times P}, D\\in \\mathbb{R}^{I\\times I}$ are the DCT matrices. The elements of $C$ are defined as:\n$C_{ij} = \\sqrt{\\frac{2}{P}} \\cdot k_i cos(\\frac{\\pi(2j+1)i}{2P}), \nk_i = \\begin{cases}\\frac{1}{\\sqrt{2}}, & \\text{if } i = 0 \\\\1, & \\text{if } i > 0.\\end{cases}$\nThe formulation is similar for D. In practice, when S(a,1,1) is highly sparse, we can further simplify the computation by $\\Delta W = a[C^TS(a,l,1)D] = \\alpha\\sum_{i=1}^{B} a_iC_{i.}\\cdot D_{i_.}$, where $C_{i.}$ is the $l_1$-th row of $C$, and $D_{i_.}$ is the $l_2$-th row of $D$. This simplification reduces the computation complexity of iDCT from $O(p^2q^2)$ to $O(Bpq)$. In contrast, when more frequency components are needed, it is recommended to use the fast DCT algorithm with an asymptotic complexity of $O(\\log(pq)pq)$.\nEstimating Location Gradient using Finite-Difference Approximation"}, {"title": "ESTIMATING LOCATION GRADIENT USING FINITE-DIFFERENCE APPROXIMATION", "content": "While the coefficients a can be directly optimized through backpropagation, the operation $S(\\cdot)$ does not produce gradients with respect to the locations 1. Furthermore, I needs to be treated as a discrete variable, which prevents us from directly learning the locations through backpropagation.\nTo address this issue, we draw inspiration from the straight-through estimator (STE) (Bengio et al., 2013), a technique that allows gradient-based optimization of neural networks with discrete variables by using a surrogate gradient. However, unlike traditional STE that simply bypasses the gradient computation for discrete variables, e.g., the STE used in VQ-VAE (Van Den Oord et al., 2017), we estimate their gradients using the central difference approximation, as we elaborate below.\nForward Pass. To enable gradient-based learning of location variables, we first redefine the lo-cations I as continuous variables. During the forward pass, we discretize I by $\\hat{l} = round(l) = \\{(\\hat{l}_1, \\hat{l}_2)_i\\}_{i=1}^{Pq}$, where round(\u00b7) maps each element of I to its nearest integer.\nBackward Pass. During the backward propagation, we estimate the gradient of the loss function L to each element in l. For clarity, we take $l_h$ and $a_h$ as an example. The location gradient is\n$\\frac{\\partial \\mathcal{L}}{\\partial l_h} = \\sum_{i=1}^{P}\\sum_{j=1}^{q} \\frac{\\partial \\mathcal{L}}{\\partial \\Delta W_{i,j}} \\cdot \\frac{\\partial \\Delta W_{i,j}}{\\partial l_h} = tr[\\Big((\\frac{\\partial \\mathcal{L}}{\\partial \\Delta W})^T \\frac{\\partial \\Delta W}{\\partial l_h}\\Big)]$.\nHere, $\\partial \\mathcal{L}/\\partial \\Delta W$ can be obtained directly through backpropagation. The tricky part is how to esti-mate $\\partial \\Delta W/\\partial l_h$. In this work, we choose to use central difference approximation, i.e.,\n$\\frac{\\partial \\Delta W}{\\partial l_h} \\approx \\alpha\\frac{C^T [S(a_h, (\\hat{l}_h + (1, l_2)), 1) - S(a_h, (\\hat{l}_h - (1, l_2)), 1)]D}{2}$"}, {"title": "ALTERNATING OPTIMIZATION STRATEGY", "content": "To effectively optimize both the coefficients a and locations I, we implement an alternating opti-mization scheme inspired by coordinate ascent methods (Wright, 2015), which have shown remark-able efficacy in tackling multi-variable optimization problems. Specifically, we initially train the coefficients a for $B_a$ steps while maintaining fixed locations I. Subsequently, we fix a and optimize the locations I for $B_l$ steps. This alternating process continues for totally $B_s$ iterations. After that, we only optimize the coefficients a until convergence. This strategy facilitates an efficient explo-ration of the frequency domain while progressively refining the selected components in the early training state, while focusing on the coefficients of the identified important frequency components in the remaining stage. A detailed training procedure can be found in Appendix E."}, {"title": "EXPERIMENTS", "content": "We mainly evaluate LoCA across four domains: natural language understanding (NLU), natural language generation (NLG), instruction tuning, and computer vision. For NLU tasks, we fine-tune RoBERTa models on the GLUE benchmark (Wang et al., 2018). For NLG, we fine-tune GPT-2 (medium/large) on E2E NLG Challenge. For instruction tuning, we fine-tune LLaMA-family mod-els on the Alpaca-52K dataset (Taori et al., 2023) and evaluate them on the MT-Bench (Zheng et al., 2024) and Vicuna (Chiang et al., 2023) datasets. For vision tasks, we fine-tune Vision Transformer (ViT) models on 8 classification datasets. More experiments can be found in Appendix.\nImplementation Details. We implement our method using the PyTorch framework. Our code is built on the PEFT library (Mangrulkar et al., 2022) from Huggingface, and all pre-trained models are sourced from Huggingface's Transformers library (Wolf et al., 2020). For the alternating op-timization, we used $B_a = 10$ and $B_l = 20$. The coefficients a are initialized to be zeros and the locations I are randomly initialized with a uniform distribution. We scale I to the range [0, 1] for op-timization. All PEFT experiments are conducted on a single NVIDIA Tesla H100 GPU. Noting that while LoCA initially optimizes both a and I, the locations are fixed after $B_s$ iterations. Therefore, the reported number of trainable parameters only includes the final coefficient parameters.\nBaseline Methods. We compare our LoCA with Full fine-tuning (FF), BitFit (Zaken et al., 2021), Adapter-based methods (Houlsby et al., 2019), LoRA (Hu et al., 2021), AdaLoRA (Zhang et al., 2023b), VeRA (Kopiczko et al., 2023), DoRA (Liu et al., 2024) and FourierFT (Gao et al., 2024)."}, {"title": "NATURAL LANGUAGE UNDERSTANDING", "content": "We evaluate our method on NLU tasks using the GLUE benchmark (Wang et al., 2018), which consists of diverse tasks that cover various aspects of language understanding, including single-sentence classification, similarity and paraphrase, and inference task. For our experiments, we fine-tune ROBERTa-base and RoBERTa-large models (Liu et al., 2019) on 8 GLUE tasks using different adaptation methods. Following Zhang et al. (2023b); Gao et al. (2024), we report the best results on the validation set for each task. Mean results are reported after 3 runs with different random seeds.\nImplementation Details. For LoRA and its variants, we use a rank $r = 8$ and a scaling value $\\alpha = 8$. To maintain consistency with FourierFT, we set the number of frequency components B"}, {"title": "NATURAL LANGUAGE GENERATION", "content": "We evaluate LoCA on the E2E NLG Challenge dataset (Novikova et al., 2017), a widely-used bench-mark for data-to-text generation. The dataset consists of over 50K samples in the restaurant domain, with each input being a set of slot-value pairs and the corresponding output being a natural language description. We conduct experiments on both GPT-2 medium and GPT-2 large.\nImplementation Details. Following Hu et al. (2021), we train our models using AdamW optimizer with a linear learning rate decay schedule for 5 epochs. We set the batch size to 32 and use a label smoothing factor of 0.1. We only adapt the query and value matrices, with 1000 frequency compo-nents for both LoCA and FourierFT."}, {"title": "INSTRUCTION TUNING", "content": "We fine-tune various LLaMA-family models (Touvron et al., 2023a;b) using the Alpaca-52K dataset (Taori et al., 2023). The Alpaca-52K dataset, derived from the self-instruct technique, provides a diverse set of instruction-following examples. In this experiment, we mainly compare our method with FF, LoRA and FourierFT. After fine-tuning, we evaluate the model on the MT-Bench (Zheng et al., 2024) and Vicuna (Chiang et al., 2023) datasets, which offer challenging multi-turn and open-ended scenarios for LLM evaluation. We employed GPT-4 to assign scores on a scale of 1-10 based on the quality, relevance, and coherence of the responses.\nImplementation Details. We apply all PEFT methods to the query and value matrices. For LORA, we set the rank r to 64 and the scaling value \u03b1 to 16. For FourierFT, we use 150K frequency components and tune other hyper-parameters to ensure the optimal performance, since we cannot reproduce the results in Gao et al. (2024). For LoCA, we also use 150K fre-quency components, and set the scaling value \u03b1 to 1. We utilize the LLM-as-a-Judge reposi-tory (Zheng et al., 2024) for fair evaluation. We train LLaMA-1-7b/LLaMA-2-7b for 3 epochs and LLaMA-1-13b/LLaMA-2-13b for 1 epoch. Quantization (Dettmers et al., 2024) is used for LLaMA-1-13b/LLaMA-2-13b to ensure feasi-ble FT on a single GPU."}, {"title": "IMAGE CLASSIFICATION", "content": "We evaluate our method on computer vision tasks by conducting experiments on 8 image classi-fication datasets, including OxfordPets (Parkhi et al., 2012), StanfordCars (Krause et al., 2013), CIFAR10 (Krizhevsky et al., 2009), DTD (Cimpoi et al., 2014), EuroSAT (Helber et al., 2019), FGVC (Maji et al., 2013), RESISC45 (Cheng et al., 2017) and CIFAR100 (Krizhevsky et al., 2009). We fine-tune ViT/16-base and ViT/16-large models (Dosovitskiy et al., 2020), both pre-trained on ImageNet-21k (Ridnik et al., 2021). In this experiment, we compares LoCA against several base-lines: Linear Probing (LP), FF, LoRA, and FourierFT. Noting that we encountered significant dis-crepancies when attempting to reproduce the results reported in Gao et al. (2024), possibly due to the lack of detailed hyperparameter setup. To ensure a fair comparison, we re-run all methods using our own hyperparameter settings. All results are obtained after 5 random trials.\nImplementation Details. To ensure a fair comparison across all methods, the classification head is configured identically for all approaches. For LoRA, we a rank of 16 and a scaling factor \u03b1 of 16. Following Gao et al. (2024), FourierFT is implemented with 3000 and 10,000 frequency components and a scaling factor of 300. For our LoCA, we also evaluate 3000 and 10,000 frequency components for both base and large models. The learning rates for all methods are carefully tuned to ensure good performance across different tasks and model sizes. We report the number of trainable parameters excluding the classification head to provide a clear comparison of parameter efficiency. Detailed hyperparameter configurations for all methods can be found in Table 9."}, {"title": "ANALYTICAL EXPERIMENTS", "content": "Effectiveness of Gradient Estimation. To validate the reliability of our estimated location gra-dients, we present the training process on 4 selected datasets in Fig. 2. The left figure shows that during the alternating optimization phase, the validation loss generally decreases in most steps, particularly for StanfordCars and CI-FAR10. The right figure demonstrates corre-sponding improvements in validation accuracy. These trends indi-cate that our central difference approximation method effectively guides the optimization pro-cess, enabling successful updates to frequency component locations.\nPerformance under Different Parameter Budgets. Fig. 3 compares various methods under same parameter budgets. Here we focus on QQP and FHVC, which present significant challenges for LoRA. The parameter budget is standardized using LoRA's rank r as the base unit. Our results reveal that FourierFT often underperforms LoRA when using fewer parameters. This ob-servation aligns with expectations, as the locations of frequency components becomes increasingly critical under constrained parameter budgets. Notably, LoCA consistently outperforms LoRA and FourierFT across the tested scenarios. It is worth noting that our theoretical analysis centers on ex-pected performance. While specific task structures may allow FourierFT to surpass LoRA in certain instances, these exceptions do not undermine our overall conclusions and analytical framework.\nChoice of Scaling value a and Alternating Optimization Steps Bs. Fig. 4 demonstrates the impact of different choices of a and Bs on the MRPC task. We empirically find that a scaling value between 1-2 can achieve better results. Additionally, setting Bs to between 10%-20% of the total training steps is more appropriate (with a total of 5750 steps for the MRPC task).\nAblation Study of the Alternating Optimization Strategy. Table 5 compares several vari-ants of our method: V1 only optimizes coefficients with randomly initialized locations. V2 alternately optimizes coefficients and locations throughout the training. V3 jointly optimizes locations and coefficients in each step for Bs steps. V4 and V5 use forward and backward difference approximation for gradient estimation, respectively. Hyperparameters are identical"}, {"title": "RELATED WORK", "content": "The recent surge in LLM research has reignited interest in PEFT research. To pursue favorable task performance while using only a small number of trainable parameters, current PEFT methods primarily lie in four categories: adding extra trainable modules (Houlsby et al., 2019; R\u00fcckl\u00e9 et al., 2020), selectively training a small subset of key parameters (Zaken et al., 2021; Lawton et al., 2023), employing reparameterization techniques like low-rank decomposition to the incremental matrices (Hu et al., 2021; Zhang et al., 2023b; Liu et al., 2024; Hao et al., 2024), or combining multiple strate-gies (Chen et al., 2023). Among them, low-rank methods have garnered significant attention due to their mergable nature and parameter efficiency. These low-rank methods, which aim to approxi-mate large weight matrices using a few principal components, is highly analogous to techniques employed in data compression. In fact, low-rank decomposition (or singular value decomposition) and frequency-domain decomposition (e.g., JPEG compression) represents two fundamental tools in image compression and signal processing.\nFor image compression, frequency-domain reconstruction (e.g., DCT) are preferred due to the in-herent smoothness prior of image data (Wallace, 1991). However, when dealing with the complex data structures of neural network parameter matrices, the relative efficacy of these approaches re-mains unexplored. To the best of our knowledge, although FourierFT (Gao et al., 2024) has made an empirical study of frequency-domain PEFT by employing Fourier Transform, no prior work has conducted a rigorous comparison between low-rank and frequency-domain decomposition methods in the context of PEFT. Our work aims to bridge this gap by providing a comprehensive theoretical analysis and designing a more efficient frequency-domain PEFT method."}, {"title": "CONCLUSION", "content": "This paper provides a theoretical foundation for frequency-domain PEFT methods. We prove that carefully selected frequency components can outperform low-rank approaches, leading to the devel-opment of location-aware frequency-domain PEFT method. Our method optimizes both coefficients and locations of frequency components using iDCT and difference approximation. We show that our method enhances expressiveness while maintaining computational efficiency. Extensive experi-ments across NLP and computer vision tasks demonstrate the superior performance and parameter efficiency compared to existing PEFT methods."}, {"title": "JUSTIFICATION OF ASSUMPTIONS", "content": "In the pre-training and fine-tuning paradigm, deep neural networks are initially trained on a large dataset with distribution P(X, Y; Wo) and subsequently fine-tuned on a specific down-stream dataset with distribution P(X, Y; W). In this context, W becomes a random variable associated with a specific data distribution.\nFirst for assumption (A1), the large dataset used for pre-training represents an aggregation of nu-merous sub-datasets. Each sub-dataset contributes to the overall distribution P(X, Y; Wo). The parameter Wo can be seen as the central tendency (mean) of the parameters for all sub-datasets. This aggregation naturally leads to a central limit theorem effect, where the mixture of multiple sub-datasets can be approximated by a normal distribution around Wo, which also reflects the idea of symmetry in the distribution of sub-datasets. In the absence of strong directional biases, it is reasonable to consider that the parameters for different sub-datasets are symmetrically distributed. Note that our proposition is based on all sub-datasets, which also follows the philosophy of the No Free Lunch (NFL) theorem in machine learning. By modeling W as a distribution centered on Wo, we account for the variability across different sub-datasets.\nRegarding assumption (A2), the asymptotic normality of M-estimators is a commonly used assump-tion in statistics and machine learning. The strongest assumption here should be that the elements of W' - W are asymptotically independent and identically distributed given W. To demonstrate the reasonability of this assumption. We first consider the asymptotically i.i.d. property of W'. While the strict i.i.d. property of parameters in trained neural networks remains a subject of ongoing research, several studies have shwon that certain statistical properties of these parameters resemble those of random i.i.d. matrices (Thamm et al., 2022; Martin & Mahoney, 2021; Lee et al., 2017). Our work extends this line by examining the spectral properties of the trained weight during LLM fine-tuning. Specifically, we use the Marchenko-Pastur (MP) law to test the fit between the empir-ical spectral densities of W' and that of random matrices. The MP law is a fundamental result in random matrix theory. It describes the asymptotic behavior of the eigenvalue distribution of large random matrices. The law can be formally stated as follows: Consider a p \u00d7 q random matrix W, where each element is an independent and identically distributed random variable with mean 0 and variance \u03c3\u00b2. Let C = (1/p)W'TW' be the covariance matrix. As p,q \u2192 \u221e with a fixed aspect ratio, the empirical spectral distribution of the eigenvalues of C converges almost surely to a deter-ministic probability distribution known as the Marchenko-Pastur distribution. Here we are dealing with large Transformer weight matrices. If they are asymptotically i.i.d. matrixes, the ESD of them should closely approximate the MP distribution corresponding to their current aspect ratios. We visualize the ESD of the fine-tuned W' across multiple layers, as shown in Fig. 5. And the results show that W' behaves like an i.i.d random matrix. As each element on W is permutable due to the equal role of different positions, we can summarize that W has a zero-expectation influence on W' - W. Therefore, the asymptotically i.i.d property of W' \u2013 W does not violate our observations. The assumption that W' \u2013 W and W are independent is analogous to treating W' \u2013 W as noise, while W is the true signal. This is a common assumption in the context of asymptotic analysis, where the estimation error (or noise) is considered to be independent of the true parameter."}, {"title": "DETAILS OF THE HYPOTHESIS TESTING", "content": "We now describe the detailed procedure of the hypothesis testing adopted in Section 2. Recall that our goal is to test whether the elements w from the weight incremental matrix AW follows a distribution that is close to a Gaussian. Formally", "Setup": "n$H_o$: $d_{TV"}, "P(w), \\mathcal{N}(w; \\mu, \\sigma^2)) \\leq \\epsilon$, $H_1$: $d_{TV} (P(w), \\mathcal{N}(w; \\mu, \\sigma^2)) > \\epsilon$\nWhere $d_{TV} (,\\cdot)$ denotes the total variation distance, P(w) is the true distribution of elements in AW, and $\\mathcal{N} (\\mu, \\sigma^2)$ is the normal distribution with sample mean and variance as parameters.\nTest Statistic:\nT = d_{TV} (\\hat{P}_n(w), \\mathcal{N}(w; \\mu, \\sigma^2))\nWhere $\\hat{P}_n (w)$ is the empirical distribution of w.\nTesting Procedure:\nGiven a $\\Delta W \\in \\mathbb{R}^{P\\times I}$ yielded by full fine-tuning, our test procedure consists of the following steps.\n1.  From the observed AW, compute the empirical mean $\\mu$ and variance $\\sigma^2$.\n2.  Generate 1e5 samples from $\\mathcal{N}(w; \\mu, \\sigma^2)$, denoted this set of samples by G.\n3.  Generate B perturbed distributions:\n    *   Add small random perturbations $\\epsilon \\sim \\mathcal{N}(\\epsilon; 0, \\sigma_{\\epsilon}^2)$ to the M samples, where $\\sigma_{\\epsilon} = 1e-5$.\n    *   Calculate the empirical distribution of the perturbed samples.\n    *   Compute the total variation distance between the obtained empirical distribution and G.\n    *   If the total variation distance is less than $\\epsilon$, keep this distribution.\n    *   Repeat until 100 valid perturbed distributions are obtained.\n4.  For each of the 100 perturbed distributions:\n    *   Sample 10 sets of p \u00d7 q points.\n    *   For each set, calculate the total variation distance between the empirical distribution of this set and G. This results in M \u00d7 P total variation distances, forming the distribution of the test statistic under Ho.\n5"]}