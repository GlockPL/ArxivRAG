{"title": "Can Domain Experts Rely on Al Appropriately? A Case Study on Al-Assisted Prostate Cancer MRI Diagnosis", "authors": ["CHACHA CHEN", "HAN LIU", "JIAMIN YANG", "BENJAMIN M. MERVAK", "BORA KALAYCIOGLU", "GRACE LEE", "EMRE CAKMAKLI", "MATTEO BONATTI", "SRIDHAR PUDU", "OSMAN KAHRAMAN", "G\u00dcL GIZEM PAMUK", "\u0391\u03a5\u03a4\u0395\u039a\u0399\u039d \u039f\u03a4\u039f", "ARITRICK CHATTERJEE", "CHENHAO TAN"], "abstract": "Despite the growing interest in human-AI decision making, experimental studies with domain experts remain rare, largely due to the complexity of working with domain experts and the challenges in setting up realistic experiments. In this work, we conduct an in-depth collaboration with radiologists in prostate cancer diagnosis based on MRI images. Building on existing tools for teaching prostate cancer diagnosis, we develop an interface and conduct two experiments to study how AI assistance and performance feedback shape the decision making of domain experts. In Study 1, clinicians were asked to provide an initial diagnosis (human), then view the Al's prediction, and subsequently finalize their decision (human+AI). In Study 2 (after a memory wash-out period), the same participants first received aggregated performance statistics from Study 1, specifically their own performance, the Al's performance, and their human+AI performance, and then directly viewed the Al's prediction before making their diagnosis (i.e., no independent initial diagnosis). These two workflows represent realistic ways that clinical AI tools might be used in practice, where the second study simulates a scenario where doctors can adjust their reliance and trust on AI based on prior performance feedback. Our findings show that, while human+AI teams consistently outperform humans alone, they still underperform the Al due to under-reliance, similar to prior studies with crowdworkers. Providing clinicians with performance feedback did not significantly improve the performance of human-AI teams, although showing AI decisions in advance nudges people to follow AI more. Meanwhile, we observe that the ensemble of human-AI teams can outperform Al alone, suggesting promising directions for human-Al collaboration. Overall, our work highlights the prevalence and persistence of under-reliance, while demonstrating hope for complementary performance.", "sections": [{"title": "1 Introduction", "content": "Al holds promise for improving human decision making in a wide range of domains [2, 16, 19, 31, 34]. Radiology is a representative example as AI outperforms or shows comparable performance with experts [11, 17, 24, 28, 30, 32, 33, 38]. Rather than complete automation, there is growing consensus that Al's optimal role in the near future will serve as an assistance tool for human radiologists in clinical decision making [1, 10, 22, 25]. On the one hand, legal and regulatory challenges stand in the way of full automation. On the other hand, human AI collaboration has the potential to achieve complementary performance, where human experts can leverage their contextual knowledge and expertise to correct AI mistakes in ways that could surpass either human or AI performance alone. However, the actual utility of integrating AI assistance tools in clinical settings remain poorly understood. In particular, very few studies examine the effectiveness of AI assistance in real clinical decision-making with domain experts [3, 26]. In this work, we conduct an in-depth collaboration with radiologists and focus on the case of prostate cancer diagnosis. Prostate cancer diagnosis with magnetic resonance imaging (MRI) remains one of the most difficult tasks for radiologists\u2014even experienced ones\u2014and inter-reader variability is high [6, 7]. Such complexity makes prostate MRI an ideal testbed for studying how AI assistance may complement human expertise. If AI can help reduce radiologists' mistakes here, it is plausible that similar technology could be effective in other radiology tasks as well.\nWe run human studies with domain experts to directly understand AI tool integration in radiology workflow, particularly for challenging diagnoses like prostate cancer. We investigate two key questions:\nQ1: Can Al-assistance help humans achieve higher diagnostic accuracy than either human experts or Al systems alone?\nQ2: How does AI-assistance shape human decision making beyond decision accuracy?\nTo answer these questions, we conducted pre-registered human subject experiments with domain experts, specifically board-certified radiologists (N=8), focusing on prostate cancer diagnosis with AI assistance. We first trained a state-of-the-art AI model [12] for prostate cancer detection from MRI scans. The AI model is able to provide both diagnostic predictions and lesion annotation maps for positive cases as assistance for radiologists. To simulate real-world clinical practice, we designed and implemented two distinct workflows, see Fig. 1 for an overview of the design of our human studies. Building on existing tools for teaching prostate cancer diagnosis, we also developed a web-based diagnostic platform that enables radiologists to review MRI scans and annotate suspicious cancer lesions seamlessly.\nIn Study 1, radiologists each evaluated 75 cases in a three-step process. For each case, they first made independent diagnoses, which helped us to establish baseline human performance. Then, they were shown the AI's predictions. In the final step, they are asked to finalize their decisions after reviewing AI predictions. In Study 2, we introduced a novel element: before starting their evaluations, radiologists first received detailed individual performance feedback from Study 1, as shown in the screenshot in Fig. 2c. This feedback included various metrics of their own performance, AI's performance, and their AI-assisted performance. To ensure engagement with this feedback, participants completed attention checks about their performance metrics before proceeding with new cases. This design allowed us to systematically examine how performance awareness influences radiologists' interaction with AI assistance. Moreover, for each case diagnosis, AI assistance was provided directly to radiologists without them making independent diagnosis.\nThese two distinct workflows represent common scenarios in the deployment of AI assistance tools in clinical practice and their evolution over time. Study 1 simulates an approach often regarded as responsible, as it allows radiologists to form independent opinions before consulting AI"}, {"title": "Can Domain Experts Rely on Al Appropriately? A Case Study on Al-Assisted Prostate Cancer MRI Diagnosis", "content": "predictions. This approach may be particularly relevant during early deployments, since radiologists may prefer minimal intervention to exercise caution. Over time, the performance information will become available in a local scenario that retains the same distribution of doctors and patients as in the earlier integration of AI tools. Through the design of Study 2, we can investigate how both the timing of AI assistance and awareness of comparative performance metrics influence diagnostic accuracy and radiologists' integration of AI recommendations.\nOur findings are consistent with prior studies on human-AI decision making. Human+AI outperforms human alone, showcasing the positive utility of AI assistance. However, Human+AI underperforms AI alone, largely driven by under-reliance. Although performance feedback and upfront AI assistance nudged radiologists to incorporate AI predictions more frequently, we did not observe statistically significant improvements in metrics such as area under the receiver operating characteristic curve (AUROC/AUC) or accuracy. We further investigate the effect of ensembling decisions. A promising finding is that the majority vote of Human-AI teams can outperform AI alone, achieving complementary performance. This observation points to exciting opportunities to identify insights into optimal ways to facilitate human-AI decision making.\nTo summarize, we make the following contributions:\n\u2022 We conduct an in-depth collaboration with domain experts and design two experiments to study the effect of AI-assistance on expert decision making.\n\u2022 We demonstrate that while human+AI outperforms human alone, they fall short of AI alone, similar to prior studies with crowdworkers.\n\u2022 We present potential opportunities in leveraging the collective wisdom of human-AI teams."}, {"title": "2 Related work", "content": "Human-Al decision making. There is a growing interest in the research community to augment human decision making with AI assistance [19]. Typically, the tasks of interest are situated in high-stakes domains such as medicine, law, and finance, where AI-assisted decisions can have significant consequences. However, due to constraints related to resources and the simplicity of participant recruitment, the majority of empirical studies in this area are conducted with crowdworkers or laypeople without expertise. For instance, instead of involving real judges, researchers have explored recidivism prediction as a testbed for Human-AI decision making using crowdworkers [4, 9, 20]. Similarly, in the medical domain, experiments on disease diagnosis have been conducted with laypeople, such as students [21]. In finance, studies have utilized crowdworkers for tasks like income prediction [39], loan approval [9], and sales forecasting [8]. In some cases, researchers have substituted real-world tasks with entirely artificial ones to facilitate experimentation with crowdworkers, such as alien medicine recommendation [18].\nWhile crowdworkers offer a convenient participant pool, it remains unclear if findings based on these populations generalize to domain experts in real cases. In our work, we work directly with domain experts.\nHuman-Al decision making with experts in the clinical context. There have been several studies with healthcare professionals in the clinical context, but experiments focused on human-AI complementary performance remain limited. While several studies have shown that AI assistance can improve diagnostic accuracy [13, 23, 35\u201337], the experts behavior in human-AI collaboration are underexamined. Existing research also reveals complex performance trade-offs: some studies reveal important trade-offs, such as improved sensitivity at the cost of reduced specificity [14, 27]. Some studies explicitly demonstrated that the performance of human-AI performance falls short of AI alone [15, 29]. To the best of our knowledge, the only work that achieves complementary"}, {"title": "3 Methods", "content": "3.1 Dataset\nWe used public data from the PI-CAI challenge\u00b9 for training and testing. The dataset originally contained 1500 cases, which we filtered down to 1411 cases by excluding cases from the same patients to avoid data leakage. We ensure that all testing cases are biopsy-confirmed. Our Al model was trained on 1211 cases, including 365 (30.1%) clinically significant prostate cancer (csPCa) cases. For study 1, the testing set includes 75 cases, of which 23 (30.6%) are csPCa. Study 2 consists of 100 cases, with 32 (32%) being csPCa. For each patient case, we used T2-weighted (T2W), diffusion-weighted imaging (DWI), and apparent diffusion coefficient (ADC) sequences as inputs for both AI and human studies. 50 cases were shared between study 1 and study 2, which allows us to directly compare performance metrics across both studies on this shared subset.\n3.2 Al model & performance\nWe use the established nnU-Net model [5, 12] as our AI model, trained from scratch with our own splits. We ensure that all testing examples have pathology groundtruth. Training examples have a mixture of different types of labels: pathology groundtruth, human expert labeled csPCa and delineation of the lesion area, and AI-labeled csPCa and lesion area [33]. The AI standalone performance on the testing sets for both studies is shown in Table 1. The AI model achieves an AUROC of 0.910 in the training set, 0.730 and 0.790 respectively for the study 1 and study 2 testing set. Note that all testing examples have pathology groundtruth while as training sample have a mixture of pseudo labels. For comprehensive details on the AI model's training configurations and performance metrics, please refer to appendix A.\n3.3 Human-Al Decision Making Interface\nWe developed a webapp to conduct the human-study. Participants can log in with their name and email. They will see a consent page when they log in for the first time. Once they give the consent, they will enter the study and see our study interface. A screenshot of the consent page can be found in appendix Fig. 9. Our human study is pre-registered and approved by the Institutional Review Board (IRB).\nStudy interface. Our study interface has three major components: the View Panel on the left, the Control Panel on the right, and the Annotation Panel as a pop-up in the center of the screen. The interface is shown in Fig. 2a. In the View Panel, we display three image sequences (T2W, ADC, BWI) from the MRI scans of the current case. In the Control Panel, participants are informed about the current study (study 1 or 2) and provided with control buttons to make decisions or proceed to the next steps. Binary case-level AI predictions are also presented in this panel. Participants make their own predictions by clicking the buttons (\u2018Annotate Cancer\" for positive cases and \u201cNo Cancer\" for negative cases) and indicate their confidence level using a sliding bar. If a participant believes the case is positive, they click the \"Annotate Cancer\" button, which triggers a pop-up window (Annotation Panel) displaying enlarged images from the T2W sequence of the current case, allowing participants to annotate the suspicious lesion areas. Participants can annotate any suspicious lesions by freely drawing on any image slice, using the sidebar to navigate between slices. The annotation interface is illustrated in Fig. 2b.\nPerformance feedback. In Study 2, the first page after the login page will be the performance feedback page, as shown in Fig. 2c. This page provides detailed individual feedback on their performance from Study 1. The feedback includes both case counts and performance metrics. Specifically, we present the total number of cases completed by the participant, the number of"}, {"title": "3.4 Experimental Design", "content": "To evaluate the effectiveness of AI assistance, we conduct two studies with practicing radiologists (N = 8). An overview of our experimental workflow is shown in Fig. 1.\nParticipant demographics, including experience levels, are detailed in Appendix B. Participants are recruited through interest forms distributed at the annual conference of RSNA (Radiological Society of North America), one of the largest radiology conferences in the world. We also use snowball recruiting, where participants refer colleagues and peers in their network. All participants are practicing radiologists and come from different regions (US and Europe), and all US-based participants are board-certified.\nStudy conditions. Our experiments include three main conditions to evaluate radiologist perfor-mance:\n\u2022 Human-only (Study 1): Independent diagnosis without AI assistance.\n\u2022 Human+AI (Study 1): Diagnosis made after independent diagnosis and reviewing AI predic-tions.\n\u2022 Human+AI (Study 2): Diagnosis made with AI predictions shown upfront, with prior feedback on individual performance metrics at the beginning of the study.\nIn Study 1, participants complete 75 test cases. After logging in and signing the consent form, we provide a toy case to familiarize participants with the interface and workflow. For each of the test cases, participants first make an independent diagnosis (human-only condition). Then they review the AI prediction and annotations. Participants have a chance to update and finalize their diagnosis before moving on to the next case (Human+AI condition for Study 1).\nBetween Study 1 and Study 2, we set a minimum memory wash-out period of 30 days to eliminate any recall effects. The actual period varies because participants complete the study at their own pace.\nIn Study 2, participants begin by reviewing a summary of their performance metrics from the Human+AI condition in Study 1. This feedback includes key metrics and interaction statistics to encourage reflection on their interaction with AI. To ensure engagement, participants answer an attention check question about the feedback before proceeding. Study 2 consists of 100 cases, 50 randomly sampled from Study 1 and 50 new cases from a separate test pool. Different from Study 1, Al predictions and annotations are shown upfront, and participants either accept the AI diagnosis or make modifications (Human+AI condition for Study 2).\nBoth studies conclude with an exit survey."}, {"title": "3.5 Metrics and Statistical Testing Methods", "content": "Patient level metrics. We evaluate the performance using AUROC, accuracy, sensitivity/recall, specificity, negative predictive value (NPV), and positive predictive value (PPV)/precision, based on the predictions of Cancer vs. Non-Cancer for each case. NPV is the proportion of cases predicted as Non-Cancer that are correctly classified. PPV/precision is the proportion of cases predicted as Cancer that are truly cancerous.\nLesion level metrics. Note that lesion-level analysis focuses only on identified lesions (i.e., no true negatives), only accuracy, sensitivity, and PPV can be calculated at that level. Prostate MRI consists of 3-D images, where lesions may span across multiple slices (images). For each 3-D connected lesion, we calculate lesion-level hits or misses based on a 10% overlap between predicted annotations vs. groundtruth annotations, for both AI and human alike.\nStatistical testing methods. We perform bootstrapped z-tests on the mean differences of metrics. For each condition, bootstrapping is conducted by resampling with replacement over 10,000 itera-tions, using a sample size of 400 for population-level analysis and 50 for participant-level analysis."}, {"title": "4 Results", "content": "We organize our findings into two parts: 1) the effect of AI assistance on the performance of human-Al decision making; 2) how AI assistance changes behavioral patterns such as reliance and decision efficiency. Overall, for (1), we observe a performance trend in order of Human alone < Human+AI < AI, with occasional instances of individual radiologists achieving complementary performance. It is also worth noting the ensemble of human+AI could outperform AI, i.e., complementary performance. For (2), we find that the different workflow does not significantly impact human performance. Radiologists are generally reluctant to adopt AI suggestions after making their own diagnosis. In contrast, providing upfront AI input increases the adoption of AI advice among experts. However, under-reliance on AI persists, preventing human+AI team from achieving complementary performance."}, {"title": "4.1 Performance of Human vs. Al vs. Human+Al Team (Q1)", "content": "We evaluate both the baseline performance of humans and their performance after receiving AI assistance. Table 1 presents an overview of performance metrics from both studies, including per-patient and per-lesion results.\nHuman-alone < AI. The workflow of Study 1 allows us to compare the baseline performance of humans and AI on the same set of patient cases. As shown in Table 1, AI consistently outperforms humans across most metrics, with statistically significant advantages in AUROC, accuracy, specificity, and PPV/precision(p < 0.05). At the lesion level, the AI also shows significant gains in accuracy, sensitivity, and PPV. Moreover, we find that for identified positive lesions, AI is less likely to miss the biopsy confirmed lesions, compared with human radiologists. Fig. 3 provides an example of this. These findings suggest that the AI is better than human radiologists in predicting csPCa, especially in identifying true negative cases and true positive lesions.\nHuman-alone < Human+AI. In Study 1, human+AI outperformed human radiologists alone, with statistical significance in AUROC, accuracy, specificity, and PPV/precision (p < 0.05), as shown in Table 1. This highlights the potential positive utility of AI assistance.\nWhile study 2 did not include a direct human-alone baseline, we conducted two statistical analysis to evaluate the impact of AI assistance. First, we performed an unpaired statistical test, comparing human-alone performance from Study 1 (75 cases) against human+AI performance from Study 2 (100 cases). This analysis shows statistically significant improvements in both AUROC and accuracy, from Table 1. Second, to further validate these findings with a common set of patient cases, we investigate specifically the 50 common cases shared between both studies to perform a paired statistical analysis. By referencing the human-alone performance from Study 1 on these exact same cases, we found that human+AI outperformed human-alone in both studies, as shown in Table 2."}, {"title": "Can Domain Experts Rely on Al Appropriately? A Case Study on Al-Assisted Prostate Cancer MRI Diagnosis", "content": "Human+AI < AI. Although the Human + AI team outperforms humans alone, it consistently underperforms AI alone in AUROC, accuracy, specificity, and PPV/precision (p < 0.05) in Study 2, while showing no significant evidence of inferiority to AI in Study 1. This trend becomes more salient when focusing on the common 50-case subset, as shown in Table 2, where all metrics except specificity show statistically significant differences in both studies. This is somewhat justified, as human radiologists in practice tend to be more cautious to avoid missing any suspicious cases (i.e., identifying true negative cases). They are inclined to send suspicious cases for biopsy. For lesion level analysis, it is more prominent that AI outperformed Human+AI in identifying positive lesions, with statistical significance in accuracy, sensitivity, and precision in Study 1.\nIndividual human radiologists can occasionally achieve complementary performance. In the common cases between Study 1 and Study 2, we evaluate individual radiologists and AI-assisted radiologists against AI model using both receiver operating characteristic (ROC) and precision-recall (PR) curves. As shown in Fig. 4, and consistent with prior discussions, the AI curve generally outperforms individual radiologists (represented by blue dots). Additionally, AI-assisted radiologists in both studies (red and orange dots) are generally positioned above individual radiologists (blue dots) in both figures, indicating that AI assistance helps improve radiologists' performance. We highlight that there are cases where AI-assisted radiologists outperform the AI curve, as shown by the red and orange dots above the AI curve. This is a promising finding as it suggests that AI assistance could augment human to achieve complementary performance (Human+AI > human and Human+AI > AI).\nEnsemble of human outperforms human but not AI, ensemble of Human+AI could outperform AI. We compiled an ensemble of results from the human radiologists' predictions in Table 3 and Fig. 5. For each test case, we do a majority vote among the predictions from the eight ra-diologists. If there is a tie among the radiologists, i.e. four cancer predictions versus four non-cancer predictions), we calculate the weighted prediction based on the radiologists' reported confidence."}, {"title": "4.2 Behavioral Analysis on Human-Al collaboration (Q2)", "content": "We now focus on the impact of different interventions, specifically the effect of performance feedback in Study 2 and the effect of providing AI assistance after humans have made their decisions."}, {"title": "Can Domain Experts Rely on Al Appropriately? A Case Study on Al-Assisted Prostate Cancer MRI Diagnosis", "content": "The different workflow does not significantly change human performance \u2013 comparison of common-50 subset results of study 1 and 2. In study 2, we share with each participant their own individual performance, the AI's performance, and their performance after reviewing AI predictions. A sample screenshot of the performance feedback provided to an individual radiologist is shown in Fig. 2c. To ensure radiologists understood their relative performance compared to the AI and whether AI assistance improved their results from Study 1, they were required to answer an attention check question before proceeding with the study. We investigate how this performance feedback affects human decision making behavior, particularly whether they tended to incorporate AI advice more, less, or without significant change. By learning about their past performance, the Al's performance, and the previous Human+AI team performance, radiologists were better informed before making new decisions in Study 2.\nWe hypothesized that radiologists would adjust their trust and reliance on AI if they realized that AI was more accurate overall. To test this, we analyze the performance of the 50 common test cases across study 1 and study 2. Despite the introduction of performance feedback, Human+AI team still does not surpass AI alone and achieves results that are relatively similar to or only slightly better than Human+AI in Study 1. Moreover, there is no statistical significance in any of the metrics comparing Human+AI (Study 2) with Human+AI (Study 1). As none of the metrics showed statistical significance, we defer the full details of the common-set results to Appendix Table 13. In conclusion, our findings suggest that performance feedback did not lead to significant improvements in the Human+AI accuracy.\nRadiologists are reluctant in adopting AI assistance after they made their own independent diagnosis. In Study 1, radiologists first make diagnostic decisions before being shown the AI's predictions. This allows us to observe how likely they are to incorporate AI suggestions. The results indicate that radiologists tend to maintain their initial diagnostic decisions even when presented with contradicting AI predictions. From Fig. 6a, the initial agreement between human and AI is about 52.4 (69.9%) vs. 22.6 (30.1%). For 52.4 cases (initial agreement), human rarely changes their decision as their decision is confirmed by AI. When the AI disagrees with their initial assessment (22.6/75 average cases), radiologists change their diagnosis in only 4.6 (20.4%) of cases. This reluctance to revise initial decisions persists even in cases where their own accuracy is low (44.4%), suggesting a significant barrier to incorporating AI assistance.\nUpfront AI input and performance feedback increase AI adoption. In Study 2, performance feedback was shown to human radiologists at the very beginning of the study to help them gain a"}, {"title": "Can Domain Experts Rely on Al Appropriately? A Case Study on Al-Assisted Prostate Cancer MRI Diagnosis", "content": "is an update of Human-alone, its recorded time includes the entire decision pro-cess from Human-alone. To mitigate outliers, we focus on median times: 123.11s for Human-alone,\nPerformance of Human ensemble is significantly improved over Human-alone, especially with precision/PPV increasing from 44.7% to 48.7% (4%) and specificity rising from 56.5% to 61.5% (5%). This improvement closes the gap between humans and AI. Moreover, Human+AI-ensemble has the highest performance among all conditions, gaining significantly better AUROC (0.771), accuracy (73.3%), and precision/PPV (54.1%) than AI. Sensitivity also reaches 87.0%, indicating a strong per-formance. This suggests that, with the help of AI, a group of experts can surpass either themselves or AI, achieving complementary performance."}, {"title": "5 Conclusion", "content": "While there is a growing interest in evaluating AI assistance with human decision makers, only a handful of previous works have attempted to evaluate AI systems directly with domain experts, and even fewer have achieved complementary performance or investigated human behavior. We contribute a comprehensive study with domain experts about how a clinical AI tools might be integrated in practice with two realistic design of workflows. Our findings suggest that while human-AI teams consistently outperform humans alone, they still underperform compared to AI due to under-reliance. More importantly, we look beyond accuracy and investigate human behavioral patterns in human-AI interaction. Even when domain experts are informed about their performance, the gap to AI performance, and their previous AI-assisted performance, it remains challenging for them to effectively calibrate their reliance and trust in AI tools. While complementary performance falls short in our work\u2014as in previous works\u2014our results on the ensemble performance of human-AI teams are promising. This highlights exciting opportunities to improve human-AI decision-making."}, {"title": "Can Domain Experts Rely on Al Appropriately? A Case Study on Al-Assisted Prostate Cancer MRI Diagnosis", "content": "Limitations. Several issues remain unresolved and present opportunities for future research. While our study show that upfront AI assistance can encourage greater adoption among radiologists, it remains unclear what factors positively contribute to complementary performance. Additionally, our research is limited to particular clinical setting and disease, which may not be generalizable to other domains or environments. Despite these limitations, we hope that our study will inspire and support the broader research community to further investigate the complexities of human-AI decision-making in relevant real-world tasks."}]}