{"title": "Distributed Stackelberg Strategies in State-based Potential Games for Autonomous Decentralized Learning Manufacturing Systems", "authors": ["Steve Yuwono", "Dorothea Schwung", "Andreas Schwung"], "abstract": "This article describes a novel game structure for autonomously optimizing decentralized manufacturing systems with multi-objective optimization challenges, namely Distributed Stackelberg Strategies in State-Based Potential Games (DS2-SbPG). DS2-SbPG integrates potential games and Stackelberg games, which improves the cooperative trade-off capabilities of potential games and the multi-objective optimization handling by Stackelberg games. Notably, all training procedures remain conducted in a fully distributed manner. DS2-SbPG offers a promising solution to finding optimal trade-offs between objectives by eliminating the complexities of setting up combined objective optimization functions for individual players in self-learning domains, particularly in real-world industrial settings with diverse and numerous objectives between the sub-systems. We further prove that DS2-SbPG constitutes a dynamic potential game that results in corresponding converge guarantees. Experimental validation conducted on a laboratory-scale testbed highlights the efficacy of DS2-SbPG and its two variants, such as DS2-SbPG for single-leader-follower and Stack DS2-SbPG for multi-leader-follower. The results show significant reductions in power consumption and improvements in overall performance, which signals the potential of DS2-SbPG in real-world applications.", "sections": [{"title": "I. INTRODUCTION", "content": "In modern manufacturing systems, the integration of Artificial Intelligence (AI), Internet of Things (IoT), and Cyber-Physical Systems (CPS) technologies has revolutionized operational efficiency by enabling functions like fault tolerance [1], self-optimization [2], and anomaly detection [3]. Modern systems prioritize adaptability and flexibility to extend productivity, leading to modular production units with hardware and software modularity controlled by decentralized control systems [4], [5]. Consequently, such evolution results in the necessity of developing distributed optimization methodologies, which assign systems to adjust to fluctuating demands within flexible manufacturing environments dynamically. Modular manufacturing systems are often considered distributed multi-agent systems (MAS) [6], where each sub-system has multiple distinct objectives. This poses challenges in determining the main focus of each agent and its local objective function to be aligned with the global system's objectives. This complexity becomes even more pronounced with a large number of agents with diverse and conflicting objectives, as found in large-scale manufacturing systems [7].\nVarious machine learning methodologies have been explored for achieving distributed self-optimization in MAS, with multi-agent reinforcement learning (MARL) [8] standing out due to its adaptability. MARL has found applications in routing solutions for mobile sensor networks [9], robotics [10], [11], resource management for unmanned aerial vehicles [12], [13], and cyber security [14]. Despite its successes, recent implementations have revealed limitations in real-world scenarios. Firstly, most MARL approaches operate in a centralized instance [15]\u2013[17], which leads to communication overhead and scalability issues for large-scale systems [18]. Secondly, the computational resources of MARL required are often beyond what is available in real industrial settings [19]. Thirdly, MARL typically requires extensive data and training time to converge [19]. Fourthly, MARL restricts dynamic interaction between agents due to its limitation to the Markov Decision Process (MDP) framework [20]. Additionally, coordination problems between agents are not well-addressed in MARL. Lastly, managing diverse multi-objectives across subsystems leads to complexity in defining their reward functions, which requires careful consideration for optimal trade-offs.\nIn recent research, we addressed the limitations of MARL by introducing a dynamic state-based game theoretical (GT) approach integrating model-free [21], [22] and model-based [7], [23] learning principles. Our methodology follows cooperative potential games [24]\u2013[26], which enables agents to collaborate effectively while maintaining fully distributed optimization methods. State-based Potential Games (SbPGs) [21] facilitate coordination among agents in self-learning modular production units which, consistently converge to Nash equilibrium and outperform MARL. However, challenges in handling diverse multi-objectives across subsystems remain unsolved. In [7], [21]\u2013[23], we employed a weighted utility function to prioritize critical objectives but acknowledge this may not be the most optimized solution, where determining the weights and designing utility functions remains difficult, with no optimality guarantees. Automated parameter tuning through grid search [27] was used but was time-consuming and may not guarantee optimality. Our current study aims to develop a solution where each player manages objectives independently without concern for inter-objective weights while retaining the effectiveness of SbPGs.\nGT includes various structures and strategies [28], [29], including the Stackelberg strategy [30], characterized by a hierarchical order of play with a leader-and-follower dynamic. In Stackelberg games, decision-making unfolds sequentially, which allows for explicit modelling of strategic interactions' hierarchical nature and potentially more efficient outcomes compared to simultaneous-move games. These games implicitly prioritize objectives by designating leaders and followers, which aims for optimality at the Stackelberg equilibrium [31], [32]. In our research, we propose a method to incorporate Stackelberg strategies in SbPGs within each player, which assigns roles based on different objectives to simplify defining utility functions for multi-objective optimizations. Our goal is to ensure convergence of the proposed approach while satisfying the principles of both potential games and Stackelberg equilibrium.\nHere are the contributions of our paper:\n\u2022 We introduce a novel approach, namely distributed Stackelberg strategies in SbPG (DS2-SbPG), to address the complexities of multi-objective optimization in distributed settings.\n\u2022 We present and analyze two DS2-SbPG variants, which are one with single-leader-follower dynamics and another, Stack DS2-SbPG, for scenarios with multiple leaders-followers. Additionally, we propose and validate an improved learning algorithm for optimizing the leader-follower dynamics in DS2-SbPG.\n\u2022 We demonstrate their effectiveness by combining potential games and Stackelberg strategies while proving their convergence guarantees.\n\u2022 We validate the practical utility through implementation in a laboratory test-scale system, enhancing overall system performance and reducing power consumption by 10.04% by DS2-SbPG and 10.61% by Stack DS2-SbPG compared to the standard SbPG.\nThis paper contains seven sections. Sec. II provides a literature review. Sec. III describes the problem. In Sec. IV, we discuss preliminary game structures. Sec. V elaborates on the proposed DS2-SbPG framework, including its variants, learning algorithm, and proof of convergence. Sec. VI outlines the training setup and experimental findings. Finally, Sec. VII summarizes the paper and annotations for future research."}, {"title": "II. LITERATURE REVIEW", "content": "In this section, we review the literature on multi-objective optimizations, decentralized learning manufacturing systems, and dynamic GT with engineering applications."}, {"title": "A. Multi-Objective Optimizations", "content": "Multi-objective optimization involves simultaneously optimizing multiple conflicting objectives, often represented mathematically as the minimization or maximization of multiple objective functions [33], [34]. This field contains diverse mathematical models and algorithms aimed at finding trade-off solutions along the Pareto frontier [35], [36], where improving one objective results in the degradation of another. Its applications span various domains [37], e.g. finance, logistics, and resource allocation, where decision-makers must balance competing objectives. In modern control technology, state-of-the-art methods in multi-objective optimization employ advanced algorithms to efficiently explore the solution space and identify Pareto-optimal solutions [35], [36], like genetic algorithms, particle swarm optimization, and evolutionary strategies.\nAnother subset of multi-objective optimizations is constrained optimization [38], which introduces additional constraints alongside objective functions that require specialized algorithms for finding solutions, e.g. SAT (satisfiability) methods [39]. While multi-objective optimizations offer decision-makers a range of trade-off solutions, they also present challenges [40], such as difficulty in determining objective weights and the absence of a single optimal solution that simultaneously optimizes all objectives. In the self-learning domain, multi-objective optimization is applied in defining objective functions guiding the learning process, for instance, reward functions in deep MARL [41], [42] and utility functions in dynamic GT [7], [22]. Previous approaches often aggregated multiple objectives into a single objective function with weighted objectives, which poses challenges in accurately defining these weights [40]. In [43], a potential solution is introduced that utilises neural networks to learn the weights of optimization objectives, serving as an inductive bias. Meanwhile, in our research, we propose an approach that maintains each objective function independently during the learning process, which allows decision-makers to navigate trade-offs internally through Stackelberg strategies."}, {"title": "B. Decentralized Learning Manufacturing Systems", "content": "Decentralized learning in manufacturing systems [44], [45] involves distributed knowledge acquisition and performance enhancement across multiple entities. This approach, often viewed as MAS [6], finds applications in production scheduling [46], resource allocation [47], and predictive maintenance [48]. Decentralized learning in MAS enhances adaptability, resilience, and efficiency in complex manufacturing environments by enabling agents to learn and decide based on local information autonomously. This enables the system to respond effectively to dynamic changes and uncertainties. In self-learning domains, state-of-the-art approaches contain methodologies like MARL [8], dynamic GT [28], [29], federated learning [49], and swarm intelligence [50]. These methodologies enable agents in MAS to learn and adapt their strategies over time, which leads to improved system performance and productivity. Our prior research [7], [22], [23] suggests that dynamic GT is more proficient and applicable in self-learning distributed MAS compared to MARL [2] and model predictive controller [51], as it facilitates coordination among agents that boosts cooperative strategies to address competing and diverse multi-objectives. Therefore, this study opts for dynamic GT for decentralized self-learning in MAS."}, {"title": "C. Dynamic GT with Engineering Applications", "content": "GT [28], [29] is a mathematical framework used to model and analyze interactions between rational decision-makers, known as players, who aim to maximize their utility or payoff. Various game structures in GT represent different scenarios and dynamics of strategic interactions, such as competition, cooperation, or coordination. Examples include potential games [24], Stackelberg games [30], Colonel Blotto games [52], and congestion games [53]. Furthermore, dynamic GT [54] extends traditional GT to analyze situations where players' decisions evolve over time, which captures the sequential nature of actions and the feedback loop between decisions and outcomes. In engineering, dynamic GT finds various applications, for instance, in resource allocation [55], edge computing on unmanned aerial vehicles [56], and optimization of manufacturing processes [22].\nGT approaches are particularly valuable in distributed self-learning MAS, where agents interact and make adaptive decisions based on local information, as in [7], [22], [23]. A notable advancement is the incorporation of state-based systems into game structures, which leads to SbPGs [21]. In recent years, SbPGs have been augmented with advancements in model-based learning [7], [23]. However, in multi-objective optimizations, determining the optimal combination of competing objectives remains challenging. Stackelberg games [30], with their hierarchical decision-making structure, aim for optimality at the Stackelberg equilibrium [31], [32] and have proven effective in various engineering fields, like in power control communications [57] and security issues [58], [59]. Hence, in this study, we propose improving SbPG by enabling each player to employ a Stackelberg strategy internally to achieve consensus on diverse conflicting multi-objectives."}, {"title": "III. PROBLEM DESCRIPTIONS", "content": "This section provides the problem description in this study, such as autonomous learning within fully distributed manufacturing systems, where we focus on modular systems divided into several subsystems, each with its own local control system and diverse multi-objectives, as shown in Fig. 1. Each sub-system includes one or more actuators, whereas, in GT terms, a controllable actuator can be considered as a player i. Our primary goal is to achieve self-optimization of these systems in a fully distributed manner, which eliminates the necessity for a centralized control instance as well as allows for scalable, flexible, and generally reusable execution across various modules through instantiations.\nThe considered distributed system has been represented to a production chain in [21] based on graph theory [60], where we model the production chain in both serial and serial-parallel process chains as an alternating sequence of actuators (e.g., motors, conveyors, feeders, pumps) and physical states representing the process status, as shown in Fig. 2. These actuators are expected to demonstrate a hybrid actuation system with both continuous and discrete operational behaviours. The production chain is defined as a dynamic sequence involving actuators N = 1,...,N with sets of continuous or discrete action $A \\subset \\mathbb{R}^C \\times \\mathbb{N}^d$ and a group of states $S \\subset \\mathbb{R}^m$. Then, $\\mathscr{E}$ denotes the edges, excludes edges of the condition $e = (A_i,A_j)$ and $e = (S_i,S_j)$ with $A_i, A_j \\in N$ and $S_i, S_j \\in S$. For each actuator $A_i \\in N$, two neighbouring states are introduced, including foregoing neighbor states $S_{prior} = \\{s_j \\in S | \\exists e = (s_j,A_i) \\in \\mathscr{E}\\}$ and following neighbor states $S_{next} = \\{s_j \\in S | \\exists e = (A_i,s_j) \\in \\mathscr{E}\\}$.\nThe assumption of the production chain consisting solely of sequences of states and actions is not restrictive. An arrangement involving more than two states can be reconstructed into a common state vector, and similar considerations apply to actions. Decentralized production lines are common in modern industrial environments. This distributed production scenario has applications across various fields in the process industry, including transportation processes of coal and grains, chemical plants, food production and pharmaceuticals.\nWe assume that each player i has a local utility $U_i(a_i, S_{A_i})$ where $S'_{A_i} \\in S_{A_i} = S_{prior} \\cup S_{next} \\cup S_\\theta$, $S_\\theta$ representing the states tied to global objectives. The local utility $U_i(a_i, S_{A_i})$ is assumed to consist of several, individual sub-objectives, $(u_1^{(i)}, u_2^{(i)}, ..., u_k^{(i)})$, with each utility function representing a different aspect of the problem. In terms of production systems, this set of objectives for individual players i includes e.g. satisfying production demand, minimizing power consumption, avoiding bottlenecks and satisfying production constraints.\nConsequently, the global objective is this work is to maximize the overall system utility\n$\\max_{a_i \\in A_i} U_i(a,S),$\nby jointly maximizing local utilities\n$\\max_{a_i \\in A_i} (U_1^{(i)}(S_i, A_i), U_2^{(i)}(S_i, A_i), ..., U_k^{(i)}(S_i, A_i)),$\nwhere k denotes the number of objective functions to be optimized.\nIn prior research [7], [21], [22], the multiple objectives are combined into a single function\n$U_i(S_i, a_i) = w_1 u_1^{(i)}(S_i, a_i) + w_2 u_2^{(i)}(s_i, a_i) + ... + w_k u_k^{(i)}(S_i, a_i),$\nwhere $a_i \\in A_i$ and the weight parameter $w_k$ indicates the importance allocated to objective k. However, determining appropriate $w_k$ proves to be a non-trivial effort due to its high sensitivity to the direction of the learners, which makes it difficult to guarantee the optimal solutions for the controlled systems.\nFurthermore, in most production environments, the local objective might have a different priority level, for instance, satisfying production demand is typically a primary objective, while reducing energy consumption serves as a secondary objective. Hence, the local objectives within each player i must be suitably combined to maximize the global objective while considering the priority level. Achieving this alignment can be challenging due to potential conflicts among the local objectives and issues related to prioritization.\nTwo major challenges arise from the discussed problem descriptions. The first challenge is managing conflicting and diverse utility functions between players to optimize the global objectives. It is important to ensure that all players cooperate rather than act selfishly, a problem handled by SbPG [21], which has been proven to converge. The second challenge is managing the multi-objectives within each local objective, where the previously used weighted method cannot guarantee optimal solutions. In this study, we address prioritization among local objectives by employing leader-follower games, assigning objectives as either leader or follower to effectively manage their prioritization. We focus on the learning process within each player i and employ Stackelberg strategies internally to manage trade-offs between objectives while maintaining SbPG as the primary game structure for interactions between players. This approach results in a combination of distributed Stackelberg strategies and SbPGs."}, {"title": "IV. PRELIMINARY GAME STRUCTURES", "content": "This section focuses on two foundational game structures, namely SbPGs and Stackelberg games, which form the basis of our proposed game structure."}, {"title": "A. State-based Potential Games", "content": "Potential games model and analyze strategic interactions among rational agents where players' payoffs (utilities) depend on their actions and the environment's state. These interactions are evaluated using a scalar potential function $\\phi$, a global objective function. Each player's utility function $U_i$ is impacted by the overall system's states rather than individual actions $a_i$. SbPGs [25] extend potential games [24] by incorporating state information into strategic interactions.\nIn [21], the game is extended for self-optimizing modular production units and solving multi-objective optimization problems by incorporating the set of states S and the state transition process P. Here is the formal definition of a SbPG [21]:\nDefinition 1. A game $\\Gamma(N, A, \\{U_i\\}, S, P, \\phi)$ constitutes an SbPG, when it ensures that the potential function $\\phi: a \\times S \\rightarrow \\mathbb{R}$ within the game and each pairing of action-state $[a,s] \\in A \\times S$ meets the following requirements:\n$U_i(a_i,s) - U_i(a'_i, a_{-i},s) = \\phi(a_i,s) - \\phi(a'_i, a_{-i},s),$\nand\n$\\phi(a_i,s') \\geq \\phi(a_i,s),$\nfor any state s' in P(a,s), where $R_i$ characterises continuous actions.\nA notable characteristic of (exact) potential games is their convergence to Nash equilibrium points under best-response dynamics [25]. In [26], the criteria of being an SbPG are demonstrated, where the convergence is sustained. This convergence is further proven for self-optimizing modular production units in [21]. In this study, we maintain SbPGs as the primary game structure to facilitate cooperative interactions among players. However, SbPGs do not guarantee effective management of multi-objectives for each player individually. As a result, we integrate a strategy to manage these objectives distributively using a hierarchical Stackelberg structure."}, {"title": "B. Stackelberg Games", "content": "Stackelberg games [30] are a GT framework that explores hierarchical interactions among rational players, unlike most GT scenarios where players act simultaneously. There are two roles in this game: leader and follower. The leader can pre-commit to a strategy, in which the follower subsequently responds to the leader's actions. This hierarchical form is often suitable for a non-cooperative game framework but can also be adapted for a cooperative game. Optimization methods such as dynamic programming, variational inequalities, or Stackelberg equilibrium concepts [31] are commonly employed to analyze and find equilibrium solutions in Stackelberg games. Here is the formal definition of a Stackelberg game [32]:\nDefinition 2. We consider a cooperative Stackelberg game between two players where Player 1 is the leader with an objective $U_1: A_1 \\rightarrow \\mathbb{R}$ and Player 2 is the follower with an objective $U_2: A_2 \\rightarrow \\mathbb{R}$. Their action spaces are defined as $A = A_1 X A_2 \\in \\mathbb{R}^m$ with $A_1 \\in \\mathbb{R}^{m_1}$ and $A_2 \\in \\mathbb{R}^{m_2}$. Thus, the leader and follower aim to solve the following optimization problem:\n$\\max_{a_1 \\in A_1} \\{U_1(a_1,a_2)|a_2 \\in \\arg \\max_{y \\in A_2} U_2(a_1,y)\\},$\n$\\max_{a_2 \\in A_2} U_2(a_1,a_2).$\nRemark 1. Definition 2 highlights its contrast to simultaneous games such as in SbPGs, where each player i faces the optimization problem $\\max_{a_i\\in A_i} U_i(a_i, a_{-i})$, leading to Eq. (3).\nThe sequential decision-making process in Stackelberg games establishes a hierarchical structure where the leader anticipates and influences the follower's actions. In most cases, the most critical objective within the set of multi-objectives assumes a higher hierarchical position, which acts as the leader objective. The final coalition decision $a_i$ is then derived by harmonizing the decisions of the leader and the follower objective. The leader aims to optimize its utility given the follower's response, while the follower plays the best response to the leader's actions. Hence, to manage multiple objectives without relying on weighted utility functions in SbPGs, we introduce the hierarchical approach of the Stackelberg strategy to each player."}, {"title": "V. DISTRIBUTED STACKELBERG STRATEGIES IN STATE-BASED POTENTIAL GAMES", "content": "This section introduces the fundamental and two variations of DS2-SbPG: (1) DS2-SbPG for single-leader-follower objective scenarios and (2) Stack DS2-SbPG for multi-leader-follower objective scenarios. We also detail the learning algorithm used to optimize the policies of leaders and followers, along with the training mechanism of DS2-SbPGs using this algorithm. Furthermore, we provide proof of convergence for the proposed DS2-SbPG and its variants. Fig. 3 provides an overview of SbPG and both DS2-SbPG variants in multi-objective optimization problems."}, {"title": "A. DS2-SbPG for Single-Leader-Follower Objective", "content": "We propose incorporating the Stackelberg strategy as an integral component for each player i in an SbPG. Using SbPG as the foundational game structure in DS2-SbPG ensures the applicability of this approach to production chains. Fig. 4 illustrates the structure of the distributed leader-follower game within a modular manufacturing unit. We design this game structure as DS2-SbPG that contains a single-leader-follower objective, defined as follows:\nDefinition 3. A game in DS2-SbPG is defined as $\\Gamma(N, A, S, P, \\{U_i, L_i, F_i\\}, \\phi)$, where $\\{a_1,a_2,...,a_N\\} \\subseteq A$. Each player i consists of a leader $L_i$ and a follower $F_i$, each with individual actions defined as follows:\n$a_L = \\pi_L(s_i),$\n$a_F = \\pi_F(s_i, a_L),$\nwhere $a_L, a_F \\in A_i$ and $\\pi_L$, $\\pi_F$ are the governing policies of the leader and follower. An action $a_i \\in A_i$ represents the coalition decision of $L_i$ and $F_i$, expressed as $a_i = a_L \\times a_F$. Correspondingly, each role has its own utility function, which transforms the overall utility function $U_i = U_i^L \\times U_i^F$.\nEach leader-follower objective pair is dedicated to optimizing a specific multi-objective function within a player i, with each leader-follower objective focusing on an individual objective function while preserving the convergence properties of both Stackelberg and potential games. Such convergences can be achieved through the proper selection of the governing policies $\\pi_L$ and $\\pi_F$, which are detailed later in Sec. V-C. In DS2-SbPG, leaders hold the strategic advantage of initiating decisions before followers react."}, {"title": "B. Stack DS2-SbPG for Multi-Leader-Follower Objective", "content": "In the second variant of DS2-SbPG, we extend the approach to accommodate strategies for multi-leader-follower objective scenarios, referred to as Stack DS2-SbPG. This variant is designed for multi-objective problems with more than two potentially conflicting objectives. In this game, each player i is no longer restricted to a single leader $L_i$ and a single follower $F_i$. Instead, we introduce a hierarchical order of the roles $H_i$ based on the priority of the objectives, as illustrated in Fig. 5. Here is the formal definition of Stack DS2-SbPG:\nDefinition 4. A Stack DS2-SbPG is defined as $\\Gamma(N, A, S, P, \\{U_i, H_i, g_i\\}, \\phi)$, where $H_i$ denotes the objective hierarchy and $g_i$ represents stacked Stackelberg games in player i. Here, $H_i = (h_1,h_2,...,h_k)$ is a permutation of the indices $\\{1,2,...,k\\}$, which indicates the order in the stacked games $g_i = (g_1,g_2,\u00b7\u00b7\u00b7,g_{k-1})$. In each $g$, two players involved in $h_z, h_{z+1}$ play a leader-follower game as $L$ and $F$. The leader $L$ first selects an action $a_L(s_i) \\in A_i$, see Eq. (8), while the follower F responds by selecting an action $a_F(s_i, a_L) \\in A_i$, see Eq. (9). The action $a_{i,z} \\in A_i$ represents the coalition strategy between the leader and follower, $a_{i,z} = a_L \\times a_F$. $a_{i,z}$ can then serve as the leader's action in the subsequent stacked game $a_{L,z+1}$, if $z \\neq k - 1$, or as the final action $a_i$, if $z = k - 1$.\nBased on its definition, Stack DS2-SbPG involves multiple leaders and followers engaging in hierarchical Stackelberg games g, with each role corresponding to a different objective. For instance, if there are five objectives, k = 5, for player i, there are five distinct roles ordered as $h_1,...,h_5$, participating in four stacked Stackelberg games. Additionally, the order of $H_i$ indicates the importance of objectives, arranged from most to least important. The advantage of utilising multiple leader-follower pairs is the ability to accommodate numerous objectives independently, without grouping. These objectives can be arranged based on their relative importance, which allows for a comprehensive optimization approach."}, {"title": "C. Learning Algorithm", "content": "After establishing the game structure", "21": "we introduced the best response learning within SbPGs", "61": "which guides the learning direction and facilitates smoother convergence towards global optima compared to random sampling. Both methods are developed only for simultaneous games. In contrast", "Maps": "We first consider the representation of the policy of each player i which needs to store the learned knowledge across states and actions. In SbPGs"}, {"61": ".", "21": ".", "follows": "n$a_i = \\sum_{l=1"}, {"map": "augmentation with additional dimensions or stacking. After evaluation", "Law": "We now turn our attention to the learning law used to update the above performance", "follows": "n$a_{L", "61": ".", "Games": "We need to approximate the utility functions of the leader and follower because"}, {"follows": "n$\\hat{U"}, {"DS2-SbPG": "We assume t is the time step of the system", "32": "."}, {"DS2-SbPG": "We assume t is the time step of the system, and each player i must update its action $a_{i,t}$ at each time step. First, at each t, each player i obtains the current states $s_{i,t}$ of the environment. In each z-Stackelberg game, g is triggered. In g, we focus on the two objectives $h_z$ and $h_{z+1}$ in the hierarchy $H_i$. If z = 1, the leader $L_i$ acts first by selecting an action $a^i_{L,t,z}(s_{i,t}) \\in A_i$. Then, the follower $F_i$ reacts by selecting an action $a^"}]}