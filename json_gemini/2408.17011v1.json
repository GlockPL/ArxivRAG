{"title": "Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities", "authors": ["Jutika Borah", "Kumaresh Sarmah", "Hidam Kumarjit"], "abstract": "Imaging techniques such as Chest X-rays, whole slide images, and optical coherence tomography serve as the initial screening and detection for a wide variety of medical pulmonary and ophthalmic conditions respectively. This paper investigates the intricacies of using pretrained deep convolutional neural networks with transfer learning across diverse medical imaging datasets with varying modalities for binary and multiclass classification. We conducted a comprehensive performance analysis with ten network architectures and model families each with pretraining and random initialization. Our finding showed that the use of pretrained models as fixed feature extractors yields poor performance irrespective of the datasets. Contrary, histopathology microscopy whole slide images have better performance. It is also found that deeper and more complex architectures did not necessarily result in the best performance. This observation implies that the improvements in ImageNet are not parallel to the medical imaging tasks. Within a medical domain, the performance of the network architectures varies within model families with shifts in datasets. This indicates that the performance of models within a specific modality may not be conclusive for another modality within the same domain. This study provides a deeper understanding of the applications of deep learning techniques in medical imaging and highlights the impact of pretrained networks across different medical imaging datasets under five different experimental settings.", "sections": [{"title": "1 Introduction", "content": "Clinical decision-making through diagnosis is a complicated process. Diagnostic processes usually encompass visual inspection of digital images and interpretation of the findings as per established protocols [10]. In medical imaging, disease diagnosis is a critical, and time-consuming process [16, 1], which relies on the knowledge, experience, and reasoning skills of an expert. Chest X-rays (CXRs), Optical Coherence Tomography (OCT), and Whole Slide Images (WSIs) are commonly investigated for diagnosis of thoracic and pulmonary diseases [20, 11, 13], ophthalmic diseases respectively. Recently, deep learning (DL) has shown the potential to automate the process of medical image interpretation for faster and more accurate healthcare delivery [6, 18, 4, 12, 19]. However, DL holds a few challenges such as it requires a huge amount of labeled training data for training in a supervised manner, whereas finding labeled data in the medical domain is hard. These challenges have prompted researchers to leverage the benefits of transfer learning (TL) in medical imaging as it has the ability to generalization and transfer information from prior experience to new conditions. DL typically suffers from insufficient data distribution, shifts during test time, and computation power. TL deals with these by utilizing the already learned knowledge without training a model from scratch. However, TL"}, {"title": "2 Materials and Methodology", "content": "This section gives brief details of the experimental study design of the proposed work."}, {"title": "2.1 Datasets", "content": "\u2022 Pneumonia CXR [8]: A total of 5,863 CXR images are present in the dataset grouped into Normal and Pneumonia classes. The dataset has three folders split into training, validation, and test sets. Class Imbalance exists in the dataset with more numbers of pneumonia images compared to normal images. The training set has a total of 5,247 images of which 3,906 images belong to pneumonia and 1,341 images of normal subjects. We grouped the images into separate training and validation datasets while keeping a set separately for testing with 234 normal images and 390 pneumonia images. The training set is now comprised of 4000 samples and the validation set is comprised of 1043 samples. These CXR datasets were collected from retrospective cohorts of pediatric patients from Guangzhou Women and Children's Medical Center, Guangzhou.\n\u2022 OCT [8]: This dataset contained 1,09,309 OCT images with separate folders for training and test sets for independent patients. Each folder has four classes of images, viz: choroidal neovascularization (CNV), diabetic macular edema (DME), DRUSEN, and NORMAL. The training set contained 1,08,309 OCT images (37,205 CNV, 11,348 DME, 8,616 DRUSEN, and 51,140 NORMAL), whereas the test set had 1,000 images (250 images for each class). The structure of the dataset and labeling of the images were given in order of disease type, randomized patient ID, and image number of the patient. A validation set was not available in the original dataset. So, we have generated a validation set by splitting the training set in the ratio of 80%-20%. Thus, our modified dataset contains 86647, 21662, and 1000 images in training, validation, and test sets respectively.\n\u2022 LC25000 [2]: LC25000 is a histopathology image pathology with 25,000 color images in 5 classes. The 5 classes are divided into separate subfolder folders each containing 5,000 images of histologic entities namely, lung adenocarcinoma, lung benign tissue, lung squamous cell carcinoma, colon adenocarcinoma, and benign colonic tissue. The images are publicly available and are de-identified, HIPAA compliant, and validated. The images were of sizes of 768 x 768 pixels in jpeg file format. For this experiment, we have taken only lung carcinoma cases and discarded the colon adenocarcinoma cases. The dataset has been split into train, validation, and test sets. The training set contains 9600 histopathology lung adenocarcinoma samples, 1680 samples in the validation set, and 720 samples in the test set."}, {"title": "2.2 Data Preparation and Preprocessing", "content": "In the pre-processing stage, the images in the datasets are resized as the images are of varying sizes due to computational constraints as it would be difficult for the networks to train optimally. And so, resizing ensure that all the images are under the same dimension and that these can be fed into the neural network for training. In a nutshell, huge file sizes are computationally expensive, requiring larger power and memory sizes. Here, we resized the images of both datasets to 256 \u00d7 256 \u00d7 3 with no aspect ratio preservation. Then, the pixels of all the resized images are normalized to [0, 1]. A few examples from the three datasets are shown in Figure 1."}, {"title": "2.3 Model Architectures", "content": "We used ten widely used pretrained DCNN architectures: VGG (16, 19), ResNet (50, 101, 152), Inceptionv3, Inception-ResNet-v2, Xception, and DenseNet (121, 201). To assess the effectiveness of these models on the two different imaging datasets for binary and multiclass classification tasks. To obtain a comprehensive understanding of the model's performance, we"}, {"title": "3 Experimental Settings, Training Strategies and Performance Evaluation", "content": "We designed an experimental framework for training and evaluation of the performance of different DCNN models on CXR, OCT, and WSI datasets for automatic classification and interpretation of diseases. Figure 3 shows a visual representation of the overview of the different experimental and evaluation settings under study.\nA. Settings with pretraining:\n\u2022 Pretrained_1(freeze): Here, we keep all the CNN layers as fixed feature extractors and fine-tune them with our chest X-ray dataset. This is done for all the ten pretrained models with their corresponding pretrained ImageNet weights.\n\u2022 Pretrained_2(top_layers): In this setting, we fine-tuned the top layers of the networks and trained them on our chest X-ray dataset while freezing the bottom layers. Here, training for all the pretrained models is accomplished with their respective pretrained weights.\n\u2022 Pretrained_3(all_layers): In this setting, we retrain all the layers of the ten pretrained models with their corresponding pretrained ImageNet weights on our chest X-ray image data.\nB. Settings with random initialization:\n\u2022 Random_1(top_layers): In this setting, we fine-tuned the top layers of the networks and trained them on our chest X-ray dataset while freezing the bottom layers. Here, training for all the pretrained models is accomplished with random weights initialization during training.\n\u2022 Random_2(all layers): In this setting, we fine-tuned the top layers of the networks and trained them on our chest X-ray dataset while freezing the bottom layers. Here, training for all the pretrained models is accomplished with their respective pretrained weights."}, {"title": "4 Experimental Results and Analysis", "content": "Figure 3, shows different experimental settings and study designs of our work. The experimental results for average CXR AUCs, OCT AUCs, and WSI AUCs each with pretraining and random initializations are visually depicted through scatter plots in Figure 3 (a), (b), and (c). The scatter plot clearly illustrates the performance difference in AUCs relative to the number of parameters of the models representing their complexity (refer Table 1). This comparison is made across five experimental settings with three independent datasets with varying modalities. The precision, sensitivity, and specificity values for each independent dataset are shown in Table 2, 3, 4. We evaluated the models' performances under three evaluation scenarios: (i) with pretraining and random initialization; (ii) with pretrained DCNN architectures, and (iii) with DCNN architectural family. In the analysis 95% confidence interval (CI) value quantifies the observed differences in performance for significant assessment of reliability."}, {"title": "4.1 Evaluation with Pretraining and Random Initialization", "content": "From our observation comparing the impact of pretraining and random initialization we found pretraining helps increase the performance of the models while for some models, random initial-izations performed better. For CXR Figure 3 (a), VGG16, performance has been boosted with pretraining in training all layers of the network with an AUC 0.9916 followed by DenseNet121 with an AUC 0.9860. In the case of random initialization VGG16 and DenseNet121 have achieved AUC 0.9824, and 0.9828 respectively. A difference of only 0.0032 is observed under pretraining and random initialization. For VGG16 a small difference in AUC of 0.0209 (95%\nCI: 0.0139, 0.0279) was observed with pretraining and random initialization in training only the top layers of the networks. For VGG19 a difference of 0.0250 (95% CI: 0.0061, 0.0374) was observed. For ResNet models, pretraining and random initialization showed small differences in performances. In pretraining ResNet152 gained the highest performance, while in random initialization ResNet50 gained performance. For ResNet152, a difference of 0.239 (95% CI: 0.0129, 0.0608) was seen between pretraining and random initialization in training all layers of the networks. For the rest of the models, the performance in AUC with pretraining and random initializations is comparable under different settings.\nIn the case of OCT multiclass classification Figure 3 (b), it is observed pretraining helps increase the performance of DenseNet121, while random initialization helps DenseNet201. In-ceptionResNetV2 obtained the highest performance in training the network with only the top layers of the networks. In random initialization, VGG16 and DenseNet201 have a boost in performance with an AUC of 0.9437, and 0.9534 with DeneseNet201 obtaining the highest performance. Compared to training all layers of the networks, in random initialization, training"}, {"title": "4.2 Evaluation with Pretrained DCNN architectures", "content": "With pretraining, we observed that the largest and smallest models show slight differences in performance. For CXR DenseNet121, the smallest model has outperformed its larger peers. VGG16 with reduced parameters than VGG19 has better performance. When pretraining only the top layers of the networks, InceptionResNetv2 has performed better with an AUC of 0.9865\n(95% CI: 0.9688, 1.0044), (95%CI: 0.9663, 1.0069) and 0.9876 (95% CI: 0.9654, 1.0099). On\nthe other hand, with random initialization VGG19 and ResNet152 fell short with an AUC of\n0.9569 (95% CI: 0.9621, 0.9646) and 0.9634 (95% CI: 0.9621, 0.9646), respectively. Among the\nlargest and the smallest architectures, there is an increase of 0.0049 (95% CI: 0.0048, 0.0353),"}, {"title": "4.3 Evaluation with Pretrained DCNN architectural family", "content": "Here, the architectural family represents a model family with an increased number of param-eters such as VGG (VGG16, VGG19), ResNet (ResNet50, ResNet101, ResNet152), DenseNet(DenseNet121, DenseNet201). In the case of CXR Figure 3 (a), the VGG family, with pre-training VGG16 performed better with AUCs of 0.9876 (95% CI: 0.9781, 0.9971) and 0.9916(95% CI: 0.9897, 0.9935) in training top layers and all layers of the network respectively. Whilewith random initializations, VGG19 has performed better with an AUC of 0.9859 (95% CI:0.9688, 1.0031) in training all layers of the network. With ResNet families, ResNet152 showedsuperior performance with AUCs of 0.9855 (95% CI: 0.9798, 0.9913) and 0.9873 (95% CI:0.9492, 1.0254) with pretraining. Meanwhile, ResNet50 performed better with an AUC of0.9829 (95% CI: 0.9594, 1.0064) under random initializations, in training all layers of the net-works. Within the DenseNet family, DenseNet121 performed better with AUCs 0.9868 (95%CI: 0.9804, 0.9932), and 0.9860 (95% CI: 0.9663, 1.0057) with pretraining. DenseNet201, onthe other hand, demonstrated improved performance with AUCs of 0.9865.\nFor OCT Figure 3 (b), with pretraining ResNet family showed the least performance com-pared to other architectures. However, within the ResNet family, ResNet50 showed the highestperformance with an AUC of 0.9366 (95% CI: 0.9209, 0.9523) compared to two larger archi-tectural counterparts ResNet101, and ResNet152. However, ResNet101 achieved the highestperformance in training all network layers with an AUC of 0.9646 (95% CI: 0.9595, 0.9697) un-der pretraining. Within the highest-performing model, VGG16 gained the highest performance.DenseNet121, and DensNet201 have a performance difference of 0.0032 (95% CI: 0.0032, 0.0096)with pretraining, with DenseNet121 gaining the highest performance with an AUC 0.9622 (95%CI: 0.9539, 0.9706) in training top layers of the networks. With random initialization, VGG16attained the highest performance with an AUC of 0.9437 (95% CI: 0.9252, 0.9622) compared toVGG19. Among the ResNet family, in training all layers of the network, ResNet50 obtained thehighest performance with an AUC of 0.9015 (95% CI: 0.8954, 0.9077) and ResNet152 attainedthe least performance with an AUC of 0.8875 (95% CI: 0.8619, 0.9130). While DenseNet201achieved the highest performance with an AUC of 0.9534 (95% CI: 0.9407, 0.9660) comparedto DenseNet121."}, {"title": "5 Discussion", "content": "In this work, the use of different pretrained DCNNs on CXR, OCT, and histopathology WSI for binary and multiclass disease classification and their performance on these three specific datasets are carefully analyzed and reported. A general question that arises on using pretrained models with TL is whether performances on ImageNet remain consistent with domain shift with vary-ing medical datasets and imaging modalities. Another question is whether pretraining helps in CXR, OCT, and WSI interpretation. Pretraining on datasets like ImageNet can improve the models' performance. However, performance on medical imaging tasks varies depending on several factors including data type used for training, pretrained models, and downstream medical data. Domain shift also adds variability in prediction impacting the performance glob-ally. Improved performance can be achieved through fine-tuning with other settings. Careful fine-tuning is required when there is a difference between the pretrained data and downstream data. Transferring weights without any adaptation does not guarantee optimal performance on shifts.\nImprovement in pretraining will be pronounced in modalities where visual features are more or less complementary to those in natural images. The key to maximizing the benefits lies in careful adaptation and fine-tuning to specific medical imaging contexts. In the OCT dataset, there is a drop in models' performance with poor accuracies and AUCs. However, in the WSI dataset, the performance has been boosted significantly compared to CXR and OCT. Moreover, during pretraining with freeze layers, WSI performance has increased by 20% compared to the other two imaging modalities.\nThis observation indicates that for medical image classification, the performance of models varies significantly across modalities like CXR, OCT, and WSI. The reason behind this is that each modality provides inherent characteristics and information content. Additionally, each modality presents unique challenges that require tailored approaches in model design, with specific requirements of model parameters for training, and evaluation. The differences in structure with varying modalities are defined by anatomical regions like lungs and heart in CXR. The variability in CXRs can come from different patient positions, X-ray machines, and acquisition settings. OCT images with axial and lateral dimensions show layered structures of tissue, such as retinal layers. Variability in OCT can arise from different eye conditions, imaging angles, and devices, that affect the appearance of the retinal layers and structure. WSI with tiling contains highly detailed and complex information at the cellular level that has intricate patterns and subtle characteristics in cellular morphology. Biological variability at the cellular level is high, with different staining techniques, tumor heterogeneity, and varying tissue types. All these parameters lead to complexity or ease in model generalization. WSI images are usually high-resolution images and require tiling. These data contain highly detailed and complex information at the cellular level which requires the models to learn intricate patterns and subtle characteristics in cellular morphology. The scarcity of annotated data makes it even harder to train models. Models need to handle and integrate information from multiple"}, {"title": "6 Challenges And Future Prospects", "content": "Different modalities have vastly different image resolutions and sizes. Each imaging modality diagnoses different pathologies, so requires a versatile model to identify a wide range of con-ditions. A single model architecture that performs well across all imaging modalities poses significant challenges due to the diversity of data, its size, and the requirements that are spe-cific to each imaging task. If the training dataset includes real-world challenges and variations similar to those of clinical datasets, the models will be efficient at handling such challenges.\nConventional supervised learning is the most commonly used technique in machine learning ap-plications. Although training in a supervised manner is an integral part of building intelligent models, the transfer of knowledge between categories is an essential part of scaling to several added categories. Using TL instead of fine-tuning the entire model, selectively fine-tune certain layers of the networks that are more likely to capture domain-specific features. TL has solved the problem of training data insufficiency and time constraints. Most TL approaches rely on creating connections in embedding or labeling spaces between the source domains and the tar-get domains. Domain relevance ensures that the learned features apply to the target clinical tasks, improving performance and robustness against noise and variations. Also, the domain relevance of pre-training data helps to mitigate the effects of domain shift where source and target datasets differ significantly impacting transferability to real-world clinical applications.\nAppropriate transfer of knowledge can occur only when the source distribution and the target distribution share the same specific modalities. In recent times, insufficient medical training data and disparities within the same domain distribution have emerged as two of the most significant challenges in machine learning. TL has raised increased attention recently for its robustness to training efficacy and shifting resilience. Nevertheless, these algorithms suffer from performance degradation when there occur domain shifts (natural to medical). Inductive learning as an alternative for the acquisition of knowledge has been applied in TL where inference about the future instances is made on general patterns of the observed data, but the downside is that it is prone to noise with computation cost. Transductive learning is also"}, {"title": "7 Conclusion", "content": "From this study, we tried to find answers for the performances of DCNNs with TL for CXR, OCT, and WSI datasets with two initialization settings. The study analyzed the performance of various DL models when applied to different medical imaging classification tasks. The study found that within architectural families, the increase in parameters did not guarantee the highest performance. Furthermore, fine-tuning with different settings improved the performance of the models, but this may vary across datasets of the same modality. The results also showed that the performance of the models varied across different architectural families. This performance is not conclusive for interpretation in CT, MRI, USD, and mammogram images. These insights have important implications for the design and deployment of medical image classification systems. By understanding the conditions under which pretraining is most beneficial, researchers and practitioners can make more informed decisions about the use of transfer learning in their medical imaging applications. Further, exploration of these topics can help in advancing the field of medical image analysis and improve the performance of AI-powered clinical decision support systems. Expert knowledge integration i.e. incorporating expert annotations, medical atlases, or known anatomical structures into training will improve interpretability and performance."}]}