{"title": "Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI Refactoring", "authors": ["Markus Borg"], "abstract": "In the software industry, the drive to add new features often overshadows the need to improve existing code. Large Language Models (LLMs) offer a new approach to improving codebases at an unprecedented scale through AI-assisted refactoring. However, LLMs come with inherent risks such as braking changes and the introduction of security vulnerabilities. We advocate for encapsulating the interaction with the models in IDEs and validating refactoring attempts using trustworthy safeguards. However, equally important for the uptake of AI refactoring is research on trust development. In this position paper, we position our future work based on established models from research on human factors in automation. We outline action research within CodeScene on development of 1) novel LLM safeguards and 2) user interaction that conveys an appropriate level of trust. The industry collaboration enables large-scale repository analysis and A/B testing to continuously guide the design of our research interventions.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have quickly become an integral part of Software Engineering (SE) practice. JetBrain's annual developer survey 2023 revealed that already a year ago, 77% of developers use ChatGPT and 46% use GitHub Copilot [1]. While AI promises a coding revolution with tools to expedite code writing, we posit that the core challenge lies elsewhere. Developers spend most of their time understanding and maintaining existing code [2]. While high-quality code is known to improve productivity [3], reduce defect counts and vulnerabilities [4], and increase developer satisfaction [5], organizations routinely prioritize implementing new features over improving existing code [6]. Our premise is that there is substantial potential in applying LLMs to improve the maintainability of existing code we call this Al refactoring. Despite the potential of LLMs, they come with inherent risks. Research shows that LLMs' prompt-based generation of non-deterministic code poses several dangers [7], [8]. Within the context of AI refactoring, these risks manifest as potential security vulnerabilities and unexpected changes in functional behavior. Our own investigation into four function-level JavaScript code smells exposed that the raw output from state-of-the-art LLMs provides correct refactorings in only 37% of cases [9], a finding corroborated in recent work by Pomian et al. [10]. Furthermore, we have observed several instances where subtle defects were introduced. We advocate turning the large numbers of blunt refactoring recommenda-tions into a reduced set of sharp edits, effectively prioritizing precision over recall in line with recommendations from Google Research [11].\nTo address the unreliable nature of raw LLM output, we advocate for encapsulating the interaction with the models in IDEs. We discourage developers from refactoring code by copying solutions from LLM interactions in web browsers. Instead, AI refactoring should seamlessly integrate into IDES as pioneered by GitHub Copilot \u2013 and come equipped with safeguards to validate their output. We draw inspiration from our previous work on using safety cages to enable the safe use of deep learning in automotive systems [12]. In context of Al refactoring, we argue that research on effective safeguards is a necessary step toward more capable AI refactoring that transcends the function level.\nAI refactoring within IDEs presents an ideal point for targeted research interventions. The IDE can act as the first gatekeeper for LLM output before it gets committed to the codebase. Additionally, studying IDE-level interventions al-lows for fine-grained data collection, guiding UI design for an optimal developer experience \u2013 crucial for acceptance and realizing our vision of widespread adoption of AI refactoring. However, we posit that trustworthiness is only one side of the coin. Equally important for the uptake of AI refactoring is user trust. Analogous to previous paradigm shifts, e.g., mechanization and electrification, AI uptake requires users to trust the technology. In the AI refactoring context, developers must develop trust in the technology to accept increasing automation levels. Developers are a diverse population, and the trust development journey can vary widely across individ-uals, influenced by their experiences, preferences, and work contexts.\nWe argue that the IDE is the best arena to foster developer trust. As the developers' primary workspace, it provides a familiar and effective starting point. However, building trust is a complex process, and research is needed to enable trust calibration within the IDE achieving the right balance between trust and trustworthiness. An imbalance can result in either reckless reliance on AI refactoring or unnecessary delays in adopting a highly valuable tool. Addressing this challenge demands interdisciplinary research that bridges hu-man factors in automation, human-machine interaction, and computer science."}, {"title": "II. RELATED WORK", "content": "This section presents an overview of research on LLM-based software maintenance and recommendation delivery in IDEs. Amid an avalanche of academic papers on LLM and SE, two sets of researchers have undertaken ambitious literature reviews to assess the current landscape. Hou et al. identified 229 papers in a systematic literature review [13] and Fan et al. found 244 related preprints on arXiv [14]. Both papers list primary studies that use LLMs for various SE tasks, including code generation, testing, maintenance, and documentation. Within maintenance, Automatic Program Repair (APR) emerges as the most common application, while refactoring has received less attention.\nWe investigated the subset of refactoring and APR papers to explore 1) how proposed patches were validated for correct-ness and 2) which datasets were studied. Our findings reveal that most papers rely on successful compilation and passing unit tests for validation, despite research highlighting con-cerns regarding the effectiveness of test suites [15]. In terms of static code analysis, we found examples of vulnerability analysis [16], abstract syntax tree checks [17], and similarity measurements [18]. Finally, most papers target small Java and Python datasets, i.e., there is a need for larger studies.\nIn the context of refactoring assistance, there has been a no-table evolution from recommendation systems (RecSys) [19], [20] to software bots [21]. While both approaches offer developers guidance, bots typically provide higher levels of automation [22], often encompassing task execution, and fre-quently engage in dynamic two-way interactions. Regardless of the approach, an effective UI is essential for conveying recommendations and rationales. Google's latest work on their internal reviewing assistant stresses how they successfully bal-anced moderate LLM results with a carefully crafted UI [11].\nA critical UI consideration is the delivery of patch proposals to developers. UI design remains a very active topic in the general RecSys community, with calls for more practitioner-led research to investigate underlying questions such as how \"various implementations affect design qualities such as trust, fun, transparency, serendipity, sense of control\" [23]. In the RecSys SE literature, fundamental guidelines include de-signing for understandability, transparency, and assessability before considering trust cultivation [24]. With the increasing focus on bots, interaction has taken the front seat. Non-intrusiveness was a topic already for RecSys, but this turns even more important in the bot setting [25]. In the same vein, it is important not to overload the developers with noise [26]. To conclude, while existing guidelines for designing APR bots will support our work on AI refactoring [23], the literature clearly motivates further research on UI design for developer acceptance and trust [27]."}, {"title": "III. TRUST IN THE CONTEXT OF AI REFACTORING", "content": "Various research fields have studied the socio-psychological concept of trust, and many researchers have tried explain-ing the multi-faceted concept. A systematic review by Hoff and Bashir reports that most explanations contain three con-stituents [28]. First, there must be a truster to give trust (e.g., AI assistant), a trustee to accept the trust (e.g., a developer), and something must be at stake (e.g., code integrity). Second, the trustee must have an incentive to perform some task (e.g., code smell removal). Third, there must be a risk that the trustee will fail to perform the task, leading to potential consequences (e.g., breaking the build).\nTrust is related to reliance, explained by Baier as \u201ccontinued relationship on the basis of dependable habits\" [29]. In terms of our vision for widespread adoption of AI refactoring, the development community needs a continuous working relationship with AI assistants (the trusters) based on the dependable habits toward the developers, testers, and other roles (the trustees). A well-cited review article by Lee and See presents substantial evidence that trust guides reliance [30], i.e., trust is a meaningful construct for us to study and support. Furthermore, the two authors propose the definition of trust that steers our work: \"the attitude that an agent will help achieve an individual's goals in a situation characterized by uncertainty and vulnerability.\"\nUnfortunately, misalignment between human trust and the trustworthiness of automation is common. Fig. 1 depicts the phenomenon, based on seminal work by Parasuraman and Riley [31]. The terms misuse and disuse explain failures from flawed partnerships between humans and automation. Misuse refers to the failures that occur when users overtrust automa-tion, i.e., trust > trustworthiness. Disuse denotes failures that occur when people distrust the capabilities of automation, i.e., trust < trustworthiness. The diagonal line represents appropri-ate trust, a balanced state between trust and trustworthiness, i.e., trust = trustworthiness.\nFig. 1 also presents two essential concepts related to obtain-ing appropriate trust. Calibration (blue arrows) is efforts taken to remedy misalignment between trust and trustworthiness. Resolution represents how \"precisely a judgment of trust differentiates levels of automation\u201d [31]. The gray area shows an example of poor resolution, as different levels of trustwor-thiness map to the same trust. In the AI refactoring context, this pertains to differently capable AI assistants being entrusted equally by developers. The poor resolution effectively turns trust into a nonlinear function of trustworthiness. On top of this, human psychology also contributes to non-linearities [32]. Examples include the lasting effect of the initial experience and that an AI assistant's worst behaviors disproportionally impact trust implications that can persist across several subsequent releases, even with improvements.\nFig. 2 displays the trust development of a developer that initially distrusts AI assistants. The y-axis indicates three critical breakpoints: I) Interest: recognizing a potential option,"}, {"title": "IV. TRUST DEVELOPMENT IN THE IDE", "content": "III) Try: the willingness to experiment with the tool, and III) Rely: readiness for habitual use. First, the developer's trust development is gradual until reaching I). Trust devel-opment is initially gradual until reaching I), after which it accelerates, particularly at II), before eventually reaching III). Striped intervals underscore significant individual variations. The black arrow shows our aim to expedite trust calibration from skepticism to appropriate trust among developers, which will be guided by Hoff and Bashir's conceptual model for dynamic trust development (described in Sec. IV).\nParasuraman and Riley also define specificity to denote the degree of trust associated with a particular component or aspect of the automation. Specificity is highly relevant to Al refactoring, as the effectiveness of solutions can vary across different code smells. To maximize their usefulness, developers may need support in understanding the specific contexts in which AI assistants excel, rather than treating their performance as uniform. How to best tackle this challenge is another topic for future research.\nSeveral researchers have developed models to describe the development of trust in automation. Since trust is a multi-faceted concept, trust development inevitably follows suit. Lee and See discuss how trust evolves as users process information about the capabilities of automation [30], i.e., its trustworthiness. The authors describe how three different types of information processing are involved. Most effective is the affective process, i.e., \"emotional responses to violations and confirmations of implicit expectancies.\u201d The remaining two ways are the analytical process (rational evaluation) and the analogical process (comparing to opinions of others, including the online developer community [33]). Previous work clearly shows that users must both think and feel to develop trust in automation, but feelings dominate trust is emotional.\nWe embark from the body of knowledge on trust devel-opment. Primarily, our starting point is a conceptual model of factors that influence trust and reliance in automation developed by Hoff and Bashir [28]. This model, in turn, builds on three different components of trust identified by Marsh and Dibben [34]. Dispositional trust represents an individual's enduring tendency to trust automation. This level of trust tends to be stable over time, primarily influenced by culture, age, gender, and personality traits. Situational trust reflects how the trust development depends on the situation, i.e., external variability (e.g., type of system, complexity, and task difficulty) and internal variability (e.g., self-confidence and mood). Learned trust is split into 1) Initial Learned trust based on pre-existing knowledge before interaction (e.g., experience with similar systems, reputation of the brand, and technology insights) and 2) Dynamic Learned trust that evolves during interaction with the system.\nFig. 3 shows a simplified version of Hoff and Bashir's model [28]. Dashed arrows indicate factors that can change during a single interactive session. The model is primarily organized into factors prior to interaction (the left side) and during interaction (the gray area). A) depicts the three com-ponents of trust that develop prior to system interaction, i.e., Dispositional, Situational, and Initial Learned trust. Situational factors that are not directly related to trust can influence both B) the Initial Reliance Strategy prior to interaction and C) Reliance on the AI Assistant during interaction. The Initial Reliance Strategy influences how users interact with the AI Assistant, subsequently D) influencing Refactoring Performance. In the same vein, the user's Reliance on the AI Assistant during interaction E) influences the Refactoring Performance. Users continuously process information related to the Refactoring Performance, which F) affects the Dynamic Learned trust, subsequently G) influencing the Reliance on the AI Assistant. E-F-G constitute a closed loop, illustrating how the user's trust develops as they observe the performance of the automation [32].\nTo conclude the section on trust, we paraphrase Lee and See [30]: \"appropriate trust and reliance depend on how well the AI refactoring trustworthiness is conveyed to the potential users in the development community.\" To support conveyance, Lee and See argue that technology providers should either 1) make the automation simpler or 2) reveal their operation more clearly. As simpler solutions cannot replace the LLMs used in"}, {"title": "V. CONCLUSIONS AND THE ROAD AHEAD", "content": "Al refactoring opaque Al technology is essential there is no alternative to developing trustworthy AI. Researchers must find the appropriate level of detail to explain how AI refactoring work, and customize it based on personal user preferences that dynamically change over time.\nOur vision is improved software maintainability thanks to widespread adoption of highly capable AI refactoring. Meeting the vision revolves around scientific foundation around two complementary perspectives of trust. First, trustworthy AI refactoring focuses on enhancing the safeguards to increase the refactoring confidence by rejecting low-confidence code editing. Second, trusted AI refactoring involves developing the UI to foster developers' trust by customizing the delivery of LLM-based recommendations in the IDE. Previous research on human factors suggests that misalignment between trust and trustworthiness can severely impede industry adoption [31] \u2013 we aspire to calibrate these two sides of trust.\nOur research will progress through action research, i.e., \u201ca disciplined process of inquiry conducted by and for those taking the action\" [35]. Action research is an appropriate method when the main objective is to facilitate change, e.g., improved Al refactoring, and the researcher is an integral part of the change process [36]. Implementing research interventions within CodeScene's mature SE intelligence platform offers an opportunity for large-scale evaluations with real users in proprietary projects. Leveraging CodeScene's IDE extensions for VS Code and IntelliJ, we aim to conduct detailed telemetry for A/B testing, enabling continuous enhancement of both the trustworthy safeguards and the trust-cultivating UI, i.e., a user-centered development of our research interventions.\nRegarding safeguards, integrating state-of-the-art program analysis [37], [38] and vulnerability analysis [39] are crucial first steps. Furthermore, a range of potential complementary techniques, such as anomaly detection [40] and mutation analysis [15], will be explored and prioritized as part of our future work. We conclude this position paper with two pivotal research questions that will guide our continued research:\nRQ1 How can we design effective safeguards to enhance the trustworthiness of AI refactoring?\nRQ2 In what ways can the UI for AI refactoring be crafted to facilitate trust calibration?"}]}