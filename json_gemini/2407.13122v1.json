{"title": "MO-EMT-NAS: Multi-Objective Continuous Transfer of Architectural Knowledge Between Tasks from Different Datasets", "authors": ["Peng Liao", "XiLu Wang", "Yaochu Jin", "WenLi Du"], "abstract": "Deploying models across diverse devices demands tradeoffs among multiple objectives due to different resource constraints. Arguably, due to the small model trap problem in multi-objective neural architecture search (MO-NAS) based on a supernet, existing approaches may fail to maintain large models. Moreover, multi-tasking neural architecture search (MT-NAS) excels in handling multiple tasks simultaneously, but most existing efforts focus on tasks from the same dataset, limiting their practicality in real-world scenarios where multiple tasks may come from distinct datasets. To tackle the above challenges, we propose a Multi-Objective Evolutionary Multi-Tasking framework for NAS (MO-EMT-NAS) to achieve architectural knowledge transfer across tasks from different datasets while finding Pareto optimal architectures for multi-objectives, model accuracy and computational efficiency. To alleviate the small model trap issue, we introduce an auxiliary objective that helps maintain multiple larger models of similar accuracy. Moreover, the computational efficiency is further enhanced by parallelizing the training and validation of the weight-sharing-based supernet. Experimental results on seven datasets with two, three, and four task combinations show that MO-EMT-NAS achieves a better minimum classification error while being able to offer flexible trade-offs between model performance and complexity, compared to the state-of-the-art single-objective MT-NAS algorithms. The runtime of MO-EMT-NAS is reduced by 59.7% to 77.7%, compared to the corresponding multi-objective single-task approaches.", "sections": [{"title": "1 Introduction", "content": "EMT-NAS [17] is a recently proposed multi-tasking NAS algorithm that aims to address the challenge of multiple tasks from different datasets. Different from"}, {"title": "2 Related Work", "content": "Neural architecture search aims at automatically finding neural network architectures that are competitive with those manually designed by human experts [6, 12, 13, 32, 43, 45]. Reinforcement learning (RL) [27], gradient descent (GD) [21,38], and evolutionary algorithms (EA) [34,37] are three typical search strategies used in NAS. Most NAS algorithms, however, suffer from huge computational costs [22], leading to the development of one-shot NAS that can significantly reduce GPU days and lower the high demand for computational resources through parameter sharing [28].\nMulti-objective NAS has been developed to search for neural network models for optimizing objectives in addition to the accuracy required in real-world applications. Among the existing MO-NAS approaches, evolutionary MO, a population-based method, has been widely adopted as it is capable of achieving a set of Pareto optimal neural architectures in a single run. NASGNet [26] was proposed to generate a set of architectures by simultaneously minimizing classification error and model complexity (floating-point operations per second, FLOPs) with a Bayesian network as a performance predictor. Alternatively, MT-ENAS [2] adopted the network performance and model size as two objectives and used multi-task training to construct a radial-basis-function neural network [18] as a performance predictor. It is worth noting that they utilized separate populations for each objective without knowledge transfer. In NSGANetV2 [25], five objectives are simultaneously optimized with the help of multiple performance predictors. Since most MO-NAS approaches rely heavily on the quality of the performance predictors, we demonstrate the differences between MO-EMT-NAS and them in that MO-EMT-NAS performs training and validation based on a weight-sharing supernet to reduce the computational overhead, instead of using performance predictors [10] or zero-shot metric evaluation [19]. Although the use of parameter sharing allows candidate submodels to be easily evaluated without training from scratch [29], candidates with small model sizes generally achieve better validation accuracy at the beginning of the search. Hence, promising candidate models with large sizes fail to survive to the next generation, resulting in a search bias towards small models."}, {"title": "Multi-tasking NAS", "content": "NAS has evolved from single-task and transfer learning to multi-task learning, with the latest search focusing on different datasets. Since the NAS algorithm [48] was proposed, NAS has demonstrated much success in automatically designing effective neural architectures. Initially, it was employed for optimizing models for specific single tasks, such as the classification task on CIFAR-10 [15], termed single-task learning. As related tasks can be encountered, researchers resorted to transfer learning to improve NAS by transferring knowledge from similar previous tasks. For example, a pre-trained model is employed to guide the search for a new task [20, 23]. Meanwhile, it has been observed that different tasks can stem from the same dataset. For example, instance and semantic segmentation and depth prediction can be performed on a large dataset for road scene understanding, CityScapes [4]. By learning shared representations across these tasks, a common neural architecture for all tasks is constructed via multi-task learning, instead of searching for a task-specific model for each task in the traditional approach [8,33]. Regardless of its well-known efficiency, this line of research is limited to considering multiple tasks from the same dataset [3]. Recently, the presence of tasks from different datasets poses challenges for multi-tasking NAS, due to the fact that two tasks from different datasets show lower relatedness scores compared to those originating from the same dataset [14]. Arguably, EMT-NAS [17] was first developed as an MT-NAS with the help of an evolutionary multi-tasking framework to address tasks on different datasets.\nAlthough recently proposed methods have shed light on the advantages of incorporating transfer learning and multi-task learning into NAS, our work establishes a multi-objective multi-tasking framework and focuses on handling multiple tasks on different datasets and providing a set of Pareto optimal architectures by balancing the model error and model size."}, {"title": "3 Approach", "content": "In this work, we used the search space of [49]. The encoding of a neural architecture consists of normal and reduction cells, with each cell comprising five blocks, and each block containing two input bits and two operation bits, amounting to a total of 40 bits. For each operation bit, candidate operators include depthwise-separable convolution, dilated convolution, max pooling, average pooling and identity. More details can be found in Supplementary Material A."}, {"title": "3.1 MO-EMT-NAS", "content": "The overall framework of MO-EMT-NAS is shown in Fig. 1, including two main parts: the search algorithm and the training and validation of the weight-sharing-based supernet. Implementing multiprocessing enables parallelized training and validation of the supernet for various tasks, significantly boosting computational efficiency. See Supplementary Material B for more details and the pseudocode.\nThe architecture search algorithm is shown on the left panel of Fig. 1. First, individuals of the initial population are randomly assigned to different tasks as"}, {"title": "3.2 Auxiliary Third Objective", "content": "Consider a minimization problem with M objectives, individual A dominates individual B, i.e., A is better than B, if:\n$$f_m(A) \u2264 f_m(B), \u2200m \u2208 {1,2,...,M},$$\n$$f_m(A) <f_m(B), \u2203m\u2208 {1,2, . . ., M}.$$\nIf A does not dominate B and B does not dominate A, then A and B are non-dominated to each other, indicating A and B are similar. Similarly, A is dominated by B means that A is worse than B. The selection of non-dominated solutions in NSGA-II [5] is outlined as follows and depicted in Fig. 1: 1) Non-dominated sorting is performed on the combined population of the parent and offspring, resulting in a non-dominated level/rank for each individual. 2) A predefined number of individuals are selected to survive to the next generation based on their non-dominated level. 3) If the number of individuals at the last accepted level exceeds the predefined population size, the crowding distance of each individual (indicating its contribution to solution diversity) is used as the selection criterion. Solutions with a large crowding distance will be prioritized to ensure the diversity of the population.\nThe key insight the multi-objective NAS approach can offer is to provide a bigger picture of the trade-offs between multiple important objectives for a real-world application. Unfortunately, achieving a set of diverse and promising architectures to simultaneously minimize validation error and model size is non-trivial, as previously discussed. Figure 2 visualizes the populations obtained by NSGA-II with different objectives at different generations. In Fig. 2a, the population obtained by minimizing both the model error and size converges rapidly towards small-size models as the evolution proceeds. This can be attributed to the fact that small-size models can achieve a superior validation error in the initial phase when using a weight-sharing-based supernet, resulting in losing all large models in the final population. To mitigate this issue, a practical approach is to include an auxiliary extra objective for improving the diversity of candidate architectures when performing non-dominated sorting. Figure 2b"}, {"title": "3.3 Parallel Training and Evaluation", "content": "The training and validation of the weight-sharing-based supernet is shown on the right panel of Fig. 1. The multi-tasking framework allows the training and validation of the supernet for each task to be parallelized. Specifically, the available number of iterations is divided by the number of individuals to obtain the number of iterations $train_n$ for each individual. This process utilizes only one epoch of training data. Following this, individuals of each task are decoded as a neural architecture and trained for $train_n$ times. Finally, the validation error and the number of model parameters of the population of each task are obtained. This way, each neural architecture can substantially enhance its validation accuracy during the successive training iterations, effectively reflecting its true performance [41]."}, {"title": "4 Experiments", "content": "We adopt a Multi-Objective Single-Tasking NAS (MO-ST-NAS) baseline by only removing the multi-tasking setting from MO-EMT-NAS to show the promising advantages of transferring architectural knowledge across related tasks on diverse datasets. Similarly, MO-EMT-NAS is compared with a representative single-objective evolutionary MT-NAS (EMT-NAS [17]) to demonstrate the efficiency of MO with the auxiliary objective.\nWe select seven datasets and conduct experiments on two, three and four tasks for performance evaluation. 1) We design a two-task experiment on the classical datasets CIFAR-10 and CIFAR-100 [15]. Additionally, the obtained architectures are retrained on ImageNet [31] in order to examine our MO-EMT-NAS's transferability. 2) Multiple tasks, i.e., two-, three- and four-task settings, are designed on MedMNIST [39] to validate the generalization ability of our method. Four datasets, namely PathMNIST, OrganMNIST_Axial, OrganMNIST_Coronal, and OrganMNIST_Sagittal, are selected to simulate various medical imaging scenarios such as rectal cancer pathology and 2D images from 3D computed tomography (CT) images of liver tumors in different planes. Both our baseline and proposed approach are performed five independent runs, and the hyperparameters are listed in Table 1. See Supplementary Material C for more experimental setups.\nFollowing the practice in [37,40], to better visualize and compare the optimal architectures obtained from the MO algorithms, the final population is divided into four groups based on the model size as shown in the example given in Fig. 1, and the architecture with the smallest error is selected from each group (denoted as A, B, C, and D)."}, {"title": "4.2 Performance Indicator", "content": "We adopt hypervolume (HV) [47] to evaluate the sets of architectures found by different approaches in terms of convergence and diversity. HV calculates the"}, {"title": "4.3 Two-task on CIFAR-10 and CIFAR-100", "content": "The results in Table A in the Supplementary Material show that models found by MO-EMT-NAS dominate all models found by other methods under comparison, except for Baseline-A. This indicates that MO-EMT-NAS overwhelmingly outperforms all compared approaches. As shown in Fig. 4, MO-EMT-NAS achieves a set of diverse and superior architectures (the red line is at the bottom-left). Interestingly, MO-EMT-NAS approaches are more competitive compared to single-objective MT-NAS approaches, indicating that simultaneously optimizing multiple conflicting objectives enhances the maintenance of large models without undue sacrifice of the validation error. Besides, the comparison between MO-EMT-NAS and MO-ST-NAS demonstrates that architecture knowledge transfer between tasks facilitates the search for neural architectures. The average of the HV values over five runs on CIFAR-10 and CIFAR-100 for MO-ST-NAS and MO-EMA-NAS in Table 2 further demonstrates the better convergence and diversity of our approach. It is important to highlight that the runtime of the algorithms under comparison is summarized in Table A in the Supplementary"}, {"title": "4.4 Transfer to ImageNet", "content": "The 16 neural architectures obtained by MO-ST-NAS and MO-EMT-NAS on CIFAR-10 and CIFAR-100, as plotted in Fig. 4, are transferred to ImageNet for retraining. The results of the transferred architectures on ImageNet are compared with several representative algorithms and given in Table B in the Supplementary Material. From Fig. 5, we observe that MO-EMT-NAS shows superior results in terms of the Top-1 accuracy than other algorithms under comparison, while providing a series of trade-off models with the number of parameters ranging from 1.57M to 3.25M. And the architectures transferred from MO-EMT-NAS always yields better performances than that from MO-ST-NAS. The model"}, {"title": "4.5 Medical Multi-Objective Multi-Tasking", "content": "PathMNIST, OrganMNIST_Axial, OrganMNIST_Coronal, and OrganMNIST_Sagittal abbreviated as P, A, C, and S.\nMulti-Objective NAS: Figures 6a-6k show that MO enables MO-EMT-NAS to yield a set of promising models with respect to the accuracy, model size or both. This further confirms the advantage of using the MO methods with the auxiliary objective. Importantly, MO-EMT-NAS finds a set of neural architectures with a low error that dominate the single models found by both single-objective NAS architectures, the Single-Tasking and EMT-NAS.\nEvolutionary Multi-Tasking NAS: In Table 3, the obtained Pareto optimal architecture set for each task is evaluated by the HV metric. Compared with MO-ST-NAS, MO-EMT-NAS achieves higher HV values, i.e., better performance in terms of convergence and diversity, on various task combinations. This is accomplished by using the knowledge transfer across tasks to promote the multi-tasking optimization. Across all settings, MO-EMT-NAS consistently achieves better accuracy while being significantly faster than MO-ST-NAS.\nScalability of MO-EMT-NAS: The scalability of MO-EMT-NAS is tested by setting the number of tasks to two, three, and four, respectively. As illustrated in Figs. 6a-6k, MO-EMT-NAS consistently exhibits superior performance compared to single-objective NAS approaches, confirming the promising scalability of MO-EMT-NAS. Specifically, architectures discovered by MO-EMT-NAS consistently dominate (with better performance) or are non-dominated (with similar performance) compared to those found by EMT-NAS and Single-Tasking NAS.\nMultitasking with Different Similarities: Using ResNet-50 as a feature extractor, we conduct the representation similarity analysis [7] to obtain task relatedness scores (RS) [1] between the four medical datasets. The RS results presented in Fig. 6l vary from 0.09 to 0.50. Notably, one can observe lower scores between P and A, C, S and higher scores between A, C, S. According to Fig. 6a-6c, a performance drop in terms of the error can be observed with the decrease of the RS. For example, MO-EMT-NAS finds a set of Pareto optimal models with errors ranging from 6.8% to 8.0% on the dataset P for the two tasks with $RS = 0.25$ in Fig. 6a while obtaining models with errors ranging from 7.3% to 9.6% on the dataset P in the two tasks PS with $RS = 0.09$ in Fig. 6c. A possible reason is that the lack of similarity between tasks poses challenges in architectural knowledge transfer, since less transferable information can be obtained.\nSearch Efficiency: The runtime during the experiments is recorded and the percentage of the saved time by each algorithm compared with MO-ST-NAS is measured and presented in Table 3.\nThe time saved by MO-EMT-NAS with addressing tasks in parallel is denoted as \"GPU Days I (%)\". One can observe that compared with the multi-objective"}, {"title": "4.6 Ablation studies", "content": "To validate the efficiency of the auxiliary objective, MO-EMT-NAS with and without the auxiliary objective $f_a$ are performed on CIFAR-10 and CIFAR-100. The fact that the HV values achieved by MO-EMT-NAS, i.e., 0.571 for CIFAR-10 and 0.532 for CIFAR-100, are better than that achieved by MO-EMT-NAS without $f_a$, i.e., 0.443 for CIFAR-100 and 0.356 for CIFAR-100, convincingly"}, {"title": "4.7 Sensitivity Analysis", "content": "The random mating probability (RMP) is an important parameter that controls the degree of knowledge transfer between tasks. Hence, RMP is set to 0, 0.2, 0.4, 0.6, 0.8 and 1 to test its impact on the performance, and the HV results on two datasets, Path and Organ_A, are summarized in Table 5. Accordingly, we find that MO-EMT-NAS with a higher RMP value tends to achieve better performance on Path, shedding lights on the potential advantage of encouraging the architectural knowledge transfer. Indeed, a higher degree of knowledge transfer indicated by a larger RMP does not always improve the performance on Organ_A, but the best performance is achieved when RMP = 1.\nWe have also extensively tuned the crossover and mutation probabilities, population size and the number of generations. The results in terms of HV values are presented in Supplementary Material D."}, {"title": "5 Conclusion", "content": "In this work, we propose a multi-objective multi-tasking NAS framework with the help of weight-sharing-based supernets to efficiently achieve a set of promising architectures with diverse model sizes. The multi-tasking framework enables architecture knowledge acquired from different tasks to be implicitly transferred, thereby effectively solving multiple tasks from different datasets. To mitigate the small model trap problem, we introduce an auxiliary objective that prefers larger models over smaller ones when they achieve similar accuracy, thereby achieving a set of promising architectures with various model sizes. Extensive experiments demonstrate that the architectures obtained by MO-EMT-NAS exhibit superior performance at a lower computational cost than the state of the art while being able to maintain a high degree of diversity in model sizes."}]}