{"title": "MO-EMT-NAS: Multi-Objective Continuous Transfer of Architectural Knowledge Between Tasks from Different Datasets", "authors": ["Peng Liao", "XiLu Wang", "Yaochu Jin", "WenLi Du"], "abstract": "Deploying models across diverse devices demands tradeoffs among multiple objectives due to different resource constraints. Arguably, due to the small model trap problem in multi-objective neural architecture search (MO-NAS) based on a supernet, existing approaches may fail to maintain large models. Moreover, multi-tasking neural architecture search (MT-NAS) excels in handling multiple tasks simultaneously, but most existing efforts focus on tasks from the same dataset, limiting their practicality in real-world scenarios where multiple tasks may come from distinct datasets. To tackle the above challenges, we propose a Multi-Objective Evolutionary Multi-Tasking framework for NAS (MO-EMT-NAS) to achieve architectural knowledge transfer across tasks from different datasets while finding Pareto optimal architectures for multi-objectives, model accuracy and computational efficiency. To alleviate the small model trap issue, we introduce an auxiliary objective that helps maintain multiple larger models of similar accuracy. Moreover, the computational efficiency is further enhanced by parallelizing the training and validation of the weight-sharing-based supernet. Experimental results on seven datasets with two, three, and four task combinations show that MO-EMT-NAS achieves a better minimum classification error while being able to offer flexible trade-offs between model performance and complexity, compared to the state-of-the-art single-objective MT-NAS algorithms. The runtime of MO-EMT-NAS is reduced by 59.7% to 77.7%, compared to the corresponding multi-objective single-task approaches.", "sections": [{"title": "1 Introduction", "content": "EMT-NAS [17] is a recently proposed multi-tasking NAS algorithm that aims to address the challenge of multiple tasks from different datasets. Different from the initial shared representation of tasks from the same dataset, EMT-NAS considers knowledge transfer from one task to a related task, e.g., transferring knowledge from playing squash to playing tennis [44]. EMT-NAS ensures that each task has a separate set of supernet parameters, skillfully alleviating the negative transfer [35] that may result from joint training of weight parameters for multiple tasks. However, it evaluates the architectural accuracy only and therefore tends to favor larger models in the search space, lacking control over the model size. To address the above limitations, the present work aims to identify a set of architectures that can balance multiple objectives for each task by means of multi-objective optimization (MO), successfully tackling the complexities posed by multiple classification tasks from diverse datasets.\nMany existing studies on single-task NAS have adopted the MO approach to strike a trade-off between the accuracy and other indicators required in a wide range of settings, such as computational complexity, CPU and GPU latency [16], adversarial robustness [9], and data privacy [46]. Evolutionary MO-NAS based on the weight-sharing-based supernet has achieved remarkable success, which significantly reduces computational consumption by allowing all possible architectures to share parameters. However, it has been found that when considering the deployment of models on diverse devices with varying resources, simultaneously optimizing the classification error and model size often drives the population to quickly converge to smaller models. This phenomenon arises because smaller models converge faster in the early stage of evolutionary optimization [42], resulting in lower classification errors for small models [30]. Hence, environmental selection based on the non-dominance relationship favors smaller models and eliminates larger ones. To address the above issue, CARS [40] designed two environmental selection strategies, one minimizing the model error and size that favors small models, and the other minimizing both the model error and convergence speed that favors larger models. It was found, however, that the population was able to maintain a good degree of diversity in the initial stages; however, the population became polarized into large and small models as the evolution proceeded, losing the models of medium sizes.\nTo tackle the challenges of multi-objective MT-NAS, this work adopts multi-objective multi-factor evolutionary algorithms (MO-MFEAs) [11] to transfer latent similarity knowledge across different tasks. Furthermore, an auxiliary objective is introduced to maintain the architecture diversity so that small, medium, and large models can be retained, ensuring that the final solution set contains trade-off models of a wide range of model sizes. Key contributions are as follows:\nWe propose an MO-EMT-NAS framework to effectively search for multiple Pareto optimal architectures for tasks from different datasets, utilizing transferable architecture knowledge across the tasks to facilitate continuous architecture search.\nMO-EMT-NAS considers both the classification error and model size, and the auxiliary objective that mitigates the search bias towards small neural architectures. With the help of multiobjectivization, MO-EMT-NAS maintains architecture diversity in terms of model sizes."}, {"title": "2 Related Work", "content": "Neural architecture search aims at automatically finding neural network architectures that are competitive with those manually designed by human experts [6, 12, 13, 32, 43, 45]. Reinforcement learning (RL) [27], gradient descent (GD) [21,38], and evolutionary algorithms (EA) [34,37] are three typical search strategies used in NAS. Most NAS algorithms, however, suffer from huge computational costs [22], leading to the development of one-shot NAS that can significantly reduce GPU days and lower the high demand for computational resources through parameter sharing [28].\nMulti-objective NAS has been developed to search for neural network models for optimizing objectives in addition to the accuracy required in real-world applications. Among the existing MO-NAS approaches, evolutionary MO, a population-based method, has been widely adopted as it is capable of achieving a set of Pareto optimal neural architectures in a single run. NASGNet [26] was proposed to generate a set of architectures by simultaneously minimizing classification error and model complexity (floating-point operations per second, FLOPs) with a Bayesian network as a performance predictor. Alternatively, MT-ENAS [2] adopted the network performance and model size as two objectives and used multi-task training to construct a radial-basis-function neural network [18] as a performance predictor. It is worth noting that they utilized separate populations for each objective without knowledge transfer. In NSGANetV2 [25], five objectives are simultaneously optimized with the help of multiple performance predictors. Since most MO-NAS approaches rely heavily on the quality of the performance predictors, we demonstrate the differences between MO-EMT-NAS and them in that MO-EMT-NAS performs training and validation based on a weight-sharing supernet to reduce the computational overhead, instead of using performance predictors [10] or zero-shot metric evaluation [19]. Although the use of parameter sharing allows candidate submodels to be easily evaluated without training from scratch [29], candidates with small model sizes generally achieve better validation accuracy at the beginning of the search. Hence, promising candidate models with large sizes fail to survive to the next generation, resulting in a search bias towards small models."}, {"title": "Multi-tasking NAS", "content": "NAS has evolved from single-task and transfer learning to multi-task learning, with the latest search focusing on different datasets. Since the NAS algorithm [48] was proposed, NAS has demonstrated much success in automatically designing effective neural architectures. Initially, it was employed for optimizing models for specific single tasks, such as the classification task on CIFAR-10 [15], termed single-task learning. As related tasks can be encountered, researchers resorted to transfer learning to improve NAS by transferring knowledge from similar previous tasks. For example, a pre-trained model is employed to guide the search for a new task [20, 23]. Meanwhile, it has been observed that different tasks can stem from the same dataset. For example, instance and semantic segmentation and depth prediction can be performed on a large dataset for road scene understanding, CityScapes [4]. By learning shared representations across these tasks, a common neural architecture for all tasks is constructed via multi-task learning, instead of searching for a task-specific model for each task in the traditional approach [8,33]. Regardless of its well-known efficiency, this line of research is limited to considering multiple tasks from the same dataset [3]. Recently, the presence of tasks from different datasets poses challenges for multi-tasking NAS, due to the fact that two tasks from different datasets show lower relatedness scores compared to those originating from the same dataset [14]. Arguably, EMT-NAS [17] was first developed as an MT-NAS with the help of an evolutionary multi-tasking framework to address tasks on different datasets.\nAlthough recently proposed methods have shed light on the advantages of incorporating transfer learning and multi-task learning into NAS, our work establishes a multi-objective multi-tasking framework and focuses on handling multiple tasks on different datasets and providing a set of Pareto optimal architectures by balancing the model error and model size."}, {"title": "3 Approach", "content": "In this work, we used the search space of [49]. The encoding of a neural architecture consists of normal and reduction cells, with each cell comprising five blocks, and each block containing two input bits and two operation bits, amounting to a total of 40 bits. For each operation bit, candidate operators include depthwise-separable convolution, dilated convolution, max pooling, average pooling and identity. More details can be found in Supplementary Material A."}, {"title": "3.1 MO-EMT-NAS", "content": "The overall framework of MO-EMT-NAS is shown in Fig. 1, including two main parts: the search algorithm and the training and validation of the weight-sharing-based supernet. Implementing multiprocessing enables parallelized training and validation of the supernet for various tasks, significantly boosting computational efficiency. See Supplementary Material B for more details and the pseudocode.\nThe architecture search algorithm is shown on the left panel of Fig. 1. First, individuals of the initial population are randomly assigned to different tasks as the corresponding parent population. The parent individuals are sampled from the supernet and trained on each task, and then their objectives are evaluated. Then, the main loop is performed as follows. Individuals are selected (called mate selection) from the parent population to generate an offspring population by exploring the same tasks and transferring knowledge across different tasks. Both offspring and parent individuals are sampled from the supernet and trained on each task and then their objectives, i.e., the model error, size and the auxiliary objective, are evaluated. The non-dominated sorting with the crowding distance is performed on the combined parent and offspring population to select the population for the next generation. After repeating the main loop for several generations, a set of Pareto optimal solutions for each task is obtained.\nIn this work, block-based crossover and bit-based mutation of [17] are adopted due to the discrete coding of NAS. The block-based crossover uses the block as the basic unit and allows the selected two parents to exchange blocks at a predefined crossover probability. Bit-based mutation adopts bits as the basic unit of bits and randomly varies the bits of a selected single parent within a candidate range at the mutation probability. Within the framework of MO-EMT-NAS, the generation of offspring populations enables implicit knowledge transfer between tasks: 1) For parents assigned to the same task, the offspring is generated via crossover, and mutation operators can explore the corresponding task. When parents come from different tasks, the generation of offspring is controlled by a parameter called random mating probability (RMP). 2) Architectural knowledge transfer is triggered at a probability of RMP, where the offspring are generated from the parents through crossover and mutation, and are assigned to the tasks as one of its parents. 3) otherwise, no knowledge transfer will happen, i.e., the parents independently undergo mutation to generate the corresponding offspring, and these offspring inherit the task of their parents.\nIn MO-EMT-NAS, environmental selection must consider multiple conflicting objectives on multiple tasks. Accordingly, the population should be divided into subpopulations by equipping individuals with different tasks. Thus each task can execute its environmental selection separately. Subsequently, a multi-objective environmental selection is performed to consider the validation error, the number of model parameters, and the auxiliary objective, aiming to enhance the architecture diversity and provide a set of promising architectures."}, {"title": "3.2 Auxiliary Third Objective", "content": "Consider a minimization problem with M objectives, individual A dominates individual B, i.e., A is better than B, if:\n$$f_m(A) \\leq f_m(B), \\forall m \\in \\{1,2,...,M\\},$$\n$$f_m(A) < f_m(B), \\exists m\\in \\{1,2, . . ., M\\}.$$"}, {"title": "3.3 Parallel Training and Evaluation", "content": "The training and validation of the weight-sharing-based supernet is shown on the right panel of Fig. 1. The multi-tasking framework allows the training and validation of the supernet for each task to be parallelized. Specifically, the available number of iterations is divided by the number of individuals to obtain the number of iterations $train_n$ for each individual. This process utilizes only one epoch of training data. Following this, individuals of each task are decoded as a neural architecture and trained for $train_n$ times. Finally, the validation error and the number of model parameters of the population of each task are obtained. This way, each neural architecture can substantially enhance its validation accuracy during the successive training iterations, effectively reflecting its true performance [41]."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Settings", "content": "We adopt a Multi-Objective Single-Tasking NAS (MO-ST-NAS) baseline by only removing the multi-tasking setting from MO-EMT-NAS to show the promising advantages of transferring architectural knowledge across related tasks on diverse datasets. Similarly, MO-EMT-NAS is compared with a representative single-objective evolutionary MT-NAS (EMT-NAS [17]) to demonstrate the efficiency of MO with the auxiliary objective.\nWe select seven datasets and conduct experiments on two, three and four tasks for performance evaluation. 1) We design a two-task experiment on the classical datasets CIFAR-10 and CIFAR-100 [15]. Additionally, the obtained architectures are retrained on ImageNet [31] in order to examine our MO-EMT-NAS's transferability. 2) Multiple tasks, i.e., two-, three- and four-task settings, are designed on MedMNIST [39] to validate the generalization ability of our method. Four datasets, namely PathMNIST, OrganMNIST_Axial, OrganMNIST_Coronal, and OrganMNIST_Sagittal, are selected to simulate various medical imaging scenarios such as rectal cancer pathology and 2D images from 3D computed tomography (CT) images of liver tumors in different planes. Both our baseline and proposed approach are performed five independent runs, and the hyperparameters are listed in Table 1. See Supplementary Material C for more experimental setups.\nFollowing the practice in [37,40], to better visualize and compare the optimal architectures obtained from the MO algorithms, the final population is divided into four groups based on the model size as shown in the example given in Fig. 1, and the architecture with the smallest error is selected from each group (denoted as A, B, C, and D)."}, {"title": "4.2 Performance Indicator", "content": "We adopt hypervolume (HV) [47] to evaluate the sets of architectures found by different approaches in terms of convergence and diversity. HV calculates the volume of the objective space dominated by a set of non-dominated solutions P and bounded by a reference point r (see Fig. 3a),\n$$HV(P) = VOL (\\bigcup_{y\\in P}[y,r]),$$\nwhere VOL() denotes the usual Lebesgue measure, [y, r] represents the hyper-rectangle bounded by y and r. A larger HV value means better performance: In Fig. 3b, the set of converged and well-distributed green dots, exhibiting a higher HV value, achieves better performance compared with the set of black dots. For each task, after each separate run of all compared algorithms, the maximum values of each objective across all solutions form the reference point r. Therefore, r varies across different tables that involve different algorithms."}, {"title": "4.3 Two-task on CIFAR-10 and CIFAR-100", "content": "The results in Table A in the Supplementary Material show that models found by MO-EMT-NAS dominate all models found by other methods under comparison, except for Baseline-A. This indicates that MO-EMT-NAS overwhelmingly outperforms all compared approaches. As shown in Fig. 4, MO-EMT-NAS achieves a set of diverse and superior architectures (the red line is at the bottom-left). Interestingly, MO-EMT-NAS approaches are more competitive compared to single-objective MT-NAS approaches, indicating that simultaneously optimizing multiple conflicting objectives enhances the maintenance of large models without undue sacrifice of the validation error. Besides, the comparison between MO-EMT-NAS and MO-ST-NAS demonstrates that architecture knowledge transfer between tasks facilitates the search for neural architectures. The average of the HV values over five runs on CIFAR-10 and CIFAR-100 for MO-ST-NAS and MO-EMA-NAS in Table 2 further demonstrates the better convergence and diversity of our approach. It is important to highlight that the runtime of the algorithms under comparison is summarized in Table A in the Supplementary Material. Among these algorithms, MO-EMT-NAS emerges as the most efficient computationally, requiring just 0.38 GPU days for CIFAR-10 and CIFAR-100."}, {"title": "4.4 Transfer to ImageNet", "content": "The 16 neural architectures obtained by MO-ST-NAS and MO-EMT-NAS on CIFAR-10 and CIFAR-100, as plotted in Fig. 4, are transferred to ImageNet for retraining. The results of the transferred architectures on ImageNet are compared with several representative algorithms and given in Table B in the Supplementary Material. From Fig. 5, we observe that MO-EMT-NAS shows superior results in terms of the Top-1 accuracy than other algorithms under comparison, while providing a series of trade-off models with the number of parameters ranging from 1.57M to 3.25M. And the architectures transferred from MO-EMT-NAS always yields better performances than that from MO-ST-NAS. The model with the highest accuracy, Ours-C-100-D, has an accuracy of 75.47% and 3.25M number of parameters. Note that the experiment on the ImageNet (a single task with a large dataset) aims to evaluate the architecture transferability of each method rather than its ability to solve multiple tasks."}, {"title": "4.5 Medical Multi-Objective Multi-Tasking", "content": "PathMNIST, OrganMNIST_Axial, OrganMNIST_Coronal, and OrganMNIST_Sagittal abbreviated as P, A, C, and S.\nMulti-Objective NAS: Figures 6a-6k show that MO enables MO-EMT-NAS to yield a set of promising models with respect to the accuracy, model size or both. This further confirms the advantage of using the MO methods with the auxiliary objective. Importantly, MO-EMT-NAS finds a set of neural architectures with a low error that dominate the single models found by both single-objective NAS architectures, the Single-Tasking and EMT-NAS.\nEvolutionary Multi-Tasking NAS: In Table 3, the obtained Pareto optimal architecture set for each task is evaluated by the HV metric. Compared with MO-ST-NAS, MO-EMT-NAS achieves higher HV values, i.e., better performance in terms of convergence and diversity, on various task combinations. This is accomplished by using the knowledge transfer across tasks to promote the multi-tasking optimization. Across all settings, MO-EMT-NAS consistently achieves better accuracy while being significantly faster than MO-ST-NAS.\nScalability of MO-EMT-NAS: The scalability of MO-EMT-NAS is tested by setting the number of tasks to two, three, and four, respectively. As illustrated in Figs. 6a-6k, MO-EMT-NAS consistently exhibits superior performance compared to single-objective NAS approaches, confirming the promising scalability of MO-EMT-NAS. Specifically, architectures discovered by MO-EMT-NAS consistently dominate (with better performance) or are non-dominated (with similar performance) compared to those found by EMT-NAS and Single-Tasking NAS.\nMultitasking with Different Similarities: Using ResNet-50 as a feature extractor, we conduct the representation similarity analysis [7] to obtain task relatedness scores (RS) [1] between the four medical datasets. The RS results presented in Fig. 6l vary from 0.09 to 0.50. Notably, one can observe lower scores between P and A, C, S and higher scores between A, C, S. According to Fig. 6a-6c, a performance drop in terms of the error can be observed with the decrease of the RS. For example, MO-EMT-NAS finds a set of Pareto optimal models with errors ranging from 6.8% to 8.0% on the dataset P for the two tasks with RS = 0.25 in Fig. 6a while obtaining models with errors ranging from 7.3% to 9.6% on the dataset P in the two tasks PS with RS = 0.09 in Fig. 6c. A possible reason is that the lack of similarity between tasks poses challenges in architectural knowledge transfer, since less transferable information can be obtained.\nSearch Efficiency: The runtime during the experiments is recorded and the percentage of the saved time by each algorithm compared with MO-ST-NAS is measured and presented in Table 3.\nThe time saved by MO-EMT-NAS with addressing tasks in parallel is denoted as \"GPU Days I (%)\". One can observe that compared with the multi-objective single-tasking baseline, the proposed MO-EMT-NAS reduces the runtime from 59.7% to 77.7% for jointly addressing two, three and four tasks, while reaching a better balance between the error and model size. Besides, the time saved by MO-EMT-NAS unsurprisingly increases with the increase of the number of tasks solved jointly. The main reason is that the parallel training and evaluation of multiple tasks in MO-EMT-NAS significantly improves the computational efficiency and caps the overall runtime to that of the slowest task. Hence, for two-task settings, the time saving does not exceed 50%. Besides, while MO-EMT-NAS handles multiple tasks simultaneously, MO-ST-NAS solves tasks one by one, resulting in much more computational cost.\nTo further investigate the efficiency of MO-EMT-NAS, the time reduced by MO-EMT-NAS without the parallelization of the training and evaluation on multiple tasks is denoted as \"GPU Days II (%)\". More specifically, the time saved for training and validation and that for searching are measured and denoted as \"Time I (%)\" and \"Time II (%)\", respectively. The results of \"GPU Days II (%)\" show that MO-EMT-NAS obtains up to 53.5% time savings compared with MO-ST-NAS, indicating the efficiency gained from the multi-tasking framework. Interestingly, by comparing \"GPU Days I (%)\" and \"GPU Days II (%)\", we can confirm the existence of heterogeneous time costs of different tasks. According to \"Time I (%)\", MO-EMT-NAS reduces up to 60.9% the time cost for training and validation. Similarly, the time spent on searching using an evolutionary algorithm is significantly reduced with the increase of the number of tasks. The reason is that the EA requires almost the same time for each task, accordingly the time for searching is doubled if addressing tasks one by one."}, {"title": "4.6 Ablation studies", "content": "To validate the efficiency of the auxiliary objective, MO-EMT-NAS with and without the auxiliary objective $f_a$ are performed on CIFAR-10 and CIFAR-100. The fact that the HV values achieved by MO-EMT-NAS, i.e., 0.571 for CIFAR-10 and 0.532 for CIFAR-100, are better than that achieved by MO-EMT-NAS without $f_a$, i.e., 0.443 for CIFAR-100 and 0.356 for CIFAR-100, convincingly showcasing the advantage of using $f_a$. The HV values in Table 4 indicate that MO-EMT-NAS with the help of the proposed auxiliary objective yields a set of well-converged and diverse non-dominated architectures."}, {"title": "4.7 Sensitivity Analysis", "content": "The random mating probability (RMP) is an important parameter that controls the degree of knowledge transfer between tasks. Hence, RMP is set to 0, 0.2, 0.4, 0.6, 0.8 and 1 to test its impact on the performance, and the HV results on two datasets, Path and Organ_A, are summarized in Table 5. Accordingly, we find that MO-EMT-NAS with a higher RMP value tends to achieve better performance on Path, shedding lights on the potential advantage of encouraging the architectural knowledge transfer. Indeed, a higher degree of knowledge transfer indicated by a larger RMP does not always improve the performance on Organ_A, but the best performance is achieved when $RMP = 1$.\nWe have also extensively tuned the crossover and mutation probabilities, population size and the number of generations. The results in terms of HV values are presented in Supplementary Material D."}, {"title": "5 Conclusion", "content": "In this work, we propose a multi-objective multi-tasking NAS framework with the help of weight-sharing-based supernets to efficiently achieve a set of promising architectures with diverse model sizes. The multi-tasking framework enables architecture knowledge acquired from different tasks to be implicitly transferred, thereby effectively solving multiple tasks from different datasets. To mitigate the small model trap problem, we introduce an auxiliary objective that prefers larger models over smaller ones when they achieve similar accuracy, thereby achieving a set of promising architectures with various model sizes. Extensive experiments demonstrate that the architectures obtained by MO-EMT-NAS exhibit superior performance at a lower computational cost than the state of the art while being able to maintain a high degree of diversity in model sizes."}]}