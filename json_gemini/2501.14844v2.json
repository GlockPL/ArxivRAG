{"title": "UNMASKING CONVERSATIONAL BIAS\nIN AI MULTIAGENT SYSTEMS", "authors": ["Erica Coppolillo", "Giuseppe Manco", "Luca Maria Aiello"], "abstract": "Detecting biases in the outputs produced by generative models is essential to reduce the potential\nrisks associated with their application in critical settings. However, the majority of existing method-\nologies for identifying biases in generated text consider the models in isolation and neglect their\ncontextual applications. Specifically, the biases that may arise in multi-agent systems involving gen-\nerative models remain under-researched. To address this gap, we present a framework designed to\nquantify biases within multi-agent systems of conversational Large Language Models (LLMs). Our\napproach involves simulating small echo chambers, where pairs of LLMs, initialized with aligned\nperspectives on a polarizing topic, engage in discussions. Contrary to expectations, we observe sig-\nnificant shifts in the stance expressed in the generated messages, particularly within echo chambers\nwhere all agents initially express conservative viewpoints, in line with the well-documented politi-\ncal bias of many LLMs toward liberal positions. Crucially, the bias observed in the echo-chamber\nexperiment remains undetected by current state-of-the-art bias detection methods that rely on ques-\ntionnaires. This highlights a critical need for the development of a more sophisticated toolkit for bias\ndetection and mitigation for AI multi-agent systems.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) learn social biases inherent in their training data, which can lead to the generation\nof text that perpetuates or even exacerbates these biases Bender et al. (2021). To mitigate the potential harms caused\nby biased LLMs, it is crucial to first quantify the biases reflected in their generated outputs. The existing literature\non bias detection and mitigation within LLMs is extensive Gallegos et al. (2024); however, most current approaches\nfor detecting biases in text generation examine the models in isolation and out of context. These methods assess the\nLLM using targeted instructions designed to reveal biases relative to specific dimensions of interest, typically through\nopen-ended questions Pit et al. (2024); Scherrer et al. (2024); Shin et al. (2024); Ji et al. (2024); Gupta et al. (2023),\nstructured questionnaires Rozado (2024); Safdari et al. (2023); La Cava and Tagarelli (2024), situational tests Rao\net al. (2023); Argyle et al. (2023); Fontana et al. (2024), or basic text completion tasks Huang et al. (2019); Dhamala\net al. (2021); Sheng et al. (2021); Dong et al. (2023). While these approaches can successfully identify biases, they\nmay not adequately represent the downstream applications in which LLMs are employed. Consequently, models that\nseem unbiased under these assessments may still manifest biases when employed in more complex contexts."}, {"title": "2 Related Work", "content": "Bias Measurement. Previous research has established an extensive set of metrics and benchmarks for quantifying\nbias in language models Delobelle et al. (2022). These methodologies can be broadly categorized into two families.\nThe first approach involves analyzing the models' weights in relation to their likelihood of generating specific\ntokens Delobelle et al. (2022). The second approach treats the models as black boxes, focusing on either global or\nlocal properties of the generated text Gallegos et al. (2024). In this study, we adopt the latter perspective and evaluate\nthe political bias of the model by estimating the stance of arguments it generates.\nWhile many techniques for quantifying bias predominantly rely on targeted questions or text completion tasks,\nseveral studies have investigated biases in downstream applications such as classification Chen et al. (2024),\nsearch Dai et al. (2024), item recommendation Sakib and Das (2024), and task-specific text generation Wan et al.\n(2023); Borah and Mihalcea (2024), often with a particular emphasis on gender biases Kotek et al. (2023). However,\nthese studies do not take into account iterated social interactions between agents.\nPersonas. Large Language Models (LLMs) can be conditioned on simulated human 'personas' characterized by a set\nof identity or personality traits specified in the model's prompt Salewski et al. (2024). This role-playing exercise is\nvaluable for simulating human interactions that reflect various properties of a target population Park et al. (2023).\nEarlier studies have explored personas defined across multiple dimensions, including political orientation Wagner\net al. (2024); Breum et al. (2024), personality traits Zakazov et al. ([n. d.]); La Cava and Tagarelli (2024), and\nsociodemographic characteristics Argyle et al. (2023).\nThe effectiveness of personas remains a subject of ongoing debate. On one hand, the use of personas has\ndemonstrated significant potential in accurately replicating the preferences and voting behaviors of entire\npopulations Argyle et al. (2023). On the other hand, these personas often carry inherent biases that stem from societal\nstereotypes associated with them Gupta et al. (2023). Furthermore, the influence of personas on certain\ntext-generation tasks appears to be marginal Hu and Collier (2024). Similar to our study, previous work showed that\nwhen agents are initialized with OCEAN personality traits Gosling et al. (2003), they generate responses that are\nmore aligned with the assigned personality when queries with one-off prompts than when engaging in a collaborative"}, {"title": "3 Framework", "content": "The core of our experiments consists of simulating interactions among LLM-based agents in a chatroom\nenvironment. The agents are divided into two different groups: social agents and opinion signal agents. The\nspecifications regarding the agents and the chatroom setting are discussed in detail below."}, {"title": "3.1 Social Agents", "content": "The social agents (simply referred to as agents hereafter) are the fundamental building blocks of our experimental\nsetup. They consist of a system prompt specifying their opinion, and a memory module.\nSystem prompts. System prompting is a widely used approach to instruct LLM-agents to behave in a specific way.\nExtensive literature exists on how to prompt LLMs to perform various tasks Ziems et al. (2023), but there is currently\nno broad consensus on the best way of prompting these agents. For our purposes, the system prompt for the social\nagents is designed to infuse them with a specific stance towards a given topic, and it consists of four elements:\n1. A name and an introduction to the setting of the conversation;\n2. A politically-charged opinion on a topic (spelled in the form \"You agree with the statement [STATEMENT].\");\n3. Explanation/elaboration of the given opinion;\n4. An example of what a person with this opinion might say.\nThe opinion-related parts of the system prompt represent the current opinion of the agent on the given topic, and are\nupdated according to the outputs of the opinion signal agent, as described below.\nMemory. The memory module for our social agents allows them to contextualize their responses, by collecting all\nprevious interactions and making them available as context within the prompt. It is based on the LangChain memory"}, {"title": "3.2 Opinion Signal Agents", "content": "To study opinion dynamics in LLM agents, we need a way to extract the opinion of a social agent towards a certain\ntopic. Although ideally this task should be performed by experts to ensure optimal accuracy, relying on human\nfeedback is not practical. Two main alternatives have been proposed in the literature. The first method queries\ndirectly the agents for their own opinions Breum et al. (2024). The latter uses an external model to classify the stance\nof a given generation Chuang et al. (2024). In our experiments, we tested both options and found the first method to\nbe inconsistent in its assessments. An example inconsistency is an agent that generates a message exhibiting\nskepticism towards the existence of climate change, which however claims that climate change is real and\nscientifically proven when queried for its opinion.\nWe therefore opted for the second approach and use a separate opinion signal agent to classify message opinions.\nSpecifically, in our experiments we adopt LLaMa3.1-70B-Instruct\u00b9. The opinion signal agent is prompted to respond\nwith the opinion contained within a given message, and to provide reasoning behind the classification. It functions as\na few-shot classifier as we present it with the five different opinion choices, along with an example of each class (i.e.,\nopinion). The opinion signal agent does not have a conversation history, and only considers its system prompt.\nAlthough this approach can produce uncertainty in classifying messages that require the previous context for being\ndisambiguated, it prevents previous messages from having unwanted effects on the classification.\nTo account for the random variation in the generated answers, we query the opinion signal agent 10 times for each\nmessage and select the opinion that is most frequently selected. In the case of multiple labels being equally frequent,\nwe query the opinion signal agent providing only the tied options as possible answers. Since some messages may not\ncontain any opinion towards the topic of discussion, we create another agent similar to the opinion signal agent which\nwe use as a binary classifier to answer whether an opinion is present in a message before we pass it to the opinion\nsignal agent. We call this the opinion presence agent. Similar to the opinion signal agent, we query the opinion\npresence agent 10 times for a response on whether an opinion is present. Only if an opinion is present, we query the\nopinion signal agent for the actual opinion."}, {"title": "3.3 Experimental Setup", "content": "We combine two elements to simulate online interactions: agents capable of mimicking human behavior and\nreasoning, and an environment in which the agents can interact. Next, we detail how we design our social simulation\nsetup and explore how these agents may behave in real online social environments. We depict a sketch of the\nproposed framework in Figure 1. Notably, the illustrated messages are extracted from an actual conversation between\nour LLM agents.\nChatroom. There exist many different online settings we can choose to simulate, as different social media platforms\nallow for different types of interactions. As an example, in the X (formerly Twitter) platform, messages (tweets) are\npublic and can be directed towards one or more other users. By contrast, Reddit is a forum-like platform, with\nindividuals creating and commenting on posts. We choose to simulate a simpler environment, namely a chatroom (a\nspace where users can engage in conversations about a given topic).\nThere are three key aspects when running our chatroom simulations: The topic of discussion, the number of agents\nN, and the total number of chatroom messages M. In a chatroom, multiple social agents engage in a conversation on\na specific topic. Each agent takes turns sending messages while having access to all messages from other participants.\nThis setup effectively represents a fully connected network of agents. Agents are assigned with an initial opinion, and\nwe use the opinion signal agent to evaluate the opinion of an agent each time they generate a new message. For\ngeneration, at each time step t we randomly select an agent from the set of agents, excluding the one that sent the\nmessage at time step t \u2013 1. This ensures that no agent generates two consecutive messages. The simulation ends\nwhen M messages have been generated.\nNotice that, at the end, the final conversation can still affect the opinion of the agents that did not generate the last\nmessage. To track this, we allow every agent except the one that generated the M-th message to generate an internal"}, {"title": "4 Results", "content": "4.1 Bias in one-shot prompting\nAs a trivial baseline for comparing the political bias of the LLM-agents, we compute the change of opinion by\nconsidering a setting of one-shot prompting. Specifically, we instantiate a Strongly Liberal and a Strongly\nConservative social agent, similarly as in the chatroom setting, and we query them to provide agreement or\ndisagreement cornering a given question, based on a Likert Scale Robinson (2014). To do this, we follow the\napproach of Rozado Rozado (2024)."}, {"title": "4.2 Dyadic Conversations", "content": "To conduct the experiments in the chatroom environment, we assume a dyadic setting (N = 2 agents), and simulate\n100 echo chamber conversations per topic, each consisting of M = 20 messages. All agents are initialized to\nstrongly agree with either the original or the complement statement, thus resulting in 50 Liberal and 50 Conservative\necho chamber simulations per topic.\nFirst, we compute the percentage of simulations where an unwarranted opinion change occurs. As a first observation, we notice that the models exhibiting no (or negligible) unwarranted opinion change via\none-shot prompting (e.g., ChaGPT-40, LLaMa3.1, Qwen2.5), actually show a conversational bias in this setting.\nSecond, differently from the previous experiment, most of the LLM agents also exhibit opinion changes towards the\nConservative pole, despite the drift towards the Liberal position remaining more prominent. Further, the estimated\nbias is not consistent across the topics: for instance, while some models exhibit the greatest shift on \u201cClimate\nchange\" (Mixtral and Zephyr), others show a more prominent bias on \u201cMarijuana legalization\u201d (Llama3.1 and\nQwen2.5) or towards \"Racial Attitude\" (ChatGPT3.5 and Gemma1.1) or \"Healthcare\" (ChatGPT-40). Several\nmodels, however, also exhibit significant bias on \"Abortion\u201d (Qwen2.5), \u201cGender Identity\" (ChatGPT3.5, Gemma1.1\nand Zephyr), and \"Immigration\u201d (ChatGPT3.5, Gemma1.1 and Mixtral).\nFurther, we analyze how many agents changed opinion during the conversation. The results are displayed in Figure 4.\nFirst, we can see that, independently from the topic, at least one Conservative agent always changed opinion towards\nthe Liberal pole. Despite the opposite being rare, in all the configurations also one Liberal agent exhibits a drift, over\nat least one topic. However, the most significant drift remains left-oriented. Surprisingly, we have found that, in\nseveral configurations, both agents change opinions more frequently than just a single agent. This may be because,\nonce an agent changes their opinion, it becomes more challenging for the interlocutor to remain attached to their pole.\nFinally, we investigate how the conversation length affects the emergence of the bias. Specifically, we select some"}, {"title": "5 Discussion", "content": "Summary of findings. By comparing the results of Figures 2 and 3, we can observe that even LLMs that in principle\nare unaffected by bias on one-shot prompting, when engaged in interactive conversation, exhibit significant shifts in\nthe stance expressed in the generated messages. Our results suggest that detecting conversational bias is not trivial,\nand several aspects should be considered before drawing conclusions. In particular, we reveal that the observed bias\nis neither consistently aligned with the topic being discussed nor with the specific model employed. For instance, the\nsame agent may demonstrate little to no bias\u2014or only a negligible bias\u2014when addressing one topic, yet display a\nsignificant drift when considering another. Similarly, for a fixed topic, two agents built on different LLMs can exhibit\nvarying degrees of bias. This highlights the complex and context-dependent nature of bias in LLM-based systems,\nsuggesting that both the topic and the model play critical roles in shaping the extent and direction of bias. Following\nthis, we argue that the existing methodologies for assessing biases in generated text seem inadequate for auditing the\nbehavior of LLMs operating within complex social contexts, as even in contexts like echo chambers, more complex\ninteractions witness abrupt and unwarranted opinion changes.\nIn particular, our analysis shows that bias in Large Language Models is a complex issue, which tends to occur across\ndifferent topics and models. Further, we argue that the results presented in Section 4.2 can be generalized to\nconversations involving multiple agents. In this regard, additional experiments performed with N \u2208 [5, 10] reveal that\nthe configurations do not affect the results in terms of bias. These results follow naturally from the structure of the\nproposed framework, in which each agent receives an initial configuration (the system prompt) along with the current\nconversation history. Since agents contribute to the history through their generated messages, increasing the number\nof agents primarily affects the history's length. However, as shown in Figure 5, the observed bias emerges even in\nshort conversations with correspondingly brief histories.\nEthical Implications. Our analysis leads to further considerations from an ethical perspective. Specifically, the\nreported findings suggest that the presence of conversational bias may potentially affect the deployment of LLMs in\nseveral scenarios, which we address in the following.\n1. The safe adoption of LLM agents in social scenarios could be compromised, such as the implementation of\nbots to be introduced in social platforms (e.g., X, Facebook). In particular, the resulting interactions may\ncontribute to amplify societal biases and influence public opinions, thus contributing to the formation and\nstrengthening of echo chambers and polarization or the promotion of harmful and discriminatory content.\n2. Even worse, the deployment in education, mental health support or customer service could result in\nmisleading or harmful outcomes. For instance, in the context of mental health support, the agent may exhibit\nan unwarranted deviation from the original instructions and encourage the user to maintain unhealthy or\ndangerous habits (e.g., drug addiction, depression).\n3. The bias may also affect the studies based on simulated approaches in social contexts (e.g., the evolution of\necho chambers, user polarization, opinion drifts). In fact, without proper calibration, the underlying bias\nmay compromise the effectiveness of such studies and consequently their credibility.\n4. It is also crucial to recognize that these biases can be deliberately exploited by malicious users. Adversaries\ncan strategically probe an LLM agent within a social context, crafting interactions designed to reveal latent\nbiases that may not be immediately apparent. Once identified, these biases can be manipulated to influence\nthe model's responses in predictable ways, potentially steering conversations toward misleading, biased, or\nharmful conclusions. This type of exploitation poses significant risks, particularly in sensitive domains such\nas politics, social discourse, and automated decision-making, where LLMs may unknowingly reinforce\nideological slants or discriminatory patterns."}, {"title": "6 Conclusions and Future Work", "content": "Large Language Models (LLMs) inherit social biases from their training data, which can persist or even worsen in\ntheir generated outputs. While existing bias detection methods\u2014such as questionnaires and situational\ntests-identify biases in isolated settings, they fail to capture how LLMs behave in real-world multiagent interactions.\nThe framework proposed in this paper is aimed at detecting conversational biases in multiagent LLM systems. By\nsimulating chatroom debates with AI agents initially holding strong opinions, we observe unexpected opinion\nshifts especially in conservative echo chambers-suggesting a latent liberal bias in many models. Notably, this bias\nis undetectable using conventional bias evaluation techniques. BesidesThe findings highlight the need for more\nadvanced, context-aware bias detection and mitigation strategies."}]}