{"title": "CONTROLLABLEGPT: A Ground-Up Designed\nControllable GPT for Molecule Optimization", "authors": ["Xuefeng Liu", "Songhao Jiang", "Bo Li", "Rick Stevens"], "abstract": "Large Language Models (LLMs) employ three popular training approaches:\nMasked Language Models (MLM), Causal Language Models (CLM), and\nSequence-to-Sequence Models (seq2seq). However, each approach has its strengths\nand limitations, and faces challenges in addressing specific tasks that require con-\ntrollable and bidirectional generation, such as drug optimization. To address this\nchallenge, inspired by the biological processes of growth and evolution, which\ninvolve the expansion, shrinking, and mutation of sequences, we introduce CON-\nTROLLABLEGPT. This initiative represents the first effort to combine the ad-\nvantages of MLM, CLM, and seq2seq into a single unified, controllable GPT\nframework. It enables the precise management of specific locations and ranges\nwithin a sequence, allowing for expansion, reduction, or mutation over chosen\nor random lengths, while maintaining the integrity of any specified positions or\nsubsequences. In this work, we designed CONTROLLABLEGPT for drug opti-\nmization from the ground up, which included proposing the Causally Masked\nSeq2seq (CMS) objective, developing the training corpus, introducing a novel\npre-training approach, and devising a unique generation process. We demonstrate\nthe effectiveness and controllability of CONTROLLABLEGPT by conducting exper-\niments on drug optimization tasks for both viral and cancer benchmarks, surpassing\ncompeting baselines.", "sections": [{"title": "1 Introduction", "content": "The Generative Pre-trained Transformer (GPT) [19, 69] has achieved significant success in appli-\ncations such as ChatGPT [44, 65] for chatbox, Copilot [7] for code generation and VideoGPT [68]\nfor video creation. However, GPT operates unidirectionally and lacks controllability [18], which\nmeans it still faces the challenge in handling the real-world scenarios that require both controllable\nand bidirectional generation capabilities, as dictated by the current design of GPT.\nOn the other hand, drug discovery [8] has become increasingly important since the advent of COVID-\n19 [42]. The search for more effective drugs is becoming more urgent but remains underexplored. De\nNovo Drug Discovery incurs billions of dollars in costs and still confronts a high failure rate in its\nearly stages [56]. Drug Improvement addresses the limitations of De Novo drug discovery by building\nupon existing FDA-approved drugs. The DrugImprover framework [39] leads the way in tackling\ndrug optimization by using Tanimoto similarity [33] to maintain beneficial properties while targeting\nmultiple objectives with its novel Advantage-alignment Policy Optimization (APO) algorithm. It also\nprovides a specialized dataset for optimizing drugs against COVID and cancer proteins. REINVENT\n4 [25, 26, 41], in further, utilize the advanced generative capabilities of Transformers and large\nlanguage models (LLMs) to address the drug optimization problem."}, {"title": "2 Related Work", "content": "2.1 Causally Masked Language Modeling\nCausal Language Modeling (CLM) is an autoregressive method employed in models such as GPT-\n4 [2], predicting the next token using only prior token information. While effective in applications"}, {"title": "2.2 Sequence-to-Sequence Modeling", "content": "Seq2Seq, or Sequence-to-Sequence models [15, 24, 66, 60, 43], employ an encoder-decoder structure\nwhere the encoder interprets the input sequence, and the decoder constructs the output sequence.\nThis method is frequently utilized in tasks such as machine translation [11, 55, 61], summariza-\ntion [45, 51], and question-answering [54, 64]. Due to their ability to manage complex tasks that\nrequire transforming input into output, Seq2Seq models are highly versatile and suitable for a broad\nspectrum of NLP applications. Nevertheless, Seq2seq models exhibit limitations in coherence, context\nunderstanding, handling variable-length inputs, training efficiency, and capturing bidirectional context\nwhen compared to CLMs and MLMs. In this study, we introduce Causally Masked Seq2seq (CMS)\nmodeling, conceptualizing the seq2seq model as a controllable conditional mutation component in\nbiological sequences, and harnessing the strengths of seq2seq models, CLMs, and MLMs."}, {"title": "2.3 Controllable Generation.", "content": "In the field of computer vision, the introduction of generative adversarial networks [23] enhanced\nthe quality of image generation. Subsequent research focused on methods to control the generative\nprocess and improve the estimation of generative distributions [31, 12, 5]. In the realm of natural\nlanguage processing, language models are often developed as conditional models tailored for specific\ntext generation tasks [10, 53, 49]. Typically, prompts created by models or those written by humans\nserve merely as a rough starting point for the generated text. This raises questions about how to achieve\nmore explicit control over text generation. Recent advancements in transformer architecture [59, 46]\nand diffusion models [27] have led to improved control in both text and image generation [34, 30,\n47, 36, 16, 70, 37]. However, these techniques are not specifically adapted for biological sequences,\nwhich may involve unique challenges in nature, such as expansion, reduction, or mutation at specific\nlocations and ranges with desired properties. CONTROLLABLEGPT is specifically designed for\nbiological sequences and addresses these challenges by introducing a novel CMS objective."}, {"title": "2.4 Large Language Models for Drug Optimization", "content": "Large language models have been employed in molecule generation, as evidenced by studies such as\nMolGPT [6], C5T5 [48], and ChemGPT [20]. More recently, ERP [67] has utilized LLMs for drug\ndiscovery. In contrast, our work focuses on the drug optimization domain to improve upon existing\ndrugs rather than designing from scratch. In the drug optimization domain, DrugImprover [39] starts to\neffectively define the drug optimization problem by using reinforcement learning with a combination\nof multiple objectives. Moreover, it integrates Tanimoto similarity [32] as an additional term in the\nrewards function to ensure that the RL-fine-tuned model generates molecules similar to existing\ndrugs. However, DrugImprover employs an LSTM as the generative model, which has limitations\nin scalability, capacity, and contextual understanding. Reinvent 4 [25, 26, 41] has made efforts in\ndeveloping transformer-based generative models and achieving state-of-the-art performance in the\nDrug Optimization domain with an emphasis on pretraining with simply adopt REINFORCE [63]\nfinetuning. Although pretraining aids in producing molecules that resemble those in the training\ndataset, it naturally limits the scope of exploration because of biases inherent in the training data. In\naddition, REINVENT 4 lacks controllability during generation, a crucial aspect for drug optimization.\nIn this study, we tackle controllability issues and surpass the current state-of-the-art, REINVENT 4,\nin drug optimization benchmarks."}, {"title": "3 Preliminaries", "content": "LLM. Let $X = [X_1,X_2,\\cdots, X_n]$ be a sequence of tokens representing an input sentence (prompt),\nwhere each $x_i$ is a token from a vocabulary V. Let $Y = [Y_1, Y_2,\\ldots, Y_T]$, $Y_i \\in Y$ be the output\nsequence of tokens with vocabulary Y. V and Y are potentially different vocabularies. Note that\n$Y_{<t} = [Y_1,\\ldots, Y_{t-1}]$, $Y_T := Y$. T represents the length of sequence. Each training corpus begins\nwith a start token [BOS], follows with a sequence of tokens y where each $y_i$ belongs to V, and\nconcludes with a termination action [EOS]. Each molecule is depicted using a sequence of tokens y\nto assemble a SMILES string, applicable to both incomplete and complete molecular structures. Let\nus denote o as string concatenation, and let V* represent the Kleene closure of V. The set of training\ncorpus C is defined as: $C := {[BOS] \\circ v \\circ [EOS] | v \\in V^* }$.\nThe LLM generator policy $\\pi_\\theta$, which is parameterized by a deep neural network (DNN) with learned\nweights $\\theta$, is defined as a product of probability distributions: $\\pi_\\theta (y|x) = \\prod_{t=1}^T \\pi_\\theta (y_t|x, y_{<t})$, where\n$\\pi_\\theta (y_t|x, y_{<t}) = P(y_t|y_{<t}, X)$ is a distribution of next token $y_t$. The text generation decoding\nprocess is designed to select the most probable hypothesis from all possible candidates by addressing\nthe following optimization problem: $y^* = \\arg \\max_{y\\in Y^T} \\log \\pi_\\theta (y|x)$.\nCLM. CLM is a variant of language modeling where the model is trained to estimate the probability\nof $x_i$ conditioned on the preceding tokens $X_{<i>, which could be formulated\nas $\\max_\\theta \\sum_{i=1}^n \\log P (x_i|X_{<i>; \\theta)$, where $P (x_i|X_{<i>; \\theta)$ is the conditional probability of observing\ntoken $x_i$ given all the preceding tokens $X_{<i>. Causal Language Modeling is particularly powerful for\ngenerating text, as it conditions on all previous tokens, ensuring that each generated word is based on\nthe full history of the text generated so far.\nMLM. In MLM, a subset (around 15%) of the tokens in X is randomly selected and replaced with\na special token [MASK]. Let us denote this masked sequence as M and unmasked sequence as S,\n$S = {x_i }, X_i \\in X$ and $x_i \\notin M$. The objective of the MLM is to predict the original tokens of the\nmasked positions based solely on the unmasked context S, which can be represented as maximizing\nthe likelihood: $\\mathcal{L}_{MLM} = \\prod_{i \\in M} P(x_i|S; \\theta)$, where $P (x_i|S; \\theta)$ represents the conditional probability\nof observing token $x_i$ given the context provided by the unmasked tokens in S. $\\theta$ represents the\nparameters of the model. The parameters $\\theta$ of the model are optimized to maximize the likelihood of\nthe correct tokens at the masked positions. During training, the model learns to utilize the surrounding\ncontext to predict the masked tokens, which helps it develop a deep understanding of language\nstructure and usage. MLM has proven effective for pre-training language models that are later\nfine-tuned for various downstream tasks.\nSeq2Seq. Sequence-to-sequence (seq2seq) modeling is a framework in natural language processing\ndesigned to convert sequences from input sequence to output sequence. Seq2seq models typically\nconsist of two main components: an encoder and a decoder, with model parameter $\\theta_{enc}$ and $\\theta_{dec}\nrespectively. The encoder processes the input sequence X to a fixed-dimensional vector representation\nc to capture the semantic or contextual information. The decoder's objective is to generate the target\nsequence Y given the encoded representation c. The objective in training seq2seq models is typically\nto maximize the log likelihood of the correct output sequence Y given the input sequence X\nacross a dataset of paired sequences: $\\max_{\\theta_{enc},\\theta_{dec}} \\sum_{(x,y)} \\log P (Y|X)$, where P is product of the\nconditional probabilities of each output token and $P (Y|X) = \\prod_{j=1}^T P (y_j|Y_{<i>, c; \\theta_{dec})$. Training\ninvolves adjusting both the encoder and decoder parameters to optimize this objective. Seq2seq\nmodels are powerful because they can handle variable-length input and output sequences and are\ncapable of learning complex transformations between different types of sequence data.\nLimitation. The existing models -CLM, MLM, and seq2seq- have limitations in controllable\ngeneration, which is especially important for drug optimization tasks that need to preserve specific\nstructures and allow expansion, shrinking, or mutation at specific positions. The current state of\nthe art in drug optimization, REINVENT 4, although it incorporates various similarity metrics in\nbuilding the training corpus for pre-training the transformer model, still does not yield ideal results\ndue to a lack of controllability in generation. The beneficial structure of the original drug often fails\nto preserve. In this work, we propose CONTROLLABLEGPT, which effectively addresses above\nlimitations of current GPT models in controllable drug optimization."}, {"title": "4 CONTROLLABLEGPT", "content": "In this section, we propose CONTROLLABLEGPT, a ground-up designed GPT model for molecule\noptimization. We first introduce the novel Causally Masked Seq2seq (CMS) Objective as the\nfoundation of CONTROLLABLEGPT. Then, we discuss the design of GPT, including designing the\ntraining corpus, a pre-training strategy, and a generation process.\n4.1 Causally Masked Seq2seq (CMS) Objective.\nMasked, causal, and seq2seq language modeling each offer unique benefits and limitations. Masked\nmodels encode bi-directional contexts but only decode about 15% of the tokens during training.\nCausal models, being decoder-only, process every token but are restricted to left-to-right contexts.\nSeq2seq models are versatile yet often lack bidirectional context and precise generation control. To\ncombine the strengths of MLM, CLM, and seq2seq models and draw inspiration from biological\nmolecule evolution using SMILES representation\u2014which allows for molecular expansion, shrinking,\nand mutation\u2014we introduce the Causally Masked Seq2seq (CMS) Objective. The CMS objective\nenables per-token generation, incorporating optional bidirectional and seq2seq functionality for\ngreater adaptability. It allows for precise control over specific positions and spans within sequences,\nsupporting the expansion, contraction, or mutation of segments while maintaining the integrity of\ndesignated areas. The construction of the CMS objective involves the following steps:\nDesigning the corpus. Our methodology for developing the CMS objective to a SMILES [62]\nstring of length L begins with the most basic corpus suitable for the CLM objective as C =\n{[BOS], x1,\uff65\uff65\uff65,x\u0442, [EOS]}.\nBlending the MLM objective. We then build the MLM objective on top of CLM. It involves a\nprobability p to determine the total number of tokens to mask as $\\lfloor L \\cdot p \\rfloor$. Let us denote $N \\in \\mathbb{R}^+$ as\nthe number of span of mask in the source document.\n$[BOS'], X_1,\\cdots, X_{idx_1},\\cdots, X_{idx_1+\\lfloor L\\cdot p\\rfloor},\\cdots, X_T, [EOS],\\qquad (1)$\nFor N = 1, let us choose a random starting index $idx_1 \\sim [0, L - \\lfloor L\\cdot p \\rfloor - 1]$, and proceed to mask\ntokens in range $[idx_1, idx_1 + \\lfloor L\\cdot p \\rfloor]$. For N = 2, we divide $\\lfloor L\\cdot p \\rfloor$ into two segments, $m_1$ and $m_2$,\nensuring $m_1 + m_2 = \\lfloor L\\cdot p \\rfloor$ and that each segment's length is uniformly selected from the range\n$[1, \\lfloor L \\cdot p \\rfloor]$. We then identify a starting point $idx_1$ within $[0, L - \\lfloor L \\cdot p \\rfloor - 1]$ for the first mask span\nand a second starting point $idx_2$ from the range $[idx_1 + m_1 + 1, L - m_2 - 1]$ for the second mask\nspan, ensuring that the two masked segments are non-overlapping and sequentially ordered in the\nSMILES string. Following the same strategy for selection and masking of these segments, we could\nreach for any N. For the nth span of mask, we replace the span by the token $\\text{<mask\\_n : L \\cdotp>}$,\nwhere n and L. p represents for the nth masked segment with size hint length L. p, which specify\nthe desired length of text to generate for replacing the mask conditioning on tokens length. Finally,\nwe reposition the masked spans to the end of the SMILES string, maintaining their sequence order as\nillustrated in Fig. 2. In this work, we embed the size hint within the mask token as <mask_i : n> to\navoid the ambiguity seen in prior works Aghajanyan et al. that use <mask_i>n. This format prevents\nmisinterpretation by models, as numerical values in chemical structures can indicate ring closures or\nchain lengths."}, {"title": "Blending the seq2seq objective.", "content": "Finally, we establish CMS objective by applying seq2seq objective\non top of MLM and CLM. Initially, we train a GPT model using the MLM and CLM objectives,\ndenoted as $\\pi_{CM}$. Given a SMILES string, we randomly mask a seq2seq span starting at position\n81 and of length L, ensuring it does not overlap with previously masked spans, while regard the\nremaining tokens as Z. Our goal is to transform this s2s span $[x_{s1}, \\ldots, x_{s1+L}]$ into a target span with\ndesired length Lt. To create this training corpus, we utilize $\\pi_{CM}$ to generate Lt tokens $[m_1,\\ldots,m_{Lt}]$\nwith regarded to Z. We then construct the training corpus by mapping the s2s span to the subsequence\ngenerated by $\\pi_{MLM}$.\n$X_1,...,x_{s1-1}, \\text{<mask\\_1 : Lt>}, X_{s1+L+1},\\cdots,x_T, \\text{<mask\\_1 : Lt>} \\rightarrow [m_1,\\ldots, m_{Lt}]$\n$X_1,\\ldots, (s2s\\_i\\_Lt : X_{s1}, \\cdots, X_{s1+L}),...,x_T, (s2s\\_i\\_L^{+}: X_{s1},\\ldots, X_{s1+L}), [M_1,\\ldots,m_{L_t}]$\nwhere $(s2s\\_i\\_Lt :X_{s1},\\cdots,X_{s1+L})$ denotes the seq2seq objective conditioned on a specific subse-\nquence $X_{s1},...,x_{s1+L}$ and its bidirectional unmasked tokens. The index i indicates the i-th span,\nand Lt represents the target length of the generated subsequence. Unlike conventional sequence-to-\nsequence models, our work on seq2seq is also conditioned on and benefits from the bidirectional\ncontext surrounding the seq2seq span. This approach allows for the incorporation of task-specific\nlength priors into prompts, resulting in outputs that are more precise and controlled."}, {"title": "4.2 The Design of the Controllable GPT.", "content": "Pretraining. In this work, we propose a novel three-phase training approach to train a GPT model\nunder CMS objective.\nIn the initial phase. Our objective is to train a LLM specifically designed for understanding molecules.\nThis training employs a CLM approach. CLM is an autoregressive technique where the model\nlearns to predict the next token in a sequence based solely on the preceding tokens. This creates a\nunidirectional context model, which means it only considers past information and ignores any future\ncontext when making predictions. For this phase, the model is trained on a dataset comprised of texts\nabout ligands. This dataset enables the model to accurately learn the representation of compounds,\nincluding their chemical structures and properties.\nIn the second phase. Building on the success of the LLM developed in Phase 1, which demonstrated\nhigh accuracy in generating molecular structures, we proceed to refine the model's training. This\nphase employs a causally masked objective with multiple mask tokens, each with a size hint, as\nillustrated in Figure 2. In this phase, the model, denoted as $\\pi_{CM}$, benefits from both Causal Language\nModeling (CLM) and Masked Language Modeling (MLM), which enhance CLM's performance\nby utilizing bidirectional context. $\\pi_{CM}$ is capable of generating molecules in a controlled manner,\nspecifying both the target length and the position for expansion.\nIn the third phase. Ultimately, we achieve building the GPT under CMS objective by further refining\nthe causally masked model,$\\pi_{CM}$, through the integration of a sequence-to-sequence objective. We\ntrained our model, denoted as $\\pi_{CMS}$, using the training corpus outlined in Fig. 3 to refine the\ncausally masked model $\\pi_{CM}$ developed in Phase 2. This advancement aims to enhance the model's\ncontrollable generation in terms of both contraction and mutation. It mimics the mutation behavior in\nbiological sequences. Thus, our $\\pi_{CMS}$ achieves controllable generation in expansion, contraction,\nand mutation at specific positions or ranges, in either a random or specified length."}, {"title": "Loss function.", "content": "Instead of altering the standard cross-entropy loss to consider the loss from predicting\nmasked tokens negligible, we treat masked tokens like regular tokens, subject to the usual loss\ncalculations. This method is used because our training data may contain multiple masked tokens, each\nwith size hint information indicating the number of tokens to generate in place of the mask. Thus, it's\ncrucial to accurately predict both the presence of these masked tokens and their corresponding size\nhints."}, {"title": "Generation Process.", "content": "The prompt, output string, and generated SMILES for CONTROLLABLEGPT\ncan be viewed in figure 4 and figure 5. More specifically, in the process of generating new molecular\nstructures, CONTROLLABLEGPT employs a method that either modifies existing molecules or adds\nnew elements to them without altering the original essential structure, showcasing the flexibility\nand precision of the model in generating novel molecular designs. This is illustrated through two\nexamples:\nModifying the Original Molecule: Initially, two segments of the original molecule's SMILES string are\nidentified and replaced with mask and seq2seq token respectively, which are placeholders indicating\nwhere and how long the new segments should be. These mask tokens are then processed by the model,\nwhich generates new segments in their place. The generated segments, highlighted in green, are\nmanually repositioned to replace the original masked segments, effectively changing the molecule's\nstructure and construct a new molecule. This process is depicted in Fig. 4, where the mask and\nseq2seq token are shown in red and the newly generated segments in green. The caption for Fig. 4\nexplains this process in detail, emphasizing the manual reintegration of generated tokens.\nAdding to the Original Molecule Without Modification: In this scenario, instead of replacing parts of\nthe SMILES string, one mask token and one seq2seq token are inserted at random positions within\nthe string. These tokens serve as prompts for the model to generate new molecular segments that are\nthen manually inserted into the specified positions, expanding the original molecule without altering\nits existing structure. This approach is visualized in Fig. 5, with the mask tokens again represented in\ngreen and the generated segments in red. The caption for Fig. 5 provides a clear explanation of this\nadditive process."}, {"title": "5 Experiments", "content": "The language model. We employ the Byte Pair Encoding (BPE) method [22, 50] to initially\npre-train our tokenizer using raw SMILES strings, and GPT-2-like Transformers for causal language\nmodeling. We use the standard 11M Drug-like Zinc dataset for training, excluding entries with empty\nscaffold SMILES. The dataset is divided into a 90/10 split for training and validation, respectively.\n(see Appendix A.1 for more details).\nDataset. We employ, from the most recent Cancer and COVID dataset of Liu et al. [39], 1 million\ncompounds from the ZINC15 dataset docked to the 3CLPro (PDB ID: 7BQY) protein associated\nwith SARS-CoV-2 and the RTCB (PDB ID: 4DWQ) human cancer protein.\nBaselines. In this study, we use baseline models such as DrugImprover [39], which leverages an\nLSTM-based generator fine-tuned with APO, Molsearch [52], a search-based strategy utilizing Monte\nCarlo Tree Search (MCTS) for molecule generation and optimization, MIMOSA [21], a graph-based\nmolecular optimization method driven by sampling and DrugEx v3[40], which utilizes transformer-\nbased reinforcement learning for scaffold-driven drug optimization. Additionally, we incorporate the\ncurrent state of art model, REINVENT 4, proposed by He et al. [25, 26], Loeffler et al. [41], which\ntrains a transformer to follow the Matched Molecular Pair (MMP) [29, 57] guidelines. Specifically,\ngiven a set {(X, Y, Z)}, where X represents source molecule, Y the target molecule, and Z the\nproperty change between X and Y, the model learns a mapping from (X, Z) \u2208 X \u00d7 Z \u21d2 Y\u2208 Y\nduring training. REINVENT 4 defined six different kinds of property change Z, including MMP\nfor user-specified changes, different similarity thresholds, and scaffold-based alterations, where\nmolecules share the same scaffold or generic scaffold. All baselines are fine-tuned using the cancer\nand COVID dataset according to their respective fine-tuning methods.\nCritics and evaluation metric. We evaluate seven key attributes for pharmaceutical drug discovery:\n1) Average normalized reward is the average of the normalized values of the docking score, drug-\nlikeness, synthesizability, solubility, and similarity across all valid molecules. This is regarded as the\nmost crucial metric.; 2) Average top 10% normalized reward is the average of the normalized reward\nof the top 10% of molecules based on their average normalized reward; 3) Docking score (generated,\nfor efficient calculation, with a surrogate docking model: see Appendix A.6) evaluates the potential\nof a drug to inhibit the target site. 4) Druglikeness assesses the probability of a molecule being a\nsuitable drug candidate; 5) Synthesizability measures the synthesizability of a molecule, assigning a\nscore of 1 for easy synthesis and a score of 10 for difficult synthesis [17]; and 6) Similarity evaluates\nthe similarity between original and generated SMILES using Tanimoto similarity."}, {"title": "5.1 Main results", "content": "Table 1 illustrates the performance comparison between CONTROLLABLEGPT and the competing\nbaseline methods. The results indicate that CONTROLLABLEGPT outperforms the competing\nbaselines across all the metrics except for synthesizability. Notably, CONTROLLABLEGPT achieves\nthe highest Tanimoto similarity score, surpassing both the current state-of-the-art, REINVENT 4, and"}, {"title": "5.2 Ablation studies", "content": "Adding to the original molecule without modification. Table 2 (Task 1) visual-\nizes the addition to the original molecule while preserving the complete original struc-\nture. In this experiment, a given original molecule with the SMILES representation\nO=C(Nc1ccccc1C(=O)n1cnc2ccccc21)c1ccc[nH]c1=O serves as the basis. Our objective is to ex-\ntend the ring in the molecule. We designed the prompt by adding a mask token < mask1 : 7 > to\nthe specific position adjacent to the ring in the SMILES. Finally, we obtained the generated molecule\nwith the desired features (additional ring in red) while maintaining the completeness of the original\nmolecule structure. This study demonstrates the ability of CONTROLLABLEGPT to extend at specific\npositions with a specific length.\nModifying the Original Molecule. In this experiment, our goal is to alter a portion of the original\nmolecule by modifying bonds and atoms connecting the two rings. For this purpose, we construct\nthe prompt by substituting the original structure Nc1cc with a masked token < mask1 :3 >.\nTable 2 (Task 2) illustrates the modification of the original molecule by removing the ring and\nintroducing a few atoms, while retaining the majority of the structure. This demonstrate the ability of\nCONTROLLABLEGPT by modifying partial of molecule and random generated in specific length.\nConditional Modifying to the Original Molecule: Contraction and Expansion. This experiment\naims to showcase conditional modifications to the original molecule. Unlike Task 1&2, where the\nfocus is on modifications and expansion in a random manner, here we concentrate on generating\nsubsequences conditioned on a partial molecule. We undertake two tasks: expanding and shrinking\npartial molecules based on a given subsequence. For the simplification task, we successfully reduce\na length 5 subsequence, Nc1cc, to a length 2 token using the prompt token < s2s_1_2 : Nc1cc >.\nConversely, for the expansion task, we extend the subsequence to a length of 10 tokens using the\nprompt token < s2s_1_10 : Nc1cc >. Both tasks yield the desired molecules, as depicted in\nTable 2 (Task 3&4). This demonstrates that CONTROLLABLEGPT is capable of generating molecules\ncontrollably for contraction and expansion, conditioned on specific segments of the molecule, to\ntarget specific lengths of subsequences.\nPenicillin Toxicity Reduction. In this study, we utilize the ToxSmi Model [9], which was trained\non the Tox21 [1] dataset, encompassing 12 different types of environmental toxicities. The toxicities\nreported in Table 2 (Tasks 5&6) represent the sum of 12 toxicity scores. The original molecule,\nPenicillin, has a predicted toxicity score of 2.54. Our proposed controllable methods demonstrate\na significant reduction in the toxicity scores of the generated molecule, while preserving the core\nscaffold structure for preseving desired beneficial properties."}, {"title": "6 Conclusion", "content": "In this study, we introduce the novel Causally Masked Seq2Seq (CMS) objective and CONTROL-\nLABLEGPT, which allows precise control over specific sequence areas for expansion, reduction, or\nmutation while preserving key regions and biologial structure. CONTROLLABLEGPT demonstrated\nsuperiority over eight competing baselines in Covid and Cancer drug optimization benchmarks, main-\ntaining high Tanimoto similarity and enhancing drug properties. It also demonstrated its controllability\nthrough specific examples in ablation studies. This method highlights CONTROLLABLEGPT's capa-\nbility for precise generation in drug optimization tasks, despite its limitations. For future directions,\nwe encourage applying CONTROLLABLEGPT in fields beyond our current research scope."}, {"title": "A Appendix", "content": "A.1 Pre-training Details\nWe used the ZINC dataset, filtering for Standard, In-Stock, and Drug-Like molecules, resulting in\napproximately 11 million molecules.\nIn the second phase of pre-training, we first trained for 10 epochs using a single mask. Subsequently,\nwe trained for another 40 epochs with an equal probability of using either one or two masks. For\neach epoch, the masks were regenerated to create a more comprehensive masked dataset.\nIn the third phase of pre-training, we applied different mask configurations with specific probabilities:\n[one mask (0.1), two masks (0.1), one mask and one seq2seq (0.4), two masks and one seq2seq\n(0.4)] and train 20 epochs.Similar to the second phase, the masks were regenerated for each epoch to\nenhance the comprehensiveness of the masked dataset.\nA.2 Baselines fine-tuning datasets\nAs outlined in Section 5, all baseline models are fine-tuned using the Cancer and COVID dataset,\nfollowing their respective fine-tuning methodologies. For this process, we utilize one million\ncompounds from the ZINC15 dataset, docked to the 3CLPro protein (PDB ID: 7BQY), which is\nlinked to SARS-CoV-2, and the RTCB protein (PDB ID: 4DWQ), associated with human cancer.\nThese datasets, sourced from the latest Cancer and COVID dataset by Liu et al. [39], are consistently\napplied across all baselines.\nAdditionally, these datasets are employed for molecular generation in our proposed methods, with\nfurther details on the generation process provided in Section A.3.\nA.3 Generation\nFor each mask and seq2seq, we utilize three random variables: the start index, the number of tokens\nto be masked, and the number of tokens to be generated. During generation, we apply two settings:\n[one mask + one seq2seq, and two masks], resulting in a total of six random variables for each setting.\nDuring the generation phase, we randomly sample these six variables 10,000 times, using them as\nprompts for generation, regardless of whether the generated SMILES are valid or not. In addition, for\na given prompt molecule, we adopt TOPPK [67] for generation strategy.\nAfter generation, for each prompt molecule/SMILES, we select the top 10 generated\nmolecules/SMILES based on their average normalized reward. The mean of these top 10\nmolecules/SMILES is then used to obtain the final result for the prompt molecule/SMILES.\nA.4 Baseline REINVENT 4\nFollowing are detailed description of six different kinds of property change Z included in REINVENT\n4 He et al. [26, 25]\n\u2022 MMP: There are user-defined desirable property changes between molecules X and Y.\n\u2022 Similarity > 0.5: The Tanimoto similarity between molecules X and Y is greater than 0.5.\n\u2022 Similarity \u2208 [0.5,0.7): The Tanimoto similarity between the pair (X, Y) ranges from 0.5\nto 0.7.\n\u2022 Similarity > 0.7: The Tanimoto similarity between molecules X and Y is greater than 0.7.\n\u2022 Scaffold: Molecules X and Y share the same scaffold.\n\u2022 Scaffold generic: Molecules X and Y share the same generic scaffold.\n\u0391.5 \u0392\u03a1\u0395 Tokenization\nByte Pair Encoding (BPE) is a tokenization algorithm initially designed for data compression and\nlater adapted for use in NLP, particularly in the preprocessing of text for deep learning models. The\ncore idea behind BPE is to iteratively merge the most frequent pair of consecutive bytes (or characters\nin the context of text) into a single, new byte (or token), thereby reducing the size of the data to be"}, {"title": "A.6 Surrogate model", "content": "The surrogate model [58"}]}