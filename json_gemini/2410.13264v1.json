{"title": "THE LATENT ROAD to Atoms:\nBACKMAPPING COARSE-GRAINED PROTEIN STRUC-\nTURES WITH LATENT DIFFUSION", "authors": ["Xu Han", "Yuancheng Sun", "Kai Chen", "Kang Liu", "Qiwei Ye"], "abstract": "Coarse-grained(CG) molecular dynamics simulations offer computational effi-\nciency for exploring protein conformational ensembles and thermodynamic prop-\\perties. Though coarse representations enable large-scale simulations across ex-\ntended temporal and spatial ranges, the sacrifice of atomic-level details limits\ntheir utility in tasks such as ligand docking and protein-protein interaction predic-\ntion. Backmapping, the process of reconstructing all-atom structures from coarse-\ngrained representations, is crucial for recovering these fine details. While recent\nmachine learning methods have made strides in protein structure generation, chal-\nlenges persist in reconstructing diverse atomistic conformations that maintain ge-\nometric accuracy and chemical validity. In this paper, we present Latent Diffusion\nBackmapping (LDB), a novel approach leveraging denoising diffusion within la-\ntent space to address these challenges. By combining discrete latent encoding\nwith diffusion, LDB bypasses the need for equivariant and internal coordinate ma-\nnipulation, significantly simplifying the training and sampling processes as well\nas facilitating better and wider exploration in configuration space. We evaluate\nLDB's state-of-the-art performance on three distinct protein datasets, demonstrat-\ning its ability to efficiently reconstruct structures with high structural accuracy\nand chemical validity. Moreover, LDB shows exceptional versatility in capturing\ndiverse protein ensembles, highlighting its capability to explore intricate confor-\nmational spaces. Our results position LDB as a powerful and scalable approach for\nbackmapping, effectively bridging the gap between CG simulations and atomic-\nlevel analyses in computational biology.", "sections": [{"title": "1 INTRODUCTION", "content": "Coarse-Grained Molecular Dynamics (CG-MD) simulation has become an indispensable tool in\ncomputational biology for simulating large biomolecular systems (Das & Baker, 2008; Liwo et al.,\n2014; Kmiecik et al., 2016; Souza et al., 2021; Majewski et al., 2023; Arts et al., 2023). Through\ngrouping atoms into super-atoms or beads, CG models significantly decrease computational require-\nments and allow the observation of long-time processes such as folding, aggregation, and self-\nassembly (Lequieu et al., 2019; Shmilovich et al., 2020; Mohr et al., 2022). However, CG rep-\nresentations inherently sacrifice atomistic details of protein structures, limiting their application to\na bunch of important downstream tasks in drug discovery, such as molecular recognition, signaling\npathways deciphering, and allosteric sites prediction (Badaczewska-Dawid et al., 2020; Vickery &\nStansfeld, 2021; Zambaldi et al., 2024). Under such circumstances, backmapping, i.e., reconstruct-\ning all-atom structures from CG representations, is essential for a comprehensive understanding and\nwider applications of CG-MD (Huang et al., 2016; \u015aled\u017a & Caflisch, 2018; Peng et al., 2019; Kim,\n2023).\nTwo primary challenges are faced with backmapping coarse-grained protein representations to\nall-atom structures. The first challenge is the high dimensionality involved in modeling large\nbiomolecules. Proteins, in particular, consist of thousands of atoms and intricate structural patterns,\nmaking it difficult for models to learn and extract relevant features effectively (Rogers et al., 2023;"}, {"title": "2 RELATED WORK", "content": "Traditional Methods. Traditional backmapping methods utilize rule-based heuristics to generate\ninitial atomic structures (Lombardi et al., 2016), which are subsequently refined through geometric\noptimization or energy minimization (Vickery & Stansfeld, 2021). However, these approaches often\nresult in non-physical imperfections, such as atomic clashes and abnormal bond angles (Xu et al.,\n2019), and the refinement process can be computationally expensive and biased toward specific min-\nimization schemes (Badaczewska-Dawid et al., 2020). Additionally, these methods are deterministic\nand do not capture the thermodynamic diversity of atomic structures that correspond to a single CG\nrepresentation (Yang & G\u00f3mez-Bombarelli, 2023).\nData-driven Methods. Data-driven approaches aim to overcome these limitations by predicting\natomic structures from CG representations. While deterministic models like MLPs (An & Desh-\nmukh, 2020) and SE(3)-Transformers (Heo & Feig, 2023) offer high precision, they struggle with\nthe one-to-many nature of backmapping, leading to reduced structural diversity.\nGenerative models, including GANs (Li et al., 2020; Stieffenhofer et al., 2020; 2021; Shmilovich\net al., 2022) and VAEs (Wang & G\u00f3mez-Bombarelli, 2019; Wang et al., 2022; Yang & G\u00f3mez-\nBombarelli, 2023), address these challenges by learning multimodal distributions of atomic struc-\ntures. However, GANs are often ineffective at modeling complex distributions, and VAEs tend to\nprioritize common structures, limiting their ability to generate diverse conformations.\nRecent work has shown that diffusion models, such as those proposed by Li et al. (2024) and Jones\net al. (2023), are particularly effective for backmapping. These models condition on CG inputs to\ngenerate diverse and detailed atomic structures. However, diffusion in atomic space suffers from\nhigh computational cost and limited flexibility, particularly for large systems. Moreover, the exces-\nsive freedom in exploration can lead to generated structures that deviate from the target conforma-\ntions."}, {"title": "3 BACKGROUND", "content": ""}, {"title": "3.1 PROBLEM DEFINITION", "content": "Notations: Consider an all-atom protein structure as a set of atoms AA = {(xi, vi)}n i=1, where n\ndenotes the number of protein atoms. The vector x = {x1,...,xn} \u2208 Rn\u00d73 represents their three-\ndimensional coordinates, and v \u2208 Rnf signifies the element types of the protein (Guan et al., 2023).\nThe coarse-grained structure of the AA is represented as CG = {(Xi, Vi)}Ni=1, where N < n and\nX = {X1,..., XN} \u2208 RN\u00d73 indicates the CG coordinates, with V \u2208 RNf denoting the amino\nacid types. We define the sets [n] and [N] as {1, 2, . . ., n} and {1, 2, . . ., N}, respectively. The CG\noperation is then characterized by a surjective mapping m : [n] \u2192 [N], which assigns each FG atom\nto a CG atom.\nInternal Coordinates representation: To reconstruct FG structures from CG models, we utilize\nan internal coordinate representation that describes the adjacency relationships among points as\nT = {(di, Oi, Ti)}M i=1, where di denotes bond lengths, \u03b8i represents bond angles, and \u03c4i indicates\ndinedral angles. For each point, we specifically calculate its relative relationships with neighboring\npoints: the bond length to one neighbor, the angle formed with two neighboring points, and the\ndinedral angle involving three surrounding points. Residues with fewer than 13 heavy atoms are\nexcluded.\nProblem Definition: Given a protein's coarse-grained structure, defined by coordinates X and the\ncorresponding amino acid types V, the task of protein backmapping is to generate the corresponding\nall-atom coordinates x, where the atom types v are determined by the amino acid sequence. The goal\nis to learn and efficiently sample from the conditional distribution p(x | X, V). In this work, we rely\non the Co atoms as they provide a robust representation of protein-protein interactions and serve as\na reliable granularity for reverse mapping, following established methods in the field (Badaczewska-\nDawid et al., 2020; Yang & G\u00f3mez-Bombarelli, 2023; Jones et al., 2023)."}, {"title": "3.2 DIFFUSION MODEL FOR CONTINUOUS FEATURES", "content": "The Denoising Diffusion Probabilistic Model (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020)\nis a generative modeling framework that transforms complex data distributions into Gaussian noise\nthrough a forward diffusion process and subsequently learns to reverse this process to generate new\ndata samples. This model leverages the principles of diffusion processes and denoising autoencoders\nto achieve high-quality generative performance.\nForward Diffusion Process: Given a data point xo ~ q(xo), the forward diffusion process pro-\ngressively and independently adds a small amount of Gaussian noise to the data over T time steps.\nUtilizing the properties of Gaussian distributions, we can express the noise adding process and the\ndistribution of xt given xo as:\nq(xt | xt\u22121) = N (xt; \u221a1 \u2212 \u03b2t xt\u22121, \u03b2tI), q(xt | xo) = N (xt; \u221a\u03b1t xo, (1 \u2212 \u03b1t)I),(1)\nwhere \u03b2t \u2208 (0,1) is a predefined variance schedule controlling the noise amount added at each\nstep, I is the identity matrix, \u03b1t = 1 \u2212 \u03b2t and \u0101t = \u03a0t i=1 \u03b1i is the cumulative product up to time\nt. As t approaches T, the distribution of xt converges to a standard normal distribution due to the\ncumulative effect of the added noise.\nReverse Diffusion Process: The reverse diffusion process aims to recover x0 from xT by sequen-\ntially removing the added noise. This process is also modeled as a Markov chain but with learned\nparameters:\np\u03b8(xt\u22121 | xt) = N (xt\u22121; \u00b5\u03b8(xt, t), \u03c3\u03b82I),\nwhere \u00b5\u03b8 (xt, t) is a neural network parameterized by \u03b8, predicting the mean of the reverse transition,\nand \u03c3\u03b8 is the variance, often set to \u03b2t or learned separately.\nTraining Objective: To streamline the learning process, up-to-date methods (Ho et al., 2020) usu-\nally parameterize p\u03b8(xt,t) with the noise component at t timestep with \u03f5\u03b8(x,t), and train the\ndenoising model \u03f5\u03b8 by minimizing the variational bound on the negative log-likelihood:\n\u00b5\u03b8 (xt,t) = 1/\u221a\u03b1t (xt \u2212 \u03b2t/\u221a1\u2212\u0101t \u03f5\u03b8(x, t)), L(\u03b8) = Ex0,\u03f5,t [\u2225\u03f5 \u2212 \u03f5\u03b8 (x, t)\u22252 ]."}, {"title": "4 METHOD", "content": "In this section, we introduce the proposed Latent Diffusion Backmapping (LDB) framework. Our\nwork is inspired by the success of Stable Diffusion (Rombach et al., 2022), which has demonstrated\nthe effectiveness of generating high-resolution images in latent space. However, extending this\nconcept to complex protein structures presents unique challenges (Winter et al., 2022; Xu et al.,\n2023; Hayes et al., 2024). We address these challenges by first compress the complex all-atom\nprotein structure into discrete latent codes, and then apply conditional diffusion in the latent space.\nIn the following sections, we detail the design of the discrete latent encoding and latent diffusion in\nSections 4.1 and 4.2, respectively. An overview of the framework is provided in Figure 1."}, {"title": "4.1 DISCRETE LATENT AUTOENCODING", "content": "We designed a node-level latent representation\nto efficiently compress and represent protein\nstructures, as shown in Figure 2. Unlike tradi-\ntional methods that extract both node and edge\nfeatures, our approach focuses solely on node-\nlevel representations, improving flexibility and\nreducing complexity. This allows the diffu-\nsion model to avoid simultaneous processing of\nnoise addition and removal for both nodes and\nedges, simplifying the architecture.\nTo construct this latent space, we treat each\namino acid as a minimal compression unit, re-\nducing the dimensionality of full-atom struc-\ntures. Given the invariance of protein struc-\ntures under geometric transformations like ro-\ntation and translation, we employ an SE(3)-\nequivariant graph neural network within the\nGenzProt (Yang & G\u00f3mez-Bombarelli, 2023)\nframework to extract robust node-level repre-\nsentations.\nWe used internal coordinates as training targets for autoencoder, which include bond lengths and an-\ngles, ensuring physical consistency in reconstructed structures. This approach is particularly suited\nfor backmapping tasks, as it reconstructs full-atom structures from coarse-grained representations.\nDespite challenges posed by the scarcity and uneven distribution of protein conformation data, we\nemployed a Vector Quantized Variational Autoencoder (VQ-VAE) (Van Den Oord et al., 2017). The\nVQ-VAE discretizes continuous features into a fixed-size codebook, limiting the learning space and\nensuring that bond lengths, bond angles, and dihedral angles fall within reasonable physical ranges.\nTo further enhance efficiency, we compressed the latent representation by mapping it to a lower-\ndimensional space. This decouples the code lookup from the high-dimensional embedding, allowing\nfor the retrieval of latent variables in a lower-dimensional space, which are then projected back into\nthe original embedding. This method improves the training and diffusion processes.\nThe encoder Ef encodes the all-atom structure into latent space z, preserving rotation and transla-\ntion consistency. The latent variables are quantized via a codebook, and the decoder Dry generates\ninternal coordinates, which are used to reconstruct the full-atom structure based on predefined an-\nchor points, following the hierarchical placement algorithm described by (Jing et al., 2022)."}, {"title": "4.2 GRAPH LATENT DIFFUSION", "content": "In this section, we describe the noise addition and removal processes in the latent space, as derived\nearlier. Unlike traditional diffusion models that operate in high-dimensional coordinate space, our\napproach simplifies diffusion by leveraging a lower-dimensional discrete latent codes, avoiding the\ncomplexity of geometric parameters, as shown in Figure 1."}, {"title": "5 EXPERIMENT", "content": "In this section, we evaluate LDB across three diverse protein datasets to demonstrate its broad ap-\nplicability. (1) On the widely-used PED benchmark (Lazar et al., 2021; Ghafouri et al., 2024),\nwhich contains approximately 100 frames with each of the 85 proteins, LDB achieved state-of-the-\nart (SOTA) structural and chemistry accuracy in reconstructing protein structures. (2) On the larger\nATLAS dataset (Vander Meersche et al., 2024), comprising 300 conformations with each of the\n1297 proteins, LDB exhibits superior performance in generating diverse protein ensembles, show-\ncasing its capability in capturing conformational variability. (3) Finally, We demonstrate LDB's\nability to generalize across the extensive PDB dataset (Berman et al., 2000), containing 62,105\nreal-world, single-conformation proteins, highlighting its potential for practical backmapping appli-\ncations. These results collectively underscore the robustness and versatility of the proposed method.\nFor detailed descriptions of the datasets and preprocessing steps, please refer to Appendix A.1."}, {"title": "5.1 EXPERIMENTAL SETTINGS", "content": "Baselines. We selected two recent SOTA backmapping methods as our baselines: GenZProt (Khe-\nmakhem et al., 2020), and DiAMONDBack (Jones et al., 2023). GenZProt is based on the VAE\nframework, which employs two encoders to map full-atom and coarse-grained structures into a la-\ntent space, aligning the two representations. Due to its learning mechanism, the model's learned\nprior distribution does not extend into low-probability regions, limiting its ability to capture the full\ndiversity of protein ensembles. DiAMONDBack utilizes a diffusion-based framework that defines"}, {"title": "5.2 RESULTS ON THE PED DATASET", "content": "The experimental results on the PED dataset, as shown in Table 1, highlight LDB's SOTA perfor-\nmance in addressing backmapping challenges. The PED dataset, a benchmark for medium confor-\nmational space, was used to evaluate the methods. We sampled each protein structure ten times and\nreported the mean and standard deviation to ensure robustness.\nLDB excels in structural accuracy, outperforming GenZProt and DiAMONDBack in RMSD for most\ntest proteins. It also achieves significantly lower GED scores, likely due to the internal coordinate\nrepresentation, which helps maintain valid bond lengths and preserves the original graph structure.\nThis enables LDB to explore a broad conformational space while maintaining fine-grained structural\nprecision, critical for backmapping tasks.\nIn terms of efficiency and structural validity, LDB consistently delivers superior or competitive re-\nsults across clash, interaction, and GDR metrics. This demonstrates that LDB not only produces\naccurate structures but also ensures their chemical and physical validity. Its leading clash score\nindicates fewer unrealistic atomic overlaps, while strong interaction and bond graph accuracy re-\nflect adherence to expected chemical interactions. Although DiAMONDBack produces reasonable"}, {"title": "5.3 RESULTS ON THE ATLAS DATASET", "content": "The results on the ATLAS dataset, as shown in Table 2 (left half), demonstrate LDB's ability to\nhandle significantly larger and more diverse conformational spaces than PED. Given the extensive\nvariety of proteins in the test set, we selected examples with the best and worst clash loss generated\nby our method for illustration. The ATLAS dataset includes 15 times more proteins and spans a\nconformational space 300 times larger than PED, making it a considerably more complex challenge.\nImportantly, we did not include DiAMONDBack in this analysis, as its reproduced results exhibited\nexcessive GED errors. Upon further inspection of the generated structures, we observed frequent\ngraph structure disconnections, likely due to the vast conformational space of the ATLAS dataset,\nwhich caused DiAMONDBack to produce overly diverse and erroneous structures that deviated from\nthe intended targets.\nRegarding structural accuracy, LDB consistently outperforms both GenZProt in RMSD across all\nATLAS test sets, particularly achieving the lowest RMSD scores in both the overall and worst-case\nscenarios. This highlights LDB's ability to accurately reconstruct protein structures across a wide\nrange of conformations. The GED results further reinforce this observation, where LDB exhibits\nsignificantly lower GED values, indicating its capacity to maintain the correct bond graph structure\neven in the challenging ATLAS dataset.\nIn terms of structural validity, LDB also leads in metrics such as Clash and Interaction scores,\nachieving fewer steric clashes and preserving physical interactions more effectively than the base-\nlines. The consistently lower GDR values across all test cases underline LDB's superior capability\nin generating chemically valid and physically realistic structures, ensuring that even within larger\nand more diverse conformational spaces, the model remains robust and reliable.\nThe visualization of generated samples, as shown in Figure 5, further exemplifies LDB's ability\nto produce realistic and valid protein structures in challenging conditions. These results substan-\ntiate LDB's SOTA performance and validate the effectiveness of our approach in addressing both\nchallenges of large-scale conformational exploration and computational efficiency."}, {"title": "5.4 RESULTS ON THE PDB DATASET", "content": "The results on the PDB dataset, as shown in Table 2 (right half), demonstrate LDB's robustness\nin handling large static datasets with over 60,000 single-conformation proteins\u2014700 times more\nthan PED. Unlike dynamic datasets such as ATLAS and PED, PDB contains steady-state structures\nwithout molecular dynamics data, posing the challenge of reconstructing static structures in the\nabsence of conformational diversity.\nLDB achieves superior or competitive results in RMSD and GED compared to GenZProt and Di-\nAMONDBack, particularly excelling in overall RMSD and both best- and worst-case structures.\nThis highlights LDB's consistent ability to reconstruct high-fidelity structures, even without confor-\nmational diversity. GED results further confirm the model's ability to maintain structural integrity\nacross a wide range of protein types.\nIn terms of structural validity, LDB outperforms the baselines in Clash and GDR scores, ensuring\nboth accuracy and physical plausibility. LDB also achieves the highest Interaction scores, preserving\ncritical atomic interactions essential for functional analysis. These results confirm LDB's capability\nin generating chemically valid, physically realistic steady-state structures.\nFigure 6 visually illustrates LDB's effectiveness, showcasing its ability to produce structurally sound\nresults on real-world protein data. Overall, LDB demonstrates consistent superiority in accuracy\n(RMSD, GED) and structural validity (Interaction, GDR), without sacrificing inference efficiency,\nmaking it well-suited for large-scale applications in protein modeling and drug discovery."}, {"title": "5.5 ABLATION STUDIES", "content": "To evaluate the contributions of key model components, we conducted ablation studies on the PED\ndataset, comparing our discrete latent space approach (VQ-VAE+diffusion) with two alternatives:\na continuous latent space model (VAE+diffusion) and a flow-based variant (VQ-VAE+flow). Flow\nmatching, known for balancing stochasticity and structure in recent tasks, offers efficient probability\nflows (Irwin et al., 2024; Jing et al., 2024), but for protein backmapping tasks with large confor-\nmational spaces, diffusion's ability to explore diverse conformations proves more effective. Each\ncomponent plays a distinct role in improving structural accuracy and validity.\nFirstly, our discrete latent space (VQ-VAE) shows clear advantages over the continuous VAE-based\nmethod. By discretizing the latent space, our model can better preserve the bond graph consistency,\nwhich is crucial for maintaining accurate internal structures. This is reflected in the significantly\nlower GED scores as shown in Table 3. The discrete space effectively reduces errors related to bond\nlengths and angles, which leads to better structural precision.\nSecondly, the diffusion process proves superior to the flow-based approach (VQ-VAE+flow) in han-\ndling large conformational spaces. Diffusion leverages stochastic noise, which allows for explo-\nration across diverse conformations while maintaining structure validity. This is evident in the"}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced LDB, a denoising diffusion backmapping method operating in la-\ntent space. By implicitly incorporating equivariance and internal coordinates into a discrete low-\ndimensional node-level latent representation, we effectively preserved structural information while\nsimplifying the diffusion process, thereby enhancing both efficiency and performance. This method\naddresses the inefficiencies and accuracy challenges of direct diffusion in coordinate space, as well\nas the difficulties in learning simple prior distributions that struggle to capture diverse conforma-\ntional spaces. Our experiments demonstrate that LDB achieves SOTA accuracy across various\ndatasets while maintaining higher structural validity. For future work, we aim to extend this frame-\nwork to model continuous time trajectories, which will allow better prediction of dynamic protein\nbehaviors. Additionally, this versatile framework can be adapted for other tasks in protein design\nand beyond."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 DATASET PREPROCCESS", "content": "PED: The PED contains structural ensembles of various proteins, including numerous intrinsi-\ncally disordered proteins (IDPs). In line with the approach taken by the GenZProt model (Yang &\nG\u00f3mez-Bombarelli, 2023), we initially selected 88 proteins from the PED dataset. To ensure com-\npatibility with prior work, we further filtered out three proteins\u2014PED00125e000, PED00126e000,\nand PED00161e002\u2014that contain non-canonical amino acids, following the methodology of Di-\nAMONDBAck. This left us with a total of 85 proteins for training. For evaluation purposes,\nwe used the same test set as previous studies, consisting of four PED proteins: PED00151ecut0,\nPED00090e000, PED00055e000, and PED00218e000, which contain 20 to 140 frames, and the\nremaining proteins were used for training.\nATLAS: The ATLAS dataset consists of all-atom molecular dynamics (MD) simulations for 1,390\nnon-membrane proteins, each chosen to represent all eligible ECOD structural classes (Schaeffer\net al., 2017). For each protein, three replicate simulations of 100 ns are provided, with each sim-\nulation containing 10,000 frames. Following the preprocessing steps used in the Alphaflow frame-\nwork (Jing et al., 2024), 300 conformations per protein were randomly sampled for training. To\nmaintain consistency in our experment, we excluded 95 sequences with lengths greater than 512\nresidues. The final test set was composed of proteins whose corresponding PDB entries were de-\nposited after May 1, 2019.\nPDB: The PDB dataset comprises protein structures from the Protein Data Bank (PDB), collated in\nthe SidechainNet extension of ProteinNet. In accordance with the preprocessing strategy used by\nDiAMONDBAck (Jones et al., 2023), we filtered out sequences with incomplete side-chain coordi-\nnates for non-terminal residues, as well as configurations with Ca-Ca distances outside the 2.7-4.1\n\u00c5 range. Additionally, we removed sequences containing four or more disconnected chains and\nthose with fewer than five residues. After these steps, we retained 65,360 structures for training.\nFinally, we further refined the dataset by excluding 3,270 structures with sequence lengths greater\nthan 512 residues, ensuring a robust dataset for our experiments."}, {"title": "A.2 EVALUATION METRICS", "content": "Root Mean Squared Distance (RMSD): The RMSD calculates the average distance between cor-\nresponding atoms in two structures, effectively quantifying the difference between the real and re-\nconstructed structures, thereby assessing the quality of the reconstruction.\nGraph Edit Distance (GED): The quality of generated samples is assessed based on how well\nthey retain the original chemical bond graph structure, quantified by the graph edit distance ratio\n\u0393(Ggen, Gtrue) between the generated graph and the ground truth graph.\nSteric Clash Score: The generated structure should have a reasonable atomic distribution. We\nreport the ratio of steric clashes among all atom-atom pairs; a distance smaller than 1.2 \u00c5 between\nany two atoms is considered a steric clash (Yang & G\u00f3mez-Bombarelli, 2023; Jones et al., 2023).\nInteraction Score: We define the Interaction Score as a single value to evaluate the physical plau-\nsibility of the generated structures. This score captures two types of interactions: (1) hydrogen\nbonds, ion-ion interactions, and dipole-dipole interactions between atom pairs within 3.3 \u00c5; and (2)\n\u03c0-\u03c0 stacking interactions among aromatic ring pairs (PHE, TYR, TRP, HIS) with center distances\nsmaller than 5.5 \u00c5. The Interaction Score is computed as:\nL = \u2211(x,y)\u2208A max(||x \u2212 y||2 \u2212 4.0, 0.0) + \u2211(x,y)\u2208P max(||x \u2212 y||2 \u2212 6.0, 0.0)\nwhere A and P are sets of pairs of atoms and pairs of aromatic rings, respectively. A lower score\nindicates that the generated structure better preserves expected atomic interactions, reflecting a more\nphysically reasonable configuration.\nGraph Difference Ratio (GDR): We evaluate the structural accuracy of generated molecules using\nthe GDR, which compares their bond connectivity to that of a reference structure. The reference"}, {"title": "A.3 REPRESENTATION FOR PROTEIN STRUCTURE", "content": "Protein structure representations are essential for tasks such as protein design, folding prediction,\nand structural backmapping. Numerous approaches have been developed to represent protein struc-\ntures in a computationally efficient manner. Below, we discuss several common methods of repre-\nsentation.\nVoxel Representation Voxel representations divide 3D space into a grid, where each voxel in-\ndicates the presence or absence of atoms. This method provides a clear way to capture spatial\ninformation, but it can be computationally demanding due to the high dimensionality of the voxel\ngrid, especially when applied to large macromolecules. It is mainly utilized in tasks that require\nexplicit spatial reasoning, such as molecular docking simulations. Several studies (Masuda et al.,\n2020; Stieffenhofer et al., 2020; 2021; Shmilovich et al., 2022) have implemented atomic density\ngrids, allowing for the entire molecule to be generated in one step by producing a density over the\nvoxelized 3D space. However, these grids lack the desirable property of equivariance and often\nnecessitate separate fitting algorithms, which adds complexity to the modeling process.\nCoordinate Representation Coordinate representation captures the precise spatial arrangement\nof each atom in a protein using Cartesian coordinates, making it a standard approach in many molec-\nular modeling techniques. This method effectively preserves the geometric properties of protein\nstructures, facilitating accurate modeling tasks. However, directly integrating Cartesian coordinates\ninto deep learning models presents challenges, particularly the need for translational and rotational\ninvariance, which necessitates specific constraints within the network. Furthermore, the high dimen-\nsionality of coordinate data increases computational complexity, especially in large-scale datasets,\nwhile uneven data distribution can impede learning efficiency. Consequently, advanced learning\nstrategies are often required to address these challenges (Hoogeboom et al., 2022; Wu et al., 2022).\nInternal Coordinate Representation The internal coordinate representation utilizes bond\nlengths, bond angles, and dihedral angles to reduce the degrees of freedom compared to Carte-\nsian coordinates, resulting in a more compact and efficient representation (Jing et al., 2022; Eguchi\net al., 2022). This approach inherently encodes the geometric constraints of molecular structures,\nenhancing computational efficiency while eliminating redundant spatial information. It is particu-\nlarly well-suited for backmapping tasks, where known reference points facilitate the reconstruction\nof full-atom coordinates. By relying on internal coordinates, the process conforms to the physical\nand chemical constraints of the system, enabling the accurate and efficient generation of all-atom\nstructures.\nLatent Representation Latent diffusion models have demonstrated significant success across var-\nious generative tasks, including image (Vahdat et al., 2021), point cloud (Zeng et al., 2022), text (Li\net al., 2022), audio (Liu et al., 2023), and molecular generation (Xu et al., 2023). In the context\nof protein structures, latent representations offer a compact and efficient method for modeling by\nembedding them into lower-dimensional spaces, thereby simplifying both the generation and de-\nsign processes. (Xu et al., 2023) introduced a geometric latent diffusion model for 3D molecular\ngeneration that ensures roto-translational equivariance within the latent space, enhancing the mod-\neling of small molecular geometries. (Fu et al., 2024) proposed a latent diffusion model that adeptly\ncaptures protein geometry, facilitating the efficient generation of novel protein backbones through\nlatent node and edge features. Similarly, (Hayes et al., 2024) employed latent space modeling to\nsimulate protein evolution, showcasing its capability to co-design protein sequences and structures.\nCollectively, these methods reduce computational complexity while preserving high-quality protein\ngeneration and designability."}, {"title": "A.4 SAMPLE STRUCTURE", "content": ""}]}