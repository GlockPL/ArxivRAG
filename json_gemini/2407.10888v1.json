{"title": "Leveraging Multimodal CycleGAN for the Generation\nof Anatomically Accurate Synthetic CT Scans from\nMRIS", "authors": ["Leonardo Crespi", "Samuele Camnasio", "Damiano Dei", "Nicola Lambri", "Pietro Mancosu", "Marta Scorsetti", "Daniele Loiacono"], "abstract": "In many clinical settings, the use of both Computed Tomography (CT) and Magnetic Resonance (MRI) is necessary to pursue\na thorough understanding of the patient's anatomy and to plan a suitable therapeutical strategy; this is often the case in\nMRI-based radiotherapy, where CT is always necessary to prepare the dose delivery, as it provides the essential information\nabout the radiation absorption properties of the tissues. Sometimes, MRI is preferred to contour the target volumes. However,\nthis approach is often not the most efficient, as it is more expensive, time-consuming and, most importantly, stressful for the\npatients. To overcome this issue, in this work, we analyse the capabilities of different configurations of Deep Learning models to\ngenerate synthetic CT scans from MRI, leveraging the power of Generative Adversarial Networks (GANs) and, in particular, the\nCycleGAN architecture, capable of working in an unsupervised manner and without paired images, which were not available.\nSeveral CycleGAN models were trained unsupervised to generate CT scans from different MRI modalities with and without\ncontrast agents. To overcome the problem of not having a ground truth, distribution-based metrics were used to assess the\nmodel's performance quantitatively, together with a qualitative evaluation where physicians were asked to differentiate between\nreal and synthetic images to understand how realistic the generated images were. The results show how, depending on\nthe input modalities, the models can have very different performances; however, models with the best quantitative results,\naccording to the distribution-based metrics used, can generate very difficult images to distinguish from the real ones, even for\nphysicians, demonstrating the approach's potential.", "sections": [{"title": "1 Introduction", "content": "Computed Tomography (CT) serves as a widely spread imaging modality for diagnostic purposes across the medical field, as it\nis one of the most reliable and accurate techniques available, and it allows the visualisation of internal anatomical structures in\na non-invasive way precisely. Another widely used imaging technique is Magnetic Resonance (MRI), which offers distinct\nviews of the targeted anatomical district, evidencing areas with a high content of hydrogen, such as fat and soft tissues; the two\ntechniques are not interchangeable as they are based on radically different physical principles and are used to highlight different\ntissues and structures, depending on the clinical necessity\u00b9.\nThese two tools are only occasionally used side by side, as, typically, there is a specific clinical question to answer that\nrequires only one. However, scenarios where having both is necessary do exist: it is the case for specific radiotherapy (RT)\ntreatments, like Total Marrow and Lymphoid Irradiation (TMLI)2, where there is the necessity of whole-body contouring,\nbetter performed exploiting the rich contrast in MRI images. With contouring, we refer to the process of identifying and\ndelineating the anatomical structures of interest in the images, which is a crucial step in the RT treatment planning process as\nit allows to define the target volumes to be irradiated with the dose and the tissues to be spared, thus minimising the risk of\ndamaging healthy tissues and the occurrence of unwanted side effects. CT scans are extensively used for this task, bearing the\nadvantage of being necessary for every RT treatment to create the delivery plan, as they contain the specifics about the radiation\nabsorption properties of the patient's tissues, which are fundamental to computing the dose distribution, and seldom offer\nsufficient information to contour regions of interest (ROI) adequately. In some cases, however, with whole-body treatments\nbeing a prominent example, radiation oncologists prefer using MRI\u00b3 for the contouring.\nAn approach with the double examination introduces, however, a series of issues from different standpoints: primarily, for\nthe patients, it means having to undergo two distinct examinations, which adds a lot of stress and discomfort, both physical and\npsychological; it has to be noted that, in the case of TMLI for instance, the patients in question are already in a very fragile\nstate: generally, they are affected by haematological malignancies, like acute leukaemia or lymphoma, and are undergoing\na bone marrow transplant; also, they seldom are towards the more senior side of the population. For these reasons, every\nadditional source of stress is not desirable. Secondly, performing two scans is more expensive and requires more time, delaying\nthe treatment time because of the time needed to take the exams and process and use the images for the planning. Another\nreason for additional concern in specific scenarios might be the extra dose of ionising radiation provided by the CT scanner;\nhowever, in the case of TMLI and RT in general, this is not particularly relevant as the dose delivered during the treatments\nmakes the radiation absorbed during the acquisition negligible. Moreover, the MRI and the CT have to be co-registered, adding\na non-trivial step to the process and a possible additional source of error.\nFrom these premises, it can be inferred how a system capable of providing physicians with both CT and MRI from a single\nacquisition would be a great advantage as it would significantly reduce costs and time, improve the efficiency of the protocols\nand diminish the stress for the patients. Deep Learning (DL) can help in this sense, as with the recent advancements in the\nfield, models based on Convolutional Neural Networks (CNN) have shown great promise in transforming the image domain;\nin this work, we aimed at studying this task, applied to the transition from MRI to CT, as in the case previously described,\nthe relevant anatomical structures of most significant interest for the use-cases are most evident in MRI images. Incidentally,\nsuch a tool would be a precious resource to generate accurate and realistic data in a field where data is notoriously relatively\nscarce and difficult to obtain. Having a system capable of generating accurate synthetic medical images would make it possible\nto increase the richness of the dataset of choice, including patients from, possibly, different hospitals and populations, thus\nreducing the risks of overfitting and bias in the learnt representations4. Also, generated data is less subject to privacy concerns\nas the synthetic images do not generally carry with them information that would allow to track back the original patient, or they\nmight even be used as harmonisation tools to transform the original data and prevent leakages.\nThe core objective of this work revolves around leveraging advancements in artificial intelligence and deep learning for\nimage-to-image translation between the two imaging modalities. The aim is to synthesise CT images from available MRI\ninputs, with a vision to mitigate the operational and financial burdens on healthcare practitioners and patients and reduce\nthe associated psychological or physical challenges from obtaining dual imaging modalities. Said images must represent the\nsame anatomical structures, with their geometry, opacity and specific textural properties, to be as medically and anatomically\naccurate as possible. Unpaired image-to-image translation architectures have been analysed to select suitable architectures\nto achieve these objectives. The focal anatomical district in this work is the abdominal region, encompassing critical organs\nsuch as the liver, spleen, kidneys, and the inferior section of the lungs. This anatomical selection offers a broader relevance\ncompared to other regions; for instance, while significant, studies centred on cerebral images are inherently specific to that\nregion only, bearing less universality with the rest of the body as the variety of tissues is less diverse. The investigative lens is\nprimarily focused on CycleGAN architectures. This work dissects the generation capabilities of various models trained to\nyield CT scans from different data sources and connects the generation with the combination of multiple MRI modalities. The\nmethodology described encompasses diverse evaluation strategies to gauge model performance, ascertaining the effectiveness\nof generative models without a paired validation dataset. This work is carried out in collaboration with medical personnel from\nthe Humanitas Research Hospital, who qualitatively assessed the images generated from a medical perspective."}, {"title": "2 Related Works", "content": "DL image generation, not only in medical imaging but in general, has been a very active research field in the last few years.\nMany interesting applications are available, especially for artistic and media-related purposes, as they can be highly catchy for\nthe general public. The use of image generation in medical imaging is, however, mainly related to the generation of synthetic\ndata, which can be used for a variety of purposes, from data augmentation to create missing modalities in MRI,6. In general,\nthe common denominator for this kind of system and application is the use of Generative Adversarial Network (GAN), which,\nintroduced in 2014 by Ian Goodfellow et al.7, constitute a framework employing two distinct models: a generator G and a\ndiscriminator D, that play a min-max game, respectively trying to generate realistic data and to distinguish between real and\ngenerated data.\nThe architecture's potency emanates from the adversarial\nloss capability to manifest a distribution in the synthetic im-\nages analogous to the actual data distribution. Since GAN's\ninception, numerous methodologies drawing on its founda-\ntional principle have emerged, including the CycleGAN5,\nour principal reference model, which endeavours to produce\nimages indistinguishable from the target dataset. Several\nGAN variants with enhanced network architectures and\nmathematical optimization techniques have been introduced\nsince its first appearence9\u201316. A category of GANs referred\nto as conditional GANS (CGANs) facilitates conditional im-\nage generation from the generator, producing images with\nspecific attributes17,18. In this sense, both CycleGAN and\nPix2Pix19, among the most influential models, fall under the umbrella of CGANs and cater to image-to-image translation,\nconverting an input image into a synthetic image that meets a specific criterion. These architectures allow data creation via\ndomain adaptation, implying that the trained models can incorporate adjustments from one data source to another20. Data\naugmentation is one of the foremost applications of GANs in medical imaging, a notorious field for scarce data availability\nwhere this kind of tool is a necessity: several efforts have been made to develop systems to address this particular problem\nexploiting the capabilities of this family of neural models with particular success targeting classification tasks21\u201326. Specifically,\nXu et al.23 introduced a semi-supervised attention-guided CycleGAN for MRI image augmentation aimed at brain tumour\nclassification, capable of producing tumour images from normal ones and vice versa. Muramatsu et al.21 can be an illustrative\nexample of CycleGAN utilized to create mammographic masses from lung nodules, aiming at data augmentation for breast\nmass classification. Wang et al.25 ascertained that merging WGAN27 with progressive growth28 outperformed the WGAN with\ngradient penalty (WGAN-GP)29 and Pix2Pix GAN19 for pulmonary adenocarcinoma classification; they incorporated a stitch\nlayer into the generator net (dubbed StitchGAN) to merge low-dimensional images into a comprehensive image, simplifying the\nintricacy of full-size image transformation and generating MRI image pairs for clinically significant classification. Ge et al.24\ndevised a pairwise generative architecture to craft synthetic multimodal MRI images, thus augmenting a multimodal dataset\ntailored for brain tumour classification. In another innovative twist, Yu et al.26 suggested substituting the conventional CNN\nwith a Capsule network30 in the GAN's discriminator, enhancing the modelling of the image's hierarchical relationships. An\nadditional difficulty to image synthesis, particularly relevant in the medical field, is posed when the necessity is to find data\naugmentation methods for segmentation tasks, as in this case, the synthetic images have to be paired with the corresponding\nlabels, which in this context typically refers to an image where pixels or voxels are associated with a category index, denoting\nspecific relevant objects. Several recent works have obtained significant results31\u201336. Most of these proposed methodologies are\ngrounded in the domain adaptation concept discussed previously. For instance, semantic labels of anatomical structures might\nproduce coherent artificial medical images. When paired with the labels, the resulting images can be incorporated into the\ntraining dataset for a segmentation network. Cao et al.36, and Shin et al.31 adopted this label image translation technique to\nproduce PET and CT images, as well as atypical brain MRI images, respectively, aiming at tumour segmentation. Augmented\nlabels, allowing tumour location, shape, and size alterations, were the input for generating synthetic images showcasing\ndiverse anatomical characteristics. Such cross-domain translation can also be observed between various imaging techniques:\nSandfort et al.32 employed a CycleGAN to convert contrast-enhanced CT images to their non-contrast-enhanced counterparts,\ntargeting organ segmentation. Conversely, Jiang et al.33 orchestrated pseudo-MRI image synthesis from CT images, regulated\nby a pair of concurrently trained GANs, focusing on lung tumour segmentation within MRI. These papers have preserved\nstructural correspondence between synthetic images and labels derived from the original image sources, a key aspect. A\ncomprehensive analysis by Skandarani et al.37 evaluates the performance of GANs trained on medical datasets for generating\nsynthetic images that remarkably resemble original images. This analysis juxtaposes various GAN architectures, shedding\nlight on the most effective ones for image generation and convincingly demonstrating the potential of these generated images\nto deceive human experts in a visual Turing test. However, this work focuses on image translation, specifically synthesizing\na synthetic CT scan corresponding to a provided MRI. For tasks of image-to-image translation, an exemplary methodology\nwas presented by Armanious et al. 38, who utilized a GAN architecture amalgamated with non-adversarial losses, facilitating\nthe translation of PET to CT for brain imagery with promising results. The MedGAN framework found applicability in\nan unsupervised setting, as demonstrated in another study by the same authors39. A dual experiment incorporating paired\nand unpaired data for the translation of brain CT to MR was introduced by Cheng-Bin Jin et al.40 through the MR-GAN\narchitecture. Other explorations in image generation entail the generation of absent modalities, as exemplified in the study\nby Alogna et al.41. This research utilized multi-input pix2pix and GAN architectures to generate absent MRI brain image\nmodalities, specifically with T1-weighted scans (both with and without contrast enhancement) and T2-weighted alongside\nT2-fluid-attenuated inversion recovery scans. Another noteworthy endeavour by Gao et al.42 aimed at generating CT scans\nfrom sub-par beam CTs, employing three distinct architectures, pix2pix, CycleGAN, and AGGAN, to assess the efficacy in this\nhigh-definition image generation task. Studies have also focused solely on unpaired data generation, especially concerning\nbrain tumour scans. A pertinent example would be the investigation by Jelmer M. Wolterink et al.43, which employed the\nclassic CycleGAN architecture introduced by Zhu et al.5 to synthesize CT scans using sagittal MRI scans of the human brain as\ninput. This study compares the performance of the CycleGAN in this particular task with traditional paired image-to-image\ntranslation techniques, highlighting instances where models trained on unpaired data surpassed their paired counterparts in"}, {"title": "3 Methodology", "content": "The models presented in this work are based on the CycleGAN architecture and make use of different combinations of input,\nresulting in single input and multimodal networks, depending on the number of images taken in input; the input is always\n2D, using single slices of both MRI and CTs and keeping the complexity lower than with a three-dimensional approach, as\nsometimes in medical imaging this allows for better outcomes44. Regarding the implementation, the generator architecture is\ninspired by Johnson et al.'s design45. It incorporates nine residual blocks sandwiched between two downsampling and two\nupsampling blocks, as shown in figure 2a. The downsampling blocks are built with, in sequence: convolution, normalization,\nand activation (ReLU). Similarly, after the residual blocks, there are two upsampling blocks with the same scheme except for\nusing transpose convolutions instead of regular convolutions, aimed at increasing the size of the layer's input.\nThe discriminator draws its architecture from the PatchGAN design, detailed in the Pix2Pix paper19. This approach assesses\nthe authenticity of 70 \u00d7 70 overlapping patches within the source image. Such a design permits a reduced parameter set\nin comparison to a full-image discriminator and offers the flexibility to process images without concerns about their size;\na high-level scheme is shown in figure 2b. Two types of CycleGAN networks were used in this study: single-input and\nmultimodal: the former uses a single slice as an input, while the latter takes two corresponding slices from two different\nmodalities of the same slice. Often, T1 MRIs are acquired in both in-phase (IP) and out-of-phase (OOP) modalities, and it\nis possible to use both of them as input for the model, as their differences can help distinguish fat from tissues with similar\nproperties\u00b9. The same cannot be said for T2 MRI, as they have different purposes than T1-weighted ones, and it is less frequent\nthat they are both acquired. Therefore, developing a system using both would be more challenging to apply in a clinical setting.\nThe multimodal CycleGAN is configured to accept 2 or 3 inputs for the MRI-to-CT generator, with the reverse transformation\nproducing the corresponding 2 or 3 outputs. Consequently, there is a dedicated discriminator for the CT image and one for\neach MRI modality. In the forward generator, a single branch is used for each input modality to extract features, which are\nsubsequently merged into a latent space for output CT generation 2c. Conversely, the backward generator begins with a singular\ninput that transitions to its specific latent space, facilitating the generation of multiple MRI modalities as output to keep cycle\nconsistency. The feature extraction block at the end of the input branches is organized as one convolution block, followed by\ntwo downsampling blocks and four residual blocks.\nSeveral setups have been tested, varying inputs and modalities, to understand how the model behaves using different\nmodalities:\n\u2022 All-in: single input model, trained with both in-phase and out-of-phase T1 MRI and contrast-enhanced CT, all data from\nthe Chaos Dataset;\n\u2022 In-phase: single input model, trained with in-phase only MRI and contrast-enhanced CT, all data from Chaos Dataset;\n\u2022 In-phase+: single input model trained with T1-weighted MRI from the Chaos Dataset plus some from the Cancer\nImaging Archive to test a more varied dataset, CT are contrast-enhanced from the Chaos dataset;\n\u2022 T2: single input model with T2 weighted MRI and contrast-enhanced CT from the Chaos dataset;\n\u2022 In 'n out: 2-input model, with correlated in-phase and out-of-phase T1 MRI images and contrast-enhanced CT, all\nimages from Chaos Dataset;\n\u2022 In 'n out HUM: 2-input model, in-phase and out-of-phase MRI from Chaos Dataset and non-contrast-enhanced CT from\nthe Humanitas Dataset;\n\u2022 Triple: 3-input model, with correlated T1(in-phase and out-of-phase) and T2 MRI images and contrast-enhanced CT, all\nimages from Chaos Dataset.\nThree sets of images have been used in this study:\n\u2022 Chaos CT: CT scans of the abdominal region from the CHAOS challenge46,47; 40 scans from different patients are\ncontained, from potential liver donors with no tumours, lesions or other pathologies; the orientation and alignment are\nthe same for all the images, which consist of 16-bit DICOM files with a resolution of 512 \u00d7 512, x-y spacing between 0.7\nand 0.8mm, and 3 to 3.2 mm inter-slice axial distance, corresponding to an average of 90 slices per scan with a total of\n1367 images; all the scans are contrast-enhanced.\n\u2022 Chaos MRI: MRI scans of the same region from the CHAOS challenge; it contains 40 patients for which two sequences\nwere acquired; T1-Dual, which is a combination of in-phase and out-of-phase T1-weighted images, and T2; 12-bit\nDICOM images with 256 \u00d7 256 resolution are contained in the set; the inter-slice axial distance varies between 5.5 and\n9mm, x-y spacing is between 1.36 and 1.89mm and the average number of slice is 36, with a total of 1594 slices. They\nare not paired with the CT scans or from the same patients, making it impossible to use a supervised method as no ground\ntruth is available.\n\u2022 AUTOMI CT: full body CT scans from the Humanitas Research Hospital in Milan; 100 patients are contained in DICOM\nfiles with 16-bit depth and 512 \u00d7 512 resolution, 3 \u2013 5mm axial distance and 1 \u2013 2mm pixel spacing; the scans are not\ncontrast-enhanced and are not paired with any MRI; the abdominal region has been cropped from the whole body scans\nto match the other sets, resulting in 5970 images. These sets were acquired from patients undergoing TMLI or similar\ntreatments.\nCT sets are used independently so that models are trained either on contrast-enhanced images or non-contrast-enhanced ones,\nbut not both together as they are incompatible, and the capability of generating both would imply introducing a conditioning\nto the network, adding further complexity to the task. Conversely, the same MRI set is used for all the experiments, and the\ndesired modalities for each model are selected. It must also be noted that contrast-enhanced CT scans are not generally used in\nRT, and in general, models generating this kind of CT scans would not have broad practical uses in a clinical setting, but they\nremain an interesting benchmark."}, {"title": "Evaluation", "content": "The lack of ground truth to compare the results calls for distribution-based metrics to assess, from a statistical point of view,\nthe similarity of the generated images with the real ones. Three metrics have been selected for this purpose, among the most\npopular in literature: Fr\u00e9chet Inception Distance (FID)48, Kullback-Leibler Divergence (KL) and histogram comparison."}, {"title": "Fr\u00e9chet Inception Distance", "content": "It was introduced specifically for the evaluation of generative models48, GANs in particular, as it gives an estimate of how\ndistant the generated images are from the real ones, using a features space derived from a pre-trained CNN, normally the\nInceptionV3 (hence the name, Inception), stripped of its last layers and only used as a features extractor. The resulting\ndistributions are then compared using the Frechet distance, also called the Wasserstein-2 distance27. The lower the score,\nthe closer the two distributions and, hence, the more similar the images. Notably, FID doesn't merely quantify the image's\ndistribution but aims to assess image similarity akin to human perception. The FID is computed as:\n$d\u00b2((\\mathbf{m}, C), (\\mathbf{m_w}, C_w)) = ||\\mathbf{m} \u2013 \\mathbf{m_w}||2 + Tr(C+C_w - 2(CC_w)^{1/2})$\nHere, (m, C) represents the mean and covariance of the Gaussian distribution derived from the actual image set, while (mw, Cw)\ncorresponds to the generated dataset."}, {"title": "Kullback-Leibler Divergence", "content": "The Kullback-Leibler (KL) divergence can quantify the dissimilarity between two probability distributions. In its approximated\nform49, it computes the difference between the cross-entropy of distributions p(x) and q(x), and the entropy of p(x), represented,\nin this work, respectively the distribution of real CT images and the synthetic one. The formulation used in this work is the\nfollowing:\n$D_{KL}(p,q) = H(p,q) \u2013 H(p) = \\int p(x) \\log (\\frac{p(x)}{q(x)}) dx = -\\int p(x) \\log (\\frac{q(x)}{p(x)}) dx$\nA low KL divergence score implies greater similarity between the compared distributions. Distributions are derived by averaging\nimage histograms, producing an average detail of the pixels' distribution across the sets."}, {"title": "Histogram comparison", "content": "Conceptually similar to the KL divergence, histogram comparison has been used to scrutinize the distribution of the generated\nimages. The methodology for distribution computation mirrors that of the KL divergence, and the histogram is compared\naccording to two metrics:\n\u2022 Correlation:\n$d(H_1,H_2) = \\frac{\\sum_{I}(H_1(I) \u2013 \\overline{H_1})(H_2(I) \u2013 \\overline{H_2})}{\\sqrt{\\sum_{I}(H_1(I) \u2013 \\overline{H_1})^2\\sum_{I}(H_2(I) \u2013 \\overline{H_2})^2}}$\nwhere:\n$\\overline{H_k} = \\frac{1}{N} \\sum_{I} H_k(I)$\nand N stands for the histogram bin count.\n\u2022 Intersection:\n$d(H_1,H_2) = \\sum_{I}min(H_1(I),H_2(I))$\nFor these metrics, high scores indicate a higher similarity. Specifically, Correlation peaks at 1.0 for flawless matching,\nwhile Intersection, being the cumulative minimum value across bins, theoretically ranges from [0, inf). The formulations for the\nmetrics are adapted from the official OpenCV documentation50."}, {"title": "Spectral Analysis", "content": "An additional method used to assess the quality of generated images is spectral analysis, which is used to compare the frequency\ncontent of the images. The inspiration was drawn from the work of Frank et al.51, which put forth the idea of training a neural\nnetwork to differentiate between genuine and synthetically generated images from various network designs by scrutinizing their\nspectrum. This strategy hinged on the hypothesis that the spectrum of synthetic images may manifest anomalies, primarily\nbecause of the upsampling and downsampling operations that transpire during the image-to-image transformation process;\nfor example, transposed convolution layers, often used in GANs, are known to introduce checkerboard-shaped artefacts52. In\nthis work, instead of training a neural network, images have been converted into their spectral representation, and the distance\nbetween generated and original ones has been computed through their correlation."}, {"title": "4 Results", "content": "One of the most pressing challenges in this work is the lack of paired data, as distribution-based metrics serve more as an\nestimate of the actual quality of the generated images; even if the distribution is the same, artefacts and structures may have\nunrealistic shapes or present wrongful intensity patterns while still performing well metrics-wise.\nTwo subsets of the two real CT datasets, contrast and non-contrast-enhanced from Chaos and AUTOMI, have been kept\napart to be compared with the other real images to compute a baseline representing the score obtained from comparing sets\nof real images. The scores of our models are then normalized by dividing by their baseline, showing the ratio between the\nbaseline's score and the models'. This allows for a better comparison of models trained on different kinds of images. Baseline\nrefers to the CHAOS CT baseline, while hum-baseline refers to the Automi CT baseline. All the scores obtained from models\ntrained with contrast-enhanced CT scans are normalized by dividing by the baseline value of their layer. Also, the averaged\nvalue is divided by the average value of its baseline. The non-contrast-enhanced model is divided by the hum-baseline.\nGiven the variability from the upper to the lower anatomical regions of the abdomen and the inconsistency in volumes'\nacquisitions starting or ending at possibly different heights, the anatomical distribution may be substantially different between\nthe two compared datasets; consequently, images were categorized into axial layers depending on their position along the\naxial axis. Ten layers were defined based on the size of observable organs (liver, spleen, kidneys, and lungs) to harmonize the\nanatomical differences, enhanced by the different sources and settings and usually particularly evident at the borders; The split\naims to consider anatomical subsections of the body which contain similar structures and compare them against each other"}, {"title": "FID", "content": "The detailed and average scores of the FID are presented in fig. 5. The findings indicate that the superior scores are derived\nfrom the multi-input CycleGAN when utilizing in-phase and out-of-phase T1-weighted MRI images. This consistency is\nevident in models trained with contrast-enhanced and non-contrast-enhanced CT, with the former aligning closely with its\nbaseline (hum-baseline). A decline in performance can be noted with the addition of T2-weighted images as the MRI input,\nwhich is somewhat unexpected. A possible reason might be the subtle misalignment of T2 images with corresponding T1\nscans, attributed to different acquisition frequencies, which introduces ambiguity and compromises the outcome. Among the\nsingle-input models, the model using T2 images underperformed, hinting at the relative inefficacy of T2-weighted images\nas a source for synthetic CT. The darker values of T2 images, compared to T1, potentially render the contrast of the scans\nless favourable than their counterpart to generate accurate distributions. Other models demonstrated commendable outcomes,\nnotably the All-in and In-phase models. Their results are closely aligned with the In-n-out score. However, performance\ndeteriorated with varied image types, as observed in the In-phase+ model.\nA pattern in the results showcases subpar outcomes in layers 9 and 10, corresponding to the lung sections, wherein numerous\nflawed images were produced. The initial layers, representing the abdomen's lower segments with a glimpse of the liver, also\nposed challenges, as evidenced by its score contrasting with the scores from the central ones. The compromised results in the\nabdomen's extremities could be attributed to limited data availability for these sections or their inherent variability compared to\nother zones. For instance, the lung images, often depicted in black, analogous to air or water, or sometimes displaying minimal\nliver portions, seem particularly difficult."}, {"title": "KL Divergence", "content": "Two analyses have been performed with the KL divergence metric. Scores were derived from images processed as both\n256-bin and 3-bin histograms. The 256-bin histograms were employed for grayscale value comparisons to assess the pixel-wise\ndistribution considering all the possible values and, thus, the real distribution itself. The 3-bins approach is suitable for value\ncomparison related to internal body structure classifications, clustering them according to their radio-opacity: gases and liquids\n(first bin and darkest values), soft tissues and organs (second bin), and bones (third bin and brightest values); in this way, it is\neasier to assess if the overall types of structures are maintained in the synthetic images and if the tissue distribution is coherent,\nbut the analysis is rougher. The results are shown in fig. 6. In alignment with the FID explanation, the 256-bins methodology\nsuggests that the In-n-out and In-n-out HUM models exhibit the best performance as they marginally surpass the In-phase and\nAll-in models. The In-phase+ model presents acceptable outcomes, whereas the T2 model yields slightly inferior results, and\nthe Triple model appears to be underperforming."}, {"title": "Histogram comparison", "content": "Histogram comparison has been performed with the two methods of intersection and correlation, as described in section 3. The\nplots with the scores are shown in fig. 7. Scores with intersection metric, in this case, show comparable performance across\nall the settings, with the In-n-out and In-n-out HUM models standing above the other in the central and rostral layers while\nbeing outshined by the others in the caudal ones, which is surprising considering the other metrics. However, considering the\ncorrelation metric, there is a better result from the In-n-out HUM alone, which can keep a robust performance along the axial\nplane. In contrast, all the other models have particularly oscillating scores, even in the central portions. Once again, Triple and\nT2 show the worst results, closely followed, this time, by In-phase+. An evident performance worsening in the external layers\nis not noticeable, but all the models seem to follow the same trend, except for the In-n-out HUM."}, {"title": "Spectral analysis", "content": "The outcomes were derived across different layers, akin to the methodology used in the quantitative analysis, and their mean\nwas ascertained and presented in table 1. The score was computed for all the trained models; in the same way as before,\nwe incorporated two baselines for non-contrast-enhanced and contrast-enhanced CT scans. The scores obtained were then\nnormalized with respect to their baselines.\nThe computed scores do not show any significant distinction in image quality between genuine and synthesized images.\nHowever, a qualitative observation of the spectra can highlight something more accessible for a human observer to understand.\nBy presenting specific analogous CT scans alongside their spectral counterparts, it is evident how the spectrum of generated\nimages generally appears to be more noisy and with less defined patterns, even if the images are similar and show the same"}, {"title": "Qualitative assessment", "content": "In the subsequent figures, we aim to discuss the tendencies exhibited by the various models from a qualitative perspective,\ncomparing their output on the same MRI slice used in the different modalities available to produce the required outputs.\nBeginning with fig. 9a, it's evident that the model integrating in-phase and out-of-phase inputs delivers superior images\ncompared to the Triple model encompassing both T1 and T2 modalities. This reaffirms the evidence, already clear from the\ndistribution analyses, that T2-weighted images do not substantially contribute much to generating realistic CT scans when\npaired with the T1-weighted images. Observing fig. 9c, it can be seen that the model leveraging in-phase and out-of-phase\nT1 inputs but producing non-contrast-enhanced outputs yields results strikingly similar in organ shape to its counterpart\ngenerating contrast-enhanced outputs. Fig. 9b delineates the in-phase and out-of-phase images processed as distinct entities\nby the single-input CycleGAN model with the All-in architecture. Here, the in-phase image seems marginally superior to its\nout-of-phase counterpart, although, under ideal circumstances, both should yield analogous CT images. This proves the model\ncannot yield reliable results as different MRI modalities produce different outputs. It has to be noted that it would almost be\nimpossible to get the same results, but here, it is clear how the generated synthetic anatomical structures are not consistent at all.\nFurthermore, fig. 9d demonstrates that the CycleGAN model, when trained solely on in-phase T1 images, produces outcomes\nakin to the in-phase image from the model trained on both in-phase and out-of-phase images independently. This mitigates the\ninconsistency of the All-in model, as with in-phase T1 images, its output is paired with other specialized models trained on that.\nLastly, delving into the performance of the model trained with multiple MRI modalities, fig. 9e reveals an anomaly where it\nproduces a fictitious kidney, signifying that the inclusion of images from such radically different sources can be problematic as\nthe increment in the sample variability seems to lead to a more difficult distribution to learn for the model, which resorts to the\ncreation of unrealistic structures, but the models seem to be able to learn how some structures are supposed to look (in this case,\nslices usually contain both kidneys).\nThese are only some examples, but the choice has been made on an exemplificative slice, capable of showing the most\nrelevant differences between the models and performing comparisons that highlight the strengths and weaknesses of the\nmodels. The considerations taken here are generally backed up by the quantitative results that provide a clear ranking of models\naccording to the fidelity of the distribution learnt with respect to the real one from the dataset. From these examples, it is possible\nto infer that the quantitative analysis leads to a realistic direction in the model's evaluation. This considered, the best models\nseem to be the multimodal ones, especially theones trained on the Automi dataset and thus producing non-contrast-enhanced\nCT scans, which is expected as the use of contrast agents might introduce an additional layer of complexity to the problem."}, {"title": "Blind evaluation", "content": "A user study has been conducted in collaboration with Humanitas Research Hospital to evaluate the quality of the results further.\nThis partnership was part of the AUTOMI project and aimed to gauge how medical professionals perceive the generated CT\nscans compared to actual images. Four different surveys were administered to the radiation oncologists, each about a different\nmodel:\n\u2022 Survey 1 for the In-n-out model;\n\u2022 Survey 2 for the All-in model;\n\u2022 Survey 3 for the Triple model;\n\u2022 Survey 4 for the In-n-out HUM model.\nEach survey presented the participants with 20 scans: 10 real and 10 synthetic. As depicted in figure 3, the medical\npersonnel were prompted for each scan to categorize it as real, synthetic or indeterminable. They were also invited to state\nthe rationale behind their decisions, potentially pointing out specific features or organs that influenced their judgment. This\napproach helped us discern potential pitfalls in our models and furnished insights into the overall quality of the generated scans.\nIn table 2, we provide a breakdown of the percentage of correct guesses across the different surveys. These percentages are\ndelineated for the entire set of scans and separated into real-only and synthetic-only groupings. Notably, a percentage closer to\n50% indicates a desirable outcome, suggesting that the medical professionals found little to no discernible distinction between\nreal and synthetic scans. The users did not know the number of real and synthetic slices in advance.\nThe medical practitioner's feedback resonated with the findings of the quantitative evaluations. Specifically, the In-n-out\nandIn-n-out HUM models surfaced as the top performers, with the doctors correctly identifying 51.8% and 52.3% of the scans,"}, {"title": "5 Discussion", "content": "Our experiments with various CycleGAN architectures revealed the versatility of these models in generating synthetic CT\nscans from corresponding MRI scans. Notably, we discerned the superior performance of MRI T1-weighted scans over their\nT2 counterparts. The multi-input methodologies outperformed other configurations, particularly those leveraging in-phase\nand out-of-phase T1-weighted MRIs. Nevertheless, single-input models, such as the MRI in-phase only, still exhibited\ncommendable efficacy. Remarkably, our models adeptly generated both contrast-enhanced and non-contrast-enhanced CTs,\nachieving respectable results in each modality.\nA non-trivial challenge encountered was evaluating generated images from unpaired data sets. Our assessment matrix\nencompassed metrics like FID and KL divergence, enabling us to gauge the efficacy of our models with precision. These\nquantitative metrics, juxtaposed with our qualitative evaluations, yielded insights into the performance landscape, highlighting\nboth the strengths and limitations of the trained models, proving that the models which were able to learn the distribution from\nthe original sets better were indeed the multimodal ones, particularly the so-called In-n-out HUM trained with MRI from the\nChaos dataset and non-contrast-enhanced CT scans from the Automi dataset. Through a spectral analysis, we delved deeper\ninto understanding the quality of the synthesized images, thus confirming that the capabilities of the various models indeed\nemerged with the purely quantitative analysis of pixel distribution.\nQualitative evaluation is, however, an essential part of the work, as, since the field is particularly tricky in terms of structures,\nproportions, textures, relative positions and so on, it is of paramount importance to produce something which, more than on a\nstatistical point of view, is accurate from a medical standpoint. Working with physicians in a blind trial to test if they could\ndistinguish real from synthetic images, it has emerged that the accuracy and quality of the generated images are relatively high,\nproven by the fact that they could not classify them as real or synthetic consistently. This, however, poses some approach\nproblems that make the test not wholly reliable, as this is not a task that a physician would typically do in their daily routine or\never. First of all, the images were not shown in a clinical context but in a more relaxed environment, which is not the same as\nwhere they would typically work. This environment did not allow for adjusting the image's contrast, which is very important for\nradiation oncologists, especially when analyzing CT scans; they can play with it to enhance the visibility of certain structures or\nareas. Doing so, especially on uniform structures such as the bones, it is possible to see a non-uniform texture in the synthetic\nimages, which would make it easier to distinguish them from the real ones. Moreover, a limitation of this approach is the use of\na 2D-only approach and the generation of CT slice by slice without information on the 3rd dimension. Usually, a doctor would\nwork on a whole volume, which is extremely difficult to recreate.\nNonetheless, the approach presented can be considered extremely promising as the statistical metric shows interesting\nperformances, and the qualitative evaluation is still very positive, even if not wholly reliable. In synthesis, our holistic evaluation\napproach-merging quantitative analyses, spectral evaluations, and real-world testing\u2014has furnished robust insights into the\npromising behaviour of the models trained. These findings validate our approaches' efficacy and pave the way for future\nadvancements in this realm."}]}