{"title": "Leveraging Multimodal CycleGAN for the Generation of Anatomically Accurate Synthetic CT Scans from MRIS", "authors": ["Leonardo Crespi", "Samuele Camnasio", "Damiano Dei", "Nicola Lambri", "Pietro Mancosu", "Marta Scorsetti", "Daniele Loiacono"], "abstract": "In many clinical settings, the use of both Computed Tomography (CT) and Magnetic Resonance (MRI) is necessary to pursue a thorough understanding of the patient's anatomy and to plan a suitable therapeutical strategy; this is often the case in MRI-based radiotherapy, where CT is always necessary to prepare the dose delivery, as it provides the essential information about the radiation absorption properties of the tissues. Sometimes, MRI is preferred to contour the target volumes. However, this approach is often not the most efficient, as it is more expensive, time-consuming and, most importantly, stressful for the patients. To overcome this issue, in this work, we analyse the capabilities of different configurations of Deep Learning models to generate synthetic CT scans from MRI, leveraging the power of Generative Adversarial Networks (GANs) and, in particular, the CycleGAN architecture, capable of working in an unsupervised manner and without paired images, which were not available. Several CycleGAN models were trained unsupervised to generate CT scans from different MRI modalities with and without contrast agents. To overcome the problem of not having a ground truth, distribution-based metrics were used to assess the model's performance quantitatively, together with a qualitative evaluation where physicians were asked to differentiate between real and synthetic images to understand how realistic the generated images were. The results show how, depending on the input modalities, the models can have very different performances; however, models with the best quantitative results, according to the distribution-based metrics used, can generate very difficult images to distinguish from the real ones, even for physicians, demonstrating the approach's potential.", "sections": [{"title": "1 Introduction", "content": "Computed Tomography (CT) serves as a widely spread imaging modality for diagnostic purposes across the medical field, as it is one of the most reliable and accurate techniques available, and it allows the visualisation of internal anatomical structures in a non-invasive way precisely. Another widely used imaging technique is Magnetic Resonance (MRI), which offers distinct views of the targeted anatomical district, evidencing areas with a high content of hydrogen, such as fat and soft tissues; the two techniques are not interchangeable as they are based on radically different physical principles and are used to highlight different tissues and structures, depending on the clinical necessity\u00b9. These two tools are only occasionally used side by side, as, typically, there is a specific clinical question to answer that requires only one. However, scenarios where having both is necessary do exist: it is the case for specific radiotherapy (RT) treatments, like Total Marrow and Lymphoid Irradiation (TMLI)2, where there is the necessity of whole-body contouring, better performed exploiting the rich contrast in MRI images. With contouring, we refer to the process of identifying and delineating the anatomical structures of interest in the images, which is a crucial step in the RT treatment planning process as it allows to define the target volumes to be irradiated with the dose and the tissues to be spared, thus minimising the risk of damaging healthy tissues and the occurrence of unwanted side effects. CT scans are extensively used for this task, bearing the advantage of being necessary for every RT treatment to create the delivery plan, as they contain the specifics about the radiation absorption properties of the patient's tissues, which are fundamental to computing the dose distribution, and seldom offer sufficient information to contour regions of interest (ROI) adequately. In some cases, however, with whole-body treatments being a prominent example, radiation oncologists prefer using MRI\u00b3 for the contouring. An approach with the double examination introduces, however, a series of issues from different standpoints: primarily, for the patients, it means having to undergo two distinct examinations, which adds a lot of stress and discomfort, both physical and psychological; it has to be noted that, in the case of TMLI for instance, the patients in question are already in a very fragile state: generally, they are affected by haematological malignancies, like acute leukaemia or lymphoma, and are undergoing a bone marrow transplant; also, they seldom are towards the more senior side of the population. For these reasons, every additional source of stress is not desirable. Secondly, performing two scans is more expensive and requires more time, delaying the treatment time because of the time needed to take the exams and process and use the images for the planning. Another reason for additional concern in specific scenarios might be the extra dose of ionising radiation provided by the CT scanner; however, in the case of TMLI and RT in general, this is not particularly relevant as the dose delivered during the treatments makes the radiation absorbed during the acquisition negligible. Moreover, the MRI and the CT have to be co-registered, adding a non-trivial step to the process and a possible additional source of error. From these premises, it can be inferred how a system capable of providing physicians with both CT and MRI from a single acquisition would be a great advantage as it would significantly reduce costs and time, improve the efficiency of the protocols and diminish the stress for the patients. Deep Learning (DL) can help in this sense, as with the recent advancements in the field, models based on Convolutional Neural Networks (CNN) have shown great promise in transforming the image domain; in this work, we aimed at studying this task, applied to the transition from MRI to CT, as in the case previously described, the relevant anatomical structures of most significant interest for the use-cases are most evident in MRI images. Incidentally, such a tool would be a precious resource to generate accurate and realistic data in a field where data is notoriously relatively scarce and difficult to obtain. Having a system capable of generating accurate synthetic medical images would make it possible to increase the richness of the dataset of choice, including patients from, possibly, different hospitals and populations, thus reducing the risks of overfitting and bias in the learnt representations4. Also, generated data is less subject to privacy concerns as the synthetic images do not generally carry with them information that would allow to track back the original patient, or they might even be used as harmonisation tools to transform the original data and prevent leakages. The core objective of this work revolves around leveraging advancements in artificial intelligence and deep learning for image-to-image translation between the two imaging modalities. The aim is to synthesise CT images from available MRI inputs, with a vision to mitigate the operational and financial burdens on healthcare practitioners and patients and reduce the associated psychological or physical challenges from obtaining dual imaging modalities. Said images must represent the same anatomical structures, with their geometry, opacity and specific textural properties, to be as medically and anatomically accurate as possible. Unpaired image-to-image translation architectures have been analysed to select suitable architectures to achieve these objectives. The focal anatomical district in this work is the abdominal region, encompassing critical organs such as the liver, spleen, kidneys, and the inferior section of the lungs. This anatomical selection offers a broader relevance compared to other regions; for instance, while significant, studies centred on cerebral images are inherently specific to that region only, bearing less universality with the rest of the body as the variety of tissues is less diverse. The investigative lens is primarily focused on CycleGAN architectures. This work dissects the generation capabilities of various models trained to yield CT scans from different data sources and connects the generation with the combination of multiple MRI modalities. The methodology described encompasses diverse evaluation strategies to gauge model performance, ascertaining the effectiveness of generative models without a paired validation dataset. This work is carried out in collaboration with medical personnel from the Humanitas Research Hospital, who qualitatively assessed the images generated from a medical perspective."}, {"title": "2 Related Works", "content": "DL image generation, not only in medical imaging but in general, has been a very active research field in the last few years. Many interesting applications are available, especially for artistic and media-related purposes, as they can be highly catchy for the general public. The use of image generation in medical imaging is, however, mainly related to the generation of synthetic data, which can be used for a variety of purposes, from data augmentation to create missing modalities in MRI,6. In general, the common denominator for this kind of system and application is the use of Generative Adversarial Network (GAN), which, introduced in 2014 by Ian Goodfellow et al.7, constitute a framework employing two distinct models: a generator G and a discriminator D, that play a min-max game, respectively trying to generate realistic data and to distinguish between real and generated data. The architecture's potency emanates from the adversarial loss capability to manifest a distribution in the synthetic images analogous to the actual data distribution. Since GAN's inception, numerous methodologies drawing on its foundational principle have emerged, including the CycleGAN5, our principal reference model, which endeavours to produce images indistinguishable from the target dataset. Several GAN variants with enhanced network architectures and mathematical optimization techniques have been introduced since its first appearence9\u201316. A category of GANs referred to as conditional GANS (CGANs) facilitates conditional image generation from the generator, producing images with specific attributes17,18. In this sense, both CycleGAN and Pix2Pix19, among the most influential models, fall under the umbrella of CGANs and cater to image-to-image translation, converting an input image into a synthetic image that meets a specific criterion. These architectures allow data creation via domain adaptation, implying that the trained models can incorporate adjustments from one data source to another20. Data augmentation is one of the foremost applications of GANs in medical imaging, a notorious field for scarce data availability where this kind of tool is a necessity: several efforts have been made to develop systems to address this particular problem exploiting the capabilities of this family of neural models with particular success targeting classification tasks21\u201326. Specifically, Xu et al.23 introduced a semi-supervised attention-guided CycleGAN for MRI image augmentation aimed at brain tumour classification, capable of producing tumour images from normal ones and vice versa. Muramatsu et al.21 can be an illustrative example of CycleGAN utilized to create mammographic masses from lung nodules, aiming at data augmentation for breast mass classification. Wang et al.25 ascertained that merging WGAN27 with progressive growth28 outperformed the WGAN with gradient penalty (WGAN-GP)29 and Pix2Pix GAN19 for pulmonary adenocarcinoma classification; they incorporated a stitch layer into the generator net (dubbed StitchGAN) to merge low-dimensional images into a comprehensive image, simplifying the intricacy of full-size image transformation and generating MRI image pairs for clinically significant classification. Ge et al.24 devised a pairwise generative architecture to craft synthetic multimodal MRI images, thus augmenting a multimodal dataset tailored for brain tumour classification. In another innovative twist, Yu et al.26 suggested substituting the conventional CNN with a Capsule network30 in the GAN's discriminator, enhancing the modelling of the image's hierarchical relationships. An additional difficulty to image synthesis, particularly relevant in the medical field, is posed when the necessity is to find data augmentation methods for segmentation tasks, as in this case, the synthetic images have to be paired with the corresponding labels, which in this context typically refers to an image where pixels or voxels are associated with a category index, denoting specific relevant objects. Several recent works have obtained significant results31\u201336. Most of these proposed methodologies are grounded in the domain adaptation concept discussed previously. For instance, semantic labels of anatomical structures might produce coherent artificial medical images. When paired with the labels, the resulting images can be incorporated into the training dataset for a segmentation network. Cao et al.36, and Shin et al.31 adopted this label image translation technique to produce PET and CT images, as well as atypical brain MRI images, respectively, aiming at tumour segmentation. Augmented labels, allowing tumour location, shape, and size alterations, were the input for generating synthetic images showcasing diverse anatomical characteristics. Such cross-domain translation can also be observed between various imaging techniques: Sandfort et al.32 employed a CycleGAN to convert contrast-enhanced CT images to their non-contrast-enhanced counterparts, targeting organ segmentation. Conversely, Jiang et al.33 orchestrated pseudo-MRI image synthesis from CT images, regulated by a pair of concurrently trained GANs, focusing on lung tumour segmentation within MRI. These papers have preserved structural correspondence between synthetic images and labels derived from the original image sources, a key aspect. A comprehensive analysis by Skandarani et al.37 evaluates the performance of GANs trained on medical datasets for generating synthetic images that remarkably resemble original images. This analysis juxtaposes various GAN architectures, shedding light on the most effective ones for image generation and convincingly demonstrating the potential of these generated images to deceive human experts in a visual Turing test. However, this work focuses on image translation, specifically synthesizing a synthetic CT scan corresponding to a provided MRI. For tasks of image-to-image translation, an exemplary methodology was presented by Armanious et al. 38, who utilized a GAN architecture amalgamated with non-adversarial losses, facilitating the translation of PET to CT for brain imagery with promising results. The MedGAN framework found applicability in an unsupervised setting, as demonstrated in another study by the same authors39. A dual experiment incorporating paired and unpaired data for the translation of brain CT to MR was introduced by Cheng-Bin Jin et al.40 through the MR-GAN architecture. Other explorations in image generation entail the generation of absent modalities, as exemplified in the study by Alogna et al.41. This research utilized multi-input pix2pix and GAN architectures to generate absent MRI brain image modalities, specifically with T1-weighted scans (both with and without contrast enhancement) and T2-weighted alongside T2-fluid-attenuated inversion recovery scans. Another noteworthy endeavour by Gao et al.42 aimed at generating CT scans from sub-par beam CTs, employing three distinct architectures, pix2pix, CycleGAN, and AGGAN, to assess the efficacy in this high-definition image generation task. Studies have also focused solely on unpaired data generation, especially concerning brain tumour scans. A pertinent example would be the investigation by Jelmer M. Wolterink et al.43, which employed the classic CycleGAN architecture introduced by Zhu et al.5 to synthesize CT scans using sagittal MRI scans of the human brain as input. This study compares the performance of the CycleGAN in this particular task with traditional paired image-to-image translation techniques, highlighting instances where models trained on unpaired data surpassed their paired counterparts in"}, {"title": "3 Methodology", "content": "The models presented in this work are based on the CycleGAN architecture and make use of different combinations of input, resulting in single input and multimodal networks, depending on the number of images taken in input; the input is always 2D, using single slices of both MRI and CTs and keeping the complexity lower than with a three-dimensional approach, as sometimes in medical imaging this allows for better outcomes44. Regarding the implementation, the generator architecture is inspired by Johnson et al.'s design45. It incorporates nine residual blocks sandwiched between two downsampling and two upsampling blocks, as shown in figure 2a. The downsampling blocks are built with, in sequence: convolution, normalization, and activation (ReLU). Similarly, after the residual blocks, there are two upsampling blocks with the same scheme except for using transpose convolutions instead of regular convolutions, aimed at increasing the size of the layer's input. The discriminator draws its architecture from the PatchGAN design, detailed in the Pix2Pix paper19. This approach assesses the authenticity of 70 \u00d7 70 overlapping patches within the source image. Such a design permits a reduced parameter set in comparison to a full-image discriminator and offers the flexibility to process images without concerns about their size; a high-level scheme is shown in figure 2b. Two types of CycleGAN networks were used in this study: single-input and multimodal: the former uses a single slice as an input, while the latter takes two corresponding slices from two different modalities of the same slice. Often, T1 MRIs are acquired in both in-phase (IP) and out-of-phase (OOP) modalities, and it is possible to use both of them as input for the model, as their differences can help distinguish fat from tissues with similar properties\u00b9. The same cannot be said for T2 MRI, as they have different purposes than T1-weighted ones, and it is less frequent that they are both acquired. Therefore, developing a system using both would be more challenging to apply in a clinical setting. The multimodal CycleGAN is configured to accept 2 or 3 inputs for the MRI-to-CT generator, with the reverse transformation producing the corresponding 2 or 3 outputs. Consequently, there is a dedicated discriminator for the CT image and one for each MRI modality. In the forward generator, a single branch is used for each input modality to extract features, which are subsequently merged into a latent space for output CT generation 2c. Conversely, the backward generator begins with a singular input that transitions to its specific latent space, facilitating the generation of multiple MRI modalities as output to keep cycle consistency. The feature extraction block at the end of the input branches is organized as one convolution block, followed by two downsampling blocks and four residual blocks. Several setups have been tested, varying inputs and modalities, to understand how the model behaves using different modalities: \u2022 All-in: single input model, trained with both in-phase and out-of-phase T1 MRI and contrast-enhanced CT, all data from the Chaos Dataset; \u2022 In-phase: single input model, trained with in-phase only MRI and contrast-enhanced CT, all data from Chaos Dataset; \u2022 In-phase+: single input model trained with T1-weighted MRI from the Chaos Dataset plus some from the Cancer Imaging Archive to test a more varied dataset, CT are contrast-enhanced from the Chaos dataset; \u2022 T2: single input model with T2 weighted MRI and contrast-enhanced CT from the Chaos dataset; \u2022 In 'n out: 2-input model, with correlated in-phase and out-of-phase T1 MRI images and contrast-enhanced CT, all images from Chaos Dataset; \u2022 In 'n out HUM: 2-input model, in-phase and out-of-phase MRI from Chaos Dataset and non-contrast-enhanced CT from the Humanitas Dataset; \u2022 Triple: 3-input model, with correlated T1(in-phase and out-of-phase) and T2 MRI images and contrast-enhanced CT, all images from Chaos Dataset. Three sets of images have been used in this study: \u2022 Chaos CT: CT scans of the abdominal region from the CHAOS challenge46,47; 40 scans from different patients are contained, from potential liver donors with no tumours, lesions or other pathologies; the orientation and alignment are the same for all the images, which consist of 16-bit DICOM files with a resolution of 512 \u00d7 512, x-y spacing between 0.7 and 0.8mm, and 3 to 3.2 mm inter-slice axial distance, corresponding to an average of 90 slices per scan with a total of 1367 images; all the scans are contrast-enhanced. \u2022 Chaos MRI: MRI scans of the same region from the CHAOS challenge; it contains 40 patients for which two sequences were acquired; T1-Dual, which is a combination of in-phase and out-of-phase T1-weighted images, and T2; 12-bit DICOM images with 256 \u00d7 256 resolution are contained in the set; the inter-slice axial distance varies between 5.5 and 9mm, x-y spacing is between 1.36 and 1.89mm and the average number of slice is 36, with a total of 1594 slices. They are not paired with the CT scans or from the same patients, making it impossible to use a supervised method as no ground truth is available. \u2022 AUTOMI CT: full body CT scans from the Humanitas Research Hospital in Milan; 100 patients are contained in DICOM files with 16-bit depth and 512 \u00d7 512 resolution, 3 \u2013 5mm axial distance and 1 \u2013 2mm pixel spacing; the scans are not contrast-enhanced and are not paired with any MRI; the abdominal region has been cropped from the whole body scans to match the other sets, resulting in 5970 images. These sets were acquired from patients undergoing TMLI or similar treatments. CT sets are used independently so that models are trained either on contrast-enhanced images or non-contrast-enhanced ones, but not both together as they are incompatible, and the capability of generating both would imply introducing a conditioning to the network, adding further complexity to the task. Conversely, the same MRI set is used for all the experiments, and the desired modalities for each model are selected. It must also be noted that contrast-enhanced CT scans are not generally used in RT, and in general, models generating this kind of CT scans would not have broad practical uses in a clinical setting, but they remain an interesting benchmark. The lack of ground truth to compare the results calls for distribution-based metrics to assess, from a statistical point of view, the similarity of the generated images with the real ones. Three metrics have been selected for this purpose, among the most popular in literature: Fr\u00e9chet Inception Distance (FID)48, Kullback-Leibler Divergence (KL) and histogram comparison."}, {"title": "Fr\u00e9chet Inception Distance", "content": "It was introduced specifically for the evaluation of generative models48, GANs in particular, as it gives an estimate of how distant the generated images are from the real ones, using a features space derived from a pre-trained CNN, normally the InceptionV3 (hence the name, Inception), stripped of its last layers and only used as a features extractor. The resulting distributions are then compared using the Frechet distance, also called the Wasserstein-2 distance27. The lower the score, the closer the two distributions and, hence, the more similar the images. Notably, FID doesn't merely quantify the image's distribution but aims to assess image similarity akin to human perception. The FID is computed as: $d\u00b2((m,C), (mw, Cw)) = ||m \u2013 mw||2 + Tr(C+Cw - 2(CCw)1/2)$ Here, (m, C) represents the mean and covariance of the Gaussian distribution derived from the actual image set, while (mw, Cw) corresponds to the generated dataset."}, {"title": "Kullback-Leibler Divergence", "content": "The Kullback-Leibler (KL) divergence can quantify the dissimilarity between two probability distributions. In its approximated form49, it computes the difference between the cross-entropy of distributions p(x) and q(x), and the entropy of p(x), represented, in this work, respectively the distribution of real CT images and the synthetic one. The formulation used in this work is the following: $D_{KL}(p,q) = H(p,q) \u2013 H(p) = \\sum_x p(x) \\log \\left( \\frac{p(x)}{q(x)} \\right) = - \\sum_x p(x) \\log \\left( \\frac{q(x)}{p(x)} \\right)$ A low KL divergence score implies greater similarity between the compared distributions. Distributions are derived by averaging image histograms, producing an average detail of the pixels' distribution across the sets."}, {"title": "Histogram comparison", "content": "Conceptually similar to the KL divergence, histogram comparison has been used to scrutinize the distribution of the generated images. The methodology for distribution computation mirrors that of the KL divergence, and the histogram is compared according to two metrics: \u2022 Correlation: $d(H_1,H_2) = \\frac{\\sum_I(H_1(I) \u2013 \\overline{H_1})(H_2(I) \u2013 \\overline{H_2})}{\\sqrt{\\sum_I(H_1(I) \u2013 \\overline{H_1})^2\\sum_I(H_2(I) \u2013 \\overline{H_2})^2}}$ where: $\\overline{H_k} = \\frac{1}{N} \\sum_I H_k(I)$ and N stands for the histogram bin count. \u2022 Intersection: $d(H_1,H_2) = \\sum_I \\min(H_1(I),H_2(I))$ For these metrics, high scores indicate a higher similarity. Specifically, Correlation peaks at 1.0 for flawless matching, while Intersection, being the cumulative minimum value across bins, theoretically ranges from [0, inf). The formulations for the metrics are adapted from the official OpenCV documentation50."}, {"title": "Spectral Analysis", "content": "An additional method used to assess the quality of generated images is spectral analysis, which is used to compare the frequency content of the images. The inspiration was drawn from the work of Frank et al.51, which put forth the idea of training a neural network to differentiate between genuine and synthetically generated images from various network designs by scrutinizing their spectrum. This strategy hinged on the hypothesis that the spectrum of synthetic images may manifest anomalies, primarily because of the upsampling and downsampling operations that transpire during the image-to-image transformation process; for example, transposed convolution layers, often used in GANs, are known to introduce checkerboard-shaped artefacts52. In this work, instead of training a neural network, images have been converted into their spectral representation, and the distance between generated and original ones has been computed through their correlation."}, {"title": "4 Results", "content": "One of the most pressing challenges in this work is the lack of paired data, as distribution-based metrics serve more as an estimate of the actual quality of the generated images; even if the distribution is the same, artefacts and structures may have unrealistic shapes or present wrongful intensity patterns while still performing well metrics-wise. Two subsets of the two real CT datasets, contrast and non-contrast-enhanced from Chaos and AUTOMI, have been kept apart to be compared with the other real images to compute a baseline representing the score obtained from comparing sets of real images. The scores of our models are then normalized by dividing by their baseline, showing the ratio between the baseline's score and the models'. This allows for a better comparison of models trained on different kinds of images. Baseline refers to the CHAOS CT baseline, while hum-baseline refers to the Automi CT baseline. All the scores obtained from models trained with contrast-enhanced CT scans are normalized by dividing by the baseline value of their layer. Also, the averaged value is divided by the average value of its baseline. The non-contrast-enhanced model is divided by the hum-baseline. Given the variability from the upper to the lower anatomical regions of the abdomen and the inconsistency in volumes' acquisitions starting or ending at possibly different heights, the anatomical distribution may be substantially different between the two compared datasets; consequently, images were categorized into axial layers depending on their position along the axial axis. Ten layers were defined based on the size of observable organs (liver, spleen, kidneys, and lungs) to harmonize the anatomical differences, enhanced by the different sources and settings and usually particularly evident at the borders; The split aims to consider anatomical subsections of the body which contain similar structures and compare them against each other while having a more profound and finer understanding of the models' performances throughout the volumes. For instance, the topmost layers show parts of the lungs, while the bottom layers reach the lower abdominal organs, capturing the appearance of the liver or spleen. Representative images from each layer, highlighting inter-layer differences, can be seen in figure 4. Metrics were computed on images within the same layer, after which an average value was calculated. This method provided insights into the model's performance across different organs across layers, pinpointing which anatomical structures the generative models excelled or faltered with more substantial variability than the central ones. A worsening in scores can be expected."}, {"title": "FID", "content": "The detailed and average scores of the FID are presented in fig. 5. The findings indicate that the superior scores are derived from the multi-input CycleGAN when utilizing in-phase and out-of-phase T1-weighted MRI images. This consistency is evident in models trained with contrast-enhanced and non-contrast-enhanced CT, with the former aligning closely with its baseline (hum-baseline). A decline in performance can be noted with the addition of T2-weighted images as the MRI input, which is somewhat unexpected. A possible reason might be the subtle misalignment of T2 images with corresponding T1 scans, attributed to different acquisition frequencies, which introduces ambiguity and compromises the outcome. Among the single-input models, the model using T2 images underperformed, hinting at the relative inefficacy of T2-weighted images as a source for synthetic CT. The darker values of T2 images, compared to T1, potentially render the contrast of the scans less favourable than their counterpart to generate accurate distributions. Other models demonstrated commendable outcomes, notably the All-in and In-phase models. Their results are closely aligned with the In-n-out score. However, performance deteriorated with varied image types, as observed in the In-phase+ model. A pattern in the results showcases subpar outcomes in layers 9 and 10, corresponding to the lung sections, wherein numerous flawed images were produced. The initial layers, representing the abdomen's lower segments with a glimpse of the liver, also posed challenges, as evidenced by its score contrasting with the scores from the central ones. The compromised results in the abdomen's extremities could be attributed to limited data availability for these sections or their inherent variability compared to other zones. For instance, the lung images, often depicted in black, analogous to air or water, or sometimes displaying minimal liver portions, seem particularly difficult."}, {"title": "KL Divergence", "content": "Two analyses have been performed with the KL divergence metric. Scores were derived from images processed as both 256-bin and 3-bin histograms. The 256-bin histograms were employed for grayscale value comparisons to assess the pixel-wise distribution considering all the possible values and, thus, the real distribution itself. The 3-bins approach is suitable for value comparison related to internal body structure classifications, clustering them according to their radio-opacity: gases and liquids (first bin and darkest values), soft tissues and organs (second bin), and bones (third bin and brightest values); in this way, it is easier to assess if the overall types of structures are maintained in the synthetic images and if the tissue distribution is coherent, but the analysis is rougher. The results are shown in fig. 6. In alignment with the FID explanation, the 256-bins methodology suggests that the In-n-out and In-n-out HUM models exhibit the best performance as they marginally surpass the In-phase and All-in models. The In-phase+ model presents acceptable outcomes, whereas the T2 model yields slightly inferior results, and the Triple model appears to be underperforming. The three bins model reveals the same ranking, with the difference that the In-phase model has a very similar score to the one obtained by the In-n-out model, which is understandable considering the rougher comparison. Once again, layers in the middle show closer results to the baseline's distribution, demonstrating consistencies in the generation capabilities where the anatomical structures are more consistent. However, the models' performance is significantly lower in the external layers, especially towards the lungs."}, {"title": "Histogram comparison", "content": "Histogram comparison has been performed with the two methods of intersection and correlation, as described in section 3. The plots with the scores are shown in fig. 7. Scores with intersection metric, in this case, show comparable performance across all the settings, with the In-n-out and In-n-out HUM models standing above the other in the central and rostral layers while being outshined by the others in the caudal ones, which is surprising considering the other metrics. However, considering the correlation metric, there is a better result from the In-n-out HUM alone, which can keep a robust performance along the axial plane. In contrast, all the other models have particularly oscillating scores, even in the central portions. Once again, Triple and T2 show the worst results, closely followed, this time, by In-phase+. An evident performance worsening in the external layers is not noticeable, but all the models seem to follow the same trend, except for the In-n-out HUM."}, {"title": "Spectral analysis", "content": "The outcomes were derived across different layers, akin to the methodology used in the quantitative analysis, and their mean was ascertained and presented in table 1. The score was computed for all the trained models; in the same way as before, we incorporated two baselines for non-contrast-enhanced and contrast-enhanced CT scans. The scores obtained were then normalized with respect to their baselines. The computed scores do not show any significant distinction in image quality between genuine and synthesized images. However, a qualitative observation of the spectra can highlight something more accessible for a human observer to understand. By presenting specific analogous CT scans alongside their spectral counterparts, it is evident how the spectrum of generated images generally appears to be more noisy and with less defined patterns, even if the images are similar and show the same structures."}, {"title": "4.2 Qualitative assessment", "content": "In the subsequent figures, we aim to discuss the tendencies exhibited by the various models from a qualitative perspective, comparing their output on the same MRI slice used in the different modalities available to produce the required outputs. Beginning with fig. 9a, it's evident that the model integrating in-phase and out-of-phase inputs delivers superior images compared to the Triple model encompassing both T1 and T2 modalities. This reaffirms the evidence, already clear from the distribution analyses, that T2-weighted images do not substantially contribute much to generating realistic CT scans when paired with the T1-weighted images. Observing fig. 9c, it can be seen that the model leveraging in-phase and out-of-phase T1 inputs but producing non-contrast-enhanced outputs yields results strikingly similar in organ shape to its counterpart generating contrast-enhanced outputs. Fig. 9b delineates the in-phase and out-of-phase images processed as distinct entities by the single-input CycleGAN model with the All-in architecture. Here, the in-phase image seems marginally superior to its out-of-phase counterpart, although, under ideal circumstances, both should yield analogous CT images. This proves the model cannot yield reliable results as different MRI modalities produce different outputs. It has to be noted that it would almost be impossible to get the same results, but here, it is clear how the generated synthetic anatomical structures are not consistent at all. Furthermore, fig. 9d demonstrates that the CycleGAN model, when trained solely on in-phase T1 images, produces outcomes akin to the in-phase image from the model trained on both in-phase and out-of-phase images independently. This mitigates the inconsistency of the All-in model, as with in-phase T1 images, its output is paired with other specialized models trained on that. Lastly, delving into the performance of the model trained with multiple MRI modalities, fig. 9e reveals an anomaly where it produces a fictitious kidney, signifying that the inclusion of images from such radically different sources can be problematic as the increment in the sample variability seems to lead to a more difficult distribution to learn for the model, which resorts to the creation of unrealistic structures, but the models seem to be able to learn how some structures are supposed to look (in this case, slices usually contain both kidneys.). These are only some examples, but the choice has been made on an exemplificative slice, capable of showing the most relevant differences between the models and performing comparisons that highlight the strengths and weaknesses of the models. The considerations taken here are generally backed up by the quantitative results that provide a clear ranking of models according to the fidelity of the distribution learnt with respect to the real one from the dataset. From these examples, it is possible to infer that the quantitative analysis leads to a realistic direction in the model's evaluation. This considered, the best models seem to be the multimodal ones, especially the ones trained on the Automi dataset and thus producing non-contrast-enhanced CT scans, which is expected as the use of contrast agents might introduce an additional layer of complexity to the problem."}, {"title": "Blind evaluation", "content": "A user study has been conducted in collaboration with Humanitas Research Hospital to evaluate the quality of the results further. This partnership was part of the AUTOMI project and aimed to gauge how medical professionals perceive the generated CT scans compared to actual images. Four different surveys were administered to the radiation oncologists", "model": "Survey 1 for the In-n-out model; \u2022 Survey 2 for the All-in model; \u2022 Survey 3 for the Triple model; \u2022 Survey 4 for the In-n-out HUM model. Each survey presented the participants with 20 scans: 10 real and 10 synthetic. As depicted in figure 3, the medical personnel were prompted for each scan to categorize it as real, synthetic or indeterminable. They were also invited to state the rationale behind their decisions, potentially pointing out specific features or organs that influenced their judgment. This approach helped us discern potential pitfalls in our models and furnished insights into the overall quality of the generated scans. In table 2, we provide a breakdown of the percentage of correct guesses across the different surveys. These percentages are delineated for the entire set of scans and separated into real-only and synthetic-only groupings. Notably, a percentage closer to 50% indicates a desirable outcome, suggesting that the medical professionals found little to no discernible distinction between real and synthetic scans. The users did not know the number of real and synthetic slices in advance. The medical practitioner's feedback"}]}