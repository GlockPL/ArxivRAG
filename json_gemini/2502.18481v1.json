{"title": "MDE: Modality Discrimination Enhancement for Multi-modal Recommendation", "authors": ["Hang Zhou", "Yucheng Wang", "Huijing Zhan"], "abstract": "Multi-modal recommendation systems aim to enhance performance by integrating an item's content features across various modalities with user behavior data. Effective utilization of features from different modalities requires addressing two challenges: preserving semantic commonality across modalities (modality-shared) and capturing unique characteristics for each modality (modality-specific). Most existing approaches focus on aligning feature spaces across modalities, which helps represent modality-shared features. However, modality-specific distinctions are often neglected, especially when there are significant semantic variations between modalities. To address this, we propose a Modality Distinctiveness Enhancement (MDE) framework that prioritizes extracting modality-specific information to improve recommendation accuracy while maintaining shared features. MDE enhances differences across modalities through a novel multi-modal fusion module and introduces a node-level trade-off mechanism to balance cross-modal alignment and differentiation. Extensive experiments on three public datasets show that our approach significantly outperforms other state-of-the-art methods, demonstrating the effectiveness of jointly considering modality-shared and modality-specific features.", "sections": [{"title": "I. INTRODUCTION", "content": "With the surge of massive multimedia information [1], [2], [3] across various online platforms, e.g., e-commerce applications and content-sharing communities, multi-modal recommendation systems [4], [5], [6] have gained increasing popularity. By leveraging the combined information from multiple modalities along with users' historical behavior patterns, the system can effectively capture user preferences for better recommendation. To enhance the quality of recommendations, the key objective is to design an effective approach for integrating comprehensive and complementary information from multiple modalities.\nTo fully leverage the multi-modality information, a key challenge persists: how to strike a good trade-off between both the semantic commonalities (modality-shared) and the unique characteristics (modality-specific) of each modality. Currently, most existing works [7], [8] have mainly focused on the modality-shared knowledge, which is often achieved through cross-modality alignment using contrastive learning [9] and using data augmentation techniques to generate multiple views of the same node, maximizing agreement between these views [10], [11].\nAlthough these approaches have shown the effectiveness of leveraging the multiple modalities, modality-specific information, which is critical for capturing the unique characteristics of items, is frequently overlooked. For instance, image features of a product typically describe its shape and color, while text features convey its function and price. These distinct types of information can provide valuable recommendations from different perspectives. Although recent research [12], [13] has acknowledged the importance of preserving the modality-specific semantics during cross-modal alignment, they still face a common limitation: the inability to fully exploit and further enhance the unique and discriminative information (i.e., modality discrimination) of each modality.\nTo leverage both modality-shared and modality-specific information for effective recommendation, we propose a novel framework, Modality Discrimination Enhancement (MDE). This framework aims to extract and enhance modality-specific information by amplifying differences between modalities while simultaneously maintaining modality-shared information through modality alignment. Notably, modality alignment inherently conflicts with the amplification of modality differences. To resolve this, we introduce a node-level trade-off mechanism based on learnable modality preferences to balance these two objectives. Specifically, as illustrated in Fig. 1, we first learn multi-modal feature representations by constructing a heterogeneous user-item graph, along with homogeneous graphs for the user-user co-occurrence and item-item semantic similarity. We then introduce a node-level learnable preference weight to effectively fuse the multi-modal features. Finally, we enhance modality discrimination and balance modality-shared and modality-specific information by introducing weighted modality discrimination and modality alignment losses, with the learned modality preferences determining the trade-off. We hypothesize that for a given node, if its preferences across different modalities show larger variations, we should prioritize enhancing modality discrimination, and vice versa. Extensive experiments conducted on several large-scale benchmarks demonstrate the necessity and effectiveness of our modality discrimination method."}, {"title": "II. METHODOLOGY", "content": "A. Problem Definition\nAssume we have a set of users $U = {u_i}_{i=1}^{N}$ and a set of items $I = {i_t}_{t=1}^{M}$. For each item, $i_t$ has different types of multimedia features (i.e., generated from pre-trained models), denoted as $e_m$, where $m \\in M$, where $M$ refers to the visual and textual modalities, respectively. We then denote the"}, {"title": "B. Multi-modal Feature Learning", "content": "Heterogeneous Graph construction. Graph neural networks have been used in this work due to their effectiveness in recommend system to construct user-item and modality-aware auxiliary graphs [14], [15]. Inspired by [16], we construct both heterogeneous (user-item graph) and homogeneous graphs (user-user graph and item-item graph) to learn high-order connectivity between users and items. Let $E_m$ and $E_m$ denote the modal feature matrices for users and items, respectively. These matrices are initialized randomly for users and using features extracted from pre-trained models for items. Here $m \\in {v,t}$ with $v$ refers to the visual modality and $t$ refers to the textual modality. We adopt LightGCN [17] to perform message propagation and the forward propagation at the $l+ 1$ graph convolution layer is formulated as follows:\n$\\mathbb{E}_m^{l+1} = (\\tilde{D}^{-1} \\tilde{A} \\tilde{D}^{-1}) \\mathbb{E}_m$        (2)\nhere A is the adjacency matrix of the user-item interaction graph G and D is the diagonal degree matrix of A with\n$D_{ii} = \\sum_j A_{ij}$. Here $\\mathbb{E}^0_m$ is initialized as the concatenation of $\\mathbb{E}^u_m$ and $\\mathbb{E}^i_m$, denoted as $\\mathbb{E}_m = \\mathbb{E}^u_m || \\mathbb{E}^i_m$. The final feature representation from the heterogeneous graph $H_m$ is the average of the embedding of all layers:\n$H_m = \\frac{1}{L+1} \\sum_{l=0}^{L} \\mathbb{E}^{l}_m$.        (3)\nHomogeneous Graph construction. To better explore higher-order relationships between users and items, we also construct homogeneous graphs for both users and items. Drawing the inspiration from [18], [13], we develop a user-user co-occurrence graph and an item-item similarity graph. The user-user co-occurrence graph is created by calculating the number of commonly interacted items between users. For each user, we sample the top-K frequent co-occurred users, resulting in the generation of the graph $G_u = {G_u, G_u}$. In contrast, the item-item similarity graph is constructed by computing the cosine similarity between item features, from which the top-K similar items are sampled to create the item-item similarity graphs, $G_i = {G_i, G_i}$. We then apply a simple graph convolution to these homogeneous graphs:\n$H_m^* = A_m H_m, $         (4)\nwhere $A_m = {A^u_m, A^i_m}$ is the adjacency matrix of the homogeneous graph $G_u$ and $G_i$. It is important to note that the graph convolution operation is applied to each modality of the homogeneous graph using its respective adjacency matrix.\nMulti-modal Feature Fusion and Prediction. The significance of different modalities often varies in the context of recommendations. For user nodes, individual users may exhibit distinct modality preferences, while for item nodes, one modality may offer more informative insights than another."}, {"title": "C. Modality Discrimination Enhancement", "content": "Modality Difference Amplification (MDA). To extract the modality-specific information, we enhance the discrimination between modalities. Specifically, we maximize the disparity between modality features before the final fusion step:\n$H_{diff} = H^*_v \\odot W_{diff} - H^*_t \\odot W_{diff}$         (6)\n$L_{diff} = \\O_{diff} \\cdot ||H_{diff} ||_2,$        \nwhere $\\odot$ denotes the absolute value function and $||\\u00b7||$ represents the L2 norm.  $\\odot$ denotes the Hadamard product. $\\O_{diff}$ is a hyper-parameter and $W_{diff}$ is the node-level weight matrix, which will be introduced in the following section. By minimizing $L_{diff}$, we can maximize the differences between modality features, thereby preserving the distinctive and informative information in each modality.\nModality Similarity Alignment (MSA). Although we prioritize extracting modality-specific information by maximizing modality differences, how to retain modality-shared information remains essential. We achieve this by the modality similarity alignment, which maps the features of different modalities into a common feature space. We leverage contrastive learning [9] to perform the modality alignment:\n$L^{cl} = -log\\frac{exp(h_i \\cdot h_i^+ /\\tau)}{\\sum_{i^\\prime \\in I}exp(h_i \\cdot h_{i^\\prime} /\\tau)}$\n$\\mathbb{L}^a_{cl} = \\sigma_l \\cdot (L^v_{cl} + L^t_{cl})$,           (7)\nwhere $W_{cl}$ is the node-level weight matrix and $\\sigma_l$ is the hyper-parameter.\nNode-Level Trade-off (NLT). In this section, we discuss the trade-off component to balance the modality difference amplification and the modality alignment with these two weight matrices, $W_{diff}$ and $W_{cl}$, defined as follows:\n$W_{diff} = f_b (|P_{i,v} - P_{i,t}|)$,\n$W_{cl} = f_b (1 - |P_{i,v} - P_{i,t}|)$,           (8)\nwhere $f_b(\\cdot): \\mathbb{R}^{|I| \\times 1} \\to \\mathbb{R}^{|I| \\times d}$ is a broadcast function and d is the dimension of features. We leverage the difference of modality preference weight, as shown in Eq. 5, to determine the trade-off weight. The assumption here is that a larger difference in modality preference implies more importance for modality difference amplification. Conversely, a smaller difference in modality preference indicates a higher weight for the modality alignment, as it suggests that different modalities contribute similarly to the recommendation for that node."}, {"title": "D. Optimization", "content": "We utilize the Bayesian Personalized Ranking (BPR) Loss [19] as the optimization objective:\n$L_g = \\sum_{(u,i,i') \\in B} -log (\\sigma (\\hat{y}_{u,i} - \\hat{y}_{u,i'}))$,        (9)\nwhere each triple $(u,i,i')$ satisfies $R_{ui} = 1$ and $R_{ui'} = 0$. $\\sigma$ is the sigmoid function. We also introduce two additional modality-specific loss $L_v$ and $L_t$:\n$L_v = \\sum_{(u,i,i') \\in B} -log (\\sigma (h^T_v u h_v i - h^T_v u h_v {i'}))$,        (10)\nNote that $L_t$ is omitted due to space constraints, as it is calculated in a similar manner. The final objective function to be optimized is shown as below:\n$L = L_B + L_v + L_t + L_{diff} + L_{cl} + \\theta_{reg}||\\Theta||_2$,         (11)\nwhere $\\Theta$ represents the model parameters and $\\theta_{reg}$ is the regularization coefficient."}, {"title": "III. EXPERIMENT", "content": "A. Datasets and Evaluation Setting\nWe evaluate our proposed model using three widely recognized Amazon datasets: Baby, Sports, and Clothing [20]. We follow the experimental settings established in previous work [21]. The proposed model and all baseline models are implemented using MMRec [22]. For all models, we fix the embedding size for both users and items at 64, set the training batch size to 2048, use Adam [23] as the optimizer, and apply the Xavier method [24] to initialize the model parameters. The hyperparameters $\\sigma_{cl}$, $\\sigma_{diff}$ and $\\theta_{reg}$ are determined through grid search on the validation set. We set the number of GCN layers L in the heterogeneous graph as 2. The values of K for constructing the item similarity graph and the user co-occurrence graph are set to 10 and 40, respectively."}, {"title": "B. Performance Comparison", "content": "Table I presents the Top-5 recommendation performance achieved by various methods. It can be seen that MDE demonstrates significant improvements over state-of-the-art on both the Baby and Sports datasets. The relatively lower performance on the Clothing dataset can be partly attributed to the instability of user modality preferences in the clothing recommendation scenario, which reduces the effectiveness of the modality-preference-based node-level trade-off employed in MDE."}, {"title": "C. Ablation Study", "content": "We conduct ablation experiments to investigate the impact of each individual module on overall model performance. Specifically, we evaluate the performance without the Modality Difference Amplification (MDA), Modality Similarity Alignment (MSA), and Node-Level Trade-Off (NLT) components to assess the framework's effectiveness in the absence of specific modules. The results reported in Table II reveal several insights based on Recall@5 (R@5)\u201d: (1) The performance of the variant \"w/o MDA\" demonstrates that enhancing modality differences is crucial for effectively leveraging multi-modal features. (2) The \"w/o MSA\" results indicate that neglecting modality similarity alignment without constraints leads to a significant drop in performance. (3) The \"w/o NLT\" results confirm the advantages of the proposed adaptive node-level trade-off strategy."}, {"title": "D. Trade-off Mechanism Analysis", "content": "To further assess the effectiveness of the proposed preference-based trade-off mechanism, we compare MDE with the following two variants on Baby dataset: (A) We fix the same modality preference for all nodes while maintaining the preference-based trade-off strategy. In this scenario, the trade-off weights in the loss function are determined by a static modality preference weight setting. The results for variant A are displayed as bar charts in Fig. 2, corresponding to different preference weight settings, where the preference weight for the text modality $w_t$ is fixed and the weight for the visual modality is 1- $w_t$. (B) We use node-level independent learnable weights instead of preference-based weights as the trade-off weights in the loss function, with the results for variant B shown as a blue dotted line. From the results in Fig. 2, we observe the following: (1) The contributions of different modalities are uneven, even when individual modality preferences are not considered; on the Baby dataset, the text modality plays a more significant role than images. (2) The results of variants A and B indicate that modality preferences vary individually, suggesting that accounting for node-level modality preferences can enhance performance. (3) The comparison between variant B and MDE demonstrates that the modality-preference-based trade-off mechanism effectively balances modality differentiation and modality alignment."}, {"title": "IV. CONCLUSION", "content": "In this paper, we propose a novel framework for multi-modal recommendation, named MDE. The proposed approach enhances the modality distinctiveness to effectively extract modality-specific information while maintaining shared features. To achieve this, we introduce the modality difference amplification and modality similarity alignment, aiming to capture both the common and unique knowledge across modalities. To balance these two aspects, we introduce a novel node-level trade-off method based on learnable modality preferences. Extensive experiments on three datasets demonstrate the superiority of our model over state-of-the-art approaches."}]}