{"title": "MagicFace: Training-free Universal-Style Human Image Customized Synthesis", "authors": ["Yibin Wang", "Weizhong Zhang", "Cheng Jin"], "abstract": "Existing human image personalized generation methods often require tedious training: either fine-tuning with a few images or retraining on large-scale datasets. In such cases, these methods are prone to overfitting and encounter difficulties when personalizing individuals of diverse styles. Moreover, these training-based approaches also struggle with multi-concept human image customizing. To this end, we propose MagicFace, the first method for universal-style human image personalized synthesis that enables single/multi-concept customization for humans of any style in a training-free manner. MagicFace introduces a coarse-to-fine generation pipeline, involving two sequential stages: semantic scene construction and concept feature injection. This is achieved by our Reference-aware Self-Attention (RSA) and Region-grouped Blend Attention (RBA) mechanisms. Specifically, in the first stage, RSA enables the latent image to query features from reference concepts simultaneously, extracting the coarse-grained overall semantic understanding to facilitate the initial semantic layout establishment. In the second stage, we employ an attention-based semantic segmentation method to pinpoint the generated regions of all concepts in the latent image at each step. Following this, RBA divides the pixels of the latent image into semantic groups, with each group querying fine-grained features from its reference concept, which ensures precise attribute alignment and feature injection. Throughout the two-stage process, a weight mask strategy is employed to ensure the model focuses more on the reference concepts. Extensive experiments demonstrate our superiority in both human-centric subject-to-image synthesis and multi-concept human image customization. Our approach also can be applied to texture transformation, further enhancing its versatility and applicability. The project page is here.", "sections": [{"title": "1. Introduction", "content": "Text-to-image generation has undergone remarkable advancements with the emergence of large-scale text-to-image diffusion models such as Stable Diffusion [23]. One of the most popular and challenging topics in these developments is the person-centric subject-to-image generation, which aims to personalize individuals in novel scenes, styles, and actions"}, {"title": "3. MagicFace", "content": ""}, {"title": "3.1. Overview", "content": "We present an overview of our proposed MagicFace in Fig. 3. Our goal is to achieve high-fidelity human image synthesis, enabling single/multi-concept customization across various styles in a training-free manner. To accomplish this, we propose a coarse-to-fine generation pipeline, incorporating our Reference-aware Self-Attention (RSA) and Region-grouped Blend Attention (RBA) mechanisms. Specifically, our sampling process involves two sequential stages with a total of T denoising steps.\nIn the first stage, we employ RSA to extract the overall semantic understanding from all given concepts, facilitating the initial semantic scene construction.\nIn the second stage, a latent semantic map is derived to pinpoint the generated positions of all concepts at the pixel level in each step. We then utilize RBA to accurately inject the features of each reference concept into their corresponding positions, ensuring that the generated concepts closely resemble the reference images.\nTo emphasize the model's attention to the reference concepts, a weighted mask strategy is implemented during customized synthesis."}, {"title": "3.1.1 Vanilla Self-attention in Diffusion Model", "content": "In this work, we adopt pre-trained Stable Diffusion (SD) [22] as our base model, utilizing its U-Net 6 as the denoiser. The original SD U-Net is composed of 16 layers with each layer including a residual block, a self-attention module, and"}, {"title": "3.1.2 Dual-path Guided Customization", "content": "As illustrated in Fig. 3, the overall inference pipeline involves two paths: reference path and customization path, which will be elaborated in the following.\nReference Path. For each concept image in the reference path, we first apply a diffusion forward process [11] to compute the noised reference latent zy. At each step t, we input z and corresponding text prompt into the U-Net \u20ac\u03b8. In the self-attention layer l and time step t, we extract the Ki,l,t and Vi,l,t of i-th concept to guide the synthesis in the customization path.\nCustomization Path. This path starts by sampling the latent zy from a Gaussian distribution N(0, I). Then, we modify the vanilla self-attention module of SD U-Net, extending it into our RSA/RBA. At each step t, we pass the latent zt and target prompt P into modified U-Net \u20ac, integrating concept features from Ki,l,t and Vi,l,t derived in reference path to guide generated concept imitate the reference image. The final denoised result is the ultimate customized image."}, {"title": "3.2. Coarse-to-Fine Generation Process", "content": "MagicFace synthesizes images by progressing from coarse-grained semantic scene construction with aT steps to fine-grained feature injection with T(1-a) steps in an evolving scheme. In the latter stage, we employ an attention-based semantic segmentation method, which paves the way for precise feature injection. We will delve into the details of each stage in the following."}, {"title": "3.2.1 Semantic Scene Construction", "content": "Reference-aware Self-Attention. RSA enables the latent image to query features from all reference concepts simultaneously to integrate coarse-grained semantic information for initial semantic scene construction. Specifically, in each self-attention layer l and time step t, the query, key, and value feature from zt are Q1,t, K\u0131,t and Vi,t. We concatenate K\u0131,t and Vi,t with N injected key and value features from reference path respectively, resulting in K = [K, K1, K2, ..., KN] and V = [V, V1, V2, ..., VN] where we omit l,t for brevity.\nHowever, we aim to query features exclusively from the concept region, as irrelevant background information in the"}, {"title": "3.2.2 Latent Semantic Map Generation", "content": "To ensure precise feature injection at the pixel level for each concept, we need to identify their generated region. However, this task is challenging because the latent image at each step is not accessible during the generation process. Fortunately, the attention layers in the U-Net contain rich semantic information, which can be utilized to identify semantic units efficiently [5]. Therefore, in this work, we compute the latent semantic map based on attention maps, involving two sequential steps: cross-attention-based semantic segmentation and self-attention-based segmentation completion.\nAttention Maps Generation. In the attention layer l and time step t, a self-attention map S and a cross-attention map C are calculated over linear projections of the intermediate image spatial feature or text embedding e,\n$S_l = Softmax\\left(\\frac{Q_l(z_t)K_l(z_t)^T}{\\sqrt{d}}\\right),$\n$C_l = Softmax\\left(\\frac{Q_c(z_t)K_c(e)^T}{\\sqrt{d}}\\right),$\nwhere $Q_*(\\cdot)$ and $K_*(\\cdot)$ are linear projections with the dimension d. Though self-attention modules are modified in this framework, we still compute the original self-attention maps for latent semantic map generation in each step.\nSemantic Segmentation. Cross-attention maps contain the similarity values between image patch s and text token P\u2081. Therefore, in each row of C, a higher probability C[s, i] indicates a closer relationship between s and Pi. Based on this, we segment the zt as the set of regions masked by [m1, m2, ..., mK], with K denotes the number of tokens and mi \u2208 {0, 1} indicate the latent semantic region of token Pi.\nSpecifically, we first upsample all cross-attention maps C to the same size, then average and renormalize them along the spatial dimension, resulting in the final attention map Ct. This map estimates the likelihood of assigning patch s for the token Pi. Ultimately, the argmax operation is applied"}, {"title": "3.2.3 Concept Feature Injection", "content": "Region-grouped Blend Attention. With the latent semantic masks M, RBA divides the latent image into concept groups, enabling each generated concept to query features from its respective reference image. Specifically, at each self-attention layer l and time step t, the query, key, value feature from zt are Q, K, and V. We omit l, t for brevity. Next, we segment the Q into groups [90, 91, \u2026, 9N] based on M. For each group q\u2081 where i > 0, we concatenate K and V with its corresponding injected key and value features from reference path, resulting in K\u2081 = [K, Ki] and V\u2081 = [V, Vi]. For the feature group qo corresponding to the background, we have Ko = K and V\u2081 = V. Based on this, we individually compute the masked region-grouped attention output xi as:\n$x_i = Softmax\\left(\\frac{M_i \\odot Q_i K_i^T}{\\sqrt{d}}\\right)V_i,$\nwhere Mi = [1, Mi]. Finally, we blend the outputs [x0,x1,..., XN] into a single output by putting the pixels from each output into their corresponding positions based on M. This process effectively ensures precise attribute alignment and feature injection for each generated concept."}, {"title": "3.2.4 Weighted Mask Strategy", "content": "Despite the segmentation mask helping to mitigate unrelated features from reference images, the model still struggles to accurately capture the unique attributes of the target concept, particularly in preserving human identity. To address this, we introduce a scaling factor wi to each mask Mi during the RSA and RBA processes, aiming to enhance the model's focus on the desired concept features. The effectiveness of this strategy is examined in Fig. 6 and further discussed in Sec. 4.3."}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Implementation details", "content": ""}, {"title": "Datasets.", "content": "Subject-to-Image Generation. The evaluation dataset consists of 35 human identities (IDs): 24 from previous work [18, 34] and 11 newly collected. Each ID includes 3 reference images and 1 target image. We also collect 30 prompts covering expressions, attributes, actions, etc. from [17].\nMulti-concept Customization. The evaluation dataset consists of 30 sample images collected from the previous works [1, 7, 34] and the Internet. Each image includes an ID and 2-3 reference accessories. The segmentation masks of all reference concepts are generated by segment-anything [15].\nConfigurations. Our method is training-free and employs Stable-Diffusion v1.5 as the base model for evaluation using an NVIDIA 3090 GPU. We use 50 steps of the DDIM sampler [29]. The scale of classifier-free guidance is set to 7.5. The mask weight w is set to 3 for each concept. We only modify the self-attention module in blocks 5 and 6 of the U-Net. The hyperparameter a is set to 0.4.\nEvaluation. We use CLIP-I [8] and DINO [4] metrics to measure image fidelity, and CLIP-T [21] to assess prompt consistency. Face similarity is calculated by detecting facial regions with MTCNN [38] and then computing pairwise ID similarity using FaceNet [26]."}, {"title": "4.2. Results", "content": "For a comprehensive evaluation, we compare our method with baselines in both human-centric subject-to-image generation and multi-concept human generation domains. The quantitative compared results are shown in Tab. 1. Notably, our method significantly improves the CLIP-T metric compared to subject-to-image generation baselines. This improvement is expected, as these methods, being trained on large-scale datasets, tend to overfit and consequently struggle with generating unseen semantic scenes. For multi-concept customization, all competing methods exhibit poor performance on the face similarity metric, highlighting their significant challenges in preserving facial identity. The qualitative compared results are provided in Fig. 2 and Fig. 7."}, {"title": "4.3. Ablation Studies", "content": "Visualizatoin of RSA and RBA. We visualize the correspondence between each feature in the generated image and the concept image based on their attention maps in RSA, as well as the attention maps between several image patches and their respective reference concepts in RBA. As shown in Fig. 5, (a) the similar regions in the generated and reference images indicate higher correlation and (b) these patches precisely extract features from their reference concepts.\nChoice of weights w. We explore the impacts of different weight settings in Fig. 6. We observe that the results improved significantly when adopting the weighted masks. Although higher weights for each concept lead to more faithful generation, as shown in the first row, excessively high weights can cause coherence issues for easily generated objects such as the tie in this case. Therefore, balancing the weights is crucial to ensure that the generated concepts are both accurate and harmoniously integrated within the scene.\nChoice of the hyperparameter a. In this work, RSA handles semantic layout construction for aT steps, and RBA takes over for concept feature injection during the remaining steps. We analyze the optimal value of a in Fig. 8. When a = 0, the generated concepts differ significantly from the reference concepts on shape, such as facial form, due to the absence of RSA. Conversely, when a = 1, which excludes RBA, the results struggle with identity preservation. These observations highlight the effectiveness of both RSA and RBA. Finally, we find that a = 0.4 achieves the best balance between semantic layout and identity preservation."}, {"title": "4.4. Applications", "content": "Universal-style human customization. MagicFace is capable of personalizing humans across diverse styles with multiple concepts as shown in Fig. 1. Unlike existing methods, which are constrained by their training datasets to photorealistic styles, MagicFace is training-free and adept at customizing any style by accurately embedding reference concept features into the generated images.\nTexture transfer. MagicFace is also highly effective for texture transfer, as demonstrated in Fig. 9. By precisely injecting features from input images, our method seamlessly integrates these appearances into generated objects, showcasing its versatility and effectiveness in diverse applications."}, {"title": "5. Conclusion", "content": "In this work, we present MagicFace, the first method for universal-style human image personalization in a training-free manner. MagicFace features a coarse-to-fine generation pipeline with two sequential stages: RSA for establishing the initial semantic layout by extracting overall semantic understanding from reference images, and Region-RBA for precise"}, {"title": "I. Limitation", "content": "MagicFace is capable of synthesizing high-fidelity human images, which, while impressive, raises significant privacy and security concerns. This high level of fidelity can lead to the unauthorized use of personal face portraits, potentially resulting in ethical issues and hindering the broader adoption of such technology. Ensuring that these tools are used responsibly and with proper consent is crucial to mitigating these risks."}, {"title": "J. Societal impact", "content": "The societal impact of personalized human image generation technologies, such as MagicFace, is profound. These innovations drive creativity in entertainment, virtual reality, and augmented reality, enabling highly realistic content in video games and films that greatly enhance user experiences. However, as these technologies become increasingly accessible, they raise significant concerns about privacy, consent, and potential misuse. Balancing innovation with ethical considerations is essential to fully realize the benefits of subject-driven text-to-image generation while safeguarding societal interests."}, {"title": "K. Superiorities compared with baselines", "content": "Training-Free Approach. Existing human-centric subject-to-image generation methods typically rely on extensive retraining on large-scale datasets or fine-tuning with dozens of images. These approaches involve time-consuming processes, making rapid deployment challenging. In contrast, MagicFace is entirely training-free, eliminating the need for large-scale pre-training and the associated computational overhead. By requiring only a single image per concept, our approach is significantly more efficient and practical, reducing both time and computational resource demands.\nHigh-fidelity Results. Despite its simplicity, our method consistently delivers more natural and realistic human personalization results. Extensive quantitative and qualitative evaluations demonstrate that our approach matches or even surpasses the performance of more complex, training-based methods, highlighting its effectiveness in producing high-fidelity human images.\nVersatility in Applications. (1) Universal-style human customization. Unlike existing methods constrained by their training datasets to only photorealistic styles, our method excels at customizing a wide range of styles. It accurately embeds reference concept features into the generated image in an evolving scheme during the sampling process, making it the first method capable of universal-style human customization. (2) Texture transfer. Our approach is not only superior in human image synthesis but also highly effective for texture transfer. By precisely extracting appearance features from input images and seamlessly integrating these features into generated objects, our method shows its robustness across different applications.\nMulti-concept customization. Compared to current human-centric subject-to-image generation methods which fail to personalize humans with multiple given concepts, our approach enables high-quality multi-concept human customization. For a more thorough evaluation, we also compare our method against specialized baselines in the multi-concept customization domain. While these methods handle general objects with coarse-grained textures reasonably well, our experimental results indicate that they falter in human-centric customization. In contrast, our approach precisely preserves human identity, setting a new benchmark for multi-concept human image synthesis."}]}