{"title": "MagicFace: Training-free Universal-Style Human Image Customized Synthesis", "authors": ["Yibin Wang", "Weizhong Zhang", "Cheng Jin"], "abstract": "Existing human image personalized generation methods often require tedious training: either fine-tuning with a few images or retraining on large-scale datasets. In such cases, these methods are prone to overfitting and encounter difficulties when personalizing individuals of diverse styles. Moreover, these training-based approaches also struggle with multi-concept human image customizing. To this end, we propose MagicFace, the first method for universal-style human image personalized synthesis that enables single/multi-concept customization for humans of any style in a training-free manner. MagicFace introduces a coarse-to-fine generation pipeline, involving two sequential stages: semantic scene construction and concept feature injection. This is achieved by our Reference-aware Self-Attention (RSA) and Region-grouped Blend Attention (RBA) mechanisms. Specifically, in the first stage, RSA enables the latent image to query features from reference concepts simultaneously, extracting the coarse-grained overall semantic understanding to facilitate the initial semantic layout establishment. In the second stage, we employ an attention-based semantic segmentation method to pinpoint the generated regions of all concepts in the latent image at each step. Following this, RBA divides the pixels of the latent image into semantic groups, with each group querying fine-grained features from its reference concept, which ensures precise attribute alignment and feature injection. Throughout the two-stage process, a weight mask strategy is employed to ensure the model focuses more on the reference concepts. Extensive experiments demonstrate our superiority in both human-centric subject-to-image synthesis and multi-concept human image customization. Our approach also can be applied to texture transformation, further enhancing its versatility and applicability. The project page is here.", "sections": [{"title": "1. Introduction", "content": "Text-to-image generation has undergone remarkable advancements with the emergence of large-scale text-to-image diffusion models such as Stable Diffusion [23]. One of the most popular and challenging topics in these developments is the person-centric subject-to-image generation, which aims to personalize individuals in novel scenes, styles, and actions"}, {"title": "2. Related Work", "content": "Human image personalized generation is one of the most widely demanded and challenging directions in subject-driven image generation. The goal is to synthesize new images depicting specific individuals in novel scenes, styles, and actions. Current methods can be mainly divided into two categories: tuning-based customization and zero-shot customization.\nTuning-based Customization. Tuning-based methods rely on additional subject-specific optimization during test time. The pioneering work, Dreambooth [24], fine-tunes a text-to-image diffusion model using numerous reference images to bind a unique identifier to the given subject. A concurrent work, Textual Inversion [9] transforms subject images into a simple learnable text embedding to encode the subject's identity. Following this, subsequent works such as NeTI [1] and XTI [31] introduce implicit time-aware representation and layer-wise learnable embedding respectively to achieve better performance. Additionally, Tuning-Encoder [3] generates an initial set of latent codes using a pre-trained encoder and refines these codes with minimal fine-tuning iterations to better preserve subject identities. Despite their effectiveness, these methods always encounter inefficiency due to their demand of considerable time and computing resources for fine-tuning in the test time.\nZero-shot Customization. Zero-shot methods attempt to perform customization using a single image with a single forward pass, significantly accelerating the personalization process. For example, ELITE [35] and InstantBooth [27] achieve this by utilizing a global mapping network to encode reference images into word embeddings and a local map-ping network to inject reference image patch features into cross-attention layers. Fastcomposer [36] and PhotoMaker [17] extract identity-centric embeddings by fine-tuning the image encoder and merging the class and image embeddings. Face-Diffuser [34] reveals the training imbalance and quality compromise issues in these methods and addresses them by proposing a novel collaborative generation pipeline. Instan-"}, {"title": "3. MagicFace", "content": "3.1. Overview\nWe present an overview of our proposed MagicFace in Fig. 3. Our goal is to achieve high-fidelity human image synthesis, enabling single/multi-concept customization across various styles in a training-free manner. To accomplish this, we pro-pose a coarse-to-fine generation pipeline, incorporating our Reference-aware Self-Attention (RSA) and Region-grouped Blend Attention (RBA) mechanisms. Specifically, our sam-pling process involves two sequential stages with a total of T denoising steps.\nIn the first stage, we employ RSA to extract the overall semantic understanding from all given concepts, facilitating the initial semantic scene construction.\nIn the second stage, a latent semantic map is derived to pinpoint the generated positions of all concepts at the pixel level in each step. We then utilize RBA to accurately inject the features of each reference concept into their correspond-ing positions, ensuring that the generated concepts closely resemble the reference images.\nTo emphasize the model's attention to the reference con-cepts, a weighted mask strategy is implemented during cus-tomized synthesis.\n3.1.1 Vanilla Self-attention in Diffusion Model\nIn this work, we adopt pre-trained Stable Diffusion (SD) [22] as our base model, utilizing its U-Net 6 as the denoiser. The original SD U-Net is composed of 16 layers with each layer including a residual block, a self-attention module, and a cross-attention module. The self-attention (SA) layer in U-Net is crucial for establishing the layout and detailing the content, which can be formulated as\n$\\text{SA}(Q, K, V) = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d}}) V.$\nwhere Q, K, and V are the query, key, and value projected from the latent image feature using different linear mappings with the dimension d.\n3.1.2 Dual-path Guided Customization\nAs illustrated in Fig. 3, the overall inference pipeline involves two paths: reference path and customization path, which will be elaborated in the following.\nReference Path. For each concept image in the refer-ence path, we first apply a diffusion forward process [11] to compute the noised reference latent zy. At each step t, we input z and corresponding text prompt into the U-Net \u20ac\u03b8. In the self-attention layer l and time step t, we extract the Ki,l,t and Vi,l,t of i-th concept to guide the synthesis in the customization path.\nCustomization Path. This path starts by sampling the la-tent zy from a Gaussian distribution N(0, I). Then, we mod-ify the vanilla self-attention module of SD U-Net, extending it into our RSA/RBA. At each step t, we pass the latent zt and target prompt P into modified U-Net \u20ac, integrating concept features from Ki,l,t and Vi,l,t derived in reference path to guide generated concept imitate the reference image. The final denoised result is the ultimate customized image.\n3.2. Coarse-to-Fine Generation Process\nMagicFace synthesizes images by progressing from coarse-grained semantic scene construction with aT steps to fine-grained feature injection with T(1-a) steps in an evolving scheme. In the latter stage, we employ an attention-based semantic segmentation method, which paves the way for precise feature injection. We will delve into the details of each stage in the following.\n3.2.1 Semantic Scene Construction\nReference-aware Self-Attention. RSA enables the latent image to query features from all reference concepts simulta-neously to integrate coarse-grained semantic information for initial semantic scene construction. Specifically, in each self-attention layer l and time step t, the query, key, and value feature from zt are Q1,t, K\u0131,t and Vi,t. We concatenate K\u0131,t and Vi,t with N injected key and value features from refer-ence path respectively, resulting in K = [K, K1, K2, ..., KN] and V = [V, V1, V2, ..., VN] where we omit l,t for brevity. However, we aim to query features exclusively from the concept region, as irrelevant background information in the"}, {"title": "3.2.2 Latent Semantic Map Generation", "content": "reference image may distract the model and reduce its effec-tiveness. To address this issue, we introduce segmentation masks of reference concepts and concatenate them with an all-ones matrix to obtain M = [1, M1, M2, ..., MN] to rectify the model's attention. Ultimately, the RSA can be formulated as follows:\n$\\text{RSA}(Q, K, V,M) = \\text{Softmax}(\\frac{M\\odot QK^T}{\\sqrt{d}}) V.$\nHere,\n$\\odot$ denotes the Hadamard product. This process ensures the generated image effectively interacts with the overall se-mantic information from the reference images while filtering out unrelated noise.\n3.2.2 Latent Semantic Map Generation\nTo ensure precise feature injection at the pixel level for each concept, we need to identify their generated region. How-ever, this task is challenging because the latent image at each step is not accessible during the generation process. Fortu-nately, the attention layers in the U-Net contain rich semantic information, which can be utilized to identify semantic units efficiently [5]. Therefore, in this work, we compute the latent semantic map based on attention maps, involving two se-quential steps: cross-attention-based semantic segmentation and self-attention-based segmentation completion.\nAttention Maps Generation. In the attention layer l and time step t, a self-attention map S and a cross-attention map C are calculated over linear projections of the intermediate image spatial feature or text embedding e,\n$S = \\text{Softmax}\\left(\\frac{Q_s(z_t)K_s(z_t)^T}{\\sqrt{d}}\\right)$,\n$C = \\text{Softmax}\\left(\\frac{Q_c(z_t)K_c(e)^T}{\\sqrt{d}}\\right)$\nwhere Q() and K*(\u00b7) are linear projections with the dimen-sion d. Though self-attention modules are modified in this framework, we still compute the original self-attention maps for latent semantic map generation in each step.\nSemantic Segmentation. Cross-attention maps contain the similarity values between image patch s and text token Pi. Therefore, in each row of C, a higher probability C[s, i] indicates a closer relationship between s and Pi. Based on this, we segment the zt as the set of regions masked by [m1, m2, ..., mK], with K denotes the number of tokens and mi \u2208 {0, 1} indicate the latent semantic region of token Pi. Specifically, we first upsample all cross-attention maps C to the same size, then average and renormalize them along the spatial dimension, resulting in the final attention map Ct. This map estimates the likelihood of assigning patch s for the token Pi. Ultimately, the argmax operation is applied to the token dimension to determine the activation of each patch:\nis = arg max Ct[s, i].\nAlong this line, we compute mi by setting the element in the patch set {s : is = i} as 1, and others to 0. Note that we only need the semantic masks of N reference concepts for feature injection. Hence, we retain the masks of concept-specific tokens and merge the remaining ones into a new mask mo indicating the background region, yielding M = [m0, m1, m2, ..., mn].\nVisualization. The first row in Fig. 4 demonstrates the result of the aforementioned semantic segmentation results. The semantic maps effectively identify the rough locations of the generated concepts. However, they often exhibit un-clear boundaries and may contain internal gaps, leading to problematic results. To address this issue, we refine and complete the semantic map using self-attention maps [32], as detailed in the following section."}, {"title": "3.2.3 Concept Feature Injection", "content": "Region-grouped Blend Attention. With the latent seman-tic masks M, RBA divides the latent image into concept groups, enabling each generated concept to query features from its respective reference image. Specifically, at each self-attention layer l and time step t, the query, key, value feature from zt are Q, K, and V. We omit l, t for brevity. Next, we segment the Q into groups [90, 91, \u2026, 9N] based on M. For each group q\u2081 where i > 0, we concatenate K and V with its corresponding injected key and value features from reference path, resulting in K\u2081 = [K, Ki] and V\u2081 = [V, Vi]. For the feature group qo corresponding to the background, we have Ko = K and V\u2081 = V. Based on this, we individu-ally compute the masked region-grouped attention output xi as:\nxi = Softmax\\left(\\frac{MqK^TV_i}{\\sqrt{d}}\\right)\nwhere Mi = [1, Mi]. Finally, we blend the outputs [x0,x1,..., XN] into a single output by putting the pixels from each output into their corresponding positions based on M. This process effectively ensures precise attribute alignment and feature injection for each generated concept.\n3.2.4 Weighted Mask Strategy\nDespite the segmentation mask helping to mitigate unrelated features from reference images, the model still struggles to accurately capture the unique attributes of the target concept, particularly in preserving human identity. To address this, we introduce a scaling factor wi to each mask Mi during the RSA and RBA processes, aiming to enhance the model's focus on the desired concept features. The effectiveness of this strategy is examined in Fig. 6 and further discussed in Sec. 4.3."}, {"title": "4. Experiment", "content": "4.1. Implementation details\nDatasets.\nSubject-to-Image Generation. The evaluation dataset con-sists of 35 human identities (IDs): 24 from previous work [18, 34] and 11 newly collected. Each ID includes 3 refer-ence images and 1 target image. We also collect 30 prompts covering expressions, attributes, actions, etc. from [17].\nMulti-concept Customization. The evaluation dataset con-sists of 30 sample images collected from the previous works [1, 7, 34] and the Internet. Each image includes an ID and 2-3 reference accessories. The segmentation masks of all reference concepts are generated by segment-anything [15].\nConfigurations. Our method is training-free and employs Stable-Diffusion v1.5 as the base model for evaluation us-ing an NVIDIA 3090 GPU. We use 50 steps of the DDIM sampler [29]. The scale of classifier-free guidance is set to 7.5. The mask weight w is set to 3 for each concept. We only modify the self-attention module in blocks 5 and 6 of the U-Net. The hyperparameter a is set to 0.4.\nEvaluation. We use CLIP-I [8] and DINO [4] metrics to measure image fidelity, and CLIP-T [21] to assess prompt consistency. Face similarity is calculated by detecting facial regions with MTCNN [38] and then computing pairwise ID similarity using FaceNet [26].\n4.2. Results\nFor a comprehensive evaluation, we compare our method with baselines in both human-centric subject-to-image gen-eration and multi-concept human generation domains. The quantitative compared results are shown in Tab. 1. Notably, our method significantly improves the CLIP-T metric com-pared to subject-to-image generation baselines. This im-provement is expected, as these methods, being trained on large-scale datasets, tend to overfit and consequently struggle with generating unseen semantic scenes. For multi-concept customization, all competing methods exhibit poor perfor-mance on the face similarity metric, highlighting their signif-icant challenges in preserving facial identity. The qualitative compared results are provided in Fig. 2 and Fig. 7."}, {"title": "4.3. Ablation Studies", "content": "Visualizatoin of RSA and RBA. We visualize the corre-spondence between each feature in the generated image and the concept image based on their attention maps in RSA, as well as the attention maps between several image patches and their respective reference concepts in RBA. As shown in Fig. 5, (a) the similar regions in the generated and refer-ence images indicate higher correlation and (b) these patches precisely extract features from their reference concepts.\nChoice of weights w. We explore the impacts of differ-ent weight settings in Fig. 6. We observe that the results improved significantly when adopting the weighted masks. Although higher weights for each concept lead to more faith-ful generation, as shown in the first row, excessively high weights can cause coherence issues for easily generated ob-jects such as the tie in this case. Therefore, balancing the weights is crucial to ensure that the generated concepts are both accurate and harmoniously integrated within the scene.\nChoice of the hyperparameter a. In this work, RSA han-dles semantic layout construction for aT steps, and RBA takes over for concept feature injection during the remaining steps. We analyze the optimal value of a in Fig. 8. When a = 0, the generated concepts differ significantly from the reference concepts on shape, such as facial form, due to the absence of RSA. Conversely, when a = 1, which excludes RBA, the results struggle with identity preservation. These observations highlight the effectiveness of both RSA and RBA. Finally, we find that a = 0.4 achieves the best balance between semantic layout and identity preservation."}, {"title": "4.4. Applications", "content": "Universal-style human customization. MagicFace is ca-pable of personalizing humans across diverse styles with multiple concepts as shown in Fig. 1. Unlike existing meth-ods, which are constrained by their training datasets to pho-torealistic styles, MagicFace is training-free and adept at customizing any style by accurately embedding reference concept features into the generated images.\nTexture transfer. MagicFace is also highly effective for texture transfer, as demonstrated in Fig. 9. By precisely in-jecting features from input images, our method seamlessly integrates these appearances into generated objects, show-casing its versatility and effectiveness in diverse applications."}, {"title": "5. Conclusion", "content": "In this work, we present MagicFace, the first method for universal-style human image personalization in a training-free manner. MagicFace features a coarse-to-fine generation pipeline with two sequential stages: RSA for establishing the initial semantic layout by extracting overall semantic under-standing from reference images, and Region-RBA for precise"}, {"title": "A. More compared baselines", "content": "We also compare our method with tuning-based baselines, i.e., Dreambooth [24] and Text Inversion [9], and a zero-shot baseline, IP-adapter [37]. The quantitative and qualitative results are provided in Tab. 2 and Fig. 10, respectively."}, {"title": "B. User Study", "content": "We conduct a user study to make a more comprehensive comparison. Specifically, we invite 30 participants through voluntary participation and assign them the task of complet-ing 40 ranking questions. Each question includes reference images, the corresponding text prompt, and the generated images of competitive methods. We anonymize the names of all methods and randomize the order of methods in each question. The ranking criteria comprehensively considered factors including ID Fidelity, Image Quality, and Prompt Consistency. We collect a total of 23 valid questionnaires for subject-to-image generation and 26 for multi-concept customization. The results are presented in Tab. 3 and Tab. 4. Notably, our method received favorable feedback from the majority of participants."}, {"title": "C. More visual results in photorealism style", "content": "We provide more visual results of photorealism style in Fig. 12."}, {"title": "D. More visual results in various styles", "content": "We provide more visual results of diverse styles in Fig. 13."}, {"title": "E. Choice of self-attention layer replacement", "content": "We explore the optimal choice of replacing the original self-attention in the basic block with our RSA/RBA, as shown in Fig. 16 and Fig. 17. The results indicate that replacing the self-attention layers in blocks 5 and 6 produces the highest fidelity images."}, {"title": "F. Choice of weight w", "content": "We provide more cases for exploring the impacts of different weight settings in Fig. 15."}, {"title": "G. Choice of hyperparameter a", "content": "We provide an additional case for exploring the optimal value of a as shown in Fig. 11."}, {"title": "H. Visualizatoin of RSA and RBA", "content": "We provide more visualization results of RSA and RBA in Fig. 14."}, {"title": "I. Limitation", "content": "MagicFace is capable of synthesizing high-fidelity human images, which, while impressive, raises significant privacy and security concerns. This high level of fidelity can lead to the unauthorized use of personal face portraits, potentially resulting in ethical issues and hindering the broader adop-tion of such technology. Ensuring that these tools are used responsibly and with proper consent is crucial to mitigating these risks."}, {"title": "J. Societal impact", "content": "The societal impact of personalized human image genera-tion technologies, such as MagicFace, is profound. These innovations drive creativity in entertainment, virtual reality, and augmented reality, enabling highly realistic content in video games and films that greatly enhance user experiences. However, as these technologies become increasingly acces-sible, they raise significant concerns about privacy, consent, and potential misuse. Balancing innovation with ethical considerations is essential to fully realize the benefits of subject-driven text-to-image generation while safeguarding societal interests."}, {"title": "K. Superiorities compared with baselines", "content": "Training-Free Approach. Existing human-centric subject-to-image generation methods typically rely on extensive retraining on large-scale datasets or fine-tuning with dozens of images. These approaches involve time-consuming pro-cesses, making rapid deployment challenging. In contrast, MagicFace is entirely training-free, eliminating the need for large-scale pre-training and the associated computational overhead. By requiring only a single image per concept, our approach is significantly more efficient and practical, reducing both time and computational resource demands.\nHigh-fidelity Results. Despite its simplicity, our method consistently delivers more natural and realistic human per-sonalization results. Extensive quantitative and qualitative evaluations demonstrate that our approach matches or even surpasses the performance of more complex, training-based methods, highlighting its effectiveness in producing high-fidelity human images.\nVersatility in Applications. (1) Universal-style human cus-tomization. Unlike existing methods constrained by their training datasets to only photorealistic styles, our method excels at customizing a wide range of styles. It accurately embeds reference concept features into the generated image in an evolving scheme during the sampling process, making it the first method capable of universal-style human cus-tomization. (2) Texture transfer. Our approach is not only superior in human image synthesis but also highly effective for texture transfer. By precisely extracting appearance fea-tures from input images and seamlessly integrating these features into generated objects, our method shows its robust-ness across different applications.\nMulti-concept customization. Compared to current human-centric subject-to-image generation methods which fail to personalize humans with multiple given concepts, our ap-proach enables high-quality multi-concept human customiza-tion. For a more thorough evaluation, we also compare our method against specialized baselines in the multi-concept customization domain. While these methods handle general objects with coarse-grained textures reasonably well, our ex-perimental results indicate that they falter in human-centric customization. In contrast, our approach precisely preserves human identity, setting a new benchmark for multi-concept human image synthesis."}]}