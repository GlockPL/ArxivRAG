{"title": "Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events", "authors": ["Aditya Chinchure", "Sahithya Ravi", "Raymond Ng", "Vered Shwartz", "Boyang Li", "Leonid Sigal"], "abstract": "The commonsense reasoning capabilities of vision- language models (VLMs), especially in abductive reasoning and defeasible reasoning, remain poorly understood. Most benchmarks focus on typical visual scenarios [1, 23, 42], making it difficult to discern whether model performance stems from perception skills, pure statistical recall, or actual reasoning. We argue that by focusing on atypical events in videos, clearer insights can be gained on the core capabilities of VLMs. Explaining and understanding such out-of-distribution events requires models to extend beyond basic pattern recognition and regurgitation of prior knowledge. To this end, we introduce BlackSwanSuite, a benchmark for evaluating VLMs' ability to reason about unexpected events through abductive and defeasible tasks. Our tasks artificially limit the amount of visual information provided to models while questioning them about hidden unexpected events, or provide new visual information that could change an existing hypothesis about the event. We curate a comprehensive benchmark suite comprising over 3,800 MCQ, 4,900 generative and 6,700 yes/no tasks, spanning 1,655 videos. After extensively evaluating various state-of-the-art VLMs, we find significant performance gaps of up to 32% from humans on these tasks, revealing key limitations in current VLMs.", "sections": [{"title": "1. Introduction", "content": "Vision-language models (VLMs) are becoming increas- ingly capable of reasoning about the world thanks to their exposure to vast amounts of visual data, and the emer- gent capabilities of their underlying large language models (LLMs). Recent multi-frame video-language models such as VILA [15], Video-LLaMA [4], and LLaVA-Video [43] show promising results in tasks such as video captioning and question answering. However, it remains unclear to what extent they can reason about unexpected events.\nUnexpected events pose a unique challenge to AI mod- els, as the vast majority of training videos contain typical events, leaving models poorly equipped to handle novel scenarios [2, 11, 28]. In contrast, humans excel at un- derstanding, rationalizing, and responding to unexpected events. This ability is underpinned by abductive reason- ing, which draws the most likely explanation from limited observations, and defeasible reasoning, where initial con- clusions are revised in light of new or conflicting evidence. For example, a human may observe two damaged cars in an intersection, and hypothesize that one of the drivers ran the red light (abductive). Later, on observing that the traf- fic lights were malfunctioning, they instead think the crash was caused by the lights rather than the drivers (defeasabil- ity). If AI models are to function as autonomous decision makers (e.g., in self-driving vehicles [20, 35]), reasoning about unexpected events abductively and defeasibly would be critical to their safety and real-world utility.\nTraditional benchmarks for video reasoning primarily focus on predictable scenarios (e.g. [39]), overlooking the critical challenge of assessing how models handle rare, un- foreseen events. Furthermore, these benchmarks often do not require models to revise their reasoning in response to conflicting or new evidence.\nMotivated by the need to evaluate the gap in abduc- tive and defeasible reasoning abilities between humans and models, and the limitations of existing benchmarks, we pro- pose the novel BlackSwanSuite benchmark. Our bench- mark contains a range of tasks that require nuanced per- ception, comprehension, and reasoning abilities. We fo- cus on leveraging the visual information provided in videos of expectation-violating events, including but not limited to surprises, accidents, pranks and other uncommon situations. Such videos are ideal for evaluating abductive and defea- sible reasoning capabilities of models, as these events oc-"}, {"title": "2. Background and Related Work", "content": "We first provide background on abductive reasoning (\u00a72.1) and defeasible reasoning (\u00a72.2) along with related work, and then describe related video language benchmarks (\u00a72.3)."}, {"title": "2.1. Abductive Reasoning", "content": "Abductive reasoning focuses on finding the most plausible explanation H for a set of observations O = {01,..., On} [25]. Consider the following example:\n\u2022 Observation 1 (01): \u201cThe door was left open.\"\n\u2022 Observation 2 (O2): \u201cA broken vase lay on the floor.\"\nThe following hypothesis H is a plausible explanation for what might have happened between 01 and 02:\n\u2022 Hypothesis (H): \u201cA cat entered through the open door and knocked over the vase.\"\nThough other explanations exist (e.g., \"a black swan flew in and knocked over the vase\"), abductive reasoning favors the most likely hypothesis based on typical scenarios.\nComputational abductive reasoning has drawn interest in NLP, with Bhagavatula et al. [3] initiating a task to gener- ate plausible explanations for narratives. Du et al. [6] and Paul and Frank [24] extended this work, using graph-based methods and models predicting event sequences. Qin et al. [26] explored abductive and counterfactual reasoning, while Liu et al. [17] emphasized unexpected scenarios. Earlier, Ovchinnikova et al. [22] addressed abductive reasoning in discourse parsing.\nIn the vision-language domain, Sherlock [9] provides a visual abductive dataset where models rank 10 inference candidates, aligning with human preference on inferred ex- planations. Liang et al. [14] introduced causality in pre- diction tasks by generating explanation events based on premises. VideoABC [44] framed an abductive reasoning task tailored to instructional videos, where the model must infer likely sequences of keyframes.\nOur work differs from the prior approaches [14, 44] by focusing explicitly on unexpected events and measuring de- feasibility. This task requires reasoning grounded in visual content rather than relying solely on language cues, mark- ing a distinct shift in the abductive reasoning landscape."}, {"title": "2.2. Defeasible Reasoning", "content": "Defeasible reasoning is a form of reasoning where conclu- sions are drawn tentatively, allowing for revision if conflict- ing evidence arises [30]. Formally, let P represent initial premises and let C be a defeasible conclusion drawn from P. If new information P' is introduced, it may lead to \u00acC (i.e., the invalidation of C'), thereby \"defeating\" the initial conclusion. For example, given the initial premises P, we might infer C."}, {"title": "2.3. Foundational Vision Language Reasoning", "content": "Traditional benchmarks in vision and language reasoning focus on reasoning about commonly occurring video events. For instance, early video reasoning benchmarks such as TGIF-QA [10] and Activity Net-QA [41] challenge mod- els to answer questions involving spatio-temporal reason- ing. Moving beyond this, CLEVRER [40] and NEXT-QA [37] introduce causal and counterfactual reasoning, while Causal-VidQA [12] emphasizes causal inference within video content. Human-centered reasoning benchmarks, such as VCR [42] and MovieQA [32], further test multi- modal understanding by requiring models to interpret social cues in videos. These benchmarks assess sophisticated rea- soning within human-centric environments, though they are typically limited to normative situations without unexpected elements. Some benchmarks specifically target surprising scenarios. For example, FunQA [38] asks questions about funny videos. However, these questions do not specifically involve abduction or defeasibility, and target video caption- ing and conversation instead."}, {"title": "3. Tasks", "content": "Surprising or unexpected events often follow a structured narrative, beginning with a normal scenario, followed by an unexpected event that deviates from the norm, and conclud- ing with an unlikely outcome. We leverage this narrative structure of our videos. As shown in Figure 1, each video is divided into three parts: the pre-event (Vpre), showing the premise, or the events leading up to the unexpected event; the main event (Vmain), where the unexpected event occurs; and the post-event (Vpost), which reveals the outcome of the main event and concludes the video. BlackSwanSuite has three tasks based on the amount of available video informa- tion to a model, each testing different reasoning abilities:"}, {"title": "3.1. Forecaster : Reason about the Future", "content": "In this task, models are only shown the pre-event, Vpre and asked to predict the next event. This tests the model's ability to evaluate the scenario in the video, and explain future tra- jectories, and which lays the groundwork for defeasability. This task only contains one sub-task:\n\u2022 Forecaster -Gen: Generate a free-text answer to \"What happens next?\""}, {"title": "3.2. Detective : Investigate the Outcome", "content": "This task presents models with Vpre and Vpost and asks them to reason about what could have happened in-between, i.e., in the main event Vmain, requiring abductive reasoning. Furthermore, this task tests the defeasible reasoning ability of the model by asking it to validate or invalidate a hypoth- esis of what could be happening in Vmain. This task has three sub-tasks:\n\u2022 Detective -Gen: Generate a free-text answer to \"What happened in the middle?\"\n\u2022 Detective -MCQ: Choose one of three options for \u201cWhat happened in the middle?\"\n\u2022 Detective -Y/N: Validate a previous hypothesis about Vmain."}, {"title": "3.3. Reporter : Explain the Events", "content": "In this task, models see Vpre, Vmain, and Vpost (the en- tire video) and are asked to explain the entire sequence of events. In addition, it tests defeasability by asking mod- els to validate or invalidate a previous hypothesis using the context provided by the entire video. This task has three sub-tasks:\n\u2022 Reporter -Gen: Generate a free-text explanation of the entire video.\n\u2022 Reporter -MCQ: Choose the best description of the video's events.\n\u2022 Reporter -Y/N: Confirm if a hypothesis about Vmain holds with full context.\nWith the Forecaster, Detective and Reporter tasks de- fined, we now explain our data collection process to build our generative (Gen), multiple-choice (MCQ), and yes or no validation (Y/N) questions."}, {"title": "4. The BlackSwanSuite Dataset", "content": "The data collection process for BlackSwanSuite dataset summarized in Fig 4 in Appendix A. Below, we describe the source and types of videos in our dataset (\u00a74.1) and the annotation process (\u00a74.2), the creation of the task variants (\u00a74.3), and the dataset statistics (\u00a74.4)."}, {"title": "4.1. Videos", "content": "Source. Videos in BlackSwanSuite are short clips that contain one surprising event. We obtain the videos from the test set of the Oops! dataset [7], which consists of YouTube fail videos along with localization annotations for the main event occurring in the video. We filter out videos for which there was poor inter-annotator agreement on the localiza- tion, or the video contained multiple scenes.\nSplitting to parts. We divide each video into the three parts defined in Sec. 3: pre-event (Vpre), main event (Vmain), and post-event (Vpost). We use the provided lo- calization annotation to identify the main event, and use a combination of an automatic scene splitter and heuristics, as described in Appendix A.1 to obtain the three parts of the video. Following the filtering criterion and the splitting process, we have 1655 videos, each with three parts that are at least one second long."}, {"title": "4.2. Annotation Process", "content": "We collected annotations for the three tasks defined in Sec. 3. The annotation task was done in three correspond- ing steps. In the first step, we showed annotators only Vpre and asked them to come up with three possible scenarios for what could happen next (Forecaster ).\nIn the second step, we revealed Vpost, and asked anno- tators the abductive question \u201cWhat could have happened in the middle part of the video?\u201d (Detective ). They were first asked to validate or invalidate their responses to the first step, and then to write new responses to the ones they invalidated.\nIn the final step, we revealed Vmain. At this stage, the entire video was visible. Again, we asked annotators to val- idate or invalidate their answers for the second step. Finally, we asked them to write an explanation of what happened, much like a caption describing the unexpected events in the video (Reporter ).\nOur annotation process was conducted through the CloudConnect Platform by Cloud Research. Each video was annotated by a single qualified annotator who was com- pensated $0.85 per annotation task, which we estimate sums up to $10.2 per hour. We further filtered out the worker pool following the validation of 10% of the collected data by one of the authors, annotating the rest of the data by workers who were adept in the task."}, {"title": "4.3. Task Variants", "content": "Using the annotations, we build three variants of tasks, as described below.\nGenerative (Gen): Every question in Forecaster -Gen comes with 3 ground-truth hypotheses that were proposed by annotators in step 1. Questions in Detective -Gen also come with 3 ground-truth hypotheses which include valid hypotheses from step 1 and new hypotheses collected in step 2. Finally, Reporter -Gen has a single reference which is the caption collected from annotators in the last step.\nMCQ: Each MCQ for Detective has 3 choices. For the correct choice, we used explanations from step 1 (Forecaster) that were validated in step 2 as well as an- swers for step 2 (Detective), duplicating the generative question into multiple MCQs where multiple correct an- swers were available. For the distractors we used hypothe- ses from step 1 that were invalidated in step 2 after observ- ing the new information in Vpost. For questions in which we had fewer than 3 incorrect answers, we generated a cap- tion of Vpre using a VLM (LLaVA-Video [43]) and used an LLM (GPT-40 [21]) to edit it to match the style of the"}, {"title": "4.4. Dataset Statistics", "content": "BlackSwanSuite contains 1,655 videos from a wide range of topics (Fig 2), ranging from vehicle or road accidents, through children videos, to pranks and scare clips. Figure 5 (Appendix A) shows the distribution of video lengths, where the median video length is 8.83 seconds. Only 29 videos have a length greater than 25 seconds."}, {"title": "5. Experimental Setup", "content": "We evaluate the performance (\u00a75.1) of various baselines (\u00a75.2) on BlackSwanSuite."}, {"title": "5.1. Evaluation Metrics", "content": "We report models' accuracy on the MCQ and Y/N tasks. The quality of outputs generated for the generative variants of the tasks is evaluated using a combination of CLIP-based and LLM-based metrics and human evaluation. Given the open-ended nature of Forecaster and Detective, we gen- erate 3 responses for these tasks from each model. For Reporter, we only generate a single explanation, since the entire video is revealed to the model.\nCLIP Score. We embed each model-generated response and each reference explanation in CLIP [27], and compute a pair-wise similarity score. We report the maximum pair- wise similarity for each question, since we want to reward models for coming up with any plausible explanation.\nLLM-Match. Inspired by OpenEQA [19], we prompt the LLM to rate the similarity between two sentences on a scale of 1-5, providing it with every pair of reference and system- generated explanation (the full prompt is given in Appendix B). We compute the average similarity score across all pairs. We use Llama 3.1 8B [34] for this process, since it is open source and enables reproducibility.\nHuman Evaluation. A detailed description of the human evaluation setup, including the template, is in Appendix C. In summary, we ask humans to evaluate a generated re- sponse on four aspects: Correctness (between 1-5, rate how well does it answer the task question), Thoughtfulness (be- tween 1-5, rate the thoughtfulness of the sentence), Detail (between 1-5, rate how well does it describe the scene, the people/objects and actions), and Visual Entailment (0 or 1, is the description possible for the video)."}, {"title": "5.2. Baselines", "content": "Our evaluation encompasses both open-source and closed- source VLMs. In general, these models may be Video LMs (where the input is a video file, and the frames are sampled by the model) or multi-frame VLMs (where we directly provide uniformly sampled frames). We attempt to test the latest variants of these models. Our baselines include OpenAI's GPT-40 [21] and Google's Gemini 1.5 Pro [33], both leading closed-source VLMs with video un- derstanding capabilities. Furthermore, among open-source models, we test LLaVA-Video [43] (latest in LLaVA-Next series), VILA [16], VideoChat2 [13], and VideoLLaMA 2 [4]. These models have shown competitive performance on benchmarks such as MLVU [45]. Specific details about each model, including the prompts used for each task, and the variants of each model used are shown in Appendix D. Finally, we also report human performance. For MCQ and Y/N variants for Detective and Reporter, we ask a human expert 150 MCQ questions for each sub-task. For the generative variant, we crowd source human annotations for 20 videos, across all three tasks."}, {"title": "6. Results", "content": "We show results on all tasks in (\u00a76.1) to compare model and human performance, and show qualitative results (\u00a76.2)."}, {"title": "6.1. Main Results", "content": "6.1.1. MCQ and Y/N Sub-tasks\nIn Table 2, we present the results for both the MCQ and Y/N variants of Detective and Reporter. In Detective MCQ and Y/N tasks, we observe notable advantages of human performance over the best model, GPT-40, by 24.9% on the"}, {"title": "6.1.2. Generative Sub-tasks", "content": "Evaluating open-ended generation is challenging and no perfect solutions exist. The reference-based automatic met- rics may penalize plausible responses that are different from those the references. Alternatively, they could also overlook subtle semantic differences and reward models for mak- ing relevant but incorrect predictions. Human evaluation, thus, provides a more complete perspective. Still, judges may sometimes be distracted by stylistic factors like minor grammatical errors. We attempt to interpret results by bal- ancing between the two types of evaluations.\nForecaster -Gen. We consider Forecaster to be the sim- plest task in our set, as it does not inherently require abduc- tive reasoning or defeasibility assessment. Table 3 presents the results on Forecaster -Gen. We observe that models and humans perform within a similar relative margin on both CLIP-Score and LLM-Match metrics. This may be because models are trained on large-scale datasets contain- ing similar event-forecasting tasks. Human raters also pre- ferred model-generated responses, particularly those from closed-source models like GPT-40, over the human-written responses. This may be due to style preferences, since model-generated responses are typically grammatical and more detailed than human answers.\nDetective -Gen. Table 4 presents the results for Detective. The CLIP metric indicates that closed source models perform on par with humans, while open source models lag behind. LLM-Match does not provide us with a strong consensus here. Per human ratings, which are more accurate than the automatic metrics, humans surpass"}, {"title": "7. Further Analysis", "content": "We conduct a series of experiments to further study the model capabilities along the aspects of perception, compre- hension and reasoning (\u00a77.1), a Chain-of-Thought approach (\u00a77.2), and evaluating models on a hard subset (\u00a77.3)."}, {"title": "7.1. Perception, Comprehension and Reasoning", "content": "Answering an abductive reasoning question (Detective ) requires three key steps: (1) perception of objects, people, and actions in the videos, (2) comprehending the flow of events in the video based on the differences between Vpre and Vpost, and (3) abductive reasoning about what could be happening in the middle. We investigate the models' capabilities along each of these aspects by substituting sys- tem components with corresponding human-written inputs. In particular, to factor out perception, we include in the prompt the human-written captions for Vpre and Vpost (col- lected independently), and for comprehension we provide human-written comparisons between Vpre and Vpost. See Appendix F.1 for the annotation details.\nEvaluating on a subset of 150 MCQ questions with LLaVA-Video (see Table 6), we observe that performance improves by significant margins when perception (+6.4%) or perception and comprehension (+10%) are provided. This suggests that current models can improve on founda- tional perception and comprehension abilities."}, {"title": "7.2. Chain-of-Thought and Reasoning", "content": "Chain-of-thought (CoT) reasoning requires a model to come up with a step-by-step reasoning chain before arriving at a final answer. It is often shown to improve performance in reasoning tasks [36]. We evaluated the best performing open source and closed source models, LLaVA-Video and GPT-40 with CoT reasoning on a random sample of 150 questions for each of Detective and Reporter. We ask"}, {"title": "7.3. Hard Subset", "content": "Does accuracy vary depending on the predictability of the events? We consider the subset of questions where humans failed to correctly guess what is happening in the video un- til the entire video was revealed. We identify the hard sub- set by selecting MCQs for which all the annotations from Detective were marked as invalid in Reporter. Table 8 shows as much as a 10.1% drop in performance on the hard subset compared to the easy subset, suggesting models may struggle with highly unpredictable events."}, {"title": "8. Conclusions", "content": "BlackSwanSuite is a novel task to evaluate both abduc- tive and defeasible reasoning with unexpected events. Our benchmark reveals key limitations in VLMs: deficiencies in perception and comprehension, difficulty identifying nu- anced information across visual and textual modalities, and challenges in detecting and reasoning about sudden scene changes. Addressing these limitations is crucial step in models that promise to gain innately human capabilities (such as understanding humor), and are perceptually faith- ful and logical. Ultimately, we hope that it will drive the development of VLMs with deeper reasoning capabilities, enhancing their performance across a wide range of tasks."}, {"title": "G. Limitations", "content": "Evaluation Metrics : Although we defined our MCQ and Y/N tasks to challenge models to perform more com- plex reasoning, quantitative metrics like accuracy might not reflect the depth of reasoning or the logical processes in- volved. For generative tasks, current metrics struggle to capture the nuances involved. We address this by asking humans to evaluate the thoughtfulness and visual contradic- tions in reasoning, but further research is needed to auto- matically evaluate explanations generated by models.\nPre-training strategies: The models evaluated are pri- marily trained on language modeling, and may not have been explicitly trained for abductive and defeasible reason- ing, potentially limiting their performance. Further research is required to study how different pretraining or finetuning approaches may perform on this task.\nExplanation complexity: Our current annotation pro- cess focuses on free-form explanations for the unexpected scenarios, however, it may be interesting to study how more scientific reasoning (e.g., using intuitive physics to explain a fall) may influence the performance of models.\nSize and diversity: Although BlackSwanSuite includes 15,469 samples, the size and diversity might still be insuf- ficient to generalize findings across all types of reasoning challenges and rare and highly uncommon events might still be underrepresented."}]}