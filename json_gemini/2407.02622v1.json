{"title": "RISC-V R-Extension: Advancing Efficiency with Rented-Pipeline for Edge DNN Processing", "authors": ["Won Hyeok Kim", "Hyeong Jin Kim", "Tae Hee Han"], "abstract": "Abstract-The proliferation of edge devices necessitates efficient computational architectures for lightweight tasks, particularly deep neural network (DNN) inference. Traditional NPUs, though effective for such operations, face challenges in power, cost, and area when integrated into lightweight edge devices. The RISC-V architecture, known for its modularity and open-source nature, offers a viable alternative. This paper introduces the RISC-V R-extension, a novel approach to enhancing DNN process efficiency on edge devices. The extension features rented-pipeline stages and architectural pipeline registers (APR), which optimize critical operation execution, thereby reducing latency and memory access frequency. Furthermore, this extension includes new custom instructions to support these architectural improvements. Through comprehensive analysis, this study demonstrates the boost of R-extension in edge device processing, setting the stage for more responsive and intelligent edge applications.\nIndex Terms-Architectural pipeline register, Custom instruction set architecture, DNN Acceleration, Lightweight edge devices, Rented-pipeline, RISC-V", "sections": [{"title": "I. INTRODUCTION", "content": "In the rapidly evolving landscape of computing, the efficiency of processing units is paramount, especially for smalledge devices like home appliances and IoT devices. These devices require deep neural network (DNN) inference workloads that adhere to strict power and space limitations. The RISC-V architecture, known for its open-source and modular nature, is a promising candidate for these challenges.\nCentral to edge device functionality is the processing of DNNs. Recently, NPUs and TPUs have been the mainstay for such tasks in smartphone-size devices. These specialized units offer optimized performance for neural network computations but are often impractical for integration into smaller, more resource-constrained edge devices due to their size, power, and cost. Consequently, there is a growing need for efficient CPU-based solutions, especially for devices that cannot accommodate separate neural processing hardware.\nPrevious advancements in RISC-V for DNN processing include F-extension for floating-point (FP) processing and multiply-accumulate (MAC) instructions. However, this research introduces R-extension, which improves upon these earlier developments. With its unique rented-pipeline mechanism and use of architectural pipeline registers (APR), the R-extension is tailored to enhance the processing of DNNs in edge devices. The main contributions of R-extension are as follows:\n\u2022 Rented-pipeline: Rent a memory (MEM) stage on the 5- stage pipeline when fetching proposed instructions. Renting a MEM stage allows the processor to consume two stages during execution without increasing the number of stages or critical path delay.\n\u2022 APR (Architectural Pipeline Register): Located on the MEM/write-back (WB) pipeline register, accumulated results are constantly updated in the APR. By using APR, the processor does not need to use memory for read and write during MAC operations. When accumulation ends, the processor writes back APR data into the destination register and resets APR.\n\u2022 Custom instructions: This paper represents two new in- structions, rfmac.s and rfsmac.s, to support the previous two contributions. rfmac.s stands for single-precision FP MAC operation on R-extension, and rfsmac.s stands for store result of single-precision FP MAC operation working on R-extension.\n\u2022 Versatility: The proposed microarchitecture facilitates the addition of further instructions for diverse accumulation operations, such as integer MAC operation and difference accumulator [1], enhancing its alterability. The potential integration into vector architectures also paves the way for increased parallelism, offering prospects for system speed enhancements.\nThe performance metrics of the R-extension are noteworthy. Compared to the F-extension's FP multiplication approach (RV64F), our R-extension (RV64R) achieves an increment of 29% in instructions-per-cycle (IPC) and a 34% decrement in the number of memory accesses. Furthermore, compared to the baseline that solely added a MAC instruction without altering the pipeline, RV64R exhibits 15% IPC and 22% memory access improvement with a negligible increase in area.\nAdvancements in R-extension can improve edge device processing for lightweight AI tasks, achieving remarkable efficiency and adaptability. This paper will delve into the details of the R-extension and its comparative advantages."}, {"title": "II. BACKGROUND", "content": "The challenge in processing DNN inference on CPUs, particularly in lightweight devices, is managing frequent memory usage, which is essential to maintaining process capability within the power constraints of small-scale devices. In the context of RISC-V implementations for such devices, complex pipelining techniques like those used in superscalar processors, generally optimized for high performance CPUs, are unsuitable due to their high power consumption and area costs.\nMAC operations, characterized by repetitive multiply and add sequences, are distinct from complex operations requiring diversified pipelines. These diversified pipelines, which are effective for handling extended latency operations, are not optimized for the continuous accumulation required in MAC operations and also involve higher area costs and power consumption because of more pipeline stages, which are critical constraints in small devices [2]. In contrast, using rentedpipelining, the R-extension for RISC-V offers an accelerated solution for compact devices using the execution (EX) stage on multiplication and the MEM stage on accumulation. This approach is bespoken for MAC operations, which are fundamental in DNN inferences.\nIn MAC operations within assembly sequences shown at Figure 1(a-2), read-after-write (RAW) hazards occur when the processor attempts to accumulate new multiplication results with data not yet updated from a previous WB stage. Conventional data forwarding methods, which preempt RAW hazards by bypassing register write and read cycles, may not suffice for MAC scenarios where accumulated data is pending in the write-back stage and not immediately available for forwarding."}, {"title": "B. RISC-V F Standard Extension", "content": "The compiler utilizes the F-extension when computing convolution in original RISC-V architectures with FP parameters [4]. This extension handles single-precision FP convolution operations, primarily through fadd.s (FP add) and fmul.s (FP multiply) instructions shown at Figure 1(a). These instructions are integral to the computational processes underlying DNNs.\nAs shown in Figure 3, in addition to requiring two types of source registers; a destination register and source registerwhich are standard for most RISC-V instructionsthe FP operation instructions also necessitate fields to specify the format fmt and rounding mode rm. The format, encoded as a 2-bit field, indicates the precision of the FP. The rounding mode, crucial for the precision of computations, is defined in the control and status register (CSR), which guarantees adherence to the IEEE 754 standard [5] and maintains the necessary precision for the given application.\nOur study proposes the R-extension, a novel approach tailored for DNN workloads building upon the capabilities of the F-extension. The R-extension introduces custom instructions, specifically rfmac.s and rfsmac.s, designed to reduce the instruction count (IC) and memory usage during DNN operations."}, {"title": "C. Customized Instruction", "content": "Various efforts have focused on optimizing DNN processing through specialized instructions within the RISC-V architecture's evolving landscape. One such example is the development of a Winograd-based convolution acceleration instruction [6], which performs a convolution calculation between a 4\u00d74 input matrix and a 4 x 4 convolution kernel matrix and generates a 2 \u00d7 2 output matrix. While this custom instruction, conv23, enhances the efficiency of matrix convolutions for specific convolutional neural network (CNN) algorithms, it is restricted in its applicability. It does not tackle the core issue of MAC operations that are heavily utilized not only in convolution and fully connected layers of CNNs but also in other DNN architectures. This emphasizes the requirement for more adaptable solutions catering to a wide range of DNN architectures and operations.\nFurther research introduces vmac (MAC operation on RISCV V-extension [4]) and vload (vector register load) instructions, mainly through enhancements in dot product computations and data transfer efficiency [7]. By supporting single instruction, multiple data (SIMD) architecture, these extensions substantially enhance the computational performance of DNN tasks. However, vmac is only for paralleling MAC operations; it does not improve memory usage. We transformed vmac to work in an FP scalar process for an experiment, and the instruction is called fmac.s with usage as Figure 1(b), our baseline that implements a MAC module at the EX stage and is compared with the R-extension.\nThe R-extension represents not merely an optimization for scalar processor environments but also an expandable entity within the RISC-V architecture. Harmonizing with the established architectural framework offers a viable extension path toward SIMD V-extension integration. This versatility underscores the extension's potential to evolve with advancing vector processors, positioning the RISC-V architecture to meet the expanding computational demands."}, {"title": "III. METHODOLOGY", "content": "The specialized pipeline came out for MAC operation, and to support this architecture, there are two types of instructions."}, {"title": "A. Instruction Set Architecture (ISA)", "content": "To add custom instructions on RISC-V ISA to compile, we should first assign the instruction symbol, type, constituents, MASK, and MATCH. MASK is the 32-bit binary to filter out the opcode and functions(e.g., funt3, funct5, funct7, etc). MATCH is the 32-bit binary that contains actual opcodes and functions for each designated seat on instruction. After selecting the components for compilation, we need to add the required format to riscv-gnu-toolchain [8], then reconfigure and build the compiler.\nFigure 3 presents two innovative instructions within the RISC-V R-extension. The instruction rfmac.s requires two source registers, rs1 and rs2, and facilitates the multiplication of these sources, accumulating the result in the APR. This process eliminates the need for an immediate write-back to a destination register rd, as the intermediate result is stored in the APR for further accumulation. As illustrated in Figure 1(c1), the rfsmac.s instruction, employed at the end of the convolution loop where the final result is held in the APR, is tasked with writing this result from the APR to the rd during the instruction-decode (ID) stage and resetting the APR at the MEM stage, as illustrated in Figure 5.\nThe opcode for these instructions adheres to the F-extension to maintain consistency within FP operations. Unique functions mapped in Figure 4 are assigned to each instruction to ensure no overlap with existing instructions. Newly set MASK and MATCH leave only the essential parts, such as rm, rs1, rs2, and rd."}, {"title": "B. Microarchitecture", "content": "The letter R on R-extension stands for the rented-pipelining concept. At the original pipeline stages, instruction-fetch (IF), ID, EX, MEM, and WB, each stage has its exclusive role. However, fmac.s instruction of baseline in Figure 5 does not use the MEM stage and just waits for one cycle, and data instantly goes to the WB stage. It is a waste of pipeline stage and area cost on DNN operation.\nThe rented-pipeline maintains the performance and number of pipeline stages even though it optimizes accumulation operation into the RISC-V CPU. In Figure 5, an extra accumulator in the MEM stage named the rented execution stage (R_EX) is turned on instead of memory read and write when introduced instructions are fetched. Using EX and R_EX stages, two of the five pipelines are used for execution without area dissipation.\nAPR is focused on reducing memory usage by eliminating unnecessary flw (FP load) and fsw (FP store) instructions during DNN operation as compared between Figure 1(a-2), (b-2), and (c-2). In Figure 5, APR is located at the MEM/WB pipeline register. It only needs an extra 32-bit register to keep updating the partial sum from the accumulator at the R_EX stage. The input to the APR is determined by a MUX that selects between the accumulated data when the rfmac.s instruction is executed and zero for initialization when the rfsmac.s instruction is issued. Furthermore, the output of APR is interfaced with the R_EX stage to facilitate accumulation with newly multiplied data and with the ID stage, which is governed by the control bit determined by the presence of rfmac.s instruction. With these two hardware contributions, rented-pipeline and APR of R-extension, the CPU can accelerate DNN inference with low power and area overhead avoidance."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": ""}, {"title": "A. Simulation Result", "content": "To use R-extension instructions in C language, we should use inline assembly 'asm' and 'volatile' like Figure 6. After compiling with customized riscv-gnu-toolchain, the binary file containing R-extension instructions came out. Highlights on the right side of Figure 1 are the main instructions that are more repeated than others. F-extension has six instructions with four memory loads, one memory store, and two arithmetic operations. Baseline, with little advanced, has five instructions with three memory loads, one memory store, and one operation. R-extension reduces half of the memory-related instructions compared with the previous two architecture and arithmetic operation instructions. This condition leads to DNN acceleration with low area cost and power consumption described below.\nBefore using the gem5 simulator [9], we define the processor execution behavior in the gem5 instruction decoding algorithm. We make gem5 to accumulate data into the internal register APR when the opcode and function of rfmac.s are detected. In the case of rfsmac.s, the APR data is stored in the destination register and reset to zero.\nAs illustrated in Table II, we configured a system with separate L1 instruction and data caches, each with specific parameters such as size, associativity, and latency. The total cache size is set to 1 MB, operating under a 1 GHz clock domain. This setting was chosen to align with ARM Cortex-A55 specifications [10], which is usually used for home appliances SoC, such as smart TVs, representing the most demanding DNN inference scenarios. Such alignment ensures that our simulation environment realistically reflects the performance requirements of these applications. This setup utilizes a one-level cache architecture to precisely monitor the CPU's memory access patterns via the L1 cache, providing insights for understanding overall system performance.\nWe benchmark DNN inference on edge devices using three models: LeNet for simple tasks, ResNet-20 for moderate complexity, and MobileNet-V1 for advanced applications, assessing a range from basic to complex AI capabilities [11][13]. RV64F is a RISC-V 64-bit F-extension that contains FP multiplication and addition, as well as FP load and storage. The baseline architecture is RV64F with a na\u00efve MAC operation module integrated into the EX stage, and fmac.s instruction activates the module. We evaluate five major results: simulation runtime, IC, IPC, number of instructions for memory operations, and number of L1 cache accesses.\nTable III shows that the enhancement rates are expressed in percentages, highlighting the performance gains of RV64R over RV64F and baseline. Across all models, RV64R demonstrates substantial enhancements in all results compared to RV64F and baseline. This indicates that the RV64R architecture is more efficient and faster than RV64F and the baseline configuration.\nThe overall advancement rates compile the gains across all three neural network models, providing a comprehensive view of the performance boost. The RV64R architecture has advanced by about 50% over RV64F and about 32% over the baseline in terms of runtime, marking a significant increase in speed. The progress is also noteworthy for other metrics, especially regarding IPC and L1 cache accesses, which are vital for rapid and efficient processing."}, {"title": "B. Implementation Result", "content": "To explore the resource overhead of our R-extension, we use the Xilinx Vivado implementation 2023 tool to synthesize and implement the R-extension core and baseline core; F-extension with Na\u00efve MAC operation into the xcvu095-ffva2104-2-e FPGA device. The Vivado tool recommends synthesizing FP IP (FP multiplier, FP adder) into DSP modules. However, we changed the options to synthesize them into LUT for comparative clarity. The resource utilization results are shown in Table IV. Compared with the baseline, the R-extension core overhead is 1.76% less LUT and 1.63% more FF resources. This result is primarily due to the R-extension's design, which involves adding only a few multiplexers (MUXs) and altering the position of the accumulator, thus minimizing changes to the overall architecture. It is a tiny cost effect on changing the F-extension into the proposed R-extension, and it is worth considering the improvements in the upper result."}, {"title": "V. CONCLUSION", "content": "The research presented introduces advancements in RISCV architecture through R-extension (RV64R). This extension, which incorporates novel elements like the rented-pipeline and APR, enhances the efficiency of MAC operations, a critical aspect for processing in neural network models. By integrating the new instructions rfmac.s and rfsmac.s, the RV64R architecture demonstrates performance improvements over the F-extension and the baseline architecture in neural network inference. This is evident in metrics such as benchmark runtime, IPC, and L1 cache accesses, where R-extension outperforms its counterparts, indicating a more efficient and faster processing capability. Notably, reducing cache accesses also leads to power consumption savings, a critical factor in designing energy-efficient computing systems [14].\nMoreover, the research delves into the practical aspects of implementing the R-extension. Utilizing the Xilinx Vivado tool for RV64R synthesis and implementation, the study highlights the minimal resource overhead in transitioning from the baseline to the proposed R-extension. The slight increase in FF resources and the decrease in LUT usage underline the feasibility and efficiency of the R-extension. Considering the substantial performance gains in processing speed and efficiency, particularly in neural network models, and the added advantage of reduced power consumption through decreased cache access, the RV64R emerges as a valuable contribution to the RISC-V ecosystem, paving the way for efficient computing solutions in various applications."}]}