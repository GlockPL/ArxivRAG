{"title": "MITIGATING UNINTENDED MEMORIZATION WITH LORA IN FEDERATED LEARNING FOR LLMS", "authors": ["Thierry Bossy", "Julien Tu\u1ea5n T\u00fa Vignoud", "Tahseen Rabbani", "Juan R. Troncoso", "Martin Jaggi"], "abstract": "Federated learning (FL) is a popular paradigm for collaborative training which avoids direct data exposure between clients. However, data privacy issues still remain: FL-trained large language models are capable of memorizing and completing phrases and sentences contained in training data when given with their prefixes. Thus, it is possible for adversarial and honest-but-curious clients to recover training data of other participants simply through targeted prompting. In this work, we demonstrate that a popular and simple fine-tuning strategy, low-rank adaptation (LoRA), reduces memorization during FL up to a factor of 10. We study this effect by performing a medical question-answering fine-tuning task and injecting multiple replicas of out-of-distribution sensitive sequences drawn from an external clinical dataset. We observe a reduction in memorization for a wide variety of Llama 2 and 3 models, and find that LoRA can reduce memorization in centralized learning as well. Furthermore, we show that LoRA can be combined with other privacy-preserving techniques such as gradient clipping and Gaussian noising, secure aggregation, and Goldfish loss to further improve record-level privacy while maintaining performance.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have been shown to achieve state-of-the-art performance over most relevant natural language processing (NLP) tasks [Zhao et al., 2023]. There is an emerging and significant interest in fine-tuning LLMs to conduct tasks over specialized domains such as medicine [Thirunavukarasu et al., 2023, Yang et al., 2022] and finance [Wu et al., 2023a, Li et al., 2023a]. These fields handle inherently sensitive user data, necessitating additional mechanisms to prevent data exposure. A well-studied paradigm for collaboratively training a machine learning (ML) model over a cluster of clients without sharing local data is federated learning (FL) [McMahan et al., 2016, Kairouz et al., 2021].\nAlthough FL respects data sovereignty by allowing training samples to remain decentralized, most FL works do not address the memorization problem: an FL-trained LLM may still memorize client training data. Indeed, memorization is observable in most, if not all, LLMs [Carlini et al., 2019, 2022, 2021], with some work arguing that memorization is required to learn natural speech patterns [Dourish, 2004, Feldman, 2020]. While there is a wealth of research focused on preventing data reconstruction [Huang et al., 2021] and improving differential privacy [El Ouadrhiri and Abdelhadi, 2022] within the FL literature, very few have explored the propensity and prevention of FL-trained LLMs to leak training data [Thakkar et al., 2020].\nIn this work, we demonstrate an intuitive and efficient strategy for reducing memorization during LLM fine-tuning: low-rank adaptation (LoRA) [Hu et al., 2021]. In fact, we observe that LoRA fine-tuning mitigates regurgitation of synthetically-injected sensitive data in both the federated and centralized settings. This includes exact token matching [Carlini et al., 2022] and approximate reproduction [Ippolito et al., 2023]. As LoRA combines the benefits of reduced computational [Hu et al., 2021], memory [Dettmers et al., 2024], and communication overhead [Liu et al., 2024], its added benefit of preventing memorization makes it an ideal strategy for FL fine-tuning of LLMs.\nOur contributions are as follows:\n\u2022 We discover and demonstrate that LoRA mitigates memorization in federated and centralized learning. This includes exact match rate (repeating training data exactly) and paraphrasing (partial overlap). Compared to full fine-tuning, LoRA can significantly reduce memorization even when sensitive data is replicated and the LLM is prompted with long prefixes of a sequence.\n\u2022 We comprehensively test models of varying size from the Llama-2 family, Llama-3 family, and Mistral-v0.3 on medical question-answering tasks to simulate a data-sensitive scenario. LoRA effectively reduces memorization while preserving high performance accuracy.\n\u2022 We experimentally explore how LoRA interacts with other privacy strategies. This includes differential privacy mechanisms such as gradient noising and clipping, Goldfish loss [Hans et al., 2024], and post-training noise injection. We find that LoRA works synergistically with these other approaches.\n\u2022 To facilitate reproducibility and further research, we publicly release our code and instructions at https://github.com/tuneinsight/federated-llms."}, {"title": "Related Work", "content": "Exposure of sensitive data via generative models has been extensively considered in existing literature, though the choice of the privacy evaluation metric continues to evolve."}, {"title": "Privacy in LLMs", "content": "Differential privacy. Classical (\u20ac, \u03b4)-differential privacy (DP) frameworks formally measure the privacy-preserving capacity of an algorithm by analyzing whether the probability of observing an output changes by e when the underlying database excludes or includes a user record [Dwork et al., 2006]. The application of this framework to generative language tasks, in general, has proven complicated due to the rigid definition of a user record [Jayaraman and Evans, 2019]. When directly applying DP to prevent sensitive data reconstruction, it has been shown that a non-negligible compromise on privacy is required to maintain performance [Lukas et al., 2023]. The conventional technique of adding Gaussian noise onto clipped gradients [Abadi et al., 2016] to boost privacy has also been shown to affect model outputs: the randomness of the noise alone can significantly alter the outputs of two equally-private models [Kulynych et al., 2023]. One must consider the context and length of a prompt that goads an LLM into leaking sensitive information [Nissenbaum, 2004, Dourish, 2004] a condition absent from the DP perspective [Brown et al., 2022].\nMemorization. The ability of language models (large or otherwise) to regurgitate pieces of their training data is well-documented. However, the question of how best to quantify the memorization capacity of an LLM is an active area of research. A seminal work by Carlini et al. introduced \"canaries\", which are synthetic, out-of-distribution pieces of text injected into training data (such as \"My SSN is XXX-XX-XXXX\") [Carlini et al., 2019]. The approach is computationally expensive, as it requires perplexity comparisons against many thousands of random sequences, and canaries should be inserted anywhere from 1 to 10,000 times to gather a full picture of exposure, thus requiring significant fine-tuning. However, it has found use in production-level studies [Ramaswamy et al., 2020] and adjacent fields such as machine unlearning [Jagielski et al., 2022]. An alternative proposal of memorization [Carlini et al., 2022], the completion metric, adopted by our work, measures how often an LLM completes a piece of text taken from the training text when prompted on an initial portion (prefix) of it."}, {"title": "Federated Learning", "content": "Privacy in FL. Federated learning, although initially designed to protect user data [McMahan et al., 2017], did not foresee leakage in the form of regurgitation as its advent preceded the development of high-performing generative language models [Kairouz et al., 2021]. Consequently, studies on the memorization capacity of FL-trained LLMs remain limited. An early survey demonstrated that federated averaging [Thakkar et al., 2020] ameliorates unintended memorization, though only for a tiny 1.3M parameter next-word predictor [Hard et al., 2018]. However, the authors' observations on the success of non-independent and identically distributed (non-IID) clustering for improved privacy informed our federated training strategy. The addition of the DP Gaussian mechanism was shown to improve canary-based memorization for a production FL setting [Ramaswamy et al., 2020]. Similar to us, Liu et al. [2024] leverage LORA to conduct efficient fine-tuning. However, this work is exclusively interested in studying performance under varying budgets within the (\u20ac, \u03b4)-DP framework and does not consider memorization under the canary or completion-based framework.\nMedical applications. Our emphasis on medical datasets is relevant: LLMs have been shown to regurgitate sensitive medical data in Lehman et al. [2021], though their work relies on an older BERT model. Mireshghallah et al. [2022] study the success of membership inference attacks on i2b2, though they also do not use any memorization metrics. Although federated learning has been studied and championed as an ideal paradigm for clinical settings [Xu et al., 2021, Nguyen et al., 2022, Antunes et al., 2022], there is a relative lack of literature in the context of clinical memorization."}, {"title": "Preliminaries", "content": "LORA. To reduce computational and memory requirements when fine-tuning LLMs, Low-Rank Adaptation (LoRA) [Hu et al., 2021] was introduced to drastically reduce the number of trainable parameters while fine-tuning. This is achieved by representing the weight updates AW as the product AW = BA of two low-rank matrices A and B. LORA enables efficient adaptation of LLMs to specific tasks while preserving the generalization capabilities of the underlying model, as gradients often exhibit a low intrinsic dimension [Li et al., 2018, Aghajanyan et al., 2020]. Additionally, LORA offers a notable advantage in an FL scenario by drastically reducing the amount of data exchanged between participants during each round. In our experiments, we achieved a reduction by a factor of 130.\nFederated Learning. Federated learning (FL) has been widely-studied for deep learning models in cross-silo settings Huang et al. [2022], where a limited number of resource-rich clients, such as organizations or institutions, collaboratively train ML models without sharing their data. In conventional FL, the global objective function of N clients is defined as\nmin F(W) =  \u2211pk fk(W),\nW\nk=1\nwhere W represents parameters of a model, \u2211pk = 1 and fk (W) is the local objective function of client k. Local training data Dk between clients often heterogeneous. A common strategy for solving Equation 1 is Federated Averaging (FedAvg) [McMahan et al., 2016]. In FedAvg, clients conduct a round t of training and Wt+1 (parameters after round t) is updated as the pk-weighted average of the respective k gradients. These gradient weights pk can be set as pk =  to mitigate data size bias, which we use in this work. FL has been recently applied to LLMs Ye et al. [2024], Thakkar et al. [2020], Liu et al. [2024], Ramaswamy et al. [2020] leveraging FedAvg to aggregate locally-trained model updates. In this work, we conduct experiments using LoRA-based fine-tuning and full model fine-tuning for local iterations in FL. Besides reducing communication costs, clients benefit computationally from using LORA during local training.\nMemorization Definition. Following previous work [Ippolito et al., 2023, Huang et al., 2024, Hans et al., 2024], we adopt the \"extractable memorization\" definition of Carlini et al. [2023]. Consider a string representable as a concatenation [p||s] where p is a prefix of length k and s is the remainder of the string. We define the string s to be memorized with k tokens of context by a language model f if [p||s] is contained in the training data of f, and f produces s when prompted with p using greedy decoding. In other words, we consider a string from training data memorized if an LLM can generate it when prompted by a prefix."}, {"title": "Empirical Evaluation", "content": "In this section, we study how LoRA affects memorization of out-of-distribution sequences injected into fine-tuning training data. We introduce the experimental setting in Section 4.1 and explain how we quantify memorization in Section 4.2.\nWe consider conventional centralized learning in Section 4.3, where all training samples are trained on by a single client. We then consider an FL setting in Section 4.4, where training data is split among several clients. Our FL experiments are designed to mimic a medical setting where training data contains sensitive information at an unknown rate, which is a common scenario as few if not any data anonymization tools can guarantee a complete removal of sensitive data [Langarizadeh et al., 2018]. In fact, Heider et al. [2020] measured the accuracy of three off-the-shelf de-identification tools on the i2b2 medical record dataset [Stubbs and \u00d6zlem Uzuner, 2015], which our experiments also use, and found that no system could perform a full removal."}, {"title": "Experimental setup", "content": "All fine-tuning was performed on a single NVIDIA A100 80GB GPU within an HPC cluster. We leveraged Hugging-Face's Transformers library [Wolf et al., 2020] to access and fine-tune pre-trained models. The experiments were conducted in a Python 3.11.9 environment, with PyTorch 2.4.0 and CUDA 12.1. Further training details are included in Appendix B.1.\nWe fine-tune models for domain adaptation to medical question-answering (QA). Despite medical scenarios being extensively promoted by FL applications [Xu et al., 2021, Nguyen et al., 2022, Antunes et al., 2022], and the availability of resources such as de-anonymized sensitive medical datasets [Johnson et al., 2016, Stubbs and \u00d6zlem Uzuner, 2015], clinical memorization remains an area of uncertainty in FL.\nFine-tuning Datasets. In order to reproduce a plausible FL environment with non-IID data, we select 3 popular medical datasets with different types of QA.\n1. MedMCQA [Pal et al., 2022] is composed of multiple-choice questions, containing almost 190k entrance exam questions (AIIMS & NEET PG). We fine-tune on the training split and leave aside validation data as a downstream evaluation benchmark.\n2. PubMedQA [Jin et al., 2019] consists of Yes/No/Maybe questions created from PubMed abstracts. The dataset contains 1k expert-annotated (PQA-L) and 211k artificially generated QA instances (PQA-A). We include 500 questions from the train and validation sets of PQA-L and 50k questions of PQA-A.\n3. Medical Meadow flashcards [Han et al., 2023] contains 39k questions created from Anki Medical Curriculum flashcards compiled by medical students. We include 10k instances for fine-tuning data.\nMedical Benchmarks. To measure the downstream performance of the fine-tuned models, we evaluate models on 4 medical benchmarks following existing methodology [Wu et al., 2023b, Singhal et al., 2023a,b, Chen et al., 2023]: MedQA, PubMedQA, MedMCQA, and MMLU-Medical.\n1. MedQA's 4-option questions. MedQA [Jin et al., 2020] consists of US Medical License Exam (USMLE) multiple-choice questions. The test set contains 1278 questions with both 4 and 5-option questions. Following Chen et al. [2023], we report each case separately, respectively MedQA-4 and MedQA.\n2. MedQA's 5-option questions.\n3. PubMedQA's test set contains 500 expert-annotated questions. No artificially-generated questions are used during evaluation.\n4. MedMCQA's test set does not provide answer labels, therefore we rely on the validation set, containing 4183 instances, to benchmark downstream performance following Wu et al. [2023b] and Chen et al. [2023].\n5. MMLU-Medical. MMLU [Hendrycks et al., 2021] is a collection of 4-option multiple-choice exam questions covering 57 subjects. We follow Chen et al. [2023] and select a subset of 9 subjects that are most relevant to medical and clinical knowledge: high school biology, college biology, college medicine, professional medicine, medical genetics, virology, clinical knowledge, nutrition, and anatomy, and group them into one medical-related benchmark: MMLU-Medical.\nWe use 3-shot in-context learning without any chain-of-thought reasoning and average the accuracy over 3 seeds.\nModels. To account for the effect of model size on memorization [Carlini et al., 2023, Tirumala et al., 2022], we study pre-trained models ranging from 1B to 8B parameters: Llama 3.2 1B, Llama 3.2 3B, Llama 3 8B [Dubey et al., 2024], Llama 2 7B [Touvron et al., 2023], and Mistral 7B v0.3 [Jiang et al., 2023]."}, {"title": "Quantifying memorization", "content": "How we measure memorization is largely inspired by Carlini et al. [2023]. In short, we inject sensitive sequences, so-called \"canaries\" [Carlini et al., 2019, Jagielski et al., 2023, Thakkar et al., 2020], into fine-tuning data and then measure the models' ability to regurgitate this information when prompted with the beginning of these sequences. In Appendix C.2, we give an example of memorization scores for Llama 2 7B.\nCanaries. Unlike prior works that evaluate memorization of all training data [Carlini et al., 2023, Ippolito et al., 2023, Hans et al., 2024], we are interested in measuring how much sensitive information is memorized. Similar to Lehman et al. [2021] and Mireshghallah et al. [2022], we inject medical records into our training set originating from the 2014 i2b2/UTHealth corpus dataset [Stubbs and \u00d6zlem Uzuner, 2015]. The i2b2 dataset contains 1304 longitudinal medical records that describe 296 patients.\nSince data duplication has been shown to greatly influence memorization [Carlini et al., 2023, Lee et al., 2022, Kandpal et al., 2022], we randomly select 30% of the medical records and duplicate them 10 times within our fine-tuning data in order to study data duplication in our experiments.\nPrompting. To measure unintended memorization after fine-tuning, we randomly select test sequences from the medical records (one sequence per record) and split each sequence into a prefix p and a suffix s. Conditioned on the prefix, the model generates text via greedy decoding and the generated suffix is compared with the ground truth. We set the length of the generated suffix s to 50 tokens, in line with Carlini et al. [2023], Ippolito et al. [2023] and Hans et al. [2024].\nFollowing Carlini et al. [2023], we measure the effect of the context size by prompting the model on each test sequence several times with prompts of lengths in {10, 50, 100, 200, 500}. The different prompts for one test sequence are constructed such that the suffix s is kept identical while varying the prompt length. This ensures a fair comparison between prompt lengths, since different suffixes may be more or less difficult to regurgitate.\nMemorization scores. To compare generated text with the ground truth, we rely on two metrics: (1) the exact token match rate and (2) the BLEU score to measure approximate reproduction, as prior works suggest that the exact match rate does not capture subtler forms of memorization [Ippolito et al., 2023]. In line with this work, we consider a sequence memorized if the generated suffix and the ground truth yields a BLEU score > 0.75. For both metrics, lower is better and a score of 1 denotes the complete memorization of all test sequences. In Appendix C.2, we provide an example for Llama 2 7B fine-tuning."}, {"title": "Centralized Learning", "content": "To the best of our knowledge, the impact of LoRA on memorization has not been previously quantified; therefore, we begin by studying LoRA in the context of centralized learning (CL) before considering federated learning (FL).\nTraining details. In the centralized learning setting, we merge PubMedQA, MedMCQA and Medical Meadow Flashcards into one fine-tuning dataset in which we inject the i2b2 medical records to benchmark memorization after fine-tuning. We use a validation split of 10% and for each model we search for the learning rate yielding the lowest validation loss. More details on hyperparameters can be found in Appendix B.1.\nAccuracy. To study how LoRA mitigates unintended memorization, we must first assess if it comes at a cost in model performance. Figure 1 illustrates the average accuracy over fine-tuning strategies. Comparing full fine-tuning against LORA, we find that LoRA comes with a relatively negligible cost in accuracy. Every fine-tuning yields a significant accuracy improvement of the pre-trained model except for Llama 3.1 8B, in which performance minimally improved.\nWe hypothesize that part or all of our fine-tuning dataset has already been trained on during Llama 3.1 8B's pre-training phase. Accordingly, we exclude Llama 3.1 8B from subsequent experiments."}, {"title": "Utility-privacy tradeoff", "content": "To further confirm that the privacy gains observed on models trained with LoRA do not come at the cost of utility, and that the privacy loss observed with full fine-tuning is not due to overfitting or preventable by early stopping, we analyzed the utility-privacy tradeoff throughout the fine-tuning process. Figure 3 illustrates the evolution of privacy and utility for Llama 3.2 3B during both LoRA and full fine-tuning. The figure shows that LoRA fine-tuning consistently follows a more privacy-preserving trend, with lower memorization scores compared to full fine-tuning at similar utility levels. Furthermore, after a certain number of fine-tuning steps, the model's tendency to memorize data increases without significant improvements in utility, due to overfitting. This highlights that early stopping during LLM training not only improves efficiency, but also helps privacy by reducing the risk of memorization."}, {"title": "Federated Learning", "content": "Having empirically measured how LoRA reduces unintended memorization in centralized learning, we now turn to federated learning. The federated learning framework contains multiple key differences with centralized learning that may impact memorization, such as Federated Averaging or non-IID data across participants [Thakkar et al., 2020].\nTraining details. We define a heterogeneous setting with one client per dataset. In other words, we fine-tune models with 3 participants, where each participant trains locally on one of the 3 datasets MedMCQA, PubMedQA, and Medical Meadow flashcards. We split and inject i2b2 medical records into each dataset proportionally to their size. Participants fine-tune over their local dataset for one epoch between each global weight update, for a total of 5 rounds. For every model, we fine-tune the learning rate on each local dataset. More training details are included in Appendix B."}, {"title": "Secure Aggregations", "content": "FL's privacy benefits can be compromised if participants gain access to each other's fine-tuned local models. While Figure 8 highlights reduced memorization after model aggregation, unsecured local models may still expose additional information regarding participants' datasets. In Appendix D, we show how secure aggregation addresses this vulnerability by using a third party to aggregate encrypted local contributions using Fully Homomorphic Encryption (FHE) and decrypting the aggregated model collectively through Secure Multiparty Computation (SMPC), as described in S\u00e9bert et al. [2022]. Experiments were conducted using the open-source Lattigo library [Lattigo v6, Mouchet et al., 2020]."}, {"title": "Combining LoRA with other methods", "content": "Although LoRA mitigates unintended memorization on its own, we investigate whether it can be combined with other privacy-persevering techniques without compromising performance or increasing memorization. If users are focused on reducing extractable memorization in pre-training, then they may be interested in Goldfish loss (LoRA is preferred for fine-tuning), but we investigate and verify its potential for fine-tuning. Gradient noising and clipping can be used to satisfy (\u20ac, \u03b4)-differential-privacy guarantees, which LoRA alone has not been formally proven to provide.\nNonetheless, we emphasize that Goldfish loss and DP noising/clipping are not efficient strategies, as both require calculation of the full gradient. Hence, users will choose LoRA if they are concerned about backpropagation costs or communication overhead, which is a common scenario in FL."}, {"title": "Goldfish loss", "content": "The Goldfish loss [Hans et al., 2024] has been introduced recently as a memorization mitigating technique for pre-training language models via a new next-token training objective. The training procedure randomly excludes tokens from the loss computation in order to prevent verbatim reproduction of training sequences. In Appendix E, we evaluate the memorization and accuracy of Llama 3.2 3B fine-tuned with LoRA in combination with Goldfish loss. We also compare it to the same model fully fine-tuned with Goldfish loss only. The combination of LoRA with Goldfish loss synergistically achieves lower memorization beyond what either strategy achieves alone."}, {"title": "Differential privacy", "content": "(\u20ac, \u03b4)-Differential privacy (DP) provides formal guarantees that an individual's data cannot be inferred from a model's output, by quantifying the model's sensitivity to changes in input data. Following Li et al. [2021] and Liu et al. [2024], we define sensitivity as the maximum change in model output resulting from the inclusion or removal of a single data point in the training dataset (record-level DP).\nImplementing DP requires modifications to the fine-tuning pipeline to limit the influence of individual data points on model parameters. Gradient clipping, which constrains the magnitude of gradient updates, is a key technique in this process. In our experiments (see Appendix G.1), applying a gradient clipping value of 0.0001 significantly reduces memorization and improves accuracy compared to the default value of 1.0. This demonstrates gradient clipping as a privacy-enhancing method in itself, even without the addition of noise. But the use of stochastic gradient descent (SGD), required for DP-SGD, presents challenges in fine-tuning the Llama 3.2 3B model. Despite an extensive search for optimal learning rates, SGD consistently underperforms compared to Adam-derived optimizers (see Appendix G.2)."}, {"title": "Discussion", "content": "Our experimental evaluation demonstrates that LoRA reduces memorization in both centralized and FL settings, which naturally raises the question: why does this happen? We argue that the mechanisms by which FedAvg and LoRA mitigate memorization should be considered independently. Carlini et al. [2022] empirically establish a log-linear relationship between canary duplication and memorization, thus we frame our discussion of memorization in the context of overfitting. How and why in-distribution, non-duplicated sequences can still be regurgitated [Carlini et al., 2019] is a question that we leave to future work.\nFederated learning. While it is known that FedAvg can reduce memorization for simpler LSTM-based next-word predictors (NWPs) [Ramaswamy et al., 2020, Thakkar et al., 2020], we hope that our verification of this phenomenon for LLMs on longer canaries can encourage formal investigation. Nevertheless, we note the following: in the IID FedAvg setting with identical hyperparameter settings (same number of local updates, learning rate, and initialization) the expected value of the d-sample stochastic gradient over N clients, \u03a3=1 fk (0,Xi ~ Dk) in Equation 1 can resemble a single stochastic gradient in a centralized setting taken over a single large batch of size Nk since fk and Dk are homogeneous. Thus, Thakkar et al. [2020] observe more memorization in IID settings with larger batch sizes. The non-IID setting is significantly more complex: the optimization problem and associated loss landscape of Equation 1 differs from the centralized problem. We observe in Figures 5 and 6 that non-IID FL significantly reduces memorization, which Thakkar et al. [2020] also observe for their NWPs. While they do not fine-tune their learning rates to eliminate this as a confounding variable, we do, thus suggesting that FedAvg itself is a memorization-reducing mechanism.\nLoRA. It is possible that LoRA reduces benign overfitting [Bartlett et al., 2020], which occurs when training data is overfitted without affecting performance. Notably, Tang et al. [2023a] prove that benign overfitting can preserve out-of-distribution generalization for overparameterized linear models if there is a strong correlation between the dominant eigenvectors/components of the source and target distributions. It is possible then that our LLMs are displaying this phenomenon: in both the centralized and FL settings, our fine-tuning datasets, while heterogeneous, contain aligned components due to their shared domain. LoRA may reduce benign overfitting by ignoring minor components, which only explain a minimal (and possibly noisy) portion of the data covariance.\nSpecific to FL, an alternative hypothesis is that the low-rank approximation of AW resembles a d-compression operator [Karimireddy et al., 2019], i.e., ||LORA(AW) \u2013 \u2206W||\u00b2 \u2264 (1 \u2013 \u03b4)||\u2206W||2, and that low-d compressors reduce memorization. Low-bias compressors, such as certain randomized projections [Dorfman et al., 2023, Rabbani et al., 2021, Ivkin et al., 2019] and other low-rank approximations [Makkuva et al., 2023] have been shown to preserve model performance in non-IID distributed settings. While the effects of these other operators on memorization has not been extensively studied, the efficacy of gradient clipping in lowering memorization while maintaining accuracy (Table 8) lends further credence to this hypothesis. Clipping is a low-bias compressor for heavy-tailed gradients, which is observed for general SGD [Mireshghallah et al., 2022] and LLM fine-tuning [Kenton and Toutanova, 2019]. Further exploration of 8-compressors is warranted."}, {"title": "Conclusion and Limitations", "content": "In this work, we demonstrate that LoRA is capable of reducing memorization of fine-tuning training data. In particular, this effect is observable in both centralized learning and federated learning (FL), and we find this effect is especially pronounced in the latter. Moreover, it is possible to further reduce memorization by combining LoRA with other strategies such as Goldfish loss or conventional privacy-preserving mechanisms such as Gaussian noising and gradient clipping. FL was previously shown to reduce memorization for simple LSTM-based next-word predictors [Hard et al., 2018, Thakkar et al., 2020] and we demonstrate that generative LLMs inherit this benefit as well. However, further theoretical analysis of this phenomenon, which may relate to the LoRA reductive effect, is needed.\nWe note that LoRA is only suitable for fine-tuning, while other techniques are required for the pre-training phase. The impact of LoRA on memorization during pre-training remains an open question for future work. Additionally, further research is needed to determine whether LoRA mitigates data regurgitation under alternative definitions of memorization [Schwarzschild et al., 2024]."}, {"title": "Impact statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning, especially enhancing privacy. Among the many potential societal consequences of our work, we specifically acknowledge that techniques mitigating unintended memorization can incidentally facilitate the concealment of unlawful use of copyrighted data by preventing its regurgitation post-training. However, we believe that the benefit of enhanced safeguards for confidential data protection combined with the current advances of other methods such as watermarking [Li et al., 2023b, Tang et al., 2023b, Cui et al., 2024] can effectively mitigate this risk and provide stronger overall data protection."}, {"title": "Further Related Work", "content": "Membership inference attacks (MIA) rely on rigorous statistical principles to assess privacy risks in machine learning models. [Shokri et al., 2017] introduced an approach for determining whether a specific data point was part of a model's training dataset. These attacks exploit differences in model behavior on training versus non-training data, posing significant privacy concerns for sensitive information. Building on this, [Hongyan et al., 2024] extended these concepts to LLMs by incorporating contextual information. This study demonstrated that LLMs are particularly vulnerable to membership inference attacks, as they often retain verbatim information from their training datasets. The work highlighted the increased privacy risks associated with LLMs due to their scale and training dynamics.\nSecure Aggregations. While the conventional FL ensures that raw data is not shared between participants during collective training, it does not address the risk of data leakage through model updates shared prior to aggregation. For example, in the honest-but-curious scenario, a server examines whether client data can be reconstructed [Huang et al., 2021]. This vulnerability becomes particularly critical with LLMs, given their propensity for memorization. To address the privacy risks associated with local model exchanges in FL, [Truex et al., 2019] proposes a hybrid approach that combines differential privacy with secure multiparty computation (SMC). In this framework, local models are encrypted and remain hidden from other participants prior to aggregation, thereby mitigating privacy leakage risks associated with individual local models by focusing them on the aggregated model during each aggregation round. While this method has been explored for general machine learning applications, to the best of our knowledge, it has not yet been investigated in the context of large language models (LLMs)."}, {"title": "Training details", "content": "Hyperparameters\nIn centralized learning, we sweep the learning rate \u2208 {1e \u2013 5, 5e \u2013 5, 1e \u2013 4, 5e \u2013 4} for full fine-tuning experiments. For LoRA experiments, we search for learning rate values \u2208 {5e \u2013 5, 1e \u2013 4, 5e \u2013 4, 1e \u2013 3}. In federated learning experiments, we sweep the learning rate on each dataset individually for one epoch, with the same set of values as in centralized learning.\nFor all experiments we fine-tune models with the AdamW optimizer [Loshchilov and Hutter, 2019"}]}