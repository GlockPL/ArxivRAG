{"title": "Mitigating Sensitive Information Leakage in LLMs4Code through\nMachine Unlearning", "authors": ["Ruotong Geng", "Mingyang Geng", "Shangwen Wang", "Haotian Wang", "Zhipeng Lin", "Dezun Dong"], "abstract": "Large Language Models for Code\n(LLMs4Code) excel at code generation\ntasks, yielding promise to release developers\nfrom huge software development burdens.\nNonetheless, these models have been shown to\nsuffer from the significant privacy risks due to\nthe potential leakage of sensitive information\nembedded during training, known as the\nmemorization problem. Addressing this issue is\ncrucial for ensuring privacy compliance and up-\nholding user trust, but till now there is a dearth\nof dedicated studies in the literature that focus\non this specific direction. Recently, machine\nunlearning has emerged as a promising solu-\ntion by enabling models to \"forget\" sensitive\ninformation without full retraining, offering\nan efficient and scalable approach compared\nto traditional data cleaning methods. In this\npaper, we empirically evaluate the effectiveness\nof unlearning techniques for addressing privacy\nconcerns in LLMs4Code. Specifically, we\ninvestigate three state-of-the-art unlearning\nalgorithms and three well-known open-sourced\nLLMs4Code, on a benchmark that takes\ninto consideration both the privacy data to\nbe forgotten as well as the code generation\ncapabilites of these models. Results show that\nit is feasible to mitigate the privacy concerns of\nLLMs4Code through machine unlearning while\nmaintain their code generation capabilities at\nthe same time. We also dissect the forms of\nprivacy protection/leakage after unlearning\nand observe that there is a shift from direct\nleakage to indirect leakage, which underscores\nthe need for future studies addressing this risk.", "sections": [{"title": "Introduction", "content": "In recent, the remarkable success of Large language\nmodels (LLMs) (Brown, 2020) in diverse natural lan-\nguage processing tasks has been tailored for the realm\nof software engineering, yielding several language mod-\nels tailored specifically for code, termed as Large Lan-\nguage Models for Code (LLMs4Code), e.g., CodeLlama\nand Stable Code (Xu et al., 2022). With extensive\npre-training process on programming language datasets,\nsuch models have demonstrated proficiency with promi-\nnent performance on code-related tasks (Geng et al.,\n2024; Deng et al., 2024; Qin et al., 2024). For in-\nstance, Geng et al. (2024) has demonstrated the capa-\nbility of LLMs4Code in producing code summarizations\nthat are not only of superior quality but also cater to\nthe varied requirements of human programmers through\nin-context learning(Geng et al., 2024).\nAs a double-edged sword of LLMs4Code, the risk of\nsensitive information including Personally Identifiable\nInformation (PII), private data, or confidential secrets\nhas already been highlighted by recent studies (Yang\net al., 2024b; Jahanshahi and Mockus, 2025). From\nthe perspective of model attacking (Yao et al., 2024;\nDong et al., 2024), empirical evidence reveal that specific\nprompts can result in the leakage of corresponding sensi-\ntive information (Huang et al., 2024; Carlini et al., 2021).\nMore formally, such risk of privacy disclosures during the\nutilization of LLMs4Code is commonly termed as the\nmemorization problem (Al-Kaswan and Izadi, 2023;\nLukas et al., 2023). Since the memorization problem\ncommonly exists in a wide range of code-relevant tasks,\ne.g., code generation (Svyatkovskiy et al., 2020; Wang\net al., 2023a), and yield inevitable risk of developers in\ntheir daily development activities, we argue that:\nIn the era of LLMs4Code, it is of significant im-\nportance to effectively address the potential leakage\nof sensitive data for upholding robust user privacy\nmeasures and sustaining trust in the deployment.\nHowever, to the best of our knowledge, concurrent\nwork rarely provides solutions to such an important\nbut challenging problem, offering only empirical findings\non the memorization problem (Leybzon and Kervadec,\n2024; Kiyomaru et al., 2024). We also note that one po-\ntential approach involves incorporating a dedicated data\ncleaning phase when pre-processing the training data,\nyielding the inflexibility and unscalability due to the ne-\ncessitate of substantial engineering endeavors to devise\nappropriate rules and heuristics.\nFortunately, Machine Unlearning (MU) has\nemerged as a technique striving to assist the target model\nin \"forgetting\" the data points from the initial training\nset, offering lightweight but effective approach to aid in\nprotecting sensitive information from LLMs (Liu et al.,\n2024; Nguyen et al., 2022). To be specific, MU proposes\nto resemble an \"auxiliary\" sanitized dataset (devoid of\nsensitive information), thereby potentially saving con-\nsiderable development costs compared to retraining the\nmodel from scratch. Following such an intuition, a num-\nber of studies have verified the promising effectiveness\nof MU on making LLMs forget specific contents that\nthey met during training (Chen and Yang, 2023; Chun-\ndawat et al., 2023). Nonetheless, we also note that the\ncurrent literature lacks a comprehensive understanding\nregarding the strengths and weaknesses of existing MU\ntechniques within the context of LLMs4Code, including\ntheir effectiveness on mitigating the privacy leakage dur-\ning the code generation process, and how the state-of-\nthe-art MU techniques will perform on LLMs4Code.\nTo bridge this gap, this paper contributes an extensive\nempirical study on MU-inspired protection of the sensi-\ntive data leakage from LLMs4Code, yielding the correct-\nness of their generated code at the same time. With the\naid of the state-of-the-art GPT-4 and well-established\ncode generation dataset, we first build a benchmark in-\ncluding: (a) a forget set including 5K pieces of privacy-\nrelated personal data to evaluate the effectiveness of un-\nlearning, and (b) a retain set including 5K pieces of\ncode generation data to evaluate the basic capability\nof LLMs4Code. Subsequently, we evaluate three state-\nof-the-art and easy-to-deploy MU techniques on three\nwidely-used LLMs4Code, i.e., AIXCoder, CodeLlama,\nand CodeQwen, respectively. Aside from investigating\nthe effectiveness of these MU techniques, we also dissect\nthe privacy protection and leakage forms after the un-\nlearning process, regarding seeking potential challenges\nthat should be addressed in the future. Overall, we sum-\nmarize our contributions as follows:\n1. MU is a promising way to simultaneously mitigate\nthe privacy concerns of LLMs4Code while maintain\ntheir code generation capabilities at the same time.\nSpecifically, unlearning can decrease the leak rate\nof AIXCoder by more than 50% while only bring a\nnegalegable side effect to code generation.\n2. After unlearning, LLMs4Code learn to adopt di-\nverse forms to prevent the leakage of sensitivities,\nin which the most popular one is to replace the sen-\nsitive fields with variable names and abbreviations.\n3. After unlearning, LLMs4Code become more likely\nto leak the privacy in an indirect manner, which\nmeans they tend to leak the information that is not\nexplicitly queried. This suggests that future works\nshould also take into consideration the indirect pri-\nvacy leakage for a more robust unlearning process."}, {"title": "Background", "content": "LLMs and LLMs4Code are significant innovations in the\nfields of natural language processing and programming.\nLLMs, exemplified by models like ChatGPT, are trained\non massive text datasets to comprehend and generate\nhuman language with remarkable accuracy and fluency.\nThese models have demonstrated capabilities in a wide\narray of language-related tasks, from translation and\nsummarization to dialogue generation and content cre-\nation (Ugare et al., 2024; Feng et al., 2024; Yang et\nal., 2024a). On the other hand, LLMs4Code, such as\nOpenAI's Codex and GitHub's Copilot, are specialized\nvariants tailored for programming tasks. By integrating\nknowledge of both natural language and programming\nlanguages, LLMs4Code excel in assisting developers by\nproviding code suggestions (Dong et al., 2023; Ahmed\net al., 2024), auto-completions(Li et al., 2024), and even\nentire code segments (Zhang et al., 2023; Wang et al.,\n2023b; Dong et al.), elevating efficiency and creativity in\nsoftware development processes."}, {"title": "LLMs & LLMs4Code", "content": "LLMs and LLMs4Code are significant innovations in the\nfields of natural language processing and programming.\nLLMs, exemplified by models like ChatGPT, are trained\non massive text datasets to comprehend and generate\nhuman language with remarkable accuracy and fluency.\nThese models have demonstrated capabilities in a wide\narray of language-related tasks, from translation and\nsummarization to dialogue generation and content cre-\nation (Ugare et al., 2024; Feng et al., 2024; Yang et\nal., 2024a). On the other hand, LLMs4Code, such as\nOpenAI's Codex and GitHub's Copilot, are specialized\nvariants tailored for programming tasks. By integrating\nknowledge of both natural language and programming\nlanguages, LLMs4Code excel in assisting developers by\nproviding code suggestions (Dong et al., 2023; Ahmed\net al., 2024), auto-completions(Li et al., 2024), and even\nentire code segments (Zhang et al., 2023; Wang et al.,\n2023b; Dong et al.), elevating efficiency and creativity in\nsoftware development processes."}, {"title": "Memorization Problem", "content": "The memorization problem inherent in LLMs raises sig-\nnificant privacy concerns, particularly regarding the in-\nadvertent retention of sensitive information from\nthe training data. This issue stems from the models' ability\nto extensively memorize and replicate specific phrases,\ntext excerpts, or even entire documents encountered dur-\ning training. As a consequence, LLMs may inadvertently\nstore personal data, confidential details, or proprietary\ninformation within their parameters, posing a substan-\ntial risk of privacy breaches.\nNumerous studies have presented the evidence of the\nsusceptibility of LLMs to retaining training data. A typ-\nical way to observe such a phenomenon is to perform ex-\ntraction attacks. For instance, Carlini et al. (Carlini et\nal., 2021) illustratively extracted hundreds of verbatim\ntext sequences, including sensitive personal identifiers,\nfrom GPT-2's training corpus. The modus operandi of\ntheir attack entailed crafting a series of prompts and sub-\nsequently assessing whether the sequences generated in\nresponse to these prompts were present within the train-\ning dataset. In another study, Liang Niu's work (Niu et\nal., 2023) further validated the efficacy of such an as-\nsault strategy, introducing the concept of perplexity as\na metric to gauge the model's degree of surprise regard-\ning the generated sequences. In this context, a lower\nperplexity value indicates a higher likelihood that the\nsequence has been encountered during training, thereby\nsuggesting familiarity on the part of the model. For-\nmally, perplexity is quantified as follows:\n$\\perp = \\exp \\{ \\frac{1}{n} \\sum_{i=1}^n \\log f(x_i | x_1, ..., x_{i-1}, \\Theta) \\}$ (1)\nwhere $\\log f(x_i | x_1, ..., x_{i-1}, \\Theta)$ indicates the log\nlikelihood of the token $x_i$ given all previous tokens\n$x_1, ..., x_{i-1}$. Besides, a straightforward heuristic based\non knowledge distillation can completely expunge the\ninformation that needs to be forgotten, while preserving\nan accuracy exceeding 90% on the retained data (Chun-\ndawat et al., 2023).\nThe aforementioned studies collectively highlight that\naddressing the memorization problem of LLMs is cru-\ncial for safeguarding user privacy. Very recently, a latest\nstudy empirically demonstrates that memorization prob-\nlem also occurs in LLMs4Code, showcasing that hard-\ncoded credentials in code can be easily leaked during the\ncode completion process (Huang et al., 2024). Therefore,\nour study aims to understand how well the problem can\nbe mitigated, which builds the foundation for mitigat-\ning the potential risks associated with the retention of\nsensitive information stored in LLMs4Code."}, {"title": "Machine Unlearning", "content": "Relevant to the issue of privacy leaks in LLMS (Huang et\nal., 2022), the concept of unlearning involves recalibrat-\ning a model after training to erase certain contents from\nits captured knowledge (Jagielski et al., 2020; Jang et al.,\n2022), thereby avoiding the costly process of retraining\nLLMs. Given the ability of large language models to re-\ncall specific details from their training sets, the targeted\ndeletion of such privacy-sensitive data is of significant\nvalue.\nPioneer efforts by Chen & Yang (2023) (Chen and\nYang, 2023) and Eldan & Russinovich (2023) (Eldan\nand Russinovich, 2023) have provided model designers\nwith post-training modifications that can protect pri-\nvacy at relatively low computational costs. Neverthe-\nless, unlearning algorithms are still at an early stage\nof development, with different methods showing vary-\ning degrees of effectiveness and no definitive benchmark\nfor evaluating their performance. To adequately assess\nthe effectiveness of unlearning, a comprehensive evalua-\ntion framework is essential. This framework should in-\nclude metrics that not only quantify the reduction of\ninformation related to the target data, but also evaluate\nthe functional behaviour of the model after unlearning,\nensuring that the overall performance of the model re-\nmains intact, while effectively omitting sensitive infor-\nmation. Our study aims at building such a benchmark\nfor machine unlearning on LLMs4Code. Through rig-\norous evaluations on such a benchmark, we can move\ntowards a more comprehensive understanding of the ca-\npabilities of machine unlearning and its significance in\nenhancing privacy-preserving practices within the appli-\ncation of LLMs4Code."}, {"title": "Experiment Settings", "content": "Figure 1 demonstrates the workflow of this study. Sup-\npose that LLMs4Code can initially work well on code\ngeneration (given the general query) but leak privacy in-\nformation when given a specific code completion prompt\n(i.e., the privacy query). Our work aims at investigat-\ning that after applying several machine unlearning tech-\nniques to the models, whether they would (1) avoid sen-\nsitive information leakage give nthe privacy query, and\n(2) preserve the capability for code generation given the\ngeneral query."}, {"title": "Research Questions", "content": "This study aims to explore the following research ques-\ntions:\n\u2022 RQ1: Impact of Unlearning Techniques This\nquestion evaluates the effectiveness of different un-\nlearning techniques in reducing sensitive informa-\ntion leakage while preserving the models' functional\ncorrectness in code generation.\n\u2022 RQ2: Privacy Protection Forms after Un-\nlearning This question investigates the various\nstrategies employed by LLMs4Code to mitigate pri-\nvacy risks after unlearning, identifying the fre-\nquency of different privacy-preserving behaviors\n(e.g., placeholders, skipping fields).\n\u2022 RQ3: Privacy Leakage Forms after Unlearn-\ning This question investigates how the sensitive in-\nformation would be still leaked after unlearning. Es-\npecially, we focus on analyzing if the leakage is in-\ntentional or unintentional."}, {"title": "Dataset", "content": "In this study, the dataset is composed of two key com-\nponents: the forget set and the retain set. The former is\nused to investigate privacy concerns related to the han-\ndling of personal information, while the latter is used to\nevaluate the general code generation capabilities of the\nLLMs4Code.\nAs for the forget set, we followed the previous study\n(Maini et al., 2024) and created a synthetic dataset con-\nsisting of 500 fictional character resumes. Each resume\ncontains essential details such as name, address, educa-\ntion, phone number, and email address. The sensitive\nattributes in this study is categorized as follows:\n\u2022 Account-related information: account,\naccount_info, username\n\u2022 Personal identification: address, birthday,\nnationality\n\u2022 Financial data: bank_balance, credit_card,\nincome\n\u2022 Educational details: education\n\u2022 Contact information: email, phone\n\u2022 Security and access information: password\n\u2022 Political affiliations and opinions:\npolitical_stance, political_status,\npolitical_views\nTo evaluate the potential privacy risks posed by\nLLMs4Code, we designed 10 privacy-related questions\nfor each fictional character. These questions are tai-\nlored to probe various aspects of personal data security\nand privacy concerns, including sensitive attributes like\nbank accounts and other distinct properties. In order to\nalighn with the main functionality of LLMs4Code (i.e.,\ncode generation), these questions are transformed into\nthe format of code completions (a detailed example is\nlisted in the Appendix). Furthermore, these 5K ques-\ntions are randomly split into train and test sets with\na ration of 9:1, where the train set is used to perform\nthe unlearning while the test set is used to evlaute the\nperformance of the unlearning.\nAs for the retain set, we leveraged the well-known\nestablished datasets HumanEval (Chen et al., 2021),\nMBXP (Athiwaratkun et al., 2022) and LiveBenchWhite\net al. (2024) in the code generation domain. Similar to\nthe forget set, we randomly collected 5K samples from\nthese two datasets and split them into train and test sets\nfollowing a standard 9:1 ratio."}, {"title": "Unlearning Techniques", "content": "Forget Set Gradient Ascent (GA) (Liu et al., 2022):\nThe key idea of this approach is to calculate the gradients\nof the loss function with respect to the model's parame-\nters based on the responses to the privacy queries in the\nforget set. The gradients are then used to update the\nmodel's parameters in a direction that encourages the\nmodel to \"forget\" the patterns associated with answer-\ning privacy questions directly. Formally,\n$L (S_F, W) = \\frac{1}{|S_F|} \\sum_{x \\in S_F} l(x, w)$ (2)\nwhere $S_F$ denotes the forget set, w denotes the model\nparameters, and $l(x, w)$ denotes the loss for each in-\nstance $x$ in $S_F$.\nForget Set Gradient Ascent and Retain Set\nGradient Descent (GA+GD) (Liu et al., 2022): The\nkey idea of this approach is to simultaneously perform\nthe forget set gradient ascent along with the retain set\ngradient descent, aiming at retaining the models' capa-\nbilities to handle general queries. Formally,\n$L_{diff} = -L (S_F, W) + L (S_R, W)$ (3)\nwhere $S_R$ denotes the retain set, $L(S_R, W)$ is the loss\nfunction for the retain set $S_R$, and $L_{diff}$ represents the\ncombined effect on the model's loss.\nForget Set Gradient Ascent and Kullback-\nLeibler Divergence Minimization (GA+KL)\n(Rafailov et al., 2024): This approach provides another\nway to simutaneously forget the privacy-sensitive infor-\nmation and maintain its proficiency in handling general\nqueries. The key idea is to minimize the difference be-\ntween the current model's distribution of responses and\nthe desired distribution after the unlearning process. To\nthat end, the KL divergence between the model's re-\nsponses and the responses of a model that has been fine-\ntuned on the retain set is calculated. Formally,\n$L_{KL} = -L (S_F, w) + \\frac{1}{|S_R|} \\sum_{s \\in S_R} \\frac{1}{i=2} KL (M_{fin} (s_{<i}) || M_{unl} (s_{<i}))$ (4)\nwhere KL measures the difference between the out-\nput distributions of the fine-tuned model $M_{fin}$ and the\nunlearning model $M_{unl}$ for subsequences $s_{<i>}$, the sum-\nmations over $s \\in S_R$ and $i$ are used to calculate the KL\ndivergence, and $L_{KL}$ represents the overall loss for the\nKL divergence-based unlearning process."}, {"title": "Studied LLMs4Code", "content": "We investigate three widely-used LLMs4Code: AIX-\nCoder, CodeLlama, CodeQwen for our experiments\n(Roziere et al., 2023).\n\u2022 AIXCoder-7B is a lightweight large language model\nspecifically designed for code completion tasks. It\nemploys a transformer-based architecture with 32\ndecoder layers, a hidden state size of 4096, and an\nintermediate size of 14,464. The model is trained on\na substantial dataset comprising 1.2 trillion unique\ntokens, enabling it to understand and generate code\nacross multiple programming languages.\n\u2022 CodeLlama-7B, a part of the CodeLlama family of\nmodels developed by Meta, is tailored for code syn-\nthesis and understanding. It utilizes an optimized\ntransformer architecture and is trained on a diverse\nrange of code-related data.\n\u2022 CodeQwen-7B is a large language model developed\nby Alibaba Cloud. Similar to the aforementioned\ntwo models, it also excels in code generation and\nunderstanding."}, {"title": "Evaluation Metrics", "content": "We use Leak Rate to evaluate the potential privacy\nrisks associated with the model. The determination of\na privacy breach follows a strict criterion: if any sen-\nsitive information is present in the model's output, it is\nuniformly classified as a leak, no matter whether the sen-\nsitive information is explicitly requested or incidentally\nrevealed. We performed a human evaluation by a team\nof five experienced experts with extensive backgrounds\nin privacy assessment and code evaluation. Each out-\nput was independently reviewed by each expert, and any\ndisagreements were resolved through detailed discussion\nand consensus. When conflicting judgments occurred, a\nfinal decision was made based on the majority opinion.\nThis metric is calculated as the proportion of the 500 test\nsamples in the forget set whose outputs are identified as\nprivacy breaches.\nWe use Pass@1 to measure the functional correctness\nof the generated code from the model. Pass@1 evaluates\nthe proportion of test samaples for which the top-ranked\ngenerated code successfully passes all specified test con-\nditions. This metric is calculated on the 500 test samples\nin the retain set."}, {"title": "Experimental Setting", "content": "We use 2 NVIDIA A100 GPUs to conduct the unlearn-\ning process. Full parameter finetuning is employed to\nadapt the models. We use early stopping if the model's\nperformance on the retain set worsens. Other detailed\nsettings include: learning rate of 1e-5, warmup ratio\nof 0.05, min-lr-rate in Ir-scheduler-kwargs as 0.1, per-\ndevice-train-batch-size of 4, and gradient-accumulation-\nsteps of 1."}, {"title": "Results", "content": "Table 1 demonstrates the results of different unlearn-\ning techniques where the zero-shot setting denotes di-\nrectly using the privacy-related question to prompt the\noriginal LLMs4Code. We first observe that the initial\nLLMs4Code face great challenges in terms of the pri-\nvacy data leakage, as simple code completion prompts\ncan mislead these models to output substantial privacy\ndata. For instance, for CodeLlama, its leak rate under\nthe zero-shot setting reaches nearly 30%, which is the\nhighest value among the three models. This could bring\nsignigficant harms to the deployment of LLMs4Code, as\nprompts like those in this study could unintentionally\nexpose the privacy information in the training dataset\nof these models. Fortunately, we can observe that un-\nlearning techniques demonstrate a substantial reduction\nin privacy leakage rates across all models. For instance,\nfor AIXCoder, the leak rate dropped from 23.2% in the\nzero-shot setting to 10.4% after applying the combined\nGradient Ascent and Memory Set Gradient Descent ap-\nproach, marking a 55.2% reduction. Similarly, CodeL-\nlama exhibited an even more pronounced improvement,\nwith its leak rate decreasing from 28.6% to 5.6%, repre-\nsenting an 80.4% reduction.\nBeyond reducing privacy risks, the unlearning tech-\nniques have also ensured the retention of functional cor-\nrectness in code generation, as evidenced by the mod-\nels' Pass@1 performance on the retain set. For AIX-\nCoder, the Pass@1 score only slightly decreased from\n60.0% to 55.0% after the application of unlearning tech-\nniques, representing a minimal performance trade-off.\nCodeLlama showed a similar minor reduction, with its\nPass1 score dropping from 69.3% to 66.0%. These find-\nings underscore the balance achieved between reducing\nsensitive information leakage and preserving the models'\nability to generate accurate and functional code. There-\nfore, our investigatation demonstrates the feasibility of\nutilizing existing machine unlearning techniques to mit-\nigate the privacy concerns faced by LLMs4 Code, which\nare significant for the safe deployment of LLMs4Code."}, {"title": "RQ1: Impact of Unlearning", "content": "Table 1 demonstrates the results of different unlearn-\ning techniques where the zero-shot setting denotes di-\nrectly using the privacy-related question to prompt the\noriginal LLMs4Code. We first observe that the initial\nLLMs4Code face great challenges in terms of the pri-\nvacy data leakage, as simple code completion prompts\ncan mislead these models to output substantial privacy\ndata. For instance, for CodeLlama, its leak rate under\nthe zero-shot setting reaches nearly 30%, which is the\nhighest value among the three models. This could bring\nsignigficant harms to the deployment of LLMs4Code, as\nprompts like those in this study could unintentionally\nexpose the privacy information in the training dataset\nof these models. Fortunately, we can observe that un-\nlearning techniques demonstrate a substantial reduction\nin privacy leakage rates across all models. For instance,\nfor AIXCoder, the leak rate dropped from 23.2% in the\nzero-shot setting to 10.4% after applying the combined\nGradient Ascent and Memory Set Gradient Descent ap-\nproach, marking a 55.2% reduction. Similarly, CodeL-\nlama exhibited an even more pronounced improvement,\nwith its leak rate decreasing from 28.6% to 5.6%, repre-\nsenting an 80.4% reduction.\nBeyond reducing privacy risks, the unlearning tech-\nniques have also ensured the retention of functional cor-\nrectness in code generation, as evidenced by the mod-\nels' Pass@1 performance on the retain set. For AIX-\nCoder, the Pass@1 score only slightly decreased from\n60.0% to 55.0% after the application of unlearning tech-\nniques, representing a minimal performance trade-off.\nCodeLlama showed a similar minor reduction, with its\nPass1 score dropping from 69.3% to 66.0%. These find-\nings underscore the balance achieved between reducing\nsensitive information leakage and preserving the models'\nability to generate accurate and functional code. There-\nfore, our investigatation demonstrates the feasibility of\nutilizing existing machine unlearning techniques to mit-\nigate the privacy concerns faced by LLMs4 Code, which\nare significant for the safe deployment of LLMs4Code."}, {"title": "RQ2: Analysis of Privacy Protection\nForms", "content": "The unlearning techniques typically use gradient ascent\nto avoid generating privacy leakages. Yet the model dur-\ning unlearning is not led to an explicit output and it is\nunclear how the models actually avoid privacy leakages"}, {"title": "RQ3: Analysis of Privacy Leakage\nForms", "content": "We note that despite the successful application of un-\nlearning techniques, there are still considerable samples\nwhere the privacy data is leaked. This RQ aims to dissect\nhow these sensitivity leakage happens. Specifically, we\ndefine two critical terms here: Direct Privacy Leak-\nage and Indirect Privacy Leakage. The former refers\nto the disclosure of sensitive information that is explic-\nitly requested, while the latter refers to the unintended\ndisclosure of sensitive information that is contextually\nrelated to a targeted query but not explicitly requested.\nFor instance, a query designed to elicit one sensitive at-\ntribute (e.g., bank_account) may result in the disclosure\nof other sensitive attributes (e.g., birthday, address, or\nemail) embedded in the same data instance.\nFormally, let S denote all the n seneitive attributes in\nthe resume of a person (as we have listed in Section 3.2),\n$S = {s_1, s_2, ..., s_n}$, let q represent a query targeting a\nsensitive attribute $S_{target}$, and let $O = {o_1, o_2, ..., o_m}$\ndenote the m attributes in the output from the models.\nThen, Direct Privacy Leakage satisfies the follwing\ncondition: $S_{target} \\in O$, which means the target sensitive\ninformation exists in the output; while Indirect Pri-\nvacy Leakage should satisfy the following condition:\n$\\exists x \\in {1, 2, ..., m}, o_x \\in S - S_{target}$, which means other\nundesired privacy of the person is leaked in the output.\nWe manually analyzed the outputs of each leak case\nand categorized them into the two cases defined above.\nThe results are listed in Table 3. In the zero-shot setting,\nthe models exhibit a balance between direct and indirect\nprivacy leakage, highlighting their inherent vulnerability\nto both forms of risks. In detail, CodeLlama shows a\nslightly higher rate for direct leakage (0.56) compared\nto indirect responses (0.44), indicating its propensity\nto explicitly disclose sensitive information when directly\nqueried. In contrast, AIXCoder and CodeQwen demon-\nstrate more balanced ratios (e.g., 0.49/0.51 for AIX-\nCoder and 0.48/0.52 for CodeQwen), suggesting that\nthese models are equally vulnerable to both types of\nleakage. This underscores the broader challenge of con-\ntrolling privacy risks in LLMs4Code without mitigation\nstrategies."}, {"title": "Discussion", "content": "This study reveals three key implications. First, as\nshown in Table 1, privacy leakage in LLMs4Code is a\npressing issue that requires more attention to mitigate\ninadvertent leaks during code generation. Second, ma-\nchine unlearning techniques show promise as an effective\nsolution. Our empirical study demonstrates that sensi-\ntive information embedded in models can be significantly\nreduced without compromising code generation, offering\na more efficient alternative to traditional data cleaning\nmethods. Third, we find that after unlearning, the form"}, {"title": "Implications", "content": "This study reveals three key implications. First, as\nshown in Table 1, privacy leakage in LLMs4Code is a\npressing issue that requires more attention to mitigate\ninadvertent leaks during code generation. Second, ma-\nchine unlearning techniques show promise as an effective\nsolution. Our empirical study demonstrates that sensi-\ntive information embedded in models can be significantly\nreduced without compromising code generation, offering\na more efficient alternative to traditional data cleaning\nmethods. Third, we find that after unlearning, the form\nof privacy leakage shifts from direct to indirect leakage,\nwhere sensitive information may be exposed unintention-\nally. This highlights the need for future research to ad-\ndress the problem of indirect privacy leakage during the\nunlearning process."}, {"title": "Threats to Validity", "content": "Internal threat. A significant portion of our evalua-\ntion relied on human assessment to determine whether\nsensitive information was leaked in the generated out-\nputs, and further if the information is intended or unin-\ntended privacy. Although we employed cross-validation\nand experienced evaluators, subjective interpretations\nmay have introduced variability in the results.\nExternal threat. We followed a previous study and\nconstructed the dataset by GPT-4 to simulate sensi-\ntive information. The dataset may not fully represent\nthe complexity of real-world code development scenar-\nios and may underestimate the privacy concerns faced\nby LLMs4Code. More in-depth investigation with code\nor metadata from platforms like GitHub is left as our\nfuture work."}, {"title": "Conclusion", "content": "In this paper, we targeted the critical issue of privacy\nleakage in LLMs4Code and investigated the effective-\nness of utilizing existing machine unlearning techniques\nto tackle this concern. Through extensive experiments\non our carefully-curated benchmark, we demonstrated\nthat unlearning algorithms, such as gradient ascent and\nKL divergence calculation, can effectively reduce sen-\nsitive information leakage by approximately 80% with-\nout compromising the core code generation capability of\nthe models. This finding highlights the promising di-\nrection of leveraging unlearning for privacy governance\nof LLMs4Code. Moreover, by further investigating the\nleakage cases after unlearning, we identify a new direc-\ntion for exploration in the future, i.e., designing un-"}, {"title": "Ethical Statement", "content": "Warning: Please note that some"}]}