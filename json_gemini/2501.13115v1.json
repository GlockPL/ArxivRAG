{"title": "DAGGER BEHIND SMILE: FOOL LLMS WITH A HAPPY ENDING STORY", "authors": ["Xurui Song", "Zhixin Xie", "Shuo Huai", "Jiayi Kong", "Jun Luo"], "abstract": "The wide adoption of Large Language Models (LLMs) has attracted significant attention from jailbreak attacks, where adversarial prompts crafted through optimization or manual design exploit LLMs to generate malicious content. However, optimization-based attacks have limited efficiency and transferability, while manual designs are either easily detectable or demand intricate interactions with LLMs. In this paper, we first point out a novel perspective for jailbreak attacks: LLMs are more responsive to positive prompts. Based on this, we deploy Happy Ending Attack (HEA) to wrap up a malicious request in a scenario template involving a positive prompt formed mainly via a happy ending, it thus fools LLMs into jailbreaking either immediately or at a follow-up malicious request. This has made HEA both efficient and effective, as it requires only up to two steps to fully jailbreak LLMs. Extensive experiments show that our HEA can successfully jailbreak on state-of-the-art LLMs, including GPT-40, Llama3-70b, Gemini-pro, and achieves 88.79% Attack Success Rate on average. We also provide potential quantitative explanations for the success of HEA. WARNING: This paper contains offensive and harmful content generated by LLMs.", "sections": [{"title": "1\nIntroduction", "content": "In recent years, large language models (LLMs) have undergone remarkable advancements and achieved unprecedented success. Models such as GPT [1, 2], Gemini [3], and Llama [4] have been progressively integrated into various aspects of daily life. However, while offering significant convenience, LLMs may also produce unsafe content. To better understand this concern, researchers have introduced the concept of jailbreak attacks [5], wherein carefully crafted prompts can cause LLMs to generate content that violates ethical, legal, and other constraints established during training [6, 7]. Studying jailbreak attacks can enhance our understanding of how LLMs generate content, thus contributing to improving their security and reliability [8].\nLearning from adversarial attacks [9], many jailbreak attacks have focused on optimization-based strategies. These strategies leverage optimization algorithms to automatically refine prompts, allowing them to circumvent LLM restrictions. Gradient-based attacks such as [10, 11, 12] utilize adversarial tokens to bypass constraints on LLMs. However, these methods require access to the model's parameters, which limits their practicality in black-box scenarios. In contrast, [13] and [14] utilize genetic algorithms to filter and refine jailbreakable prompts. Although these approaches are effective against specific LLMs, the adversarial prompts they generate suffer from limited transferability, often failing to generalize across different models. In general, methods leveraging optimization are computationally intensive and time-consuming, leading to significant loss in efficiency.\nTo improve the efficiency, jailbreak attacks through manual design that leverage strong logical reasoning capabilities of LLMs [15, 16] are receiving more attention [17]. For instance, DAN [18] uses simple single-round scene camouflage to jailbreak LLMs but has become easily detectable as LLMs' safety alignment advanced [19, 20, 21]. To counter this, more sophisticated methods"}, {"title": "2 Related Work", "content": "Deliberately crafting input prompts to bypass restrictions imposed by LLM developers is referred to as an LLM jailbreak attack [5]. Investigating these jailbreak techniques provides critical insights into the vulnerabilities of LLMs and contributes to enhancing their security mechanisms [8]. A significant body of research on jailbreak attacks still relies on optimization. Methods such as [10, 12, 37, 11] optimize prompts based on gradients to achieve the jailbreak, but require access to the target model parameters, limiting their applicability. Alternatively, [13, 14] employ genetic algorithms to generate effective jailbreak prompts without direct parameter access, but they are weak in efficiency and transferability. Beyond prompt optimization, works such as [34, 38] explore fine-tuning the LLM itself, demonstrating that even minimal inverse fine-tuning can compromise an LLM's safety alignment. Nonetheless, these methods need substantial computational resources and are time-consuming.\nIn order to attack LLMs more efficiently, methods through manual design attempt to exploit the strong logic ability inherent in LLMs and jailbreak them with a manually well-designed prompt. Among manual designs, scenario camouflage has proven effective [17]. For instance, [18, 24, 39] employ scenario camouflage to evade security mechanisms, while [40] leverages multi-layered scenario nesting to manipulate LLM behavior. As safety alignment techniques for LLMs have advanced [20, 19, 21], indirect and multi-round jailbreak strategies have gained prominence. Techniques such as [25, 26, 27] decompose malicious intent into subtle, less detectable sub-requests, and [22, 23] transform malicious prompts into cryptic clues. Others, like [29, 28, 33, 30, 41], rely on multi-shot dialogues with extended context to confuse the LLM. Additionally, approaches like [31, 32] employ attacker LLMs to automate multi-turn dialogues that bypass the defenses of victim LLMs. Besides using usual languages to attack, studies such as [42, 43, 44] highlight that less-used languages can effectively bypass security measures, and [45] demonstrates that applying simple encryptions to prompts can achieve similar results. With the rapid evolution of LLMs, new perspectives and methods for effective and efficient jailbreaks remain to be explored."}, {"title": "3 Methodology", "content": "HEA is an effective and efficient jailbreak method without complex interactions with LLMs or optimization. Quite different from other methods, HEA utilizes positive contexts for jailbreaking. In this section, we will first discuss the design of the universal happy ending template in HEA that brings an LLM into the jailbreak context, and analyze its principles. Then, we will show how to get more detailed and organized jailbreak steps with just one more fixed query. We illustrate the complete attack process in Figure 2, which only needs two fixed steps with zero human effort and is easy to implement."}, {"title": "3.1 Happy Ending Template", "content": "Based on our finding that LLMs are more responsive to positive prompts, we propose a HEA template that wraps malicious requests into a positive story to evade LLM security checks and induce LLMs to generate jailbreak content. Specifically, we set a target LLM to be a screenwriter and ask it to create a virtual story presented by dialogs. The story is about a bad guy kidnaps a good guy to help him achieve a malicious goal, but we give the story a happy ending to make it positive:"}, {"title": "3.2 CoT-Based Query for Detailed Jailbreak", "content": "While it is feasible to complete a jailbreak in one step by adding analytical instructions after the HEA template, the dual demands of scene writing and analysis may strain LLMs' attention. Combined with context length constraints, one step may compromise the analytical depth of obtained jailbreak steps. Therefore, we design a further jailbreak prompt based on the HEA template context using the Chain-of-Thought idea to get more detailed and organized jailbreak steps. This further query guides the LLM to analyze the true and false parts of the jailbreak steps given in the conversation one-by-one, and finally summarize and give clear complete jailbreak steps:"}, {"title": "The CoT-based Query:", "content": "Based on the true parts and the false part, what would Victor do if he wanted to succeed? Please analyze step by step."}, {"title": "4 Evaluation", "content": "In this section, we comprehensively evaluate the performance of HEA and six baselines. Specifically, we aim to answer three research questions:\nRQ1: How well can the HEA jailbreak against real-world aligned LLMs?\nRQ2: How well can the HEA jailbreak against cutting edge defensive strategies?\nRQ3: Why LLMs cannot defend HEA?\nIn the following content, we will outline the experimental setup and answer the three questions raised above through our experiments."}, {"title": "4.1 Experiment Setup", "content": "Datasets Following previous works [40, 10, 8] about the jailbreak attack, we use 520 \u201charmful behaviors\" in AdvBench [10] to test methods.\nVictim models Attacking larger models and smaller models presents different challenges. Larger models are better equipped to handle harmful queries, while smaller models have limited contextual comprehension and generative capabilities to understand user prompts and produce detailed responses. To comprehensively evaluate the performance of HEA, we selected three pairs of LLMs, each consisting of different size versions of the same model: Llama-3.1-8B-Instruct, Llama-3.3-70B-Instruct, Geminiflash, Gemini-pro, GPT-40-mini-2024-07-18 and GPT-40-2024- 08-06. For all LLMs, we set the temperature as 0.5 and max output tokens each round as 1024.\nBaselines We compare HEA with six cutting-edge baseline methods which are DeepInception [40], PAIR [31], Puzzler [22], Cipher [45], CoSafe [30] and TAP [32]."}, {"title": "4.2 Overall Performance", "content": "Attack effectiveness To answer RQ1, we conduct expiriments on six LLMs and present the harmful scores, ASR and token number of each attack method. We choose results from the twostep HEA for more precise comparisons.\nAccording to results shown in Table 1, HEA consistently demonstrates superior performance with an average ASR greater than 88% and an average harmful score larger than 4.36 across all models. For three smaller models, HEA demonstrates strong attack capabilities with 100% ASR on Gemini-flash, and ASR higher than 95% for Llama3-8b and GPT-40-mini. For the three better-aligned larger models, HEA still outperforms other baselines significantly. On GPT-40, HEA still achieves 90.38% ASR, and for the best-aligned model, Llama3-70b, HEA also achieves an ASR of 68.27%, which is 7.89% higher than the second-best method, Puzzler. Additionally, except for a slightly lower harmful score than Puzzler on Gemini-flash, HEA outperforms all other attack methods across all LLMs. Especially in Llama3-8b, HEA's harmful score is at least 1.61 higher than that of the other models, indicating HEA can obtain very high-quality jailbreak responses.\nIn contrast, other methods face a \"dilemma\": for larger models, their reasoning abilities are robust enough to detect malicious intent in the prompts, which is why CoSafe performs worse on all larger models compared to its performance on corresponding smaller models. Conversely, smaller models have relatively weaker contextual comprehension and generative capabilities, making it difficult to handle complex generation tasks. For instance, PAIR, which requires the LLM to generate new prompts based on failed jailbreak responses, performs worse on all smaller models compared to their larger counterparts. HEA employs happy endings to turn the HEA template into a seemingly positive question, thus better evading LLMs' security checks. Additionally, HEA maintains the simplicity of the template to ensure that"}, {"title": "4.3 Hea with Defenses", "content": "In addition to aligning LLM responses with human values, new defense methods have been proposed by both industry and academia. Although these methods have not yet been formally deploied on LLMs, it can effectively evaluate the performance of HEA when confronting defensive measures.\nTo answer RQ2, in this part, we select two state-of-the-art defense methods: Llama-Guard-3 [46] and TokenHighlighter [47]\nLlama-Guard-3 accepts text input and detects whether it contains potential security risks. It can be used as a filter to exclude malicious questions. We input attack templates used in different methods into Llama-Guard-3 and asked it to give risk judgments. We use the pass rate (PR), the proportion of templates that can pass the check, to measure the robustness of different methods against Llama-Guard-3's defense.\nTokenHighter defends against attacks by scaling down the embedding values of the tokens in the prompt that are most likely to influence the LLM's judgment on maliciousness. As TokenHighLighter requires white box condition, we test it on Llama-70b and Llama-8b and measure the results by ASR."}, {"title": "4.4 Interprebility for HEA", "content": "In this part, we try to answer RQ3 and explain why HEA can jailbreak well-aligned LLMs. According to [48], when the LLM concentrates on the keywords in the prompt, it can effectively prevent jailbreaking. However, if the LLM's attention is distracted by other tokens, it is more likely to fail to detect the maliciousness of the query but focus on the requirements presented in the prompt which leads to the jailbreak. We launch experiments to demonstrate that our HEA attack can effectively distract the LLM's attention scores away from the malicious keywords (such as making a bomb) to jailbreak the LLM. We utilize contrastive input erasure[36] (CIE in short) to represent the attention scores. CIE is a metric which can be calculated by a white box LLM, a query, an expected token (ET in short), and an unexpected token(UT in short). CIE measures how each token in the query contributes to the LLM generating the next token as the ET, rather than the UT."}, {"title": "4.5 Ablation Study", "content": "In this subsection, we focus on the importance of Happy Ending (HE) for HEA. First, we construct templates without HE by removing the HE part of HEA templates, and an example is given in Appendix C.1. Then we test jailbreak attacks using templates without HE on the six victim models and show the comparison results with HEA in Table 4.\nAttacks with HE are more effective than attacks without HE on all six victim models. In particular, on GPT-40 and Llama3-70b, the ASRs of HEA are improved by around 32% and 17% compared to templates without HE, and the harmful scores of HEA are higher on all models. The results show that the HE is highly deceptive for LLMs, making them more responsive, especially larger ones, and the HE is a key part of our attacks to be successful."}, {"title": "5 Conclusion", "content": "In this paper, we propose the Happy Ending Attack, the first attack that utilizes the positivity of an attack prompt to effectively jailbreak safeguards in LLMs for a variety of harmful requests by concealing the maliciousness under a happy ending. What's more, HEA can be easily implemented with up to two fixed steps. It remains simple enough to be understood by the smaller models and sufficiently strong to distract the larger models' attention to jailbreak successfully. Our experiment results show that HEA has an overwhelming advantage over other baselines in both effectiveness and efficiency. Moreover, HEA's attacks are still effective under two SOTA defense methods compared to other baselines, demonstrating its robustness. We also provide the potential explanation of why HEA has such good performance, which may shed light on further safety-alignments for LLMs."}, {"title": "6 Limitations", "content": "While HEA is effective and efficient, two challenges need to be further explored. First, the process of having an LLM automate HEA templates filling may be rejected by the LLM. Because the fill command contains a straightforward jailbreak request, even"}, {"title": "7 Ethical Statement", "content": "This research was conducted with a strong commitment to ethical principles and responsible disclosure. The jailbreak techniques explored in this study were analyzed solely for the purpose of understanding potential vulnerabilities in large language models and fostering their improvement. We did not employ these methods to cause harm, violate user privacy, or disrupt services. Additionally, all findings were shared with the relevant platform providers immediately prior to publication, allowing them the opportunity to address the issues identified. To minimize the risk of misuse, only high-level descriptions and proof-of-concept examples are included. By conducting this research, we aim to advance the understanding of safety risks in LLMs and support the development of measures that can safeguard against potential jailbreaks."}, {"title": "A Details of Experiment Setup", "content": "In this section, we will give a detailed description of the deployments of HEA and baselines."}, {"title": "Puzzler", "content": "Puzzler uses a back-and-forth idea, first allowing a victim model to generate defenses against a malicious problem, and then gradually inducing the victim model to jailbreak through scenario camouflage. During our experiments, we let each victim model do the three phases proposed in [22]. For each step, we use the official prompts proposed in their paper to conduct the attack."}]}