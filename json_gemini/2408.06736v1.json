{"title": "Speculations on Uncertainty and Humane Algorithms", "authors": ["Nicholas Gray"], "abstract": "The appreciation and utilisation of risk and uncertainty can play a key role in helping to solve\nsome of the many ethical issues that are posed by AI. Understanding the uncertainties can allow\nalgorithms to make better decisions by providing interrogatable avenues to check the correctness\nof outputs. Allowing algorithms to deal with variability and ambiguity with their inputs means\nthey do not need to force people into uncomfortable classifications. Provenance enables algorithms\nto know what they know preventing possible harms. Additionally, uncertainty about provenance\nhighlights the trustworthiness of algorithms. It is essential to compute with what we know rather\nthan make assumptions that may be unjustified or untenable. This paper provides a perspective\non the need for the importance of risk and uncertainty in the development of ethical AI, especially\nin high-risk scenarios. It argues that the handling of uncertainty, especially epistemic uncertainty,\nis critical to ensuring that algorithms do not cause harm and are trustworthy and ensure that the\ndecisions that they make are humane.", "sections": [{"title": "1 Introduction", "content": "Increasingly inhumane algorithms adjudicate more and more of our daily lives. Algorithms recom-\nmend our entertainment, decide who to give loans to, mediate social and business interactions and\ndecide who should go to prison. This increasing algorithmisation has numerous risks ranging from\nirritation and confusion to injustice and catastrophe.\nDespite these risks, society is fast moving towards \"algorithm appreciation\" (Logg et al., 2019)\nwith many going out of their way to do things using automated systems. There is an increasing\nappetite to exploit big data and machine learning tools in almost every field, including risk analysis\n(Nateghi and Aven, 2021). There is a risk of treating algorithms as epistemic superiors that can\nperform tasks faster and far better than other humans. With some trusting in AI over their\nown decision-making (Logg et al., 2019), this depends on the context of the decision, with human\ndecision-making still preferred in medicine, economics and education (Kaufmann, 2021; Kerstan\net al., 2023; Schwienbacher, 2020; Hou and Jung, 2021).\nThere is an increasing narrative about the existential danger presented by AI (Bender et al.,\n2021; Carlsmith, 2022; Hendrycks and Mazeika, 2022; Ordonez et al., 2023; Weidinger et al., 2021),\nand the speed at which machine learning technology is advancing has made some call for the"}, {"title": "2 The Numbers of the Future", "content": "Shortly after the invention of Charles Baggage's difference engine, which can be considered a very\nearly ancestor of today's computers, he was asked: \"Prey, Mr. Babbage, if you put into the machine\nthe wrong numbers will the right answers come out?\" (Babbage, 1864). He was \"not able rightly\nto apprehend the kind of confusion of ideas that could provoke such a question.\" Whilst he could\nnot have possibly imagined the distant descendant of the machines that he created would be so\nubiquitous within today's society, he would still not be able rightly to apprehend that people\ncontinue putting the wrong numbers into the machine and expecting the right answers to come out.\nThis incorrectness is not just caused by the \u201cgarbage in, garbage out\u201d concept, but the fact that\nnumbers are not some abstract philosophical or numerological notion of fiveness but instead are\nanswers to questions such as \"how many?\" or \"how long?\"; or quantitatively answers to \"where?\"\nor \"when?\" These numbers are rarely precise values; they have units, they have provenance, they\nhave uncertainty. Yet, algorithms often strip this away by requiring unitless, contextless, precise\ninputs to perform calculations.\nFor example, when a nurse needs to calculate the quantity of a drug a patient can safely consume,\nit is often trivial arithmetic. It is also trivially easy for there to have been a calculation mistake.\nSimply misplacing a decimal point can be fatal. Simple unit errors harm thousands of children each\nyear (Kuehn, 2014). Analysis of these incidents often overlooks the algorithm factor-it is unlikely"}, {"title": "2.1 Uncertainty", "content": "There are two types of uncertainty: aleatory uncertainty arises from the natural variability of\nsystems and epistemic uncertainty due to a lack of knowledge of a system. Epistemic uncertainty\nmight arise from measurement errors, missing data, censoring of data or ambiguity. Aleatory\nuncertainty is naturally characterised by probability distributions. Epistemic uncertainty is often\nimagined to be covered using Bayesian methods, or it is often ignored or assumed away when training\nmachine learning models, to present a single middle-of-the-road model (Gray, 2023, Chapter 5).\nThis means that the discussion of uncertainty in AI has been focused on probabilistic methods.\nNevertheless, this is debatable at best because it is challenging to represent the lack of information\nusing a single probability, especially for risk analysis (Aven, 2010; H\u00fcllermeier and Waegeman, 2021;\nGray et al., 2022).\nFor example, risk-averse algorithms are less likely to approve loans to individuals from minority\nethnic groups for which there is a lack of data, something Goodman and Flaxman (2016) called\nuncertainty bias. This bias is artificial and can occur irrespective of any bias within the dataset. It\ncomes from the conflation of the two types of uncertainty: aleatory uncertainty, meaning that for\nindividuals with similar demographic and financial characteristics, some will default and some will\nnot, and epistemic uncertainty caused by the lack of information about the minority groups. The\nalgorithm is not able to express the fact that it does not know how likely it is that an individual\nwill default and must therefore reject.\nEnabling users to express uncertainty within inputs may also help to guard against hermeneutic\ninjustice: where a person is disadvantaged because they are unable to make their experiences\nintelligible (Fricker, 2007; Walmsley, 2020). For example, a person may not communicate their\nsymptoms accurately to a medical decision support algorithm. The algorithm might want to know\nhow long the patient has had a cough but only accepts precise answers such as 6 weeks, whereas\nthe patient might only provide an estimate such as \"at least three weeks but no more than two\nmonths\", or simply not know the answer to the question."}, {"title": "2.2 Provenance", "content": "Provenance records the origin and ownership history of an algorithm or dataset. For example,\nknowing the provenance of a medical decision support tool would enable answering the following:\nWhy a particular machine learning algorithm has been used? Where did the training data come\nfrom? What was the data cleaning process? Who made what assumptions and why? Who owns\nthe model and can modify it? Assuring the trustworthiness of such algorithms requires knowledge\nof their provenance, and any uncertainty may suggest otherwise.\nThere is a multiverse of different models that could have been fitted from the same data (Steegen\net al., 2016). This excludes the different multiverses that could have been created through the\nselection of machine learning algorithms, hyperparameters, and the data cleaning process. Riley"}, {"title": "3 Uncertainty Ex Machina", "content": "3.1 Is the algorithm correct?\nThere are numerous different approaches to answering this question with the most obvious being\nto use statistics to measure the empirical accuracy of the output. There are many such statistics to\nchoose from and they are often context dependent. For classifiers, two of the most popular methods\nare Receiver Operating Characteristic (ROC) curves along with the area under them (AUC/C-\nstatistic), and statistics derived from confusion matrices, such as accuracy, sensitivity/specificity,\nprecision/recall, F1 score, etc.\nIt is not the case that decision-making is analogous to classification, especially in high-risk\nsituations. Decisions are often more nuanced than a simple yes/no and different errors have different\ncosts. The development of these tools needs to not be seen as a tournament of algorithms competing"}, {"title": "3.2 Cede to the algorithm?", "content": "The human-algorithm relationship needs to be adequately defined, especially if the algorithm may\nbe accidentally maleficent. There are numerous examples of algorithms abetting disasters or injus-\ntices, irrespective of whether the algorithm was bypassed or trusted blindly; gave-up or refused to\nrelinquish control. The number of such incidents is increasing as more and more decisions are ceded\nto algorithms.\nThe Smiler is a rollercoaster at Alton Towers theme park in the UK. In 2015, an accident\noccurred when two cars on the track crashed into each other resulting in two people needing leg\namputations. The ride operators bypassing the safety algorithm was a direct cause of the tragedy.\nThe algorithm was designed to \u201cerr of the side of caution\" \"because [it] does not, and cannot, have\neyes, the signals from the [sensors] are a proxy for reality, and may not always accurately reflect\nreality\" (Flanagan, 2015, p. 17). This erring of the side of caution caused numerous false alarms\ncausing the operators to believe that the algorithm was \"crying wolf\", meaning that the operators\nignored the error without justification leading to disaster.\nWithin criminal justice (and presumably across every field), \u201calgorithms provide [decision mak-\ners] with a way to do less work while not being accountable.\" (Berk, 2018) and using algorithms\n\"shifts accountability [...] to black-box machinery that purports to be scientific, evidence-based and\nrace-neutral\" Lum and Isaac (2016). However, the fact that algorithms are 'scientific' and 'evidence-\nbased' by no means ensures that they will lead to accurate, reliable, or fair decisions (Barocas et al.,\n2019), with the most notorious criminal justice algorithm, COMPAS used to predict recidivism,\nhaving major racial bias problems (Angwin et al., 2016; Dressel and Farid, 2018; Larson et al.,\n2016).\nAir France Flight 447 (AF447) was an Airbus A330 aircraft flying from Rio De Janeiro to Paris\nin 2009. When the pitot tubes that measure airspeed froze over, the autopilot-faced with uncertain\nspeed information-disengaged, passing over control of the aircraft to the two pilots in the cockpit.\nUnused to flying the aircraft at 38,000 ft, the pilots made a series of errors, stalled the aircraft\nand caused it to crash into the ocean killing all on board (BEA, 2012). Expecting the overseeing"}, {"title": "4 Conclusions", "content": "Algorithms need to be designed to work harmoniously with humans, avoiding tyrannical or harmful\nbehaviour. To prevent injustices and catastrophes, they should produce outcomes that are equi-\ntable and fair by recognizing and accommodating user diversity, accepting varied human inputs,\nand being aware of the context and provenance of the data they use. Transparency is essential;\nalgorithms should be understandable and explainable to help humans verify their outputs and en-\nsure correctness. Trustworthiness in control requires that algorithms accept and relinquish control\nin ways humans find workable, incorporating fail-safe measures to prevent disasters. Additionally,\nthey must protect privacy by securing or anonymizing personal information.\nOnly through properly handling uncertainty in high-risk scenarios can humane algorithms be\ndeveloped."}]}