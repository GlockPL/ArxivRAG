{"title": "Speculations on Uncertainty and Humane Algorithms", "authors": ["Nicholas Gray"], "abstract": "The appreciation and utilisation of risk and uncertainty can play a key role in helping to solve some of the many ethical issues that are posed by AI. Understanding the uncertainties can allow algorithms to make better decisions by providing interrogatable avenues to check the correctness of outputs. Allowing algorithms to deal with variability and ambiguity with their inputs means they do not need to force people into uncomfortable classifications. Provenance enables algorithms to know what they know preventing possible harms. Additionally, uncertainty about provenance highlights the trustworthiness of algorithms. It is essential to compute with what we know rather than make assumptions that may be unjustified or untenable. This paper provides a perspective on the need for the importance of risk and uncertainty in the development of ethical AI, especially in high-risk scenarios. It argues that the handling of uncertainty, especially epistemic uncertainty, is critical to ensuring that algorithms do not cause harm and are trustworthy and ensure that the decisions that they make are humane.", "sections": [{"title": "Introduction", "content": "Increasingly inhumane algorithms adjudicate more and more of our daily lives. Algorithms recommend our entertainment, decide who to give loans to, mediate social and business interactions and decide who should go to prison. This increasing algorithmisation has numerous risks ranging from irritation and confusion to injustice and catastrophe.\nDespite these risks, society is fast moving towards \"algorithm appreciation\" (Logg et al., 2019) with many going out of their way to do things using automated systems. There is an increasing appetite to exploit big data and machine learning tools in almost every field, including risk analysis (Nateghi and Aven, 2021). There is a risk of treating algorithms as epistemic superiors that can perform tasks faster and far better than other humans. With some trusting in AI over their own decision-making (Logg et al., 2019), this depends on the context of the decision, with human decision-making still preferred in medicine, economics and education (Kaufmann, 2021; Kerstan et al., 2023; Schwienbacher, 2020; Hou and Jung, 2021).\nThere is an increasing narrative about the existential danger presented by AI (Bender et al., 2021; Carlsmith, 2022; Hendrycks and Mazeika, 2022; Ordonez et al., 2023; Weidinger et al., 2021), and the speed at which machine learning technology is advancing has made some call for the"}, {"title": "The Numbers of the Future", "content": "Shortly after the invention of Charles Baggage's difference engine, which can be considered a very early ancestor of today's computers, he was asked: \"Prey, Mr. Babbage, if you put into the machine the wrong numbers will the right answers come out?\" (Babbage, 1864). He was \"not able rightly to apprehend the kind of confusion of ideas that could provoke such a question.\" Whilst he could not have possibly imagined the distant descendant of the machines that he created would be so ubiquitous within today's society, he would still not be able rightly to apprehend that people continue putting the wrong numbers into the machine and expecting the right answers to come out.\nThis incorrectness is not just caused by the \u201cgarbage in, garbage out\u201d concept, but the fact that numbers are not some abstract philosophical or numerological notion of fiveness but instead are answers to questions such as \"how many?\" or \"how long?\"; or quantitatively answers to \"where?\" or \"when?\" These numbers are rarely precise values; they have units, they have provenance, they have uncertainty. Yet, algorithms often strip this away by requiring unitless, contextless, precise inputs to perform calculations.\nFor example, when a nurse needs to calculate the quantity of a drug a patient can safely consume, it is often trivial arithmetic. It is also trivially easy for there to have been a calculation mistake. Simply misplacing a decimal point can be fatal. Simple unit errors harm thousands of children each year (Kuehn, 2014). Analysis of these incidents often overlooks the algorithm factor-it is unlikely"}, {"title": "Uncertainty", "content": "There are two types of uncertainty: aleatory uncertainty arises from the natural variability of systems and epistemic uncertainty due to a lack of knowledge of a system. Epistemic uncertainty might arise from measurement errors, missing data, censoring of data or ambiguity. Aleatory uncertainty is naturally characterised by probability distributions. Epistemic uncertainty is often imagined to be covered using Bayesian methods, or it is often ignored or assumed away when training machine learning models, to present a single middle-of-the-road model (Gray, 2023, Chapter 5).\nThis means that the discussion of uncertainty in AI has been focused on probabilistic methods. Nevertheless, this is debatable at best because it is challenging to represent the lack of information using a single probability, especially for risk analysis (Aven, 2010; H\u00fcllermeier and Waegeman, 2021; Gray et al., 2022).\nFor example, risk-averse algorithms are less likely to approve loans to individuals from minority ethnic groups for which there is a lack of data, something Goodman and Flaxman (2016) called uncertainty bias. This bias is artificial and can occur irrespective of any bias within the dataset. It comes from the conflation of the two types of uncertainty: aleatory uncertainty, meaning that for individuals with similar demographic and financial characteristics, some will default and some will not, and epistemic uncertainty caused by the lack of information about the minority groups. The algorithm is not able to express the fact that it does not know how likely it is that an individual will default and must therefore reject.\nEnabling users to express uncertainty within inputs may also help to guard against hermeneutic injustice: where a person is disadvantaged because they are unable to make their experiences intelligible (Fricker, 2007; Walmsley, 2020). For example, a person may not communicate their symptoms accurately to a medical decision support algorithm. The algorithm might want to know how long the patient has had a cough but only accepts precise answers such as 6 weeks, whereas the patient might only provide an estimate such as \"at least three weeks but no more than two months\", or simply not know the answer to the question."}, {"title": "Provenance", "content": "Provenance records the origin and ownership history of an algorithm or dataset. For example, knowing the provenance of a medical decision support tool would enable answering the following: Why a particular machine learning algorithm has been used? Where did the training data come from? What was the data cleaning process? Who made what assumptions and why? Who owns the model and can modify it? Assuring the trustworthiness of such algorithms requires knowledge of their provenance, and any uncertainty may suggest otherwise.\nThere is a multiverse of different models that could have been fitted from the same data (Steegen et al., 2016). This excludes the different multiverses that could have been created through the selection of machine learning algorithms, hyperparameters, and the data cleaning process. Riley and Collins (2023) have shown that fitting models with different representative samples from the same population can lead to models that produce significantly different results. Models can be highly unstable-implying large uncertainty in models-especially with low sample sizes. This uncertainty means that one model within the multiverse is not necessarily more correct than another and different models may make different decisions (Riley et al., 2023). Whilst bootstrapping can be used (Riley and Collins, 2023), knowing the provenance of the data is the best way to check for this instability and assess the correctness of the model.\nKnowing the provenance can also have other benefits. There is more disinformation than information on social media about COVID-19 (Edwards, 2022), with much of it originating from spurious claims made by only twelve individuals (Center for Countering Digital Hate, 2020, 2021). Being aware of the origins of the claims may allow algorithms to detect dangerous falsehoods when recommending content (Baeth and Aktas, 2018).\nGenerative AI systems are likely to exacerbate these problems, making the question of prevalence even more critical. For instance, after the 2022 Russian invasion of Ukraine, deep-fake images were being circulated propagandically (Farid, 2022). The ability of generative AI systems to flood the internet with misinformation risks undermining trust in legitimate sources (Epstein et al., 2023). There is also the risk of generative AI systems 'hallucinating'-returning unintended text that is often unfactual or incorrect in some other way (Maynez et al., 2020; Ji et al., 2023). There are numerous potential harms, from making mistakes falsifying scientific references (Alkaissi and McFarlane, 2023; Emsley, 2023), insider trading (Wain and Rahman-Jones, 2023), or encouraging high treason (Zaccaro, 2023).\nProvenance is a potential solution to many problems, helping algorithms to highlight their trustworthiness (Ferrara, 2023), but uncertainty can also play its part (Xiao et al., 2022), since many hallucinations may be caused by algorithms inventing facts when faced with uncertainty (Lightman et al., 2023). Uncertainty in provenance means that it is likely the case that one is not able to trust that the outputs are correct or trustworthy. If a large language model can express what they don't know then, as opposed to generating nonsense-but not-nonsensical-prose, then any hallucinogenic harms may be avoided.\nThere are lots of potential benefits of algorithms being able to uncover unknown-knowns, information that exists within datasets that could only be uncovered by big data or machine learning methodologies. However, it is only by appreciating the known-unknowns that algorithms can be more humane."}, {"title": "Uncertainty Ex Machina", "content": "There are numerous different approaches to answering this question with the most obvious being to use statistics to measure the empirical accuracy of the output. There are many such statistics to choose from and they are often context dependent. For classifiers, two of the most popular methods are Receiver Operating Characteristic (ROC) curves along with the area under them (AUC/C-statistic), and statistics derived from confusion matrices, such as accuracy, sensitivity/specificity, precision/recall, F1 score, etc.\nIt is not the case that decision-making is analogous to classification, especially in high-risk situations. Decisions are often more nuanced than a simple yes/no and different errors have different costs. The development of these tools needs to not be seen as a tournament of algorithms competing"}, {"title": "Is the algorithm correct?", "content": "There are numerous different approaches to answering this question with the most obvious being to use statistics to measure the empirical accuracy of the output. There are many such statistics to choose from and they are often context dependent. For classifiers, two of the most popular methods are Receiver Operating Characteristic (ROC) curves along with the area under them (AUC/C-statistic), and statistics derived from confusion matrices, such as accuracy, sensitivity/specificity, precision/recall, F1 score, etc.\nIt is not the case that decision-making is analogous to classification, especially in high-risk situations. Decisions are often more nuanced than a simple yes/no and different errors have different costs. The development of these tools needs to not be seen as a tournament of algorithms competing to be the most empirically correct. The ability of large language models to pass medical or legal exams (Bommarito II and Katz, 2022; Jung et al., 2023; Singhal et al., 2023), says nothing of their ability to perform these talks in the real world. Imagine an algorithm is used to predict the likelihood of somebody attempting suicide. Numerous false positives may be annoying but a single false negative is likely to be catastrophic. It needs to be acknowledged that a patient presentation is a unique instance and that their future will be influenced by a unique set of countless environmental, physiological and psychological factors in constant interplay, ergo using a single dataset to inform high-risk decision making is flawed (Nathan et al., 2021; Nathan and Bhandari, 2024).\nAssessing the empirical accuracy alone is not sufficient when considering the correctness of algorithms. Defining moral correctness is harder as it depends on what is deemed sociologically acceptable although bad apples are often easy to spot and are widely reported. For example, Apple and Goldman Sachs were accused of systematically granting men higher credit card limits than women (BBC News, 2019). Much of the AI literature on fairness is focused on defining metrics for bias that algorithms can assess so that they can be designed to achieve an appropriate morality level (Corbett-Davies et al., 2023; Jacobs and Wallach, 2021; Jui and Rivas, 2024; Miko\u0142ajczyk-Bare\u0142a and Grochowski, 2023; Pagano et al., 2022). There is unlikely to ever be a precise algorithm for fairness, there will always be conflicting and ambiguous definitions of fairness, and as such being able to correctly handle uncertainty will be fundamental to counteract the risk of unfairness.\nExplainability is widely considered another important aspect to consider the correctness of algorithms (Doshi-Velez and Kim, 2017; Gilpin et al., 2018). This often takes the form of a second post-hoc model being used to explain the first black box model (Rudin, 2019; Angelov et al., 2021). The idea is: if the black box answers the what?/how much?/etc, then the explainability part answers the why?/how?\nSome argue that explainability is not the remedy that is needed to protect against malicious algorithms (Edwards and Veale, 2017). At the same time, explainability may not be possible, and there are situations where a complete understanding of the algorithm is neither desired nor required (Walmsley, 2020). It has even been argued that in high-risk domains the use of black-box machine learning models, even if they come with post-hoc explainability, should be entirely avoided. Instead, effort should be focused on ensuring models are naturally interpretable (i.e. they produce logical rules or rely on classical statistical methodologies) (Rudin, 2019; Joyce et al., 2023).\nAside from accuracy and explainability, letting AI express epistemic uncertainty is critical to assessing its correctness and gives another intelligible avenue for appeal. Although many algorithms are inherently probabilistic, outputting naked probabilities as though they are the beginning and end of an algorithm's uncertainty is misguided. BASH-GN is a medical risk prediction tool that assesses the risk of a patient having obstructive sleep apnea (Huo et al., 2023). The output of the model is a naked probability alongside a low/high-risk classification (e.g. 30.9% - low risk). This presentation of risk as a single probability is often misunderstood (Aven, 2010, 2023; Gigerenzer et al., 2005; Wegwarth and Gigerenzer, 2011). Improvements can be made by presenting information in ways people find easier to understand. Natural frequencies are a more intuitive way for people to understand probabilities, and it is easy for algorithms to return expressions such as \"31 out of 100\" instead of Pr = 31% (Gigerenzer and Hoffrage, 1995; Gigerenzer, 2011). Icon arrays are another approach that can be used to express information from algorithms (Galesic et al., 2009),"}, {"title": "Cede to the algorithm?", "content": "The human-algorithm relationship needs to be adequately defined, especially if the algorithm may be accidentally maleficent. There are numerous examples of algorithms abetting disasters or injustices, irrespective of whether the algorithm was bypassed or trusted blindly; gave-up or refused to relinquish control. The number of such incidents is increasing as more and more decisions are ceded to algorithms.\nThe Smiler is a rollercoaster at Alton Towers theme park in the UK. In 2015, an accident occurred when two cars on the track crashed into each other resulting in two people needing leg amputations. The ride operators bypassing the safety algorithm was a direct cause of the tragedy. The algorithm was designed to \u201cerr of the side of caution\" \"because [it] does not, and cannot, have eyes, the signals from the [sensors] are a proxy for reality, and may not always accurately reflect reality\" (Flanagan, 2015, p. 17). This erring of the side of caution caused numerous false alarms causing the operators to believe that the algorithm was \"crying wolf\", meaning that the operators ignored the error without justification leading to disaster.\nWithin criminal justice (and presumably across every field), \u201calgorithms provide [decision makers] with a way to do less work while not being accountable.\" (Berk, 2018) and using algorithms \"shifts accountability [...] to black-box machinery that purports to be scientific, evidence-based and race-neutral\" Lum and Isaac (2016). However, the fact that algorithms are 'scientific' and 'evidence-based' by no means ensures that they will lead to accurate, reliable, or fair decisions (Barocas et al., 2019), with the most notorious criminal justice algorithm, COMPAS used to predict recidivism, having major racial bias problems (Angwin et al., 2016; Dressel and Farid, 2018; Larson et al., 2016).\nAir France Flight 447 (AF447) was an Airbus A330 aircraft flying from Rio De Janeiro to Paris in 2009. When the pitot tubes that measure airspeed froze over, the autopilot-faced with uncertain speed information-disengaged, passing over control of the aircraft to the two pilots in the cockpit. Unused to flying the aircraft at 38,000 ft, the pilots made a series of errors, stalled the aircraft and caused it to crash into the ocean killing all on board (BEA, 2012). Expecting the overseeing humans to 'save the day' in this way can be dangerous, not least as there is the possibility that they have forgotten how to do the task in the first place, a problem that Bainbridge (1983) called the irony of automation.\nThe Boeing 737-MAX had a Manoeuvring Characteristic Augmentation System (MCAS) that suffered from almost the reverse problem to that of AF447. MCAS was designed to ensure that the flight control system mimicked the behaviour of older generation aircraft by adjusting the control surfaces when it detected, using a single angle of attack sensor, the aircraft may be approaching stall (Hart, 2019). Two crashes occurred as the pilots of the aircraft were unable to shut MCAS off completely when sensors started receiving incorrect readings after take-off (Cusumano, 2021; ECAA, 2019; KNKT, 2019). Boeing implicitly decided that the algorithm would never make an error and ensured that it could never fully cede control to the human pilots.\nThe expression of unsureness is critical to helping prevent these problems. Trusting that an algorithm can say \"I don't know\" when it cannot make a decision can be of fundamental importance in helping to make the lives of its human supervisors easier. This in turn can help prevent incorrect results by providing a framework for the algorithm to ask for human intervention. If The Smiler could have distinguished between when it knew the track was not safe and when it did not know that the track was safe then the supervisors might have trusted it more. If COMPAS was able to communicate uncertainty about its decision in the ways discussed above, instead of a simple low/high risk, it may have prevented judges from blindly trusting its outputs, which may have reduced injustices. Such uncertainty communication could also help alleviate the uncertainty bias problem discussed above. If the A330 autopilot had been able to communicate the uncertainty in the airspeed measurement to the pilots and given them contextualised information to help address it, instead of leaving them (literally) in the dark, it may not have crashed. Equally, if the 737-MAX's MCAS system had understood that its single sensor did not always accurately reflect reality, then those disasters might not have occurred.\nThere is unlikely to ever be a universal algorithm that can be deployed to prevent injustices or catastrophes, the important thing is that in high-risk situations algorithms are designed to act humanely."}, {"title": "Conclusions", "content": "Algorithms need to be designed to work harmoniously with humans, avoiding tyrannical or harmful behaviour. To prevent injustices and catastrophes, they should produce outcomes that are equitable and fair by recognizing and accommodating user diversity, accepting varied human inputs, and being aware of the context and provenance of the data they use. Transparency is essential; algorithms should be understandable and explainable to help humans verify their outputs and ensure correctness. Trustworthiness in control requires that algorithms accept and relinquish control in ways humans find workable, incorporating fail-safe measures to prevent disasters. Additionally, they must protect privacy by securing or anonymizing personal information.\nOnly through properly handling uncertainty in high-risk scenarios can humane algorithms be developed."}]}