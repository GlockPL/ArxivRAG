{"title": "The Essence of Contextual Understanding in Theory of Mind:\nA Study on Question Answering with Story Characters", "authors": ["Chulun Zhou", "Qiujing Wang", "Mo Yu", "Xiaoqian Yue", "Rui Lu", "Jiangnan Li", "Yifan Zhou", "Shunchi Zhang", "Jie Zhou", "Wai Lam"], "abstract": "Theory-of-Mind (ToM) is a fundamental psy-chological capability that allows humans to un-derstand and interpret the mental states of oth-ers. Humans infer others' thoughts by integrat-ing causal cues and indirect clues from broadcontextual information, often derived from pastinteractions. In other words, human ToM heav-ily relies on the understanding about the back-grounds and life stories of others. Unfortu-nately, this aspect is largely overlooked in exist-ing benchmarks for evaluating machines' ToMcapabilities, due to their usage of short narra-tives without global backgrounds. In this paper,we verify the importance of understanding longpersonal backgrounds in ToM and assess theperformance of LLMs in such realistic evalua-tion scenarios. To achieve this, we introducea novel benchmark, CHARTOM-QA, compris-ing 1,035 ToM questions based on charactersfrom classic novels. Our human study reveals asignificant disparity in performance: the samegroup of educated participants performs dra-matically better when they have read the novelscompared to when they have not. In parallel,our experiments on state-of-the-art LLMs, in-cluding the very recent ol model, show thatLLMs still perform notably worse than humans,despite that they have seen these stories duringpre-training. This highlights the limitations ofcurrent LLMs in capturing the nuanced contex-tual information required for ToM reasoning.1", "sections": [{"title": "1 Introduction", "content": "Theory of Mind (ToM) is a psychological term that\nrefers to the process to understand oneself or others\nby ascribing mental states (typically including the\ndimensions of belief, intention, emotion, desire,\netc) to them (Premack and Woodruff, 1978). For\nhuman, ToM holds an important position in daily\nsocial interactions, as it enables people to explain\nand predict the behavior of others with no direct\naccess to their minds (Apperly, 2010).\nRecently, a range of emerging Large Language\nModels (LLMs) have shown remarkable perfor-mance in solving complex tasks and generatinghuman-like language. To further enhance LLMs'\nability to understand user requirements, it is fasci-nating to explore the extent to which LLMs capture\nthe ToM capabilities and ultimately to equip them\nwith human-level ToM. To this end, researchers\nhave developed numerous benchmarks to evaluate\nthe ToM capabilities of LLMs (Grant et al., 2017;\nNematzadeh et al., 2018; Sap et al., 2019; Le et al.,\n2019; Gandhi et al., 2023; Kosinski, 2023; Zhou\net al., 2023a; Wu et al., 2023; Shapira et al., 2024;\nChen et al., 2024). These benchmarks simulate\nthe experimental design in psychology or cognitive\nresearch (Wimmer and Perner, 1983; Dyck et al.,\n2001), and contain elaborately crafted, story-based\ntasks that target ToM dimensions originally defined\nin psychological research.\nDespite their comprehensive coverage of TOM\ndimensions, these existing benchmarks all largely\noverlook the importance of comprehensive con-\ntextual understanding in human ToM. In daily\nlives, humans understand the minds of others based\non the long historical contexts of their complex so-cial relationships and interactions, rather than with\nonly the local circumstances on the spot. This is a\nnatural phenomenon as the behaviors and mental\nstates of a person are greatly influenced by other\nglobal background factors, such as their personality\nand past experience. However, the testing scenarios\nin existing ToM datasets are usually short of cap-turing these factors, as they are typically depicted\nwith brief text pieces consisting of only superficial\ncharacter actions and surrounding environment.\nIn this paper, we propose CHARTOM-QA\nbenchmark that evaluates the capability of LLMs\non understanding ToM of characters in famous\nnovel books. In CharToM-QA, the task takes"}, {"title": "the form of ToM-related question answering about\ncharacters within story plots. This setting natu-rally addresses the aforementioned challenges of\nmost existing datasets due to the intrinsic features\nof story plots in novel books: 1) diverse social\nscenarios; 2) rich in complex social relationships\nand interactions; 3) high relevance to the whole\nbook storyline. Thus, it alleviates heavy reliance\non pre-determined rules to generate testing scenar-ios and raises higher requirements for comprehen-sively understanding context when evaluating ToM\ncapability of current LLMs.\nFor the dataset creation, we adopt an AI-assisted\nhuman annotation strategy that exploits publicly\navailable book notes written by online reading app\nusers. As users read, they can underline a specific\ntext fragment and write down their comments as\nbook notes. These book notes are often closely re-lated to the selected fragments and their surround-ing plots, which contain valuable interpretation,\nanalysis and thoughts of human readers. We first\ncollect a large number of user notes and their linked\ntext from novel books. Then, as in most previ-ous ToM assessments (Nematzadeh et al., 2018;\nSap et al., 2019; Tracey et al., 2022; Gandhi et al.,\n2023; Ma et al., 2023; Wu et al., 2023; Zhou et al.,\n2023b), we keep the notes reflecting certain ToM\ndimensions of appointed characters, including be-lief, intention, emotion and desire. Based on these\nnotes, GPT-40 is leveraged to generate ToM de-scriptions of the four dimensions about given char-acters. Afterwards, these descriptions are used\nto construct ToM-related questions with answers.\nDuring the whole process, strict validation proce-dures are carried out by experts with strong litera-ture background to ensure data quality.\nFor evaluation, both generative and multichoice\nquestion answering (QA) are conducted. In genera-tive QA, models are asked to give their responses\ngiven a question and its corresponding story plot.\nThe qualities of responses are assessed based on\nthe annotated answers. Particularly, traditional\ntoken-based metrics (e.g. Rouge (Lin, 2004)) and\nembedding-based metrics (e.g. cosine similarity\nbetween sentence embeddings) cannot give a fine-grained assessment that reflects how well a re-sponse matches the answer. Therefore, we design\nan evaluation protocol that mimics the process of\ngrading papers. Specifically, we extract critical\npoints from the answers, which are expected to\nbe included in responses as bonus points. Then,\nGPT-40 is used as evaluator for assessment based", "content": "2 Related Work\nWith the advent of LLMs, many benchmarks have\nbeen developed to evaluate the ToM capability of\nLLMs by simulating the cognitive experiments\noriginally in psychology (Grant et al., 2017; Ne-matzadeh et al., 2018; Sap et al., 2019; Le et al.,\n2019; Gandhi et al., 2023; Kosinski, 2023; Zhou\net al., 2023a; Wu et al., 2023; Shapira et al., 2024;\nChen et al., 2024). Le et al. (2019) proposed an\nimproved evaluation protocol and dataset by care-fully examining the answer space to control for\ndata regularities to mitigate dataset bias. Sap et al.\n(2019) introduced SOCIAL IQA that tests social and\nemotional intelligence, requiring models to infer\npeople's intentions and emotions. Wu et al. (2023)\nexplored higher-order ToM that involves recursive\nreasoning on others' beliefs. TOM-IN-MAC (Yu\net al., 2024) assessed meta-learning capability of\nmachine ToM by designing the task of few-shot\ncharacter understanding. Chen et al. (2024) intro-duced ToMBench that encompasses 8 tasks and 31\nabilities in social cognition and extensively evalu-"}, {"title": "3 Problem Definition of ToM in Global\nContexts", "content": "Problem Formulation Similar to existing works\non story understanding (Ko\u010disk\u1ef3 et al., 2018; Pang\net al., 2022; Xu et al., 2022; Yu et al., 2023), our\ntask adopts a question-answering (QA) format. We\ndenote the global context of a book as G, which in\npractice can be the list of all consecutive paragraphs\nof the book. Each QA pair (q, a) is associated with\na plot window $W \\subset G$, which is a book snippet.\nThe task is then to answer the question according\nto W, with the necessary usage of the contextual\nknowledge from G\u00b9:\n$P(a|q, W, G)$.\nTOM Dimensions Studied We consider a set of\nToM dimensions prevalently studied in previous\nwork in Section 2, Belief, Intention, Emotion and\nDesire. The importance of these dimensions in\ndaily lives have been discussed in (Apperly, 2010).\nWe follow the standard definitions of these dimen-sions in (Apperly, 2010), with the detailed defini-tions provided in Appendix B."}, {"title": "4 CHARTOM-QA: Assessing the\nContextual Aspect of ToM", "content": "We create the CHARTOM-QA benchmark, de-signed to assess the ToM capability of LLMs in un-"}, {"title": "4.1 Book Notes Collection & Filtering", "content": "In recent online reading apps (e.g. Douban, Kin-dle), users can underline a text fragment and take\nnotes while reading, as shown in Figure 1. These\nnotes usually contain valuable interpretation, anal-ysis and thoughts of readers about the selected\nfragment and corresponding story plot. Follow-ing (Wan et al., 2019; Yu et al., 2023), we use the\nuser notes as a proxy to assist the human annotation\nof our dataset. We first download a set of public\nbooks available in the Gutenberg project and use\ntheir Chinese-translated versions with valid usage\nlicenses. For the notes, we collect and use them\nwith necessary procedures to protect the privacy of\nonline reading app users who wrote the notes. For\nannotators participating in the benchmark construc-tion, they are adequately paid according to their\nworking hours."}, {"title": "Construction of Generative QA Task", "content": "Key Note Extraction & Paraphrasing (i.e., An-swer Generation). After collection and filtering,\nwe have obtained a pool of highly ToM-related user\nnotes. From these notes, we aim to fetch concrete\nToM descriptions of characters within story plots.\nSpecifically, we take a two-stage approach consist-ing of extraction and paraphrasing operations, as\nthe case in Figure 2.\nAt the extraction stage, the annotators are in-structed to extract the critical part of a note that\nexplicitly reflects one of the four ToM dimensions\nof a target character, which we call as \"key note\".\nThe basic principle of this stage is that annotators\nshould only consider the literal meaning of a note\nwith no additional inference. We elaborately craft\nexamples for every ToM dimension and write a\ndetailed manual to guide annotators. An annotator\nis only considered qualified after passing a labeling\ntrial. They are given necessary information linked\nwith a user note, including the target character, the\nunderlined text and its surrounding context. After\nextraction, the key notes are kept for each ToM\ndimension. Nevertheless, the extracted key notes"}, {"title": "5 Evaluation Protocols for Generative QA", "content": "Until now, we have acquired ToM-related question-answer pairs along with their linked story plots,\nwhich can be used to conduct evaluation in a gener-ative QA setting. Models are given story plots and\nasked questions to make responses whose qualities\nare assessed using corresponding answers as ref-erence. Conventionally, token-based metrics (e.g.\nRouge (Lin, 2004)) and embedding-based metrics\n(e.g. cosine similarity between sentence embed-dings) are mainly used for the assessment of genera-tive QA tasks. However, these two types of metrics\nhave their inherent limitations. For token-based\nmetrics, they cannot deal with the cases where 1)\nsame meanings are expressed with totally differ-ent tokens; 2) non-critical tokens dominate in a\nsentence. Meanwhile, embedding-based metrics\nonly coarsely measure the semantic similarity but\ncannot explicitly indicate the pros and cons of a\nresponse.\nTherefore, inspired by the process of grading\npapers, we design an evaluation protocol that in-spects the bonus points and penalty of a model\nresponse. Specifically, for each question, we ex-tract the critical points of its reference answer with\nthe assistance of GPT-40, which are expected to be\nincluded as bonus points in a response. The core re-quirements for bonus point extraction are: 1) Bonus\npoints must be derived from reference answers with\nno hallucination; 2) Different bonus points should\norient to different aspects of an answer. The statis-tics about the bonus points of questions in each\ndimension is given in Appendix C. During evalu-ation, a GPT-40 evaluator measures the coverage\nof bonus points as an indicator of response qual-ity. Moreover, it also criticizes the defects in a\nresponse as penalty if there is any inappropriate\nstatement. In this way, both bonus point cover-age and penalty rate are used as a comprehensive\nassessment of response quality. The prompts for\nextracting bonus points and conducting such evalu-ation protocol are detailed in Appendices G and J,\nrespectively. Appendix K also gives illustrative\ncases of such evaluation."}, {"title": "Experiments", "content": "Setup\nWe conduct experiments on our generative and mul-tichoice QA tasks in English and Chinese. Mod-els are instructed to respond with vanilla prompts\nas in Appendix I. To investigate the capability of\nthe model to exploit contextual information for\nToM comprehension, we vary the context lengths"}, {"title": "Human Performance", "content": "In this section, we conduct human study to explore\nhow humans perform on our task, and to assess the\nrole of long-context dependency in human ToM\nwhen understanding fictional characters. Our study\naddresses the following research questions:\n\u2022 RQ1: What is the performance gap between hu-mans with and without knowledge on the histori-cal contexts of the characters? This question ex-plores the key hypothesis of this paper regarding\nthe long-dependency nature of human ToM. To\ninvestigate this, we divided the human annotators\ninto two groups: those who have read the books\nand are familiar with the appointed characters\n(w/. history) and those who have not read the\nbooks (w/o. history). The performance gap be-tween the two groups highlights the importance\nof long-context dependency in human ToM.\n\u2022 RQ2: Can humans outperform LLMs? We com-pare the performance of the two human groups\ndescribed earlier with GPT-40 on the same sam-ples, to understand how human ToM can help to\nbetter solve our task.\n\u2022 RQ3: Can human performance benefit fromlonger input windows? To investigate this, eachannotator is initially asked to answer a questionusing a limited plot window (c=0). Then, they areprovided with extended plot window and decide\nwhether to revise their choices (c=2k).\nTo enable direct comparison with LLMs, we use\nthe multichoice QA setting and recruit 8 educated\nparticipants, all of whom are either PhD candidates\nor hold PhD degrees across various disciplines. We\nsampled 150 questions from 5 books, ensuring that\neach question is assigned to one person who had\nread the book and one who had not. Each partici-pant is given an equal number of questions about\nbooks they had read and had not. Combined with\nthe two variations of plot windows, the participants\ntotally make choices on 600 questions."}, {"title": "6.4 Analysis and Discussions", "content": "Can CoT (01 models) help? We further explore\nwhether CoT prompting could bring improvement\nof ToM comprehension. For this study, we use the\nmost advanced OpenAI ol model. Because of the\nhigh complexity and cost of o1-model, we compare\nit on the human study subset of 150 instances. As\nshown in Table 3, o1-model improves over GPT-40,\nshowing the benefit from stronger reasoning ability.\nHowever, it still lags 10% behind humans.\nLLMs Struggles at Exploiting Longer Plot Win-dows. While many studies have shown the supe-rior capability of LLMs in handling long inputs,\neven up to 100k tokens (Dubey et al., 2024; Hurst\net al., 2024), we find they fail to utilize longer plot\nwindows in our tasks. In both generative and multi-choice QA settings, the performances of LLMs al-most remain stable as the plot window is enlarged.3"}, {"title": "7 Conclusion", "content": "We introduce CHARTOM-QA benchmark that\naims to evaluate machines' ToM capability in re-alistic scenarios with complex social relationships\nand interactions. The benchmark focuses on the\naspect of contextual understanding about the com-plex background stories, which is largely igonored\nin existing benchmarks. The tasks in the bench-mark take the forms of generative and multichoice\nQA. Particularly, for generative QA, we design an\nevaluation protocol that inspects the bonus points\nand penalty existing in model responses. Experi-mental results reveal that current advanced LLMs,\nincluding 01, still lag behind humans and struggle\nat exploiting context to enhance the understanding\nabout the TOM of characters."}, {"title": "Limitations", "content": "Our proposed CHARTOM-QA benchmark is built\nfrom the notes written by readers when reading\nclassic novels. For the creation, as we adopt an\nAI-assisted annotation strategy, the annotation re-sults will be affected by the personal understand-ing from the annotators and GPT-40. Thus, it re-quires the annotators to basically know the story of\nthese books. Besides, the benchmark includes the\nToM dimensions of belief, intention, emotion and\ndesire, which are prevalently studied in previous\nwork. However, there are still other ToM dimen-sions that can be investigated, such as knowledge\nand thoughts.\nWe also design an evaluation protocol that as-sesses model responses by BPC and PR, where\nGPT-40 is used as an evaluator. Such evaluation\nwould cost much if the dataset is very large or the\nstories are excessively long. For those who can-not afford massively calling OpenAI APIs, other\nlocally-deployed open-source LLMs can also be\nused to make assessments."}, {"title": "Ethical Consideration", "content": "The construction of CHARTOM-QA benchmark\ninvolves the usage of public books and notes writ-ten by reading app users. During the process of\nusing these data, we strictly adhere to the guide-lines in Internet Research Ethics. All the books\nare available in the Gutenberg project and their\nChinese-translated version are properly used with\nvalid license. For the notes, we collect and use them\nwith necessary procedures to protect the privacy of\nonline reading app users who wrote the notes. For\nannotators participating in the benchmark construc-tion, they are adequately paid according to their\nworking hours."}, {"title": "Potential Risks", "content": "As our benchmark is derived from classic novels,\nthe stories are essentially fictional and might have\nlimitations of the time. Meanwhile, the notes writ-ten by users reflect their personal interpretation of\nthe story, which may contain content expressing\nnegative emotions. However, please note that the\nconstruction approaches and evaluation protocols\nin this paper can also be used in other type of tasks."}]}