{"title": "COLLAPSE OR THRIVE? PERILS AND PROMISES OF SYNTHETIC DATA IN A SELF-GENERATING WORLD", "authors": ["Joshua Kazdan", "Rylan Schaeffer", "Apratim Dey", "Matthias Gerstgrasser", "Rafael Rafailov", "David Donoho", "Sanmi Koyejo"], "abstract": "The increasing presence of AI-generated content on the internet raises a critical question: What happens when generative machine learning models are pretrained on web-scale datasets containing data created by earlier models? Some authors prophesy model collapse under a \u2018replace' scenario: a sequence of models, the first trained with real data and each later one trained only on synthetic data from its preceding model. In this scenario, models successively degrade. Others see collapse as easily avoidable; in an 'accumulate' scenario, a sequence of models is trained, but each training uses all real and synthetic data generated so far. In this work, we deepen and extend the study of these contrasting scenarios. First, collapse versus avoidance of collapse is studied by comparing the replace and accumulate scenarios on each of three prominent generative modeling settings; we find the same contrast emerges in all three settings. Second, we study a compromise scenario; the available data remains the same as in the accumulate scenario \u2013 but unlike accumulate and like replace, each model is trained using a fixed compute budget; we demonstrate that model test loss on real data is larger than in the accumulate scenario, but apparently plateaus, unlike the divergence seen with replace. Third, we study the relative importance of cardinality and proportion of real data for avoiding model collapse. Surprisingly, we find a non-trivial interaction between real and synthetic data, where the value of synthetic data for reducing test loss depends on the absolute quantity of real data. Our insights are particularly important when forecasting whether future frontier generative models will collapse or thrive, and our results open avenues for empirically and mathematically studying the context-dependent value of synthetic data.", "sections": [{"title": "INTRODUCTION: MODEL COLLAPSE & WHY IT MATTERS", "content": "With each passing day, the internet contains increasingly more AI-generated content (Altman, 2024). What is the impact of this for future of deep generative models pretrained on web-scale datasets containing data generated by their predecessors? Previous work forewarned that such model-data feedback loops can exhibit model collapse, a phenomenon whereby model performance degrades with each model-fitting iteration such that newer models trend towards useless (Shumailov et al.,"}, {"title": "TESTING TWO MODEL COLLAPSE CLAIMS IN THREE NEW GENERATIVE MODELING SETTINGS", "content": "Gerstgrasser et al. (2024) recently made two claims about model collapse:\n1. Many previous papers induced model collapse by deleting past data en masse and training largely (or solely) on synthetic data from the latest generative model, and\n2. If new synthetic data are instead added to real data, i.e., data accumulate over time, then model collapse is avoided.\nThese two claims are important for forecasting the future of generative models because, if correct, model collapse is then less likely to pose a realistic threat since accumulating data over time is a more realistic modeling assumption; as a partner at Andreessen Horowitz elegantly explained, deleting data en masse is \u201cnot what is happening on the internet. We won't replace the Mona Lisa or Lord of the Rings with AI generated data, but the classics will continue to be part of the training data set\u201d (Appenzeller, 2024).\nHowever, these claims have not been tested in three new generative modeling settings recently introduced by prominent work (Shumailov et al., 2024) for studying model collapse:\n1. Multivariate Gaussian Modeling: Multivariate Gaussians are repeatedly fit to data and then used to sample new synthetic data for future Gaussian fitting.\n2. Kernel Density Estimation: Kernel density estimators are repeatedly fit to data and then used to sample new synthetic data for future kernel density estimators.\n3. Supervised Finetuning of Language Models: Language models are finetuned in a supervised manner and then used to sample new synthetic text for future finetuning."}, {"title": "MODEL COLLAPSE IN MULTIVARIATE GAUSSIAN MODELING", "content": "We consider repeatedly fitting multivariate Gaussians to data and sampling from the fit Gaussians. We begin with n real data drawn from a multivariate Gaussian with mean \u03bc(0) and covariance \u03a3(0);\n$X^{(0)}_1,..., X^{(0)}_n \\stackrel{i.i.d.}{\\sim} N(\\mu^{(0)}, \\Sigma^{(0)})$."}, {"title": "MODEL COLLAPSE IN KERNEL DENSITY ESTIMATION", "content": "We next turn to the second generative modeling setting for studying model collapsed introduced by Shumailov et al. (2024): kernel density estimation (KDE). Similar to multivariate Gaussian modeling, we begin with n real data points drawn from an initial probability distribution p(0):\n$X_1^{(0)}, ..., X_n^{(0)} \\stackrel{i.i.d.}{\\sim} p^{(0)}$. We then iteratively fit KDEs to the data and sample new synthetic data from these estimators. In the Replace setting, we fit the KDE to n data samples from the most recently fit model, whereas in the Accumulate setting, we fit the KDE to all data points from all previous iterations, with the number of points growing linearly as n(t + 1):\n$p^{(t+1)}_{Replace}(x) \\stackrel{\\text{def}}{=} \\frac{1}{n h^{(t+1)}} \\sum_{j=1}^n K(\\frac{x - X_j^{(t)}}{h^{(t+1)}})$,\n$p^{(t+1)}_{Accumulate} (x) \\stackrel{\\text{def}}{=} \\frac{1}{n(t+1)} h^{(t+1)} \\sum_{i=0}^t \\sum_{j=1}^n K(\\frac{x - X_j^{(i)}}{h^{(t+1)}})$,\nwhere K is the kernel function and h is the bandwidth parameter. We consider a standard Gaussian kernel. For sampling, at each iteration, we draw n new synthetic data points from the fitted kernel density estimators. We evaluate the performance using the negative log-likelihood (NLL) on real held-out test data; lower NLL indicates better performance. For data, we use four standard synthetic datasets from sklearn (Pedregosa et al., 2011): blobs, circles, moons, and swiss roll."}, {"title": "MODEL COLLAPSE IN SUPERVISED FINETUNING OF LANGUAGE MODELS", "content": "We now turn to the third setting for studying model collapse introduced by Shumailov et al. (2024): supervised finetuning of language models. We begin with an instruction following dataset \u2013 Nvidia's HelpSteer2 (Wang et al., 2024) \u2013 and finetune a language model before sampling new text data from it. We choose Google's Gemma2 2B model (Team et al., 2024) because it is high performing and relatively small. For Replace, we fine-tune the n-th language model only on data generated by the (n \u2212 1) language model. For Accumulate, we instead fine-tune the n-th language model on the starting real data plus all the synthetic data sampled from all previous models; thus, the amount of data for Replace is constant ~ 12.5k, whereas the amount of data for Accumulate grows linearly ~ 12.5k * t. Consistent with our results and with Gerstgrasser et al. (2024), we find that deleting data after each iteration leads to collapse whereas accumulating data avoids collapse."}, {"title": "MODEL COLLAPSE UNDER A FIXED COMPUTE BUDGET", "content": "Thus far, we have focused on two data paradigms: Replace and Accumulate. As discussed in Sec. 2, Replace is unlikely to be an faithful model of reality because we do not delete the internet after pretraining each model, but one might also argue that Accumulate is similarly unfaithful because Accumulate requires that every new model is trained on (linearly) more data and thus requires more compute than its predecessor. Whether this criticism is valid in practice is unclear, since newer models are trained on increasing data (e.g., 1.4T tokens for Llama 1, 2T tokens for Llama 2, 15T tokens from Llama 3) and increasing GPUs (e.g., 2k GPUs for Llama1, 4K for Llama2, 16k for Llama3 (Goyal, 2024)). Nevertheless, for the sake of understanding the space of possible outcomes and predicting likely outcomes for future generative models, we ask and answer:\nDoes model collapse occur when data accumulate but models are trained under a fixed compute budget?\nWe call this data paradigm Accumulate-Subsample because data accumulate but are then subsampled to ensure constant data and thus constant compute at each model-fitting iteration. To study whether model collapse occurs in Accumulate-Subsample, we use the same three generative modeling settings we've studied (multivariate Gaussian modeling, supervised finetuning of language models and kernel density estimation) plus two new generative modeling settings studied by prior work (Mobahi et al., 2020; Dohmatob et al., 2024a; Gerstgrasser et al., 2024): linear regression and pretraining language models on a GPT3.5/GPT4-generated dataset of kindergarten-level text (Eldan & Li, 2023).\nTo explain how linear regression can be used as a generative model, we briefly here and direct the reader to prior work (Mobahi et al., 2020; Dohmatob et al., 2024a; Gerstgrasser et al., 2024) for a more thorough description. We begin with our real covariates $X \\in R^{n \\times d}$ and true linear relationship $w^{(0)}$. Initializing $\\hat{w}^{(0)} = w^{(0)}$, we sample the regression targets as:\n$y^{(t)} \\stackrel{\\text{def}}{=} X \\hat{w}^{(t)} + E^{(t)} \ttext{;} E^{(t)} \\sim N(0, \\sigma^2 I_d)$\nAssuming $X^T X$ is full rank, e.g., $n \\gg d$, we fit the next linear model using ordinary least squares:\n$\\hat{w}^{(t+1)} \\stackrel{\\text{def}}{=} (X^T X)^{-1} X^T y^{(t)}$\nFollowing Gerstgrasser et al. (2024), we additionally pretrain sequences of small variants of common large language models \u2013 GPT (Radford et al., 2019; Brown et al., 2020) and Llama (Touvron et al., 2023a;b) on TinyStories (Eldan & Li, 2023), a synthetic dataset of simple short stories; this combination of models, parameters and data was chosen to faithfully study model collapse in as realistic a setting as possible, subject to our limited computational budget.\nAcross all five generative modeling settings, we find that Accumulate-Subsample's test loss on real data lies between the test losses of Replace and Accumulate (Fig. 4 Center). Specifically, Accumulate-Subsample (Fig. 4 center) exhibits higher test loss than Accumulate (Fig. 4, Right) but lower test loss than Replace (Fig. 4 Left), showing that the fixed compute budget imposes some cost. In a qualitative difference, test losses on real data typically plateaus for both Accumulate-Subsample and Accumulate, whereas test losses for Replace typically diverge in an apparently unbounded manner. These results collectively tell a consistent story: under more realistic conditions, where data accumulate and compute is bounded, model performance on real test data is unlikely to diverge."}, {"title": "CARDINALITY OF REAL DATA VS PROPORTION OF REAL DATA IN MITIGATING MODEL COLLAPSE", "content": "We conclude by turning to a question asked by Gerstgrasser et al. (2024) that, to the best of our knowledge, remains open:\nWhich matters more for avoiding model collapse: the cardinality of real data or the proportion of real data? Relatedly, how does the value of synthetic data for reducing test loss on real data depend on the amount of real data?\nThese questions are highly pertinent to researchers sampling from web-scale data in order to pretrain or finetune language models. We conduct our investigation of this question as follows: First, we perform SFT on the HelpSteer2 dataset for Google's Gemma 2 2B model. We sample 100k completions from the finetuned model and filter for those that are fewer than 512 tokens in length. This leaves us with over 55,000 remaining completions. We aggregate datasets containing various numbers of real and synthetic synthetic data, which are given in Figure 5, and perform SFT on these datasets starting from the original Gemma 2B model. We record and display the final test loss from this process.\nThis experiment provides several insights. First, both the number and proportion of real data have an impact on the test loss following SFT. To assess this, we first transformed the number of real datapoints n as n^{1/2}, in keeping with intuitions from classical statistics on how the log likelihood scales with the number of data points. Then, based on observation of the data, we computed\n$\\log{\\frac{\\text{real data}}{\\text{real data + synthetic data}}}$\nto best capture the relationship between the fraction of real data and the log likelihood. We measured R2 values of 0.59 for the transformed number of real data and 0.34 for the proportion of real data. We then computed F-statistics for the one-term versus two term models involving each of these covariates, which gave us p-values of 6.9 \u00d7 10-25 and 4.6 \u00d7 10-25. These statistics suggest that both the proportion and the cardinality of real data have a statistically significant effect on the test loss, and explain a sizable fraction of the variance in the test loss.\nSecond, we find a difference in the effect that synthetic data has on test loss in high versus low real data regimes. In our experiments specifically, when the number of real data is 1024 or lower, we find that there is an small but non-zero amount of synthetic data that improves the test loss when it is included. This suggests that practitioners fine-tuning with insufficient amounts of real data should consider supplementing with synthetic data to improve model quality.\nOn the other hand, when real data are plentiful, we find that more synthetic data almost always harms final model quality when the number of real data is held constant. In some cases, datasets containing only real data prove to be more valuable than datasets that contain ten times more real data mixed with synthetic data.\nAlthough these results are preliminary, they raise some interesting questions about the role of synthetic data in SFT that deserve further exploration. In some of our experiments, we achieve better results"}, {"title": "DISCUSSION", "content": "Our work sought to clarify and unify understanding of model-data feedback loops. We demonstrated in three new generative modeling settings that accumulating data over time avoids model collapse, whereas replacing data over time induces model collapse. We then demonstrated in five generative modeling settings that even when each model is trained on a fixed compute budget with a mixture of real and synthetic data, model performance does deteriorate more, but still tends to plateau. The consistency of these results across different model types and datasets suggests that this distinction is a general phenomenon, and is not specific to any particular model, dataset, or learning algorithm.\nLastly, we explored the value of synthetic data for reducing the test loss on real data and found two different regimes: when real data are plentiful, synthetic data is harmful, but when real data are scarce, there exists an optimal amount of synthetic data that are helpful.\nIn our view, the data paradigm in which synthetic data accumulates from a host of models in conjunction with a constant influx of real-world data is more realistic. Under such dynamics, where new synthetic data are added to existing real and synthetic data, model collapse appears unlikely. Our experiments take a pessimistic viewpoint, in the sense that our experiments pay no attention to the quality of data, whereas in practice, engineers heavily filter data based on various indicators of data quality, e.g., (Brown et al., 2020; Lee et al., 2023; Wettig et al., 2024; Penedo et al., 2024; Li et al., 2024; Sachdeva et al., 2024); for a recent review, see Albalak et al. (2024)."}, {"title": "FUTURE DIRECTIONS", "content": "An especially interesting future direction is how to combine synthetic data generation with filtering techniques to enable performant and efficient pretraining at scale using synthetic data. As we saw in kernel density estimation (Fig. 2) and in language model pretraining on TinyStories (Fig. 4), training on accumulating real and synthetic data can yield lower loss on real test data than training on real data alone. Identifying under what conditions, and why, this is possible is a tantalizing prospect.\nOur results in Section 4 suggest that removing low-quality synthetic data from model training sets can improve test loss more than gathering additional high-quality data. Developing efficient removal techniques for detrimental data would streamline the model fine-tuning process and produce better alignment. Future work that further studies and quantifies the harm done by lower-quality synthetic data across broader settings could greatly improve the efficiency of model training."}]}