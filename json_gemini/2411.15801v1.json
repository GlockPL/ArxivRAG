{"title": "A review on Machine Learning based User-Centric Multimedia Streaming Techniques", "authors": ["Monalisa Ghosh", "Chetna Singhal"], "abstract": "The multimedia content and streaming are a major means of information exchange in the modern era\nand there is an increasing demand for such services. This coupled with the advancement of future wire-\nless networks B5G/6G and the proliferation of intelligent handheld mobile devices, has facilitated the\navailability of multimedia content to heterogeneous mobile users. Apart from the conventional video,\nthe 360\u00b0 videos have gained significant attention and are quickly emerging as the popular multimedia\nformat for virtual reality experiences. All formats of videos (conventional and 360\u00b0) undergo processing,\ncompression, and transmission across dynamic wireless channels with restricted bandwidth to facilitate\nthe streaming services. This causes video impairments, leading to quality degradation and poses chal-\nlenges for the content providers in delivering good Quality-of-Experience (QoE) to the viewers. The\nQoE is a prominent subjective measure of quality, which has become a crucial component in assessing\nmultimedia services and operations. So, there has been a growing preference for QoE-aware multimedia\nservices over heterogeneous networks with a need to address design issues like how to evaluate and\nquantify end-to-end QoE. Efficient multimedia streaming techniques can improve the service quality\nwhile dealing with dynamic network and end-user challenges. A paradigm shift in user-centric multime-\ndia services is envisioned with a focus on Machine Learning (ML) based QoE modeling and streaming\nstrategies. This survey paper presents a comprehensive overview of the overall and continuous, time\nvarying QoE modeling for the purpose of QoE management in multimedia services. It also examines the\nrecent research on intelligent and adaptive multimedia streaming strategies, with a special emphasis on\nML based techniques for video (conventional and 360\u00b0) streaming. This paper discusses the overall and\ncontinuous QoE modeling to optimize the end-user viewing experience, efficient video streaming with\na focus on user-centric strategies, associated datasets for modeling and streaming, along with existing\nshortcoming and open challenges.", "sections": [{"title": "1. Introduction", "content": "The popularity of video data and video streaming services has increased exponentially in the re-\ncent years. Numerous multimedia applications that include video teleconferencing, video streaming"}, {"title": null, "content": "and video-on-demand will dominate the next-generation wireless networks due to the widespread use of\ninternet. Besides the use of conventional video format, 360\u00b0 videos are being used for the augmented\nreality (AR) and virtual reality (VR) applications that are gaining immense popularity. The emergence\nof the Metaverse has led to the proliferation of AR and VR in the field of entertainment. In such appli-\ncations the 360\u00b0 videos are the preferred multimedia format that provide immersive viewing experience\nto the end-users.\nVideo delivery has emerged as a major component in the bulk of the present day's internet traffic\n(more than 70%) and is expected to grow even further. According to Ericsson reports [1], globally the\naverage data consumption per smart phone is expected to exceed 21 GB per month by the end of 2024.\nParticularly, video devices will raise the volume of existing traffic, accounting for 74% of total traffic by\nthe end of 2024. On a global scale, the average monthly consumption of video streaming will amount\nto 16.3 GB. The UltraHigh Definition (UHD), or 4K, and UHD-2 (8K) video streaming has increased\nthe impact of video devices on traffic because the bit-rate of 4K video, which is 15 to 18 Mbps, and the\nbit-rate of 8K, which is 20 to 26 Mbps, is more than twice/thrice that of High Definition (HD) and nine\ntimes that of Standard Definition (SD) videos. Sixty-six percent (66%) of all new flat-panel TV sets are\npredicted to be UHD. As per the latest Ericsson Mobility Report [1], it is estimated that 5G will account\nfor nearly 75% of mobile data traffic by 2029. Furthermore, the number of 5G mobile subscriptions\nis expected to reach a total of 5.6 billion in 2029. Mobile video transmission is undoubtedly a crucial\nservice provided by the 5G mobile networks, and will continue to be so in the upcoming beyond 5G\n(B5G) era. Service providers are under immense pressure to enhance Quality of Service (QoS) due to\nthe exponential growth in wireless data utilization (primarily multimedia). Increasing video traffic over\nwireless networks increases the demand for superior multimedia content delivery.\nThere is an increasing demand for multimedia applications. Video content streaming, exchange and\nsharing of video-based information are becoming more and more popular among a large number of\nsubscribers. This has compelled service providers to deliver video content of superior quality. Thus,\nthere is an increasing need to ensure that users enjoy a higher Quality of Experience (QoE). In our\nday-to-day lives, we rely heavily on video content sharing through video calling apps and social media\ncontent uploads (in Facebook, Instagram). The majority of popular online services stream video to\nheterogeneous devices over bandwidth constrained communication networks.\nThe limited bandwidth and unreliable channel conditions pose further challenge. In order to meet\nthe requirements of the users, high data-rate transmission is required. To facilitate the transmission of\nHD videos at an increased data rate, it is imperative to employ a system that possesses the capability to\nadapt its configuration based on the prevailing network conditions. A feasible solution is the presence of\nfeedback mechanism between the server and the client, which enables the regular updating of the channel\nstate matrix and other user-centric feedback factors at the server. Recently, DASH [2, 3]-Dynamic\nAdaptive Streaming over HTTP\u2014 has become the industry standard for online video streaming. For\nDASH systems, a variety of rate adaption techniques are suggested to provide video quality in accordance\nwith the network throughput."}, {"title": null, "content": "In AR/VR applications, the seamless streaming of 360\u00b0 video content there is a need for greater\nbandwidth and reduced latency compared to existing methods of 2D video delivery. Meeting the band-\nwidth needs becomes increasingly challenging when streaming the same content to several VR clients. A\nconventional setup for viewing 360\u00b0 video involves a user engaging with the scene via a Head-Mounted\nDisplay (HMD) device such as the Oculus Rift, Samsung Gear VR, HTC VIVE, Google Cardboard,\nand Daydream. The 360\u00b0 video adaptation can be done in a manner similar to conventional videos,\nas in DASH. A number of research solutions that aid in facilitating an immersive visual experience\nof 360\u00b0 video include viewport-dependent/independent and tile-based adaptive streaming. Viewport\nindependent solutions often lead to wastage of network resources as entire content is streamed to the\nusers irrespective of their viewport position."}, {"title": "1.1. Survey novelty and contributions", "content": "This article provides a state-of-the-art survey on the existing solutions for predicting video quality\nand the methods and strategies that can be used to improve the streaming experience. The major\ncontributions of this survey are as follows:\n1.  To provide a review of different existing and emerging video quality assessment models that\nforecast the video QoE over short duration.\n2.  To provide an overview of continuous, time varying video QoE estimation models that are partic-\nularly useful in streaming sessions.\n3.  Survey on efficient user-centric multimedia streaming techniques that have been developed so far\nfor DASH-based streaming, and 360\u00b0 immersive streaming applications.\n4.  A thorough analysis of the most recent Machine Learning (ML)-based QoE prediction models and\nadaptive streaming techniques.\n5.  To provide a list of open source datasets that are accessible publicly for QoE modeling and\nadaptive streaming of 2D and 360\u00b0 videos.\n6.  The summary of findings, list of open challenges and future scope are highlighted.\nThis review provides an analysis of the recent and developing techniques for user-centric multimedia\nstreaming and a thorough examination of the latest ML-based models for predicting QoE. The structure\nof the survey is as follows: Section 2 starts with a discussion on QoE definition, assessment method-\nologies, need for QoE modeling and correlation existing between Video Quality Assessment (VQA)\nmeasures and subjective video quality. In Section 3, we briefly introduce the QoE driven, intelligent\nand adaptive multimedia streaming. In Section 4, we present evaluation metrics and key considerations\nfor ML-based multimedia streaming. In Section 5, we discuss the various state-of-the-art models for\noverall and continuous, time-varying QoE prediction. Section 6 presents an elaborate review of the\nintelligent and adaptive streaming techniques for 2D and 360\u00b0 videos. Section 7 contains several video\nquality databases and performance metrics that can serve as a helpful tool for future researchers in de-\nsigning and validating the models. These include the subjective datasets, network traces, 360\u00b0 viewport"}, {"title": null, "content": "traces, head movement, and eye tracking datasets as well as different performance metrics to evaluate\nthe effectiveness of the QoE prediction models. In Section 8, we identify the issues/ challenges that\nneed to be addressed in the future research. We provide a concise summary of our observations and\nconclude the paper in Section 9."}, {"title": "2. Need for video QoE Modeling in user-centric multimedia streaming", "content": null}, {"title": "2.1. Video QoE: Definition and assessment methodologies", "content": "According to the European Union (EU) Qualinet Community [4], QoE is defined as \u201cthe degree of\ndelight or annoyance of the user of an application or service. It results from the fulfillment of his/her\nexpectations with respect to the utility and/enjoyment of the application or service in the light of the\nuser's personality and current state\u201d. As per International Telecommunications Union (ITU), \u201cQoE is\ndefined as the overall quality of an application or a service as perceived subjectively by the end user\u201d\n[5]. Several factors influencing QoE include- System Influence Factors (IFs), Human IFs, Context IFs,\nand Content IFs. Fig. 1 shows different factors influencing QoE. System IFs are largely concerned\nwith technical elements of quality. Human IFs include, but are not limited to, a user's physical and\nmental condition, state of mind, memory, and focus, as well as their needs from the service, recency\neffects, prior use of the application, and more. Factors like location, end-user surrounding, period of\nthe day, merely casual surfing, service intake period (off-hours, peak hours), are considered as context\nIFs. The content IFs focus on the content's individual traits that include content-related details about\nthe service or application under examination."}, {"title": "2.2. Need for video QoE modeling", "content": "As videos pass through multiple processing stages before reaching the end-users, the effect of most\nof them is to degrade the video quality. Videos can have significant distortions at several stages, such as\nduring processing (image acquisition), compression (encoding), or transmission. At most of the stages,\nvideo quality deteriorates. Fig. 3 presents the flow diagram of video transmission process which shows\nthat the videos are subject to several processing stages before and in the course of delivery to viewers"}, {"title": null, "content": "that results in potential loss of video quality. Recent developments in video coding and compression\nenable customers to access high-quality video services. The content providers use several compression\ntechniques to offer affordable services. Video encoding causes distortions due to compression. The\napplication of block-based coding and motion compensation techniques by most video coding and com-\npression standards subjects the decoded videos to one or more compression artifacts. The compression\nstandards, e.g., MPEG 2, MPEG 4, H.263, H.264/Advanced Video Coding (AVC), H.265/High Effi-\nciency Video Coding (HEVC), and VP9 exhibit blocking, ringing, and blurring artifacts [20]. Spatial\nartifacts that frequently result from encoding comprise false contouring, mosaic patterns, and contrast\ndistortion. In addition, videos often experience significant temporal artifacts, primarily resulting from\ntransmission across communication networks. Video transmission via wireless or IP networks leads to\npacket loss or corrupted frames that suffer from temporal artifacts [21], like jitter, additive noise, and\nmotion compensation mismatch, which affect individual frames.\nThis is even more challenging incase of 360\u00b0 videos. There are several issues associated with efficient\nencoding and processing of 360\u00b0 videos [22, 23]. It is necessary to project the 360\u00b0 videos onto a flat\nsurface because most filters and coding tools are based on 2D images. At every stage of the 360\u00b0 video\nprocessing pipeline, distortion may be introduced, starting with the acquisition of images. Omnidirec-\ntional images and videos are typically stitched from numerous cameras [24], which can incorporate a\nvariety of peripheral problems. Typical issues include loss of information, misaligned edges, temporal\ncoherence, ghosting, camera jittering, dominant foreground objects traveling across views [25], and dif-\nferences in exposure that are most pronounced at the poles. Temporal discontinuities can also occur\nin the videos, like objects appearing and vanishing. Most cameras fail to capture these, while some of\nthem are subsequently reconstructed in the post-processing stage.\nHEVC allows independently decodable tiling of 360\u00b0 videos with less overhead, allowing adaptive\nstreaming according to the user's Field Of View (FoV).\nAfter acquisition of 360\u00b0 video frame, it must be transformed into a planar representation meant for\nencoding and storage. The Equi-Rectangular Projection (ERP), is the most prevalent projection for\n360\u00b0 video. It divides/cuts the visible sphere into several sets of rectangles all sharing the same solid\nangle. The ERP is used in studies in [26], [27], [28]. This projection is inefficient due to distortion at\nthe poles, since more pixels are used to encode the poles compared to the equator. As viewers mostly\npay attention in the vicinity of the equator, the poles tend to fall beyond the FoV. The Cubic Mapping\nProjection (CMP) [29] is another type of projection in which a cube is built around the sphere with\nrays radiating outward from the center. The projection mapping is the result of each ray intersecting\nwith only one point on both solids' surfaces. Compared to the ERP, the CMP results in less geometric\ndistortion at the poles than at the edges/corners of a face. The CMP reproduces a sphere better towards\nthe middle of each face, making this intuitive. In [30], a hybrid version consisting of equi-rectangular\nCMP is used to improve the coding efficiency. The projection uses a mapping function and accomplishes\na higher level of uniform sampling, thus minimizing the presence of boundary artifacts."}, {"title": null, "content": "Few other projections include- Polar square projection [32] (similar to barrel projection, but maps"}, {"title": null, "content": "the poles to squares), Octagonal [33] projection (can reduce oversampling areas and assemble points into\nan octagon that can be rearranged into a rectangle prior to encoding), Rotated Sphere Projection (RSP)\n[34] (unfolds the sphere under two rotation angles and sutures it in the form of a baseball, improving\ncoding efficiency), and offset projection [35] (higher number of pixels are used to encode areas near the\nestimated gaze direction, whereas areas at greater angles from it are compressed more tightly; thus,\nsaving bandwidth).\nOn generating the planar representation of the 360\u00b0 video, tiling (dividing into tiles) is done, which\nis a step in the encoding process that can have a considerable effect on the streaming performance. For\neffective compression of 360\u00b0 videos, the projection and tiling strategy substantially affect the intensity\nof geometric distortion. It is possible to independently encode the Quality Emphasized Region (QER)\nof a video at a higher quality compared to the rest parts [36]. With Motion Constrained Tiles (MCT),\ntiled video segments can be efficiently encoded and decoded independently [22, 37]. The work in [23]\npredicts the FoV tiles and viewport with high accuracy by using Long Short Term Memory (LSTM).\nIt is necessary to develop methods that leverage ensemble learning [38] to enhance viewport prediction\naccuracy and assign high resolution to tiles where a user's viewpoint might appear. Determining the\nright tile size is necessary for conserving bandwidth. The videos can be encoded depending on the\npopular viewing areas, as in Macrotile [39]. It is critical to identify such larger viewing areas and adjust\nthe macrotiles as per random head movements. This can maximize the quality and reduce the data\nto be downloaded, resulting in bandwidth and energy savings. Still, the user's immersive experience\nwill be impacted by any tiles that might be missed while streaming. Thus, video quality needs to be\nmonitored, assessed, and improved."}, {"title": "2.3. VQA/IQA correlation with subjective video quality", "content": "Computing the correlation coefficient between the objective metrics and subjective video qual-\nity assessment scores indicates/helps assess the relationship between objective and subjective ratings\n(MOS/DMOS). The correlation coefficient indicates the dependency of DMOS on a given objective\nscore. Correlation coefficient gives the statistical relationship between PSNR and DMOS, SSIM and\nDMOS, MSSIM and DMOS, Spatial MOVIE and DMOS etc. The computed correlation coefficient\nvalues are depicted in Table 3 for the LIVE [31] dataset. Fig. 4(a) shows the correlation (SROCC) ex-\nisting between video quality of frames at current instant and the past instants for videos v1 to v6 shown\nin Fig. 4(b) from [48, 49, 50]. The correlation gradually decreases for a past frames is further away in\nthe timeline. This correlation is useful for building the continuous, time varying QoE models [51].\nThe video QoE modeling and user-centric multimedia streaming with special emphasis on ML are\nless considered in many previous research works [40]-[47]. Table 4 provides a comparative summary"}, {"title": null, "content": "of our article with respect to existing surveys in literature. In this survey we holistically cover the\nML-based QoE modeling, adaptive and ML-based streaming, as well as bit-rate prediction methods\nalong with the prevalent datasets used in this domain for the conventional and 360\u00b0 video applications."}, {"title": "3. User-Centric Multimedia Streaming", "content": "This section discusses adaptive streaming of multimedia content, wherein the client has complete\ncontrol over the streaming session and can possibly adapt the multimedia stream to its context, such\nas network conditions, device capabilities, perceptual quality, etc. The adaptive multimedia streaming\nsolutions employ an explicit adaptation loop/logic where clients perform different measurements and\npush the information towards the server using sophisticated schemes/algorithms."}, {"title": "3.1. QoE driven multimedia streaming", "content": "In recent times, there has been a notable worldwide increase in the use of multimedia streaming\napplications. The prevailing major participants in the present global market for these kinds of appli-\ncations are the Akamai Technologies, Netflix, Apple Inc, Amazon Web Services, and Hulu. The rapid\nevolution of communication networks and widespread usage of smartphones and smart portable devices,\nwith enhanced processing capabilities, facilitate seamless access to multimedia content."}, {"title": null, "content": "DASH is an over-the-top wireless streaming technology that is prominent and effectively enables\nthe adaptability of content delivery in response to changing network conditions. Fig. 5 depicts the\nvisual representation of the streaming concept. The fundamental concept behind DASH is to break up\nthe multimedia content into small chunks. To have various interpretations, multiple sets of parameters\n(i.e., bit-rate, quantization parameter (QP), framerate, resolution) are utilised to encode each chunk.\nThe DASH server hosts the video segments (of certain duration, generally 2, 4, or 10 seconds), where\neach segment is encoded to have various representation of quality levels. The obtained representations\nare logged in the Media Presentation Description (MPD) file that gives an index for the listed media\nsegments at the server.\nDASH effectively manages diverse network conditions through the dynamic adjustment of video\nparameters. The client-side of DASH is responsible for monitoring various network parameters such as\nnetwork conditions and buffer size. Based on this information, the client-side determines the appropri-\nate representation of the media chunk to be played next. DASH depends on HTTP and Transmission\nControl Protocol (TCP) that ensures reliable delivery of data to intended destination. The acknowl-\nedgement between the sender and receiver occurs when the receiver notifies the sender on the successful"}, {"title": null, "content": "reception of packets, as well as any instances of lost or erroneous packets, prompting the sender to\nre-transmit them.\nTransmission protocols play an important role on QoE performance in video streaming applications.\nBoth TCP and User Datagram Protocol (UDP) are widely utilized for live video streaming. Never-\ntheless, numerous live streaming industries are adopting UDP for high-motion videos that typically\npossess more intricate temporal information. The end-to-end delays associated with UDP are far lower\nthan those of TCP, which is essential for events like live sports. UDP doesn't use handshakes, delivery\nassurances, or duplicate protections to make sure data is correct. Instead, it relies on fundamental\nmechanisms and checksums to maintain data integrity. The likelihood of certain video distortions,\nincluding frame loss and flicker, is increased when UDP is employed. Reliable Multidestination Data\nTransport Protocol (RMDT) [52], an UDP based-transport protocol has proved to be more optimal for\ncertain point-to-multipoint streaming scenarios. RMDT is developed to mitigate common IP network\nimpairments, including packet losses, latency, and jitter, to guarantee reliable transmission. RMDT\nmay serve as a viable alternative to TCP and UDP in future high-bandwidth applications.\nThe latest version of dash.js [53], v 5.0, is a free, open source MPEG-DASH player that functions\nas a reference client and is useful for academic purposes. The other commercial players in the market\ninclude Apple's AVPlayer, Shaka Player, and HLS.js. The dash.js provides a number of functionalities\npertaining to adaptive media streaming. This encompasses the playback of both dynamic and static\ncontent via DASH and Smooth Streaming formats. The period transitions operate differently in dash.js,\npermitting codec alterations, whereas, due to single presentation in Shaka Player, the codecs may remain\nunchanged. The dash.js v 5.0 supports BOLA [54], buffer [55] and throughput based ABR algorithms.\nThe codecs govern the multimedia data representation format in adaptive video streaming ap-\nplications. There has been an ongoing effort towards developing next-generation coding solutions.\nThe AVC/H.264 is a popular codec that uses a 16x16 macroblock configuration for frame encoding.\nHEVC/H.265 is a more recent codec that reduces video bitrate by around 50% relative to AVC/H.264\nwhile maintaining similar subjective quality. The enhanced coding efficiency of HEVC facilitate the 4K\nvideo streaming with superior fidelity, high dynamic range (HDR), and wide chromatic gamut (WCG).\nHEVC/H.265 provides a tiling feature for optimized video streaming, utilizing a 64x64 Coding Tree\nUnit (CTU) structure to encode each tile. Thus, attaining a superior compression ratio compared to\nAVC/H.264. The VVC/H.266 is the latest codec that has a coding efficiency of ~ 50% higher for similar\nsubjective video quality compared to HEVC, particularly for HD and UHD video resolutions and ~ \n75% higher than AVC [56]. The new features in the latest VVC were developed to carry out adaptive\nstreaming with resolution variations, 360\u00b0 immersive video and ultralow-latency streaming.\nVideo quality measurement holds significant importance for video service providers as it pertains to\nenhancing service delivery and ensuring satisfactory performance under typical network impairments.\nAccording to a study conducted by Conviva [57], network operators have experienced significant financial\nlosses as a result of inferior streaming quality. Hence, network providers strive to enhance streaming\nquality and optimize the whole viewing experience. The network-related video impairment examination"}, {"title": null, "content": "is of the utmost importance due to its detrimental impact on QoE. Different methods for evaluating\nvideo quality contribute to the monitoring of quality, thereby ensuring the fulfilment of QoS standards\nand enhancing the overall performance of the system.\nThe success of a video streaming service or multimedia application is reliant upon the level of\ncontentment experienced by its end-users [58]. Eventually, it is humans that benefit from the utilisation\nof these services. The perceptual expectations of end consumers are consistently increasing with the\nhope of superior quality. QoE depends on many things, such as the analysis of the observer, the\ntype of service, the variety of user tools, and the length of the video. QoE is the end-user's overall\nimpression or appreciation of a service. In [59], a survey investigated QoE to be the most desirable user\nchoice when it comes to video delivery, surpassing other factors such as the type of content, ease-of-use,\nmobility, timing, and sharing. QoE indicates the extent of a viewer's comprehensive perspective and\ncontentment with various aspects, including the video content, communication networks, service quality,\nand environmental factors. Therefore, enhancing QoE and satisfying the requirements for superior video\nquality has emerged as a key goal for content providers, academicians, and video streaming companies.\nDue to persistently bad network conditions, video data in the buffer depletes, owing to late arrival\nof video packets, causing playback to stall, popularly known as rebuffering. The frequent appearance\nof rebuffering events introduces impairments that have a detrimental effect on the QoE. The frequency\nand length of stalling, the amplitude and frequency of quality transitions, as well as their temporal\npositions, are some of the aspects that are taken into consideration for impairment measurements. The\nestimation of QoE is of utmost importance in order to optimize the delivered quality. Furthermore,\ncompression as well as video bit-rate implemented by DASH, results in degradation of customer's QoE.\nThe adoption of the rate adaptation approach results in a persistent variation in perceptual quality\nover time, often referred to as time-varying quality [60]. The resultant QoE is influenced by a combi-\nnation of rate adaptation and rebuffering events. Hence, the video quality encountered by end users\nthroughout a streaming session is characterised by a dynamic variation over time. The continuous\nassessment of QoE in a streaming session poses a significant challenge due to the presence of non-linear\nrelationships among several elements (for instance, video quality, stalling and frequent bit-rate adap-\ntion) that influence QoE. In addition to spatial distortions, the quality of streaming video exhibits\nintricate temporal correlations. Continuous monitoring of QoE throughout a streaming session is cru-\ncial for effectively managing shared resources among users and maximizing the perceived video quality.\nMoreover, it has the potential to mitigate the loss of quality by effectively adjusting the video bit-rate\nat the recipient's end."}, {"title": "3.2. QoE-aware 360\u00b0 video streaming for VR/AR", "content": "The 360\u00b0 video transmission provides viewers with an immersive experience and is a fundamental\ncomponent of numerous applications, including Metaverse. The 360\u00b0 video content offers AR/VR\napplication users an immersive experience through the usage of HMD. Users have the ability to rotate\ntheir heads in any direction and maintain a seamless, uninterrupted view of the surrounding with these"}, {"title": null, "content": "360\u00b0 videos. Fig. 6 displays the parameters indicating the direction of head navigation for the users of\n360\u00b0 videos. Fig. 6(b) and 6(c) depict the user's viewport and the equirectangular projection of 360\u00b0\nvideo content, respectively. In equirectangular projection, the sphere is spread out on a flat surface like\na cylinder on a 2D sheet.\nThere is a growing concern over the safety and visual comfort while viewing VR/AR content. A\nfew studies [61], [62] have reported a range of symptoms including headaches, trouble concentrating,\nand dizziness, that attribute to VR sickness, also termed as cybersickness. Contributing factors to cy-\nbersickness include optical flow, VR fidelity, user interaction (e.g., navigation methods, controllability),\nage, gender, VR experience, FoV, latency, and HMD types. Chattha et al. [63] empirically evaluated\nmotion sickness in VR.\nWe study the user head orientations while viewing a 360-degree video using a head mounted stream-\ning device for a set of 55 independent viewings, based on the dataset [64, 65, 23]. Fig. 7(a) shows the\nnumber of users watching the specific tile numbers based on their FoV during the viewing experiment.\nCertain tiles (57-64) are the least viewed, while others (45-54) are viewed by most of the users. Fig 7(b)\nand Fig. 7(c) show the bit-rate and PSNR (respectively) of individual tiles of the 360\u00b0 video for low\n(QP=15) and high (QP=35) QP values. A low QP tile has a high bit-rate and a high PSNR than a\nhigh QP one. However, these values are not the same for all the tiles of the video at the same QP value,\nmotivating FoV based bit-rate adaptation for efficient 360\u00b0 video streaming.\nAt higher resolutions, these videos require an exceptionally high bit-rate to deliver an immersive\nexperience [23]. Excessive bandwidth is wasted while delivering portions out-of-viewport that the end-\nuser never watches. It is difficult to accurately predict the specific content that would interest the\nviewer in the future playback. This is due to the limited exposure of users to a few 360\u00b0 videos in the"}, {"title": null, "content": "past and their fluctuating psychological states and emotional conditions during each playback. Users\nmight adhere to entirely different content depending on moods and mental state. Therefore, it is crucial\nto develop effective strategies for transmitting 360\u00b0 videos over resource-limited wireless networks while\noptimizing the viewing experience. Using AR and VR representations, the 360\u00b0 videos aid in simulating\nan immersive experience of the real world.\nThe most important challenge for successful multimedia streaming is in ensuring the perceptual\nsatisfaction/ pleasure of the end-users. Multiple users may demand different immersive experiences\nsince user equipments' ability for FoV prediction might vary depending on various FoV prediction\nmethods. This necessitates the development of novel approaches for synchronous and asynchronous\nframe structures comprising a basic tier and an enhancement tier for video transmission in multiuser\ncellular networks, wherein the basic tier is utilized to boost resilience against FoV prediction failures\n[66]. The users tend to quit the streaming session if the viewing experience falls below a certain\nthreshold level. Accurate QoE estimation/ prediction can help in adapting the content transmission to\nenhance the viewing experience of the end users. So, a lot of effort is directed towards accessing the\n360\u00b0 video quality and improving the immersive experience. The overall video quality is assessed for\ndifferent segments of the video with short durations ranging from 5-10 seconds. The assessment of video\nstreaming QoE in a continuous manner (on a per-frame basis) is essential for the purpose of regulating\nthe degradation in video quality throughout the entire streaming session. The use of optimized ML\nmodels helps in accurately evaluating the impact of several impairments on the viewers perceptual QoE\nusing different input features (VQA objective metrics and impairment factors). Fig. 8 shows the general\ndesign of a ML-based QoE predictor."}, {"title": "3.3. Intelligent and adaptive multimedia streaming", "content": "The DASH server delivers videos at different bit-rates depending on user needs [67]. The display\ndevices are capable of adapting various levels of quality based on the bandwidth that is available.\nConsequently, this can result in the occurrence of compression or scaling artifacts. For instance, video\ncontent that has been encoded at lower resolution may be upscaled to a significantly higher resolution\non the viewing device. If the video bit-rate is lower than the available bandwidth, it not only leads\nto smooth playing but also results in the inefficient use of resources that could otherwise be allocated"}, {"title": null, "content": "towards improving the video quality.\nThe standardization of DASH does not mention the execution of the adaptation strategy. For exam-\nple, how a client can adaptively select the video quality according to the present network statistics and\nother factors. In order to switch between multiple streams, a controlling mechanism can be configured\neither at the server/ client side that dynamically predicts and subsequently requests an optimal video\nsegment representation. This request is made considering several factors like network conditions, buffer\noccupancy, requester's device features etc.\nRecent studies [54, 68, 69, 38, 39", "72": "demonstrates\ntheir viability as a solution in scenarios where prior assumptions about the operational environment are\nnot required.\nA 360\u00b0 video is a bounding sphere that includes all of the surrounding content. A higher bit-rate\nis needed for smooth streaming at high resolution (\u22654K) and high frame rate (\u226560 fps) to provide"}]}