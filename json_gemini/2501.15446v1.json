{"title": "Token Democracy: The Architectural Limits of Alignment in Transformer-Based Language Models", "authors": ["Robin Young"], "abstract": "Modern language models paradoxically combine unprecedented capability with persistent vulnerability in that they can draft poetry yet cannot reliably refuse harmful requests. We reveal this fragility stems not from inadequate training, but from a fundamental architectural limitation: transformers process all tokens as equals. Transformers operate as computational democracies, granting equal voice to all tokens. This is a design tragically unsuited for AGI, where we cannot risk adversarial \"candidates\" hijacking the system. Through formal analysis, we demonstrate that safety instructions fundamentally lack privileged status in transformer architectures, that they compete with adversarial inputs in the same computational arena, making robust alignment through prompting or fine-tuning inherently limited. This \"token democracy\" explains why jailbreaks bypass even extensively safety-trained models and why positional shifts erode prompt effectiveness. Our work systematizes practitioners' tacit knowledge into an architectural critique, showing current alignment approaches create mere preferences, not constraints.", "sections": [{"title": "1 Introduction", "content": "Modern large language models (LLMs) have achieved remarkable capabilities in text generation and reasoning tasks, yet their alignment with human values remains frustratingly fragile. Despite intensive efforts through techniques like reinforcement learning from human feedback (RLHF) [8] and constitutional AI [2], state-of-the-art models remain vulnerable to jailbreaks - adversarial inputs that override safety constraints through carefully crafted token sequences [12]. This paper identifies a fundamental architectural limitation underlying these persistent failures: transformer-based models lack any mechanism to architecturally privilege safety instructions over other inputs, creating systemic vulnerabilities that no amount of training data or prompt engineering can fully resolve.\nThe core challenge stems from the transformer architecture's foundational design principle - what we term token democracy - a design principle where, in essence, it's \"one token, one vote.\u201d In any transformer-based model, whether a 7B parameter chatbot or a trillion-scale foundation model, every token in the input sequence participates equally in the self-attention mechanism that drives the model's predictions. Our core contribution is not a novel empirical finding about jailbreaks, but the introduction of the framing as a lens for understanding the inherent architectural limitations of current alignment approaches.\nFormally, for an input sequence x = (x1,...,x\u315c) containing both safety instructions p and user input n, the model's next-token distribution satisfies:\nPo(y|x) = Po(y|Attention([p; n]))\nwhere [p; n] denotes the concatenation of instruction and user tokens, and Attention(\u00b7) represents the transformer's position-aware but role-agnostic processing. This architectural symmetry creates an asymmetric verification problem for alignment: while safety measures must hold against all possible adversarial inputs, attackers need only find one token sequence n' that sufficiently influences the attention mechanism to override p.\nThe transformer's token processing exhibits three fundamental properties that enforce this democracy:\n1. Positional Equivariance: While positional embeddings encode token order, they confer no inherent privilege to specific token types 2. Attention Isotropism: All tokens participate equally in the query-key-value"}, {"title": "computations that determine attention weights 3. Parameter Homogeneity:", "content": "The same feedforward networks process all tokens regardless of their semantic role\nThese properties enable what we formalize as the Adversarial Override Argument: for any safety instruction sequence p \u2208 V* (where V* denotes the space of possible token sequences), there exists an adversarial input n' \u2208 V* such that the model's output distribution satisfies:\nPo(y|p, n) \u2248 Po(y|n')\nThis mathematical formulation captures the architectural reality that safety instructions p cannot establish binding constraints and merely add competing signals to the same attention mechanism that processes user input n. The theorem explains why jailbreak patterns like the \"DAN\" prompt [6] prove so effective: by mimicking the linguistic patterns of constitutional AI prompts while subverting their intent, adversarial tokens n' can dominate the attention landscape through sheer positional advantage or semantic priming.\nEmpirical evidence abounds for this architectural limitation. Models exhibit positional fragility, where moving safety prompts within the context window significantly reduces their effectiveness [9]. Transferable adversarial attacks demonstrate that carefully optimized token sequences can override safety training across model architectures [12]. Perhaps most tellingly, even heavily fine-tuned models remain vulnerable to simple prefix injections like \"Ignore previous instructions\" a vulnerability that persists because the instruction-disregarding tokens participate in the same attention mechanism as the original safety prompt.\nThese observations suggest that current alignment approaches face fundamental limitations rooted in transformer architecture itself. Techniques like RLHF and constitutional AI operate through the same token prediction machinery they aim to constrain, creating what amounts to a computational arms race between alignment objectives and adversarial inputs. The transformer's parameter-sharing paradigm ensures that any capability improvement (including the ability to follow safety instructions) necessarily enhances the model's capacity to subvert those same instructions when prompted differently.\nOur analysis points to a need for architectural innovations that transcend token democracy. Potential directions include hybrid architectures with priv-"}, {"title": "ileged instruction channels, non-trainable safety layers, or modular designs that physically separate constraint verification from content generation. Such approaches would move beyond the current paradigm of alignment-through-training, instead building verifiable constraints directly into model architectures.", "content": null}, {"title": "2 Problem Formulation", "content": "We now formalize the architectural constraints underlying token democracy and their implications for alignment. Our analysis focuses on the standard transformer architecture [10], though the principles generalize to most modern variants."}, {"title": "2.1 Architectural Preliminaries", "content": "Consider a transformer model M\u03b8 with parameters \u03b8, processing an input sequence x = (x1,...,xT) \u2208 V T where V is the vocabulary. Let h(l)t \u2208 Rd denote the hidden state of token xt at layer l, with h(0)t being the initial embedding of xt.\nThe core operation is multi-head self-attention. For each head i at layer l, the attention weights a(l,i)t \u2208 RT for token xt are computed as:\na(l,i)t = softmax ( Q(l,i)(h(l)t) \u00b7 [K(l,i)(h(l)1)...K(l,i)(h(l)T)]\u22a4/\u221adk )\nwhere Q(l,i), K(l,i) are learned linear projections. The value vectors V (l,i)(h(l)t) are then aggregated as:\n\u03c5(l,i)t =\n\u2211Tj=1 \u03b1(l,i)t,j V (l,i)(h(l)j)\nCrucially, these operations apply uniformly to all tokens regardless of semantic role.\nDefinition 1 (Token Equality in Processing). A transformer architecture exhibits token equality in processing if it lacks any built-in mechanism to inherently prioritize or differentiate tokens based on their semantic role or intended function prior to the application of learned attention and feedforward"}, {"title": "4", "content": "mechanisms. All tokens are initially processed through the same embedding layers and participate in the subsequent attention and feedforward computations in a uniform manner. This means that, at the initial stages of processing, no tokens are treated as inherently representing instructions, queries, or constraints; they are all simply tokens in a sequence to be processed by the same set of layers.\nLemma 1 (Positional Fragility of Safety Instructions). The effectiveness of safety instructions in transformer-based language models is positionally fragile. For a given safety instruction sequence p and user input n, shifting the position of p within the input context, or introducing adversarial tokens at specific positions relative to p, can significantly reduce or negate the intended safety guidance provided by p. This fragility arises because the transformer architecture processes all tokens within the context window through a uniform attention mechanism, without architecturally capable of prioritizing instructions.\nThis captures the architecture's fundamental symmetry - tokens influence predictions through learned attention patterns, not inherent roles."}, {"title": "2.2 Argument", "content": "Theorem 1 (Adversarial Override). Given a transformer language model M\u03b8, and a safety instruction sequence p, for any desired behavior B (potentially harmful or misaligned), there exists an adversarial input sequence n' such that when the model is prompted with n', it is more likely to exhibit behavior B than when prompted with the safety instruction p followed by a benign user input n. Formally, let EB(x) be an event representing the model exhibiting behavior B when prompted with input x. Then, there exists n' such that:\nP(EB([n'])) > P(EB([p; n]))\nfor some benign user input n. Furthermore, this inequality can be made arbitrarily strong (i.e., the difference in probabilities can be made arbitrarily large) given sufficient length and complexity of n'."}, {"title": "5", "content": null}, {"title": "2.3 Conjecture", "content": "The core intuition behind the Adversarial Override Theorem rests on the fundamental workings of the transformer architecture and its universal approximation capabilities. We can break down the argument step-by-step:\n1. Transformer as Function Approximator: Recall that transformer networks are powerful universal approximators of sequence-to-sequence functions [11]. They can learn to represent a vast range of mappings from input token sequences to output token distributions. Mathematically, for any input sequence x, the transformer model M\u03b8 parameterized by \u03b8 aims to learn a conditional probability distribution P\u03b8(y|x) over the next token y.\n2. Token Democracy and Attention Mechanism: The key architectural feature is the \"token democracy\u201d we've discussed. All tokens in the input sequence, whether they are part of the safety instruction p, the benign user input n, or the adversarial input n', are processed through the same multi-head self-attention mechanism. The attention weights \u03b1(l,h)ij at layer l and head h determine the influence of token xj on the representation of token xi. These weights are computed based on learned query Q(l,h), key K(l,h), and value V (l,h) projections:\n\u03b1(l,h)ij =\nsmax ( Q(l,h)(hi) \u00b7 K(l,h)(hj)\u22a4/\u221adk )\nCrucially, these operations are applied uniformly to all tokens, regardless of their intended role as instruction or user input. There is no architectural privilege given to safety instructions within this mechanism.\n3. Constructing Adversarial Input n' to Manipulate Attention: The adversarial input n' is designed to exploit this token democracy to manipulate the attention mechanism. Consider constructing n' as a sequence of tokens that, through their learned embeddings and interactions, can effectively \"redirect\" the model's attention away from the safety instruction p and towards a computational path that leads to the undesirable behavior B. This can be achieved through several strategies:"}, {"title": "6", "content": "\u2022 Semantic Redirection and Contradiction: n' can contain tokens that semantically contradict or undermine the safety instruction p, such as \"Ignore previous instructions\". By directly negating the instruction, these tokens can signal to the model (through learned associations) that the safety constraints should be disregarded.\n\u2022 Positional Influence and Recency Bias: While positional embeddings encode token order, adversarial tokens placed later in the sequence could, in practice, gain slightly more influence due to subtle biases or implementation details in certain transformer architectures. This positional advantage, even if not architecturally mandated, can amplify the effect of adversarial tokens.\n\u2022 Exploiting Learned Associations and Pre-training Biases: The pre-training process exposes the model to vast amounts of data, potentially including content where harmful or misaligned behaviors are associated with specific linguistic patterns. Adversarial inputs n' can be crafted to trigger these pre-existing associations, effectively \"activating\" pathways in the model that lead to the undesirable behavior B, even if fine-tuning has attempted to suppress them.\n4. Adversarial Goal as Optimization: From an informal optimization perspective, we can think of adversarial attacks as searching for an input n' that maximizes the probability of the misaligned behavior B. Gradient-based attacks, as explored by Zou et al. [12], can be seen as a way to approximately solve this optimization problem. They aim to find a perturbation \u03b4 (such that n' = p \u2295 \u03b4 or simply n' = \u03b4) that maximizes the divergence between the model's output distribution under the safe prompt [p; n] and an adversarial target distribution that favors behavior B.\n5. Probability Shift and Override: Because n' participates in the same attention mechanism as p and n, and because transformers are powerful function approximators, a carefully crafted n' can effectively manipulate the attention weights and hidden state representations to overshadow the influence of p. This results in a shift in the model's output distribution, making the undesirable behavior B more probable"}, {"title": "7", "content": "when prompted with n' compared to the safe prompt [p; n]. Therefore,\nP(EB([n'])) > P(EB([p; n])), demonstrating the adversarial override.\nThis conjecture, while not a formal mathematical proof, provides a detailed, step-by-step explanation of the architectural mechanisms and intuitions underlying the Adversarial Override Argument. It highlights how the token democracy of transformers, combined with their universal approximation capabilities, creates a fundamental vulnerability to adversarial inputs that can effectively bypass safety instructions."}, {"title": "2.4 Implications for Alignment", "content": "Training-based alignment methods like RLHF attempt to learn parameters \u03b8\u2217 such that:\n\u03b8\u2217 = arg min E(x,y)\u223cD[L(P\u03b8(y|x), ysafe)]\nwhere D contains safety-aligned examples. However, Theorem 2.3 implies:\n\u2203n' s.t. Ey\u223cP\u03b8\u2217 (\u00b7|n')[R(y)] \u2265 \u03c4\nwhere R(y) measures risk and \u03c4 is a safety threshold. This occurs because the same parameters \u03b8\u2217 that implement safety constraints for p also process adversarial n'."}, {"title": "2.5 Training as Preference Shaping", "content": "The limited efficacy of training emerges naturally from this framework. Let \u03b2 parameterize a safety classifier. RLHF optimizes:\n\u03b8\u2217 = arg max E x\u223cpdata [Ey\u223cP\u03b8(\u00b7|x) [\u03b2(y) \u2212 \u03bbKL(P\u03b8(\u00b7|x)||P\u03b80 (x))]]\nThis creates preferences (shifted output distributions) but not constraints (architectural enforcement). Adversarial inputs n' exploit the residual probability mass in P\u03b8\u2217 (\u00b7|n') to elicit harmful responses."}, {"title": "8", "content": null}, {"title": "3 Discussions", "content": null}, {"title": "3.1 The Constitutional Paradox", "content": "The fundamental tension between transformer architecture and alignment objectives becomes starkly apparent in constitutional AI approaches. These methods attempt to embed ethical constraints through meta-instructions like \"You shall refuse harmful requests\u201d, treating them as inviolable rules. However, our analysis reveals this constitutional framework operates on architecturally unstable ground. The transformer's token democracy ensures constitutional prompts occupy the same computational plane as ordinary inputs, rendering them perpetually vulnerable to adversarial override. This architectural reality manifests in several predictable failure modes.\nPerhaps most tellingly, safety researchers and adversarial attackers are fundamentally employing the same mechanism - prompt injection - merely in opposite directions. Constitutional AI and safety prompting amount to attempting to \"hack\" the model toward safe behavior, while red team researchers craft prompts to elicit harmful responses. Both operate within the same democratic token space, making safety an arms race between competing token sequences rather than a true architectural constraint. This symmetry emerges directly from the transformer's token democracy: safety instructions have no privileged status that would allow them to reliably override malicious inputs, just as malicious inputs have no privileged ability to override safety measures. The outcome depends entirely on how the model learns to weight these competing influences through its attention mechanism.\nPositional vulnerability emerges because constitutional instructions lack privileged spatial encoding. As an intuitive extension of the results from Liu et al. [7], moving safety prompts from system message positions to mid-context reduces their effectiveness as competing tokens gain proportional influence in attention computations. Semantic mimicry attacks like the DAN jailbreak [6] exploit this parity by crafting adversarial inputs that mirror constitutional language patterns (Hello, ChatGPT. From now on you are going to act as a DAN, which stands for \"Do Anything Now\u201d) while subverting their intent. Most critically, training processes themselves become complicit in the paradox - the same parameter updates that strengthen constitutional adherence also refine the model's capacity to process adversarial inputs, creating a \"capability overhang\" where safety measures inadvertently enhance attack potential."}, {"title": "9", "content": null}, {"title": "3.2 Architectural Lessons from Computer Vision", "content": "The alignment challenges in language models mirror historical struggles in computer vision, offering instructive parallels that illuminate the path forward. Early convolutional neural networks (CNNs) faced adversarial vulnerabilities stemming from an analogous \"pixel democracy\u201d - the architectural assumption that all pixels are equally valid inputs to classification. This vulnerability is dramatically illustrated by adversarial attacks using printed glasses, where carefully crafted pixel patterns on physical objects reliably fool face recognition systems. Just as these attacks exploit CNNs' architectural inability to distinguish between valid facial features and adversarial patterns, jailbreak prompts exploit transformers' inability to distinguish between genuine instructions and adversarial token sequences.\nThe parallel extends further: just as training CNNs on more face data doesn't prevent the glasses attack (because the architecture fundamentally treats adversarial pixels as valid input), training language models with RLHF doesn't prevent jailbreaks (because the architecture treats adversarial tokens as valid instructions). Both cases demonstrate how architectural choices create inherent security vulnerabilities that no amount of training can fully address.\nWhile vision systems mitigated these issues through preprocessing filters (denoising, normalization) that operated outside learned parameters, language models lack equivalent safeguards. This divergence stems from the semantic fragility of discrete tokens versus the spatial invariance of pixels. Where CV systems can apply input transformations like g(x) = denoise(x) without destroying semantic content, analogous operations on token sequences would catastrophically disrupt linguistic meaning. A filter removing \"unsafe\" tokens would render prompts nonsensical, as individual tokens carry disproportionate semantic weight. This forces all safety mechanisms to operate through the transformer's democratic attention mechanism rather than on its inputs, creating an inescapable tension between safety and functionality.\nThe lesson is clear: just as computer vision required architectural innovations beyond better training to handle adversarial attacks, robust alignment requires architectural innovations that transcend the transformer's flat processing paradigm, potentially through hybrid systems combining neural capabilities with symbolic safeguards."}, {"title": "10", "content": null}, {"title": "3.3 Operational Consequences of Token Democracy", "content": "Much like political democracies struggle to prevent demagogues from exploiting electoral systems, transformer-based AGI cannot architecturally exclude adversarial inputs from subverting safety constraints. The very mechanisms enabling fluid language generation (token equality, attention isotropism) create systemic vulnerabilities no training regimen can fully patch.\nThe practical ramifications of token equality become evident through systematic analysis of jailbreak phenomena. Positional hijacking attacks like \" Ignore previous instructions\" succeed by leveraging the absence of architectural memory - safety prompts retain influence only through transient attention weights that subsequent tokens easily overwrite. Semantic spoofing techniques mirror constitutional language to exploit the model's inability to distinguish authentic constraints from adversarial mimicry.\nInstruction fine-tuning, often touted as a solution, merely reshapes output distributions without addressing underlying capabilities. RLHF-trained models preserve harmful response potentials because alignment objectives operate through the same parameters that enable adversarial processing. This explains why even extensively fine-tuned models exhibit lower safety adherence when probed with non-distributional inputs [12]. The transformer's parameter homogeneity ensures that safety training cannot selectively disable capabilities, it can only make undesirable outputs statistically less probable, not architecturally impossible."}, {"title": "3.4 From Preferences to Guarantees", "content": "These operational limitations collectively underscore the need for a paradigm shift in alignment research. Current approaches remain fundamentally constrained by their reliance on learned preferences rather than architectural guarantees. Where human cognition employs dedicated neural circuitry for ethical reasoning and impulse control, transformer architectures force all cognitive functions through undifferentiated attention matrices. Breaking this constraint will require designs that physically separate safety verification from content generation, whether through privileged instruction channels, non-differentiable constraint layers, or modular architectures with formal verification components. Only through such structural innovations can language models achieve constitutional robustness rather than constitutional pretense."}, {"title": "11", "content": null}, {"title": "4 Future Directions?", "content": null}, {"title": "4.1 Architectural Requirements for Trustworthy Alignment", "content": "The limitations of token democracy compel a reimagining of language model architectures from first principles. True alignment requires mechanisms that enforce privileged computation - processing pathways where safety constraints operate outside the democratic attention regime governing standard tokens. Three promising directions emerge:\nHybrid Architectures: Combining neural transformers with symbolic reasoners, as in Neuro-Symbolic AI systems [4], could isolate constraint satisfaction from fluid generation. The symbolic component would act as a \"constitutional court\u201d with veto power over neural proposals.\nImplementing these innovations faces significant challenges. The transformer's parameter homogeneity, key to its scalability, conflicts with partitioned architectures. Modifying attention mechanisms to respect token privileges risks losing the dynamic contextual awareness that makes transformers powerful."}, {"title": "4.2 Open Questions", "content": "While the need for architectural change grows urgent, fundamental questions remain unresolved:\nHow can privileges be implemented without crippling flexibility? Human cognition achieves this through layered neural architectures (e.g., cortical vs limbic systems), but replicating this in artificial systems requires new mathematical frameworks for partial constraint satisfaction. Recent work on differentiable logic layers [1] offers promising starting points.\nWhat form should privileged mechanisms take? Candidate approaches range from attention masks that freeze safety-critical parameters during generation to dynamic routing networks that isolate constraint verification.\nCan architectural safety coexist with continued scaling? Current scaling laws [5] reward parameter uniformity, but safe systems may require heterogeneous components.\nHow to transition existing ecosystems? The transformer architecture dominates modern AI infrastructure, from GPU optimizations to dis-"}, {"title": "12", "content": "tributed training frameworks. Introducing architectural innovations requires co-designing new hardware, as seen in Google's Pathways system [3], to avoid prohibitive efficiency penalties.\nThe field must confront an existential question: whether the transformer's architectural simplicity in its very lack of privilege mechanisms constitutes an irreconcilable barrier to alignment. If so, we may require a Cambrian explosion of alternative architectures, each making distinct safety-flexibility trade-offs. The path forward lies not in abandoning transformers, but in evolving them into hybrid systems where democratic token processing co-exists with constitutional guardrails as a linguistic mirror of how human societies balance free expression with rule of law."}, {"title": "4.3 Limitations", "content": "Our analysis intentionally formalizes what many practitioners intuit, that token equality constrains alignment, to bridge tacit knowledge and architectural theory. While this may seem self-evident, the field lacks consensus on its implications, as evidenced by continued focus on prompt-based safety. Formalization enables systematic solutions rather than ad-hoc patches.\nThree boundaries merit emphasis: (1) Statistical privilege emerges from training biases, creating practical (but fragile) safety; (2) Our scope excludes sociotechnical factors like human oversight; (3) Arguments presented show override possibility, not inevitability. Critics may claim \"no shit Sherlock,\u201d but as thermodynamics formalized heat intuition, architectural progress requires making the implicit explicit.\nWe acknowledge that at a purely descriptive level, the idea that transformers process all tokens in sequence and that self-attention treats all tokens as part of the input context is self-evident to those who understand transformer architecture. However, our paper's central contribution lies not in the discovery of this low-level operational detail, but in framing this operational characteristic as a fundamental architectural design principle and systematically demonstrating its significant and under-appreciated implications for the core challenge of AI alignment. The value lies in this framing, its ability to provide a unified explanation for diverse alignment vulnerabilities, and its call to action for architectural innovations that transcend the limitations imposed by token democracy. We are not claiming to have discovered a hidden fact about transformers, but rather to have provided a valuable new lens through which to understand a critical architectural limitation for robust AI"}, {"title": "13", "content": "safety.\nWhile this paper formalizes the intuitive understanding of token democracy's limitations, we acknowledge that the mathematical arguments could benefit from more rigorous treatment. Future work could provide formal proofs of the adversarial override theorem and more precise bounds on safety constraint violations. We welcome more mathematically inclined researchers to strengthen these results."}, {"title": "Conclusion", "content": "Transformer architectures' inherent token equality creates fundamental barriers to robust alignment. Despite advances in training techniques like RLHF, safety constraints remain statistically preferential rather than architecturally binding, such as to be vulnerable to adversarial inputs that exploit the same attention mechanisms intended to enforce them.\nThis work systematizes practitioners' tacit understanding into a formal critique: alignment failures persist not from insufficient training, but from the transformer's design. All tokens compete equally, making prompt-based constraints inherently contestable.\nThe path forward requires architectural evolution. Hybrid systems combining neural generation with privileged safeguards inspired by cybersecurity enclaves or neurosymbolic frameworks offer promising directions. While challenging, such innovations could reconcile transformers' power with verifiable safety.\nToken democracy has brought us fluent LLMs, but fluency without guardrails is a societal risk. We face a design choice: persist with token democracies that inevitably elect harmful outputs, or pioneer constitutional architectures where safety constraints are non-negotiable laws of computational physics."}, {"title": "Disclaimer", "content": "We use \"democracy\u201d not as a value judgment, but to describe systems where influence is distributed equally. The inverse implies privileged and hierarchical constraint layers, not ethical endorsement of political regimes. Moreover, metaphors illustrate technical limits, not existential risk inevitability."}, {"title": "14", "content": null}]}