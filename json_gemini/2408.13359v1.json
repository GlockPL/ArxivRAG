{"title": "POWER SCHEDULER: A BATCH SIZE AND TOKEN NUMBER AGNOSTIC LEARNING RATE SCHEDULER", "authors": ["Yikang Shen", "Matthew Stallone", "Mayank Mishra", "Gaoyuan Zhang", "Shawn Tan", "Aditya Prasad", "Adriana Meza Soria", "David D. Cox", "Rameswar Panda"], "abstract": "Finding the optimal learning rate for language model pretraining is a challenging task. This is not only because there is a complicated correlation between learning rate, batch size, number of training tokens, model size, and other hyperparameters but also because it is prohibitively expensive to perform a hyperparameter search for large language models with Billions or Trillions of parameters. Recent studies propose using small proxy models and small corpus to perform hyperparameter searches and transposing the optimal parameters to large models and large corpus. While the zero-shot transferability is theoretically and empirically proven for model size related hyperparameters, like depth and width, the zero-shot transfer from small corpus to large corpus is underexplored. In this paper, we study the correlation between optimal learning rate, batch size, and number of training tokens for the recently proposed WSD scheduler. After thousands of small experiments, we found a power-law relationship between variables and demonstrated its transferability across model sizes. Based on the observation, we propose a new learning rate scheduler, Power scheduler, that is agnostic about the number of training tokens and batch size. The experiment shows that combining the Power scheduler with Maximum Update Parameterization (\u00b5P) can consistently achieve impressive performance with one set of hyperparameters regardless of the number of training tokens, batch size, model size, and even model architecture. Our 3B dense and MoE models trained with the Power scheduler achieve comparable performance as state-of-the-art small language models. We open source these pretrained models at https://ibm.biz/BdKhLa.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning rate is a critical hyperparameter for deep neural network training. In the context of Large Language Models (LLMs), the cosine learning rate scheduler is the most commonly used strategy. It has been shown to be effective across multiple state-of-the-art models, including Llama 3 (Dubey et al., 2024), Gopher (Rae et al., 2021), etc. However, the cosine scheduler requires pre-defined training step counts to achieve the optimal loss. This results in two main drawbacks: 1) the intermediate training checkpoints are suboptimal, 2) continual training of an existing language model becomes complicated.\nMiniCPM (Hu et al., 2024) proposes the Warmup-Stable-Decay (WSD) learning rate scheduler (illustrated in Figure 1) to address these issues. The WSD learning rate schedule is divided into three phases: 1) warmup phase, linearly increase the learning rate from 0 to peak; 2) stable phase, maintain the learning rate at peak value and training the model for most of the time; 3) decay phase, annealing the learning rate to 0 in a relatively short period. The main advantage of this schedule is that specifying the number of training steps in advance is not required. This is particularly convenient for large runs, as the decay can be applied at any time to observe model performance and decide whether to stop. It also allows for continual learning by default, as training can be resumed from a stable phase checkpoint. Moreover, the data mixture can be changed during the decay phase to increase the ratio of high-quality data. This data curriculum is shown to be effective in several recent language models (Team et al., 2023; Hu et al., 2024; Dubey et al., 2024; Shen et al., 2024)."}, {"title": "2 BACKGROUND", "content": "However, is WSD really agnostic to token count? In our experiments, we found that although the WSD scheduler could, in theory, continue in the stable phase forever, the optimal learning rates are different for different amounts of training tokens. In other words, the optimal learning rate scheduler is tied to the number of training tokens. Thus, the WSD scheduler still faces the same issue as the cosine scheduler: the intermediate and continual training checkpoints are suboptimal if the number of tokens is too different from the original plan.\nFurthermore, deciding the optimal learning rate is still challenging for large-scaling pretraining. As our experiments will show, there is a complicated correlation between hyperparameters, including learning rate, model size, batch size, and number of training steps. The size and training cost of modern LLMs make it impossible to do a hyperparameter search on the target model size and training flops. Researchers have proposed using small proxy models to run hyperparameter searches and predict the optimal hyperparameters of large models from search results (Dey et al., 2023; Hu et al., 2024; Yang et al., 2022). Among these methods, \u00b5Transfer (Yang et al., 2022; 2023) is proposed to facilitate zero-shot hyperparameter transfer between different model sizes. \u00b5Transfer has been successfully applied to several language models, including Cerebras-GPT (Dey et al., 2023), miniCPM (Hu et al., 2024), and AFM (Gunter et al., 2024).\nIn this paper, we first combine the WSD scheduler and \u00b5Transfer to study the learning rate transfer between proxy and large models. Our extensive experiments show that \u00b5Transfer does not provide direct zero-shot learning rate transferability across the numbers of tokens and batch sizes for the WSD optimizer. Instead, the optimal learning rate Nopt satisfies a power-law relation with respect to batch size \u03b2 and number of tokens T:\n$\\Nopt = \u03b2 \u03b1Tb$ (1)\nwhere a and b are power-low coefficients. Furthermore, our experiment confirms the zero-shot transferability across model sizes, different model sizes share very similar coefficients. Inspired by this observation, we propose a new learning rate scheduler, PowerLR, that is agnostic to batch size and token number. It allows direct transfer of the optimal learning rate scheduling across batch size, token numbers, and model size. Thus, the expensive pretraining runs can be trained without specifying the number of training tokens, enabling early stop and continual pretraining without sacrificing convergence."}, {"title": "2.1 MAXIMUM UPDATE PARAMETRIZA\u03a4\u0399\u039f\u039d (\u03bc\u03a1)", "content": "Maximal Update Parameterization (\u00b5P) (Yang & Hu, 2020; Yang et al., 2022; 2023) controls initialization, layer-wise learning rates, and activation magnitudes to ensure analytically stable training, independent of a model's width and depth. In addition to improving training stability, \u00b5P improves the transferability of training hyperparameters from small proxy models to large models, a technique called \u00b5Transfer. The hyperparameter transferability of \u00b5P is theoretically justified for width (Yang et al., 2022) and depth (Yang et al., 2023). Previous work also show empirical evidence for transferability across batch size, training steps, and training sequence length."}, {"title": "2.2 WARMUP-STABLE-DECAY (WSD) SCHEDULER", "content": "MiniCPM (Hu et al., 2024) proposes the WSD learning rate scheduler to divide pretraining into three stages: the warmup stage, the stable training stage, and the remaining decay stage. The WSD scheduler is defined as:\nWSD (n) = \n$\\begin{cases} \\frac{n}{Nwarmup} \\cdot \u03b7 & \\text{if } n < Nwarmup \\\\ \u03b7 & \\text{if } Nwarmup < n \u2264 N - Ndecay, \\\\ f(n, N, Ndecay) \\cdot \u03b7 & \\text{if } n > N - Ndecay \\end{cases}$\n(2)\nwhere n is the current number of steps, \u03b7 is the stable learning rate, N is the total number of steps, Nwarmup is the number of warmup steps, Ndecay is the number of decay steps, f (n, N, Ndecay) is the learning rate decay function. The main advantage of WSD scheduler over other schedulers (e.g. cosine and linear) is that specifying the number of training steps in advance is not required. Since the learning rate in the stable training stage does not depend on the number of training steps, the WSD scheduler does not need to specify the number of training steps in advance. This is particularly convenient for large runs, as the cooldown can be initiated at any time to observe model behavior and decide whether to stop (H\u00e4gele et al., 2024). It also facilitates extended pre-training, which can be applied to the last checkpoint in the stable training phase. Moreover, the data mixture can be changed during the cooldown phase to incorporate more high-quality data toward the end of the training. This data mixture strategy has been proven effective in many recent language models (Hu et al., 2024; Dubey et al., 2024; Team et al., 2024)."}, {"title": "3 SEARCH OPTIMAL LEARNING RATE FOR WSD SCHEDULER WITH \u00b5P", "content": "In this section, we focus on finding the optimal learning rate \u03b7 for a small proxy model. Due to our use of \u00b5P, we expect the optimal learning rate to be invariant across different model sizes, batch sizes, and training steps. To verify this, we conducted extensive experiments to find the optimal learning rate for different model sizes and numbers of training tokens. Table 2 shows our overall hyperparameter search configurations. We use the WSD scheduler with 10% tokens in the decay phase following previous works (Hu et al., 2024; H\u00e4gele et al., 2024). All models in this section and the next section are trained on the RedPajama (Computer, 2023) corpus and tested on a holdout test set."}, {"title": "3.1 DOES OPTIMAL LEARNING RATE TRANSFER?", "content": "We conduct two controlled experiments to verify the optimal learning rate's transferability. First, we fix the batch size \u03b2 = 128 and swept across all combinations of learning rates \u03b7 and numbers of training tokens T. Figure 2(a) shows that the optimal learning rate, Nopt, does not transfer across a"}, {"title": "3.2 WHAT IS THE RELATIONSHIP BETWEEN Nopt, \u03b2 AND T?", "content": "To better understand the relationship between Nopt, \u03b2, and T, we swept across all possible combinations of these three variables. Figure 3 shows the optimal learning rate for each batch size and training token combination. Like our previous observation, the optimal learning rate consistently decreases with respect to the number of training tokens across different batch sizes. Furthermore, we"}, {"title": "4 POWER SCHEDULER", "content": "Inspired by previous observations, we propose a new power learning rate based on the observation from the previous section:\n$\\Npower (n) = \\min (\\Nmax, \u03b2\\cdot an^b)$ (7)\nwhere \u1e9e is the batch size, n is the number of tokens already trained, a is the amplitude of the learning rate, b is a power-law exponent for decaying the learning with respect to the number of trained tokens, and max is the learning rate upper bound that rejects very large learning. Like the constant learning rate, the power learning rate also does not require specifying the number of training steps or total"}, {"title": "5 PRE-TRAINING EXPERIMENTS", "content": "training tokens beforehand, since the learning rate only depends on the current number of training tokens.\nLike the WSD scheduler, we can combine the power learning rate with warmup and decay to get the final Power scheduler:\nPower (n) = \n$\\begin{cases}\n    \\frac{n}{Nwarmup} \\cdot Npower (Nwarmup) & \\text{if } n < Nwarmup \\\\\n    Npower (n) & \\text{if } Nwarmup < n \u2264 N - Ndecay \\\\\n    f(n, N, Ndecay) \\cdot Npower (N \u2013 Ndecay) & \\text{if } n > N - Ndecay\n\\end{cases}$\n(8)\nThis section compares the Power scheduler with the WSD and Cosine scheduler across different scenarios. In the first part, we conduct a controlled experiment to train 1B transformer models and 1B mixture-of-experts (MoE) models with different learning rate schedulers to show that the Power scheduler is comparable to or better than other schedulers. In the second part, we take a more realistic setting, training a 3B transformer language model and a 3B MoE language model with high-quality data in the decay phase to compare with strong open-source language models. For all the Power scheduler experiments in this section, we will use the hyperparameters from Section 4, a = 4, b = -0.51, and max 0.02."}, {"title": "5.1 1B CONTROLLED EXPERIMENT", "content": "We train a series of 1B parameter dense and Mixture-of-Experts (MoE) transformer models using WSD, cosine, and Power schedulers. All models are trained with 1T tokens. We use the optimal hyperparameters proposed in MiniCPM (Hu et al., 2024) and adapt them to our 1B setting with \u00b5Transfer. All models are trained with batch size 1024. For WSD and cosine scheduler, we set \u03b7 = 0.01, following the optimal learning rate from MiniCPM. For the Power and WSD scheduler, we exponentially decay the learning rate to 0 for the last 100B tokens. The MoE models are implemented with ScatterMoE (Tan et al., 2024). More model details can be found in Table 4.\nWe evaluate all 1B models on language model tasks and multiple-choice tasks from LM evalua- tion Harness (Gao et al., 2024). The multiple-choice tasks include grade-school science questions (ARC, Clark et al. (2018)), yes/no questions (BoolQ, Clark et al. (2019)), common sense reasoning (Hellaswag, Zellers et al. (2019)), open book question answering (OpenBookQA, Mihaylov et al. (2018)), physical questions (PIQA, Bisk et al. (2020)), and Winograd schema task (Winogrande, Sak- aguchi et al. (2021)). Table 5 shows the performance. The Power scheduler provides consistently better or comparable performance for both language modeling and downstream tasks. Surprisingly,"}, {"title": "5.2 3B REALISTIC EXPERIMENT", "content": "To compare performance against strong open-source language models, we pretrain two 3B language models: 1) PowerLM-3B, a 3B dense language model, and 2) PowerMoE-3B, a 3B MoE language model\u00b9. We pretrain these two models using the two-stage training schema in Hu et al. (2024) and Power scheduler. Stage 1 linearly warms up the learning rate and then applies the power decay. The training corpus is a mix of large-scale, medium-quality open-source datasets with permissive licenses. PowerLM-3B is trained on 1T tokens, and PowerMoE-3B on 2.5T tokens. Stage 2 exponentially decays the learning rate to zero. The training corpus has 250B tokens, mixed from stage 1 data and a small amount of high-quality open-source/synthetic corpora with permissive licenses. The training batch size is 1024.\nTable 6 shows the multi-choices performance of our model and state-of-the-art models. Table 7 shows MMLU and generative performance in the math and code domain. The results show that, despite being trained with relatively fewer tokens, our PowerLM-3B still achieves comparable performance with state-of-the-art 2B to 4B language models. Furthermore, our PowerMoE-3B uses only 800M active parameters but performs similarly to state-of-the-art 1B to 2B dense models."}, {"title": "6 CONCLUSION", "content": "In this paper, we systematically study the relationship between optimal learning rate, batch size, and number of training tokens. We observed a power-law relation between these variables while using the WSD learning rate scheduler. Inspired by this observation, we propose a new learning rate scheduler, the Power scheduler, that is invariant with respect to the number of tokens and batch size. The experiment shows that combining the Power scheduler with Maximum Update Parameterization (\u00b5P) can consistently achieve impressive performance with one set of hyperparameters regardless of the number of training tokens, batch size, model size, and even model architecture."}]}