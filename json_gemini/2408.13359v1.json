{"title": "POWER SCHEDULER: A BATCH SIZE AND TOKEN NUMBER AGNOSTIC LEARNING RATE SCHEDULER", "authors": ["Yikang Shen", "Matthew Stallone", "Mayank Mishra", "Gaoyuan Zhang", "Shawn Tan", "Aditya Prasad", "Adriana Meza Soria", "David D. Cox", "Rameswar Panda"], "abstract": "Finding the optimal learning rate for language model pretraining is a challenging task. This is not only because there is a complicated correlation between learning rate, batch size, number of training tokens, model size, and other hyperparameters but also because it is prohibitively expensive to perform a hyperparameter search for large language models with Billions or Trillions of parameters. Recent studies propose using small proxy models and small corpus to perform hyperparameter searches and transposing the optimal parameters to large models and large corpus. While the zero-shot transferability is theoretically and empirically proven for model size related hyperparameters, like depth and width, the zero-shot transfer from small corpus to large corpus is underexplored. In this paper, we study the correlation between optimal learning rate, batch size, and number of training tokens for the recently proposed WSD scheduler. After thousands of small experiments, we found a power-law relationship between variables and demonstrated its transferability across model sizes. Based on the observation, we propose a new learning rate scheduler, Power scheduler, that is agnostic about the number of training tokens and batch size. The experiment shows that combining the Power scheduler with Maximum Update Parameterization (\u00b5P) can consistently achieve impressive performance with one set of hyperparameters regardless of the number of training tokens, batch size, model size, and even model architecture. Our 3B dense and MoE models trained with the Power scheduler achieve comparable performance as state-of-the-art small language models.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning rate is a critical hyperparameter for deep neural network training. In the context of Large Language Models (LLMs), the cosine learning rate scheduler is the most commonly used strategy. It has been shown to be effective across multiple state-of-the-art models, including Llama 3 (Dubey et al., 2024), Gopher (Rae et al., 2021), etc. However, the cosine scheduler requires pre-defined training step counts to achieve the optimal loss. This results in two main drawbacks: 1) the intermediate training checkpoints are suboptimal, 2) continual training of an existing language model becomes complicated.\nMiniCPM (Hu et al., 2024) proposes the Warmup-Stable-Decay (WSD) learning rate scheduler (illustrated in Figure 1) to address these issues. The WSD learning rate schedule is divided into three phases: 1) warmup phase, linearly increase the learning rate from 0 to peak; 2) stable phase, maintain the learning rate at peak value and training the model for most of the time; 3) decay phase, annealing the learning rate to 0 in a relatively short period. The main advantage of this schedule is that specifying the number of training steps in advance is not required. This is particularly convenient for large runs, as the decay can be applied at any time to observe model performance and decide whether to stop. It also allows for continual learning by default, as training can be resumed from a stable phase checkpoint. Moreover, the data mixture can be changed during the decay phase to increase the ratio of high-quality data. This data curriculum is shown to be effective in several recent language models (Team et al., 2023; Hu et al., 2024; Dubey et al., 2024; Shen et al., 2024).\n*Corresponding Author: rpanda@ibm.com"}, {"title": "2 BACKGROUND", "content": "2.1 MAXIMUM UPDATE PARAMETRIZA\u03a4\u0399\u039f\u039d (\u03bc\u03a1)\nMaximal Update Parameterization (\u00b5P) (Yang & Hu, 2020; Yang et al., 2022; 2023) controls initialization, layer-wise learning rates, and activation magnitudes to ensure analytically stable training, independent of a model's width and depth. In addition to improving training stability, \u00b5P improves the transferability of training hyperparameters from small proxy models to large models, a technique called \u00b5Transfer. The hyperparameter transferability of \u00b5P is theoretically justified for width (Yang et al., 2022) and depth (Yang et al., 2023). Previous work also show empirical evidence for transferability across batch size, training steps, and training sequence length.\n2.2 WARMUP-STABLE-DECAY (WSD) SCHEDULER\nMiniCPM (Hu et al., 2024) proposes the WSD learning rate scheduler to divide pretraining into three stages: the warmup stage, the stable training stage, and the remaining decay stage. The WSD scheduler is defined as:\nWSD (n) = \\begin{cases}\n    \\frac{n}{N_{\\text{warmup}}} \\cdot \\eta & \\text{if } n < N_{\\text{warmup}} \\\\\n    \\eta & \\text{if } N_{\\text{warmup}} < n \\leq N - N_{\\text{decay}}, \\\\\n    f(n, N, N_{\\text{decay}}) \\cdot \\eta & \\text{if } n > N - N_{\\text{decay}}\n\\end{cases}\nwhere n is the current number of steps, \u03b7 is the stable learning rate, N is the total number of steps, $N_{warmup}$ is the number of warmup steps, $N_{decay}$ is the number of decay steps, f(n, N, Ndecay) is the learning rate decay function. The main advantage of WSD scheduler over other schedulers (e.g. cosine and linear) is that specifying the number of training steps in advance is not required. Since the learning rate in the stable training stage does not depend on the number of training steps, the WSD scheduler does not need to specify the number of training steps in advance. This is particularly convenient for large runs, as the cooldown can be initiated at any time to observe model behavior and decide whether to stop (H\u00e4gele et al., 2024). It also facilitates extended pre-training, which can be applied to the last checkpoint in the stable training phase. Moreover, the data mixture can be changed during the cooldown phase to incorporate more high-quality data toward the end of the training. This data mixture strategy has been proven effective in many recent language models (Hu et al., 2024; Dubey et al., 2024; Team et al., 2024)."}, {"title": "3 SEARCH OPTIMAL LEARNING RATE FOR WSD SCHEDULER WITH \u00b5P", "content": "In this section, we focus on finding the optimal learning rate \u03b7 for a small proxy model. Due to our use of \u00b5P, we expect the optimal learning rate to be invariant across different model sizes, batch sizes, and training steps. To verify this, we conducted extensive experiments to find the optimal learning rate for different model sizes and numbers of training tokens.\n3.1 DOES OPTIMAL LEARNING RATE TRANSFER?\nWe conduct two controlled experiments to verify the optimal learning rate's transferability. First, we fix the batch size \u03b2 = 128 and swept across all combinations of learning rates \u03b7 and numbers of training tokens T."}, {"title": "3.2 WHAT IS THE RELATIONSHIP BETWEEN $\u03b7_{opt}$, \u03b2 AND T?", "content": "To better understand the relationship between $\u03b7_{opt}$, \u03b2, and T, we swept across all possible combi- nations of these three variables. Like our previous observation, the optimal learning rate consistently decreases with respect to the number of training tokens across different batch sizes. Furthermore, we also notice that the ratio between the optimal learning rate $\u03b7_{opt}$ and batch size \u03b2 is relatively stable for each number of training tokens. Based on this observation, we make our first hypothesis:\nHypothesis 1 The optimal learning rate $\u03b7_{opt}$ for the WSD scheduler and a given pair of (T, \u03b2) is proportional to the training batch size \u03b2.\nThus, we define y as the ratio between $\u03b7_{opt}$ and \u03b2:\n\u03b3 = \\frac{\u03b7_{opt}}{\u03b2}\nTo verify Hypothesis 1, we conducted an extensive hyperparameter search for three model sizes: 12M, 36M, and 121M. After finding the optimal learning rate $\u03b7_{opt}$ for every combination of (T, \u03b2, model size), we only keep the three best batch sizes to focus on the optimal scenario. The y of the three best batch sizes for each T and model size are plotted in . These results show that, given a fixed number of training tokens T, y falls in a relatively small region.\nFurthermore, we notice that y approximately follows a power-law relation with respect to the number of training tokens. Thus, we make a second hypothesis:\nHypothesis 2 The Learning rate to batch size ratio y has a power-law correlation with T:\n\u03b3 = \u03b1T^{b}\nwhen using \u00b5P, the correlation can be transferred across model sizes.\nTo verify the Hypothesis 2, we compute the average y of the best batch sizes to estimate the real y for each number of tokens B.  shows that all three model sizes share a similar power-law correlation. After aggregating the results from three sizes, we get the following correlation:\n\u03b3 = 4.6T^{-0.51}\nwhere x is the number of training tokens.\nIn other words, the relation between optimal learning rate $\u03b7_{opt}$ for the WSD scheduler, batch size \u03b2, and the number of training tokens T can be approximately modeled with the following equations:\n\u03b7_{opt} = \u03b2 \\cdot \u03b1T^{b}\nEquation 6 provides an easy way to predict the optimal learning rate given the number of training tokens and training batch size for the WSD scheduler. As in prior work, a and b can be easily estimated through hyperparameter search on a small proxy model."}, {"title": "4 POWER SCHEDULER", "content": "Inspired by previous observations, we propose a new power learning rate based on the observation from the previous section:\n\u03b7_{power} (n) = \\text{min} (\u03b7_{max}, \u03b2 \\cdot an^{b})\nwhere \u03b2 is the batch size, n is the number of tokens already trained, a is the amplitude of the learning rate, b is a power-law exponent for decaying the learning with respect to the number of trained tokens, and $\u03b7_{max}$ is the learning rate upper bound that rejects very large learning. Like the constant learning rate, the power learning rate also does not require specifying the number of training steps or total training tokens beforehand, since the learning rate only depends on the current number of training tokens.\nLike the WSD scheduler, we can combine the power learning rate with warmup and decay to get the final Power scheduler:\n\u03b7_{\\text{Power}}(n) = \\begin{cases}\n    \\frac{n}{N_{\\text{warmup}}} \\cdot \u03b7_{\\text{Power}} (N_{\\text{warmup}}) & \\text{if } n < N_{\\text{warmup}} \\\\\n    \u03b7_{\\text{Power}} (n) & \\text{if } N_{\\text{warmup}} < n \\leq N - N_{\\text{decay}} \\\\\n    f(n, N, N_{\\text{decay}}) \\cdot \u03b7_{\\text{Power}} (N - N_{\\text{decay}}) & \\text{if } n > N - N_{\\text{decay}}\n\\end{cases}"}, {"title": "5 PRE-TRAINING EXPERIMENTS", "content": "This section compares the Power scheduler with the WSD and Cosine scheduler across different scenarios. In the first part, we conduct a controlled experiment to train 1B transformer models and 1B mixture-of-experts (MoE) models with different learning rate schedulers to show that the Power scheduler is comparable to or better than other schedulers. In the second part, we take a more realistic setting, training a 3B transformer language model and a 3B MoE language model with high-quality data in the decay phase to compare with strong open-source language models. For all the Power scheduler experiments in this section, we will use the hyperparameters from Section 4, a = 4, b = -0.51, and max 0.02.\n5.1 1B CONTROLLED EXPERIMENT\nWe train a series of 1B parameter dense and Mixture-of-Experts (MoE) transformer models using WSD, cosine, and Power schedulers. All models are trained with 1T tokens. We use the optimal hyperparameters proposed in MiniCPM (Hu et al., 2024) and adapt them to our 1B setting with \u00b5Transfer. All models are trained with batch size 1024. For WSD and cosine scheduler, we set \u03b7 = 0.01, following the optimal learning rate from MiniCPM. For the Power and WSD scheduler, we exponentially decay the learning rate to 0 for the last 100B tokens. The MoE models are implemented with ScatterMoE (Tan et al., 2024).\nWe evaluate all 1B models on language model tasks and multiple-choice tasks from LM evalua- tion Harness (Gao et al., 2024). The multiple-choice tasks include grade-school science questions (ARC, Clark et al. (2018)), yes/no questions (BoolQ, Clark et al. (2019)), common sense reasoning (Hellaswag, Zellers et al. (2019)), open book question answering (OpenBookQA, Mihaylov et al. (2018)), physical questions (PIQA, Bisk et al. (2020)), and Winograd schema task (Winogrande, Sak- aguchi et al. (2021))."}, {"title": "5.2 3B REALISTIC EXPERIMENT", "content": "To compare performance against strong open-source language models, we pretrain two 3B language models: 1) PowerLM-3B, a 3B dense language model, and 2) PowerMoE-3B, a 3B MoE language model\u00b9. We pretrain these two models using the two-stage training schema in Hu et al. (2024) and Power scheduler. Stage 1 linearly warms up the learning rate and then applies the power decay. The training corpus is a mix of large-scale, medium-quality open-source datasets with permissive licenses. PowerLM-3B is trained on 1T tokens, and PowerMoE-3B on 2.5T tokens. Stage 2 exponentially decays the learning rate to zero. The training corpus has 250B tokens, mixed from stage 1 data and a small amount of high-quality open-source/synthetic corpora with permissive licenses. The training batch size is 1024.\nTable 6 shows the multi-choices performance of our model and state-of-the-art models. Table 7 shows MMLU and generative performance in the math and code domain. The results show that, despite being trained with relatively fewer tokens, our PowerLM-3B still achieves comparable performance with state-of-the-art 2B to 4B language models. Furthermore, our PowerMoE-3B uses only 800M active parameters but performs similarly to state-of-the-art 1B to 2B dense models."}, {"title": "6 CONCLUSION", "content": "In this paper, we systematically study the relationship between optimal learning rate, batch size, and number of training tokens. We observed a power-law relation between these variables while using the WSD learning rate scheduler. Inspired by this observation, we propose a new learning rate scheduler, the Power scheduler, that is invariant with respect to the number of tokens and batch size. The experiment shows that combining the Power scheduler with Maximum Update Parameterization (\u00b5P) can consistently achieve impressive performance with one set of hyperparameters regardless of the number of training tokens, batch size, model size, and even model architecture."}]}