{"title": "Semantic-guided Search for Efficient Program Repair with Large Language Models", "authors": ["THANH LE-CONG", "BACH LE", "TOBY MURRAY"], "abstract": "Fixing software bugs is crucial yet demands significant resources from developers. Automated Program Repair (APR) is a promising solution to address this challenging task. The emergence of Large Language Models (LLMs) has opened a new era of LLM-based APR, substantially progressed the APR field further. However, conventional LLM-based Program Repair methods face significant challenges regarding memory inefficiency, hindering their scalability and effectiveness. This is largely due to the beam search utilized in the patch generation phase of LLM-based Program Repair, which often requires large beam sizes to search for more potentially good repair candidates.\nIn this paper, we first show that increases in beam size of even just small-sized LLM (1B-7B parameters) require an extensive GPU resource consumption, leading to up to 80% of recurring crashes due to memory overloads in LLM-based APR. Seemingly simple solutions to reduce memory consumption are (1) to quantize LLM models, i.e., converting the weights of a LLM from high-precision values to lower-precision ones. and (2) to make beam search sequential, i.e., forwarding each beam through the model sequentially and then concatenate them back into a single model output. However, we show that these approaches still do not work via both theoretical analysis and experiments.\nTo address this, we introduce FLAMES, a novel LLM-based APR technique that employs semantic-guided patch generation to enhance repair effectiveness and memory efficiency. Unlike conventional methods that rely on beam search, FLAMES utilizes greedy decoding to enhance memory efficiency while steering the search to more potentially good repair candidates via a semantic-guided best-first search algorithm. At each decoding step, FLAMES uses semantic feedback from test validation such as the number of passing and failing test cases to select the most promising token to explore further. Our empirical evaluation on the Defects4J and HumanEval-Java datasets shows that FLAMES not only substantially reduces memory consumption by up to 83% compared to conventional LLM-based APR, but also accelerates the repair process. Remarkably, FLAMES successfully generated 133 and 103 correct fixes for 333 and 163 bugs in the Defects4J and HumanEval-Java datasets, respectively. This suggests that FLAMES is not only more efficient but also outperforms state-of-the-art techniques, fixing at least 10 and 11 more bugs than SOTA baselines in the Defects4J and HumanEval-Java datasets, respectively.", "sections": [{"title": "1 INTRODUCTION", "content": "Fixing software bugs is a complex and time-consuming task for developers [66]. Automated Program Repair (APR)[16, 18] has emerged as a promising solution to alleviate this burden by automatically repairing bugs. Over the past two decades, APR has seen significant advancements [9, 17, 27, 32, 34, 36, 39, 43, 47, 72, 77, 87], with practical applications in the software industry [4, 30, 46, 67]. Recently, the rise of Large Language Models (LLMs), pre-trained on vast datasets, has further advanced APR, opening a new era of LLM-based APR [20, 58, 69]. LLM-based techniques typically adapt models for APR either through fine-tuning on APR-specific datasets [19, 20, 54, 58, 81, 88] or by using prompting with commercial models [5, 80]. In this study, we focus on fine-tuning approaches, as prompting often relies on closed-source and commercial models like ChatGPT, which lack transparency, hindering reproducibility in open science and raising data privacy concerns.\nFine-tuning-based approaches typically refine the weights of Code LLMs, such as CodeLLama [57] and Incoder [15] by fine tuning these models on APR datasets. During inference, they generate token sequences to construct candidate patches, prioritizing sequences with high probability scores from the fine-tuned model. Traditionally, beam search, a widely-used search algorithm from natural language processing [60], is used to optimize this process by maintaining only the top-n nodes at each decoding step, where n is the beam size. A larger beam size provides a more precise approximation to exact decoding, i.e., thoroughly exploring all possible sequences, thereby enhancing the quality of the outputs. This hyper-parameter is even more important in the context of APR, which covers a broader range of possible identifiers and a larger search space than natural language [9, 22, 45]. It has been shown that a substantial beam size is essential for generating adequate candidate patches for optimal results [22, 45, 85], and a bigger beam size leads to higher performance of APR techniques that are based on small-sized deep learning models (less than 300M parameters) [63, 78, 85].\nBut, is it true that increasing the beam size is all we need to improve the performance of APR? We revisit the impact of beam size on the effectiveness of LLM-based APR techniques on larger models (1B-7B parameters). Our findings reveal that increasing the beam size boosts the effectiveness of these techniques, but once the beam size is reached a threshold the performance drops significantly due to memory overloads that cause recurring crashes even on state-of-the-art hardware (NVIDIA A100 GPU, equipped with 80 GB of VRAM). For instances, state-of-the art LLM-based APR techniques such as InCoder [20], CodeGen [20], and RepairLLama [58] produced between 21% and 46% more plausible patches when the beam size was increased from 10 to 25. Ideally, we expect that further increasing the beam size would lead to better performance of the models for APR. However, when beam size is set higher at 50, 100, and 200, our experiments reveal that performance of these models significantly drops.\nOur experiments uncover that the above phenomenon is due to the extensive GPU resource consumption, particularly on Video Random Access Memory (VRAM), caused by larger beam sizes. Often, upgrading VRAM to accommodate larger beam sizes requires GPU upgrades, which are expensive, and thus this creates a costly trade-off between memory efficiency and the performance of LLM-based APRs. For instance, achieving a 46% performance improvement with InCoder-1B required 58% more average and 111% more peak VRAM usage on our dataset. Consequently, further performance gains through increasing beam size demand substantial VRAM, often leading to out-of-memory (OOM) crashes even on cutting-edge hardware and thereby unduly reducing the effectiveness of LLM-based APR techniques. Our experiments using the NVIDIA A100 GPU, equipped with 80 GB of VRAM, showed that Incoder-6B crashed on nearly 80% of the evaluated bugs when using a beam size of 200. This resulted in a 60% reduction in performance compared to a beam size of 10. In summary, we conclude that simply increasing the beam size is not a solution as it comes at the cost of chasing for state-of-the-art hardware.\nA seemingly straightforward approach to this problem is to reduce the memory usage of LLM-based program repair through engineering efforts. This can be achieved using two common strate-gies: (1) quantizing LLM models by converting their weights from high-precision to lower-precision values, and (2) making beam search sequential, where each beam is processed individually through the model before being combined into a single output. However, our findings show that while these methods can reduce memory usage, their memory demands still escalate significantly as beam size increases, leading to substantially high out-of-memory (OOM) rates. Consequently, these engineering efforts alone cannot solve the problem. This realization motivates us to develop a patch generation algorithm that can effectively enhance the performance of LLM-based APR techniques without relying on increasing the beam size and compromising memory efficiency.\nIn this paper, we introduce FLAMES, an efficient LLM-based Program Repair technique using semantic-guided patch generation. Unlike conventional methods that rely solely on a language model's knowledge and beam search, our approach aim to leverage semantic feedback from test validations to guide LLMs in patch generation. The core idea of FLAMES is to combine LLM-based and search-based APR. This process begins with generating initial patches using a greedy decoding, i.e., beam search with beam size of 1, followed by iterative, semantic-guided searches to refine these solutions. To achieve this, we employ a best-first search algorithm, specifically PG-TD [83], along with semantic feedback from test validations to guide the patch generation process in LLMs. FLAMES provides three primary advantages in addressing the limitations associated with beam search. First, it significantly reduces VRAM consumption by using greedy decoding, enhancing memory efficiency. Second, the approach scales well as it does not increase VRAM usage while generating more candidate patches. Third, FLAMES facilitates seamless information exchange between the patch generation and validation phases, enabling the efficient and effective exploration of plausible candidate patches.\nWe conducted an empirical evaluation of our proposed approach, FLAMES, on a dataset including 333 real-world bugs from Defects4J and 163 synthetic bugs from the HumanEval dataset. Our experiments demonstrate that FLAMES successfully repaired 133 and 103 bugs from the Defects4J and HumanEval datasets, respectively. We compared FLAMES against 15 leading APR techniques. Our results indicate that FLAMES substantially outperforms these baselines, achieving at least 10 and 11 more correct fixes in the Defects4J and HumanEval datasets, respectively. We also found that FLAMES's semantic-guided patch generation substantially outperforms widely-used patch generation strategies in LLM-based APR, including beam search and multiple sampling. The improvement rates range from 43% to 1145% across five different Large Language Models (LLMs) with three different architectures. Additionally, our analysis of memory efficiency showed that our method could reduce VRAM consumption by 42% to 83%, decreasing peak VRAM requirements from over 80 GB to as low as 12.7 GB across various configurations and models. Additionally, FLAMES also significantly boosts the time efficiency of LLMs, often generating plausible patches faster than conventional LLMs with beam search.\nIn summary, we made the following contributions:\n\u2022 We empirically study the impact of beam size on the effectiveness and memory efficiency of five different LLM-based APR techniques, highlighting the challenges of scaling the search space due to extensive memory consumption of the techniques.\n\u2022 We introduce a novel approach, namely FLAMES, which fuses LLM-based and search-based APR, utilizing semantics feedback from patch validation to efficiently discover plausible candidate patches."}, {"title": "2 MOTIVATION STUDY", "content": "Beam size is a critical hyper-parameter in beam search, the fundamental algorithm for patch generation in LLM-based APR. In this section, we evaluate the impact of beam size on the effectiveness and memory efficiency of LLM-based APR techniques using LLMs with 1B to 7B parameters. While prior studies [22, 45, 64, 72] showed that a substantial beam size is needed for optimal performance for APR, our experiments reveal that increasing the beam size unduly hinders memory efficiency and subsequently degrades the performance of LLM-based APR even on state-of-the-art hardware. This motivates a new search algorithm, which we propose in Section 3, to improve the efficiency and effectiveness of LLM-based APR. We structure our empirical study with the following two research questions:\n\u2022 RQ1: How does beam size affect memory efficiency of LLM-based APR? We measure VRAM usage and the frequency of out-of-memory crashes in LLM-based APR techniques with different beam sizes to understand the impact of this parameter on memory efficiency. We selected beam sizes of 10, 25, 50, 100, and 200, as these are common in prior works [69, 72, 78, 87]. We did not go beyond beam size of 200 due to hardware constraints.\n\u2022 RQ2: How does beam size affect the effectiveness of LLM-based APR? We investigate the impact of beam size on the effectiveness of 5 LLM-based APR techniques by measuring the number of plausible patches. Similarly to RQ1, we also selected beam sizes of 10, 25, 50, 100, and 200.\nFor both research questions, we also examine the impact of memory reduction techniques, as described in Section 2.1.3, on our findings."}, {"title": "2.1 Experimental Design", "content": "2.1.1 Benchmark Dataset. To address these research questions, we experiment on Defects4J, a well-established benchmark. This dataset was collected by Just et al. [24] from 17 open-source Java projects, comprising 835 real-world bugs. In this study, following previous works [9, 40, 43, 72, 78], we focus on single-hunk bugs, where the patch is limited to a single contiguous chunk of code at a single location. We select this kind of bug as the existing LLM-based APR, which are studied in our work, such as InCoder and CodeGen [20], are only fine-tuned for single-hunk bugs (refer to next sections for details).In total, we identified 333 single-hunk bugs using the script provided by the original authors of Defects4J.\n2.1.2 Studied LLM-based Program Repair techniques. To select the target techniques for our empiri-cal study, we conducted a literature review and identified suitable techniques based on the following criteria: (1) use beam search as the patch generation strategy, (2) employ LLM with at least one billion parameters, (3) provide accessible fine-tuned models for Automated Program Repair.\nBased on these criteria, we identified five LLM-based APR techniques from recent works [20, 58] for our study including CodeGen-2B [52], CodeGen-6B [52], InCoder-1B [15], InCoder-6B [15], and RepairLlama [58]. The CodeGen and InCoder models are LLMs which has been fine-tuned for Program Repair by Jiang et al.[20] and shown the best performance among evaluated techniques.\n2.1.3 Memory Reduction techniques. In this study, we also investigated the potential of memory reduction techniques through engineering efforts and their impact on our findings. These techniques include:\n\u2022 Quantization: A widely used technique to reduce the memory usage of LLMs by converting their weights from high-precision (32-bit) to lower-precision values, such as 8-bit or 4-bit. In this work, we apply 4-bit quantization, reducing the LLM's weights from 32-bit to 4-bit.\n\u2022 Sequential Beam Search: An implementation variant of beam search proposed by Saibo Geng [11]. This approach processes each beam sequentially through the model before concatenating them into a single output.\n2.1.4 Implementation Details. We implement LLM-based APR techniques using Python, lever-aging the PyTorch and HuggingFace libraries for efficient execution. To enhance the accuracy of our implementations and minimize potential bias, we collected and integrated the inference code, including repair prompts and trained models, from their original repositories into a unified framework. All implementations utilize HuggingFace's standard beam search, a widely used method in LLM-based techniques. We perform inference with a batch size of one, i.e., trying to repair one buggy program at a time. All experiments are conducted on a single NVIDIA H100 GPU with 80GB of VRAM, a state-of-the-art and widely-used hardware for Deep Learning and Large Language Models."}, {"title": "2.2 RQ1: Impact of Beam Size on Memory Efficiency", "content": "Figure 1 illustrates the experimental results of RQ1. Overall, there is a significant increase in VRAM usage as the beam size expands from 0 to 200. For instance, in CodeGen-2B, average memory usage jumps from approximately 20GB at a beam size of 10 to around 70GB at a beam size of 200. It is important to note that our figures are capped at 80GB due to hardware limitations, representing only the lower bound of the actual numbers, which may be much higher. Similarly, peak memory usage climbs dramatically from about 30GB to 80GB, which is the maximum our hardware can support. These trends are consistent across other LLM-based APR techniques. Notably, this increase in VRAM usage induces a substantial rise in crashes due to OOM errors. For example, the OOM ratio in CodeGen-2B increases from nearly negligible at smaller beam sizes to about 60% at a beam size of 200. Larger models, such as CodeGen-6B and InCoder-6B, show even higher OOM crash rates, exceeding 80\nNext, we explore the potential of 4-bit quantization and sequential beam search, which are widely used to optimize model memory requirements, to reduce VRAM usage. Figure 1 shows that quantization significantly reduces both average and peak VRAM usage, decreasing the frequency of OOM crashes. However, sequential beam search does not notably reduce the memory usage of LLM-based APR techniques and even increases the frequency of OOM crashes. Sequential beam search involves trade-offs and does not guarantee reduced memory usage in all scenarios. This observation has been also confirmed by the authors of sequential beam search. Further details of this can be found in our online Appendix [3].\nMore importantly, we also observe that VRAM usage for LLM-based APR techniques with both quantization and sequential beam search still increases substantially as beam size increases. For instance, average VRAM usage for CodeGen-2B rises from less than 10GB at a beam size of 10 to around 60GB at a beam size of 200. Its peak memory usage also quickly approaches the hardware limit of 80GB. Consequently, even with memory reduction techniques, LLM-based APR techniques continue to experience a significant number of OOM crashes in our benchmark dataset, ranging from 30% to 75% at a beam size of 200."}, {"title": "2.3 RQ2: Impact of Beam Size on Effectiveness", "content": "Table 1 present our experimental results for RQ2. We can see that the number of plausible patches, only shows an upward trend at smaller beam sizes and remarkably declines at larger sizes. For instance, the number of plausible patches generated by RepairLlama increases from 139 at a beam size of 10 to 169 at 25, then drops significantly to 134 at 100 and further to 75 at 200, although the ratio of plausible patches remains at 0.60. This reduction is primarily due to the high frequency of OOM crashes, which range from 30-60% at larger beam sizes, as depicted in Figure 1. These results highlight the significant impact of LLMs' extensive memory usage on their performance with large beam sizes.\nNext, similar to RQ1, we examine the impact of 4-bit quantization and sequential beam search on memory usage and the effectiveness of LLM-based APR techniques. Overall, the effectiveness of LLM-based APR techniques remains consistent with sequential beam search, maintaining the same trends. Meanwhile, quantization sustains an upward trend in effectiveness up to a beam size of 100. For example, the number of plausible patches generated by the quantized CodeGen-2B increased from 64 at a beam size of 10 to 107 at a beam size of 100, surpassing the original model by 42.7% at the same beam size. This improvement is due to a reduction in OOM crashes, from 30% to less than 5%. However, despite these gains, the effectiveness of quantized models still declines at larger beam sizes, such as 200, similar to the trend seen in the original models. For instance, the effectiveness of the quantized CodeGen-2B dropped from 114 plausible patches at a beam size of 50 to 91 at 100, and further to 55 at 200, reflecting declines of 20% and 48%, respectively."}, {"title": "3 APPROACH", "content": "Our findings in Section 2 highlight that scaling beam size to a certain threshold may help improve the effectiveness of APR at the cost of extensive memory consumption, above which the effectiveness quickly degrades even on state-of-the-art hardware.\nTo address these limitations, we propose a novel approach, FLAMES, that combines LLM-based and search-based APR for an efficient, semantic-guided patch generation. At high level, FLAMES initially generates candidate patches using greedy decoding (i.e., beam search with a beam size of 1) and then leverages semantic feedback from test validation and a specialized best-first search, namely PG-TD [83], to iteratively explore potential candidate patches. FLAMES uses a small beam size to keep memory usage low, while allowing it to explore a greater number of potential candidate via the guidance of semantic feedback with best-first search.\nFigure 2 illustrates the overall pipeline of FLAMES. The search algorithm of FLAMES guides the patch generation process in LLM-based APR through four main phases: Selection, Expansion, Simulation, and Backpropagation. In this section, we first re-formulate the patch generation task in LLM-based APR as a search problem. Then, we present details of these four stages used to generate candidate patches for a given buggy program."}, {"title": "3.1 Problem Formulation", "content": "We first formulate the patch generation process in APR as a Markov Decision Process [61] which contains state, action, transition function and reward function as inspried by prior works [13, 83].\nState. The state is a triplet $s = (pp, spec, bp)$, where $bp$ is the buggy program which needs to be fixed, $pp$ is a partial patch for $bp$ represented by a sequence of tokens and $spec$ is a specification which describes desired behaviors of $bp$. The initial state is (($sos$), $spec$, $bp$) where ($sos$) denotes the start of sequence token in LLMs.\nAction. An action $a$ is a token from the vocabulary of fine-tuned LLM for APR. We also have special tokens, i.e., terminal tokens, that indicates that a program is complete. These terminal tokens are pre-defined for each LLM. We define that a partial patch $pp$ is complete if and only if the last token of $pp$ is a terminal token.\nTransition function. The transition $T$, takes the set of partially-contructed patch $pp$ and applies the action $a$ as follows:\n$T(pp, a) = pp \\oplus [a]$   (1)\nwhere $\\oplus$ denote a sequence concatenation.\nReward. The reward of a state $s = (pp, spec, bp)$ is defined to be either:\n\u2022 0 if $pp$ is a partial patch;\n\u2022 $R(pp, spec)$ if $pp$ a complete patch, where $R$ is a function which measure how likely $pp$ statisfies $spec$.\nIntuitively, $R$ serves for same purpose as objective function in search-based APR [36, 41]."}, {"title": "3.2 Overview", "content": "To efficiently and effectively search for repairs, FLAMES maintains a tree structure where each node represents a token and the path from the root to each node represents a partially constructed patch. Then, FLAMES starts from an empty patch and search for a complete patch with optimal reward. The process includes four stages: (1) selection phase for selecting the most promising partial patch, i.e., branch of tree, to visit using a policy algorithm (Section 3.3); (2) expansion phase for expanding the search tree by adding potentially tokens (Section 3.4); (3) evaluation phase for estimating the reward of current state using simulation (Section 3.5); and (4) backpropagation phase for updating value of partial patches that their potential (Section 3.6). We elaborate details of the stages below."}, {"title": "3.3 Selection", "content": "At this stage, our goal is to select the most promising partial patch to explore. To achieve this, we employ a policy algorithm that strikes a balance between exploiting potentially high-reward states and exploring less-visited ones. Specifically, an action (i.e., adding a token) is chosen based on three criteria: (1) the action's historical reward in relation to the current state is high, indicating strong potential; (2) language models (LLMs) predict that the action is a suitable next token, as reflected by its prediction probability; and (3) the resulting state after taking the action is relatively under-explored.\nTo implement this approach, we evaluate three commonly-used policy algorithms in Monte Carlo Tree Search (MCTS): UCB [31], and two variants of P-UCB [59]-one with fixed weights (Fixed P-UCB) and the other with variable weights (Variable P-UCB). Our results indicate that Variable P-UCB is the most effective algorithm for the APR task, leading us to adopt it as our primary policy algorithm (See Section 4.6 for further details.)"}, {"title": "3.4 Expansion", "content": "This stage aims to expand the search tree by adding possible next tokens. As the number of possible tokens in vocabulary is huge, a random sampling, which is used in conventional MCTS [31] may likely return in invalid and low-potential tokens. Therefore, FLAMES leverages fine-tuned LLM to suggest top-k next tokens with the highest prediction score given the current state following PG-TD. Note that, as a state can be re-visited multiple times during the tree search, FLAMES also leverages a caching mechanism, namely tree structure caching, to avoid redudant computation. In particular, whenever LLMs make a prediction, the input sequence, i.e., state and top-k next tokens is cached using a hash map. Then, when the algorithm needs LLM to predict top-k next tokens, such information can be reused if it exists in the cache. In this work, we set $k = 10$ and further discuss the impact of k on our method in Sections 4.6.2."}, {"title": "3.5 Evaluation", "content": "This stage aims to evaluate the potential of the current state, i.e., how likely the partial patch leads to a optimal (complete) patch. Similar to expansion, it is impossible to evaluate the current state using an random simulation as used in conventional MCTS algorithm. Consequently, FLAMES leverages beam search to automatically complete the current state, i.e., a partial patch $pp$, to form a complete patch $cp$. However, it is noteworthy that leveraging the beam search as in original PG-TD still hinders memory efficiency. Therefore, instead of beam search, FLAMES utilizes a greedy search, that is equivalent to a beam search with a beam of size 1, to find the complete patch for current state while keeping low memory consumption.\nThen, given the completed patch, FLAMES estimates the potential of current state by calculating a reward function $R(cp, spec)$. Theoretically, $spec$ and reward function $R$ can takes any kinds of specification and a corresponding pre-defined reward function. In this work, following common practices in literature [16], we leverage developer-written test cases as our specification and utilize the passing ratio, which is widely-used in search based APR [16], as our reward function. More formally, given a program $cp$ and a set of test cases $spec$, we define our reward function as follows:\n$R(cp, spec) = \\frac{f_{pass}}{f_{pass} + f_{fail}}$ with $f_{pass}$ and $f_{fail}$ is the number of passing and failing test cases observed when running $cp$ on $spec$."}, {"title": "3.6 Backpropagation", "content": "This stage aims to update the value of nodes in the tree for guiding next steps of the search process. Intuitively, the stage allows FLAMES to provide feedback from test validation for guiding the patch generation process of LLMs. Particularly, the reward of a state (calculated based on automatically-completed program) is backpropagated to its parents recursively until the root is reached. For all state and action pair ($s'', a''$) along the path from current state to root node, FLAMES updates their Q-value as follows:\n$Q(s'') = max(Q(s'), R(cp, spec))$   (2)\nwhere $R(cp, spec)$ is reward value of current state - action pair ($s, a$) with $cp$ is automatically-completed patch of s."}, {"title": "3.7 Implementation Details.", "content": "We implemented FLAMES by extending the PG-TD [83] framework, based on the DynaGym toolkit. It is developed in Python using PyTorch, HuggingFace, and OpenAI Gym. We use RepairL-Lama, the best-performing model among LLM-based repair techniques, as our default LLM-based repair model. For repair, FLAMES generates up to 200 patches and sets a 30-minute timeout for each bug, as recommended by developers [53]. Experiments are conducted on an NVIDIA A100 GPU with 80GB VRAM, 250 GB RAM, and a 32-core Intel Xeon CPU at 2.90GHz, running Red Hat Enterprise Linux 9.3 and Java 1.8.0_241. This setup is comparable to those used in related works [58, 64]."}, {"title": "4 EMPIRICAL EVALUATION", "content": "4.1 Research Questions\nContinuing from our motivation study in Section 2, our empirical evaluation in this section aims to answer the following four research questions:\n\u2022 RQ3: How effective is FLAMES? We assess the effectiveness of FLAMES on the Defects4J [24] and HumanEval-Java [20] datasets, comparing it to 15 leading APR techniques described in Section 4.2.\n\u2022 RQ4: How efficient is FLAMES? We evaluate the time and memory efficiency of FLAMES and compare it to the conventional beam search algorithm.\n\u2022 RQ5: How effective is FLAMES\u2019s semantic-guided patch generation? We investigate the effectiveness of FLAMES's semantic-guided patch generation by implementing our approach with five different LLM-based APR techniques and comparing their effectiveness to that of the original models using two widely-used patch generation strategies: beam search [58, 78] and multiple sampling [64, 68].\n\u2022 RQ6: How do different configurations affect the performance of FLAMES? Finally, we examine the impact of various key factors on FLAMES's effectiveness, including reward function, expansion size, and policy algorithm.\n4.2 Experimental Setup\n4.2.1 Benchmark Dataset. Similar to our initial motivation study (Section 2.1), we evaluate our method using 333 single-hunk bugs from the Defects4J dataset [24]. We also employ the HumanEval-Java dataset, which includes 163 artificial bugs introduced by Jiang et al. [20]. This dataset, recently developed by injecting bugs into the HumanEval benchmark[8], presents a reduced risk of data leakage, where evaluation data might overlap with training data, compared to Defects4J.\n4.2.2 APR Baselines. To evaluate effectiveness of our approach (RQ3), we compare FLAMES against leading baselines across various APR categories: Template-based, NMT-based, Cloze-based, and LLM-based APR techniques. For LLM-based APR, we evaluate five top models-CodeGen-2B, CodeGen-6B, InCoder-1B, InCoder-6B, and RepairLlama-using their best variants identified in RQ2 with beam sizes from 10 to 200. For Cloze-based APR, we include FitRepair [68], AlphaRe-pair [72], and Repilot [64]. We also compare FLAMES with NMT-based methods: ITER [79], Recoder [87], KNOD [21], and RewardRepair [78], as well as template-based approaches: GAMMA [82], TENURE [48], and TBar [43]. Following standard practices [9, 20, 22, 64, 72], we perform evaluations under the assumption of perfect fault localization, which ensures that variations in fault localization techniques do not affect the results. For LLM-based techniques, we use the same configurations as in Section 3.7. For other techniques, we collect bug fix results from their original papers and replication packages, following common practices in prior works [22, 64, 72, 78].\n4.2.3 Patch Generation Algorithm Baselines. To evaluate the effectiveness of FLAMES's semantic-guided patch generation (RQ5), we also compared our approach two well-known patch generation strategies: beam search and multiple sampling. For beam search, we leverage the standard imple-mentations from Huggingface library,widely adopted toolset for training and deploying LLMs. For multiple sampling, we first set temperature of LLM to 1 for maximizing their diversity. Then, we repeat the default patch generation of these models (using beam search) until reaching 200 patches.\n4.2.4 Evaluation Metrics and Patch Correctness Assessment. We assess the effectiveness of APR techniques using a standard metric: the number of correct patches. To identify patch correctness, following prior works [33, 40, 45, 72, 78], we conduct a manual evaluation to determine if they are either syntactically or semantically equivalent to developer-written patches. Note that, while manual patch correctness assessment is precise, it is resource-intensive and necessitates considerable human effort. Therefore, due to limited human resources, we restrict the use of the number of correct patches as the sole metric for evaluating RQ3, which compares the effectiveness of FLAMES with state-of-the-art baselines. For other research questions, we use the number of plausible patches as a proxy for effectiveness, following prior research [20, 58]."}, {"title": "4.3 RQ3: Comparison with Existing Techniques", "content": "Table 2 reports the number of correct fixes of our proposed technique, FLAMES, and baseline approaches in the Defects4J and HumanEval-Java dataset.\n4.3.1 Repaired Bugs. Overall, our approach, FLAMES, successfully generates correct fixes for 133 out of 333 single-hunk bugs in Defects4J, outperforming all existing APR techniques. Notably, it corrects 21 more bugs than RepairLlama, the best-performing LLM-based method, which utilizes the same core LLM as FLAMES. This superior performance is primarily due to the memory limitations of RepairLlama, which frequently crashes due to OOM errors with larger beam sizes, as discussed in Section 2.2. Consequently, it cannot scale to higher beam sizes, with the best-performing model capped at a beam size of 25. This limitation results in a lower number of validated patches of 25. In contrast, FLAMES can generate up to 200 patches, addressing scalability issues and boosting repair success rates.\nMoreover, our proposed technique also outperforms the leading cloze-based approach, FitRe-pair, and the top NMT-based technique, KNOD, by repairing 10 and 23 more bugs respectively, corresponding to improvement rates of 8% and 21%. It is noteworthy that FLAMES achieves this performance with a budget of only 200 validated patches and a 30-minute timeout for each bug, while FitRepair and KNOD require 1000-5000 validated patches and a 5-hour timeout for each bug.\n4.3.2 Unique Repaired Bugs. Next, to further demonstrate the effectiveness of our approach, we evaluate the number of unique bugs that only FLAMES can fix. Figure 3 illustrates our uniqueness analysis on Defects4J dataset of our approach with respects to the best baselines for NMT-based (KNOD [21", "68": "and LLM-based (RepairLLama [58"}]}