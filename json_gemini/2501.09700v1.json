{"title": "Cueless EEG imagined speech for subject identification: dataset and benchmarks", "authors": ["Ali Derakhshesh", "Zahra Dehghanian", "Reza Ebrahimpour", "Hamid R. Rabiee"], "abstract": "Electroencephalogram (EEG) signals have emerged as a promising modality for biometric identification. While previous studies have explored the use of imagined speech with semantically meaningful words for subject identification, most have relied on additional visual or auditory cues. In this study, we introduce a cueless EEG-based imagined speech paradigm, where subjects imagine the pronunciation of semantically meaningful words without any external cues. This innovative approach addresses the limitations of prior methods by requiring subjects to select and imagine words from a predefined list naturally. The dataset comprises over 4,350 trials from 11 subjects across five sessions. We assess a variety of classification methods, including traditional machine learning techniques such as Support Vector Machines (SVM) and XGBoost, as well as time-series foundation models and deep learning architectures specifically designed for EEG classification, such as EEG Conformer and Shallow ConvNet. A session-based hold-out validation strategy was employed to ensure reliable evaluation and prevent data leakage. Our results demonstrate outstanding classification accuracy, reaching 97.93%. These findings highlight the potential of cueless EEG paradigms for secure and reliable subject identification in real-world applications, such as brain-computer interfaces (BCIs).", "sections": [{"title": "1. Introduction", "content": "A biometric security system could be developed using the integration of biometric databases with recognition techniques. This system works by capturing biometric information from individuals, extracting a set of discriminative features, and matching these features against pre-stored templates within the biometric database.\u00b9 A significant number of researchers employ machine learning or deep learning models as the aforementioned recognition techniques, utilizing datasets composed of biometric samples from individuals. These systems could be developed using two distinct approaches: subject identification and subject authentication, which Bidgoly et al.\u00b2 clearly distinguished. In subject identification, each individual is treated as a separate class, and the model performs multi-class classification to assign the input to one of these classes. On the other hand, in subject authentication, the model functions as a binary classifier to either accept or deny the claimed identity of the input.\nIn biometric subject recognition literature, various types of biometric data have been utilized. For instance, Grosz et al.\u00b3 used fingerprint data and developed a deep learning architecture named AFR-Net for both subject identification and authentication. Alay et al.\u2074 utilized multimodal biometrics, including iris, face, and finger vein data, and implemented a CNN-based model for subject identification. Additionally, other studies have investigated the use of palmprints and ECG signals for subject identification.\nAn electroencephalogram (EEG) is a bioelectric signal generated by neural processes in the brain, carrying extensive information about the brain activity. The acquisition methods of EEG are classified into surface EEG and deep EEG. Surface EEG refers to the use of electrodes on the scalp to record the brain's electrical activity.\u2077 In this work, when we refer to EEG, we specifically mean surface EEG. The signal recorded by EEG has poor spatial resolution and a low signal-to-noise ratio; however, due to its safety and high temporal resolution in capturing neural signals' dynamic changes, it remains popular among researchers.\u2078\nEEG has attracted significant interest as a biometric for subject identification due to its unique advantages. Unlike fingerprints, which can be forged or spoofed, EEG is difficult to replicate. In coercive situations, EEG patterns change, allowing systems to detect if a subject is under duress. Furthermore, EEG ensures the presence of a living subject, as a dead body cannot produce EEG signals.\u00b2\nHere, we design a cueless EEG imagined speech paradigm and provide a dataset based on this paradigm, which includes multiple sessions for each subject. We then implement a classification framework to identify subjects from this dataset using both machine learning and deep learning approaches."}, {"title": "II. Related works", "content": "Researchers have employed various EEG-based paradigms for identifying subjects through brain signals. Some have employed the resting-state EEG paradigm, with eyes-closed and eyes-open conditions, similar to the work of Di et al.\u2079. Another commonly used paradigm is motor imagery, where Das et al.\u00b9\u2070 identified subjects based on EEG signals recorded during imagined movements of the right arm, left arm, right leg, and left leg. Additionally, other paradigms, such as steady-state visual evoked potentials (SSVEPs)\u00b9\u00b9 and eye blinking\u00b9\u00b2, have also been explored for subject identification.\nImagined speech as a speech-related paradigm has also been utilized for subject identification. Nieto et al.\u00b9\u00b3 distinguished various speech-related paradigms and defined imagined speech as the motor imagery of speaking, where participants are required to feel as though they are producing speech focusing on the mental simulation of articulatory gestures without any actual physical movements.\nBrigham et al.\u00b9\u2074 explored imagined speech for subject identification using the syllables /ba/ and /ku/. However, Moctezuma et al.\u00b9 raised concerns about this approach for not incorporating words with semantic meaning and subsequently introduced a new dataset featuring the imagined words \"up,\" \"down,\" \"left,\" \"right,\" and \"select\" in Spanish. In their study, they employed a random forest classifier on discrete wavelet transform energy coefficients to identify subjects. Despite their contributions, their model evaluation approach has limitations, as they used 10-fold cross-validation, which involves shuffling the data. Since their dataset contains only one session per subject, shuffling results in training and test samples being drawn from the same session failing to demonstrate the model's robustness across sessions over time.\nFurthermore, several other datasets containing imagined speech of words with semantic meanings are available"}, {"title": "III. METHODS AND MATERIALS", "content": ""}, {"title": "A. PARTICIPANTS", "content": "Eleven healthy participants, all right-handed native Persian speakers with no history of neurological issues, voluntarily participated in this experiment and provided written informed consent. The group consists of 7 males and 4 females, aged between 21 and 28 years, with a mean age of 23.82 years (std = \u00b12.44). The participants engaged in approximately three hours of recording. In this study, participants are identified by IDs ranging from \u201cSub-01\u201d to \u201cSub-11\"."}, {"title": "B. EXPERIMENTAL PROTOCOL", "content": "During the recording sessions, subjects sat comfortably in an armchair about 60 centimeters from an LCD, which was only used to signal when they should imagine the pronunciation, in a dark, silent room. To capture intra-subject variance, data was collected in five sessions on the same day for each subject. This approach can help researchers design identification systems that are more robust to intra-subject distribution shifts. The necessary instructions were provided during the installation of the EEG cap and electrodes on the subject's head.\nSince the subjects are native Persian speakers, five Persian words were selected as targets. These words correspond to \"Left,\" \"Right,\" \"Forward,\" \"Backward,\" and \"Stop\" in English. Information about the pronunciation and the corresponding English equivalent of each word is provided\nThe experiment paradigm was developed using PsychoPy\u00ae\u00b2\u2070 software. At the start of each session, a welcome message displaying the list of words is shown on the screen for 10 seconds. We emphasize that this message is shown only at the beginning of each session, not before each trial. The subject independently selects one of the words from the list before pressing the \"space\" button on the keyboard to begin each trial. Since movements can affect EEG signals, fixation intervals are used to minimize this. After pressing the \"space\" button, the subject must avoid moving any body parts, including hands. A gray circle will then appear on the screen for a random duration, uniformly distributed between 1 and 2 seconds. This gap allows the subject to stabilize, ensuring that any hand movement artifacts from pressing the keyboard do not interfere with the EEG signals during the imagined speech phase in the upcoming part. This method helps minimize noise in the EEG data, making the recording more accurate. Afterward, the circle turns blue, indicating that the subject should imagine the pronunciation of the chosen word. The subject imagines the pronunciation once, keeping their eyes open while looking at a blank black screen. The imagination begins as soon as the subject notices the color change to blue, and the blue circle remains on the screen for 2 seconds. Next, the circle turns gray again and remains for 1 second before disappearing. The subject then sees a message on the screen instructing them to select the imagined word by pressing a specific key on the keyboard: 0 key for \"Stop\", the left arrow key for \"Left\", the right arrow key for \"Right\", the up arrow key for \"Forward\", and the down arrow key for \"Backward\". If any issues occurred during the 2 second imagination phase, such as unintended movements, imagining multiple words, or an incorrect imagined pronunciation, subject could press the return/enter key to mark the trial as a bad trial for removal in future processing."}, {"title": "C. DATA ACQUISITION", "content": "An active EEG device from Liv Intelligent Technology was used for data acquisition (https://www.lliivv.com/en/product). It includes EEG caps in various sizes, with electrodes positioned according to the 10-20 system\u00b2\u00b9. We used the device in this setup, which includes 30 EEG electrodes, one reference electrode, and two ground electrodes which are placed on the left and right earlobes. The EEG channels used in this study are illustrated in Figure 2.The sampling rate was set to 250Hz. For each subject, we used the appropriate size cap. To position the cap on the subject's head, the midpoint between the nasion and inion was identified, and the Cz channel was aligned with this central point. One useful feature of this EEG device is that each EEG electrode is equipped with an LED indicator. The LED glows red when there is no proper contact with the scalp and turns blue once the electrode establishes good contact. Conductive gel was applied to each electrode to ensure proper contact and signal quality. For each session, data from the EEG device is stored in a .tdms file, while the imagined word in each trial is stored in a .csv file."}, {"title": "D. PRE-PROCESS", "content": "To improve the data structure and make information extraction easier for researchers, several pre-processing steps were applied. The pre-processing was performed in Python 3.10.13, primarily using the MNE library\u00b2\u00b2. The Python script for this part is available in the \"signal_preprocessing.py\" file. Additionally, the raw device output data has been anonymized and is provided alongside the processed data, allowing researchers to adjust or modify the processing steps if needed in the future.\nData structuring and file integration.\nAs previously mentioned, for each session, a .tdms file containing EEG data and a .csv file with the corresponding words are stored. The .tdms output file is not well-structured and includes only raw triggers indicating when the subject imagined the word. We developed a Python script to integrate the .tdms and .csv files for each session. Using MNE, we annotated the events within the data and removed trials where the subject pressed the return/enter key. Finally, the results were saved in a more organized .fif format. The relevant code for this step can be found in the \"data_structuring_file_integration.py\" file.\nSignal pre-processing.\nPre-processing includes several key techniques to enhance the signal quality, such as applying filters to remove noise, identifying and correcting bad channels, and re-referencing the data. These steps could be done to ensure the EEG recordings are properly cleaned and prepared for further analysis.\n\u2022 Notch filter\nResearchers commonly use a Notch filter to remove power line noise, as seen in studies such as Nguyen et al.\u00b2\u00b3 and Rusnac et al.\u00b2\u2074. In this study, an overlap-add FIR (Finite Impulse Response) Notch filter was applied using the notch_filter function from MNE to remove 50 Hz line noise, as well as its harmonics (e.g., 100 Hz), considering the Nyquist frequency for a sampling rate of 250 Hz.\n\u2022 Bandpass filter\nTo enhance the signal-to-noise ratio in this study, a FIR bandpass filter with a frequency range of 3 to 45 Hz was applied to the data. The filter utilized a Hamming window, with transition bandwidths of 2 Hz for both the low and high ends.\n\u2022 Detect and interpolate bad channels\nBad channels are those with a low signal-to-noise ratio\u00b2\u2075. This typically occurs due to poor electrode contact with the scalp. To keep this preprocessing pipeline fully automated and eliminate the need for manual detection of bad channels, we employed the find_bad_by_correlation function from the pyprep library\u00b2\u2075,\u00b2\u2076. In our setup using this function, the data is divided into non-overlapping 1-second windows, and the absolute correlation between each channel and all others is calculated for each window. A window is flagged as bad if its maximum correlation with another channel falls below the threshold of 0.4. Finally, a channel is then marked as bad if the proportion of bad windows exceeds the threshold of 0.02. To maintain data dimensionality across sessions and subjects, bad channels were interpolated rather than being completely removed. We used spherical splines for this interpolation, following the method by Perrin et al.\u00b2\u2077. For implementation, we utilized the interpolate_bads function from MNE, which applies this interpolation method.\n\u2022 Re-referencing\nThe common average reference removes the common information recorded across electrodes at the same time, which improves the signal-to-noise ratio\u00b9. We applied this method using the set_eeg_reference function from MNE, with the ref_channels parameter set to average."}, {"title": "IV. Classification Framework", "content": "To Show the validity and evaluate our dataset and also establish comprehensive benchmarks, we investigated two distinct approaches for subject identification using pre-processed EEG signals. The first approach follows a two-stage pipeline where features are first extracted from the signals and then fed into classifiers. The second approach uses end-to-end deep learning architectures, specifically designed for EEG classification, that learn both feature representations and classification boundaries directly from the EEG signals to identify the subjects. These approaches are detailed in the following sections."}, {"title": "A. Two-Stage Classification Approaches", "content": "Feature extraction followed by shallow classifiers is an established approach in EEG-based subject identification. In this work, we employ two methods for feature extraction. The first method involves manual extraction of domain-specific features from the preprocessed EEG signals, utilizing statistical and signal processing techniques. The second method leverages deep feature extraction, where we fine-tune a foundational time-series deep learning model to extract embeddings from the preprocessed"}, {"title": "Manual Feature Extraction", "content": "We computed two categories of features from the EEG signals: statistical-based and wavelet-based features. For statistical features, we extracted a set of time-domain characteristics including mean, variance, skewness, and kurtosis for each EEG channel. These features capture the statistical properties of the signal's amplitude distribution over time. For wavelet-based features, we computed the energy of wavelet decomposition coefficients for each EEG channel. This approach captures both temporal and spectral characteristics of the signal, providing a suitable representation of the EEG dynamics."}, {"title": "Deep Feature Extraction", "content": "For this method, we explored the use of time-series foundation models to extract embeddings directly from EEG signals, rather than relying on handcrafted features. We utilized the MOMENT\u00b3\u00b9 model, a transformer-based architecture, and fine-tuned it on our pre-processed dataset to derive meaningful embeddings. These embeddings are designed to capture distinctive characteristics of the EEG signals, enabling effective discrimination between subjects within the embedding space."}, {"title": "MOMENT", "content": "MOMENT is a transformer-based model pre-trained on a large collection of public time-series data using a masked prediction method. This self-supervised learning task involves masking portions of the input data and training the model to accurately reconstruct the missing parts. By leveraging this technique, Moment captures intrinsic patterns and characteristics of time-series data.\nThe authors of MOMENT pre-trained the model using three different architectures for the encoder: T5-Small, T5-Base, and T5-Large. This results in MOMENT having three distinct configurations: the Small model with an embedding dimension of 512, the Base model with an embedding dimension of 768, and the Large model with an embedding dimension of 1024.\nThe pre-training approach allows Moment to generalize across various types of time-series datasets. The authors demon-strated that, even without dataset-specific fine-tuning, Moment can learn distinct and class-specific representations for diverse time-series data across different fields. In our study, we fine-tuned different sizes of Moment on our dataset to adapt the embeddings for subject identification, utilizing its capability to capture distinctive features from EEG signals that are relevant for distinguishing between subjects."}, {"title": "Shallow Classifiers", "content": "For classification using the extracted features, we employed two established machine learning algorithms SVM and XGBoost. These algorithms, often referred to as shallow classifiers to distinguish them from deep learning approaches, have demonstrated strong performance in EEG classification tasks\u00b3\u00b2,\u00b3\u00b3. SVM excels in high-dimensional feature spaces and can effectively handle non-linear relationships through kernel functions, while XGBoost provides robust performance through ensemble learning of decision trees.\nSVM is a supervised learning algorithm which aims to find the optimal hyperplane that maximizes the margin between two classes in the feature space. Given the training data {(x\u2081, y\u2081), ..., (X\u2099, y\u2099)} SVM solves the following optimization problem :\nmin\nw,b 1/2 ||w||\u00b2 subject to y\u1d62(w\u1d40x\u1d62 \u2013 b) \u2265 1, \u2200i\nwhere w is the weight vector, b is the bias term, and y\u1d62 are the class labels. Then the decision function to classify the data is :\nf(x) = sgn(w\u1d40x-b)\nSVM could also use a kernel function in cases of non-linearly separable data. in this study the radial basis function (RBF) is used as the kernel which is defined as :\nK(x,x') = exp(-||x-x'||\u00b2/2\u03c3\u00b2)\nOur second shallow classifier, XGBoost is an efficient and scalable implementation of gradient boosting based on the work by Friedman\u00b3\u2074. XGBoost builds an ensemble of weak learners, in this study decision trees,by minimizing the following loss"}, {"title": "B. End-to-End Deep Learning Approaches", "content": "End-to-end deep learning models have become increasingly popular in EEG signal processing because they can learn optimal feature representations directly from data, eliminating the need for manual feature engineering. This automated feature learning can potentially capture subtle patterns and dependencies in EEG signals that might be overlooked by traditional handcrafted features. Additionally, by jointly optimizing feature extraction and classification, these models can adapt to dataset-specific characteristics and potentially achieve better performance.\nHere, we utilized several deep learning architectures that have been specifically introduced for EEG classification and brain-computer interface (BCI) tasks. The models employed include EEGNet\u00b3\u2075, Shallow ConvNet\u00b3\u2076 and EEG conformer\u00b3\u2077. The architecture of these models is designed to automatically capture complex discriminative features of EEG signals with respect to the target classes, allowing for more accurate and efficient subject identification. In our study we applied these models directly to the pre-processed EEG signals to identify subjects."}, {"title": "EEGNet", "content": "EEGNet is a compact convolutional neural network (CNN) specifically designed for EEG-based BCIs\u00b3\u2075. It utilizes separable convolutions, which reduce the number of trainable parameters, making the model more compact without decreasing the performance. EEGNet leverages spatial and temporal convolutions that are applied to EEG signals, which have distinctive frequency and spatial characteristics.\nThe architecture is structured in two main blocks. Block 1 applies temporal convolutions followed by depthwise spatial convolutions to capture both temporal dynamics and spatial relationships across EEG channels. Block 2 employs separable convolutions to further decrease model complexity while combining the extracted features. The output features are then passed through a softmax classifier to perform the final classification."}, {"title": "Shallow ConvNet", "content": "Shallow ConvNet is a CNN architecture inspired by the Filter Bank Common Spatial Patterns (FBCSP) approach introduced by Ang et al.\u00b3\u2078. Unlike FBCSP, which separates feature extraction and classification into distinct stages, Shallow ConvNet integrates these stages within a single network, allowing the model to learn all transformations jointly. The architecture begins with temporal convolution and spatial filtering layers, similar to the bandpass and common spatial pattern (CSP) steps in FBCSP. Following these transformations, the model applies a squaring nonlinearity, mean pooling, and a logarithmic activation function, which together replicate the log-variance computation performed in the FBCSP pipeline. Additionally the Shallow ConvNet use of multiple pooling regions within a trial allows it to learn the temporal structure of band power changes, which enhances classification performance."}, {"title": "EEG Conformer", "content": "EEG Conformer is a novel convolutional transformer architecture which combines convolutional layers with self-attention mechanisms\u00b3\u2079 to capture both local and global features of EEG signals. Unlike conventional convolutional networks that typically focus on local temporal patterns, EEG Conformer incorporates a self-attention module that enables the model to capture long-range dependencies. The convolution module is responsible for learning low-level temporal and spatial features, while the self-attention module extracts global correlations from these local features, allowing the model to understand complex interdependencies in EEG signals."}, {"title": "V. Results", "content": "In this section, we present the results of applying the above methods to the pre-processed EEG signals for the subject identification task. In all approaches, each subject is treated as a separate class. To prevent data leakage and ensure proper"}, {"title": "VI. Conclusion", "content": "In this study, we introduced a novel cueless EEG paradigm for subject identification, where participants imagine words without external cues. Based on this paradigm, we created a comprehensive dataset containing over 4,350 trials from 11 subjects across multiple sessions. To validate the dataset and establish performance benchmarks, we conducted extensive experiments using a broad spectrum of classification approaches, ranging from traditional feature-based methods to state-of-the-art deep learning architectures. Our evaluation methodology employed session-based hold-out validation, where models were trained and tested on data from different recording sessions. This approach ensures the practical applicability of our results by demonstrating the models ability to generalize across temporal variations. The experimental results demonstrated classification accuracies reaching 97.93%, validating both the quality of our dataset and the effectiveness of modern classification approaches for EEG-based subject identification."}, {"title": "Code Availability", "content": "The dataset utilized in this study is publicly accessible at https://huggingface.co/datasets/Alidr79/cueless_EEG_subject_identification. All scripts and code required for data loading, signal processing, training shal-low classifiers, fine-tuning MOMENT, and training end-to-end models are available in the GitHub repository at https://github.com/Alidr79/cueless_EEG_subject_identification. The repository includes detailed instruc-tions on how to use it."}]}