{"title": "Gesture2Text: A Generalizable Decoder for Word-Gesture\nKeyboards in XR Through Trajectory Coarse Discretization and\nPre-training", "authors": ["Junxiao Shen", "Khadija Khaldi", "Enmin Zhou", "Hemant Bhaskar Surale", "Amy Karlson"], "abstract": "Abstract-Text entry with word-gesture keyboards (WGK) is emerging as a popular method and becoming a key interaction for\nExtended Reality (XR). However, the diversity of interaction modes, keyboard sizes, and visual feedback in these environments\nintroduces divergent word-gesture trajectory data patterns, thus leading to complexity in decoding trajectories into text. Template-\nmatching decoding methods, such as SHARK2 [32], are commonly used for these WGK systems because they are easy to implement\nand configure. However, these methods are susceptible to decoding inaccuracies for noisy trajectories. While conventional neural-\nnetwork-based decoders (neural decoders) trained on word-gesture trajectory data have been proposed to improve accuracy, they\nhave their own limitations: they require extensive data for training and deep-learning expertise for implementation. To address these\nchallenges, we propose a novel solution that combines ease of implementation with high decoding accuracy: a generalizable neural\ndecoder enabled by pre-training on large-scale coarsely discretized word-gesture trajectories. This approach produces a ready-to-use\nWGK decoder that is generalizable across mid-air and on-surface WGK systems in augmented reality (AR) and virtual reality (VR),\nwhich is evident by a robust average Top-4 accuracy of 90.4% on four diverse datasets. It significantly outperforms SHARK2 with a\n37.2% enhancement and surpasses the conventional neural decoder by 7.4%. Moreover, the Pre-trained Neural Decoder's size is only\n4 MB after quantization, without sacrificing accuracy, and it can operate in real-time, executing in just 97 milliseconds on Quest 3.", "sections": [{"title": "1 INTRODUCTION", "content": "Extensive techniques have been proposed and developed to enable fast\nand accurate text entry in Extended Reality (XR) environments [15,21,\n33,46,60]. Among these, word-gesture keyboards (WGK) emerges as\na promising solution, achieving text entry speeds ranging from 20 to 40\nwords per minute (WPM) for proficient users [16, 22, 26, 35, 39, 50, 59,\n62,66]. This method not only offers excellent learnability, being widely\nadopted on touchscreen devices [34], but also inherently handles noisy\nand ambiguous input due to the ambiguous nature of word-gesture\ntrajectories [32]. This is analogous to word-gesture typing on small\nscreens of smartwatches, which effectively addresses the 'fat finger'\nproblem [19].\nHowever, the majority of previous studies focus on the development\nof new interaction techniques. The word-gesture decoding process,\nwhich translates the word-gesture trajectory into text, predominantly\nemploys the classic SHARK\u00b2 [32] decoder. SHARK2 [32] is a template-\nmatching algorithm that computes the similarity of the input trajectory\nwith the pre-defined word-gesture templates constructed from a word\ncorpus and gives top-ranked predictions based on trajectory-template\nsimilarity. The popularity of template-matching decoders stems from\ntheir simplicity; they merely require pre-defining word-gesture tem-\nplates and similarity metrics, enabling the matching algorithm to be\nused in a plug-and-play fashion. However, these algorithms are not with-\nout their limitations, including inability to predict out-of-vocabulary\n(OOV) words and lack of decoding accuracy for noisy input trajecto-\nries [47].\nAlsharif et al. [7] and Shen et al. [50] propose training a neural-\nnetwork-based decoder, or neural decoder, to decode word-gesture\ntrajectories into text. Shen et al. [50] explicitly compared neural de-\ncoders with SHARK2, suggesting that neural decoders significantly\noutperform SHARK2. However, the adoption of neural decoders is\nlimited due to a significant drawback: they require substantial amounts"}, {"title": "2 RELATED WORK", "content": "of training data. We hypothesize that different WGK systems exhibit\ndistinct trajectory patterns, rendering a neural decoder trained on one\nsystem non-generalizable to another. This limitation is due not only to\nvariations in interaction modes, but also to the different keyboard sizes\nand user behaviors, as indicated by the touch point distributions of the\non-surface and mid-air WGK systems in AR and VR in Figure 1. More-\nover, it requires deep expertise to develop and train a neural decoder, as\nthese models are more complex to build and train compared to some\nclassic deep-learning models which already have many open-sourced\ncodebases [11,58]. We aim for a generalizable neural decoder that\ncombines ease of configuration and implementation with high decoding\naccuracy.\nTo achieve this, we propose a novel trajectory representation, which\nis one-hot encoded coarse discretized trajectories (see example illus-\ntration in Figure 6c). This novel representation tolerates high noise\nand differences in trajectories from different WGK interaction modes,\nkeyboard sizes, and user behaviors. Therefore, it enables pre-training a\nneural decoder on datasets from one WGK system to be generalized\nacross other WGK systems without explicit fine-tuning. This eliminates\nthe need for collecting training data and complicated configuration.\nMore specifically, we pre-trained the decoder on a public word-\ngesture trajectory dataset collected from Mobile Phone WGK [34]\ncombined with a dataset of synthetic word-gesture trajectories [49].\nWe further validated our Pre-trained Neural Decoder on four datasets:\none publicly available dataset for mid-air WGK in AR from Shen\net al. [50,51] and three datasets collected ourselves, including mid-\nair WGK in VR with two different interaction modes and on-surface\nWGK (illustrated in Figure 1). Initially, we compared the Pre-trained\nNeural Decoder with a conventional neural decoder from Alsharif et\nal. [7] and SHARK2, demonstrating a 7.4% improvement and a 37.2%\nimprovement, respectively, in Top-4 accuracy. To this end, the Pre-\ntrained Neural Decoder could be used out of the box with an average\ndecoding Top-4 accuracy of 90.4%, which is adequate for large-scale\napplications [47]. We also investigated fine-tuning the pre-trained\ndecoder, achieving a modest improvement. Additionally, we explored\nits individual components, including different discretization techniques,\nencoding methods and model structures. Lastly, we conducted a model\nefficiency validation test by deploying the model onto Quest 3 [42]\nthrough model quantization without sacrificing decoding accuracy [23].\nThe latency was approximately 97 millisecond (ms), which is adequate\nfor commercial use.\nIn conclusion, our contributions are as follows:\n1. We propose a novel trajectory coarse discretization approach to\nenable the pre-training of a word-gesture neural decoder that can be\ngeneralized to on-surface and mid-air word-gesture keyboards in AR\nand VR. The resulting Pre-trained Neural Decoder is fast and easy to\ndeploy and does not require additional data collection for training or\nfine-tuning.\n2. We validated the Pre-trained Neural Decoder on datasets from four\ndifferent AR/VR word-gesture keyboard systems and showed that the\nPre-trained Neural Decoder performs significantly better than conven-\ntional decoders, with a Top-4 word prediction accuracy of 90.4%.\n3. We tested the real-time performance of the pre-trained decoder on a\nmobile VR device, Quest 3, by quantizing the model, suggesting low\nlatency of 97 ms and real-world applicability of the model."}, {"title": "2.1 Word-Gesture Keyboards", "content": "The word-gesture keyboard (WGK), originally developed for text entry\non personal digital assistants (PDAs), Tablet PCs, and mobile phones\nusing a stylus [32, 64], has become one of the dominant text entry\nmethods on modern touchscreen devices. Its success is attributed to\nthe ease of learning [31,64, 65] and the high entry rates achievable\n[31,32,47].\nLeiva et al. [34] conducted a study to collect word-gesture typing\ndata from 909 users. As this study focused solely on data collection, no\nstatistical decoding process was implemented in the keyboard. Expert\nusers achieved a typing speed of 50 words per minute (WPM), while"}, {"title": "2.2 Word-Gesture Decoders", "content": "those unfamiliar with gesture keyboards reached 40 WPM. In contrast,\nReyal et al. [47] conducted a separate study with 12 participants using\nGoogle Keyboard (Gboard) to assess text entry speeds. In this study, the\nentry rate for participants increased from 33.6 WPM in the initial block\nto 39.1 WPM by the ninth block. Despite the difference in participant\nnumbers, with the latter study involving only 12 participants, a clear gap\nin text entry rate (50 WPM vs 40 WPM) is evident between the actual\nword-gesture keyboard text entry rates and the ideal rates assuming\nperfect decoding. This discrepancy underscores the critical need for\nthe development of a more accurate decoder to enhance performance.\nGiven the benefits of word-gesture keyboards on mobile devices,\nresearchers have explored adapting this input method for use in AR and\nVR environments [12, 22, 39, 49, 57, 59, 61, 62]. In these studies, the\nentry rates for the most proficient participants or expert users typically\nspan from 20 to 40 WPM. A commonality among these methods is the\nutilization of the SHARK2 decoder, chosen for its ease of configuration,\nas these studies assert their novelty lies in the unique interaction modal-\nities they introduce. However, the potential for enhanced performance\nthrough the adoption of a more advanced decoder is acknowledged,\nalbeit with the caveat of requiring substantial implementation efforts.\nTherefore, there is a need for an advanced decoder ensuring both high\ndecoding accuracy and plug-and-play capability in academic and re-\nsearch applications.\nIn this section, we first formally define the word-gesture decoding\nproblem. Then, we introduce two commonly used approaches for word-\ngesture decoding: template-matching decoders and neural decoders.\nLastly, we discuss the advantages and disadvantages of the two types\nof decoder, highlighting the need for an efficient joint approach."}, {"title": "2.2.1 Problem Formulation", "content": "When a cursor moves on a virtual keyboard, the movement is captured\nas a word-gesture trajectory, denoted by g. The initial step involves\nencoding this trajectory into a structured representation, E(g). One\ncommonly used encoding approach involves using the Cartesian coor-\ndinate positions of the trajectory at sequential time intervals (illustrated\nin Figure 6a), expressed as E(g) = [x\u2081,x\u2082,...,xT], where T is the length\nof the resampled cursor trajectory, and each x\u209c \u2208 \u211d\u00b2 is the location of\nthe cursor at time t. Both the current template-matching decoder and\nneural decoder primarily use this encoding approach.\nFollowing this, word-gesture decoding is mathematically articulated\nas finding w* = arg maxw P(w|E (g)), which essentially means deter-\nmining the word w that, given the encoded trajectory E(g), has the\nhighest probability of being the intended input."}, {"title": "2.2.2 Template-Matching Decoder", "content": "Template-matching algorithm is a process that involves comparing an\ninput trajectory g with a set of pre-defined word-gesture templates P\nto identify the best match. P = \u222a\u1d62 P\u1d62 is constructed from a lexicon\nC = {w\u2081,w\u2082,..., wn} where n denotes the number of unique words\nin the lexicon. Each word w\u1d62 in the lexicon is mapped to a word-\ngesture template P\u1d62 through a specific process (ConstructTemplate). A\nnaive (ConstructTemplate) process is to sequentially connect the cen-\ntral points of each letter within a word w\u1d62 on a keyboard to generate\na simple template P\u1d62. More specifically, for a given encoded input tra-\njectory E(g), the algorithm computes a similarity score S(E(g), P\u1d62) (eg.\nEuclidean distance) for each pre-defined template P\u1d62. The algorithm\nthen selects the template P\u2c7c with the highest similarity score as the best\nmatch for the encoded trajectory E(g): P\u2c7c = argmax S(E(g), P\u1d62).\ni\u2208{1,2,...,n}\nSHARK2 [32] is one of the most frequently used template-matching\ndecoders. The SHARK2 algorithm employs a sophisticated template-\nmatching technique that leverages a multi-channel architecture to en-\nhance recognition accuracy for word-gestures [32]. Through these\nmechanisms, SHARK2 efficiently narrows down the vast space of\npotential matches by quickly discarding non-viable candidates and fo-\ncusing on those most likely to be correct. This multi-channel approach,"}, {"title": "2.2.3 Neural Decoder", "content": "A neural decoder [25, 50] is represented by a function De, which\nmaps an input y to a probability distribution over the alphabet for each\ntime step. This function is parameterized by weights and biases \u03b8,\nwhich are learned during training. The model is trained using the\nConnectionist Temporal Classification (CTC) loss [20], which aligns\nthe output sequence with the target label sequence z by maximizing\nthe probability of z given across all possible alignments, considering\ninsertions of the CTC blank label  where necessary.\nFor each time step t, the model outputs a probability distribution\n\u03c0t over the extended character alphabet L'char The extended alphabet\nincludes all the characters in the model's alphabet plus a special token\nfor the CTC blank label, denoted as . If the original alphabet Lchar =\n{a,...,z} represents 26 lowercase English letters, then the extended\nalphabet L'char = Lchar\u222a{ } includes these letters plus the blank token.\nThe output \u03c0t for each time step t is a vector in the simplex \u2206char,\nmeaning that it represents a probability distribution across the extended\nalphabet. The dimension of \u03c0t is |L'char|, where |L'char| is the size of the\nextended alphabet. If we consider just the lowercase letters plus the\nblank token, L'char = 27.\nThe complete output of the model for the entire swipe gesture g\nis a sequence of these probability distributions across all time steps,\nso \u03c0 = [\u03c0\u2081, \u03c0\u2082,...,\u03c0T]. Each \u03c0t \u2208 \u2206|L'char|, making the dimension of\n\u03c0 to be T \u00d7 |L'char|. Figures 6b and 6d both visualize the probability\ndistributions \u03c0 using heatmaps.\nFormally, the neural decoder model can be mathematically repre-\nsented as follows:\nDe : \u211d\u1d50\u02e3\u1d40 \u2192 (\u2206/\u1d38'\u1d9c\u02b0\u1d43\u02b3/)\u1d40\nwhere Do(y) = \u03c0 and each \u03c0\u209c \u220b \u2206|\u1d38'\u1d9c\u02b0\u1d43\u02b3| for t = 1,...,T."}, {"title": "2.2.4 Comparison Between SHARK2 and Neural Decoder", "content": "\u2022 SHARK2 is Easier to Configure and Implement.\n\u2022 SHARK2 supports customizable (ConstructTemplate) process, thus\nit enables the adaptability to different technologies. In contrast, a\nneural decoder requires a large dataset for effective training, espe-\ncially for novel WGK systems in AR and VR. One could theoretically\nutilize existing word-gesture data from Mobile Phone WGK [34] to\nadapt the data to different keyboard sizes through various transfor-\nmation functions. However, as illustrated in Figure 1, the intrinsic\nproperties of the data may significantly differ, resulting in suboptimal\nperformance. Additionally, creating a neural decoder demands deep\nlearning expertise, and costly resources on complex training and\nhyperparameter tuning processes.\n\u2022 Neural Decoder has Significantly Better Decoding Accuracy.\nDespite SHARK2's ease of configuration, SHARK2 suffers from\npersistent performance gaps compared to neural decoders. Shen et\nal. [50] demonstrated a substantial improvement in accuracy when\nemploying a neural decoder, achieving a low error rate of 5.41%, as\nopposed to the higher Character Error Rate (CER) of 35.34% found\nwith SHARK2. Character Error Rate is the percentage of characters\nthat were incorrectly predicted compared to the total number of pre-\ndicted characters. Naturally, for efficient text entry, high prediction\naccuracy from word-gesture decoders is essential.\n\u2022 SHARK2 is a Word-Level Model and Neural Decoder is\nCharacter-Level Model.\nSHARK2 uses word-gesture templates from a lexicon, enabling pre-\ndictions only for words within this set. Thus, SHARK2 is a word-\nlevel model. Conversely, a neural decoder, embodying a character-\nlevel model, predicts characters sequentially without relying on a set\nof pre-defined templates. This character-level model can progres-\nsively predict with each character word-gesture, offering enhanced\nflexibility and immediacy in text entry."}, {"title": "3 PRE-TRAINING A NEURAL DECODER INTO A GENERALIZ-ABLE MODEL", "content": "As outlined in the comparison between SHARK2 and the conventional\nneural decoder discussed in Section 2.2.4, there is a critical need for\na model to achieve a balance between ease of configuration and high\naccuracy in word-gesture decoding. To this end, we introduce a model\nthat combines the simplicity of setup with robust performance, thereby\nmitigating the weaknesses of each of the former approaches. Draw-\ning inspiration from large language models known for their zero-shot\nlearning capabilities, which are achieved through pre-training on vast\ncorpora [10,30], we aim to pre-train a neural decoder with a substantial\nvolume of data to achieve generalizability for different WGK systems.\nHowever, direct training using the Cartesian coordinate sequence x\nutilized by both SHARK2 [32] and the neural decoder [7] is impracti-\ncal, as these sequences are fine-grained and vary significantly across\ndifferent WGK systems in AR and VR.\nWe propose a novel encoding method that encodes the continuous\ntrajectory sequence into a coarse discretized representation, as illus-\ntrated by Figure 2. This approach addresses the challenge of dataset\nvariability and enhances our model's understanding of trajectory pat-\nterns. In the following sections, we will elaborate on our discretization\nmethodology, provide details of our training data, and describe the\nspecialized architecture of our model designed to optimize pre-training\neffectiveness."}, {"title": "3.1 Word-Gesture Trajectory Discretization", "content": "Encoding is the process of transforming unstructured data into struc-\ntured elements that a computer can process. In previous attempts to\nencode word-gesture trajectories [7,32, 50], the strategy involved uti-\nlizing Cartesian coordinate positions. From an information theory\nperspective, using coordinate positions directly for word-gesture decod-\ning has limitations due to their continuous nature, whereas the desired\noutput (characters) is discrete. Continuous data can vary infinitely\nwithin a given range, leading to a high degree of uncertainty and re-\nquiring more information to specify each point precisely. In contrast,\ndiscrete outputs, such as characters, have a finite set of possibilities.\nThis mismatch means that directly mapping continuous input to dis-\ncrete output can introduce inefficiencies and inaccuracies, necessitating\nalgorithms to effectively bridge this gap by discretizing the continuous\ninput or employing strategies to reduce the information loss during this\nconversion.\nOur method discretizes a word-gesture trajectory into coarse dis-\ncretized 'pixel' regions on the keyboard, a process we refer to as word-\ngesture trajectory discretization. Figure 2 demonstrate this discretiza-\ntion process and Figure 6c visualize the discretized trajectory in one-hot-\nencoding for the word 'quickly' using a heatmap. Instead of tracking\ncontinuous movement, we assign each segment of a word-gesture tra-\njectory to the corresponding 'pixel' region it traverses according to\na mapping function. This approach simplifies complex word-gesture\ntrajectories into a sequence of discrete 'pixels.' This discretization\nenhances the accuracy of recognizing word-gesture patterns by min-\nimizing the impact of noise in position tracking, variations in user\nbehavior and keyboard sizes, thereby making the neural decoder more\nefficient at predicting user input. This method effectively bridges the\ngap between the continuous nature of word-gesture trajectory and the\ndiscrete structure of text input, enabling more accurate and efficient\ndecoding of word-gesture trajectories. Formally:"}, {"title": "3.2 Training Dataset", "content": "1. Discretizing the keyboard into discretized regions based on the\npositions of the keys, where each region is defined by a square with\nheight and width proportional to the key tab's dimensions, adjusted by\na ratio factor.\n2. Define a mapping function C(x) that maps a given trajectory point\n(xi) to a region based on the discretized regions it falls within. This\nfunction embodies the discretization process, assigning each point on\nthe keyboard to a specific region.\n3. The discretized trajectory T can be expressed as T = {C(x\u1d62) | (x\u1d62) \u2208\nS}, where each element C(xi) is the region corresponding to the seg-\nment of the trajectory passing through that region on the keyboard. The\ndiscretized trajectory is then one-hot encoded and used as the input for\nthe neural decoder.\nThis mathematical formulation encapsulates the process of trans-\nforming continuous word-gesture trajectory into discrete sequences of\n'pixels'."}, {"title": "3.3 Implementation Details", "content": "We pre-trained the pixellated neural decoder using two datasets:\n1. Mobile Phone WGK Dataset: Our research utilizes a public dataset,\n'how we swipe' [34], gathered through a web-based custom virtual\nkeyboard on mobile devices. This dataset includes 8,831,733 touch\npoints corresponding to 11,318 unique English words gestured by 1,338\nusers, with 11,295 unique words correctly gestured and 3,767 words\ngestured inaccurately.\n2. Synthetic Dataset: Our study integrates the GAN (Generative Ad-\nversarial Network)-Imitation model proposed by Shen et al. [48], which\nthey subsequently applied this model to synthesize word-gesture tra-\njectories [49]. They then performed extensive evaluations and compar-\nisons of the GAN-Imitation model with other techniques for generating\nsynthetic word-gesture trajectory data to train neural decoders [50].\nFor our research, we adopted the synthetic strategy detailed in [50],\ntraining the synthetic model using the Mobile Phone WGK Dataset.\nIn this paper, we do not analyze synthetic data generation methods,\ninstead concentrating our efforts on examining the discretization and\npre-training approaches.\nWe have constructed a large-scale training dataset comprising 95,649\ntrajectory samples from the Mobile Phone WGK Dataset, alongside\n100,000 trajectory samples from the Synthetic Dataset. This dataset\nconsists of 32,347 unique words.\nHere is a detailed description of our model and the associated training\nspecifics. We conducted a comprehensive hyperparameter optimization\nprocess to determine the values of the hyperparameters [9].\n\u2022 Model Configuration: We use PyText [41] to implement our model.\nThe core of our model comprises a Bi-directional Long Short-Term\nMemory (BiLSTM) [25,53] layer, which is crucial for understanding\nthe temporal dependencies within the input sequences. This repre-\nsentation layer consists of two stacked LSTM layers with a hidden\ndimension of 222, allowing the model to capture both forward and\nbackward context effectively. To prevent overfitting, a dropout rate of\n0.3 is applied within this layer, providing regularization by randomly\nomitting a subset of features during training. Following the creation\nof the representation layer, a dense fully connected layer serves as an\nintermediary, facilitating the transition from the LSTM output to the\ndecoder. This layer employs a Rectified Linear Unit (ReLU) [6] acti-\nvation function, layer normalization [8], and an additional dropout\nrate of 0.3 to maintain regularization. The final component of our\nmodel is the decoder layer, which utilizes a Connectionist Temporal\nClassification (CTC) [20] beam decoder. This decoder implements a\nbeam search algorithm [18] to efficiently explore the most probable\nword candidates by evaluating combinations of character probabil-\nities at each timestep. Beam search enhances the model's ability\nto predict sequences accurately by considering multiple hypotheses\nconcurrently."}, {"title": "4 VALIDATION DATASETS", "content": "\u2022 Training Details: For training our model, we employed the\nAdam [28] optimizer with a learning rate of 0.01, complemented\nby a minimal weight decay of 0.00001 to prevent overfitting further.\nThe training process was conducted over 600 epochs, with an early\nstopping mechanism disabled to allow the model to fully converge.\nThe batch size for training, evaluation, and testing was uniformly\nset to 128 to balance computational efficiency and training stability.\nThe model was initialized with random weights for training. To\naccommodate the computational demands of training and ensure\nnumerical stability, we adopted mixed-precision training using the\n'FP16OptimizerFairseq' from Fairseq [44], starting with an initial\nloss scale of 128. This approach not only accelerates training but\nalso reduces the memory footprint, allowing for the use of larger\nbatch sizes or models. Additionally, our training regimen included\nreporting metrics to TensorBoard [5] for real-time monitoring and\nanalysis of the model's performance. The best model configuration,\nas determined by Top-4 word prediction accuracy, was automati-\ncally saved and loaded post-training to ensure that our results were\nbased on the peak performance of the model. This configuration\nwas achieved through a hyperparameter sweep, experimenting with\nover 100 configurations, including the number of LSTM layers, the\ndimension of the LSTM layer, the dropout rate, and the learning rate,\nto find the optimal model that achieves the highest accuracy while\nensuring the model size remains below 5 MB.\nWe posit two hypotheses: firstly, that distinct WGK systems exhibit\nunique data patterns; and secondly, that our Pre-trained Neural Decoder\nserves as a universally applicable solution across various WGK systems.\nTo explore these hypotheses, we have collected an array of datasets\nfrom different WGK systems, encompassing a range of interactions\nincluding on-surface touch, mid-air poking (pointing), and mid-air\npinching (raycasting) [43] across both AR and VR platforms featuring\nkeyboards of different sizes. Table 1 gives an overview of the four\ndatasets."}, {"title": "4.1 Mid-Air WGK in VR", "content": "We begin our exploration by examining WGKs within virtual reality\n(VR), a domain that has seen extensive investigation [14]. Notably,\nplatforms such as Quest Headsets [42] have integrated mid-air WGK\nfunctionalities in their v56 updates [40]. Our primary focus lies on\ntwo interaction modes: mid-air poke WGK and mid-air pinch WGK."}, {"title": "5 WORD-GESTURE DATA ANALYSIS", "content": "Despite extensive studies exploring these two interactions [15,52], there\nare no public datasets available on VR mid-air WGK. To address this\ngap, we undertook our own data collection, involving a substantial scale\nof 200 users. The details of our study are as follows:\n1. Participants: We recruited 200 volunteers as participants through\nan internal mailing list, with an age distribution as follows: 46 in their\n20s, 74 in their 30s, 51 in their 40s, and 29 in their 50s. The group\ncomprised 100 males and 100 females. We collected responses from\nparticipants about the frequency of their word-gesture typing usage on\nmobile phones: 20% had rarely or never used it, 32% sometimes used\nit (at least once a month), 26% often used this feature (at least once a\nweek), and 22% always used the feature (at least once a day).\n2. Procedure: Data collection was conducted in the participants'\nhomes using the retail Quest 2, with the Android Application Package\n(APK) delivered via the experiment app on the Oculus Store. Partici-\npants were randomly assigned to two groups, each experiencing two\nconditions: pinch first and poke first. Before these conditions, users\nwere introduced to the study through a step-by-step tutorial explaining\nthe experiment. Figure 3a and Figure 3b demonstrate the word-gesture\ntyping technique with pinch and poke interaction modes used in the\nstudy. The experiment application uploaded the study logs via a Drop-\nbox API upon completion. We employed a 'Wizard of Oz' decoding\nstrategy, akin to that described in Shen et al. [51], to guarantee the\nintegrity of the data collected. Each condition for one participant takes\naround 30 minutes to complete.\n3. Phrase Set: The phrase dataset was curated from the GLUE [56],\nMacKenzie [37], and Enron [29] datasets, resulting in a collection\nof 2,700 unique phrases comprising 3,000 unique words. No other\ninformation except text entry data was collected during the study, and\nthe text entry data was anonymized."}, {"title": "4.2 Mid-Air WGK in AR", "content": "We investigate the word-gesture typing datasets through two analyti-\ncal perspectives: the assessment of touch point distributions and the\nevaluation of a geometric feature: curvature."}, {"title": "5.1 Touch Point Distribution Analysis", "content": "The distinction between AR and VR in the context of mid-air WGK lies\nin the interaction feedback when directly poking the virtual keyboard:\nAR allows for 'real' hand interaction with a virtual keyboard, whereas\nVR facilitates interaction through a virtual hand with a virtual keyboard.\nConsequently, VR interaction is more precise because users can accu-\nrately see the virtual hand's position relative to the virtual keyboard,\ncreating a closed-loop feedback system from the user's physical hand\nto the virtual hand control. Conversely, in AR, the absence of a virtual\nhand and the inaccuracies in hand tracking lead to an open-loop nature\nin hand interaction and control, potentially resulting in noisier input\ndata [27]. In contrast, AR and VR mid-air pinch word-gesture typing\noperate similarly, offering a closed-loop system of projected remote\ncursor control on the virtual keyboard without significant differences.\nTherefore, in AR, the focus is on poke-based mid-air WGK.\nWe utilized two public datasets regarding AR mid-air pinch word-\ngesture typing, which were taken from the studies conducted by Shen et\nal. [50,51]. In one study, Shen et al. [51] introduced AdaptiKeyboard, a\npersonalizable mid-air WGK for AR specifically designed for HoloLens\n2. This keyboard employs multi-objective Bayesian optimization to\ndynamically adjust the keyboard size, aiming to optimize both speed\nand accuracy concurrently. The data collected for this study involved"}, {"title": "5.2 Geometric Feature Analysis", "content": "We utilized the visualization methods from Chen et al. [35] to compute\nthe touch point distributions (visualized by plotting 95% confidence\nellipses for each key) across various validation datasets. Additionally,\nwe compute the statistics of the touch point distributions for each\ndataset:\n\u2022 Average Distance to Key Center (ADKC): The average distance\nfrom the center of each ellipse to the centers of the corresponding\nkeys, again averaged over all keys. This metric assesses the bias in\ntouch point locations relative to the intended target. The equation for\ncomputing this statistic is:\nADKC = 1/N \u03a3\u1d62\u221a((x\u1d9c,\u1d62 - x\u1d4f,\u1d62)\u00b2 + (y\u1d49,\u1d62 - y\u1d4f,\u1d62)\u00b2)\nwhere (x\u1d9c,\u1d62, y\u1d9c,\u1d62) denotes the center of the ellipse for key i, and\n(x\u1d4f,\u1d62, y\u1d4f,\u1d62) represents the center of key i.\n\u2022 Average Major Axis Length (AMAL): The average of the major\naxes lengths of the 95% confidence ellipses, calculated across all keys.\nThis quantifies the spatial dispersion of touch points for each key,\nproviding a measure of touch accuracy and precision. The equation\nfor this statistic is:\nAMAL = 1/N \u03a3\u1d62 \u221a(2\u03bb\u1d62,\u2098\u2090\u2093)\nwhere \u03bb\u1d62,\u2098\u2090\u2093 represents the largest eigenvalue of the covariance ma-\ntrix for the touch points on key i, and N is the total number of keys.\nThe computation of these statistics is based on normalizing trajectory\ndata to a unit-width keyboard for a fair comparison between keyboards\nof different sizes. The plots for these distributions are displayed in\nthe second row of Figure 1, and the statistics are presented in Table 1.\nOur observations revealed distinct word-gesture typing interactions,\neach showcasing a unique distribution of touch points. Specifically, AR\nmid-air WGK exhibits more noise compared to VR WGK, attributable"}, {"title": "6 EXPERIMENTS", "content": "We evaluate the geometric distinctions across datasets through a geo-\nmetric feature: curvature [49]\u00b9.\nCurvature quantifies the local bending at any point along a curve,\nserving as an indicator of the degree to which it diverges from a straight\nline. A high curvature value signifies a pronounced bend, whereas a\nlow curvature corresponds to a minor bend or a linear trajectory.\nFigure 5 showcases the diversity in curvature present across the\ndatasets. We note that VR-based WGK exhibits a broader and lower\ncurvature range, suggesting a predominance of more linear trajecto-\nries. This pattern could stem from extensive body movements in a\nlarge interaction space, which tend to produce more straightforward\ntrajectories. Conversely, AR maintains less linear trajectories due to\nthe challenges associated with the interaction between physical hands\nand virtual keyboard. Among the datasets analyzed, on-surface WGK\ndisplays the most pronounced curvature, indicative of highly curved\ntrajectories. This observation is attributed to the less direct control over\nthe cursor compared to the control mechanisms found in Mobile Phone\nWGK and Mid-Air WGKs.\nOur research uses a series of experiments to assess the efficacy of a\nPre-trained Neural Decoder across the four datasets. Initially, we eval-"}, {"title": "6.1 Baselines Comparison", "content": "uate the performance of the pre-trained decoder against benchmarks,\nspecifically SHARK2 and a conventional neural decoder, to establish"}]}