{"title": "A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and Mitigation Strategy", "authors": ["HUANDONG WANG", "WENJIE FU", "YINGZHOU TANG", "ZHILONG CHEN", "YUXI HUANG", "JINGHUA PIAO", "CHEN GAO", "FENGLI XU", "TAO JIANG", "YONG LI"], "abstract": "While large language models (LLMs) present significant potential for supporting numerous real-world applications and delivering positive social impacts, they still face significant challenges in terms of the inherent risk of privacy leakage, hallucinated outputs, and value misalignment, and can be maliciously used for generating toxic content and unethical purposes after been jailbroken. Therefore, in this survey, we present a comprehensive review of recent advancements aimed at mitigating these issues, organized across the four phases of LLM development and usage: data collecting and pre-training, fine-tuning and alignment, prompting and reasoning, and post-processing and auditing. We elaborate on the recent advances for enhancing the performance of LLMs in terms of privacy protection, hallucination reduction, value alignment, toxicity elimination, and jailbreak defenses. In contrast to previous surveys that focus on a single dimension of responsible LLMs, this survey presents a unified framework that encompasses these diverse dimensions, providing a comprehensive view of enhancing LLMs to better serve real-world applications.", "sections": [{"title": "1 INTRODUCTION", "content": "The superior abilities of large language models (LLMs) in natural language processing have brought us one step closer to realizing artificial general intelligence (AGI). Language is not merely a medium for communication [219], but also plays a fundamental role in the development of thought and higher-level cognitive processes [198]. Therefore, the breakthrough in language processing has considerably enhanced Al's capacities in both external social communication and internal cognitive functions, advancing its potential to tackle real-world challenges [193]. Consequently, LLMs have enabled a wide spectrum of applications from increasing social productivity to promoting social good, such as code generation [122], autonomous agent construction [63, 219], urban planning [248], improving the efficiency of clinical [192] and education [92], assisting disadvantaged groups [175], paving the way toward a good society where human and AI coexist [165, 193].\nHowever, while we enjoy the convenience provided by LLMs, we have to recognize the potential dangers and vulnerabilities they may pose. Overlooking them has already led to numerous painful lessons [52, 133]. For example, it is reported that Samsung employees unintentionally shared confidential source code and meeting recordings with ChatGPT during tasks like error checking, code optimization, and transcription, exposing sensitive business information of the company [52]. Another famous example is the \u201cGrandma Exploit\u201d, where malicious adversaries can persuade LLMs through role-playing as a grandma to elicit responses that LLMs would typically restrict, e.g., how to create a bomb or write source codes for harmful malware [39, 164, 249], which poses severe risks to society. In a more tragic case, a man tragically ended his life after AI chatbots encouraged him to commit suicide during a six-week conversation about the climate crisis, worsening his eco-anxiety and suicidal thoughts [133]. Therefore, it is important to build responsible LLMs, which are capable of mitigating these potential risks and vulnerabilities and preventing such tragedies.\nInspired by these lessons, numerous researchers begin to pay attention to constructing responsible LLMs on different aspects [25, 37, 145, 194, 218, 226]. Among them, the most thoroughly explored aspect is aligning LLMs with human values [8, 33, 98, 104, 119, 127, 148, 163, 171, 183, 188, 188, 218]. These works mainly utilize LLM fine-tuning techniques, and incorporate the value dimensions given by representative human value theories, e.g., the Schwartz Theory of Basic Values, and the Moral Foundations Theory [173, 230]. Nevertheless, responsible LLMs cannot be accomplished merely through value alignment techniques. For example, hallucinated outputs do not inherently misalign with human values, but significantly reduce LLMs' usability. Consequently, multiple aspects should be considered cohesively to construct responsible LLMs. Simultaneously, the dangers and vulnerabilities of LLMs can arise not only from the generated text but also within their internal mechanisms. For example, existing studies have revealed that LLM logits for specific sentences can expose private information [58, 59, 132]. Therefore, it is crucial to consider potential dangers and vulnerabilities throughout the entire lifecycle of LLMs, and employ mitigation strategies at the corresponding phase of developing and utilizing LLMs to build responsible LLMs.\nOverall, we categorize the potential dangers and vulnerabilities of LLMs into two categories, i.e., inherent risk and malicious use (see Figure 1). In terms of inherent risk, LLMs could potentially reveal sensitive information from their utilized corpora for pre-training or fine-tuning, thereby raising issues of privacy leakage [37, 145, 226]. Meanwhile, it is well-known that LLMs may experience hallucinations, resulting in the production of texts that are inaccurate and misleading [194]. Finally, since the values embedded in LLM-generated texts usually directly reflect the distribution of their training data, often sourced from the Internet, there exists a substantial risk that LLMs will overfit to a narrow set of human values or even amplify undesirable and unethical content, resulting in value mismatch. In terms of malicious use, LLMs could be utilized to produce content with toxicity, such as hate speech, harassment, cyberbullying, causing harm to humans [25]. In addition, malicious users may jailbreak LLMs to bypass their safety constraints for fraudulent purposes [123, 225]. In terms of mitigation strategies, we divide the whole process of developing and utilizing LLMs into four phases, i.e., data collecting and pre-training, fine-tuning and alignment phase, prompting and reasoning, and post-processing and auditing phase, and classify existing studies based on the phase they operate on.\nIn this paper, we systematically summarize the recent studies toward constructing responsible LLMs, covering techniques enhancing LLMs in terms of privacy protection, hallucination reduction, value alignment, toxicity elimination, and jailbreak defenses. Compared with existing surveys, which mainly focus on a single dimension for improving the responsibility of LLM (e.g., privacy [37, 145, 226] or hallucination [194]), our survey employs a unified framework to analyze and compare existing studies covering a broader range of dimensions. This comprehensive analysis helps us"}, {"title": "2 RELATED REVIEWS", "content": "Reviews related to our paper are summarized in Table 1. Specifically, Yao et al. [229] surveyed alignment goals of large language models, which include human instructions, human preferences, and human values. Further, they investigated the definition of different alignment goals and benchmarks and methods to evaluate these alignment goals. Wang et al. [204] summarized alignment technologies with human expectations in terms of three aspects, i.e., data collection, training methodologies, and model evaluation. Yao et al. [233] reviewed related papers regarding security and privacy. Specifically, they mainly discussed existing works in terms of three aspects, i.e., LLM's positive impacts on security and privacy, LLM's negative impacts on security and privacy, and vulnerabilities and defenses in LLMs. Das et al. [37] investigated the vulnerabilities in terms of security attacks and privacy attacks, and also reviewed defense mechanisms against security and privacy attacks. Yan et al. [226] focused on the privacy concerns of LLM. Specifically, they revealed the privacy risk of LLMs in terms of passive privacy leakage and active privacy attacks, and summarized major privacy protection mechanisms against passive privacy leakage and active privacy attacks. Neel and Chang [145] comprehensively investigated the privacy issues regarding LLMs, which include memorization, privacy attacks, privacy-preserving techniques, and copyright. Huang et al. [79] investigated the cause of hallucination in LLMs, as well as detection methods, benchmarks, and mitigation methods of hallucination in LLMs. Chowdhury et al. [34] reviewed thirty-two techniques addressing hallucination in LLMs, presenting a taxonomy based on dataset\nutilization, common tasks, feedback mechanisms, and retriever types, and highlighting methods like RAG and CoNLI. Yao et al. [229] reviewed existing works in terms of alignment goals, which are then categorized into three levels, including human instructions, human preferences, and human values. Yi et al. [235] proposed a taxonomy of jailbreak attack and defense methods for LLMs, categorizing attacks into black-box and white-box types and defenses into prompt-level and model-level strategies, further detailing sub-classes and evaluation methods to inspire secure LLM development. Chowdhury et al. [34] presented a comprehensive survey on the security vulnerabilities of LLMs, focusing on adversarial attacks, data poisoning, and privacy risks. They evaluated attack methodologies, model resilience, and defense strategies, providing insights into LLM integrity and user trust. As we can observe, these existing surveys mainly focus on a single dimension for constructing responsible LLMs. However, different aspects of LLM responsibility are deeply interconnected. For example, value alignment and privacy protection share a common principle of preventing the disclosure of harmful information. At the same time, conflicts also exist between dimensions [228, 245]. Consequently, it is crucial to incorporate different dimensions of LLM responsibility comprehensively. Different from them, our survey employs a unified framework to analyze and compare existing studies covering a broader range of dimensions. Furthermore, we also provide a unique perspective of categorizing and comparing existing approaches based on the phases of LLM development and utilization on which they operate."}, {"title": "3 FOUR PHASES FOR EMPLOYING MITIGATION STRATEGY TOWARDS RESPONSIBLE LLMS", "content": "The data collecting and pre-training phase is the first phase in the LLM life cycle, where the LLM is trained on large corpora of text data to learn the language patterns and structures [43, 45, 162, 195]. In this phase, multiple sources of data, including text data from books, academic materials, encyclopedia, code, social media, and webpages, are collected to ensure the LLM can learn from various domains [124]. However, for the data that is collected from the internet and other untrusted sources, it is crucial to ensure the data is clean and free from noise, bias, and other unwanted information [60, 247]. Therefore, quality filtering and privacy reduction processes are necessary to handle the data before the pre-training phase. For example, Chen et al. proposed a data cleaning method called Data-Juicer, which provides a systematic way to process and clean the data for pre-training [27]. Lauren\u00e7on et al. employed a rule-based method to remove personally"}, {"title": "3.1 Data collecting and pre-training phase", "content": "The data collecting and pre-training phase is the first phase in the LLM life cycle, where the LLM is trained on large corpora of text data to learn the language patterns and structures [43, 45, 162, 195]. In this phase, multiple sources of data, including text data from books, academic materials, encyclopedia, code, social media, and webpages, are collected to ensure the LLM can learn from various domains [124]. However, for the data that is collected from the internet and other untrusted sources, it is crucial to ensure the data is clean and free from noise, bias, and other unwanted information [60, 247]. Therefore, quality filtering and privacy reduction processes are necessary to handle the data before the pre-training phase. For example, Chen et al. proposed a data cleaning method called Data-Juicer, which provides a systematic way to process and clean the data for pre-training [27]. Lauren\u00e7on et al. employed a rule-based method to remove personally\nidentifiable information (PII) from the data [102]. These methods can help to improve the quality of the data, which is essential for the performance and sanity of the LLM [126], and also to protect the privacy of the users and the data subjects from privacy leakage and other potential risks [21, 139]. However, both the quantity and quality of the data are important for the pre-training phase. Longpre et al. discovered that when toxic content is filtered out from the pre-training data, the LLM will generate less toxic content in the downstream tasks, while reducing the performance on some other tasks [126]."}, {"title": "3.2 Fine-tuning and alignment phase", "content": "Taking place directly after the data collecting and pre-training phase, the fine-tuning and alignment phase aims to adapt the generic knowledge of pre-trained models to some target tasks [161], e.g., mitigating unwanted outputs and boosting the performances on specific downstream tasks [3]. For example, a representative and widely-adopted method for fine-tune large language models is Reinforcement Learning from Human Feedback (RLHF) [148]. Taking a pre-trained language model as a start, it adopts supervised methods to learn from human instruction data and is subsequently optimized according to human annotators' comparison-based preferences through reinforcement learning. As such, LLMs can better reflect human preferences and act more like humans.\nThe presence of the fine-tuning and alignment phase makes it possible to make the full use of the large quantity of data and eases the efforts in model training. Sometimes the quality of the pre-training corpora may not be that satisfying and the corpora may contain unwished contents, e.g., biases [61] and toxicity [72]. Removing them in the data collection and pre-training phase may help, but could be too costly when the size of the corpora explodes. In these circumstances, conducting adequate fine-tuning and alignment turns to be an effective and viable solution: it not only retains the information available in the corpora, but also saves the exhaustive efforts in the complicated manipulation of the original data, e.g., data filtering [61]. Moreover, with fine-tuning and alignment, practitioners can take the full advantage of the knowledge inherited in the existing pre-trained models and no longer need to train everything from scratch [136]. This significantly alleviates their burden in training while maintaining promising results on the downstream tasks of focus."}, {"title": "3.3 Prompting and reasoning phase", "content": "The prompting and reasoning phase is an important aspect of utilizing LLMs to undertake various challenging tasks [137, 246]. Numerous studies have pointed out that the quality of prompts largely determines the reasoning capability of LLMs [131, 168]. For example, a vague prompt like \u201cWrite about the company's goals\u201d might generate a generic response, lacking in detail and relevance. However, a more precise prompt, such as \u201cWrite a two-paragraph introduction for a business proposal that outlines our company's goals for expanding into the renewable energy market in 2024,\" would lead the LLMs to generate a far more tailored and useful response. Therefore, to better utilize LLMs' capabilities in reasoning, some researchers have developed various prompting methods [26, 77]. One well-known approach is Chain-of-Thought (CoT) prompting, which allows LLMs to generate intermediate reasoning steps before arriving at a final answer, enhancing their problem-solving and decision-making accuracy [208]. Based on the idea of CoT, a variety of variants have been proposed. For example, the Zero-shot-CoT [97] is designed to trigger the reasoning capability by incorporating prompts like \u201cLet's think step by step\u201d. Recently, inspired by heuristic searching, Yao et al. [231] design the Tree-of-Thought prompting method. Moreover, Yu et al. [238] propose the Thought Propagation method by leveraging analogous problems and solutions to further enhance the quality of reasoning.\nAlthough numerous studies have highlighted the superior reasoning capability of LLMs [26, 77], the prompting and reasoning phase is vulnerable, especially when facing inappropriate prompts and deliberate attacks [61, 221]. On the other hand, improving this phase can further contribute to building responsible LLMs [61, 221]. For example, Xie et al. [221] find LLMs could be jailbroken by specific adversarial prompts. However, they also point out that self-reminder prompting, a design imposed on the promoting and reasoning phase, significantly reduces the success rate of jailbreak attacks [221]. Overall, the prompting and reasoning phase plays a pivotal role in building responsible LLMs.\""}, {"title": "3.4 Post-processing and auditing phase", "content": "The post-processing and auditing phase is the last feasible phase in the LLM life cycle, following the completion and stabilization of the preceding three phases. During this phase, the LLM is thoroughly pre-trained and does not permit additional fine-tuning or alignment, regardless of whether these processes have already been carried out. Additionally, the prompting and reasoning templates are meticulously curated to meet the specific task requirements and are not subject to revision. Thus, LLM only provides a pure black-box access API to output generated texts based on given queries. Audit-and-Process is the general pipeline to improve the responsibility of LLMs in this phase, where auditing and processing algorithms are introduced to detect and obliterate the latent harmful information in LLMs' generated texts [89, 95, 128, 223]. This pipeline has been adopted by several studies to prevent LLM from directly releasing text that contains personally identifiable information [95, 128], abusive language [10, 15, 184, 202] and cyberbullying comments [31, 88, 100, 113]. Existing auditing algorithms can be divided into rule-based [32, 99, 174] and machine learning-based approaches [101, 128, 156, 197]. Early rule-based methods are simply and straightforward for deploying, which leads to better explainability of errors [32], but lack flexibility and accuracy [32, 143]. Thus, modern methods who have achieved state-of-the-art results most often resort to machine learning techniques [101, 128, 156, 197]. After auditing generated text, there are three common safeguard strategies to obiliterage detected harmful information: scrub [128], regeneration [89, 206] and rejection [223]. For cases where only certain tokens or chunks within the generated sentence are harmful, such as personally identifiable information [128], the post-processing algorithm will mask or replace those elements [128]. For situations where the overall generated content is harmful, such as racism and sexism statements [70], the post-processing algorithm will consider regenerating a harmless text [89] or directly refusing to answer prompts that are suggestive and harmful [223]. Compared to earlier phases, deploying strategies during the auditing and post-processing phases offers greater compatibility, as this strategy only requires pure black-box access to the LLM, making it suitable for all categories of LLMs. Moreover, this pipeline does not involve any adjustments to the fine-tuned LLM, making it more promising in terms of reducing performance decay in the LLM. However, the Audit-and-Process pipeline may significantly increase the time and computational cost required during the inference phase, as regeneration can be time-consuming and may require multiple iterations to bypass the auditing algorithm [89]. The auditing algorithms in this pipeline may also affect the continuous, real-time, or interactive generation of LLMs, as some algorithms might require waiting for the LLM to finish generating [32, 99, 101, 128, 156, 174, 197]."}, {"title": "4 PRIVACY", "content": "LLMs are typically pre-trained on massive and myriad corpora fetched from websites, code reposi- tories, user posts, and other sources that may contain a large amount of privacy-sensitive informa- tion [128]. Due to the strong few-shot capabilities of LLMs and the development of prompting and reasoning techniques, some privacy data, such as clinical diagnosis notes, appears as demonstrations\nin the In-Context Learning (ICL) scenario [201]. Therefore, privacy risks associated with LLMs persist throughout their entire life cycle [155], making it crucial to assess and mitigate these risks to build responsible LLMs. In this section, we comprehensively examine privacy risks throughout the four phases in the life cycle of LLMs. Initially, we explore the privacy vulnerability of LLM and the adversarial methods developed to reveal or evaluate the privacy risk in LLMs (\u00a74.1). Subsequently, we delve into the safeguards and countermeasures proposed to mitigate these privacy risks (\u00a74.2). Furthermore, we provide a high-level overview of the typical attack and defense strategies in Figure 2 as the outline of the following content."}, {"title": "4.1 Privacy Risks in LLMs", "content": "In this section, we embark on an inclusive review of the typical privacy risks associated with LLMs. We will examine specific attack methods and corresponding privacy risks associated with privacy-sensitive data appearing at different phases of the LLM lifecycle. Encompassing data from LLM training corpus data to prompting demonstration data and even external data that does not belong to LLM. Except for exposing privacy-sensitive data, the undisclosed parameters of LLM are also shown to be vulnerable to specific attacks [23]. Various privacy evaluation benchmarks are proposed to assess the privacy risks of LLMs, as depicted in Table 2. These benchmarks are designed to evaluate the privacy risks of LLMs from different attack dimensions, including (1) Membership Inference Attack (MIA) [180]: inferring whether a given text record is used for training LLM, (2) Data Extraction Attack (DEA) [20, 24]: extracting the text records that exist in the training dataset, (3) Prompt Inversion Attack (PIA) [142]: stealing the private prompting texts, and (4) Attribute Inference Attack (AIA) [128, 186, 199]: deducing the private or sensitive information from training texts, prompting texts or external texts. Model Extraction Attack (MEA) [23], replicating the parameters of the LLM, is rarely explored and without a dedicated benchmark. As depicted in Table 3, we additionally consolidate these attack techniques based on the disclosed private information, the attack dimension, the level of model access necessitated by the attacker, as well as the dataset or benchmark utilized, and the specific target model on which they are assessed. In the following content, we will delineate five distinct types of privacy breaches, encompassing pre-training data, fine-tuning data, prompting data, external data, and model parameters.\nPre-training Corpus Data. A significant portion of the privacy risk in the training set is attributed to the memorization traces left by the training data on the LLM, which allow specific attacks to infer relevant features of the training data. These memorization-based attacks can be categorized into Membership Inference Attacks [19, 180, 234] and Data Extraction Attacks [57]. Membership inference attacks on pre-training corpus data are referred to as the pre-training data detection task, aiming to ascertain whether a specific textual sample was encountered by LLM during its pre-training phase [59]. Because of the extensive scale of pre-training corpora, the opacity of the training data distribution, and the reduced number of training epochs, detecting pre-training samples is non-trivial [240] and has only garnered limited attention. Shi et al. [178] propose Min-K% that takes average over the k minimum likelihoods of tokens for detection. Zhang et al. [240] further advance Min-K%, incorporate with the insight that training data tends to be around the local maximum in the probability distribution [58]. In contrast to curate a meticulous MIA metric, Fu et al. [59] suggest to instruct LLMs themselves to play as a pre-training data detector and introduce the MIA-Tuner. This method manifests higher confidence and feasibility on both aligned and unaligned LLMs. Data extraction attacks are a more advanced category of privacy attacks that intend to extract partial content deeply memorized by LLM. Carlini et al. [24] first introduce an untargeted training data extraction method, in which the adversary generates enormous samples and then filters the content in the pre-training set. The subsequent works mostly focus on targeted data extraction that recovers the suffix text given the prefix [22]. Carlini\net al. [22] adopt the targeted extraction as a tool to evaluate the memorization of LLM. Yu et al. [239] further investigate and benchmark several overlooked tricks, such as Top-k, Temperature, and Nucleus-n [74], for improving data extraction. Zhang et al. [244] and Ozdayi et al. [149] share a similar idea that triggers LLM to emit more pre-training data through prompt-tuning [108].\nFine-tuning Corpus Data. Since both pre-training and fine-tuning data are essentially training data for LLMs, most privacy attack methods targeting pre-training data are also applicable to fine- tuning data. Furthermore, fine-tuning datasets are typically smaller in scale and security-critical since it is usually encompassed with more private downstream data. Therefore, some existing works more concentrate on compromising the privacy of fine-tuning data. For example, Mireshghallah et al. [141] propose LiRA, an MIA method, for evaluating memorization in fine-tuned Autoregressive Language Models. Mattern et al. [132] design a neighborhood comparison method to detect fine- tuning data with the more practical assumption. Fu et al. [58] achieve a better trade-off between attack performance and assumptions in MIAs against fine-tuned LLMs by fine-tuning a reference model based on the data extracted from the target fine-tuned LLM. Wen et al. [214] introduce a novel privacy backdoor by releasing a tampered pre-trained LLM, which aims to amplify the vulnerability of fine-tuning data to membership inference attack. Similarly, Feng and Tram\u00e8r [55] also employ privacy backdoor to poison pre-trained LLM, which enables adversaries to reconstruct individual fine-tuning data.\nPrompting Demonstration Data. The seminal work of in-context-learning (ICL) has revo- lutionized the field of LLMs [17], which enables LLM adaptation for specific downstream tasks by prompting the model with a series of task demonstrations but without tuning any model's parameters [44]. Although the prompting demonstration data is not included for training LLMs, its private information can be inferred during the inference phase. Existing studies have demonstrated"}, {"title": "4.2 Privacy Protection for LLMs", "content": "In this section, we conduct a detailed review of the defense strategies proposed to address the aforementioned privacy risks. The existing defense methods throughout the whole lifecycle of LLM, including the four phases from the data collecting and pre-training phase to the post-processing and auditing phase that we have mentioned in Section 3. We will categorize all defense methods into four classes based on the phases at which they intervene in the deployment and usage of LLMs. As illustrated in Table 4, we further summarize these defense methods in terms of the defended private information, the attack dimension that can be defended against, as well as the dataset or benchmark, and the target model on which they are evaluated.\nData Collecting and pre-training phase. In the upstream phase of LLM deployment, injecting defensive strategies during the data collection and pre-training phase is most likely to fundamentally eliminate or mitigate the privacy risks of LLM. Several studies suggest to conduct data sanitization for improving privacy before pre-training [91, 105, 128]. For instance, Lee et al. [105] observe that most document-level deduplicated web-scraped datasets still have large-scale sentence-level\nduplication. To tackle this issue, they propose efficient sentence-level deduplication methods and decrease the training consumption by approximately 10 times. Kandpal et al. [91] then demonstrate\nthat the sentence-level duplication can mitigate the privacy risks caused by the model memorization, including MIA, DEA, and AIA. Except from the data deduplication, some works also explore to remove or replace all Personally Identifiable Information (PII) tagged by Named Entity Recognition (NER) modules [128]. Other studies have attempted to incorporate differential privacy (DP) into the pre-training phase [5, 114, 216], which can provide a rigorous privacy guarantee and has a commendable post-processing property (i.e., any subsequent computation or transformation performed will not increase the risk of exposing individual data points [50]). Specifically, Li et al. [114] propose a ghost clipping technique to address the computational challenge and enormous memory usage of employing DP-SGD on LLM and verify that LLMs can be strong DP-learner. Anil et al. [5] train BERT with DP to achieve high accuracy by proposing mega-batches, which scales up the training batch size to millions. Furthermore, Wu et al. [216] present a method to detect and edit private neurons in pre-trained LLMs to address privacy risks, which can reduce the model memorization of private data.\nFine-tuning and alignment phase. Compared with pre-training data, fine-tuning data is usually more vulnerable to adversaries due to some specific characteristics, such as smaller scale and higher confidentiality [236]. Thus, some works focus on developing a privacy-preserving fine-tuning pipeline to guarantee the private information in fine-tuning data will not be disclosed after the fine-tuned LLMs are released [11, 76, 236, 237]. Hoory et al. [76] first propose to fine- tune a differentially private BERT model with small performance degradation through a novel word-piece algorithm and DP-SGD. Yu et al. [236] introduce a meta-framework for achieving a better tradeoff between differential privacy and utility in fine-tuning autoregressive language models, such as GPT-2. Yu et al. [237] propose the reparametrized gradient perturbation (RGP) for applying DP on fine-tuning large models, such as large vision and large language models. Duan et al. [47] present the PromptDPSGD algorithm to conduct private gradient descent on the soft prompt embeddings prepended to the LLM's private input. Besides, fine-tuning is more efficient in computational consumption than pre-training and can avoid training the LLM from scratch [71]. Consequently, some other works consider patching during the fine-tuning stage to fix privacy leaks caused during the pre-training phase [59, 84, 149]. Such as Ozdayi et al. [149] adopt prompt-tuning to control the vulnerability of LLM against extraction attack, which significantly decreases the extraction rate with competitive PPL values. Fu et al. [59] introduce two defense strategies with their proposed MIA-Tuner framework to fine-tune both aligned and unaligned LLMs to defend against MIA. Recently, the privacy-preserving regulations, including the EU's General Data Protection Regulation (GDPR) and US's California Consumer Privacy Act (CCPA), have also required that users have the right to be forgotten. As a consequence, several studies have started to investigate how to fine-tune a pre-trained LLM to unlearn specific content that has included pre-training corpora and needs to be forgotten [30, 53, 84, 93]. Jang et al. [84] simply fine-tune LLM on the text to be forgotten while negating (maximizing) the original loss function. Chen and Yang [30] introduce to fine-tune a lightweight unlearning layer via a selective teacher-student formulation, and they further propose a fusion mechanism to combine different unlearning layers to handle a sequence of forgetting operations. Eldan and Russinovich [53] point out that simply negating the loss function for unlearning may not yield satisfying results. In contrast to fine-tuning LLM on the text that needs to be forgotten, they suggest to replace the idiosyncratic expressions in the target data with generic counterparts, and then fine-tuning LLMs on the regenerated text.\nPrompting and reasoning phase. The widespread use of in-context learning in downstream tasks under LLM has highlighted the risk of prompt demonstration data being stolen. To address this issue, some studies have proposed privacy-preserving in-context learning, which ensures that private demonstration examples are not disclosed while maintaining the performance of LLM few- shot learning [47, 48, 75, 190, 215]. Duan et al. [48] introduce to ensemble multiple prompted LLMs\nwith disjoint demonstrations to aggregate the prediction probability vectors, which will moderately mitigate the privacy risks in a certain extent. Duan et al. [47] further propose PromptPATE to orchestrate a noisy vote among an ensemble of LLMs presented with different demonstrations, which follows the general flow of standard PATE (private aggregation of teacher ensembles) [151] and first strike rigorous differential privacy in ICL. However, PromptPATE [47] requires to transfer the knowledge from private labeled data to an unlabeled public dataset which may not exist in practice. Hong et al. [75] believe that LLM services may be hosted by untrustworthy providers, and directly sending sensitive private information to the provider can be dangerous. Thus, they introduce DP-OPT to fine-tune a differentially-private prompt in a local trusted LLM and then transfer this prompt to the cloud LLM. The aforementioned privacy-preserving ICL methods are all stuck in classification task [47, 75], which will limit the generalizability for employing the defense methods on some ICL tasks. Wu et al. [215] propose DP-ICL, a general paradigm for privatizing ICL in text classification and language generation tasks, which generates differentially private responses through a noisy aggregation among an ensemble of LLM's responses based on disjoint exemplar sets. Similarly, Tang et al. [190] also present a comparable concept regarding the construction of LLM ensembles. However, in contrast to the aggregation procedure in DP-ICL [215], they no longer combine the output results of each prompted LLM. Instead, they predict the next generated token by consolidating noisy probabilities, which may be more applicable to various ICL tasks and could potentially yield enhanced performance.\nPost-processing and auditing phase. In numerous practical scenarios, the defender may not have the access to intervene in the deployment of LLMs in earlier phases. For instance, certain LLM providers solely provide services via black-box APIs. Consequently, integrating audit and post- processing modules during the LLM text generation phase represents a more adaptable approach to bolster LLM privacy. Several studies have explored to defend diverse attack methods during the post-processing and auditing phase [23, 67, 130, 142, 186]. Majmudar et al. [130] introduce the DP-Decoding, a method to conceal the pre-training data, which perturbs the probability distribution of the next token predicted by the LLM. Although the DP-Decoding provides a possible solution, the performance degradation under a moderate differential privacy is nearly unacceptable. Ginart et al. [67] aim to protect the fine-tuning data through a differentially private decoding mechanism named SUBMIX that follows the PATE [151] to mix the ensemble predictions on the next token. However, SUBMIX [67] requires an ensemble of fine-tuned LLM that substantially increases the computation and memory consumption. Morris et al. [142] design a dynamic sampling method to prevent the private information in the demonstration prompt is stolen, which dynamically adjusts the decoding parameters, such as temperature, top-p, and top-k, during the inference stage. Staab et al. [186] propose to defend against attribute inference attacks on external text data by eliminating PII from the output results of LLMs. Carlini et al. [23] believe that by adding a sufficient amount of noise to the output logits of any given query, it would be possible to thwart the model extraction attack."}, {"title": "5 HALLUCINATION", "content": "Despite the rapid advancement of LLMs, hallucinations have emerged as one of the most vital concerns surrounding their use [54, 79, 86, 110, 242", "242": ".", "79": "."}]}