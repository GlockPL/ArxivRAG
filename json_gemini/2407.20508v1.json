{"title": "Unveiling the Potential of Spiking Dynamics in Graph Representation Learning through Spatial-Temporal Normalization and Coding Strategies", "authors": ["Mingkun Xu", "Huifeng Yin", "Yujie Wu", "Guoqi Li", "Faqiang Liu", "Jing Pei", "Shuai Zhong", "Lei Deng"], "abstract": "In recent years, spiking neural networks (SNNs) have attracted substantial interest due to their potential to replicate the energy-efficient and event-driven processing of biological neurons. Despite this, the application of SNNs in graph representation learning, particularly for non-Euclidean data, remains underexplored, and the influence of spiking dynamics on graph learning is not yet fully understood. This work seeks to address these gaps by examining the unique properties and benefits of spiking dynamics in enhancing graph representation learning. We propose a spike-based graph neural network model that incorporates spiking dynamics, enhanced by a novel spatial-temporal feature normalization (STFN) technique, to improve training efficiency and model stability. Our detailed analysis explores the impact of rate coding and temporal coding on SNN performance, offering new insights into their advantages for deep graph networks and addressing challenges such as the oversmoothing problem. Experimental results demonstrate that our SNN models can achieve competitive performance with state-of-the-art graph neural networks (GNNs) while considerably reducing computational costs, highlighting the potential of SNNs for efficient neuromorphic computing", "sections": [{"title": "1. Introduction", "content": "Spiking neural networks (SNNs) are brain-inspired computational models that replicate the spatio-temporal dynamics and rich coding schemes inherent in biological neural systems. These features make SNNs particularly adept at mimicking neuroscience-inspired models and performing efficient computations on neuromorphic hardware. SNNs have been successfully applied to a variety of tasks, including image classification, voice recognition, object tracking, and neuromorphic perception [1, 2, 3, 4, 5, 6]. However, most existing studies have focused on processing unstructured data in Euclidean space, such as images and language [7]. Meanwhile, biological spiking neurons, characterized by their complex coding schemes and dynamic properties, are foundational in creating cognitive maps in the hippocampal-entorhinal system. These maps are crucial for organizing and integrating spatial relationships and relational memory, facilitating advanced cognitive functions [8]. Thus, there is a strong motivation to explore effective algorithms for training SNNs that can integrate topological relationships and structural knowledge from graph-based data.\nIn recent years, there have been attempts to combine SNNs with graph-based scenarios, but these have primarily focused on applying graph theory to analyze spiking neuron characteristics and network topologies [9, 10, 11], or utilizing spiking neuron features to tackle simple graph-related problems such as shortest path, clustering, and minimal spanning tree problems [12, 13]. A recent study [14] introduced graph convolution to preprocess tactile data for SNN classification, achieving high performance in sensor data classification. However, this approach faces challenges in accommodating general graph operations and adapting to other scenarios. In addition, although some works have investigated the modelling of SNN and graph learning [15, 16, 17], they overlooked a more in-depth and systematic exploration of how spiking dynamics could impact and enhance graph representation learning. On the other hand, numerous graph"}, {"title": "2. Related Work", "content": "neural network (GNN) models, such as Graph Convolution Network (GCN) and Graph Attention Network (GAT), have made significant advances in solving graph-related tasks [18, 19]. Despite these advancements, few works have explored the interplay between graph theory and neural dynamics, and many suffer from substantial computational overhead when applied to large-scale data, potentially limiting their practical applications.\nThis study aims to address these challenges by investigating the potential of spiking dynamics in graph representation learning, focusing on the development of a comprehensive spike-based modeling framework. We also introduce spatial-temporal feature normalization techniques to enhance training efficiency and model stability. Our approach leverages the unique benefits of SNNs in processing structured data, providing insights into reducing computational costs and improving stability in graph-based learning scenarios.\nTo investigate the potential and benefits of spiking dynamics in the context of graph-structured data, we introduce an innovative graph SNN framework designed to handle non-Euclidean data using flexible aggregation methods. The development of such SNN models presents two major challenges. The first is integrating multiple graph convolution operations with spiking dynamics. To address this challenge, we unfold binary node features across both temporal and spatial dimensions, proposing a comprehensive spiking message-passing technique that integrates graph filters with spiking dynamics iteratively. This approach allows for the effective merging of graph convolution operations and spiking dynamics, leveraging the unique properties of SNNs for processing complex, structured information. By iteratively incorporating graph filters into the spiking message-passing process, our framework can dynamically adapt to the intricate data structures typical of non-Euclidean spaces, enhancing the model's ability to abstract and generalize features from graph-based input The second major challenge is ensuring the convergence and performance of training in graph SNNs. Given the intricate spiking dynamics and the diverse nature of graph operations, directly training SNNs on graph-structured data remains an underexplored and challenging area. While various normalization techniques have proven effective for enhancing network"}, {"title": "2.1. Spiking Neural Networks", "content": "Over the past few years, Spiking Neural Networks (SNNs) have gained significant attention in the field of neuromorphic computing, which seeks to emulate the biological plausibility of the human brain. Unlike traditional Artificial Neural Networks (ANNs), SNNs are adept at processing sequential data. These networks mimic the behavior of biological neurons by incorporating synaptic and neuronal dynamics, utilizing binary spikes that are either 0 or 1. Among the various SNN models, the Leaky Integrate-and-Fire (LIF)[23] and Hodgkin-Huxley (HH)[24] models are notably prominent. Due to the high computational demand of the HH model, the LIF model is more commonly adopted, as it offers a practical compromise between accuracy and computational efficiency. The LIF model can be mathematically described as follows:\n$\\T \\frac{du(t)}{dt} = -u(t) + I(t)$\nIn this context, u(t) denotes the membrane potential of the neuron at a given time t, I(t) signifies the external input current, and \\$\\tau\\$ represents the membrane time constant, which influences the rate of potential decay.\nInspired by the complex and varied connectivity observed in the brain, spiking neural networks (SNNs) can be organized into various architectures. These include fully-connected networks[25], liquid state machines[26], spiking convolutional neural networks[27], and spiking Boltzmann machines[28], among others. Such diverse structures enable SNNs to efficiently and effectively process intricate, sparse, and noisy spatio-temporal information. Despite their potential, training SNNs poses significant challenges due to the non-differentiable nature of spike-based communication. Currently, there are three primary methods for training SNNs[1]: unsupervised learning (such as spike-timing-dependent plasticity (STDP)[1]), conversion learning (also known as the ANN-to-SNN conversion method), and supervised learning (which employs gradient descent to train multi-layer SNNs, achieving accuracy comparable to ANNs on complex datasets[1])."}, {"title": "2.2. Graph Neural Networks", "content": "Graphs are common data structures used to represent complex systems, consisting of vertices (nodes) that symbolize entities and edges that indicate relationships between these nodes. Depending on the nature of these relationships, graphs can be classified into several categories, such as directed or undirected, weighted or unweighted, and cyclic or acyclic. Graph Neural Networks (GNNs) are specialized models designed to work with graph data. They leverage both the features of the nodes and the graph's structural information to learn from complex graph datasets. GNNs can be applied to various tasks, including node-level tasks (predicting properties for individual nodes) and graph-level tasks (predicting a single property for the entire graph).\nThe architecture of Graph Neural Networks (GNNs) can be broadly classified into four categories: Convolutional Graph Neural Networks (GCNs)[29, 30], Graph Attention Networks (GATs)[31], Graph Autoencoders (GAEs)[32], and Spatial-Temporal Graph Neural Networks (STGNNs)[33]. GCNs are among the most widely used GNN models, employing convolution operations on graphs to aggregate information from neighboring nodes. These can be further divided into spectral versions, which operate in the spectral domain, and spatial versions, which work directly within the graph's node space. GATs utilize attention mechanisms to prioritize the influence of neighboring nodes, allowing the model to focus on the most relevant information during the learning process. GAEs are unsupervised learning models that use an encoder to project input graph data into a low-dimensional vector space and a decoder to reconstruct the graph or specific properties from this space. Finally, STGNNs are designed to manage graph data with both spatial and temporal aspects, making them particularly useful for"}, {"title": "2.3. Normalization", "content": "Normalization has been crucial in the training of deep learning models, enhancing training efficiency and mitigating issues like vanishing or exploding gradients and internal covariate shift. By scaling data with varying distributions to a consistent range, normalization also speeds up the model's convergence. A pivotal advancement in this area was Batch Normalization (BN)[20], introduced by Ioffe and Szegedy, which normalizes activations within a mini-batch during training, thus accelerating convergence. BN works by adjusting the output of each neuron by subtracting the batch mean and dividing by the batch standard deviation. Subsequent techniques like Layer Normalization (LN)[21] and Group Normalization (GN)[35] have emerged as effective regularizers, helping to reduce overfitting and enhance the generalization capabilities of models.\nIn graph neural networks (GNNs), adapting normalization techniques poses unique challenges due to the non-Euclidean nature of the data and the complex relationships between entities. Layer normalization, tailored for GNNs, helps stabilize training and enhance convergence[31]. Normalizing the adjacency matrix using the degree matrix, known as adjacency matrix normalization, is another crucial step that ensures scale-invariant information propagation[18]. Additionally, spectral normalization, which controls the Lipschitz constant of the network, has been applied to GNNs to prevent overfitting to specific graph structures or nodes[36]. To address the over-smoothing issue and simultaneously increase GNN depth, Zhou et al. introduced the NodeNorm"}, {"title": "3. Graph Spiking Neural Networks", "content": "method[34]. This method scales the hidden features of GNNs based on the standard deviation of each individual node, making the normalization effect controllable and compatible with GCNs in various scenarios.\nDespite the development of these normalization techniques for GNNs, they have not been effectively applied to Graph Spiking Neural Networks. This gap represents a significant challenge and offers a promising research opportunity."}, {"title": "3.1. Spiking Graph Convolution", "content": "We present our Graph SNN framework, focusing on the following four key aspects: (1) a method for integrating graph convolution operations into the spiking computing paradigm; (2) an iterative spiking message passing model, along with an overall training framework that consists of two phases; (3) a spatial-temporal feature normalization technique to facilitate convergence; and (4) implementations of specific spiking graph models.\nGiven an attributed graph G = (V,E), where V represents the set of nodes and E represents the set of edges, the graph attributes are typically described by an adjacency matrix A \u2208 \\$\\mathbb{R}^{N \\times N}$\\$ and a node feature matrix X \u2208 \\$\\mathbb{R}^{N \\times C}$\\$ = [x1;x2;...;xN]. Here,"}, {"title": "3.2. Iterative Spiking Message Passing", "content": "each row xi of X represents the feature vector of a node, and the adjacency matrix A satisfies Aii = 0, meaning there are no self-loops. If the signal of a single node is represented by a feature vector x \u2208 \\$\\mathbb{R}^{C}$\\$, the spectral convolution is defined as the multiplication of a filter \\$F_{\\Theta}$\\$ = diag(\\$\\Theta\\$) with the node signal x, expressed as:\n$\\F_{\\Theta} * x = U F_{\\Theta} U^{T} x,$\nwhere \\$F_{\\Theta}$\\$ is parameterized by \\$\\Theta \\in \\mathbb{R}^{C}$\\$, and U is the eigenvector matrix of the Laplacian matrix \\$L = I - D^{- \\frac{1}{2}} A D^{- \\frac{1}{2}} = U \\Lambda U^{T}$\\$. Here, \\$U^{T} x$\\$ can be interpreted as the graph Fourier transform of the signal x, and the diagonal matrix \\$\\Lambda\\$, containing the eigenvalues, can be filtered using the function \\$F_{\\Theta}(\\Lambda)$\\$. However, due to the high computational cost associated with this approach, some methods have proposed using approximations and stacking multiple nonlinear layers to reduce the overhead, which has been successfully implemented in recent work[18].\nTo unify the flow of information within spiking dynamics, we convert the initial node signals X into binary components {X0, X1, ..., Xt, ..., XT-1}, where T represents the length of the time window. The encoding process can be either probabilistic, following distributions such as Bernoulli or Poisson, or deterministic, involving methods like quantitative approximation or the use of an encoding layer to produce spikes[37]. This approach transforms the attributed graph from its original state into a spiking representation. We denote the spiking node embedding at time t in the n-th layer as \\$H_{i}^{n}\\$, with \\$H_{t}^{0} = X_{t}$\\$ (where the tilde indicates binary variables in spike form). The layer-wise spiking graph convolution in the spatial-temporal domain is then defined as:\n$\\H_{t}^{n} = \\Phi(G_{c}(A, H_{t}^{n-1}) W^{n}, H_{t}^{n-1}),$\nIn this context, \\$G_{c}(A, H_{t})\\$ represents the spiking feature propagation along the graph's topological structure, which can be implemented using various propagation methods, such as the Chebyshev filter or the first-order model. \\$\\Phi(\\cdot)$\\$ denotes the non-linear dynamic process that depends on historical states \\$H_{t}^{n-1}$\\$. The matrix \\$W^{n} \\in \\mathbb{R}^{C_{n-1} \\times C_{n}}$\\$ is a layer-specific trainable weight parameter, where \\$C_{n}$\\$ indicates the output dimension of the spiking features in the n-th layer, with \\$C_{0} = C$\\$ being the input feature dimension. This formula describes a single spiking graph convolution layer, and a"}, {"title": "3.3. Spatial-temporal Feature Normalization", "content": "We use the leaky integrate-and-fire (LIF) model as our fundamental neuron unit. This model is computationally efficient and widely used, while also preserving a certain degree of biological realism. The dynamics of the LIF neuron can be described by:\n$\\T \\frac{dV(t)}{dt} = -(V(t) - V_{reset}) + I(t),$\nIn this model, V(t) represents the membrane potential of the neuron at time t. The parameter \\$\\tau\\$ is the time constant, and I(t) denotes the pre-synaptic input, which is the result of synaptic weights combined with the activities of pre-synaptic neurons or external stimuli. When V(t) exceeds a certain threshold \\$V_{th}$\\$, the neuron generates a spike and the membrane potential is reset to \\$V_{reset}$\\$. After emitting a spike, the neuron begins to accumulate membrane potential V(t) again in the following time steps.\nThe spiking message passing process comprises an information propagation step and an update step, both of which occur over T time steps. Let \\$\\overline{H}_{t} = [h_{t}^{0}; ...; h_{t}^{t - 1}] \\epsilon \\mathbb{R}^{N \\times C}$\\$ represent the node embeddings at time t, where each \\$h_{i}^{t}$\\$ corresponds to the feature vector of node i. For a node v, we provide a general formulation for its spiking message passing as:\n$\\h_{t}^{v} = U(\\sum_{u \\in \\mathcal{N}(v)} P(G_{c}(h_{t}^{u}, e_{vu}), h_{t}^{v-1})),$\nIn this context, P(\u00b7) denotes the spiking message aggregation from neighboring nodes, which can be implemented using various graph convolution operations, represented as \\$G_{c}(\\cdot)$\\$. The function U(\u00b7) signifies the state update, governed by a non-linear dynamic system. \\$\\mathcal{N}(v)$\\$ represents the set of all neighbors of node v in the graph G, and \\$e_{vu}$\\$ indicates the static edge connection between nodes v and u, which can be naturally extended to the formalism of directed multigraphs. Equation (3) provides a specific implementation of this general formulation.\nTo integrate the LIF model into the above framework, we employ the Euler method to convert the first-order differential equation in Eq. (4) into an iterative form. We"}, {"title": "3.4. Graph SNNs Framework", "content": "introduce a decay factor \u03ba to represent the term (1 \u2013 dt) and express the pre-synaptic input I as \\$\\sum_{j} W_{ij} G_{c}(A, H_{t+1}^{j})\\$. Here, graph convolution is used to implement the propagation step P(\u00b7). Incorporating the scaling effect of dat into the weight term, we derive the following formulation:\n$V_{t+1} = \\kappa V_{t} + \\sum_{j} W_{ij} G_{c}(A, H_{t+1}^{j}).$\n\\$G_{c}(A, H_{t+1}^{j})\\$ represents the aggregated feature from the pre-synaptic neurons, with the superscript j indicating the index of the pre-synapse. By incorporating the firing-and-resetting mechanism and assuming \\$V_{reset} = 0$\\$, the update equation can be expressed as follows:\n$\\V_{i}^{l(n)} = \\kappa V_{i}^{l(n)} (1 - H_{t+1}^{l(n)}) + \\sum_{j} w_{n,ij} G_{c}(A, H_{t}^{n-1,j}),\\$\n\\$H_{t}^{l(n)} = g(V_{i}^{l(n)} - V_{th}),\\$\nHere, n denotes the nth layer, and l(n) indicates the number of neurons in that layer. \\$W_{ij}^{n}$\\$ represents the synaptic weight from the jth neuron in the pre-layer to the ith neuron in the post-layer. The function g(\u00b7) is the Heaviside step function, which is used to model the neuron's spiking behavior. By this approach, we transform the implicit differential equation into an explicit iterative form, which can effectively describe the message propagation and update process outlined in equation (5) and illustrated in Figure 1.\nMoreover, we observe that the computation order of the affine transformation and graph convolution can be interchanged when the graph convolution operation is linear (e.g., \\$G_{c}(A, H) = D^{- \\frac{1}{2}} A D^{- \\frac{1}{2}} H$\\$). Given that the matrix W is typically dense, while H is sparse and binary, prioritizing the calculation of H and W can reduce computational overhead by converting multiplications into additions. Under these conditions, the process in equation (7) can be reformulated as follows:\n$\\V_{i}^{l(n)} = \\kappa V_{i}^{l(n)} (1 - H_{t+1}^{l(n)}) + G_{c}(A, \\sum_{j} w_{n,ij} H_{t}^{n-1,j}).$\nIn this way, we present a universal spiking message passing framework in an iterative manner. By specifying \\$G_{c}(\\cdot)$\\$, most proposed graph convolution operations[18, 30, 38]"}, {"title": "3.5. Coding Strategy", "content": "Due to the inclusion of temporal dynamics and the event-driven nature of spiking binary representation, traditional normalization techniques cannot be directly applied to Spiking Neural Networks (SNNs). Additionally, the direct training of SNNs on graph tasks does not ensure convergence or optimal performance. This challenge has led us to propose a spatial-temporal feature normalization (STFN) algorithm specifically designed to address the spiking dynamics in graph-based scenarios.\nConsidering the feature map calculation step, let St \u2208 \\$\\mathbb{R}^{N \\times C}$\\$ represent the instantaneous membrane potential output of all neurons in a layer at time step t. This can be expressed as \\$\\sum_{j}^{l(n)} w_{n,ij} G_{c}(A, \\hat{H}_{t}^{n-1,i})\\$ as shown in equation (7), or as \\$\\sum_{j}^{l(n)} w_{n,ij} \\hat{H}_{t}^{n-1,j}\\$ as shown in equation (9). In the STFN process, pre-synaptic inputs are normalized along the feature dimension C independently for each node. Given the importance of temporal effects in transforming node features within the topology space, normalization is also applied along the temporal dimension across consecutive time steps. Let \\$S_{k,v}^{t}$\\$ denote the kth element in the feature vector of node v at time t. The normalization of \\$S_{k,v}^{t}$\\$ is performed as follows:\n$\\hat{S}_{k,v}^{t} = \\frac{S_{k,v}^{t} - \\rho V_{th} (S_{k,v}^{t} - \\mathbb{E}[S_{v}])}{\\sqrt{Var[S_{v}] + \\epsilon}},\\$\n$\\gamma_{k,v}^{t} = \\lambda \\hat{S}_{k,v}^{t} + \\gamma_{k,v}^{t},\\$\nwhere \\$\\rho\\$ is a hyperparameter optimized during the training process, and \\$\\epsilon\\$ is a small constant to prevent division by zero. \\$\\lambda_{k,v}$\\$ and \\$\\gamma_{k,v}$\\$ are two trainable parameter vectors specific to node v. \\$\\mathbb{E}[S_{v}]\\$ and Var[Sv] represent the mean and variance of the feature values of node v, respectively, computed across the feature and temporal dimensions."}, {"title": "3.6. Model Instantiating: GC-SNN and GA-SNN", "content": "can be incorporated into this model, making it versatile and adaptable to various graph scenarios. This framework provides a cohesive approach for integrating different types of graph convolution operations, facilitating its application across diverse graph-based tasks."}, {"title": "3.7. Spatial-temporal Embedding for Downstream Tasks", "content": "In addition to the rate encoding scenario discussed previously, this framework also supports various other temporal encoding schemes. One such scheme is Rank Order Coding (ROC), which this paper explores to highlight the interesting properties of our graph learning framework under different encoding conditions. This demonstrates the framework's compatibility and scalability with various encoding methods.\nRank Order Coding assumes that biological neurons encode information based on the order of firing within a neuron ensemble. Consider a target neuron i receiving input from a presynaptic neuron set Qn in the n-th layer, where each neuron fires only once, with its activation denoted as \\$H_{j}^{n}\\$. ROC records the relative firing order of these neurons and updates the activation of the target neuron \\$V_{i}^{n+1}$\\$ as follows:\n$V_{i}^{n+1} = \\sum_{j \\in Q_{n}} \\rho^{order(H_{j}^{n})} w_{ij}^{n+1},$\nwhere \\$r\\in (0,1)\\$ is a given penalty constant, and order (\\$H_{j}^{n,i}\\$) represents the firing order of neuron j in the presynaptic ensemble. Equation (14) indicates that the sorting factor \\$\\rho^{order(a)}\\$ is crucial for Rank Order Coding, encouraging neurons to fire early while penalizing those firing later. As this encoding scheme emphasizes information in the early spikes, it is well-suited for promoting network sparsity and facilitating rapid decision-making [39]. This encoding scheme can be seamlessly integrated into the propagation process of the graph learning framework, influencing membrane potential updates and spike generation. At the decision layer, a winner-takes-all strategy is employed, directly outputting the feature corresponding to the neuron with the fastest spike, enabling rapid decoding within short time steps. The characteristics of Rank Order Coding are advantageous for swift recognition and inference in static graph tasks.\nTo illustrate the effectiveness of our framework and normalization technique, we implement the framework into specific models by incorporating commonly used propagation methods in GNNs, such as graph convolution aggregators and graph attention mechanisms[18, 19]. In this implementation, our Graph Convolution Spiking Neural"}, {"title": "4. Experiments", "content": "We further specify node and graph classification tasks on multi-graph datasets to validate the effectiveness of our method. For different downstream tasks, we generate different level representations by using the output of the last GCN layer and provide a loss function for training the network parameters.\nNode classification task. Since the node features can be obtained directly from the GCN output, we directly input the node features into the MLP to obtain logits \\$Y_{i, pred} \\in \\mathbb{R}^{C}$\\$ for each class[40]. The formula can be expressed as:\n$Y_{i, pred} = A ReLU (W \\hat{h}_{t}^{n,i}),$\nwhere A \u2208 \\$\\mathbb{R}^{d \\times C}$\\$, W \u2208 \\$\\mathbb{R}^{d \\times d}$\\$, and the \\$\\hat{h}_{t}^{n,i}$\\$ represents the ith node's feature of the last GCN layer n at the last time step t. To train the network parameters, we use cross entropy between logits and the ground truth labels as the loss function."}, {"title": "4.1. Datasets and Pre-processing", "content": "Graph classification task. For graph classification tasks, we take the average of node features outputted by GCN to generate a representation of the entire graph yg. The formula can be expressed as:\n$y_{G} = \\frac{1}{\\nu} \\sum_{i=0}^{\\nu} \\hat{h}_{t}^{n,i},$\nwhere \\$\\hat{h}_{t}^{n,i}$\\$ represents the ith node's feature of the last GCN layer n at the last time step t. Then the representation of the graph is inputted into an MLP to obtain logits \\$Y_{pred}$\\$[40]$.\n$Y_{pred} = A ReLU (W y_{G}),$\nwhere A \u2208 \\$\\mathbb{R}^{d \\times C}$\\$, W \u2208 \\$\\mathbb{R}^{d \\times d}$\\$. Afterward, the cross-entropy between the logits and ground truth labels is used as the error for training.\nIn this section, We firstly investigate the capability of our models over semi-supervised node classification on three citation datasets to examine their performance. Then we demonstrate the effectiveness of the proposed STFN and provide some analysis visualization for the model efficiency.\nBasic experiments. We first use three standard citation network benchmark datasets\u2014Cora, Pubmed, and Citeseer, where nodes represent paper documents and edges are (undirected) citation links. We summarize the dataset statistics used in our experiments in Table 2. The datasets contain sparse bag-of-words feature vectors for each document and a list of citation links between documents. In Table 2, nodes represent paper documents and edges represent citation links. Label rate denotes the number of labels used for training, and features denote the dimension of feature vector for each node. The Cora dataset contains 2708 nodes, 5429 edges, 7 classes and 1433 features per node. The Pubmed dataset contains 19717 nodes, 44338 edges, 3 classes and 500 features per node. The Citeseer dataset contains 3327 nodes, 4732 edges, 6 classes and 3703 features per node. Each document node has a class label. We only use 20 labels per class during training with all feature vectors.\nWe model the citation links as (undirected) edges and construct a binary, symmetric adjacency matrix A. Note that node features correspond to elements of binary bag-of-words representation. Thus, We treat the binary representations as spike vectors, and re-organize them as an assembling sequence set w.r.t timing, where each component vector is considered equal at different time steps for simplicity. In this manner, we train our SNN modes with spike signals and evaluate them on 1000 test nodes, and we"}, {"title": "4.2. Performance Verification", "content": "use 500 additional nodes for validation following the configuration used in the work [18].\nWe reproduce GCN and GAT and implement our GC-SNN and GA-SNN models with deep graph library. The experimental settings of models with same propagation operator (graph convolution and graph attention) are kept the same in each dataset for fairness. We use Adam optimizer [41] with an initial learning rate of 0.01 for GCN and GC-SNN, and 0.005 for GAT and GA-SNN. We use the dropout technique for avoiding over-fitting, which is set to 0.1 for GC-SNN and 0.6 for GA-SNN model. All models run for 200 epochs and are repeated 10 trials with different random seeds. In each trial, the models are initialized by a uniform initialization and trained by minimizing the cross-entropy loss on the training nodes. We use an L2 regularization with weight decay as 0.0005, and only use the available labels in training set and test the accuracy using 1000 testing samples.\nFor Graph SNN, we set the time window T as 8, and set the threshold \\$V_{th}$\\$ as 0.25 for basic performance evaluation. We use the Integrate-and-Fire (IF) neurons in our models, where the historical membrane potential will not decay as time step continues. Specially, in order to train the Graph SNN models effectively via gradient descent paradigm, we adopt gradient substitution method in backward path [1] and take the rectangular function to approximate the derivative of spike activity. It yields\n$r(u) = \\frac{1}{\\nu} sign(|V - V_{th}| < \\frac{\\nu}{2},\\$\nwhere v represents the width of r(u), and set as 0.5 in all experiments.\nFor GAT and GA-SNN, we adopt an MLP structure [Input-64-Output] with 8 attention heads. For GCN and GC-SNN, we adopt an MLP structure [Input-400-16-Output]. In view of the linear propagation operators used in this work, we follow the formula (8) and conduct the feature transformation first then perform aggregation operation. Besides, we use the proposed STFN technique for GA-SNN and GC-SNN. All experiments in this work are implemented by PyTorch with an acceleration of 4 RTX 2080Ti GPUs.\nExtended experiments. In addition, we conduct extended experiments for verifying the scability of our model on multi-graph datasets regarding node and graph level.\nNode level. Further, we used two datasets, Pattern and Cluster, for the node classification task on multiple graphs, which are generated with the Stochastic Block Model (SBM). SBM is a traditional graph generation model in which each node belongs to a different community, and each community is connected with different probabilities[42].\nThe Pattern dataset is used to identify a specific graph pattern P within larger graphs G of varying sizes. In this dataset, graphs G were generated with 5 communities of random sizes between 5 and 35. The intra-communities and extra-communities probabilities for each community were set at 0.5 and 0.35 respectively, and node features were generated using a uniform random distribution with a vocabulary of size 3, i.e., {0,1,2}. The Pattern dataset contains 14,000 graphs, 1,664,491 nodes, 66,488,100 edges, and 2 classes. And it includes 10,000 graphs for training, 2,000 for validation, and 2,000 for testing.\nThe Cluster dataset is used for semi-supervised clustering. It includes 6 clusters which are generated by the Stochastic Block Model (SBM) with random sizes between 5 and 35, and intra-communities and extra-communities probabilities of 0.55 and 0.25, respectively. Each node is given an input feature value chosen from the set {0, 1, 2, ..., 6}. If the value is 1, it is assigned to class 0, if the value is 2, it is assigned to class 1, and so on till value 6, which is assigned to class 5. If the value is 0, then the class of the node is undetermined and will be determined by the GNN. One labeled node is randomly assigned to each community, and most node features are set to 0. The Cluster dataset contains 12,000 graphs, 1,406,436 nodes, 51,620,640 edges, and 6 classes. And it includes 10,000 for training, 1,000 for validation, and 1,000 for testing.\nGraph level. In the graph level, we convert each image in the popular MNIST and CIFAR10 datasets into graph using super-pixels[43] and classify these graphs. The node features of the graph are generated by the intensity and position of the super-pixels, and the edges are k nearest neighbor super-pixels. For both MNIST and CIFAR10, the value of k is set to 8. The resulting graphs are of sizes 40-75 nodes for MNIST and"}, {"title": "4.3. Ablation Study for the Impact of STFN", "content": "85-150 nodes for CIFAR10.\nThe MNIST dataset contains 70000 graphs, 4939668 nodes, 39517100 edges, 10 classes and it includes 55,000 graphs for training, 5,000 for validation, and 10,000 for testing.\nThe CIFAR10 dataset contains 60,000 graphs, 7,058,005 nodes, 56,464,200 edges, 10 classes and it includes 45,000 graphs for training, 5,000 for validation, and 10,000 for testing.\nTo further validate the impact of STFN on SNNs in graph learning tasks, this study conducted systematic ablation experiments on the Cora, Citeseer, and Pubmed datasets. Figures 4(a)(b) depict the validation accuracy curves and loss convergence curves during the training process, respectively. The results reveal that STFN exhibits advantages in two main aspects: firstly, it enhances the generalization capability of SNNs, leading to higher recognition accuracy on test samples across all datasets; secondly, it significantly accelerates convergence. Compared to the baseline SNN model without STFN, the SNN with STFN achieves faster convergence on all three datasets, with substantially lower loss errors. These observations demonstrate that STFN facilitates faster convergence, alleviates overfitting, and significantly enhances performance in semi-supervised learning tasks for SNNs.\nTo investigate the influence of STFN on learned synaptic weights, we further visualizes the weight data distribution in Figure 4(c). The results indicate that STFN can concentrate the distribution of synaptic weights around the zero-value range. In this scenario, the normalized membrane potential state can coordinate distribution differences and pulse-triggering thresholds, resulting in sparser weights. This is advantageous for the deployment of neuromorphic hardware and the development of low-power graph computing applications."}, {"title": "4.4. Rate Coding versus Rank Order Coding"}]}