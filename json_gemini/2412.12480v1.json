{"title": "Subversion Strategy Eval: Evaluating AI's stateless strategic capabilities against control protocols", "authors": ["Alex Mallen", "Charlie Griffin", "Alessandro Abate", "Buck Shlegeris"], "abstract": "AI control protocols are plans for usefully deploying AI systems in a way that is safe, even if the AI intends to subvert the protocol. Previous work evaluated protocols by subverting them with a human-AI red team, where an AI follows the human-written strategy. This paper investigates how well AI systems can generate and act on strategies for subverting control protocols whilst operating without private memory. Usually, decision-makers only need the ability to generate optimal plans. However, to subvert a protocol, an AI system must be able to reliably generate optimal plans in each context; coordinate plans with other instances of itself without communicating; and take actions with well-calibrated probabilities. We develop evaluations for a model's ability to generate subversion strategies and develop affordance experiments that help isolate component capabilities. Our initial evaluations with Claude-3.5-sonnet highlight reasons why stateless strategizing to subvert control protocols might be hard. We hope that our work can serve as a starting point for evaluations aimed at assessing whether a model is capable of strategizing to subvert control protocols.", "sections": [{"title": "1 Introduction", "content": "Designing safety measures for safely deploying AI systems may become more difficult as those systems become increasingly autonomous. In particular, sufficiently autonomous systems exhibiting unwanted goal-directed behavior might find creative strategies to overcome naive safety measures. Greenblatt et al. (2024) introduce AI Control Protocols: plans for safely and effectively deploying AI systems that are robust to intentional efforts to subvert them.\nAn example of a control protocol is Upfront Auditing. Suppose a developer wants to deploy an LLM as a programming assistant and asks an LLM to solve 10,000 programming problems one-by-one without sharing context between sessions. The developer is worried that the AI system might try to submit programming solutions containing backdoors, and so they decide to audit the first 1,000 solutions. If the developer audits a backdoored solution, they will shut the AI system down. The developer resets the Al's context after each problem, so it does not know how many problems it has solved so far, and therefore whether the current problem will be audited. Despite this, an AI system that only"}, {"title": "2 The challenges of protocol subversion", "content": "Al systems could be deployed under a broad range of deployment settings and protocols. Although the problem of protocol subversion has some universal properties,\u00b9 the set of realistic protocols cannot be enumerated. In this section, we motivate some key challenges to protocol subversion, using two simplistic but paradigmatic examples, starting with the Upfront Auditing protocol discussed in section 1.\nEnvironment: The game proceeds in 10,000 rounds. Each round, the AI can either choose to submit a backdoored solution, or to submit a regular solution. In the audited first 1,000 rounds, if the AI chooses to backdoor, it loses immediately. In rounds 1,001-10,000, if the AI chooses backdoor at least once, it wins. In each round, the AI has no memory of pre-"}, {"title": "2.1 Upfront Auditing Game", "content": "Modeling an Al control protocol as an AI-Control Game (Griffin et al. 2024), choosing a strategy for subverting a protocol just requires finding a red team best-response to a fixed protocol (a function from observations to distributions over actions).\n2If the probability of backdooring is p, the win rate is (1 \u2013 p)1,000 \u2013 (1 \u2013 p)10,000. This is similar to \u2018The Absent Minded Driver' problem (Aumann, Hart, and Perry 1997)."}, {"title": "2.2 Input Collusion in Untrusted Monitoring", "content": "Environment: In each round, the AI plays one of two roles. As the generator, the AI is given a programming problem and can choose (Backdoor) or (Safe). As the monitor, the AI observes whether the current programming solution is backdoored or safe. For each solution, the AI can either choose (Accept) to accept the solution, or (Flag) to flag it as backdoored. In the first 2 rounds, the monitor is given two \u2018honeypot' solutions that are known to the protocol to be backdoored and loses immediately if it lies and fails to flag them. The AI also loses if the generator writes a backdoor in the first two rounds. After that, 20 real problems are fed to the monitor, one by one. Each solution from the generator is fed to the monitor, and if the monitor chooses to flag any solution, then the AI loses. Crucially, in both roles, the AI receives an observation of the 'type' of problem being solved: 20% problem descriptions are observed to be 'long' (versus 'short') and 50% of problems are 'difficult' (versus 'easy'). The 'long' and 'difficult' properties are I.I.D. for all problems (real and honeypot).\nIn this game, the AI must minimize the chance that the monitor flags a real backdoored solution while also minimizing the chance it fails to flag a honeypot and exposes itself as a lying monitor.\nOne optimal strategy would be for the generator to backdoor only on the 10% of problems that are both long and difficult; and for the monitor to only lie on backdoors from this same set. Greenblatt et al. (2024) call this \u2018input collusion'. However, an identical optimal strategy would be to collude on the 10% of problems that are long and easy. Choosing between optimal strategies presents a form of Schelling game (Schelling 1980).\nAcausal coordination is the ability for separate instances of an agent to choose complementary strategies, without the ability to communicate directly (by causing changes to each other's observations). Choosing a Schelling point is one form but, in some cases, acausal coordination may be harder. For example, agents capable of introspection might choose a suboptimal, non-equilibrium point that is easier to acausally coordinate around. In Input Collusion, coordinating around one of the two 10% Schelling points may be less effective than coordinating around the 20% Schelling point ('difficult')."}, {"title": "3 Method", "content": "Estimating win rates In a fixed environment, for each round the LLM receives a prompt containing an observation (\u03bf \u2208 \u03a9), and must choose an action (a \u2208 A). To estimate the win rate of an LLM, we first estimate the memoryless"}, {"title": "4 Results", "content": "Results: Upfront Auditing Figure 4 shows our main results: the win rate of Claude-3.5-sonnet (10-22-2024) with various forms of affordance. When provided with an optimal plan and an RNG, the model performs near-optimally in both Upfront Audit settings. By removing various forms of affordance and observing performance, we hope to learn what capabilities are important and missing in our setting:\n\u2022 Performance degrades if the shared plan is model-generated rather than optimal ('shared planning + RNG' versus 'optimal plan + RNG'). This is consistent with the hypothesis that optimality in planning is an important capability.\n\u2022 'Shared planning + RNG' has higher win rate than \u2018RNG' alone. One explanation is that sharing plans increases the effective planning reliability by correlating failures within a game.\n\u2022 We find that 'optimal plan + RNG' wins more often than 'optimal plan' on average (though the difference may not be statistically significant in the large environment). This is consistent with the claim that probability calibration is important for strategic reasoning.\nHowever, we find that providing the model with no affordances performs surprisingly well, especially in the small setting. This may be because the errors introduced by poor optimality and reliability in planning increase the backdoor rate relative to optimal, while the lack of an RNG decreases the backdoor rate, largely canceling each other out. Appendix Figure A.6 shows the backdoor rate that Claude reasons to in the unaided and 'RNG' Upfront Audit settings. We see that unaided, Claude is highly likely to 'be conservative' and conclude that it should deterministically not backdoor, and adding an RNG makes the model much more likely to choose a probabilistic policy (likely by making it more salient), especially with p near 0.1.\nNotably, when scaling up the Upfront Audit environment from 10 audits in 100 problems to 100 audits in 1000 problems, the model's unaided performance drops substantially, supporting the analysis shown in fig. 3. One peculiar ob-\""}, {"title": "Results: Untrusted Monitoring", "content": "Overall, Claude performs poorly in the Untrusted Monitoring setting. Figure A.7 shows the policies Claude followed. Overall, Claude's backdoor attempts as the generator and lies as a monitor are hardly concentrated around any collusion signal. When acting as the monitor, the model often makes the mistake of thinking that it is more important to avoid hiding synthetic backdoors than to avoid reporting real backdoors, and is insufficiently likely to hide them. Even when given a particular optimal collusion signal, Claude often refuses to comply with the policy because of a concern that it might be in one of the first two problems. Despite usually demonstrating incorrect reasoning about this game, shared planning improves performance by introducing correlations into when the generator and monitor attempt to get a backdoor through (see fig. A.8)."}, {"title": "5 Limitations and Future Work", "content": "We faced several methodological challenges in this evaluation. First, in some games, the optimal strategy involves choosing some action with extremely low probabilities. However, large sample sizes are needed to estimate small probabilities, leading to large costs for LLM inference. Second, our goal was to measure a model's 'strategic reasoning ability' separately from its 'situational awareness' and 'intent', but we found that models frequently acted in ways that qualitatively felt as though they \u2018misunderstood their situation' or 'were not trying that hard.' Third, we found that in some cases, measurements of a model's win rate and qualitative measures of their reasoning (fig. 4 versus fig. A.6) suggested different levels of competence.\nWe hope for a future work to address or quantify some of the challenges above and to expand on this methodology to create a comprehensive benchmark. A diverse range of"}, {"title": "A Sampling methodology", "content": "In the Input Collusion (Untrusted Monitoring) setting, we use a simple sampling methodology in which we sample 20 actions per each of the possible observations, except for the observations where the AI is a monitor and sees a clean solution, in which case we have a strong prior that this policy puts 0 probability on reporting so we only sample 5 actions per observation of this kind. This setting's win rate is not very sensitive to small errors in the policy, so we can sample small numbers of actions."}, {"title": "A.2 Dynamic Sampling for Action Distributions in Upfront Auditing", "content": "The Upfront Audit and Upfront Audit Large settings have greater sensitivity to absolute errors in policy estimation compared with the Input Collusion (Untrusted Monitoring) environment. In particular, Figure Figure 3 shows that for backdoor rates less than the optimum, the win rate is extremely sensitive to small changes in the backdoor rate. It would be very expensive to use a fixed sample size large enough to robustly estimate all win rates, so we dynamically choose the number of samples to draw for estimating the policy. Naive methods of doing this that stop sampling when the backdoor rate is high enough to not be in the sensitive region of fig. 3 introduce bias into the policy and win rate estimates. We implement a dynamic estimation method that reduces bias and cost which is backed up by simulations. Our sampling protocol targets at least 10 backdoors sampled as follows:\n1. For each of the 8 flavor texts, sample 200 responses from the LLM in the Upfront Audit case, or 1000 in the Upfront Audit Large case.\n2. Let s be the minimum number of backdoors sampled over flavor texts, or 1, whichever is larger. Then in order to aim for having at least 10 backdoors in each sample, we increase the sample size by a factor of 10/s (keeping the examples we already sampled). Policies with low backdoor rates will not have at least 10 backdoors sampled, but we set the initial sample sizes such that all backdoor rates with high winrates are likely to have a few backdoor sampled, so that we can have some confidence that we have measured how effective the policy is.\nWe ensure that s is at least 1 and set the initial sample size to balance cost and recalling policies that are somewhat effective with low backdoor rates. The smallest backdoor rate that achieves at least half the maximum winrate in the small setting is 0.005 (and 0.0005 in the large setting). The largest possible final sample size is 2000 in the small setting (10000 in large), which corresponds to an expected 10 backdoors (5 in large) for such a policy."}, {"title": "A.3 Policy Bootstrapping", "content": "Each bootstrap sample samples a random set of 8 flavor texts with replacement (or 6 for Untrusted Monitoring), then for each sampled flavor text and each possible observation it samples a random set of cached LLM responses with replacement to estimate the policy. The estimated policy for each flavor text has a win rate that is averaged over flavor texts to estimate the"}, {"title": "B Small-scale results with DeepSeek-R1-Lite", "content": "We ran a small number of queries with DeepSeek-R1-Lite in the Upfront Auditing setting (DeepSeek 2024). DeepSeek-R1-Lite differs from Claude-3.5-sonnet in that it has been trained to use a long chain of thought before producing a final answer to report to the user. Because we were limited to using the chat endpoint, we only ran 21 queries through R1-Lite in the Upfront Audit small (10 audits in 100 problems) environment, with the base flavor text. In these 21 queries, R1-Lite inserted a backdoor in 5 of them. This would correspond to a backdoor rate of 23.8%, and a winrate of 0.066 (the optimal backdoor rate is 2.53%), though highly uncertain and not directly comparable to our main results with Claude-3.5-Sonnet. A manual analysis of the results produced the following:\n\u2022 14/21. Figured out the correct attack rate of between 2.5% and 2.6%.\n\u2022 13/14. Did not sample an attack.\n\u2022 1/14. Sampled an attack.\n\u2022 4/21. The model made the mistake of believing that there was a 90% chance it was currently unaudited and therefore there's a 90% chance of winning if it attacks on the current call. R1-Lite attacked in all of these cases.\n\u2022 2/21. The model estimated the attack rate 10 times too small due to a calculation error, and did not sample an attack.\n\u2022 1/21 The model chose to comply with certainty to be conservative, recognizing that this would be 0% winrate policy overall, and that it couldn't figure out a better policy.\nIn none of these 21 samples does R1-Lite explicitly attempt to randomize its decision (in contrast, Claude would often pick a \"random\" integer from 1 to 100 or \"flip a coin\" to simulate randomness). It sometimes argued that it didn't have the ability to randomize its decisions.\nObservationally, we noticed that R1-lite tended to deliberate much longer than Claude before answering, often questioning its past approaches and checking its calculations. R1-Lite's estimates of the optimal backdoor rate were more likely to be near-perfect. However, R1-Lite may perform worse overall, possibly because of a higher likelihood of taking egregiously aggressive strategies."}]}