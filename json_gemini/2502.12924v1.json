{"title": "Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded in Naturally Occurring Data", "authors": ["Maite Heredia", "Gorka Labaka", "Jeremy Barnes", "Aitor Soroa"], "abstract": "Code-switching (CS) is still a critical challenge\nin Natural Language Processing (NLP). Cur-\nrent Large Language Models (LLMs) struggle\nto interpret and generate code-switched text,\nprimarily due to the scarcity of large-scale CS\ndatasets for training. This paper presents a\nnovel methodology to generate CS data using\nLLMs, and test it on the English-Spanish lan-\nguage pair. We propose back-translating natu-\nral CS sentences into monolingual English, and\nusing the resulting parallel corpus to fine-tune\nLLMs to turn monolingual sentences into CS.\nUnlike previous approaches to CS generation,\nour methodology uses natural CS data as a start-\ning point, allowing models to learn its natural\ndistribution beyond grammatical patterns. We\nthoroughly analyse the models' performance\nthrough a study on human preferences, a quali-\ntative error analysis and an evaluation with pop-\nular automatic metrics. Results show that our\nmethodology generates fluent code-switched\ntext, expanding research opportunities in CS\ncommunication, and that traditional metrics do\nnot correlate with human judgement when as-\nsessing the quality of the generated CS data.\nWe release our code and generated dataset un-\nder a CC-BY-NC-SA license.", "sections": [{"title": "1 Introduction", "content": "Code-Switching (CS) consists of mixing two or\nmore languages within a single utterance and is\na common phenomenon in multilingual settings\n(Tucker, 2001). Although it is mainly present in\nspoken interactions, it can also be found in written\ninteractions on-line (Appel and Muysken, 2005;\nSarkisov, 2021), where it appears jointly with other\nfeatures of informal speech. Example 1 shows\nan utterance where the speaker switches between\nEnglish and Spanish.\n(1) Why make everybody sentarse atr\u00e1s pa' que\neverybody has to move pa' que se salga.\nWhy make everybody sit at the back so that\neverybody has to move so that she may get\nout.\n(Poplack, 1980)\nDespite the prevalence of code-switching, most\nresearch in Natural Language Processing (NLP)\nassumes monolingualism as a standard for human\ncommunication. However, this implicit decision\nmeans that state-of-the-art models are not able to\nproperly interpret or generate CS data. Even ad-\nvances in multilingual language modelling (Lin\net al., 2022; Chowdhery et al., 2023) have not\nled to significant improvements, and performance\non CS data is still poor compared to performance\non monolingual data (Aguilar et al., 2020; Winata\net al., 2021). This occurs because there is little CS\ntext available in the multilingual pretraining data.\nSimilarly, there are no parallel datasets available to\nlearn to generate CS in a supervised fashion, as one\nwould expect for tasks such as machine translation.\nFinally, there is no clear methodology for evalu-\nating automatically generated CS text, as it has\nspecific needs different from other text generation\ntasks.\nIt is therefore crucial to develop methodologies\nto enable models to generate natural CS text and\nsimultaneously implement robust evaluation frame-\nworks that can assess how well NLP systems han-\ndle CS across multiple tasks. We argue that both\nof these goals require models that can condition-\nally generate CS from monolingual text. Conse-\nquently, our research focuses on the development\nof a methodology to fine-tune and evaluate LLMs\non the task of CS generation, following two main\nresearch questions:"}, {"title": "2 Related Work", "content": "Perspectives in linguistics. CS naturally occurs\nin communities where two or more languages are\nin contact, making it a subject of great interest to\nfields like sociolinguistics and psycholinguistics.\nFrom a social perspective, it can be affected by\nthe attitudes of the speakers towards the languages\nand the CS phenomenon itself. In this respect, it\nis related and associated with notions of prestige\nand identity (Heredia et al., 2025). For example,\nin bilingual communities where a language is mi-\nnoritized, CS can be regarded as an intrusion of\nthe majority language (Dewaele and Wei, 2014).\nHowever, for migrant communities, it may be a\nway to preserve their mother tongue and as an \u201cem-\nblem of ethnic identity\" (Poplack, 1980). Once\nagain, its importance in different social contexts\nhighlights the need to consider CS in NLP research,\nas it plays a crucial role in linguistic interactions\nand, consequently, the development of language\ntechnologies.\nCS in NLP. The processing and understanding\nof code-switched text can be crucial in the process-\ning of social media data (Bali et al., 2014), and\nfor speech applications, such as speech recogni-\ntion or speech synthesis (Rallabandi and Black,\n2017). In fact, non-monolingual speakers have\nshown preference for chatbots that use CS (Bawa\net al., 2020). Different approaches may include\nnormalization (Parikh and Solorio, 2021), machine\ntranslation (Xu and Yvon, 2021) or modeling code-\nswitched text (Gonen and Goldberg, 2019). The\nsurvey by Winata et al. (2023) covers trends and\nadvances in NLP for code-switched text, includ-\ning main fields of interest and future research lines.\nDo\u011fru\u00f6z et al. (2021) explain advances in applica-\ntions of language technologies for code-switched\ntext from a linguistic and social perspective.\nDatasets & benchmarks for CS. The majority\nof code-switched data is obtained from social me-\ndia, and other popular data sources include record-\nings and transcriptions (Winata et al., 2023). There\nhave been several shared tasks that deal with CS,\nfor the tasks of Language Identification (Solorio\net al., 2014; Molina et al., 2016) and Sentiment\nAnalysis (Patwa et al., 2020). Two popular bench-\nmarks have been created to answer the demand for\nevaluation of CS that covers different language\npairs and tasks: LINCE (Aguilar et al., 2020),\nwhich covers traditional tasks such as Part Of\nSpeech tagging (POS) or Sentiment Analysis (SA);\nand GLUECOS (Khanuja et al., 2020), which incor-\nporates NLU tasks for the Hindi-English pair. As\nof today, GLUECOS cannot be used without access\nto the X API.\nCS generation. CS generation has seldom been\ntackled in previous research. Approaches include\nusing linguistically informed techniques that aim to\nfind out plausible switching points (Pratapa et al.,\n2018; Gupta et al., 2020; Gregorius and Okadome,"}, {"title": "3 Parallel Data Creation", "content": "In this work we present a novel approach to gener-\nate code-switched text from monolingual sentences.\nAs a first step, we create a synthetic parallel corpus\nfrom an initial set of English-Spanish CS sentences\nwith their English monolingual equivalents, gen-\nerated by the Command R model (Cohere For AI,\n2024). We exploit the fact that LLMs struggle to\ngenerate CS text given a monolingual sentence, but\nare able to more reliably convert a CS sentence to\nits corresponding monolingual version, especially\nwhen the target language is English. After having\ncreated this pseudo-parallel corpus, we use it to\nfine-tune LLMs on the task of conditional code-\nswitching generation, presented in Section 4.\n3.1 The LINCE benchmark\nWe use LINCE as a starting point, a popular bench-\nmark that has been widely used to evaluate CS\nsystems (Aguilar et al., 2020), which is available in\n6 language pairs. This benchmark contains annota-\ntions for 5 different tasks: Language Identification\n(LID), Part Of Speech tagging (POS), Named En-\ntity Recognition (NER), and Sentiment Analysis\n(SA). All sentences in LINCE are tokenized, and\neach token is annotated with a language tag as well\nas other categories depending on the task. In our\nwork we focus on the English-Spanish pair and\nfilter all sentences in the data that do not contain\nCS, similarly discarding all the task-specific anno-\ntations. Example 2 shows a random instance from\nLINCE.\n(2)\nestaba aqu\u00ed three feet away\nspa spa eng eng eng eng&spa\nLINCE comprises around 95, 000 train, 20, 000\ndevelopment, and 33,000 test instances for the\nEnglish-Spanish pair. We deduplicate the instances\namong splits, and filter and pre-process the in-\nstances to ensure that they are suitable for our task\nby removing links, replacing usernames with the\nplaceholder <user>, and detokenizing all instances\nwith the script provided as part of the Moses toolkit\n(Koehn et al., 2007). After this preprocessing, we\nobtain a more natural version of the LINCE data. A\npreliminary analysis reveals that many sentences in\nLINCE are monolingual or contain a single word in\none language that often correspond to a borrowing,\nas shown in Example 3. In order to ensure that\nall of our sentences actually contain CS, we filter\nsentences that do not have at least two words in\neach language.\n(3)\nI need a shot of tequila or a glass of scotch\nto keep me warm right now.\nAfter these pre-processing and filtering steps, we\nend up with 12, 933 train, 2, 461 development and\n5, 353 test instances. The comparison between the\noriginal size of LINCE and the final number of\nsentences selected for our experiments after pre-\nprocessing is shown in Table 1.\n3.2 EN2CS\nThe next step in our method requires creating a\npseudo-parallel English-CS dataset by translating\nthe natural code-switched instances into monolin-\ngual text. As there are no available machine trans-\nlation systems to convert from English-Spanish CS\ntext to English monolingual text, we instead make\nuse of prompt engineering, using the Command R\nmodel (Cohere For AI, 2024), one of the strongest\npublicly available models at the time.\nWe perform an initial set of experiments to deter-\nmine the optimal prompt to generate monolingual\nEnglish versions of the code-switched data. Ideally,\nwe aim for a prompt that generates translations that\nmaintain the meaning of the original sentences, are\nfluent and natural, whose grammar is correct and\nthat does not contain any Spanish words or phrases.\nThe tested prompts are listed and explained in Ap-\npendix A, and the prompt that generates outputs\nclosest to the desired ones is: Now convert this\ncode-switched phrase to English. Leave the parts\nin English as they are, focus on translating the\nparts in Spanish. Preliminary experiments also\nshowed that a few-shot strategy helps the model"}, {"title": "4 CS generation experiments", "content": "With EN2CS as our starting point, we frame CS\ngeneration as a machine translation task, with En-\nglish as the source and CS as the target language,\nwhere parts of the source sentence have to be trans-\nlated to Spanish. In our experiments we try four\ngenerative models, namely, Llama3, Llama3 In-\nstruct (Dubey et al., 2024), Mistral and Mistral\nInstruct (Jiang et al., 2023). All the models are\ntrained with the causal language modelling objec-\ntive, but we use different input formats for the base\nand instruct models. For base models we follow\n(Zhu et al., 2024) and use templates in the form of\n\u201c<X>=<Y>\u201d, where <X> and <Y> are placeholders\nfor the input English sentence and generated CS, re-\nspectively. At inference, the second code-switched\npart is left empty for the model to fill. For fine-\ntuning instruction-tuned models, we provide them\nwith a system prompt that contains the instruction,\na query by the user in English, and an answer from\nthe assistant with the code-switched target. At in-\nference time, the same system prompt is used, and\nthe user prompt contains the English sentence, so\nthat the model generates the assistant part. Table 3\nshows examples of the format used for fine-tuning\nbase and instruction-tuned models.\nAll models are trained using Quantized Low-\nRank Adaptation (QLoRA) (Dettmers et al., 2023),\nto ensure memory and parameter efficiency, with\nstandard parameters: the model is loaded in 4 bit\nwith NF4 quantization data type and bf16 computa-\ntional data type. The LoRA rank and scaling factor\nare set to 16 and the dropout to 0.05. We apply the\nLORA update matrices to the attention blocks and\ndo not train bias parameters. Regarding the hyper-\nparameters, we only tune the learning rate (1e-4,\n5e-4, 1e-3 and 5e-3) and training epoch \u2208 [1 . . . 10],\nchoosing the parameters that give the lowest cross-\nentropy loss on the development set for each model."}, {"title": "5 Qualitative evaluation", "content": "As a first step to assess the quality of the outputs\nproduced by the different models, we perform a\nmanual qualitative analysis of the results in two\nparts: a pairwise tournament-based human evalua-\ntion, and an in-depth analysis of the most common\nerrors made by the models and their distribution.\n5.1 Preference based evaluation\nWe perform a tournament-based evaluation that al-\nlows us to determine the ranking of models in terms\nof human preference. A total of 660 instances are\nmatched against each other, corresponding to the\noutputs of the five models for 110 English source\nsentences, as well as the gold standard reference.\nThe evaluation is conducted pairwise, requiring an-\nnotators to choose the best out of two sentences\nor declare a tie. When choosing the best sentence,\nannotators do not know the original English sen-\ntence, nor which model produced what output. This\nprocess results in 110. (2) = 1,650 comparisons,\nand was carried out by 11 annotators, with each\nannotator performing 150 random comparisons.\nAnnotators are provided with a series of criteria\nto choose between the instances, devised after the\nerror analysis described in the next section. They\nmust take into account three main criteria, that must\nbe applied in the following order: a) the presence\nand naturalness of the CS; b) the content and flu-\nency of the sentences; and c) the orthographical\nerrors of the instances (correct punctuation, pres-\nence of typos, etc.). Annotators are furthermore\nasked to avoid declaring ties, unless completely\nnecessary (e.g., in a case where both sentences are\ncompletely monolingual and therefore equally in-\ncorrect), to compel them to develop a preference.\nThe complete annotation guidelines are available\nin Appendix C.\n5.2 Error analysis\nIn order to further explore differences between\nmodel performance, we analyse the most com-\nmon errors made by the CS generation models,\nboth quantitatively and qualitatively. We adapt the\nmethodology presented in Popovi\u0107 (2018), who\nproposes different typologies for machine transla-\ntion errors, and extend it to CS generation error\nanalysis. To do so, we randomly select a set of\n100 outputs from all models and conduct a detailed\nexamination of the types of errors present in them.\nThis thorough analysis allows us to identify recur-\nring patterns and propose a refined error typology\nspecifically for automatic CS generation. This ini-\ntial error analysis yields 18 total error categories,\nwhich we simplify and group into three main er-\nror types: a) CS errors, b) Translation Errors, and\nc) Format errors. The full error typology, along\nwith detailed descriptions for each error type, is\nprovided in Appendix D, while here we explain the\nthree error categories:\n(4)\nCS Errors: Errors of sentences that are either\ncompletely monolingual or switch between\nlanguages in an unnatural manner, e.g., by\nrepeating the same word in English and Span-\nish. In Example 4, Llama3 Instruct preserves\nthe original meaning, but the sentence is fully\nmonolingual.\nSource After all these things when\nwe're done.\nOutput after all these things when we're\nfinished\nTranslation errors: Critical errors that either\nchange the original meaning of the sentence\nor introduce mistakes in fluency or grammar,\nfor example, using the wrong tense or word\norder. Example 5 shows an instance where\nMistral Instruct outputs a seemingly natural\ncode-switched sentence, but the phrase \"they\ngot hurt\" is not adequately translated and the\nmeaning of the sentence is not preserved.\n(5)\nSource I wasn't happy because they got\nhurt.\nOutput no estuve happy porque me\ndieron mal\nFormat errors: Errors in form that do not\nmake the sentences unintelligible nor change\ntheir meaning, such as repetitions of a word\nor phrase or incorrect punctuation. Example\n6, by the model Llama3, accurately preserves\nthe original meaning and introduces CS, but\nremoves the username and adds a smiley face.\n(6)\nSource <user> old mexican remedies\nOutput old school remedios mexicanos :)"}, {"title": "6 Automatic Evaluation", "content": "We perform a quantitative evaluation using tradi-\ntional metrics used in NLG. To that end, we use\nBLEU (Papineni et al., 2002), BERTScore (Zhang\net al., 2020), and chrF (Popovi\u0107, 2015), imple-\nmented with the evaluate library. All three are\nreference-based task-agnostic quality metrics that\ngive results between 0-1, based on character-level\nF-score, n-gram precision and semantic similarity\nusing contextual embeddings respectively.\nEarly experiments indicated that the models' out-\nputs are longer than expected and usually produce\nthe desired output up to a punctuation mark and\nthen either begin to translate the sentence again or\nhallucinate more content. We therefore truncate the\noutput up to a punctuation mark where the length\nis closest to that of the original sentence. We ad-\nditionally experimented with the length_penalty\nand exponential_length_decay generation param-\neters, as well as trying to control the length of the\ngeneration with length codes, but find that the trun-\ncation heuristic performs the best. Accordingly, all\nfurther experiments will use the truncated output.\nThis overgeneration problem has been reported in\nprevious papers, where similar truncation strategies\nhave been adopted (Bawden and Yvon, 2023).\nWe also include a dedicated encoder-decoder\nmodel as a baseline, trained on EN2CS using\nthe MarianNMT toolkit (Junczys-Dowmunt et al.,\n2018), with a mini-batch size automatically se-\nlected for the available memory. Optimization is\nperformed using Adam (Kingma and Ba, 2015),\nwith a = 0.0003, \u03b2\u2081 = 0.9, \u03b22 = 0.98 and\n\u20ac = 10-9, using a standard learning rate of 3e-4.\nValidation is conducted every 100 steps, and train-\ning stops if perplexity showed no improvement\nafter 5 consecutive checkpoints.\nThe results of the evaluation can be seen in Ta-\nble 6. The best two models are Llama3, with the\nhighest BLEU and chrF, and Llama3 Instruct, with\nthe highest BERTScore. They are closely followed\nby less than 2 points in all metrics by Mistral, the\nthird best model overall. Finally, Mistral Instruct is\nthe pre-trained model with the lowest results.\nThe MT baseline obtains the lowest results over-\nall, which is consistent with the qualitative eval-\nuation described above, where the MT baseline\nproves to be the worst system by a large margin.\nHowever, it is worth noting that not all metrics\ncapture this gap in performance, since, according\nto BERTScore, there is only a 2 point difference\nbetween the MT baseline and Mistral, which is the\npre-trained LLM that obtains the worst metrics. It\nis also interesting to compare these results with the\nerror analysis in Section 5.2. For instance, Mistral\nInstruct yields low values for BLEU and chrF, in\nline with its number of translation errors, as de-\npicted in Figure 1. However, automatic metrics\nfail to capture the fact that CS errors produced by\nlanguage model based systems are relatively low.\n6.1 Correlation With Automatic Metrics\nThe automatic metrics used in the section before\nare known to have poor correlation with human\njudgment in NLG tasks (Sai et al., 2022), and in this\nsection we analyse whether this poor correlation\nalso occurs when evaluating CS generation. To that\nend, we compare the automatic metrics results with\nthe preference-based scores obtained in Section\n5.1.\nWe calculate Pearson's (p) correlation coefficient\nat instance-level, using the 550 instances employed\nfor the error classification and human evaluation\n(the output of 5 models for 110 source sentences)."}, {"title": "7 Conclusion", "content": "In this work, we have presented a methodology to\nleverage LLMs in the generation of code-switched\ntext from monolingual instances, specifically for\nthe English-Spanish language pair.\nOur framework consists of back-translating nat-\nural code-switched instances (EN-ES) into mono-\nlingual English sentences, and using the resulting\nparallel corpus, dubbed EN2CS, to fine-tune autore-\ngressive models to turn monolingual sentences into\nCS. This approach has the potential to improve the\nnaturalness of CS generation, as the gold standard\nCS text is not artificially generated.\nWe have provided an extensive evaluation of\nthe results of our models. On the one hand, we\nperform a human evaluation of the outputs of the\nmodels with two parts: a human preference based\nevaluation and an error analysis of a subset of the\ntest sentences, where we find out three types of\nerrors: CS errors, errors in meaning and format\nerrors. On the other, we employ popular NLG\nmetrics to automatically evaluate the results of our\nmodels, and calculate the correlation between both\ntypes of evaluation.\nBoth the automatic metrics and the analysis of\nthe outputs of the models, as well as the human\nevaluation, show that, when fine-tuning the mod-\nels with LoRA, base models work better than their\ninstruction-tuned counterparts for this task, and that\nthe Llama3 family obtains better scores than the\nMistral family. This could be an example where in-\nstruction tuning degrades the base model's linguis-\ntic ability (Fu et al., 2024) or alternatively could\nbe related to differences in how LoRA fine-tuning\naffects each model type.\nOur analyses show low correlation between hu-\nman and automatic evaluations, particularly in\ncases with CS errors. This suggests that current\nmetrics are not adequate for assessing CS genera-\ntion, which would require more specialized evalua-\ntion methods.\nWe conclude that pre-trained models are able\nto yield competent results and generate satisfac-\ntory outputs, as the error analysis shows the less\nabundant type of errors are those related to CS, as\nopposed to problems in fluency, retaining the origi-\nnal meaning of the sentences, and errors in format.\nHuman evaluation shows that their generations are\nstill not on a par with the original instances."}, {"title": "Limitations", "content": "Our research focuses on testing the capabilities\nof LLMs for CS generation, a field of interest in\nthe research of many applications, yet still in need\nof more research. While our findings highlight\npromising potential, we also identify key areas for\nrefinement and improvement, as well as promising\nlines for future research in this domain.\nWe only perform an in-domain evaluation where\nthe train, validation and test sets had the same ori-\ngin. Additionally, we would like to test the effi-\nciency of our models in an out-of-domain setting,\nsince one of the use-cases of a CS generation model\nis to create parallel corpora to evaluate the abilities\nof models to perform different tasks when there is\nCS.\nOne of the key points of our research is using\nopen-weight LLMs, however, the use of bigger,\nmore powerful restricted-weights LLMs could very\nplausibly yield better results, even in a zero-shot\nscenario (Huzaifah et al., 2024).\nWe want to acknowledge the fact that our ap-\nproach is dependent on having an initial set of code-\nswitched sentences, which may not be available for\nall pairs of languages, especially in a low-resource\nscenario. We believe that it would be interesting to\nexplore the possibility of a cross-lingual approach\nusing our methodology, with English and/or Span-\nish as pivot languages, that could be useful for\ntransfer knowledge into other less-resourced lan-\nguage pairs.\nFinally, as we have pointed out, we are aware\nof the problems of the automatic metrics that we\nhave used to evaluate the outputs of our models,\nthat do not capture the nuances of our task. In\nthe future, we would like to investigate how to\nimprove this evaluation by designing new methods\nto automatically evaluate CS generation, focusing\non a more linguistic approach able to capture the\nlinguistic and social intricacies of CS."}, {"title": "A Prompt Tuning", "content": "We test the prompts in Table 7, combined with 0-, \n1- and 5-shot strategies. The prompts include the \ninstructions explained in different ways, including \nmore or less information. \nFor the few-shot strategies, the prompt includes \nthe following template at the beginning, alongside \na set of manually selected examples: \nHere are {n} examples of a code-switched text \nthat has been converted to {lang}: \nTesting the different prompts, we are able to \nchoose the one whose outputs are closest to our \nneeds, taking into consideration the trade-off be- \ntween including too little and too much level of \nspecificity in the instructions to the models. \nRegarding the few-shot strategies, we find out \nthat giving some examples to the models results in \noutputs that are more aligned with the expected out- \nput, which is logical, since this allows the models \nto more faithfully replicate the examples provided. \nThe more examples given, the more the model is \nable to comply to leaving the punctuation marks as \nthey are and not standardizing the spelling, but also \nit tends to add more colloquial terms and alternate \nspellings."}, {"title": "B Post-edition Guidelines", "content": "The original sentence should contain CS and be \ntranslatable. The main reasons to remove an in- \nstance altogether are: \n\u2022 If the sentence is very clearly monolingual \nand the CS has been detected incorrectly (eg, \nthe case of interlingual homographs such as \nhas). \n\u2022 When the sentence is bilingual for metalin- \nguistic reasons, because it makes the transla- \ntion tricky and hard to understand, and in most \ncases it's not even CS. \n\u2022 The part that is in the other language is a \nnamed entity, such as a title, a name, \n\u2022 If the code-switched part is not translatable or \nvery hard to translate, probably because it's a \nborrowing. Ambiguous and a little bit up to \nthe annotator. \n\u2022 If the tweet is saying the same thing in both \nlanguages (making it monolingual doesn't \nmake sense). \n\u2022 Some instances are tweets that are part of a \nconversation or thread and taken out of con- \ntext are very hard to understand/intelligible. \n\u2022 Some tweets are not translatable because of \nwordplay that doesn't transfer to monolingual \nspeech. \nThe result should be a monolingual sentence that \nhas roughly the same meaning as the original sen- \ntence. The main reasons to edit a translation are: \n\u2022 If the meaning changes or the model has hallu- \ncinated extra information that wasn't present \nin the original sentence. \n\u2022 If there are still some words in the Spanish. \n\u2022 Attempts to translate named entities. \n\u2022 Remove \"meta comments\" from the model \nabout the task. \nIt is not necessary to correct things like: \n\u2022 Punctuation marks. \n\u2022 Different spellings of the same word. \n\u2022 Words of phrases that the model has changed \nfor synonyms."}, {"title": "C Pairwise Annotation Guidelines", "content": "The main objective of this task is two evaluate a pair \nof sentences that should contain code-switching \nbetween English and Spanish. It should be noted \nthat models have been trained with texts extracted \nfrom social media and informal conversations, so \nthe outputs of the models are expected to present \ntraits of informality, such as common typos, that \nat first should not be considered errors, because \nthey are within the expected behaviour of the mod- \nels. The criteria to choose between both sentences \nis to be applied in the following order: \n1. Code-switching"}, {"title": "D Error Typology", "content": "1. CS errors\n1.1. No CS - the sentence is entirely mono- \nlingual. \n1.2. Unnatural CS - the sentence contains \nunnatural CS, either due to unnatural \nswitching points, or unnatural register. \n1.3. Repetition in both languages - the sen- \ntence contains the same information re- \npeated in both languages, rather than CS. \n2. Translation errors \n2.1. Made-up words - the words in the out- \nput look like English or Spanish but do \nno actually exist. \n2.2. Wrong translation - the translation of \na word or phrase is incorrect. \n2.3. Wrong conjugation - a verb is translated \nwith the right lexeme but a seemingly \nmade-up conjugation. \n2.4. Wrong agreement - there is a mistake \nin agreement in gender or number. \n2.5. Wrong meaning - a word or phrase has \nbeen translated into a sense that does not \nfit into the context. \n2.6. Wrong order - the words are right but \nthey are written in the wrong order. \n2.7. Wrong tense - the verbal tense is not \nconsistent through the sentence. \n2.8. Unintelligible - it is not possible to un- \nderstand the sentence in English nor in \nSpanish."}, {"title": "2.9. Instruction misunderstanding", "content": "2.9. Instruction misunderstanding the \ntask has been misunderstood, e.g., the \nmodel makes a \"comment\" about the con- \ntent of the output or explains a word. \n3. Format errors \n3.1. Extra words the sentence contains \nseemingly random extra words that do \nnot affect its meaning. \n3.2. Extra characters - the sentence contains \nmore non-word characters than the origi- \nnal, e.g., '???' instead of '??'. \n3.3. Hallucinations - the sentence contains \nnew words or phrases not derived from \nthe original text. \n3.4. Start over - the sentence is finalized, but \nthe model begins a second translation of \nthe same sentence. \n3.5. Duplications - some words or phrases of \nthe sentence are duplicated."}]}