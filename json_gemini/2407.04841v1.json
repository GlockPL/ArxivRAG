{"title": "Associative Recurrent Memory Transformer", "authors": ["Ivan Rodkin", "Yuri Kuratov", "Aydar Bulatov", "Mikhail Burtsev"], "abstract": "This paper addresses the challenge of creating a neural architecture for very long sequences that\nrequires constant time for processing new information at each time step. Our approach, Associative\nRecurrent Memory Transformer (ARMT), is based on transformer self-attention for local context\nand segment-level recurrence for storage of task specific information distributed over a long context.\nWe demonstrate that ARMT outperfors existing alternatives in associative retrieval tasks and sets a\nnew performance record in the recent BABILong multi-task long-context benchmark by answering\nsingle-fact questions over 50 million tokens with an accuracy of 79.9%. The source code for training\nand evaluation is available on github.", "sections": [{"title": "1. Introduction", "content": "Memory plays a crucial role in creating models capable of processing extremely long contexts\nand utilizing remote past information. Starting from RNNs and evolving through LSTM [13] and\nMemory Networks [24, 28], we are now in the era of Transformer-based [27] Large Language\nModels [2, 17, 26]. Various methods for extending transformers context length have emerged [4,\n19, 31], including approaches based on transformer segment-level recurrence [3, 5, 6, 20] and\nnovel architectures that combine the efficiency of transformer parallelization during training with\nrecurrence at inference [7, 8, 10, 11, 18]. Alternatively, Retrieval-Augmented Generation (RAG)\nfocuses on retrieving information from external storage [1, 12, 23] or self-retrieving from past\ninputs [21, 29]. However, retrieval fails on complex tasks that require reasoning over multiple pieces\nof information [16].\nIn this work we propose the Associative Recurrent Memory Transformer (ARMT) as an extension\nof the segment-level recurrent model RMT [3] with associative memory. Compared to RWKV [18]\nand Mamba [10], which use association-based techniques, ARMT benefits from full local self-\nattention and has constant time and space complexity of processing new segment, similar to RMT.\nTo study ARMT performance we use the BABILong [16] benchmark, because it allows to generate\ntest samples up to 50 million tokens and beyond, compared to other methods. Additionally, we use\nAssociative Retrieval task with multiple key-value pairs to estimate memory capacity of models.\nMain contributions of this work include: (1) a novel ARMT architecture for long context\nprocessing with segment-level recurrence and associative memory; (2) demonstration that ARMT\noutcompetes existing memory based models like RMT [3] and Mamba [10] on associative retrieval\nand long context processing tasks, achieving 80% accuracy of single fact QA on unprecedented input"}, {"title": "2. Associative Recurrent Memory Transformer", "content": "We extend RMT [3](Fig. 1a) by addition of layerwise associative memory $A^l$ over segmented input\n$X^l_s$ (Fig. 1b). At every input segment s for each layer l memory tokens $M^{l+1}_s$ generated for preceding\nsegment are added to $A^l_s$ (Fig. 1c) used to update input sequence and memory embeddings:\n$[X^{l+1}_s; M^{l+1}_s] = TrBlock(AssocBlock([X^l_s; M^l_s], A^l_s)); A^{l+1}_s = MemUpdate(A^{l-1}_s; M^{l+1}_s)$.\nThe mechanism of associative block (Fig. 1c) is similar to linear transformers [15], but attends\nonly to special memory tokens and is calculated differenty. After each segment, memory tokens are\nconverted to keys and values via linear mapping and then stored in quasi-linear key-value memory\n[22] using non-linearity \u222e. Given a memory token $m_i \\in M^{l+1}_s$, we calculate the keys, values, and\nimportance scalars $\u03b2_i$. We then recall the previous association $v_i$ with this key, add the new value $v_i$\nto the memory, erase the previous value $\\overline{v_i}$ associated with $k_i$, and update the normalization vector.\n$k_i, V_i = W_k m_i, W_v m_i; \u03b2_i = \u03c3(W_\u03b2 m_i); A^l_0 = 0; z^l_0 = 0; \\newline$\n\\overline{v_i} = \\frac{A^{l-1} \u03c6(k_i)}{(z^{l-1}_i)^T \u03c6(k_i)}; \u03b3 = 1-\\frac{(z^{l-1}_i)^T \u03c6(k_i)}{||\u03c6(k_i)||^2};$\n$A^l_i = A^{l-1}_i + \u03b2_i (V_i - \\overline{V_i}) \u03c6(k_i)^T; z^l_i = z^{l-1}_i+ \u03b3\u03c6(k_i)$.\nOnce we updated $A^l_i$ with information from previous segment, we recall an association $y_j$ for a token\n$x_j$. Associations $y_j$ for each token in the segment are then passed to the next transformer layer:\n$q_j = W_Q x_j; Y_j = \\frac{A^l \u03c6(q_j)}{(z^l)^T \u03c6(q_j)}$\nFor the non-linearity function 4, we used the proposed in [22] DPFP-3 function because, in this\npaper, it has shown significant superiority over other methods, which is also consistent with our\nfindings.\nNote that without \u03b3 (\u03b3 = 1) this approach suffers from catastrophic forgetting on some tasks.\nThe reason is that while we erase the information $\\overline{v_i}$ from the A-matrix, the corresponding keys"}, {"title": "3. Evaluation of Associative Retrieval and Long Context Memory Retention", "content": "We test memory capacity of ARMT in comparison to recent computationally efficient long-context\nmodels Mamba [10] and RMT [3] on the following two variants of associative retrieval.\nRemember task requires memorization of all key-value pairs with unique keys from the prior\ncontext with subsequent recalling a value corresponding to one of the keys (Fig. 2a). We estimate the\ntotal number of key-value pairs stored in memory, based on the exact-match metric (details are in\nAppendix B). Since ARMT has the same recurrent memory size as Mamba, calculated as the number\nof floats in recurrent states, it can be concluded that ARMT makes better use of its internal associative\nmemory. Both ARMT and Mamba outperform RMT on this task. PRMT does not improve RMT\nperformance (Fig. 2a). This indicates that the associative memory plays a critical role in ARMT\nperformance compared to RMT. Additionally, we ablated ARMT on normalization correction, as\ndetailed in Appendix F.\nIn Rewrite task the keys are not unique and the goal is to recall the latest value that corresponds\nto one of the keys from the prior context. This task evaluates the model's ability to dynamically\nchange the memory storage. The results, shown in Fig. 2b, indicate that ARMT is robust to the\nnumber of memory rewrite operations, while RMT and Mamba experience slight degradation after\nexceeding their training lengths. Notably, ARMT maintains perfect memory recall on lengths\nexceeding 10 times those used in training."}, {"title": "4. Conclusion", "content": "In this work, we propose and evaluate recurrent memory transformer augmented with associative\nmemory mechanism for long-context processing and find that it scales up to an unprecedented 50\nmillion tokens on the BABILong benchmark. ARMT architecture adds an associative memory\nmechanism for segment-level recurrent model RMT. Based on our evaluation on associative retrieval\ntasks ARMT demonstrates significant advantage in memory capacity and generalisation compared to"}, {"title": "Appendix A. Related Work", "content": "AutoCompressor [5] is a strategy that stacks memory to minimize information loss at the price of\nquadratic computation cost.\nRecurrent Memory Transformers Recent challenges in long-context processing tasks demon-\nstrated recurrent memory superiority over attention mechanism [16, 31] (Fig. 1 (a)). It was shown\nthat this type of memory performs well even in contexts of size 11M [16]. But still, this memory has\nsome issues with capacity and training. Capacity remains limited as the suggested memory states are\nlimited to a small number of memory tokens. The training is still challenging, as the whole training\nprocess requires backpropagation through time for hundreds of layers. Our approach is supposed to\nmitigate these problems by leveraging the association matrix as a connector for different segments.\nIn contrast to RMT, it has different parameters for memory (linear attention projections described in\nSection 2) and makes this memory hierarchical by creating different association matrices for different\nlayers.\nContext explosion prevention In the attention sinks paper [30], authors demonstrated the\nneed for some sinking tokens for attention for efficient extrapolation in long contexts. Recurrent\nMemory in RMT [3] as well as our model successfully perform this function. In RMT the memory\ntokens can act as attention sinks while in our model the very association matrix can play this role.\nThe ARMT can also be thought of as a kind of compressed-memory RMT [3], that attends to all\nprevious memory tokens with layerwise memory.\nRecurrent Sequence Models The problem of the quadratic cost of attention mechanism led to\nthe development of recurrent architectures with transformer-like performance [7]. The vast majority\nof them are at least related to the State-Space Models (SSMs) [8, 10, 18, 25]. Despite comparable\nperformance with transformers on LM tasks, SSMs are known to be less efficient in memorization\ntasks, especially when the question is asked after the information [14]. Our model performs well\neven on these types of tasks, because it has the large and flexible storage for keeping the associations\nin memory, simultaneously having the direct access to the local context via the vanilla attention."}, {"title": "Appendix B. Memory capacity estimation", "content": "Theorem:\nGiven:\nexact_match = \u03b1; n = number of pairs; \u03c5 = number of possible values\nThen the number of memorized pairs can be estimated with the formula:\n$k = \\frac{n v \u03b1}{\u03c5 - 1}$\nProof:\nWe can precisely predict the associated value if we remember k pairs and then extract the key\nfrom these pairs. We output the random value if we obtain the key from any other pair. As a result,\nthe following is the mathematical expectation of an exact match:\n\u03b1 = \\frac{k}{n}.1+\\frac{n-k}{n}.\\frac{1}{v} = \\frac{k}{n} + \\frac{1}{v} - \\frac{k(v-1)}{nv} = \\frac{k(v-1) + n}{nv}$"}, {"title": "Additionally:", "content": "$k = \\frac{n\u03bd\u03b1}{\u03bd\u03b1 - \u03b1} = \\frac{n}{\\frac{\u03bd\u03b1}{\u03b1} - \\frac{\u03bd - 1}{\u03bd\u03b1} - 1}$"}, {"title": "Appendix C. Curriculum learning", "content": "We train all models with curriculum learning. This means we incrementally increase the complexity\nof the task during the training. In particular, we train all models on short sequences first and then\nincrease the length of the sequences until it reaches the maximum length (16k tokens for babilong\nexperiments, 200 pairs for Associative Retrieval Remember, 50 pairs for Associative Retireval\nRewrite, and 1024 tokens for language modeling experiments (8 segments, 128 each))."}, {"title": "Appendix D. Babilong training details", "content": "We consider segments of size 512 for RMT and ARMT to process the long sequences. The curriculum\nlearning process uses the following number of sequences consecutively: 2, 3, 5, 8, 16, 32. So the\ntraining ends when we finish training on 32 segments, 512 tokens each. We also randomly sample\nthe number of segments during training, as we find it helps the model generalize better."}, {"title": "Appendix E. Associative Retrieval training details", "content": "Due to the task's simplicity and training efficiency, we are considering small models (about 500k\nparameters each) for the Associative Retrieval dataset studies. Every model that we compare has\nfour layers. 128 is the hidden dimension. If the memory dimension parameter (state size in Mamba\nand memory dimension in ARMT) is present in the model, it is assumed to be 32; if the contrary is\nnot indicated.\nMoreover, if the model supports segmentation (like RMT and ARMT), we use different segments\nfor different key-value pairs. Thus, if we have, for instance, 200 pairs, 200 segments are passed\nthrough the model, and after that, in the 201st segment, we expect the model to generate the value.\nBoth keys and values consist of several integers from 0 to 15.\nWe also use the curriculum with the following number of key-value pairs: 1, 2, 3, 5, 10, 20, 40,\n50, and 200. We increase the key size if necessary, so the final key size for remember task is 3 (so we\nhave $16^3$ unique keys) and for rewrite task it remains 1 (16 unique keys). For the Remember task,\nwe also consider sampling different numbers of pairs during training for better generalization."}, {"title": "Appendix F. Ablation", "content": "Due to an improper normalization vector z update, the proposed fast-weights technique [22] (also\nknown as delta-rule) does not have the length generalization (Fig. 4(a)). The information in the\nassociation matrix A is erased, but not from z, which is the source of the issue."}, {"title": "Appendix G. Language Modeling experiments", "content": "We utilized the Wikitext-103 dataset to train ARMT and RMT models in order to assess our\narchitecture's performance on real texts. Next, we examined the cross-entropy losses derived from\nvarious model segments on the test dataset, as illustrated in Figure 6. In this manner, we can estimate\nthe amount of language data that can be stored in memory.\nNevertheless, we demonstrate that despite having a larger theoretical capacity than RMT, ARMT\nstill performs similarly to RMT in language modeling.\nWe use the GPT-2 model as the base model for our architecture changes. We consider the RMT\nand ARMT models' segment sizes to be equal to 128 tokens and train these models to solve the\nlanguage modeling task on 8 segments, so in total, we train the model to autoregressively predict\n1024 tokens. Then we evaluate the models' performance on each of the 15 segments of test texts\n(1920 tokens)."}, {"title": "Appendix H. Why is mamba slow for long contexts?", "content": "We faced some difficulties in evaluating mamba on long-context (500k+) due to it's specific segmen-\ntation abilities shown in Figure 7."}, {"title": "Appendix I. Associative Retrieval sample structure", "content": "A sample of Remember dataset contains a concatenated context, query, and answer. The context\nis a set of key-value pairs (k, v), separated by a special token. All keys are sequences of tokens.\nTokens in this sequence can intersect, but the whole sequence corresponding to any key is unique\nin this particular sample. The query is one of the keys in the context. And the answer is a value\ncorresponding to the key from the query. Thus, we can control the number of pairs in the sample and\ncheck how many pairs fit in our memory.\nThis is how the dataset's sample appears:\n<key1>:<value1>,<key2>:<value2>, <key3>:<value3>,<key2>-<value2>\nThe model is thought to be trained to produce the value following the \"-\" character."}, {"title": "Appendix J. RWKV-5 model", "content": "We also tried to train the RWKV-v5 [18] model to slove both associative retrieval and babilong tasks\n(training from scratch for AR tasks and finetuning 430M model for babilong task). Unfortunately, the\nmodel was training poorly and hadn't achieved reasonable scores. We used the very same parameters\nas for training other models. Perhaps the RWKV training process requires accurate adjustments.\nHowever, we haven't succeeded."}, {"title": "Appendix K. Limitations", "content": "The proposed architecture, however, has several drawbacks. The first is the lack of efficient parallel\nimplementation: you have to process all segments consecutively. This doesn't mean that it is slow. It's\nfast enough to process millions of tokens in a reasonable time. However, on short and medium-length\nsequences (less than 300k tokens), it is much slower than, for instance, Mamba and RWKV, which\nhave the parallel form. Moreover, as we have shown in Appendix G, it's still challenging to train\nARMT to solve the language modeling task well. However, we believe that this problem is not in the\nvery architecture, but in training process: we observe that ARMT tends to keep in memory only the\nlast segment, and therefore struggles to extrapolate on longer sequences."}]}