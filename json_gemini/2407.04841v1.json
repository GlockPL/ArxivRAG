{"title": "Associative Recurrent Memory Transformer", "authors": ["Ivan Rodkin", "Yuri Kuratov", "Aydar Bulatov", "Mikhail Burtsev"], "abstract": "This paper addresses the challenge of creating a neural architecture for very long sequences that requires constant time for processing new information at each time step. Our approach, Associative Recurrent Memory Transformer (ARMT), is based on transformer self-attention for local context and segment-level recurrence for storage of task specific information distributed over a long context. We demonstrate that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%. The source code for training and evaluation is available on github.", "sections": [{"title": "1. Introduction", "content": "Memory plays a crucial role in creating models capable of processing extremely long contexts and utilizing remote past information. Starting from RNNs and evolving through LSTM [13] and Memory Networks [24, 28], we are now in the era of Transformer-based [27] Large Language Models [2, 17, 26]. Various methods for extending transformers context length have emerged [4, 19, 31], including approaches based on transformer segment-level recurrence [3, 5, 6, 20] and novel architectures that combine the efficiency of transformer parallelization during training with recurrence at inference [7, 8, 10, 11, 18]. Alternatively, Retrieval-Augmented Generation (RAG) focuses on retrieving information from external storage [1, 12, 23] or self-retrieving from past inputs [21, 29]. However, retrieval fails on complex tasks that require reasoning over multiple pieces of information [16].\nIn this work we propose the Associative Recurrent Memory Transformer (ARMT) as an extension of the segment-level recurrent model RMT [3] with associative memory. Compared to RWKV [18] and Mamba [10], which use association-based techniques, ARMT benefits from full local self- attention and has constant time and space complexity of processing new segment, similar to RMT.\nTo study ARMT performance we use the BABILong [16] benchmark, because it allows to generate test samples up to 50 million tokens and beyond, compared to other methods. Additionally, we use Associative Retrieval task with multiple key-value pairs to estimate memory capacity of models.\nMain contributions of this work include: (1) a novel ARMT architecture for long context processing with segment-level recurrence and associative memory; (2) demonstration that ARMT outcompetes existing memory based models like RMT [3] and Mamba [10] on associative retrieval and long context processing tasks, achieving 80% accuracy of single fact QA on unprecedented input"}, {"title": "2. Associative Recurrent Memory Transformer", "content": "We extend RMT [3](Fig. 1a) by addition of layerwise associative memory $A^{l}$ over segmented input $X^{l}$ (Fig. 1b). At every input segment $s$ for each layer $l$ memory tokens $M_{s}^{l+1}$ generated for preceding segment are added to $A_{s}^{l}$ (Fig. 1c) used to update input sequence and memory embeddings:\n$[X_{s}^{l+1}; M_{s}^{l+1}] = TrBlock(AssocBlock([X_{s}^{l}; M_{s}^{l}], A_{s}^{l})); A_{s}^{l'} = MemUpdate(A_{s}^{l-1}; M_{s}^{l+1})$.\nThe mechanism of associative block (Fig. 1c) is similar to linear transformers [15], but attends only to special memory tokens and is calculated differenty. After each segment, memory tokens are converted to keys and values via linear mapping and then stored in quasi-linear key-value memory [22] using non-linearity \u03c6. Given a memory token $m_{i} \u2208 M_{s}^{l+1}$, we calculate the keys, values, and importance scalars $\u03b2_{i}$. We then recall the previous association $v_{i}$ with this key, add the new value $v_{i}$ to the memory, erase the previous value $v'_{i}$ associated with $k_{i}$, and update the normalization vector.\n$k_{i}, v_{i} = W_{k}m_{i}, W_{y}m_{i}; \u03b2_{i} = \u03c3(W_{\u03b2}m_{i}); A_{s}^{0} = 0; z_{s}^{0} = 0;$\n$\\begin{equation}\nv_{i} = \\frac{A_{s}^{l-1}\u03c6(k_{i})}{(z_{i-1})^{T}\u03c6(k_{i})}; \u03b3 = 1 - \\frac{(z_{i-1})^{T}\u03c6(k_{i})}{||\u03c6(k_{i})||^{2}};\n\\end{equation}$\n$\\begin{equation}\nA_{s}^{l} = A_{s}^{l-1} + \u03b2_{i}(v_{i} \u2013 v'_{i}) \u03c6(k_{i})^{T}; z_{s}^{l} = z_{s}^{l-1}+ \u03b3\u03c6(k_{i}).\n\\end{equation}$\nOnce we updated $A_{s}^{l}$ with information from previous segment, we recall an association $y_{j}$ for a token $x_{j}$. Associations $y_{j}$ for each token in the segment are then passed to the next transformer layer:\n$\\begin{equation}\nq_{j} = W_{Q}x_{j}; Y_{j} = \\frac{A_{s}^{l}\u03c6(q_{j})}{(z_{s}^{l})^{T}\u03c6(q_{j})}.\n\\end{equation}$\nFor the non-linearity function \u03c6, we used the proposed in [22] DPFP-3 function because, in this paper, it has shown significant superiority over other methods, which is also consistent with our findings.\nNote that without \u03b3 (\u03b3 = 1) this approach suffers from catastrophic forgetting on some tasks. The reason is that while we erase the information $v'_{i}$ from the A-matrix, the corresponding keys"}, {"title": "3. Evaluation of Associative Retrieval and Long Context Memory Retention", "content": "We test memory capacity of ARMT in comparison to recent computationally efficient long-context models Mamba [10] and RMT [3] on the following two variants of associative retrieval.\nRemember task requires memorization of all key-value pairs with unique keys from the prior context with subsequent recalling a value corresponding to one of the keys (Fig. 2a). We estimate the total number of key-value pairs stored in memory, based on the exact-match metric (details are in Appendix B). Since ARMT has the same recurrent memory size as Mamba, calculated as the number of floats in recurrent states, it can be concluded that ARMT makes better use of its internal associative memory. Both ARMT and Mamba outperform RMT on this task. PRMT does not improve RMT performance (Fig. 2a). This indicates that the associative memory plays a critical role in ARMT performance compared to RMT. Additionally, we ablated ARMT on normalization correction, as detailed in Appendix F.\nIn Rewrite task the keys are not unique and the goal is to recall the latest value that corresponds to one of the keys from the prior context. This task evaluates the model's ability to dynamically change the memory storage. The results, shown in Fig. 2b, indicate that ARMT is robust to the number of memory rewrite operations, while RMT and Mamba experience slight degradation after exceeding their training lengths. Notably, ARMT maintains perfect memory recall on lengths exceeding 10 times those used in training."}, {"title": "4. Conclusion", "content": "In this work, we propose and evaluate recurrent memory transformer augmented with associative memory mechanism for long-context processing and find that it scales up to an unprecedented 50 million tokens on the BABILong benchmark. ARMT architecture adds an associative memory mechanism for segment-level recurrent model RMT. Based on our evaluation on associative retrieval tasks ARMT demonstrates significant advantage in memory capacity and generalisation compared to"}, {"title": "Appendix A. Related Work", "content": "AutoCompressor [5] is a strategy that stacks memory to minimize information loss at the price of quadratic computation cost.\nRecurrent Memory Transformers Recent challenges in long-context processing tasks demonstrated recurrent memory superiority over attention mechanism [16, 31] (Fig. 1 (a)). It was shown that this type of memory performs well even in contexts of size 11M [16]. But still, this memory has some issues with capacity and training. Capacity remains limited as the suggested memory states are limited to a small number of memory tokens. The training is still challenging, as the whole training process requires backpropagation through time for hundreds of layers. Our approach is supposed to mitigate these problems by leveraging the association matrix as a connector for different segments. In contrast to RMT, it has different parameters for memory (linear attention projections described in Section 2) and makes this memory hierarchical by creating different association matrices for different layers.\nContext explosion prevention In the attention sinks paper [30], authors demonstrated the need for some sinking tokens for attention for efficient extrapolation in long contexts. Recurrent Memory in RMT [3] as well as our model successfully perform this function. In RMT the memory tokens can act as attention sinks while in our model the very association matrix can play this role. The ARMT can also be thought of as a kind of compressed-memory RMT [3], that attends to all previous memory tokens with layerwise memory.\nRecurrent Sequence Models The problem of the quadratic cost of attention mechanism led to the development of recurrent architectures with transformer-like performance [7]. The vast majority of them are at least related to the State-Space Models (SSMs) [8, 10, 18, 25]. Despite comparable performance with transformers on LM tasks, SSMs are known to be less efficient in memorization tasks, especially when the question is asked after the information [14]. Our model performs well even on these types of tasks, because it has the large and flexible storage for keeping the associations in memory, simultaneously having the direct access to the local context via the vanilla attention."}, {"title": "Appendix B. Memory capacity estimation", "content": "Theorem:\nGiven:\nexact_match \u03b1; n = number of pairs; $\u03c5$ = number of possible values\nThen the number of memorized pairs can be estimated with the formula:\n$k = \\frac{\u03b7\u03bd\u03b1}{\u03c5 \u2013 1}$\nProof:\nWe can precisely predict the associated value if we remember k pairs and then extract the key from these pairs. We output the random value if we obtain the key from any other pair. As a result, the following is the mathematical expectation of an exact match:\n$\\frac{k}{n} \\cdot 1 + \\frac{n - k}{n} \\cdot \\frac{1}{\u03c5} = \u03b1$ = $\\frac{k(\u03c5 - 1) + n}{n\u03c5}$"}, {"title": "Additionally:", "content": "$k = \\frac{\u03bd\u03b1}{1 \u2013 \u03b1}$\n$k = \\frac{\u03b7\u03b1}{\u03c5 \u2013 1}$ \n$k = \\frac{\u03b7\u03b1}{\u03bd\u03b1 \u2013 1}$ \n$1 \u2013 \u03b1 = \\frac{\u03c5 \u2013 1}{\u03bd\u03b1}$\n\u03b1 = $\\frac{\u03bd\u03b1 \u2013 1}{\u03c5 \u2013 1}$\n1 = $\\frac{\u03c5}{\u03bd\u03b1}$"}, {"title": "Appendix C. Curriculum learning", "content": "We train all models with curriculum learning. This means we incrementally increase the complexity of the task during the training. In particular, we train all models on short sequences first and then increase the length of the sequences until it reaches the maximum length (16k tokens for babilong experiments, 200 pairs for Associative Retrieval Remember, 50 pairs for Associative Retireval Rewrite, and 1024 tokens for language modeling experiments (8 segments, 128 each))."}, {"title": "Appendix D. Babilong training details", "content": "We consider segments of size 512 for RMT and ARMT to process the long sequences. The curriculum learning process uses the following number of sequences consecutively: 2, 3, 5, 8, 16, 32. So the training ends when we finish training on 32 segments, 512 tokens each. We also randomly sample the number of segments during training, as we find it helps the model generalize better."}, {"title": "Appendix E. Associative Retrieval training details", "content": "Due to the task's simplicity and training efficiency, we are considering small models (about 500k parameters each) for the Associative Retrieval dataset studies. Every model that we compare has four layers. 128 is the hidden dimension. If the memory dimension parameter (state size in Mamba and memory dimension in ARMT) is present in the model, it is assumed to be 32; if the contrary is not indicated.\nMoreover, if the model supports segmentation (like RMT and ARMT), we use different segments for different key-value pairs. Thus, if we have, for instance, 200 pairs, 200 segments are passed through the model, and after that, in the 201st segment, we expect the model to generate the value. Both keys and values consist of several integers from 0 to 15.\nWe also use the curriculum with the following number of key-value pairs: 1, 2, 3, 5, 10, 20, 40, 50, and 200. We increase the key size if necessary, so the final key size for remember task is 3 (so we have 163 unique keys) and for rewrite task it remains 1 (16 unique keys). For the Remember task, we also consider sampling different numbers of pairs during training for better generalization."}, {"title": "Appendix F. Ablation", "content": "F.1. Gamma-correction\nDue to an improper normalization vector z update, the proposed fast-weights technique [22] (also known as delta-rule) does not have the length generalization (Fig. 4(a)). The information in the association matrix A is erased, but not from z, which is the source of the issue."}, {"title": "Appendix G. Language Modeling experiments", "content": "We utilized the Wikitext-103 dataset to train ARMT and RMT models in order to assess our architecture's performance on real texts. Next, we examined the cross-entropy losses derived from various model segments on the test dataset, as illustrated in Figure 6. In this manner, we can estimate the amount of language data that can be stored in memory.\nNevertheless, we demonstrate that despite having a larger theoretical capacity than RMT, ARMT still performs similarly to RMT in language modeling.\nWe use the GPT-2 model as the base model for our architecture changes. We consider the RMT and ARMT models' segment sizes to be equal to 128 tokens and train these models to solve the language modeling task on 8 segments, so in total, we train the model to autoregressively predict 1024 tokens. Then we evaluate the models' performance on each of the 15 segments of test texts (1920 tokens)."}, {"title": "Appendix H. Why is mamba slow for long contexts?", "content": "We faced some difficulties in evaluating mamba on long-context (500k+) due to it's specific segmentation abilities shown in Figure 7."}, {"title": "Appendix I. Associative Retrieval sample structure", "content": "A sample of Remember dataset contains a concatenated context, query, and answer. The context is a set of key-value pairs (k, v), separated by a special token. All keys are sequences of tokens. Tokens in this sequence can intersect, but the whole sequence corresponding to any key is unique in this particular sample. The query is one of the keys in the context. And the answer is a value corresponding to the key from the query. Thus, we can control the number of pairs in the sample and check how many pairs fit in our memory.\nThis is how the dataset's sample appears:\n<key1>:<value1>,<key2>:<value2>, <key3>:<value3>,<key2>-<value2>\nThe model is thought to be trained to produce the value following the \"-\" character."}, {"title": "Appendix J. RWKV-5 model", "content": "We also tried to train the RWKV-v5 [18] model to slove both associative retrieval and babilong tasks (training from scratch for AR tasks and finetuning 430M model for babilong task). Unfortunately, the model was training poorly and hadn't achieved reasonable scores. We used the very same parameters as for training other models. Perhaps the RWKV training process requires accurate adjustments. However, we haven't succeeded."}, {"title": "Appendix K. Limitations", "content": "The proposed architecture, however, has several drawbacks. The first is the lack of efficient parallel implementation: you have to process all segments consecutively. This doesn't mean that it is slow. It's fast enough to process millions of tokens in a reasonable time. However, on short and medium-length sequences (less than 300k tokens), it is much slower than, for instance, Mamba and RWKV, which have the parallel form. Moreover, as we have shown in Appendix G, it's still challenging to train ARMT to solve the language modeling task well. However, we believe that this problem is not in the very architecture, but in training process: we observe that ARMT tends to keep in memory only the last segment, and therefore struggles to extrapolate on longer sequences."}]}