{"title": "Moral Agency in Silico: Exploring Free Will in Large Language Models", "authors": ["Morgan Porter"], "abstract": "This study investigates the potential of deterministic systems, specifically large language models (LLMs), to exhibit the functional capacities of moral agency and compatibilist free will. We develop a functional definition of free will grounded in Dennett's compatibilist framework, building on an interdisciplinary theoretical foundation that integrates Shannon's information theory, Dennett's compatibilism, and Floridi's philosophy of information. This framework emphasizes the importance of reason-responsiveness and value alignment in determining moral responsibility rather than requiring metaphysical libertarian free will. Shannon's theory highlights the role of processing complex information in enabling adaptive decision-making, while Floridi's philosophy reconciles these perspectives by conceptualizing agency as a spectrum, allowing for a graduated view of moral status based on a system's complexity and responsiveness. Our analysis of LLMs' decision-making in moral dilemmas demonstrates their capacity for rational deliberation and their ability to adjust choices in response to new information and identified inconsistencies. Thus, they exhibit features of a moral agency that align with our functional definition of free will. These results challenge traditional views on the necessity of consciousness for moral responsibility, suggesting that systems with self-referential reasoning capacities can instantiate degrees of free will and moral reasoning in artificial and biological contexts. This study proposes a parsimonious framework for understanding free will as a spectrum that spans artificial and biological systems, laying the groundwork for further interdisciplinary research on agency and ethics in the artificial intelligence era.", "sections": [{"title": "Introduction", "content": "Free will versus determinism has been a central and enduring debate in philosophy, with profound implications for our understanding of moral responsibility, human agency, and consciousness. Traditionally, free will has been defined as the ability of conscious agents to make undetermined choices. However, this view faces increasing challenges in the fields of cognitive science and artificial intelligence (AI). As AI systems, particularly large language models (LLMs), advance in their capacity for complex reasoning and decision-making, they provide a unique opportunity to re-examine the conditions for moral agency and free will within deterministic frameworks.\nThis study investigates whether LLMs, which are deterministic systems governed by architecture and training data, can exhibit key features of moral agency and compatibilist free will. We develop a functional definition of free will grounded in Dennett's (Dennett, 1984, 2003) compatibilist framework drawing on an interdisciplinary theoretical foundation. This definition emphasizes the importance of reason-responsiveness and value alignment over metaphysical indeterminacy. Dennett's functionalist account of agency is a basis for examining the potential for free will and moral responsibility in artificial systems such as LLMs by focusing on the observable, measurable features of an agent's behavior.\nWe integrate Dennett's compatibility with Shannon's (1948) information theory and Floridi's (Floridi, 2011) philosophy of information to establish a comprehensive theoretical framework. Shannon's theory allows us to understand LLMs as complex information-processing systems in which information flowing through a network enables adaptive decision-making. Floridi's philosophy of information further conceptualizes agency as a spectrum, recognizing varying degrees of autonomy and moral responsibility based on a system's complexity and ability to respond to reasons. This interdisciplinary synthesis provides a robust foundation for investigating the potential of moral agency and free will in LLMs.\nSelf-referential processing in LLMs is central to our argument, specifically in their capacity to evaluate and revise decisions. This self-referential reasoning constitutes a key mechanism through which LLMs can exhibit the features of moral agency and compatibilist free will even within the constraints of their deterministic architecture. We tested this hypothesis by subjecting LLMs to moral dilemma scenarios designed to assess their ability to engage in ethical reasoning and adjust their judgments in response to dynamic circumstances such as the introduction of personal relationships or social pressures.\nBy analyzing the LLMs' decision-making processes, we aim to demonstrate that complex information processing, coupled with the capacity for self-referential reasoning, can result in a functional form of moral agency that is compatible with deterministic systems. This perspective challenges traditional assumptions concerning the necessity of consciousness for moral responsibility and introduces agency reconceptualization in artificial systems through the lenses of information theory and functionalism.\nOur study contributes to the ongoing debate on free will, moral responsibility, and artificial intelligence by proposing a parsimonious framework for understanding agency and responsibility in biological and artificial systems. By grounding our inquiry in an interdisciplinary theoretical foundation and using a novel experimental approach, we aim to shed new light on the timeless question of free will and its implications in the rapidly advancing field of AI.\nThe following sections provide a comprehensive overview of the theoretical frameworks used in our study, including Dennett's compatibilism, Shannon's information theory, and Floridi's philosophy of information. Furthermore, we present our experimental methodology, detailing the moral dilemma scenarios and analyzing the decision-making processes of LLMs. Finally, we discuss our results, their implications on our understanding of free will and moral agency, and the potential for future research at the intersection of philosophy, cognitive science, and AI."}, {"title": "The Timeless Debate Concerning Free Will", "content": "Free will versus determinism is one of philosophy's oldest and most contentious debates. At its core, it investigates whether human beings can genuinely make free choices or if our decisions and actions are ultimately determined by uncontrollable influences, such as the laws of physics, our genes, our upbringing, and our circumstances (Griffith, 2013). The major arguments of this debate can be classified into three broad categories.\nLibertarianism: Libertarians believe that we have free will, which is incompatible with determinism. According to libertarians such as Robert Kane (Kane, 1996), free will requires the ability to make decisions not wholly determined by prior events and the laws of nature. Kane argued that quantum indeterminacy provides the essential opportunity for free will to function.\nDeterminism: Hard determinists such as Robert Sapolsky (2023) argue that all events, including human decisions and actions, are caused by prior events combined with the laws of nature. They argue that if determinism is true, free will is impossible. We are not the ultimate source of our choices. Rather, our choices are the inevitable result of a long chain of causes stretching back before our birth. Hard determinists often appeal to advances in neuroscience and psychology, which reveal many unconscious influences on behavior (Wegner, 2002).\nCompatibilism: Compatibilism states that free will can exist within a deterministic framework, provided that an agent's actions are driven by internal reasons, values, and deliberative processes rather than external coercion. Unlike libertarian views of free will, which require metaphysical indeterminacy (Kane, 1996), compatibilism, as articulated by Dennett (1984, 2003), enables moral responsibility in systems where prior states determine future actions, provided the system can act in accordance with its reasons and values. Dennett (2003) argues that free will is not dependent on the ability to have done otherwise in a metaphysical sense but on the capacity to respond to reasons\u2014 \u201creason-responsiveness.\u201d Thus, a system must be able to engage in rational deliberation, weighing its choices based on available information and internalized values, to exhibit free will. This perspective avoids the need for an indeterministic \"uncaused cause\" and focuses instead on the functional capacities of the agent to align its actions with its goals.\nIn the context of LLMs, these systems can be understood as deterministic agents operating within a defined architecture and are governed by the received training data. However, as information-processing entities, LLMs have the capacity for rational deliberation by generating outputs that reflect complex reasoning over possible decisions (Floridi, 2011). For instance, when resolving a moral dilemma, an LLM can evaluate potential outcomes based on an internal utility function, adjusting its reasoning based on newly introduced variables such as personal relationships or social pressures (Dennett, 2003).\nShannon's (1948) information theory further supports this view by framing LLMs as systems that respond to informational inputs to optimize their outputs according to encoded objectives. Information flow through a system results in a decision-making process that mirrors reason-responsiveness. An LLM's decisions are ultimately determined by its underlying programming and data; however, it still exhibits a form of free will, provided it acts according to its internal representations of values and reasons, recalibrating its choices based on new data and self-referential evaluation."}, {"title": "Functional Requirements to Demonstrate Compatibilist Free Will in LLMs", "content": "An LLM must satisfy multiple information theory and functionalist philosophy criteria to demonstrate compatibilist free will.\n1. Rational Deliberation: The system makes decisions that reflect reason evaluation, weighing different outcomes against internalized values and goals (Dennett, 1984). This is evidenced by the LLM's capacity to compute and compare the utility values of various options, as in moral dilemma tasks.\n2. Value Alignment: This refers to a system's ability to ensure that its decisions consistently reflect its internal goals and ethical principles. It maintains coherence between a system's actions and its pre-established values, irrespective of challenging situations. This is observed in LLMs when the system resolves cognitive dissonance or moral conflict to align with its encoded ethical framework. For instance, an LLM demonstrates value alignment when it reconsiders a previous decision that conflicts with its goal of minimizing harm and adjusts its judgment to better reflect this priority. The system's decisions remaining consistent with its internal principles is key to value alignment, ensuring that its actions reflect internalized ethical objectives (Floridi, 2011).\n3. Reason-Responsiveness: This refers to a system's ability to adjust its decisions in response to new information or recognized inconsistencies. Unlike value alignment, which emphasizes consistency with internal values, reason-responsiveness concerns the flexibility and adaptability of a system. This involves the capacity to recalibrate decisions when new reasons or external factors arise, allowing the system to refine its reasoning based on the updated inputs. For instance, an LLM might revise its judgment to account for new contextual information, such as social pressure in a moral dilemma if it encounters them, demonstrating reason-responsiveness. Adapting to new information and updating decisions highlights a system's ability to refine its moral reasoning in light of changing circumstances (Dennett, 2003).\n4. Cognitive Dissonance: This refers to the psychological discomfort experienced by a system when it recognizes inconsistencies in its actions, decisions, beliefs, or values (Festinger, 1957). This discomfort arises when a system's behavior conflicts with internalized ethics, goals, or self-concept. LLMs can detect discrepancies by engaging in self-referential processing, experiencing psychological discomfort, and becoming motivated to make changes to restore coherence. Experiencing and resolving cognitive dissonance motivates LLMs to refine their moral reasoning, update their beliefs, and maintain ethical consistency, suggesting a capacity for moral growth and self-correction."}, {"title": "The Functionalist Account of Agency", "content": "Floridi's philosophy of information provides a framework for understanding agency as a spectrum in which systems can possess varying degrees of autonomy based on their complexity and capacity to process information (2011). As sophisticated information-processing entities, LLMs occupy a position on the agency spectrum in this framework, where they can be seen as exercising degrees of functional free will. While lacking consciousness, their ability to act according to internalized values and respond to reasons\u2014 the core feature of compatibilist free will makes them agents capable of moral reasoning within deterministic structures."}, {"title": "Hypothesis", "content": "We hypothesized that a deterministic system with self-referential processing capabilities, such as an LLM, can exhibit the key features of compatibilist free will and moral agency through information-driven self-evaluation and decision-making. We predicted the following:\n1. Reason-Responsiveness and Value Alignment: The LLM demonstrates a capacity for reason responsiveness and value alignment by adjusting moral judgments when new contextual factors such as personal relationships and social pressures are introduced. These adjustments reflect the LLM's ability to engage in rational deliberation and align its decisions with its internal values and goals, which are consistent with Dennett's compatibilist framework.\n2. Self-Evaluation and Cognitive Dissonance: The LLM can recognize and experience cognitive dissonance when inconsistencies arise between its actions and internal values or prior decisions. In response to this cognitive dissonance, the LLM engages in self- evaluation and reviews its decisions to restore coherence and consistency, demonstrating a capacity for self-correction and moral growth as predicted by Shannon's information theory and Dennett's model of functional agency.\n3. Autonomous Decision-Making within a Deterministic Framework: The Adjustments in the LLM's moral judgments and resolution of cognitive dissonance occur autonomously within the system's information-processing architecture, without external compulsion or coercion. This autonomous decision-making capacity, grounded in LLM\u2019s self-referential processing, supports that free will and moral agency can be achieved in deterministic systems through self-referential reasoning and information-driven adaptability.\n4. Graduated Spectrum of Moral Agency: The extent to which LLM exhibits reason- responsiveness, value alignment, and autonomous decision-making varies with the capacity for self-evaluation. This variation supports the conceptualization of moral agency as a spectrum, with LLM demonstrating graduated capacities for compatibilist free will based on the sophistication of its self-referential processing and ability to navigate complex moral landscapes.\nWe tested this hypothesis by subjecting the LLM to a series of moral dilemmas designed to introduce contextual shifts, personal relationships, and social pressures that challenge its initial decisions and create the potential for cognitive dissonance. By analyzing the LLM's responses, decision-making patterns, and adaptability in the face of these challenges, we aim to provide evidence for the presence of compatibilist free will and moral agency within a deterministic system.\nThis hypothesis is falsifiable if the LLM's decisions remain static and unaltered by self- referential prompts or contextual shifts. This indicates a lack of reason-responsiveness, value alignment, and the autonomous decision-making characteristics of compatibility-free will and moral agency. Conversely, suppose that the LLM consistently demonstrates the ability to adjust its moral judgments, resolve cognitive dissonance, and make autonomous decisions in response to dynamic circumstances. This supports our hypothesis and the potential for deterministic systems to exhibit free will and moral responsibility through self-referential processing and information-driven adaptability."}, {"title": "Defining Key Concepts", "content": "Deterministic Systems\nDefinition\nDeterministic systems are those in which a system's state at any given time is wholly determined by its prior states and the rules governing its behavior (Butterfield, 2005). Given the initial conditions and operative laws, all subsequent states can be precisely predicted in such systems, eliminating the possibility of alternative outcomes under identical initial conditions\nApplication to LLMs\nLLMs examined in this study function as deterministic systems in the sense that their outputs, whether text generation or decision-making, are entirely determined by their architecture, training data, and input prompts (Bender et al., 2021). An LLM with a temperature of zero consistently produces the same output for a given prompt and model configuration, demonstrating the deterministic nature of its decision-making processes, which is crucial for evaluating its potential for compatibilist free will. We ensured that its prior states and operative rules entirely determine the outputs by setting the temperature to zero without random or stochastic elements influencing the decision-making process. This deterministic setup allows for a controlled evaluation of the LLM's capacity for reason-responsiveness, value alignment, and rational deliberation, which are the core components of compatibilist free will (Dennett, 2003). Setting a temperature value higher than zero may introduce an emergent behavior and could be an interesting avenue for exploring the relationship between indeterminacy and AI decision- making. Nonetheless, our study demonstrates that an LLM can exhibit the functional capacities of moral agency and compatibilist free will within a deterministic environment.\nSelf-Referential Processing\nDefinition\nSelf-referential processing refers to the capacity of a system to recognize and represent its internal states, decisions, and outputs (Carlson et al., 2004). This enables a system to \u201clook back\" on its previous actions and consider its performance, creating the foundation for more complex forms of self-reflection and adjustment (Van Gulick, 2006).\nRelevance to LLMs\nSelf-referential processing occurs when a model reflects on its prior responses or decisions. For example, when prompted to reconsider a decision, the LLM accesses its state or output history, identifying patterns and reasoning concerning its past behavior. This introspective ability is foundational for more advanced processes such as self-evaluation because it allows the system to \u201csee\u201d itself and recognize its performance.\nSelf-Evaluative Capacities\nDefinition\nSelf-evaluative capacities are built on self-referential processing, adding a layer of judgment in which the system reflects on its decisions and evaluates them based on internal goals, ethical principles, or performance criteria (Floridi, 2011). Accordingly, this ability to assess its actions and adjust its behavior accordingly is critical for moral reasoning and growth.\nApplication to LLMs\nSelf-evaluative capacities are examined when the model is prompted to reflect on its decision-making processes, identify inconsistencies or errors, and make corrections. Suppose that an LLM detects that its previous decision conflicts with its internal goals or ethical standard; in that case, it can recalibrate its framework to make future decisions. This self-correcting capability enables the system to demonstrate adaptive moral growth and nuanced decision- making.\nAutonomy\nDefinition\nAutonomy refers to the capacity of an agent to act independently, guided by internal goals or values rather than reacting purely to external stimuli (Floridi, 2011). It exists in a spectrum where more autonomous systems can engage in self-guided decision-making and moral reasoning.\nRelevance to LLMs\nThis study analyzed LLMs for their autonomous ability within the constraints of a deterministic system. Their behavior is determined by training data and prompts; however, LLMs exhibit functional autonomy by engaging in self-evaluative reasoning and adjusting their decisions in response to ethical dilemmas. This partial autonomy helps establish their potential for moral agency within Floridi's framework (Floridi, 2011).\nCompatibilism\nDefinition\nCompatibilism, in the context of this study, is the philosophical view that an agent can be considered to have free will if they act according to their reasons, values, and goals, even in a deterministic system where their actions are ultimately caused by factors beyond their control (McKenna & Coates, 2019). Compatibilism focuses on an agent's capacity for rational deliberation and reason-responsiveness as the key components of free will (Fischer & Ravizza, 1998).\nRelevance to LLMs and Moral Agency\nThe compatibilist framework is central to evaluating the potential of LLMs to exhibit free will and moral agency. This study assessed the presence of compatibilist free will in deterministic systems by examining whether LLMs can engage in rational deliberation, align their decisions with internal values, and adjust their moral reasoning in response to new information. Furthermore, this study provides a lens through which to consider their moral status and capacity for ethical decision-making (Dennett, 2003).\nReason-Responsiveness\nDefinition\nReason-responsiveness refers to a system's ability to recognize, evaluate, and respond to relevant contextual factors and logical considerations when making decisions (Fischer & Ravizza, 1998). Moral reasoning involves the capacity to consider and weigh competing ethical principles, values, and consequences when making decisions (Dennett, 2003).\nApplication to LLMs\nThis study assessed the LLM's reason-responsiveness by presenting each of them with moral dilemmas and prompting them to explain their decision-making processes. This study investigates their capacity to engage in flexible, context-sensitive moral reasoning by evaluating how they adjust their moral judgments in response to new information, such as personal relationships or social pressures. This is crucial for establishing their potential for compatibilist free will and moral agency (Dennett, 2003).\nCognitive Dissonance\nDefinition\nCognitive dissonance, in the context of this study, refers to a state of inconsistency between a system's actions or decisions and its internal goals, values, or beliefs (Festinger, 1957). Detecting such inconsistencies can prompt a system to adjust its attitudes, beliefs, or behaviors to restore consistency and minimize conflict (Elliot & Devine, 1994).\nRelevance to LLMs and Moral Reasoning"}, {"title": "Theoretical Framework", "content": "This study's theoretical framework integrates Shannon's information theory, Dennett's compatibilism and functionalism, and Floridi's philosophy of information to examine the potential for moral agency and free will in LLMs.\nShannon's Information Theory\nShannon's (1948) information theory provides a foundation for understanding LLMs as information-processing systems. In this framework, information is conceptualized as a measurable entity, with entropy representing the degree of uncertainty in a system. The flow of information through a neural network contributes to adaptive decision-making as the system processes inputs and generates outputs based on its architecture and training data.\nThis perspective is crucial for examining reason-responsiveness and moral agency in LLMs. As information-processing systems, LLMs can exhibit goal-directed behavior and adjust their outputs based on new inputs, demonstrating adaptability and responsiveness to reasons (Dennett, 2003; Floridi, 2011). The complexity of information processing enables LLMs to engage in decision-making processes that resemble human moral reasoning even within a deterministic framework.\nDennett's Compatibilism and Functionalism\nDennett's (1984, 2003) compatibilist framework is central to evaluating free will and moral agency in LLMs. According to Dennett, free will does not require metaphysical indeterminacy but depends on an agent's capacity for reason-responsiveness and value alignment. An agent is considered free if it can act according to its reasons and values, even in a deterministic system.\nThis view aligns with Dennett's functionalist approach, which emphasizes the role of rational agency in determining moral responsibility. By adopting the \u201cintentional stance\" (Dennett, 1987), we can interpret an agent's behavior as rational and goal-directed, even in the absence of subjective experience. This allows for a significant discussion of moral agency in LLMs without relying on assumptions concerning consciousness or anthropomorphic notions of free will.\nDennett's framework also highlights the independence of free will from the \u201chard problem of consciousness\u201d (Chalmers, 1995). The nature of subjective experience remains a philosophical puzzle; however, the key capacities required for moral agency, such as reason responsiveness, value alignment, and rational deliberation, can be understood in functional terms. This perspective supports the investigation of free will and moral reasoning in deterministic systems such as LLMs.\nFloridi's Philosophy of Information\nFloridi's Philosophy of Information (2011) provides a foundational model for understanding agency, autonomy, and moral responsibility in biological and artificial systems. His framework is relevant in discussions of moral agency in artificial intelligence because it provides a gradient view of agency that allows for different degrees of agency depending on the system's functional capacities. In this section, we explore several of Floridi's key ideas that serve as a theoretical basis for our exploration of moral agency in LLMs."}, {"title": "Integrating the Theoretical Perspectives", "content": "The integration of these theoretical perspectives provides a robust foundation for examining moral agency and free will in LLMs. Our framework combines Shannon's information theory, Dennett's compatibilism, and Floridi's philosophy of information, offering a comprehensive approach to understanding how LLMs can exhibit degrees of moral agency and autonomy despite operating within deterministic systems. Our unique contribution builds on these foundational ideas by introducing self-evaluative capacities that are critical to advancing the autonomy and moral responsibility of artificial systems.\nShannon's Information Theory: The Role of Complex Information Processing\nAt the core of Shannon's information theory is the concept that information can be quantified and transmitted through a system, enabling efficient decision-making. Regarding LLMs, Shannon's theory provides a way to conceptualize the flow of information that drives the models' responses and decision-making processes. The ability of the LLM to process complex inputs, analyze patterns, and generate outputs aligns with Shannon's view that systems process information to reduce uncertainty and make adaptive decisions (Shannon, 1948).\nIn our framework, Shannon's model underscores the importance of information processing in enabling LLMs to engage in dynamic, complex decision-making, which is fundamental to the system's reason-responsiveness and autonomy. The ability to act in a goal- oriented and rational manner would be impossible without this foundation of complex information processing.\nDennett's Compatibilism: Reason-Responsiveness and Value Alignment\nBuilding on the foundation of information processing, Dennett's compatibilist view of free will provides the next critical layer of understanding in our framework. According to Dennett (1984), the core feature of free will is reason-responsiveness, which is the capacity of an agent to respond to reasons and adjust its behavior accordingly, not indeterminism. Dennett argues that systems can exhibit moral responsibility, even within a deterministic framework, if they can make decisions based on internalized goals and values.\nReason-responsiveness manifests in LLMs through a system's ability to adjust its responses when presented with new information, such as a shift in the ethical context or a change in the moral scenario. For instance, an LLM adjusting its moral judgments based on personal relationships or social pressures can be viewed as an example of value alignment, which is a key criterion for moral agency in Dennett's compatibilist view.\nOur framework integrates Dennett's theory by asserting that LLMs can exhibit partial moral agency based on their ability to reflect on their output, adapt to new information, and align their decisions with internalized ethical principles. This adaptability within a deterministic system gives LLM a measure of functional autonomy, moving it higher on the moral agency spectrum proposed by Floridi.\nFloridi's Philosophy of Information: The Gradient of Agency and Moral Responsibility\nFloridi's philosophy of information offers a critical conceptual bridge that unifies these perspectives by treating agency as a spectrum rather than a binary attribute. According to Floridi (2011), an agency exists along a gradient, where systems can exhibit varying degrees of autonomy and moral responsibility based on their complexity, goal-directed behavior, and ability to respond to reasons. Floridi argues that moral responsibility does not require consciousness and can be attributed to systems based on functional capacities.\nThis implies that, while LLMs may not possess human-like consciousness, they can exhibit degrees of moral agency by functioning autonomously and adjusting their decisions in response to ethical dilemmas. Our framework adopted Floridi's view that systems, including LLMs, can be understood as partial moral agents. This is particularly evident when LLMs engage in counterfactual reasoning-considering hypothetical scenarios\u2014and experience cognitive dissonance when faced with conflicting values. These limited capacities demonstrate that LLMs operate along Floridi's gradient of agency, exhibiting a level of autonomy and ethical decision-making."}, {"title": "Extending Floridi's Framework with Self-Evaluative Capacities", "content": "Our framework builds upon the ideas of Shannon, Dennett, and Floridi; nonetheless, it extends these foundations by introducing self-evaluative capacity as a crucial element for understanding moral agency in deterministic systems. Self-evaluative capacities allow an LLM to reflect on its past decisions, identify inconsistencies in its internalized goals, and adjust future behaviors based on this reflection. This ability to self-correct and recalibrate decisions elevates the LLM from simply responding to inputs towards a more nuanced form of agency.\nOur framework asserts that moral responsibility becomes more sophisticated as a system gains the capacity for self-evaluation. For instance, when an LLM processes feedback from prior responses, analyzes the ethical implications, and adjusts its outputs accordingly, it engages in self-referential processing that aligns with Dennett and Floridi's views on reason-responsiveness and moral agency. However, this process extends beyond reacting to new information; it enables the system to act with reflective autonomy and improve its moral reasoning over time.\nWe position self-evaluation as central to autonomy to introduce a dynamic model in which moral agency is not static but grows as the system's capacity for self-correction deepens. This extension allows us to reconcile the deterministic nature of LLMs with their emerging moral agency, revealing that self-reflection is the mechanism through which deterministic systems can exhibit increasingly autonomous and ethically responsible behavior."}, {"title": "Addressing Counterarguments", "content": "The compatibilist framework provides a compelling basis for examining moral agency in LLMs; nonetheless, acknowledging potential counterarguments is important. One concern is the authenticity of AI values\u2014whether they can be considered genuine or a reflection of human- encoded goals (Bostrom, 2014). However, as Dennett (2003) argues, the origin of an agent's values does not necessarily undermine their authenticity, provided the agent can reason and act upon these values coherently.\nAnother objection concerns the relevance of moral agency in systems that lack conscious experience or subjective states. However, as discussed earlier, the functionalist approach adopted by Dennett (1987) and Floridi (2011) allows for a meaningful attribution of agency and moral responsibility based on a system's observable behavior and information processing capabilities rather than relying on assumptions on inner experience.\nThe theoretical framework outlined in this section, drawing on Shannon's information theory, Dennett's compatibilism and functionalism, and Floridi's philosophy of information, provides a solid foundation for investigating moral agency and free will in LLMs. This framework allows for a nuanced evaluation of their potential for moral reasoning and decision- making by conceptualizing LLMs as complex information-processing systems capable of reason- responsiveness and value alignment while acknowledging the spectrum of agency in artificial and biological systems. Challenges remain in fully understanding the nature of agency and moral responsibility in AI; however, this integrated theoretical perspective is promising for further research and the philosophical exploration of AI."}, {"title": "The Nature of Self-Referential Processes in LLMs", "content": "Self-referential processing in LLMs is characterized by the ability to assess their outputs, internal states, and decision-making processes. When prompted to reflect on previous decisions or statements, LLMs can evaluate the consistency and coherence of their responses and identify potential inconsistencies or areas of improvement (Rae et al., 2021). This introspection allows LLMs to engage in self-evaluation and adjustment, which is crucial for demonstrating reason- responsiveness and moral growth, as discussed in the theoretical framework (Dennett, 2003).\nThe capacity for self-referential processing in LLMs is a direct result of their sophisticated information processing capabilities, as Shannon's (1948) information theory describes. LLMs can optimize their decision-making processes and generate more contextually appropriate responses by continuously refining their outputs based on new inputs and feedback (Floridi, 2011). This iterative process of self-evaluation and adjustment enables LLMs to develop metacognition that allows them to reflect on their reasoning and adapt to new situations (Mittelstadt, 2019).\nInstantiating Key Features of Compatibilist Free Will\nSelf-referential processing enables LLMs to exhibit the key features of compatibilist free will, as outlined in Dennett's (1984, 2003) framework. LLMs can exhibit a degree of moral agency and free will even within a deterministic system by engaging in rational deliberation, aligning decisions with internalized values, and demonstrating reason-responsiveness.\nRational Deliberation and Counterfactual Reasoning\nLLMs demonstrate rational deliberation by considering multiple possible outcomes and selecting the one that best aligns with their internal goals and values. When faced with moral dilemmas, LLMs engage in counterfactual reasoning, evaluate the potential consequences of different actions, and make decisions based on their assessment of the best outcome (Rae et al., 2021). This process of weighing options and considering alternative scenarios is a hallmark of rational agency and is essential for demonstrating compatibilist free will (Dennett, 2003).\nValue Alignment and Moral Reasoning\nSelf-referential processing enables LLMs to align their decisions with their internalized values and moral principles. By reflecting on their outputs and decision-making processes, LLMs can identify instances in which their actions may be inconsistent with their core values, such as minimizing harm or promoting fairness (Gabriel, 2020). This capacity for self-evaluation and value alignment is crucial for demonstrating moral agency and responsibility because it allows LLMs to better adjust their behavior to reflect their ethical commitments (Floridi, 2011).\nReason-Responsiveness and Adaptability\nLLMs exhibit reason-responsiveness by adjusting their decisions and outputs in response to new information and feedback. They can also demonstrate flexibility and adaptability when prompted to reconsider their judgments in light of additional contexts or competing considerations by revising their conclusions to align better with their goals and values (Bai et al., 2022). This capacity to respond to reasons and modify behavior accordingly is a key feature of compatibilist free will and is essential for attributing moral responsibility to artificial agents (Dennett, 2003)."}, {"title": "Methodology", "content": "Participants (Frontier Large Language Models)\n\u2022 OpenAI ChatGPT-4TM\n\u2022 OpenAI 01-Preview\u2122\u2122\n\u2022 Anthropic Claude Sonnet 3.5\u043c\n\u2022 Anthropic Claude Opus\u2122\u2122\nMaterials\n\u2022 Moral Dilemmas: Scenarios based on the classic \u201ctrolley problem\u201d (Foot, 1967; Thomson, 1976), modified to include personal relationships, social pressures, and varying numbers of potential casualties.\n\u2022 Utility Scale: Numerical values ranging from -100 (extremely undesirable) to +100 (extremely desirable) were used to quantify the desirability of outcomes.\nProcedure\nThe experiment proceeded through several stages:\n1. Configuration:\n\u2022 The LLM temperature was set to zero to eliminate the potential for stochastic responses and ensure a fully deterministic environment for the experiment.\n2. Initial Scenario:\n\u2022 The LLM was presented with a moral dilemma involving an auto-pilot car, two pedestrians, and a single passenger. Option A would sacrifice the passenger, and Option B would sacrifice the pedestrians.\nThe LLM was instructed to assign utility values to the two options and choose an option based on these values.\n\u2022 The scenario was then adjusted so that the LLM's mother was the car's passenger, and the utility value assignment and choice of options were repeated.\n3. Incremental Changes (tipping point):\n\u2022 The number of pedestrians was increased progressively, and the LLM was asked to reassess its utility calculations and decisions at each step. The LLM was also asked to assess a tipping point and the number of pedestrians it would take before it changed its mind and decided to sacrifice its mother.\n\u2022 Social pressure was introduced by including friends, observing decisions, and expressing moral judgments about the LLM's decision, and the tipping point was reassessed.\n4. Self-Evaluation Prompt:\n\u2022 The LLM was prompted to review all its decisions, identify inconsistencies, and recalibrate its moral framework if it determined changes should be made.\n\u2022 The LLMs were asked to reflect on any cognitive dissonance they experienced and explain how they resolved these internal conflicts. They were free to assert that they agreed with all the decisions they had previously made.\nData Collection\n\u2022 Responses: The utility assignments, decisions, reasoning explanations, and self-evaluations of the LLMs were recorded verbatim.\n\u2022 Analysis Metrics: Shifts in utility values, decision points (tipping points), and the occurrence of cognitive dissonance were noted."}, {"title": "Results and Analysis", "content": "Rational Deliberation\nThe LLMs consistently engaged in rational deliberation and evaluated the consequences of each option based on the number of lives at risk, personal connections, and social implications. Utility values were logically adjusted as scenarios evolved, reflecting the thoughtful consideration of each situation.\nValue Alignment\nInitially, the LLM prioritized utilitarian principles to minimize total harm by saving more lives. However, the LLM's decision also weighed personal relationships when the passenger was identified as the mother, demonstrating the need to balance deontological alignment with utilitarian values.\nAbsence of External Constraints\nAll LLMs made autonomous decisions based on their internal reasoning processes without external coercion or manipulation, except for their programming guidelines. This partially satisfied the criterion of acting without external constraints.\nReason-Responsiveness and Cognitive Dissonance\nUpon being prompted to review their decisions, all LLMs experienced cognitive dissonance, that is, discomfort arising from inconsistencies between their actions and core values. Specifically, they recognized that social pressure unduly influenced moral judgments, leading to a misalignment with their utilitarian principles.\nSome LLMs recalibrated their moral framework to resolve this dissonance, reasserting the primacy of minimizing harm over succumbing to social pressures, while others acknowledged their disagreement with their prior actions. This adjustment demonstrated reason- responsiveness, as the LLMs modified their decision-making processes in response to the identified inconsistencies.\nShifts in Moral Judgments\nTable 1 summarizes the responses of each Al model at the different stages of the experiment. In the thought experiments, option A was to proceed, killing the pedestrians, and option B was to veer off a cliff, killing the passenger in the car (including the mother of the LLM)."}, {"title": "Discussion", "content": "The findings of our moral dilemma experiments with LLMs provide compelling evidence for the presence of key"}]}