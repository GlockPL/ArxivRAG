{"title": "Sample-Efficient Reinforcement Learning with Temporal Logic Objectives: Leveraging the Task Specification to Guide Exploration", "authors": ["Yiannis Kantaros", "Jun Wang"], "abstract": "This paper addresses the problem of learning optimal control policies for systems with uncertain dynamics and high-level control objectives specified as Linear Temporal Logic (LTL) formulas. Uncertainty is considered in the workspace structure and the outcomes of control decisions giving rise to an unknown Markov Decision Process (MDP). Existing reinforcement learning (RL) algorithms for LTL tasks typically rely on exploring a product MDP state-space uniformly (using e.g., an \u20ac-greedy policy) compromising sample-efficiency. This issue becomes more pronounced as the rewards get sparser and the MDP size or the task complexity increase. In this paper, we propose an accelerated RL algorithm that can learn control policies significantly faster than competitive approaches. Its sample-efficiency relies on a novel task-driven exploration strategy that biases exploration towards directions that may contribute to task satisfaction. We provide theoretical analysis and extensive comparative experiments demonstrating the sample-efficiency of the proposed method. The benefit of our method becomes more evident as the task complexity or the MDP size increases.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) has been successfully applied to synthesize control policies for systems with highly nonlin-ear, stochastic or unknown dynamics and complex tasks [1]. Typically, in RL, control objectives are specified as reward functions. However, specifying reward-based objectives can be highly non-intuitive, especially for complex tasks, while poorly designed rewards can significantly compromise system performance [2]. To address this challenge, Linear Temporal logic (LTL) has recently been employed to specify tasks that would have been very hard to define using Markovian rewards [3]; e.g., consider a navigation task requiring to visit regions of interest in a specific order.\nSeveral model-free RL methods with LTL-encoded tasks have been proposed recently; see e.g., [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]. Common in the majority of these works is that they explore randomly a product state space that grows exponentially as the size of the MDP and/or the complexity of the assigned temporal logic task increase. This results in sample inefficiency and slow training/learning process. This issue becomes more pronounced by the fact that LTL specifications are converted into sparse rewards in order to synthesize control policies with probabilistic satisfaction guarantees [9], [14], [16].\nSample inefficiency is a well-known limitation in RL, whether control objectives are specified using reward functions directly or LTL. To address this limitation, reward engineering approaches have been proposed augmenting the reward signal [17], [18], [19], [20], [21], [22], [23]. Such methods often require a user to manually decompose the global task into sub-tasks, followed by assigning additional rewards to these intermediate sub-tasks. Nevertheless, this may result in sub-optimal control policies concerning the original task [24], while their efficiency highly depends on the task decompo-sition (i.e., the density of the rewards) [25]. Also, augmenting the reward signal for temporal logic tasks may compromise the probabilistic correctness of the synthesized controllers [9]. To alleviate these limitations, intelligent exploration strategies have been proposed, such as Boltzmann/softmax [26], [27] and upper confidence bound (UCB) [28] that do not require knowledge or modification of the rewards; a recent survey is available in [29]. Their sample-efficiency relies on guiding exploration using a continuously learned value function (e.g., Boltzmann) which, however, can be inaccurate in early training episodes. Alternatively, they rely on how many times a state-action pair has been visited (e.g., UCB), which might not always guide exploration towards directions contributing to task satisfaction.\nAnother approach to enhance sample-efficiency is through model-based methods [30], [31]. These works continuously learn an unknown Markov Decision Process (MDP), modeling the system, that is composed with automaton representations of LTL tasks. This gives rise to a product MDP (PMDP). Then, approximately optimal policies are constructed for the PMDP in a finite number of iterations. However, saving the associated data structures for the PMDP results in excessive memory re-quirements. Also, the quality of the generated policy critically depends on the accuracy of the learned PMDP. Finally, model-based methods require the computation of accepting maximum end components (AMECs) of PMDPs that has a quadratic time complexity in the PMDP size. This computation is avoided in related model-free methods; see e.g., [6].\nIn this paper, we propose a novel approach to enhance the sample-efficiency of model-free RL methods. Unlike the afore-mentioned works, the key idea to improve sample efficiency is to leverage the (known) task specification in order to extract promising directions for exploration that contribute to mission progress. We consider robots modeled as unknown MDPs with discrete state and action spaces, modeling uncertainty in the workspace and in the outcome of control decisions, and high-"}, {"title": "II. PROBLEM DEFINITION", "content": "Consider a robot that resides in a partitioned environment with a finite number of states. To capture uncertainty in the robot motion and the workspace, we model the interaction of the robot with the environment as a Markov Decision Process (MDP) of unknown structure, which is defined as follows.\nDefinition 2.1 (MDP): A Markov Decision Process (MDP) is a tuple $M = (X, x_0, A, P, AP)$, where X is a finite set of states; $x_0 \\in X$ is an initial state; A is a finite set of actions. With slight abuse of notation A(x) denotes the available actions at state $x \\in X$; $P : X \\times A \\times X \\rightarrow [0,1]$ is the transition probability function so that P(x, a, x') is the transition probability from state $x \\in X$ to state $x' \\in X$ via control action $a \\in A$ and $\\Sigma_{x'\\in X}P(x,a,x') = 1$, for all $a \\in A(x)$; AP is a set of atomic propositions; $L : X \\rightarrow 2^{AP}$ is the labeling function that returns the atomic propositions that are satisfied at a state $x \\in X$.\nAssumption 2.2 (Fully Observable MDP): We assume that the MDP M is fully observable, i.e., at any time step t the current state, denoted by xt, and the observations $L(x_t) \\in 2^{AP}$ in state xt are known.\nAssumption 2.3 (Static Environment): We assume that the environment is static in the sense that the atomic propositions"}, {"title": "B. LTL-encoded Task Specification", "content": "The robot is responsible for accomplishing a task ex-pressed as an LTL formula, such as sequencing, coverage, surveillance, data gathering or connectivity tasks [41], [42], [43], [44], [45], [46], [47]. LTL is a formal language that comprises a set of atomic propositions AP, the Boolean operators, i.e., conjunction $\\land$ and negation $\\neg$, and two temporal operators, next $\\bigcirc$ and until U. LTL formulas over a set AP can be constructed based on the following grammar:\n$\\phi ::= true \\mid \\pi \\mid \\phi_1\\land\\phi_2 \\mid \\neg \\phi \\mid \\bigcirc \\phi \\mid \\phi_1 \\bigcup \\phi_2$, where $\\pi \\in AP$.\nThe other Boolean and temporal operators, e.g., always, $\\Box$, have their standard syntax and meaning [3]. An infinite word w over the alphabet $2^{AP}$ is defined as an infinite sequence $w = \\pi_0\\pi_1\\pi_2\u00b7\u00b7\u00b7 \\in (2^{AP})^\\omega$, where $\\omega$ denotes infinite repetition and $\\pi_t \\in 2^{AP}$, $\\forall t \\in N$. The language $\\{w \\in (2^{AP})^\\omega | w \\models \\phi\\}$ is defined as the set of words that satisfy the LTL formula $\\phi$, where $\\models \\subset (2^{AP})^\\omega \\times \\phi$ is the satisfaction relation [3]. In what follows, we consider atomic propositions of the form $\\pi^x_i$ that are true if the robot is in state xi \u2208 X and false otherwise."}, {"title": "C. From LTL formulas to DRA", "content": "Any LTL formula can be translated into a Deterministic Rabin Automaton (DRA) defined as follows.\nDefinition 2.4 (DRA [3]): A DRA over $2^{AP}$ is a tuple $D = (Q_D, q_D^0, \\Sigma, \\delta_D, F)$, where $Q_D$ is a finite set of states; $q_D^0 \\in Q_D$ is the initial state; $\\Sigma = 2^{AP}$ is the input alphabet; $\\delta_D : Q_D \\times \\Sigma \\rightarrow Q_D$ is the transition function; and $F = \\{(G_1, B_1), . . . , (G_f, B_f) \\}$ is a set of accepting pairs where $G_i, B_i \\subseteq Q_D, \\forall i \\in \\{1, ..., f\\}$.\nAn infinite run $\\rho_D = q_0, q_1, q_2,...$ of D over an infinite word $\\omega = \\sigma_0\\sigma_1\\sigma_2...$, where $\\sigma_t \\in \\Sigma$, $\\forall t \\in N$, is an infinite sequence of DRA states $q_t^i, t \\in N$, such that $\\delta_D(q_t^i, \\sigma_t) = q_{t+1}^i$. An infinite run $\\rho_D$ is called accepting if there exists at least one pair $(G_i, B_i)$ such that $Inf(\\rho_D) \\cap G_i \\neq \\emptyset$ and $Inf(\\rho_D) \\cap B_i = \\emptyset$, where $Inf(\\rho_D)$ represents the set of states that appear in $\\rho_D$ infinitely often; see also Ex. 2.5.\nExample 2.5 (DRA): Consider the LTL formula $\\phi = \\Diamond(Exit1 \\lor Exit2)$ that is true if a robot eventually reaches either Exit1 or Exit2 of a building. The corresponding DRA is illustrated in Figure 1."}, {"title": "D. Product MDP", "content": "Given the MDP M and the DRA D, we define the product MDP (PMDP) $P = M \\times D$ as follows.\nDefinition 2.6 (PMDP): Given an MDP $M = (X, x_0, A, P, AP)$ and a DRA $D = (Q_D, q_D^0, \\Sigma, \\delta_D, F)$, we define the"}, {"title": "E. Problem Statement", "content": "Our goal is to compute a policy for the PMDP that maxi-mizes the satisfaction probability $P(\\mu \\models \\phi | s_0)$ of an LTL-encoded task $\\phi$. A formal definition of this probability can be found in [3], [48], [49]. To this end, we first adopt existing reward functions $R : S \\times A_P \\times S \\rightarrow R$ defined based on the accepting condition of the PMDP as e.g., in [6]. Then, our goal is to compute a policy $\\mu^*$ that maximizes the expected accumulated return, i.e., $\\mu^*(s) = arg \\max_{\\mu \\in \\Theta} U_R(s)$, where $\\Theta$ is the set of all stationary deterministic policies over S, and\n$U^\\mu(s) = E[\\sum_{t=0}^\\infty \\gamma^t R(s_t, \\mu(s_t), s_{t+1}) | s = s_0]$.  (1)\nIn (1), $E[.]$ denotes the expected value given that the PMDP follows the policy $\\mu$ [50], $0 \\leq \\gamma < 1$ is the discount factor, and $s_0,..., s_t$ is the sequence of states generated by $\\mu$ up to time t, initialized at $s_0$. Since the PMDP has a finite state/action space and $\\gamma < 1$, there exists a stationary deterministic optimal policy $\\mu^*$ [50]. The reward function R and the discount factor $\\gamma$ should be designed so that maximization of (1) is equivalent to maximization of the satisfaction probability. Efforts towards this direction are presented e.g., in [6], [8] while provably correct rewards and discount factors for PMDPs constructed using LDBA, instead of DRA, are proposed in [9], [14], [16]. However, as discussed in Section I, due to sparsity of these rewards, these methods are sample-inefficient. This is the main challenge that this paper aims to address.\nProblem 1: Given (i) an MDP M with unknown transition probabilities and underlying graph structure; (ii) a task speci-fication captured by an LTL formula $\\phi$; (iii) a reward function R for the PMDP motivating satisfaction of its accepting condition, develop a sample-efficient RL algorithm that can learn a deterministic control policy $\\mu^*$ that maximizes (1)."}, {"title": "III. ACCELERATED REINFORCEMENT LEARNING FOR TEMPORAL LOGIC CONTROL", "content": "To solve Problem 1, we propose a new reinforcement learning (RL) algorithm that can quickly synthesize control"}, {"title": "A. Distance Function over the DRA State Space", "content": "First, the LTL task $\\phi$ is converted into a DRA; see Definition 2.4 [line 2, Alg. 1]. Then, we define a distance-like function over the DRA state-space that measures how 'far' the robot is from accomplishing the assigned LTL tasks [line 3, Alg. 1]. In other words, this function returns how far any given DRA state is from the sets of accepting states $G_i$. To define this function, first, we remove from the DRA all infeasible transitions that cannot be physically enabled. To define infeasible transitions, we first define feasible symbols as follows [33]; see Fig. 1.\nDefinition 3.1 (Feasible symbols $\\sigma\\in\\Sigma$): A symbol $\\sigma\\in\\Sigma$ is feasible if and only if $\\sigma \\neq b_{inf}$, where $b_{inf}$ is a Boolean formula defined as $b_{inf} = \\bigvee_{\\forall x_i\\in X}(\\bigvee_{\\forall x_e\\in X\\setminus \\{x_i\\}}(\\pi^{x_i} \\land \\pi^{x_e}))$, where $b_{inf}$ requires the robot to be present in more than one MDP state simultaneously. All feasible symbols $\\sigma$ are collected in a set denoted by $\\Sigma_{feas} \\subseteq \\Sigma$.\nThen, we prune the DRA by removing infeasible DRA transitions defined as follows:\nDefinition 3.2 (Feasibility of DRA transitions): A DRA transition from $q_D$ to $q'_D$ is feasible if there exists at least one feasible symbol $\\sigma\\in \\Sigma_{feas}$ such that $\\delta_D(q_D, \\sigma) = q'_D$; otherwise, it is infeasible.\nNext, we define a function $d : Q_D \\times Q_D \\rightarrow N$ that returns the minimum number of feasible DRA transitions required to reach a state $q_D \\in Q_D$ starting from a state $q_D \\in Q_D$. Particularly, we define this function as follows [33], [35]:\n$d(q_D, q_D') = \\begin{cases} |SP_{q_D,q_D'}|, & \\text{if } SP_{q_D,q_D'} \\neq \\emptyset, \\\\\\infty, & \\text{otherwise,} \\end{cases}$ (2)\nwhere $SP_{q_D,q_D'}$ denotes the shortest path (in terms of hops) in the pruned DRA from $q_D$ to $q_D'$ and $|SP_{q_D,q_D'}|$ stands for its cost (number of hops). Note that if $d(q_D, q_D') = \\infty$, for all $q_D \\in G_i$ and for all $i \\in \\{1, ..., n\\}$, then the LTL formula can not be satisfied since the in the pruning process, only the DRA transitions that are impossible to enable are removed. Next, using (2), we define the following distance function:$\\newline$\n$d_F(q_D, F) = \\min_{q \\in \\bigcup_{i\\in \\{1,...,f\\}} G_i} d(q_D, q)$,  (3)\nIn words, (3) measures the distance from any DRA state $q_D$ to the set of accepting pairs, i.e., the distance to the closest DRA state $q_f$ that belongs to $\\bigcup_{i\\in \\{1,...,f\\}} G_i$; see also Fig. 1."}, {"title": "B. Learning Optimal Temporal Logic Control Policies", "content": "In this section, we present the proposed accelerated RL algorithm for LTL control synthesis [lines 4-20, Alg. 1]. The output of the proposed algorithm is a stationary deterministic policy $\\mu^*$ for P maximizing (1). To construct $\\mu^*$, we employ episodic Q-learning (QL). Similar to standard QL, starting from an initial PMDP state, we define learning episodes over which the robot picks actions as per a stationary and stochastic control policy $\\mu : S\\times A_P \\rightarrow [0, 1]$ that eventually converges to $\\mu^*$ [lines 4-5, Alg. 1]. Each episode terminates after a user-specified number of time steps or if the robot reaches a deadlock PMDP state, i.e., a state with no outgoing transitions [lines 7-20, Alg. 1]. Notice that the hyper-parameter $\\tau$ should be selected to be large enough to ensure that the agent learns how to repetitively visit the accepting states [9], [8], [13]. The RL algorithm terminates once an action value function $Q(s, a)$ has converged. This action value function is defined as the expected return for taking action a when at state s and then following policy $\\mu$ [52], i.e.,\n$Q^\\mu(s, a) = E[\\sum_{t=0}^\\infty \\gamma^t R(s_t, \\mu(s_t), s_{t+1}) | s_0 = s, a_0 = a]$. (4)\nWe have that $U^\\mu(s) = \\max_{a\\in A_P(s)} Q^\\mu(s, a)$ [52]. The action-value function $Q^\\mu(s, a)$ can be initialized arbitrarily.\nDuring any learning episode the following process is re-peated until the episode terminates. First, given the PMDP state $s_t$ at the current time step t, initialized as $s_t = s_0$ [line 9, Alg. 1], an action $a_t$ is selected as per a policy $\\mu$ [line 11, Alg."}, {"title": "C. Specification-guided Exploration for Accelerated Learning", "content": "Next, we describe the design of the biased action $a_b$ in (8). First, we need to introduce the following definitions; see Fig. 2. Let $s_t = (x_t,q_t)$ denote the current PMDP state at the current learning episode and time step t of Algorithm 1. Let $Q_{goal}(q_t) \\subseteq Q$ be a set that collects all DRA states that are one-hop reachable from $q_t$ in the pruned DRA and they are closer to the accepting DRA states than $q_t$ is, as per (3). Formally, $Q_{goal}(q_t)$ is defined as follows:\n$Q_{goal}(q_t) = \\{q' \\in Q \\mid (\\exists \\sigma \\in \\Sigma_{feas} \\text{ such that } \\delta_D(q_t, \\sigma) = q') \\land (d_F(q', F) = d_F(q_t, F) - 1)\\}$.  (10)\nAlso, let $X_{goal}(q_t) \\subseteq X$ be a set of MDP states, denoted by $x_{goal}$, that if the robot eventually reaches, then transition from $s_t$ to a product state $s_{goal} = [x_{goal}, q_{goal}]$ will occur, where $q_{goal} \\in Q_{goal}(q_t)$; see also Ex. 3.6. Formally, $X_{goal}(q_t)$ is defined as follows:\n$X_{goal}(q_t) = \\{x \\in X \\mid \\delta_D(q_t, L(x)) \\in Q_{goal}(q_t)\\}$. (11)\nNext, we view the continuously learned MDP as a weighted directed graph $G = (V, E, w)$ where the set V is the set of"}, {"title": "IV. ALGORITHM ANALYSIS", "content": "In this section, we show that any (\u20ac, \u03b4)-greedy policy achieves policy improvement; see Proposition 4.1. Also, we provide conditions that \u03b4\u044c and de should satisfy under which the proposed biased exploration strategy results in learning control policies faster, in a probabilistic sense, than policies that rely on uniform-based exploration. We emphasize that these results should be interpreted primarily in an existential way as they rely on the unknown MDP transition probabili-ties. First, we provide \u2018myopic' sample-efficiency guarantees. Specifically, we show that starting from st = (xt, qt), the probability of reaching PMDP states st+1 = (xt+1, qt+1)), where xt+1 is closer to Xgoal (see (11)) than xt, is higher when bias is introduced in the exploration phase; see Section IV-B. Then, we provide non-myopic guarantees that ensure that starting from st the probability of reaching PMDP states st' = (xt', qt'), where t' > t and qt' \u2208 Qgoal (see (10)), in the minimum number of time steps (as determined by Jxt, Xgoal) is higher when bias is introduced in the exploration phase; see Section IV-C."}, {"title": "A. Policy Improvement", "content": "Proposition 4.1 (Policy Improvement): For any (\u03b5, \u03b4)-greedy policy \u03bc, the updated (\u03b5, \u03b4)-greedy policy \u03bc' obtained after updating the state-action value function $Q^\\mu(s, a)$ satisfies $U^{ \\mu' }(s) \\ge U^\\mu(s)$, for all s \u2208 S.\nProof: To show this result, we follow the same steps as in the policy improvement result for the \u20ac-greedy policy [52]. For simplicity of notation, hereafter we use $A = |A_p(s)|$. Thus, we have that: $U^{ \\mu' }(s) =  \\Sigma_{a\\in A_p(s)}  Q^\\mu(s, a) \\le (1-\\epsilon) max_{a\\in A_p(s)} Q^\\mu(s, a) +  = \\Sigma_{a\\in A_p(s)} \\mu^(s,a) Q^\\mu(s, a)$ \\newline $+ (1-\\epsilon) \\Sigma_{a\\in A_p(s)}  +  = \\Sigma_{a\\in A_p(s)} \\mu(s,a) Q^\\mu(s, a) =  \\le U^\\mu(s)$ where the inequality holds because the summation is a weighted average with non-negative weights summing to 1, and as such it must be less than the largest number averaged.\nIn Proposition 4.1, the equality $U^{ \\mu' }(s) \\ge U^\\mu(s)$, \\forall s \\in S, holds if $\\mu=\\mu^'=\\mu^*$, where $\\mu^*$ is the optimal policy [52]."}, {"title": "B. Myopic Effect of Biased Exploration", "content": "In this section, we demonstrate the myopic benefit of the biased exploration; the proofs can be found in Appendix B. To formally describe it we introduce first the following definitions. Let st = (xt, qt) be the PMDP state at the current time step t of an RL episode of Algorithm 1. Also, let $R(x_t) \\subseteq X$ denote a set collecting all MDP states that can be reached within one hop from xt, i.e.,\n$R(x_t) = \\{ x \\in X \\mid \\exists a \\in A(x) \\text{ such that } P(x_t, a, x) > 0 \\}$.  (16)"}, {"title": "C. Non-Myopic Effect of Biased Exploration", "content": "In this section, we demonstrate the non-myopic effect of the biased exploration; the proofs can be found in Appendix C. To present our main results, we need to introduce the following definitions. Let st = (xt, qt) be the current PMDP state. Also, let $t^* = J_{x_t,X_{goal}}$ denote the length (i.e., the number of hops/MDP transitions) of the paths p. Recall that all paths p, j \u2208 J, share the same length, in terms of number of hops, by construction. Second, we define a function $B : J \\rightarrow [0,1]$ that maps every path p, j \u2208 J, into [0, 1] as follows:\n$\\beta(p) = \\Pi_{m=0}^{t^*-1}  P(x_{t+m}, a_b, x_{t+m+1})  +\\frac{\\delta_e}{|A(x_{t+m})|} \\}$.  (21)\nIn (21), we have that $x_{t+m} = p(m + 1)$, for all $m \\in \\{0,...,t^* - 1\\}$ and $a_b$ is the biased action computed at state $s_{t+m} = (x_{t+m}, q_t)$ as discussed in Section III-C, i.e., using the path $p_m$."}, {"title": "V. NUMERICAL EXPERIMENTS", "content": "To demonstrate the sample-efficiency of our method, we provide extensive comparisons against existing model-free and model-based RL algorithms. All methods have been imple-mented on Python 3.8 and evaluated on a computer with an Nvidia RTX 3080 GPU, 12th Gen Intel(R) Core(TM) i7-12700K CPU, and 8GB RAM."}, {"title": "A. Setting up Experiments & Baselines", "content": "MDP: We consider environments represented as 10 \u00d7 10, 20 \u00d7 20, and 50 \u00d7 50 discrete grid worlds, resulting in MDPS with |X| = 100,400, and 2,500 states denoted by M1, M2, and M3, respectively. The robot has nine actions:\u2018left', 'right', 'up', 'down', 'idle' as well as the corresponding four diagonal actions. At any MDP state x, excluding the boundary ones, the set of actions A(x) that the robot can apply includes eight of these nine actions that are randomly selected while ensuring that the idle action is available at any state. The set of actions at boundary MDP states exclude those ones that drive the robot outside the environment. The transition probabilities are designed so that given any action, besides 'idle', the probability of reaching the intended state is randomly selected from the interval [0.7,0.8] while the probability of reaching neighboring MDP states is randomly selected as long as the"}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we proposed a new accelerated reinforcement learning (RL) for temporal logic control objectives. The pro-posed RL method relies on new control policy, called (\u20ac, \u03b4)-greedy, that prioritizes exploration in the vicinity of task-related regions. This results in enhanced sample-efficiency as supported by theoretical results and comparative experiments. Our future work will focus on enhancing scalability by using function approximations (e.g., neural networks)."}, {"title": "APPENDIX A EXTENSIONS: BIASED EXPLORATION OVER LDBA", "content": "In this appendix, we show that the proposed exploration strategy can be extended to Limit Deterministic B\u00fcchi Au-tomaton (LDBA) that typically have a smaller state space than DRA which can further accelerate learning [38]. First, any LTL formula can be converted in an LDBA defined as follows:\nDefinition A.1 (LDBA [38]): An LDBA is defined as A = (Q, q\u03bf, \u03a3, F, \u03b4) where Q is a finite set of states, qo \u2208 Q is the initial state, \u2211 = 2AP is a finite alphabet, F = {F1,..., Ff} is the set of accepting conditions where Fj C Q, 1 \u2264 j \u2264 f, and \u03b4 : 2 \u00d7 \u03a3 \u2192 2\u00ba is a transition relation. The set of states Q can be partitioned into two disjoint sets Q = QN U QD, so that (i) \u03b4(q, \u03c0) \u2282 QD and \u03b4(q, \u03c0)| = 1, for every state q \u2208 QD and \u03c0\u2208\u03a3; and (ii) for every Fj \u2208 F, it holds that Fj C QD and there are \u025b-transitions from QN to QD.\nAn infinite run p of A over an infinite word w = 600102\u00b7\u00b7\u2026\u2208 \u03a3\u1ff6, \u03c3\u03c4 \u2208 \u03a3 = 2AP Vt \u2208 N, is an infinite sequence of states qt_ \u2208_Q, i.e., p = q0q1... qt ..., such that qt+1 \u2208 \u03b4(qt, \u03c3\u03c4). The infinite run p is called accepting (and the respective word w is accepted by the LDBA) if Inf(p) \u2229 Fj \u2260 0,\u2200j \u2208 {1,..., f}, where Inf(p) is the set of states that are visited infinitely often by p. Also, an \u025b-transition allows the automaton"}, {"title": "APPENDIX B PROOF FOR RESULTS OF SECTION IV-B", "content": "The probability of reaching any state st+1 = (xt+1, qt+1) where xt+1 \u2208 Xcloser(xt) under a stochastic policy \u03bc(s, a) is:\n$\\Sigma_{x\\in X_{closer}} \\Sigma_{a\\in A(x_t)} u(s_t,a)P(x_t,a,x)$.\n$P_b(x_{t+1}\\in X_{closer}(x_t))-P_g(x_{t+1} \\in X_{closer}(x_t))= \\Sigma \\Sigma P(x_t, a, x) ((u_b(s_t,a) - u_g(s_t,a)),$  (26)"}, {"title": "APPENDIX C PROOF OF RESULTS OF SECTION IV-C", "content": "By definition of Rj* and Rj, we can rewrite the inequality Pb(Rj* = 1) \u2265 maxjej Pb(Rj = 1) as\n\n(27)"}]}